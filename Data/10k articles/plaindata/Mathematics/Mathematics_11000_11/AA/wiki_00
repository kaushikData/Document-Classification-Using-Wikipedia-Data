{"id": "42617238", "url": "https://en.wikipedia.org/wiki?curid=42617238", "title": "2-factor theorem", "text": "2-factor theorem\n\nIn the mathematical discipline of graph theory, 2-factor theorem discovered by Julius Petersen, is one of the earliest works in graph theory and can be stated as follows:\n\nIn order to prove this generalized form of the theorem, Petersen first proved that a 4-regular graph can be factorized into two 2-factors by taking alternate edges in a Eulerian trail. He noted that the same technique used for the 4-regular graph yields a factorization of a 2\"k\"-regular graph into two \"k\"-factors.\nTo prove this theorem, first we remark that it is sufficient to consider connected graphs. A connected graph with even degree has an Eulerian trail. Traversing this Eulerian trail we get an orientation \"D\" of \"G\" such that every point has indegree and outdegree = \"k\". Then we replace every point \"v\" ϵ \"V\"(\"D\") by two points \"v’\", \"v”\", and for every directed line \"uv\" ϵ \"E\"(\"D\") we draw in one line from \"u’\" to \"v”\". Since \"D\" has in- and outdegree equal to \"k\" the resulting bigraph \"G’\" is \"k\"-regular. The lines of \"G’\" can be decomposed into \"k\" perfect matchings. Now if we identify \"v’\" and \"v”\" for every \"v\", we recover the graph \"G\", and these \"k\" perfect matchings of \"G’\" will be mapped onto \"k\" 2-factors of \"G\" which partition the lines.\n\nIn other words, for the general case the factorization of a 2\"k\"-regular graph into two factors is an easy consequence of Euler’s theorem: By applying the same argument to each component, we may assume that G is connected and 2\"k\"-regular with vertices \"v\", ..., \"v\". Let \"C\" be an Eulerian circuit of \"G\", followed in a particular direction. Then use Eulerian circuit of \"G\" to create an supplementary \"k\"-regular bipartite graph H, such that a perfect matching in \"H\" corresponds to a 2-factor in \"G\".\n\nThe theorem was discovered by Julius Petersen, a Danish mathematician. It is in fact, one of the first results in graph theory. The theorem appears first in the 1891 article \"Die Theorie der regulären graphs\". To prove the theorem Petersen's fundamental idea was to 'colour' the edges of a trial or a path alternatingly red and blue, and then to use the edges of one or both colours for the construction of other paths or trials.\n"}
{"id": "2296389", "url": "https://en.wikipedia.org/wiki?curid=2296389", "title": "223 (number)", "text": "223 (number)\n\n223 (two hundred [and] twenty-three) is the natural number between 222 and 224.\n\n223 is a prime number. It is the smallest prime for which the two nearest primes on either side of it are 16 units apart. Among the 720 permutations of the numbers from 1 to 6, exactly 223 of them have the property that at least one of the numbers is fixed in place by the permutation and the numbers less than it and greater than it are separately permuted among themselves.\n\nIn connection with Waring's problem, 223 requires the maximum number of terms (37 terms) when expressed as a sum of positive fifth powers, and is the only number that requires that many terms.\n\n"}
{"id": "341671", "url": "https://en.wikipedia.org/wiki?curid=341671", "title": "40 (number)", "text": "40 (number)\n\n40 (forty) is the natural number following 39 and preceding 41.\n\nThough the word is related to \"four\" (4), the spelling \"forty\" replaced \"fourty\" in the course of the 17th century and is now the standard form.\n\n\n\n\nThe number 40 is used in Jewish, Christian, Islamic, and other Middle Eastern traditions to represent a large, approximate number, similar to \"umpteen\".\n\n\nChristianity similarly uses forty to designate important time periods.\n\n\n\n\n\n\nIn the Hindu system some of the popular fasting periods consist 40 days and is called the period One 'Mandl kal' Kal means a period and Mandal kal means a period of 40 days. For example, the devotees of Swami Ayyappa, the name of a Hindu god very popular in Kerala, India (Sabarimala Swami Ayyappan) strictly observed forty days fasting and visit (only male devotees are permitted to enter into the god's temple) with their holy submittance or offerings on 41st or a convenient day after a minimum 40 days practice of fasting. The offering is called \"Kanikka\".\n\nA large number of myths about Enki have been collected from many sites, stretching from Southern Iraq to the Levantine coast. He figures in the earliest extant cuneiform inscriptions throughout the region and was prominent from the third millennium down to Hellenistic times.\nThe exact meaning of his name is uncertain: the common translation is \"Lord of the Earth\": the Sumerian en is translated as a title equivalent to \"lord\"; it was originally a title given to the High Priest; ki means \"earth\"; but there are theories that ki in this name has another origin, possibly kig of unknown meaning, or kur meaning \"mound\". The name Ea is allegedly Hurrian in origin while others claim that it is possibly of Semitic origin and may be a derivation from the West-Semitic root *hyy meaning \"life\" in this case used for \"spring\", \"running water.\" In Sumerian E-A means \"the house of water\", and it has been suggested that this was originally the name for the shrine to the God at Eridu.\n\n\n\nForty is also:\n"}
{"id": "1857196", "url": "https://en.wikipedia.org/wiki?curid=1857196", "title": "AF-heap", "text": "AF-heap\n\nIn computer science, the AF-heap is a type of priority queue for integer data, an extension of the fusion tree using an atomic heap proposed by M. L. Fredman and D. E. Willard.\n\nUsing an AF-heap, it is possible to perform insert or decrease-key operations and delete-min operations on machine-integer keys in time . This allows Dijkstra's algorithm to be performed in the same time bound on graphs with edges and vertices, and leads to a linear time algorithm for minimum spanning trees, with the assumption for both problems that the edge weights of the input graph are machine integers in the transdichotomous model.\n\n"}
{"id": "974181", "url": "https://en.wikipedia.org/wiki?curid=974181", "title": "Abelian surface", "text": "Abelian surface\n\nIn mathematics, an abelian surface is 2-dimensional abelian variety.\n\nOne-dimensional complex tori are just elliptic curves and are all algebraic, but Riemann discovered that most complex tori of dimension 2 are not algebraic. The algebraic ones are called abelian surfaces and are exactly the 2-dimensional abelian varieties.\nMost of their theory is a special case of the theory of higher-dimensional tori or abelian varieties. Criteria to be a product of two elliptic curves (up to isogeny) were a popular study in the nineteenth century.\n\nInvariants: The plurigenera are all 1. The surface is diffeomorphic to \"S\"×\"S\"×\"S\"×\"S\" so the fundamental group is Z.\n\nHodge diamond:\nExamples: A product of two elliptic curves. The Jacobian variety of a genus 2 curve.\n\n\n"}
{"id": "49637474", "url": "https://en.wikipedia.org/wiki?curid=49637474", "title": "Aksharapalli", "text": "Aksharapalli\n\nAksharapalli (Akṣarapallī) is a certain type of alphasyllabic numeration scheme extensively used in the pagination of manuscripts produced in India in pre-modern times. The name \"Aksharapalli\" can be translated as the \"letter system\". In this system the letters or the syllables of the script in which the manuscript is written are used to denote the numbers. In contrast to the Aksharapalli system, the ordinary decimal system is called the \"Ankapalli\" system.\n\nThe following tables give examples of syllables used to represent numerals. The lists are not exhaustive.\n\nWhen the Aksharapalli system is used, the various syllables that constitute a number are placed one below the other as in the Chinese language and they are written in the margins of the various leaves of the manuscript. This arrangement may be the consequence an attempt to save space for the contents of the manuscript. This method can be seen in the earliest available manuscript containing the Aksharapalli system which is a manuscript of sixth century CE.\n\nNothing definitely is known about the origin of the system. It is conjectured that the system might have evolved from the ciphered-additive numeral system of Brahmi. This system has been extensively used in Jain manuscripts up to sixteenth century. The system has also survived for a long time in Nepal. The system was in use as late as the nineteenth century in those regions of India which now constitute the Kerala State.\n"}
{"id": "16334749", "url": "https://en.wikipedia.org/wiki?curid=16334749", "title": "Algorithmic game theory", "text": "Algorithmic game theory\n\nAlgorithmic game theory is an area in the intersection of game theory and computer science, whose objective is to understand and design algorithms in strategic environments. Typically, in Algorithmic Game Theory problems, the input to a given algorithm is distributed among many players who have a personal interest in the output. In those situations, the agents might not report the input truthfully because of their own personal interests. On top of the usual requirements in classical algorithm design, say \"polynomial-time running time\", \"good approximation ratio\", ... the designer must also care about incentive constraints. We can see Algorithmic Game Theory from two perspectives:\n\n\nIn 1999, the seminal paper of Nisan and Ronen drew the attention of the Theoretical Computer Science community to designing algorithms for selfish (strategic) users. As they claim in the abstract:\n\nThis paper coined the term algorithmic mechanism design and was recognized by the 2012 Gödel Prize committee as one of \"three papers laying foundation of growth in Algorithmic Game Theory\".\n\nThe other two papers cited in the 2012 Gödel Prize for fundamental contributions to Algorithmic Game Theory introduced and developed the concept of \"Price of Anarchy\". \nIn their 1999 paper \"Worst-case Equilibria\", Koutsoupias and Papadimitriou proposed a new measure of the degradation of system efficiency due to the selfish behavior of its agents: the ratio of between system efficiency at an optimal configuration, and its efficiency at the worst Nash equilibrium. \nThe Internet created a new economy—both as a foundation for exchange and commerce, and in its own right. The computational nature of the Internet allowed for the use of computational tools in this new emerging economy. On the other hand, the Internet itself is the outcome of actions of many. This was new to the classic, ‘top-down’ approach to computation that held till then. Thus, game theory is a natural way to view the Internet and interactions within it, both human and mechanical.\n\nGame theory studies equilibria (such as the Nash equilibrium). An equilibrium is generally defined as a state in which no player has an incentive to change their strategy. Equilibria are found in several fields related to the Internet, for instance financial interactions and communication load-balancing. Game theory provides tools to analyze equilibria, and a common approach is then to ‘find the game’—that is, to formalize specific Internet interactions as a game, and to derive the associated equilibria.\n\nRephrasing problems in terms of games allows the analysis of Internet-based interactions and the construction of mechanisms to meet specified demands. If equilibria can be shown to exist, a further question must be answered: can an equilibrium be found, and in reasonable time? This leads to the analysis of algorithms for finding equilibria. Of special importance is the complexity class PPAD, which includes many problems in algorithmic game theory.\n\nThe main areas of research in algorithmic game theory include:\n\n\nAnd the area counts with diverse practical applications:\n\n\n\n\n"}
{"id": "52773150", "url": "https://en.wikipedia.org/wiki?curid=52773150", "title": "Algorithmic transparency", "text": "Algorithmic transparency\n\nAlgorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are impacted by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.\n\nThe phrases \"algorithmic transparency\" and \"algorithmic accountability\" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, \"algorithmic transparency\" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair. \"Algorithmic accountability\" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.\n\nCurrent research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a \"right to explanation\" of decisions made by algorithms, though it is unclear what this means.\n\n"}
{"id": "25830260", "url": "https://en.wikipedia.org/wiki?curid=25830260", "title": "Annulus theorem", "text": "Annulus theorem\n\nIn mathematics, the annulus theorem (formerly called the annulus conjecture) states roughly that the region between two well-behaved spheres is an annulus. It is closely related to the stable homeomorphism conjecture (now proved) which states that every orientation-preserving homeomorphism of Euclidean space is stable.\n\nIf \"S\" and \"T\" are topological spheres in Euclidean space, with \"S\" contained in \"T\", then it is not true in general that the region between them is an annulus, because of the existence of wild spheres in dimension at least 3. So the annulus theorem has to be stated to exclude these examples, by adding some condition to ensure that \"S\" and \"T\" are well behaved. There are several ways to do this.\n\nThe annulus theorem states that if any homeomorphism \"h\" of R to itself maps the unit ball \"B\" into its interior, then \"B\" − \"h\"(interior(\"B\")) is homeomorphic to the annulus S×[0,1].\n\nThe annulus theorem is trivial in dimensions 0 and 1. It was proved in dimension 2 by , in dimension 3 by , in dimension 4 by , and in dimensions at least 5 by .\n\nA homeomorphism of R is called stable if it is a product of homeomorphisms each of which is the identity on some non-empty open set. \nThe stable homeomorphism conjecture states that every orientation-preserving homeomorphism of R is stable. previously showed that the stable homeomorphism conjecture is equivalent to the annulus conjecture, so it is true.\n\n"}
{"id": "630741", "url": "https://en.wikipedia.org/wiki?curid=630741", "title": "Arithmetic group", "text": "Arithmetic group\n\nIn mathematics, an arithmetic group is a group obtained as the integer points of an algebraic group, for example formula_1 They arise naturally in the study of arithmetic properties of quadratic forms and other classical topics in number theory. They also give rise to very interesting examples of Riemannian manifolds and hence are objects of interest in differential geometry and topology. Finally, these two topics join in the theory of automorphic forms which is fundamental in modern number theory.\n\nOne of the origins of the mathematical theory of arithmetic groups is algebraic number theory. The classical reduction theory of quadratic and Hermitian forms by Charles Hermite, Hermann Minkowski and others can be seen as computing fundamental domains for the action of certain arithmetic groups on the relevant symmetric spaces. The topic was related to Minkowski's Geometry of numbers and the early development of the study of arithmetic invariant of number fields such as the discriminant. Arithmetic groups can be thought of as a vast generalisation of the unit groups of number fields to a noncommutative setting.\n\nThe same groups also appeared in analytic number theory as the study of classical modular forms and their generalisations developed. Of course the two topics were related, as can be seen for example in Langlands' computation of the volume of certain fundamental domains using analytic methods. This classical theory culminated with the work of Siegel, who showed the finiteness of the volume of a fundamental domain in many cases.\n\nFor the modern theory to begin foundational work was needed, and was provided by the work of Armand Borel, André Weil, Jacques Tits and others on algebraic groups. Shortly afterwards the finiteness of covolume was proven in full generality by Borel and Harish-Chandra. Meanwhile, there was progress on the general theory of lattices in Lie groups by Atle Selberg, Grigori Margulis and David Kazhdan, M. S. Raghunathan and others. The state of the art after this period was essentially fixed in Raghunathan's treatise, published in 1972.\n\nIn the seventies Margulis revolutionised the topic by proving that in \"most\" cases the arithmetic constructions account for all lattices in a given Lie group. Some limited results in this direction had been obtained earlier by Selberg, but Margulis' methods (the use of ergodic-theoretical tools for actions on homogeneous spaces) were completely new in this context and were to be extremely influential on later developments, effectively renewing the old subject of geometry of numbers and allowing Margulis himself to prove the Oppenheim conjecture; stronger results (Ratner's theorems) were later obtained by Marina Ratner.\n\nIn another direction the classical topic of modular forms has blossomed into the modern theory of automorphic forms. The driving force behind this effort is mainly the Langlands program initiated by Robert Langlands. One of the main tool used there is the trace formula originating in Selberg's work and developed in the most general setting by James Arthur.\n\nFinally arithmetic groups are often used to construct interesting examples of locally symmetric Riemannian manifolds. A particularly active research topic has been arithmetic hyperbolic 3-manifolds, which as William Thurston wrote, \"...often seem to have special beauty.\"\n\nIf formula_2 is an algebraic subgroup of formula_3 for some formula_4 then we can define an arithmetic subgroup of formula_5 as the group of integer points formula_6 In general it is not so obvious how to make precise sense of the notion of \"integer points\" of a formula_7-group, and the subgroup defined above can change when we take different embeddings formula_8\n\nThus a better notion is to take for definition of an arithmetic subgroup of formula_5 any group formula_10 which is commensurable (this means that both formula_11 and formula_12 are finite sets) to a group formula_13 defined as above (with respect to any embedding into formula_14). With this definition, to the algebraic group formula_2 is associated a collection of \"discrete\" subgroups all commensurable to each other.\n\nA natural generalisation of the construction above is as follows: let formula_16 be a number field with ring of integers formula_17 and formula_2 an algebraic group over formula_16. If we are given an embedding formula_20 defined over formula_16 then the subgroup formula_22 can legitimately be called an arithmetic group.\n\nOn the other hand, the class of groups thus obtained is not larger than the class of arithmetic groups as defined above. Indeed, if we consider the algebraic group formula_23 over formula_7 obtained by restricting scalars from formula_16 to formula_7 and the formula_27-embedding formula_28 induced by formula_29 (where formula_30) then the group constructed above is equal to formula_31.\n\nThe classical example of an arithmetic group is formula_32, or the closely related groups formula_33, formula_34 and formula_35. For formula_36 the group formula_37, or sometimes formula_32, is called the modular group as it is related to the modular curve. Similar examples are the Siegel modular groups formula_39.\n\nOther well-known and studied examples include the Bianchi groups formula_40 where formula_41 is a square-free integer and formula_42 is the ring of integers in the field formula_43 and the Hilbert—Blumenthal modular groups formula_44.\n\nAnother classical example is given by the integral elements in the orthogonal group of a quadratic form defined over a number field, for example formula_45. A related construction is by taking the unit groups of orders in quaternion algebras over number fields (for example the Hurwitz quaternion order). Similar constructions can be performed with unitary groups of hermitian forms, a well-known example is the Picard modular group.\n\nWhen formula_46 is a Lie group one can define an arithmetic lattice in formula_46 as follows: for any algebraic groups formula_2 defined over formula_7 such that there is a morphism formula_50 with compact kernel, the image of an arithmetic subgroup in formula_5 is an arithmetic lattice in formula_46. Thus, for example, if formula_53 and formula_46 is a subgroup of formula_55 then formula_56 is an arithmetic lattice in formula_46 (but there are many more, corresponding to other embeddings); for instance, formula_32 is an arithmetic lattice in formula_59.\n\nA lattice in a Lie group is usually defined as a discrete subgroup with finite covolume. The terminology introduced above is coherent with this, as a theorem due to Borel and Harish-Chandra states that an arithmetic subgroup in a semisimple Lie group is of finite covolume (the discreteness is obvious).\n\nThe theorem is more precise: it tells that the arithmetic lattice is cocompact if and only if the \"form\" of formula_46 used to define it (i.e. the formula_27-group formula_2) is anisotropic. For example, the arithmetic lattice associated to a quadratic form in formula_4 variables over formula_7 will be co-compact in the associated orthogonal group if and only if the quadratic form does not vanish at any point in formula_65.\n\nThe spectacular result that Margulis obtained is a partial converse to the Borel—Harish-Chandra theorem: for certain Lie groups \"any\" lattice is arithmetic. This result is true for all irreducible lattice in semisimple Lie groups of real rank larger than two. For example, all lattices in formula_59 are arithmetic when formula_67. The main new ingredient that Margulis used to prove his theorem was the superrigidity of lattices in higher-rank groups that he proved for this purpose.\n\nIrreducibility only plays a rôle when formula_46 has a factor of real rank one (otherwise the theorem always holds) and is not simple: it means that for any product decomposition formula_69 the lattice is not commensurable to a product of lattices in each of the factors formula_70. For example, the lattice formula_71 in formula_72 is irreducible, while formula_73 is not.\n\nThe Margulis arithmeticity (and superrigidity) theorem holds for certain rank 1 Lie groups, namely formula_74 for formula_75 and the exceptional group formula_76. It is known not to hold in all groups formula_77 for formula_78 (ref to GPS) and for formula_79 when formula_80. There are no known non-arithmetic lattices in the groups formula_81 when formula_82.\n\nAn arithmetic Fuchsian group is constructed from the following data: a totally real number field formula_16, a quaternion algebra formula_84 over formula_16 and an order formula_86 in formula_84. It is asked that for one embedding formula_88 the algebra formula_89 be isomorphic to the matrix algebra formula_90 and for all others to the Hamilton quaternions. Then the group of units formula_91 is a lattice in formula_92 which is isomorphic to formula_93 and it is co-compact in all cases except when formula_84 is the matrix algebra over formula_95 All arithmetic lattices in formula_96 are obtained in this way (up to commensurability).\n\nArithmetic Kleinian groups are constructed similarly except that formula_16 is required to have exactly one complex place and formula_84 to be the Hamilton quaternions at all real places. They exhaust all arithmetic commensurability classes in formula_99\n\nFor every semisimple Lie group formula_46 it is in theory possible to classify (up to commensurability) all arithmetic lattices in formula_46, in a manner similar to the cases formula_102 explained above. This amounts to classifying the algebraic groups whose real points are isomorphic up to a compact factor to formula_46.\n\nA congruence subgroup is (roughly) a subgroup of an arithmetic group defined by taking all matrices satisfying certain equations modulo an integer, for example the group of 2 by 2 integer matrices with diagonal (respectively off-diagonal) coefficients congruent to 1 (respectively 0) modulo a positive integer. These are always finite-index subgroups and the congruence subgroup problem roughly asks whether all subgroups are obtained in this way. The conjecture (usually attributed to Jean-Pierre Serre is that this is true for (irreducible) arithmetic lattices in higher-rank groups and false in rank-one groups. It is still open in this generality but there are many results establishing it for specific lattices (in both its positive and negative cases).\n\nInstead of taking integral points in the definition of an arithmetic lattice one can take points which are only integral away from a finite number of primes. This leads to the notion of an \"formula_104-arithmetic lattice\" (where formula_104 stands for the set of primes inverted). The prototypical example is formula_107. They are also naturally lattices in certain topological groups, for example formula_107 is a lattice in formula_109\n\nThe formal definition of an formula_104-arithmetic group for formula_104 a finite set of prime numbers is the same as for arithmetic groups with formula_34 replaced by formula_113 where formula_114 is the product of the primes in formula_104.\n\nThe Borel–Harish-Chandra theorem generalizes to formula_104-arithmetic groups as follows: if formula_13 is an formula_104-arithmetic group in a formula_7-algebraic group formula_2 then formula_13 is a lattice in the locally compact group\n\nArithmetic groups with Kazhdan's property (T) or the weaker property (formula_123) of Lubotzky and Zimmer can be used to construct expander graphs (Margulis), or even Ramanujan graphs(Lubotzky—Phillips—Sarnak). Such graphs are known to exist in abundance by probabilistic results but the explicit nature of these constructions makes them interesting.\n\nCongruence covers of arithmetic surfaces are known to give rise to surfaces with large injectivity radius. Likewise the Ramanujan graphs constructed by Lubotzky—Phillips—Sarnak have large girth. It is in fact known that the Ramanujan property itself implies that the local girths of the graph are almost always large.\n\nArithmetic groups can be used to construct isospectral manifolds. This was first realised by Marie-France Vignéras and numerous variations on her construction have appeared since. The isospectrality problem is in fact particularly amenable to study in the restricted setting of arithmetic manifolds.\n\nA fake projective plane is a complex surface which has the same Betti numbers as the projective plane formula_124 but is not biholomorhic to it; the first example was discovered by Mumford. By work of Klingler (also proved independently by Yeung) all such are quotients of the 2-ball by arithmetic lattices in formula_125. The possible lattices have been classified by Prasad and Yeung and the classification was completed by Cartwright and Steger who checked that they actually correspond to fake projective planes.\n"}
{"id": "42717030", "url": "https://en.wikipedia.org/wiki?curid=42717030", "title": "Banach Journal of Mathematical Analysis", "text": "Banach Journal of Mathematical Analysis\n\nThe Banach Journal of Mathematical Analysis is a peer-reviewed mathematics journal published by Tusi Mathematical Research Group. It was established in 2006. The journal publishes articles on functional analysis and operator theory.\n\nThe journal is abstracted and indexed in Science Citation Index Expanded, Current Contents/Physical, Chemical & Earth Sciences, Scopus, \"Mathematical Reviews\", and Zentralblatt MATH. Its 2016 MCQ is 0.63. According to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 0.833.\n"}
{"id": "52777879", "url": "https://en.wikipedia.org/wiki?curid=52777879", "title": "Base change theorems", "text": "Base change theorems\n\nIn mathematics, the base change theorems relate the direct image and the pull-back of sheaves. More precisely, they are about the base change map, given by the following natural transformation of sheaves:\n\nwhere\n\nis a Cartesian square of topological spaces and formula_3 is a sheaf on \"X\".\n\nSuch theorems exist in different branches of geometry: for (essentially arbitrary) topological spaces and proper maps \"f\", in algebraic geometry for (quasi-)coherent sheaves and \"f\" proper or \"g\" flat, similarly in analytic geometry, but also for étale sheaves for \"f\" proper or \"g\" smooth.\n\nA simple base change phenomenon arises in commutative algebra when \"A\" is a commutative ring and \"B\" and \"A' \"are two \"A\"-algebras. Let formula_4. In this situation, given a \"B\"-module \"M\", there is an isomorphism (of \"A' \"-modules):\n\nHere the subscript indicates the forgetful functor, i.e., formula_6 is \"M\", but regarded as an \"A\"-module.\nIndeed, such an isomorphism is obtained by observing \n\nThus, the two operations, namely forgetful functors and tensor products commute in the sense of the above isomorphism.\nThe base change theorems discussed below are statements of a similar kind.\n\nThe base change theorems presented below all assert that (for different types of sheaves, and under various assumptions on the maps involved), that the following \"base change map\"\n\nis an isomorphism, where\n\nare continuous maps between topological spaces that form a Cartesian square and formula_3 is a sheaf on \"X\". Here formula_11 denotes the higher direct image of formula_12 under \"f\", i.e., the derived functor of the direct image (also known as pushforward) functor formula_13.\n\nThis map exists without any assumptions on the maps \"f\" and \"g\". It is constructed as follows: since formula_14 is left adjoint to formula_15, there is a natural map (called unit map)\nand so \n\nThe Grothendieck spectral sequence then gives the first map and the last map (they are edge maps) in:\n\nCombining this with the above yields\n\nUsing the adjointness of formula_20 and formula_21 finally yields the desired map.\n\nThe above-mentioned introductory example is a special case of this, namely for the affine spectra formula_22 and, consequently, formula_23, and the quasi-coherent sheaf formula_24 associated to the \"B\"-module \"M\".\n\nIt is conceptually convenient to organize the above base change maps, which only involve only a single higher direct image functor, into one which encodes all formula_25 at a time. In fact, similar arguments as above yield a map in the derived category of sheaves on \"S':\"\n\nwhere formula_27 denotes the (total) derived functor of formula_13.\n\nIf \"X\" is a Hausdorff topological space, \"S\" is a locally compact Hausdorff space and \"f\" is universally closed (i.e., formula_29 is a closed map for any continuous map formula_30), then\nthe base change map\n\nis an isomorphism. Indeed, we have: for formula_32,\n\nand so for formula_34\n\nTo encode all individual higher derived functors of formula_13 into one entity, the above statement may equivalently be rephrased by saying that the base change map\n\nis a quasi-isomorphism.\n\nThe assumptions that the involved spaces be Hausdorff have been weakened by .\n\nIf the map \"f\" is not closed, the base change map need not be an isomorphism, as the following example shows (the maps are the standard inclusions) :\n\nOne the one hand formula_39 is always zero, but if formula_12 is a local system on formula_41 corresponding to a representation of the fundamental group formula_42 (which is isomorphic to Z), then formula_43 can be computed as the invariants of the monodromy action of formula_44 on the stalk formula_45 (for any formula_46), which need not vanish.\n\nTo obtain a base-change result, the functor formula_13 (or its derived functor) has to be replaced by the direct image with compact support formula_48. For example, if formula_49 is the inclusion of an open subset, such as in the above example, formula_50 is the extension by zero, i.e., its stalks are given by\n\nIn general, there is a map formula_52, which is a quasi-isomorphism if \"f\" is proper, but not in general. The proper base change theorem mentioned above has the following generalization: there is a quasi-isomorphism\n\n\"Proper base change theorems\" for quasi-coherent sheaves apply in the following situation: formula_49 is a proper morphism between noetherian schemes, and formula_3 is a coherent sheaf which is flat over \"S\" (i.e., formula_45 is flat over formula_57). In this situation, the following statements hold:\n\nAs the stalk of the sheaf formula_73 is closely related to the cohomology of the fiber of the point under \"f\", this statement is paraphrased by saying that \"cohomology commutes with base extension\".\n\nThese statements are proved using the following fact, where in addition to the above assumptions formula_74: there is a finite complex formula_75 of finitely generated projective \"A\"-modules and a natural isomorphism of functors\non the category of formula_77-algebras.\n\nThe base change map\nis an isomorphism for a quasi-coherent sheaf formula_12 (on formula_80), provided that the map formula_81 is \"flat\" (together with a number of technical conditions: \"f\" needs to be a separated morphism of finite type, the schemes involved need to be Noetherian).\n\nA far reaching extension of flat base change is possible when considering the base change map\nin the derived category of sheaves on \"S',\" similarly as mentioned above. Here formula_83 is the (total) derived functor of the pullback of formula_84-modules (because formula_85 involves a tensor product, formula_20 is not exact when is not flat and therefore is not equal to its derived functor formula_83).\nThis map is a quasi-isomorphism provided that the following conditions are satisfied:\n\nOne advantage of this formulation is that the flatness hypothesis has been weakened. However, making concrete computations of the cohomology of the left- and right-hand sides now requires the Grothendieck spectral sequence.\n\nDerived algebraic geometry provides a means to drop the flatness assumption, provided that the pullback formula_119 is replaced by the homotopy pullback. In the easiest case when \"X\", \"S\", and formula_95 are affine (with the notation as above), the homotopy pullback is given by the derived tensor product\nThen, assuming that the schemes (or, more generally, derived schemes) involved are quasi-compact and quasi-separated, the natural transformation\nis a quasi-isomorphism for any quasi-coherent sheaf, or more generally a complex of quasi-coherent sheaves.\nThe afore-mentioned flat base change result is in fact a special case since for \"g\" flat the homotopy pullback (which is locally given by a derived tensor product) agrees with the ordinary pullback (locally given by the underived tensor product), and since the pullback along the flat maps \"g\" and \"g' \"are automatically derived (i.e., formula_123). The auxiliary assumptions related to the Tor-independence or Tor-amplitude in the preceding base change theorem also become unnecessary.\n\nIn the above form, base change has been extended by to the situation where \"X\", \"S\", and \"S' \"are (possibly derived) stacks, provided that the map \"f\" is a perfect map (which includes the case that \"f\" is a quasi-compact, quasi-separated map of schemes, but also includes more general stacks, such as the classifying stack \"BG\" of an algebraic group in characteristic zero).\n\nProper base change also holds in the context of complex manifolds.\nThe theorem on formal functions is a variant of the proper base change, where the pullback is replaced by a completion operation.\n\nThe see-saw principle and the theorem of the cube, which are foundational facts in the theory of abelian varieties, are a consequence of proper base change.\n\nA base-change also holds for D-modules: if \"X\", \"S\", \"X',\" and \"S' \"are smooth varieties (but \"f\" and \"g\" need not be flat or proper etc.), there is a quasi-isomorphism\nwhere formula_125 and formula_126 denote the inverse and direct image functors for \"D\"-modules.\n\nFor étale torsion sheaves formula_12, there are two base change results referred to as \"proper\" and \"smooth base change\", respectively: base change holds if formula_128 is proper. It also holds if \"g\" is smooth, provided that \"f\" is quasi-compact and provided that the torsion of formula_12 is prime to the characteristic of the residue fields of \"X\".\n\nClosely related to proper base change is the following fact (the two theorems are usually proved simultaneously): let \"X\" be a variety over a separably closed field and formula_3 a constructible sheaf on formula_131. Then formula_132 are finite in each of the following cases:\n\nUnder additional assumptions, extended the proper base change theorem to non-torsion étale sheaves.\n\nIn close analogy to the topological situation mentioned above, the base change map for an open immersion \"f\", \nis not usually an isomorphism. Instead the extension by zero functor formula_135 satisfies an isomorphism\nThis fact and the proper base change suggest to define the \"direct image functor with compact support\" for a map \"f\" by\nwhere formula_138 is a \"compactification\" of \"f\", i.e., a factorization into an open immersion followed by a proper map.\nThe proper base change theorem is needed to show that this is well-defined, i.e., independent (up to isomorphism) of the choice of the compactification.\nMoreover, again in analogy to the case of sheaves on a topological space, a base change formula for formula_21 vs. formula_48 does hold for non-proper maps \"f\".\n\nFor the structural map formula_141 of a scheme over a field \"k\", the individual cohomologies of formula_142, denoted by formula_143 referred to as cohomology with compact support. It is an important variant of usual étale cohomology.\n\nSimilar ideas are also used to construct an analogue of the functor formula_48 in A-homotopy theory.\n\n\n\n"}
{"id": "35170441", "url": "https://en.wikipedia.org/wiki?curid=35170441", "title": "Benson's algorithm", "text": "Benson's algorithm\n\nBenson's algorithm, named after Harold Benson, is a method for solving multi-objective linear programming problems and vector linear programs. This works by finding the \"efficient extreme points in the outcome set\". The primary concept in Benson's algorithm is to evaluate the upper image of the vector optimization problem by cutting planes.\n\nConsider a vector linear program\nfor formula_2, formula_3, formula_4 and a polyhedral convex ordering cone formula_5 having nonempty interior and containing no lines. The feasible set is formula_6. In particular, Benson's algorithm finds the extreme points of the set formula_7, which is called upper image.\n\nIn case of formula_8, one obtains the special case of a multi-objective linear program (multiobjective optimization).\n\nThere is a dual variant of Benson's algorithm, which is based on geometric duality for multi-objective linear programs.\n\nBensolve - a free VLP solver\nInner\n"}
{"id": "52341205", "url": "https://en.wikipedia.org/wiki?curid=52341205", "title": "Buridan formula", "text": "Buridan formula\n\nIn quantified modal logic, the Buridan formula and the converse Buridan formula (more accurately, schemata rather than formulas) (i) syntactically state principles of interchange between quantifiers and modalities; (ii) semantically state a relation between domains of possible worlds. The formulas are named in honor of the medieval philosopher Jean Buridan by analogy with the Barcan formula and the converse Barcan formula introduced as axioms by Ruth Barcan Marcus.\n\nThe Buridan formula is:\n\nIn English, the schema reads: If possibly everything is F, then everything is possibly F. It is equivalent in a classical modal logic (but not necessarily in other formulations of modal logic) to\n\nThe converse Buridan formula is:\n\nformula_3.\n\nIn medieval scholasticism, nominalists held that universals exist only subsequent to particular things or pragmatic circumstances, while realists followed Plato in asserting that universals exist independently of, and superior to, particular things.\n"}
{"id": "34693670", "url": "https://en.wikipedia.org/wiki?curid=34693670", "title": "Canadian Mathematical Bulletin", "text": "Canadian Mathematical Bulletin\n\nThe Canadian Mathematical Bulletin (; print: , online: ) is a mathematics journal published by the Canadian Mathematical Society. It is published four times per year. The current editors-in-chief of the journal are Jie Xiao and Xiaoqiang Zhao. The journal publishes short articles in all areas of mathematics that are of sufficient interest to the general mathematical public.\n"}
{"id": "7445", "url": "https://en.wikipedia.org/wiki?curid=7445", "title": "Classification of finite simple groups", "text": "Classification of finite simple groups\n\nIn mathematics, the classification of the finite simple groups is a theorem stating that every finite simple group belongs to one of four broad classes described below. These groups can be seen as the basic building blocks of all finite groups, in a way reminiscent of the way the prime numbers are the basic building blocks of the natural numbers. The Jordan–Hölder theorem is a more precise way of stating this fact about finite groups. However, a significant difference from integer factorization is that such \"building blocks\" do not necessarily determine a unique group, since there might be many non-isomorphic groups with the same composition series or, put in another way, the extension problem does not have a unique solution.\n\nGroup theory is central to many areas of pure and applied mathematics and the classification theorem is one of the great achievements of modern mathematics. The proof consists of tens of thousands of pages in several hundred journal articles written by about 100 authors, published mostly between 1955 and 2004. Gorenstein (d.1992), Lyons, and Solomon are gradually publishing a simplified and revised version of the proof.\n\nThe classification theorem has applications in many branches of mathematics, as questions about the structure of finite groups (and their action on other mathematical objects) can sometimes be reduced to questions about finite simple groups. Thanks to the classification theorem, such questions can sometimes be answered by checking each family of simple groups and each sporadic group.\n\nDaniel Gorenstein announced in 1983 that the finite simple groups had all been classified, but this was premature as he had been misinformed about the proof of the classification of quasithin groups. The completed proof of the classification was announced by after Aschbacher and Smith published a 1221-page proof for the missing quasithin case.\n\n wrote two volumes outlining the low rank and odd characteristic part of the proof, and \nwrote a 3rd volume covering the remaining characteristic 2 case. The proof can be broken up into several major pieces as follows:\n\nThe simple groups of low 2-rank are mostly groups of Lie type of small rank over fields of odd characteristic, together with five alternating and seven characteristic 2 type and nine sporadic groups.\n\nThe simple groups of small 2-rank include:\nThe classification of groups of small 2-rank, especially ranks at most 2, makes heavy use of ordinary and modular character theory, which is almost never directly used elsewhere in the classification.\n\nAll groups not of small 2 rank can be split into two major classes: groups of component type and groups of characteristic 2 type. This is because if a group has sectional 2-rank at least 5 then MacWilliams showed that its Sylow 2-subgroups are connected, and the balance theorem implies that any simple group with connected Sylow 2-subgroups is either of component type or characteristic 2 type. (For groups of low 2-rank the proof of this breaks down, because theorems such as the signalizer functor theorem only work for groups with elementary abelian subgroups of rank at least 3.)\n\nA group is said to be of component type if for some centralizer \"C\" of an involution, \"C\"/\"O\"(\"C\") has a component (where \"O\"(\"C\") is the core of \"C\", the maximal normal subgroup of odd order).\nThese are more or less the groups of Lie type of odd characteristic of large rank, and alternating groups, together with some sporadic groups.\nA major step in this case is to eliminate the obstruction of the core of an involution. This is accomplished by the B-theorem, which states that every component of \"C\"/\"O\"(\"C\") is the image of a component of \"C\".\n\nThe idea is that these groups have a centralizer of an involution with a component that is a smaller quasisimple group, which can be assumed to be already known by induction. So to classify these groups one takes every central extension of every known finite simple group, and finds all simple groups with a centralizer of involution with this as a component. This gives a rather large number of different cases to check: there are not only 26 sporadic groups and 16 families of groups of Lie type and the alternating groups, but also many of the groups of small rank or over small fields behave differently from the general case and have to be treated separately, and the groups of Lie type of even and odd characteristic are also quite different.\n\nA group is of characteristic 2 type if the generalized Fitting subgroup \"F\"*(\"Y\") of every 2-local subgroup \"Y\" is a 2-group.\nAs the name suggests these are roughly the groups of Lie type over fields of characteristic 2, plus a handful of others that are alternating or sporadic or of odd characteristic. Their classification is divided into the small and large rank cases, where the rank is the largest rank of an odd abelian subgroup normalizing a nontrivial 2-subgroup, which is often (but not always) the same as the rank of a Cartan subalgebra when the group is a group of Lie type in characteristic 2.\n\nThe rank 1 groups are the thin groups, classified by Aschbacher, and the rank 2 ones are the notorious quasithin groups, classified by Aschbacher and Smith. These correspond roughly to groups of Lie type of ranks 1 or 2 over fields of characteristic 2.\n\nGroups of rank at least 3 are further subdivided into 3 classes by the trichotomy theorem, proved by Aschbacher for rank 3 and by Gorenstein and Lyons for rank at least 4.\nThe three classes are groups of GF(2) type (classified mainly by Timmesfeld), groups of \"standard type\" for some odd prime (classified by the Gilman–Griess theorem and work by several others), and groups of uniqueness type, where a result of Aschbacher implies that there are no simple groups.\nThe general higher rank case consists mostly of the groups of Lie type over fields of characteristic 2 of rank at least 3 or 4.\n\nThe main part of the classification produces a characterization of each simple group. It is then necessary to check that there exists a simple group for each characterization and that it is unique. This gives a large number of separate problems; for example, the original proofs of existence and uniqueness of the monster group totaled about 200 pages, and the identification of the Ree groups by Thompson and Bombieri was one of the hardest parts of the classification. Many of the existence proofs and some of the uniqueness proofs for the sporadic groups originally used computer calculations, most of which have since been replaced by shorter hand proofs.\n\nIn 1972 announced a program for completing the classification of finite simple groups, consisting of the following 16 steps:\n\nMany of the items in the list below are taken from . The date given is usually the publication date of the complete proof of a result, which is sometimes several years later than the proof or first announcement of the result, so some of the items appear in the \"wrong\" order.\nThe proof of the theorem, as it stood around 1985 or so, can be called \"first generation\". Because of the extreme length of the first generation proof, much effort has been devoted to finding a simpler proof, called a second-generation classification proof. This effort, called \"revisionism\", was originally led by Daniel Gorenstein.\n\n, seven volumes of the second generation proof have been published . In 2012 Solomon estimated that the project would need another 5 volumes, but said that progress on them was slow. It is estimated that the new proof will eventually fill approximately 5,000 pages. (This length stems in part from second generation proof being written in a more relaxed style.) Aschbacher and Smith wrote their two volumes devoted to the quasithin case in such a way that those volumes can be part of the second generation proof.\n\nGorenstein and his collaborators have given several reasons why a simpler proof is possible.\n\nGorenstein has discussed some of the reasons why there might not be a short proof of the classification similar to the classification of compact Lie groups.\n\n\nThis section lists some results that have been proved using the classification of finite simple groups.\n\n\n\n\n"}
{"id": "603780", "url": "https://en.wikipedia.org/wiki?curid=603780", "title": "Coherent sheaf", "text": "Coherent sheaf\n\nIn mathematics, especially in algebraic geometry and the theory of complex manifolds, coherent sheaves are a class of sheaves closely linked to the geometric properties of the underlying space. The definition of coherent sheaves is made with reference to a sheaf of rings that codifies this geometric information.\n\nCoherent sheaves can be seen as a generalization of vector bundles. Unlike vector bundles, they form an abelian category, and so they are closed under operations such as taking kernels, images, and cokernels. The quasi-coherent sheaves are a generalization of coherent sheaves and include the locally free sheaves of infinite rank.\n\nCoherent sheaf cohomology is a powerful technique, in particular for studying the sections of a given coherent sheaf.\n\nA quasi-coherent sheaf on a ringed space (\"X\",\"O\") is a sheaf \"F\" of \"O\"-modules which has a local presentation, that is, every point in \"X\" has an open neighborhood \"U\" in which there is an exact sequence\nfor some sets \"I\" and \"J\" (possibly infinite).\n\nA coherent sheaf on a ringed space (\"X\",\"O\") is a quasi-coherent sheaf \"F\" satisfying the following two properties:\n\nMorphisms between (quasi-)coherent sheaves are the same as morphisms of sheaves of \"O\"-modules.\n\nWhen \"X\" is a scheme, the general definitions above are equivalent to more explicit ones. A sheaf \"F\" of \"O\"-modules is quasi-coherent if and only if over each open affine subscheme \"U\"=Spec(\"R\") the restriction \"F\"| is isomorphic to a sheaf formula_2 associated to the module \"M\"=\"Γ(U, F)\" over \"R\". When \"X\" is a locally Noetherian scheme, \"F\" is coherent if and only if it is quasi-coherent and the modules \"M\" above can be taken to be finitely generated.\n\nOn an affine scheme \"U\" = Spec \"A\", there is an equivalence of categories from \"A\"-modules to quasi-coherent sheaves, taking a module \"M\" to the associated sheaf . The inverse equivalence takes a quasi-coherent sheaf \"F\" on \"U\" to the \"A\"-module \"F\"(\"U\") of global sections of \"F\".\n\nHere are several further characterizations of quasi-coherent sheaves on a scheme.\nOn an arbitrary ringed space quasi-coherent sheaves do not necessarily form an abelian category. On the other hand, the quasi-coherent sheaves on any scheme form an abelian category, and they are extremely useful in that context.\n\nOn any ringed space \"X\", the coherent sheaves form an abelian category, a full subcategory of the category of \"O\"-modules. (Analogously, the category of coherent modules over any ring \"R\" is a full abelian subcategory of the category of all \"R\"-modules.) So the kernel, image, and cokernel of any map of coherent sheaves are coherent. The direct sum of two coherent sheaves is coherent; more generally, an \"O\"-module that is an extension of two coherent sheaves is coherent.\n\nA submodule of a coherent sheaf is coherent if it is of finite type. A coherent sheaf is always an \"O\"-module of \"finite presentation\", meaning that each point \"x\" in \"X\" has an open neighborhood \"U\" such that the restriction \"F\"| of \"F\" to \"U\" is isomorphic to the cokernel of a morphism \"O\"| → \"O\"| for some natural numbers \"n\" and \"m\". If \"O\" is coherent, then, conversely, every sheaf of finite presentation over \"O\" is coherent.\n\nThe sheaf of rings \"O\" is called coherent if it is coherent considered as a sheaf of modules over itself. In particular, the Oka coherence theorem states that the sheaf of holomorphic functions on a complex analytic space \"X\" is a coherent sheaf of rings. The main part of the proof is the case \"X\" = C. Likewise, on a locally Noetherian scheme \"X\", the structure sheaf \"O\" is a coherent sheaf of rings.\n\n\n\n\n\n\n\n\n\n\nLet ƒ: \"X\" → \"Y\" be a morphism of ringed spaces (for example, a morphism of schemes). If \"F\" is a quasi-coherent sheaf on \"Y\", then the inverse image \"O\"-module (or pullback) \"f\"*\"F\" is quasi-coherent on \"X\". For a morphism of schemes \"f\": \"X\" → \"Y\" and a coherent sheaf \"F\" on \"Y\", the pullback \"f\"\"F\" is not coherent in full generality (for example, \"f\"*\"O\" = \"O\", which might not be coherent), but pullbacks of coherent sheaves are coherent if \"X\" is locally Noetherian. An important special case is the pullback of a vector bundle, which is a vector bundle.\n\nIf \"f\": \"X\" → \"Y\" is a quasi-compact quasi-separated morphism of schemes and \"E\" is a quasi-coherent sheaf on \"X\", then the direct image sheaf (or pushforward) \"f\"\"E\" is quasi-coherent on \"Y\".\n\nThe direct image of a coherent sheaf is often not coherent. For example, for a field \"k\", let \"X\" be the affine line over \"k\", and consider the morphism \"f\": \"X\" → Spec(\"k\"); then the direct image \"f\"\"O\" is the sheaf on Spec(\"k\") associated to the polynomial ring \"k\"[\"x\"], which is not coherent because \"k\"[\"x\"] has infinite dimension as a \"k\"-vector space. On the other hand, the direct image of a coherent sheaf under a proper morphism is coherent, by results of Grauert and Grothendieck.\n\nAn important feature of coherent sheaves \"F\" is that the properties of \"F\" at a point \"p\" control the behavior of \"F\" in a neighborhood of \"p\", more than would be true for an arbitrary sheaf. For example, Nakayama's lemma says (in geometric language) that if \"F\" is a coherent sheaf on a scheme \"X\", then the fiber \"F\"⊗\"k\"(\"p\") of \"F\" at a point \"p\" (a vector space over the residue field \"k\"(\"p\")) is zero if and only if the sheaf \"F\" is zero on some open neighborhood of \"p\". A related fact is that the dimension of the fibers of a coherent sheaf is upper-semicontinuous. Thus a coherent sheaf has constant rank on an open set, while the rank can jump up on a lower-dimensional closed subset.\n\nIn the same spirit: a coherent sheaf \"F\" on a scheme \"X\" is a vector bundle if and only if its stalk \"F\" is a free module over the local ring \"O\" for every point \"p\" in \"X\".\n\nOn a general scheme, one cannot determine whether a coherent sheaf is a vector bundle just from its fibers (as opposed to its stalks). On a reduced locally Noetherian scheme, however, a coherent sheaf is a vector bundle if and only if its rank is locally constant.\n\nFor a morphism of schemes \"X\" → \"Y\", let Δ: \"X\" → \"X\" × \"X\" be the diagonal morphism, which is a closed immersion if \"X\" is separated over \"Y\". Let \"I\" be the ideal sheaf of \"X\" in \"X\" × \"X\". Then the sheaf of differentials Ω can be defined as the pullback Δ*(\"I\") of \"I\" to \"X\". Sections of this sheaf are called 1-forms on \"X\" over \"Y\", and they can be written locally on \"X\" as finite sums ∑ \"f\" d\"g\" for regular functions \"f\" and \"g\". If \"X\" is locally of finite type over a field \"k\", then Ω is a coherent sheaf on \"X\".\n\nIf \"X\" is smooth over \"k\", then Ω (meaning Ω) is a vector bundle over \"X\", called the cotangent bundle of \"X\". Then the tangent bundle \"TX\" is defined to be the dual bundle (Ω)*. For \"X\" smooth over \"k\" of dimension \"n\" everywhere, the tangent bundle has rank \"n\".\n\nIf \"Y\" is a smooth closed subscheme of a smooth scheme \"X\" over \"k\", then there is a short exact sequence of vector bundles on \"Y\":\nwhich can be used as a definition of the normal bundle \"N\" to \"Y\" in \"X\".\n\nFor a smooth scheme \"X\" over a field \"k\" and a natural number \"a\", the vector bundle Ω of \"a\"-forms on \"X\" is defined as the \"a\"th exterior power of the cotangent bundle, Ω = ΛΩ. For a smooth variety \"X\" of dimension \"n\" over \"k\", the canonical bundle \"K\" means the line bundle Ω. Thus sections of the canonical bundle are algebro-geometric analogs of volume forms on \"X\". For example, a section of the canonical bundle of affine space \"A\" over \"k\"\ncan be written as\nwhere \"f\" is a polynomial with coefficients in \"k\".\n\nLet \"R\" be a commutative ring and \"n\" a natural number. For each integer \"j\", there is an important example of a line bundle on projective space P over \"R\", called \"O\"(\"j\"). To define this, consider the morphism of \"R\"-schemes\ngiven in coordinates by (\"x\"...,\"x\") ↦ [\"x\"...,\"x\"]. (That is, thinking of projective space as the space of 1-dimensional linear subspaces of affine space, send a nonzero point in affine space to the line that it spans.) Then a section of \"O\"(\"j\") over an open subset \"U\" of P is defined to be a regular function \"f\" on π(\"U\") that is homogeneous of degree \"j\", meaning that\nas regular functions on (\"A\" − 0) × π(\"U\"). For all integers \"i\" and \"j\", there is an isomorphism \"O\"(\"i\") ⊗ \"O\"(\"j\") ≅ \"O\"(\"i\"+\"j\") of line bundles on P.\n\nIn particular, every homogeneous polynomial in \"x\"...,\"x\" of degree \"j\" over \"R\" can be viewed as a global section of \"O\"(\"j\") over P. Note that every closed subscheme of projective space can be defined as the zero set of some collection of homogeneous polynomials, hence as the zero set of some sections of the line bundles \"O\"(\"j\"). This contrasts with the simpler case of affine space, where a closed subscheme is simply the zero set of some collection of regular functions. The regular functions on projective space P over \"R\" are just the \"constants\" (the ring \"R\"), and so it is essential to work with the line bundles \"O\"(\"j\").\n\nSerre gave an algebraic description of all coherent sheaves on projective space, more subtle than what happens for affine space. Namely, let \"R\" be a Noetherian ring (for example, a field), and consider the polynomial ring \"S\" = \"R\"[\"x\"...,\"x\"] as a graded ring with each \"x\" having degree 1. Then every finitely generated graded \"S\"-module \"M\" has an associated coherent sheaf on P over \"R\". Every coherent sheaf on P arises in this way from a finitely generated graded \"S\"-module \"M\". (For example, the line bundle \"O\"(\"j\") is the sheaf associated to the \"S\"-module \"S\" with its grading lowered by \"j\".) But the \"S\"-module \"M\" that yields a given coherent sheaf on P is not unique; it is only unique up to changing \"M\" by graded modules that are nonzero in only finitely many degrees. More precisely, the abelian category of coherent sheaves on P is the quotient of the category of finitely generated graded \"S\"-modules by the Serre subcategory of modules that are nonzero in only finitely many degrees.\n\nThe tangent bundle of projective space P over a field \"k\" can be described in terms of the line bundle \"O\"(1). Namely, there is a short exact sequence, the Euler sequence:\nIt follows that the canonical bundle \"K\" (the dual of the determinant line bundle of the tangent bundle) is isomorphic to \"O\"(−\"n\"−1). This is a fundamental calculation for algebraic geometry. For example, the fact that the canonical bundle is a negative multiple of the ample line bundle \"O\"(1) means that projective space is a Fano variety. Over the complex numbers, this means that projective space has a Kähler metric with positive Ricci curvature.\n\nConsider a smooth degree formula_28 hypersurface formula_29 defined by the homogeneous polynomial formula_30. Then, there is an exact sequence\nwhere the second map is the pullback of differential forms, and the first map sends\nNote that this sequence tells us that formula_33 is the conormal sheaf of formula_34 in formula_35. Dualizing this yields the exact sequence\nhence formula_37 is the normal bundle of formula_34 in formula_35. If we use the fact that given an exact sequence\nof ranks formula_41, there is an isomorphism\nof line bundles, then we see that there is the isomorphism\nshowing that\n\nA vector bundle \"E\" on a smooth variety \"X\" over a field has Chern classes in the Chow ring of \"X\", \"c\"(\"E\") in CH(\"X\") for \"i\" ≥ 0. These satisfy the same formal properties as Chern classes in topology. For example, for any short exact sequence\nof vector bundles on \"X\", the Chern classes of \"B\" are given by\n\nIt follows that the Chern classes of a vector bundle \"E\" depend only on the class of \"E\" in the Grothendieck group \"K\"(\"X\"). By definition, for a scheme \"X\", \"K\"(\"X\") is the quotient of the free abelian group on the set of isomorphism classes of vector bundles on \"X\" by the relation that [B] = [A] + [C] for any short exact sequence as above. Although \"K\"(\"X\") is hard to compute in general, algebraic K-theory provides many tools for studying it, including a sequence of related groups \"K\"(\"X\") for integers \"i\".\n\nA variant is the group \"G\"(\"X\") (or \"K\"'(\"X\")), the Grothendieck group of coherent sheaves on \"X\". (In topological terms, \"G\"-theory has the formal properties of a Borel–Moore homology theory for schemes, while \"K\"-theory is the corresponding cohomology theory.) The natural homomorphism \"K\"(\"X\") → \"G\"(\"X\") is an isomorphism if \"X\" is a regular separated Noetherian scheme, using that every coherent sheaf has a finite resolution by vector bundles in that case. For example, that gives a definition of the Chern classes of a coherent sheaf on a smooth variety over a field.\n\nMore generally, a Noetherian scheme \"X\" is said to have the resolution property if every coherent sheaf on \"X\" has a surjection from some vector bundle on \"X\". For example, every quasi-projective scheme over a Noetherian ring has the resolution property.\n\nWhen vector bundles and locally free sheaves of finite constant rank are used interchangeably,\ncare must be given to distinguish between bundle homorphisms and sheaf homorphisms. Specifically, given vector bundles formula_47, by definition, a bundle homorphism formula_48 is a scheme morphism over \"X\" (i.e., formula_49) such that, for each geometric point \"x\" in \"X\", formula_50 is a linear map of rank independent of \"x\". Thus, it induces the sheaf homorphism formula_51 of constant rank between the corresponding locally free formula_3-modules (sheaves of dual sections). But there may be an formula_3-module homorphism that does not arise this way; namely, those not having constant rank.\n\nIn particular, a subbundle formula_54 is a subsheaf (formula_55 viewed as sheaves). But the converse can fail; for example, for an effective Cartier divisor \"D\" on \"X\", formula_56 is a subsheaf but typically not a subbundle (since any line bundle has only two subbundles).\n\nQuasi-coherent sheaves on any scheme form an abelian category. Gabber showed that, in fact, the quasi-coherent sheaves on any scheme form a particularly well-behaved abelian category, a Grothendieck category. A quasi-compact quasi-separated scheme \"X\" (such as an algebraic variety over a field) is determined up to isomorphism by the abelian category of quasi-coherent sheaves on \"X\", by Rosenberg, generalizing a result of Gabriel.\n\nThe fundamental technical tool in algebraic geometry is the cohomology theory of coherent sheaves. Although it was introduced only in the 1950s, many earlier techniques of algebraic geometry are clarified by the language of sheaf cohomology applied to coherent sheaves. Broadly speaking, coherent sheaf cohomology can be viewed as a tool for producing functions with specified properties; sections of line bundles or of more general sheaves can be viewed as generalized functions. In complex analytic geometry, coherent sheaf cohomology also plays a foundational role.\n\nAmong the core results of coherent sheaf cohomology are results on finite-dimensionality of cohomology, results on the vanishing of cohomology in various cases, duality theorems such as Serre duality, relations between topology and algebraic geometry such as Hodge theory, and formulas for Euler characteristics of coherent sheaves such as the Riemann–Roch theorem.\n\n\n\n"}
{"id": "49348672", "url": "https://en.wikipedia.org/wiki?curid=49348672", "title": "Comma code", "text": "Comma code\n\nA comma code is a type of prefix-free code in which a comma, a particular symbol or sequence of symbols, occurs at the end of a code word and never occurs otherwise.\n\nFor example, Fibonacci coding is a comma code in which the comma is codice_1. codice_1 and codice_3 are valid Fibonacci code words, but codice_4, codice_5, and codice_6 are not.\n\n\n"}
{"id": "2975155", "url": "https://en.wikipedia.org/wiki?curid=2975155", "title": "Dependence relation", "text": "Dependence relation\n\nIn mathematics, a dependence relation is a binary relation which generalizes the relation of linear dependence.\n\nLet formula_1 be a set. A (binary) relation formula_2 between an element formula_3 of formula_1 and a subset formula_5 of formula_1 is called a \"dependence relation\", written formula_7, if it satisfies the following properties: \n\nGiven a \"dependence relation\" formula_2 on formula_1, a subset formula_5 of formula_1 is said to be \"independent\" if formula_28 for all formula_29 If formula_30, then formula_5 is said to \"span\" formula_14 if formula_33 for every formula_34 formula_5 is said to be a \"basis\" of formula_1 if formula_5 is \"independent\" and formula_5 \"spans\" formula_39 \n\nRemark. If formula_1 is a non-empty set with a dependence relation formula_2, then formula_1 always has a basis with respect to formula_43 Furthermore, any two bases of formula_1 have the same cardinality. \n\n\n"}
{"id": "57546644", "url": "https://en.wikipedia.org/wiki?curid=57546644", "title": "Elayne Arrington", "text": "Elayne Arrington\n\nElayne Arrington is an American mathematician and engineer. She was the first African American woman to graduate from the University of Pittsburgh's School of Engineering. In 2017 Arrington received the Distinguished Alumnus Award from the African American Alumni Council.\n\nElayne Arrington grew up in West Mifflin, Pennsylvania, and graduated from Homestead High School in 1957 where she was valedictorian, receiving the country's second highest mathematics SAT score (797 out of 800). She received was recommended for a scholarship to study mechanical engineering at the University of Pittsburgh, but the sponsor, the Mesta Machine Company withdrew her application for the scholarship saying that the money had to be given to a man because women do not finish engineering programs. In 1961 she became the first woman to graduate from the University of Pittsburgh's School of Engineering. She earned a master's in mathematics from the University of Dayton, Ohio. While working on her master's Arrington also took math courses at Oxford University. She became the 17th African-American woman in the United States to earn a PhD in mathematics.\n\nArrington became an aerospace engineer at the Wright-Patterson Air Force Base's Foreign Technology Division in Dayton, Ohio, where she worked on performance analysis of Soviet Union aircraft. Later she returned to the University of Pittsburgh where she became an associate professor of mathematics.\n"}
{"id": "56015707", "url": "https://en.wikipedia.org/wiki?curid=56015707", "title": "Fabio Conforto", "text": "Fabio Conforto\n\nFabio Conforto (13 August 1909 – 24 February 1954) was an Italian mathematician. His contributed to the fields of algebraic geometry, projective geometry and analytic geometry.\n"}
{"id": "10830506", "url": "https://en.wikipedia.org/wiki?curid=10830506", "title": "Face diagonal", "text": "Face diagonal\n\nIn geometry, a face diagonal of a polyhedron is a diagonal on one of the faces, in contrast to a space diagonal passing through the interior of the polyhedron.\n\nA cuboid has twelve face diagonals (two on each of the six faces), and it has four space diagonals. The cuboid's face diagonals can have up to three different lengths, since the faces come in congruent pairs and the two diagonals on any face are equal. The cuboid's space diagonals all have the same length. If the edge lengths of a cuboid are \"a\", \"b\", and \"c\", then the distinct rectangular faces have edges (\"a\", \"b\"), (\"a\", \"c\"), and (\"b\", \"c\"); so the respective face diagonals have lengths formula_1 formula_2 and formula_3\n\nThus each face diagonal of a cube with side length \"a\" is formula_4.\n\nA regular dodecahedron has 60 face diagonals (and 100 space diagonals).\n"}
{"id": "13005617", "url": "https://en.wikipedia.org/wiki?curid=13005617", "title": "Flat (geometry)", "text": "Flat (geometry)\n\nIn geometry, a flat is a subset of a Euclidean space that is congruent to a Euclidean space of lower dimension. The flats in two-dimensional space are points and lines, and the flats in three-dimensional space are points, lines, and planes.\n\nIn a -dimensional space, there are flats of every dimension from 0 to . Flats of dimension are called hyperplanes.\n\nFlats are the affine subspaces of Euclidean spaces, which means that they are similar to linear subspaces, except that they need not pass through the origin. Flats occurs in linear algebra, as geometric realizations of solution sets of systems of linear equations.\n\nA flat is manifold and an algebraic variety, and is sometimes called \"linear manifold\" or \"linear variety\" for distinguishing it from other manifolds or varieties.\n\nA flat can be described by a system of linear equations. For example, a line in two-dimensional space can be described by a single linear equation involving and :\nIn three-dimensional space, a single linear equation involving , , and defines a plane, while a pair of linear equations can be used to describe a line. In general, a linear equation in variables describes a hyperplane, and a system of linear equations describes the intersection of those hyperplanes. Assuming the equations are consistent and linearly independent, a system of equations describes a flat of dimension .\n\nA flat can also be described by a system of linear parametric equations. A line can be described by equations involving one parameter:\n\nwhile the description of a plane would require two parameters:\n\nIn general, a parameterization of a flat of dimension would require parameters .\n\nAn intersection of flats is either a flat or the empty set.\n\nIf every line from the first flat is parallel to some line from the second flat, then these flats are parallel. Two parallel flats of the same dimension either coincide or do not intersect; they can be described by two systems of linear equations which differ only in their right-hand sides.\n\nIf flats do not intersect, and no line from the first flat is parallel to a line from the second flat, then these are skew flats. It is possible only if sum of their dimensions is less than dimension of the ambient space.\n\nFor two flats of dimensions and there exists the minimal flat which contains them, of dimension at most . If two flats intersect, then the dimension of the containing flat equals to minus the dimension of the intersection.\n\nThese two operations (referred to as \"meet\" and \"join\") make the set of all flats in the Euclidean -space a lattice and can build systematic coordinates for flats in any dimension, leading to Grassmann coordinates or dual Grassmann coordinates. For example, a line in three-dimensional space is determined by two distinct points or by two distinct planes.\n\nHowever, the lattice of all flats is not a distributive lattice.\nIf two lines and intersect, then is a point. If is a point not lying on the same plane, then , both representing a line. But when and are parallel, this distributivity fails, giving on the left-hand side and a third parallel line on the right-hand side.\n\nThe aforementioned facts do not depend on the structure being that of Euclidean space (namely, involving Euclidean distance) and are correct in any affine space. In a Euclidean space:\n\n\n\n\n\n"}
{"id": "29265641", "url": "https://en.wikipedia.org/wiki?curid=29265641", "title": "Folkman's theorem", "text": "Folkman's theorem\n\nFolkman's theorem is a theorem in mathematics, and more particularly in arithmetic combinatorics and Ramsey theory. According to this theorem, whenever the natural numbers are partitioned into finitely many subsets, there exist arbitrarily large sets of numbers all of whose sums belong to the same subset of the partition. The theorem had been discovered and proved independently by several mathematicians, before it was named \"Folkman's theorem\", as a memorial to Jon Folkman, by Graham, Rothschild, and Spencer.\n\nLet N be the set {1, 2, 3, ...} of positive integers, and suppose that N is partitioned into \"k\" different subsets \"N\", \"N\", ... \"N\", where \"k\" is any positive integer. Then Folkman's theorem states that, for every positive integer \"m\", there exists a set \"S\" and an index \"i\" such that \"S\" has \"m\" elements and such that every sum of a nonempty subset of \"S\" belongs to \"N\".\n\nSchur's theorem in Ramsey theory states that, for any finite partition of the positive integers, there exist three numbers \"x\", \"y\", and \"x\" + \"y\" that all belong to the same partition set. That is, it is the special case \"m\" = 2 of Folkman's theorem.\n\nRado's theorem in Ramsey theory concerns a similar problem statement in which the integers are partitioned into finitely many subsets; the theorem characterizes the integer matrices A with the property that the system of linear equations can be guaranteed to have a solution in which every coordinate of the solution vector \"x\" belongs to the same subset of the partition. A system of equations is said to be \"regular\" whenever it satisfies the conditions of Rado's theorem; Folkman's theorem is equivalent to the regularity of the system of equations\nwhere \"T\" ranges over each nonempty subset of the set \n\nIt is possible to replace addition by multiplication in Folkman's theorem: if the natural numbers are finitely partitioned, there exist arbitrarily large sets \"S\" such that all products of nonempty subsets of \"S\" belong to a single partition set. Indeed, if one restricts \"S\" to consist only of powers of two, then this result follows immediately from the additive version of Folkman's theorem. However, it is open whether there exist arbitrarily large sets such that all sums and all products of nonempty subsets belong to a single partition set. It is not even known whether there must necessarily exist a set of the form } for which all four elements belong to the same partition set.\n\nLet formula_2 denote the set of all finite sums of elements of formula_3. Let formula_4 be a (possibly infinite) coloring of the positive integers, and let formula_5 be an arbitrary positive integer. There exists formula_3 such that at least one of the following 3 conditions holds.\n\n1) formula_2 is a monochromatic set.\n\n2) formula_2 is a rainbow set.\n\n3) For any formula_9, the color of formula_10 is determined solely by formula_11.\n\nVariants of Folkman's theorem had been proved by Richard Rado and by J. H. Sanders. Folkman's theorem was named in memory of Jon Folkman by Ronald Graham, Bruce Lee Rothschild, and Joel Spencer, in their book on Ramsey theory.\n"}
{"id": "25844292", "url": "https://en.wikipedia.org/wiki?curid=25844292", "title": "Fundamental theorem of linear programming", "text": "Fundamental theorem of linear programming\n\nIn mathematical optimization, the fundamental theorem of linear programming states, in a weak formulation, that the maxima and minima of a linear function over a convex polygonal region occur at the region's corners. Further, if an extreme value occurs at two corners, then it must also occur everywhere on the line segment between them.\n\nConsider the optimization problem \n\nWhere formula_2. If formula_3 is a bounded polyhedron (and thus a polytope) and formula_4 is an optimal solution to the problem, then formula_4 is either an extreme point (vertex) of formula_3, or lies on a face formula_7 of optimal solutions.\n\nSuppose, for the sake of contradiction, that formula_8. Then there exists some formula_9 such that the ball of radius formula_10 centered at formula_4 is contained in formula_3, that is formula_13. Therefore,\n\nHence formula_4 is not an optimal solution, a contradiction. Therefore, formula_4 must live on the boundary of formula_3. If formula_4 is not a vertex itself, it must be the convex combination of vertices of formula_3, say formula_21. Then formula_22 with formula_23 and formula_24. Observe that \n\nSince formula_26 is an optimal solution, all terms in the sum are nonnegative. Since the sum is equal to zero, we must have that each individual term is equal to zero. Hence, formula_27 for each formula_28, so every formula_28 is also optimal, and therefore all points on the face whose vertices are formula_21, are optimal solutions.\n\n"}
{"id": "16312085", "url": "https://en.wikipedia.org/wiki?curid=16312085", "title": "Hu Washizu principle", "text": "Hu Washizu principle\n\nIn continuum mechanics, and in particular in finite element analysis, the Hu-Washizu principle is a variational principle which says that the action\n\nis stationary, where formula_2 is the elastic stiffness tensor. The Hu-Washizu principle is used to develop mixed finite element methods. The principle is named after Hu Haichang and Kyuichiro Washizu.\n\n"}
{"id": "6993953", "url": "https://en.wikipedia.org/wiki?curid=6993953", "title": "Immersion (mathematics)", "text": "Immersion (mathematics)\n\nIn mathematics, an immersion is a differentiable function between differentiable manifolds whose derivative is everywhere injective. Explicitly, is an immersion if\n\nis an injective function at every point \"p\" of \"M\" (where \"TX\" denotes the tangent space of a manifold \"X\" at a point \"p\" in \"X\"). Equivalently, \"f\" is an immersion if its derivative has constant rank equal to the dimension of \"M\":\n\nThe function \"f\" itself need not be injective, only its derivative.\n\nA related concept is that of an embedding. A smooth embedding is an injective immersion that is also a topological embedding, so that \"M\" is diffeomorphic to its image in \"N\". An immersion is precisely a local embedding – i.e., for any point there is a neighbourhood, , of \"x\" such that is an embedding, and conversely a local embedding is an immersion. For infinite dimensional manifolds, this is sometimes taken to be the definition of an immersion.\nIf \"M\" is compact, an injective immersion is an embedding, but if \"M\" is not compact then injective immersions need not be embeddings; compare to continuous bijections versus homeomorphisms.\n\nA regular homotopy between two immersions \"f\" and \"g\" from a manifold \"M\" to a manifold \"N\" is defined to be a differentiable function such that for all \"t\" in the function defined by for all is an immersion, with , . A regular homotopy is thus a homotopy through immersions.\n\nHassler Whitney initiated the systematic study of immersions and regular homotopies in the 1940s, proving that for every map of an \"m\"-dimensional manifold to an \"n\"-dimensional manifold is homotopic to an immersion, and in fact to an embedding for ; these are the Whitney immersion theorem and Whitney embedding theorem.\n\nStephen Smale expressed the regular homotopy classes of immersions as the homotopy groups of a certain Stiefel manifold. The sphere eversion was a particularly striking consequence.\n\nMorris Hirsch generalized Smale's expression to a homotopy theory description of the regular homotopy classes of immersions of any \"m\"-dimensional manifold \"M\" in any \"n\"-dimensional manifold \"N\".\n\nThe Hirsch-Smale classification of immersions was generalized by Mikhail Gromov.\n\nThe primary obstruction to the existence of an immersion is the stable normal bundle of \"M\", as detected by its characteristic classes, notably its Stiefel–Whitney classes. That is, since R is parallelizable, the pullback of its tangent bundle to \"M\" is trivial; since this pullback is the direct sum of the (intrinsically defined) tangent bundle on \"M\", \"TM\", which has dimension \"m\", and of the normal bundle \"ν\" of the immersion \"i\", which has dimension , for there to be a codimension \"k\" immersion of \"M\", there must be a vector bundle of dimension \"k\", \"ξ\", standing in for the normal bundle \"ν\", such that is trivial. Conversely, given such a bundle, an immersion of \"M\" with this normal bundle is equivalent to a codimension 0 immersion of the total space of this bundle, which is an open manifold.\n\nThe stable normal bundle is the class of normal bundles plus trivial bundles, and thus if the stable normal bundle has cohomological dimension \"k\", it cannot come from an (unstable) normal bundle of dimension less than \"k\". Thus, the cohomology dimension of the stable normal bundle, as detected by its highest non-vanishing characteristic class, is an obstruction to immersions.\n\nSince characteristic classes multiply under direct sum of vector bundles, this obstruction can be stated intrinsically in terms of the space \"M\" and its tangent bundle and cohomology algebra. This obstruction was stated (in terms of the tangent bundle, not stable normal bundle) by Whitney.\n\nFor example, the Möbius strip has non-trivial tangent bundle, so it cannot immerse in codimension 0 (in R), though it embeds in codimension 1 (in R).\n\nCodimension 0 immersions are equivalently \"relative\" dimension 0 \"submersions\", and are better thought of as submersions. A codimension 0 immersion of a closed manifold is precisely a covering map, i.e., a fiber bundle with 0-dimensional (discrete) fiber. By Ehresmann's theorem and Phillips' theorem on submersions, a proper submersion of manifolds is a fiber bundle, hence codimension/relative dimension 0 immersions/submersions behave like submersions.\n\nFurther, codimension 0 immersions do not behave like other immersions, which are largely determined by the stable normal bundle: in codimension 0 one has issues of fundamental class and cover spaces. For instance, there is no codimension 0 immersion , despite the circle being parallelizable, which can be proven because the line has no fundamental class, so one does not get the required map on top cohomology. Alternatively, this is by invariance of domain. Similarly, although S and the 3-torus T are both parallelizable, there is no immersion – any such cover would have to be ramified at some points, since the sphere is simply connected.\n\nAnother way of understanding this is that a codimension \"k\" immersion of a manifold corresponds to a codimension 0 immersion of a \"k\"-dimensional vector bundle, which is an \"open\" manifold if the codimension is greater than 0, but to a closed manifold in codimension 0 (if the original manifold is closed).\n\nA \"k\"-tuple point (double, triple, etc.) of an immersion is an unordered set of distinct points with the same image . If \"M\" is an \"m\"-dimensional manifold and \"N\" is an \"n\"-dimensional manifold then for an immersion in general position the set of \"k\"-tuple points is an -dimensional manifold. Every embedding is an immersion without multiple points (where ). Note, however, that the converse is false: there are injective immersions that are not embeddings.\n\nThe nature of the multiple points classifies immersions; for example, immersions of a circle in the plane are classified up to regular homotopy by the number of double points.\n\nAt a key point in surgery theory it is necessary to decide if an immersion of an \"m\"-sphere in a 2\"m\"-dimensional manifold is regular homotopic to an embedding, in which case it can be killed by surgery. Wall associated to \"f\" an invariant \"μ\"(\"f\") in a quotient of the fundamental group ring Z[π(\"N\")] which counts the double points of \"f\" in the universal cover of \"N\". For , \"f\" is regular homotopic to an embedding if and only if by the Whitney trick.\n\nOne can study embeddings as \"immersions without multiple points\", since immersions are easier to classify. Thus, one can start from immersions and try to eliminate multiple points, seeing if one can do this without introducing other singularities – studying \"multiple disjunctions\". This was first done by André Haefliger, and this approach is fruitful in codimension 3 or more – from the point of view of surgery theory, this is \"high (co)dimension\", unlike codimension 2 which is the knotting dimension, as in knot theory. It is studied categorically via the \"calculus of functors\" by Thomas Goodwillie, John Klein, and Michael S. Weiss.\n\n\n\nImmersed plane curves have a well-defined turning number, which can be defined as the total curvature divided by 2\"π\". This is invariant under regular homotopy, by the Whitney–Graustein theorem – topologically, it is the degree of the Gauss map, or equivalently the winding number of the unit tangent (which does not vanish) about the origin. Further, this is a complete set of invariants – any two plane curves with the same turning number are regular homotopic.\n\nEvery immersed plane curve lifts to an embedded space curve via separating the intersection points, which is not true in higher dimensions. With added data (which strand is on top), immersed plane curves yield knot diagrams, which are of central interest in knot theory. While immersed plane curves, up to regular homotopy, are determined by their turning number, knots have a very rich and complex structure.\n\nThe study of immersed surfaces in 3-space is closely connected with the study of knotted (embedded) surfaces in 4-space, by analogy with the theory of knot diagrams (immersed plane curves (2-space) as projections of knotted curves in 3-space): given a knotted surface in 4-space, one can project it to an immersed surface in 3-space, and conversely, given an immersed surface in 3-space, one may ask if it lifts to 4-space – is it the projection of a knotted surface in 4-space? This allows one to relate questions about these objects.\n\nA basic result, in contrast to the case of plane curves, is that not every immersed surface lifts to a knotted surface. In some cases the obstruction is 2-torsion, such as in \"Koschorke's example\", which is an immersed surface (formed from 3 Möbius bands, with a triple point) that does not lift to a knotted surface, but it has a double cover that does lift. A detailed analysis is given in , while a more recent survey is given in .\n\nA far-reaching generalization of immersion theory is the homotopy principle:\none may consider the immersion condition (the rank of the derivative is always \"k\") as a partial differential relation (PDR), as it can be stated in terms of the partial derivatives of the function. Then Smale–Hirsch immersion theory is the result that this reduces to homotopy theory, and the homotopy principle gives general conditions and reasons for PDRs to reduce to homotopy theory.\n\n\n\n"}
{"id": "2147961", "url": "https://en.wikipedia.org/wiki?curid=2147961", "title": "Invariants of tensors", "text": "Invariants of tensors\n\nIn mathematics, in the fields of multilinear algebra and representation theory, invariants of tensors are coefficients of the characteristic polynomial of the tensor \"A\":\n\nwhere formula_2 is the identity tensor and formula_3 is the polynomial's indeterminate (it is important to bear in mind that a polynomial's indeterminate formula_4 may also be a non-scalar as long as power, scaling and adding make sense for it, e.g., formula_5 is legitimate, and in fact, quite useful).\n\nThe first invariant of an \"n\"×\"n\" tensor A (formula_6) is the coefficient for formula_7 (because the coefficient for formula_8 is always 1), the second invariant (formula_9) is the coefficient for formula_10, etc., the \"n\"th invariant is the free term.\n\nThe first invariant (trace) is always the sum of the diagonal components:\nThe \"n\"th invariant is just formula_12, the determinant of formula_13 (up to sign).\n\nThe invariants do not change with rotation of the coordinate system (they are objective). Obviously, any function of the invariants only is also objective.\n\nMost tensors used in engineering are symmetric 3×3.\nFor this case the invariants can be calculated as:\n\n(the sum of principal minors). This expression holds for non-symmetric tensors. \nwhere formula_17, formula_18, formula_19 are the eigenvalues of tensor \"A\".\n\nBecause of the Cayley–Hamilton theorem the following equation is always true:\nwhere \"E\" is the second-order identity tensor.\n\nA similar equation holds for tensors of higher order.\n\nA scalar valued tensor function \"f\" that depends merely on the three invariants of a symmetric 3×3 tensor formula_13 is objective, i.e., independent from rotations of the coordinate system. Moreover, every objective tensor function depends only on the tensor's invariants. Thus, objectivity of a tensor function is fulfilled if, and only if, for some function formula_22 we have\n\nA common application to this is the evaluation of the potential energy as function of the strain tensor, within the framework of linear elasticity. Exhausting the above theorem the free energy of the system reduces to a function of 3 scalar parameters rather than 6. Within linear elasticity the free energy has to be quadratic in the tensor's elements, which eliminates an additional scalar. Thus, for an isotropic material only two independent parameters are needed to describe the elastic properties, known as Lamé coefficients. Consequently, experimental fits and computational efforts may be eased significantly.\n\nThis technique was first introduced into isotropic turbulence by Howard P. Robertson in 1940 and he was able to derive Kármán–Howarth equation from the invariant principle. George Batchelor and Subrahmanyan Chandrasekhar exploited this technique by extending to axisymmetric turbulence.\n\nSince the invariants are constant in any reference, functions of invariants are also constant. Some sources define the three invariants of the 3×3 tensors as\n\nTherefore,\n\n"}
{"id": "46900869", "url": "https://en.wikipedia.org/wiki?curid=46900869", "title": "Label Propagation Algorithm", "text": "Label Propagation Algorithm\n\nLabel Propagation is a semi-supervised machine learning algorithm that assigns labels to previously unlabeled data points. At the start of the algorithm, a (generally small) subset of the data points have labels (or classifications). These labels are propagated to the unlabeled points throughout the course of the algorithm.\n\nWithin complex networks, real networks tend to have community structure. Label propagation is an algorithm for finding communities. In comparison with other algorithms label propagation has advantages in its running time and amount of a priori information needed about the network structure (no parameter is required to be known beforehand). The disadvantage is that it produces no unique solution, but an aggregate of many solutions.\n\nAt initial condition, the nodes carry a label that denotes the community they belong to. Membership in a community changes based on the labels that the neighboring nodes possess. This change is subject to the maximum number of labels within one degree of the nodes. Every node is initialized with a unique label, then the labels diffuse through the network. Consequently, densely connected groups reach a common label quickly. When many such dense (consensus) groups are created throughout the network, they continue to expand outwards until it is impossible to do so.\n\nThe process has 5 steps:\n\n1. Initialize the labels at all nodes in the network. For a given node x, C (0) = x.\n\n2. Set t = 1.\n\n3. Arrange the nodes in the network in a random order and set it to X.\n\n4. For each x ∈ X chosen in that specific order, let C(t) = f(C(t), ...,C(t),C (t − 1), ...,C (t − 1)). f here returns the label occurring with the highest frequency among neighbours.\n\n5. If every node has a label that the maximum number of their neighbours have, then stop the algorithm. Else, set t = t + 1 and go to (3).\n\nIn contrast with other algorithms label propagation can result in various community structures from the same initial condition. The range of solutions can be narrowed if some nodes are given preliminary labels while others are held unlabelled. Consequently, unlabelled nodes will be more likely to adapt to the labelled ones. For a more accurate finding of communities, Jaccard’s index is used to aggregate multiple community structures, containing all important information.\n\n"}
{"id": "138677", "url": "https://en.wikipedia.org/wiki?curid=138677", "title": "Landau's function", "text": "Landau's function\n\nIn mathematics, Landau's function \"g\"(\"n\"), named after Edmund Landau, is defined for every natural number \"n\" to be the largest order of an element of the symmetric group \"S\". Equivalently, \"g\"(\"n\") is the largest least common multiple (lcm) of any partition of \"n\", or the maximum number of times a permutation of \"n\" elements can be recursively applied to itself before it returns to its starting sequence.\n\nFor instance, 5 = 2 + 3 and lcm(2,3) = 6. No other partition of 5 yields a bigger lcm, so \"g\"(5) = 6. An element of order 6 in the group \"S\" can be written in cycle notation as (1 2) (3 4 5). Note that the same argument applies to the number 6, that is, \"g\"(6) = 6. There are arbitrarily long sequences of consecutive numbers \"n\", \"n\" + 1, …, \"n\" + \"m\" on which the function \"g\" is constant.\n\nThe integer sequence \"g\"(0) = 1, \"g\"(1) = 1, \"g\"(2) = 2, \"g\"(3) = 3, \"g\"(4) = 4, \"g\"(5) = 6, \"g\"(6) = 6, \"g\"(7) = 12, \"g\"(8) = 15, ... is named after Edmund Landau, who proved in 1902 that\n(where ln denotes the natural logarithm). In other words, formula_2.\n\nThe statement that\nfor all sufficiently large \"n\", where Li denotes the inverse of the logarithmic integral function, is equivalent to the Riemann hypothesis.\n\nIt can be shown that\nand indeed\n\n"}
{"id": "404009", "url": "https://en.wikipedia.org/wiki?curid=404009", "title": "List of mathematics-based methods", "text": "List of mathematics-based methods\n\nThis is a list of mathematics-based methods.\n\n\n"}
{"id": "5646030", "url": "https://en.wikipedia.org/wiki?curid=5646030", "title": "Lute of Pythagoras", "text": "Lute of Pythagoras\n\nThe lute of Pythagoras is a self-similar geometric figure made from a sequence of pentagrams.\n\nThe lute may be drawn from a sequence of pentagrams.\nThe centers of the pentagraphs lie on a line and (except for the first and largest of them) each shares two vertices with the next larger one in the sequence.\n\nAn alternative construction is based on the golden triangle, an isosceles triangle with base angles of 72° and apex angle 36°. Two smaller copies of the same triangle may be drawn inside the given triangle, having the base of the triangle as one of their sides. The two new edges of these two smaller triangles, together with the base of the original golden triangle, form three of the five edges of the polygon. Adding a segment between the endpoints of these two new edges cuts off a smaller golden triangle, within which the construction can be repeated.\n\nSome sources add another pentagram, inscribed within the inner pentagon of the largest pentagram of the figure. The other pentagons of the figure do not have inscribed pentagrams.\n\nThe convex hull of the lute is a kite shape with three 108° angles and one 36° angle. The sizes of any two consecutive pentagrams in the sequence are in the golden ratio to each other, and many other instances of the golden ratio appear within the lute.\n\nThe lute is named after the ancient Greek mathematician Pythagoras, but its origins are unclear. An early reference to it is in a 1990 book on the golden ratio by Boles and Newman.\n\n"}
{"id": "104509", "url": "https://en.wikipedia.org/wiki?curid=104509", "title": "Maxim Kontsevich", "text": "Maxim Kontsevich\n\nMaxim Lvovich Kontsevich (;; born 25 August 1964) is a Russian and French mathematician. He is a professor at the Institut des Hautes Études Scientifiques and a distinguished professor at the University of Miami. He received the Henri Poincaré Prize in 1997, the Fields Medal in 1998, the Crafoord Prize in 2008, the Shaw Prize and Fundamental Physics Prize in 2012, and the Breakthrough Prize in Mathematics in 2014.\n\nHe was born into the family of Lev Rafailovich Kontsevich, Soviet orientalist and author of the Kontsevich system. After ranking second in the All-Union Mathematics Olympiads, he attended Moscow State University but left without a degree in 1985 to become a researcher at the Institute for Problems of Information Transmission in Moscow. While at the institute he managed to publish some papers that got the interest of the Max Planck institute in Bonn so he got invited for 3 months there just before its end he attended a 5 day international meeting:the Arbeitstagung,he sketched a proof of the Witten conjecture to the amazement of Michael Atiyah and other mathematicians so his invitation to the institute was extended to 3 years,the next year he finished the proof and worked on various topics on mathematical physics and in 1992 received his Ph.D. at the University of Bonn under Don Bernard Zagier. His thesis outlines a proof of a conjecture by Edward Witten that two quantum gravitational models are equivalent.\n\nHis work concentrates on geometric aspects of mathematical physics, most notably on knot theory, quantization, and mirror symmetry. One of his results is a formal deformation quantization that holds for any Poisson manifold. He also introduced knot invariants defined by complicated integrals analogous to Feynman integrals. In topological field theory, he introduced the moduli space of stable maps, which may be considered a mathematically rigorous formulation of the Feynman integral for topological string theory.\n\nIn 1998, he won the Fields Medal for his \"contributions to four problems of Geometry\". In July 2012, he was an inaugural awardee of the Fundamental Physics Prize, the creation of physicist and internet entrepreneur, Yuri Milner. In 2014, he was awarded Breakthrough Prize in Mathematics.\n\n\n\n"}
{"id": "2115190", "url": "https://en.wikipedia.org/wiki?curid=2115190", "title": "Meyer's theorem", "text": "Meyer's theorem\n\nIn number theory, Meyer's theorem on quadratic forms states that an indefinite quadratic form \"Q\" in five or more variables over the field of rational numbers nontrivially represents zero. In other words, if the equation\n\nhas a non-zero real solution, then it has a non-zero rational solution (the converse is obvious). By clearing the denominators, an integral solution \"x\" may also be found.\n\nMeyer's theorem is usually deduced from the Hasse–Minkowski theorem (which was proved later) and the following statement: \n\nMeyer's theorem is best possible with respect to the number of variables: there are indefinite rational quadratic forms \"Q\" in four variables which do not represent zero. One family of examples is given by \n\nwhere \"p\" is a prime number that is congruent to 3 modulo 4. This can be proved by the method of infinite descent using the fact that if the sum of two perfect squares is divisible by such a \"p\" then each summand is divisible by \"p\".\n\n\n"}
{"id": "39338633", "url": "https://en.wikipedia.org/wiki?curid=39338633", "title": "Nested triangles graph", "text": "Nested triangles graph\n\nIn graph theory, a nested triangles graph with \"n\" vertices is a planar graph formed from a sequence of \"n\"/3 triangles, by connecting pairs of corresponding vertices on consecutive triangles in the sequence. It can also be formed geometrically, by gluing together \"n\"/3 − 1 triangular prisms on their triangular faces.\nThis graph, and graphs closely related to it, have been frequently used in graph drawing to prove lower bounds on the area requirements of various styles of drawings.\n\nThe nested triangles graph with two triangles is the graph of the triangular prism, and the nested triangles graph with three triangles is the graph of the triangular bifrustum.\nMore generally, because the nested triangles graphs are planar and 3-vertex-connected, it follows from Steinitz's theorem that they all can be represented as convex polyhedra.\n\nAn alternative geometric representation of these graphs may be given by gluing triangular prisms end-to-end on their triangular faces; the number of nested triangles is one more than the number of glued prisms. However, using right prisms, this gluing process will cause the rectangular faces of adjacent prisms to be coplanar, so the result will not be strictly convex.\n\nThe nested triangles graph was named by , who used it to show that drawing an \"n\"-vertex planar graph in the integer lattice (with straight line-segment edges) may require a bounding box of size at least \"n\"/3 × \"n\"/3. In such a drawing, no matter which face of the graph is chosen to be the outer face, some subsequence of at least \"n\"/6 of the triangles must be drawn nested within each other, and within this part of the drawing each triangle must use two rows and two columns more than the next inner triangle. If the outer face is not allowed to be chosen as part of the drawing algorithm, but is specified as part of the input, the same argument shows that a bounding box of size 2\"n\"/3 × 2\"n\"/3 is necessary, and a drawing with these dimensions exists.\n\nFor drawings in which the outer face may be freely chosen, the area lower bound of may not be tight.\nVariants of the nested triangles graph have been used for many other lower bound constructions in graph drawing, for instance on area of rectangular visibility representations, area of drawings with right angle crossings or relative area of planar versus nonplanar drawings.\n"}
{"id": "51719806", "url": "https://en.wikipedia.org/wiki?curid=51719806", "title": "Parallel redrawing", "text": "Parallel redrawing\n\nIn geometric graph theory, and the theory of structural rigidity, a parallel redrawing of a graph drawing with straight edges in the Euclidean plane or higher-dimensional Euclidean space is another drawing of the same graph such that all edges of the second drawing are parallel to their corresponding edges in the first drawing. A parallel morph of a graph is a continuous family of drawings, all parallel redrawings of each other.\n\nParallel redrawings include translations, scaling, and other modifications of the drawing that change it more locally. For instance, for graphs drawn as the vertices or edges of a simple polyhedron, a parallel drawing can be obtained by translating the plane of one of the polyhedron's face, and adjusting the positions of the vertices and edges that border that face. A polyhedron is said to be tight if its only parallel redrawings are similarities (combinations of translation and scaling); among the Platonic solids, the cube and dodecahedron are not tight (because of the possibility of translating one face while keeping the others fixed), but the tetrahedron, octahedron, and icosahedron are tight.\n\nIn three dimensions, even for drawings where all edges are axis-parallel and the drawing forms the boundary of a polyhedron, there may exist parallel redrawings that cannot be connected by a parallel morph. For two-dimensional planar drawings, with parallel edges required to preserve their orientation, a morph always exists when the slope number is two, but it is NP-hard to determine the existence of a morph for three or more slopes.\nAny parallel morph can be parameterized so that the each point moves with constant speed along a line. The graphs that remain planar throughout such a motion can be derived from pseudotriangulations.\n\nIn structural rigidity, the existence of (infinitesimal) parallel redrawings of a structural framework is dual to the existence of an infinitesimal motion, one that preserves its edge lengths but not their orientations. Thus, a framework has one kind of motion if it has the other kind, but detecting the existence of a parallel redrawing may be easier than detecting the existence of an infinitesimal motion.\n"}
{"id": "892899", "url": "https://en.wikipedia.org/wiki?curid=892899", "title": "Parsing expression grammar", "text": "Parsing expression grammar\n\nIn computer science, a parsing expression grammar, or PEG, is a type of analytic formal grammar, i.e. it describes a formal language in terms of a set of rules for recognizing strings in the language. The formalism was introduced by Bryan Ford in 2004 and is closely related to the family of top-down parsing languages introduced in the early 1970s.\nSyntactically, PEGs also look similar to context-free grammars (CFGs), but they have a different interpretation: the choice operator selects the first match in PEG, while it is ambiguous in CFG. This is closer to how string recognition tends to be done in practice, e.g. by a recursive descent parser.\n\nUnlike CFGs, PEGs cannot be ambiguous; if a string parses, it has exactly one valid parse tree. It is conjectured that there exist context-free languages that cannot be recognized by a PEG, but this is not yet proven. PEGs are well-suited to parsing computer languages (and artificial human languages such as Lojban), but not natural languages where the performance of PEG algorithms is comparable to general CFG algorithms such as the Earley algorithm.\n\nFormally, a parsing expression grammar consists of:\n\nEach parsing rule in \"P\" has the form \"A\" ← \"e\", where \"A\" is a nonterminal symbol and \"e\" is a \"parsing expression\". A parsing expression is a hierarchical expression similar to a regular expression, which is constructed in the following fashion:\n\nThe fundamental difference between context-free grammars and parsing expression grammars is that the PEG's choice operator is \"ordered\". If the first alternative succeeds, the second alternative is ignored. Thus ordered choice is not commutative, unlike unordered choice as in context-free grammars. Ordered choice is analogous to soft cut operators available in some logic programming languages.\n\nThe consequence is that if a CFG is transliterated directly to a PEG, any ambiguity in the former is resolved by deterministically picking one parse tree from the possible parses. By carefully choosing the order in which the grammar alternatives are specified, a programmer has a great deal of control over which parse tree is selected.\n\nLike boolean context-free grammars, parsing expression grammars also add the and- and not- syntactic predicates. Because they can use an arbitrarily complex sub-expression to \"look ahead\" into the input string without actually consuming it, they provide a powerful syntactic lookahead and disambiguation facility, in particular when reordering the alternatives cannot specify the exact parse tree desired.\n\nEach nonterminal in a parsing expression grammar essentially represents a parsing function in a recursive descent parser, and the corresponding parsing expression represents the \"code\" comprising the function. Each parsing function conceptually takes an input string as its argument, and yields one of the following results:\n\nAn atomic parsing expression consisting of a single terminal (i.e. literal) succeeds if the first character of the input string matches that terminal, and in that case consumes the input character; otherwise the expression yields a failure result. An atomic parsing expression consisting of the empty string always trivially succeeds without consuming any input.\nAn atomic parsing expression consisting of a nonterminal \"A\" represents a recursive call to the nonterminal-function \"A\". A nonterminal may succeed without actually consuming any input, and this is considered an outcome distinct from failure.\n\nThe sequence operator \"e\" \"e\" first invokes \"e\", and if \"e\" succeeds, subsequently invokes \"e\" on the remainder of the input string left unconsumed by \"e\", and returns the result. If either \"e\" or \"e\" fails, then the sequence expression \"e\" \"e\" fails.\n\nThe choice operator \"e\" / \"e\" first invokes \"e\", and if \"e\" succeeds, returns its result immediately. Otherwise, if \"e\" fails, then the choice operator backtracks to the original input position at which it invoked \"e\", but then calls \"e\" instead, returning \"e\"'s result.\n\nThe zero-or-more, one-or-more, and optional operators consume zero or more, one or more, or zero or one consecutive repetitions of their sub-expression \"e\", respectively. Unlike in context-free grammars and regular expressions, however, these operators \"always\" behave greedily, consuming as much input as possible and never backtracking. (Regular expression matchers may start by matching greedily, but will then backtrack and try shorter matches if they fail to match.) For example, the expression a* will always consume as many a's as are consecutively available in the input string, and the expression (a* a) will always fail because the first part (a*) will never leave any a's for the second part to match.\n\nThe and-predicate expression &\"e\" invokes the sub-expression \"e\", and then succeeds if \"e\" succeeds and fails if \"e\" fails, but in either case \"never consumes any input\".\n\nThe not-predicate expression !\"e\" succeeds if \"e\" fails and fails if \"e\" succeeds, again consuming no input in either case.\n\nThis is a PEG that recognizes mathematical formulas that apply the basic four operations to non-negative integers.\n\nIn the above example, the terminal symbols are characters of text, represented by characters in single quotes, such as codice_1 and codice_2. The range codice_3 is also a shortcut for ten characters, indicating any one of the digits 0 through 9. (This range syntax is the same as the syntax used by regular expressions.) The nonterminal symbols are the ones that expand to other rules: \"Value\", \"Product\", \"Sum\", and \"Expr\".\n\nThe following recursive rule matches standard C-style if/then/else statements in such a way that the optional \"else\" clause always binds to the innermost \"if\", because of the implicit prioritization of the '/' operator. (In a context-free grammar, this construct yields the classic dangling else ambiguity.)\nThe following recursive rule matches Pascal-style nested comment syntax, (* which can (* nest *) like this *). The comment symbols appear in single quotes to distinguish them from PEG operators.\nThe parsing expression foo &(bar) matches and consumes the text \"foo\" but only if it is followed by the text \"bar\". The parsing expression foo !(bar) matches the text \"foo\" but only if it is \"not\" followed by the text \"bar\". The expression !(a+ b) a matches a single \"a\" but only if it is not part of an arbitrarily long sequence of a's followed by a b.\n\nThe parsing expression ('a'/'b')* matches and consumes an arbitrary-length sequence of a's and b's. The production rule describes the simple context-free \"matching language\" formula_1.\nThe following parsing expression grammar describes the classic non-context-free language formula_2:\nAny parsing expression grammar can be converted directly into a recursive descent parser. Due to the unlimited lookahead capability that the grammar formalism provides, however, the resulting parser could exhibit exponential time performance in the worst case.\n\nIt is possible to obtain better performance for any parsing expression grammar by converting its recursive descent parser into a \"packrat parser\", which always runs in linear time, at the cost of substantially greater storage space requirements. A packrat parser\nis a form of parser similar to a recursive descent parser in construction, except that during the parsing process it memoizes the intermediate results of all invocations of the mutually recursive parsing functions, ensuring that each parsing function is only invoked at most once at a given input position. Because of this memoization, a packrat parser has the ability to parse many context-free grammars and \"any\" parsing expression grammar (including some that do not represent context-free languages) in linear time. Examples of memoized recursive descent parsers are known from at least as early as 1993.\nNote that this analysis of the performance of a packrat parser assumes that enough memory is available to hold all of the memoized results; in practice, if there is not enough memory, some parsing functions might have to be invoked more than once at the same input position, and consequently the parser could take more than linear time.\n\nIt is also possible to build LL parsers and LR parsers from parsing expression grammars, with better worst-case performance than a recursive descent parser, but the unlimited lookahead capability of the grammar formalism is then lost. Therefore, not all languages that can be expressed using parsing expression grammars can be parsed by LL or LR parsers.\n\nCompared to pure regular expressions (i.e. without back-references), PEGs are strictly more powerful, but require significantly more memory. For example, a regular expression inherently cannot find an arbitrary number of matched pairs of parentheses, because it is not recursive, but a PEG can. However, a PEG will require an amount of memory proportional to the length of the input, while a regular expression matcher will require only a constant amount of memory.\n\nAny PEG can be parsed in linear time by using a packrat parser, as described above.\n\nMany CFGs contain ambiguities, even when they're intended to describe unambiguous languages. The \"dangling else\" problem in C, C++, and Java is one example. These problems are often resolved by applying a rule outside of the grammar. In a PEG, these ambiguities never arise, because of prioritization.\n\nPEG parsing is typically carried out via \"packrat parsing\", which uses memoization to eliminate redundant parsing steps. Packrat parsing requires storage proportional to the total input size, rather than the depth of the parse tree as with LR parsers. This is a significant difference in many domains: for example, hand-written source code has an effectively constant expression nesting depth independent of the length of the program—expressions nested beyond a certain depth tend to get refactored.\n\nFor some grammars and some inputs, the depth of the parse tree can be proportional to the input size,\nso both an LR parser and a packrat parser will appear to have the same worst-case asymptotic performance. A more accurate analysis would take the depth of the parse tree into account separately from the input size. This is similar to a situation which arises in graph algorithms: the Bellman–Ford algorithm and Floyd–Warshall algorithm appear to have the same running time (formula_3) if only the number of vertices is considered. However, a more precise analysis which accounts for the number of edges as a separate parameter assigns the Bellman–Ford algorithm a time of formula_4, which is quadratic for sparse graphs with formula_5.\n\nA PEG is called \"well-formed\" if it contains no \"left-recursive\" rules, i.e., rules that allow a nonterminal to expand to an expression in which the same nonterminal occurs as the leftmost symbol. For a left-to-right top-down parser, such rules cause infinite regress: parsing will continually expand the same nonterminal without moving forward in the string.\n\nTherefore, to allow packrat parsing, left recursion must be eliminated. For example, in the arithmetic grammar below, it would be tempting to move some rules around so that the precedence order of products and sums could be expressed in one line:\n\nIn this new grammar, matching an Expr requires testing if a Product matches while matching a Product requires testing if an Expr matches. Because the term appears in the leftmost position, these rules make up a circular definition that cannot be resolved. (Circular definitions that can be resolved exist—such as in the original formulation from the first example—but such definitions are required not to exhibit pathological recursion.) However, left-recursive rules can always be rewritten to eliminate left-recursion. For example, the following left-recursive CFG rule:\ncan be rewritten in a PEG using the plus operator:\nThe process of rewriting \"indirectly\" left-recursive rules is complex in some packrat parsers, especially when semantic actions are involved.\n\nWith some modification, traditional packrat parsing can support direct left recursion, but doing so results in a loss of the linear-time parsing property which is generally the justification for using PEGs and packrat parsing in the first place. Only the OMeta parsing algorithm supports full direct and indirect left recursion without additional attendant complexity (but again, at a loss of the linear time complexity), whereas all GLR parsers support left recursion.\n\nPEG packrat parsers cannot recognize some unambiguous nondeterministic CFG rules, such as the following:\n\nNeither LL(k) nor LR(k) parsing algorithms are capable of recognizing this example. However, this grammar can be used by a general CFG parser like the CYK algorithm. Note however that the \"language\" in question can be recognised by all these types of parser, since it is in fact a regular language (that of strings of an odd number of x's).\n\nIt is an open problem to give a concrete example of a context-free language which cannot be recognized by a parsing expression grammar.\n\nLL(k) and LR(k) parser generators will fail to complete when the input grammar is ambiguous. This is a feature in the common case that the grammar is intended to be unambiguous but is defective. A PEG parser generator will resolve unintended ambiguities earliest-match-first, which may be arbitrary and lead to surprising parses.\n\n\n"}
{"id": "54323260", "url": "https://en.wikipedia.org/wiki?curid=54323260", "title": "Pi Day Die Day", "text": "Pi Day Die Day\n\nPi Day Die Day is a 2016 horror comedy produced by One Stoplight Productions and Cullen Park Productions that revolves around a group of detectives seeking to thwart the plans of a killer at a local high school on Pi Day. It was directed by Michael E. Cullen II and written by Lindsey LaForest and stars Ari Lehman. Shot on location in Ohio, a fundraising campaign was started on Indiegogo to help supplement the film's budget. The film had its official premiere on March 12, 2016, gathering mixed reviews, and was also released on home media.\n\nA killer is on the loose at a respectable high school in Euclid Falls, and the principal and a group of detectives must enlist the help of a mathematics teacher to discover the killer's identity and thwart his diabolical plans.\n\n\"Pie Day Die Day\" was directed by Michael E. Cullen II, and most of the filming took place in Haskins, Ohio; the film was written by co-producer Lindsey LaForest, who described it as a \"slashomedy\" (slasher-cum-comedy). Actor Colton Tapp, who stars in the film, also expressed that the film \"pays homage to the big teen slashers\". The entire cast comprises \"local talent\" including Tapp and Ari Lehman (\"Friday the 13th\"), as well as comedian Steve Sabo, who made a cameo appearance. One Stoplight Productions and Cullen Productions also received an additional $3,970 in funds through the crowdfunding website Indiegogo.\n\n\"Pie Day Die Day\" had a limited release in the United States on March 12, 2016. It premiered at the Cle-Zel Theatre in Bowling Green, and was screened at the Maumee Indoor Theatre two days later on Pi Day. It is also available on DVD. The film was given 2.5 out of 5 stars by Matt Boiselle of \"Dread Central\", who writes that \"the schizophrenic mix of both practical and CGI effects\" overwhelm the \"entertaining\" aspects of the film. \"Reverend Leviathan\", writing for the web magazine \"DarkestMagazine Goth\", gave the film a positive review, lauding in particular the \"tongue-in-cheek humor\" and the \"surprise ending\".\n\n"}
{"id": "44108758", "url": "https://en.wikipedia.org/wiki?curid=44108758", "title": "Quantum machine learning", "text": "Quantum machine learning\n\nQuantum machine learning is an emerging interdisciplinary research area at the intersection of quantum physics and machine learning. The most common use of the term refers to machine learning algorithms for the analysis of classical data executed on a quantum computer. This includes hybrid methods that involve both classical and quantum processing, where computationally expensive subroutines are outsourced to a quantum device. Furthermore, quantum algorithms can be used to analyze quantum states instead of classical data. Beyond quantum computing, the term \"quantum machine learning\" is often associated with machine learning methods applied to data generated from quantum experiments, such as learning quantum phase transitions or creating new quantum experiments. Quantum machine learning also extends to a branch of research that explores methodological and structural similarities between certain physical systems and learning systems, in particular neural networks. For example, some mathematical and numerical techniques from quantum physics carry over to classical deep learning and vice versa. Finally, researchers investigate more abstract notions of learning theory with respect to quantum information, sometimes referred to as \"quantum learning theory\".\n\nQuantum-enhanced machine learning refers to quantum algorithms that solve tasks in machine learning, thereby improving a classical machine learning method. Such algorithms typically require one to encode the given classical dataset into a quantum computer to make it accessible for quantum information processing. After this, quantum information processing routines can be applied and the result of the quantum computation is read out by measuring the quantum system. For example, the outcome of the measurement of a qubit could reveal the result of a binary classification task. While many proposals of quantum machine learning algorithms are still purely theoretical and require a full-scale universal quantum computer to be tested, others have been implemented on small-scale or special purpose quantum devices.\n\nA number of quantum algorithms for machine learning are based on the idea of \"amplitude encoding\", that is, to associate the amplitudes of a quantum state with the inputs and outputs of computations. Since a state of formula_1qubits is described by formula_2 complex amplitudes, this information encoding can allow for an exponentially compact representation. Intuitively, this corresponds to associating a discrete probability distribution over binary random variables with a classical vector. The goal of algorithms based on amplitude encoding is to formulate quantum algorithms whose resources grow polynomially in the number of qubits formula_1, which amounts to a logarithmic growth in the number of amplitudes and thereby the dimension of the input.\n\nMany quantum machine learning algorithms in this category are based on variations of the quantum algorithm for linear systems of equations which, under specific conditions, performs a matrix inversion using an amount of physical resources growing only logarithmically in the dimensions of the matrix. One of these conditions is that a Hamiltonian which entrywise corresponds to the matrix can be simulated efficiently, which is known to be possible if the matrix is sparse or low rank. For reference, any known classical algorithm for matrix inversion requires a number of operations that grows at least quadratically in the dimension of the matrix.\n\nQuantum matrix inversion can be applied to machine learning methods in which the training reduces to solving a linear system of equations, for example in least-squares linear regression, the least-squares version of support vector machines, and Gaussian processes.\n\nA crucial bottleneck of methods that simulate linear algebra computations with the amplitudes of quantum states is state preparation, which often requires one to initialise a quantum system in a state whose amplitudes reflect the features of the entire dataset. Although efficient methods for state preparation are known for specific cases, this step easily hides the complexity of the task.\n\nAnother approach to improving classical machine learning with quantum information processing uses amplitude amplification methods based on Grover's search algorithm, which has been shown to solve unstructured search problems with a quadratic speedup compared to classical algorithms. These quantum routines can be employed for learning algorithms that translate into an unstructured search task, as can be done, for instance, in the case of the k-medians and the k-nearest neighbors algorithms. Another application is a quadratic speedup in the training of perceptron.\n\nAmplitude amplification is often combined with quantum walks to achieve the same quadratic speedup. Quantum walks have been proposed to enhance Google's PageRank algorithm as well as the performance of reinforcement learning agents in the projective simulation framework.\n\nReinforcement learning is a third branch of machine learning, distinct from supervised and unsupervised learning, which also admits quantum enhancements. In quantum-enhanced reinforcement learning, a quantum agent interacts with a classical environment and occasionally receives rewards for its actions, which allows the agent to adapt its behavior—in other words, to learn what to do in order to gain more rewards. In some situations, either because of the quantum processing capability of the agent, or due to the possibility to probe the environment in superpositions, a quantum speedup may be achieved. Implementations of these kinds of protocols in superconducting circuits and in systems of trapped ions have been proposed.\n\nSampling from high-dimensional probability distributions is at the core of a wide spectrum of computational techniques with important applications across science, engineering, and society. Examples include deep learning, probabilistic programming, and other machine learning and artificial intelligence applications.\n\nA computationally hard problem, which is key for some relevant machine learning tasks, is the estimation of averages over probabilistic models defined in terms of a Boltzmann distribution. Sampling from generic probabilistic models is hard: algorithms relying heavily on sampling are expected to remain intractable no matter how large and powerful classical computing resources become. Even though quantum annealers, like those produced by D-Wave Systems, were designed for challenging combinatorial optimization problems, it has been recently recognized as a potential candidate to speed up computations that rely on sampling by exploiting quantum effects.\n\nSome research groups have recently explored the use of quantum annealing hardware for training Boltzmann machines and deep neural networks. The standard approach to training Boltzmann machines relies on the computation of certain averages that can be estimated by standard sampling techniques, such as Markov chain Monte Carlo algorithms. Another possibility is to rely on a physical process, like quantum annealing, that naturally generates samples from a Boltzmann distribution. The objective is to find the optimal control parameters that best represent the empirical distribution of a given dataset.\n\nThe D-Wave 2X system hosted at NASA Ames Research Center has been recently used for the learning of a special class of restricted Boltzmann machines that can serve as a building block for deep learning architectures. Complementary work that appeared roughly simultaneously showed that quantum annealing can be used for supervised learning in classification tasks. The same device was later used to train a fully connected Boltzmann machine to generate, reconstruct, and classify down-scaled, low-resolution handwritten digits, among other synthetic datasets. In both cases, the models trained by quantum annealing had a similar or better performance in terms of quality. The ultimate question that drives this endeavour is whether there is quantum speedup in sampling applications. Experience with the use of quantum annealers for combinatorial optimization suggests the answer is not straightforward.\n\nInspired by the success of Boltzmann machines based on classical Boltzmann distribution, a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian was recently proposed. Due to the non-commutative nature of quantum mechanics, the training process of the quantum Boltzmann machine can become nontrivial. This problem was, to some extent, circumvented by introducing bounds on the quantum probabilities, allowing the authors to train the model efficiently by sampling. It is possible that a specific type of quantum Boltzmann machine has been trained in the D-Wave 2X by using a learning rule analogous to that of classical Boltzmann machines.\n\nQuantum annealing is not the only technology for sampling. In a prepare-and-measure scenario, a universal quantum computer prepares a thermal state, which is then sampled by measurements. This can reduce the time required to train a deep restricted Boltzmann machine, and provide a richer and more comprehensive framework for deep learning than classical computing. The same quantum methods also permit efficient training of full Boltzmann machines and multi-layer, fully connected models and do not have well-known classical counterparts. Relying on an efficient thermal state preparation protocol starting from an arbitrary state, quantum-enhanced Markov logic networks exploit the symmetries and the locality structure of the probabilistic graphical model generated by a first-order logic template. This provides an exponential reduction in computational complexity in probabilistic inference, and, while the protocol relies on a universal quantum computer, under mild assumptions it can be embedded on contemporary quantum annealing hardware.\n\nQuantum analogues or generalizations of classical neural nets are often referred to as quantum neural networks. The term is claimed by a wide range of approaches, including the implementation and extension of neural networks using photons, layered variational circuits or quantum Ising-type models.\n\nHidden Quantum Markov Models (HQMMs) are a quantum-enhanced version of classical Hidden Markov Models (HMMs), which are typically used to model sequential data in various fields like robotics and natural language processing. Unlike the approach taken by other quantum-enhanced machine learning algorithms, HQMMs can be viewed as models inspired by quantum mechanics that can be run on classical computers as well. Where classical HMMs use probability vectors to represent hidden 'belief' states, HQMMs use the quantum analogue: density matrices. Recent work has shown that these models can be successfully learned by maximizing the log-likelihood of the given data via classical optimization, and there is some empirical evidence that these models can better model sequential data compared to classical HMMs in practice, although further work is needed to determine exactly when and how these benefits are derived. Additionally, since classical HMMs are a particular kind of Bayes net, an exciting aspect of HQMMs is that the techniques used show how we can perform quantum-analogous Bayesian inference, which should allow for the general construction of the quantum versions of probabilistic graphical models.\n\nIn the most general case of quantum machine learning, both the learning device and the system under study, as well as their interaction, are fully quantum. This section gives a few examples of results on this topic.\n\nOne class of problem that can benefit from the fully quantum approach is that of 'learning' unknown quantum states, processes or measurements, in the sense that one can subsequently reproduce them on another quantum system. For example, one may wish to learn a measurement that discriminates between two coherent states, given not a classical description of the states to be discriminated, but instead a set of example quantum systems prepared in these states. The naive approach would be to first extract a classical description of the states and then implement an ideal discriminating measurement based on this information. This would only require classical learning. However, one can show that a fully quantum approach is strictly superior in this case. (This also relates to work on quantum pattern matching.) The problem of learning unitary transformations can be approached in a similar way.\n\nGoing beyond the specific problem of learning states and transformations, the task of clustering also admits a fully quantum version, wherein both the oracle which returns the distance between data-points and the information processing device which runs the algorithm are quantum. Finally, a general framework spanning supervised, unsupervised and reinforcement learning in the fully quantum setting was introduced in, where it was also shown that the possibility of probing the environment in superpositions permits a quantum speedup in reinforcement learning.\n\nThe term quantum machine learning is also used for approaches that apply classical methods of machine learning to the study of quantum systems. A prime example is the use of classical learning techniques to process large amounts of experimental data in order to characterize an unknown quantum system (for instance in the context of quantum information theory and for the development of quantum technologies), but there are also more exotic applications.\n\nThe ability to experimentally control and prepare increasingly complex quantum systems brings with it a growing need to turn large and noisy data sets into meaningful information. This is a problem that has already been studied extensively in the classical setting, and consequently, many existing machine learning techniques can be naturally adapted to more efficiently address experimentally relevant problems. For example, Bayesian methods and concepts of algorithmic learning can be fruitfully applied to tackle quantum state classification, Hamiltonian learning, and the characterization of an unknown unitary transformation. Other problems that have been addressed with this approach are given in the following list:\n\n\nHowever, the characterization of quantum states and processes is not the only application of classical machine learning techniques. Some additional applications include\n\n\nQuantum learning theory pursues a mathematical analysis of the quantum generalizations of classical learning models and of the possible speed-ups or other improvements that they may provide. The framework is very similar to that of classical computational learning theory, but the learner in this case is a quantum information processing device, while the data may be either classical or quantum. Quantum learning theory should be contrasted with the quantum-enhanced machine learning discussed above, where the goal was to consider \"specific problems\" and to use quantum protocols to improve the time complexity of classical algorithms for these problems. Although quantum learning theory is still under development, partial results in this direction have been obtained.\n\nThe starting point in learning theory is typically a \"concept class\", a set of possible concepts. Usually a concept is a function on some domain, such as formula_4. For example, the concept class could be the set of disjunctive normal form (DNF) formulas on \"n\" bits or the set of Boolean circuits of some constant depth. The goal for the learner is to learn\n(exactly or approximately) an unknown \"target concept\" from this concept class. The learner may be actively interacting with the target concept, or passively receiving samples from it.\n\nIn active learning, a learner can make \"membership queries\" to the target concept \"c\", asking for its value \"c(x)\" on inputs \"x\" chosen by the learner. The learner then has to reconstruct the exact target concept, with high probability. In the model of \"quantum exact learning\", the learner can make membership queries in quantum superposition. If the complexity of the learner is measured by the number of membership queries it makes, then quantum exact learners can be polynomially more efficient than classical learners for some concept classes, but not more. If complexity is measured by the amount of \"time\"\nthe learner uses, then there are concept classes that can be learned efficiently by\nquantum learners but not by classical learners (under plausible complexity-theoretic assumptions).\n\nA natural model of passive learning is Valiant's probably approximately correct (PAC) learning. Here the learner receives random examples \"(x,c(x))\", where \"x\" is distributed according to some unknown distribution \"D\". The learner's goal is to output a hypothesis function \"h\" such that \"h(x)=c(x)\" with high probability when \"x\" is drawn according to \"D\". The learner has to be able to produce such an 'approximately correct' \"h\" for every \"D\" and every target concept \"c\" in its concept class.\nWe can consider replacing the random examples by potentially more powerful quantum examples formula_5. In the PAC model (and the related agnostic model), this doesn't significantly reduce the number of examples needed: for every concept class, classical and\nquantum sample complexity are the same up to constant factors. However, for learning under some\nfixed distribution \"D\", quantum examples can be very helpful, for example for learning DNF under\nthe uniform distribution. When considering \"time\" complexity, there exist concept classes that can be PAC-learned efficiently by quantum learners, even from classical examples, but not by classical learners (again, under plausible complexity-theoretic assumptions).\n\nThis passive learning type is also the most common scheme in supervised learning: a learning algorithm typically takes the training examples fixed, without the ability to query the label of unlabelled examples. Outputting a hypothesis \"h\" is a step of induction. Classically, an inductive model splits into a training and an application phase: the model parameters are estimated in the training phase, and the learned model is applied an arbitrary many times in the application phase. In the asymptotic limit of the number of applications, this splitting of phases is also present with quantum resources.\n\nThe earliest experiments were conducted using the adiabatic D-Wave quantum computer, for instance, to detect cars in digital images using regularized boosting with a nonconvex objective function in a demonstration in 2009. Many experiments followed on the same architecture, and leading tech companies have shown interest in the potential of quantum machine learning for future technological implementations. In 2013, Google Research, NASA, and the Universities Space Research Association launched the Quantum Artificial Intelligence Lab which explores the use of the adiabatic D-Wave quantum computer. A more recent example trained a probabilistic generative models with arbitrary pairwise connectivity, showing that their model is capable of generating handwritten digits as well as reconstructing noisy images of bars and stripes and handwritten digits.\n\nUsing a different annealing technology based on nuclear magnetic resonance (NMR), a quantum Hopfield network was implemented in 2009 that mapped the input data and memorized data to Hamiltonians, allowing the use of adiabatic quantum computation. NMR technology also enables universal quantum computing, and it was used for the first experimental implementation of a quantum support vector machine to distinguish hand written number ‘6’ and ‘9’ on a liquid-state quantum computer in 2015. The training data involved the pre-processing of the image which maps them to normalized 2-dimensional vectors to represent the images as the states of a qubit. The two entries of the vector are the vertical and horizontal ratio of the pixel intensity of the image. Once the vectors are defined on the feature space, the quantum support vector machine was implemented to classify the unknown input vector. The readout avoids costly quantum tomography by reading out the final state in terms of direction (up/down) of the NMR signal.\n\nPhotonic implementations are attracting more attention, not the least because they do not require extensive cooling. Simultaneous spoken digit and speaker recognition and chaotic time-series prediction were demonstrated at data rates beyond 1 gigabyte per second in 2013. Using non-linear photonics to implement an all-optical linear classifier, a perceptron model was capable of learning the classification boundary iteratively from training data through a feedback rule. A core building block in many learning algorithms is to calculate the distance between two vectors: this was first experimentally demonstrated for up to eight dimensions using entangled qubits in a photonic quantum computer in 2015.\n\nRecently, based on a neuromimetic approach, a novel ingredient has been added to the field of quantum machine learning, in the form of a so-called quantum memristor, a quantized model of the standard classical memristor. This device can be constructed by means of a tunable resistor, weak measurements on the system, and a classical feed-forward mechanism. An implementation of a quantum memristor in superconducting circuits has been proposed, and an experiment with quantum dots performed. A quantum memristor would implement nonlinear interactions in the quantum dynamics which would aid the search for a fully functional quantum neural network.\n\n"}
{"id": "1729951", "url": "https://en.wikipedia.org/wiki?curid=1729951", "title": "Qutrit", "text": "Qutrit\n\nA qutrit (or quantum trit) is a unit of quantum information that is realized by a quantum system described by a superposition of three mutually orthogonal quantum states.\n\nThe qutrit is analogous to the classical base-3 trit, just as the qubit, a quantum system described by a superposition of two orthogonal states, is analogous to the classical base-2 bit.\n\nA qutrit has three orthonormal basis states or vectors, often denoted formula_1, formula_2, and formula_3 in Dirac or bra–ket notation.\nThese are used to describe the qutrit as a superposition state vector in the form of a linear combination of the three orthonormal basis states:\nwhere the coefficients are complex probability amplitudes, such that the sum of their squares is unity (normalization):\n\nThe qubit's orthonormal basis states formula_6 span the two-dimensional complex Hilbert space formula_7, corresponding to spin-up and spin-down of a spin-1/2 particle. Qutrits require a Hilbert space of higher dimension, namely the three-dimensional formula_8 spanned by the qutrit's basis formula_9\n, which can be realized by a three-level quantum system. Note, however, that not all three-level quantum systems are qutrits.\n\nA string of \"n\" qutrits represents 3 different states simultaneously, i.e., a superposition state vector in 3-dimensional complex Hilbert space.\n\nQutrits have several peculiar features when used for storing quantum information. For example, they are more robust to decoherence under certain environmental interactions. In reality, manipulating qutrits directly might be tricky, and one way to do that is by using an entanglement with a qubit.\n\n\n"}
{"id": "11565453", "url": "https://en.wikipedia.org/wiki?curid=11565453", "title": "Raimundo Teixeira Mendes", "text": "Raimundo Teixeira Mendes\n\nRaimundo Teixeira Mendes (5 January 1855 – 1927) was a Brazilian philosopher and mathematician. He is credited with creating the national motto, \"Order and Progress\", as well as the national flag on which it appears.\n\nTeixeira Mendes was born in Caxias, Maranhão.\n\nTeixeira Mendes was heavily influenced by Comtism and is classed as a \"Humanity Apostle\" by Brazil's Religion of Humanity, which is called \"Igreja Positivista do Brasil\" or in English \"Positivist Church of Brazil.\" In life he led the Positivist Church after 1903. For him the Positivist viewpoint meant he opposed most wars and believed in the eventual disappearance of nations. He also opposed Christian missionary work toward the indigenous Brazilians and instead favored a policy based on protection and gradual assimilation. He deemed their societies \"fetishistic\", but believed a gradual non-coercive assimilation was the way to turn them into Positivists.\n\nHe died in Rio de Janeiro, aged 72.\n"}
{"id": "3591868", "url": "https://en.wikipedia.org/wiki?curid=3591868", "title": "Robert Calderbank", "text": "Robert Calderbank\n\nRobert Calderbank (born 28 December 1954) is a professor of Computer Science, Electrical Engineering, and Mathematics and director of the Information Initiative at Duke. He received a BSc from Warwick University in 1975, an MSc from Oxford in 1976, and a PhD from Caltech, all in mathematics. He joined Bell Labs in 1980, and retired from AT&T Labs in 2003 as Vice President for Research and Internet and network systems. He then went to Princeton as a professor of Electrical Engineering, Mathematics and Applied and Computational Mathematics, before moving to Duke in 2010 to become Dean of Natural Sciences.\n\nHis contributions to coding and information theory won the IEEE Information Theory Society Paper Award in 1995 and 1999. While at Bell Labs, he co-discovered space–time coding. He was elected to the US National Academy of Engineering in 2005, became a fellow of the American Mathematical Society in 2012, and won the 2013 IEEE Richard W. Hamming Medal and the 2015 Claude E. Shannon Award.\n\nHe is married to Ingrid Daubechies.\n\n"}
{"id": "26830155", "url": "https://en.wikipedia.org/wiki?curid=26830155", "title": "Superfunction", "text": "Superfunction\n\nIn mathematics, superfunction is a nonstandard name for an iterated function for complexified continuous iteration index. \nRoughly, for some function \"f\" and for some variable \"x\", the superfunction could be defined by the expression\nThen, \"S(z;x)\" can be interpreted as the superfunction of the function \"f(x)\".\nSuch a definition is valid only for a positive integer index \"z\". The variable \"x\" is often omitted. \nMuch study and many applications of superfunctions employ various \"extensions of these superfunctions to complex and continuous indices\"; and the analysis of the existence, uniqueness and their evaluation. The Ackermann functions and tetration can be interpreted in terms of super-functions.\n\nAnalysis of superfunctions arose from applications of the evaluation of fractional iterations of functions. Superfunctions and their inverses allow evaluation of not only the first negative power of a function (inverse function), but also of any real and even complex iterate of that function. Historically, an early function of this kind considered was formula_2; the function formula_3 has then been used as the logo of the Physics department of the Moscow State University.\nAt that time, these investigators did not have computational access for the evaluation of such functions, but the function formula_4 was luckier than formula_5: at the very least, the existence of the holomorphic function\nformula_6 such that formula_7 had been demonstrated in 1950 by Hellmuth Kneser.\n\nRelying on the elegant functional conjugacy theory of Schröder's equation, for his proof, Kneser had constructed the \"superfunction\" of the exponential map through the corresponding \"Abel function\" formula_8, satisfying the related Abel equation\nso that formula_10. The inverse function Kneser found, \nis an entire super-exponential, although it is not real on the real axis; it cannot be interpreted as tetrational, because the condition formula_12 cannot be realized for the entire super-exponential. The real formula_4 can be constructed with the tetrational (which is also a superexponential); while the real formula_3 can be constructed with the superfactorial.\n\nThe recurrence formula of the above preamble can be written as \nInstead of the last equation, one could write the identity function,\nand extend the range of definition of the superfunction \"S\" to the non-negative integers. Then, one may posit\nand extend the range of validity to the integer values larger than −2.\n\nThe following extension, for example,\n\nis not trivial, because the inverse function may happen to be not defined for some values of formula_20.\nIn particular, tetration can be interpreted as super-function of exponential for some real base formula_21; in this case,\nThen, at \"x\"=1,\nbut\nis not defined.\n\nFor extension to non-integer values of the argument, the superfunction should be defined in a different way.\n\nFor complex numbers formula_25 and formula_26, such that formula_25 belongs to some connected domain formula_28,\nthe superfunction (from formula_29 to formula_21) of a holomorphic function \"f\" on the domain formula_31 is\nfunction formula_32, holomorphic on domain formula_31, such that\n\nIn general, the superfunction is not unique.\nFor a given base function formula_36, from a given formula_37 superfunction formula_38, another formula_37 superfunction formula_40 could be constructed as\nwhere formula_42 is any 1-periodic function, holomorphic at least in some vicinity of the real axis, such that formula_43.\n\nThe modified super-function may have a narrower range of holomorphy.\nThe variety of possible super-functions is especially large in the limiting case, when the width of the range of holomorphy becomes zero; in this case, one deals with real-analytic superfunctions.\n\nIf the range of holomorphy required is large enough, then, the super-function is expected to be unique,\nat least in some specific base functions formula_44. In particular, the formula_45 super-function of\nformula_46, for formula_47, is called tetration and is believed to be unique, at least for\nformula_48; for the case formula_49,\nbut up to 2009, the uniqueness was more conjecture than a theorem with a formal mathematical proof.\n\nThis short collection of elementary superfunctions is illustrated in. Some superfunctions can be expressed through elementary functions;\nthey are used without mention that they are superfunctions.\nFor example, for the transfer function \"++\", which means unity increment,\nthe superfunction is just addition of a constant.\n\nChose a complex number formula_50 and define the function formula_51 as \nformula_52. Further define the function formula_53 as\nformula_54.\n\nThen, the function formula_55 is the superfunction (0 to \"c\")\nof the function formula_56 on C.\n\nExponentiation formula_57 is superfunction (from 1 to formula_50) of function formula_59.\n\nThe examples but the last one, below, are essentially from Schröder's pioneering 1870 paper.\n\nLet formula_60.\nThen,\nis a formula_62 superfunction (iteration orbit) of \"f\".\n\nIndeed,\nand formula_64\n\nIn this case, the superfunction formula_38 is periodic, with period\nformula_66;\nand the superfunction approaches unity in the negative direction of the real axis,\n\nSimilarly,\nhas an iteration orbit\n\nIn general, the transfer (step) function \"f(x)\" needs not be an entire function. An example involving a meromorphic function \"f\" reads,\nIts iteration orbit (superfunction) is \non C, the set of complex numbers except for the singularities of the function \"S\".\nTo see this, recall the double angle trigonometric formula\n\nLet\nformula_47,\nformula_75,\nformula_76.\nThe tetration formula_77 is then a formula_78 superfunction of formula_46.\n\nThe inverse of a superfunction for a suitable argument \"x\" can be interpreted as the Abel function, the solution of the Abel equation,\nand hence \nThe inverse function when defined, is \nfor suitable domains and ranges, when they exist. The recursive property of \"S\" is then self-evident.\nThe figure at left shows an example of transition from\nformula_83 to\nformula_84.\nThe iterated function formula_85 versus real argument is plotted for\nformula_86. The tetrational and ArcTetrational were used as superfunction\nformula_38 and Abel function formula_88 of the exponential.\nThe figure at right shows these functions in the complex plane.\nAt non-negative integer number of iteration, the iterated exponential is an entire function; at non-integer values, it has two branch points, thich correspond to the fixed point formula_89 and\nformula_90 of natural logarithm. At formula_91, function formula_92 remains holomorphic at least in the strip formula_93 along the real axis.\n\nSuperfunctions, usually the superexponentials, are proposed as a fast-growing function for an\nupgrade of the floating point representation of numbers in computers. Such an upgrade would greatly extend the\nrange of huge numbers which are still distinguishable from infinity.\n\nOther applications include the calculation of fractional iterates (or fractional powers) of a function. Any holomorphic function can be identified to a transfer function, and then its super-functions and corresponding Abel functions can be considered.\nIn the investigation of the nonlinear response of optical materials, the sample is supposed to be optically thin, in such a way, that the intensity of the light does not change much as it goes through. Then one can consider, for example, the absorption as function of the intensity. However, at small variation of the intensity in the sample, the precision of measurement of the absorption as function of intensity is not good. The reconstruction of the superfunction from the transfer function allows to work with relatively thick samples, improving the precision of measurements. In particular, the transfer function of the similar sample, which is half thiner, could be interpreted as the square root (i.e. half-iteration) of the transfer function of the initial sample.\n\nSimilar example is suggested for a nonlinear optical fiber.\n\nIt may make sense to characterize the nonlinearities in the attenuation of shock waves in a homogeneous tube. This could find an application in some advanced muffler, using nonlinear acoustic effects to withdraw the energy of the sound waves without to disturb the flux of the gas. Again, the analysis of the nonlinear response, i.e. the transfer function, may be boosted with the superfunction.\n\nIn analysis of condensation, the growth (or vaporization) of a small drop of liquid can be considered,\nas it diffuses down through a tube with some uniform concentration of vapor.\nIn the first approximation, at fixed concentration of the vapor,\nthe mass of the drop at the output end can be interpreted as the transfer function of the input mass.\nThe square root of this transfer function will characterize the tube of half length.\n\nThe mass of a snowball that rolls down a hill can be considered as a function of the path it has already passed. At fixed length of this path\n(that can be determined by the altitude of the hill) this mass can be considered also as a Transfer Function of the input mass. The mass of the snowball could be measured at the top of the hill and at the bottom, giving the Transfer Function; then, the mass of the snowball, as a function of the length it passed, is a superfunction.\n\nIf one needs to build up an operational element with some given transfer function formula_44,\nand wants to realize it as a sequential connection of a couple of identical operational elements, then, each of these two elements should have transfer function\nformula_95. Such a function can be evaluated through the superfunction and the Abel function of the transfer function formula_44.\n\nThe operational element may have any origin: it can be realized as an electronic microchip,\nor a mechanical couple of curvilinear grains, or some asymmetric U-tube filled with different liquids, and so on.\n\n"}
{"id": "58552803", "url": "https://en.wikipedia.org/wiki?curid=58552803", "title": "Supersingular isogeny graph", "text": "Supersingular isogeny graph\n\nIn mathematics, the supersingular isogeny graphs are a class of expander graphs that arise in computational number theory and have been applied in elliptic-curve cryptography. Their vertices represent supersingular elliptic curves over finite fields and their edges represent isogenies between curves.\n\nA supersingular isogeny graph is determined by choosing a large prime number formula_1 and a small prime number formula_2, and considering the class of all supersingular elliptic curves defined over the finite field formula_3. There are approximately formula_4 such curves, each two of which can be related by isogenies. The vertices in the supersingular isogeny graph represent these curves (or more concretely, their -invariants, elements of formula_3) and the edges represent isogenies of degree formula_2 between two curves.\n\nThe supersingular isogeny graphs are formula_7-regular graphs, meaning that each vertex has exactly formula_7 neighbors. They were proven by Pizer to be Ramanujan graphs, graphs with optimal expansion properties for their degree. The proof is based on Pierre Deligne's proof of the Ramanujan–Petersson conjecture.\n\nOne proposal for a cryptographic hash function involves starting from a fixed vertex of a supersingular isogeny graph, using the bits of the binary representation of an input value to determine a sequence of edges to follow in a walk in the graph, and using the identity of the vertex reached at the end of the walk as the hash value for the input. The security of the proposed hashing scheme rests on the assumption that it is difficult to find paths in this graph that connect arbitrary pairs of vertices.\n\nIt has also been proposed to use walks in two supersingular isogeny graphs with the same vertex set but different edge sets (defined using different choices of the formula_2 parameter) to develop a key exchange primitive analogous to Diffie–Hellman key exchange, called supersingular isogeny key exchange.\n\nAdditional cryptographic methods based on these graphs include signature schemes and public-key cryptography. They have been suggested as a form of post-quantum cryptography: , there are no known subexponential-time methods for breaking these schemes, even on quantum computers.\n"}
{"id": "41530834", "url": "https://en.wikipedia.org/wiki?curid=41530834", "title": "The Emergence of Probability", "text": "The Emergence of Probability\n\nThe Emergence of Probability: A Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference is a 1975 book by the philosopher Ian Hacking.\n\nHacking's work has been described as ground-breaking. \n\nThe philosopher James Franklin argued that Hacking's contention that there was no concept of uncertain evidence before about 1650 is incorrect, as it neglects the extensive Latin scholastic literature on legal evidence and aleatory contracts and on induction.\n\n"}
{"id": "1894394", "url": "https://en.wikipedia.org/wiki?curid=1894394", "title": "UML tool", "text": "UML tool\n\nA UML tool or UML modeling tool is a software application that supports some or all of the notation and semantics associated with the Unified Modeling Language (UML), which is the industry standard general purpose modeling language for software engineering.\n\n\"UML tool\" is used broadly here to include application programs which are not exclusively focused on UML, but which support some functions of the Unified Modeling Language, either as an \"add-on\", as a \"component\" or as a \"part\" of their overall functionality.\n\nUML tools support the following kinds of functionality:\n\n\"Diagramming\" in this context means \"creating\" and \"editing\" UML diagrams; that is diagrams that follow the graphical notation of the Unified Modeling Language.\n\nThe use of UML diagrams as a means to draw diagrams of – mostly – object-oriented software is generally agreed upon by software developers. When developers draw diagrams of object-oriented software, they usually follow the UML notation. On the other hand, it is often debated whether those diagrams are needed at all, during what stages of the software development process they should be used, and how (if at all) they should be kept up to date. The primacy of software code often leads to the diagrams being deprecated.\n\nRound-trip engineering refers to the ability of a UML tool to perform code generation from models, and model generation from code (a.k.a., reverse engineering), while keeping both the model and the code semantically consistent with each other. Code generation and reverse engineering are explained in more detail below.\n\"Code generation\" in this context means that the user creates UML diagrams, which have some connected model data, and the UML tool derives from the diagrams part or all of the source code for the software system. In some tools the user can provide a skeleton of the program source code, in the form of a source code template, where predefined tokens are then replaced with program source code parts during the code generation process.\n\nThere is some debate among software developers about how useful code generation as such is . It certainly depends on the specific problem domain and how far code generation should be applied. There are well known areas where code generation is an established practice, not limited to the field of UML.\n\nThe idea of completely leaving the \"code level\" and starting to do \"programming\" directly from the UML diagram level (i.e., design level) is quite debated among developers . That is the vision for Model-driven architecture (MDA). This idea is not in such widespread use compared to other software development tools like compilers or software configuration management systems.\n\nAn often cited criticism is that the UML diagrams lack the detail that is needed to contain the same information as is covered with the program source: Jack W. Reeves states that the final embodiment of the design lies in the source code. (His often quoted statement that \"the Code \"is\" the design\" has been misinterpreted to mean that there is no need for intermediate- and high-level software-design artifacts, such as UML diagrams or software-requirements documents).\n\n\"Reverse engineering\" in this context means, that the UML tool reads program source code as input and \"derives\" model data and corresponding graphical UML diagrams from it (as opposed to the somewhat broader meaning described in the article \"Reverse engineering\").\n\nSome of the challenges of reverse engineering are:\n\nXML Metadata Interchange (XMI) is the format for UML model interchange. XMI does not support UML Diagram Interchange, which allows the importation of UML diagrams from one model to another.\n\nA key concept associated with the model-driven architecture initiative is the capacity to transform a model into another model. For example, one might want to transform a platform-independent domain model into a Java platform-specific model for implementation. It is also possible to refactor UML models to produce more concise and well-formed UML models. It is possible to generate UML models from other modeling notations, such as BPMN, which is itself a UML profile. The standard that supports this is called QVT for Queries/Views/Transformations. One example of an open-source QVT-solution is the ATL language built by INRIA.\n\n\n"}
{"id": "4910264", "url": "https://en.wikipedia.org/wiki?curid=4910264", "title": "Umbilic torus", "text": "Umbilic torus\n\nThe umbilic torus or umbilic bracelet is a single-edged 3-dimensional shape. The lone edge goes three times around the ring before returning to the starting point. The shape also has a single external face. A cross section of the surface forms a deltoid.\n\nThe umbilic torus occurs in the mathematical subject of singularity theory, in particular in the classification of umbilical points which are determined by real cubic forms formula_1. The equivalence classes of such cubics form a three-dimensional real projective space and the subset of parabolic forms define a surface – the umbilic torus. Christopher Zeeman named this set the umbilic bracelet in 1976.\n\nThe torus is defined by the following set of parametric equations.\n\nJohn Robinson created a sculpture \"Eternity\" based on the shape in 1989, this had a triangular cross-section rather than a deltoid of a true Umbilic bracelet. This appeared on the cover of Geometric Differentiation by Ian R. Porteous.\n\nHelaman Ferguson has created a 27-inch (69 centimeters) bronze sculpture, \"Umbilic Torus\", and it is his most widely known piece of art. In 2010, it was announced that Jim Simons had commissioned an Umbilic Torus sculpture to be constructed outside the Math and Physics buildings at Stony Brook University, in proximity to the Simons Center for Geometry and Physics. The torus is made out of cast bronze, and is mounted on a stainless steel column. The total weight of the sculpture is 65 tonnes, and has a height of . The torus has a diameter of , the same diameter as the granite base. Various mathematical formulas defining the torus are inscribed on the base. Installation was completed in September, 2012.\n\n\n"}
