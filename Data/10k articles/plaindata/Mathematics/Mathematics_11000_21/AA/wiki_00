{"id": "1819739", "url": "https://en.wikipedia.org/wiki?curid=1819739", "title": "139 (number)", "text": "139 (number)\n\n139 (One hundred [and] thirty-nine) is the natural number following 138 and preceding 140.\n\n139 is the 34th prime number. It is a twin prime with 137. Because 141 is a semiprime, 139 is a Chen prime. 139 is the smallest prime before a prime gap of length 10.\n\nThis number is the sum of five consecutive prime numbers (19 + 23 + 29 + 31 + 37).\n\nIt is the smallest factor of 64079. It is also the smallest factor of the first nine terms of the Euclid–Mullin sequence, making it the tenth term.\n\n139 is a happy number and a strictly non-palindromic number.\n\n\n\n139 is also:\n\nPsalm 139\n"}
{"id": "34620850", "url": "https://en.wikipedia.org/wiki?curid=34620850", "title": "Absorption (logic)", "text": "Absorption (logic)\n\nAbsorption is a valid argument form and rule of inference of propositional logic. The rule states that if formula_1 implies formula_2, then formula_1 implies formula_1 and formula_2. The rule makes it possible to introduce conjunctions to proofs. It is called the law of absorption because the term formula_2 is \"absorbed\" by the term formula_1 in the consequent. The rule can be stated:\n\nwhere the rule is that wherever an instance of \"formula_9\" appears on a line of a proof, \"formula_10\" can be placed on a subsequent line.\n\nThe \"absorption\" rule may be expressed as a sequent:\n\nwhere formula_12 is a metalogical symbol meaning that formula_10 is a syntactic consequences of formula_14 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic. The principle was stated as a theorem of propositional logic by Russell and Whitehead in \"Principia Mathematica\" as:\n\nwhere formula_1, and formula_2 are propositions expressed in some formal system.\n\nIf it will rain, then I will wear my coat.<br>\nTherefore, if it will rain then it will rain and I will wear my coat.\n\n<br>\n\n"}
{"id": "21633880", "url": "https://en.wikipedia.org/wiki?curid=21633880", "title": "Alfred William Flux", "text": "Alfred William Flux\n\nSir Alfred William Flux CB (8 April 1867 – 16 July 1942) was a British economist and statistician.\n\nFlux was born in the Landport district of Portsmouth in 1867, the son of a cement maker. He attended Portsmouth Grammar School then studied mathematics at St John's College, Cambridge where he was a Senior Wrangler in 1887 (sharing the honour in a tie with three others). While at Cambridge he became friends with Alfred Marshall, who interested him in economics. He was a foundation member of the Economic Society (1890), and from 1893 until 1908 taught economics, at Manchester and then at McGill University, Montreal. In 1897, while in Manchester he married Harriet Emily Hansen, a Danish woman.\n\nFlux returned to London in 1908 to take up a post as advisor to the Commercial, Labour and Statistics Department. In 1918, he was appointed Head of the Statistics Department of the Board of Trade.\nThe Royal Statistical Society awarded him the Guy Medal in Silver in 1921 and in Gold in 1930. He also served as President of the Society between 1928 and 1930.\n\nFlux retired to Denmark in 1932 and was knighted in 1934. He died of pneumonia in 1942, aged 75.\n\n"}
{"id": "2280157", "url": "https://en.wikipedia.org/wiki?curid=2280157", "title": "Andreas Floer", "text": "Andreas Floer\n\nAndreas Floer (; 23 August 1956 – 15 May 1991) was a German mathematician who made seminal contributions to the areas of geometry, topology, and mathematical physics, in particular the invention of Floer homology.\n\nHe was an undergraduate student at the Ruhr-Universität Bochum and received a Diplom in mathematics in 1982. He then went to the University of California, Berkeley and undertook PhD work on monopoles on 3-manifolds, under the supervision of Clifford Taubes; but he did not complete it when interrupted by his obligatory alternative service in Germany. He received his PhD (Dr. phil.) at Bochum in 1984, under the supervision of Eduard Zehnder.\n\nFloer's first pivotal contribution was a solution of a special case of Arnold's conjecture on fixed points of a symplectomorphism. Because of his work on Arnold's conjecture and his development of instanton homology, he achieved wide recognition and was invited as a plenary speaker for the International Congress of Mathematicians held in Kyoto in August 1990. He received a Sloan Fellowship in 1989.\n\nIn 1988 he became an Assistant Professor at the University of California, Berkeley and was promoted to Full Professor of Mathematics in 1990. From 1990 he was Professor of Mathematics at the Ruhr-Universität Bochum, until his suicide in 1991.\n\n\"Andreas Floer's life was tragically interrupted, but his mathematical visions and striking contributions have provided powerful methods which are being applied to problems which seemed to be intractable only a few years ago.\"\n\nSimon Donaldson wrote: \"The concept of Floer homology is one of the most striking developments in differential geometry over the past 20 years. ... The ideas have led to great advances in the areas of low-dimensional topology and symplectic geometry and are intimately related to developments in Quantum Field Theory\" and \"the full richness of Floer's theory is only beginning to be explored\".\n\n\"Since its introduction by Andreas Floer in the late nineteen eighties, Floer theory has had a tremendous influence on many branches of mathematics including geometry, topology and dynamical systems. The development of new Floer theoretic tools continues at a remarkable pace and underlies many of the recent breakthroughs in these diverse fields.\"\n\n\n\n\n\n"}
{"id": "7754592", "url": "https://en.wikipedia.org/wiki?curid=7754592", "title": "Andrews–Curtis conjecture", "text": "Andrews–Curtis conjecture\n\nIn mathematics, the Andrews–Curtis conjecture states that every balanced presentation of the trivial group can be transformed into a trivial presentation by a sequence of Nielsen transformations on the relators together with conjugations of relators, named after James J. Andrews and Morton L. Curtis who proposed it in 1965. It is difficult to verify whether the conjecture holds for a given balanced presentation or not.\n\nIt is widely believed that the Andrews–Curtis conjecture is false. While there are no counterexamples known, there are numerous potential counterexamples. It is known that the Zeeman conjecture on collapsibility implies the Andrews–Curtis conjecture.\n\n"}
{"id": "4671396", "url": "https://en.wikipedia.org/wiki?curid=4671396", "title": "Atomistix Virtual NanoLab", "text": "Atomistix Virtual NanoLab\n\nAtomistix Virtual NanoLab (VNL) is a commercial point-and-click software for simulation and analysis of physical and chemical properties of nanoscale devices. Virtual NanoLab is developed and sold commercially by QuantumWise A/S.\n\nWith its graphical interface, Virtual NanoLab provides a user-friendly approach to atomic-scale modeling. The software contains a set of interactive instruments that allows the user to design nanosystems, to set up and execute numerical calculations, and to visualize the results.\nSamples such as molecules, nanotubes, crystalline systems, and two-probe systems (i.e. a nanostructure coupled to two electrodes) are built with a few mouse clicks. \n\nVirtual NanoLab contains a 3D visualization tool, the Nanoscope, where atomic geometries and computed results can be viewed and analyzed. One can for example plot Bloch functions of nanotubes and crystals, molecular orbitals, electron densities, and effective potentials. \nThe numerical engine that carries out the actual simulations is Atomistix ToolKit, which combines density functional theory and non-equilibrium Green's functions to ab initio electronic-structure and transport calculations. Atomistix ToolKit is developed from the academic codes TranSIESTA and McDCal.\n\n\n"}
{"id": "19200651", "url": "https://en.wikipedia.org/wiki?curid=19200651", "title": "Baer–Specker group", "text": "Baer–Specker group\n\nIn mathematics, in the field of group theory, the Baer–Specker group, or Specker group, named after Reinhold Baer and Ernst Specker, is an example of an infinite Abelian group which is a building block in the structure theory of such groups.\n\nThe Baer–Specker group is the group \"B\" = Z of all integer sequences with componentwise addition, that is, the direct product of countably many copies of Z. \n\nReinhold Baer proved in 1937 that this group is \"not\" free abelian; Specker proved in 1950 that every countable subgroup of \"B\" is free abelian.\n\nThe group of homomorphisms from the Baer–Specker group to a free abelian group of finite rank is a free abelian group of countable rank. This provides another proof that the group is not free. See also E. F. Cornelius, Jr., \"Endomorphisms and product bases of the Baer-Specker group\", Int'l J. Math. and Math. Sciences, 2009, article 396475.\n\n\n\n"}
{"id": "2605212", "url": "https://en.wikipedia.org/wiki?curid=2605212", "title": "Bitangent", "text": "Bitangent\n\nIn mathematics, a bitangent to a curve \"C\" is a line \"L\" that touches \"C\" in two distinct points \"P\" and \"Q\" and that has the same direction as \"C\" at these points. That is, \"L\" is a tangent line at \"P\" and at \"Q\".\n\nIn general, an algebraic curve will have infinitely many secant lines, but only finitely many bitangents.\n\nBézout's theorem implies that a plane curve with a bitangent must have degree at least 4. The case of the 28 bitangents of a quartic was a celebrated piece of geometry of the nineteenth century, a relationship being shown to the 27 lines on the cubic surface.\n\nThe four bitangents of two disjoint convex polygons may be found efficiently by an algorithm based on binary search in which one maintains a binary search pointer into the lists of edges of each polygon and moves one of the pointers left or right at each steps depending on where the tangent lines to the edges at the two pointers cross each other. This bitangent calculation is a key subroutine in data structures for maintaining convex hulls dynamically . describe an algorithm for efficiently listing all bitangent line segments that do not cross any of the other curves in a system of multiple disjoint convex curves, using a technique based on pseudotriangulation.\n\nBitangents may be used to speed up the visibility graph approach to solving the Euclidean shortest path problem: the shortest path among a collection of polygonal obstacles may only enter or leave the boundary of an obstacle along one of its bitangents, so the shortest path can be found by applying Dijkstra's algorithm to a subgraph of the visibility graph formed by the visibility edges that lie on bitangent lines .\n\nA bitangent differs from a secant line in that a secant line may cross the curve at the two points it intersects it. One can also consider bitangents that are not lines; for instance, the symmetry set of a curve is the locus of centers of circles that are tangent to the curve in two points.\n\nBitangents to pairs of circles figure prominently in Jakob Steiner's 1826 construction of the Malfatti circles, in the belt problem of calculating the length of a belt connecting two pulleys, in Casey's theorem characterizing sets of four circles with a common tangent circle, and in Monge's theorem on the collinearity of intersection points of certain bitangents.\n\n"}
{"id": "15989869", "url": "https://en.wikipedia.org/wiki?curid=15989869", "title": "Carlos Castillo-Chavez", "text": "Carlos Castillo-Chavez\n\nCarlos Castillo-Chavez (born 1952) is a Regents Professor, and Joaquín Bustoz, Jr. Professor of Mathematical Biology at Arizona State University<ref name=\"titleAwards/Honors\"></ref> and the executive director of the Mathematical and Theoretical Biology Institute and Institute for Strengthening the Understanding of Mathematics and Science as well as the founding director of the Simon A. Levin Mathematical, Computational Modeling Sciences Center at the same university. He is also a former rector at Yachay Tech University in Ecuador.\n\nCastillo-Chavez came to the United States from Mexico in 1974, at the age of 22, and began working at a cheese factory in Wisconsin to support himself. He then returned to his mathematics studies by applying to the University of Wisconsin–Stevens Point, where he graduated in 1976, with dual degrees in math and Spanish literature. He continued his MS in Mathematics at University of Wisconsin–Milwaukee. He holds a Ph.D. in mathematics from the University of Wisconsin–Madison (1984). Prior to moving to Arizona State University in 2004, he spent 18 years as a professor at Cornell University. He has published scientific articles and books, and served on panels and committees for organizations such as the National Science Foundation, the Alfred P. Sloan Foundation, the National Institutes of Health, Society for Industrial and Applied Mathematics, and the American Mathematical Society.\n\nHis research interests as a mathematical epidemiologist relate to the mechanisms underlying the spread of disease, and their containment (prevention of spread) and elimination. A 2006 editorial at Arizona State University, a year after his arrival there, described him as one of the most prominent mathematicians in the country, an expert in epidemiological modelling, and among the top research contributors to literature on the progression of diseases. He is acclaimed for his work on enhancing prospects for academic success and providing research opportunities for underrepresented groups in mathematics and biology.\n\nHe has won awards by the American Association for the Advancement of Science (AAAS) Mentor Award and Fellow (2007), the Stanislaw M. Ulam Distinguished Scholar by the Center for Nonlinear Studies at Los Alamos National Laboratory (2003), the Society for Advancement of Chicanos and Native Americans in Science (SACNAS) Distinguished Scientist Award (2001), the Presidential Award for Excellence in Science, Mathematics and Engineering Mentoring (1997), and the Presidential Faculty Fellowship Award from the National Science Foundation and the Office of the President of the United States (1992–1997). In 2012 he became a fellow of the American Mathematical Society.\n\nHe was a member of the Board of Higher Education and Workforce at the National Academy of Sciences (2009-2015) and served in President Barack Obama's Committee on the National Medal of Science (2010-2015).\n\nHe is the inaugural recipient of the inaugural Dr. William Yslas Outstanding STEM Award, by Victoria Foundation Award, co-sponsored by the Pasqua Yaqui Tribe of Arizona (2015). Castillo-Chavez has been elected as a Member-at-Large of the Section on Mathematics of the AAAS (2016 through 2020).\n\nOn February 24, 2016, the University Francisco Gavidia inaugurated the Centro de Modelaje Matemático Carlos Castillo-Chavez, in the City of San Salvador, in the Republic of El Salvador and has been appointed to NSF’s Advisory Committee for Education and Human Resources (2016-2019). He has been named George Polya Lecturer by the MAA 2017.. He is also served as rector of Yachay Tech University in Ecuador (2016-2018). http://www.yachaytech.edu.ec/en/\n\n\n"}
{"id": "47341174", "url": "https://en.wikipedia.org/wiki?curid=47341174", "title": "Collaborative diffusion", "text": "Collaborative diffusion\n\nCollaborative Diffusion is a type of pathfinding algorithm which uses the concept of \"antiobjects\", objects within a computer program that function opposite to what would be conventionally expected. Collaborative Diffusion is typically used in video games, when multiple agents must path towards a single target agent. For example, the ghosts in Pac-Man. In this case, the background tiles serve as antiobjects, carrying out the necessary calculations for creating a path and having the foreground objects react accordingly, whereas having foreground objects be responsible for their own pathing would be conventionally expected.\n\nCollaborative Diffusion is favored for its efficiency over other pathfinding algorithms, such as A*, when handling multiple agents. Also, this method allows elements of competition and teamwork to easily be incorporated between tracking agents. Notably, the time taken to calculate paths remains constant as the number of agents increases.\n"}
{"id": "2761968", "url": "https://en.wikipedia.org/wiki?curid=2761968", "title": "Combinatorial design", "text": "Combinatorial design\n\nCombinatorial design theory is the part of combinatorial mathematics that deals with the existence, construction and properties of systems of finite sets whose arrangements satisfy generalized concepts of \"balance\" and/or \"symmetry\". These concepts are not made precise so that a wide range of objects can be thought of as being under the same umbrella. At times this might involve the numerical sizes of set intersections as in block designs, while at other times it could involve the spatial arrangement of entries in an array as in sudoku grids.\n\nCombinatorial design theory can be applied to the area of design of experiments. Some of the basic theory of combinatorial designs originated in the statistician Ronald Fisher's work on the design of biological experiments. Modern applications are also found in a wide gamut of areas including; Finite geometry, tournament scheduling, lotteries, mathematical biology, algorithm design and analysis, networking, group testing and cryptography.\n\nGiven a certain number \"n\" of people, is it possible to assign them to sets so that each person is in at least one set, each pair of people is in exactly one set together, every two sets have exactly one person in common, and no set contains everyone, all but one person, or exactly one person? The answer depends on \"n\".\n\nThis has a solution only if \"n\" has the form \"q\" + \"q\" + 1. It is less simple to prove that a solution exists if \"q\" is a prime power. It is conjectured that these are the \"only\" solutions. It has been further shown that if a solution exists for \"q\" congruent to 1 or 2 mod 4, then \"q\" is a sum of two square numbers. This last result, the Bruck–Ryser theorem, is proved by a combination of constructive methods based on finite fields and an application of quadratic forms.\n\nWhen such a structure does exist, it is called a finite projective plane; thus showing how finite geometry and combinatorics intersect. When \"q\" = 2, the projective plane is called the Fano plane.\n\nCombinatorial designs date to antiquity, with the Lo Shu Square being an early magic square. One of the earliest datable application of combinatorial design is found in India in the book \"Brhat Samhita\" by Varahamihira, written around 587 AD, for the purpose of making perfumes using 4 substances selected from 16 different substances using a magic square. Combinatorial designs developed along with the general growth of combinatorics from the 18th century, for example with Latin squares in the 18th century and Steiner systems in the 19th century. Designs have also been popular in recreational mathematics, such as Kirkman's schoolgirl problem (1850), and in practical problems, such as the scheduling of round-robin tournaments (solution published 1880s). In the 20th century designs were applied to the design of experiments, notably Latin squares, finite geometry, and association schemes, yielding the field of algebraic statistics.\n\nThe classical core of the subject of combinatorial designs is built around balanced incomplete block designs (BIBDs), Hadamard matrices and Hadamard designs, symmetric BIBDs, Latin squares, resolvable BIBDs, difference sets, and pairwise balanced designs (PBDs). Other combinatorial designs are related to or have been developed from the study of these fundamental ones.\n\n\n\n\n\nThe \"Handbook of Combinatorial Designs\" has, amongst others, 65 chapters, each devoted to a combinatorial design other than those given above. A partial listing is given below:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "17156914", "url": "https://en.wikipedia.org/wiki?curid=17156914", "title": "Computational particle physics", "text": "Computational particle physics\n\nComputational particle physics refers to the methods and computing tools developed in and used by particle physics research. Like computational chemistry or computational biology, it is, for particle physics both a specific branch and an interdisciplinary field relying on computer science, theoretical and experimental particle physics and mathematics.\nThe main fields of computational particle physics are: lattice field theory (numerical computations), automatic calculation of particle interaction or decay (computer algebra) and event generators (stochastic methods).\n\n\nParticle physics played a role in the early history of the internet, the World-Wide Web was created by Tim Berners-Lee when working at CERN in 1991.\n\nNote: This section contains an excerpt from 'Computer Algebra in Particle Physics' by Stefan Weinzierl\n\nParticle physics is an important field of application for computer algebra and exploits the capabilities of Computer Algebra Systems (CAS). This leads to valuable feed-back for the development of CAS. Looking at the history of computer algebra systems, the first programs date back to the 1960s. The first systems were almost entirely based on LISP (\"LISt Programming language\"). LISP is an interpreted language and, as the name already indicates, designed for the manipulation of lists. Its importance for symbolic computer programs in the early days has been compared to the importance of FORTRAN for numerical programs in the same period. Already in this first period, the program REDUCE had some special features for the application to high energy physics. An exception to the LISP-based programs was SCHOONSHIP, written in assembler language by Martinus J. G. Veltman and specially designed for applications in particle physics. The use of assembler code lead to an incredible fast program (compared to the interpreted programs at that time) and allowed the calculation of more complex scattering processes in high energy physics. It has been claimed the program's importance was recognized in 1998 by awarding the half of the Nobel prize to Veltman. Also the program MACSYMA deserves to be mentioned explicitly, since it triggered important development with regard to algorithms. In the 1980s new computer algebra systems started to be written in C. This enabled the better exploitation of the resources of the computer (compared to the interpreted language LISP) and at the same time allowed to maintain portability (which would not have been possible in assembler language). This period marked also the appearance of the first commercial computer algebra system, among which Mathematica and Maple are the best known examples. In addition, also a few dedicated programs appeared, an example relevant to particle physics is the program FORM by J. Vermaseren as a (portable) successor to SCHOONSHIP. More recently issues of the maintainability of large projects became more and more important and the overall programming paradigma changed from procedural programming to object-oriented design. In terms of programming languages this was reflected by a move from C to C++. Following this change of paradigma, the library GiNaC was developed. The GiNac library allows symbolic calculations in C++. \n\nCode generation for computer algebra can also be used in this area.\n\nLattice field theory was created by Kenneth Wilson in 1974. Simulation techniques were later developed from statistical mechanics.\n\nSince the early 1980s, LQCD researchers have pioneered the use of massively parallel computers in large scientific applications, using virtually all available computing systems including traditional main-frames, large PC clusters, and high-performance systems. In addition, it has also been used as a benchmark for high-performance computing, starting with the IBM Blue Gene supercomputer.\n\nEventually national and regional QCD grids were created: LATFOR (continental Europe), UKQCD and USQCD. The ILDG (International Lattice Data Grid) is an international venture comprising grids from the UK, the US, Australia, Japan and Germany, and was formed in 2002.\n\n\n"}
{"id": "2732435", "url": "https://en.wikipedia.org/wiki?curid=2732435", "title": "Davis–Putnam algorithm", "text": "Davis–Putnam algorithm\n\nThe Davis–Putnam algorithm was developed by Martin Davis and Hilary Putnam for checking the validity of a first-order logic formula using a resolution-based decision procedure for propositional logic. Since the set of valid first-order formulas is recursively enumerable but not recursive, there exists no general algorithm to solve this problem. Therefore, the Davis–Putnam algorithm only terminates on valid formulas. Today, the term \"Davis–Putnam algorithm\" is often used synonymously with the resolution-based propositional decision procedure that is actually only one of the steps of the original algorithm.\n\nThe procedure is based on Herbrand's theorem, which implies that an unsatisfiable formula has an unsatisfiable ground instance, and on the fact that a formula is valid if and only if its negation is unsatisfiable. Taken together, these facts imply that to prove the validity of \"φ\" it is enough to prove that a ground instance of \"¬φ\" is unsatisfiable. If \"φ\" is not valid, then the search for an unsatisfiable ground instance will not terminate.\n\nThe procedure roughly consists of these three parts:\n\nThe last part is probably the most innovative one, and works as follows:\n\n\nAt each step, the intermediate formula generated is equisatisfiable, but possibly not equivalent, to the original formula. The resolution step leads to a worst-case exponential blow-up in the size of the formula. \n\nThe Davis–Putnam–Logemann–Loveland algorithm is a 1962 refinement of the propositional satisfiability step of the Davis–Putnam procedure which requires only a linear amount of memory in the worst case. It still forms the basis for today's (as of 2015) most efficient complete SAT solvers.\n\n\n"}
{"id": "47285", "url": "https://en.wikipedia.org/wiki?curid=47285", "title": "Distinct (mathematics)", "text": "Distinct (mathematics)\n\nIn mathematics, two things are called distinct if they are not equal. In physics two things are distinct if they cannot be mapped to each other.\n\nA quadratic equation over the complex numbers has two roots.\n\nThe equation\n\nfactors as\n\nand thus has as roots \"x\" = 1 and \"x\" = 2.\nSince 1 and 2 are not equal, these roots are distinct.\n\nIn contrast, the equation:\n\nfactors as\n\nand thus has as roots \"x\" = 1 and \"x\" = 1.\nSince 1 and 1 are (of course) equal, the roots are not distinct; they \"coincide\".\n\nIn other words, the first equation has distinct roots, while the second does not. (In the general theory, the discriminant is introduced to explain this.)\n\nIn order to prove that two things \"x\" and \"y\" are distinct, it often helps to find some property that one has but not the other.\nFor a simple example, if for some reason we had any doubt that the roots 1 and 2 in the above example were distinct, then we might prove this by noting that 1 is an odd number while 2 is even.\nThis would prove that 1 and 2 are distinct.\n\nAlong the same lines, one can prove that \"x\" and \"y\" are distinct by finding some function \"f\" and proving that \"f\"(\"x\") and \"f\"(\"y\") are distinct.\nThis may seem like a simple idea, and it is, but many deep results in mathematics concern when you can prove distinctness by particular methods. For example,\n"}
{"id": "44902322", "url": "https://en.wikipedia.org/wiki?curid=44902322", "title": "Dual module", "text": "Dual module\n\nIn mathematics, the dual module of a left (resp. right) module \"M\" over a ring \"R\" is the set of module homomorphisms from \"M\" to \"R\" with the pointwise right (resp. left) module structure. The dual module is typically denoted \"M\" or .\n\nIf the base ring \"R\" is a field, then a dual module is a dual vector space.\n\nEvery module has a canonical homomorphism to the dual of its dual (called the double dual). A reflexive module is one for which the canonical homomorphism is an isomorphism. A torsionless module is one for which the canonical homomorphism is injective.\n\nExample: If formula_1 is a finite commutative group scheme represented by a Hopf algebra \"A\" over a commutative ring \"k\", then the Cartier dual formula_2 is the Spec of the dual \"k\"-module of \"A\".\n"}
{"id": "27231492", "url": "https://en.wikipedia.org/wiki?curid=27231492", "title": "Elementary diagram", "text": "Elementary diagram\n\nIn the mathematical field of model theory, the elementary diagram of a structure is the set of all sentences with parameters from the structure that are true in the structure. It is also called the complete diagram.\n\nLet \"M\" be a structure in a first-order language \"L\". An extended language \"L\"(\"M\") is obtained by adding to \"L\" a constant symbol \"c\" for every element \"a\" of \"M\". The structure \"M\" can be viewed as an \"L\"(\"M\") structure in which the symbols in \"L\" are interpreted as before, and each new constant \"c\" is interpreted as the element \"a\". The elementary diagram of \"M\" is the set of all \"L\"(\"M\") sentences that are true in \"M\" (Marker 2002:44).\n\n"}
{"id": "8858463", "url": "https://en.wikipedia.org/wiki?curid=8858463", "title": "Eric Allender", "text": "Eric Allender\n\nEric Warren Allender (born in 1956) is an American computer scientist active in the field of computational complexity theory. In 2006 he was inducted as a Fellow of the Association for Computing Machinery. He is currently a professor at Rutgers University where he chaired the Department of Computer Science from 2006 until 2009.\n\nAllender went to Mount Pleasant High School. He graduated from the University of Iowa in 1979 with a double major in Computer Science and Theater. He then graduated from the Georgia Institute of Technology with a Ph.D. in Computer Science in 1985. His doctoral advisor was K. N. King.\n\nAfter graduation, he was a professor at Rutgers University.\n\n"}
{"id": "4346067", "url": "https://en.wikipedia.org/wiki?curid=4346067", "title": "Force of mortality", "text": "Force of mortality\n\nIn actuarial science, force of mortality represents the instantaneous rate of mortality at a certain age measured on an annualized basis. It is identical in concept to failure rate, also called hazard function, in reliability theory.\n\nIn a life table, we consider the probability of a person dying from age \"x\" to \"x\" + 1, called \"q\". In the continuous case, we could also consider the conditional probability of a person who has attained age (\"x\") dying between ages \"x\" and \"x\" + \"Δx\", which is\n\nwhere F(x) is the cumulative distribution function of the continuous age-at-death random variable, X. As \"Δx\" tends to zero, so does this probability in the continuous case. The approximate force of mortality is this probability divided by \"Δx\". If we let \"Δx\" tend to zero, we get the function for force of mortality, denoted by formula_2:\n\nSince \"f\"(\"x\")=\"F\" '(\"x\") is the probability density function of \"X\", and \"S\"(\"x\") = 1 - \"F\"(\"x\") is the survival function, the force of mortality can also be expressed variously as:\n\nTo understand conceptually how the force of mortality operates within a population, consider that the ages, \"x\", where the probability density function \"f\"(\"x\") is zero, there is no chance of dying. Thus the force of mortality at these ages is zero. The force of mortality \"μ\"(\"x\") uniquely defines a probability density function \"f\"(\"x\").\n\nThe force of mortality formula_2 can be interpreted as the \"conditional\" density of failure at age \"x\", while \"f\"(\"x\") is the \"unconditional\" density of failure at age \"x\". The unconditional density of failure at age \"x\" is the product of the probability of survival to age \"x\", and the conditional density of failure at age \"x\", given survival to age \"x\".\n\nThis is expressed in symbols as\n\nor equivalently\n\nIn many instances, it is also desirable to determine the survival probability function when the force of mortality is known. To do this, integrate the force of mortality over the interval \"x\" to \"x + t\"\n\nBy the fundamental theorem of calculus, this is simply\n\nand taking the exponent to the base \"e\" results in\n\nTherefore, the survival probability of an individual of age \"x\" is written in terms of the force of mortality as\n\n\n\n\n\n"}
{"id": "17156700", "url": "https://en.wikipedia.org/wiki?curid=17156700", "title": "Functionally graded element", "text": "Functionally graded element\n\nIn materials science and mathematics, functionally graded elements are elements used in finite element analysis. They can be used to describe a functionally graded material.\n\n"}
{"id": "35326250", "url": "https://en.wikipedia.org/wiki?curid=35326250", "title": "Glossary of classical algebraic geometry", "text": "Glossary of classical algebraic geometry\n\nThe terminology of algebraic geometry changed drastically during the twentieth century, with the introduction of the general methods, initiated by David Hilbert and the Italian school of algebraic geometry in the beginning of the century, and later formalized by André Weil, Jean-Pierre Serre and Alexander Grothendieck. Much of the classical terminology, mainly based on case study, was simply abandoned, with the result that books and papers written before this time can be hard to read. This article lists some of this classical terminology, and describes some of the changes in conventions.\n\nThe change in terminology from around 1948 to 1960 is not the only difficulty in understanding classical algebraic geometry. There was also a lot of background knowledge and assumptions, much of which has now changed. This section lists some of these changes.\n\n\n\nenv\n\n\n"}
{"id": "5734146", "url": "https://en.wikipedia.org/wiki?curid=5734146", "title": "High- and low-level", "text": "High- and low-level\n\nHigh-level and low-level, as technical terms, are used to classify, describe and point to specific goals of a systematic operation; and are applied in a wide range of contexts, such as, for instance, in domains as widely varied as computer science and business administration.\n\nHigh-level describe those operations that are more abstract in nature; wherein the overall goals and systemic features are typically more concerned with the wider, macro system as a whole.\n\nLow-level describes more specific individual components of a systematic operation, focusing on the details of rudimentary micro functions rather than macro, complex processes. Low-level classification is typically more concerned with individual components within the system and how they operate.\n\nFeatures which emerge only at a high level of description are known as epiphenomena.\n\nDue to the nature of complex systems, the high-level description will often be completely different from the low level one; and, therefore, the (different) descriptions that each deliver are consequent upon the level at which each (differently) direct their study. For example,\n\n\n\n"}
{"id": "2336235", "url": "https://en.wikipedia.org/wiki?curid=2336235", "title": "Hilbert's twentieth problem", "text": "Hilbert's twentieth problem\n\nHilbert's twentieth problem is one of the 23 Hilbert problems set out in a celebrated list compiled in 1900 by David Hilbert. It asks whether all boundary value problems can be solved (that is, do variational problems with certain boundary conditions have solutions).\n\nHilbert noted that there existed methods for solving partial differential equations where the function's values were given at the boundary, but the problem asked for methods for solving partial differential equations with more complicated conditions on the boundary (e.g., involving derivatives of the function), or for solving calculus of variation problems in more than 1 dimension (for example, minimal surface problems or minimal curvature problems)\n\nThe original problem statement in its entirety is as follows:\n\nAn important problem closely connected with the foregoing [referring to Hilbert's nineteenth problem] is the question concerning the existence of solutions of partial differential equations when the values on the boundary of the region are prescribed. This problem is solved in the main by the keen methods of H. A. Schwarz, C. Neumann, and Poincaré for the differential equation of the potential. These methods, however, seem to be generally not capable of direct extension to the case where along the boundary there are prescribed either the differential coefficients or any relations between these and the values of the function. Nor can they be extended immediately to the case where the inquiry is not for potential surfaces but, say, for surfaces of least area, or surfaces of constant positive gaussian curvature, which are to pass through a prescribed twisted curve or to stretch over a given ring surface. It is my conviction that it will be possible to prove these existence theorems by means of a general principle whose nature is indicated by Dirichlet's principle. This general principle will then perhaps enable us to approach the question: Has not every regular variation problem a solution, provided certain assumptions regarding the given boundary conditions are satisfied (say that the functions concerned in these boundary conditions are continuous and have in sections one or more derivatives), and provided also if need be that the notion of a solution shall be suitably extended?\n\nIn the field of differential equations, a boundary value problem is a differential equation together with a set of additional constraints, called the boundary conditions. A solution to a boundary value problem is a solution to the differential equation which also satisfies the boundary conditions.\n\nTo be useful in applications, a boundary value problem should be well posed. This means that given the input to the problem there exists a unique solution, which depends continuously on the input. Much theoretical work in the field of partial differential equations is devoted to proving that boundary value problems arising from scientific and engineering applications are in fact well-posed.\n\n"}
{"id": "31698050", "url": "https://en.wikipedia.org/wiki?curid=31698050", "title": "Homomorphic signatures for network coding", "text": "Homomorphic signatures for network coding\n\nNetwork coding has been shown to optimally use bandwidth in a network, maximizing information flow but the scheme is very inherently vulnerable to pollution attacks by malicious nodes in the network. A node injecting garbage can quickly affect many receivers. The pollution of network packets spreads quickly since the output of (even an) honest node is corrupted if at least one of the incoming packets is corrupted. An attacker can easily corrupt a packet even if it is encrypted by either forging the signature or by producing a collision under the hash function. This will give an attacker access to the packets and the ability to corrupt them. Denis Charles, Kamal Jain and Kristin Lauter designed a new homomorphic encryption signature scheme for use with network coding to prevent pollution attacks. The homomorphic property of the signatures allows nodes to sign any linear combination of the incoming packets without contacting the signing authority. In this scheme it is computationally infeasible for a node to sign a linear combination of the packets without disclosing what linear combination was used in the generation of the packet. Furthermore, we can prove that the signature scheme is secure under well known cryptographic assumptions of the hardness of the discrete logarithm problem and the computational Elliptic curve Diffie–Hellman.\n\nLet formula_1 be a directed graph where formula_2 is a set, whose elements are called vertices or nodes, and formula_3 is a set of ordered pairs of vertices, called arcs, directed edges, or arrows. A source formula_4 wants to transmit a file formula_5 to a set formula_6 of the vertices. One chooses a vector space formula_7(say of dimension formula_8), where formula_9 is a prime, and views the data to be transmitted as a bunch of vectors formula_10. The source then creates the augmented vectors formula_11 by setting formula_12 where formula_13 is the formula_14-th coordinate of the vector formula_15. There are formula_16 zeros before the first '1' appears in formula_17. One can assume without loss of generality that the vectors formula_17 are linearly independent. We denote the linear subspace (of formula_19 ) spanned by these vectors by formula_2 . Each outgoing edge formula_21 computes a linear combination, formula_22, of the vectors entering the vertex formula_23 where the edge originates, that is to say\n\nwhere formula_25. We consider the source as having formula_26 input edges carrying the formula_26 vectors formula_15. By induction, one has that the vector formula_22 on any edge is a linear combination formula_30 and is a vector in formula_2 . The k-dimensional vector formula_32 is simply the first \"k\" coordinates of the vector formula_22. We call the matrix whose rows are the vectors formula_34, where formula_35 are the incoming edges for a vertex formula_36, the global encoding matrix for formula_37 and denote it as formula_38. In practice the encoding vectors are chosen at random so the matrix formula_38 is invertible with high probability. Thus any receiver, on receiving formula_40 can find formula_41 by solving\n\nwhere the formula_43 are the vectors formed by removing the first formula_26 coordinates of the vector formula_45.\n\nEach receiver, formula_36, gets formula_26 vectors formula_40 which are random linear combinations of the formula_17’s.\nIn fact, if\n\nthen\n\nThus we can invert the linear transformation to find the formula_17’s with high probability.\n\nKrohn, Freedman and Mazieres proposed a theory in 2004 that if we have a hash function \nformula_53 such that: \n\nThen server can securely distribute formula_60 to each receiver, and to check if\n\nwe can check whether\n\nThe problem with this method is that the server needs to transfer secure information to each of the receivers. The hash functions formula_54 needs to be transmitted to all the nodes in the network through a separate secure channel.formula_54 is expensive to compute and secure transmission of formula_54 is not economical either.\n\n\nThe homomorphic property of the signatures allows nodes to sign any linear combination of the incoming packets without contacting the signing authority.\n\nElliptic curve cryptography over a finite field is an approach to public-key cryptography based on the algebraic structure of elliptic curves over finite fields.\n\nLet formula_66 be a finite field such that formula_67 is not a power of 2 or 3. Then an elliptic curve formula_3 over formula_66 is a curve given by an equation of the form\n\nwhere formula_71 such that formula_72\n\nLet formula_73, then,\n\nforms an abelian group with O as identity. The group operations can be performed efficiently.\n\nWeil pairing is a construction of roots of unity by means of functions on an elliptic curve formula_3, in such a way as to constitute a pairing (bilinear form, though with multiplicative notation) on the torsion subgroup of formula_3. Let formula_77 be an elliptic curve and let formula_78 be an algebraic closure of formula_66. If formula_80 is an integer, relatively prime to the characteristic of the field formula_66, then the group of formula_80-torsion points,\nformula_83.\n\nIf formula_77 is an elliptic curve and formula_85 then\n\nThere is a map formula_87 such that:\n\n\nAlso, formula_92 can be computed efficiently.\n\nLet formula_9 be a prime and formula_67 a prime power. Let formula_95 be a vector space of dimension formula_5 and formula_77 be an elliptic curve such that formula_98.\nDefine formula_99 as follows:\nformula_100.\nThe function formula_101 is an arbitrary homomorphism from formula_2 to formula_103.\n\nThe server chooses formula_104 secretly in formula_105 and publishes a point formula_106 of p-torsion such that formula_107 and also publishes formula_108 for formula_109.\nThe signature of the vector formula_110 is \nformula_111\nNote: This signature is homomorphic since the computation of h is a homomorphism.\n\nGiven formula_110 and its signature formula_113, verify that\n\nThe verification crucially uses the bilinearity of the Weil-pairing.\n\nThe server computes formula_115 for each formula_116. Transmits formula_117.\nAt each edge formula_118 while computing\nformula_119\nalso compute\nformula_120\non the elliptic curve formula_3.\n\nThe signature is a point on the elliptic curve with coordinates in formula_66. Thus the size of the signature is formula_123 bits (which is some constant times formula_124 bits, depending on the relative size of formula_9 and formula_67), and this is the transmission overhead. The computation of the signature formula_127 at each vertex requires formula_128 bit operations, where formula_129 is the in-degree of the vertex formula_130. The verification of a signature requires formula_131 bit operations.\n\nAttacker can produce a collision under the hash function.\n\nIf given formula_132 points in formula_103 find\nformula_134 and formula_135\n\nsuch that formula_136 and\n\nProposition: There is a polynomial time reduction from discrete log on the cyclic group of order formula_9 on elliptic curves to Hash-Collision.\n\nIf formula_139, then we get formula_140. Thus formula_141.\nWe claim that formula_142 and formula_143. Suppose that formula_144, then we would have formula_145, but formula_106 is a point of order formula_9 (a prime) thus formula_148. In other words formula_149 in formula_105. This contradicts the assumption that formula_151 and formula_152 are distinct pairs in formula_153. Thus we have that formula_154, where the inverse is taken as modulo formula_9.\n\nIf we have r > 2 then we can do one of two things. Either we can take formula_156 and formula_157 as before and set formula_158 for formula_159 > 2 (in this case the proof reduces to the case when formula_139), or we can take formula_161 and formula_162 where formula_163 are chosen at random from formula_105. We get one equation in one unknown (the discrete log of formula_106). It is quite possible that the equation we get does not involve the unknown. However, this happens with very small probability as we argue next. Suppose the algorithm for Hash-Collision gave us that\n\nThen as long as formula_167, we can solve for the discrete log of Q. But the formula_163’s are unknown to the oracle for Hash-Collision and so we can interchange the order in which this process occurs. In other words, given formula_169, for formula_170, not all zero, what is the probability that the formula_163’s we chose satisfies formula_172? It is clear that the latter probability is formula_173 . Thus with high probability we can solve for the discrete log of formula_106.\n\nWe have shown that producing hash collisions in this scheme is difficult. The other method by which an adversary can foil our system is by forging a signature. This scheme for the signature is essentially the Aggregate Signature version of the Boneh-Lynn-Shacham signature scheme. Here it is shown that forging a signature is at least as hard as solving the elliptic curve Diffie–Hellman problem. The only known way to solve this problem on elliptic curves is via computing discrete-logs. Thus forging a signature is at least as hard as solving the computational co-Diffie–Hellman on elliptic curves and probably as hard as computing discrete-logs.\n\n\n"}
{"id": "681265", "url": "https://en.wikipedia.org/wiki?curid=681265", "title": "Hopf–Rinow theorem", "text": "Hopf–Rinow theorem\n\nHopf–Rinow theorem is a set of statements about the geodesic completeness of Riemannian manifolds. It is named after Heinz Hopf and his student Willi Rinow, who published it in 1931.\n\nLet (\"M\", \"g\") be a connected Riemannian manifold. Then the following statements are equivalent:\n\n\nFurthermore, any one of the above implies that given any two points \"p\" and \"q\" in \"M\", there exists a length minimizing geodesic connecting these two points (geodesics are in general critical points for the length functional, and may or may not be minima).\n\n\n"}
{"id": "21473801", "url": "https://en.wikipedia.org/wiki?curid=21473801", "title": "Inversion (discrete mathematics)", "text": "Inversion (discrete mathematics)\n\nIn computer science and discrete mathematics a sequence has an inversion where two of its elements are out of their natural order.\n\nLet formula_1 be a permutation. If formula_2 and formula_3, either the pair of places formula_4 or the pair of elements formula_5 is called an inversion of formula_1.\n\nThe inversion is usually defined for permutations, but may also be defined for sequences:<br>\nLet formula_7 be a sequence (or multiset permutation). If formula_2 and formula_9, either the pair of places formula_4 or the pair of elements formula_11 is called an inversion of formula_7.\n\nFor sequences inversions according to the element-based definition are not unique, because different pairs of places may have the same pair of values.\n\nThe inversion set is the set of all inversions. A permutation's inversion set according to the place-based definition is that of the inverse permutation's inversion set according to the element-based definition, and vice versa, just with the elements of the pairs exchanged.\n\nThe inversion number is the cardinality of inversion set. It is a common measure of the sortedness of a permutation or sequence.\n\nIt is the number of crossings in the arrow diagram of the permutation, its Kendall tau distance from the identity permutation, and the sum of each of the inversion related vectors defined below.\n\nIt does not matter if the place-based or the element-based definition of inversion is used to define the inversion number, because a permutation and its inverse have the same number of inversions.\n\nOther measures of (pre-)sortedness include the minimum number of elements that can be deleted from the sequence to yield a fully sorted sequence, the number and lengths of sorted \"runs\" within the sequence, the Spearman footrule (sum of distances of each element from its sorted position), and the smallest number of exchanges needed to sort the sequence. Standard comparison sorting algorithms can be adapted to compute the inversion number in time .\n\nThree similar vectors are in use that condense the inversions of a permutation into a vector that uniquely determines it. They are often called \"inversion vector\" or \"Lehmer code\". (A list of sources is found .)\n\nThis article uses the term \"inversion vector\" (formula_13) like Wolfram. The remaining two vectors are sometimes called \"left\" and \"right inversion vector\", but to avoid confusion with the inversion vector this article calls them \"left inversion count\" (formula_14) and \"right inversion count\" (formula_15). Interpreted as a factorial number the left inversion count gives the permutations reverse colexicographic, and the right inversion count gives the lexicographic index.\nInversion vector formula_13:<br>\nWith the \"element-based\" definition formula_17 is the number of inversions whose \"smaller\" (right) component is formula_18.\n\nLeft inversion count formula_14:<br>\nWith the \"place-based\" definition formula_25 is the number of inversions whose \"bigger\" (right) component is formula_18.\n\nRight inversion count formula_15, often called \"Lehmer code\":<br>\nWith the \"place-based\" definition formula_33 is the number of inversions whose \"smaller\" (left) component is formula_18.\n\nBoth formula_13 and formula_15 can be found with the help of a Rothe diagram, which is a permutation matrix with the 1s represented by dots, and an inversion (often represented by a cross) in every position that has a dot to the right and below it. formula_33 is the sum of inversions in row formula_18 of the Rothe diagram, while formula_17 is the sum of inversions in column formula_18. The permutation matrix of the inverse is the transpose, therefore formula_13 of a permutation is formula_15 of its inverse, and vice versa.\n\nThe following sortable table shows the 24 permutations of four elements with their place-based inversion sets, inversion related vectors and inversion numbers. (The small columns are reflections of the columns next to them, and can be used to sort them in colexicographic order.)\n\nIt can be seen that formula_13 and formula_14 always have the same digits, and that formula_14 and formula_15 are both related to the place-based inversion set. The nontrivial elements of formula_14 are the sums of the descending diagonals of the shown triangle, and those of formula_15 are the sums of the ascending diagonals. (Pairs in descending diagonals have the right components 2, 3, 4 in common, while pairs in ascending diagonals have the left components 1, 2, 3 in common.)\n\nThe default order of the table is reverse colex order by formula_1, which is the same as colex order by formula_14. Lex order by formula_1 is the same as lex order by formula_15.\n\nThe set of permutations on \"n\" items can be given the structure of a partial order, called the weak order of permutations, which forms a lattice.\n\nThe Hasse diagram of the inversion sets ordered by the subset relation forms the skeleton of a permutohedron.\n\nIf a permutation is assigned to each inversion set using the place-based definition, the resulting order of permutations is that of the permutohedron, where an edge corresponds to the swapping of two elements with consecutive values. This is the weak order of permutations. The identity is its minimum, and the permutation formed by reversing the identity is its maximum.\n\nIf a permutation were assigned to each inversion set using the element-based definition, the resulting order of permutations would be that of a Cayley graph, where an edge corresponds to the swapping of two elements on consecutive places. This Cayley graph of the symmetric group is similar to its permutohedron, but with each permutation replaced by its inverse.\n\n\nSequences in the OEIS:\n\n\n"}
{"id": "13028634", "url": "https://en.wikipedia.org/wiki?curid=13028634", "title": "Jet Propulsion Laboratory Development Ephemeris", "text": "Jet Propulsion Laboratory Development Ephemeris\n\nThe name Jet Propulsion Laboratory Development Ephemeris (followed by a number), the abbreviation JPL DE(number), or just DE(number) designates one of a series of models of the Solar System produced at the Jet Propulsion Laboratory in Pasadena, California, primarily for purposes of spacecraft navigation and astronomy. The models consist of computer representations of positions, velocities and accelerations of major Solar System bodies, tabulated at equally spaced intervals of time, covering a specified span of years. Barycentric rectangular coordinates of the Sun, eight major planets and Pluto, and geocentric coordinates of the Moon are tabulated.\n\nDE405, created in May 1997, include both nutations and librations, and is considered the fundamental planetary and lunar ephemerides of The Astronomical Almanac. It is very large at 62.4 Megabytes, so smaller, more targeted versions have been created based on DE405.\n\nThere have been many versions of the JPL DE, from the 1960s through the present, in support of both robotic and manned spacecraft missions. Available documentation is sketchy, but we know DE69 was announced in 1969 to be the third release of the JPL Ephemeris Tapes, and was a special purpose, short-duration ephemeris. The then-current JPL Export Ephemeris was DE19. These early releases were distributed on magnetic tape.\n\nIn the days before personal computers, computers were large and expensive, and numerical integrations such as these were run by large organizations with ample resources. The JPL ephemerides prior to DE405 were integrated on a Univac mainframe in double precision. For instance, DE102, which was created in 1977, took six million steps and ran for nine days on a Univac 1100/81. DE405 was integrated on a DEC Alpha in quadruple precision.\n\nIn the 1970s and early 1980s, much work was done in the astronomical community to update the astronomical almanacs from the theoretical work of the 1890s to modern, relativistic theory. From 1975 through 1982, six ephemerides were produced at JPL using the modern techniques of least-squares adjustment of numerically-integrated output to high precision data: DE96 in Nov. 1975, DE102 in Sep. 1977, DE111 in May 1980, DE118 in Sep. 1981, and DE200 in 1982. DE102 was the first numerically integrated so-called Long Ephemeris, covering much of history for which useful astronomical observations were available: 1141 BC to AD 3001. DE200, a version of DE118 migrated to the J2000.0 reference frame, was adopted as the fundamental ephemeris for the new almanacs starting in 1984. DE402 introduced coordinates referred to the International Celestial Reference Frame (ICRF).\n\nThe JPL ephemerides have been the basis of the Astronomical Almanac since 1981's DE200. The current (2016) Almanac is derived from DE430.\n\nEach ephemeris was produced by numerical integration of the equations of motion, starting from a set of initial conditions. Due to the precision of modern observational data, the analytical method of general perturbations could no longer be applied to a high enough accuracy to adequately reproduce the observations. The method of special perturbations was applied, using numerical integration to solve the \"n\"-body problem, in effect putting the entire Solar System into motion in the computer's memory, accounting for all relevant physical laws. The initial conditions were both constants such as planetary masses, from outside sources, and parameters such as initial positions and velocities, adjusted to produce output which was a \"best fit\" to a large set of observations. A least-squares technique was used to perform the fitting. As of DE421, perturbations from 343 asteroids, representing about 90% of the mass of the main asteroid belt, have been included in the dynamical model.\n\nThe physics modeled included the mutual Newtonian gravitational accelerations and their relativistic corrections (a modified form of the Einstein-Infeld-Hoffmann equation), the accelerations caused by the tidal distortion of the Earth, the accelerations caused by the figure of the Earth and Moon, and a model of the lunar librations.\n\nThe observational data in the fits has been an evolving set, including: ranges (distances) to planets measured by radio signals from spacecraft, direct radar-ranging of planets, two-dimensional position fixes (on the plane of the sky) by VLBI of spacecraft, transit and CCD telescopic observations of planets and small bodies, and laser-ranging of retroreflectors on the Moon, among others. DE102, for instance, was fit to 48,479 observations.\n\nThe time argument of the integrated ephemerides is a relativistic coordinate time scale called T, necessary in precise work to account for the small relativistic effects of time dilation and simultaneity. In later ephemerides, T is essentially equivalent to the IAU definition of TCB.\n\nPositions and velocities of the Sun, Earth, Moon, and planets, along with the orientation of the Moon, are stored as Chebyshev polynomial coefficients fit in 32-day-long segments. The ephemerides are now available via World Wide Web and FTP as data files containing the Chebyshev coefficients, along with source code to recover (calculate) positions and velocities. Files vary in the time periods they cover, ranging from a few hundred years to several thousand, and bodies they include. Data may be based on each planet's true center or its barycenter.\n\nThe use of Chebyshev polynomials enables highly precise calculations for a given point in time. DE405 recovery (calculation) for the inner planets is about 0.001 arcseconds (equivalent to about 1 km at the distance of Mars); for the outer planets it is generally about 0.1 arcseconds. The 'reduced accuracy' DE406 ephemeris gives an interpolating precision (relative to the full ephemeris values) no worse than 25 metres for any planet and no worse than 1 metre for the moon.\n\nNote that these precision numbers are for the interpolated values relative original tabulated coordinates. The overall precision and accuracy of interpolated values for describing the actual motions of the planets will be a function of both the precision of the ephemeris tabulated coordinates and the precision of the interpolation.\n\n\nDE430 was created in 2013 and Is intended for use in analyzing modern data. It covers the dates 1550 Jan 01 to 2650 Jan 22 with the most accurate lunar ephemeris. From 2015 onwards this ephemeris is utilized in Astronomical Almanac. Beginning with this release only Mars' Barycenter was included due to the small masses of its moons Phobos and Deimos which create a very small offset from the planet's center. The complete ephemerides files is 128 megabytes but several alternative versions have been made available by JPL\n\nDE431 \nwas created in 2013 and is intended for analysis of earlier historical observations of the Sun, Moon, and planets. It covers a longer time span than DE430 (13201 BC to AD 17191) agreeing with DE430 within 1 meter over the time period covered by DE430. Position of the Moon is accurate within 20 meters between 1913-2113 and that error grows quadratically outside of that range. It is the largest of the ephemerides files at 3.4 gigabytes.\n\nDE432 was created April 2014. It includes librations but no nutations. DE432 is a minor update to DE430, and is intended primarily to aid the New Horizons project targeting of Pluto.\n\nDE102 was created in 1981, includes nutations but not librations. Referred to the dynamical equator and equinox of 1950. Covers JED 1206160.5 (-1410 APR 16) to JED 2817872.5 (3002 DEC 22).\n\nDE200 was created in 1981; includes nutations but not librations. Referred to the dynamical equator and equinox of 2000. Covers JED 2305424.5 (1599 DEC 09) to JED 2513360.5 (2169 MAR 31). This ephemeris was used for the Astronomical Almanac from 1984 to 2003.\n\nDE202 was created in 1987; includes nutations and librations. Referred to the dynamical equator and equinox of 2000. Covers JED 2414992.5 (1899 DEC 04) to JED 2469808.5 (2050 JAN 02).\n\nDE402 was released in 1995, and was quickly superseded by DE403.\n\nDE403 was created 1993, released in 1995. F JPL ephemeris was expressed in the coordinates of the International Earth Rotation Service (IERS) reference frame, essentially the ICRF. The data crunched by JPL to derive the ephemeris began to move away from limited-accuracy telescopic observations and more toward higher-accuracy radar-ranging of the planets, radio-ranging of spacecraft, and very-long-baseline-interferometric (VLBI) observations of spacecraft, especially for the four inner planets. Telescopic observations remained important for the outer planets because of their distance, hence the inability to bounce radar off of them, and the difficulty of parking a spacecraft near them. The perturbations of 300 asteroids were included, vs DE118/DE200 which included only the five asteroids determined to cause the largest perturbations. Better values of the planets' masses had been found since DE118/DE200, further refining the perturbations. Lunar Laser Ranging accuracy was improved, giving better positions of the Moon. DE403 covered the time span Apr 1599 to Jun 2199.\n\nDE404 was released in 1996. A so-called Long Ephemeris, this condensed version of DE403 covered 3000 BC to AD 3000. While both DE403 and DE404 were integrated over the same timespan, the interpolation of DE404 was somewhat reduced in accuracy and nutation of the Earth and libration of the Moon were not included.\n\nDE405 was released in 1998. It added several years' extra data from telescopic, radar, spacecraft, and VLBI observations (of the Galileo spacecraft at Jupiter, in particular). The method of modeling the asteroids' perturbations was improved, although the same number of asteroids were modeled. The ephemeris was more accurately oriented onto the ICRF. DE405 covered 1600 to 2200 to full precision. This ephemeris was utilized in the Astronomical Almanac from 2003 until 2014.\n\nDE406 was released with DE405 in 1998. A Long Ephemeris, this was the condensed version of DE405, covering 3000 BC to AD 3000 with the same limitations as DE404. This is the same integration as DE405, with the accuracy of the interpolating polynomials has been lessened to reduce file size for the longer time span covered by the file.\n\nDE407 was apparently unreleased. Details in readily-available sources are sketchy.\n\nDE408 was an unreleased ephemeris, created in 2005 as a longer version of DE406, covering 20,000 years.\n\nDE409 was released in 2003 for the MER spacecraft arrival at Mars and the Cassini arrival at Saturn. Further spacecraft ranging and VLBI (to the Mars Global Surveyor, Mars Pathfinder and the Mars Odyssey spacecraft) and telescopic data were included in the fit. The orbits of the Pioneer and Voyager spacecraft were reprocessed to give data points for Saturn. These resulted in improvements over DE405, especially to the predicted positions of Mars and Saturn. DE409 covered the years 1901 to 2019.\n\nDE410 was also released in 2003 covered 1901 - 2019, with improvements from DE409 in the masses for Venus, Mars, Jupiter, Saturn and the Earth-Moon system based on recent research. Though the masses had not yet been adopted by the IAU. The ephemerides were created to support the arrivals of the MER and Cassini spacecraft.\n\nDE411 was widely cited in the astronomical community, but not publicly released by JPL\n\nDE412 was widely cited in the astronomical community, but not publicly released by JPL\n\nDE413 was released in 2004 with updated ephemeris of Pluto in support of the occultation of a star by its satellite Charon on 11 Jul 2005. DE413 was fit to new CCD telescopic observations of Pluto in order to give improved positions of the planet and its moon.\n\nDE414 was created in 2005 and released in 2006. The numerical integration software was updated to use quadruple-precision for the Newtonian part of the equations of motion. Ranging data to the Mars Global Surveyor and Mars Odyssey spacecraft were extended to 2005, and further CCD observations of the five outer planets were included in the fit. Some data was accidentally left out of the fit, namely Magellan Venus data for 1992-94 and Galileo Jupiter data for 1996-97. Some ranging data to the NEAR Shoemaker spacecraft orbiting the asteroid Eros was used to derive the Earth/Moon mass ratio. DE414 covered the years 1599 to 2201.\n\nDE418 was released in 2007 for planning the New Horizons mission to Pluto. New observations of Pluto, which took advantage of the new astrometric accuracy of the Hipparcos star catalog, were included in the fit. Mars spacecraft ranging and VLBI observations were updated through 2007. Asteroid masses were estimated differently. Lunar laser ranging data for the Moon was added for the first time since DE403, significantly improving the lunar orbit and librations. Estimated position data from the Cassini spacecraft was included in the fit, improving the orbit of Saturn, but rigorous analysis of the data was deferred to a later date. DE418 covered the years 1899 to 2051, and JPL recommended not using it outside of that range due to minor inconsistencies which remained in the planets' masses due to time constraints.\n\nDE421 was released in 2008. It included additional ranging and VLBI measurements of Mars spacecraft, new ranging and VLBI of the Venus Express spacecraft, the latest estimates of planetary masses, additional lunar laser ranging, and two more months of CCD measurements of Pluto. When initially released in 2008, the DE421 ephemeris covered the years 1900 to 2050. An additional data release in 2013 extended the coverage to the year 2200.\n\nDE422 was created in 2009 for the MESSENGER mission to Mercury. A Long Ephemeris, it was intended to replace DE406, covering 3000 BC to AD 3000.\n\nDE423 was released in 2010. Position estimates of the MESSENGER spacecraft and additional range and VLBI data from the Venus Express spacecraft were fit. DE423 covered the years 1799 to 2200.\n\nDE424 was created in 2011 to support the Mars Science Laboratory mission.\n\n\n\n"}
{"id": "7792761", "url": "https://en.wikipedia.org/wiki?curid=7792761", "title": "Literal (mathematical logic)", "text": "Literal (mathematical logic)\n\nIn mathematical logic, a literal is an atomic formula (atom) or its negation. \nThe definition mostly appears in proof theory (of classical logic), e.g. in conjunctive normal form and the method of resolution.\n\nLiterals can be divided into two types:\n\nFor a literal formula_1, the complementary literal is a literal corresponding to the negation of formula_1, \nwe can write formula_3 to denote the complementary literal of formula_1. More precisely, if formula_5 then formula_3 is formula_7 and if formula_8 then formula_3 is formula_10.\n\nIn the context of a formula in the conjunctive normal form, a literal is pure if the literal's complement does not appear in the formula.\n\nIn Boolean functions, the variables that appear either in complemented or uncompleted form is a literal. For example, if formula_11, formula_12 and formula_13are variables then the expressions formula_14contains three literal and formula_15contains three literals.\n\nIn propositional calculus a literal is simply a propositional variable or its negation.\n\nIn predicate calculus a literal is an atomic formula or its negation, where an atomic formula is a predicate symbol applied to some terms, formula_16 with the terms recursively defined starting from constant symbols, variable symbols, and function symbols. For example, formula_17 is a negative literal with the constant symbol 2, the variable symbols \"x\", \"y\", the function symbols \"f\", \"g\", and the predicate symbol \"Q\".\n"}
{"id": "43588298", "url": "https://en.wikipedia.org/wiki?curid=43588298", "title": "Mildly context-sensitive grammar formalism", "text": "Mildly context-sensitive grammar formalism\n\nIn computational linguistics, the term mildly context-sensitive grammar formalisms refers to several grammar formalisms that have been developed with the ambition to provide adequate descriptions of the syntactic structure of natural language.\n\nEvery mildly context-sensitive grammar formalism defines a class of mildly context-sensitive grammars (the grammars that can be specified in the formalism), and therefore also a class of mildly context-sensitive languages (the formal languages generated by the grammars).\n\nBy 1985, several researchers in descriptive and mathematical linguistics had provided evidence against the hypothesis that the syntactic structure of natural language can be adequately described by context-free grammars.\nAt the same time, the step to the next level of the Chomsky hierarchy, to context-sensitive grammars, appeared both unnecessary and undesirable.\nIn an attempt to pinpoint the exact formal power required for the adequate description of natural language syntax, Aravind Joshi characterized \"grammars (and associated languages) that are only slightly more powerful than context-free grammars (context-free languages)\".\nHe called these grammars \"mildly context-sensitive grammars\" and the associated languages \"mildly context-sensitive languages\".\n\nJoshi’s characterization of mildly context-sensitive grammars was biased toward his work on tree-adjoining grammar (TAG).\nHowever, together with his students Vijay Shanker and David Weir, Joshi soon discovered that TAGs are equivalent, in terms of the generated string languages, to the independently introduced head grammar (HG).\nThis was followed by two similar equivalence results, for linear indexed grammar (LIG) and combinatory categorial grammar (CCG), which showed that the notion of mildly context-sensitivity is a very general one and not tied to a specific formalism.\n\nThe TAG-equivalent formalisms were generalized by the introduction of linear context-free rewriting systems (LCFRS).\nThese grammars define an infinite hierarchy of string languages in between the context-free and the context-sensitive languages, with the languages generated by the TAG-equivalent formalisms at the lower end of the hierarchy.\nIndependently of and almost simultaneously to LCFRS, Hiroyuki Seki et al. proposed the essentially identical formalism of multiple context-free grammar (MCFG).\nLCFRS/MCFG is sometimes regarded as the most general formalism for specifying mildly context-sensitive grammars.\nHowever, several authors have noted that some of the characteristic properties of the TAG-equivalent formalisms are not preserved by LCFRS/MCFG, and that there are languages that have the characteristic properties of mildly context-sensitivity but are not generated by LCFRS/MCFG.\n\nRecent years have seen increased interest in the restricted class of \"well-nested\" linear context-free rewriting systems/multiple context-free grammars, which define a class of grammars that properly includes the TAG-equivalent formalisms but is properly included in the unrestricted LCFRS/MCFG hierarchy.\n\nDespite a considerable amount of work on the subject, there is no generally accepted formal definition of mild context-sensitivity.\n\nAccording to the original characterization by Joshi, a class of mildly context-sensitive grammars should have the following properties:\nIn addition to these, it is understood that every class of mildly context-sensitive grammars should be able to generate all context-free languages.\n\nJoshi’s characterization is not a formal definition. He notes:\nOther authors have proposed alternative characterizations of mild context-sensitivity, some of which take the form of formal definitions.\nFor example, Laura Kallmeyer takes the perspective that mild context-sensitivity should be defined as a property of classes of languages rather than, as in Joshi’s characterization, classes of grammars.\nSuch a language-based definition leads to a different notion of the concept than Joshi’s.\n\nThe term \"cross-serial dependencies\" refers to certain characteristic word ordering patterns, in particular to the verb–argument patterns observed in subordinate clauses in Dutch and Swiss German.\nThese are the very patterns that can be used to argue against the context-freeness of natural language; thus requiring mildly context-sensitive grammars to model cross-serial dependencies means that these grammars must be more powerful than context-free grammars.\n\nKallmeyer identifies the ability to model cross-serial dependencies with the ability to generate the \"copy language\"\n\nformula_1\n\nand its generalizations to two or more copies of \"w\", up to some bound.\nThese languages are not context-free, which can be shown using the pumping lemma.\n\nA formal language is of \"constant growth\" if every string in the language is longer than the next shorter strings by at most a (language-specific) constant.\nLanguages that violate this property are often considered to be beyond human capacity, although some authors have argued that certain phenomena in natural language do show a growth that cannot be bounded by a constant.\n\nMost mildly context-sensitive grammar formalisms (in particular, LCFRS/MCFG) actually satisfy a stronger property than constant growth called \"semilinearity\".\nA language is semilinear if its image under the Parikh-mapping (the mapping that \"forgets\" the relative position of the symbols in a string, effectively treating it as a bag of words) is a regular language.\nAll semilinear languages are of constant growth, but not every language with constant growth is semilinear.\n\nA grammar formalism is said to have \"polynomial parsing\" if its membership problem can be solved in deterministic polynomial time.\nThis is the problem to decide, given a grammar \"G\" written in the formalism and a string \"w\", whether \"w\" is generated by \"G\" – that is, whether \"w\" is \"grammatical\" according to \"G\".\nThe time complexity of this problem is measured in terms of the combined size of \"G\" and \"w\".\n\nUnder the view on mild context-sensitivity as a property of classes of languages, \"polynomial parsing\" refers to the language membership problem.\nThis is the problem to decide, for a fixed language \"L\", whether a given string \"w\" belongs to \"L\".\nThe time complexity of this problem is measured in terms of the length of \"w\"; it ignores the question how \"L\" is specified.\n\nNote that both understandings of \"polynomial parsing\" are idealizations in the sense that for practical applications one is interested not only in the yes/no question whether a sentence is grammatical, but also in the syntactic structure that the grammar assigns to the sentence.\n\nOver the years, a large number of grammar formalisms have been introduced that satisfy some or all of the characteristic properties put forth by Joshi.\nSeveral of them have alternative, automaton-based characterizations that are not discussed in this article; for example, the languages generated by tree-adjoining grammar can be characterized by embedded pushdown automata.\n\n\n\n\nLinear context-free rewriting systems/multiple context-free grammars form a two-dimensional hierarchy of generative power with respect to two grammar-specific parameters called \"fan-out\" and \"rank\".\nMore precisely, the languages generated by LCFRS/MCFG with fan-out  and rank  are properly included in the class of languages generated by LCFRS/MCFG with rank  and fan-out \"f\", as well as the class of languages generated by LCFRS/MCFG with rank \"r\" and fan-out .\nIn the presence of well-nestedness, this hierarchy collapses to a one-dimensional hierarchy with respect to fan-out; this is because every well-nested LCFRS/MCFG can be transformed into an equivalent well-nested LCFRS/MCFG with the same fan-out and rank 2.\nWithin the LCFRS/MCFG hierarchy, the context-free languages can be characterized by the grammars with fan-out 1; for this fan-out there is no difference between general and well-nested grammars.\nThe TAG-equivalent formalisms can be characterized as well-nested LCFRS/MCFG of fan-out 2.\n\n"}
{"id": "34626783", "url": "https://en.wikipedia.org/wiki?curid=34626783", "title": "Monothetic group", "text": "Monothetic group\n\nIn mathematics, a monothetic group is a topological group with a dense cyclic subgroup. They were introduced by . An example is the additive group of p-adic integers, in which the integers are dense.\n\nA monothetic group is necessarily abelian. \n\n"}
{"id": "405028", "url": "https://en.wikipedia.org/wiki?curid=405028", "title": "Multiply–accumulate operation", "text": "Multiply–accumulate operation\n\nIn computing, especially digital signal processing, the multiply–accumulate operation is a common step that computes the product of two numbers and adds that product to an accumulator. The hardware unit that performs the operation is known as a multiplier–accumulator (MAC, or MAC unit); the operation itself is also often called a MAC or a MAC operation. The MAC operation modifies an accumulator \"a\":\n\nWhen done with floating point numbers, it might be performed with two roundings (typical in many DSPs), or with a single rounding. When performed with a single rounding, it is called a fused multiply–add (FMA) or fused multiply–accumulate (FMAC).\n\nModern computers may contain a dedicated MAC, consisting of a multiplier implemented in combinational logic followed by an adder and an accumulator register that stores the result. The output of the register is fed back to one input of the adder, so that on each clock cycle, the output of the multiplier is added to the register. Combinational multipliers require a large amount of logic, but can compute a product much more quickly than the method of shifting and adding typical of earlier computers. The first processors to be equipped with MAC units were digital signal processors, but the technique is now also common in general-purpose processors.\n\nWhen done with integers, the operation is typically exact (computed modulo some power of two). However, floating-point numbers have only a certain amount of mathematical precision. That is, digital floating-point arithmetic is generally not associative or distributive. (See Floating point#Accuracy problems.)\nTherefore, it makes a difference to the result whether the multiply–add is performed with two roundings, or in one operation with a single rounding (a fused multiply–add). IEEE 754-2008 specifies that it must be performed with one rounding, yielding a more accurate result.\n\nA \"fused\" multiply–add (sometimes known as \"FMA\" or \"fmadd\")\nis a floating-point multiply–add operation performed in one step, with a single rounding. That is, where an unfused multiply–add would compute the product \"b\"×\"c\", round it to \"N\" significant bits, add the result to \"a\", and round back to \"N\" significant bits, a fused multiply–add would compute the entire expression \"a\"+\"b\"×\"c\" to its full precision before rounding the final result down to \"N\" significant bits.\n\nA fast FMA can speed up and improve the accuracy of many computations that involve the accumulation of products:\n\n\nFused multiply–add can usually be relied on to give more accurate results. However, William Kahan has pointed out that it can give problems if used unthinkingly. If is evaluated as using fused multiply–add, then the result may be negative even when due to the first multiplication discarding low significance bits. This could then lead to an error if, for instance, the square root of the result is then evaluated.\n\nWhen implemented inside a microprocessor, an FMA can actually be faster than a multiply operation followed by an add. However, standard industrial implementations based on the original IBM RS/6000 design require a 2\"N\"-bit adder to compute the sum properly.\n\nA useful benefit of including this instruction is that it allows an efficient software implementation of division (see division algorithm) and square root (see methods of computing square roots) operations, thus eliminating the need for dedicated hardware for those operations.\n\nSome machines combine multiple fused multiply add operations into a single step, e.g. performing a four-element dot-product on two 128-bit SIMD registers \"a0\"×\"b0\"+\"a1\"×\"b1\"+\"a2\"×\"b2\"+\"a3\"×\"b3\" with single cycle throughput.\n\nThe FMA operation is included in IEEE 754-2008.\n\nThe DEC VAX's POLY instruction is used for evaluating polynomials with Horner's rule using a succession of multiply and add steps. Instruction descriptions do not specify whether the multiply and add are performed using a single fma step. This instruction has been a part of the VAX instruction set since its original 11/780 implementation in 1977.\n\nThe 1999 standard of the C programming language supports the FMA operation through the codice_1 standard math library function, and standard pragmas controlling optimizations based on FMA.\n\nThe fused multiply–add operation was introduced as \"multiply–add fused\" in the IBM POWER1 (1990) processor, but has been added to numerous other processors since then:\n\n"}
{"id": "7244592", "url": "https://en.wikipedia.org/wiki?curid=7244592", "title": "Ombient", "text": "Ombient\n\nOmbient is the moniker under which Mike Hunter performs his completely improvised ambient/drone music.\n\nOmbient's ambient/drone music, being of a live and improvisational nature, is representative of the feeling of the moment in which it is performed and of the subtle feedback between the audience and the cool performer. It features amplified guitar which is processed and layered using digital looping equipment enabling a single guitar to produce symphonic levels of density. Ombient has in the last 3 years delved deeply into the world of analog synthesis and more recently analog modular synthesis.\n\nMike Hunter also plays in the progressive/world/ambient/space rock band known as Brainstatik, of which Mike Hunter is a member.\n\nMike Hunter is also a Fractal artist.\n\nMike Hunter also hosts the long time running FM radio program Music With Space on WPRB 103.3 FM\n\n"}
{"id": "47070665", "url": "https://en.wikipedia.org/wiki?curid=47070665", "title": "Prismanes", "text": "Prismanes\n\nThe prismanes are a class of hydrocarbon compounds consisting of prism-like polyhedra of various numbers of sides on the polygonal base. Chemically, it is a series of fused cyclobutane rings (a ladderane, with all-cis/all-syn geometry) that wraps around to join its ends and form a band, with cycloalkane edges. Their chemical formula is (CH), where \"n\" is the number of cyclobutane sides (the size of the cycloalkane base), and that number also forms the basis for a system of nomenclature within this class. The first few chemicals in this class are:\n\nTriprismane, tetraprismane, and pentaprismane have been synthesized and studied experimentally, and many higher members of the series have been studied using computer models. The first several members do indeed have the geometry of a regular prism, with flat \"n\"-gon bases. As \"n\" becomes increasingly large, however, modeling experiments find that highly symmetric geometry is no longer stable, and the molecule distorts into less-symmetric forms. One series of modelling experiments found that starting with [11]prismane, the regular-prism form is not a stable geometry. For example, the structure of [12]prismane would have the cyclobutane chain twisted, with the dodecagonal bases non-planar and non-parallel. \n\nFor large base-sizes, some of the cyclobutanes can be fused anti to each other, giving a non-convex polygon base. These are geometric isomers of the prismanes. Two isomers of [12]prismane that have been studied computationally are named helvetane and israelane, based on the star-like shapes of the rings that form their bases. This was explored computationally after originally being proposed as an April fools joke.\n\nThe polyprismanes consist of multiple prismanes stacked base-to-base. The carbons at each intermediate level—the \"n\"-gon bases where the prismanes fuse to each other—have no hydrogen atoms attached to them.\n\nThe asteranes contain a methylene group bridge on each edge between the two \"n\"-gon bases. Each side is thus a cyclohexane rather than a cyclobutane.\n"}
{"id": "1797188", "url": "https://en.wikipedia.org/wiki?curid=1797188", "title": "Pulation square", "text": "Pulation square\n\nIn category theory, a branch of mathematics, a pulation square (also called a Doolittle diagram) is a diagram that is simultaneously a pullback square and a pushout square. It is a self-dual concept.\n\n"}
{"id": "664488", "url": "https://en.wikipedia.org/wiki?curid=664488", "title": "Reciprocal lattice", "text": "Reciprocal lattice\n\nIn physics, the reciprocal lattice represents the Fourier transform of another lattice (usually a Bravais lattice). In normal usage, this first lattice (whose transform is represented by the reciprocal lattice) is usually a periodic spatial function in real-space and is also known as the \"direct lattice\". While the direct lattice exists in real-space and is what one would commonly understand as a physical lattice, the reciprocal lattice exists in reciprocal space (also known as \"momentum space\" or less commonly as \"K-space\", due to the relationship between the Pontryagin duals momentum and position.) The reciprocal lattice of a reciprocal lattice, then, is the original direct lattice again, since the two lattices are Fourier transforms of each other.\n\nThe reciprocal lattice plays a fundamental role in most analytic studies of periodic structures, particularly in the theory of diffraction. In neutron and X-ray diffraction, due to the Laue conditions, the momentum difference between incoming and diffracted X-rays of a crystal is a reciprocal lattice vector. The diffraction pattern of a crystal can be used to determine the reciprocal vectors of the lattice. Using this process, one can infer the atomic arrangement of a crystal.\n\nThe Brillouin zone is a Wigner-Seitz cell of the reciprocal lattice.\n\nAssuming a two-dimensional Bravais lattice\n\nAny quantity, e.g. the electronic density in an atomic crystal can be written as a periodic function\n\nDue to the periodicity, it is useful to write formula_4 as a Fourier series\n\nSince formula_6 for any formula_7 then the last formula is true for the particular case formula_8\n\nso must be true formula_10 that means\n\nMathematically, we can describe the reciprocal lattice as the set of all vectors formula_13 that satisfy the above identity for all lattice point position vectors R.\n\nThis reciprocal lattice is itself a Bravais lattice, and the reciprocal of the reciprocal lattice is the original lattice, which reveals the Pontryagin duality of their respective vector spaces.\n\nFor an infinite two-dimensional lattice, defined by its primitive vectors formula_14, its reciprocal lattice can be determined by generating its two reciprocal primitive vectors, through the following formulae,\n\nWhere,\n\nHere formula_17 represents a 90 degree rotation matrix.\n\nFor an infinite three-dimensional lattice, defined by its primitive vectors formula_18, its reciprocal lattice can be determined by generating its three reciprocal primitive vectors, through the formulae\n\nwhere\n\nNote that the denominator is the scalar triple product. Using column vector representation of (reciprocal) primitive vectors, the formulae above can be rewritten using matrix inversion:\n\nThis method appeals to the definition, and allows generalization to arbitrary dimensions. The cross product formula dominates introductory materials on crystallography.\n\nThe above definition is called the \"physics\" definition, as the factor of formula_22 comes naturally from the study of periodic structures. An equivalent definition, the \"crystallographer's\" definition, comes from defining the reciprocal lattice to be formula_23 which changes the definitions of the reciprocal lattice vectors to be\n\nand so on for the other vectors. The crystallographer's definition has the advantage that the definition of\nformula_25 is just the reciprocal magnitude of formula_26 in the direction of formula_27, dropping the factor of formula_22. This can simplify certain mathematical manipulations, and expresses reciprocal lattice dimensions in units of spatial frequency. It is a matter of taste which definition of the lattice is used, as long as the two are not mixed.\n\nEach point formula_29 in the reciprocal lattice corresponds to a set of lattice planes formula_29 in the real space lattice. The direction of the reciprocal lattice vector corresponds to the normal to the real space planes. The magnitude of the reciprocal lattice vector is given in reciprocal length and is equal to the reciprocal of the interplanar spacing of the real space planes.\n\nReciprocal lattices for the cubic crystal system are as follows.\n\nThe simple cubic Bravais lattice, with cubic primitive cell of side formula_31, has for its reciprocal a simple cubic lattice with a cubic primitive cell of side formula_32 (formula_33 in the crystallographer's definition). The cubic lattice is therefore said to be self-dual, having the same symmetry in reciprocal space as in real space.\n\nThe reciprocal lattice to an FCC lattice is the body-centered cubic (BCC) lattice.\n\nConsider an FCC compound unit cell. Locate a primitive unit cell of the FCC; i.e., a unit cell with one lattice point. Now take one of the vertices of the primitive unit cell as the origin. Give the basis vectors of the real lattice. Then from the known formulae, you can calculate the basis vectors of the reciprocal lattice. These reciprocal lattice vectors of the FCC represent the basis vectors of a BCC real lattice. Note that the basis vectors of a real BCC lattice and the reciprocal lattice of an FCC resemble each other in direction but not in magnitude.\n\nThe reciprocal lattice to a BCC lattice is the FCC lattice.\n\nIt can be easily proven that only the Bravais lattices which have 90 degrees between formula_34 (cubic, tetragonal, orthorhombic) have formula_35 parallel to their real-space vectors.\n\nThe reciprocal to a simple hexagonal Bravais lattice with lattice constants c and a is another simple hexagonal lattice with lattice constants formula_36 and formula_37 rotated through 30° about the c axis with respect to the direct lattice. The simple hexagonal lattice is therefore said to be self-dual, having the same symmetry in reciprocal space as in real space.\n\nFrom its definition we know that the vectors of the Bravais lattice must be closed under vector addition and subtraction. Thus it is sufficient to say that if we have\n\nand\n\nthen the sum and difference formula_40 satisfy the same.\n\nThus we have shown the reciprocal lattice is closed under vector addition and subtraction. Furthermore, we know that a vector G in the reciprocal lattice can be expressed as a linear combination of its primitive vectors.\n\nFrom our earlier definition of formula_25, we can see that:\n\nwhere formula_45 is the Kronecker delta. We let R be a vector in the direct lattice, which we can express as a linear combination of \"its\" primitive vectors.\n\nFrom this we can see that:\n\nFrom our definition of the reciprocal lattice we have shown that formula_48 must satisfy the following identity.\n\nFor this to hold we must have formula_50 equal to formula_51 times an integer. This is fulfilled because formula_52 and formula_53. Therefore, the reciprocal lattice is also a Bravais lattice.\n\nFurthermore, if the vectors formula_48 construct a reciprocal lattice, it is clear that any vector formula_55 satisfying the equation:\n\n… is a reciprocal lattice vector of the reciprocal lattice. Due to the definition of formula_48, when formula_55 is the direct lattice vector formula_17, we have the same relationship.\n\nAnd so we can conclude that the reciprocal lattice of the reciprocal lattice is the original direct lattice.\n\nOne path to the reciprocal lattice of an arbitrary collection of atoms comes from the idea of scattered waves in the Fraunhofer (long-distance or lens back-focal-plane) limit as a Huygens-style sum of amplitudes from all points of scattering (in this case from each individual atom). This sum is denoted by the complex amplitude F in the equation below, because it is also the Fourier transform (as a function of spatial frequency or reciprocal distance) of an effective scattering potential in direct space:\n\nHere g = q/(2π) is the scattering vector q in crystallographer units, N is the number of atoms, f[g] is the atomic scattering factor for atom j and scattering vector g, while r is the vector position of atom j. Note that the Fourier phase depends on one's choice of coordinate origin.\n\nFor the special case of an infinite periodic crystal, the scattered amplitude F = M F from M unit cells (as in the cases above) turns out to be non-zero only for integer values of formula_29, where\n\nwhen there are j=1,m atoms inside the unit cell whose fractional lattice indices are respectively {u, v, w}. To consider effects due to finite crystal size, of course, a shape convolution for each point or the equation above for a finite lattice must be used instead.\n\nWhether the array of atoms is finite or infinite, one can also imagine an \"intensity reciprocal lattice\" I[g], which relates to the amplitude lattice F via the usual relation I = FF where F is the complex conjugate of F. Since Fourier transformation is reversible, of course, this act of conversion to intensity tosses out \"all except 2nd moment\" (i.e. the phase) information. For the case of an arbitrary collection of atoms, the intensity reciprocal lattice is therefore:\n\nHere r is the vector separation between atom j and atom k. One can also use this to predict the effect of nano-crystallite shape, and subtle changes in beam orientation, on detected diffraction peaks even if in some directions the cluster is only one atom thick. On the down side, scattering calculations using the reciprocal lattice basically consider an incident plane wave. Thus after a first look at reciprocal lattice (kinematic scattering) effects, beam broadening and multiple scattering (i.e. dynamical) effects may be important to consider as well.\n\nThere are actually two versions in mathematics of the abstract dual lattice concept, for a given lattice \"L\" in a real vector space \"V\", of finite dimension.\n\nThe first, which generalises directly the reciprocal lattice construction, uses Fourier analysis. It may be stated simply in terms of Pontryagin duality. The dual group \"V\"^ to \"V\" is again a real vector space, and its closed subgroup \"L\"^ dual to \"L\" turns out to be a lattice in \"V\"^. Therefore, \"L\"^ is the natural candidate for \"dual lattice\", in a different vector space (of the same dimension).\n\nThe other aspect is seen in the presence of a quadratic form \"Q\" on \"V\"; if it is non-degenerate it allows an identification of the dual space \"V\" of \"V\" with \"V\". The relation of \"V\" to \"V\" is not intrinsic; it depends on a choice of Haar measure (volume element) on \"V\". But given an identification of the two, which is in any case well-defined up to a scalar, the presence of \"Q\" allows one to speak to the dual lattice to \"L\" while staying within \"V\".\n\nIn mathematics, the dual lattice of a given lattice \"L\" in an abelian locally compact topological group \"G\" is the subgroup \"L\" of the dual group of \"G\" consisting of all continuous characters that are equal to one at each point of \"L\".\n\nIn discrete mathematics, a lattice is a locally discrete set of points described by all integral linear combinations of dim = n linearly independent vectors in R. The dual lattice is then defined by all points in the linear span of the original lattice (typically all of R^n) with the property that an integer results from the inner product with all elements of the original lattice. It follows that the dual of the dual lattice is the original lattice.\n\nFurthermore, if we allow the matrix B to have columns as the linearly independent vectors that describe the lattice, then the matrix\n\nformula_65\nhas columns of vectors that describe the dual lattice.\n\nReciprocal space (also called \"k-space\") is the space in which the Fourier transform of a spatial function is represented (similarly the frequency domain is the space in which the Fourier transform of a time dependent function is represented). A Fourier transform takes us from \"real space\" to reciprocal space or \"vice versa\". Reciprocal space comes into play regarding wave-mechanics: As a plane wave can be written by an oscillatory term formula_66 with wave vector formula_67 and angular frequency formula_68, it can be regarded as both a function of formula_67 and formula_70 (and the spectroscopic part as a function of both formula_68 and formula_72). In space, the periodicity oscillates with formula_73 - therefore for a given phase, formula_67 and formula_70 are reciprocal to each other: formula_76 and formula_77.\n\nA reciprocal lattice is a periodic set of points in this space, and contains the formula_78 points that compose the Fourier transform of a periodic spatial lattice. The Brillouin zone is a volume within this space that contains all the unique k-vectors that represent the periodicity of classical or quantum waves allowed in a periodic structure.\n\n\n"}
{"id": "332090", "url": "https://en.wikipedia.org/wiki?curid=332090", "title": "Recursively enumerable set", "text": "Recursively enumerable set\n\nIn computability theory, traditionally called recursion theory, a set \"S\" of natural numbers is called recursively enumerable, computably enumerable, semidecidable, provable or Turing-recognizable if:\n\n\nOr, equivalently,\n\n\nThe first condition suggests why the term \"semidecidable\" is sometimes used; the second suggests why \"computably enumerable\" is used. The abbreviations r.e. and c.e. are often used, even in print, instead of the full phrase.\n\nIn computational complexity theory, the complexity class containing all recursively enumerable sets is RE. In recursion theory, the lattice of r.e. sets under inclusion is denoted formula_1.\n\nA set \"S\" of natural numbers is called recursively enumerable if there is a partial recursive function whose domain is exactly \"S\", meaning that the function is defined if and only if its input is a member of \"S\".\n\nThe following are all equivalent properties of a set \"S\" of natural numbers:\n\nThe equivalence of semidecidability and enumerability can be obtained by the technique of dovetailing.\n\nThe Diophantine characterizations of a recursively enumerable set, while not as straightforward or intuitive as the first definitions, were found by Yuri Matiyasevich as part of the negative solution to Hilbert's Tenth Problem. Diophantine sets predate recursion theory and are therefore historically the first way to describe these sets (although this equivalence was only remarked more than three decades after the introduction of recursively enumerable sets).\nThe number of bound variables in the above definition of the Diophantine set is the best known so far; it might be that a lower number can be used to define all diophantine sets.\n\n\nIf \"A\" and \"B\" are recursively enumerable sets then \"A\" ∩ \"B\", \"A\" ∪ \"B\" and \"A\" × \"B\" (with the ordered pair of natural numbers mapped to a single natural number with the Cantor pairing function) are recursively enumerable sets. The preimage of a recursively enumerable set under a partial recursive function is a recursively enumerable set.\n\nA set is recursively enumerable if and only if it is at level formula_12 of the arithmetical hierarchy.\n\nA set formula_13 is called co-recursively enumerable or co-r.e. if its complement formula_14 is recursively enumerable. Equivalently, a set is co-r.e. if and only if it is at level formula_15 of the arithmetical hierarchy.\n\nA set \"A\" is recursive (synonym: computable) if and only if both \"A\" and the complement of \"A\" are recursively enumerable. A set is recursive if and only if it is either the range of an increasing total recursive function or finite.\n\nSome pairs of recursively enumerable sets are effectively separable and some are not.\n\nAccording to the Church–Turing thesis, any effectively calculable function is calculable by a Turing machine, and thus a set \"S\" is recursively enumerable if and only if there is some algorithm which yields an enumeration of \"S\". This cannot be taken as a formal definition, however, because the Church–Turing thesis is an informal conjecture rather than a formal axiom.\n\nThe definition of a recursively enumerable set as the \"domain\" of a partial function, rather than the \"range\" of a total recursive function, is common in contemporary texts. This choice is motivated by the fact that in generalized recursion theories, such as α-recursion theory, the definition corresponding to domains has been found to be more natural. Other texts use the definition in terms of enumerations, which is equivalent for recursively enumerable sets.\n\n"}
{"id": "8612907", "url": "https://en.wikipedia.org/wiki?curid=8612907", "title": "Relative interior", "text": "Relative interior\n\nIn mathematics, the relative interior of a set is a refinement of the concept of the interior, which is often more useful when dealing with low-dimensional sets placed in higher-dimensional spaces.\n\nFormally, the relative interior of a set \"S\" (denoted formula_1) is defined as its interior within the affine hull of \"S\". In other words,\nwhere formula_3 is the affine hull of \"S\", and formula_4 is a ball of radius formula_5 centered on formula_6. Any metric can be used for the construction of the ball; all metrics define the same set as the relative interior.\n\nFor any nonempty convex set formula_7 the relative interior can be defined as\n\n"}
{"id": "6355948", "url": "https://en.wikipedia.org/wiki?curid=6355948", "title": "Representation (mathematics)", "text": "Representation (mathematics)\n\nIn mathematics, representation is a very general relationship that expresses similarities between objects. Roughly speaking, a collection \"Y\" of mathematical objects may be said to \"represent\" another collection \"X\" of objects, provided that the properties and relationships existing among the representing objects \"y\" conform in some consistent way to those existing among the corresponding represented objects \"x\". Somewhat more formally, for a set \"Π\" of properties and relations, a \"Π\"-representation of some structure \"X\" is a structure \"Y\" that is the image of \"X\" under a homomorphism that preserves \"Π\". The label \"representation\" is sometimes also applied to the homomorphism itself.\n\nPerhaps the most well-developed example of this general notion is the subfield of abstract algebra called representation theory, which studies the representing of elements of algebraic structures by linear transformations of vector spaces.\n\nAlthough the term \"representation theory\" is well established in the algebraic sense discussed above, there are many other uses of the term \"representation\" throughout mathematics.\n\nAn active area of graph theory is the exploration of isomorphisms between graphs and other structures.\nA key class of such problems stems from the fact that, like adjacency in undirected graphs, intersection of sets\n(or, more precisely, non-disjointness) is a symmetric relation.\nThis gives rise to the study of intersection graphs for innumerable families of sets.\nOne foundational result here, due to Paul Erdős and colleagues, is that every \"n\"-vertex graph may be represented in terms of intersection among subsets of a set of size no more than \"n\"/4.\n\nRepresenting a graph by such algebraic structures as its adjacency matrix and Laplacian matrix gives rise to the field of spectral graph theory.\n\nDual to the observation above that every graph is an intersection graph\nis the fact that every partially ordered set is isomorphic to a collection of sets ordered by the containment (or inclusion) relation ⊆.\nAmong the posets that arise as the containment orders for natural classes of objects are the Boolean lattices and the orders of dimension \"n\".\n\nMany partial orders arise from (and thus can be represented by) collections of geometric objects.\nAmong them are the \"n\"-ball orders.\nThe 1-ball orders are the interval-containment orders,\nand the 2-ball orders are the so-called \"circle orders\",\nthe posets representable in terms of containment among disks in the plane.\nA particularly nice result in this field is the characterization of the planar graphs as those graphs\nwhose vertex-edge incidence relations are circle orders.\n\nThere are also geometric representations that are not based on containment.\nIndeed, one of the best studied classes among these are the interval orders,\nwhich represent the partial order in terms of what might be called \"disjoint precedence\" of intervals on the real line:\neach element \"x\" of the poset is represented by an interval [\"x\", \"x\"] such that\nfor any \"y\" and \"z\" in the poset, \"y\" is below \"z\" if and only if \"y\" < \"z\".\n\nUnder certain circumstances, a single function \"f\":\"X\" → \"Y\" is at once an isomorphism from several mathematical structures on \"X\".\nSince each of those structures may be thought of, intuitively, as a meaning of the image \"Y\"—one of the things that \"Y\" is trying to tell us—this phenomenon is called polysemy,\na term borrowed from linguistics.\nExamples include:\n\n\n\n\n"}
{"id": "45031983", "url": "https://en.wikipedia.org/wiki?curid=45031983", "title": "Ricardo Baeza Rodríguez", "text": "Ricardo Baeza Rodríguez\n\nRicardo Baeza Rodríguez is a Chilean mathematician who works as a professor at the University of Talca. He earned his Ph.D. in 1970 from Saarland University, under the joint supervision of Robert W. Berger and Manfred Knebusch. His research interest is in number theory.\n\nBaeza became a member of the Chilean Academy of Sciences in 1983. He was the 2009 winner of the Chilean National Prize for Exact Sciences. In 2012, he became one of the inaugural fellows of the American Mathematical Society, the only Chilean to be so honored.\n"}
{"id": "26414007", "url": "https://en.wikipedia.org/wiki?curid=26414007", "title": "Rosemary A. Bailey", "text": "Rosemary A. Bailey\n\nRosemary A. Bailey (born 1947) is a British statistician who works in the design of experiments and the analysis of variance and in related areas of combinatorial design, especially in association schemes. She has written books on the design of experiments, on association schemes, and on linear models in statistics. She is Professor Emerita of Statistics in the School of Mathematical Sciences at Queen Mary, University of London, England. She is currently Professor of Mathematics and Statistics in the School of Mathematics and Statistics at the University of St Andrews, Scotland.\n\nShe is a Fellow of the Institute of Mathematical Statistics and in 2015 was elected a Fellow of the Royal Society of Edinburgh. \n\n\n"}
{"id": "4868226", "url": "https://en.wikipedia.org/wiki?curid=4868226", "title": "S. Rao Kosaraju", "text": "S. Rao Kosaraju\n\nSambasiva Rao Kosaraju is a professor of computer science at Johns Hopkins University, and division director for Computing & Communication Foundations at the National Science Foundation. He has done extensive work in the design and analysis of parallel and sequential algorithms.\n\nIn 1978, he wrote a paper describing a method to efficiently compute strongly connected members of a directed graph, a method later called Kosaraju's algorithm. Along with Paul Callahan he published many articles on efficient algorithms for computing the well-separated pair decomposition of a point set. His research efforts include efficient algorithms for pattern matching, data structure simulations, universal graphs, DNA sequence assembly, derandomization and investigations of immune system responses.\n\nIn 1995 he was inducted as a Fellow of the Association for Computing Machinery. He is also a fellow of the IEEE. A common saying at Johns Hopkins University, \"At some point, the learning stops and the pain begins.\" has been attributed to him. There used to be a shrine in the CS Undergraduate Lab in his honor.\n\nHe was born in India, and he did his bachelors in Engineering from Andhra University, and Masters from IIT Kharagpur, and is a PhD from University of Pennsylvania.\n\n"}
{"id": "2556705", "url": "https://en.wikipedia.org/wiki?curid=2556705", "title": "Schur's inequality", "text": "Schur's inequality\n\nIn mathematics, Schur's inequality, named after Issai Schur,\nestablishes that for all non-negative real numbers\n\"x\", \"y\", \"z\" and a positive number \"t\",\n\nwith equality if and only if \"x = y = z\" or two of them are equal and the other is zero. When \"t\" is an even positive integer, the inequality holds for all real numbers \"x\", \"y\" and \"z\".\n\nWhen formula_2, the following well-known special case can be derived:\n\nSince the inequality is symmetric in formula_4 we may assume without loss of generality that formula_5. Then the inequality\n\nclearly holds, since every term on the left-hand side of the equation is non-negative. This rearranges to Schur's inequality.\n\nA generalization of Schur's inequality is the following:\nSuppose \"a,b,c\" are positive real numbers. If the triples \"(a,b,c)\" and \"(x,y,z)\" are similarly sorted, then the following inequality holds:\n\nIn 2007, Romanian mathematician Valentin Vornicu showed that a yet further generalized form of Schur's inequality holds: \n\nConsider formula_8, where formula_9, and either formula_10 or formula_11. Let formula_12, and let formula_13 be either convex or monotonic. Then,\nThe standard form of Schur's is the case of this inequality where \"x\" = \"a\", \"y\" = \"b\", \"z\" = \"c\", \"k\" = 1, \"ƒ\"(\"m\") = \"m\".\n\nAnother possible extension states that if the non-negative real numbers formula_15 with and the positive real number \"t\" are such that \"x\" + \"v\" ≥ \"y\" + \"z\" then\n"}
{"id": "359175", "url": "https://en.wikipedia.org/wiki?curid=359175", "title": "Serre duality", "text": "Serre duality\n\nIn algebraic geometry, a branch of mathematics, Serre duality is a duality present on non-singular projective algebraic varieties \"V\" of dimension \"n\" (and in greater generality for vector bundles and further, for coherent sheaves). It shows that a cohomology group \"H\" is the dual space of another one, \"H\".\n\nIn the case for holomorphic vector bundle \"E\" over a smooth compact complex manifold \"V\", the statement is in the form:\n\nin which \"V\" is not necessarily projective.\n\nThe case of algebraic curves was already implicit in the Riemann-Roch theorem. For a curve \"C\" the coherent groups \"H\" vanish for \"i\" > 1; but \"H\" does enter implicitly. In fact, the basic relation of the theorem involves \"l\"(\"D\") and \"l\"(\"K\"−\"D\"), where \"D\" is a divisor and \"K\" is a divisor of the canonical class. After Serre we recognise \"l\"(\"K\"−\"D\") as the dimension of \"H\"(\"D\"), where now \"D\" means the line bundle determined by the divisor \"D\". That is, Serre duality in this case relates groups \"H\"(\"D\") and \"H\"(\"KD\"*), and we are reading off dimensions (notation: \"K\" is the canonical line bundle, \"D\"* is the dual line bundle, and juxtaposition is the tensor product of line bundles).\n\nIn this formulation the Riemann-Roch theorem can be viewed as a computation of the Euler characteristic of a sheaf\n\nin terms of the genus of the curve, which is\n\nand the degree of \"D\". It is this expression that can be generalised to higher dimensions.\n\nSerre duality of curves is therefore something very classical; but it has an interesting light to cast. For example, in Riemann surface theory, the deformation theory of complex structures is studied classically by means of quadratic differentials (namely sections of \"L\"(\"K\")). The deformation theory of Kunihiko Kodaira and D. C. Spencer identifies deformations via \"H\"(\"T\"), where \"T\" is the tangent bundle sheaf \"K\"*. The duality shows why these approaches coincide.\n\nThere is an algebrogeometric analogue of Serre duality for equidimensional projective schemes formula_2 over an algebraically closed field formula_3: if we suppose formula_4 is very ample (meaning it gives the embedding of formula_2 into formula_6) there this a sheaf formula_7 called the dualizing sheaf such that for every coherent sheaf formula_8 there are functorial maps\nwhich are isomorphisms. These dualizing sheaves can be computed as\nwhere formula_11 is the codimension of formula_2 in formula_6. If formula_2 is a local complete intersection, then the Koszul complex of formula_15 can be used to compute this Ext group (since it is a locally-free resolution of formula_15), giving\nwhere formula_18 is the ideal sheaf of formula_2.\n\nConsider a local complete intersection formula_20 with presentation given by the ideal formula_21 generated by forms of degrees formula_22. Then,\ngiving us that\nSince formula_25 we get\nas the dualizing sheaf. For example, the dualizing sheaf of a plane curve of degree formula_27 is given by formula_28. This is a special case of the proof of the adjunction formula, which uses the second fundamental (conormal) exact sequence. This sequence is exact on the left when formula_2 is (locally) a complete intersection.\n\nThe origin of the theory lies in Serre's earlier work on several complex variables. In the generalisation of Alexander Grothendieck, Serre duality becomes a part of coherent duality in a much broader setting. While the role of \"K\" above in general Serre duality is played by the determinant line bundle of the cotangent bundle, when \"V\" is a manifold, in full generality \"K\" cannot merely be a \"single\" sheaf in the absence of some hypothesis of non-singularity on \"V\". The formulation in full generality uses a derived category and Ext functors, to allow for the fact that \"K\" is now represented by a chain complex of sheaves, namely, the dualizing complex. Nevertheless, the statement of the theorem is recognisably Serre's.\n\n"}
{"id": "219962", "url": "https://en.wikipedia.org/wiki?curid=219962", "title": "Simpson's rule", "text": "Simpson's rule\n\nIn numerical analysis, Simpson's rule is a method for numerical integration, the numerical approximation of definite integrals. Specifically, it is the following approximation for formula_1 equally spaced subdivisions (where formula_1 is even): (General Form)\n\nwhere formula_4 and formula_5.\n\nFor unequally spaced points, see Cartwright.\n\nSimpson's rule also corresponds to the three-point Newton-Cotes quadrature rule.\n\nIn English, the method is credited to the mathematician Thomas Simpson (1710–1761) of Leicestershire, England. However, Johannes Kepler used similar formulas over 100 years prior, and for this reason the method is sometimes called Kepler's rule, or \"Keplersche Fassregel\" (Kepler's barrel rule) in German.\n\nOne derivation replaces the integrand formula_6 by the quadratic polynomial (i.e. parabola)formula_7 which takes the same values as formula_6 at the end points \"a\" and \"b\" and the midpoint \"m\" = (\"a\" + \"b\") / 2. One can use Lagrange polynomial interpolation to find an expression for this polynomial,\nUsing integration by substitution one can show that\nIntroducing the step size formula_11 this is also commonly written as\nBecause of the formula_13 factor Simspon's rule is also referred to as Simpson's 1/3 rule (see below for generalization).\n\nThe calculation above can be simplified if one observes that (by scaling) there is no loss of generality in assuming that formula_14。\n\nAnother derivation constructs Simpson's rule from two simpler approximations: the midpoint rule\nand the trapezoidal rule\nThe errors in these approximations are\nrespectively, where formula_18 denotes a term asymptotically proportional to formula_19. The two formula_18 terms are not equal; see Big O notation for more details. It follows from the above formulas for the errors of the midpoint and trapezoidal rule that the leading error term vanishes if we take the weighted average\nThis weighted average is exactly Simpson's rule.\n\nUsing another approximation (for example, the trapezoidal rule with twice as many points), it is possible to take a suitable weighted average and eliminate another error term. This is Romberg's method.\n\nThe third derivation starts from the \"ansatz\"\n\nThe coefficients α, β and γ can be fixed by requiring that this approximation be exact for all quadratic polynomials. This yields Simpson's rule.\n\nThe error in approximating an integral by Simpson's rule is\n\nwhere formula_24 (the Greek letter xi) is some number between formula_25 and formula_26.\n\nThe error is asymptotically proportional to formula_27. However, the above derivations suggest an error proportional to formula_28. Simpson's rule gains an extra order because the points at which the integrand is evaluated are distributed symmetrically in the interval formula_29.\n\nSince the error term is proportional to the fourth derivative of formula_30 at formula_24, this shows that Simpson's rule provides exact results for any polynomial formula_30 of degree three or less, since the fourth derivative of such a polynomial is zero at all points.\n\nIf the interval of integration formula_33 is in some sense \"small\", then Simpson's rule will provide an adequate approximation to the exact integral. By small, what we really mean is that the function being integrated is relatively smooth over the interval formula_33. For such a function, a smooth quadratic interpolant like the one used in Simpson's rule will give good results.\n\nHowever, it is often the case that the function we are trying to integrate is not smooth over the interval. Typically, this means that either the function is highly oscillatory, or it lacks derivatives at certain points. In these cases, Simpson's rule may give very poor results. One common way of handling this problem is by breaking up the interval formula_33 into a number of small subintervals. Simpson's rule is then applied to each subinterval, with the results being summed to produce an approximation for the integral over the entire interval. This sort of approach is termed the \"composite Simpson's rule\".\n\nSuppose that the interval formula_33 is split up into formula_1 subintervals, with formula_1 an even number. Then, the composite Simpson's rule is given by\n\nwhere formula_40 for formula_41 with formula_42; in particular, formula_43 and formula_44. This composite rule with formula_45 corresponds with the regular Simpson's Rule of the preceding section.\n\nThe error committed by the composite Simpson's rule is\n\nwhere formula_24 is some number between formula_25 and formula_26 and formula_42 is the \"step length\". The error is bounded (in absolute value) by\n\nThis formulation splits the interval formula_52 in subintervals of equal length. In practice, it is often advantageous to use subintervals of different lengths, and concentrate the efforts on the places where the integrand is less well-behaved. This leads to the adaptive Simpson's method.\n\nSimpson's 3/8 rule is another method for numerical integration proposed by Thomas Simpson. It is based upon a cubic interpolation rather than a quadratic interpolation. Simpson's 3/8 rule is as follows:\nwhere \"b\" − \"a\" = 3\"h\". \nThe error of this method is:\nwhere formula_24 is some number between formula_25 and formula_26. Thus, the 3/8 rule is about twice as accurate as the standard method, but it uses one more function value. A composite 3/8 rule also exists, similarly as above.\n\nA further generalization of this concept for interpolation with arbitrary-degree polynomials are the Newton–Cotes formulas.\n\nDividing the interval formula_52 into formula_1 subintervals of length formula_42 and introducing the nodes formula_61 we have\n\nWhile the remainder for the rule is shown as:\n\nNote, we can only use this if formula_1 is a multiple of three.\n\nA simplified version of Simpson's rules is used in naval architecture. The 3/8th rule is also called Simpson's second rule.\n\nThis is another formulation of a composite Simpson's rule: instead of applying Simpson's rule to disjoint segments of the integral to be approximated, Simpson's rule is applied to overlapping segments, yielding:\n\nThe formula above is obtained by combining the original composite Simpson's rule with the one consisting of using Simpson's 3/8 rule in the extreme subintervals and the standard 3-point rule in the remaining subintervals. The result is then obtained by taking the mean of the two formulas. \n\nIn the task of estimation of full area of narrow peak-like functions, Simpson's rules are much less efficient than trapezoidal rule. Namely, composite Simpson's 1/3 rule requires 1.8 times more points to achieve the same accuracy as trapezoidal rule. Composite Simpson's 3/8 rule is even less accurate. Integral by Simpson's 1/3 rule can be represented as a sum of 2/3 of integral by trapezoidal rule with step h and 1/3 of integral by rectangle rule with step 2h. No wonder that error of the sum corresponds lo less accurate term. Averaging of Simpson's 1/3 rule composite sums with properly shifted frames produces following rules:\n\nformula_66\n\nwhere two points outside of integrated region are exploited and\n\nformula_67\n\nThose rules are very much similar to Press's alternative extended Simpson's rule. Coefficients within the major part of the region being integrated equal one, differences are only at the edges. These three rules can be associated with Euler-MacLaurin formula with the first derivative term and named Euler-MacLaurin integration rules. They differ only in the way, how the first derivative at the region end is calculated. \n\n\n\n"}
{"id": "2524727", "url": "https://en.wikipedia.org/wiki?curid=2524727", "title": "Stationary distribution", "text": "Stationary distribution\n\nStationary distribution may refer to:\n\n\nIn some fields of application, the term stable distribution is used for the equivalent of a stationary (marginal) distribution, although in probability and statistics the term has a rather different meaning: see stable distribution.\n\nCrudely stated, all of the above are specific cases of a common general concept. A stationary distribution is a specific entity which is unchanged by the effect of some matrix or operator: it need not be unique. Thus stationary distributions are related to eigenvectors for which the eigenvalue is unity.\n\n"}
{"id": "320578", "url": "https://en.wikipedia.org/wiki?curid=320578", "title": "Tarski's theorem about choice", "text": "Tarski's theorem about choice\n\nIn mathematics, Tarski's theorem, proved by , states that in ZF the theorem \"For every infinite set formula_1, there is a bijective map between the sets formula_1 and formula_3\" implies the axiom of choice. The opposite direction was already known, thus the theorem and axiom of choice are equivalent.\n\nTarski told that when he tried to publish the theorem in Comptes Rendus de l'Académie des Sciences Paris, Fréchet and Lebesgue refused to present it. Fréchet wrote that an implication between two well known propositions is not a new result. Lebesgue wrote that an implication between two false propositions is of no interest.\n\nOur goal is to prove that the axiom of choice is implied by the statement \"For every infinite set formula_1: formula_5\".\nIt is known that the well-ordering theorem is equivalent to the axiom of choice, thus it is enough to show that the statement implies that for every set formula_6 there exist a well-order.\n\nFor finite sets it is trivial, thus we will assume that formula_6 is infinite.\n\nSince the collection of all ordinals such that there exist a surjective function from formula_6 to the ordinal is a set, there exist a minimal non-zero ordinal, formula_9, such that there is no surjective function from formula_6 to formula_9.\nWe assume without loss of generality that the sets formula_6 and formula_9 are disjoint.\nBy our initial assumption, formula_14, thus there exists a bijection formula_15.\n\nFor every formula_16, it is impossible that formula_17, because otherwise we could define a surjective function from formula_6 to formula_9.\nTherefore, there exists at least one ordinal formula_20, such that formula_21, thus the set formula_22 is not empty.\n\nWith this fact in our mind we can define a new function: formula_23.\nThis function is well defined since formula_24 is a non-empty set of ordinals, hence it has a minimum.\nRecall that for every formula_25 the sets formula_24 and formula_27 are disjoint.\nTherefore, we can define a well order on formula_6, for every formula_29 we shall define formula_30, since the image of formula_31, i.e. formula_32, is a set of ordinals and therefore well ordered.\n\n"}
{"id": "376699", "url": "https://en.wikipedia.org/wiki?curid=376699", "title": "Ten15", "text": "Ten15\n\nTen15 is an algebraically specified abstract machine. It was developed by Foster, Currie et al. at the Royal Signals and Radar Establishment at Malvern, Worcestershire, during the 1980s. It arose from earlier work on the Flex machine, which was a capability computer implemented via microcode. Ten15 was intended to offer an intermediate language common to all implementations of the Flex architecture for portability purposes. It had the side effect of making the benefits of that work available on modern processors lacking a microcode facility.\n\nTen15 served as an intermediate language for compilers, but with several unique features, some of which have still to see the light of day in everyday systems. Firstly, it was strongly typed, yet wide enough in application to support most languages — C being an exception, chiefly because C deliberately treats an array similar to a pointer to the first element of that array. This ultimately led to Ten15's development into TDF, which in turn formed the basis for ANDF. Secondly, it offered a persistent, write-only filestore mechanism, allowing arbitrary data structures to be written and retrieved without conversion into an external representation.\n\nWhy 'Ten15'? Nic Peeling reports that during early discussions of the concepts of Ten15, it was agreed that this was important and should have a name - but what? Ian Currie looked up at the clock and said 'Why not call it 10:15?'\n\n\n"}
{"id": "49554717", "url": "https://en.wikipedia.org/wiki?curid=49554717", "title": "Trigonometric functions of matrices", "text": "Trigonometric functions of matrices\n\nThe trigonometric functions (especially sine and cosine) for real or complex square matrices occur in solutions of second-order systems of differential equations. They are defined by the same Taylor series that hold for the trigonometric functions of real and complex numbers:\n\nwith being the th power of the matrix , and being the identity matrix of appropriate dimensions. \n\nEquivalently, they can be defined using the matrix exponential along with the matrix equivalent of Euler's formula, , yielding \n\nFor example, taking to be a standard Pauli matrix,\none has\nas well as, for the cardinal sine function,\n\nThe analog of the Pythagorean trigonometric identity holds:\n\nIf is a diagonal matrix, and are also diagonal matrices with and , that is, they can be calculated by simply taking the sines or cosines of the matrice's diagonal components.\n\nThe analogs of the trigonometric addition formulas are true if and only if :\n\nThe tangent, as well as inverse trigonometric functions, hyperbolic and inverse hyperbolic functions have also been defined for matrices:\nand so on.\n"}
{"id": "54617362", "url": "https://en.wikipedia.org/wiki?curid=54617362", "title": "Walk-regular graph", "text": "Walk-regular graph\n\nIn discrete mathematics, a walk-regular graph is a simple graph where the number of closed walks of any length from a vertex to itself does not depend on the choice of vertex.\n\nSuppose that formula_1 is a simple graph. Let formula_2 denote the adjacency matrix of formula_1, formula_4 denote the set of vertices of formula_1, and formula_6 denote the characteristic polynomial of the vertex-deleted subgraph formula_7 for all formula_8Then the following are equivalent:\n\n\n\n"}
{"id": "38298427", "url": "https://en.wikipedia.org/wiki?curid=38298427", "title": "Wave maps equation", "text": "Wave maps equation\n\nIn mathematical physics, the wave maps equation is a geometric wave equation that solves\n\nwhere formula_2 is a connection.\n\nIt can be considered a natural extension of the wave equation for Riemannian manifolds.\n"}
{"id": "12096417", "url": "https://en.wikipedia.org/wiki?curid=12096417", "title": "Wirtinger inequality (2-forms)", "text": "Wirtinger inequality (2-forms)\n\nIn mathematics, the Wirtinger inequality for 2-forms, named after Wilhelm Wirtinger, states that on a Kähler manifold formula_1, the exterior formula_2 power of the symplectic form (Kähler form) ω, when evaluated on a simple (decomposable) formula_3-vector ζ of unit volume, is bounded above by formula_4. That is,\n\nIn other words, formula_6 is a calibration on formula_7. An important corollary is that every complex submanifold of a Kähler manifold is volume minimizing in its homology class.\n\n\n"}
