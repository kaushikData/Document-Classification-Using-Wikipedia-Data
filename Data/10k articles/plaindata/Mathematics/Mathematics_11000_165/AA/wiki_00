{"id": "6942723", "url": "https://en.wikipedia.org/wiki?curid=6942723", "title": "Access structure", "text": "Access structure\n\nAccess structures are used in the study of security system where multiple parties need to work together to obtain a resource. Groups of parties that are granted access are called qualified. In set theoretic terms they are referred to as qualified sets. In turn, the set of all such qualified sets is called the access structure of the system. Less formally it is a description of who needs to cooperate with whom in order to access the resource. In its original use in cryptography, the resource was a secret shared among the participants. Only subgroups of participants contained in the access structure, are able to join their shares to recompute the secret. More generally, the resource can also be a task that a group of people can complete together, such as creating a digital signature, or decrypting an encrypted message.\n\nIt is reasonable to assume that access structures are monotone in the sense that, if a subset S is in the access structure, all sets that contain S as a subset should also form part of the access structure.\n\n"}
{"id": "57504451", "url": "https://en.wikipedia.org/wiki?curid=57504451", "title": "Algorithms and Combinatorics", "text": "Algorithms and Combinatorics\n\nAlgorithms and Combinatorics () is a book series in mathematics, and particularly in combinatorics and the design and analysis of algorithms. It is published by Springer Science+Business Media, and was founded in 1987.\n\n, the books published in this series include:\n"}
{"id": "51988929", "url": "https://en.wikipedia.org/wiki?curid=51988929", "title": "Arithmetices principia, nova methodo exposita", "text": "Arithmetices principia, nova methodo exposita\n\nThe 1889 treatise Arithmetices principia, nova methodo exposita (\"The principles of arithmetic, presented by a new method\"; 1889) by Giuseppe Peano is a seminal document in mathematical logic and set theory, introducing what is now the standard axiomatization of the natural numbers, and known as the Peano axioms, as well as some pervasive notations, such as the symbols for the basic set operations ∈, ⊂, ∩, ∪, and \"A\"−\"B\".\n\nThe treatise is written in Latin, which was already somewhat unusual at the time of publication, Latin having fallen out of favour as the lingua franca of scholarly communications by the end of the 19th century. The use of Latin in spite of this reflected Peano's belief in the universal importance of the work – which is now generally regarded as his most important contribution to arithmetic – and in that of universal communication. Peano would publish later works both in Latin and in his own artificial language, Latino sine flexione, which is a grammatically simplified version of Latin.\n\nPeano also continued to publish mathematical notations in a series from 1895 to 1908 collectively known as Formulario mathematico.\n\n"}
{"id": "1021", "url": "https://en.wikipedia.org/wiki?curid=1021", "title": "Aspect ratio", "text": "Aspect ratio\n\nThe aspect ratio of a geometric shape is the ratio of its sizes in different dimensions. For example, the aspect ratio of a rectangle is the ratio of its longer side to its shorter sidethe ratio of width to height, when the rectangle is oriented as a \"landscape\".\n\nThe aspect ratio is most often expressed as two integer numbers separated by a colon (x:y), less commonly as a simple or decimal fraction. The values x and y do not represent actual widths and heights but, rather, the proportion between width and height. As an example, 8:5, 16:10, 1.6:1, and 1.6 are all ways of representing the same aspect ratio.\n\nIn objects of more than two dimensions, such as hyperrectangles, the aspect ratio can still be defined as the ratio of the longest side to the shortest side.\n\nThe term is most commonly used with reference to:\n\nFor a rectangle, the aspect ratio denotes the ratio of the width to the height of the rectangle. A square has the smallest possible aspect ratio of 1:1.\n\nExamples:\n\nFor an ellipse, the aspect ratio denotes the ratio of the major axis to the minor axis. An ellipse with an aspect ratio of 1:1 is a circle.\n\nIn geometry, there are several alternative definitions to aspect ratios of general compact sets in a d-dimensional space:\n\n\nIf the dimension d is fixed, then all reasonable definitions of aspect ratio are equivalent to within constant factors.\n\nAspect ratios are mathematically expressed as \"x\":\"y\" (pronounced \"x-to-y\").\n\nCinematographic aspect ratios are usually denoted as a (rounded) decimal multiple of width vs unit height, while photographic and videographic aspect ratios are usually defined and denoted by whole number ratios of width to height. In digital images there is a subtle distinction between the \"display\" aspect ratio (the image as displayed) and the \"storage\" aspect ratio (the ratio of pixel dimensions); see Distinctions.\n\n"}
{"id": "2909690", "url": "https://en.wikipedia.org/wiki?curid=2909690", "title": "Bak–Sneppen model", "text": "Bak–Sneppen model\n\nThe Bak–Sneppen model is a simple model of co-evolution between interacting species. It was developed to show how self-organized criticality may explain key features of the fossil record, such as the distribution of sizes of extinction events and the phenomenon of punctuated equilibrium. It is named after Per Bak and Kim Sneppen.\n\nThe model dynamics repeatedly eliminates the least adapted species and mutates it and its neighbors to recreate the interaction between species. A comprehensive study of the details of this model can be found in \"Phys. Rev. E\" 53, 414–443 (1996). A solvable version of the model has been proposed in \"Phys. Rev. Lett.\" 76, 348–351 (1996), which shows that the dynamics evolves sub-diffusively, driven by a long-range memory.\n\nAn evolutionary local search heuristic based on the Bak–Sneppen model, called extremal optimization, has been introduced in \"Artificial Intelligence\" 119, 275–286 (2000). The Bak–Sneppen model has been applied to the theory of scientific progress.\n\nWe consider \"N\" species, which are associated with a fitness factor \"f\"(\"i\"). They are indexed by integers \"i\" around a ring. The algorithm consists in choosing the least fit species, and then replacing it and its two closest neighbors (previous and next integer) by new species, with a new random fitness. After a long run there will be a minimum required fitness, below which species don't survive. These \"long-run\" events are referred to as avalanches, and the model proceeds through these avalanches until it reaches a state of relative stability where all species' fitness are above a certain threshold.\n\n\n"}
{"id": "311509", "url": "https://en.wikipedia.org/wiki?curid=311509", "title": "Bounded function", "text": "Bounded function\n\nIn mathematics, a function \"f\" defined on some set \"X\" with real or complex values is called bounded, if the set of its values is bounded. In other words, there exists a real number \"M\" such that \nfor all \"x\" in \"X\". A function that is \"not\" bounded is said to be unbounded.\n\nSometimes, if \"f\"(\"x\") ≤ \"A\" for all \"x\" in \"X\", then the function is said to be bounded above by \"A\". On the other hand, if \"f\"(\"x\") ≥ \"B\" for all \"x\" in \"X\", then the function is said to be bounded below by \"B\".\n\nThe concept should not be confused with that of a bounded operator.\n\nAn important special case is a bounded sequence, where \"X\" is taken to be the set N of natural numbers. Thus a sequence \"f\" = (\"a\", \n\"a\", \"a\", ...) is bounded if there exists a real number \"M\" such that\nfor every natural number \"n\". The set of all bounded sequences, equipped with a vector space structure, forms a sequence space.\n\nThis definition can be extended to functions taking values in a metric space \"Y\". Such a function \"f\" defined on some set \"X\" is called bounded if for some \"a\" in \"Y\" there exists a real number \"M\" such that its distance function \"d\" (\"distance\") is less than \"M\", i.e.\n\nfor all \"x\" in \"X\".\n\nIf this is the case, there is also such an \"M\" for each other \"a\", by the triangle Here we saw the examples below\n\n\n"}
{"id": "38731321", "url": "https://en.wikipedia.org/wiki?curid=38731321", "title": "Cantic octagonal tiling", "text": "Cantic octagonal tiling\n\nIn geometry, the tritetratrigonal tiling or shieldotritetragonal tiling is a uniform tiling of the hyperbolic plane. It has Schläfli symbol of t(4,3,3). It can also be named as a cantic octagonal tiling, h{8,3}.\n\n\n\n"}
{"id": "3269639", "url": "https://en.wikipedia.org/wiki?curid=3269639", "title": "Carpenter's rule problem", "text": "Carpenter's rule problem\n\nThe carpenter's rule problem is a discrete geometry problem, which can be stated in the following manner: \"Can a simple planar polygon be moved continuously to a position where all its vertices are in convex position, so that the edge lengths and simplicity are preserved along the way?\" A closely related problem is to show that any non-self-crossing polygonal chain can be straightened, again by a continuous transformation that preserves edge distances and avoids crossings.\n\nBoth problems were successfully solved by .\n\nSubsequently to their work, Ileana Streinu provided a simplified combinatorial proof formulated in the terminology of robot arm motion planning. Both the original proof and Streinu's proof work by finding non-expansive motions of the input, continuous transformations such that no two points ever move towards each other. Streinu's version of the proof adds edges to the input to form a pointed pseudotriangulation, removes one added convex hull edge from this graph, and shows that the remaining graph has a one-parameter family of motions in which all distances are nondecreasing. By repeatedly applying such motions, one eventually reaches a state in which no further expansive motions are possible, which can only happen when the input has been straightened or convexified.\n\n generalized the Carpenter's rule problem to rectifiable curves. He showed that every rectifiable Jordan curve can be made convex, without increasing its length and without decreasing the distance between any pair of points. This research, performed while he was still a high school student, won the second-place prize for Pardon in the 2007 Intel Science Talent Search .\n\n\n\n"}
{"id": "51386092", "url": "https://en.wikipedia.org/wiki?curid=51386092", "title": "Certifying algorithm", "text": "Certifying algorithm\n\nIn theoretical computer science, a certifying algorithm is an algorithm that outputs, together with a solution to the problem it solves, a proof that the solution is correct. A certifying algorithm is said to be \"efficient\" if the combined runtime of the algorithm and a proof checker is slower by at most a constant factor than the best known non-certifying algorithm for the same problem.\n\nThe proof produced by a certifying algorithm should be in some sense simpler than the algorithm itself, for otherwise any algorithm could be considered certifying (with its output verified by running the same algorithm again). Sometimes this is formalized by requiring that a verification of the proof take less time than the original algorithm, while for other problems (in particular those for which the solution can be found in linear time) simplicity of the output proof is considered in a less formal sense. For instance, the validity of the output proof may be more apparent to human users than the correctness of the algorithm, or a checker for the proof may be more amenable to formal verification.\n\nImplementations of certifying algorithms that also include a checker for the proof generated by the algorithm may be considered to be more reliable than non-certifying algorithms. For, whenever the algorithm is run, one of three things happens: it produces a correct output (the desired case), it detects a bug in the algorithm or its implication (undesired, but generally preferable to continuing without detecting the bug), or both the algorithm and the checker are faulty in a way that masks the bug and prevents it from being detected (undesired, but unlikely as it depends on the existence of two independent bugs).\n\nMany examples of problems with checkable algorithms come from graph theory.\nFor instance, a classical algorithm for testing whether a graph is bipartite would simply output a Boolean value: true if the graph is bipartite, false otherwise. In contrast, a certifying algorithm might output a 2-coloring of the graph in the case that it is bipartite, or a cycle of odd length if it is not. Any graph is bipartite if and only if it can be 2-colored, and non-bipartite if and only if it contains an odd cycle. Both checking whether a 2-coloring is valid and checking whether a given odd-length sequence of vertices is a cycle may be performed more simply than testing bipartiteness.\n\nAnalogously, it is possible to test whether a given directed graph is acyclic by a certifying algorithm that outputs either a topological order or a directed cycle. It is possible to test whether an undirected graph is a chordal graph by a certifying algorithm that outputs either an elimination ordering (an ordering of all vertices such that, for every vertex, the neighbors that are later in the ordering form a clique) or a chordless cycle. And it is possible to test whether a graph is planar by a certifying algorithm that outputs either a planar embedding or a Kuratowski subgraph.\n\nThe extended Euclidean algorithm for the greatest common divisor of two integers and is certifying: it outputs three integers (the divisor), , and , such that . This equation can only be true of multiples of the greatest common divisor, so testing that is the greatest common divisor may be performed by checking that divides both and and that this equation is correct.\n\n"}
{"id": "1683941", "url": "https://en.wikipedia.org/wiki?curid=1683941", "title": "Cesare Arzelà", "text": "Cesare Arzelà\n\nCesare Arzelà (6 March 1847 – 15 March 1912) was an Italian mathematician who taught at the University of Bologna and is recognized for his contributions in the theory of functions, particularly for his characterization of sequences of continuous functions, generalizing the one given earlier by Giulio Ascoli in the Arzelà-Ascoli theorem.\n\nHe was a pupil of the Scuola Normale Superiore of Pisa where he graduated in 1869. Arzelà came from a poor household; therefore he could not start his study until 1871, when he studied in Pisa under Enrico Betti and Ulisse Dini.\n\nHe was working in Florence (from 1875) and in 1878 obtained the Chair of Algebra at the University of Palermo.\n\nAfter that he became a professor in 1880 at the University of Bologna at the department of analysis. He conducted research in the field of theory of functions. His most famous student was Leonida Tonelli.\n\nIn 1889 he generalized the Ascoli theorem to Arzelà–Ascoli theorem, an important theorem in theory of functions.\n\nHe was a member of the Accademia Nazionale dei Lincei, and of several other academies.\n\n\n"}
{"id": "26193465", "url": "https://en.wikipedia.org/wiki?curid=26193465", "title": "Comparison of vector algebra and geometric algebra", "text": "Comparison of vector algebra and geometric algebra\n\nVector algebra and geometric algebra are complementary approaches to providing additional algebraic structures on vector spaces, with geometric interpretations, particularly vector fields in multivariable calculus and applications in mathematical physics.\n\nVector algebra is specific to Euclidean 3-space, while geometric algebra uses multilinear algebra and applies in all dimensions and signatures, notably 3+1 spacetime as well as 2 dimensions. They are mathematically equivalent in 3 dimensions, where this article is focused, though the approaches differ. Vector algebra is more widely used in elementary multivariable calculus, while geometric algebra is used in more advanced treatments, and is proposed for elementary use as well.\n\nGeometric algebra (GA) is an extension or completion of vector algebra (VA). The reader is herein assumed to be familiar with the basic concepts and operations of VA and this article will mainly concern itself with operations in formula_1 the GA of 3D space (nor is this article intended to be mathematically rigorous). In GA, vectors are not normally written boldface as the meaning is usually clear from the context.\n\nThe fundamental difference is that GA provides an additional product of vectors called the \"geometric product\". Elements of GA are graded multivectors, scalars are grade 0, the usual vectors are grade 1, bivectors are grade 2 and the highest grade (3 in the 3D case) is traditionally called the pseudoscalar and designated formula_2.\n\nThe ungeneralized 3D vector form of the geometric product is :\n\nthat is the sum of the usual dot (inner) product and the outer (exterior) product (this last is closely related to the cross product and will be explained below).\n\nIn VA, entities such as pseudovectors and pseudoscalars need to be bolted on, whereas in GA the equivalent bivector and pseudovector respectively exist naturally as subspaces of the algebra.\n\nFor example, applying vector calculus in 2 dimensions, such as to compute torque or curl, requires adding an artificial 3rd dimension and extending the vector field to be constant in that dimension, or alternately considering these to be scalars. The torque or curl is then a normal vector field in this 3rd dimension. By contrast, geometric algebra in 2 dimensions defines these as a pseudoscalar field (a bivector), without requiring a 3rd dimension. Similarly, the scalar triple product is ad hoc, and can instead be expressed uniformly using the exterior product and the geometric product.\n\nHere are some comparisons between standard formula_4 vector relations and their corresponding exterior product and geometric product equivalents. All the exterior and geometric product equivalents here are good for more than three dimensions, and some also for two. In two dimensions the cross product is undefined even if what it describes (like torque) is perfectly well defined in a plane without introducing an arbitrary normal vector outside of the space.\n\nMany of these relationships only require the introduction of the exterior product to generalize, but since that may not be familiar to somebody with only a background in vector algebra and calculus, some examples are given.\n\nformula_5 is perpendicular to the plane containing formula_6 and formula_7.\nformula_8 is an oriented representation of the same plane.\n\nWe have the pseudoscalar formula_9 (right handed orthonormal frame) and so\n\nThis yields a convenient definition for the cross product of traditional vector algebra:\n\n(this is antisymmetric). Relevant is the distinction between axial and polar vectors in vector algebra, which is natural in geometric algebra as the distinction between vectors and bivectors (elements of grade two).\n\nThe formula_14 here is a unit pseudoscalar of Euclidean 3-space, which establishes a duality between the vectors and the bivectors, and is named so because of the expected property\n\nThe equivalence of the formula_16 cross product and the exterior product expression above can be confirmed by direct multiplication of formula_17 with a determinant expansion of the exterior product\n\nSee also Cross product as an exterior product. Essentially, the geometric product of a bivector and the pseudoscalar of Euclidean 3-space provides a method of calculation of the Hodge dual.\n\nOrdinarily:\n\nMaking use of the geometric product and the fact that the exterior product of a vector wedge with itself is zero:\n\nIn three dimensions the product of two vector lengths can be expressed in terms of the dot and cross products\n\nThe corresponding generalization expressed using the geometric product is\n\nThis follows from expanding the geometric product of a pair of vectors with its reverse\n\nLinear algebra texts will often use the determinant for the solution of linear systems by Cramer's rule or for and matrix inversion.\n\nAn alternative treatment is to axiomatically introduce the wedge product, and then demonstrate that this can be used directly to solve linear systems. This is shown below, and does not require sophisticated math skills to understand.\n\nIt is then possible to define determinants as nothing more than the coefficients of the wedge product in terms of \"unit \"k\"-vectors\" (formula_26 terms) expansions as above.\n\nWhen linear system solution is introduced via the wedge product, Cramer's rule follows as a side-effect, and there is no need to lead up to the end results with definitions of minors, matrices, matrix invertibility, adjoints, cofactors, Laplace expansions, theorems on determinant multiplication and row column exchanges, and so forth.\n\nMatrix inversion (Cramer's rule) and determinants can be naturally expressed in terms of the wedge product.\n\nThe use of the wedge product in the solution of linear equations can be quite useful for various geometric product calculations.\n\nTraditionally, instead of using the wedge product, Cramer's rule is usually presented as a generic algorithm that can be used to solve linear equations of the form formula_33 (or equivalently to invert a matrix). Namely\n\nThis is a useful theoretic result. For numerical problems row reduction with pivots and other methods are more stable and efficient.\n\nWhen the wedge product is coupled with the Clifford product and put into a natural geometric context, the fact that the determinants are used in the expression of formula_35 parallelogram area and parallelepiped volumes (and higher-dimensional generalizations thereof) also comes as a nice side-effect.\n\nAs is also shown below, results such as Cramer's rule also follow directly from the wedge product's selection of non-identical elements. The end result is then simple enough that it could be derived easily if required instead of having to remember or look up a rule.\n\nTwo variables example\n\nPre- and post-multiplying by formula_37 and formula_38,\n\nProvided formula_41 the solution is\n\nFor formula_43, this is Cramer's rule since the formula_44 factors of the wedge products\n\ndivide out.\n\nSimilarly, for three, or \"N\" variables, the same ideas hold\n\nAgain, for the three variable three equation case this is Cramer's rule since the formula_48 factors of all the wedge products divide out, leaving the familiar determinants.\n\nA numeric example with three equations and two unknowns:\nIn case there are more equations than variables and the equations have a solution, then each of the k-vector quotients will be scalars.\n\nTo illustrate here is the solution of a simple example with three equations and two unknowns.\n\nThe right wedge product with formula_50 solves for formula_51\n\nand a left wedge product with formula_53 solves for formula_54\n\nObserve that both of these equations have the same factor, so\none can compute this only once (if this was zero it would\nindicate the system of equations has no solution).\n\nCollection of results for\nformula_51 and formula_54 yields a Cramer's rule-like form:\n\nWriting formula_59, we have the end result:\n\nFor the plane of all points formula_61 through the plane passing through three independent points formula_62, formula_63, and formula_64, the normal form of the equation is\n\nThe equivalent wedge product equation is\n\nUsing the Gram–Schmidt process a single vector can be decomposed into two components with respect to a reference vector, namely the projection onto a unit vector in a reference direction, and the difference between the vector and that projection.\n\nWith, formula_67, the projection of formula_68 onto formula_69 is\n\nOrthogonal to that vector is the difference, designated the rejection,\n\nNote the similarity in form to the \"w\", \"u\", \"v\" trivector itself\n\nwhich, if you take the set of formula_73 as a basis for the trivector space, suggests this is the natural way to define the length of a trivector. Loosely speaking the length of a vector is a length, length of a bivector is area, and the length of a trivector is volume.\n\nIf a vector is factored directly into projective and rejective terms using the geometric product formula_74, then it is not necessarily obvious that the rejection term, a product of vector and bivector is even a vector. Expansion of the vector bivector product in terms of the standard basis vectors has the following form\n\nThe trivector term is formula_76. Expansion of formula_77 yields the same trivector term (it is the completely symmetric part), and the vector term is negated. Like the geometric product of two vectors, this geometric product can be grouped into symmetric and antisymmetric parts, one of which is a pure k-vector. In analogy the antisymmetric part of this product can be called a generalized dot product, and is roughly speaking the dot product of a \"plane\" (bivector), and a vector.\n\nThe properties of this generalized dot product remain to be explored, but first here is a summary of the notation\n\nLet formula_82, where formula_83, and formula_84. Expressing formula_85 and the formula_86, products in terms of these components is\n\nWith the conditions and definitions above, and some manipulation, it can be shown that the term formula_88, which then justifies the previous solution of the normal to a plane problem. Since the vector term of the vector bivector product the name dot product is zero\nwhen the vector is perpendicular to the plane (bivector), and this vector, bivector \"dot product\" selects only the components that are in the plane, so in analogy to the vector-vector dot product this name itself is justified by more than the fact this is the non-wedge product term of the geometric vector-bivector product.\n\nIt can be shown that a unit vector derivative can be expressed using the cross product\n\n\\frac{d}{dt}\\left(\\frac{\\mathbf r}{\\Vert \\mathbf r \\Vert}\\right)\n"}
{"id": "6211", "url": "https://en.wikipedia.org/wiki?curid=6211", "title": "Context-sensitive grammar", "text": "Context-sensitive grammar\n\nA context-sensitive grammar (CSG) is a formal grammar in which the left-hand sides and right-hand sides of any production rules may be surrounded by a context of terminal and nonterminal symbols. Context-sensitive grammars are more general than context-free grammars, in the sense that there are languages that can be described by CSG but not by context-free grammars. Context-sensitive grammars are less general (in the same sense) than unrestricted grammars. Thus, CSG are positioned between context-free and unrestricted grammars in the Chomsky hierarchy.\n\nA formal language that can be described by a context-sensitive grammar, or, equivalently, by a noncontracting grammar or a linear bounded automaton, is called a context-sensitive language. Some textbooks actually define CSGs as non-contracting, although this is not how Noam Chomsky defined them in 1959. This choice of definition makes no difference in terms of the languages generated (i.e. the two definitions are weakly equivalent), but it does make a difference in terms of what grammars are structurally considered context-sensitive; the latter issue was analyzed by Chomsky in 1963.\n\nChomsky introduced context-sensitive grammars as a way to describe the syntax of natural language where it is often the case that a word may or may not be appropriate in a certain place depending on the context. Walter Savitch has criticized the terminology \"context-sensitive\" as misleading and proposed \"non-erasing\" as better explaining the distinction between a CSG and an unrestricted grammar.\n\nAlthough it is well-known that certain features of languages (e.g. cross-serial dependency) are not context-free, it is an open question how much of CSG's expressive power is needed to capture the context sensitivity found in natural languages. Subsequent research in this area has focused on the more computationally tractable mildly context-sensitive languages. The syntaxes of some visual programming languages can be described by context-sensitive graph grammars.\n\nA formal grammar \"G\" = (\"N\", Σ, \"P\", \"S\"), where \"N\" is a set of nonterminal symbols, Σ is a set of terminal symbols, \"P\" is a set of production rules, and \"S\" is the start symbol, is context-sensitive if all rules in \"P\" are of the form\nwhere \"A\" ∈ \"N\", α,β ∈ (\"N\"∪Σ) and γ ∈ (\"N\"∪Σ).\n\nA string \"u\" ∈ (\"N\"∪Σ) directly yields, or directly derives to, a string \"v\" ∈ (\"N\"∪Σ), denoted as \"u\" ⇒ \"v\", if \"u\" can be written as \"l\"α\"A\"β\"r\", and \"v\" can be written as \"l\"αγβ\"r\", for some production rule (α\"A\"β→αγβ) ∈ \"P\", and some context strings \"l\", \"r\" ∈ (\"N\"∪Σ).\nMore generally, \"u\" is said to yield, or derive to, \"v\", denoted as \"u\" ⇒ \"v\", if \"u\" = \"u\" ⇒ ... ⇒ \"u\" = \"v\" for some \"n\"≥0 and some strings \"u\", ..., \"u\" (\"N\"∪Σ). That is, the relation (⇒) is the reflexive transitive closure of the relation (⇒).\n\nThe language of the grammar \"G\" is the set of all terminal symbol strings derivable from its start symbol, formally: \"L\"(\"G\") = { \"w\" ∈ Σ: \"S\" ⇒ \"w\" }.\nDerivations that do not end in a string composed of terminal symbols only are possible, but don't contribute to \"L\"(\"G\").\n\nThe only difference between this definition of Chomsky and that of unrestricted grammars is that γ can be empty in the unrestricted case.\n\nSome definitions of a context-sensitive grammar only require that for any production rule of the form u → v, the length of u shall be less than or equal to the length of v. This seemingly weaker requirement is in fact weakly equivalent, see Noncontracting grammar#Transforming into context-sensitive grammar.\n\nIn addition, a rule of the form\nwhere λ represents the empty string and \"S\" does not appear on the right-hand side of any rule is permitted. The addition of the empty string allows the statement that the context sensitive languages are a proper superset of the context-free languages, rather than having to make the weaker statement that all context-free grammars with no →λ productions are also context sensitive grammars.\n\nThe name \"context-sensitive\" is explained by the α and β that form the context of \"A\" and determine whether \"A\" can be replaced with γ or not. This is different from a context-free grammar where the context of a nonterminal is not taken into consideration. Indeed, every production of a context-free grammar is of the form \"V\" → \"w\" where \"V\" is a \"single\" nonterminal symbol, and \"w\" is a string of terminals and/or nonterminals; \"w\" can be empty.\n\nIf the possibility of adding the empty string to a language is added to the strings recognized by the noncontracting grammars (which can never include the empty string) then the languages in these two definitions are identical.\n\nThe left-context- and right-context-sensitive grammars are defined by restricting the rules to just the form α\"A\" → αγ and to just \"A\"β → γβ, respectively. The languages generated by these grammars are also the full class of context-sensitive languages. The equivalence was established by Penttonen normal form.\n\nThe following grammar, with start symbol \"S\", generates the canonical non-context-free language { \"a\"\"b\"\"c\" : \"n\" ≥ 1 } :\n\nRules 1 and 2 allow for blowing-up \"S\" to \"a\"\"BC\"(\"BC\"); rules 3 to 6 allow for successively exchanging each \"CB\" to \"BC\" (four rules are needed for that since a rule \"CB\" → \"BC\" wouldn't fit into the scheme α\"A\"β → αγβ); rules 7–10 allow replacing a non-terminals \"B\" and \"C\" with its corresponding terminals \"b\" and \"c\" respectively, provided it is in the right place.\nA generation chain for is:\n\nMore complicated grammars formula_1can be used to parse { \"a\"\"b\"\"c\"\"d\": \"n\" ≥ 1 }, and other languages with even more letters. Here we show a simpler approach using non-contracting grammars:\nStart with a kernel of regular productions generating the sentential forms\nformula_2 and then include the non contracting productions\nformula_3,\nformula_4,\nformula_5,\nformula_6,\nformula_7,\nformula_8,\nformula_9,\nformula_10,\nformula_11,\nformula_12.\nA non contracting grammar (for which there is an equivalent formula_1) for the language formula_14 is defined by\nformula_15\nand\nformula_16,\nformula_17, formula_18, formula_19, formula_20, formula_21, formula_22.\n\nWith these definitions, a derivation for formula_23 is:\nformula_24.\nA noncontracting grammar for the language { \"a\" : i ≥ 1 } is constructed in Example 9.5 (p. 224) of (Hopcroft, Ullman, 1979):\n\nformula_25\n\nformula_26\n\nformula_27\n\nformula_28\n\nformula_29\n\nformula_30\n\nformula_31\n\nformula_32\n\nEvery context-sensitive grammar which does not generate the empty string can be transformed into a weakly equivalent one in Kuroda normal form. \"Weakly equivalent\" here means that the two grammars generate the same language. The normal form will not in general be context-sensitive, but will be a noncontracting grammar.\n\nThe Kuroda normal form is an actual normal form for non-contracting grammars.\n\nA formal language can be described by a context-sensitive grammar if and only if it is accepted by some linear bounded automaton (LBA). In some textbooks this result is attributed solely to Landweber and Kuroda. Others call it the Myhill-Landweber-Kuroda Theorem. (Myhill introduced the concept of deterministic LBA in 1960. Peter S. Landweber published in 1963 that the language accepted by a deterministic LBA is context sensitive. Kuroda introduced the notion of non-deterministic LBA and the equivalence between LBA and CSGs in 1964.)\n\nContext-sensitive languages are closed under complement. This 1988 result is known as the Immerman–Szelepcsényi theorem.\nMoreover, they are closed under union, intersection, concatenation, substitution, inverse homomorphism, and Kleene plus.\n\nEvery recursively enumerable language \"L\" can be written as \"h\"(\"L\") for some context-sensitive language \"L\" and some string homomorphism \"h\".\n\nThe decision problem that asks whether a certain string \"s\" belongs to the language of a given context-sensitive grammar \"G\", is PSPACE-complete. Moreover, there are context-sensitive grammars whose languages are PSPACE-complete. In other words, there is a context-sensitive grammar \"G\" such that deciding whether a certain string \"s\" belongs to the language of \"G\" is PSPACE-complete (so \"G\" is fixed and only \"s\" is part of the input of the problem).\n\nThe emptiness problem for context-sensitive grammars (given a context-sensitive grammar \"G\", is \"L\"(\"G\")=∅ ?) is undecidable.\n\nThe LuZc parser is a working example of a program which can parse Context-sensitive grammars.\n\nSavitch has proven the following theoretical result, on which he bases his criticism of CSGs as basis for natural language: for any recursively enumerable set \"R\", there exists a context-sensitive language/grammar \"G\" which can be used as a sort of proxy to test membership in \"R\" in the following way: given a string \"s\", \"s\" is in \"R\" if and only if there exists a positive integer \"n\" for which \"sc\" is in G, where \"c\" is an arbitrary symbol not part of \"R\".\n\nIt has been shown that nearly all natural languages may in general be characterized by context-sensitive grammars, but the whole class of CSG's seems to be much bigger than natural languages. Worse yet, since the aforementioned decision problem for CSG's is PSPACE-complete, that makes them totally unworkable for practical use, as a polynomial-time algorithm for a PSPACE-complete problem would imply P=NP.\n\nIt was proven that some natural languages are not context-free, based on identifying so-called cross-serial dependencies and unbounded scrambling phenomena. However this does not necessarily imply that all the class CSG is necessary to capture \"context sensitivity\" in the colloquial sense of these terms in natural languages. For example, the strictly weaker (than CSG) linear context-free rewriting systems (LCFRS) can account for the phenomenon of cross-serial dependencies; one can write a LCFRS grammar for {\"abcd\" | \"n\" ≥ 1} for example.\n\nOngoing research on computational linguistics has focused on formulating other classes of languages that are \"mildly context-sensitive\" whose decision problems are feasible, such as tree-adjoining grammars, combinatory categorial grammars, coupled context-free languages, and linear context-free rewriting systems. The languages generated by these formalisms properly lie between the context-free and context-sensitive languages.\n\nMore recently, the class PTIME has been identified with range concatenation grammars, which are now considered to be the most expressive of the mild-context sensitive languages.\n\n\n"}
{"id": "12930296", "url": "https://en.wikipedia.org/wiki?curid=12930296", "title": "Decorrelation theory", "text": "Decorrelation theory\n\nIn cryptography, decorrelation theory is a system developed by Serge Vaudenay in 1998 for designing block ciphers to be provably secure against differential cryptanalysis, linear cryptanalysis, and even undiscovered cryptanalytic attacks meeting certain broad criteria. Ciphers designed using these principles include COCONUT98 and the AES candidate DFC, both of which have been shown to be vulnerable to some forms of cryptanalysis not covered by the theory.\n\nAccording to Vaudenay, the decorrelation theory has four tasks: 1) the definition of a measurement for the decorrelation, which usually relies on a matrix norm; 2) the construction of simple primitive or \"decorrelation module\" with a quite good decorrelation; 3) the construction of cryptographic algorithms with decorrelation modules so that the primitive can be inherited by the algorithm; and, 4) proving that the decorrelation provides security against attacks. \n\n"}
{"id": "1338725", "url": "https://en.wikipedia.org/wiki?curid=1338725", "title": "Delegated Path Validation", "text": "Delegated Path Validation\n\nDelegated Path Validation (DPV) is a method for offloading to a trusted server the work involved in validating a public key certificate.\n\nCombining certificate information supplied by the DPV client with certificate path and revocation status information obtained by itself, a DPV server is able to apply complex validation policies that are prohibitive for each client to perform. \n\nThe requirements for DPV are described in RFC 3379.\n\n"}
{"id": "35987528", "url": "https://en.wikipedia.org/wiki?curid=35987528", "title": "Distribution algebra", "text": "Distribution algebra\n\nIn algebra, the distribution algebra formula_1 of a \"p\"-adic Lie group \"G\" is the \"K\"-algebra of \"K\"-valued distributions on \"G\". (See the reference for a more precise definition.)\n"}
{"id": "3495507", "url": "https://en.wikipedia.org/wiki?curid=3495507", "title": "Eigenvector centrality", "text": "Eigenvector centrality\n\nIn graph theory, eigenvector centrality (also called eigencentrality) is a measure of the influence of a node in a network. Relative scores are assigned to all nodes in the network based on the concept that connections to high-scoring nodes contribute more to the score of the node in question than equal connections to low-scoring nodes. A high eigenvector score means that a node is connected to many nodes who themselves have high scores.\n\nGoogle's PageRank and the Katz centrality are variants of the eigenvector centrality.\n\nFor a given graph formula_1 with formula_2 vertices let formula_3 be the adjacency matrix, i.e. formula_4 if vertex formula_5 is linked to vertex formula_6, and formula_7 otherwise. The relative centrality score of vertex formula_5 can be defined as:\n\nwhere formula_10 is a set of the neighbors of formula_5 and formula_12 is a constant. With a small rearrangement this can be rewritten in vector notation as the eigenvector equation\n\nIn general, there will be many different eigenvalues formula_12 for which a non-zero eigenvector solution exists. However, the additional requirement that all the entries in the eigenvector be non-negative implies (by the Perron–Frobenius theorem) that only the greatest eigenvalue results in the desired centrality measure. The formula_15 component of the related eigenvector then gives the relative centrality score of the vertex formula_5 in the network. The eigenvector is only defined up to a common factor, so only the ratios of the centralities of the vertices are well defined. To define an absolute score one must normalise the eigen vector e.g. such that the sum over all vertices is 1 or the total number of vertices \"n\". Power iteration is one of many eigenvalue algorithms that may be used to find this dominant eigenvector. Furthermore, this can be generalized so that the entries in \"A\" can be real numbers representing connection strengths, as in a stochastic matrix.\n\nIn neuroscience, the eigenvector centrality of a neuron in a model neural network has been found to correlate with its relative firing rate.\n\n"}
{"id": "9318685", "url": "https://en.wikipedia.org/wiki?curid=9318685", "title": "Elementary proof", "text": "Elementary proof\n\nIn mathematics, an elementary proof is a mathematical proof that only uses basic techniques. More specifically, the term is used in number theory to refer to proofs that make no use of complex analysis. For some time it was thought that certain theorems, like the prime number theorem, could only be proved using \"higher\" mathematics. However, over time, many of these results have been reproved using only elementary techniques.\n\nWhile the meaning has not always been defined precisely, the term is commonly used in mathematical jargon. An elementary proof is not necessarily simple, in the sense of being easy to understand: some elementary proofs can be quite complicated.\n\nThe distinction between elementary and non-elementary proofs has been considered especially important in regard to the prime number theorem. This theorem was first proved in 1896 by Jacques Hadamard and Charles Jean de la Vallée-Poussin using complex analysis. Many mathematicians then attempted to construct elementary proofs of the theorem, without success. G. H. Hardy expressed strong reservations; he considered that the essential \"depth\" of the result ruled out elementary proofs:\n\nHowever, in 1948, Atle Selberg produced new methods which led him and Paul Erdős to find elementary proofs of the prime number theorem. \n\nA possible formalization of the notion of \"elementary\" in connection to a proof of a number-theoretical result is the restriction that the proof can be carried out in Peano arithmetic. Also in that sense, these proofs are elementary.\n\nHarvey Friedman conjectured, \"Every theorem published in the \"Annals of Mathematics\" whose statement involves only finitary mathematical objects (i.e., what logicians call an arithmetical statement) can be proved in elementary arithmetic.\" The form of elementary arithmetic referred to in this conjecture can be formalized by a small set of axioms concerning integer arithmetic and mathematical induction. For instance, according to this conjecture, Fermat's Last Theorem should have an elementary proof; Wiles' proof of Fermat's Last Theorem is not elementary. However, there are other simple statements about arithmetic such as the existence of iterated exponential functions that cannot be proven in this theory.\n"}
{"id": "10288115", "url": "https://en.wikipedia.org/wiki?curid=10288115", "title": "Examples of generating functions", "text": "Examples of generating functions\n\nThe following examples of generating functions are in the spirit of George Pólya, who advocated learning mathematics by doing and re-capitulating as many examples and proofs as possible. The purpose of this article is to present common ways of creating generating functions.\n\nNew generating functions can be created by extending simpler generating functions. For example, starting with\n\nand replacing formula_2 with formula_3, we obtain\n\nOne can define generating functions in several variables, for series with several indices. These are often called super generating functions, and for 2 variables are often called bivariate generating functions.\n\nFor instance, since formula_5 is the generating function for binomial coefficients for a fixed \"n,\" one may ask for a bivariate generating function that generates the binomial coefficients formula_6 for all \"k\" and \"n.\"\nTo do this, consider formula_5 as \"itself\" a series (in \"n\"), and find the generating function in \"y\" that has these as coefficients. Since the generating function for formula_8 is just formula_9, the generating function for the binomial coefficients is:\nand the coefficient on formula_11 is the formula_6 binomial coefficient.\n\nConsider the problem of finding a closed formula for the Fibonacci numbers \"F\" defined by \"F\" = 0, \"F\" = 1, and \"F\" = \"F\" + \"F\" for \"n\" ≥ 2. We form the ordinary generating function\n\nfor this sequence. The generating function for the sequence (\"F\") is \"xf\" and that of (\"F\") is \"x\"\"f\". From the recurrence relation, we therefore see that the power series \"xf\" + \"x\"\"f\" agrees with \"f\" except for the first two coefficients:\n\nTaking these into account, we find that\n\n(This is the crucial step; recurrence relations can almost always be translated into equations for the generating functions.) Solving this equation for \"f\", we get\n\nThe denominator can be factored using the golden ratio φ = (1 + )/2 and φ = (1 − )/2, and the technique of partial fraction decomposition yields\n\nThese two formal power series are known explicitly because they are geometric series; comparing coefficients, we find the explicit formula\n\nThe number of ways \"a\" to make change for \"n\" cents using coins with values 1, 5, 10, and 25 is given by the generating function\nFor example there are two unordered ways to make change for 6 cents; one way is six 1-cent coins, the other way is one 1-cent coin and one 5-cent coin. See .\n\nOn the other hand, the number of ways \"b\" to make change for \"n\" cents using coins with values 1, 5, 10, and 25 is given by the generating function\nFor example there are three ordered ways to make change for 6 cents; one way is six 1-cent coins, a second way is one 1-cent coin and one 5-cent coin, and a third way is one 5-cent coin and one 1-cent coin. Compare to , which differs from this example by also including coins with values 50 and 100.\n\n"}
{"id": "18797498", "url": "https://en.wikipedia.org/wiki?curid=18797498", "title": "Experimental Mathematics (journal)", "text": "Experimental Mathematics (journal)\n\nExperimental Mathematics is a quarterly scientific journal of mathematics published by A K Peters, Ltd. until 2010, now by Taylor & Francis. The journal publishes papers in experimental mathematics, broadly construed. The journal's mission statement describes its scope as follows: \"Experimental Mathematics publishes original papers featuring formal results inspired by experimentation, conjectures suggested by experiments, and data supporting significant hypotheses.\" the editor-in-chief is Sergei Tabachnikov (ICERM, Brown University).\n\n\"Experimental Mathematics\" was established in 1992 by David Epstein, Silvio Levy, and Klaus Peters. \"Experimental Mathematics\" was the first mathematical research journal to concentrate on experimental mathematics and to explicitly acknowledge its importance for mathematics as a general research field. The journal's launching was described as \"something of a watershed\". Indeed, the launching of the journal in 1992 was surrounded by some controversy in the mathematical community about the value and validity of experimentation in mathematical research. Some critics of the new journal suggested that it be renamed as the \"Journal of Unproved Theorems\". In a 1995 article in the \"Notices of the American Mathematical Society\", in part responding to such criticism, Epstein and Levy described the journal's aims as follows:\nBut the main difference reflects the philosophy above: we are interested not only in theorems and proofs but also in the way in which they have been or can be reached. Note that we do value proofs: experimentally inspired results that can be proved are more desirable than conjectural ones. However, we do publish significant conjectures or explorations in the hope of inspiring other, perhaps better-equipped researchers to carry on the investigation. The objective of Experimental Mathematics is to play a role in the discovery of formal proofs, not to displace them.\n\nIn recent years a number of other research journals in pure mathematics have substantially expanded their coverage of experimental mathematics and new journals devoted in large part to experimental mathematics have been launched. Thus, in 1998 the London Mathematical Society launched \"LMS Journal of Computation and Mathematics\" and in 2004 the \"Journal of Algebra\" started a new section called \"Computational Algebra\". \"LMS Journal of Computation and Mathematics\" was closed to new submissions in October 2015.\n\nDespite the initial controversy, \"Experimental Mathematics\" quickly established a solid reputation and is now a highly respected mathematical publication. The journal is reviewed cover-to-cover in \"Mathematical Reviews\" and \"Zentralblatt MATH\" and is indexed in the Web of Science.\n"}
{"id": "42247504", "url": "https://en.wikipedia.org/wiki?curid=42247504", "title": "Fat object", "text": "Fat object\n\nIn geometry, a fat object is an object in two or more dimensions, whose lengths in the different dimensions are similar. For example, a square is fat because its length and width are identical. A 2-by-1 rectangle is thinner than a square, but it is fat relative to a 10-by-1 rectangle. Similarly, a circle is fatter than a 1-by-10 ellipse and an equilateral triangle is fatter than a very obtuse triangle.\n\nFat objects are especially important in computational geometry. Many algorithms in computational geometry can perform much better if their input consists of only fat objects; see the applications section below.\n\nGiven a constant \"R\"≥1, an object \"o\" is called \"R\"-fat if its \"slimness factor\" is at most \"R\". The \"slimness factor\" has different definitions in different papers. A common definition is:\n\nwhere \"o\" and the cubes are \"d\"-dimensional. A 2-dimensional cube is a square, so the slimness factor of a square is 1 (since its smallest enclosing square is the same as its largest enclosed disk). The slimness factor of a 10-by-1 rectangle is 10. The slimness factor of a circle is √2. Hence, by this definition, a square is 1-fat but a disk and a 10×1 rectangle are not 1-fat. A square is also 2-fat (since its slimness factor is less than 2), 3-fat, etc. A disk is also 2-fat (and also 3-fat etc.), but a 10×1 rectangle is not 2-fat. Every shape is ∞-fat, since by definition the slimness factor is always at most ∞.\n\nThe above definition can be termed two-cubes fatness since it is based on the ratio between the side-lengths of two cubes. Similarly, it is possible to define two-balls fatness, in which a d-dimensional ball is used instead. A 2-dimensional ball is a disk. According to this alternative definition, a disk is 1-fat but a square is not 1-fat, since its two-balls-slimness is √2.\n\nAn alternative definition, that can be termed enclosing-ball fatness (also called \"thickness\") is based on the following slimness factor:\n\nThe exponent 1/\"d\" makes this definition a ratio of two lengths, so that it is comparable to the two-balls-fatness.\n\nHere, too, a cube can be used instead of a ball.\n\nSimilarly it is possible to define the enclosed-ball fatness based on the following slimness factor:\n\nThe enclosing-ball/cube-slimness might be very different from the enclosed-ball/cube-slimness.\n\nFor example, consider a lollipop with a candy in the shape of a 1×1 square and a stick in the shape of a \"b\"×(1/\"b\") rectangle (with \"b\">1>(1/\"b\")). As \"b\" increases, the area of the enclosing cube (≈\"b\") increases, but the area of the enclosed cube remains constant (=1) and the total area of the shape also remains constant (=2). Thus the enclosing-cube-slimness can grow arbitrarily while the enclosed-cube-slimness remains constant (=√2). See this GeoGebra page for a demonstration.\n\nOn the other hand, consider a rectilinear 'snake' with width \"1/b\" and length \"b\", that is entirely folded within a square of side length 1. As \"b\" increases, the area of the enclosed cube(≈1/\"b\") decreases, but the total areas of the snake and of the enclosing cube remain constant (=1). Thus the enclosed-cube-slimness can grow arbitrarily while the enclosing-cube-slimness remains constant (=1).\n\nWith both the lollipop and the snake, the two-cubes-slimness grows arbitrarily, since in general:\n\nSince all slimness factor are at least 1, it follows that if an object \"o\" is R-fat according to the two-balls/cubes definition, it is also R-fat according to the enclosing-ball/cube and enclosed-ball/cube definitions (but the opposite is not true, as exemplified above).\n\nThe volume of a \"d\"-dimensional ball of radius \"r\" is: formula_4, where \"V\" is a dimension-dependent constant:\n\nA \"d\"-dimensional cube with side-length 2\"a\" has volume (2\"a\"). It is enclosed in a \"d\"-dimensional ball with radius \"a√d\" whose volume is \"V\"(\"a√d\"). Hence for every \"d\"-dimensional object:\n\nFor even dimensions (\"d\"=2\"k\"), the factor simplifies to: formula_7. In particular, for two-dimensional shapes \"V\"=π and the factor is: √(0.5 π)≈1.25, so:\n\nFrom similar considerations:\n\nA \"d\"-dimensional ball with radius \"a\" is enclosed in a \"d\"-dimensional cube with side-length 2\"a\". Hence for every \"d\"-dimensional object:\n\nFor even dimensions (\"d\"=2\"k\"), the factor simplifies to: formula_10. In particular, for two-dimensional shapes the factor is: 2/√π≈1.13, so:\n\nFrom similar considerations:\n\nMultiplying the above relations gives the following simple relations:\n\nThus, an \"R\"-fat object according to the either the two-balls or the two-cubes definition is at most \"R\"√\"d\"-fat according to the alternative definition.\n\nThe above definitions are all \"global\" in the sense that they don't care about small thin areas that are part of a large fat object.\n\nFor example, consider a lollipop with a candy in the shape of a 1×1 square and a stick in the shape of a 1×(1/\"b\") rectangle (with \"b\">1>(1/\"b\")). As \"b\" increases, the area of the enclosing cube (=4) and the area of the enclosed cube (=1) remain constant, while the total area of the shape changes only slightly (=1+1/\"b\"). Thus all three slimness factors are bounded: enclosing-cube-slimness≤2, enclosed-cube-slimness≤2, two-cube-slimness=2. Thus by all definitions the lollipop is 2-fat. However, the stick-part of the lollipop obviously becomes thinner and thinner.\n\nIn some applications, such thin parts are unacceptable, so local fatness, based on a local slimness factor, may be more appropriate. For every global slimness factor, it is possible to define a local version. For example, for the enclosing-ball-slimness, it is possible to define the local-enclosing-ball slimness factor of an object \"o\" by considering the set \"B\" of all balls whose center is inside \"o\" and whose boundary intersects the boundary of \"o\" (i.e. not entirely containing \"o\"). The local-enclosing-ball-slimness factor is defined as:\n\nThe 1/2 is a normalization factor that makes the local-enclosing-ball-slimness of a ball equal to 1. The local-enclosing-ball-slimness of the lollipop-shape described above is dominated by the 1×(1/\"b\") stick, and it goes to ∞ as \"b\" grows. Thus by the local definition the above lollipop is not 2-fat.\n\nLocal-fatness implies global-fatness. Here is a proof sketch for fatness based on enclosing balls. By definition, the volume of the smallest enclosing ball is ≤ the volume of any other enclosing ball. In particular, it is ≤ the volume of any enclosing ball whose center is inside \"o\" and whose boundary touches the boundary of \"o\". But every such enclosing ball is in the set \"B\" considered by the definition of local-enclosing-ball slimness. Hence:\n\nHence:\n\nFor a convex body, the opposite is also true: local-fatness implies global-fatness. The proof is based on the following lemma. Let \"o\" be a convex object. Let \"P\" be a point in \"o\". Let \"b\" and \"B\" be two balls centered at \"P\" such that \"b\" is smaller than \"B\". Then \"o\" intersects a larger portion of \"b\" than of \"B\", i.e.:\n\nProof sketch: standing at the point \"P\", we can look at different angles \"θ\" and measure the distance to the boundary of \"o\". Because \"o\" is convex, this distance is a function, say \"r\"(\"θ\"). We can calculate the left-hand side of the inequality by integrating the following function (multiplied by some determinant function) over all angles:\n\nSimilarly we can calculate the right-hand side of the inequality by integrating the following function:\n\nBy checking all 3 possible cases, it is possible to show that always formula_16. Thus the integral of \"f\" is at least the integral of \"F\", and the lemma follows.\n\nThe definition of local-enclosing-ball slimness considers \"all\" balls that are centered in a point in \"o\" and intersect the boundary of \"o\". However, when \"o\" is convex, the above lemma allows us to consider, for each point in \"o\", only balls that are maximal in size, i.e., only balls that entirely contain \"o\" (and whose boundary intersects the boundary of \"o\"). For every such ball \"b\":\n\nwhere formula_18 is some dimension-dependent constant.\n\nThe diameter of \"o\" is at most the diameter of the smallest ball enclosing \"o\", and the volume of that ball is: formula_19. Combining all inequalities gives that for every \"convex\" object:\n\nFor non-convex objects, this inequality of course doesn't hold, as exemplified by the lollipop above.\n\nThe following table shows the slimness factor of various shapes based on the different definitions. The two columns of the local definitions are filled with \"*\" when the shape is convex (in this case, the value of the local slimness equals the value of the corresponding global slimness):\n\nSlimness is invariant to scale, so the slimness factor of a triangle (as of any other polygon) can be presented as a function of its angles only. The three ball-based slimness factors can be calculated using well-known trigonometric identities.\n\nThe largest circle contained in a triangle is called its incircle. It is known that:\n\nwhere \"Δ\" is the area of a triangle and \"r\" is the radius of the incircle. Hence, the enclosed-ball slimness of a triangle is:\n\nThe smallest containing circle for an acute triangle is its circumcircle, while for an obtuse triangle it is the circle having the triangle's longest side as a diameter.\n\nIt is known that:\n\nwhere again \"Δ\" is the area of a triangle and \"R\" is the radius of the circumcircle. Hence, for an acute triangle, the enclosing-ball slimness factor is:\n\nIt is also known that:\n\nwhere \"c\" is any side of the triangle and \"A\",\"B\" are the adjacent angles. Hence, for an obtuse triangle with acute angles A and B (and longest side \"c\"), the enclosing-ball slimness factor is:\n\nNote that in a right triangle, formula_26, so the two expressions coincide.\n\nThe inradius \"r\" and the circumradius \"R\" are connected via a couple of formulae which provide two alternative expressions for the two-balls slimness of an acute triangle:\n\nFor an obtuse triangle, \"c\"/2 should be used instead of \"R\". By the Law of sines:\n\nHence the slimness factor of an obtuse triangle with obtuse angle \"C\" is:\n\nNote that in a right triangle, formula_30, so the two expressions coincide.\n\nThe two expressions can be combined in the following way to get a single expression for the two-balls slimness of any triangle with smaller angles \"A\" and \"B\":\n\nTo get a feeling of the rate of change in fatness, consider what this formula gives for an isosceles triangle with head angle \"θ\" when \"θ\" is small:\n\nThe following graphs show the 2-balls slimness factor of a triangle:\n\nThe ball-based slimness of a circle is of course 1 - the smallest possible value. \nFor a circular segment with central angle \"θ\", the circumcircle diameter is the length of the chord and the incircle diameter is the height of the segment, so the two-balls slimness (and its approximation when \"θ\" is small) is:\nFor a circular sector with central angle \"θ\" (when \"θ\" is small), the circumcircle diameter is the radius of the circle and the incircle diameter is the chord length, so the two-balls slimness is:\n\nFor an ellipse, the slimness factors are different in different locations. For example, consider an ellipse with short axis \"a\" and long axis \"b\". the length of a chord ranges between formula_35 at the narrow side of the ellipse and formula_36 at its wide side; similarly, the height of the segment ranges between formula_37 at the narrow side and formula_38 at its wide side. So the two-balls slimness ranges between:\n\nand:\n\nIn general, when the secant starts at angle Θ the slimness factor can be approximated by:\n\nA convex polygon is called \"r\"-separated if the angle between each pair of edges (not necessarily adjacent) is at least \"r\". \n\nLemma: The enclosing-ball-slimness of an \"r\"-separated convex polygon is at most formula_43.\n\nA convex polygon is called \"k,r\"-separated if:\n\nLemma: The enclosing-ball-slimness of a \"k,r\"-separated convex polygon is at most formula_44. improve the upper bound to formula_45.\n\nIf an object \"o\" has diameter 2\"a\", then every ball enclosing \"o\" must have radius at least \"a\" and volume at least \"Va\". Hence, by definition of enclosing-ball-fatness, the volume of an \"R\"-fat object with diameter 2\"a\" must be at least: \"Va\"/\"R\". Hence:\n\nFor example (taking \"d\"=2, \"R\"=1 and \"C\"=3): The number of non-overlapping disks with radius at least 1 contained in a circle of radius 3 is at most 3=9. (Actually, it is at most 7).\n\nIf we consider local-fatness instead of global-fatness, we can get a stronger lemma:\n\nFor example (taking \"d\"=2, \"R\"=1 and \"C\"=0): the number of non-overlapping disks with radius larger than 1 that touch a given unit disk is at most 4=16 (this is not a tight bound since in this case it is easy to prove an upper bound of 5).\n\nThe following generalization of fatness were studied by for 2-dimensional objects.\n\nA triangle ∆ is a (β, δ)-triangle of a planar object \"o\" (0<β≤π/3, 0<δ< 1), if ∆ ⊆ \"o\", each of the angles of ∆ is at least β, and the length of each of its edges is at least δ·diameter(\"o\"). An object \"o\" in the plane is (β,δ)-covered if for each point P ∈ \"o\" there exists a (β, δ)-triangle ∆ of \"o\" that contains P.\n\nFor convex objects, the two definitions are equivalent, in the sense that if \"o\" is α-fat, for some constant α, then it is also (β,δ)-covered, for appropriate constants β and δ, and vice versa. However, for non-convex objects the definition of being fat is more general than the definition of being (β, δ)-covered.\n\nFat objects are used in various problems, for example:\n"}
{"id": "23708391", "url": "https://en.wikipedia.org/wiki?curid=23708391", "title": "Formal manifold", "text": "Formal manifold\n\nIn geometry and topology, a formal manifold can mean one of a number of related concepts:\n"}
{"id": "53277729", "url": "https://en.wikipedia.org/wiki?curid=53277729", "title": "Franz Harress", "text": "Franz Harress\n\nFranz Harress (died circa 1915) was a mathematician and contemporary of Albert Einstein and is best known for his experiment on the propagation of light in a rotating glass device. This experiment sparked an argument between Einstein and Paul Harzer related to the theory of Special Relativity.\nHarress was a student of Professor Otto Julius Heinrich Knopf at Friedrich-Schiller-Universität Jena in 1912. His dissertation was \"Die Geschwindigkeit des Lichtes in bewegten Körpern\" (\"The speed of light in moving bodies\"). His work on the Sagnac effect was analyzed by Max von Laue in his 1920 paper \"On the Experiment of F. Harress.\"\n"}
{"id": "9211532", "url": "https://en.wikipedia.org/wiki?curid=9211532", "title": "Gyula O. H. Katona", "text": "Gyula O. H. Katona\n\nGyula O. H. Katona (born March 16, 1941 in Budapest) is a Hungarian mathematician known for his work in combinatorial set theory, and especially for the Kruskal–Katona theorem and his beautiful and elegant proof of the Erdős–Ko–Rado theorem in which he discovered a new method, now called Katona's cycle method. Since then, this method has become a powerful tool in proving many interesting results in extremal set theory. He is affiliated with the Alfréd Rényi Institute of Mathematics of the Hungarian Academy of Sciences.\n\nKatona was secretary-general of the János Bolyai Mathematical Society from 1990 to 1996. In 1966 and 1968 he won the Grünwald Prize, awarded by the Bolyai Society to outstanding young mathematicians, he was awarded the Alfréd Rényi Prize of the Hungarian Academy of Sciences in 1975, and the same academy awarded him the Prize of the Academy in 1989. In 2011 the Alfréd Rényi Institute, the János Bolyai Society and the Hungarian Academy of Sciences organized a conference in honor of Katona's 70th birthday.\n\nGyula O.H. Katona is the father of Gyula Y. Katona, another Hungarian mathematician with similar research interests to those of his father.\n\n"}
{"id": "14220", "url": "https://en.wikipedia.org/wiki?curid=14220", "title": "History of mathematics", "text": "History of mathematics\n\nThe area of study known as the history of mathematics is primarily an investigation into the origin of discoveries in mathematics and, to a lesser extent, an investigation into the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, together with Ancient Egypt and Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the field of astronomy and to formulate calendars and record time.\n\nThe most ancient mathematical texts available are from Mesopotamia and Egypt - \"Plimpton 322\" (Babylonian c. 1900 BC), the \"Rhind Mathematical Papyrus\" (Egyptian c. 2000–1800 BC) and the \"Moscow Mathematical Papyrus\" (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples and so, by inference, the Pythagorean theorem, seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.\n\nThe study of mathematics as a \"demonstrative discipline\" begins in the 6th century BC with the Pythagoreans, who coined the term \"mathematics\" from the ancient Greek \"μάθημα\" (\"mathema\"), meaning \"subject of instruction\". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.\n\nMany Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century. At the end of the 19th century the International Congress of Mathematicians was founded and continues to spearhead advances in the field.\n\nThe origins of mathematical thought lie in the concepts of number, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the \"number\" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between \"one\", \"two\", and \"many\", but not of numbers larger than two.\n\nPrehistoric artifacts discovered in Africa, dated 20,000 years old or more suggest early attempts to quantify time. The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a \"tally\" of the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that \"no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10.\" The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.\n\nPredynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.\n\nBabylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.\nIn contrast to the sparsity of sources in Egyptian mathematics, our knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.\n\nThe earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC. From around 2500 BC onwards, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.\nBabylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 x 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is likely the sexagesimal system was chosen because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a true place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different than multiplying integers, similar to our modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computation accuracy and power; for example, the Babylonian tablet YBC 7289 gives an approximation of accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.\n\nOther topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.\n\nEgyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars.\n\nThe most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.\n\nAnother significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called \"word problems\" or \"story problems\", which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).\n\nFinally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.\n\nGreek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.\n\nGreek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.\n\nGreek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.\n\nThales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was \"All is number\". It was the Pythagoreans who coined the term \"mathematics\", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the \"mensa Pythagorica\".\n\nPlato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus, came. Plato also discussed the foundations of mathematics, clarified some of the definitions (e.g. that of a line as \"breadthless length\"), and reorganized the assumptions. The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.\n\nEudoxus (408–c. 355 BC) developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.\nIn the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the \"Elements\", widely considered the most successful and influential textbook of all time. The \"Elements\" introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the \"Elements\" were already known, Euclid arranged them into a single, coherent logical framework. The \"Elements\" was known to all educated people in the West until the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the \"Elements\" was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.\nArchimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, . He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.\nApollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola (\"place beside\" or \"comparison\"), \"ellipse\" (\"deficiency\"), and \"hyperbola\" (\"a throw beyond\"). His work \"Conics\" is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.\n\nAround the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the \"Golden Age\" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the \"Almagest\" of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.\nFollowing a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the \"Silver Age\" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as \"Diophantine analysis\". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the \"Arithmetica\", a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The \"Arithmetica\" had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the \"Arithmetica\" (that of dividing a square into two squares). Diophantus also made significant advances in notation, the \"Arithmetica\" being the first instance of algebraic symbolism and syncopation.\nAmong the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His \"Collection\" is a major source of knowledge on Greek mathematics as most of it has survived. Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.\n\nThe first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed. Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius. Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Haghia Sophia. Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.\n\nAlthough ethnic Greek mathematicians continued to live under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison. Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks. It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.\n\nUsing calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury. Siculus Flaccus, one of the Roman \"gromatici\" (i.e. land surveyor), wrote the \"Categories of Fields\", which aided Roman surveyors in measuring the surface areas of allotted lands and territories. Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns. Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.\n\nThe creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year. In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February. This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle. This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (), virtually the same solar calendar used in modern times as the international standard calendar.\n\nAt roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC - c. 15 BC). The device was used at least until the reign of emperor Commodus (), but its design seems to have been lost until experiments were made during the 15th century in Western Europe. Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.\n\nAn analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the \"Zhoubi Suanjing\", variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.\nOf particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called \"rod numerals\" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for \"1\", followed by the symbol for \"100\", then the symbol for \"2\" followed by the symbol for \"10\", followed by the symbol for \"3\". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the \"suan pan\", or Chinese abacus. The date of the invention of the \"suan pan\" is not certain, but the earliest written mention dates from AD 190, in Xu Yue's \"Supplementary Notes on the Art of Figures\".\n\nThe oldest existent work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The \"Mo Jing\" described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well. It also defined the concepts of circumference, diameter, radius, and volume.\nIn 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is \"The Nine Chapters on the Mathematical Art\", the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. The treatise also provides values of π, which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724, as well as 3.162 by taking the square root of 10. Liu Hui commented on the \"Nine Chapters\" in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159). Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (i.e. 3.141592), which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.\n\nThe high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the \"Precious Mirror of the Four Elements\" by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The \"Precious Mirror\" also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).\n\nEven after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.\n\nJapanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere. Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644). For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers. Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.\n\nThe earliest civilization on the Indian subcontinent is the Indus Valley Civilization (mature phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.\n\nThe oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.\n\nPāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called \"mātrāmeru\").\n\nThe next significant mathematical documents from India after the \"Sulba Sutras\" are the \"Siddhantas\", astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words \"sine\" and \"cosine\" derive from the Sanskrit \"jiya\" and \"kojiya\".\nAround 500 AD, Aryabhata wrote the \"Aryabhatiya\", a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. Though about half of the entries are wrong, it is in the \"Aryabhatiya\" that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the \"Aryabhatiya\" as a \"mix of common pebbles and costly crystals\".\n\nIn the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in \"Brahma-sphuta-siddhanta\", he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.\n\nIn the 12th century, Bhāskara II lived in southern India and wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, derivatives, the mean value theorem and the derivative of the sine function. To what extent he anticipated the invention of calculus is a controversial subject among historians of mathematics.\n\nIn the 14th century, Madhava of Sangamagrama, the founder of the so-called Kerala School of Mathematics, found the Madhava–Leibniz series, and, using 21 terms, computed the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the \"Yukti-bhāṣā\". However, the Kerala School did not formulate a systematic theory of differentiation and integration, nor is there any direct evidence of their results being transmitted outside Kerala.\n\nThe Islamic Empire established across Persia, the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, most of them were not written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time. Persians contributed to the world of Mathematics alongside Arabs.\n\nIn the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote several important books on the Hindu–Arabic numerals and on methods for solving equations. His book \"On the Calculation with Hindu Numerals\", written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word \"algorithm\" is derived from the Latinization of his name, Algoritmi, and the word \"algebra\" from the title of one of his works, \"Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala\" (\"The Compendious Book on Calculation by Completion and Balancing\"). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of \"reduction\" and \"balancing\", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as \"al-jabr\". His algebra was also no longer concerned \"with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study.\" He also studied an equation for its own sake and \"in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.\"\n\nIn Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.\n\nFurther developments in algebra were made by Al-Karaji in his treatise \"al-Fakhri\", where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being \"the first who introduced the theory of algebraic calculus.\" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.\n\nIn the late 11th century, Omar Khayyam wrote \"Discussions of the Difficulties in Euclid\", a book about what he perceived as flaws in Euclid's \"Elements\", especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.\n\nIn the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating \"n\"th roots, which was a special case of the methods given many centuries later by Ruffini and Horner.\n\nOther achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.\n\nDuring the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.\n\nIn the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics. Maya numerals utilized a base of 20, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures. The Mayas used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy. While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Mayas developed a standard symbol for it.\n\nMedieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's \"Timaeus\" and the biblical passage (in the \"Book of Wisdom\") that God had \"ordered all things in measure, and number, and weight\".\n\nBoethius provided a place for mathematics in the curriculum in the 6th century when he coined the term \"quadrivium\" to describe the study of arithmetic, geometry, astronomy, and music. He wrote \"De institutione arithmetica\", a free translation from the Greek of Nicomachus's \"Introduction to Arithmetic\"; \"De institutione musica\", also derived from Greek sources; and a series of excerpts from Euclid's \"Elements\". His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.\n\nIn the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's \"The Compendious Book on Calculation by Completion and Balancing\", translated into Latin by Robert of Chester, and the complete text of Euclid's \"Elements\", translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.\n\nLeonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote \"Liber Abaci\" in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which was used as an unremarkable example within the text.\n\nThe 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.\n\nThomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:\nV = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.\nOne of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed \"by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant\".\n\nHeytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that \"a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]\".\n\nNicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's \"Elements\", Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.\n\nDuring the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as \"abbaco\" in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.\n\nPiero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including \"De Prospectiva Pingendi (On Perspective for Painting)\", \"Trattato d’Abaco (Abacus Treatise)\", and \"De corporibus regularibus (Regular Solids)\".\nLuca Pacioli's \"Summa de Arithmetica, Geometria, Proportioni et Proportionalità\" (Italian: \"Review of Arithmetic, Geometry, Ratio and Proportion\") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, \"Particularis de Computis et Scripturis\" (Italian: \"Details of Calculation and Recording\"). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In \"Summa Arithmetica\", Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. \"Summa Arithmetica\" was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.\n\nIn Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.\n\nSimon Stevin's book \"De Thiende\" ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation, which influenced all later work on the real number system.\n\nDriven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his \"Trigonometria\" in 1595. Regiomontanus's table of sines and cosines was published in 1533.\n\nDuring the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that involved, were studied intensely.\n\nThe 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based on a toy imported from Holland. Tycho Brahe had gathered an enormous quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.\nThe analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.\n\nBuilding on earlier work by many predecessors, Isaac Newton discovered the laws of physics explaining Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, who is arguably one of the most important mathematicians of the 17th century, developed calculus and much of the calculus notation still in use today. Science and mathematics had become an international endeavor, which would soon spread over the entire world.\n\nIn addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th–19th century.\n\nThe most influential mathematician of the 18th century was arguably Leonhard Euler. His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol \"i\", and he popularized the use of the Greek letter formula_1 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.\n\nOther important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Laplace who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.\n\nThroughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.\nThis century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.\nThe Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces.\n\nThe 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in computer science.\n\nAugustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.\n\nAlso, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians utilized this in their proofs that straightedge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.\n\nAbel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.\n\nIn the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.\n\nThe 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.\n\nIn 1897, Hensel introduced p-adic numbers.\n\nThe 20th century saw mathematics become a major profession. Every year, thousands of new Ph.D.s in mathematics were awarded, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.\n\nIn a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.\nNotable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998 Thomas Callister Hales proved the Kepler conjecture.\n\nMathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the \"enormous theorem\"), whose proof between 1955 and 1983 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym \"Nicolas Bourbaki\", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.\nDifferential geometry came into its own when Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.\nMeasure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.\n\nNon-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.\n\nThe development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas-Lehmer test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the Fast Fourier Transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.\n\nAt the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus one of addition and multiplication, was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incompletable. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.\nOne of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.\n\nPaul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the \"collaborative distance\" between a person and Paul Erdős, as measured by joint authorship of mathematical papers.\n\nEmmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.\n\nAs in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century there were hundreds of specialized areas in mathematics and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.\n\nIn 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems, and in 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).\n\nMost mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive towards open access publishing, first popularized by the arXiv.\n\nThere are many observable trends in mathematics, the most notable being that the subject is growing ever larger, computers are ever more important and powerful, the application of mathematics to bioinformatics is rapidly expanding, and the volume of data being produced by science and industry, facilitated by computers, is explosively expanding.\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "41536799", "url": "https://en.wikipedia.org/wiki?curid=41536799", "title": "Hrvoje Kraljević", "text": "Hrvoje Kraljević\n\nHrvoje Kraljević (born 16 March 1944) is Croatian mathematician and a former politician.\n\nHe was born in Zagreb. He graduated theoretical physics in 1966 and received his PhD in mathematics in 1973 at the Faculty of Science, University of Zagreb. He worked at the Faculty since 1966, becoming a full professor in 1982. He was awarded fellowships in the United States (Princeton, 1974/75), France (Palaiseau, 1978) and Italy (Trento in 1979, 1981).\n\nAs a mathematician, he investigates representation theory, functional analysis and algebra. His most significant result is the research and of unitary and nonunitary dual group SU(\"n\", 1), index generalization for semisimple Banach algebras, contributions to the research of Landau-type inequalities for infinitesimal generators to the theory of almost-summability.\n\nHe was the director of Department of Mathematics at the Faculty of Science (1983–89), dean of the Faculty (1991–98), Editor-in-Chief of the journal \"Glasnik matematički\" (1990–1992) and editor of the conference proceedings \"Functional Analysis\" (1982, 1986, 1990, 1994). He was awarded Ruđer Bošković Science Award in 1983. He became a corresponding member of the Croatian Academy of Sciences and Arts in 1992.\n\nHe served as the Minister of Science and Technology in Croatia in 2000–2002, as a Member of Parliament and Chairman of the Parliamentary Committee for Science, Higher Education and Culture in 2002–2003.\n"}
{"id": "49307795", "url": "https://en.wikipedia.org/wiki?curid=49307795", "title": "Infinite sites model", "text": "Infinite sites model\n\nThe Infinite sites model (ISM) is a mathematical model of molecular evolution first proposed by Motoo Kimura in 1969. Like other mutation models, the ISM provides a basis for understanding how mutation develops new alleles in DNA sequences. Using allele frequencies, it allows for the calculation of heterozygosity, or genetic diversity, in a finite population and for the estimation of genetic distances between populations of interest.\n\nThe assumptions of the ISM are that (1) there are an infinite number of sites where mutations can occur, (2) every new mutation occurs at a novel site, and (3) there is no recombination. The term ‘site’ refers to a single nucleotide base pair. Because every new mutation has to occur at a novel site, there can be no homoplasy, or back-mutation to an allele that previously existed. All identical alleles are identical by descent. The four gamete rule can be applied to the data to ensure that they do not violate the model assumption of no recombination.\n\nThe mutation rate (formula_1) can be estimated as follows, where formula_2 is the number of mutations found within a randomly selected DNA sequence (per generation), formula_3 is the effective population size. The coefficient is the product of twice the gene copies in individuals of the population; in the case of diploid, biparentally-inherited genes the appropriate coefficient is 4 whereas for uniparental, haploid genes, such as mitochondrial genes, the coefficient would be 2 but applied to the \"female\" effective population size which is, for most species, roughly half of formula_3.\n\nformula_5\n\nWhen considering the length of a DNA sequence, the expected number of mutations is calculated as follows\n\nformula_6\n\nWhere k is the length of a DNA sequence and formula_7 is the probability a mutation will occur at a site.\n\nWatterson developed an estimator for mutation rate that incorporates the number of segregating sites (Watterson's estimator).\n\nOne way to think of the ISM is in how it applies to genome evolution. To understand the ISM as it applies to genome evolution, we must think of this model as it applies to chromosomes. Chromosomes are made up of \"sites\", which are nucleotides represented by either A, C, G, or T. While individual chromosomes are not infinite, we must think of chromosomes as continuous intervals or continuous circles.\n\nMultiple assumptions are applied to understanding the ISM in terms of genome evolution:\n\n"}
{"id": "15109", "url": "https://en.wikipedia.org/wiki?curid=15109", "title": "Inverse limit", "text": "Inverse limit\n\nIn mathematics, the inverse limit (also called the projective limit or limit) is a construction that allows one to \"glue together\" several related objects, the precise manner of the gluing process being specified by morphisms between the objects. Inverse limits can be defined in any category.\n\nWe start with the definition of an inverse system (or projective system) of groups and homomorphisms. Let (\"I\", ≤) be a directed poset (not all authors require \"I\" to be directed). Let (\"A\") be a family of groups and suppose we have a family of homomorphisms \"f\": \"A\" → \"A\" for all \"i\" ≤ \"j\" (note the order), called bonding maps, with the following properties:\nThen the pair ((\"A\"), (\"f\")) is called an inverse system of groups and morphisms over \"I\", and the morphisms \"f\" are called the transition morphisms of the system.\n\nWe define the inverse limit of the inverse system ((\"A\"), (\"f\")) as a particular subgroup of the direct product of the \"A\"'s:\n\nThe inverse limit \"A\" comes equipped with \"natural projections\" π: \"A\" → \"A\" which pick out the \"i\"th component of the direct product for each \"i\" in \"I\". The inverse limit and the natural projections satisfy a universal property described in the next section.\n\nThis same construction may be carried out if the \"A\"'s are sets, semigroups, topological spaces, rings, modules (over a fixed ring), algebras (over a fixed ring), etc., and the homomorphisms are morphisms in the corresponding category. The inverse limit will also belong to that category.\n\nThe inverse limit can be defined abstractly in an arbitrary category by means of a universal property. Let (\"X\", \"f\") be an inverse system of objects and morphisms in a category \"C\" (same definition as above). The inverse limit of this system is an object \"X\" in \"C\" together with morphisms π: \"X\" → \"X\" (called \"projections\") satisfying π = \"f\" π for all \"i\" ≤ \"j\". The pair (\"X\", π) must be universal in the sense that for any other such pair (\"Y\", ψ) (i.e. ψ: \"Y\" → \"X\" with ψ = \"f\" ψ for all \"i\" ≤ \"j\") there exists a unique morphism \"u\": \"Y\" → \"X\" such that the diagram\n\ncommutes for all \"i\" ≤ \"j\", for which it suffices to show that ψ = π \"u\" for all \"i\". The inverse limit is often denoted\nwith the inverse system (\"X\", \"f\") being understood.\n\nIn some categories, the inverse limit does not exist. If it does, however, it is unique in a strong sense: given any other inverse limit \"X\"′ there exists a \"unique\" isomorphism \"X\"′ → \"X\" commuting with the projection maps.\n\nWe note that an inverse system in a category \"C\" admits an alternative description in terms of functors. Any partially ordered set \"I\" can be considered as a small category where the morphisms consist of arrows \"i\" → \"j\" if and only if \"i\" ≤ \"j\". An inverse system is then just a contravariant functor \"I\" → \"C\", and the inverse limit functor\nformula_3 is a covariant functor.\n\n\nFor an abelian category \"C\", the inverse limit functor\nis left exact. If \"I\" is ordered (not simply partially ordered) and countable, and \"C\" is the category Ab of abelian groups, the Mittag-Leffler condition is a condition on the transition morphisms \"f\" that ensures the exactness of formula_12. Specifically, Eilenberg constructed a functor\n(pronounced \"lim one\") such that if (\"A\", \"f\"), (\"B\", \"g\"), and (\"C\", \"h\") are three projective systems of abelian groups, and\nis a short exact sequence of inverse systems, then\nis an exact sequence in Ab.\n\nIf the ranges of the morphisms of an inverse system of abelian groups (\"A\", \"f\") are \"stationary\", that is, for every \"k\" there exists \"j\" ≥ \"k\" such that for all \"i\" ≥ \"j\" :formula_16 one says that the system satisfies the Mittag-Leffler condition.\n\nThe name \"Mittag-Leffler\" for this condition was given by Bourbaki in their chapter on uniform structures for a similar result about inverse limits of complete Hausdorff uniform spaces. Mittag-Leffler used a similar argument in the proof of Mittag-Leffler's theorem.\n\nThe following situations are examples where the Mittag-Leffler condition is satisfied: \n\nAn example where formula_17 is non-zero is obtained by taking \"I\" to be the non-negative integers, letting \"A\" = \"p\"Z, \"B\" = Z, and \"C\" = \"B\" / \"A\" = Z/\"p\"Z. Then\nwhere Z denotes the p-adic integers.\n\nMore generally, if \"C\" is an arbitrary abelian category that has enough injectives, then so does \"C\", and the right derived functors of the inverse limit functor can thus be defined. The \"n\"th right derived functor is denoted\nIn the case where \"C\" satisfies Grothendieck's axiom (AB4*), Jan-Erik Roos generalized the functor lim on Ab to series of functors lim such that\nIt was thought for almost 40 years that Roos had proved (in \"Sur les foncteurs dérivés de lim. Applications. \") that lim \"A\" = 0 for (\"A\", \"f\") an inverse system with surjective transition morphisms and \"I\" the set of non-negative integers (such inverse systems are often called \"Mittag-Leffler sequences\"). However, in 2002, Amnon Neeman and Pierre Deligne constructed an example of such a system in a category satisfying (AB4) (in addition to (AB4*)) with lim \"A\" ≠ 0. Roos has since shown (in \"Derived functors of inverse limits revisited\") that his result is correct if \"C\" has a set of generators (in addition to satisfying (AB3) and (AB4*)).\n\nBarry Mitchell has shown (in \"The cohomological dimension of a directed set\") that if \"I\" has cardinality formula_21 (the \"d\"th infinite cardinal), then \"R\"lim is zero for all \"n\" ≥ \"d\" + 2. This applies to the \"I\"-indexed diagrams in the category of \"R\"-modules, with \"R\" a commutative ring; it is not necessarily true in an arbitrary abelian category (see Roos' \"Derived functors of inverse limits revisited\" for examples of abelian categories in which lim, on diagrams indexed by a countable set, is nonzero for \"n\" > 1).\n\nThe categorical dual of an inverse limit is a direct limit (or inductive limit). More general concepts are the limits and colimits of category theory. The terminology is somewhat confusing: inverse limits are limits, while direct limits are colimits.\n\n\n"}
{"id": "1403190", "url": "https://en.wikipedia.org/wiki?curid=1403190", "title": "Law of identity", "text": "Law of identity\n\nIn logic, the law of identity states that each thing is identical with itself. By this it is meant that each thing (be it a universal or a particular) is composed of its own unique set of characteristic qualities or features, which the ancient Greeks called its essence. It is the first of the three classical laws of thought.\n\nIn its symbolic representation, \"\"a\"=\"a\"\", \"E\"pp\"\", or \"For all \"x\": \"x\" = \"x\"\". \n\nIn logical discourse, violations of the law of identity result in the informal logical fallacy known as equivocation. That is to say, we cannot use the same term in the same discourse while having it signify different senses or meanings – even though the different meanings are conventionally prescribed to that term. Corollaries of the law of identity, \"E\"pp\", \"p\" if and only if \"p\"\", are the law of noncontradiction, \"NE\"p\"N\"p\"\", \"not that \"p\" if and only if not \"p\"\"; and the law of excluded middle, \"J\"p\"N\"p\", \"p\" or not \"p\", exclusively\", in which the prefix \"J\" represents the exclusive or, the negation of the prefix \"E\", the logical biconditional. \n\nIn everyday language, violations of the law of identity introduce ambiguity into the discourse, making it difficult to form an interpretation at the desired level of specificity. The law of identity also allows for substitution.\n\nThe earliest recorded use of the law appears to occur in Plato's dialogue \"Theaetetus\" (185a), wherein Socrates attempts to establish that what we call \"sounds\" and \"colours\" are two different classes of thing:\n\nAristotle takes recourse to the law of identity—though he does not identify it as such—in an attempt to negatively demonstrate the law of non-contradiction. However, in doing so, he shows that the law of non-contradiction is not the more fundamental of the two:\n\nIt is used explicitly only once in Aristotle, in a proof in the \"Prior Analytics\":\n\nBoth Thomas Aquinas (\"Met.\" IV, lect. 6) and Duns Scotus (\"Quaest. sup. Met.\" IV, Q. 3) follow Aristotle. Antonius Andreas, the Spanish disciple of Scotus (d. 1320), argues that the first place should belong to the law \"Every Being is a Being\" (\"Omne Ens est Ens\", Qq. in Met. IV, Q. 4), but the late scholastic writer Francisco Suárez (\"Disp. Met.\" III, § 3) disagreed, also preferring to follow Aristotle.\n\nAnother possible allusion to the same principle may be found in the writings of Nicholas of Cusa (1431-1464) where he says: \n\nGottfried Wilhelm Leibniz claimed that the law of identity (sometimes called \"Leibniz'Law\"), which he expresses as \"Everything is what it is\", is the first primitive truth of reason which is affirmative, and the law of noncontradiction is the first negative truth (\"Nouv. Ess.\" IV, 2, § i), arguing that \"the statement that a thing is what it is, is prior to the statement that it is not another thing\" (\"Nouv. Ess.\" IV, 7, § 9). Wilhelm Wundt credits Gottfried Leibniz with the symbolic formulation, \"A is A\".\n\nGeorge Boole, in the introduction to his treatise \"The Laws of Thought\" made the following observation with respect to the nature of language and those principles that must inhere naturally within them, if they are to be intelligible:\n\nJohn Locke (\"Essay Concerning Human Understanding\" IV. vii. iv. (\"Of Maxims\") says:\n\nAfrikan Spir proclaims the law of identity as the fundamental law of knowledge, which is opposed to the changing appearance of the empirical reality.\n\n\n\n"}
{"id": "490054", "url": "https://en.wikipedia.org/wiki?curid=490054", "title": "List of exponential topics", "text": "List of exponential topics\n\nThis is a list of exponential topics, by Wikipedia page. See also list of logarithm topics.\n"}
{"id": "11619257", "url": "https://en.wikipedia.org/wiki?curid=11619257", "title": "Lofting", "text": "Lofting\n\nLofting is a drafting technique (sometimes using mathematical tables) whereby curved lines are generated, to be used in plans for streamlined objects such as aircraft and boats. The lines may be drawn on wood and the wood then cut for advanced woodworking. The technique can be as simple as bending a flexible object, such as a long strip of thin wood or thin plastic, so that it passes over three non-linear points, and scribing the resultant curved line; or as elaborate as plotting the line using computers or mathematical tables.\n\nLofting is particularly useful in boat building, when it is used to draw and cut pieces for hulls and keels. These are usually curved, often in three dimensions. Loftsmen at the mould lofts of shipyards were responsible for taking the dimensions and details from drawings and plans, and translating this information into templates, battens, ordinates, cutting sketches, profiles, margins and other data. From the early 1970s onward computer-aided design (CAD) became normal for the shipbuilding design and lofting process.\n\nLofting was also commonly used in aircraft design before the widespread adoption of computer-generated shaping programs.\n\nAs ship design evolved from craft to science, designers learned various ways to produce long curves on a flat surface. Generating and drawing such curves became a part of ship lofting; \"lofting\" means drawing full-sized patterns, so called because it was often done in large, lightly constructed mezzanines or lofts above the factory floor. When aircraft design progressed beyond the stick-and-fabric boxes of its first decade of existence, the practice of lofting moved naturally into the aeronautical realm. As the storm clouds of World War II gathered in Europe, a US aircraft company, North American Aviation, took the practice into the purely mathematical realm. One of that war's outstanding warplanes, the North American P-51 Mustang, was designed using mathematical charts and tables rather than lofting tables.\n\nLofting is the transfer of a \"Lines Plan\" to a \"Full Sized Plan\". This helps to assure that the boat will be accurate in its layout and pleasing in appearance. There are many methods to loft a set of plans.\n\nGenerally, boat building books have a detailed description of the lofting process, beyond the scope of this article. Plans can be lofted on a level wooden floor, marking heavy paper such as Red Rosin for the full-sized plans or directly on plywood sheets.\n\nThe first step is to lay out the grid, mark the Base Line along the length of the paper or plywood sheet. Then nail Battens every 12 inches (or more in some cases) where the station lines are to be set as a mark for the perpendicular line, which is marked with a T-square. The previous steps are followed in turn by marking the Top Line and the Water Line. Before continuing make sure to check the lines by using the Pythagorean theorem and make sure the grid is square.\n\nThe second step is to mark the points from the table of offsets. All measurements off the table of offsets are listed in Millimeters or the Feet, Inches and Eighths. The points are plotted at each station then use a small nail and a batten to Fair (draw with a fair curve) the boat's lines.\n\n\n\n"}
{"id": "22786540", "url": "https://en.wikipedia.org/wiki?curid=22786540", "title": "Michel Deza", "text": "Michel Deza\n\nMichel Marie Deza (27 April 1939-23 November 2016) was a Soviet and French mathematician, specializing in combinatorics, discrete geometry and graph theory. He was the retired director of research at the French National Centre for Scientific Research (CNRS), the vice president of the European Academy of Sciences, a research professor at the Japan Advanced Institute of Science and Technology, and one of the three founding editors-in-chief of the European Journal of Combinatorics.\n\nDeza graduated from Moscow University in 1961, after which he worked at the Soviet Academy of Sciences until emigrating to France in 1972. In France, he worked at CNRS from 1973 until his 2005 retirement.\nHe has written eight books and about 280 academic papers with 75 different co-authors, including four papers with Paul Erdős, giving him an Erdős number of 1.\n\nThe papers from a conference on combinatorics, geometry and computer science, held in Luminy, France in May 2007, have been collected as a special issue of the European Journal of Combinatorics in honor of Deza's 70th birthday.\n\n\n\n\n"}
{"id": "6038529", "url": "https://en.wikipedia.org/wiki?curid=6038529", "title": "Newman's lemma", "text": "Newman's lemma\n\nIn mathematics, in the theory of rewriting systems, Newman's lemma, also commonly called the diamond lemma, states that a terminating (or strongly normalizing) abstract rewriting system (ARS), that is, one in which there are no infinite reduction sequences, is confluent if it is locally confluent. In fact a terminating ARS is confluent precisely when it is locally confluent.\n\nEquivalently, for every binary relation with no decreasing infinite chains and satisfying a weak version of the diamond property, there is a unique minimal element in every connected component of the relation considered as a graph.\n\nToday, this is seen as a purely combinatorial result based on well-foundedness due to a proof of Gérard Huet in 1980. Newman's original proof was considerably more complicated.\n\nIn general, Newman's lemma can be seen as a combinatorial result about binary relations → on a set \"A\" (written backwards, so that \"a\" → \"b\" means that \"b\" is below \"a\") with the following two properties:\n\n\nIf the above two conditions hold, then the lemma states that → is confluent: whenever and , there is an element \"d\" such that and . In view of the termination of →, this implies that every connected component of → as a graph contains a unique minimal element \"a\", moreover for every element \"b\" of the component.\n\n\n\n"}
{"id": "15255049", "url": "https://en.wikipedia.org/wiki?curid=15255049", "title": "Norwegian Mathematical Society", "text": "Norwegian Mathematical Society\n\nThe Norwegian Mathematical Society () is a professional society for mathematicians. It also organizes mathematical contests and the annual Abel symposium. It was formed in 1918, with Carl Størmer elected as its first president.\n\n"}
{"id": "10935949", "url": "https://en.wikipedia.org/wiki?curid=10935949", "title": "Outliers ratio", "text": "Outliers ratio\n\nIn objective video quality assessment, the outliers ratio (OR) is a measure of the performance of an objective video quality metric. It is the ratio of \"false\" scores given by the objective metric to the total number of scores. The \"false\" scores are the scores that lie outside the interval\n\nwhere MOS is the mean opinion score and \"σ\" is the standard deviation of the MOS.\n"}
{"id": "23912960", "url": "https://en.wikipedia.org/wiki?curid=23912960", "title": "Perspectivity", "text": "Perspectivity\n\nIn geometry and in its applications to drawing, a perspectivity is the formation of an image in a picture plane of a scene viewed from a fixed point.\n\nThe science of graphical perspective uses perspectivities to make realistic images in proper proportion. According to Kirsti Andersen, the first author to describe perspectivity was Leon Alberti in his \"De Pictura\" (1435). In English, Brook Taylor presented his \"Linear Perspective\" in 1715, where he explained \"Perspective is the Art of drawing on a Plane the Appearances of any Figures, by the Rules of Geometry\". In a second book, \"New Principles of Linear Perspective\" (1719), Taylor wrote\n\nIn projective geometry the points of a line are called a projective range, and the set of lines in a plane on a point is called a pencil.\n\nGiven two lines formula_1 and formula_2 in a plane and a point \"P\" of that plane on neither line, the bijective mapping between the points of the range of formula_1 and the range of formula_2 determined by the lines of the pencil on \"P\" is called a perspectivity (or more precisely, a \"central perspectivity\" with center \"P\"). A special symbol has been used to show that points \"X\" and \"Y\" are related by a perspectivity; formula_5 In this notation, to show that the center of perspectivity is \"P\", write formula_6 Using the language of functions, a central perspectivity with center \"P\" is a function formula_7 (where the square brackets indicate the projective range of the line) defined by formula_8. This map is an involution, that is, formula_9.\n\nThe existence of a perspectivity means that corresponding points are in perspective. The dual concept, \"axial perspectivity\", is the correspondence between the lines of two pencils determined by a projective range.\n\nThe composition of two perspectivities is, in general, not a perspectivity. A perspectivity or a composition of two or more perspectivities is called a projectivity (\"projective transformation\", \"projective collineation\" and \"homography\" are synonyms).\n\nThere are several results concerning projectivities and perspectivities which hold in any pappian projective plane:\n\nTheorem: Any projectivity between two distinct projective ranges can be written as the composition of no more than two perspectivities.\n\nTheorem: Any projectivity from a projective range to itself can be written as the composition of three perspectivities.\n\nTheorem: A projectivity between two distinct projective ranges which fixes a point is a perspectivity.\n\nThe bijective correspondence between points on two lines in a plane determined by a point of that plane not on either line has higher-dimensional analogues which will also be called perspectivities.\n\nLet \"S\" and \"T\" be two distinct \"m\"-dimensional projective spaces contained in an \"n\"-dimensional projective space \"R\". Let \"P\" be an (\"n\" − \"m\" − 1)-dimensional subspace of \"R\" with no points in common with either \"S\" or \"T\". For each point \"X\" of \"S\", the space \"L\" spanned by \"X\" and \"P\" meets \"T\" in a point . This correspondence \"f\" is also called a perspectivity. The central perspectivity described above is the case with and .\n\nLet \"S\" and \"T\" be two distinct projective planes in a projective 3-space \"R\". With \"O\" and \"O\"* being points of \"R\" in neither plane, use the construction of the last section to project \"S\" onto \"T\" by the perspectivity with center \"O\" followed by the projection of \"T\" back onto \"S\" with the perspectivity with center \"O\"*. This composition is a bijective map of the points of \"S\" onto itself which preserves collinear points and is called a \"perspective collineation\" (\"central collineation\" in more modern terminology). Let φ be a perspective collineation of \"S\". Each point of the line of intersection of \"S\" and \"T\" will be fixed by φ and this line is called the \"axis\" of φ. Let point \"P\" be the intersection of line \"OO\"* with the plane \"S\". \"P\" is also fixed by φ and every line of \"S\" that passes through \"P\" is stabilized by φ (fixed, but not necessarily pointwise fixed). \"P\" is called the \"center\" of φ. The restriction of φ to any line of \"S\" not passing through \"P\" is the central perspectivity in \"S\" with center \"P\" between that line and the line which is its image under φ.\n\n\n"}
{"id": "36003058", "url": "https://en.wikipedia.org/wiki?curid=36003058", "title": "Poisson scatter theorem", "text": "Poisson scatter theorem\n\nIn probability theory, The Poisson scatter theorem describes a probability model of random scattering. It implies that the number of points in a fixed region will follow a Poisson distribution.\n\nLet there exist a chance process realized by a set of points (called hits) over a bounded region formula_1 such that:\n\nIn any region \"B\", let \"N\" be the number of hits in \"B\". Then there exists a positive constant formula_4 such that for each subregion formula_5, \"N\" has a Poisson distribution with parameter formula_6, where formula_7 is the area of \"B\" (remember that this is formula_8, in other measure spaces, formula_7 could mean different things, i.e. length in formula_10). In addition, for any non-overlapping regions formula_11, the random variables formula_12 are independent from one another.\n\nThe positive constant formula_4 is called the intensity parameter, and is equivalent to the number of hits in a unit area of \"K\". \n\nAlso, \n\nWhile the statement of the theorem here is limited to formula_8, the theorem can be generalized to any-dimensional space. Some calculations change depending on the space that the points are scattered in (as is mentioned above), but the general assumptions and outcomes still hold.\n\nConsider raindrops falling on a rooftop. The rooftop is the region formula_1, while the raindrops can be considered the hits of our system. It is reasonable to assume that the number of raindrops that fall in any particular region of the rooftop follows a poisson distribution. The Poisson Scatter Theorem, states that if one was to subdivide the rooftops into \"k\" disjoint sub-regions, then the number of raindrops that hits a particular region formula_18 with intensity formula_19 of the rooftop is independent from the number of raindrops that hit any other subregion. Suppose that 2000 raindrops fall in 1000 subregions of the rooftop, randomly. The expected number of raindrops per subregion would be 2. So the distribution of the number of raindrops on the whole rooftop is Poisson with intensity parameter 2. The distribution of the number of raindrops falling on 1/5 of the rooftop is Poisson with intensity parameter 2/5. \n\nDue to the reproductive property of the Poisson distribution, k independent random scatters on the same region can superimpose to produce a random scatter that follows a poisson distribution with parameter formula_20.\n\n^ Pitman 2003, p. 230.\n\n"}
{"id": "5600755", "url": "https://en.wikipedia.org/wiki?curid=5600755", "title": "Quotition and partition", "text": "Quotition and partition\n\nIn arithmetic, quotition and partition are two ways of viewing fractions and division.\n\nIn quotition division one asks how many parts there are; in partition division one asks what the size of each part is.\n\nFor example, the expression\n\ncan be construed in either of two ways:\n\n\nIt is a fact of elementary theoretical mathematics that the numerical answer is always the same either way: 6 ÷ 2 = 3. This is essentially equivalent to the commutativity of multiplication.\n\nDivision involves thinking about a whole in terms of its parts. One frequent division notion, a natural number of equal parts, is known as \"partition\" to educators.\nThe basic concept behind partition is \"sharing\". In sharing a whole entity becomes an integer number of equal parts.\nWhat quotition concerns is explained by removing the word \"integer\" in the last sentence. Allow \"number\" to be \"any fraction\" and you have quotition instead of partition. \n\n\n\n"}
{"id": "5482655", "url": "https://en.wikipedia.org/wiki?curid=5482655", "title": "Refactorable number", "text": "Refactorable number\n\nA refactorable number or tau number is an integer \"n\" that is divisible by the count of its divisors, or to put it algebraically, \"n\" is such that formula_1. The first few refactorable numbers are listed in as\nFor example, 18 has 6 divisors (1 and 18, 2 and 9, 3 and 6) and is divisible by 6. There are infinitely many refactorable numbers.\n\nCooper and Kennedy proved that refactorable numbers have natural density zero. Zelinsky proved that no three consecutive integers can all be refactorable. Colton proved that no refactorable number is perfect. The equation GCD(\"n\", \"x\") = τ(\"n\") has solutions only if \"n\" is a refactorable number.\n\nLet formula_2 be the number of refactorable numbers which are at most \"x\". The problem of determining an asymptotic for formula_2 is open. Spiro has proven that formula_4. Zelinsky conjectures that formula_5.\n\nThere are still unsolved problems regarding refactorable numbers. Colton asked if there are there arbitrarily large \"n\" such that both \"n\" and \"n\" + 1 are refactorable. Zelinsky wondered if there exists a refactorable number formula_6, does there necessarily exist formula_7 such that \"n\" is refactorable and formula_8.\n\nFirst defined by Curtis Cooper and Robert E. Kennedy where they showed that the tau numbers has natural density zero, they were later rediscovered by Simon Colton using a computer program he had made which invents and judges definitions from a variety of areas of mathematics such as number theory and graph theory. Colton called such numbers \"refactorable\". While computer programs had discovered proofs before, this discovery was one of the first times that a computer program had discovered a new or previously obscure idea. Colton proved many results about refactorable numbers, showing that there were infinitely many and proving a variety of congruence restrictions on their distribution. Colton was only later alerted that Kennedy and Cooper had previously investigated the topic.\n\n"}
{"id": "10202429", "url": "https://en.wikipedia.org/wiki?curid=10202429", "title": "Relative change and difference", "text": "Relative change and difference\n\nIn any quantitative science, the terms relative change and relative difference are used to compare two quantities while taking into account the \"sizes\" of the things being compared. The comparison is expressed as a ratio and is a unitless number. By multiplying these ratios by 100 they can be expressed as percentages so the terms percentage change, percent(age) difference, or relative percentage difference are also commonly used. The distinction between \"change\" and \"difference\" depends on whether or not one of the quantities being compared is considered a \"standard\" or \"reference\" or \"starting\" value. When this occurs, the term \"relative change\" (with respect to the reference value) is used and otherwise the term \"relative difference\" is preferred. Relative difference is often used as a quantitative indicator of quality assurance and quality control for repeated measurements where the outcomes are expected to be the same. A special case of percent change (relative change expressed as a percentage) called \"percent error\" occurs in measuring situations where the reference value is the accepted or actual value (perhaps theoretically determined) and the value being compared to it is experimentally determined (by measurement).\n\nGiven two numerical quantities, \"x\" and \"y\", their \"difference\", , can be called their \"actual difference\". When \"y\" is a \"reference value\" (a theoretical/actual/correct/accepted/optimal/starting, etc. value; the value that \"x\" is being compared to) then Δ is called their \"actual change\". When there is no reference value, the sign of Δ has little meaning in the comparison of the two values since it doesn't matter which of the two values is written first, so one often works with , the absolute difference instead of Δ, in these situations. Even when there is a reference value, if it doesn't matter whether the compared value is larger or smaller than the reference value, the absolute difference can be considered in place of the actual change.\n\nThe absolute difference between two values is not always a good way to compare the numbers. For instance, the absolute difference of 1 between 6 and 5 is more significant than the same absolute difference between 100,000,001 and 100,000,000. We can adjust the comparison to take into account the \"size\" of the quantities involved, by defining, for positive values of \"x\":\n\nThe relative change is not defined if the reference value (\"x\") is zero.\n\nFor values greater than the reference value, the relative change should be a positive number and for values that are smaller, the relative change should be negative. The formula given above behaves in this way only if \"x\" is positive, and reverses this behavior if \"x\" is negative. For example, if we are calibrating a thermometer which reads −6 °C when it should read −10 °C, this formula for relative change (which would be called \"relative error\" in this application) gives , yet the reading is too high. To fix this problem we alter the definition of relative change so that it works correctly for all nonzero values of \"x\":\n\nIf the relationship of the value with respect to the reference value (that is, larger or smaller) does not matter in a particular application, the absolute difference may be used in place of the actual change in the above formula to produce a value for the relative change which is always non-negative.\n\nDefining relative difference is not as easy as defining relative change since there is no \"correct\" value to scale the absolute difference with. As a result, there are many options for how to define relative difference and which one is used depends on what the comparison is being used for. In general we can say that the absolute difference is being scaled by some function of the values \"x\" and \"y\", say .\n\nAs with relative change, the relative difference is undefined if is zero.\n\nSeveral common choices for the function would be:\n\nMeasures of relative difference are unitless numbers expressed as a fraction. Corresponding values of percent difference would be obtained by multiplying these values by 100 (and appending the % sign to indicate that the value is a percentage).\n\nOne way to define the relative difference of two numbers is to take their absolute difference divided by the maximum absolute value of the two numbers.\n\nif at least one of the values does not equal zero. This approach is especially useful when comparing floating point values in programming languages for equality with a certain tolerance. Another application is in the computation of approximation errors when the relative error of a measurement is required.\n\nAnother way to define the relative difference of two numbers is to take their absolute difference divided by some functional value of the two numbers, for example, the absolute value of their arithmetic mean:\n\nThis approach is often used when the two numbers reflect a change in some single underlying entity. A problem with the above approach arises when the functional value is zero. In this example, if x and y have the same magnitude but opposite sign, then\nwhich causes division by 0. So it may be better to replace the denominator with the average of the absolute values of \"x\" and \"y\":\n\nPercent Error is a special case of the percentage form of relative change calculated from the absolute change between the experimental (measured) and theoretical (accepted) values, and dividing by the theoretical (accepted) value.\n\nThe terms \"Experimental\" and \"Theoretical\" used in the equation above are commonly replaced with similar terms. Other terms used for \"experimental\" could be \"measured,\" \"calculated,\" or \"actual\" and another term used for \"theoretical\" could be \"accepted.\" Experimental value is what has been derived by use of calculation and/or measurement and is having its accuracy tested against the theoretical value, a value that is accepted by the scientific community or a value that could be seen as a goal for a successful result.\n\nAlthough it is common practice to use the absolute value version of relative change when discussing percent error, in some situations, it can be beneficial to remove the absolute values to provide more information about the result. Thus, if an experimental value is less than the theoretical value, the percent error will be negative. This negative result provides additional information about the experimental result. For example, experimentally calculating the speed of light and coming up with a negative percent error says that the experimental value is a velocity that is less than the speed of light. This is a big difference from getting a positive percent error, which means the experimental value is a velocity that is greater than the speed of light (violating the theory of relativity) and is a newsworthy result.\n\nThe percent error equation, when rewritten by removing the absolute values, becomes:\n\nIt is important to note that the two values in the numerator do not commute. Therefore, it is vital to preserve the order as above: subtract the theoretical value from the experimental value and not vice versa.\n\nA percentage change is a way to express a change in a variable. It represents the relative change between the old value and the new one.\n\nFor example, if a house is worth $100,000 today and the year after its value goes up to $110,000, the percentage change of its value can be expressed as\n\nIt can then be said that the worth of the house went up by 10%.\n\nMore generally, if \"V\" represents the old value and \"V\" the new one,\n\nSome calculators directly support this via a or function.\n\nWhen the variable in question is a percentage itself, it is better to talk about its change by using percentage points, to avoid confusion between relative difference and absolute difference.\n\nIf a bank were to raise the interest rate on a savings account from 3% to 4%, the statement that \"the interest rate was increased by 1%\" is ambiguous and should be avoided. The absolute change in this situation is 1 percentage point (4% − 3%), but the relative change in the interest rate is:\nSo, one should say either that the interest rate was increased by 1 percentage point, or that the interest rate was increased by formula_13\n\nIn general, the term \"percentage point(s)\" indicates an absolute change or difference of percentages, while the percent sign or the word \"percentage\" refers to the relative change or difference.\n\nCar \"M\" costs $50,000 and car \"L\" costs $40,000. We wish to compare these costs. With respect to car \"L\", the absolute difference is . That is, car \"M\" costs $10,000 more than car \"L\". The relative difference is,\nand we say that car \"M\" costs 25% \"more than\" car \"L\". It is also common to express the comparison as a ratio, which in this example is,\nand we say that car \"M\" costs 125% \"of\" the cost of car \"L\".\n\nIn this example the cost of car \"L\" was considered the reference value, but we could have made the choice the other way and considered the cost of car \"M\" as the reference value. The absolute difference is now since car \"L\" costs $10,000 less than car \"M\". The relative difference,\nis also negative since car \"L\" costs 20% \"less than\" car \"M\". The ratio form of the comparison,\nsays that car \"L\" costs 80% \"of\" what car \"M\" costs.\n\nIt is the use of the words \"of\" and \"less/more than\" that distinguish between ratios and relative differences. \n\nChange in a quantity can also be expressed logarithmically using the unit of logarithmic change: the decibel and the neper (Np). Normalization with a factor of 100, as done for percent, yields the derived unit (cNp), which aligns with the definition for percentage change for very small changes:\n\nUsing cNp has two advantages. First, the magnitude of the change expressed in cNp is the same whether \"V\" or \"V\" is chosen as the reference, since formula_19. In contrast, formula_20, with the approximation error becoming more significant as \"V\" and \"V\" diverge. For example:\nThe second advantage is that the total change after a series of cNp changes equals the sum of the changes. With percent, summing the changes is only an approximation, with larger error for larger changes. For example:\n\n\n\n"}
{"id": "19186114", "url": "https://en.wikipedia.org/wiki?curid=19186114", "title": "Roger Wolcott Richardson", "text": "Roger Wolcott Richardson\n\nRoger Wolcott Richardson (30 May 1930 – 15 June 1993) was a mathematician noted for his work in representation theory and geometry. He was born in Baton Rouge, Louisiana, and educated at Louisiana State University, Harvard University and University of Michigan, Ann Arbor where he obtained a Ph.D. under the supervision of Hans Samelson. He emigrated in 1970, and subsequently worked in the United Kingdom and Australia.\n\nRichardson’s best known result states that if \"P\" is a parabolic subgroup of a reductive group, then \"P\" has a dense orbit on its nilradical, i.e., one whose closure is the whole space. This orbit is now universally known as the Richardson orbit.\n\n\n"}
{"id": "40913321", "url": "https://en.wikipedia.org/wiki?curid=40913321", "title": "Sphere packing in a cube", "text": "Sphere packing in a cube\n\nIn geometry, sphere packing in a cube is a three-dimensional sphere packing problem with the objective of packing spheres inside a cube. It is the three-dimensional equivalent of the circle packing in a square problem in two dimensions. The problem consists of determining the optimal packing of a given number of spheres inside the cube.\n\n"}
{"id": "58216843", "url": "https://en.wikipedia.org/wiki?curid=58216843", "title": "Spyros Magliveras", "text": "Spyros Magliveras\n\nSpyros Simos Magliveras (born 6 September 1938 in Athens) is a Greek-born American mathematician and computer scientist.\n\nMagliveras graduated from the University of Florida with a bachelor's degree in electrical engineering in 1961 and a master's degree in mathematics in 1963. He was from 1963 to 1964 an instructor of mathematics at Florida Presbyterian College and from 1964 to 1968 a teaching fellow in mathematics at the University of Michigan, as well as from 1965 to 1968 a programming analyst and a systems analyst at the University of Michigan Institute for Social Research. He received his PhD in mathematics from the University of Birmingham, UK in 1970 with thesis advisor Donald Livingstone and thesis \"The subgroup structure of the Higman-Sims simple group\".\n\nAt the State University of New York at Oswego, he was from 1970 to 1973 an assistant professor and from 1973 to 1978 an associate professor. From 1978 to 2000 he was a professor of mathematics and computer science at the University of Nebraska-Lincoln, retiring as professor emeritus in 2000. Since 2000 he is a professor at Florida Atlantic University (FAU). He was from 2003 to 2013 the director of FAU's Center for Cryptology and Information Security and is, from 2013 to the present, the Center's associate director.\n\nHe has been a visiting professor at the University of Birmingham (1984/85), at the University of Waterloo (1999), at the Sapienza University of Rome (two months in 2000), and at the University of Western Australia (two months in 2000).\n\nMagliveras does research on combinatorial designs, permutation groups, finite geometries, encryption of data (cryptography), and data security. In 2001 he received the Euler Medal. He is a co-author of the 2007 book \"Secure group communications over data networks\".\n\nMagliveras is married since 1962 and has two children.\n\n"}
{"id": "11479157", "url": "https://en.wikipedia.org/wiki?curid=11479157", "title": "Statistical conclusion validity", "text": "Statistical conclusion validity\n\nStatistical conclusion validity is the degree to which conclusions about the relationship among variables based on the data are correct or ‘reasonable’. This began as being solely about whether the statistical conclusion about the relationship of the variables was correct, but now there is a movement towards moving to ‘reasonable’ conclusions that use: quantitative, statistical, and qualitative data.\nFundamentally, two types of errors can occur: type I (finding a difference or correlation when none exists) and type II (finding no difference when one exists). Statistical conclusion validity concerns the qualities of the study that make these types of errors more likely.\nStatistical conclusion validity involves ensuring the use of adequate sampling procedures, appropriate statistical tests, and reliable measurement procedures. \n\nThe most common threats to statistical conclusion validity are:\n\nPower is the probability of correctly rejecting the null hypothesis when it is false (inverse of the type II error rate). Experiments with low power have a higher probability of incorrectly accepting the null hypothesis—that is, committing a type II error and concluding that there is no effect when there actually is (I.e. there is real covariation between the cause and effect). Low power occurs when the sample size of the study is too small given other factors (small effect sizes, large group variability, unreliable measures, etc.).\n\nMost statistical tests (particularly inferential statistics) involve assumptions about the data that make the analysis suitable for testing a hypothesis. Violating the assumptions of statistical tests can lead to incorrect inferences about the cause-effect relationship. The robustness of a test indicates how sensitive it is to violations. Violations of assumptions may make tests more or less likely to make type I or II errors.\n\nEach hypothesis testing involves a set risk of a type I error (the alpha rate). If a researcher searches or \"fishes\" through their data, testing many different hypotheses to find a significant effect, they are inflating their type I error rate. The more the researcher repeatedly tests the data, the higher the chance of observing a type I error and making an incorrect inference about the existence of a relationship.\n\nIf the dependent and/or independent variable(s) are not measured reliably (i.e., with large amounts of measurement error), incorrect conclusions can be drawn.\n\nRestriction of range, such as floor and ceiling effects or selection effects, reduce the power of the experiment, and increase the chance of a type II error. This is because correlations are attenuated (weakened) by reduced variability (see, for example, the equation for the Pearson product-moment correlation coefficient which uses score variance in its estimation).\n\nGreater heterogeneity of individuals participating in the study can also impact interpretations of results by increasing the variance of results or obscuring true relationships (see also sampling error. , the higher the standard deviation will be. This obscures possible interactions between the characteristics of the units and the cause-effect relationship.\n\nAny effect that can impact the internal validity of a research study may bias the results and impact the validity of statistical conclusions reached. These threats to internal validity include unreliability of treatment implementation (lack of standardization) or failing to control for extraneous variables.\n\n"}
{"id": "3925287", "url": "https://en.wikipedia.org/wiki?curid=3925287", "title": "Sudan function", "text": "Sudan function\n\nIn the theory of computation, the Sudan function is an example of a function that is recursive, but not primitive recursive. This is also true of the better-known Ackermann function. The Sudan function was the first function having this property to be published.\n\nIt was discovered (and published) in 1927 by Gabriel Sudan, a Romanian mathematician who was a student of David Hilbert.\n\nIn general, \"F\"(\"x\", \"y\") is equal to \"F\"(0, \"y\") + 2 \"x\".\n\n"}
{"id": "3422168", "url": "https://en.wikipedia.org/wiki?curid=3422168", "title": "Traian Lalescu", "text": "Traian Lalescu\n\nTraian Lalescu (; 12 July 1882 – 15 June 1929) was a Romanian mathematician. His main focus was on integral equations and he contributed to work in the areas of functional equations, trigonometric series, mathematical physics, geometry, mechanics, algebra, and the history of mathematics.\n\nHe went to the Carol I High School in Craiova, continuing high school in Roman, and graduating from the Boarding High School in Iași. After entering the University of Iași, he completed his undergraduate studies in 1903 at the University of Bucharest.\n\nHe earned his Ph.D. in Mathematics from the University of Paris in 1908. His dissertation, \"Sur les équations de Volterra\", was written under the direction of Émile Picard. In 1911, he published \"Introduction to the Theory of Integral Equations\", the first book ever on the subject of integral equations.\n\nHe was a professor at the University of Bucharest, the Polytechnic University of Timișoara (where he was the first rector, in 1920), and the Polytechnic University of Bucharest.\n\nThere are several institutions bearing his name, including Colegiul Naţional de Informatică \"Traian Lalescu\" in Hunedoara and Liceul Teoretic \"Traian Lalescu\" in Reşiţa. There is also a Traian Lalescu Street in Timişoara. \nThe National Mathematics Contest \"Traian Lalescu\" for undergraduate students is also named after him.\n\nA statue of Lalescu, carved in 1930 by Cornel Medrea, is situated in front of the Faculty of Mechanical Engineering, in Timişoara.\n\n\n"}
{"id": "9322242", "url": "https://en.wikipedia.org/wiki?curid=9322242", "title": "Whitney conditions", "text": "Whitney conditions\n\nIn differential topology, a branch of mathematics, the Whitney conditions are conditions on a pair of submanifolds of a manifold introduced by Hassler Whitney in 1965. \n\nA stratification of a topological space is a finite filtration by closed subsets \"F\" , such that the difference between successive members \"F\" and \"F\" of the filtration is either empty or a smooth submanifold of dimension \"i\". The connected components of the difference \"F\" − \"F\" are the strata of dimension \"i\". A stratification is called a Whitney stratification if all pairs of strata satisfy the Whitney conditions A and B, as defined below.\n\nLet \"X\" and \"Y\" be two disjoint locally closed submanifolds of R, of dimensions \"i\" and \"j\".\n\n\nJohn Mather first pointed out that \"Whitney's condition B\" implies \"Whitney's condition A\" in the notes of his lectures at Harvard in 1970, which have been widely distributed. He also defined the notion of Thom–Mather stratified space, and proved that every Whitney stratification is a Thom–Mather stratified space and hence is a topologically stratified space. Another approach to this fundamental result was given earlier by René Thom in 1969.\n\nDavid Trotman showed in his 1978 Warwick thesis that a stratification of a closed subset in a smooth manifold \"M\" satisfies \"Whitney's condition A\" if and only if the subspace of the space of smooth mappings from a smooth manifold \"N\" into \"M\" consisting of all those maps which are transverse to all of the strata of the stratification, is open (using the Whitney, or strong, topology). The subspace of mappings transverse to any countable family of submanifolds of \"M\" is always dense by Thom's transversality theorem. The density of the set of transverse mappings is often interpreted by saying that transversality is a 'generic' property for smooth mappings, while the openness is often interpreted by saying that the property is 'stable'.\n\nThe reason that Whitney conditions have become so widely used is because of Whitney's 1965 theorem that every algebraic variety, or indeed analytic variety, admits a Whitney stratification, i.e. admits a partition into smooth submanifolds satisfying the Whitney conditions. More general singular spaces can be given Whitney stratifications, such as semialgebraic sets (due to René Thom) and subanalytic sets (due to Heisuke Hironaka). This has led to their use in engineering, control theory and robotics. In a thesis under the direction of Wieslaw Pawlucki at the Jagellonian University in Kraków, Poland, the Vietnamese mathematician Ta Lê Loi proved further that every definable set in an o-minimal structure can be given a Whitney stratification.\n\n\n"}
{"id": "15272567", "url": "https://en.wikipedia.org/wiki?curid=15272567", "title": "Yangian", "text": "Yangian\n\nIn representation theory, a Yangian is an infinite-dimensional Hopf algebra, a type of a quantum group. Yangians first appeared in physics in the work of Ludvig Faddeev and his school in the late 1970s and early 1980s concerning the quantum inverse scattering method. The name \"Yangian\" was introduced by Vladimir Drinfeld in 1985 in honor of C.N. Yang.\n\nInitially, they were considered a convenient tool to generate the solutions of the quantum Yang–Baxter equation.\n\nThe center of Yangian can be described by quantum determinant.\n\nFor any finite-dimensional semisimple Lie algebra \"a\", Drinfeld defined an infinite-dimensional Hopf algebra \"Y\"(\"a\"), called the Yangian of \"a\". This Hopf algebra is a deformation of the universal enveloping algebra \"U\"(\"a\"[\"z\"]) of the Lie algebra of polynomial loops of \"a\" given by explicit generators and relations. The relations can be encoded by identities involving a rational \"R\"-matrix. Replacing it with a trigonometric \"R\"-matrix, one arrives at affine quantum groups, defined in the same paper of Drinfeld.\n\nIn the case of the general linear Lie algebra \"gl\", the Yangian admits a simpler description in terms of a single \"ternary\" (or \"RTT\") \"relation\" on the matrix generators due to Faddeev and coauthors. \nThe Yangian Y(\"gl\") is defined to be the algebra generated by elements formula_1 with 1 ≤ \"i\", \"j\" ≤ \"N\" and \"p\" ≥ 0, subject to the relations\n\nDefining formula_3, setting\n\nand introducing the R-matrix \"R\"(\"z\") = I + \"z\" \"P\" on Cformula_5C,\nwhere \"P\" is the operator permuting the tensor factors, the above relations can be written more simply as the ternary relation:\n\nThe Yangian becomes a Hopf algebra with comultiplication Δ, counit ε and antipode \"s\" given by\n\nAt special values of the spectral parameter formula_8, the \"R\"-matrix degenerates to a rank one projection. This can be used to define the quantum determinant of formula_9, which generates the center of the Yangian.\n\nThe twisted Yangian Y(\"gl\"), introduced by G. I. Olshansky, is the co-ideal generated by the coefficients of\n\nwhere σ is the involution of \"gl\" given by\n\nQuantum determinant is the center of Yangian.\n\nG.I. Olshansky and I.Cherednik discovered that the Yangian of \"gl\" is closely related with the branching properties of irreducible finite-dimensional representations of general linear algebras. In particular, the classical Gelfand–Tsetlin construction of a basis in the space of such a representation has a natural interpretation in the language of Yangians, studied by M.Nazarov and V.Tarasov. Olshansky, Nazarov and Molev later discovered a generalization of this theory to other classical Lie algebras, based on the twisted Yangian.\n\nThe Yangian appears as a symmetry group in different models in physics.\n\nYangian appears as a symmetry group of one-dimensional exactly solvable models such as spin chains, Hubbard model and in models of one-dimensional relativistic quantum field theory.\n\nThe most famous occurrence is in planar supersymmetric Yang–Mills theory in four dimensions, where Yangian structures appear on the level of symmetries of operators, and scattering amplitude as was discovered by Drummond, Henn and Plefka.\n\nIrreducible finite-dimensional representations of Yangians were parametrized by Drinfeld in a way similar to the highest weight theory in the representation theory of semisimple Lie algebras. The role of the highest weight is played by a finite set of \"Drinfeld polynomials\". Drinfeld also discovered a generalization of the classical Schur–Weyl duality between representations of general linear and symmetric groups that involves the Yangian of \"sl\" and the degenerate affine Hecke algebra (graded Hecke algebra of type A, in George Lusztig's terminology).\n\nRepresentations of Yangians have been extensively studied, but the theory is still under active development.\n\n\n"}
