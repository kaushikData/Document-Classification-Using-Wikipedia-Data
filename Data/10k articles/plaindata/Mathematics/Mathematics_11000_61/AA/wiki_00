{"id": "407371", "url": "https://en.wikipedia.org/wiki?curid=407371", "title": "110 (number)", "text": "110 (number)\n\n110 (\"one hundred [and] ten\") is the natural number following 109 and preceding 111.\n\n110 is a sphenic number and a pronic number. Following the prime quadruplet (101, 103, 107, 109), at 110, the Mertens function reaches a low of −5.\n\n110 is the sum of three consecutive squares, formula_1.\n\n110 is the side of the smallest square that can be tiled with distinct integer-sided squares.\n\nRSA-110 is one of the RSA numbers, large semiprimes that are part of the RSA Factoring Challenge.\n\nThe Rule 110 cellular automaton, like Conway's Game of Life, exhibits what Stephen Wolfram calls \"Class 4 behavior,\" which is neither completely random nor completely repetitive.\n\nIn base 10, the number 110 is a Harshad number and a self number.\n\n\n\nOlympic male track and field athletics run 110 metre hurdles. (Female athletes run the 100 metre hurdles instead,)\n\nThe International 110, or the 110, is a one-design racing sailboat designed in 1939 by C. Raymond Hunt.\n\n110 is also:\n\n"}
{"id": "4956726", "url": "https://en.wikipedia.org/wiki?curid=4956726", "title": "Acta Arithmetica", "text": "Acta Arithmetica\n\nActa Arithmetica is a scientific journal of mathematics publishing papers on number theory. It was established in 1935 by Salomon Lubelski and Arnold Walfisz. The journal is published by the Institute of Mathematics of the Polish Academy of Sciences.\n\n"}
{"id": "56288604", "url": "https://en.wikipedia.org/wiki?curid=56288604", "title": "Anne Greenbaum", "text": "Anne Greenbaum\n\nAnne Greenbaum (born 1951) is an American applied mathematician and professor at the University of Washington. She was named a SIAM Fellow in 2015 \"for contributions to theoretical and numerical linear algebra\". She has written graduate and undergraduate textbooks on numerical methods.\n\nGreenbaum received her bachelor's degree from the University of Michigan in 1974. She earned her PhD from the University of California, Berkeley in 1981.\n\nAfter receiving her bachelor's degree, Greenbaum worked for the Lawrence Livermore National Laboratory. She joined the Courant Institute of Mathematical Sciences in 1986, and moved to the University of Washington in 1998.\n\nGreenbaum received a Best Paper Prize from the SIAM Activity Group on Linear Algebra in 1994, together with Roland Freund, Noel Nachtigal, and Zdenek Strakos. She received the Bernard Bolzano Honorary Medal for Merit in the Mathematical Sciences from the Czech Academy of Sciences in 1997. She became a SIAM Fellow in 2015.\n"}
{"id": "31151470", "url": "https://en.wikipedia.org/wiki?curid=31151470", "title": "Bennett's inequality", "text": "Bennett's inequality\n\nIn probability theory, Bennett's inequality provides an upper bound on the probability that the sum of independent random variables deviates from its expected value by more than any specified amount. Bennett's inequality was proved by George Bennett of the University of New South Wales in 1962.\n\nLet \n\nbe independent random variables with finite variance and assume (for simplicity but without loss of generality) they all have zero expected value. Further assume almost surely for all , and define formula_1 and formula_2 \nThen for any ,\n\nwhere .\n\nFor generalizations see Freedman (1975) and Fan, Grama and Liu (2012) for a martingale version of Bennett's inequality and its improvement, respectively.\n\nHoeffding's inequality only assumes the summands are bounded almost surely, while Bennett's inequality offers some improvement when the variances of the summands are small compared to their almost sure bounds. However Hoeffding's inequality entails sub-Gaussian tails, whereas in general Bennett's inequality has Poissonian tails.\nIn both inequalities, unlike some other inequalities or limit theorems, there is no requirement that the component variables have identical or similar distributions.\n\n"}
{"id": "917006", "url": "https://en.wikipedia.org/wiki?curid=917006", "title": "Bijective proof", "text": "Bijective proof\n\nIn combinatorics, bijective proof is a proof technique that finds a bijective function \"f\" : \"A\" → \"B\" between two finite sets \"A\" and \"B\", or a size-preserving bijective function between two combinatorial classes, thus proving that they have the same number of elements, |\"A\"| = |\"B\"|. One place the technique is useful is where we wish to know the size of \"A\", but can find no direct way of counting its elements. Then establishing a bijection from \"A\" to some \"B\" solves the problem in the case when \"B\" is more easily countable. Another useful feature of the technique is that the nature of the bijection itself often provides powerful insights into each or both of the sets.\n\nThe symmetry of the binomial coefficients states that\n\nThis means there are exactly as many combinations of \"k\" in a set of \"n\" as there are combinations of \"n\" − \"k\" in a set of \"n\".\n\nThe key idea of the proof may be understood from a simple example: selecting out of a group of \"n\" children which \"k\" to reward with ice cream cones has exactly the same effect as choosing instead the \"n\" − \"k\" children to be denied them. More abstractly and generally, we note that the two quantities asserted to be equal count the subsets of size \"k\" and \"n\" − \"k\", respectively, of any \"n\"-element set \"S\". There is a simple bijection between the two families \"F\" and \"F\" of subsets of \"S\": it associates every \"k\"-element subset with its complement, which contains precisely the remaining \"n\" − \"k\" elements of \"S\". Since \"F\" and \"F\" have the same number of elements, the corresponding binomial coefficients must be equal.\n\n\"Proof\".\nWe count the number of ways to choose \"k\" elements from an \"n\"-set.\nAgain, by definition, the left hand side of the equation is the number of ways to choose \"k\" from \"n\".\nSince 1 ≤ \"k\" ≤ \"n\" − 1, we can pick a fixed element \"e\" from the \"n\"-set so that the remaining subset is not empty.\nFor each \"k\"-set, if \"e\" is chosen, there are\n\nways to choose the remaining \"k\" − 1 elements among the remaining \"n\" − 1 choices; otherwise, there are \n\nways to choose the remaining \"k\" elements among the remaining \"n\" − 1 choices. Thus, there are \n\nways to choose \"k\" elements depending on whether \"e\" is included in each selection, as in the right hand side expression. formula_6\n\nProblems that admit combinatorial proofs are not limited to binomial coefficient identities. As the complexity of the problem increases, a combinatorial proof can become very sophisticated. This technique is particularly useful in areas of discrete mathematics such as combinatorics, graph theory, and number theory.\n\nThe most classical examples of bijective proofs in combinatorics include:\n\n\n\n"}
{"id": "56181508", "url": "https://en.wikipedia.org/wiki?curid=56181508", "title": "Chaim Goodman-Strauss", "text": "Chaim Goodman-Strauss\n\nChaim Goodman-Strauss (born June 22,1967 in Austin TX) is an American mathematician who works in convex geometry, especially aperiodic tiling. He is on the faculty of the University of Arkansas and is a co-author with John H. Conway of \"The Symmetries of Things\", a comprehensive book surveying the mathematical theory of patterns.\n\nGoodman-Strauss received both his B.S. (1988) and Ph.D. (1994) in mathematics from the University of Texas at Austin. His doctoral advisor was John Edwin Luecke. He joined the faculty at the University of Arkansas, Fayetteville (UA) in 1994 and served as departmental chair from 2008 to 2015. He held visiting positions at the National Autonomous University of Mexico and at Princeton University.\n\nDuring 1995 he did research at The Geometry Center, a mathematics research and education center at the University of Minnesota, where he investigated aperiodic tilings of the plane.\n\nGoodman-Strauss has been fascinated by patterns and mathematical paradoxes for as long as he can remember. He attended a lecture about the mathematician Georg Cantor when he was 17 and says, \"I was already doomed to be a mathematician, but that lecture sealed my fate.\" He became a mathematics writer and popularizer. From 2004 to 2012, in conjunction with KUAF 91.3 FM, the University of Arkansas NPR affiliate, he presented ”The Math Factor\", a podcast website dealing with recreational mathematics. He is an admirer of Martin Gardner and is on the Math & Science Advisory Council of Gathering 4 Gardner, an organization that celebrates the legacy of the famed mathematics popularizer and Scientific American columnist, and is particularly active in the associated Celebration of Mind events.\n\nIn 2008 Goodman-Strauss teamed up with J. H. Conway and Heidi Burgiel to write \"The Symmetries of Things\", an exhaustive and reader accessible overview of the mathematical theory of patterns. He produced hundreds of full-color images for this book using software that he developed for the purpose. The Mathematical Association of America said, \"The first thing one notices when one picks up a copy … is that it is a beautiful book … filled with gorgeous color pictures … many of which were generated by Goodman-Strauss. Unlike some books which add in illustrations to keep the reader's attention, the pictures are genuinely essential to the topic of this book.\"\n\nHe also creates large-scale sculptures inspired by mathematics, and some of these have been featured at Gathering 4 Gardner conferences.\n\n\n\n"}
{"id": "40158560", "url": "https://en.wikipedia.org/wiki?curid=40158560", "title": "Chebyshev integral", "text": "Chebyshev integral\n\nIn mathematics, the Chebyshev integral, named after Pafnuty Chebyshev, is\n\nwhere formula_2 is an incomplete beta function.\n"}
{"id": "33193885", "url": "https://en.wikipedia.org/wiki?curid=33193885", "title": "Classical Mechanics (Kibble and Berkshire book)", "text": "Classical Mechanics (Kibble and Berkshire book)\n\nClassical Mechanics (5th ed.) is a well-established textbook written by Thomas Walter Bannerman Kibble, FRS, (born 1932) and Frank Berkshire of the Imperial College Mathematics Department. The book provides a thorough coverage of the fundamental principles and techniques of classical mechanics, a long-standing subject which is at the base of all of physics.\n\nThe English language editions were published as follows:\nThe first edition was published by Kibble, as Kibble, T. W. B. \"Classical Mechanics.\" London: McGraw–Hill, 1966. 296 p. \nThe second ed., also just by Kibble, was in 1973 . \nThe 4th, jointly with F H Berkshire, was is 1996\nThe 5th, jointly with F H Berkshire, in 2004\n\nThe book has been translated into several languages: \n\nThe various editions are held in 1789 libraries.\nIn comparison, the various (2011) editions of Herbert Goldstein's \"Classical Mechanics\" are held in 1772. libraries\n\nThe fourth edition was reviewed by C. Isenberg in 1997 in the \"European Journal of Physics\". It was also reviewed in \"New Scientist\" and \"Contemporary Physics\".\n\n\n"}
{"id": "2737824", "url": "https://en.wikipedia.org/wiki?curid=2737824", "title": "Clélie", "text": "Clélie\n\nIn mathematics, a Clélie or Clelia curve is a curve parameterized by\n\nA curve of this type is confined to the surface of a sphere of radius formula_4, as is obvious from the parametric equations for the surface of a sphere. When formula_5, this is Viviani's curve.\n\nThe curve was named by Luigi Guido Grandi after Clelia Borromeo.\n"}
{"id": "55268440", "url": "https://en.wikipedia.org/wiki?curid=55268440", "title": "Coherent algebra", "text": "Coherent algebra\n\nA coherent algebra is an algebra of complex square matrices that is closed under ordinary matrix multiplication, Schur product, transposition, and contains both the identity matrix formula_1 and the all-ones matrix formula_2.\n\nA subspace formula_3 of formula_4 is said to be a coherent algebra of order formula_5 if:\nA coherent algebra formula_12 is said to be:\nThe set formula_16 of \"Schur-primitive matrices\" in a coherent algebra formula_3 is defined as formula_18.\n\nDually, the set formula_19 of \"primitive matrices\" in a coherent algebra formula_3 is defined as formula_21.\n\n\n\n"}
{"id": "22756507", "url": "https://en.wikipedia.org/wiki?curid=22756507", "title": "Conley index theory", "text": "Conley index theory\n\nIn dynamical systems theory, Conley index theory, named after Charles Conley, analyzes topological structure of invariant sets of diffeomorphisms and of smooth flows. It is a far-reaching generalization of the Hopf index theorem that predicts existence of fixed points of a flow inside a planar region in terms of information about its behavior on the boundary. Conley's theory is related to Morse theory, which describes the topological structure of a closed manifold by means of a nondegenerate gradient vector field. It has an enormous range of applications to the study of dynamics, including existence of periodic orbits in Hamiltonian systems and travelling wave solutions for partial differential equations, structure of global attractors for reaction-diffusion equations and delay differential equations, proof of chaotic behavior in dynamical systems, and bifurcation theory. Conley index theory formed the basis for development of Floer homology.\n\nA key role in the theory is played by the notions of isolating neighborhood \"N\" and isolated invariant set \"S\". The Conley index \"h\"(\"S\") is the homotopy type of a certain pair (\"N\", \"N\") of compact subsets of \"N\", called an index pair. Charles Conley showed that index pairs exist and that the index of \"S\" is independent of the choice of an isolated neighborhood \"N\" and the index pair. In the special case of the negative gradient flow to a smooth function, the Conley index of a nondegenerate (Morse) critical point of index \"k\" is the pointed homotopy type of the \"k\"-sphere \"S\".\n\nA deep theorem due to Conley asserts continuation invariance: Conley index is invariant under certain deformations of the dynamical system. Computation of the index can, therefore, be reduced to the case of the diffeomorphism or a vector field whose invariant sets are well understood. \n\nIf the index is nontrivial then the invariant set \"S\" is nonempty. This principle can be amplified to establish existence of fixed points and periodic orbits inside \"N\".\n\n\n"}
{"id": "25067587", "url": "https://en.wikipedia.org/wiki?curid=25067587", "title": "David Shmoys", "text": "David Shmoys\n\nDavid Bernard Shmoys (born 1959) is a Professor in the School of Operations Research and Information Engineering and the Department of Computer Science at Cornell University. He obtained his Ph.D. from the University of California, Berkeley in 1984. His major focus has been in the design and analysis of algorithms for discrete optimization problems.\n\nIn particular, his work has highlighted the role of linear programming in the design of approximation algorithms for NP-hard problems. He is known for his pioneering research on providing first constant factor performance guarantee for several scheduling and clustering problems including the k-center and k-median problems and the generalized assignment problem. Polynomial-time approximation schemes that he developed for scheduling problems have found applications in many subsequent works. His current research includes stochastic optimization, computational sustainability and optimization techniques in computational biology. Shmoys is married to Éva Tardos, who is the Jacob Gould Schurman Professor of Computer Science at Cornell University.\n\nTwo of his key contributions are\n\n\n\nThese contributions are described briefly below:\n\nThe paper is a joint work by David Shmoys and Eva Tardos.\n\nThe generalized assignment problem can be viewed as the following problem of scheduling unrelated parallel machine with costs.\nEach of formula_1 independent jobs (denoted formula_2) have to be processed by exactly one of formula_3 unrelated parallel machines (denoted formula_4). Unrelated implies same job might take different amount of processing time on different machines. Job formula_5 takes formula_6 time units when processed by machine formula_7 and incurs a cost formula_8. Given formula_9 and formula_10, we wish to decide if there exists a schedule with total cost at most formula_9 such that for each machine formula_7 its load, the total processing time required for the jobs assigned to it is at most formula_10. By scaling the processing times, we can assume, without loss of generality, that the machine load bounds satisfy formula_14. ``In other words, generalized assignment problem is to find a schedule of minimum cost subject to the constraint that the makespan, that the maximum machine load is at most formula_15 \".\n\nThe work of Shmoys with Lenstra and Tardos cited here \ngives a 2 approximation algorithm for the unit cost case. The algorithm is based on a clever design of linear program using parametric pruning and then rounding an extreme point solution of the linear program deterministically. Algorithm for the\ngeneralized assignment problem is based on a similar LP through parametric pruning and then using a new rounding technique on a carefully designed bipartite graph. We now state the LP formulation and briefly describe the rounding technique.\n\nWe guess the optimum value of makespan formula_15 and write the following LP. This technique is known as parametric pruning.\n\nformula_17;\n\nThe obtained LP solution is then rounded to an integral solution as follows. A weighted bipartite graph \nformula_22 is constructed. One side of the bipartite graph contains the job nodes, formula_23, and the other side contains several copies of each machine node, \nformula_24, where formula_25. To construct the edges to machine nodes corresponding to say machine formula_7, first jobs are arranged in decreasing order of processing time formula_27. For simplicity, suppose, formula_28. Now find the minimum index formula_29, such that \nformula_30. Include in formula_31 all the edges formula_32 with nonzero formula_33 and set their weights to formula_34. Create the edge formula_35 and set its weight to formula_36. This ensures that the total weight of the edges incident on the vertex formula_37 is at most 1. If formula_38, then create an edge formula_39 with weight formula_40. Continue with assigning edges to formula_41 in a similar way. \n\nIn the bipartite graph thus created, each job node in formula_42 has a total edge weight of 1 incident on it, and each machine node in formula_43 has edges with total weight at most 1 incident on it. Thus the vector formula_44 is an instance of a fractional matching on formula_45 and thus it can be rounded to obtain an integral matching of same cost. \n\nNow considering the ordering of processing times of the jobs on the machines nodes during construction of formula_45 and using an easy charging argument, the following theorem can be proved:\n\nTheorem: If formula_47 has a feasible solution then a schedule can be constructed with makespan \nformula_48 and cost formula_9.\n\nSince formula_50, a 2 approximation is obtained.\n\nThe paper is a joint work by Moses Charikar, Sudipto Guha, Éva Tardos and David Shmoys. They obtain a formula_51 approximation to the metric k medians problem. This was the first paper to break the previously best known formula_52 approximation.\n\nShmoys has also worked extensively on the facility location problem. His recent results include obtaining a formula_53 approximation algorithm for the capacitated facility location problem. The joint work with Fabian Chudak, has resulted in improving the previous known formula_54 approximation for the same problem. Their algorithm is based on a variant of randomized rounding called the randomized rounding with a backup, since a backup solution is incorporated to correct for the fact that the ordinary randomized rounding rarely generates a feasible solution to the associated set covering problem. \n\nFor the uncapacitated version of the facility location problem, again in a joint work with Chudak he obtained a formula_55-approximation algorithm which was a significant improvement on the previously known approximation guarantees.\nThe improved algorithm works by rounding an optimal fractional solution of a linear programming relaxation and using the properties of the optimal solutions of the linear program and a generalization of a decomposition technique.\n\nDavid Shmoys is an ACM Fellow and a Fellow of the Institute for Operations Research and the Management Sciences (INFORMS) (2013). He received College of Engineering Sonny Yau Excellence in Teaching Award three times and has been awarded NSF Presidential Young Investigator Award and the Frederick W. Lanchester Prize (2013)\n\n"}
{"id": "7482590", "url": "https://en.wikipedia.org/wiki?curid=7482590", "title": "Edwin Hewitt", "text": "Edwin Hewitt\n\nEdwin Hewitt (January 20, 1920, Everett, Washington – June 21, 1999) was an American mathematician known for his work in abstract harmonic analysis and for his discovery, in collaboration with Leonard Jimmie Savage, of the Hewitt–Savage zero–one law.\n\nHe received his Ph.D. in 1942 from Harvard University, and served on the faculty of mathematics at the University of Washington from 1954.\n\nHewitt pioneered the construction of the hyperreals by means of an ultrapower construction (Hewitt, 1948).\n\nHewitt wrote the 1975 English translation of A. A. Kirillov's 1972 Russian monograph \"Elements of the Theory of Representations\" (Элементы Теории Представлений), and co-authored \"Abstract Harmonic Analysis\" with Kenneth A. Ross (1st edn., 1st vol. in 1963; 1st edn., 2nd vol. in 1970), an extensive work in two volumes.\n\n\n\n\n"}
{"id": "1146188", "url": "https://en.wikipedia.org/wiki?curid=1146188", "title": "Erasure code", "text": "Erasure code\n\nIn coding theory, an erasure code is a forward error correction (FEC) code under the assumption of bit erasures (rather than bit errors), which transforms a message of \"k\" symbols into a longer message (code word) with \"n\" symbols such that the original message can be recovered from a subset of the \"n\" symbols. The fraction \"r\" = \"k\"/\"n\" is called the code rate. The fraction \"k’/k\", where \"k’\" denotes the number of symbols required for recovery, is called reception efficiency.\n\nOptimal erasure codes have the property that any \"k\" out of the \"n\" code word symbols are sufficient to recover the original message (i.e., they have optimal reception efficiency). Optimal erasure codes are maximum distance separable codes (MDS codes).\n\nParity check is the special case where \"n\" = \"k\" + 1. From a set of \"k\" values formula_1, a checksum is computed and appended to the \"k\" source values:\nThe set of \"k\" + 1 values formula_3 is now consistent with regard to the checksum.\nIf one of these values, formula_4, is erased, it can be easily recovered by summing the remaining variables:\n\nIn the simple case where \"k\" = 2, redundancy symbols may be created by sampling different points along the line between the two original symbols. This is pictured with a simple example, called err-mail:\n\nAlice wants to send her telephone number (555629) to Bob using err-mail. Err-mail works just like e-mail, except \n\nInstead of asking Bob to acknowledge the messages she sends, Alice devises the following scheme.\n\n\n\nBob knows that the form of \"f\"(\"k\") is formula_6, where \"a\" and \"b\" are the two parts of the telephone number.\nNow suppose Bob receives \"D=777\" and \"E=851\".\n\nBob can reconstruct Alice's phone number by computing the values of \"a\" and \"b\" from the values (\"f\"(4) and \"f\"(5)) he has received. \nBob can perform this procedure using any two err-mails, so the erasure code in this example has a rate of 40%.\n\nNote that Alice cannot encode her telephone number in just one err-mail, because it contains six characters, and that the maximum length of one err-mail message is five characters. If she sent her phone number in pieces, asking Bob to acknowledge receipt of each piece, at least four messages would have to be sent anyway (two from Alice, and two acknowledgments from Bob). So the erasure code in this example, which requires five messages, is quite economical.\n\nThis example is a little bit contrived. For truly generic erasure codes that work over any data set, we would need something other than the \"f\"(\"i\") given.\n\nThe linear construction above can be generalized to polynomial interpolation. Additionally, points are now computed over a finite field.\n\nFirst we choose a finite field \"F\" with order of at least \"n\", but usually a power of 2. The sender numbers the data symbols from 0 to \"k\" − 1 and sends them. He then constructs a (Lagrange) polynomial \"p\"(\"x\") of order \"k\" such that \"p\"(\"i\") is equal to data symbol \"i\". He then sends \"p\"(\"k\"), ..., \"p\"(\"n\" − 1). The receiver can now also use polynomial interpolation to recover the lost packets, provided he receives \"k\" symbols successfully. If the order of \"F\" is less than 2, where b is the number of bits in a symbol, then multiple polynomials can be used.\n\nThe sender can construct symbols \"k\" to \"n\" − 1 'on the fly', i.e., distribute the workload evenly between transmission of the symbols. If the receiver wants to do his calculations 'on the fly', he can construct a new polynomial \"q\", such that \"q\"(\"i\") = \"p\"(\"i\") if symbol \"i\" < \"k\" was received successfully and \"q\"(\"i\") = 0 when symbol \"i\" < \"k\" was not received. Now let \"r\"(\"i\") = \"p\"(\"i\") − \"q\"(\"i\"). Firstly we know that \"r\"(\"i\") = 0 if symbol \"i\" < \"k\" has been received successfully. Secondly, if symbol \"i\" ≥\"k\" has been received successfully, then \"r\"(\"i\") = \"p\"(\"i\") − \"q\"(\"i\") can be calculated. So we have enough data points to construct \"r\" and evaluate it to find the lost packets. So both the sender and the receiver require \"O\"(\"n\" (\"n\" − \"k\")) operations and only \"O\"(\"n\" − \"k\") space for operating 'on the fly'.\n\nThis process is implemented by Reed–Solomon codes, with code words constructed over a finite field using a Vandermonde matrix.\n\n\"Near-optimal erasure codes\" require (1 + ε)\"k\" symbols to recover the message (where ε>0). Reducing ε can be done at the cost of CPU time.\n\"Near-optimal erasure codes\" trade correction capabilities for computational complexity: practical algorithms can encode and decode with linear time complexity.\n\nFountain codes (also known as \"rateless erasure codes\") are notable examples of \"near-optimal erasure codes\". They can transform a \"k\" symbol message into a practically infinite encoded form, i.e., they can generate an arbitrary amount of redundancy symbols that can all be used for error correction. Receivers can start decoding after they have received slightly more than \"k\" encoded symbols.\n\nRegenerating codes address the issue of rebuilding (also called repairing) lost encoded fragments from existing encoded fragments. This issue occurs in distributed \nstorage systems where communication to maintain encoded redundancy is a problem.\n\nHere are some examples of implementations of the various codes:\n\n\n\n\n\n\n"}
{"id": "30334381", "url": "https://en.wikipedia.org/wiki?curid=30334381", "title": "Fictitious domain method", "text": "Fictitious domain method\n\nIn mathematics, the Fictitious domain method is a method to find the solution of a partial differential equations on a complicated domain formula_1, by substituting a given problem\nposed on a domain formula_1, with a new problem posed on a simple domain formula_3 containing formula_1.\n\nAssume in some area formula_5 we want to find solution formula_6 of the equation:\n\nwith boundary conditions:\n\nThe basic idea of fictitious domains method is to substitute a given problem\nposed on a domain formula_1, with a new problem posed on a simple shaped domain formula_3 containing formula_1 (formula_12). For example, we can choose \"n\"-dimensional parallelepiped as formula_3.\n\nProblem in the extended domain formula_3 for the new solution formula_15:\n\nIt is necessary to pose the problem in the extended area so that the following condition is fulfilled:\n\nformula_21 solution of problem:\n\nDiscontinuous coefficient formula_23 and right part of equation previous equation we obtain from expressions:\n\nBoundary conditions:\n\nConnection conditions in the point formula_27:\n\nwhere formula_29 means:\n\nEquation (1) has analytical solution therefore we can easily obtain error:\n\nformula_21 solution of problem:\n\nWhere formula_34 we take the same as in (3), and expression for formula_35\n\nBoundary conditions for equation (4) same as for (2).\n\nConnection conditions in the point formula_27:\n\nError:\n\n"}
{"id": "18458673", "url": "https://en.wikipedia.org/wiki?curid=18458673", "title": "Fieller's theorem", "text": "Fieller's theorem\n\nIn statistics, Fieller's theorem allows the calculation of a confidence interval for the ratio of two means.\n\nVariables \"a\" and \"b\" may be measured in different units, so there is no way to directly combine the standard errors as they may also be in different units. The most complete discussion of this is given by Fieller (1954).\n\nFieller showed that if \"a\" and \"b\" are (possibly correlated) means of two samples with expectations formula_1 and formula_2, and variances formula_3 and formula_4 and covariance formula_5, and if formula_6 are all known, then a (1 − \"α\") confidence interval (\"m\", \"m\") for formula_7 is given by\n\nwhere\nHere formula_10 is an unbiased estimator of formula_11 based on r degrees of freedom, and formula_12 is the formula_13-level deviate from the Student's t-distribution based on \"r\" degrees of freedom.\n\nThree features of this formula are important in this context:\n\na) The expression inside the square root has to be positive, or else the resulting interval will be imaginary.\n\nb) When \"g\" is very close to 1, the confidence interval is infinite.\n\nc) When \"g\" is greater than 1, the overall divisor outside the square brackets is negative and the confidence interval is exclusive.\n\nOne problem is that, when \"g\" is not small, the confidence interval can blow up when using Fieller's theorem. Andy Grieve has provided a Bayesian solution where the CIs are still sensible, albeit wide. Bootstrapping provides another alternative that does not require the assumption of normality.\n\nEdgar C. Fieller (1907–1960) first started working on this problem while in Karl Pearson's group at University College London, where he was employed for five years after graduating in Mathematics from King's College, Cambridge. He then worked for the Boots Pure Drug Company as a statistician and operational researcher before becoming deputy head of operational research at RAF Fighter Command during the Second World War, after which he was appointed the first head of the Statistics Section at the National Physical Laboratory.\n\n\n"}
{"id": "31102294", "url": "https://en.wikipedia.org/wiki?curid=31102294", "title": "Geometric Algebra", "text": "Geometric Algebra\n\nGeometric Algebra is a book written by Emil Artin and published by Interscience Publishers, New York, in 1957. It was republished in 1988 in the Wiley Classics series ().\n\nIn 1962 \"Algèbra Géométrique\", translation into French by M. Lazard, was published by Gauthier-Villars, and reprinted in 1996. () In 1968 a translation into Italian was published in Milan by Feltrinelli. In 1969 a translation into Russian was published in Moscow by Nauka\n\nLong anticipated as the sequel to \"Moderne Algebra\" (1930), which Bartel van der Waerden published as his version of notes taken in a course with Artin, \"Geometric Algebra\" is a research monograph suitable for graduate students studying mathematics. From the Preface:\n\nThe book is illustrated with six geometric configurations in chapter 2, which retraces the path from geometric to field axioms previously explored by Karl von Staudt and David Hilbert.\n\nChapter one is titled \"Preliminary Notions\". The ten sections explicate notions of set theory, vector spaces, homomorphisms, duality, linear equations, group theory, field theory, ordered fields and valuations. On page vii Artin says \"Chapter I should be used mainly as a reference chapter for the proofs of certain isolated theorems.\"\nChapter two is titled \"Affine and Projective Geometry\". Artin posits this challenge to generate algebra (a field \"k\") from geometric axioms:\nThe reflexive variant of parallelism is invoked: parallel lines have either all or none of their points in common. Thus a line is parallel to itself.\n\nAxiom 1 requires a unique line for each pair of distinct points, and a unique point of intersection of non-parallel lines. Axiom 2 depends on a line and a point; it requires a unique parallel \"to\" the line and \"through\" the point. Axiom 3 requires three non-collinear points. Axiom 4a requires a translation to move any point to any other. Axiom 4b requires a dilation at \"P\" to move \"Q\" to \"R\" when the three points are collinear.\n\nArtin writes the line through \"P\" and \"Q\" as \"P\" + \"Q\". To define a \"dilation\" he writes, \"Let two distinct points \"P\" and \"Q\" and their images \"P\"′ and \"Q\"′ be given.\" To suggest the role of incidence in geometry, a dilation is specified by this property: \"If \"l\"′ is the line parallel to \"P\" + \"Q\" which passes through \"P\"′, then \"Q\"′ lies on \"l\"′.\" Of course, if \"P\"′ ≠ \"Q\"′, then this condition implies \"P\" + \"Q\" is parallel to \"P\"′ + \"Q\"′, so that the dilation is an affine transformation.\n\nThe dilations with no fixed points are translations, and the group of translations \"T\" is shown to be an invariant subgroup of the group of dilations. \nFor a dilation \"σ\" and a point \"P\", the \"trace\" is \"P\" + \"σP\". The mappings \"T\" → \"T\" that are trace-preserving homomorphisms are the elements of \"k\". First \"k\" is shown to be an associative ring with 1, then a skew field.\n\nConversely, there is an affine geometry based on any given skew field \"k\". Axioms 4a and 4b are equivalent to Desargues' theorem. When Pappus's hexagon theorem holds in the affine geometry, \"k\" is commutative and hence a field.\n\nChapter three is titled \"Symplectic and Orthogonal Geometry\". It begins with metric structures on vector spaces before defining symplectic and orthogonal geometry and describing their common and special features. There are sections on geometry over finite fields and over ordered fields.\n\nChapter four is on general linear groups. First there is Jean Dieudonne's theory of determinants over \"non-commutative fields\" (division rings). Artin describes GL(\"n, k\") group structure. More details are given about vector spaces over finite fields.\n\nChapter five is \"The Structure of Sympletic and Orthogonal Groups\". It includes sections on elliptic spaces, Clifford algebra, and spinorial norm.\n\nAlice T. Schafer wrote \"Mathematicians will find on many pages ample evidence of the author’s ability to penetrate a subject and to present material in a particularly elegant manner.\" She notes the overlap between Artin's text and Baer's \"Linear Algebra and Projective Geometry\" or Dieudonné's \"La Géometrie des Groupes Classique\".\n\nJean Dieudonné reviewed the book for Mathematical Reviews and placed it on a level with Hilbert's \"Grundlagen der Geometrie\".\n\n"}
{"id": "12781", "url": "https://en.wikipedia.org/wiki?curid=12781", "title": "Group action", "text": "Group action\n\nIn mathematics, an action of a group is a formal way of interpreting the manner in which the elements of the group correspond to transformations of some space in a way that preserves the structure of that space. Common examples of spaces that groups act on are sets, vector spaces, and topological spaces. Actions of groups on vector spaces are called representations of the group.\n\nWhen there is a natural correspondence between the set of group elements and the set of space transformations, a group can be interpreted as acting on the space in a canonical way. For example, the symmetric group of a finite set consists of all bijective transformations of that set; thus, applying any element of the permutation group to an element of the set will produce another (not necessarily distinct) element of the set. More generally, symmetry groups such as the homeomorphism group of a topological space or the general linear group of a vector space, as well as their subgroups, also admit canonical actions. For other groups, an interpretation of the group in terms of an action may have to be specified, either because the group does not act canonically on any space or because the canonical action is not the action of interest. For example, we can specify an action of the two-element cyclic group formula_1 on the finite set formula_2 by specifying that 0 (the identity element) sends formula_3, and that 1 sends formula_4. This action is not canonical.\n\nA common way of specifying non-canonical actions is to describe a homomorphism formula_5 from a group \"G\" to the group of symmetries of a set \"X\". The action of an element formula_6 on a point formula_7 is assumed to be identical to the action of its image formula_8 on the point formula_9. The homomorphism formula_5 is also frequently called the \"action\" of \"G\", since specifying formula_5 is equivalent to specifying an action. Thus, if \"G\" is a group and \"X\" is a set, then an action of \"G\" on \"X\" may be formally defined as a group homomorphism formula_5 from \"G\" to the symmetric group of \"X\". The action assigns a permutation of \"X\" to each element of the group in such a way that:\nIf \"X\" has additional structure, then formula_5 is only called an action if for each formula_6, the permutation formula_15 preserves the structure of \"X\".\n\nThe abstraction provided by group actions is a powerful one, because it allows geometrical ideas to be applied to more abstract objects. Many objects in mathematics have natural group actions defined on them. In particular, groups can act on other groups, or even on themselves. Because of this generality, the theory of group actions contains wide-reaching theorems, such as the orbit stabilizer theorem, which can be used to prove deep results in several fields.\n\nIf \"G\" is a group and \"X\" is a set, then a (\"left\") \"group action\" \"φ\" of \"G\" on \"X\" is a function\nthat satisfies the following two axioms (where we denote \"φ\"(\"g\", \"x\") as \"g\"⋅\"x\"):\n\nThe group \"G\" is said to act on \"X\" (on the left). The set \"X\" is called a (\"left\") \"G-set\".\n\nFrom these two axioms, it follows that for every \"g\" in \"G\", the function which maps \"x\" in \"X\" to \"g\"⋅\"x\" is a bijective map from \"X\" to \"X\" (its inverse being the function which maps \"x\" to \"g\"⋅\"x\"). Therefore, one may alternatively define a group action of \"G\" on \"X\" as a group homomorphism from \"G\" into the symmetric group Sym(\"X\") of all bijections from \"X\" to \"X\".\n\nIn complete analogy, one can define a \"right group action\" of \"G\" on \"X\" as an operation mapping to \"x\"⋅\"g\" and satisfying the two axioms:\n\nThe difference between left and right actions is in the order in which a product like \"gh\" acts on \"x\". For a left action \"h\" acts first and is followed by \"g\", while for a right action \"g\" acts first and is followed by \"h\". Because of the formula , one can construct a left action from a right action by composing with the inverse operation of the group. Also, a right action of a group \"G\" on \"X\" is the same thing as a left action of its opposite group \"G\" on \"X\". It is thus sufficient to only consider left actions without any loss of generality.\n\nThe action of \"G\" on \"X\" is called\n\nFurthermore, if formula_17 acts on a topological space formula_18, then the action is:\n\n\nIf \"X\" is a non-zero module over a ring \"R\" and the action of \"G\" is \"R\"-linear then it is said to be \n\nConsider a group \"G\" acting on a set \"X\". The \"orbit\" of an element \"x\" in \"X\" is the set of elements in \"X\" to which \"x\" can be moved by the elements of \"G\". The orbit of \"x\" is denoted by \"G\"⋅\"x\":\n\nThe defining properties of a group guarantee that the set of orbits of (points \"x\" in) \"X\" under the action of \"G\" form a partition of \"X\". The associated equivalence relation is defined by saying if and only if there exists a \"g\" in \"G\" with . The orbits are then the equivalence classes under this relation; two elements \"x\" and \"y\" are equivalent if and only if their orbits are the same; i.e., .\n\nThe group action is transitive if and only if it has exactly one orbit, i.e., if there exists \"x\" in \"X\" with . This is the case if and only if for \"all\" \"x\" in \"X\".\n\nThe set of all orbits of \"X\" under the action of \"G\" is written as \"X\"/\"G\" (or, less frequently: \"G\"\\\"X\"), and is called the \"quotient\" of the action. In geometric situations it may be called the ', while in algebraic situations it may be called the space of ', and written \"X\", by contrast with the invariants (fixed points), denoted \"X\": the coinvariants are a \"quotient\" while the invariants are a \"subset.\" The coinvariant terminology and notation are used particularly in group cohomology and group homology, which use the same superscript/subscript convention.\n\nIf \"Y\" is a subset of \"X\", we write \"GY\" for the set . We call the subset \"Y\" \"invariant under G\" if (which is equivalent to ). In that case, \"G\" also operates on \"Y\" by restricting the action to \"Y\". The subset \"Y\" is called \"fixed under G\" if for all \"g\" in \"G\" and all \"y\" in \"Y\". Every subset that is fixed under \"G\" is also invariant under \"G\", but not vice versa.\n\nEvery orbit is an invariant subset of \"X\" on which \"G\" acts transitively. The action of \"G\" on \"X\" is \"transitive\" if and only if all elements are equivalent, meaning that there is only one orbit.\n\nA \"G-invariant\" element of \"X\" is such that for all . The set of all such \"x\" is denoted \"X\" and called the \"G-invariants\" of \"X\". When \"X\" is a \"G\"-module, \"X\" is the zeroth group cohomology group of \"G\" with coefficients in \"X\", and the higher cohomology groups are the derived functors of the functor of \"G\"-invariants.\n\nGiven \"g\" in \"G\" and \"x\" in \"X\" with , we say \"x\" is a fixed point of \"g\" and \"g\" fixes \"x\".\n\nFor every \"x\" in \"X\", we define the \"stabilizer subgroup\" of \"G\" with respect to \"x\" (also called the \"isotropy group\" or \"little group\") as the set of all elements in \"G\" that fix \"x\":\nThis is a subgroup of \"G\", though typically not a normal one. The action of \"G\" on \"X\" is free if and only if all stabilizers are trivial. The kernel \"N\" of the homomorphism with the symmetric group, , is given by the intersection of the stabilizers \"G\" for all \"x\" in \"X\". If \"N\" is trivial, the action is said to be faithful (or effective).\n\nLet \"x\" and \"y\" be two elements in \"X\", and let \"g\" be a group element such that . Then the two stabilizer groups \"G\" and \"G\" are related by . Proof: by definition, if and only if . Applying \"g\" to both sides of this equality yields ; that is, . An opposite inclusion follows similarly by taking and supposing .\n\nThe above says that the stabilizers of elements in the same orbit are conjugate to each other. Thus, to each orbit, one can associate a conjugacy class of a subgroup of \"G\" (i.e., the set of all conjugates of the subgroup). Let formula_37 denote the conjugacy class of \"H\". Then one says that the orbit \"O\" has type formula_37 if the stabilizer formula_39 of some/any \"x\" in \"O\" belongs to formula_37. A maximal orbit type is often called a principal orbit type.\n\nOrbits and stabilizers are closely related. For a fixed formula_7, consider the map formula_42 given by formula_43. This map induces a bijection from the set formula_44 of cosets of formula_39 in formula_17 to the orbit formula_47, as the translate formula_48 depends only on the left coset formula_49. This result is known as the \"orbit-stabilizer theorem\". \n\nIf formula_17 is finite then the orbit-stabilizer theorem, together with Lagrange's theorem, gives\n\nThis result is especially useful since it can be employed for counting arguments (typically in situations where formula_18 is finite as well).\n\nA result closely related to the orbit-stabilizer theorem is Burnside's lemma:\n\nwhere formula_86 the set of points fixed by formula_87. This result is mainly of use when formula_17 and formula_18 are finite, when it can be interpreted as follows: the number of orbits is equal to the average number of points fixed per group element.\n\nFixing a group formula_17, the set of formal differences of finite formula_17-sets forms a ring called the Burnside ring of formula_17, where addition corresponds to disjoint union, and multiplication to Cartesian product.\n\n\nThe notion of group action can be put in a broader context by using the \"action groupoid\" formula_93 associated to the group action, thus allowing techniques from groupoid theory such as presentations and fibrations. Further the stabilizers of the action are the vertex groups, and the orbits of the action are the components, of the action groupoid. For more details, see the book \"Topology and groupoids\" referenced below.\n\nThis action groupoid comes with a morphism formula_94 which is a \"covering morphism of groupoids\". This allows a relation between such morphisms and covering maps in topology.\n\nIf \"X\" and \"Y\" are two \"G\"-sets, we define a \"morphism\" from \"X\" to \"Y\" to be a function such that for all \"g\" in \"G\" and all \"x\" in \"X\". Morphisms of \"G\"-sets are also called \"equivariant maps\" or \"G-maps\".\n\nThe composition of two morphisms is again a morphism.\n\nIf a morphism \"f\" is bijective, then its inverse is also a morphism, and we call \"f\" an \"isomorphism\" and the two \"G\"-sets \"X\" and \"Y\" are called \"isomorphic\"; for all practical purposes, they are indistinguishable in this case.\n\nSome example isomorphisms:\n\nWith this notion of morphism, the collection of all \"G\"-sets forms a category; this category is a Grothendieck topos (in fact, assuming a classical metalogic, this topos will even be Boolean).\n\nOne often considers \"continuous group actions\": the group \"G\" is a topological group, \"X\" is a topological space, and the map is continuous with respect to the product topology of . The space \"X\" is also called a \"G-space\" in this case. This is indeed a generalization, since every group can be considered a topological group by using the discrete topology. All the concepts introduced above still work in this context, however we define morphisms between \"G\"-spaces to be \"continuous\" maps compatible with the action of \"G\". The quotient \"X\"/\"G\" inherits the quotient topology from \"X\", and is called the \"quotient space\" of the action. The above statements about isomorphisms for regular, free and transitive actions are no longer valid for continuous group actions.\n\nIf \"X\" is a regular covering space of another topological space \"Y\", then the action of the deck transformation group on \"X\" is properly discontinuous as well as being free. Every free, properly discontinuous action of a group \"G\" on a path-connected topological space \"X\" arises in this manner: the quotient map is a regular covering map, and the deck transformation group is the given action of \"G\" on \"X\". Furthermore, if \"X\" is simply connected, the fundamental group of \"X\"/\"G\" will be isomorphic to \"G\".\n\nThese results have been generalized in the book \"Topology and Groupoids\" referenced below to obtain the fundamental groupoid of the orbit space of a discontinuous action of a discrete group on a Hausdorff space, as, under reasonable local conditions, the orbit groupoid of the fundamental groupoid of the space. This allows calculations such as the fundamental group of the symmetric square of a space \"X\", namely the orbit space of the product of \"X\" with itself under the twist action of the cyclic group of order 2 sending to .\n\nAn action of a group \"G\" on a locally compact space \"X\" is \"cocompact\" if there exists a compact subset \"A\" of \"X\" such that . For a properly discontinuous action, cocompactness is equivalent to compactness of the quotient space \"X/G\".\n\nThe action of \"G\" on \"X\" is said to be \"proper\" if the mapping that sends is a proper map.\n\nA group action of a topological group \"G\" on a topological space \"X\" is said to be \"strongly continuous\" if for all \"x\" in \"X\", the map is continuous with respect to the respective topologies. Such an action induces an action on the space of continuous functions on \"X\" by defining for every \"g\" in \"G\", \"f\" a continuous function on \"X\", and \"x\" in \"X\". Note that, while every continuous group action is strongly continuous, the converse is not in general true.\n\nThe subspace of \"smooth points\" for the action is the subspace of \"X\" of points \"x\" such that is smooth; i.e., it is continuous and all derivatives are continuous.\n\nOne can also consider actions of monoids on sets, by using the same two axioms as above. This does not define bijective maps and equivalence relations however. See semigroup action.\n\nInstead of actions on sets, one can define actions of groups and monoids on objects of an arbitrary category: start with an object \"X\" of some category, and then define an action on \"X\" as a monoid homomorphism into the monoid of endomorphisms of \"X\". If \"X\" has an underlying set, then all definitions and facts stated above can be carried over. For example, if we take the category of vector spaces, we obtain group representations in this fashion.\n\nOne can view a group \"G\" as a category with a single object in which every morphism is invertible. A (left) group action is then nothing but a (covariant) functor from \"G\" to the category of sets, and a group representation is a functor from \"G\" to the category of vector spaces. A morphism between G-sets is then a natural transformation between the group action functors. In analogy, an action of a groupoid is a functor from the groupoid to the category of sets or to some other category.\n\nIn addition to continuous actions of topological groups on topological spaces, one also often considers smooth actions of Lie groups on smooth manifolds, regular actions of algebraic groups on algebraic varieties, and actions of group schemes on schemes. All of these are examples of group objects acting on objects of their respective category.\n\n\n\n"}
{"id": "2945778", "url": "https://en.wikipedia.org/wiki?curid=2945778", "title": "Hemicontinuity", "text": "Hemicontinuity\n\nIn mathematics, the notion of the continuity of functions is not immediately extensible to multivalued mappings or correspondences between two sets \"A\" and \"B\". The dual concepts of upper hemicontinuity and lower hemicontinuity facilitate such an extension. A correspondence that has both properties is said to be continuous in an analogy to the property of the same name for functions.\n\nRoughly speaking, a function is upper hemicontinuous when (1) a convergent sequence of points in the domain maps to a sequence of sets in the range which (2) contain another convergent sequence, then the image of limiting point in the domain must contain the limit of the sequence in the range. Lower hemicontinuity essentially reverses this, saying if a sequence in the domain converges, given a point in the range of the limit, then you can find a sub-sequence whose image contains a convergent sequence to the given point.\n\nA correspondence Γ : \"A\" → \"B\" is said to be upper hemicontinuous at the point \"a\" if for any open neighbourhood \"V\" of Γ(\"a\") there exists a neighbourhood \"U\" of \"a\" such that for all \"x\" in \"U\", Γ(\"x\") is a subset of \"V\".\n\nFor a correspondence Γ : \"A\" → \"B\" with closed values, \nΓ : \"A\" → \"B\" is upper hemicontinuous at formula_1 if formula_2, formula_3 and formula_4\nIf Γ is compact-valued (i.e. Γ(\"x\") is compact for all \"x\") the converse is also true.\n\nThe graph of a correspondence Γ : \"A\" → \"B\" is the set defined by formula_6.\n\nIf Γ : \"A\" → \"B\" is an upper hemicontinuous correspondence with closed domain (that is, the set of points \"a\" ∈ \"A\" where Γ(\"a\") is not the empty set is closed) and closed values (i.e. Γ(\"a\") is closed for all \"a\" in \"A\"), then Gr(Γ) is closed. If \"B\" is compact, then the converse is also true.\n\nA correspondence Γ : \"A\" → \"B\" is said to be lower hemicontinuous at the point \"a\" \nif for any open set \"V\" intersecting Γ(\"a\") there exists a neighbourhood \"U\" of \"a\" such that Γ(\"x\") intersects \"V\" for all \"x\" in \"U\". (Here \"V\" \"intersects\" \"S\" means nonempty intersection formula_7).\n\nΓ : \"A\" → \"B\" is lower hemicontinuous at \"a\" if and only if\n\nA correspondence Γ : \"A\" → \"B\" have \"open lower sections\" if the set formula_10\nis open in \"A\" for every \"b\" ∈ \"B\". If Γ values are all open sets in \"B\", then Γ is said to have \"open upper sections\". \n\nIf Γ has an open graph \"Gr\"(Γ), then Γ has open upper and lower sections and if Γ has open lower sections then it is lower hemicontinuous.\n\nThe open graph theorem says that if Γ : \"A\" → P(R) is a convex-valued correspondence with open upper sections, then Γ has an open graph in \"A\" × R if and only if Γ is lower hemicontinuous.\n\nSet-theoretic, algebraic and topological operations on multivalued maps (like union, composition, sum, convex hull, closure)\nusually preserve the type of continuity. But this should be taken with appropriate care since, for example there exists a pair of lower hemicontinuous correspondences whose intersection is not lower hemicontinuous. This can be fixed upon strengthening continuity properties: if one of those lower hemicontinuous multifunctions has open graph then their intersection is again lower hemicontinuous.\n\nVery important part of set-valued analysis (in view of applications) constitutes the investigation of single-valued selections and approximations to multivalued maps. Typically lower hemicontinuous correspondences admit single-valued selections (Michael selection theorem, Bressan–Colombo directionally continuous selection theorem, Fryszkowski decomposable map selection), likewise upper hemicontinuous maps admit approximations (e.g. Ancel–Granas–Górniewicz–Kryszewski theorem).\n\nIf a correspondence is both upper hemicontinuous and lower hemicontinuous, it is said to be continuous. A continuous function is in all cases both upper and lower hemicontinuous.\n\nThe upper and lower hemicontinuity might be viewed as usual continuity:\n\n(For the notion of hyperspace compare also power set and function space).\n\nUsing lower and upper Hausdorff uniformity we can also define the so-called upper and lower semicontinuous maps in the sense of Hausdorff (also known as metrically lower / upper semicontinuous maps).\n\n\n"}
{"id": "37007490", "url": "https://en.wikipedia.org/wiki?curid=37007490", "title": "Hodge–Arakelov theory", "text": "Hodge–Arakelov theory\n\nIn mathematics, Hodge–Arakelov theory of elliptic curves is an analogue of classical and \"p\"-adic Hodge theory for elliptic curves carried out in the framework of Arakelov theory. It was introduced by .\n\nMochizuki's main comparison theorem in Hodge–Arakelov theory states (roughly) that the space of polynomial functions of degree less than \"d\" on the universal extension of a smooth elliptic curve in characteristic 0 is naturally isomorphic (via restriction) to the \"d\"-dimensional space of functions on the \"d\"-torsion points. It is called a comparison theorem as it is an analogue for Arakelov theory of comparison theorems in cohomology relating de Rham cohomology to singular cohomology of complex varieties or étale cohomology of \"p\"-adic varieties.\n\nIn and he pointed out that arithmetic Kodaira-Spencer map and Gauss–Manin connection may give some important hints for Vojta's conjecture, ABC conjecture and so on. \n\n"}
{"id": "43558650", "url": "https://en.wikipedia.org/wiki?curid=43558650", "title": "Index (statistics)", "text": "Index (statistics)\n\nIn statistics and research design, an index is a composite statistic – a measure of changes in a representative group of individual data points, or in other words, a compound measure that aggregates multiple indicators. Indexes summarize and rank specific observations.\n\nMuch data in the field of social sciences is represented in various indices such as Gender Gap Index, Human Development Index or the Dow Jones Industrial Average.\n\nItem in indexes are usually weighted equally, unless there are some reasons against it (for example, if two items reflect essentially the same aspect of a variable, they could have a weight of 0.5 each).\n\nConstructing the items involves four steps. First, items should be selected based on their content validity, unidimensionality, the degree of specificity in which a dimension is to be measured, and their amount of variance. Items should be empirically related to one another, which leads to the second step of examining their multivariate relationships. Third, indexes scores are designed, which involves determining their score ranges and weights for the items. Finally, indexes should be validated, which involves testing whether they can predict indicators related to the measured variable not used in their construction.\n\n"}
{"id": "4845297", "url": "https://en.wikipedia.org/wiki?curid=4845297", "title": "Infinitism", "text": "Infinitism\n\nInfinitism is the view that knowledge may be justified by an infinite chain of reasons. It belongs to epistemology, the branch of philosophy that considers the possibility, nature, and means of knowledge.\n\nSince Gettier, \"knowledge\" is no longer widely accepted as meaning \"justified true belief\" only. However, some epistemologists still consider knowledge to have a justification condition. Traditional theories of justification (foundationalism and coherentism) and indeed some philosophers consider an infinite regress not to be a valid justification. In their view, if \"A\" is justified by \"B\", \"B\" by \"C\", and so forth, then either \n\nInfinitism, the view, for example, of Peter D. Klein, challenges this consensus, referring back to work of Paul Moser (1984) and John Post (1987). In this view, the evidential ancestry of a justified belief must be infinite and non-repeating, which follows from the conjunction two principles that Klein sees as having straightforward intuitive appeal: \"The Principle of Avoiding Circularity\" and \"The Principle of Avoiding Arbitrariness.\"\n\nThe Principle of Avoiding Circularity (PAC) is stated as follows: \"For all x, if a person, S, has a justification for x, then for all y, if y is in the evidential ancestry of x for S, then x is not in the evidential ancestry of y for S.\" PAC says that the proposition to be justified cannot be a member of its own evidential ancestry, which is violated by coherence theories of justification.\n\nThe Principle of Avoiding Arbitrariness (PAA) is stated as follows: \"For all x, if a person, S, has a justification for x, then there is some reason, r1, available to S for x; and there is some reason, r2, available to S for r1; etc.\" PAA says that in order to avoid arbitrariness, for any proposition \"x\" to be justified for an epistemological agent, there must be some reason \"r\" available to the agent; this reason will in turn require the same structure of justification, and so on \"ad infinitum\". Foundationalist theories can only avoid arbitrariness by claiming that some propositions are self-justified. But if a proposition is its own justification (e.g. coherentism), then it is a member of its own evidential ancestry, and the structure of justification is circular.\n\nIn this view, the conjunction of both PAC and PAA leaves infinitism as the only alternative to skepticism.\n\nThe Availability of Reasons: Klein also relies on the notion of \"availability\". In other words, a reason must be available to the subject in order for it to be a candidate for justification. There are two conditions that need to be satisfied in order for a reason to be available: objectively and subjectively.\n\nAn objectively available reason is stated as follows: \"a belief, r, is objectively available to S as a reason for p if (1) r has some sufficiently high probability and the conditional probability of p given r is sufficiently high; or (2) an impartial, informed observer would accept r as a reason for p; or (3) r would be accepted in the long run by an appropriately defined set of people; or (4) r is evident for S and r makes p evident for S; or (5) r accords with S's deepest epistemic commitments; or (6) r meets the appropriate conversational presuppositions; or (7) an intellectually virtuous person would advance r as a reason for p.\" Any of these conditions are sufficient to describe objectively available reasons and are compatible with infinitism. Klein concedes that, ultimately, the proper characterization of objectively available need be a member of this list, but, for the scope of Klein's defense of infinitism, he need not provide a fully developed account of objectively available reasons. Objective availability could be best understood, at least as a working definition, as an existing, truth-apt reason not dependant on the subject.\n\nA subjectively available reason is stated as follows: \"S must be able to call on r.\" (Subjectively available is comparatively straightforward compared to objectively available.) The subject must be able to evoke the reason in their own mind and use the reason in the process of justification. In essence, the reason must be \"properly hooked up with S's own beliefs\" in order to be subjectively available.\n\nA reason that is both objectively and subjectively available to a subject is a candidate for justification according to infinitism (or, at least for Klein).\n\nObjection to Infinitism: Klein addresses an objection to infinitism.\n\nThe finite mind objection (attributed to John Williams): The human mind is finite and has a limited capacity. \"It is impossible to consciously believe an infinite number of propositions (because to believe something takes some time) and it is impossible to \"unconsciously believe\"...an infinite number of propositions because the candidate beliefs are such that some of them \"defeat human understanding.\" It is simply an impossibility that a subject has an infinite chain of reasons which justify their beliefs because the human mind is finite. Klein concedes that the human mind is finite and cannot contain an infinite number of reasons, but the infinitist, according to Klein, is not committed to a subject actually possessing infinite reasons. \"The infinitist is not claiming that in any finite period of time...we can consciously entertain an infinite number of thoughts. It is rather that there are an infinite number of propositions such that each one of them would be consciously thought were the appropriate circumstances to arise.\" So, an infinite chain of reasons need not be present in the mind in order for a belief to be justified rather it must merely be possible to provide an infinite chain of reasons. There will always be another reason to justify the preceding reason if the subject felt compelled to make the inquiry and had subjective access to that reason.\n\n\n"}
{"id": "15716437", "url": "https://en.wikipedia.org/wiki?curid=15716437", "title": "Jerome Ravetz", "text": "Jerome Ravetz\n\nJerome (Jerry) Ravetz is a philosopher of science. He is best known for his books analysing scientific knowledge from a social and ethical perspective, focussing on issues of quality. He is the co-author (with Silvio Funtowicz) of the NUSAP notational system and of Post-normal science. He is currently an Associate Fellow at the Institute for Science, Innovation and Society, University of Oxford.\n\nRavetz was born in Philadelphia; his grandfather was a Russian-Jewish immigrant and his father a truck driver and trade union organiser. He attended Central High School and Swarthmore College. He came to England in 1950 on a Fulbright Scholarship to Trinity College, Cambridge, where he studied for a Ph.D. in Pure Mathematics under the supervision of A.S. Besicovitch. In 1955 his passport was taken away, as part of the wave of McCarthyism; it was returned in 1958 and in 1961 he became a British citizen. He taught mathematics at the University of Pennsylvania and then at Durham University. In 1957 he moved to Leeds University to join Stephen Toulmin in the establishment of a centre in the History and Philosophy of Science. He stayed at Leeds, eventually becoming a Reader, until taking early retirement in 1983. Since then he has been an independent scholar.\n\nHe has visited at Utrecht University, Harvard University, the Institute for Advanced Study, the University of California, Santa Cruz, Fudan University (Shanghai), the University of Texas at Dallas, the Carnegie Mellon University and the University of Luxembourg. Over the years he has worked closely with colleagues at the European Commission Joint Research Centre, Ispra, Italy.\n\nHis earliest research, after mathematics, was in the history of the mathematical sciences, with books on Copernicus and Fourier. In 1971, he published the influential book \"Scientific Knowledge and Its Social Problems\". This went through several English language editions, plus German and Japanese translations, and was republished in 1996. This book raises issues of uncertainty and ethics in the social practice of science. It was an early attempt to recast the philosophy of science for the conditions of 'industrialised science' and to shift the philosophy of science from epistemology to the social and ethical aspects of science. In it he proposed a 'critical science' for a new version of the idealism that had characterised science in the pre-industrial age.\n\nIn the years around 1970 he was an active member of the British Society for Social Responsibility in Science.\n\nFrom 1973 to 1976 he was Executive Secretary of the Council for Science and Society in London, whose founder was the law reformer Paul Sieghart. He drafted its report on 'The Acceptability of Risks'. From 1977 to 1978, he was a member of the Genetic Manipulation Advisory Group, regulating research in recombinant DNA.\nWith Silvio Funtowicz he created the NUSAP notational system, described in their book Uncertainty and quality in science for policy (Reidel 1990). This was the stimulus for the development of the 'Guidance' for managing uncertainty, at the Netherlands Environment Agency. They also created the theory of Post-normal science, which applies when 'Facts are uncertain, values in dispute, stakes high and decisions urgent.' A collection of his essays, was also published in 1990. With Zia Sardar he co-authored Cyberfutures: Culture and Politics on the Information Superhighway in 1996.[2] His most recent book is The No nonsense guide to science (New Internationalist 2006).\nHis research continues in two main directions: new trends in the social practice of science; and new approaches to the management of uncertainty. On the former, he has co-authored (with Silvio Funtowicz) chapters on 'Science, New Forms of' and 'Peer Review and Quality Control' for the International Encyclopedia of Social and Behavioral Sciences (2015). On the latter he is concerned with the analysis of ignorance and the representation and manipulation of quantitative information where there is 'not even one significant digit'. He has also recently written \non the quality control crisis of science.\n\nHe is currently an Associate Fellow at the Institute for Science, Innovation and Society at the University of Oxford. He was interviewed by the Great Transition Initiative's blog on June 2016.\n\n\"The activity of modern natural science has transformed our knowledge and control of the world about us; but in the process it has also transformed itself; and it has created problems that natural science alone cannot solve\". \"Scientific Knowledge and its Social Problems\", Oxford 1971, p. 9.\n\n\"Wherever there's a system, there's a racket to beat it.\" Ibidem, p. 295.\n\n\n\n"}
{"id": "250626", "url": "https://en.wikipedia.org/wiki?curid=250626", "title": "John Hopkinson", "text": "John Hopkinson\n\nJohn Hopkinson, FRS, (27 July 1849 – 27 August 1898) was a British physicist, electrical engineer, Fellow of the Royal Society and President of the IEE (now the IET) twice in 1890 and 1896. He invented the three-wire (three-phase) system for the distribution of electrical power, for which he was granted a patent in 1882. He also worked in many areas of electromagnetism and electrostatics, and in 1890 was appointed professor of electrical engineering at King's College London, where he was also director of the Siemens Laboratory.\nHopkinson's law, the magnetic counterpart to Ohm's law, is named after him.\n\nJohn Hopkinson was born in Manchester, the eldest of 5 children. His father, also called John, was a mechanical engineer. He was educated at Queenwood School in Hampshire and Owens College in Manchester. He won a scholarship to Trinity College, Cambridge in 1867 and graduated in 1871 as Senior Wrangler, having placed first in the demanding Cambridge Mathematical Tripos examination. During this time he also studied for and passed the examination for a BSc from the University of London. Hopkinson could have followed a purely academic career but instead chose engineering as his vocation. He was a Cambridge Apostle.\n\nAfter working first in his father's engineering works, Hopkinson took a position in 1872 as an engineering manager in the lighthouse engineering department of Chance Brothers and Company in Smethwick. In 1877 Hopkinson was elected a Fellow of the Royal Society in recognition of his application of Maxwell's theory of electromagnetism to problems of electrostatic capacity and residual charge. In 1878 he moved to London to work as a consulting engineer, focusing particularly on developing his ideas about how to improve the design and efficiency of dynamos. Hopkinson's most important contribution was his three-wire distribution system, patented in 1882. In 1883 Hopkinson showed mathematically that it was possible to connect two alternating current dynamos in parallel-—a problem that had long bedevilled electrical engineers. He also studied magnetic permeability at high temperature, and discovered what was later called the Hopkinson peak effect.\n\nThe series-parallel method of electric motor control, for which Hopkinson was granted a British patent in 1881, would prove to be an important advance in the development of electric railways. He applied for a US patent in 1892, triggering an interference proceeding against American inventor Rudolph M Hunter, who had been granted a US patent for the method in 1888. The US Patent Office affirmed Hopkinson’s claim to priority of invention, but his British patent expired before the case was resolved, rendering him ineligible for a US patent (his US patent, had one been issued, would have expired concurrently with his British patent).\n\nHopkinson twice held the office of President of the Institution of Electrical Engineers. During his second term, Hopkinson proposed that the Institution should make available the technical knowledge of electrical engineers for the defence of the country. In 1897 the Volunteer Corps of Electrical Engineers was formed and Hopkinson became major in command of the corps.\n\nOn 27 August 1898, Hopkinson and three of his six children, John Gustave, Alice and Lina Evelyn, were killed in a mountaineering accident on the Petite Dent de Veisivi, Val d'Hérens, in the Pennine Alps, Switzerland.\n\nAs a memorial to John Hopkinson and his son, the 1899 extension to the Engineering Laboratory in the New Museums Site of the University of Cambridge was named after him. A plaque commemorating this is fixed to the wall in Free School Lane. The Hopkinson and Imperial Chemical Industries Professorship of Applied Thermodynamics is named in his honour.\n\nThere is a memorial sundial to Alice Hopkinson in the gardens of Newnham College, Cambridge from which she had recently graduated; the Lina Evelyn Hopkinson Scholarship is awarded to pupils at Wimbledon High School for English Literature.\n\nAt the Victoria University of Manchester the Electro-technical Laboratory (1912) in Coupland Street was named after him.\n\nHis sons Bertram and Cecil, wife Evelyn and daughter Ellen (married James Alfred Ewing in 1912) are buried in the Ascension Parish Burial Ground, Cambridge; the rest of the family are interred in Switzerland.\n\n\n\n"}
{"id": "14835725", "url": "https://en.wikipedia.org/wiki?curid=14835725", "title": "Journal of Algebraic Combinatorics", "text": "Journal of Algebraic Combinatorics\n\nJournal of Algebraic Combinatorics is a peer-reviewed scientific journal covering algebraic combinatorics. It is published by Springer Science+Business Media and was established in 1992. The editor-in-chief is Ilias S. Kotsireas (Wilfrid Laurier University).\n\nIn 2017, the journal's four editors-in-chief and editorial board resigned to protest the publisher's high prices and limited accessibility. They criticized Springer for \"double-dipping\", that is, charging large subscription fees to libraries in addition to high fees for authors who wished to make their publications open access. The board subsequently started their own open access journal, \"Algebraic Combinatorics\" which is published with the help of MathOA by Centre Mersenne.\n\nThe journal is abstracted and indexed in:\n\nAccording to the \"Journal Citation Reports\", the journal has a 2016 impact factor of 0.779.\n\n"}
{"id": "682809", "url": "https://en.wikipedia.org/wiki?curid=682809", "title": "Keysigning", "text": "Keysigning\n\nKeysigning refers to digitally signing someone else's public key using your own.\nA more correct term would be certificate signing, since the actual key material is not changed by the process of signing. However, in the PGP community it is customary not to distinguish in speaking between someone's key and certificate, and the term keysigning is used. (The term PGP refers here to all implementations of the OpenPGP standard, such as GnuPG.)\n\nUsers of PGP sign one another's keys to indicate to any third party that the signer trusts the signee. This enables someone who trusts the signer to extend her trust to the signee as well. In this way, a web of trust is built.\n\nPGP users often organize key signing parties, where many people meet in person to verify each other's identity using some printed certificate of identity and then sign each other's keys.\n\n\n"}
{"id": "1028214", "url": "https://en.wikipedia.org/wiki?curid=1028214", "title": "Kraków School of Mathematics", "text": "Kraków School of Mathematics\n\nKraków School of Mathematics () was a sub-group of Polish School of Mathematics represented by mathematicians from the Kraków universities—Jagiellonian University and the AGH University of Science and Technology, active during the interwar period (1918–1939). Their area of study was primarily classical analysis, differential equations and analytic functions.\n\nThe Kraków School of Differential Equations was founded by Tadeusz Ważewski, a student of Stanisław Zaremba, and was internationally appreciated after World War II. The Kraków School of Analytic Functions was founded by Franciszek Leja. Other notable members includes Kazimierz Żorawski, Władysław Ślebodziński, Stanisław Gołąb and Czesław Olech.\n\n"}
{"id": "13823014", "url": "https://en.wikipedia.org/wiki?curid=13823014", "title": "Krener's theorem", "text": "Krener's theorem\n\nIn mathematics, Krener's theorem is a result attributed to Arthur J. Krener in geometric control theory about the topological properties of attainable sets of finite-dimensional control systems. It states that any attainable set of a bracket-generating system has nonempty interior or, equivalently, that any attainable set has nonempty interior in the topology of the corresponding orbit. Heuristically, Krener's theorem prohibits attainable sets from being hairy.\n\nLet \nformula_1 \nbe a smooth control system, where \nformula_2 \nbelongs to a finite-dimensional manifold formula_3 and formula_4 belongs to a control set formula_5. Consider the family of vector fields formula_6.\n\nLet formula_7 be the Lie algebra generated by formula_8 with respect to the Lie bracket of vector fields. \nGiven formula_9, if the vector space formula_10 is equal to formula_11,\nthen formula_12 belongs to the closure of the interior of the attainable set from formula_12.\n\nEven if formula_14 is different from formula_11,\nthe attainable set from formula_12 has nonempty interior in the orbit topology,\nas it follows from Krener's theorem applied to the control system restricted to the orbit through formula_12.\n\nWhen all the vector fields in formula_18 are analytic, formula_19 if and only if formula_12 belongs to the closure of the interior of the attainable set from formula_12. This is a consequence of Krener's theorem and of the orbit theorem.\n\nAs a corollary of Krener's theorem one can prove that if the system is bracket-generating and if the attainable set from formula_9 is dense in formula_3, then the attainable set from formula_12\nis actually equal to formula_3.\n\n"}
{"id": "58997387", "url": "https://en.wikipedia.org/wiki?curid=58997387", "title": "Larisa Maksimova", "text": "Larisa Maksimova\n\nLarisa Lvovna Maksimova (, born 1943) is a Russian mathematical logician known for her research in non-classical logic.\n\nMaksimova was born on November 5, 1943, near Novosibirsk, the daughter of two biologists who had temporarily moved there from Tomsk State University to escape the war. She grew up in Novosibirsk, where her parents became geographers at the Novosibirsk Pedagogical Institute. She studied mechanics and mathematics at Novosibirsk State University, publishing her first paper on Wilhelm Ackermann's axioms for strict implication in relevance logic in 1964 and graduating in 1965.\n\nMeanwhile, in 1964, she joined the Sobolev Institute of Mathematics, and has remained there for the rest of her career. She defended her doctorate at Novosibirsk State University in 1968, a year after the death of her primary mentor at the university, Anatoly Maltsev. She completed a habilitation at the Sobolev Institute in 1986, and was promoted to full professor in 1993.\n\nMaksimova's books include\n\nMaksimova won the Maltsev Prize of the Russian Academy of Sciences in 2009, for her papers on definability and interpolation in non-classical logic.\nWith several others from the Sobolev Institute, she won the Russian Federation Government Prize in Education in 2010.\nShe is the subject of a festschrift, \"Larisa Maksimova on Implication, Interpolation, and Definability\" (Sergei Odintsov, ed., Springer, 2018).\n"}
{"id": "350906", "url": "https://en.wikipedia.org/wiki?curid=350906", "title": "List of geometry topics", "text": "List of geometry topics\n\nThis is a list of geometry topics, by Wikipedia page.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "205069", "url": "https://en.wikipedia.org/wiki?curid=205069", "title": "List of mathematical probabilists", "text": "List of mathematical probabilists\n\nThis list contains only probabilists in the sense of mathematicians specializing in probability theory.\n\n\n"}
{"id": "468313", "url": "https://en.wikipedia.org/wiki?curid=468313", "title": "Mask (computing)", "text": "Mask (computing)\n\nIn computer science, a mask is data that is used for bitwise operations, particularly in a bit field. Using a mask, multiple bits in a byte, nibble, word etc. can be set either on, off or inverted from on to off (or vice versa) in a single bitwise operation.\n\nTo turn certain bits on, the bitwise codice_2 operation can be used, following the principle that codice_3 and codice_4. Therefore, to make sure a bit is on, codice_2 can be used with a codice_1. To leave a bit unchanged, codice_2 is used with a codice_8.\n\nExample: Masking \"on\" the higher nibble (bits 4, 5, 6, 7) the lower nibble (bits 0, 1, 2, 3) unchanged. \n\nMore often in practice bits are \"masked \"off\"\" (or masked to codice_8) than \"masked \"on\"\" (or masked to codice_1). When a bit is codice_12ed with a 0, the result is always 0, i.e. codice_13. To leave the other bits as they were originally, they can be codice_12ed with codice_1, since codice_16.\n\nExample: Masking \"off\" the higher nibble (bits 4, 5, 6, 7) the lower nibble (bits 0, 1, 2, 3) unchanged. \n\nIt is possible to use bitmasks to easily check the state of individual bits regardless of the other bits. To do this, turning off all the other bits using the bitwise codice_12 is done as discussed above and the value is compared with codice_1. If it is equal to codice_8, then the bit was off, but if the value is any other value, then the bit was on. What makes this convenient is that it is not necessary to figure out what the value actually is, just that it is not codice_8.\n\nExample: Querying the status of the 4th bit\n\nSo far the article has covered how to turn bits on and turn bits off, but not both at once. Sometimes it does not really matter what the value is, but it must be made the opposite of what it currently is. This can be achieved using the codice_21 (exclusive or) operation. codice_21 returns codice_1 if and only if an odd number of bits are codice_1. Therefore, if two corresponding bits are codice_1, the result will be a codice_8, but if only one of them is codice_1, the result will be codice_1. Therefore inversion of the values of bits is done by codice_21ing them with a codice_1. If the original bit was codice_1, it returns codice_32. If the original bit was codice_8 it returns codice_34. Also note that codice_21 masking is bit-safe, meaning that it will not affect unmasked bits because codice_36, just like an codice_2.\n\nExample: Toggling bit values\n\nTo write arbitrary 1s and 0s to a subset of bits, first write 0s to that subset, then set the high bits:\n\nIn programming languages such as C, bit fields are a useful way to pass a set of named boolean arguments to a function. For example, in the graphics API OpenGL, there is a command, codice_38 which clears the screen or other buffers. It can clear up to four buffers (the color, depth, accumulation, and stencil buffers), so the API authors could have had it take four arguments. But then a call to it would look like\n\nwhich is not very descriptive. Instead there are four defined field bits, codice_39, codice_40, codice_41, and codice_42 and codice_38 is declared as\n\nThen a call to the function looks like this\n\nInternally, a function taking a bitfield like this can use binary codice_44 to extract the individual bits. For example, an implementation of codice_38 might look like:\n\nThe advantage to this approach is that function argument overhead is decreased. Since the minimum datum size is one byte, separating the options into separate arguments would be wasting seven bits per argument and would occupy more stack space. Instead, functions typically accept one or more 32-bit integers, with up to 32 option bits in each. While elegant, in the simplest implementation this solution is not type-safe. A codice_46 is simply defined to be an codice_47, so the compiler would allow a meaningless call to codice_48 or even codice_49. In C++ an alternative would be to create a class to encapsulate the set of arguments that glClear could accept and could be cleanly encapsulated in a library (see the external links for an example).\n\nMasks are used with IP addresses in IP ACLs (Access Control Lists) to specify what should be permitted and denied. To configure IP addresses on interfaces, masks start with 255 and have the large values on the left side: for example, IP address 209.165.202.129 with a 255.255.255.224 mask. Masks for IP ACLs are the reverse: for example, mask 0.0.0.255. This is sometimes called an inverse mask or a wildcard mask. When the value of the mask is broken down into binary (0s and 1s), the results determine which address bits are to be considered in processing the traffic. A 0 indicates that the address bits must be considered (exact match); a 1 in the mask is a \"don't care\". This table further explains the concept.\n\nMask example:\n\nnetwork address (traffic that is to be processed) \n10.1.1.0\n\nmask \n0.0.0.255\n\nnetwork address (binary) \n00001010.00000001.00000001.00000000\n\nmask (binary) \n00000000.00000000.00000000.11111111\n\nBased on the binary mask, it can be seen that the first three sets (octets) must match the given binary network address exactly (00001010.00000001.00000001). The last set of numbers is made of \"don't cares\" (.11111111). Therefore, all traffic that begins with 10.1.1. matches since the last octet is \"don't care\". Therefore, with this mask, network addresses 10.1.1.1 through 10.1.1.255 (10.1.1.x) are processed.\n\nSubtract the normal mask from 255.255.255.255 in order to determine the ACL inverse mask. In this example, the inverse mask is determined for network address 172.16.1.0 with a normal mask of 255.255.255.0.\n\n255.255.255.255 - 255.255.255.0 (normal mask) = 0.0.0.255 (inverse mask)\n\nACL equivalents\n\nThe source/source-wildcard of 0.0.0.0/255.255.255.255 means \"any\".\n\nThe source/wildcard of 10.1.1.2/0.0.0.0 is the same as \"host 10.1.1.2\"\n\nIn computer graphics, when a given image is intended to be placed over a background, the transparent areas can be specified through a binary mask. This way, for each intended image there are actually two bitmaps: the actual image, in which the unused areas are given a pixel value with all bits set to 0s, and an additional \"mask\", in which the correspondent image areas are given a pixel value of all bits set to 0s and the surrounding areas a value of all bits set to 1s. In the sample at right, black pixels have the all-zero bits and white pixels have the all-one bits.\n\nAt run time, to put the image on the screen over the background, the program first masks the screen pixel's bits with the image mask at the desired coordinates using the bitwise AND operation. This preserves the background pixels of the transparent areas while resets with zeros the bits of the pixels which will be obscured by the overlapped image.\n\nThen, the program renders the image pixel's bits by combining them with the background pixel's bits using the bitwise OR operation. This way, the image pixels are appropriately placed while keeping the background surrounding pixels preserved. The result is a perfect compound of the image over the background.\n\nThis technique is used for painting pointing device cursors, in typical 2-D videogames for characters, bullets and so on (the sprites), for GUI icons, and for video titling and other image mixing applications.\n\nAlthough related (due to being used for the same purposes), transparent colors and alpha channels are techniques which do not involve the image pixel mixage by binary masking.\n\nTo create a hashing function for a hash table, often a function is used that has a large domain. To create an index from the output of the function, a modulo can be taken to reduce the size of the domain to match the size of the array; however, it is often faster on many processors to restrict the size of the hash table to powers of two sizes and use a bitmask instead.\n\n\n"}
{"id": "6807932", "url": "https://en.wikipedia.org/wiki?curid=6807932", "title": "Minimum-cost flow problem", "text": "Minimum-cost flow problem\n\nThe minimum-cost flow problem (MCFP) is an optimization and decision problem to find the cheapest possible way of sending a certain amount of flow through a flow network. A typical application of this problem involves finding the best delivery route from a factory to a warehouse where the road network has some capacity and cost associated. The minimum cost flow problem is one of the most fundamental among all flow and circulation problems because most other such problems can be cast as a minimum cost flow problem and also that it can be solved very efficiently using the network simplex algorithm.\n\nA flow network is a directed graph formula_1 with a source vertex formula_2 and a sink vertex formula_3, where each edge formula_4 has capacity formula_5, flow formula_6 and cost formula_7, with most minimum-cost flow algorithms supporting edges with negative costs. The cost of sending this flow along an edge formula_8 is formula_9. The problem requires an amount of flow formula_10 to be sent from source formula_11 to sink formula_12.\n\nThe definition of the problem is to minimize the total cost of the flow over all edges:\n\nwith the constraints\n\nA variation of this problem is to find a flow which is maximum, but has the lowest cost among the maximum flow solutions. This could be called a minimum-cost maximum-flow problem and is useful for finding minimum cost maximum matchings.\n\nWith some solutions, finding the minimum cost maximum flow instead is straightforward. If not, one can find the maximum flow by performing a binary search on formula_10.\n\nA related problem is the minimum cost circulation problem, which can be used for solving minimum cost flow. This is achieved by setting the lower bound on all edges to zero, and then making an extra edge from the sink formula_12 to the source formula_11, with capacity formula_17 and lower bound formula_18, forcing the total flow from formula_11 to formula_12 to also be formula_10.\n\nThe problem can be specialized into two other problems：\n\nThe minimum cost flow problem can be solved by linear programming, since we optimize a linear function, and all constraints are linear.\n\nApart from that, many combinatorial algorithms exist, for a comprehensive survey, see . Some of them are generalizations of maximum flow algorithms, others use entirely different approaches.\n\nWell-known fundamental algorithms (they have many variations):\n\n\nGiven a bipartite graph \"G\" = (\"A\" ∪ \"B\", \"E\"), the goal is to find the maximum cardinality matching in \"G\" that has minimum cost. Let \"w\": \"E\" → \"R\" be a weight function on the edges of \"E\". The minimum weight bipartite matching problem or assignment problem is to find a \nperfect matching \"M\" ⊆ \"E\" whose total weight is minimized. The idea is to reduce this problem to a network flow problem.\n\nLet \"G’\" = (\"V’\" = \"A\" ∪ \"B\", \"E’\" = \"E\"). Assign the capacity of all the edges in \"E’\" to 1. Add a source vertex \"s\" and connect it to all the vertices in \"A’\" and add a sink \nvertex \"t\" and connect all vertices inside group \"B’\" to this vertex. The capacity of all the new edges is 1 and their costs is 0. It is proved that there is minimum weight perfect bipartite matching in \"G\" if and only if there a minimum cost flow in \"G’\". \n\n\n\n"}
{"id": "9966817", "url": "https://en.wikipedia.org/wiki?curid=9966817", "title": "Modes of convergence", "text": "Modes of convergence\n\nIn mathematics, there are many senses in which a sequence or a series is said to be convergent. This article describes various modes (senses or species) of convergence in the settings where they are defined. For a list of modes of convergence, see Modes of convergence (annotated index)\n\nNote that each of the following objects is a special case of the types preceding it: sets, topological spaces, uniform spaces, TAGs (topological abelian groups), normed spaces, Euclidean spaces, and the real/complex numbers. Also, note that any metric space is a uniform space.\n\nConvergence can be defined in terms of sequences in first-countable spaces. Nets are a generalization of sequences that is useful in spaces which are not first countable. Filters further generalize the concept of convergence.\n\nIn metric spaces, one can define Cauchy sequences. Cauchy nets and filters are generalizations to uniform spaces. Even more generally, Cauchy spaces are spaces in which Cauchy filters may be defined. Convergence implies \"Cauchy-convergence\", and Cauchy-convergence, together with the existence of a convergent subsequence implies convergence. The concept of completeness of metric spaces, and its generalizations is defined in terms of Cauchy sequences.\n\nIn a topological abelian group, convergence of a series is defined as convergence of the sequence of partial sums. An important concept when considering series is unconditional convergence, which guarantees that the limit of the series is invariant under permutations of the summands.\n\nIn a normed vector space, one can define absolute convergence as convergence of the series of norms (formula_1). Absolute convergence implies Cauchy convergence of the sequence of partial sums (by the triangle inequality), which in turn implies absolute-convergence of some grouping (not reordering). The sequence of partial sums obtained by grouping is a subsequence of the partial sums of the original series. The norm convergence of absolutely convergent series is an equivalent condition for a normed linear space to be Banach (i.e.: complete).\n\nAbsolute convergence and convergence together imply unconditional convergence, but unconditional convergence does not imply absolute convergence in general, even if the space is Banach, although the implication holds in formula_2.\n\nThe most basic type of convergence for a sequence of functions (in particular, it does not assume any topological structure on the domain of the functions) is pointwise convergence. It is defined as convergence of the sequence of values of the functions at every point. If the functions take their values in a uniform space, then one can define pointwise Cauchy convergence, uniform convergence, and uniform Cauchy convergence of the sequence.\n\nPointwise convergence implies pointwise Cauchy-convergence, and the converse holds if the space in which the functions take their values is complete. Uniform convergence implies pointwise convergence and uniform Cauchy convergence. Uniform Cauchy convergence and pointwise convergence of a subsequence imply uniform convergence of the sequence, and if the codomain is complete, then uniform Cauchy convergence implies uniform convergence.\n\nIf the domain of the functions is a topological space, local uniform convergence (i.e. uniform convergence on a neighborhood of each point) and compact (uniform) convergence (i.e. uniform convergence on all compact subsets) may be defined. Note that \"compact convergence\" is always short for \"compact uniform convergence,\" since \"compact pointwise convergence\" would mean the same thing as \"pointwise convergence\" (points are always compact).\n\nUniform convergence implies both local uniform convergence and compact convergence, since both are local notions while uniform convergence is global. If \"X\" is locally compact (even in the weakest sense: every point has compact neighborhood), then local uniform convergence is equivalent to compact (uniform) convergence. Roughly speaking, this is because \"local\" and \"compact\" connote the same thing.\n\nPointwise and uniform convergence of series of functions are defined in terms of convergence of the sequence of partial sums.\n\nFor functions taking values in a normed linear space, absolute convergence refers to convergence of the series of positive, real-valued functions formula_3 . \"Pointwise absolute convergence\" is then simply pointwise convergence of formula_3.\nNormal convergence is convergence of the series of non-negative real numbers obtained by taking the uniform (i.e. \"sup\") norm of each function in the series (uniform convergence of formula_3). In Banach spaces, pointwise absolute convergence implies pointwise convergence, and normal convergence implies uniform convergence.\n\nFor functions defined on a topological space, one can define (as above) local uniform convergence and compact (uniform) convergence in terms of the partial sums of the series. If, in addition, the functions take values in a normed linear space, then local normal convergence (local, uniform, absolute convergence) and compact normal convergence (absolute convergence on compact sets) can be defined.\n\nNormal convergence implies both local normal convergence and compact normal convergence. And if the domain is locally compact (even in the weakest sense), then local normal convergence implies compact normal convergence.\n\nIf one considers sequences of measurable functions, then several modes of convergence that depend on measure-theoretic, rather than solely topological properties, arise. This includes pointwise convergence almost-everywhere, convergence in \"p\"-mean and convergence in measure. These are of particular interest in probability theory.\n\n"}
{"id": "19304045", "url": "https://en.wikipedia.org/wiki?curid=19304045", "title": "Nikolai Andreevich Lebedev", "text": "Nikolai Andreevich Lebedev\n\nNikolai Andreevich Lebedev (; August 8, 1919 – January 8, 1982) was a Russian mathematician who worked on complex function theory and geometric function theory. Jointly with Isaak Milin, he proved the Lebedev–Milin inequalities that were used in the proof of the Bieberbach conjecture.\n\n\n\n"}
{"id": "409006", "url": "https://en.wikipedia.org/wiki?curid=409006", "title": "Object Constraint Language", "text": "Object Constraint Language\n\nThe Object Constraint Language (OCL) is a declarative language describing rules applying to Unified Modeling Language (UML) models developed at IBM and is now part of the UML standard. Initially, OCL was merely a formal specification language extension for UML. OCL may now be used with any Meta-Object Facility (MOF) Object Management Group (OMG) meta-model, including UML. The Object Constraint Language is a precise text language that provides constraint and object query expressions on any MOF model or meta-model that cannot otherwise be expressed by diagrammatic notation. OCL is a key component of the new OMG standard recommendation for transforming models, the Queries/Views/Transformations (QVT) specification.\n\nOCL is a descendant of Syntropy, a second-generation object-oriented analysis and design method. The OCL 1.4 definition specified a constraint language. In OCL 2.0, the definition has been extended to include general object query language definitions.\n\nOCL statements are constructed in four parts:\n\n\nOCL supplements UML by providing expressions that have neither the ambiguities of natural language nor the inherent difficulty of using complex mathematics. OCL is also a navigation language for graph-based models.\n\nOCL makes a Meta-Object Facility model more precise by associating assertions with its meta-elements.\n\nOf particular importance to Model Driven Engineering (MDE) or model-driven architecture is the notion of Model transformation. The OMG has defined a specific standard for model transformation called MOF/QVT or in short QVT. Several model transformation languages like GReAT, VIATRA, or Tefkat are presently available, with different levels of compliance with the QVT standard. Many of these languages are built on top of OCL, which is the main part of the QVT-compliance.\n\nBeing a rule-based validation language, Schematron may be considered an alternative to OCL. However Schematron works for Extensible Markup Language (XML) trees while OCL makes it possible to navigate MOF-based models and meta-models (i.e. XML Metadata Interchange (XMI) trees). In other words, OCL relates to UML or MOF similarly to how Schematron relates to XML. (Note that Schematron uses XPath to navigate inside the XML trees.)<br>Being a model specification language permitting designers to decorate a model or a meta-model with side-effect-free annotations, OCL could be replaced by languages like Alloy. Automated OCL generation is possible through Natural Language like NL2OCL.\n\n\n\n\n"}
{"id": "22055912", "url": "https://en.wikipedia.org/wiki?curid=22055912", "title": "Quantum spin model", "text": "Quantum spin model\n\nA quantum spin model is a quantum Hamiltonian model that describes a system which consists of spins either interacting or not and are an active area of research in the fields of strongly correlated electron systems, quantum information theory, and quantum computing. The physical observables in these quantum models are actually operators in a Hilbert space acting on state vectors as opposed to the physical observables in the corresponding classical spin models - like the Ising model - which are commutative variables.\n"}
{"id": "29719506", "url": "https://en.wikipedia.org/wiki?curid=29719506", "title": "Residue at infinity", "text": "Residue at infinity\n\nIn complex analysis, a branch of mathematics, the residue at infinity is a residue of a holomorphic function on an annulus having an infinite external radius. The \"infinity\" formula_1 is a point added to the local space formula_2 in order to render it compact (in this case it is a one-point compactification). This space noted formula_3 is isomorphic to the Riemann sphere. One can use the residue at infinity to calculate some integrals.\n\nGiven a holomorphic function \"f\" on an annulus formula_4 (centered at 0, with inner radius formula_5 and infinite outer radius), the residue at infinity of the function \"f\" can be defined in terms of the usual residue as follows:\n\nThus, one can transfer the study of formula_7 at infinity to the study of formula_8 at the origin.\n\nNote that formula_9, we have\n\n\n"}
{"id": "1068955", "url": "https://en.wikipedia.org/wiki?curid=1068955", "title": "Sign bit", "text": "Sign bit\n\nIn computer science, the sign bit is a bit in a signed number representation that indicates the sign of a number. Although only signed numeric data types have a sign bit, it is invariably located in the most significant bit position, so the term may be used interchangeably with \"most significant bit\" in some contexts.\n\nAlmost always, if the sign bit is 0, the number is non-negative (positive or zero). If the sign bit is 1 then the number is negative, although formats other than two's complement integers allow a signed zero: distinct \"positive zero\" and \"negative zero\" representations, the latter of which does not correspond to the mathematical concept of a negative number.\n\nIn the two's complement representation, the sign bit has the weight where is the number of bits. In the ones' complement representation, the most negative value is , but there are two representations of zero, one for each value of the sign bit. In a sign-and-magnitude representation of numbers, the value of the sign bit determines whether the numerical value is positive or negative.\n\nFloating point numbers, such as IEEE format, IBM format, VAX format, and even the format used by the Zuse Z1 and Z3 use a sign-and-magnitude representation.\n\nWhen using a complement representation, to convert a signed number to a wider format the additional bits must be filled with copies of the sign bit in order to preserve its numerical value, a process called \"sign extension\" or \"sign propagation\".\n"}
{"id": "2300482", "url": "https://en.wikipedia.org/wiki?curid=2300482", "title": "Smith's Prize", "text": "Smith's Prize\n\nThe Smith's Prize was the name of each of two prizes awarded annually to two research students in mathematics and theoretical physics at the University of Cambridge from 1769. Following the reorganization in 1998, it is now awarded under the name of Smith-Knight Prize and Rayleigh-Knight Prize.\n\nThe Smith Prize fund was founded by bequest of Robert Smith upon his death in 1768, having by his will left £3500 South Sea Company stock to the University. Every year two or more junior Bachelor of Arts students who had made the greatest progress in mathematics and natural philosophy were to be awarded a prize from the fund. The prize was awarded every year from 1769 to 1998 except 1917.\n\nFrom 1769 to 1885, the prize was awarded for the best performance in a series of examinations. In 1854 George Stokes included an examination question on a particular theorem which William Thomson had written to him about, which is now known as Stokes' theorem. T. W. Körner notes\n\nOnly a small number of students took the Smith's prize examination in the nineteenth century. When Karl Pearson took the examination in 1879, the examiners were Stokes, Maxwell, Cayley, and Todhunter and the examinees went on each occasion to an examiner's dwelling, did a morning paper, had lunch there and continued their work on the paper in the afternoon.\nIn 1885, the examination was renamed \"Part III\", (now known as the Certificate of Advanced Study in Mathematics) and the prize was awarded for the best submitted essay rather than examination performance. According to Barrow-Green\n\nBy fostering an interest in the study of applied mathematics, the competition contributed towards the success in mathematical physics that was to become the hallmark of Cambridge mathematics during the second half of the nineteenth century.\nIn the twentieth century, the competition stimulated postgraduate research in mathematics in Cambridge and the competition has played a significant role by providing a springboard for graduates considering an academic career. The majority of prize-winners have gone on to become professional mathematicians or physicists.\n\nThe Rayleigh Prize was an additional prize, which was awarded for the first time in 1911.\n\nThe Smith's and Rayleigh prizes were only available to Cambridge graduate students who had been undergraduates at Cambridge. The J.T. Knight Prize was established in 1974 for Cambridge graduates who had been undergraduates at other universities. The prize commemorates J.T. Knight (1942-1970), who had been an undergraduate student at Glasgow and a graduate student at Cambridge. He was killed in a motor car accident in Ireland in April 1970.\nOriginally, in 1769, the prizes were worth £25 each and remained at that level for 100 years. In 1867, they fell to £23 and in 1915 were still reported to be worth that amount. By 1930, the value had risen to about £30, and by 1940, the value had risen by a further one pound to £31. By 1998, a Smith’s Prize was worth around £250.\n\nIn 2007, the value of the three prize funds was roughly £175,000.\n\nIn 1998 the Smith Prize, Rayleigh Prize and J. T. Knight Prize were replaced by the Smith-Knight Prize and Rayleigh-Knight Prize, the standard for the former being higher than that required for the latter.\n\nFor the period up to 1940 a complete list is given in including titles of prize essays from 1889-1940. The following is a selection from this list.\n\nA more complete list of Rayleigh prize recipients is given in Appendix 1 (\"List of Prize Winners and their Essays 1885-1940\") of\n\n\n\n"}
{"id": "1337505", "url": "https://en.wikipedia.org/wiki?curid=1337505", "title": "Space diagonal", "text": "Space diagonal\n\nIn geometry a space diagonal (also interior diagonal or body diagonal) of a polyhedron is a line connecting two vertices that are not on the same face. Space diagonals contrast with face diagonals, which connect vertices on the same face (but not on the same edge) as each other.\n\nFor example, a pyramid has no space diagonals, while a cube (shown at right) or more generally a parallelepiped has four space diagonals.\n\nAn axial diagonal is a space diagonal that passes through the center of a polyhedron.\n\nFor example, in a cube with edge length \"a\", all four space diagonals are axial diagonals, of common length formula_1 More generally, a cuboid with edge lengths \"a\", \"b\", and \"c\" has all four space diagonals axial, with common length formula_2\n\nA regular octahedron has 3 axial diagonals, of length formula_3, with edge length \"a\". \n\nA regular icosahedron has 6 axial diagonals of length formula_4, where formula_5 is the golden ratio formula_6.\n\nFor a cube to be considered magic, the four space diagonals must each sum to the sum number as does each row, each column, and each pillar.\n\nThis section applies particularly to magic hypercubes.\n\nThe magic hypercube community has started to recognize an abbreviated expression for these \"space diagonals\". By using \"r\" as a variable to describe the various \"agonals\", a concise notation is possible.\n\nIf \"r\" = \nBy extension, if \"r\" =\n\nBecause the prefix \"pan\" indicates \"all\", we can concisely state the characteristics or a magic hypercube.\n\nFor example;\n\nThe length of an \"r\"-agonal of a hypercube with side length \"a\" is formula_7.\n\n\n\n"}
{"id": "24822937", "url": "https://en.wikipedia.org/wiki?curid=24822937", "title": "Superslow process", "text": "Superslow process\n\nSuperslow processes are processes in which values change so little that their capture is very difficult because of their smallness in comparison with the measurement error.\n\nMost of the time, the superslow processes lie beyond the scope of investigation due to the reason of their superslowness. Multiple gaps can be easily detected in biology, astronomy, physics, mechanics, economics, linguistics, ecology, gerontology, etc.\n\nTraditional scientific research in this area was focused on the describing some brain reactions.\n\nIn 2003, the term \"superslow processes\" was brought to wide attention by Russian mathematician Vladimir Miklyukov who founded \"Superslow Processes laboratory\" on the base of the Volgograd State University (Russia). Reports of seminar members are published yearly in \"Notes of the Seminar 'Superslow Processes\"'.\n\nIn mathematics, when the fluid flows through thin and long tubes it forms stagnation zones where the flow becomes almost immobile. If the ratio of tube length to its diameter is large, then the potential function and stream function are almost invariable on very extended areas. The situation seems uninteresting, but if we remember that these minor changes occur in the extra-long intervals, we see here a series of first-class tasks that require the development of special mathematical methods.\n\nApriori information regarding the stagnation zones contributes to optimization of the computational process by replacing the unknown functions with the corresponding constants in such zones. Sometimes this makes it possible to significantly reduce the amount of computation, for example in approximate calculation of conformal mapings of strongly elongated rectangles.\n\nThe obtained results are particularly useful for applications in economic geography. In a case where the function describes the intensity of commodity trade, a theorem about its stagnation zones gives us (under appropriate restrictions on the selected model) geometric dimensions estimates of the stagnation zone of the world-economy (for more information about a \"stagnation zone of the world-economy\", see Fernand Braudel, Les Jeux de L'echange).\n\nFor example, if the subarc of a domain boundary has zero transparency, and the flow of the gradient vector field of the function through the rest of the boundary is small enough, then the domain for such function is its stagnation zone.\n\nStagnation zones theorems are closely related to pre-Liouville's theorems about evaluation of solutions fluctuation, which direct consequences are the different versions of the classic Liouville theorem about conversion of the entire doubly periodic function into the identical constant.\n\nIdentification of what parameters impact the sizes of stagnation zones opens up opportunities for practical recommendations on targeted changes in configuration (reduction or increase) of such zones.\n\n"}
{"id": "1577896", "url": "https://en.wikipedia.org/wiki?curid=1577896", "title": "Symmetric graph", "text": "Symmetric graph\n\nIn the mathematical field of graph theory, a graph \"G\" is symmetric (or arc-transitive) if, given any two pairs of adjacent vertices \"u\"—\"v\" and \"u\"—\"v\" of \"G\", there is an automorphism\n\nsuch that\n\nIn other words, a graph is symmetric if its automorphism group acts transitively upon ordered pairs of adjacent vertices (that is, upon edges considered as having a direction). Such a graph is sometimes also called 1-arc-transitive or flag-transitive.\n\nBy definition (ignoring \"u\" and \"u\"), a symmetric graph without isolated vertices must also be vertex-transitive. Since the definition above maps one edge to another, a symmetric graph must also be edge-transitive. However, an edge-transitive graph need not be symmetric, since \"a\"—\"b\" might map to \"c\"—\"d\", but not to \"d\"—\"c\". Semi-symmetric graphs, for example, are edge-transitive and regular, but not vertex-transitive.\n\nEvery connected symmetric graph must thus be both vertex-transitive and edge-transitive, and the converse is true for graphs of odd degree. However, for even degree, there exist connected graphs which are vertex-transitive and edge-transitive, but not symmetric. Such graphs are called half-transitive. The smallest connected half-transitive graph is Holt's graph, with degree 4 and 27 vertices. Confusingly, some authors use the term \"symmetric graph\" to mean a graph which is vertex-transitive and edge-transitive, rather than an arc-transitive graph. Such a definition would include half-transitive graphs, which are excluded under the definition above.\n\nA distance-transitive graph is one where instead of considering pairs of adjacent vertices (i.e. vertices a distance of 1 apart), the definition covers two pairs of vertices, each the same distance apart. Such graphs are automatically symmetric, by definition.\n\nA \"t\"-arc is defined to be a sequence of \"t\"+1 vertices, such that any two consecutive vertices in the sequence are adjacent, and with any repeated vertices being more than 2 steps apart. A \"t\"-transitive graph is a graph such that the automorphism group acts transitively on \"t\"-arcs, but not on (\"t\"+1)-arcs. Since 1-arcs are simply edges, every symmetric graph of degree 3 or more must be \"t\"-transitive for some \"t\", and the value of \"t\" can be used to further classify symmetric graphs. The cube is 2-transitive, for example.\n\nCombining the symmetry condition with the restriction that graphs be cubic (i.e. all vertices have degree 3) yields quite a strong condition, and such graphs are rare enough to be listed. The Foster census and its extensions provide such lists. The Foster census was begun in the 1930s by Ronald M. Foster while he was employed by Bell Labs, and in 1988 (when Foster was 92) the then current Foster census (listing all cubic symmetric graphs up to 512 vertices) was published in book form. The first thirteen items in the list are cubic symmetric graphs with up to 30 vertices (ten of these are also distance-transitive; the exceptions are as indicated):\n\nOther well known cubic symmetric graphs are the Dyck graph, the Foster graph and the Biggs–Smith graph. The ten distance-transitive graphs listed above, together with the Foster graph and the Biggs–Smith graph, are the only cubic distance-transitive graphs.\n\nNon-cubic symmetric graphs include cycle graphs (of degree 2), complete graphs (of degree 4 or more when there are 5 or more vertices), hypercube graphs (of degree 4 or more when there are 16 or more vertices), and the graphs formed by the vertices and edges of the octahedron, icosahedron, cuboctahedron, and icosidodecahedron. The Rado graph forms an example of a symmetric graph with infinitely many vertices and infinite degree.\n\nThe vertex-connectivity of a symmetric graph is always equal to the degree \"d\". In contrast, for vertex-transitive graphs in general, the vertex-connectivity is bounded below by 2(\"d\" + 1)/3.\n\nA \"t\"-transitive graph of degree 3 or more has girth at least 2(\"t\" – 1). However, there are no finite \"t\"-transitive graphs of degree 3 or more for \"t\" ≥ 8. In the case of the degree being exactly 3 (cubic symmetric graphs), there are none for \"t\" ≥ 6.\n\n\n"}
{"id": "508070", "url": "https://en.wikipedia.org/wiki?curid=508070", "title": "Telescoping series", "text": "Telescoping series\n\nIn mathematics, a telescoping series is a series whose partial sums eventually only have a fixed number of terms after cancellation. The cancellation technique, with part of each term cancelling with part of the next term, is known as the method of differences.\n\nFor example, the series\n\n(the series of reciprocals of pronic numbers) simplifies as \n\nLet formula_3 be a sequence of numbers. Then, \n\nand, if formula_5\n\n\n\n\nIn probability theory, a Poisson process is a stochastic process of which the simplest case involves \"occurrences\" at random times, the waiting time until the next occurrence having a memoryless exponential distribution, and the number of \"occurrences\" in any time interval having a Poisson distribution whose expected value is proportional to the length of the time interval. Let \"X\" be the number of \"occurrences\" before time \"t\", and let \"T\" be the waiting time until the \"x\"th \"occurrence\". We seek the probability density function of the random variable \"T\". We use the probability mass function for the Poisson distribution, which tells us that\n\nwhere λ is the average number of occurrences in any time interval of length 1. Observe that the event {\"X\" ≥ x} is the same as the event {\"T\" ≤ \"t\"}, and thus they have the same probability. The density function we seek is therefore\n\nThe sum telescopes, leaving\n\nFor other applications, see:\n\n"}
{"id": "212390", "url": "https://en.wikipedia.org/wiki?curid=212390", "title": "Tilde", "text": "Tilde\n\nThe tilde ( or ; ˜ or ~) is a grapheme with several uses. The name of the character came into English from Spanish and from Portuguese, which in turn came from the Latin \"titulus\", meaning \"title\" or \"superscription\".\n\nThe reason for the name was that it was originally written over a letter as a scribal abbreviation, as a \"mark of suspension\", shown as a straight line when used with capitals. Thus the commonly used words \"Anno Domini\" were frequently abbreviated to \"A Dñi\", an elevated terminal with a suspension mark placed over the \"n\". Such a mark could denote the omission of one letter or several letters. This saved on the expense of the scribe's labour and the cost of vellum and ink. Medieval European charters written in Latin are largely made up of such abbreviated words with suspension marks and other abbreviations; only uncommon words were given in full. The tilde has since been applied to a number of other uses as a diacritic mark or a character in its own right. These are encoded in Unicode at and , and there are additional similar characters for different roles. In lexicography, the latter kind of tilde and the swung dash (⁓) are used in dictionaries to indicate the omission of the entry word.\n\nThis symbol (in English) informally means \"approximately\", \"about\", or \"around\", such as \"~30 minutes before\", meaning \"\"approximately\" 30 minutes before\". It can mean \"similar to\", including \"of the same order of magnitude as\", such as: \"\" meaning that and are of the same order of magnitude. Another approximation symbol is the double-tilde ≈, meaning \"approximately equal to\", the critical difference being the subjective level of accuracy: ≈ indicates a value which can be considered functionally equivalent for a calculation within an acceptable degree of error, whereas ~ is usually used to indicate a larger, possibly significant, degree of error. The tilde is also used to indicate \"equal to\" or \"approximately equal to\" by placing it over the \"=\" symbol, like so: ≅. In the computing field, especially in Unix based systems, the tilde indicates the user's home directory.\n\nThe text of the Domesday Book of 1086, relating for example, to the manor of Molland in Devon (see image left), is highly abbreviated as indicated by numerous tildes. The text with abbreviations expanded is as follows:\nThe incorporation of the tilde (~) into ASCII is a direct result of its appearance as a distinct character on mechanical typewriters in the late nineteenth century. When all character sets were pieces of metal permanently installed, and number of characters much more limited than in typography, the question of which languages and markets required which characters was an important one. Any good typewriter store had a catalog of alternative keyboards that could be specified for machines ordered from the factory.\n\nAt that time, the tilde was used only in Spanish and Portuguese typewriters (keyboards). In Modern Spanish, the tilde is used only with \"n\" and \"N\". Both were conveniently assigned to a single mechanical typebar, which sacrificed a key that was felt to be less important, usually the ½—¼ key.\n\nPortuguese, however, uses not \"ñ\" but \"nh\". It uses the tilde on the vowels \"a\" and \"o\". So as not to sacrifice two of the tightly limited keys to ã Ã õ Õ, the decision was made to make the ~ a separate \"dead\" character in which the carriage holding the paper did not move. Dead keys, which had a notch cut out to avoid hitting a mechanical linkage that triggered carriage movement, were used for characters that were intended to be combined (overstruck).\n\nOn mechanical typewriters, Spanish keyboards (the first, or one of the first, non-English keyboards) had a dead key, which contained the acute accent (´), used over any vowel, and the dieresis (¨), used only over \"u\". It was a simple matter to create a dead key for a Portuguese keyboard (created later than the Spanish one) to be overstruck with \"a\" and \"o\" and so the ~ was born as a typographical character, which did not exist previously as a type or hot-lead printing character. That was probably a product of the first and leading manufacturer of (mechanical) typewriters, Remington.\n\nAs indicated by the etymological origin of the word \"tilde\" in English, this symbol has been closely associated with the Spanish language. The connection stems from the use of the tilde above the letter \"n\" to form \"ñ\" in Spanish, a feature shared by only a few other languages, all historically connected to Spanish. This peculiarity can help non-native speakers quickly identify a text as being written in Spanish with little chance of error. In addition, most native speakers, although not all, use the word \"español\" to refer to their language. Particularly during the 1990s, Spanish-speaking intellectuals and news outlets demonstrated support for the language and the culture by defending this letter against globalisation and computerisation trends that threatened to remove it from keyboards and other standardised products and codes. The Instituto Cervantes, founded by Spain's government to promote the Spanish language internationally, chose as its logo a highly stylised Ñ with a large tilde. The 24-hour news channel CNN in the US later adopted a similar strategy on its existing logo for the launch of its Spanish-language version. And similarly to the National Basketball Association (NBA), the Spain men's national basketball team is nicknamed \"ÑBA\".\n\nConfusingly, in Spanish itself the word \"tilde\" is used more generally for diacritics, including the stress-marking acute accent. The diacritic \"~\" is more commonly called \"la virgulilla\" or \"la tilde de la eñe\", and is not considered an accent mark in Spanish, but rather simply a part of the letter ñ (much like the dot over the i).\n\nIn some languages, the tilde is used as a diacritical mark ( ˜ ) placed over a letter to indicate a change in pronunciation, such as nasalization.\n\nIt was first used in the polytonic orthography of Ancient Greek, as a variant of the circumflex, representing a rise in pitch followed by a return to standard pitch.\n\nLater, it was used to make abbreviations in medieval Latin documents. When an or followed a vowel, it was often omitted, and a tilde (i.e., a small ) was placed over the preceding vowel to indicate the missing letter; this is the origin of the use of tilde to indicate nasalization (compare the development of the umlaut as an abbreviation of .) The practice of using the tilde over a vowel to indicate omission of an or continued in printed books in French as a means of reducing text length until the 17th century. It was also used in Portuguese, and Spanish.\n\nThe tilde was also used occasionally to make other abbreviations, such as over the letter (\"q̃\") to signify the word \"que\" (\"that\").\n\nIt is also as a small that the tilde originated when written above other letters, marking a Latin which had been elided in old Galician-Portuguese. In modern Portuguese it indicates nasalization of the base vowel: \"mão\" \"hand\", from Lat. \"manu-\"; \"razões\" \"reasons\", from Lat. \"rationes\". This usage has been adopted in the orthographies of several native languages of South America, such as Guarani and Nheengatu, as well as in the International Phonetic Alphabet (IPA) and many other phonetic alphabets. For example, is the IPA transcription of the pronunciation of the French place-name \"Lyon\".\n\nIn Breton, the symbol after a vowel means that the letter serves only to give the vowel a nasalised pronunciation, without being itself pronounced, as it normally is. For example, gives the pronunciation whereas gives .\n\nThe tilded (, ) developed from the digraph in Spanish. In this language, is considered a separate letter called \"eñe\" (), rather than a letter-diacritic combination; it is placed in Spanish dictionaries between the letters and . In Spanish the word \"tilde\" can refer to diacritics in general, e.g. the acute accent in \"José\", and the diacritic in can also be called \"virgulilla\". Current languages in which the tilded () is used for the palatal nasal consonant include:\n\nIn Vietnamese, a tilde over a vowel represents a creaky rising tone (\"ngã\").\n\nIn phonetics, a tilde is used as a diacritic that is placed above a letter, below it or superimposed onto the middle of it:\n\n\nIn Estonian, the symbol stands for the close-mid back unrounded vowel, and it is considered an independent letter.\n\nSome languages and alphabets use the tilde for other purposes:\n\nThe following letters using the tilde as a diacritic exist as precomposed or combining Unicode characters:\n\nThere are many Unicode characters for tildes, symbols incorporating tildes, and characters visually similar to a tilde:\nMost modern proportional fonts align plain spacing tilde at the same level as dashes, or only slightly upper. This distinguishes it from a \"small tilde\" ( ˜ ), which is always raised. But in some monospace fonts, especially used in text user interfaces, ASCII tilde character is raised too. This apparently is a legacy of typewriters, where pairs of similar spacing and combining characters relied on one glyph. Even in line printers' age character repertoires were often not large enough to distinguish between plain tilde, small tilde and combining tilde. Overprinting of a letter by the tilde was a working method of combining a letter.\n\nThe tilde (~) is used in various ways in punctuation:\n\nIn some languages (though not generally in English), a tilde-like wavy dash may be used as punctuation (instead of an unspaced hyphen, en dash or em dash) between two numbers, to indicate a range rather than subtraction or a hyphenated number (such as a part number or model number). For example, \"12~15\" means \"12 to 15\", \"~3\" means \"up to three\", and \"100~\" means \"100 and greater\". Japanese and other East Asian languages almost always use this convention, but it is often done for clarity in some other languages as well. Chinese uses the wavy dash and full-width em dash interchangeably for this purpose. In English, the tilde is often used to express ranges and model numbers in electronics, but rarely in formal grammar or in type-set documents, as a wavy dash preceding a number sometimes represents an approximation (see below).\n\nBefore a number the tilde can mean \"approximately\"; \"~42\" means \"approximately 42\".\n\nThe is used for various purposes in Japanese, including to denote ranges of numbers, in place of dashes or brackets, and to indicate origin. The wave dash is also used to separate a title and a subtitle in the same line, as a colon is used in English.\n\nWhen used in conversations via email or instant messenger it may be used as a sarcasm mark.\n\nThe sign is used as a replacement for the chouon, katakana character, in Japanese, extending the final syllable.\n\nIn practice the , Unicode U+FF5E, is often used instead of the , Unicode U+301C, because the Shift JIS code for the wave dash, 0x8160, which is supposed to be mapped to U+301C, is instead mapped to U+FF5E in Windows code page 932 (Microsoft's code page for Japanese), a widely used extension of Shift JIS.\n\nThis avoided a shape definition error in the Unicode code charts: the wave dash reference glyph in JIS / Shift JIS matches the Unicode reference glyph for U+FF5E, while the reference glyph for U+301C was reflected, incorrectly, when Unicode imported the JIS wave dash. In other platforms such as the classic Mac OS and macOS, 0x8160 is correctly mapped to U+301C. It is generally difficult, if not impossible, for users of Japanese Windows to type U+301C, especially in legacy, non-Unicode applications.\n\nA similar situation exists regarding the Korean KS X 1001 character set, in which Microsoft maps the EUC-KR or UHC code for the wave dash (0xA1AD) to U+223C (Tilde Operator), while IBM and Apple map it to U+301C.\n\nThe current Unicode reference glyph for U+301C has been corrected to match the JIS standard in response to a 2014 proposal, which noted that while the existing Unicode reference glyph had been matched by fonts from the discontinued Windows XP, all other major platforms including later versions of Microsoft Windows matched the JIS reference glyph for U+301C.\n\nThe JIS / Shift JIS wave dash is still formally mapped to U+301C as of JIS X 0213, whereas the WHATWG Encoding Standard used by HTML5 follows Microsoft in mapping 0x8160 to U+FF5E. These two code points have a similar or identical glyph in several fonts, reducing the confusion and incompatibility.\n\nA tilde in front of a single quantity can mean \"approximately\", \"about\" or \"of the same order of magnitude as.\"\n\nIn written mathematical logic, the tilde represents negation: \"~\"p\"\" means \"not \"p\", where \"p\"\" is a proposition. Modern use often replaces the tilde with the negation symbol (¬) for this purpose, to avoid confusion with equivalence relations.\n\nIn mathematics, the tilde operator (Unicode U+223C), sometimes called \"twiddle\", is often used to denote an equivalence relation between two objects. Thus \"\" means \" is equivalent to \". It is a weaker statement than stating that equals . The expression \"\" is sometimes read aloud as \" twiddles \", perhaps as an analogue to the verbal expression of \"\".\n\nThe tilde can indicate approximate equality in a variety of ways. It can be used to denote the asymptotic equality of two functions. For example, means that .\n\nA tilde is also used to indicate \"approximately equal to\" (e.g. 1.902 ~= 2). This usage probably developed as a typed alternative to the used for the same purpose in written mathematics, which is an equal sign with the upper bar replaced by a bar with an upward hump, bump, ︎or loop in the middle (︍︍♎︎) or, sometimes, a tilde (≃). The symbol \"≈\" is also used for this purpose.︎\n\nIn physics and astronomy, a tilde can be used between two expressions (e.g. ) to state that the two are of the same order of magnitude.\n\nIn statistics and probability theory, the tilde means \"is distributed as\"; see random variable.\n\nA tilde can also be used to represent geometric similarity (e.g. , meaning triangle is similar to ). A triple tilde (≋) is often used to show congruence, an equivalence relation in geometry.\n\nThe symbol \"formula_1\" is pronounced as \"eff tilde\" or, informally, as \"eff twiddle\" or, in American English, \"eff wiggle\". This can be used to denote the Fourier transform of \"f\", or a lift of \"f\", and can have a variety of other meanings depending on the context.\n\nA tilde placed below a letter in mathematics can represent a vector quantity (e.g. formula_2).\n\nIn statistics and probability theory, a tilde placed on top of a variable is sometimes used to represent the median of that variable; thus formula_3 would indicate the median of the variable formula_4. A tilde over the letter n (formula_5) is sometimes used to indicate the harmonic mean.\n\nIn machine learning, a tilde may represent a candidate value for a cell state in GRUs or LSTM units. (e.g. c̃)\n\nOften in physics, one can consider an equilibrium solution to an equation, and then a perturbation to that equilibrium. For the variables in the original equation (for instance formula_6) a substitution formula_7 can be made, where formula_8 is the equilibrium part and formula_9 is the perturbed part.\n\nA tilde is also used in particle physics to denote the hypothetical supersymmetric partner. For example, an electron is referred to by the letter \"e\", and its superpartner the selectron is written \"ẽ\".\n\nFor relations involving preference, economists sometimes use the tilde to represent indifference between two or more bundles of goods. For example, to say that a consumer is indifferent between bundles \"x\" and \"y\", an economist would write \"x\" ~ \"y\".\n\nIt can approximate the sine wave symbol (∿, U+223F), which is used in electronics to indicate alternating current, in place of +, −, or ⎓ for direct current.\n\nOn Unix-like operating systems (including AIX, BSD, Linux and macOS), tilde normally indicates the current user's home directory. For example, if the current user's home directory is , then the command is equivalent to , , or . This convention derives from the Lear-Siegler ADM-3A terminal in common use during the 1970s, which happened to have the tilde symbol and the word \"Home\" (for moving the cursor to the upper left) on the same key. When prepended to a particular username, the tilde indicates that user's home directory (e.g., for the home directory of user , such as ).\n\nUsed in URLs on the World Wide Web, it often denotes a personal website on a Unix-based server. For example, might be the personal web site of John Doe. This mimics the Unix shell usage of the tilde. However, when accessed from the web, file access is usually directed to a subdirectory in the user's home directory, such as or .\n\nIn URLs, the characters (or ) may substitute for tilde if an input device lacks a tilde key. Thus, and will behave in the same manner.\n\nThe tilde is used in the AWK programming language as part of the pattern match operators for regular expressions:\n\nA variant of this, with the plain tilde replaced with codice_3, was adopted in Perl, and this semi-standardization has led to the use of these operators in other programming languages, such as Ruby or the SQL variant of the database PostgreSQL.\n\nIn APL and MATLAB, tilde represents the monadic logical function NOT.\n\nIn the C, C++ and C# programming languages, the tilde character is used as bitwise NOT operator, following the notation in logic (an codice_4 causes a logical NOT, instead). In C++ and C#, the tilde is also used as the first character in a class's method name (where the rest of the name must be the same name as the class) to indicate a destructor – a special method which is called at the end of the object's life.\n\nIn ASP.NET application tilde ('~') is used as a shortcut to the root of the application's virtual directory.\n\nIn the CSS stylesheet language, the tilde is used for the indirect adjacent combinator as part of a selector.\n\nIn the D programming language, the tilde is used as an array concatenation operator, as well as to indicate an object destructor and bitwise not operator. Tilde operator can be overloaded for user types, and binary tilde operator is mostly used to merging two objects, or adding some objects to set of objects. It was introduced because plus operator can have different meaning in many situations. For example, what to do with \"120\" + \"14\" ? Is this a string \"134\" (addition of two numbers), or \"12014\" (concatenation of strings) or something else? D disallows + operator for arrays (and strings), and provides separate operator for concatenation (similarly PHP programming language solved this problem by using dot operator for concatenation, and + for number addition, which will also work on strings containing numbers).\n\nIn Eiffel, the tilde is used for object comparison. If \"a\" and \"b\" denote objects, the boolean expression \"a\" ~ \"b\" has value true if and only if these objects are equal, as defined by the applicable version of the library routine \"is_equal\", which by default denotes field-by-field object equality but can be redefined in any class to support a specific notion of equality. If \"a\" and \"b\" are references, the object equality expression \"a\" ~ \"b\" is to be contrasted with \"a\" = \"b\" which denotes reference equality. Unlike the call \"a\".\"is_equal\" (\"b\"), the expression \"a\" ~ \"b\" is type-safe even in the presence of covariance.\n\nIn the Apache Groovy programming language the tilde character is used as an operator mapped to the bitwiseNegate() method. Given a String the method will produce a java.util.regex.Pattern. Given an integer it will negate the integer bitwise like in different C variants. codice_3 and codice_6 can in Groovy be used to match a regular expression.\n\nIn Haskell, the tilde is used in type constraints to indicate type equality. Also, in pattern-matching, the tilde is used to indicate a lazy pattern match.\n\nIn the Inform programming language, the tilde is used to indicate a quotation mark inside a quoted string. \n\nIn \"text mode\" of the LaTeX typesetting language a tilde diacritic can be obtained using, e.g., codice_7, yielding \"ñ\". A stand-alone tilde can be obtained by using codice_8 or codice_9.\nIn \"math mode\" a tilde diacritic can be written as, e.g., codice_10. For a wider tilde codice_11 can be used. The codice_12 command produce a tilde-like binary relation symbol that is often used in mathematical expressions, and the double-tilde ≈ is obtained with codice_13. The codice_14 package also supports entering tildes directly, e.g., codice_15.\nIn both text and math mode, a tilde on its own (codice_16) renders a white space with no line breaking.\n\nIn MediaWiki syntax, four tildes are used as a shortcut for a user's signature.\n\nIn Common Lisp, the tilde is used as the prefix for format specifiers in format strings.\nIn Max/MSP, a tilde is used to denote objects that process at the computer's sampling rate, i.e. mainly those that deal with sound.\n\nIn Standard ML, the tilde is used as the prefix for negative numbers and as the unary negation operator.\n\nIn OCaml, the tilde is used to specify the label for a labeled parameter.\n\nIn Microsoft's SQL Server Transact-SQL (T-SQL) language, the tilde is a unary Bitwise NOT operator.\n\nIn JavaScript, the tilde is used as a unary bitwise complement (or bitwise negation) operation (codice_17). Because JavaScript internally uses floats and the bitwise complement only works on integers, numbers are stripped of their decimal part before applying the operation. This has also given rise to using two tildes codice_18 as a short syntax for a cast to integer (numbers are stripped of their decimal part and changed into their complement, and then back. The net result is thus only the removal of the decimal part). For positive numbers, this is equivalent to the mathematical \"floor\" function.\n\nIn Object REXX, the twiddle is used as a \"message send\" symbol. For example, codice_19 would cause the codice_20 method to act on the object codice_21's codice_22 attribute, returning the result of the operation. codice_23 returns the object that received the method rather than the result produced. Thus it can be used when the result need not be returned or when cascading methods are to be used. codice_24 would send multiple concurrent codice_25 messages, thus invoking the codice_25 method three consecutive times on the codice_27 object.\n\nThe dominant Unix convention for naming backup copies of files is appending a tilde to the original file name.\nIt originated with the Emacs text editor and was adopted by many other editors and some command-line tools.\n\nEmacs also introduced an elaborate numbered backup scheme, with files named , and so on. It didn't catch on, as the rise of version control software eliminates the need for this usage.\n\nThe tilde was part of Microsoft's filename mangling scheme when it extended the FAT file system standard to support long filenames for Microsoft Windows. Programs written prior to this development could only access filenames in the so-called 8.3 format—the filenames consisted of a maximum of eight characters from a restricted character set (e.g. no spaces), followed by a period, followed by three more characters. In order to permit these legacy programs to access files in the FAT file system, each file had to be given two names—one long, more descriptive one, and one that conformed to the 8.3 format. This was accomplished with a name-mangling scheme in which the first six characters of the filename are followed by a tilde and a digit. For example, \" might become \".\n\nThe tilde symbol is also often used to prefix hidden temporary files that are created when a document is opened in Windows. For example, when a document \"Document1.doc\" is opened in Word, a file called \"~$cument1.doc\" is created in the same directory. This file contains information about which user has the file open, to prevent multiple users from attempting to change a document at the same time.\n\nComputer programmers use the tilde in various ways and sometimes call the symbol (as opposed to the diacritic) a squiggle, squiggly, or twiddle. According to the Jargon File, other synonyms sometimes used in programming include not, approx, wiggle, enyay (after \"eñe\") and (humorously) sqiggle . It is used in many languages as a binary inversion operator, swapping a number's binary 1's and 0's for example ~10 (binary ~1010) is equal to 5 (binary 0101).\n\nIn Perl 6, \"<nowiki>~~</nowiki>\" is used instead of \"=~\".\n\nIn the juggling notation system Beatmap, tilde can be added to either \"hand\" in a pair of fields to say \"cross the arms with this hand on top\". Mills Mess is thus represented as (~2x,1)(1,2x)(2x,~1)*.\n\nWhere a tilde is on the keyboard depends on the computer's language settings according to the following chart. On many keyboards it is primarily available through a dead key that makes it possible to produce a variety of precomposed characters with the diacritic. In that case, a single tilde can typically be inserted with the dead key followed by the space bar, or alternatively by striking the dead key twice in a row.\n\nTo insert a tilde with the dead key, it is often necessary to simultaneously hold down the Alt Gr key. On the keyboard layouts that include an \"Alt Gr\" key, it typically takes the place of the right-hand Alt key. With a Macintosh either of the Alt/Option keys function similarly.\n\nIn the US and European Windows systems, the Alt code for a single tilde is codice_28.\n\nFor Mac use option+'n' key\n\n\n"}
{"id": "32316", "url": "https://en.wikipedia.org/wiki?curid=32316", "title": "Unary numeral system", "text": "Unary numeral system\n\nThe unary numeral system is the bijective base-1 numeral system. It is the simplest numeral system to represent natural numbers: in order to represent a number \"N\", an arbitrarily chosen symbol representing 1 is repeated \"N\" times. For example, the numbers 1, 2, 3, 4, 5, ... would be represented in this system as\nThese numbers should be distinguished from repunits, which are also written as sequences of ones but have their usual decimal numerical interpretation.\n\nThis system is used in tallying. For example, using the tally mark |, the number 3 is represented as |||. In East Asian cultures, the number three is represented as “三” (1 and 2 are represented the same way), a character that is drawn with three strokes.\n\nAddition and subtraction are particularly simple in the unary system, as they involve little more than string concatenation. The Hamming weight or population count operation that counts the number of nonzero bits in a sequence of binary values may also be interpreted as a conversion from unary to binary numbers. However, multiplication is more cumbersome and has often been used as a test case for the design of Turing machines.\n\nCompared to standard positional numeral systems, the unary system is inconvenient and hence is not used in practice for large calculations. It occurs in some decision problem descriptions in theoretical computer science (e.g. some P-complete problems), where it is used to \"artificially\" decrease the run-time or space requirements of a problem. For instance, the problem of integer factorization is suspected to require more than a polynomial function of the length of the input as run-time if the input is given in binary, but it only needs linear runtime if the input is presented in unary. However, this is potentially misleading. Using a unary input is slower for any given number, not faster; the distinction is that a binary (or larger base) input is proportional to the base 2 (or larger base) logarithm of the number while unary input is proportional to the number itself. Therefore, while the run-time and space requirement in unary looks better as function of the input size, it does not represent a more efficient solution.\n\nIn computational complexity theory, unary numbering is used to distinguish strongly NP-complete problems from problems that are NP-complete but not strongly NP-complete. A problem in which the input includes some numerical parameters is strongly NP-complete if it remains NP-complete even when the size of the input is made artificially larger by representing the parameters in unary. For such a problem, there exist hard instances for which all parameter values are at most polynomially large.\n\nUnary is used as part of some data compression algorithms such as Golomb coding. It also forms the basis for the Peano axioms for formalizing arithmetic within mathematical logic.\nA form of unary notation called Church encoding is used to represent numbers within lambda calculus.\n\n"}
{"id": "33868930", "url": "https://en.wikipedia.org/wiki?curid=33868930", "title": "Veblen's theorem", "text": "Veblen's theorem\n\nIn mathematics, Veblen's theorem, introduced by , states that the set of edges of a finite graph can be written as a union of disjoint simple cycles if and only if every vertex has even degree. Thus, it is closely related to the theorem of that a finite graph has an Euler tour (a single non-simple cycle that covers the edges of the graph) if and only if it is connected and every vertex has even degree. Indeed, a representation of a graph as a union of simple cycles may be obtained from an Euler tour by repeatedly splitting the tour into smaller cycles whenever there is a repeated vertex. However, Veblen's theorem applies also to disconnected graphs, and can be generalized to infinite graphs in which every vertex has finite degree .\n\nIf a countably infinite graph \"G\" has no odd-degree vertices, then it may be written as a union of disjoint (finite) simple cycles if and only if every finite subgraph of \"G\" can be extended (by adding more edges and vertices of \"G\") to a finite Eulerian graph. In particular, every countably infinite graph with only one end and with no odd vertices can be written as a union of disjoint cycles .\n\n\n"}
{"id": "20088130", "url": "https://en.wikipedia.org/wiki?curid=20088130", "title": "Vickrey–Clarke–Groves auction", "text": "Vickrey–Clarke–Groves auction\n\nA Vickrey–Clarke–Groves (VCG) auction is a type of sealed-bid auction of multiple items. Bidders submit bids that report their valuations for the items, without knowing the bids of the other bidders. The auction system assigns the items in a socially optimal manner: it charges each individual the harm they cause to other bidders. It gives bidders an incentive to bid their true valuations, by ensuring that the optimal strategy for each bidder is to bid their true valuations of the items. It is a generalization of a Vickrey auction for multiple items.\n\nThe auction is named after William Vickrey, Edward H. Clarke, and Theodore Groves for their papers that successively generalized the idea.\n\nThe VCG auction is a specific use of the more general VCG Mechanism. While the VCG auction tries to make a socially optimal allocation of items, VCG mechanisms allow for the selection of a socially optimal outcome out of a set of possible outcomes.\n\nConsider an auction where a set of identical products are being sold. Bidders can take part in the auction by announcing the maximum price they are willing to pay to receive N products. Each buyer is allowed to declare more than one bid, since its willingness-to-pay per unit might be different depending on the total number of units it receives. Bidders cannot see other people's bids at any moment since they are sealed (only visible to the auction system). Once all the bids are made, the auction is closed.\n\nAll the possible combinations of bids are then considered by the auction system, and the one maximizing the total sum of bids is kept, with the condition that it does not exceed the total amount of products available and that at most one bid from each bidder can be used. Bidders who have made a successful bid then receive the product quantity specified in their bid. The price they pay in exchange, however, is not the amount they had bid initially but only the marginal harm their bid has caused to other bidders (which is at most as high as their original bid).\n\nThis marginal harm caused to other participants (i.e. the final price paid by each individual with a successful bid) can be calculated as: (sum of bids of the auction from the second best combination of bids) - (what other bidders have bid in the current (best) combination of bids). If the sum of bids of the second best combination of bids is the same as that of the best combination, then the price paid by the buyers will be the same as their initial bid. In all other cases, the price paid by the buyers will be lower.\n\nAt the end of the auction, the total utility has been maximized since all the goods have been attributed to the people with the highest combined willingness-to-pay. If agents are fully rational and in the absence of collusion, we can assume that the willingness to pay have been reported truthfully since only the marginal harm to other bidders will be charged to each participant, making truthful reporting a weakly-dominant strategy. This type of auction, however, will not maximize the seller's revenue unless the sum of bids of the second best combination of bids is equal to the sum of bids of the best combination of bids.\n\nFor any set of auctioned items formula_1 and any set of bidders formula_2, let formula_3 be the social value of the VCG auction for a given bid-combination. That is, how much each person values the items they've just won, added up across everyone. The value of the item is zero if they do not win. For a bidder formula_4 and item formula_5, let the bidder's bid for the item be formula_6. The notation formula_7 means the set of elements of A which are not elements of B.\nA bidder formula_4 whose bid for an item formula_5 is an \"overbid\", namely formula_6, wins the item, but pays formula_11, which is the social cost of their winning that is incurred by the rest of the agents.\nIndeed, the set of bidders other than formula_4 is formula_13. When item formula_5 is available, they could attain welfare formula_15 The winning of the item by formula_4 reduces the set of available items to formula_17, however, so that the attainable welfare is now formula_18. The difference between the two levels of welfare is therefore the loss in attainable welfare suffered by the rest of the bidders, as predicted, given the winner formula_4 got the item formula_5. This quantity depends on the offers of the rest of the agents and is unknown to agent formula_4.\n\"The winning bidder whose bid is the true value formula_22 for the item formula_5, formula_24 derives maximum utility formula_25\"\n\nSuppose two apples are being auctioned among three bidders.\n\nFirst, the outcome of the auction is determined by maximizing bids: the apples go to bidder A and bidder B, since their combined bid of $5 + $2 = $7 is greater than the bid for two apples by bidder C who is willing to pay only $6. Thus, after the auction, the value achieved by bidder A is $5, by bidder B is $2, and by bidder C is $0 (since bidder C gets nothing). Note that the determination of winners is essentially a knapsack problem.\n\nNext, the formula for deciding payments gives:\n\nAfter the auction, A is $1 better off than before (paying $4 to gain $5 of utility), B is $1 better off than before (paying $1 to gain $2 of utility), and C is neutral (having not won anything).\n\nAssume that there are two bidders, formula_26 and formula_27, two items, formula_28 and formula_29, and each bidder is allowed to obtain one item. We let formula_30 be bidder formula_4's valuation for item formula_5. Assume formula_33, formula_34, formula_35, and formula_36. We see that both formula_26 and formula_27 would prefer to receive item formula_28; however, the socially optimal assignment gives item formula_28 to bidder formula_26 (so their achieved value is formula_42) and item formula_29 to bidder formula_27 (so their achieved value is formula_45). Hence, the total achieved value is formula_46, which is optimal.\n\nIf person formula_27 were not in the auction, person formula_26 would still be assigned to formula_28, and hence person formula_26 can gain nothing. The current outcome is formula_42 hence formula_27 is charged formula_53.\n\nIf person formula_26 were not in the auction, formula_28 would be assigned to formula_27, and would have valuation formula_57. The current outcome is 3 hence formula_26 is charged formula_59.\n\nA multiple item auction with formula_60 bidders, formula_60 houses, and values formula_62, representing the value player formula_63 has for house formula_64. Possible outcomes are characterized by bipartite matchings, pairing houses with people.\nIf we know the values, then maximizing social welfare reduces to computing a maximum weight bipartite matching.\n\nIf we do not know the values, then we instead solicit bids formula_65, asking each player formula_63 how much they would wish to bid for house formula_64.\nDefine formula_68 if bidder formula_63 receives house formula_70 in the matching formula_71. Now compute formula_72, a maximum weight\nbipartite matching with respect to the bids, and compute\n\nThe first term is another max weight bipartite matching, and the second term can be computed easily from formula_72.\n\nThe following is a proof that bidding one's true valuations for the auctioned items is optimal.\n\nFor each bidder formula_4, let formula_76 be their true valuation of an item formula_77, and suppose (without loss of generality) that formula_4 wins formula_77 upon submitting their true valuations.\nThen the net utility formula_80 attained by formula_4 is given by their own valuation of the item they've won, minus the price they've paid: \n\nAs formula_83 is independent of formula_76, the maximization of net utility is pursued by the mechanism along with the maximization of corporate gross utility formula_85 for the declared bid formula_76.\n\nTo make it clearer, let us form the difference formula_87 between net utility formula_80 of formula_4 under truthful bidding formula_76 gotten item formula_77, and net utility formula_92 of bidder formula_4 under non-truthful bidding formula_94 for item formula_77 gotten item formula_5 on true utility formula_97.\n\nformula_98 is the corporate gross utility obtained with the non-truthful bidding. But the allocation assigning formula_5 to formula_4 is different from the allocation assigning formula_77 to formula_4 which gets maximum (true) gross corporate utility. Hence formula_103 and formula_104 q.e.d.\n\n"}
{"id": "57164705", "url": "https://en.wikipedia.org/wiki?curid=57164705", "title": "W. T. and Idalia Reid Prize", "text": "W. T. and Idalia Reid Prize\n\nThe W. T. and Idalia Reid Prize is an annual award presented by the Society for Industrial and Applied Mathematics (SIAM) for outstanding research in, or other contributions to, the broadly defined areas of differential equations and control theory. It was established in 1994 in memory of long-time University of Oklahoma mathematics professor W. T. Reid, who died in 1977.\nThe recipients of the W .T. and Idalia Reid Prize are:\n\n"}
