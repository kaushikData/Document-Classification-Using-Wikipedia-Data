{"id": "49840293", "url": "https://en.wikipedia.org/wiki?curid=49840293", "title": "3x + 1 semigroup", "text": "3x + 1 semigroup\n\nIn algebra, the 3\"x\" + 1 semigroup is a special subsemigroup of the multiplicative semigroup of all positive rational numbers. The elements of a generating set of this semigroup are related to the sequence of numbers involved in the yet to be proved conjecture known as the Collatz conjecture or the \"3\"x\" + 1 problem\". The 3\"x\" + 1 semigroup has been used to prove a weaker form of the Collatz conjecture. In fact, it was in such context the concept of the 3\"x\" + 1 semigroup was introduced by H. Farkas in 2005. Various generalizations of the 3\"x\" + 1 semigroup have been constructed and their properties have been investigated.\n\nThe 3\"x\" + 1 semigroup is the multiplicative semigroup of positive rational numbers generated by the set\n\nThe function \"T\" : \"Z\" → \"Z\", where \"Z\" is the set of all integers, as defined below is used in the \"shortcut\" definition of the Collatz conjecture:\nThe Collatz conjecture asserts that for each positive integer \"n\", there is some iterate of \"T\" with itself which maps \"n\" to 1, that is, there is some integer \"k\" such that \"T\"(\"n\") = 1. For example if \"n\" = 7 then the values of \"T\"(\"n\") for \"k\" = 1, 2, 3, . . . are 11, 17, 26, 13, 20, 10, 5, 8, 4, 2, 1 and \"T\"(7) = 1.\n\nThe relation between the 3\"x\" + 1 semigroup and the Collatz conjecture is that the 3\"x\" + 1 semigroup is also generated by the set formula_3.\n\nThe weak Collatz conjecture asserts the following: \"The 3\"x\" + 1 semigroup contains every positive integer.\" This was formulated by Farkas and it has been proved to be true as a consequence of the following property of the 3\"x\" + 1 semigroup.\n\"The 3\"x\" + 1 semigroup \"S\" equals the set of all positive rationals \"a\"/\"b\" in lowest terms having the property that \"b\" ≠ 0 (mod 3). In particular, \"S\" contains every positive integer.\"\n\nThe semigroup generated by the set formula_4,\nwhich is also generated by the set formula_5, is called the wild semigroup. The integers in the wild semigroup consists of all integers \"m\" such that \"m\" ≠ 0 (mod 3).\n\n"}
{"id": "41452559", "url": "https://en.wikipedia.org/wiki?curid=41452559", "title": "Approximate tangent space", "text": "Approximate tangent space\n\nIn geometric measure theory an approximate tangent space is a measure theoretic generalization of the concept of a tangent space for a differentiable manifold.\n\nIn differential geometry the defining characteristic of a tangent space is that it approximates the smooth manifold to first order near the point of tangency. Equivalently, if we zoom in more and more at the point of tangency the manifold appears to become more and more straight, asymptotically tending to approach the tangent space. This turns out to be the correct point of view in geometric measure theory.\n\nDefinition. Let formula_1 be a set that is measurable with respect to \"m\"-dimensional Hausdorff measure formula_2, and such that the restriction measure formula_3 is a Radon measure. We say that an \"m\"-dimensional subspace formula_4 is the approximate tangent space to formula_5 at a certain point formula_6, denoted formula_7, if\n\nin the sense of Radon measures. Here for any measure formula_10 we denote by formula_11 the rescaled and translated measure:\n\nCertainly any classical tangent space to a smooth submanifold is an approximate tangent space, but the converse is not necessarily true.\n\nThe parabola\n\nis a smooth 1-dimensional submanifold. Its tangent space at the origin formula_14 is the horizontal line formula_15. On the other hand, if we incorporate the reflection along the \"x\"-axis:\n\nthen formula_17 is no longer a smooth 1-dimensional submanifold, and there is no classical tangent space at the origin. On the other hand, by zooming in at the origin the set formula_17 is approximately equal to two straight lines that overlap in the limit. It would be reasonable to say it has an approximate tangent space formula_19 with multiplicity two.\n\nOne can generalize the previous definition and proceed to define approximate tangent spaces for certain Radon measures, allowing for multiplicities as explained in the section above.\n\nDefinition. Let formula_10 be a Radon measure on formula_21. We say that an \"m\"-dimensional subspace formula_4 is the approximate tangent space to formula_10 at a point formula_6 with multiplicity formula_25, denoted formula_26 with multiplicity formula_27, if\n\nin the sense of Radon measures. The right-hand side is a constant multiple of \"m\"-dimensional Hausdorff measure restricted to formula_30.\n\nThis definition generalizes the one for sets as one can see by taking formula_31 for any formula_5 as in that section. It also accounts for the reflected paraboloid example above because for formula_33 we have formula_34 with multiplicity two.\n\nThe notion of approximate tangent spaces is very closely related to that of rectifiable sets. Loosely speaking, rectifiable sets are precisely those for which approximate tangent spaces exist almost everywhere. The following lemma encapsulates this relationship:\n\nLemma. Let formula_1 be measurable with respect to \"m\"-dimensional Hausdorff measure. Then formula_5 is m-rectifiable if and only if there exists a positive locally formula_2-integrable function formula_38 such that the Radon measure\n\nhas approximate tangent spaces formula_40 for formula_2-almost every formula_42.\n\n"}
{"id": "31100511", "url": "https://en.wikipedia.org/wiki?curid=31100511", "title": "Bates distribution", "text": "Bates distribution\n\n-e^{\\tfrac{iat}{n}}) }{(b-a)t}\\right)^n</math> \n\nIn probability and statistics, the Bates distribution, named after Grace Bates, is a probability distribution of the mean of a number of statistically independent uniformly distributed random variables on the unit interval. This distribution is sometimes confused with the Irwin–Hall distribution, which is the distribution of the sum (not the mean) of \"n\" independent random variables uniformly distributed from 0 to 1.\n\nThe Bates distribution is the continuous probability distribution of the mean, \"X\", of \"n\" independent uniformly distributed random variables on the unit interval, \"U\":\n\nThe equation defining the probability density function of a Bates distribution random variable \"X\" is\n\nfor \"x\" in the interval (0,1), and zero elsewhere. Here sgn(\"nx\" − \"k\") denotes the sign function:\n\nMore generally, the mean of \"n\" independent uniformly distributed random variables on the interval [\"a\",\"b\"]\n\nwould have the probability density function (PDF) of\n\nTherefore, the PDF of the distribution is\n\nInstead of dividing by \"n\" we can also use to create a similar distribution with a constant variance (like unity). By subtracting the mean we can set the resulting mean to zero. This way the parameter \"n\" would become a purely shape-adjusting parameter, and we obtain a distribution which covers the uniform, the triangular and, in the limit, also the normal Gaussian distribution. By allowing also non-integer \"n\" a highly flexible distribution can be created (e.g. \"U\"(0,1) + 0.5\"U\"(0,1) gives a trapezodial distribution). Actually the Student-t distribution provides a natural extension of the normal Gaussian distribution for modeling of long tail data. And such generalized Bates distribution is doing so for short tail data (kurtosis < 3).\n\n\n"}
{"id": "49686608", "url": "https://en.wikipedia.org/wiki?curid=49686608", "title": "Bidirectional recurrent neural networks", "text": "Bidirectional recurrent neural networks\n\nBidirectional Recurrent Neural Networks (BRNN) connects two hidden layers of opposite directions to the same output. With this form of generative deep learning, the output layer can get information from past (backwards) and future (forward) states simultaneously. Invented in 1997 by Schuster and Paliwal, BRNNs were introduced to increase the amount of input information available to the network. For example, multilayer perceptron (MLPs) and time delay neural network (TDNNs) have limitations on the input data flexibility, as they require their input data to be fixed. Standard recurrent neural network (RNNs) also have restrictions as the future input information cannot be reached from the current state. On the contrary, BRNNs do not require their input data to be fixed. Moreover, their future input information is reachable from the current state. \n\nBRNN are especially useful when the context of the input is needed. For example, in handwriting recognition, the performance can be enhanced by knowledge of the letters located before and after the current letter.\n\nThe principle of BRNN is to split the neurons of a regular RNN into two directions, one for positive time direction (forward states), and another for negative time direction (backward states). Those two states’ output are not connected to inputs of the opposite direction states. The general structure of RNN and BRNN can be depicted in the right diagram. By using two time directions, input information from the past and future of the current time frame can be used unlike standard RNN which requires the delays for including future information.\n\nBRNNs can be trained using similar algorithms to RNNs, because the two directional neurons do not have any interactions. However, when back-propagation is applied, additional processes are needed because updating input and output layers cannot be done at once. General procedures for training are as follows: For forward pass, forward states and backward states are passed first, then output neurons are passed. For backward pass, output neurons are passed first, then forward states and backward states are passed next. After forward and backward passes are done, the weights are updated.\n\nApplications of BRNN include :\n\n\n\n\n"}
{"id": "48777793", "url": "https://en.wikipedia.org/wiki?curid=48777793", "title": "Boolean satisfiability algorithm heuristics", "text": "Boolean satisfiability algorithm heuristics\n\nGiven a Boolean expression formula_1 with formula_2 variables, finding an assignment formula_3 of the variables such that formula_4 is true is called the Boolean satisfiability problem, frequently abbreviated SAT, and is seen as the canonical NP-complete problem.\n\nAlthough no known algorithm is known to solve SAT in polynomial time, there are classes of SAT problems which do have efficient algorithms that solve them. These classes of problems arise from many practical problems in AI planning, circuit testing, and software verification. Research on constructing efficient SAT solvers has been based on various principles such as resolution, search, local search and random walk, binary decisions, and Stalmarck's algorithm.\n\nSome of these algorithms are deterministic, while others may be stochastic.\n\nAs there exist polynomial-time algorithms to convert any Boolean expression to conjunctive normal form such as Tseitin's algorithm, posing SAT problems in CNF does not change their computational difficulty. SAT problems are canonically expressed in CNF because CNF has certain properties that can help prune the search space and speed up the search process.\n\nOne of the cornerstone Conflict-Driven Clause Learning SAT solver algorithms is the DPLL algorithm. The algorithm works by iteratively assigning free variables, and when the algorithm encounters a bad assignment, then it backtracks to a previous iteration and chooses a different assignment of variables. It relies on a Branching Heuristic to pick the next free variable assignment; the branching algorithm effectively makes choosing the variable assignment into a decision tree. Different implementations of this heuristic produce markedly different decision trees, and thus have significant effect on the efficiency of the solver.\n\nEarly branching Heuristics (Bohm's Heuristic, Maximum Occurrences on Minimum sized clauses heuristic, and Jeroslow-Wang heuristic) can be regarded as greedy algorithms. Their basic premise is to choose a free variable assignment that will satisfy the most already unsatisfied clauses in the Boolean expression. However, as Boolean expressions get larger, more complicated, or more structured, these heuristics fail to capture useful information about these problems that could improve efficiency; they often get stuck in local maxima or do not consider the distribution of variables. Additionally, larger problems require more processing, as the operation of counting free variables in unsatisfied clauses dominates the run-time.\n\nAnother heuristic called Variable State Independent Decaying Sum (VSIDS) attempts to score each variable. VSIDS starts by looking at small portions of the Boolean expression and assigning each phase of a variable (a variable and its negated complement) a score proportional to the number of clauses that variable phase is in. As VSIDS progresses and searches more parts of the Boolean expression, periodically, all scores are divided by a constant. This discounts the effect of the presence of variables in earlier-found clauses in favor of variables with a greater presence in more recent clauses. VSIDS will select the variable phase with the highest score to determine where to branch.\n\nVSIDS is quite effective because the scores of variable phases is independent of the current variable assignment, so backtracking is much easier. Further, VSIDS guarantees that each variable assignment satisfies the greatest number of recently searched segments of the Boolean expression.\n\nMAX-SAT (the version of SAT in which the number of satisfied clauses is maximized) solvers can also be solved using probabilistic algorithms. If we are given a Boolean expression formula_1, with formula_2 variables and we set each variable randomly, then each clause formula_7, with formula_8 variables, has a chance of being satisfied by a particular variable assignment Pr(formula_7 is satisfied) = formula_10. This is because each variable in formula_7 has formula_12 probability of being satisfied, and we only need one variable in formula_7 to be satisfied. This works formula_14, so Pr(formula_7 is satisfied) = formula_16.\n\nNow we show that randomly assigning variable values is a formula_12-approximation algorithm, which means that is an optimal approximation algorithm unless P = NP. Suppose we are given a Boolean expression formula_18 and\n\nThis algorithm cannot be further optimized by the PCP theorem unless P = NP.\n\nOther Stochastic SAT solvers, such as WalkSAT and GSAT are an improvement to the above procedure. They start by randomly assigning values to each variable and then traverse the given Boolean expression to identify which variables to flip to minimize the number of unsatisfied clauses. They may randomly select a variable to flip or select a new random variable assignment to escape local maxima, much like a simulated annealing algorithm.\n\nUnlike general SAT problems, 2-SAT problems are tractable. There exist algorithms that can compute the satisfiability of a 2-SAT problem in polynomial time. This is a result of the constraint that each clause has only two variables, so when an algorithm sets a variable formula_21, the satisfaction of clauses, which contain formula_21 but are not satisfied by that variable assignment, depend on the satisfaction of the second variable in those clauses, which leaves only one possible assignment for those variables.\n\nSuppose we are given a Boolean expressions:\n\nWith formula_25, the algorithm can select formula_26, so to satisfy the second clause, the algorithm will need to set formula_27, and resultantly to satisfy the first clause, the algorithm will set formula_28.\n\nIf the algorithm tries to satisfy formula_29 in the same way it tried to solve formula_25, then the third clause will remain unsatisfied. This will cause the algorithm to backtrack and set formula_31 and continue assigning variables further.\n\n2-SAT problems can also be reduced to running a depth-first search on a strongly connected components of a graph. Each variable phase (a variable and its negated complement) is connected to other variable phases based on implications. In the same way when the algorithm above tried to solve\n\nHowever, when the algorithm tried solve\n\nwhich is a contradiction.\n\nOnce a 2-SAT problem is reduced to a graph, then if a depth first search finds a strongly connected component with both phases of a variable, then the 2-SAT problem is not satisfiable. Likewise, if the depth first search does not find a strongly connected component with both phases of a variable, then the 2-SAT problem is satisfiable.\n\nNumerous weighted SAT problems exist as the optimization versions of the general SAT problem. In this class of problems, each clause in a CNF Boolean expression is given a weight. The objective is the maximize or minimize the total sum of the weights of the satisfied clauses given a Boolean expression. weighted Max-SAT is the maximization version of this problem, and Max-SAT is an instance of weighted MAX-SAT problem where the weights of each clause are the same. The partial Max-SAT problem is the problem where some clauses necessarily must be satisfied (hard clauses) and the sum total of weights of the rest of the clauses (soft clauses) are to be maximized or minimized, depending on the problem. Partial Max-SAT represents an intermediary between Max-SAT (all clauses are soft) and SAT (all clauses are hard).\n\nNote that the stochastic probabilistic solvers can also be used to find optimal approximations for Max-SAT.\n\nVariable splitting is a tool to find upper and lower bounds on a Max-SAT problem. It involves splitting a variable formula_34 into new variables for all but once occurrence of formula_34 in the original Boolean expression. For example, given the Boolean expression:\nformula_36\nwill become:\nformula_37,\nwith formula_38 being all distinct variables.\n\nThis relaxes the problem by introducing new variables into the Boolean expression, which has the effect of removing many of the constraints in the expression. Because any assignment of variables in formula_1 can be represented by an assignment of variables in formula_40, the minimization and maximization of the weights of formula_40 represent lower and upper bounds on the minimization and maximization of the weights of formula_1.\n\nPartial Max-SAT can be solved by first considering all of the hard clauses and solving them as an instance of SAT. The total maximum (or minimum) weight of the soft clauses can be evaluated given the variable assignment necessary to satisfy the hard clauses and trying to optimize the free variables (the variables that the satisfaction of the hard clauses does not depend on). The latter step is an implementation of Max-SAT given some pre-defined variables. Of course, different variable assignments that satisfy the hard clauses might have different optimal free variable assignments, so it is necessary to check different hard clause satisfaction variable assignments.\n\nAs SAT solvers and practical SAT problems (e.g. circuit verification) get more advanced, the Boolean expressions of interest may exceed millions of variables with several million clauses; therefore, efficient data structures to store and evaluate the clauses must be used.\n\nExpressions can be stored as a list of clauses, where each clause is a list of variables, much like an adjacency list. Though these data structures are convenient for manipulation (adding elements, deleting elements, etc.), they rely on many pointers, which increases their memory overhead, decreases cache locality, and increases cache misses, which renders them impractical for problems with large clause counts and large clause sizes.\n\nWhen clause sizes are large, more efficient analogous implementations include storing expressions as a list of clauses, where each clause is represented as a matrix that represents the clauses and the variables present in that clause, much like an adjacency matrix. The elimination of pointers and the contiguous memory occupation of arrays serve to decrease memory usage and increase cache locality and cache hits, which offers a run-time speed up compared to the aforesaid implementation.\n"}
{"id": "4101", "url": "https://en.wikipedia.org/wiki?curid=4101", "title": "Brouwer fixed-point theorem", "text": "Brouwer fixed-point theorem\n\nBrouwer's fixed-point theorem is a fixed-point theorem in topology, named after L. E. J. (Bertus) Brouwer. It states that for any continuous function formula_1 mapping a compact convex set to itself there is a point formula_2 such that formula_3. The simplest forms of Brouwer's theorem are for continuous functions formula_1 from a closed interval formula_5 in the real numbers to itself or from a closed disk formula_6 to itself. A more general form than the latter is for continuous functions from a convex compact subset formula_7 of Euclidean space to itself.\n\nAmong hundreds of fixed-point theorems, Brouwer's is particularly well known, due in part to its use across numerous fields of mathematics.\nIn its original field, this result is one of the key theorems characterizing the topology of Euclidean spaces, along with the Jordan curve theorem, the hairy ball theorem and the Borsuk–Ulam theorem.\nThis gives it a place among the fundamental theorems of topology. The theorem is also used for proving deep results about differential equations and is covered in most introductory courses on differential geometry.\nIt appears in unlikely fields such as game theory. In economics, Brouwer's fixed-point theorem and its extension, the Kakutani fixed-point theorem, play a central role in the proof of existence of general equilibrium in market economies as developed in the 1950s by economics Nobel prize winners Kenneth Arrow and Gérard Debreu.\n\nThe theorem was first studied in view of work on differential equations by the French mathematicians around Henri Poincaré and Charles Émile Picard. Proving results such as the Poincaré–Bendixson theorem requires the use of topological methods. This work at the end of the 19th century opened into several successive versions of the theorem. The general case was first proved in 1910 by Jacques Hadamard and by Luitzen Egbertus Jan Brouwer.\n\nThe theorem has several formulations, depending on the context in which it is used and its degree of generalization. The simplest is sometimes given as follows:\n\nThis can be generalized to an arbitrary finite dimension:\n\nA slightly more general version is as follows:\n\nAn even more general form is better known under a different name:\n\nThe theorem holds only for sets that are \"compact\" (thus, in particular, bounded and closed) and \"convex\" (or homeomorphic to convex). The following examples show why the pre-conditions are important.\n\nConsider the function\nwhich is a continuous function from formula_9 to itself. As it shifts every point to the right, it cannot have a fixed point. Note that formula_9 is convex and closed, but not bounded.\n\nConsider the function\nwhich is a continuous function from the open interval (−1,1) to itself. In this interval, it shifts every point to the right, so it cannot have a fixed point. Note that (−1,1) is convex and bounded, but not closed. The function \"f\" \"does\" have a fixed point for the closed interval [−1,1], namely \"f\"(1) = 1.\n\nNote that convexity is not strictly necessary for BFPT. Because the properties involved (continuity, being a fixed point) are invariant under homeomorphisms, BFPT is equivalent to forms in which the domain is required to be a closed unit ball formula_12. For the same reason it holds for every set that is homeomorphic to a closed ball (and therefore also closed, bounded, connected, without holes, etc.).\n\nThe following example shows that BFPT doesn't work for domains with holes. Consider the following function, defined in polar coordinates:\nwhich is a continuous function from the unit circle to itself. It rotates every point on the unit circle 45 degrees counterclockwise, so it cannot have a fixed point. Note that the unit circle is closed and bounded, but it has a hole (and so it is not convex). The function \"f\" \"does\" have a fixed point for the unit disc, since it takes the origin to itself.\n\nA formal generalization of BFPT for \"hole-free\" domains can be derived from the Lefschetz fixed-point theorem.\n\nThe continuous function in this theorem is not required to be bijective or even surjective.\n\nThe theorem has several \"real world\" illustrations. Here are some examples.\n\n1. Take two sheets of graph paper of equal size with coordinate systems on them, lay one flat on the table and crumple up (without ripping or tearing) the other one and place it, in any fashion, on top of the first so that the crumpled paper does not reach outside the flat one. There will then be at least one point of the crumpled sheet that lies directly above its corresponding point (i.e. the point with the same coordinates) of the flat sheet. This is a consequence of the \"n\" = 2 case of Brouwer's theorem applied to the continuous map that assigns to the coordinates of every point of the crumpled sheet the coordinates of the point of the flat sheet immediately beneath it.\n\n2. Take an ordinary map of a country, and suppose that that map is laid out on a table inside that country. There will always be a \"You are Here\" point on the map which represents that same point in the country.\n\n3. In three dimensions a consequence of the Brouwer fixed-point theorem is that, no matter how much you stir a cocktail in a glass (or think about milk shake), when the liquid has come to rest, some point in the liquid will end up in exactly the same place in the glass as before you took any action, assuming that the final position of each point is a continuous function of its original position, that the liquid after stirring is contained within the space originally taken up by it, and that the glass (and stirred surface shape) maintain a convex volume. Ordering a cocktail shaken, not stirred defeats the convexity condition (\"shaking\" being defined as a dynamic series of non-convex inertial containment states in the vacant headspace under a lid). In that case, the theorem would not apply, and thus all points of the liquid disposition are potentially displaced from the original state. \n\nThe theorem is supposed to have originated from Brouwer's observation of a cup of coffee.\nIf one stirs to dissolve a lump of sugar, it appears there is always a point without motion.\nHe drew the conclusion that at any moment, there is a point on the surface that is not moving.\nThe fixed point is not necessarily the point that seems to be motionless, since the centre of the turbulence moves a little bit.\nThe result is not intuitive, since the original fixed point may become mobile when another fixed point appears.\n\nBrouwer is said to have added: \"I can formulate this splendid result different, I take a horizontal sheet, and another identical one which I crumple, flatten and place on the other. Then a point of the crumpled sheet is in the same place as on the other sheet.\"\nBrouwer \"flattens\" his sheet as with a flat iron, without removing the folds and wrinkles. This example is better than the coffee cup one as it shows that uniqueness of the fixed point may fail. This distinguishes Brouwer's result from other fixed-point theorems, such as Stefan Banach's, that guarantee uniqueness.\n\nIn one dimension, the result is intuitive and easy to prove. The continuous function \"f\" is defined on a closed interval [\"a\", \"b\"] and takes values in the same interval. Saying that this function has a fixed point amounts to saying that its graph (dark green in the figure on the right) intersects that of the function defined on the same interval [\"a\", \"b\"] which maps \"x\" to \"x\" (light green).\n\nIntuitively, any continuous line from the left edge of the square to the right edge must necessarily intersect the green diagonal. To prove this, consider the function \"g\" which maps \"x\" to \"f\"(\"x\") - \"x\". It is ≥ 0 on \"a\" and ≤ 0 on \"b\". By the intermediate value theorem, \"g\" has a zero in [\"a\", \"b\"]; this zero is a fixed point.\n\nBrouwer is said to have expressed this as follows: \"Instead of examining a surface, we will prove the theorem about a piece of string. Let us begin with the string in an unfolded state, then refold it. Let us flatten the refolded string. Again a point of the string has not changed its position with respect to its original position on the unfolded string.\"\n\nThe Brouwer fixed point theorem was one of the early achievements of algebraic topology, and is the basis of more general fixed point theorems which are important in functional analysis. The case \"n\" = 3 first was proved by Piers Bohl in 1904 (published in \"Journal für die reine und angewandte Mathematik\"). It was later proved by L. E. J. Brouwer in 1909. Jacques Hadamard proved the general case in 1910, and Brouwer found a different proof in the same year. Since these early proofs were all non-constructive indirect proofs, they ran contrary to Brouwer's intuitionist ideals. Although the existence of a fixed point is not constructive in the sense of constructivism in mathematics, methods to approximate fixed points guaranteed by Brouwer's theorem are now known.\n\nTo understand the prehistory of Brouwer's fixed point theorem one needs to pass through differential equations. At the end of the 19th century, the old problem of the stability of the solar system returned into the focus of the mathematical community.\nIts solution required new methods. As noted by Henri Poincaré, who worked on the three-body problem, there is no hope to find an exact solution: \"Nothing is more proper to give us an idea of the hardness of the three-body problem, and generally of all problems of Dynamics where there is no uniform integral and the Bohlin series diverge.\"\nHe also noted that the search for an approximate solution is no more efficient: \"the more we seek to obtain precise approximations, the more the result will diverge towards an increasing imprecision\".\n\nHe studied a question analogous to that of the surface movement in a cup of coffee. What can we say, in general, about the trajectories on a surface animated by a constant flow? Poincaré discovered that the answer can be found in what we now call the topological properties in the area containing the trajectory. If this area is compact, i.e. both closed and bounded, then the trajectory either becomes stationary, or it approaches a limit cycle. Poincaré went further; if the area is of the same kind as a disk, as is the case for the cup of coffee, there must necessarily be a fixed point. This fixed point is invariant under all functions which associate to each point of the original surface its position after a short time interval \"t\". If the area is a circular band, or if it is not closed, then this is not necessarily the case.\n\nTo understand differential equations better, a new branch of mathematics was born. Poincaré called it \"analysis situs\". The French Encyclopædia Universalis defines it as the branch which \"treats the properties of an object that are invariant if it is deformed in any continuous way, without tearing\". In 1886, Poincaré proved a result that is equivalent to Brouwer's fixed-point theorem, although the connection with the subject of this article was not yet apparent. A little later, he developed one of the fundamental tools for better understanding the analysis situs, now known as the fundamental group or sometimes the Poincaré group. This method can be used for a very compact proof of the theorem under discussion.\n\nPoincaré's method was analogous to that of Émile Picard, a contemporary mathematician who generalized the Cauchy–Lipschitz theorem. Picard's approach is based on a result that would later be formalised by another fixed-point theorem, named after Banach. Instead of the topological properties of the domain, this theorem uses the fact that the function in question is a contraction.\n\nAt the dawn of the 20th century, the interest in analysis situs did not stay unnoticed. However, the necessity of a theorem equivalent to the one discussed in this article was not yet evident. Piers Bohl, a Latvian mathematician, applied topological methods to the study of differential equations. In 1904 he proved the three-dimensional case of our theorem, but his publication was not noticed.\n\nIt was Brouwer, finally, who gave the theorem its first patent of nobility. His goals were different from those of Poincaré. This mathematician was inspired by the foundations of mathematics, especially mathematical logic and topology. His initial interest lay in an attempt to solve Hilbert's fifth problem. In 1909, during a voyage to Paris, he met Henri Poincaré, Jacques Hadamard, and Émile Borel. The ensuing discussions convinced Brouwer of the importance of a better understanding of Euclidean spaces, and were the origin of a fruitful exchange of letters with Hadamard. For the next four years, he concentrated on the proof of certain great theorems on this question. In 1912 he proved the hairy ball theorem for the two-dimensional sphere, as well as the fact that every continuous map from the two-dimensional ball to itself has a fixed point. These two results in themselves were not really new. As Hadamard observed, Poincaré had shown a theorem equivalent to the hairy ball theorem. The revolutionary aspect of Brouwer's approach was his systematic use of recently developed tools such as homotopy, the underlying concept of the Poincaré group. In the following year, Hadamard generalised the theorem under discussion to an arbitrary finite dimension, but he employed different methods. Hans Freudenthal comments on the respective roles as follows: \"Compared to Brouwer's revolutionary methods, those of Hadamard were very traditional, but Hadamard's participation in the birth of Brouwer's ideas resembles that of a midwife more than that of a mere spectator.\"\n\nBrouwer's approach yielded its fruits, and in 1910 he also found a proof that was valid for any finite dimension, as well as other key theorems such as the invariance of dimension. In the context of this work, Brouwer also generalized the Jordan curve theorem to arbitrary dimension and established the properties connected with the degree of a continuous mapping. This branch of mathematics, originally envisioned by Poincaré and developed by Brouwer, changed its name. In the 1930s, analysis situs became algebraic topology.\n\nThe theorem proved its worth in more than one way. During the 20th century numerous fixed-point theorems were developed, and even a branch of mathematics called fixed-point theory.\nBrouwer's theorem is probably the most important. It is also among the foundational theorems on the topology of topological manifolds and is often used to prove other important results such as the Jordan curve theorem.\n\nBesides the fixed-point theorems for more or less contracting functions, there are many that have emerged directly or indirectly from the result under discussion. A continuous map from a closed ball of Euclidean space to its boundary cannot be the identity on the boundary. Similarly, the Borsuk–Ulam theorem says that a continuous map from the \"n\"-dimensional sphere to R has a pair of antipodal points that are mapped to the same point. In the finite-dimensional case, the Lefschetz fixed-point theorem provided from 1926 a method for counting fixed points. In 1930, Brouwer's fixed-point theorem was generalized to Banach spaces. This generalization is known as Schauder's fixed-point theorem, a result generalized further by S. Kakutani to multivalued functions. One also meets the theorem and its variants outside topology. It can be used to prove the Hartman-Grobman theorem, which describes the qualitative behaviour of certain differential equations near certain equilibria. Similarly, Brouwer's theorem is used for the proof of the Central Limit Theorem. The theorem can also be found in existence proofs for the solutions of certain partial differential equations.\n\nOther areas are also touched. In game theory, John Nash used the theorem to prove that in the game of Hex there is a winning strategy for white. In economics, P. Bich explains that certain generalizations of the theorem show that its use is helpful for certain classical problems in game theory and generally for equilibria (Hotelling's law), financial equilibria and incomplete markets.\n\nBrouwer's celebrity is not exclusively due to his topological work. The proofs of his great topological theorems are not constructive, and Brouwer's dissatisfaction with this is partly what led him to articulate the idea of constructivity. He became the originator and zealous defender of a way of formalising mathematics that is known as intuitionism, which at the time made a stand against set theory. Brouwer disavowed his original proof of the fixed-point theorem. The first algorithm to approximate a fixed point was proposed by Herbert Scarf. It is important to here note a confusing subtlety — Scarf's algorithm finds a point that is \"almost fixed\" by a function \"f\", but in general cannot find a point that is close to an actual fixed point. In mathematical language, if ε is chosen to be very small, Scarf's algorithm can be used to find a point \"x\" such that \"f(x)\" is very close to \"x\", i.e., formula_14. But Scarf's algorithm cannot be used to find a point \"x\" such that \"x\" is very close to a fixed point: we cannot guarantee formula_15, where formula_16. Often this latter condition is what is meant by the informal phrase \"approximating a fixed point.\"\n\nBrouwer's original 1911 proof relied on the notion of the degree of a continuous mapping. Modern accounts of the proof can also be found in the literature.\n\nLet formula_17 denote the closed unit ball in formula_18 centered at the origin. Suppose for simplicitly that formula_19 is continuously differentiable. A regular value of formula_1 is a point formula_21 such that the Jacobian of formula_1 is non-singular at every point of the preimage of formula_23. In particular, by the inverse function theorem, every point of the preimage of formula_1 lies in formula_25 (the interior of formula_26). The degree of formula_1 at a regular value formula_21 is defined as the sum of the signs of the Jacobian determinant of formula_1 over the preimages of formula_23 under formula_1:\n\nThe degree is, roughly speaking, the number of \"sheets\" of the preimage \"f\" lying over a small open set around \"p\", with sheets counted oppositely if they are oppositely oriented. This is thus a generalization of winding number to higher dimensions.\n\nThe degree satisfies the property of \"homotopy invariance\": let formula_1 and formula_34 be two continuously differentiable functions, and formula_35 for formula_36. Suppose that the point formula_23 is a regular value of formula_38 for all \"t\". Then formula_39.\n\nIf there is no fixed point of the boundary of formula_26, then the function \n\nis well-defined, and \n\nformula_42\n\ndefines a homotopy from the identity function to it. The identity function has degree one at every point. In particular, the identity function has degree one at the origin, so formula_34 also has degree one at the origin. As a consequence, the preimage formula_44 is not empty. The elements of formula_44 are precisely the fixed points of the original function \"f\".\n\nThis requires some work to make fully general. The definition of degree must be extended to singular values of \"f\", and then to continuous functions. The more modern advent of homology theory simplifies the construction of the degree, and so has become a standard proof in the literature.\n\nThe proof uses the observation that the boundary of \"D\" is \"S\", the (\"n\" − 1)-sphere.\nThe argument proceeds by contradiction, supposing that a continuous function \"f\" : \"D\" → \"D\" has \"no\" fixed point, and then attempting to derive an inconsistency, which proves that the function must in fact have a fixed point. For each \"x\" in \"D\", there is only one straight line that passes through \"f\"(\"x\") and \"x\", because it must be the case that \"f\"(\"x\") and \"x\" are distinct by hypothesis (recall that \"f\" having no fixed points means that \"f\"(\"x\") ≠ \"x\"). Following this line from \"f\"(\"x\") through \"x\" leads to a point on \"S\", denoted by \"F\"(\"x\"). This defines a continuous function \"F\" : \"D\" → \"S\", which is a special type of continuous function known as a retraction: every point of the codomain (in this case \"S\") is a fixed point of the function.\n\nIntuitively it seems unlikely that there could be a retraction of \"D\" onto \"S\", and in the case \"n\" = 1 it is obviously impossible because \"S\" (i.e., the endpoints of the closed interval \"D\") is not even connected. The case \"n\" = 2 is less obvious, but can be proven by using basic arguments involving the fundamental groups of the respective spaces: the retraction would induce an injective group homomorphism from the fundamental group of \"S\" to that of \"D\", but the first group is isomorphic to Z while the latter group is trivial, so this is impossible. The case \"n\" = 2 can also be proven by contradiction based on a theorem about non-vanishing vector fields.\n\nFor \"n\" > 2, however, proving the impossibility of the retraction is more difficult. One way is to make use of homology groups: the homology \"H\"(\"D\") is trivial, while \"H\"(\"S\") is infinite cyclic. This shows that the retraction is impossible, because again the retraction would induce an injective group homomorphism from the latter to the former group.\n\nTo prove that a map has fixed points, one can assume that it is smooth, because if a map has no fixed points then convolving it with a smooth function of sufficiently small support and integral equal to one produces a smooth function with no fixed points. As in the proof using homology, one is reduced to proving that there is no smooth retraction \"F\" from the ball \"B\" onto its boundary \"∂B\". If ω is a volume form on the boundary then by Stokes Theorem,\ngiving a contradiction.\n\nMore generally, this shows that there is no smooth retraction from any non-empty smooth orientable compact manifold onto its boundary. The proof using Stokes's theorem is closely related to the proof using homology (or rather cohomology), because the form ω generates the de Rham cohomology group \"H\"(\"∂B\") used in the cohomology proof.\n\nThe BFPT can be proved using Sperner's lemma. We now give an outline of the proof for the special case in which \"f\" is a function from the standard \"n\"-simplex, formula_47 to itself, where\n\nFor every point formula_49 also formula_50 Hence the sum of their coordinates is equal:\n\nHence, by the pigeonhole principle, for every formula_49 there must be an index formula_53 such that the formula_54th coordinate of formula_55 is greater than or equal to the formula_54th coordinate of its image under \"f\":\n\nMoreover, if formula_55 lies on a \"k\"-dimensional sub-face of formula_47 then by the same argument, the index formula_54 can be selected from among the coordinates which are not zero on this sub-face.\n\nWe now use this fact to construct a Sperner coloring. For every triangulation of formula_47 the color of every vertex formula_55 is an index formula_54 such that formula_64\n\nBy construction, this is a Sperner coloring. Hence, by Sperner's lemma, there is an \"n\"-dimensional simplex whose vertices are colored with the entire set of available colors.\n\nBecause \"f\" is continuous, this simplex can be made arbitrarily small by choosing an arbitrarily fine triangulation. Hence, there must be a point formula_55 which satisfies the labeling condition in all coordinates: formula_66 for all formula_67\n\nBecause the sum of the coordinates of formula_55 and formula_69 must be equal, all these inequalities must actually be equalities. But this means that:\n\nThat is, formula_55 is a fixed point of formula_72\n\nThere is also a quick proof, by Morris Hirsch, based on the impossibility of a differentiable retraction. The indirect proof starts by noting that the map \"f\" can be approximated by a smooth map retaining the property of not fixing a point; this can be done by using the Weierstrass approximation theorem, for example. One then defines a retraction as above which must now be differentiable. Such a retraction must have a non-singular value, by Sard's theorem, which is also non-singular for the restriction to the boundary (which is just the identity). Thus the inverse image would be a 1-manifold with boundary. The boundary would have to contain at least two end points, both of which would have to lie on the boundary of the original ball—which is impossible in a retraction.\n\nA variation of the preceding proof does not employ the Sard's theorem, and goes as follows. If formula_73 is a smooth retraction, one considers the smooth deformation formula_74, and the smooth function\nDifferentiating under the sign of integral it is not difficult to check that \"φ′(t)=0\" for all \"t\", so \"φ\" is a constant function, which is a contradiction because \"φ(0)\" is the \"n\"-dimensional volume of the ball, while \"φ(1)\" is zero. The geometric idea is that \"φ(t)\" is the oriented area of \"g(B)\" (that is, the Lebesgue measure of the image of the ball via \"g\", taking into account multiplicity and orientation), and should remain constant (as it is very clear in the one-dimensional case). On the other hand, as the parameter \"t\" passes form \"0\" to \"1\" the map \"g\" transforms continuously from the identity map of the ball, to the retraction \"r\", which is a contradiction since the oriented area of the identity coincides with the volume of the ball, while the oriented area of \"r\" is necessarily \"0\", as its image is the boundary of the ball, a set of null measure.\n\nA quite different proof given by David Gale is based on the game of Hex. The basic theorem about Hex is that no game can end in a draw. This is equivalent to the Brouwer fixed-point theorem for dimension 2. By considering \"n\"-dimensional versions of Hex, one can prove in general that Brouwer's theorem is equivalent to the determinacy theorem for Hex.\n\nThe Lefschetz fixed-point theorem says that if a continuous map \"f\" from a finite simplicial complex \"B\" to itself has only isolated fixed points, then the number of fixed points counted with multiplicities (which may be negative) is equal to the Lefschetz number\nand in particular if the Lefschetz number is nonzero then \"f\" must have a fixed point. If \"B\" is a ball (or more generally is contractible) then the Lefschetz number is one because the only non-zero homology group is :formula_77 and \"f\" acts as the identity on this group, so \"f\" has a fixed point.\n\nIn reverse mathematics, Brouwer's theorem can be proved in the system WKL, and conversely over the base system RCA Brouwer's theorem for a square implies the weak König's lemma, so this gives a precise description of the strength of Brouwer's theorem.\n\nThe Brouwer fixed-point theorem forms the starting point of a number of more general fixed-point theorems.\n\nThe straightforward generalization to infinite dimensions, i.e. using the unit ball of an arbitrary Hilbert space instead of Euclidean space, is not true. The main problem here is that the unit balls of infinite-dimensional Hilbert spaces are not compact. For example, in the Hilbert space ℓ of square-summable real (or complex) sequences, consider the map \"f\" : ℓ → ℓ which sends a sequence (\"x\") from the closed unit ball of ℓ to the sequence (\"y\") defined by\nIt is not difficult to check that this map is continuous, has its image in the unit sphere of ℓ, but does not have a fixed point.\n\nThe generalizations of the Brouwer fixed-point theorem to infinite dimensional spaces therefore all include a compactness assumption of some sort, and in addition also often an assumption of convexity. See fixed-point theorems in infinite-dimensional spaces for a discussion of these theorems.\n\nThere is also finite-dimensional generalization to a larger class of spaces: If formula_79 is a product of finitely many chainable continua, then every continuous function formula_80 has a fixed point, where a chainable continuum is a (usually but in this case not necessarily metric) compact Hausdorff space of which every open cover has a finite open refinement formula_81, such that formula_82 if and only if formula_83. Examples of chainable continua include compact connected linearly ordered spaces and in particular closed intervals of real numbers.\n\nThe Kakutani fixed point theorem generalizes the Brouwer fixed-point theorem in a different direction: it stays in R, but considers upper hemi-continuous correspondences (functions that assign to each point of the set a subset of the set). It also requires compactness and convexity of the set.\n\nThe Lefschetz fixed-point theorem applies to (almost) arbitrary compact topological spaces, and gives a condition in terms of singular homology that guarantees the existence of fixed points; this condition is trivially satisfied for any map in the case of \"D\".\n\n\n\n"}
{"id": "8566056", "url": "https://en.wikipedia.org/wiki?curid=8566056", "title": "Chain rule for Kolmogorov complexity", "text": "Chain rule for Kolmogorov complexity\n\nThe chain rule for Kolmogorov complexity is an analogue of the chain rule for information entropy, which states:\n\nThat is, the combined randomness of two sequences \"X\" and \"Y\" is the sum of the randomness of \"X\" plus whatever randomness is left in \"Y\" once we know \"X\".\nThis follows immediately from the definitions of conditional and joint entropy, and the fact from probability theory that the joint probability is the product of the marginal and conditional probability:\n\nThe equivalent statement for Kolmogorov complexity does not hold exactly; it is true only up to a logarithmic term:\n\n(An exact version, \"KP\"(\"x\", \"y\") = \"KP\"(\"x\") + \"KP\"(\"y\"|\"x\"*) + O(1),\nholds for the prefix complexity \"KP\", where \"x*\" is a shortest program for \"x\".)\n\nIt states that the shortest program printing \"X\" and \"Y\" is obtained by concatenating a shortest program printing \"X\" with a program printing \"Y\" given \"X\", plus at most a logarithmic factor. The results implies that algorithmic mutual information, an analogue of mutual information for Kolmogorov complexity is symmetric: \"I(x:y) = I(y:x) + O(log K(x,y))\" for all \"x,y\".\n\nThe ≤ direction is obvious: we can write a program to produce \"x\" and \"y\" by concatenating a program to produce \"x\", a program to produce \"y\" given\naccess to \"x\", and (whence the log term) the length of one of the programs, so\nthat we know where to separate the two programs for \"x\" and \"y\"|\"x\" (log(\"K\"(\"x\", \"y\")) upper-bounds this length).\n\nFor the ≥ direction, it suffices to show that for all k,l such that k+l = K(x,y) we have that either\n\nConsider the list \"(a,b), (a,b), ..., (a,b)\" of all pairs \"(a,b)\" produced by programs of length exactly \"K(x,y)\" [hence K(a,b) ≤ K(x,y)]. Note that this list\n\nFirst, suppose that \"x\" appears less than \"2\" times as first element. We can specify \"y\" given \"x,k,l\" by enumerating \"(a,b), (a,b), ...\" and then selecting \"(x,y)\" in the sub-list of pairs \"(x,b)\". By assumption, the index of \"(x,y)\" in this sub-list is less than \"2\" and hence, there is a program for \"y\" given \"x,k,l\" of length \"l + O(1)\".\nNow, suppose that \"x\" appears at least \"2\" times as first element. This can happen for at most \"2 = 2\" different strings. These strings can be enumerated given \"k,l\" and hence \"x\" can be specified by its index in this enumeration. The corresponding program for \"x\" has size \"k + O(1)\". Theorem proved.\n"}
{"id": "4960605", "url": "https://en.wikipedia.org/wiki?curid=4960605", "title": "Coarse structure", "text": "Coarse structure\n\nIn the mathematical fields of geometry and topology, a coarse structure on a set \"X\" is a collection of subsets of the cartesian product \"X\" × \"X\" with certain properties which allow the \"large-scale structure\" of metric spaces and topological spaces to be defined.\n\nThe concern of traditional geometry and topology is with the small-scale structure of the space: properties such as the continuity of a function depend on whether the inverse images of small open sets, or neighborhoods, are themselves open. Large-scale properties of a space—such as boundedness, or the degrees of freedom of the space—do not depend on such features. \"Coarse geometry\" and \"coarse topology\" provide tools for measuring the large-scale properties of a space, and just as a metric or a topology contains information on the small-scale structure of a space, a coarse structure contains information on its large-scale properties.\n\nProperly, a coarse structure is not the large-scale analog of a topological structure, but of a uniform structure.\n\nA coarse structure on a set \"X\" is a collection E of subsets of \"X\" × \"X\" (therefore falling under the more general categorization of binary relations on \"X\") called \"controlled sets\", and so that E possesses the identity relation, is closed under taking subsets, inverses, and finite unions, and is closed under composition of relations. Explicitly:\n\n\nA set \"X\" endowed with a coarse structure E is a \"coarse space\".\n\nThe set \"E\"[\"K\"] is defined as {\"x\" in \"X\" : there is a \"y\" in \"K\" such that (\"x\", \"y\") is in \"E\"}. We define the \"section\" of \"E\" by \"x\" to be the set \"E\"[{\"x\"}], also denoted \"E\" . The symbol \"E\" denotes the set \"E\" [{\"y\"}]. These are forms of projections.\n\nThe controlled sets are \"small\" sets, or \"negligible sets\": a set \"A\" such that \"A\" × \"A\" is controlled is negligible, while a function \"f\" : \"X\" → \"X\" such that its graph is controlled is \"close\" to the identity. In the bounded coarse structure, these sets are the bounded sets, and the functions are the ones that are a finite distance from the identity in the uniform metric.\n\n\n\n"}
{"id": "160076", "url": "https://en.wikipedia.org/wiki?curid=160076", "title": "Combinatorial species", "text": "Combinatorial species\n\nIn combinatorial mathematics, the theory of combinatorial species is an abstract, systematic method for analysing discrete structures in terms of generating functions. Examples of discrete structures are (finite) graphs, permutations, trees, and so on; each of these has an associated generating function which counts how many structures there are of a certain size. One goal of species theory is to be able to analyse complicated structures by describing them in terms of transformations and combinations of simpler structures. These operations correspond to equivalent manipulations of generating functions, so producing such functions for complicated structures is much easier than with other methods. The theory was introduced, carefully elaborated and applied by the Canadian group of people around André Joyal.\n\nAny structure — an instance of a particular species — is associated with some set, and there are often many possible structures for the same set. For example, it is possible to construct several different graphs whose node labels are drawn from the same given set. At the same time, any set could be used to build the structures. The difference between one species and another is that they build a different set of structures out of the same base set.\n\nThis leads to the formal definition of a \"combinatorial species\". Let formula_1 be the category of finite sets, with the morphisms of the category being the bijections between these sets. A species is a functor\n\nFor a set \"A\" the set \"F\"[\"A\"] is called the set of \"F\"-structures on \"A\", or the set of structures of species \"F\" on \"A\". Further, by the definition of a functor, if φ is a bijection between sets \"A\" and \"B\", then \"F\"[φ] is a bijection between the sets of \"F\"-structures \"F\"[\"A\"] and \"F\"[\"B\"], called \"transport of F-structures along φ\".\n\nFor example, the \"species of permutations\" maps each finite set \"A\" to the set of all permutations of \"A\", and each bijection from \"A\" to another set \"B\" naturally induces a bijection from the set of all permutations of \"A\" to the set of all permutations of \"B\". Similarly, the \"species of partitions\" can be defined by assigning to each finite set the set of all its partitions, and the \"power set species\" assigns to each finite set its power set.\n\nThere is a standard way of illustrating an instance of any structure, regardless of its nature. The adjacent diagram shows a structure on a set of five elements: arcs connect the structure (red) to the elements (blue) from which it is built.\n\nThe choice of formula_1 as the category on which species operate is important. Because a bijection can only exist between two sets when they have the same size, the number of elements in \"F\"[\"A\"] depends only on the size of \"A\". (This follows from the formal definition of a functor.) Restriction to finite sets means that |\"F\"[\"A\"]| is always finite, so it is possible to do arithmetic with such quantities. In particular, the \"exponential generating series\" \"F\"(\"x\") of a species \"F\" can be defined:\nwhere formula_5 is the size of \"F\"[\"A\"] for any set \"A\" having \"n\" elements.\n\nSome examples:\n\nOperations with species are supported by SageMath and, using a special package, also by Haskell.\n\n\n"}
{"id": "417665", "url": "https://en.wikipedia.org/wiki?curid=417665", "title": "Cubane", "text": "Cubane\n\nCubane (CH) is a synthetic hydrocarbon molecule that consists of eight carbon atoms arranged at the corners of a cube, with one hydrogen atom attached to each carbon atom. A solid crystalline substance, cubane is one of the Platonic hydrocarbons and a member of the prismanes. It was first synthesized in 1964 by Philip Eaton and Thomas Cole. Before this work, researchers believed that cubic carbon-based molecules would be too unstable to exist. The cubic shape requires the carbon atoms to adopt an unusually sharp 90° bonding angle, which would be highly strained as compared to the 109.45° angle of a tetrahedral carbon. Once formed, cubane is quite kinetically stable, due to a lack of readily available decomposition paths. It is the simplest hydrocarbon with octahedral symmetry.\n\nHaving high energy but kinetic stability makes cubane and its derivative compounds useful for controlled energy storage. For example, octanitrocubane and heptanitrocubane have been studied as high-performance explosives.\n\nThese compounds also typically have a very high density for hydrocarbon molecules. The resulting high energy density means a large amount of energy can be stored in a comparably small amount of space, an important consideration for applications in fuel storage and energy transport.\n\nThe classic 1964 synthesis starts with the conversion of 2-cyclopentenone to 2-bromocyclopentadienone:\n\nAllylic bromination with \"N\"-bromosuccinimide in carbon tetrachloride followed by addition of molecular bromine to the alkene gives a 2,3,4-tribromocyclopentanone. Treating this compound with diethylamine in diethyl ether causes elimination of two equivalents of hydrogen bromide to give the diene product.\n\nThe construction of the eight-carbon cubane framework begins when 2-bromocyclopentadienone undergoes a spontaneous Diels-Alder dimerization, analogous to the dimerization of cyclopentadiene to dicyclopentadiene—two molecules of 1 react to form 2. For the subsequent steps to succeed, only the \"endo\" isomer is useful, and this is the predominant isomer formed in this reaction. This is the most likely product as a result of minimized steric interactions between the bromine of each molecule with the bromine and carbonyl of the other when the reactants approach each other and minimized like-dipole interactions in the transition state of the reaction itself. Both carbonyl groups are protected as acetals with ethylene glycol and \"p\"-toluenesulfonic acid in benzene; one acetal is then selectively deprotected with aqueous hydrochloric acid to 3.\n\nIn the next step, the \"endo\" isomer 3 (with both alkene groups in close proximity) forms the cage-like isomer 4 in a photochemical [2+2] cycloaddition. The bromoketone group is converted to ring-contracted carboxylic acid 5 in a Favorskii rearrangement with potassium hydroxide. Next, the thermal decarboxylation takes place through the acid chloride (with thionyl chloride) and the \"tert\"-butyl perester 6 (with \"tert\"-butyl hydroperoxide and pyridine) to 7; afterward, the acetal is once more removed in 8. A second Favorskii rearrangement gives 9, and finally another decarboxylation gives, via 10, cubane (11).\n\nThe synthesis of the octaphenyl derivative from tetraphenylcyclobutadiene nickel bromide by Freedman in 1962 pre-dates that of the parent compound. It is a sparingly soluble colourless compound that melts at 425–427 °C. A hypercubane, with a hypercube-like structure, was predicted to exist in a 2014 publication. Two different isomers of cubene have been synthesized, and a third analyzed computationally. The alkene in \"ortho\"-cubene is exceptionally reactive due to its pyramidalized geometry. At the time of its synthesis, this was the most pyramidalized alkene to have been successfully made. The \"meta\"-cubene isomer is even less stable, and the \"para\"-cubene isomer probably only exists as a diradical rather than an actual diagonal bond.\n\nCuneane may be produced from cubane by a metal-ion-catalyzed σ-bond rearrangement.\n\n\n"}
{"id": "2745094", "url": "https://en.wikipedia.org/wiki?curid=2745094", "title": "DPLL algorithm", "text": "DPLL algorithm\n\nIn computer science, the Davis–Putnam–Logemann–Loveland (DPLL) algorithm is a complete, backtracking-based search algorithm for deciding the satisfiability of propositional logic formulae in conjunctive normal form, i.e. for solving the CNF-SAT problem.\n\nIt was introduced in 1962 by Martin Davis, George Logemann and Donald W. Loveland and is a refinement of the earlier Davis–Putnam algorithm, which is a resolution-based procedure developed by Davis and Hilary Putnam in 1960. Especially in older publications, the Davis–Logemann–Loveland algorithm is often referred to as the \"Davis–Putnam method\" or the \"DP algorithm\". Other common names that maintain the distinction are DLL and DPLL.\n\nAfter more than 50 years the DPLL procedure still forms the basis for most efficient complete SAT solvers. It has recently been extended for automated theorem proving for fragments of first-order logic.\n\nThe SAT problem is important both from theoretical and practical points of view. In complexity theory it was the first problem proved to be NP-complete, and can appear in a broad variety of applications such as \"model checking\", automated planning and scheduling, and diagnosis in artificial intelligence.\n\nAs such, it has been a hot topic in research for many years, and competitions between SAT solvers regularly take place. DPLL's modern implementations like Chaff and zChaff, GRASP or Minisat are in the first places of the competitions these last years.\n\nAnother application that often involves DPLL is automated theorem proving or satisfiability modulo theories (SMT), which is a SAT problem in which propositional variables are replaced with formulas of another mathematical theory.\n\nThe basic backtracking algorithm runs by choosing a literal, assigning a truth value to it, simplifying the formula and then recursively checking if the simplified formula is satisfiable; if this is the case, the original formula is satisfiable; otherwise, the same recursive check is done assuming the opposite truth value. This is known as the \"splitting rule\", as it splits the problem into two simpler sub-problems. The simplification step essentially removes all clauses that become true under the assignment from the formula, and all literals that become false from the remaining clauses.\n\nThe DPLL algorithm enhances over the backtracking algorithm by the eager use of the following rules at each step:\n\n\n\nUnsatisfiability of a given partial assignment is detected if one clause becomes empty, i.e. if all its variables have been assigned in a way that makes the corresponding literals false. Satisfiability of the formula is detected either when all variables are assigned without generating the empty clause, or, in modern implementations, if all clauses are satisfied. Unsatisfiability of the complete formula can only be detected after exhaustive search.\n\nThe DPLL algorithm can be summarized in the following pseudocode, where Φ is the CNF formula:\n\nIn this pseudocode, codice_1 and codice_2 are functions that return the result of applying unit propagation and the pure literal rule, respectively, to the literal codice_3 and the formula codice_4. In other words, they replace every occurrence of codice_3 with \"true\" and every occurrence of codice_6 with \"false\" in the formula codice_4, and simplify the resulting formula. \nThe codice_8 in the codice_9 statement is a short-circuiting operator. codice_10 denotes the simplified result of substituting \"true\" for codice_3 in codice_4.\n\nThe pseudocode DPLL function only returns whether the final assignment satisfies the formula or not. In a real implementation, the partial satisfying assignment typically is also returned on success; this can be derived from the consistent set of literals of the first codice_13 statement of the function.\n\nThe Davis–Logemann–Loveland algorithm depends on the choice of \"branching literal\", which is the literal considered in the backtracking step. As a result, this is not exactly an algorithm, but rather a family of algorithms, one for each possible way of choosing the branching literal. Efficiency is strongly affected by the choice of the branching literal: there exist instances for which the running time is constant or exponential depending on the choice of the branching literals. Such choice functions are also called heuristic functions or branching heuristics.\n\nDavis, Logemann, Loveland (1962) had developed this algorithm.\nSome properties of this original algorithm are:\nAn example with visualization of a DPLL algorithm having chronological backtracking:\n\nIn the 2010s years, work on improving the algorithm has been done on three directions: \n\nA newer algorithm from 1990 is Stålmarck's method. Also since 1986 (reduced ordered) binary decision diagrams have also been used for SAT solving.\n\nRuns of DPLL-based algorithms on unsatisfiable instances correspond to tree resolution refutation proofs.\n\n\nGeneral\n\nSpecific\n"}
{"id": "8562", "url": "https://en.wikipedia.org/wiki?curid=8562", "title": "Differential topology", "text": "Differential topology\n\nIn mathematics, differential topology is the field dealing with differentiable functions on differentiable manifolds. It is closely related to differential geometry and together they make up the geometric theory of differentiable manifolds.\n\nDifferential topology considers the properties and structures that require only a smooth structure on a manifold to be defined. Smooth manifolds are 'softer' than manifolds with extra geometric structures, which can act as obstructions to certain types of equivalences and deformations that exist in differential topology. For instance, volume and Riemannian curvature are invariants that can distinguish different geometric structures on the same smooth manifold—that is, one can smoothly \"flatten out\" certain manifolds, but it might require distorting the space and affecting the curvature or volume.\n\nOn the other hand, smooth manifolds are more rigid than the topological manifolds. John Milnor discovered that some spheres have more than one smooth structure—see exotic sphere and Donaldson's theorem. Kervaire exhibited topological manifolds with no smooth structure at all. Some constructions of smooth manifold theory, such as the existence of tangent bundles, can be done in the topological setting with much more work, and others cannot.\n\nOne of the main topics in differential topology is the study of special kinds of smooth mappings between manifolds, namely immersions and submersions, and the intersections of submanifolds via transversality. More generally one is interested in properties and invariants of smooth manifolds which are carried over by diffeomorphisms, another special kind of smooth mapping. Morse theory is another branch of differential topology, in which topological information about a manifold is deduced from changes in the rank of the Jacobian of a function.\n\nFor a list of differential topology topics, see the following reference: List of differential geometry topics.\n\nDifferential topology and differential geometry are first characterized by their \"similarity\". They both study primarily the properties of differentiable manifolds, sometimes with a variety of structures imposed on them.\n\nOne major difference lies in the nature of the problems that each subject tries to address. In one view, differential topology distinguishes itself from differential geometry by studying primarily those problems which are \"inherently global\".\nConsider the example of a coffee cup and a donut (see this example). From the point of view of differential topology, the donut and the coffee cup are \"the same\" (in a sense). This is an inherently global view, though, because there is no way for the differential topologist to tell whether the two objects are the same (in this sense) by looking at just a tiny (\"local\") piece of either of them. They must have access to each entire (\"global\") object.\n\nFrom the point of view of differential geometry, the coffee cup and the donut are \"different\" because it is impossible to rotate the coffee cup in such a way that its configuration matches that of the donut. This is also a global way of thinking about the problem. But an important distinction is that the geometer does not need the entire object to decide this. By looking, for instance, at just a tiny piece of the handle, he can decide that the coffee cup is different from the donut because the handle is thinner (or more curved) than any piece of the donut.\n\nTo put it succinctly, differential topology studies structures on manifolds which, in a sense, have no interesting local structure. Differential geometry studies structures on manifolds which do have an interesting local (or sometimes even infinitesimal) structure.\n\nMore mathematically, for example, the problem of constructing a diffeomorphism between two manifolds of the same dimension is inherently global since \"locally\" two such manifolds are always diffeomorphic. Likewise, the problem of computing a quantity on a manifold which is invariant under differentiable mappings is inherently global, since any local invariant will be \"trivial\" in the sense that it is already exhibited in the topology of R. Moreover, differential topology does not restrict itself necessarily to the study of diffeomorphism. For example, symplectic topology—a subbranch of differential topology—studies global properties of symplectic manifolds. Differential geometry concerns itself with problems—which may be local \"or\" global—that always have some non-trivial local properties. Thus differential geometry may study differentiable manifolds equipped with a \"connection\", a \"metric\" (which may be Riemannian, pseudo-Riemannian, or Finsler), a special sort of \"distribution\" (such as a CR structure), and so on.\n\nThis distinction between differential geometry and differential topology is blurred, however, in questions specifically pertaining to local diffeomorphism invariants such as the tangent space at a point. Differential topology also deals with questions like these, which specifically pertain to the properties of differentiable mappings on R (for example the tangent bundle, jet bundles, the Whitney extension theorem, and so forth).\n\nThe distinction is concise in abstract terms: \n\n\n"}
{"id": "34043560", "url": "https://en.wikipedia.org/wiki?curid=34043560", "title": "Dirichlet space", "text": "Dirichlet space\n\nIn mathematics, the Dirichlet space on the domain formula_1 (named after Peter Gustav Lejeune Dirichlet), is the reproducing kernel Hilbert space of holomorphic functions, contained within the Hardy space formula_2, for which the \"Dirichlet integral\", defined by\n\nis finite (here \"dA\" denotes the area Lebesgue measure on the complex plane formula_4). The latter is the integral occurring in Dirichlet's principle for harmonic functions. The Dirichlet integral defines a seminorm on formula_5. It is not a norm in general, since formula_6 whenever \"f\" is a constant function.\n\nFor formula_7, we define\n\nThis is a semi-inner product, and clearly formula_9. We may equip formula_5 with an inner product given by\n\nwhere formula_12 is the usual inner product on formula_13 The corresponding norm formula_14 is given by\n\nNote that this definition is not unique, another common choice is to take formula_16, for some fixed formula_17.\n\nThe Dirichlet space is not an algebra, but the space formula_18 is a Banach algebra, with respect to the norm\n\nWe usually have formula_20 (the unit disk of the complex plane formula_4), in that case formula_22, and if \n\nthen \n\nand\n\nClearly, formula_26 contains all the polynomials and, more generally, all functions formula_27, holomorphic on formula_28 such that formula_29 is bounded on formula_28.\n\nThe reproducing kernel of formula_26 at formula_32 is given by\n\n\n"}
{"id": "711668", "url": "https://en.wikipedia.org/wiki?curid=711668", "title": "Discrepancy function", "text": "Discrepancy function\n\nA discrepancy function is a mathematical function which describes how closely a structural model conforms to observed data. Larger values of the discrepancy function indicate a poor fit of the model to data. In general, the parameter estimates for a given model are chosen so as to make the discrepancy function for that model as small as possible. \n\nThere are several basic types of discrepancy functions, including maximum likelihood (ML), generalized least squares (GLS), and ordinary least squares (OLS), which are considered the \"classical\" discrepancy functions. Discrepancy functions all meet the following basic criteria:\n\n\nIn order for \"maximum likelihood\" to meet the first criterion, it is used in a revised form as the deviance.\n\n"}
{"id": "1441435", "url": "https://en.wikipedia.org/wiki?curid=1441435", "title": "Equilibrium point", "text": "Equilibrium point\n\nIn mathematics, specifically in differential equations, an equilibrium point is a constant solution to a differential equation.\n\nThe points formula_1 is an equilibrium point for the differential equation \n\nif formula_3 for all formula_4.\n\nSimilarly, the point formula_1 is an equilibrium point (or fixed point) for the difference equation \n\nif formula_7 for formula_8.\n\nEquilibria can be classified by looking at the signs of the eigenvalues of the linearization of the equations about the equilibria. That is to say, by evaluating the Jacobian matrix at each of the equilibrium points of the system, and then finding the resulting eigenvalues, the equilibria can be categorized. Then the behavior of the system in the neighborhood of each equilibrium point can be qualitatively determined, (or even quantitatively determined, in some instances), by finding the eigenvector(s) associated with each eigenvalue. \n\nAn equilibrium point is \"hyperbolic\" if none of the eigenvalues have zero real part. If all eigenvalues have negative real part, the equilibrium is a stable equation. If at least one has a positive real part, the equilibrium is an unstable node. If at least one eigenvalue has negative real part and at least one has positive real part, the equilibrium is a saddle point.\n\n\n"}
{"id": "34045124", "url": "https://en.wikipedia.org/wiki?curid=34045124", "title": "Exterior space", "text": "Exterior space\n\nIn mathematics, the notion of externology in a topological space \"X\" generalizes the basic properties of the family\n\nof complements of the closed compact subspaces of \"X\", which are used to construct its Alexandroff compactification. An externology permits to introduce a notion of end point, to study the divergence of nets in terms of convergence to end points and it is a useful tool for the study and classification of some families of non compact topological spaces. It can also be used to approach a topological space as the limit of other topological spaces: the externologies are very useful when a compact metric space embedded in a Hilbert space is approached by its open neighbourhoods.\n\nLet (X,τ) be a topological space. An externology on (X,τ) is a non-empty collection ε of open subsets satisfying:\n\nAn exterior space (X,τ,ε) consists of a topological space (X,τ) together with an externology ε. An open E which is in ε is said to be an exterior-open subset. A map f:(X,τ,ε) → (X',τ',ε') is said to be an exterior map if it is continuous and f(E) ∈ ε, for all E ∈ ε'.\n\nThe category of exterior spaces and exterior maps will be denoted by E. It is remarkable that E is a complete and cocomplete category.\n\n\n\n"}
{"id": "15933750", "url": "https://en.wikipedia.org/wiki?curid=15933750", "title": "Forking extension", "text": "Forking extension\n\nIn model theory, a forking extension is an extension that is not whereas a non-forking extension is an extension that is as free as possible. This can be used to extend the notions of linear or algebraic independence to stable theories. These concepts were introduced by S. Shelah.\n\nSuppose that \"A\" and \"B\" are models of some complete ω-stable theory \"T\". \nIf \"p\" is a type of \"A\" and \"q\" is a type of \"B\" containing \"p\", then \"q\" is called a forking extension of \"p\" if its Morley rank is smaller, and a nonforking extension if it has the same Morley rank.\n\nLet \"T\" be a stable complete theory. The non-forking relation ≤ for types over \"T\" is the unique relation that satisfies the following axioms:\n\n"}
{"id": "733613", "url": "https://en.wikipedia.org/wiki?curid=733613", "title": "Geometric probability", "text": "Geometric probability\n\nProblems of the following type, and their solution techniques, were first studied in the 18th century, and the general topic became known as geometric probability.\n\n\nFor mathematical development see the concise monograph by Solomon.\n\nSince the late 20th century the topic has split into two topics with different emphases. Integral geometry sprang from the principle that the mathematically natural probability models are those that are invariant under certain transformation groups. This topic emphasises systematic development of formulas for calculating expected values associated with the geometric \nobjects derived from random points, and can in part be viewed as a sophisticated branch of multivariate calculus. Stochastic geometry emphasises the random geometrical objects themselves. For instance: different models for random lines or for random tessellations of the plane; random sets formed by making points of a spatial Poisson process be (say) centers of discs.\n\n\n"}
{"id": "42677761", "url": "https://en.wikipedia.org/wiki?curid=42677761", "title": "Graph amalgamation", "text": "Graph amalgamation\n\nIn graph theory, a graph amalgamation is a relationship between two graphs (one graph is an amalgamation of another). Similar relationships include subgraphs and minors. Amalgamations can provide a way to reduce a graph to a simpler graph while keeping certain structure intact. The amalgamation can then be used to study properties of the original graph in an easier to understand context. Applications include embeddings, computing genus distribution, and Hamiltonian decompositions.\n\nLet formula_1 and formula_2 be two graphs with the same number of edges where formula_1 has more vertices than formula_2. Then we say that formula_2 is an amalgamation of formula_1 if there is a bijection formula_7 and a surjection formula_8 and the following hold:\n\nNote that while formula_1 can be a graph or a pseudograph, it will usually be the case that formula_2 is a pseudograph.\n\nEdge colorings are invariant to amalgamation. This is obvious, as all of the edges between the two graphs are in bijection with each other. However, what may not be obvious, is that if formula_1 is a complete graph of the form formula_34, and we color the edges as to specify a Hamiltonian decomposition (a decomposition into Hamiltonian paths, then those edges also form a Hamiltonian Decomposition in formula_2.\n\nFigure 1 illustrates an amalgamation of formula_36. The invariance of edge coloring and Hamiltonian Decomposition can be seen clearly. The function formula_37 is a bijection and is given as letters in the figure. The function formula_38 is given in the table below.\n\nOne of the ways in which amalgamations can be used is to find Hamiltonian Decompositions of complete graphs with 2\"n\" + 1 vertices. The idea is to take a graph and produce an amalgamation of it which is edge colored in formula_39 colors and satisfies certain properties (called an outline Hamiltonian decomposition). We can then 'reverse' the amalgamation and we are left with formula_34 colored in a Hamiltonian Decomposition.\n\nIn Hilton outlines a method for doing this, as well as a method for finding all Hamiltonian Decompositions without repetition. The methods rely on a theorem he provides which states (roughly) that if we have an outline Hamiltonian decomposition, we could have arrived at it by first starting with a Hamiltonian decomposition of the complete graph and then finding an amalgamation for it.\n\n"}
{"id": "353042", "url": "https://en.wikipedia.org/wiki?curid=353042", "title": "Graph minor", "text": "Graph minor\n\nIn graph theory, an undirected graph \"H\" is called a minor of the graph \"G\" if \"H\" can be formed from \"G\" by deleting edges and vertices and by contracting edges.\n\nThe theory of graph minors began with Wagner's theorem that a graph is planar if and only if its minors include neither the complete graph \"K\" nor the complete bipartite graph \"K\". The Robertson–Seymour theorem implies that an analogous forbidden minor characterization exists for every property of graphs that is preserved by deletions and edge contractions.\nFor every fixed graph \"H\", it is possible to test whether \"H\" is a minor of an input graph \"G\" in polynomial time; together with the forbidden minor characterization this implies that every graph property preserved by deletions and contractions may be recognized in polynomial time.\n\nOther results and conjectures involving graph minors include the graph structure theorem, according to which the graphs that do not have \"H\" as a minor may be formed by gluing together simpler pieces, and Hadwiger's conjecture relating the inability to color a graph to the existence of a large complete graph as a minor of it. Important variants of graph minors include the topological minors and immersion minors.\n\nAn edge contraction is an operation which removes an edge from a graph while simultaneously merging the two vertices it used to connect. An undirected graph \"H\" is a minor of another undirected graph \"G\" if a graph isomorphic to \"H\" can be obtained from \"G\" by contracting some edges, deleting some edges, and deleting some isolated vertices. The order in which a sequence of such contractions and deletions is performed on \"G\" does not affect the resulting graph \"H\".\n\nGraph minors are often studied in the more general context of matroid minors. In this context, it is common to assume that all graphs are connected, with self-loops and multiple edges allowed (that is, they are multigraphs rather than simple graphs); the contraction of a loop and the deletion of a cut-edge are forbidden operations. This point of view has the advantage that edge deletions leave the rank of a graph unchanged, and edge contractions always reduce the rank by one.\n\nIn other contexts (such as with the study of pseudoforests) it makes more sense to allow the deletion of a cut-edge, and to allow disconnected graphs, but to forbid multigraphs. In this variation of graph minor theory, a graph is always simplified after any edge contraction to eliminate its self-loops and multiple edges.\n\nA function \"f\" is referred to as \"minor-monotone\" if, whenever \"H\" is a minor of \"G\", one has f(H) ≤ f(G).\n\nIn the following example, graph H is a minor of graph G:\n\nH. \n\nG. \n\nThe following diagram illustrates this. First construct a subgraph of G by deleting the dashed edges (and the resulting isolated vertex), and then contract the gray edge (merging the two vertices it connects):\n\nIt is straightforward to verify that the graph minor relation forms a partial order on the isomorphism classes of undirected graphs: it is transitive (a minor of a minor of \"G\" is a minor of \"G\" itself), and \"G\" and \"H\" can only be minors of each other if they are isomorphic because any nontrivial minor operation removes edges or vertices. A deep result by Neil Robertson and Paul Seymour states that this partial order is actually a well-quasi-ordering: if an infinite list \"G\", \"G\"... of finite graphs is given, then there always exist two indices \"i\" < \"j\" such that \"G\" is a minor of \"G\". Another equivalent way of stating this is that any set of graphs can have only a finite number of minimal elements under the minor ordering. This result proved a conjecture formerly known as Wagner's conjecture, after Klaus Wagner; Wagner had conjectured it long earlier, but only published it in 1970.\n\nIn the course of their proof, Seymour and Robertson also prove the graph structure theorem in which they determine, for any fixed graph \"H\", the rough structure of any graph which does not have \"H\" as a minor. The statement of the theorem is itself long and involved, but in short it establishes that such a graph must have the structure of a clique-sum of smaller graphs that are modified in small ways from graphs embedded on surfaces of bounded genus.\nThus, their theory establishes fundamental connections between graph minors and topological embeddings of graphs.\nFor any graph \"H\", the simple \"H\"-minor-free graphs must be sparse, which means that the number of edges is less than some constant multiple of the number of vertices. More specifically, if \"H\" has \"h\" vertices, then a simple \"n\"-vertex simple \"H\"-minor-free graph can have at most formula_1 edges, and some \"K\"-minor-free graphs have at least this many edges. Thus, if \"H\" has \"h\" vertices, then \"H\"-minor-free graphs have average degree formula_2 and furthermore degeneracy formula_2. Additionally, the \"H\"-minor-free graphs have a separator theorem similar to the planar separator theorem for planar graphs: for any fixed \"H\", and any \"n\"-vertex \"H\"-minor-free graph \"G\", it is possible to find a subset of formula_4 vertices whose removal splits \"G\" into two (possibly disconnected) subgraphs with at most 2\"n\"/3 vertices per subgraph. Even stronger, for any fixed \"H\", \"H\"-minor-free graphs have treewidth formula_4.\n\nThe Hadwiger conjecture in graph theory proposes that if a graph \"G\" does not contain a minor isomorphic to the complete graph on \"k\" vertices, then \"G\" has a proper coloring with \"k\" − 1 colors. The case \"k\" = 5 is a restatement of the four color theorem. The Hadwiger conjecture has been proven for \"k\" ≤ 6, but is unknown in the general case. call it “one of the deepest unsolved problems in graph theory.” Another result relating the four-color theorem to graph minors is the snark theorem announced by Robertson, Sanders, Seymour, and Thomas, a strengthening of the four-color theorem conjectured by W. T. Tutte and stating that any bridgeless 3-regular graph that requires four colors in an edge coloring must have the Petersen graph as a minor.\n\nMany families of graphs have the property that every minor of a graph in \"F\" is also in \"F\"; such a class is said to be \"minor-closed\". For instance, in any planar graph, or any embedding of a graph on a fixed topological surface, neither the removal of edges nor the contraction of edges can increase the genus of the embedding; therefore, planar graphs and the graphs embeddable on any fixed surface form minor-closed families.\n\nIf \"F\" is a minor-closed family, then (because of the well-quasi-ordering property of minors) among the graphs that do not belong to \"F\" there is a finite set \"X\" of minor-minimal graphs. These graphs are forbidden minors for \"F\": a graph belongs to \"F\" if and only if it does not contain as a minor any graph in \"X\". That is, every minor-closed family \"F\" can be characterized as the family of \"X\"-minor-free graphs for some finite set \"X\" of forbidden minors.\nThe best-known example of a characterization of this type is Wagner's theorem characterizing the planar graphs as the graphs having neither K nor K as minors.\n\nIn some cases, the properties of the graphs in a minor-closed family may be closely connected to the properties of their excluded minors. For example a minor-closed graph family \"F\" has bounded pathwidth if and only if its forbidden minors include a forest, \"F\" has bounded tree-depth if and only if its forbidden minors include a disjoint union of path graphs, \"F\" has bounded treewidth if and only if its forbidden minors include a planar graph, and \"F\" has bounded local treewidth (a functional relationship between diameter and treewidth) if and only if its forbidden minors include an apex graph (a graph that can be made planar by the removal of a single vertex). If \"H\" can be drawn in the plane with only a single crossing (that is, it has crossing number one) then the \"H\"-minor-free graphs have a simplified structure theorem in which they are formed as clique-sums of planar graphs and graphs of bounded treewidth. For instance, both \"K\" and \"K\" have crossing number one, and as Wagner showed the \"K\"-free graphs are exactly the 3-clique-sums of planar graphs and the eight-vertex Wagner graph, while the \"K\"-free graphs are exactly the 2-clique-sums of planar graphs and \"K\".\n\nA graph \"H\" is called a topological minor of a graph \"G\" if a subdivision of \"H\" is isomorphic to a subgraph of \"G\". It is easy to see that every topological minor is also a minor. The converse however is not true in general (for instance the complete graph \"K\" in the Petersen graph is a minor but not a topological one), but holds for graph with maximum degree not greater than three.\n\nThe topological minor relation is not a well-quasi-ordering on the set of finite graphs and hence the result of Robertson and Seymour does not apply to topological minors. However it is straightforward to construct finite forbidden topological minor characterizations from finite forbidden minor characterizations by replacing every branch set with \"k\" outgoing edges by every tree on \"k\" leaves that has down degree at least two.\n\nA graph \"H\" is called an induced minor of a graph \"G\" if it can be obtained from an induced subgraph of \"G\" by contracting edges. Otherwise, \"G\" is said to be \"H\"-induced minor-free.\n\nA graph operation called \"lifting\" is central in a concept called \"immersions\". The \"lifting\" is an operation on adjacent edges. Given three vertices \"v\", \"u\", and \"w\", where \"(v,u)\" and \"(u,w)\" are edges in the graph, the lifting of \"vuw\", or equivalent of \"(v,u), (u,w)\" is the operation that deletes the two edges \"(v,u)\" and \"(u,w)\" and adds the edge \"(v,w)\". In the case where \"(v,w)\" already was present, \"v\" and \"w\" will now be connected by more than one edge, and hence this operation is intrinsically a multi-graph operation.\n\nIn the case where a graph \"H\" can be obtained from a graph \"G\" by a sequence of lifting operations (on \"G\") and then finding an isomorphic subgraph, we say that \"H\" is an immersion minor of \"G\".\nThere is yet another way of defining immersion minors, which is equivalent to the lifting operation. We say that \"H\" is an immersion minor of \"G\" if there exists an injective mapping from vertices in \"H\" to vertices in \"G\" where the images of adjacent elements of \"H\" are connected in \"G\" by edge-disjoint paths.\n\nThe immersion minor relation is a well-quasi-ordering on the set of finite graphs and hence the result of Robertson and Seymour applies to immersion minors. This furthermore means that every immersion minor-closed family is characterized by a finite family of forbidden immersion minors.\n\nIn graph drawing, immersion minors arise as the planarizations of non-planar graphs: from a drawing of a graph in the plane, with crossings, one can form an immersion minor by replacing each crossing point by a new vertex, and in the process also subdividing each crossed edge into a path. This allows drawing methods for planar graphs to be extended to non-planar graphs.\n\nA shallow minor of a graph \"G\" is a minor in which the edges of \"G\" that were contracted to form the minor form a collection of disjoint subgraphs with low diameter. Shallow minors interpolate between the theories of graph minors and subgraphs, in that shallow minors with high depth coincide with the usual type of graph minor, while the shallow minors with depth zero are exactly the subgraphs. They also allow the theory of graph minors to be extended to classes of graphs such as the 1-planar graphs that are not closed under taking minors.\n\nAn alternative and equivalent definition of a graph minor is that \"H\" is a minor of \"G\" whenever the vertices of \"H\" can be represented by a collection of vertex-disjoint subtrees of \"G\", such that if two vertices are adjacent in \"H\", there exists an edge with its endpoints in the corresponding two trees in \"G\".\nAn odd minor restricts this definition by adding parity conditions to these subtrees. If \"H\" is represented by a collection of subtrees of \"G\" as above, then \"H\" is an odd minor of \"G\" whenever it is possible to assign two colors to the vertices of \"G\" in such a way that each edge of \"G\" within a subtree is properly colored (its endpoints have different colors) and each edge of \"G\" that represents an adjacency between two subtrees is monochromatic (both its endpoints are the same color). Unlike for the usual kind of graph minors, graphs with forbidden odd minors are not necessarily sparse. The Hadwiger conjecture, that \"k\"-chromatic graphs necessarily contain \"k\"-vertex complete graphs as minors, has also been studied from the point of view of odd minors.\n\nA different parity-based extension of the notion of graph minors is the concept of a bipartite minor, which produces a bipartite graph whenever the starting graph is bipartite. A graph \"H\" is a bipartite minor of another graph \"G\" whenever \"H\" can be obtained from \"G\" by deleting vertices, deleting edges, and collapsing pairs of vertices that are at distance two from each other along a peripheral cycle of the graph. A form of Wagner's theorem applies for bipartite minors: A bipartite graph \"G\" is a planar graph if and only if it does not have the utility graph \"K\" as a bipartite minor.\n\nThe problem of deciding whether a graph \"G\" contains \"H\" as a minor is NP-complete in general; for instance, if \"H\" is a cycle graph with the same number of vertices as \"G\", then \"H\" is a minor of \"G\" if and only if \"G\" contains a Hamiltonian cycle. However, when \"G\" is part of the input but \"H\" is fixed, it can be solved in polynomial time. More specifically, the running time for testing whether \"H\" is a minor of \"G\" in this case is \"O\"(\"n\"), where \"n\" is the number of vertices in \"G\" and the big O notation hides a constant that depends superexponentially on \"H\"; since the original Graph Minors result, this algorithm has been improved to \"O\"(\"n\") time. Thus, by applying the polynomial time algorithm for testing whether a given graph contains any of the forbidden minors, it is possible to recognize the members of any minor-closed family in polynomial time. However, in order to apply this result constructively, it is necessary to know what the forbidden minors of the graph family are. In some cases, the forbidden minors are known, or can be computed.\n\nIn the case where \"H\" is a fixed planar graph, then we can test in linear time in an input graph \"G\" whether \"H\" is a minor of \"G\". In cases where \"H\" is not fixed, faster algorithms are known in the case where \"G\" is planar.\n"}
{"id": "8712675", "url": "https://en.wikipedia.org/wiki?curid=8712675", "title": "Hamming(7,4)", "text": "Hamming(7,4)\n\nIn coding theory, Hamming(7,4) is a linear error-correcting code that encodes four bits of data into seven bits by adding three parity bits. It is a member of a larger family of Hamming codes, but the term \"Hamming code\" often refers to this specific code that Richard W. Hamming introduced in 1950. At the time, Hamming worked at Bell Telephone Laboratories and was frustrated with the error-prone punched card reader, which is why he started working on error-correcting codes.\n\nThe Hamming code adds three additional check bits to every four data bits of the message. Hamming's (7,4) algorithm can correct any single-bit error, or detect all single-bit and two-bit errors. In other words, the minimal Hamming distance between any two correct codewords is 3, and received words can be correctly decoded if they are at a distance of at most one from the codeword that was transmitted by the sender. This means that for transmission medium situations where burst errors do not occur, Hamming's (7,4) code is effective (as the medium would have to be extremely noisy for two out of seven bits to be flipped).\n\nIn quantum information, the Hamming (7,4) is used as the base for the Steane code, a type of CSS code used for quantum error correction. \n\nThe goal of the Hamming codes is to create a set of parity bits that overlap such that a single-bit error (the bit is logically flipped in value) in a data bit \"or\" a parity bit can be detected \"and\" corrected. While multiple overlaps can be created, the general method is presented in Hamming codes.\n\nThis table describes which parity bits cover which transmitted bits in the encoded word. For example, \"p\" provides an even parity for bits 2, 3, 6, and 7. It also details which transmitted bit is covered by which parity bit by reading the column. For example, \"d\" is covered by \"p\" and \"p\" but not \"p\" This table will have a striking resemblance to the parity-check matrix (H) in the next section.\n\nFurthermore, if the parity columns in the above table were removed\nthen resemblance to rows 1, 2, and 4 of the code generator matrix (G) below will also be evident.\n\nSo, by picking the parity bit coverage correctly, all errors with a Hamming distance of 1 can be detected and corrected, which is the point of using a Hamming code.\n\nHamming codes can be computed in linear algebra terms through matrices because Hamming codes are linear codes. For the purposes of Hamming codes, two Hamming matrices can be defined: the code generator matrix G and the parity-check matrix H:\n\nAs mentioned above, rows 1, 2, and 4 of G should look familiar as they map the data bits to their parity bits:\nThe remaining rows (3, 5, 6, 7) map the data to their position in encoded form and there is only 1 in that row so it is an identical copy. In fact, these four rows are linearly independent and form the identity matrix (by design, not coincidence).\n\nAlso as mentioned above, the three rows of H should be familiar. These rows are used to compute the syndrome vector at the receiving end and if the syndrome vector is the null vector (all zeros) then the received word is error-free; if non-zero then the value indicates which bit has been flipped.\n\nThe four data bits — assembled as a vector p — is pre-multiplied by G (i.e., Gp) and taken modulo 2 to yield the encoded value that is transmitted. The original 4 data bits are converted to seven bits (hence the name \"Hamming(7,4)\") with three parity bits added to ensure even parity using the above data bit coverages. The first table above shows the mapping between each data and parity bit into its final bit position (1 through 7) but this can also be presented in a Venn diagram. The first diagram in this article shows three circles (one for each parity bit) and encloses data bits that each parity bit covers. The second diagram (shown to the right) is identical but, instead, the bit positions are marked.\n\nFor the remainder of this section, the following 4 bits (shown as a column vector) will be used as a running example:\n\nSuppose we want to transmit this data (codice_1) over a noisy communications channel. Specifically, a binary symmetric channel meaning that error corruption does not favor either zero or one (it is symmetric in causing errors). Furthermore, all source vectors are assumed to be equiprobable. We take the product of G and p, with entries modulo 2, to determine the transmitted codeword x:\n\nThis means that codice_2 would be transmitted instead of transmitting codice_1.\n\nProgrammers concerned about multiplication should observe that each row of the result is the least significant bit of the Population Count of set bits resulting from the row and column being Bitwise ANDed together rather than multiplied.\n\nIn the adjacent diagram, the seven bits of the encoded word are inserted into their respective locations; from inspection it is clear that the parity of the red, green, and blue circles are even:\n\nWhat will be shown shortly is that if, during transmission, a bit is flipped then the parity of two or all three circles will be incorrect and the errored bit can be determined (even if one of the parity bits) by knowing that the parity of all three of these circles should be even.\n\nIf no error occurs during transmission, then the received codeword r is identical to the transmitted codeword x:\n\nThe receiver multiplies H and r to obtain the syndrome vector z, which indicates whether an error has occurred, and if so, for which codeword bit. Performing this multiplication (again, entries modulo 2):\n\nSince the syndrome z is the null vector, the receiver can conclude that no error has occurred. This conclusion is based on the observation that when the data vector is multiplied by G, a change of basis occurs into a vector subspace that is the kernel of H. As long as nothing happens during transmission, r will remain in the kernel of H and the multiplication will yield the null vector.\n\nOtherwise, suppose a \"single\" bit error has occurred. Mathematically, we can write\n\nmodulo 2, where e is the formula_7 unit vector, that is, a zero vector with a 1 in the formula_8, counting from 1.\n\nThus the above expression signifies a single bit error in the formula_8 place.\n\nNow, if we multiply this vector by H:\n\nSince x is the transmitted data, it is without error, and as a result, the product of H and x is zero. Thus\n\nNow, the product of H with the formula_8 standard basis vector picks out that column of H, we know the error occurs in the place where this column of H occurs.\n\nFor example, suppose we have introduced a bit error on bit #5\n\nThe diagram to the right shows the bit error (shown in blue text) and the bad parity created (shown in red text) in the red and green circles. The bit error can be detected by computing the parity of the red, green, and blue circles. If a bad parity is detected then the data bit that overlaps \"only\" the bad parity circles is the bit with the error. In the above example, the red and green circles have bad parity so the bit corresponding to the intersection of red and green but not blue indicates the errored bit.\n\nNow,\n\nwhich corresponds to the fifth column of H. Furthermore, the general algorithm used (\"see Hamming code#General algorithm\") was intentional in its construction so that the syndrome of 101 corresponds to the binary value of 5, which indicates the fifth bit was corrupted. Thus, an error has been detected in bit 5, and can be corrected (simply flip or negate its value):\n\nThis corrected received value indeed, now, matches the transmitted value x from above.\n\nOnce the received vector has been determined to be error-free or corrected if an error occurred (assuming only zero or one bit errors are possible) then the received data needs to be decoded back into the original four bits.\n\nFirst, define a matrix R:\n\nThen the received value, p, is equal to Rr. Using the running example from above\n\nIt is not difficult to show that only single bit errors can be corrected using this scheme. Alternatively, Hamming codes can be used to detect single and double bit errors, by merely noting that the product of H is nonzero whenever errors have occurred. In the adjacent diagram, bits 4 and 5 were flipped. This yields only one circle (green) with an invalid parity but the errors are not recoverable.\n\nHowever, the Hamming (7,4) and similar Hamming codes cannot distinguish between single-bit errors and two-bit errors. That is, two-bit errors appear the same as one-bit errors. If error correction is performed on a two-bit error the result will be incorrect.\n\nSimilarly, Hamming codes cannot detect or recover from an arbitrary three-bit error; Consider the diagram: if the bit in the green circle (colored red) were 1, the parity checking would return the null vector, indicating that there is no error in the codeword.\n\nSince the source is only 4 bits then there are only 16 possible transmitted words. Included is the eight-bit value if an extra parity bit is used (\"see Hamming(7,4) code with an additional parity bit\"). (The data bits are shown in blue; the parity bits are shown in red; and the extra parity bit shown in green.)\n\n"}
{"id": "243316", "url": "https://en.wikipedia.org/wiki?curid=243316", "title": "Homogeneous coordinates", "text": "Homogeneous coordinates\n\nIn mathematics, homogeneous coordinates or projective coordinates, introduced by August Ferdinand Möbius in his 1827 work \"Der barycentrische Calcül\", are a system of coordinates used in projective geometry, as Cartesian coordinates are used in Euclidean geometry. They have the advantage that the coordinates of points, including points at infinity, can be represented using finite coordinates. Formulas involving homogeneous coordinates are often simpler and more symmetric than their Cartesian counterparts. Homogeneous coordinates have a range of applications, including computer graphics and 3D computer vision, where they allow affine transformations and, in general, projective transformations to be easily represented by a matrix.\n\nIf the homogeneous coordinates of a point are multiplied by a non-zero scalar then the resulting coordinates represent the same point. Since homogeneous coordinates are also given to points at infinity, the number of coordinates required to allow this extension is one more than the dimension of the projective space being considered. For example, two homogeneous coordinates are required to specify a point on the projective line and three homogeneous coordinates are required to specify a point in the projective plane.\nThe real projective plane can be thought of as the Euclidean plane with additional points added, which are called points at infinity, and are considered to lie on a new line, the line at infinity. There is a point at infinity corresponding to each direction (numerically given by the slope of a line), informally defined as the limit of a point that moves in that direction away from the origin. Parallel lines in the Euclidean plane are said to intersect at a point at infinity corresponding to their common direction. Given a point on the Euclidean plane, for any non-zero real number \"Z\", the triple is called a \"set of homogeneous coordinates\" for the point. By this definition, multiplying the three homogeneous coordinates by a common, non-zero factor gives a new set of homogeneous coordinates for the same point. In particular, is such a system of homogeneous coordinates for the point .\nFor example, the Cartesian point can be represented in homogeneous coordinates as or . The original Cartesian coordinates are recovered by dividing the first two positions by the third. Thus unlike Cartesian coordinates, a single point can be represented by infinitely many homogeneous coordinates.\n\nThe equation of a line through the origin may be written where \"n\" and \"m\" are not both 0. In parametric form this can be written . Let \"Z\" = 1/\"t\", so the coordinates of a point on the line may be written . In homogeneous coordinates this becomes . In the limit, as \"t\" approaches infinity, in other words, as the point moves away from the origin, \"Z\" approaches 0 and the homogeneous coordinates of the point become . Thus we define as the homogeneous coordinates of the point at infinity corresponding to the direction of the line . As any line of the Euclidean plane is parallel to a line passing through the origin, and since parallel lines have the same point at infinity, the infinite point on every line of the Euclidean plane has been given homogeneous coordinates.\n\nTo summarize:\nNote that the triple is omitted and does not represent any point. The origin is represented by .\n\nSome authors use different notations for homogeneous coordinates which help distinguish them from Cartesian coordinates. The use of colons instead of commas, for example (\"x\":\"y\":\"z\") instead of , emphasizes that the coordinates are to be considered ratios. Square brackets, as in emphasize that multiple sets of coordinates are associated with a single point. Some authors use a combination of colons and square brackets, as in [\"x\":\"y\":\"z\"].\n\nThe discussion in the preceding section applies analogously to projective spaces other than the plane. So the points on the projective line may be represented by pairs of coordinates , not both zero. In this case, the point at infinity is . Similarly the points in projective \"n\"-space are represented by (\"n\" + 1)-tuples.\n\nThe use of real numbers gives the homogeneous coordinates of points in the classical case of the real projective spaces, however any field may be used, in particular, the complex numbers may be used for complex projective space. For example, the complex projective line uses two homogeneous complex coordinates and is known as the Riemann sphere. Other fields, including finite fields, can be used.\n\nHomogeneous coordinates for projective spaces can also be created with elements from a division ring (skewfield). However, in this case, care must be taken to account for the fact that multiplication may not be commutative.\n\nAnother definition of the real projective plane can be given in terms of equivalence classes. For non-zero elements of R, define to mean there is a non-zero \"λ\" so that . Then ~ is an equivalence relation and the projective plane can be defined as the equivalence classes of If is one of the elements of the equivalence class \"p\" then these are taken to be homogeneous coordinates of \"p\".\n\nLines in this space are defined to be sets of solutions of equations of the form where not all of \"a\", \"b\" and \"c\" are zero. The condition depends only on the equivalence class of so the equation defines a set of points in the projective plane. The mapping defines an inclusion from the Euclidean plane to the projective plane and the complement of the image is the set of points with . This is the equation of a line according to the definition and the complement is called the \"line at infinity\".\n\nThe equivalence classes, \"p\", are the lines through the origin with the origin removed. The origin does not really play an essential part in the previous discussion so it can be added back in without changing the properties of the projective plane. This produces a variation on the definition, namely the projective plane is defined as the set of lines in R that pass through the origin and the coordinates of a non-zero element of a line are taken to be homogeneous coordinates of the line. These lines are now interpreted as points in the projective plane.\n\nAgain, this discussion applies analogously to other dimensions. So the projective space of dimension \"n\" can be defined as the set of lines through the origin in R.\n\nHomogeneous coordinates are not uniquely determined by a point, so a function defined on the coordinates, say , does not determine a function defined on points as with Cartesian coordinates. But a condition defined on the coordinates, as might be used to describe a curve, determines a condition on points if the function is homogeneous. Specifically, suppose there is a \"k\" such that\n\nIf a set of coordinates represent the same point as then it can be written for some non-zero value of λ. Then\n\nA polynomial of degree \"k\" can be turned into a homogeneous polynomial by replacing \"x\" with \"x\"/\"z\", \"y\" with \"y\"/\"z\" and multiplying by \"z\", in other words by defining\n\nThe resulting function \"f\" is a polynomial so it makes sense to extend its domain to triples where . The process can be reversed by setting , or\n\nThe equation can then be thought of as the homogeneous form of and it defines the same curve when restricted to the Euclidean plane. For example, the homogeneous form of the equation of the line is \n\nThe equation of a line in the projective plane may be given as where \"s\", \"t\" and \"u\" are constants. Each triple determines a line, the line determined is unchanged if it is multiplied by a non-zero scalar, and at least one of \"s\", \"t\" and \"u\" must be non-zero. So the triple may be taken to be homogeneous coordinates of a line in the projective plane, that is line coordinates as opposed to point coordinates. If in \"sx\" + \"ty\" + \"uz\" = 0 the letters \"s\", \"t\" and \"u\" are taken as variables and \"x\", \"y\" and \"z\" are taken as constants then the equation becomes an equation of a set of lines in the space of all lines in the plane. Geometrically it represents the set of lines that pass through the point and may be interpreted as the equation of the point in line-coordinates. In the same way, planes in 3-space may be given sets of four homogeneous coordinates, and so on for higher dimensions.\n\nThe same relation, , may be regarded as either the equation of a line or the equation of a point. In general, there is no difference either algebraically or logically between the homogeneous coordinates of points and lines. So plane geometry with points as the fundamental elements and plane geometry with lines as the fundamental elements are equivalent except for interpretation. This leads to the concept of \"duality\" in projective geometry, the principle that the roles of points and lines can be interchanged in a theorem in projective geometry and the result will also be a theorem. Analogously, the theory of points in projective 3-space is dual to the theory of planes in projective 3-space, and so on for higher dimensions.\n\nAssigning coordinates to lines in projective 3-space is more complicated since it would seem that a total of 8 coordinates, either the coordinates of two points which lie on the line or two planes whose intersection is the line, are required. A useful method, due to Julius Plücker, creates a set of six coordinates as the determinants from the homogeneous coordinates of two points and on the line. The Plücker embedding is the generalization of this to create homogeneous coordinates of elements of any dimension \"m\" in a projective space of dimension \"n\".\n\nBézout's theorem predicts that the number of points of intersection of two curves is equal to the product of their degrees (assuming an algebraically closed field and with certain conventions followed for counting intersection multiplicities). Bézout's theorem predicts there is one point of intersection of two lines and in general this is true, but when the lines are parallel the point of intersection is infinite. Homogeneous coordinates are used to locate the point of intersection in this case. Similarly, Bézout's theorem predicts that a line will intersect a conic at two points, but in some cases one or both of the points is infinite and homogeneous coordinates must be used to locate them. For example, and have only one point of intersection in the finite (affine) plane. To find the other point of intersection, convert the equations into homogeneous form, and . This produces and, assuming not all of \"x\", \"y\" and \"z\" are 0, the solutions are and . This first solution is the point in Cartesian coordinates, the finite point of intersection. The second solution gives the homogeneous coordinates which corresponds to the direction of the \"y\"-axis. For the equations and there are no finite points of intersection. Converting the equations into homogeneous form gives and . Solving produces the equation which has a double root at . From the original equation, , so since at least one coordinate must be non-zero. Therefore, is the point of intersection counted with multiplicity 2 in agreement with the theorem.\n\nThe homogeneous form for the equation of a circle in the real or complex projective plane is . The intersection of this curve with the line at infinity can be found by setting . This produces the equation which has two solutions over the complex numbers, giving rise to the points with homogeneous coordinates and in the complex projective plane. These points are called the circular points at infinity and can be regarded as the common points of intersection of all circles. This can be generalized to curves of higher order as circular algebraic curves.\n\nJust as the selection of axes in the Cartesian coordinate system is somewhat arbitrary, the selection of a single system of homogeneous coordinates out of all possible systems is somewhat arbitrary. Therefore, it is useful to know how the different systems are related to each other.\n\nLet be the homogeneous coordinates of a point in the projective plane. A fixed matrix\nwith nonzero determinant, defines a new system of coordinates by the equation\nMultiplication of by a scalar results in the multiplication of by the same scalar, and \"X\", \"Y\" and \"Z\" cannot be all 0 unless \"x\", \"y\" and \"z\" are all zero since \"A\" is nonsingular. So are a new system of homogeneous coordinates for the same point of the projective plane.\n\nMöbius' original formulation of homogeneous coordinates specified the position of a point as the center of mass (or barycenter) of a system of three point masses placed at the vertices of a fixed triangle. Points within the triangle are represented by positive masses and points outside the triangle are represented by allowing negative masses. Multiplying the masses in the system by a scalar does not affect the center of mass, so this is a special case of a system of homogeneous coordinates.\n\nLet \"l\", \"m\", \"n\" be three lines in the plane and define a set of coordinates \"X\", \"Y\" and \"Z\" of a point \"p\" as the signed distances from \"p\" to these three lines. These are called the \"trilinear coordinates\" of \"p\" with respect to the triangle whose vertices are the pairwise intersections of the lines. Strictly speaking these are not homogeneous, since the values of \"X\", \"Y\" and \"Z\" are determined exactly, not just up to proportionality. There is a linear relationship between them however, so these coordinates can be made homogeneous by allowing multiples of to represent the same point. More generally, \"X\", \"Y\" and \"Z\" can be defined as constants \"p\", \"r\" and \"q\" times the distances to \"l\", \"m\" and \"n\", resulting in a different system of homogeneous coordinates with the same triangle of reference. This is, in fact, the most general type of system of homogeneous coordinates for points in the plane if none of the lines is the line at infinity.\n\nHomogeneous coordinates are ubiquitous in computer graphics because they allow common vector operations such as translation, rotation, scaling and perspective projection to be represented as a matrix by which the vector is multiplied. By the chain rule, any sequence of such operations can be multiplied out into a single matrix, allowing simple and efficient processing. By contrast, using Cartesian coordinates, translations and perspective projection cannot be expressed as matrix multiplications, though other operations can. Modern OpenGL and Direct3D graphics cards take advantage of homogeneous coordinates to implement a vertex shader efficiently using vector processors with 4-element registers.\n\nFor example, in perspective projection, a position in space is associated with the line from it to a fixed point called the \"center of projection\". The point is then mapped to a plane by finding the point of intersection of that plane and the line. This produces an accurate representation of how a three-dimensional object appears to the eye. In the simplest situation, the center of projection is the origin and points are mapped to the plane , working for the moment in Cartesian coordinates. For a given point in space, , the point where the line and the plane intersect is . Dropping the now superfluous \"z\" coordinate, this becomes . In homogeneous coordinates, the point is represented by and the point it maps to on the plane is represented by , so projection can be represented in matrix form as\nMatrices representing other geometric transformations can be combined with this and each other by matrix multiplication. As a result, any perspective projection of space can be represented as a single matrix.\n\n\n"}
{"id": "13580667", "url": "https://en.wikipedia.org/wiki?curid=13580667", "title": "JLO cocycle", "text": "JLO cocycle\n\nIn noncommutative geometry, the JLO cocycle is a cocycle (and thus defines a cohomology class) in entire cyclic cohomology. It is a non-commutative version of the classic Chern character of the conventional differential geometry. In noncommutative geometry, the concept of a manifold is replaced by a noncommutative algebra formula_1 of \"functions\" on the putative noncommutative space. The cyclic cohomology of the algebra formula_1 contains the information about the topology of that noncommutative space, very much as the de Rham cohomology contains the information about the topology of a conventional manifold.\n\nThe JLO cocycle is associated with a metric structure of non-commutative differential geometry known as a formula_3-summable spectral triple (also known as a formula_3-summable Fredholm module).\n\nA formula_3-summable spectral triple consists of the following data:\n\n(a) A Hilbert space formula_7 such that formula_1 acts on it as an algebra of bounded operators.\n\n(b) A formula_9-grading formula_10 on formula_7, formula_12. We assume that the algebra formula_1 is even under the formula_9-grading, i.e. formula_15, for all formula_16.\n\n(c) A self-adjoint (unbounded) operator formula_17, called the \"Dirac operator\" such that\n\nA classic example of a formula_3-summable spectral triple arises as follows. Let formula_28 be a compact spin manifold, formula_29, the algebra of smooth functions on formula_28, formula_7 the Hilbert space of square integrable forms on formula_28, and formula_17 the standard Dirac operator.\n\nThe JLO cocycle formula_34 is a sequence\n\nof functionals on the algebra formula_1, where\n\nfor formula_39. The cohomology class defined by formula_34 is independent of the value of formula_41.\n\n"}
{"id": "4743700", "url": "https://en.wikipedia.org/wiki?curid=4743700", "title": "Jordan's lemma", "text": "Jordan's lemma\n\nIn complex analysis, Jordan's lemma is a result frequently used in conjunction with the residue theorem to evaluate contour integrals and improper integrals. It is named after the French mathematician Camille Jordan.\n\nConsider a complex-valued, continuous function , defined on a semicircular contour\n\nof positive radius lying in the upper half-plane, centred at the origin. If the function is of the form\n\nwith a positive parameter , then Jordan's lemma states the following upper bound for the contour integral:\n\nwhere equal sign is when vanishes everywhere. An analogous statement for a semicircular contour in the lower half-plane holds when .\n\n\n\nJordan's lemma yields a simple way to calculate the integral along the real axis of functions holomorphic on the upper half-plane and continuous on the closed upper half-plane, except possibly at a finite number of non-real points , , …, . Consider the closed contour , which is the concatenation of the paths and shown in the picture. By definition,\n\nSince on the variable is real, the second integral is real:\n\nThe left-hand side may be computed using the residue theorem to get, for all larger than the maximum of , , …, ,\n\nwhere denotes the residue of at the singularity . Hence, if satisfies condition (), then taking the limit as tends to infinity, the contour integral over vanishes by Jordan's lemma and we get the value of the improper integral\n\nThe function\n\nsatisfies the condition of Jordan's lemma with for all with . Note that, for ,\n\nhence () holds. Since the only singularity of in the upper half plane is at , the above application yields\n\nSince is a simple pole of and , we obtain\n\nso that\n\nThis result exemplifies the way some integrals difficult to compute with classical methods are easily evaluated with the help of complex analysis.\n\nBy definition of the complex line integral,\n\nNow the inequality\n\nyields\n\nUsing as defined in () and the symmetry , we obtain\n\nSince the graph of is concave on the interval , the graph of lies above the straight line connecting its endpoints, hence\n\nfor all , which further implies\n\n"}
{"id": "665108", "url": "https://en.wikipedia.org/wiki?curid=665108", "title": "Joseph Larmor", "text": "Joseph Larmor\n\nSir Joseph Larmor FRS FRSE DCL LLD (11 July 1857 – 19 May 1942) was an Irish physicist and mathematician who made innovations in the understanding of electricity, dynamics, thermodynamics, and the electron theory of matter. His most influential work was \"Aether and Matter\", a theoretical physics book published in 1900.\n\nHe was born in Magheragall in County Antrim the son of Hugh Larmor, a Belfast shopkeeper and his wife, Anna Wright. The family moved to Belfast around 1860, and he was educated at the Royal Belfast Academical Institution, and then studied mathematics and experimental science at science at Queen's College, Belfast (BA 1874, MA 1875),where one of his teachers was John Purser. He subsequently studied at St John's College, Cambridge where in 1880 he was Senior Wrangler (J. J. Thomson was second wrangler that year) and Smith's Prizeman, getting his MA in 1883. After teaching physics for a few years at Queen's College, Galway, he accepted a lectureship in mathematics at Cambridge in 1885. In 1892 he was elected a Fellow of the Royal Society of London and was made an Honorary Fellow of the Royal Society of Edinburgh in 1910.\n\nIn 1903 he was appointed Lucasian Professor of Mathematics at Cambridge, a post he retained until his retirement in 1932. He never married. He was knighted by King Edward VII in 1909.\n\nMotivated by his strong opposition to Home Rule for Ireland, in February 1911 Larmor ran for and was elected as Member of Parliament for Cambridge University (UK Parliament constituency) with the Liberal Unionist party. He remained in parliament until the 1922 general election, at which point the Irish question had been settled. Upon his retirement from Cambridge in 1932 Larmor moved back to County Down in Northern Ireland.\n\nHe received the honorary Doctor of Laws (LL.D) from the University of Glasgow in June 1901. He was awarded the Poncelet Prize for 1918 by the French Academy of Sciences. Larmor was a Plenary Speaker in 1920 at the ICM at Strasbourg and an Invited Speaker at the ICM in 1924 in Toronto and at the ICM in 1928 in Bologna.\n\nHe died in Holywood, County Down on 19 May 1942.\n\nLarmor proposed that the aether could be represented as a homogeneous fluid medium which was perfectly incompressible and elastic. Larmor believed the aether was separate from matter. He united Lord Kelvin's model of spinning gyrostats (see Vortex theory of the atom) with this theory. Larmor held that matter consisted of particles moving in the aether. Larmor believed the source of electric charge was a \"particle\" (which as early as 1894 he was referring to as the electron). Larmor held that the flow of charged particles constitutes the current of conduction (but was not part of the atom). Larmor calculated the rate of energy radiation from an accelerating electron. Larmor explained the splitting of the spectral lines in a magnetic field by the oscillation of electrons.\n\nIn 1919, Larmor proposed sunspots are self-regenerative dynamo action on the Sun's surface.\n\nParallel to the development of Lorentz ether theory, Larmor published an approximation to the Lorentz transformations in the \"Philosophical Transactions of the Royal Society\" in 1897,\nnamely formula_1 for the spatial part and formula_2 for the temporal part, where formula_3 and the local time formula_4. He obtained the full Lorentz transformation in 1900 by inserting formula_5 into his expression of local time such that formula_6, and as before formula_7 and formula_8. This was done around the same time as Hendrik Lorentz (1899, 1904) and five years before Albert Einstein (1905). \n\nLarmor however did not possess the correct velocity transformations, which include the addition of velocities law, which were later discovered by Henri Poincaré. Larmor predicted the phenomenon of time dilation, at least for orbiting electrons, by writing (Larmor 1897): \"... individual electrons describe corresponding parts of their orbits in times shorter for the [rest] system in the ratio (1 – \"v\"/\"c\")\". He also verified that the FitzGerald–Lorentz contraction (length contraction) should occur for bodies whose atoms were held together by electromagnetic forces. In his book \"Aether and Matter\" (1900), he again presented the Lorentz transformations, time dilation and length contraction (treating these as dynamic rather than kinematic effects). Larmor was opposed to the spacetime interpretation of the Lorentz transformation in special relativity because he continued to believe in an absolute aether. He was also critical of the curvature of space of general relativity, to the extent that he claimed that an absolute time was essential to astronomy (Larmor 1924, 1927).\n\nLarmor edited the collected works of George Stokes, James Thomson and William Thomson.\n\n\n"}
{"id": "11726298", "url": "https://en.wikipedia.org/wiki?curid=11726298", "title": "Linear extension", "text": "Linear extension\n\nIn order theory, a branch of mathematics, a linear extension of a partial order is a total order (or linear order) that is compatible with the partial order. As a classic example, the lexicographic order of totally ordered sets is a linear extension of their product order.\n\nGiven any partial orders ≤ and ≤ on a set \"X\", ≤ is a linear extension of ≤ exactly when (1) ≤ is a total order and (2) for every \"x\" and \"y\" in \"X\", if , then . It is that second property that leads mathematicians to describe ≤ as extending ≤.\n\nAlternatively, a linear extension may be viewed as an order-preserving bijection from a partially ordered set \"P\" to a chain \"C\" on the same ground set.\n\nThe statement that every partial order can be extended to a total order is known as the order-extension principle. A proof using the axiom of choice was first published by Edward Marczewski in 1930. Marczewski writes that the theorem had previously been proven by Stefan Banach, Kazimierz Kuratowski, and Alfred Tarski, again using the axiom of choice, but that the proofs had not been published.\n\nIn modern axiomatic set theory the order-extension principle is itself taken as an axiom, of comparable ontological status to the axiom of choice. The order-extension principle is implied by the Boolean prime ideal theorem or the equivalent compactness theorem, but the reverse implication doesn't hold.\n\nApplying the order-extension principle to a partial order in which every two elements are incomparable shows that (under this principle) every set can be linearly ordered. This assertion that every set can be linearly ordered is known as the ordering principle, OP, and is a weakening of the well-ordering theorem. However, there are models of set theory in which the ordering principle holds while the order-extension principle does not.\n\nThe order extension principle is constructively provable for \"finite\" sets using topological sorting algorithms, where the partial order is represented by a directed acyclic graph with the set's elements as its vertices. Several algorithms can find an extension in linear time. Despite the ease of finding a single linear extension, the problem of counting all linear extensions of a finite partial order is #P-complete; however, it may be estimated by a fully polynomial-time randomized approximation scheme. Among all partial orders with a fixed number of elements and a fixed number of comparable pairs, the partial orders that have the largest number of linear extensions are semiorders.\n\nThe order dimension of a partial order is the minimum cardinality of a set of linear extensions whose intersection is the given partial order; equivalently, it is the minimum number of linear extensions needed to ensure that each critical pair of the partial order is reversed in at least one of the extensions.\n\nAntimatroids may be viewed as generalizing partial orders; in this view, the structures corresponding to the linear extensions of a partial order are the basic words of the antimatroid.\n\nThis area also includes one of order theory's most famous open problems, the 1/3–2/3 conjecture, which states that in any finite partially ordered set \"P\" that is not totally ordered there exists a pair (\"x\",\"y\") of elements of \"P\" for which the linear extensions of \"P\" in which number between 1/3 and 2/3 of the total number of linear extensions of \"P\". An equivalent way of stating the conjecture is that, if one chooses a linear extension of \"P\" uniformly at random, there is a pair (\"x\",\"y\") which has probability between 1/3 and 2/3 of being ordered as . However, for certain infinite partially ordered sets, with a canonical probability defined on its linear extensions as a limit of the probabilities for finite partial orders that cover the infinite partial order, the 1/3–2/3 conjecture does not hold.\n"}
{"id": "209103", "url": "https://en.wikipedia.org/wiki?curid=209103", "title": "List of numbers", "text": "List of numbers\n\nThis is a list of articles about numbers (\"not\" about numerals).\n\nA rational number is any number that can be expressed as the quotient or fraction of two integers, a numerator and a non-zero denominator . Since may be equal to 1, every integer is a rational number. The set of all rational numbers, often referred to as \"the rationals\", the field of rationals or the field of rational numbers is usually denoted by a boldface (or blackboard bold formula_1, Unicode ℚ); it was thus denoted in 1895 by Giuseppe Peano after \"quoziente\", Italian for \"quotient\".\n\nNatural numbers are those used for counting (as in \"there are \"six\" (6) coins on the table\") and ordering (as in \"this is the \"third\" (3rd) largest city in the country\"). In common language, words used for counting are \"cardinal numbers\" and words used for ordering are \"ordinal numbers\". There are infinitely many natural numbers.\n\nA power of ten 18 is a number 10, where \"k\" is an integer. For instance, with \"k\" = 0, 1, 2, 3, ..., the appropriate powers of ten are 1, 10, 100, 1000, ... Powers of ten can also be fractional: for instance, \"k\" = -3 gives 1/1000, or 0.001.\n\nIn scientific notation, real numbers are written in the form \"m\" × 10. The number 394,000 is written in this form as 3.94 × 10.\n\nIntegers that are notable for their mathematical properties or cultural meanings include:\n\n\nA prime number is a positive integer which has exactly two divisors: 1 and itself.\n\nThe first 100 prime numbers are:\n\nA highly composite number (HCN) is a positive integer with more divisors than any smaller positive integer. They are often used in geometry, grouping and time measurement.\n\nThe first 20 highly composite numbers are:\n\n1, 2, 4, 6, 12, 24, 36, 48, 60, 120, 180, 240, 360, 720, 840, 1260, 1680, 2520, 5040, 7560.\n\nA perfect number is an integer that is the sum of its positive proper divisors (all divisors except itself).\n\nThe first 10 perfect numbers:\n\nIn the following tables, [and] indicates that the word \"and\" is used in some dialects (such as British English), and omitted in other dialects (such as American English).\n\nThis table demonstrates the standard English construction of small cardinal numbers up to one hundred million—names for which all variants of English agree.\nThis table compares the English names of cardinal numbers according to various American, British, and Continental European conventions. See English numerals or names of large numbers for more information on naming numbers.\nThere is no consistent and widely accepted way to extend cardinals beyond centillion (centilliard).\n\nThis is a table of English names for non-negative rational numbers less than or equal to 1. It also lists alternative names, but there is no widespread convention for the names of extremely small positive numbers.\n\nKeep in mind that rational numbers like 0.12 can be represented in infinitely many ways, e.g. \"zero-point-one-two\" (0.12), \"twelve percent\" (12%), \"three twenty-fifths\" (), \"nine seventy-fifths\" (), \"six fiftieths\" (), \"twelve hundredths\" (), \"twenty-four two-hundredths\" (), etc.\n\n\nThese are irrational numbers that are thought to be, but have not yet been proved to be, transcendental.\n\n\nHypercomplex number is a traditional term for an element of a unital algebra over the field of real numbers.\n\n\n\nTransfinite numbers are numbers that are \"infinite\" in the sense that they are larger than all finite numbers, yet not necessarily absolutely infinite.\n\nVarious terms have arisen to describe commonly used measured quantities.\n\nPhysical quantities that appear in the universe are often described using physical constants.\n\nMany languages have words expressing indefinite and fictitious numbers—inexact terms of indefinite size, used forever comic effect, for exaggeration, as placeholder names, or when precision is unnecessary or undesirable. One technical term for such words is \"non-numerical vague quantifier\". Such words designed to indicate large quantities can be called \"indefinite hyperbolic numerals\".\n\n\n\n"}
{"id": "40279847", "url": "https://en.wikipedia.org/wiki?curid=40279847", "title": "List of things named after Pierre-Simon Laplace", "text": "List of things named after Pierre-Simon Laplace\n\nThis is a list of things named after Pierre-Simon Laplace\n\n\n\n\n\n\n\n\n\n"}
{"id": "1207129", "url": "https://en.wikipedia.org/wiki?curid=1207129", "title": "Location arithmetic", "text": "Location arithmetic\n\nLocation arithmetic (Latin \"arithmeticæ localis\") is the additive (non-positional) binary numeral systems, which John Napier explored as a computation technique in his treatise \"Rabdology\" (1617), both symbolically and on a chessboard-like grid.\n\nNapier's terminology, derived from using the positions of counters on the board to represent numbers, is potentially misleading in current vocabulary because the numbering system is non-positional.\n\nDuring Napier's time, most of the computations were made on boards with tally-marks or jetons. So, unlike how it may be seen by the modern reader, his goal was not to use moves of counters on a board to multiply, divide and find square roots, but rather to find a way to compute symbolically.\n\nHowever, when reproduced on the board, this new technique did not require mental trial-and-error computations nor complex carry memorization (unlike base 10 computations). He was so pleased by his discovery that he said in his preface\n\nBinary notation had not yet been standardized, so Napier used what he called location numerals to represent binary numbers. Napier's system uses sign-value notation to represent numbers; it uses successive letters from the Latin alphabet to represent successive powers of two: a = 2 = 1, b = 2 = 2, c = 2 = 4, d = 2 = 8, e = 2 = 16 and so on.\n\nTo represent a given number as a location numeral, that number is expressed as a sum of powers of two and then each power of two is replaced by its corresponding digit (letter). For example, when converting from a decimal numeral:\n\nUsing the reverse process, a location numeral can be converted to another numeral system. For example, when converting to a decimal numeral:\n\nNapier showed multiple methods of converting numbers in and out of his numeral system. These methods are similar to modern methods of converting numbers in and out of the binary numeral system, so they are not shown here. Napier also showed how to add, subtract, multiply, divide, and extract square roots.\n\nAs in any numeral system using sign-value notation (but \"not\" those using positional notation), digits (letters) can be repeated such that multiple numerals can represent a single number. For example:\n\nAdditionally, the order of digits does not matter. For example:\n\nBecause each digit in a location numeral represents twice the value of its next-lower digit, replacing any two occurrences of the same digit with one of the next-higher digit does not change the numeral's numeric value. Thus, repeatedly applying the rules of replacement aa → b, bb → c, cc → d, etc. to a location numeral removes all repeated digits from that numeral.\n\nNapier called this process abbreviation and the resulting location numeral the abbreviated form of that numeral; he called location numerals containing repeated digits extended forms. Each number can be represented by a unique abbreviated form, not considering the order of its digits (e.g., abc, bca, cba, etc. all represent the number 7).\n\nLocation numerals allow for a simple and intuitive algorithm for addition:\n\n\nFor example, to add 157 = acdeh and 230 = bcfgh, join the numerals end-to-end:\n\nrearrange the digits of the previous result (because the digits of acdehbcfgh are not in ascending order):\n\nand abbreviate the previous result:\n\nThe final result, abhi, equals 387 (abhi = 2 + 2 + 2 + 2 = 1 + 2 + 128 + 256 = 387); this is the same result achieved by adding 157 and 230 in decimal notation.\n\nSubtraction is also intuitive, but may require expanding abbreviated forms to extended forms to perform borrows.\n\nWrite the minuend (the largest number you want to diminish) and remove from it all the digits appearing in the subtrahend (the smallest number). In case the digit to be removed does not appear in the minuend, then \"borrow\" it by expanding the unit just larger. Repeat until all the digit of the subtrahend have been removed.\n\nA few examples show it is simpler than it sounds :\n\n\n\nNapier proceeded to the rest of arithmetic, that is multiplication, division and square root, on an abacus, as it was common in his times. However, since the development of micro-processor computer, a lot of applicable algorithms have been developed or revived based on doubling and halving.\n\nDoubling is done by adding a numeral to itself, which mean doubling each of its digit. This gives an extend form, which has to be abbreviated if needed.\n\nThis operation can also be done in one go by changing all the digit of a numeral by the next digit. For example, the double of a is b, the double of b is c, the double of ab is bc, the double of acfg is bdgh, etc...\n\nSimilarly, multiplying by a power of two, is just translating its digits. To multiply by c = 4, for example, is transforming the digits a → c, b → d, c → e... \nHalving is the reverse: change all the digit by the previous digit. For example, the half of bdgh is acfg.\n\nOne sees immediately that it is only feasible when the numeral to be halved does not contains an a (or, if the numeral is extended, an odd number of as). In other words, an abbreviated numeral is odd if it contains an a and even if it does not.\n\nWith these basic operations (doubling and halving), we can adapt all the binary algorithms starting by, but not limited to, the Bisection method and Dichotomic search.\n\nNapier proceeded to multiplication and division on an abacus, as it was common in his times. However, the Egyptian multiplication gives an elegant way to carry multiplication without tables using only doubling, halving and adding.\n\nMultiplying a single digit number by another single digit number is a simple process. Because all letters represent a power of 2, multiplying digits is the same as adding their exponents. This can also be thought of as finding the index of one digit in the alphabet (a = 0, b = 1, ...) and incrementing the other digit by that amount in terms of the alphabet (b + 2 => d).\n\nFor example, multiply 4 = c by 16 = e\n\nc * e = 2^2 * 2^4 = 2^6 = g\n\nor... \n\n\"AlphabetIndex\"(c) = 2, so... e => f => g\n\nTo find the product of two multiple digit numbers, make a two column table. In the left column write the digits of the first number, one below the other. For each digit in the left column, multiply that digit and the second number and record it in the right column. Finally, add all the numbers of the right column together.\n\nAs an example, multiply 238 = bcdfgh by 13 = acd\n\nThe result is the sum in the right column ' = ' = bcekl = 2+4+16+1024+2048 = 3094.\n\nIt is interesting to notice that the left column can also be obtained by successive halves of the first number, from which the even numbers are removed. In our example, acd, bc (even), ab, a. Noticing that the right column contains successive doubles of the second number, shows why the peasant multiplication is exact.\n\nDivision can be carried out by successive subtractions: the quotient is the number of time the divisor can be subtracted from the dividend, and the remainder is what is left rest after all the possible subtractions.\n\nThis process, which can be very long, may be made efficient if instead of the divisor we subtract multiple of the divisor, and computations are easier if we restrict to multiple by a power of 2.\n\nIn facts, this is what we do in the long division method.\n\nLocation arithmetic uses a square grid where each square on the grid represents a value. Two sides of the grid are marked with\nincreasing powers of two. Any inner square can be identified by two numbers on these two sides, one being vertically below the inner\nsquare and the other to its far right. The value of the square is the product of these two numbers.\n\nFor instance, the square in this example grid represents 32, as it is the product of 4 on the right column and 8 from the bottom row. The grid itself can be any size, and larger grids simply permit us to handle larger numbers.\n\nNotice that moving either one square to the left or one square up doubles the value. This property can be used to perform binary\naddition using just a single row of the grid.\n\nFirst, lay out a binary number on a row using counters to represent the 1s in the number. For example, 29 (= 11101 in binary) would be placed on the board like this:\n\nThe number 29 is clearly the sum of the values of the squares on which there are counters. Now overlay the second number on this row. Say we place 9 (= 1001 in binary) on it like this.\n\nThe sum of these two numbers is just the total value represented by the counters on the board, but some of the squares have more than one counter. Recall however, that moving to the left of a square doubles its value. So we replace two counters on a square with one counter to its left without changing the total value on the board. Note that this is the same idea used to abbreviate\nlocation numerals. Let's start by replacing the rightmost pair of counters with a counter to its left, giving:\n\nWe still have another square with two counters on it, so we do it again:\n\nBut replacing this pair created another square with two counters on it, so we replace a third time:\n\nNow each square has just one counter, and reading off the result in binary 100110 (= 38) gives the correct result.\n\nSubtracting is not much more complicated than addition: instead of adding counters on the board we remove them. To \"borrow\" a value, we replace a counter on a square with two to its right.\n\nLet's see how we might subtract 12 from 38. First place 38 (= 100110 in binary) on a row, and then place 12 (= 1100 in binary) under it:\n\nFor every counter on the lower row that has a counter above it, remove both counters. We can remove one such pair on the board,\nresulting in:\n\nNow we need to \"borrow\" counters to get rid of the remaining counter on the bottom. First replace the leftmost counter on the top row with two to its right:\n\nNow replace one of the two counters with two more to its right, giving:\n\nWe can now take away one of the counters on the top row with the remaining counter on the bottom row:\n\nand read off 26, the final result.\n\nUnlike addition and subtraction, the entire grid is used to multiply, divide, and extract square roots. The grid has some useful properties utilized in these operations. First, all the squares on any diagonal going from the bottom left to the top right have the same value.\n\nSince a diagonal move can be broken down into a move to the right (which halves the value) followed by a move\nup (which doubles the value), the value of the square stays the same.\n\nIn conjunction with that diagonal property, there's a quick way to divide the numbers on the bottom and right edges of the grid.\n\nLocate the dividend 32 along the right side and the divisor 8 on the bottom edge of the grid. Extend a diagonal from the dividend and locate the square where it intersects a vertical line from the divisor. The quotient lies at the right end of the grid from this square, which for our example is 4.\n\nWhy does this work? Moving along the diagonal doesn't change the value; the value of the square on the intersection\nis still the dividend. But we also know it is the product of the squares along the bottom and right edge. Since the square on the bottom edge is the divisor, the square on the right edge is the quotient.\n\nNapier extends this idea to divide two arbitrary numbers, as shown below.\n\nTo multiply a pair of binary numbers, first mark the two numbers\non the bottom and the right side of the grid. Say we want to\nmultiply 22 (= 10110) by 9 (= 1001).\n\nNow place counters at every \"intersection\" of vertical and\nhorizontal rows of the 1s in each number.\n\nNotice that each row of counters on the grid is just\n22 multiplied by some\npower of two. In fact, the total value of the counters is the\nsum of two rows\nSo the counters on the board actually represent the product\nof the two numbers, except it isn't possible to \"read off\" the\nanswer just yet.\n\nRecall that moving counters diagonally doesn't change the value,\nso move all the counters on inner squares diagonally until they\nhit either the bottom row or the left column.\n\nNow we make the same moves we did for addition. Replace\ntwo counters on a square with one to its left. If the square\nis on the left column, replace two counters with one \"above\"\nit. Recall that the value of a square doubles if you move up,\nso this doesn't change the value on the grid.\n\nLet's first replace the two counters on the second square\nat the bottom with one to its left which leaves two\ncounters at the corner.\n\nFinally, replace the two counters on the corner with one above it\nand \"read off\" the binary number in an L-shaped fashion, starting from\nthe top left down to the bottom left corner, and then over to the\nbottom right.\n\nRead the counters along the L but don't double count the corner square.\nYou will read the binary result 11000110 = 198 which is indeed 22*9.\n\nWhy can we read the binary number in this L-shaped fashion? The\nbottom row is of course just the first six powers of two, but\nnotice that the leftmost column has the next five powers of\ntwo. So we can directly read off an 11 digit binary number from\nthe L-shaped set of 11 squares that lie along the left and bottom\nsides of the grid.\n\nOur small 6x6 grid can only multiply numbers each up to 63, and in\ngeneral an \"n\"x\"n\" grid can multiply two numbers each up to\n2-1. This scales very fast, so board with 20 numbers per side, for\ninstance, can multiply numbers each up to a little over one million.\n\nMartin Gardner presented a slightly easier to understand\nversion of Napier's division method, which is what is\nshown here.\n\nDivision works pretty much the reverse of multiplication. Say we want\nto divide 485 by 13. First place counters for 485 (= 111100101) along\nthe bottom edge and mark 13 (= 1101) along the right edge. To save\nspace, we'll just look at a rectangular portion of the board because\nthat's all we actually use.\n\nStarting from the left, the game is to move counters diagonally into\n\"columns of divisors\" (that is, with one counter on each row marked\nwith a 1 from the divisor.) Let's demonstrate this with the leftmost\nblock of counters.\n\nNow the next block of counters we might try would begin with the\nleftmost counter on the bottom, and we might attempt something like\n\nexcept that we don't have any counters that we can move diagonally\nfrom the bottom edge into squares that would form the rest of the\n\"column of divisors.\"\n\nIn such cases, we instead \"double down\" the counter on the bottom\nrow and form a column one over to the right. As you will soon see, it\nwill always be possible to form a column this way. So first replace\nthe counter on the bottom with two to its right.\n\nand then move one diagonally to the top of the column, and move\nanother counter located on the edge of the board into its spot.\n\nIt looks like we still don't have a counter on the bottom edge to move\ndiagonally into the remaining square, but notice that we can instead\ndouble down the leftmost counter again and then move it into the\ndesired square.\n\nand now move one counter diagonally to where we want it.\n\nLet's proceed to build the next column. Once again, notice that moving\nthe leftmost counter to the top of the column doesn't leave enough\ncounters at the bottom to fill in the remaining squares.\n\nSo we double down the counter and move one diagonally into the next\ncolumn over. Let's also move the rightmost counter into the column,\nand here's how it looks after these steps.\n\nWe still have a missing square, but we just double down again and move\nthe counter into this spot and end up with\n\nAt this point, the counter on the bottom edge is so far to the right\nthat it cannot go diagonally to the top of any column, which signals\nthat we are done.\n\nThe result is \"read\" off the columns—each column with counters is\ntreated as a 1 and empty columns are 0. So the result is\n100101 (= 37) and the remainder is the binary value of any counters\nstill left along the bottom edge. There is one counter on the third\ncolumn from the right, so we read it as 100 (= 4) and we get 485\n÷ 13 = 37 with a remainder 4.\n\nThis process requires one to add counters to the abacus (board) to make square figures. The top of page 149 \nshows diagrams that explain this process. Begin by placing a single counter on the board (it will actually go on \none of the dotted squares). Adding three other counters adjacent (or with blank rows and columns between them \nand the first one placed) will result in another square figure on the abacus. Similarly adding another five counters \nto this (with or without the blank rows and columns shown) will result in an even bigger square.\nTake the number to be considered and put counters along one margin that represent its value.\nFrom the position of the largest counter in that value, follow the diagonal lines (bishop’s moves) across the \nboard until you come to a square with a dot. Place a counter on that square. \nSubtract the value represented by this single counter from the original number in the margin.\nAdd three (five, seven, ... for subsequent steps) to create a square on the board and subtract the value of the \nadded counters from the number in the margin until the number is either too large to be subtracted or there is no \nspace left on the board. You should be left with a large square of counters (perhaps with blank rows and columns \nbetween them) on the board.\nMove one of the counters in each row of the square to the margin and the positions of these marginal counters \nwill yield the square root of the number.\nNapier provides an example of determining the square root of 1238.\nThe largest counter is in the 1024 position so the first counter is placed on the dot found by moving down the 1024 \ndiagonal (at the 32,32 position). Subtracting this value (1024) from the original number leaves counters at 128, \n64, 16, 4 and 2 (= 214). \nPlacing three counters on the board to form a square with the first counter but whose value can still be subtracted \nfrom 214, results in counters at positions 32,2; 2,2; and 2,32 (whose values are 64, 4 and 64, which when subtracted \nfrom the remainder of 214 = 82.\nThe next square that can be constructed from five counters, yet the values of those five counters still being capable \nof being subtracted from 82 results in counters in positions 32,1; 2,1; 1,1; 1.2; and 1,32. The values of these five \ncounters total 69 which when subtracted from 82 leave 13 as a remainder.\nAs there is no more room on the board we have to stop.\nMove one counter from each row to the margin (rows 32, 2 and 1) and this value (35) is the square root required, \nor at least the integer part of it (the actual value is 35.1852...).\nNapier provides a second example for calculating the square root of 2209 (= 47).\n\n\n"}
{"id": "277184", "url": "https://en.wikipedia.org/wiki?curid=277184", "title": "Mathematical notation", "text": "Mathematical notation\n\nMathematical notation is a system of symbolic representations of mathematical objects and ideas. Mathematical notations are used in mathematics, the physical sciences, engineering, and economics. Mathematical notations include relatively simple symbolic representations, such as the numbers 0, 1 and 2; function symbols such as sin; operator symbols such as \"+\"; conceptual symbols such as lim and \"dy/dx\"; equations and variables; and complex diagrammatic notations such as Penrose graphical notation and Coxeter–Dynkin diagrams.\n\nA mathematical notation is a writing system used for recording concepts in mathematics.\n\nThe media used for writing are recounted below, but common materials currently include paper and pencil, board and chalk (or dry-erase marker), and electronic media. Systematic adherence to mathematical concepts is a fundamental concept of mathematical notation. (See also some related concepts: Logical argument, Mathematical logic, and Model theory.)\n\nA mathematical expression is a \"sequence\" of symbols which can be evaluated. For example, if the symbols represent numbers, the expressions are evaluated according to a conventional order of operations which provides for calculation, if possible, of any expressions within parentheses, followed by any exponents and roots, then multiplications and divisions and finally any additions or subtractions, all done from left to right. In a computer language, these rules are implemented by the compilers. For more on expression evaluation, see the computer science topics: eager evaluation, lazy evaluation, and evaluation operator.\n\nModern mathematics needs to be precise, because ambiguous notations do not allow formal proofs. Suppose that we have statements, denoted by some formal sequence of symbols, about some objects (for example, numbers, shapes, patterns). Until the statements can be shown to be valid, their meaning is not yet resolved. While reasoning, we might let the symbols refer to those denoted objects, perhaps in a model. The semantics of that object has a heuristic side and a deductive side. In either case, we might want to know the properties of that object, which we might then list in an intensional definition.\n\nThose properties might then be expressed by some well-known and agreed-upon symbols from a table of mathematical symbols. This mathematical notation might include annotation such as\n\nIn different contexts, the same symbol or notation can be used to represent different concepts. Therefore, to fully understand a piece of mathematical writing, it is important to first check the definitions that an author gives for the notations that are being used. This may be problematic if the author assumes the reader is already familiar with the notation in use.\n\nIt is believed that a mathematical notation to represent counting was first developed at least 50,000 years ago — early mathematical ideas such as finger counting have also been represented by collections of rocks, sticks, bone, clay, stone, wood carvings, and knotted ropes. The tally stick is a way of counting dating back to the Upper Paleolithic. Perhaps the oldest known mathematical texts are those of ancient Sumer. The Census Quipu of the Andes and the Ishango Bone from Africa both used the tally mark method of accounting for numerical concepts.\n\nThe development of zero as a number is one of the most important developments in early mathematics. It was used as a placeholder by the Babylonians and Greek Egyptians, and then as an integer by the Mayans, Indians and Arabs. (See The history of zero for more information.)\n\nThe earliest mathematical viewpoints in geometry did not lend themselves well to counting. The natural numbers, their relationship to fractions, and the identification of continuous quantities actually took millennia to take form, and even longer to allow for the development of notation. It was not until the invention of analytic geometry by René Descartes that geometry became more subject to a numerical notation. Some symbolic shortcuts for mathematical concepts came to be used in the publication of geometric proofs. Moreover, the power and authority of geometry's theorem and proof structure greatly influenced non-geometric treatises, Isaac Newton's Principia Mathematica, for example.\n\nThe 18th and 19th centuries saw the creation and standardization of mathematical notation as used today. Euler was responsible for many of the notations in use today: the use of \"a\", \"b\", \"c\" for constants and \"x\", \"y\", \"z\" for unknowns, \"e\" for the base of the natural logarithm, sigma (Σ) for summation, \"i\" for the imaginary unit, and the functional notation \"f\"(\"x\"). He also popularized the use of π for Archimedes constant (due to William Jones' proposal for the use of π in this way based on the earlier notation of William Oughtred). Many fields of mathematics bear the imprint of their creators for notation: the differential operator is due to Leibniz, the cardinal infinities to Georg Cantor (in addition to the lemniscate (∞) of John Wallis), the congruence symbol (≡) to Gauss, and so forth.\n\nMathematically oriented markup languages such as TeX, LaTeX and, more recently, MathML are powerful enough to express a wide variety of mathematical notations.\n\nTheorem-proving software naturally comes with its own notations for mathematics; the OMDoc project seeks to provide an open commons for such notations; and the MMT language provides a basis for interoperability between other notations.\n\nModern Arabic mathematical notation is based mostly on the Arabic alphabet and is used widely in the Arab world, especially in pre-tertiary education. (Western notation uses Arabic numerals, but the Arabic notation also replaces Latin letters and related symbols with Arabic script.)\n\nSome mathematical notations are mostly diagrammatic, and so are almost entirely script independent. Examples are Penrose graphical notation and Coxeter–Dynkin diagrams.\n\nBraille-based mathematical notations used by blind people include Nemeth Braille and GS8 Braille.\n\n\n\n"}
{"id": "5583296", "url": "https://en.wikipedia.org/wiki?curid=5583296", "title": "Metabolic control analysis", "text": "Metabolic control analysis\n\nMetabolic control analysis (MCA) is a mathematical framework for describing\nmetabolic, signaling, and genetic pathways. MCA quantifies how variables,\nsuch as fluxes and species concentrations, depend on network parameters.\nIn particular it is able to describe how network dependent properties,\ncalled control coefficients, depend on local properties called elasticities.\n\nMCA was originally developed to describe the control in metabolic pathways\nbut was subsequently extended to describe signaling and genetic networks. MCA has sometimes also been referred to as \"Metabolic Control Theory\" but this terminology was rather strongly opposed by Henrik Kacser, one of the founders.\n\nMore recent work has shown that MCA can be mapped directly on to classical control theory and are as such equivalent.\n\nBiochemical systems theory is a similar formalism, though with a rather different objectives. Both are evolutions of an earlier theoretical analysis by Joseph Higgins.\n\nA control coefficient measures the relative steady state change in a system variable, e.g. pathway flux (J) or metabolite concentration (S), in response to a relative change in a parameter, e.g. enzyme activity or the steady-state rate (formula_1) of step i. The two main control coefficients are the flux and concentration control coefficients. Flux control coefficients are defined by:\n\nformula_2\n\nand concentration control coefficients by:\n\nformula_3\n\nThe flux control summation theorem was discovered independently by the Kacser/Burns group and the Heinrich/Rapoport group in the early 1970s and late 1960s. The flux control summation theorem implies that metabolic fluxes are systemic properties and that their control is shared by all reactions in the system. When a single reaction changes its control of the flux this is compensated by changes in the control of the same flux by all other reactions.\n\nformula_4\n\nformula_5\n\nThe elasticity coefficient measures the local response of an enzyme or other chemical reaction to changes in its environment. Such changes include factors such as substrates, products or effector concentrations. For further information please refer to the dedicated page at Elasticity Coefficients.\n\nThe connectivity theorems are specific relationships between elasticities and control coefficients. They are useful because they highlight the close relationship between the kinetic properties of individual reactions and the system properties of a pathway. Two basic sets of theorems exists, one for flux and another for concentrations. The concentration connectivity theorems are divided again depending on whether the system species formula_6 is different from the local species formula_7.\n\nformula_8\n\nformula_9\n\nformula_10\n\nIt is possible to combine the summation with the connectivity theorems to obtain closed expressions that relate the control coefficients to the elasticity coefficients. For example, consider the simplest non-trivial pathway:\n\nWe assume that formula_12 and formula_13 are fixed boundary species so that the pathway can reach a steady state. Let the first step have a rate formula_14 and the second step formula_15. Focusing on the flux control coefficients, we can write one summation and one connectivity theorem for this simple pathway:\nformula_16\n\nformula_17\nUsing these two equations we can solve for the flux control coefficients to yield:\nformula_18\n\nformula_19\nUsing these equations we can look at some simple extreme behaviors. For example, let us assume that the first step is completely insensitive to its product (i.e. not reacting with it), S, then formula_20. In this case, the control coefficients reduce to:\nformula_21\n\nformula_22\nThat is all the control (or sensitivity) is on the first step. This situation represents the classic rate-limiting step that is frequently mentioned in text books. The flux through the pathway is completely dependent on the first step. Under these conditions, no other step in the pathway can affect the flux. The effect is however dependent on the complete insensitivity of the first step to its product. Such a situation is likely to be rare in real pathways. In fact the classic rate limiting step has almost never been observed experimentally. Instead, a range of limitingness is observed, with some steps having more limitingness (control) than others.\n\nWe can also derive the concentration control coefficients for the simple two step pathway:\nformula_23\n\nformula_24\n\nConsider the simple three step pathway:\n\nwhere formula_12 and formula_13 are fixed boundary species, the control equations for this pathway can be derived in a similar manner to the simple two step pathway although it is somewhat more tedious.\n\nformula_28\n\n<br>\nformula_29\n\n<br>\nformula_30\n\n<br>\nwhere D the denominator is given by:\n\n<br>\nformula_31\n\nNote that every term in the numerator appears in the denominator, this ensures that the flux control coefficient summation theorem is satisfied.\n\nLikewise the concentration control coefficients can also be derived, for formula_32\n\nformula_33\n\n<br>\nformula_34\n\n<br>\nformula_35\n\n<br>\nAnd for formula_36\n\n<br>\nformula_37\n\n<br>\nformula_38\n\n<br>\nformula_39\n\nNote that the denominators remain the same as before and behave as a normalizing factor.\n\nControl equations can also be derived by considering the effect of perturbations on the system. Consider that reaction rates formula_40 and formula_41 are determined by two enzymes formula_42 and formula_43 respectively. Changing either enzyme will result in a change to the steady state level of formula_44 and the steady state reaction rates formula_45. Consider a small change in formula_42 of magnitude formula_47. This will have a number of effects, it will increase formula_40 which in turn will increase formula_44 which in turn will increase formula_41. Eventually the system will settle to a new steady state. We can describe these changes by focusing on the change in formula_40 and formula_41. The change in formula_41, which we designate formula_54, came about as a result of the change formula_55. Because we are only considering small changes we can express the change formula_54 in terms of formula_55 using the relation:\n\nformula_58\n\nwhere the derivative formula_59 measures how responsive formula_41 is to changes in formula_44. The derivative can be computed if we know the rate law for formula_41. For example, if we assume that the rate law is formula_63 then the derivative is formula_64. We can also use a similar strategy to compute the change in formula_40 as a result of the change formula_47. This time the change in formula_40 is a result of two changes, the change in formula_42 itself and the change in formula_44. We can express these changes by summing the two individual contributions:\n\nformula_70\n\nWe have two equations, one describing the change in formula_40 and the other in formula_41. Because we allowed the system to settle to a new steady state we can also state that the change in reaction rates must be the same (otherwise it wouldn't be at steady state). That is we can assert that formula_73. With this in mind we equate the two equations and write:\n\nformula_74\n\nSolving for the ratio formula_75 we obtain:\n\nformula_76\n\nIn the limit, as we make the change formula_47 smaller and smaller, the left-hand side converges to the derivative formula_78:\n\nformula_79\n\nWe can go one step further and scale the derivatives to eliminate units. Multiplying both sides by formula_42 and dividing both sides by $x$ yields the scaled derivatives:\n\nformula_81\n\nThe scaled derivatives on the right-hand side are the elasticities, formula_82 and the scaled left-hand term is the scaled sensitivity coefficient or concentration control coefficient, formula_83\n\nformula_84\n\nWe can simplify this expression further. The reaction rate formula_40 is usually a linear function of formula_42. For example, in the Briggs-Haldane equation, the reaction rate is given by formula_87. Differentiating this rate law with respect to formula_42 and scaling yields: formula_89.\n\nUsing this result gives:\n\nformula_90\n\nA similar analysis can be done where formula_43 is perturbed. In this case we obtain the sensitivity of formula_44 with respect to formula_43:\n\nformula_94\n\nThe above expressions measure how much enzymes formula_42 and formula_43 control the steady state concentration of intermediate formula_44. We can also consider how the steady state reaction rates formula_40 and formula_41 are affected by perturbations in formula_42 and formula_43. This is often of importance to metabolic engineers who are interested in increasing rates of production. At steady state the reaction rates are often called the fluxes and abbreviated to formula_102 and formula_103. For a linear pathway such this example, both fluxes are equal at steady state so that the flux through the pathway is simply referred to as formula_104. Expressing the change in flux as a result of a perturbations in formula_42 and taking the limit as before we obtain:\nformula_106\n\nThe above expressions tell us how much enzymes formula_42 and formula_43 control the steady state flux. The key point here is that changes in enzyme concentration, or equivalently the enzyme activity, must be brought about by an external action.\n\n"}
{"id": "5744114", "url": "https://en.wikipedia.org/wiki?curid=5744114", "title": "Method of lines", "text": "Method of lines\n\nThe method of lines (MOL, NMOL, NUMOL) is a technique for solving partial differential equations (PDEs) in which all but one dimension is discretized. MOL allows standard, general-purpose methods and software, developed for the numerical integration of ODEs and DAEs, to be used. A large number of integration routines have been developed over the years in many different programming languages, and some have been published as open source resources.\n\nThe method of lines most often refers to the construction or analysis of numerical methods for partial differential equations that proceeds by first discretizing the spatial derivatives only and leaving the time variable continuous. This leads to a system of ordinary differential equations to which a numerical method for initial value ordinary equations can be applied. The method of lines in this context dates back to at least the early 1960s. Many papers discussing the accuracy and stability of the method of lines for various types of partial differential equations have appeared since.\n\nMOL requires that the PDE problem is well-posed as an initial value (Cauchy) problem in at least one dimension, because ODE and DAE integrators are initial value problem (IVP) solvers. Thus it cannot be used directly on purely elliptic partial differential equations, such as Laplace's equation. However, MOL has been used to solve Laplace's equation by using the \"method of false transients\". In this method, a time derivative of the dependent variable is added to Laplace’s equation. Finite differences are then used to approximate the spatial derivatives, and the resulting system of equations is solved by MOL. It is also possible to solve elliptical problems by a \"semi-analytical method of lines\". In this method, the discretization process results in a set of ODE's that are solved by exploiting properties of the associated exponential matrix.\n\nRecently, to overcome the stability issues associated with the method of false transients, a perturbation approach was proposed which was found to be more robust than standard method of false transients for a wide range of elliptic PDEs.\n\n"}
{"id": "2814021", "url": "https://en.wikipedia.org/wiki?curid=2814021", "title": "Mid-range", "text": "Mid-range\n\nIn statistics, the mid-range or mid-extreme of a set of statistical data values is the arithmetic mean of the maximum and minimum values in a data set, defined as:\n\nThe mid-range is the midpoint of the range; as such, it is a measure of central tendency.\n\nThe mid-range is rarely used in practical statistical analysis, as it lacks efficiency as an estimator for most distributions of interest, because it ignores all intermediate points, and lacks robustness, as outliers change it significantly. Indeed, it is one of the least efficient and least robust statistics. However, it finds some use in special cases: it is the maximally efficient estimator for the center of a uniform distribution, trimmed mid-ranges address robustness, and as an L-estimator, it is simple to understand and compute.\n\nThe midrange is highly sensitive to outliers and ignores all but two data points. It is therefore a very non-robust statistic, having a breakdown point of 0, meaning that a single observation can change it arbitrarily. Further, it is highly influenced by outliers: increasing the sample maximum or decreasing the sample minimum by \"x\" changes the mid-range by formula_2 while it changes the sample mean, which also has breakdown point of 0, by only formula_3 It is thus of little use in practical statistics, unless outliers are already handled.\n\nA trimmed midrange is known as a – the \"n\"% trimmed midrange is the average of the \"n\"% and (100−\"n\")% percentiles, and is more robust, having a breakdown point of \"n\"%. In the middle of these is the midhinge, which is the 25% midsummary. The median can be interpreted as the fully trimmed (50%) mid-range; this accords with the convention that the median of an even number of points is the mean of the two middle points.\n\nThese trimmed midranges are also of interest as descriptive statistics or as L-estimators of central location or skewness: differences of midsummaries, such as midhinge minus the median, give measures of skewness at different points in the tail.\n\nDespite its drawbacks, in some cases it is useful: the midrange is a highly efficient estimator of μ, given a small sample of a sufficiently platykurtic distribution, but it is inefficient for mesokurtic distributions, such as the normal.\n\nFor example, for a continuous uniform distribution with unknown maximum and minimum, the mid-range is the UMVU estimator for the mean. The sample maximum and sample minimum, together with sample size, are a sufficient statistic for the population maximum and minimum – the distribution of other samples, conditional on a given maximum and minimum, is just the uniform distribution between the maximum and minimum and thus add no information. See German tank problem for further discussion. Thus the mid-range, which is an unbiased and sufficient estimator of the population mean, is in fact the UMVU: using the sample mean just adds noise based on the uninformative distribution of points within this range.\n\nConversely, for the normal distribution, the sample mean is the UMVU estimator of the mean. Thus for platykurtic distributions, which can often be thought of as between a uniform distribution and a normal distribution, the informativeness of the middle sample points versus the extrema values varies from \"equal\" for normal to \"uninformative\" for uniform, and for different distributions, one or the other (or some combination thereof) may be most efficient. A robust analog is the trimean, which averages the midhinge (25% trimmed mid-range) and median.\n\nFor small sample sizes (\"n\" from 4 to 20) drawn from a sufficiently platykurtic distribution (negative excess kurtosis, defined as γ = (μ/(μ)²) − 3), the mid-range is an efficient estimator of the mean \"μ\". The following table summarizes empirical data comparing three estimators of the mean for distributions of varied kurtosis; the modified mean is the truncated mean, where the maximum and minimum are eliminated.\n\nFor \"n\" = 1 or 2, the midrange and the mean are equal (and coincide with the median), and are most efficient for all distributions. For \"n\" = 3, the modified mean is the median, and instead the mean is the most efficient measure of central tendency for values of \"γ\" from 2.0 to 6.0 as well as from −0.8 to 2.0.\n\nFor a sample of size \"n\" from the standard normal distribution, the mid-range \"M\" is unbiased, and has a variance given by:\n\nFor a sample of size \"n\" from the standard Laplace distribution, the mid-range \"M\" is unbiased, and has a variance given by:\nand, in particular, the variance does not decrease to zero as the sample size grows.\n\nFor a sample of size \"n\" from a zero-centred uniform distribution, the mid-range \"M\" is unbiased, \"nM\" has an asymptotic distribution which is a Laplace distribution.\n\nWhile the mean of a set of values minimizes the sum of squares of deviations and the median minimizes the average absolute deviation, the midrange minimizes the maximum deviation (defined as formula_6): it is a solution to a variational problem.\n\n\n"}
{"id": "14461309", "url": "https://en.wikipedia.org/wiki?curid=14461309", "title": "Nitrogen-vacancy center", "text": "Nitrogen-vacancy center\n\nThe nitrogen-vacancy center (N-V center) is one of numerous point defects in diamond. Its most explored and useful property is photoluminescence, which can be easily detected from an individual N-V center, especially those in the negative charge state (N-V). Electron spins at N-V centers, localized at atomic scales, can be manipulated at room temperature by applying a magnetic field, electric field, microwave radiation or light, or a combination, resulting in sharp resonances in the intensity and wavelength of the photoluminescence. These resonances can be explained in terms of electron spin related phenomena such as quantum entanglement, spin-orbit interaction and Rabi oscillations, and analysed using advanced quantum optics theory. An individual N-V center can be viewed as a basic unit of a quantum computer, and it has potential applications in novel, more efficient fields of electronics and computational science including quantum cryptography, spintronics, steadystomics and masers.\n\nThe nitrogen-vacancy center is a point defect in the diamond lattice. It consists of a nearest-neighbor pair of a nitrogen atom, which substitutes for a carbon atom, and a lattice vacancy.\nTwo charge states of this defect, neutral N-V and negative N-V, are known from spectroscopic studies using optical absorption, photoluminescence (PL), electron paramagnetic resonance (EPR) and optically detected magnetic resonance (ODMR), which can be viewed as a hybrid of PL and EPR; most details of the structure originate from EPR. A nitrogen atom has five valence electrons. Three of them covalently bond to the carbon atoms and two remain non-bonded and are called a lone pair. The vacancy has three unpaired electrons. Two of them make a quasi covalent bond and one remains unpaired. The overall symmetry, however, is axial (trigonal C); one can visualize this by imagining the three unpaired vacancy electrons continuously exchanging their roles.\n\nThe N-V thus has one unpaired electron and is paramagnetic. However, despite extensive efforts, electron paramagnetic resonance signals from N-V avoided detection for decades until 2008. Optical excitation is required to bring the N-V defect into the EPR-detectable excited state; the signals from the ground state are presumably too broad for EPR detection.\n\nThe N-V centers can be converted into N-V by changing the Fermi level position. This can be achieved by applying external voltage to a p-n junction made from doped diamond, e.g., in a Schottky diode.\n\nIn the negative charge state N-V, an extra electron is located at the vacancy site forming a spin S=1 pair with one of the vacancy electrons. As in N-V, the vacancy electrons are \"exchanging roles\" preserving the overall trigonal symmetry. This N-V state is what is commonly, and somewhat incorrectly, called \"the nitrogen-vacancy center\". The neutral state has not yet been explored for spin manipulations.\n\nThe N-V centers are randomly oriented within a diamond crystal. Ion implantation techniques can enable their artificial creation in predetermined positions.\n\nNitrogen-vacancy centers are typically produced from single substitutional nitrogen centers (called C or P1 centers in diamond literature) by irradiation followed by annealing at temperatures above 700 °C. A wide range of high-energy particles are suitable for such irradiation, including electrons, protons, neutrons, ions, and gamma photons. Irradiation produces lattice vacancies, which are a part of N-V centers. Those vacancies are immobile at room temperature, and annealing is required to move them. Single substitutional nitrogen produces strain in the diamond lattice; it therefore efficiently captures moving vacancies, producing the N-V centers.\n\nDuring chemical vapor deposition of diamond, a small fraction of single substitutional nitrogen impurity (typically <0.5%) traps vacancies generated as a result of the plasma synthesis. Such nitrogen-vacancy centers are preferentially aligned to the growth direction.\n\nDiamond is notorious for having a relatively large lattice strain. Strain splits and shifts optical transitions from individual centers resulting in broad lines in the ensembles of centers. Special care is taken to produce extremely sharp N-V lines (line width ~10 MHz) required for most experiments: high-quality, pure natural or better synthetic diamonds (type IIa) are selected. Many of them already have sufficient concentrations of grown-in N-V centers and are suitable for applications. If not, they are irradiated by high-energy particles and annealed. Selection of a certain irradiation dose allows tuning the concentration of produced N-V centers such that individual N-V centers are separated by micrometre-large distances. Then, individual N-V centers can be studied with standard optical microscopes or, better, near-field scanning optical microscopes having sub-micrometre resolution.\n\nN-V centers emit bright red light which can be conveniently excited by visible light sources, such as argon or krypton lasers, frequency doubled s, dye lasers, or He-Ne lasers. Excitation can also be achieved at energies below that of zero phonon emission.\nLaser illumination, however, also converts some N-V into N-V centers. Emission is very quick (relaxation time ~10 ns). At room temperature, no sharp peaks are observed because of the thermal broadening. However, cooling the N-V centers with liquid nitrogen or liquid helium dramatically narrows the lines down to a width of a few megahertz.\n\nAn important property of the luminescence from individual N-V centers is its high temporal stability. Whereas many single-molecular emitters bleach after emission of 10–10 photons, no bleaching is observed for the N-V centers at room temperature.\n\nBecause of these properties, the ideal technique to address the N-V centers is confocal microscopy, both at room temperature and at low temperature. In particular, low temperature operation is required to specifically address only the zero-phonon line (ZPL).\n\nThe energy level structure of the N-V center was established by combining optical, electron paramagnetic resonance and theoretical results, as shown in the figure. In particular, several theoretical works have been done, using the Linear Combination of Atomic Orbitals (LCAO) approach, to build the electronic orbitals to describe the possible quantum states, looking at the N-V center as a molecule. Moreover, group theory results are used, to take into account the symmetry of the diamond crystal, and so the symmetry of the N-V itself. The energy levels are labeled according to the group theory, and in particular are labelled after the irreducible representations of the C symmetry group of the defect center, \"A, A\" and \"E\".\nThe numbers 3 in A and 1 in A represent the number of allowable \"m\" spin states, or the spin multiplicity, which range from –\"S\" to \"S\" for a total of 2\"S\"+1 possible states. If \"S\" = 1, \"m\" can be −1, 0, or 1. The A level is predicted by theory but not directly observed in experiment, and it is believed to play an important role in the quenching of photoluminescence.\n\nIn the absence of an external magnetic field, the ground and excited states are split by the magnetic interaction between the two unpaired electrons at the N-V center (see microscopic model): when two electrons have parallel spins (m=±1), their energy is higher than when spins are antiparallel (m=0). The farther apart the electrons are, the weaker their interaction energy D (roughly \"D\" ~1/\"r\"). Thus the smaller splitting in the excited state can be viewed in terms of larger electron-electron separation in the excited state. When an external magnetic field is applied to the N-V center, it does not affect the \"m\"=0 states nor the A state (because it has \"S\" = 0), but it splits the \"m\" = ±1 levels. If a magnetic field is oriented along the defect axis and reaches about 1027 G (or 508 G) then the \"m\" = –1 and \"m\" = 0 states in the ground (or excited) state become equal in energy; they strongly interact resulting in so-called spin polarization, which strongly affects the intensity of optical absorption and luminescence transitions involving those states.\n\nThis happens because transitions between electronic states are mediated by a photon which cannot change overall spin. Thus optical transitions must preserve the total spin and occur between levels of the same total spin. For this reason, transitions E↔A and A ↔ A are non-radiative and quench the luminescence. Whereas \"m\" = −1 (excited state) ↔ \"m\" = 0 (ground state) transition was forbidden in the absence of an external magnetic field, it becomes allowed when a magnetic field mixes the \"m\" = −1 and \"m\" = 0 levels in the ground state. As a measurable outcome of this phenomenon, luminescence intensity can be strongly modulated by magnetic field.\n\nAn important property of the non-radiative transition between E and A is that it is stronger for m = ±1 and weaker for m = 0. This property results in a very useful manipulation of N-V center, which is called optical spin-polarization. First, consider an off-resonance excitation which has a higher frequency (typically 2.32 eV (532 nm)) than the frequencies of all transitions and thus lays in the vibronic bands for all transitions. By using a pulse of this wavelength, people can excite all spin states and create phonons as well. For a spin state with m = 0, due to conservation of spin in transition, it will be excited to the corresponding m = 0 state in E and then go back to original state. However, for a spin state with m = ±1 in A, after the excitation, it has a relatively high probability to jump to the intermediate state A by non-radiative transition and go to the ground state with m = 0. After sufficient cycles, the state of the N-V center can be regarded as in the m = 0 state. Such a process can be used in the initialization of quantum state in quantum information processing.\n\nThere is an additional level splitting in the excited E state due to the orbital degeneracy and spin-orbit interaction. Importantly, this splitting can be modulated by applying a static electric field, in a similar fashion to the magnetic field mechanism outlined above, though the physics of the splitting is somewhat more complex. Nevertheless, an important practical outcome is that the intensity and position of the luminescence lines can be modulated by applying electric or/and magnetic fields.\n\nThe energy difference between the \"m\" = 0 and \"m\" = ±1 states corresponds to the microwave region. Thus by irradiating the N-V centers with microwave radiation, one can change the relative population of those levels, thereby again modulating the luminescence intensity.\n\nThere is an additional splitting of the \"m\" = ±1 energy levels, which originates from the \"hyperfine\" interaction between the nuclear and electron spins. Thus finally, the optical absorption and luminescence from the N-V center consists of roughly a dozen sharp lines with a separation in the MHz-GHz range, and all those lines can be resolved, given proper sample preparation. The intensity and position of those lines can be modulated using the following tools:\n\nIt should be noted that the above-described energy structure is by no means exceptional for a defect in diamond or other semiconductor. It was not this structure alone, but a combination of several favorable factors (previous knowledge, easy production and excitation, etc.) which suggested the use of the N-V center.\n\nThinking of the N-V center as a multielectronic system, we can draw the diagram in the figure at right, where the states are labeled according to their symmetry and with a left superscript that indicates with a 3 if it is a triplet (S=1) and with a 1 if it is a singlet (S=0). It is well accepted today that we have two triplet states and two intermediate singlet states.\n\nThe optical excitations conserve the spin state, but there is a high probability of the states formula_1 decaying non-radiatively to the singlet state formula_2, a phenomenon called intersystem crossing (ISC). This happens at an appreciable rate because the energy curve in function of the position of the atoms for the formula_1 state intersects the curve for the formula_2 state. Therefore, for some instant during the vibrational relaxation that the ions undergo after the excitement, it is possible for the spin to flip with little or no energy required in the transition. It is important to note that this mechanism also leads to a transition from formula_5 to formula_2, but the rate of this ISC is much lower than the formula_1 states rate, therefore this transition is indicated with a thin line. The diagram also shows the non-radiative and infrared competing decay paths between the two singlet states, and the fine splitting in the triplet states, whose differences in energy correspond to microwave frequencies.\n\nSome authors explain the dynamics of the N-V center by admitting that the transition from formula_8 to formula_9 is small, but as Robledo et al. shows, only the fact that the probability of decaying to formula_2 is smaller for formula_5 than for formula_1 is enough to polarize the spin to m = 0.\n\nThe spectral shape and intensity of the optical signals from the N-V centers are sensitive to external perturbation, such as temperature, strain, electric and magnetic field. However, the use of spectral shape for sensing those perturbation is impractical, as the diamond would have to be cooled to cryogenic temperatures to sharpen the N-V signals. A more realistic approach is to use luminescence intensity (rather than lineshape), which exhibits a sharp resonance when a microwave frequency is applied to diamond that matches the splitting of the ground state levels. The resulting optically detected magnetic resonance signals are sharp even at room temperature, and can be used in miniature sensors. Such sensors can detect magnetic fields of a few nanotesla or electric fields of about 10 V/cm at kilohertz frequencies after 100 seconds of averaging. This sensitivity allows detecting a magnetic or electric field produced by a single electron located tens of nanometers away from an N-V center.\n\nUsing the same mechanism, the N-V centers were employed in scanning thermal microscopy to measure high-resolution spatial maps of temperature and thermal conductivity (see image).\n\nAnother possible use the N-V centers is as a detector to measure the full mechanical stress tensor in the bulk of the crystal. For this application, the stress-induced splitting of the zero-phonon-line is exploited, and its polarization properties. A robust frequency-modulated radio receiver using the electron-spin-dependent photoluminescence that operated up to 350 °C demonstrates the possibility for use in extreme conditions.\n\nIn addition to the quantum optical applications, luminescence from the N-V centers can be applied for imaging biological processes, such as fluid flow in living cells. This application relies on good compatibility of diamond nano-particles with the living cells and on favorable properties of photoluminescence from the N-V centers (strong intensity, easy excitation and detection, temporal stability, etc.). Compared with large single-crystal diamonds, nanodiamonds are cheap (about 1 USD per gram) and available from various suppliers. N-V centers are produced in diamond powders with sub-micrometre particle size using the standard process of irradiation and annealing described above. Those nanodiamonds are introduced in a cell, and their luminescence is monitored using a standard fluorescence microscope.\n\nFurther N-V center has been hypothesized to be a potential bio-mimetic system for emulating radical pair spin dynamics of the avian compass.\n\nStimulated emission from the N-V center has been demonstrated, though it could be achieved only from the phonon side-band (i.e. broadband light) and not from the ZPL. For this purpose the center has to be excited at wavelength longer than ~650 nm, as higher-energy excitation ionizes the center.\n\nThe first continuous-wave room-temperature maser has been demonstrated. It used 532-nm pumped N-V centers held within a high Purcell factor microwave cavity and an external magnetic field of 4300 G. Continuous maser oscillation generated a coherent signal at ~9.2 GHz.\n\nThe N-V center can have a very long spin coherence time approaching the second regime. This is advantageous for applications in quantum sensing and quantum communication. Disadvantageous for these applications is the long radiative lifetime (~12 ns) of the N-V center and the strong phonon sideband in its emission spectrum. Both issues can be addressed by putting the N-V center in an optical cavity .\n\nThe microscopic model and most optical properties of ensembles of the N-V centers have been firmly established in the 1970s based on the optical measurements combined with uniaxial stress and on the electron paramagnetic resonance. However, a minor error in EPR results (it was assumed that illumination is required to observe N-V EPR signals) resulted in the incorrect multiplicity assignments in the energy level structure. In 1991 it was shown that EPR can be observed without illumination, which established the energy level scheme shown above. The magnetic splitting in the excited state has been measured only recently.\n\nThe characterization of single N-V centers has become a very competitive field nowadays, with many dozens of papers published in the most prestigious scientific journals. One of the first results was reported back in 1997. In that paper, it was demonstrated that the fluorescence of single N-V centers can be detected by room-temperature fluorescence microscopy and that the defect shows perfect photostability. Also one of the outstanding properties of the N-V center was demonstrated, namely room-temperature optically detected magnetic resonance.\n\n"}
{"id": "16400356", "url": "https://en.wikipedia.org/wiki?curid=16400356", "title": "Prouhet–Tarry–Escott problem", "text": "Prouhet–Tarry–Escott problem\n\nIn mathematics, the Prouhet–Tarry–Escott problem asks for two disjoint multisets \"A\" and \"B\" of \"n\" integers each, whose first \"k\" power sum symmetric polynomials are all equal.\nThat is, the two multisets should satisfy the equations\nfor each integer \"i\" from 1 to a given \"k\". It has been shown that \"n\" must be strictly greater than \"k\". Solutions with formula_2 are called \"ideal solutions\". Ideal solutions are known for formula_3 and for formula_4. No ideal solution is known for formula_5 or for formula_6.\n\nThis problem was named after Eugène Prouhet, who studied it in the early 1850s, and Gaston Tarry and Escott, who studied it in the early 1910s. The problem originates from letters of Christian Goldbach and Leonhard Euler (1750/1751).\n\n\nand { 1, 2, 10, 12, 20, 21 }, because:\n\nFor \"n\" = 12, an ideal solution is given by \"A\" = {±22, ±61, ±86, ±127, ±140, ±151} and \"B\" = {±35, ±47, ±94, ±121, ±146, ±148}.\n\n\nProuhet used the Thue–Morse sequence to construct a solution with formula_7 for any formula_8. Namely, partition the numbers from 0 to formula_9 into the evil numbers and the odious numbers; then the two sets of the partition give a solution to the problem. For instance, for formula_10 and formula_11, Prouhet's solution is:\n\nA higher dimensional version of the Prouhet–Tarry–Escott problem has been introduced and studied by Andreas Alpers and Robert Tijdeman in 2007: Given parameters formula_12, find two different multi-sets formula_13, formula_14 of points from formula_15 such that\n\nfor all formula_17 with formula_18 This problem is related to discrete tomography and also leads to special Prouhet-Tarry-Escott solutions over the Gaussian integers (though solutions to the Alpers-Tijdeman problem do not exhaust the Gaussian integer solutions to Prouhet-Tarry-Escott).\n\nA solution for formula_19 and formula_20 is given, for instance, by:\n\nNo solutions for formula_23 with formula_24 are known.\n\n\n"}
{"id": "1182348", "url": "https://en.wikipedia.org/wiki?curid=1182348", "title": "Provable security", "text": "Provable security\n\nProvable security refers to any type or level of security that can be proved. It is used in different ways by different fields.\n\nUsually, this refers to mathematical proofs, which are common in cryptography. In such a proof, the capabilities of the attacker are defined by an adversarial model (also referred to as attacker model): the aim of the proof is to show that the attacker must solve the underlying hard problem in order to break the security of the modelled system. Such a proof generally does not consider side-channel attacks or other implementation-specific attacks, because they are usually impossible to model without implementing the system (and thus, the proof only applies to this implementation).\n\nOutside of cryptography, the term is often used in conjunction with secure coding and security by design, both of which can rely on proofs to show the security of a particular approach. As with the cryptographic setting, this involves an attacker model and a model of the system. For example, code can be verified to match the intended functionality, described by a model: this can be done through static checking. These techniques are sometimes used for evaluating products (see Common Criteria): the security here depends not only on the correctness of the attacker model, but also on the model of the code.\n\nFinally, the term provable security is sometimes used by sellers of security software that are attempting to sell security products like firewalls, antivirus software and intrusion detection systems. As these products are typically not subject to scrutiny, many security researchers consider this type of claim to be selling snakeoil.\n\nIn cryptography, a system has provable security if its security requirements can be stated formally in an adversarial model, as opposed to heuristically, with clear assumptions that the adversary has access to the system as well as enough computational resources. The proof of security (called a \"reduction\") is that these security requirements are met provided the assumptions about the adversary's access to the system are satisfied and some clearly stated assumptions about the hardness of certain computational tasks hold. An early example of such requirements and proof was given by Goldwasser and Micali for semantic security and the construction based on the quadratic residuosity problem. Some proofs of security are in given theoretical models such as the random oracle model, where real cryptographic hash functions are represented by an idealization.\n\nThere are several lines of research in provable security. One is to establish the \"correct\" definition of security for a given, intuitively understood task. Another is to suggest constructions and proofs based on general assumptions as much as possible, for instance the existence of a one-way function. A major open problem is to establish such proofs based on P ≠ NP, since the existence of one-way functions is not known to follow from the P ≠ NP conjecture.\n\nSeveral researchers have found mathematical fallacies in proofs that had been used to make claims about the security of important protocols. In the following partial list of such researchers, their names are followed by first a reference to the original paper with the purported proof and then a reference to the paper in which the researchers reported on flaws:\nV. Shoup;\nA. J. Menezes;\nA. Jha and M. Nandi;\nD. Galindo;\nT. Iwata, K. Ohashi, and K. Minematsu;\nM. Nandi;\nJ.-S. Coron and D. Naccache;\nD. Chakraborty, V. Hernández-Jiménez, and P. Sarkar;\nP. Gaži and U. Maurer;\nS. A. Kakvi and E. Kiltz;\nand T. Holenstein, R. Künzler, and S. Tessaro.\n\nKoblitz and Menezes have claimed that provable security results for important cryptographic protocols frequently have fallacies in the proofs; are often interpreted in a misleading manner, giving false assurances; typically rely upon strong assumptions that may turn out to be false; are based on unrealistic models of security; and serve to distract researchers' attention from the need for \"old-fashioned\" (non-mathematical) testing and analysis. Their series of papers supporting these claims have been controversial in the community. Among the researchers who have rejected the viewpoint of Koblitz-Menezes is Oded Goldreich, a leading theoretician and author of \"Foundations of Cryptography.\" He wrote a refutation of their first paper \"Another look at `provable security'\" that he titled \"On post-modern cryptography.\" Goldreich wrote: \"...we point out some of the fundamental philosophical flaws that underly the said article and some of its misconceptions regarding theoretical research in Cryptography in the last quarter of a century.\" In his essay Goldreich argued that the rigorous analysis methodology of provable security is the only one compatible with science, and that Koblitz and Menezes are \"reactionary (i.e., they play to the hands of the opponents of progress).\"\n\nIn 2007, Koblitz published \"The Uneasy Relationship Between Mathematics and Cryptography,\" which contained some controversial statements about provable security and other topics. Researchers Oded Goldreich, Boaz Barak, Jonathan Katz, Hugo Krawczyk, and Avi Wigderson wrote letters responding to Koblitz's article, which were published in the November 2007 and January 2008 issues of the journal. Katz, who is coauthor of a highly regarded cryptography textbook, called Koblitz's article \"snobbery at its purest\"; and Wigderson, who is a permanent member of the Institute for Advanced Study in Princeton, accused Koblitz of \"slander.\"\n\nIvan Damgård later wrote position paper at ICALP 2007 on the technical issues, and it was recommended by Scott Aaronson as a good in-depth analysis.\nBrian Snow, former Technical Director of the Information Assurance Directorate of the U.S. National Security Agency, recommended the Koblitz-Menezes paper \"The brave new world of bodacious assumptions in cryptography\" to the audience at the RSA Conference 2010 Cryptographers Panel.\n\nClassical provable security primarily aimed at studying the relationship between asymptotically defined objects. Instead, practice-oriented provable security is concerned with concrete objects of cryptographic practice, such as hash functions, block ciphers, and protocols as they are deployed and used. Practice oriented provable security uses concrete security to analyse practical constructions with fixed key sizes. \"Exact security\" or \"concrete security\" is the name given to provable security reductions where one quantifies security by computing precise bounds on computational effort, rather than an asymptotic bound which is guaranteed to hold for \"sufficiently large\" values of the security parameter.\n"}
{"id": "15985233", "url": "https://en.wikipedia.org/wiki?curid=15985233", "title": "Quantum Byzantine agreement", "text": "Quantum Byzantine agreement\n\nByzantine fault tolerant protocols are algorithms that are robust to arbitrary types of failures in distributed algorithms. With the advent and popularity of the Internet, there is a need to develop algorithms that do not require any centralized control that have some guarantee of always working correctly. The Byzantine agreement protocol is an essential part of this task. In this article the quantum version of the Byzantine protocol, which works in constant time is described.\n\nThe Byzantine Agreement protocol is a protocol in distributed computing.\nIt takes its name from a problem formulated by Lamport, Shostak and Pease in 1982, which itself is a reference to a historical problem. The Byzantine army was divided into divisions with each division being led by a General with the following properties:\n\n(See for the proof of the impossibility result). \nThe problem usually is equivalently restated in the form of a commanding General and loyal Lieutenants with the General being either loyal or a traitor and the same for the Lieutenants with the following properties.\n\n\nFailures in an algorithm or protocol can be categorized into three main types:\n\nA Byzantine resilient or Byzantine fault tolerant protocol or algorithm is an algorithm that is robust to all the kinds of failures mentioned above. For example, given a space shuttle with multiple redundant processors and some of the processors give incorrect data, which processors or sets of processors should be believed? The solution can be formulated as a Byzantine fault tolerant protocol.\n\nWe will sketch here the asynchronous algorithm \nThe algorithm works in two phases:\nThere are two types of coin flipping protocols\n\n\n\nTo generate a random coin assign an integer in the range [0,n-1] to each player and each player is not allowed to choose its own \nrandom ID as each player formula_17 selects a random number formula_18 for every other player formula_19 and distributes this using a verifiable secret sharing scheme.\n\nAt the end of this phase players agree on which secrets were properly shared, the secrets are then opened and each player formula_11 is assigned the value formula_21\nThis requires private information channels so we replace the random secrets by the superposition formula_22. In which the state is encoded using a quantum verifiable secret sharing protocol (QVSS). We cannot distribute the state formula_23 since the bad players can collapse the state. To prevent bad players from doing so we encode the state using the Quantum verifiable secret sharing (QVSS) and send each player their share of the secret. Here again the verification requires Byzantine Agreement, but replacing the agreement by the grade-cast protocol is enough.\n\nA grade-cast protocol has the following properties using the definitions in \nInformally, a graded broadcast protocol is a protocol with a designated player called “dealer” (the one who broadcasts) such that:\nA protocol P is said to be achieve graded broadcast if, at the beginning of the protocol, a designated player D (called the dealer) holds a value v, and at the end of the protocol, every player formula_19 outputs a pair formula_25 such that the following properties hold: \nformula_26\n\nFor formula_38 the verification stage of the QVSS protocol guarantees that for a good dealer the correct state will be encoded, and that for any, possibly faulty dealer, some particular state will be recovered during the recovery stage. We note that for the purpose of our Byzantine quantum coin flip protocol the recovery stage is much simpler. Each player measures his share of the QVSS and sends the classical value to all other players. The verification stage guarantees, with high probability, that in the presence of up to formula_38 faulty players all the good players will recover the same classical value (which is the same value that would result from a direct measurement of the encoded state).\n\nIn 2007, a quantum protocol for Byzantine Agreement was demonstrated experimentally using a four-photon polarization-entangled state. This shows that the quantum implementation of classical Byzantine Agreement protocols is indeed feasible.\n"}
{"id": "7941780", "url": "https://en.wikipedia.org/wiki?curid=7941780", "title": "Quantum Markov chain", "text": "Quantum Markov chain\n\nIn mathematics, the quantum Markov chain is a reformulation of the ideas of a classical Markov chain, replacing the classical definitions of probability with quantum probability.\n\nVery roughly, the theory of a quantum Markov chain resembles that of a measure-many automaton, with some important substitutions: the initial state is to be replaced by a density matrix, and the projection operators are to be replaced by positive operator valued measures.\n\nMore precisely, a quantum Markov chain is a pair formula_1 with formula_2 a density matrix and formula_3 a quantum channel such that\n\nis a completely positive trace-preserving map, and formula_5 a C-algebra of bounded operators. The pair must obey the quantum Markov condition, that\n\nfor all formula_7.\n\n\n"}
{"id": "3119343", "url": "https://en.wikipedia.org/wiki?curid=3119343", "title": "Sample exclusion dimension", "text": "Sample exclusion dimension\n\nIn computational learning theory, sample exclusion dimensions arise in the study of exact concept learning with queries. \n\nIn algorithmic learning theory, a concept over a domain \"X\" is a Boolean function over \"X\". Here we only consider finite domains. A partial approximation \"S\" of a concept \"c\" is a Boolean function over formula_1 such that \"c\" is an extension to \"S\".\n\nLet \"C\" be a class of concepts and \"c\" be a concept (not necessarily in \"C\"). Then a specifying set for c w.r.t. \"C\", denoted by \"S\" is a partial approximation \"S\" of \"c\" such that \"C\" contains at most one extension to \"S\". If we have observed a specifying set for some concept w.r.t. \"C\", then we have enough information to verify a concept in \"C\" with at most one more mind change.\nThe exclusion dimension, denoted by \"XD\"(\"C\"), of a concept class is the maximum of the size of the minimum specifying set of \"c\"<nowiki>'</nowiki> with respect to \"C\", where \"c\"<nowiki>'</nowiki> is a concept not in \"C\".\n"}
{"id": "362651", "url": "https://en.wikipedia.org/wiki?curid=362651", "title": "Scottish Café", "text": "Scottish Café\n\nThe Scottish Café () was the café in Lwów, Poland (now Lviv, Ukraine) where, in the 1930s and 1940s, mathematicians from the Lwów School collaboratively discussed research problems, particularly in functional analysis and topology.\n\nStanislaw Ulam recounts that the tables of the café had marble tops, so they could write in pencil, directly on the table, during their discussions. To keep the results from being lost, and after becoming annoyed with their writing directly on the table tops, Stefan Banach's wife provided the mathematicians with a large notebook, which was used for writing the problems and answers and eventually became known as the \"Scottish Book\". The book—a collection of solved, unsolved, and even probably unsolvable problems—could be borrowed by any of the guests of the café. Solving any of the problems was rewarded with prizes, with the most difficult and challenging problems having expensive prizes (during the Great Depression and on the eve of World War II), such as a bottle of fine brandy.\n\nFor problem 153, which was later recognized as being closely related to Stefan Banach's \"basis problem\", Stanisław Mazur offered the prize of a live goose. This problem was solved only in 1972 by Per Enflo, who was presented with the live goose in a ceremony that was broadcast throughout Poland.\n\nThe café building now houses the Szkotcka Restaurant & Bar (named for the original Scottish Café) and the Atlas Deluxe hotel at the street address of 27 Taras Shevchenko Prospekt.\n\nThe following mathematicians were associated with the Lwów School of Mathematics or contributed to \"The Scottish Book\":\n\n\n"}
{"id": "5558590", "url": "https://en.wikipedia.org/wiki?curid=5558590", "title": "Shapiro inequality", "text": "Shapiro inequality\n\nIn mathematics, the Shapiro inequality is an inequality proposed by H. Shapiro in 1954.\n\nSuppose formula_1 is a natural number and formula_2 are positive numbers and:\n\n\nThen the Shapiro inequality states that\n\nwhere formula_8.\n\nFor greater values of formula_1 the inequality does not hold and the strict lower bound is formula_10 with formula_11.\n\nThe initial proofs of the inequality in the pivotal cases formula_12 (Godunova and Levin, 1976) and formula_13 (Troesch, 1989) rely on numerical computations. In 2002, P.J. Bushell and J.B. McLeod published an analytical proof for formula_12.\n\nThe value of formula_15 was determined in 1971 by Vladimir Drinfeld, who won a Fields Medal in 1990. Specifically, Drinfeld showed that the strict lower bound formula_15 is given by formula_17, where formula_18 is the function convex hull of formula_19</sup> and formula_20. (That is, the region above the graph of formula_18 is the convex hull of the union of the regions above the graphs of 'formula_22 and formula_23.)\n\nInterior local mimima of the left-hand side are alwaysformula_24 (Nowosad, 1968).\n\nThe first counter-example was found by Lighthill in 1956, for formula_26:\nThen the left-hand side is equal to formula_29, thus lower than 10 when formula_28 is small enough.\n\nThe following counter-example for formula_31 is by Troesch (1985):\n\n"}
{"id": "20823677", "url": "https://en.wikipedia.org/wiki?curid=20823677", "title": "Sidon sequence", "text": "Sidon sequence\n\nIn number theory, a Sidon sequence (or Sidon set), named after the Hungarian mathematician Simon Sidon, is a sequence \"A\" = {\"a\", \"a\", \"a\", ...} of natural numbers in which all pairwise sums \"a\" + \"a\" (\"i\" ≤ \"j\") are different. Sidon introduced the concept in his investigations of Fourier series.\n\nThe main problem in the study of Sidon sequences, posed by Sidon, is to find the largest number of elements a Sidon sequence \"A\" can have smaller than some given number \"x\". Despite a large body of research, the question remained unsolved for almost 80 years. In 2010, it was finally settled by J. Cilleruelo, I. Ruzsa and C. Vinuesa.\n\nPaul Erdős and Pál Turán proved that, for every \"x\" > 0, the number of elements smaller than \"x\" in a Sidon sequence is at most formula_1. Using a construction of J. Singer, they showed that there exist Sidon sequences that contain formula_2 terms less than \"x\".\n\nErdős also showed that if we consider any particular infinite Sidon sequence \"A\" and let \"A\"(\"x\") denote the number of its elements up to \"x\", then\n\nThat is, infinite Sidon sequences are thinner than the densest finite Sidon sequences.\n\nFor the other direction, Chowla and Mian observed that the greedy algorithm gives an infinite Sidon sequence with formula_4 for every \"x\". Ajtai, Komlós, and Szemerédi improved this with a construction of a Sidon sequence with\n\nThe best lower bound to date was given by Imre Z. Ruzsa, who proved that a Sidon sequence with\n\nexists. Erdős conjectured that an infinite Sidon set \"A\" exists for which formula_7 holds. He and Rényi showed the existence of a sequence {\"a\",\"a\"...} with the conjectural density but satisfying only the weaker property that there is a constant \"k\" such that for every natural number \"n\" there are at most \"k\" solutions of the equation \"a\" + \"a\" = \"n\". (To be a Sidon sequence would require that \"k\" = 1.)\n\nErdős further conjectured that there exists a nonconstant integer-coefficient polynomial whose values at the natural numbers form a Sidon sequence. Specifically, he asked if the set of fifth powers is a Sidon set. Ruzsa came close to this by showing that there is a real number \"c\" with 0 < \"c\" < 1 such that the range of the function \n\"f\"(\"x\") = \"x\" + [\"cx\"] is a Sidon sequence, where [.] denotes integer part. As \"c\" is irrational, this function \"f\"(\"x\") is not a polynomial. The statement that the set of fifth powers is a Sidon set is a special case of the later conjecture of Lander, Parkin and Selfridge.\n\nAll finite Sidon sets are Golomb rulers, and vice versa.\n\nTo see this, suppose for a contradiction that \"S\" is a Sidon set and not a Golomb ruler. Since it is not a Golomb ruler, there must be four members such that formula_8. It follows that formula_9, which contradicts the proposition that \"S\" is a Sidon set. Therefore all Sidon sets must be Golomb rulers. By a similar argument, all Golomb rulers must be Sidon sets.\n\n"}
{"id": "26413136", "url": "https://en.wikipedia.org/wiki?curid=26413136", "title": "Theory of pure equality", "text": "Theory of pure equality\n\nIn mathematical logic the theory of pure equality is a first-order theory. It has a signature consisting of only the equality relation symbol, and includes no non-logical axioms at all (Monk 1976:240–242). This theory is consistent, as any set with the usual equality relation provides an interpretation.\n\nThe theory of pure equality was proven to be decidable by Löwenheim in 1915. If an additional axiom is added saying either that there are exactly \"m\" objects, for a fixed natural number \"m\", or an axiom scheme is added stating there are infinitely many objects, the resulting theory is complete.\n"}
{"id": "35758279", "url": "https://en.wikipedia.org/wiki?curid=35758279", "title": "U-rank", "text": "U-rank\n\nIn model theory, a branch of mathematical logic, U-rank is one measure of the complexity of a (complete) type, in the context of stable theories. As usual, higher U-rank indicates less restriction, and the existence of a U-rank for all types over all sets is equivalent to an important model-theoretic condition: in this case, superstability.\n\nU-rank is defined inductively, as follows, for any (complete) n-type p over any set A:\n\n\nWe say that \"U\"(\"p\") = \"α\" when the \"U\"(\"p\") ≥ \"α\" but not \"U\"(\"p\") ≥ \"α\" + 1.\n\nIf \"U\"(\"p\") ≥ \"α\" for all ordinals \"α\", we say the U-rank is unbounded, or \"U\"(\"p\") = ∞.\n\nNote: U-rank is formally denoted formula_1, where p is really p(x), and x is a tuple of variables of length n. This subscript is typically omitted when no confusion can result.\n\nU-rank is monotone in its domain. That is, suppose \"p\" is a complete type over \"A\" and \"B\" is a subset of \"A\". Then for \"q\" the restriction of \"p\" to \"B\", \"U\"(\"q\") ≥ \"U\"(\"p\").\n\nIf we take \"B\" (above) to be empty, then we get the following: if there is an \"n\"-type \"p\", over some set of parameters, with rank at least \"α\", then there is a type over the empty set of rank at least \"α\". Thus, we can define, for a complete (stable) theory \"T\", formula_2.\n\nWe then get a concise characterization of superstability; a stable theory \"T\" is superstable if and only if formula_3 for every \"n\".\n\n\n"}
{"id": "28832206", "url": "https://en.wikipedia.org/wiki?curid=28832206", "title": "UCL Faculty of Mathematical and Physical Sciences", "text": "UCL Faculty of Mathematical and Physical Sciences\n\nThe UCL Faculty of Mathematical and Physical Sciences is one of the 11 constituent faculties of University College London (UCL). The Faculty, the UCL Faculty of Engineering Sciences and the UCL Faculty of the Built Envirornment (The Bartlett) together form the UCL School of the Built Environment, Engineering and Mathematical and Physical Sciences.\n\nThe Faculty currently comprises the following departments:\n\n\nThe Faculty is closely involved with the following research centres and institutes:\n\n\nIn the 2013 \"Academic Ranking of World Universities\", UCL is ranked joint 51st to 75th in the world (and joint 12th in Europe) for Natural Sciences and Mathematics.\n\nIn the 2013 \"QS World University Rankings\", UCL is ranked 38th in the world (and 12th in Europe) for Natural Sciences. In the 2014 \"QS World University Rankings by Subject\", UCL is ranked joint 51st-100th in the world (and joint 12th in Europe) for Chemistry, joint 27th in the world (and 8th in Europe) for Earth & Marine Sciences, joint 51st-100th in the world (and joint 13th in Europe) for Materials Science, joint 36th in the world (and joint 10th in Europe) for Mathematics, 35th in the world (and 13th in Europe) for Physics & Astronomy, and 47th in the world (and 9th in Europe) for Statistics & Operational Research.\n\nIn the 2013/14 \"Times Higher Education World University Rankings\", UCL is ranked 51st in the world (and 16th in Europe) for Physical Sciences.\n\n\n\n"}
{"id": "167053", "url": "https://en.wikipedia.org/wiki?curid=167053", "title": "Unit vector", "text": "Unit vector\n\nIn mathematics, a unit vector in a normed vector space is a vector (often a spatial vector) of length 1. A unit vector is often denoted by a lowercase letter with a circumflex, or \"hat\": formula_1 (pronounced \"i-hat\"). The term direction vector is used to describe a unit vector being used to represent spatial direction, and such quantities are commonly denoted as d. Two 2D direction vectors, d1 and d2 are illustrated. 2D spatial directions represented this way are numerically equivalent to points on the unit circle.\n\nThe same construct is used to specify spatial directions in 3D. As illustrated, each unique direction is equivalent numerically to a point on the unit sphere.\n\nThe normalized vector or versor û of a non-zero vector u is the unit vector in the direction of u, i.e.,\n\nwhere |u| is the norm (or length) of u. The term \"normalized vector\" is sometimes used as a synonym for \"unit vector\".\n\nUnit vectors are often chosen to form the basis of a vector space. Every vector in the space may be written as a linear combination of unit vectors.\n\nBy definition, in a Euclidean space the dot product of two unit vectors is a scalar value amounting to the cosine of the smaller subtended angle. In three-dimensional Euclidean space, the cross product of two arbitrary unit vectors is a third vector orthogonal to both of them having length equal to the sine of the smaller subtended angle. The normalized cross product corrects for this varying length, and yields the mutually orthogonal unit vector to the two inputs, applying the right-hand rule to resolve one of two possible directions.\n\nUnit vectors may be used to represent the axes of a Cartesian coordinate system. For instance, the unit vectors in the direction of the \"x\", \"y\", and \"z\" axes of a three dimensional Cartesian coordinate system are\n\nThey are sometimes referred to as the versors of the coordinate system, and they form a set of mutually orthogonal unit vectors, typically referred to as a standard basis in linear algebra.\n\nThey are often denoted using normal vector notation (e.g., i or formula_4) rather than standard unit vector notation (e.g., formula_5). In most contexts it can be assumed that i, j, and k, (or formula_6 formula_7 and formula_8) are versors of a 3-D Cartesian coordinate system. The notations formula_9, formula_10, formula_11, or formula_12, with or without hat, are also used, particularly in contexts where i, j, k might lead to confusion with another quantity (for instance with index symbols such as \"i\", \"j\", \"k\", used to identify an element of a set or array or sequence of variables).\n\nWhen a unit vector in space is expressed, with Cartesian notation, as a linear combination of i, j, k, its three scalar components can be referred to as direction cosines. The value of each component is equal to the cosine of the angle formed by the unit vector with the respective basis vector. This is one of the methods used to describe the orientation (angular position) of a straight line, segment of straight line, oriented axis, or segment of oriented axis (vector).\n\nThe three orthogonal unit vectors appropriate to cylindrical symmetry are: \nThey are related to the Cartesian basis formula_18, formula_19, formula_20 by:\n\nIt is important to note that formula_13 and formula_16 are functions of formula_28, and are \"not\" constant in direction. When differentiating or integrating in cylindrical coordinates, these unit vectors themselves must also be operated on. For a more complete description, see Jacobian matrix. The derivatives with respect to formula_29 are:\n\nThe unit vectors appropriate to spherical symmetry are: formula_33, the direction in which the radial distance from the origin increases; formula_34, the direction in which the angle in the \"x\"-\"y\" plane counterclockwise from the positive \"x\"-axis is increasing; and formula_35, the direction in which the angle from the positive \"z\" axis is increasing. To minimize redundancy of representations, the polar angle formula_36 is usually taken to lie between zero and 180 degrees. It is especially important to note the context of any ordered triplet written in spherical coordinates, as the roles of formula_16 and formula_35 are often reversed. Here, the American \"physics\" convention is used. This leaves the azimuthal angle formula_39 defined the same as in cylindrical coordinates. The Cartesian relations are:\n\nThe spherical unit vectors depend on both formula_39 and formula_36, and hence there are 5 possible non-zero derivatives. For a more complete description, see Jacobian matrix and determinant. The non-zero derivatives are:\n\nCommon general themes of unit vectors occur throughout physics and geometry:\n\nIn general, a coordinate system may be uniquely specified using a number of linearly independent unit vectors formula_50 equal to the degrees of freedom of the space. For ordinary 3-space, these vectors may be denoted formula_51. It is nearly always convenient to define the system to be orthonormal and right-handed:\n\nwhere δ is the Kronecker delta (which is 1 for \"i\" = \"j\" and 0 otherwise) and formula_54 is the Levi-Civita symbol (which is 1 for permutations ordered as \"ijk\" and −1 for permutations ordered as \"kji\").\n\nA unit vector in ℝ was called a right versor by W. R. Hamilton as he developed his quaternions ℍ ⊂ ℝ. In fact, he was the originator of the term \"vector\" as every quaternion formula_55 has a scalar part \"s\" and a vector part \"v\". If \"v\" is a unit vector in ℝ, then the square of \"v\" in quaternions is –1. By Euler's formula then, formula_56 is a versor in the 3-sphere. When θ is a right angle, the versor is a right versor: its scalar part is zero and its vector part \"v\" is a unit vector in ℝ.\n\n\n"}
