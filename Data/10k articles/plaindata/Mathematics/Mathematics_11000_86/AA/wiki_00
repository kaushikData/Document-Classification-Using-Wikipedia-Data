{"id": "506330", "url": "https://en.wikipedia.org/wiki?curid=506330", "title": "3SUM", "text": "3SUM\n\nIn computational complexity theory, the 3SUM problem asks if a given set of formula_1 real numbers contains three elements that sum to zero. A generalized version, \"k\"-SUM, asks the same question on \"k\" numbers. 3SUM can be easily solved in formula_2 time, and matching formula_3 lower bounds are known in some specialized models of computation .\n\nIt was widely conjectured that any deterministic algorithm for the 3SUM requires formula_4 time.\nIn 2014, the original 3SUM conjecture was refuted by Allan Grønlund and Seth Pettie who gave a deterministic algorithm that solves 3SUM in formula_5 time .\nAdditionally, Grønlund and Pettie showed that the 4-linear decision tree complexity of 3SUM is formula_6.\nThese bounds were subsequently improved;\nthe current best known algorithm for 3SUM runs in formula_7 time, and the randomized 4-linear decision tree complexity of 3SUM is formula_8 .\nIt is still conjectured that 3SUM is unsolvable in formula_9 expected time.\n\nWhen the elements are integers in the range formula_10, 3SUM can be solved in formula_11 time by representing the input set formula_12 as a bit vector, computing the set formula_13 of all pairwise sums as a discrete convolution using the Fast Fourier transform, and finally comparing this set to formula_14.\n\nSuppose the input array is formula_15. In integer (word RAM) models of computing, 3SUM can be solved in formula_2 time on average by inserting each number formula_17 into a hash table, and then for each index formula_18 and formula_19, checking whether the hash table contains the integer formula_20.\n\nIt is also possible to solve the problem in the same time in a comparison-based model of computing or real RAM, for which hashing is not allowed. The algorithm below first sorts the input array and then tests all possible pairs in a careful order that avoids the need to binary search for the pairs in the sorted list, achieving worst-case formula_2 time, as follows.\n\nThe following example shows this algorithm's execution on a small sorted array. Current values of a are shown in green, values of b and c are shown in blue.\n\nThe correctness of the algorithm can be seen as follows. Suppose we have a solution a + b + c = 0. Since the pointers only move in one direction, we can run the algorithm until the leftmost pointer points to a. Run the algorithm until either one of the remaining pointers points to b or c, whichever occurs first. Then the algorithm will run until the last pointer points to the remaining term, giving the affirmative solution.\n\nInstead of looking for numbers whose sum is 0, it is possible to look for numbers whose sum is any constant \"C\" in the following way:\n\nFor e.g., if A=[1,2,3,4] and if you are asked to find 3sum for C=4, then subtract all the elements of A by 4/3 and solve it in the usual 3sum way, i.e., (a-C/3) + (b-C/3) + (c-C/3) = 0\n\nInstead of searching for the 3 numbers in a single array, we can search for them in 3 different arrays. I.e., given three arrays X, Y and Z, find three numbers , such that . Call the 1-array variant 3SUM×1 and the 3-array variant 3SUM×3.\n\nGiven a solver for 3SUM×1, the 3SUM×3 problem can be solved in the following way (assuming all elements are integers): \nBy the way we transformed the arrays, it is guaranteed that .\n\nInstead of looking for arbitrary elements of the array such that:\nthe \"convolution 3sum\" problem (Conv3SUM) looks for elements in specific locations:\n\nGiven a solver for 3SUM, the Conv3SUM problem can be solved in the following way.\n\nCorrectness proof:\n\nGiven a solver for Conv3SUM, the 3SUM problem can be solved in the following way.\n\nThe reduction uses a hash function. As a first approximation, assume that we have a linear hash function, i.e. a function \"h\" such that:\nSuppose that all elements are integers in the range: 0...\"N\"-1, and that the function \"h\" maps each element to an element in the smaller range of indices: 0...\"n\"-1. Create a new array \"T\" and send each element of \"S\" to its hash value in \"T\", i.e., for every \"x\" in \"S\":\nInitially, suppose that the mappings are unique (i.e. each cell in \"T\" accepts only a single element from \"S\"). Solve Conv3SUM on \"T\". Now:\n\nThis idealized solution doesn't work, because any hash function might map several distinct elements of \"S\" to the same cell of \"T\". The trick is to create an array \"T*\" by selecting a single random element from each cell of \"T\", and run Conv3SUM on \"T*\". If a solution is found, then it is a correct solution for 3SUM on \"S\". If no solution is found, then create a different random \"T*\" and try again. Suppose there are at most \"R\" elements in each cell of \"T\". Then the probability of finding a solution (if a solution exists) is the probability that the random selection will select the correct element from each cell, which is formula_37. By running Conv3SUM formula_38 times, the solution will be found with a high probability.\n\nUnfortunately, we do not have linear perfect hashing, so we have to use an almost linear hash function, i.e. a function \"h\" such that:\nThis requires to duplicate the elements of \"S\" when copying them into \"T\", i.e., put every element formula_41 both in formula_42 (as before) and in formula_43. So each cell will have 2\"R\" elements, and we will have to run Conv3SUM formula_44 times.\n\nA problem is called 3SUM-hard if solving it in subquadratic time implies a subquadratic-time algorithm for 3SUM. The concept of 3SUM-hardness was introduced by . They proved that a large class of problems in computational geometry are 3SUM-hard, including the following ones. (The authors acknowledge that many of these problems are contributed by other researchers.)\n\n\nBy now there are a multitude of other problems that fall into this category. An example is the decision version of X + Y sorting: given sets of numbers and of elements each, are there distinct for ?\n\n\n"}
{"id": "1674987", "url": "https://en.wikipedia.org/wiki?curid=1674987", "title": "Abstract index notation", "text": "Abstract index notation\n\nAbstract index notation is a mathematical notation for tensors and spinors that uses indices to indicate their types, rather than their components in a particular basis. The indices are mere placeholders, not related to any basis and, in particular, are non-numerical. Thus it should not be confused with the Ricci calculus. The notation was introduced by Roger Penrose as a way to use the formal aspects of the Einstein summation convention to compensate for the difficulty in describing contractions and covariant differentiation in modern abstract tensor notation, while preserving the explicit covariance of the expressions involved.\n\nLet \"V\" be a vector space, and \"V\" its dual. Consider, for example, an order-2 covariant tensor \nformula_1. Then \"h\" can be identified with a bilinear form on \"V\". In other words, it is a function of two arguments in \"V\" which can be represented as a pair of \"slots\":\n\nAbstract index notation is merely a \"labelling\" of the slots with Latin letters, which have no significance apart from their designation as labels of the slots (i.e., they are non-numerical):\n\nA tensor contraction (or trace) between two tensors is represented by the repetition of an index label, where one label is contravariant (an \"upper index\" corresponding to a tensor in \"V\") and one label is covariant (a \"lower index\" corresponding to a tensor in \"V\"). Thus, for instance,\n\nis the trace of a tensor \"t\" = \"t\" over its last two slots. This manner of representing tensor contractions by repeated indices is formally similar to the Einstein summation convention. However, as the indices are non-numerical, it does not imply summation: rather it corresponds to the abstract basis-independent trace operation (or duality pairing) between tensor factors of type \"V\" and those of type \"V\".\n\nA general homogeneous tensor is an element of a tensor product of copies of \"V\" and \"V\", such as\n\nLabel each factor in this tensor product with a Latin letter in a raised position for each contravariant \"V\" factor, and in a lowered position for each covariant \"V\" position. In this way, write the product as\n\nor, simply\n\nThe last two expressions denote the same object as the first. Tensors of this type are denoted using similar notation, for example:\n\nIn general, whenever one contravariant and one covariant factor occur in a tensor product of spaces, there is an associated \"contraction\" (or \"trace\") map. For instance,\n\nis the trace on the first two spaces of the tensor product.\n\nis the trace on the first and last space.\n\nThese trace operations are signified on tensors by the repetition of an index. Thus the first trace map is given by\n\nand the second by\n\nTo any tensor product on a single vector space, there are associated braiding maps. For example, the braiding map\ninterchanges the two tensor factors (so that its action on simple tensors is given by formula_14). In general, the braiding maps are in one-to-one correspondence with elements of the symmetric group, acting by permuting the tensor factors. Here, we use formula_15 to denote the braiding map associated to the permutation formula_16 (represented as a product of disjoint cyclic permutations).\n\nBraiding maps are important in differential geometry, for instance, in order to express the Bianchi identity. Here let formula_17 denote the Riemann tensor, regarded as a tensor in formula_18. The first Bianchi identity then asserts that\n\nAbstract index notation handles braiding as follows. On a particular tensor product, an ordering of the abstract indices is fixed (usually this is a lexicographic ordering). The braid is then represented in notation by permuting the labels of the indices. Thus, for instance, with the Riemann tensor\nthe Bianchi identity becomes\n\nA general tensor may be antisymmetrized or symmetrized, and there is according notation.\n\nWe demonstrate the notation by example. Let's antisymmetrize the type-(0,3) tensor formula_22, where formula_23 is the symmetric group on three elements.\n\nformula_24\n\nSimilarly, we may symmetrize:\n\nformula_25\n\n\n"}
{"id": "24598717", "url": "https://en.wikipedia.org/wiki?curid=24598717", "title": "Activity selection problem", "text": "Activity selection problem\n\nThe activity selection problem is a combinatorial optimization problem concerning the selection of non-conflicting activities to perform within a given time frame, given a set of activities each marked by a start time (s) and finish time (f). The problem is to select the maximum number of activities that can be performed by a single person or machine, assuming that a person can only work on a single activity at a time. The activity selection problem is also known as the Interval scheduling maximization problem (ISMP), which is a special type of the more general Interval Scheduling problem.\n\nA classic application of this problem is in scheduling a room for multiple competing events, each having its own time requirements (start and end time), and many more arise within the framework of operations research.\n\nAssume there exist \"n\" activities with each of them being represented by a start time \"s\" and finish time \"f\". Two activities \"i\" and \"j\" are said to be non-conflicting if \"s\" ≥ \"f\" or \"s\" ≥ \"f\". The activity selection problem consists in finding the maximal solution set (S) of non-conflicting activities, or more precisely there must exist no solution set S' such that |S'| > |S| in the case that multiple maximal solutions have equal sizes.\n\nThe activity selection problem is notable in that using a greedy algorithm to find a solution will always result in an optimal solution. A pseudocode sketch of the iterative version of the algorithm and a proof of the optimality of its result are included below.\n\nLine 1: This algorithm is called \"Greedy-Iterative-Activity-Selector\", because it is first of all a greedy algorithm, and then it is iterative. There's also a recursive version of this greedy algorithm.\n\nNote that these arrays are indexed starting from 1 up to the length of the corresponding array.\n\nLine 3: Sorts in \"increasing order of finish times\" the array of activities formula_1 by using the finish times stored in the array formula_4. This operation can be done in formula_8 time, using for example merge sort, heap sort, or quick sort algorithms.\n\nLine 5: Creates a set formula_9 to store the \"selected activities\", and initialises it with the activity formula_10 that has the earliest finish time.\n\nLine 6: Creates a variable formula_11 that keeps track of the index of the last selected activity.\n\nLine 10: Starts iterating from the second element of that array formula_1 up to its last element.\n\nLines 11,12: If the \"start time\" formula_13 of the formula_14 activity (formula_15) is greater or equal to the \"finish time\" formula_16 of the \"last selected activity\" (formula_17), then formula_15 is compatible to the selected activities in the set formula_9, and thus it can be added to formula_9.\n\nLine 13: The index of the last selected activity is updated to the just added activity formula_15.\n\nLet formula_22 be the set of activities ordered by finish time. Assume that formula_23 is an optimal solution, also ordered by finish time; and that the index of the first activity in \"A\" is formula_24, i.e., this optimal solution \"does not\" start with the greedy choice. We will show that formula_25, which begins with the greedy choice (activity 1), is another optimal solution. Since formula_26, and the activities in A are disjoint by definition, the activities in B are also disjoint. Since \"B\" has the same number of activities as \"A\", that is, formula_27, \"B\" is also optimal.\n\nOnce the greedy choice is made, the problem reduces to finding an optimal solution for the subproblem. If \"A\" is an optimal solution to the original problem \"S\", then formula_28 is an optimal solution to the activity-selection problem formula_29.\n\nWhy? If we could find a solution \"B\"′ to \"S\"′ with more activities than \"A\"′, then adding 1 to \"B\"′ would yield a solution \"B\" to \"S\" with more activities than \"A\", contradicting the optimality.\n\nThe generalized version of the activity selection problem involves selecting an optimal set of non-overlapping activities such that the total weight is maximized. Unlike the unweighted version, there is no greedy solution to the weighted activity selection problem. However, a dynamic programming solution can readily be formed using the following approach:\n\nConsider an optimal solution containing activity . We now have non-overlapping activities on the left and right of . We can recursively find solutions for these two sets because of optimal sub-structure. As we don't know , we can try each of the activities. This approach leads to an formula_30 solution. This can be optimized further considering that for each set of activities in formula_31, we can find the optimal solution if we had known the solution for formula_32, where is the last non-overlapping interval with in formula_31. This yields an formula_34 solution. This can be further optimized considering the fact that we do not need to consider all ranges formula_31 but instead just formula_36. The following algorithm thus yields an formula_37 solution:\n\n"}
{"id": "4083815", "url": "https://en.wikipedia.org/wiki?curid=4083815", "title": "Aleksandr Korkin", "text": "Aleksandr Korkin\n\nAleksandr Nikolayevich Korkin (; – ) was a Russian mathematician. He made contribution to the development of partial differential equations, and was second only to Chebyshev among the founders of the Saint Petersburg Mathematical School.\n\n\n"}
{"id": "57517434", "url": "https://en.wikipedia.org/wiki?curid=57517434", "title": "Alessandra Celletti (mathematician)", "text": "Alessandra Celletti (mathematician)\n\nAlessandra Celletti (born 12 February 1962) is an Italian mathematician. She earned a master's degree in mathematics in 1984 at the University of Rome La Sapienza, and a PhD in 1989 at the Swiss Federal Institute of Technology in Zurich (ETH) under the supervision of Jürgen Moser and Jörg Waldvogel. Her research activity concerns dynamical systems, Kolmogorov–Arnold–Moser (KAM) theory, and celestial mechanics. \n\nShe is a founding member of the Italian Society of Celestial Mechanics and Astrodynamics, which she chaired from 2001 to 2013. Since 1993 she has coordinated the international CELMEC meetings. She is full professor of Mathematical Physics at the University of Rome Tor Vergata. \nSince 2010 she has been an honorary member of the \"Celestial Mechanics Institute\". Since 2016 she is editor-in-chief of the journal \"Celestial Mechanics and Dynamical Astronomy\". In 2012 she was an invited speaker at the 6th European Congress of Mathematics. In 2015 she was elected vice-President of the Scientific Committee in Celestial Mechanics of the International Astronomical Union. Since 2012 she has been a member of the Scientific Committee of the European Women in Mathematics (EMS/EWM) and since 2017 she has chaired the Women in Mathematics Committee of the European Mathematical Society.\n\nIn 2007, her book \"Ordine e caos nel sistema solare\" in collaboration with Ettore Perozzi was a finalist for the Galileo Popular Science Prize. \n\nThe asteroid 2005 DJ1 has been named in her honour (117539 Celletti).\n\n\n"}
{"id": "36595677", "url": "https://en.wikipedia.org/wiki?curid=36595677", "title": "Andrews plot", "text": "Andrews plot\n\nIn data visualization, an Andrews plot or Andrews curve is a way to visualize structure in high-dimensional data. It is basically a rolled-down, non-integer version of the Kent–Kiviat radar m chart, or a smoothened version of a parallel coordinate plot.\n\nA value formula_1 is a high-dimensional datapoint if it is an element of formula_2. We can represent high-dimensional data with a number for each of their dimensions, formula_3. To visualize them, the Andrews plot defines a finite Fourier series:\n\nThis function is then plotted for formula_5. Thus each data point may be viewed as a line between formula_6 and formula_7. This formula can be thought of as the projection of the data point onto the vector:\n\nIf there is structure in the data, it may be visible in the Andrews' curves of the data.\n\nThese curves have been utilized in fields as different as biology, neurology, sociology and semiconductor manufacturing. Some of their uses include the quality control of products, the detection of period and outliers in time series, the visualization of learning in artificial neural networks, and correspondence analysis.\n\nTheoretically, it is possible to project them onto an \"n\"-sphere. The projection onto the circle results in the aforementioned Radar chart. This could prove to be a powerful computer-aided multivariable visualization method if, upon projection onto the common tridimensional sphere, the layers are given some degree of transparency on their alpha channel so as to allow to see through.\n\n"}
{"id": "105967", "url": "https://en.wikipedia.org/wiki?curid=105967", "title": "Cayley's theorem", "text": "Cayley's theorem\n\nIn group theory, Cayley's theorem, named in honour of Arthur Cayley, states that every group \"G\" is isomorphic to a subgroup of the symmetric group acting on \"G\". This can be understood as an example of the group action of \"G\" on the elements of \"G\".\n\nA permutation of a set \"G\" is any bijective function taking \"G\" onto \"G\"; and the set of all such functions forms a group under function composition, called \"the symmetric group on\" \"G\", and written as Sym(\"G\").\n\nCayley's theorem puts all groups on the same footing, by considering any group (including infinite groups such as (R,+)) as a permutation group of some underlying set. Thus, theorems that are true for subgroups of permutation groups are true for groups in general. Nevertheless, Alperin and Bell note that \"in general the fact that finite groups are imbedded in symmetric groups has not influenced the methods used to study finite groups\".\n\nThe regular action used in the standard proof of Cayley's theorem does not produce the representation of \"G\" in a \"minimal\"-order permutation group. For example, formula_1, itself already a symmetric group of order 6, would be represented by the regular action as a subgroup of formula_2 (a group of order 720). The problem of finding an embedding of a group in a minimal-order symmetric group is rather more difficult.\n\nWhile it seems elementary enough, it should be noted that at the time, the modern definitions didn't exist, and when Cayley introduced what are now called \"groups\" it wasn't immediately clear that this was equivalent to the previously known groups, which are now called \"permutation groups\". Cayley's theorem unifies the two.\n\nAlthough Burnside \nattributes the theorem \nto Jordan, \nEric Nummela \nnonetheless argues that the standard name—\"Cayley's Theorem\"—is in fact appropriate. Cayley, in his original 1854 paper, \nshowed that the correspondence in the theorem is one-to-one, but he failed to explicitly show it was a homomorphism (and thus an embedding). However, Nummela notes that Cayley made this result known to the mathematical community at the time, thus predating Jordan by 16 years or so.\n\nThe theorem was later published by Walther Dyck in 1882 and is attributed to Dyck in the first edition of Burnside's book.\n\nIf \"g\" is any element of a group \"G\" with operation ∗, consider the function , defined by . By the existence of inverses, this function has a two-sided inverse, formula_3. So multiplication by \"g\" acts as a bijective function. Thus, \"f\" is a permutation of \"G\", and so is a member of Sym(\"G\").\n\nThe set is a subgroup of Sym(\"G\") that is isomorphic to \"G\". The fastest way to establish this is to consider the function with for every \"g\" in \"G\". \"T\" is a group homomorphism because (using · to denote composition in Sym(\"G\")):\n\nfor all \"x\" in \"G\", and hence:\nThe homomorphism \"T\" is injective since (the identity element of Sym(\"G\")) implies that for all \"x\" in \"G\", and taking \"x\" to be the identity element \"e\" of \"G\" yields , i.e. the kernel is trivial. Alternatively, \"T\" is also injective since implies that (because every group is cancellative).\n\nThus \"G\" is isomorphic to the image of \"T\", which is the subgroup \"K\".\n\n\"T\" is sometimes called the \"regular representation of\" \"G\".\n\nAn alternative setting uses the language of group actions. We consider the group formula_6 as a G-set, which can be shown to have permutation representation, say formula_7.\n\nFirstly, suppose formula_8 with formula_9. Then the group action is formula_10 by classification of G-orbits (also known as the orbit-stabilizer theorem).\n\nNow, the representation is faithful if formula_7 is injective, that is, if the kernel of formula_7 is trivial. Suppose formula_13 Then, formula_14 by the equivalence of the permutation representation and the group action. But since formula_15, formula_16 and thus formula_17 is trivial. Then formula_18 and thus the result follows by use of the first isomorphism theorem.\n\nThe identity group element corresponds to the identity permutation. All other group elements correspond to a permutation that does not leave any element unchanged. Since this also applies for powers of a group element, lower than the order of that element, each element corresponds to a permutation that consists of cycles all of the same length: this length is the order of that element. The elements in each cycle form a right coset of the subgroup generated by the element.\n\nZ = {0,1} with addition modulo 2; group element 0 corresponds to the identity permutation e, group element 1 to permutation (12). E.g. 0 +1 = 1 and 1+1 = 0, so 1 -> 0 and 0 -> 1, as they would under a permutation.\n\nZ = {0,1,2} with addition modulo 3; group element 0 corresponds to the identity permutation e, group element 1 to permutation (123), and group element 2 to permutation (132). E.g. 1 + 1 = 2 corresponds to (123)(123)=(132).\n\nZ = {0,1,2,3} with addition modulo 4; the elements correspond to e, (1234), (13)(24), (1432).\n\nThe elements of Klein four-group {e, a, b, c} correspond to e, (12)(34), (13)(24), and (14)(23).\n\nS (dihedral group of order 6) is the group of all permutations of 3 objects, but also a permutation group of the 6 group elements, and the latter is how it is realized by its regular representation.\n\nA more general statement of Cayley's theorem consist of considering the core of an arbitrary group formula_6. In general if formula_6 is a group and formula_21 is a subgroup with formula_22, then formula_23 is isomorphic to a subgroup of formula_24. In particular if formula_6 is a finite group and we set formula_26 then we get the classic result.\n\n"}
{"id": "8088700", "url": "https://en.wikipedia.org/wiki?curid=8088700", "title": "Circular points at infinity", "text": "Circular points at infinity\n\nIn projective geometry, the circular points at infinity (also called cyclic points or isotropic points) are two special points at infinity in the complex projective plane that are contained in the complexification of every real circle.\n\nA point of the complex projective plane may be described in terms of homogeneous coordinates, being a triple of complex numbers , where two triples describe the same point of the plane when the coordinates of one triple are the same as those of the other aside from being multiplied by the same nonzero factor. In this system, the points at infinity may be chosen as those whose \"z\"-coordinate is zero. The two circular points at infinity are two of these, usually taken to be those with homogeneous coordinates\n\nA real circle, defined by its center point (\"x\",\"y\") and radius \"r\" (all three of which are real numbers) may be described as the set of real solutions to the equation\nConverting this into a homogeneous equation and taking the set of all complex-number solutions gives the complexification of the circle. The two circular points have their name because they lie on the complexification of every real circle. More generally, both points satisfy the homogeneous equations of the type \nThe case where the coefficients are all real gives the equation of a general circle (of the real projective plane). In general, an algebraic curve that passes through these two points is called circular.\n\nThe circular points at infinity are the points at infinity of the isotropic lines.\nThey are invariant under translations and rotations of the plane.\n\nThe concept of angle can be defined using the circular points, natural logarithm and cross-ratio:\nSommerville configures two lines on the origin as formula_3 Denoting the circular points as \"ω\" and ω\"′, he obtains the cross ratio\n\n"}
{"id": "17268791", "url": "https://en.wikipedia.org/wiki?curid=17268791", "title": "Competitive altruism", "text": "Competitive altruism\n\nCompetitive altruism is a possible mechanism for the persistence of cooperative behaviors, specifically those that are performed unconditionally. The theory of reciprocal altruism can be used to explain behaviors that are performed by a donor who receives some sort of benefit in the future. When no such compensation is received, however, reciprocity fails to explain altruistic behavior.\n\nTo explain competitive altruism, Roberts uses the example of preening among birds. Because certain birds cannot reach parasites on all parts of their bodies, particularly their necks, they benefit from preening one another. For any given bird, there is an entire flock of potential preeners, who compete in hopes of establishing a beneficial relationship. Cheaters, or those birds that try to be preened without preening others, do not compete and thus are excluded from these relationships. Their fitness is lowered because they are ostracized by members of the flock.\n\nMcNamara et al. quantitatively analyzed this theory. Like Robert Axelrod, they created a computer program to simulate repeated interactions among individuals. The program involved players with two genetically determined traits, a ‘cooperative trait’ and a ‘choosiness trait.’ They found the following results:\n‘Paradoxical’ trait combinations yield particularly low payoffs: individuals with low choosiness but high effort tend to get exploited by their co-players; individuals with high choosiness but low effort waste their time searching for better co-players, which are, however, unlikely to accept them. The positive correlation between choosiness and cooperativeness leads to a positive assortment between cooperative types – an essential feature of all mechanisms that promote cooperation.\n\nThe development of such cooperation requires variation in the degree of cooperation and choosiness, which the researchers attributed to genetic mutation and variation. McNamara et al. also determined that since a period of searching is required for ‘mutually acceptable’ players to find one another, competitive altruism is more likely to arise in animals with long life spans.\n\nTo relate this condition to the prisoner's dilemma, an individual may benefit the most in a one-time interaction with another by defecting (i.e. receiving benefits without incurring any cost to itself). However, in an iterated prisoner's dilemma, where individuals interact more than once, if the act of defecting makes the individual less likely to attract a fit mate in the future, then cooperative behavior will be selected for.\n\nThis selection for cooperation is even stronger if an individual's action in an interaction is observed by third-party individuals, for the possibility of forming a reputation arises. Amotz Zahavi, famous for his work with the altruistic Arabian babbler, suggests that this level of \"social prestige\" will affect which individuals interact with one another and how they behave. \n\nCompetitive altruism has been demonstrated repeatedly in studies with humans. For instance, individuals are more generous when their behaviour is visible to others and altruistic individuals receive more social status and are selectively preferred as collaboration partners and group leaders... Adding insights from sexual selection theory research has also found that men behave more altruistically in the presence of an (attractive) female and altruistic males are selectively preferred as long-term sexual partners.\n\nThe theory of competitive altruism also helps one connect such behavior to the handicap principle. With competitive altruism, cooperation is considered a trait that provides a signaling benefit, and thus is subject to sexual selection. Like a peacock's tail, cooperation persists and is magnified, even though it carries a cost to the individual. Cooperation must be significantly costly to the individual, such that only a limited proportion of the population is fit enough to partake.\n\nRoberts builds on the idea of altruism as a signaling benefit with his \"free gift theory\". Because the recipient gains some benefit from the interaction with the donor, there is an incentive to pay attention to the signal. For example, some male birds will offer food to a potential mate. Such behavior, called courtship feeding, not only benefits the female, who receives a meal without expending any energy, but also conveys the ability of the male to forage. Consequently, the signal is kept true (i.e. it remains a correct reflection on the fitness of the mate).\n\nHowever, the connection between competitive altruism and signaling is not without criticism. Wright raises the point that an altruistic signaling behavior like gift giving would cause a \"flow of fitness from the higher quality individual to the lower quality one\" and reduce the veracity of the signal. To account for this likely trend, Wright stipulates that the altruistic behavior must be directed at a mate or ally. In order for the theory to hold true, the signaling benefit would have to be shown to improve the individual's fitness beyond the benefit gained from the \"investment\" in the partner.\n\n"}
{"id": "549445", "url": "https://en.wikipedia.org/wiki?curid=549445", "title": "Completeness (order theory)", "text": "Completeness (order theory)\n\nIn the mathematical area of order theory, completeness properties assert the existence of certain infima or suprema of a given partially ordered set (poset). The most familiar example is the completeness of the real numbers. A special use of the term refers to complete partial orders or complete lattices. However, many other interesting notions of completeness exist.\n\nThe motivation for considering completeness properties derives from the great importance of suprema (least upper bounds, joins, \"formula_1\") and infima (greatest lower bounds, meets, \"formula_2\") to the theory of partial orders. Finding a supremum means to single out one distinguished least element from the set of upper bounds. On the one hand, these special elements often embody certain concrete properties that are interesting for the given application (such as being the least common multiple of a set of numbers or the union of a collection of sets). On the other hand, the knowledge that certain types of subsets are guaranteed to have suprema or infima enables us to consider the computation of these elements as \"total operations\" on a partially ordered set. For this reason, posets with certain completeness properties can often be described as algebraic structures of a certain kind. In addition, studying the properties of the newly obtained operations yields further interesting subjects.\n\nAll completeness properties are described along a similar scheme: one describes a certain class of subsets of a partially ordered set that are required to have a supremum or infimum. Hence every completeness property has its dual, obtained by inverting the order-dependent definitions in the given statement. Some of the notions are usually not dualized while others may be self-dual (i.e. equivalent to their dual statements).\n\nThe easiest example of a supremum is the empty one, i.e. the supremum of the empty set. By definition, this is the least element among all elements that are greater than each member of the empty set. But this is just the least element of the whole poset, if it has one, since the empty subset of a poset P is conventionally considered to be both bounded from above and from below, with every element of P being both an upper and lower bound of the empty subset. Other common names for the least element are bottom and zero (0). The dual notion, the empty lower bound, is the greatest element, top, or unit (1).\n\nPosets that have a bottom are sometimes called pointed, while posets with a top are called unital or topped. An order that has both a least and a greatest element is bounded. However, this should not be confused with the notion of \"bounded completeness\" given below.\n\nFurther simple completeness conditions arise from the consideration of all non-empty finite sets. An order in which all non-empty finite sets have both a supremum and an infimum is called a lattice. It suffices to require that all suprema and infima of \"two\" elements exist to obtain all non-empty finite ones. A straightforward induction shows that every finite non-empty supremum/infimum can be decomposed into a finite number of binary suprema/infima. Thus the central operations of lattices are binary suprema formula_1 and infima It is in this context that the terms meet for formula_2 and join for formula_1 are most common.\n\nA poset in which only non-empty finite suprema are known to exist is therefore called a join-semilattice. The dual notion is meet-semilattice.\n\nThe strongest form of completeness is the existence of all suprema and all infima. The posets with this property are the complete lattices. However, using the given order, one can restrict to further classes of (possibly infinite) subsets, that do not yield this strong completeness at once.\n\nIf all directed subsets of a poset have a supremum, then the order is a directed-complete partial order (dcpo). These are especially important in domain theory. The seldom-considered dual notion of a dcpo is the filtered-complete poset. Dcpos with a least element (\"pointed dcpos\") are one of the possible meanings of the phrase complete partial order (cpo).\n\nIf every subset that has \"some\" upper bounds has also a least upper bound, then the respective poset is called bounded complete. The term is used widely with this definition that focuses on suprema and there is no common name for the dual property. However, bounded completeness can be expressed in terms of other completeness conditions that are easily dualized (see below). Although concepts with the names \"complete\" and \"bounded\" were already defined, confusion is unlikely to occur since one would rarely speak of a \"bounded complete poset\" when meaning a \"bounded cpo\" (which is just a \"cpo with greatest element\"). Likewise, \"bounded complete lattice\" is almost unambiguous, since one would not state the boundedness property for complete lattices, where it is implied anyway. Also note that the empty set usually has upper bounds (if the poset is non-empty) and thus a bounded-complete poset has a least element.\n\nOne may also consider the subsets of a poset which are totally ordered, i.e. the chains. If all chains have a supremum, the order is called chain complete. Again, this concept is rarely needed in the dual form.\n\nIt was already observed that binary meets/joins yield all non-empty finite meets/joins. Likewise, many other (combinations) of the above conditions are equivalent.\n\n\nAs explained above, the presence of certain completeness conditions allows to regard the formation of certain suprema and infima as total operations of an ordered set. It turns out that in many cases it is possible to characterize completeness solely by considering appropriate algebraic structures in the sense of universal algebra, which are equipped with operations like formula_1 or formula_2. By imposing additional conditions (in form of suitable identities) on these operations, one can then indeed derive the underlying partial order exclusively from such algebraic structures. Details on this characterization can be found in the articles on the \"lattice-like\" structures for which this is typically considered: see semilattice, lattice, Heyting algebra, and Boolean algebra. Note that the latter two structures extend the application of these principles beyond mere completeness requirements by introducing an additional operation of \"negation\".\n\nAnother interesting way to characterize completeness properties is provided through the concept of (monotone) Galois connections, i.e. adjunctions between partial orders. In fact this approach offers additional insights both in the nature of many completeness properties and in the importance of Galois connections for order theory. The general observation on which this reformulation of completeness is based is that the construction of certain suprema or infima provides left or right adjoint parts of suitable Galois connections.\n\nConsider a partially ordered set (\"X\", ≤). As a first simple example, let 1 = {*} be a specified one-element set with the only possible partial ordering. There is an obvious mapping \"j\": \"X\" → 1 with \"j\"(\"x\") = * for all \"x\" in \"X\". \"X\" has a least element if and only if the function \"j\" has a lower adjoint \"j\": 1 → \"X\". Indeed the definition for Galois connections yields that in this case \"j\"(*) ≤ \"x\" if and only if * ≤ \"j\"(\"x\"), where the right hand side obviously holds for any \"x\". Dually, the existence of an upper adjoint for \"j\" is equivalent to \"X\" having a greatest element.\n\nAnother simple mapping is the function \"q\": \"X\" → (\"X\" x \"X\") given by \"q\"(\"x\") = (\"x\", \"x\"). Naturally, the intended ordering relation for (\"X\" x \"X\") is just the usual product order. \"q\" has a lower adjoint \"q\" if and only if all binary joins in \"X\" exist. Conversely, the join operation formula_1: (\"X\" x \"X\") → \"X\" can always provide the (necessarily unique) lower adjoint for \"q\". Dually, \"q\" allows for an upper adjoint if and only if \"X\" has all binary meets. Thus the meet operation formula_2, if it exists, always is an upper adjoint. If both formula_1 and formula_2 exist and, in addition, formula_2 is also a lower adjoint, then the poset \"X\" is a Heyting algebra—another important special class of partial orders.\n\nFurther completeness statements can be obtained by exploiting suitable completion procedures. For example, it is well known that the collection of all lower sets of a poset \"X\", ordered by subset inclusion, yields a complete lattice D(\"X\") (the downset-lattice). Furthermore, there is an obvious embedding \"e\": \"X\" → D(\"X\") that maps each element \"x\" of \"X\" to its principal ideal {\"y\" in \"X\" | \"y\" ≤ \"x\"}. A little reflection now shows that \"e\" has a lower adjoint if and only if \"X\" is a complete lattice. In fact, this lower adjoint will map any lower set of \"X\" to its supremum in \"X\". Composing this with the function that maps any subset of \"X\" to its lower closure (again an adjunction for the inclusion of lower sets in the powerset), one obtains the usual supremum map from the powerset 2 to \"X\". As before, another important situation occurs whenever this supremum map is also an upper adjoint: in this case the complete lattice \"X\" is \"constructively completely distributive\". See also the articles on complete distributivity and distributivity (order theory).\n\nThe considerations in this section suggest a reformulation of (parts of) order theory in terms of category theory, where properties are usually expressed by referring to the relationships (morphisms, more specifically: adjunctions) between objects, instead of considering their internal structure. For more detailed considerations of this relationship see the article on the categorical formulation of order theory.\n\n\n"}
{"id": "51024643", "url": "https://en.wikipedia.org/wiki?curid=51024643", "title": "Curve complex", "text": "Curve complex\n\nIn mathematics, the curve complex is a simplicial complex \"C\"(\"S\") associated to a finite type surface S, which encodes the combinatorics of simple closed curves on \"S\". The curve complex turned out to be a fundamental tool in the study of the geometry of the Teichmüller space, of mapping class groups and of Kleinian groups.\n\nLet formula_1 be a finite type connected oriented surface. More specifically, let formula_2 be a connected oriented surface of genus formula_3 with formula_4 boundary components and formula_5 punctures.\n\nThe \"curve complex\" formula_6 is the simplicial complex defined as follows:\n\n\nFor surfaces of small complexity (essentially the torus, punctured torus, and four-holed sphere), with the definition above the curve complex has infinitely many connected components. One can give an alternate and more useful definition by joining vertices if the corresponding curves have minimal intersection number. With this alternate definition, the resulting complex is isomorphic to the Farey graph.\n\nIf formula_1 is a compact surface of genus formula_11 with formula_12 boundary components the dimension of formula_6 is equal to formula_14. In what follows, we will assume that formula_15. The complex of curves is never locally finite (i.e. every vertex has infinitely many neighbors). A result of Harer asserts that formula_6 is in fact it is homotopically equivalent to a wedge sum of spheres.\n\nThe combinatorial distance on the 1-skeleton of formula_6 is related to the intersection number between simple closed curves on a surface, which is the smallest number of intersections of two curves in the isotopy classes. For example\n\nfor any two nondisjoint simple closed curves formula_19. One can compare in the other direction but the results are much more subtle (for example there is no uniform lower bound even for a given surface) and harder to prove.\n\nIt was proved by Masur and Minsky that the complex of curves is a Gromov hyperbolic space. Later work by various authors gave alternate proofs of this fact and better information on the hyperbolicity.\n\nThe mapping class group of formula_1 acts on the complex formula_6 in the natural way: it acts on the vertices by formula_22 and this extends to an action on the full complex. This action allows to prove many interesting properties of the mapping class groups.\n\nWhile the mapping class group itself is not an hyperbolic group, the fact that formula_6 is hyperbolic still has implications for its structure and geometry.\n\nThere is a natural map from Teichmüller space to the curve complex, which takes a marked hyperbolic structures to the collection of closed curves realising the smallest possible length (the systole). It allows to read off certain geometric properties of the latter, in particular it explains the empirical fact that while Teichmüller space itself is not hyperbolic it retains certain features of hyperbolicity.\n\nA simplex in formula_6 determines a \"filling\" of formula_1 to a handlebody. Choosing two simplices in formula_6 thus determines a Heegaard splitting of a three-manifold, with the additional data of an Heegaard diagram (a maximal system of disjoint simple closed curves bounding disks for each of the two handlebodies). Some properties of Heegaard splittings can be read very efficiently off the relative positions of the simplices:\n\n\nIn general the minimal distance between simplices representing diagram for the splitting can give information on the topology and geometry (in the sense of the geometrisation conjecture of the manifold) and vice versa. A guiding principle is that the minimal distance of a Heegaard splitting is a measure of the complexity of the manifold.\n\nAs a special case of the philosophy of the previous paragraph, the geometry of the curve complex is an important tool to link combinatorial and geometric propertoes of hyperbolic 3-manifolds, and hence it is a useful tool in the study of Kleinian groups. For example, it has been used in the proof of the ending lamination conjecture.\n\nA possible model for random 3-manifolds is to take random Heegaard splittings. The proof that this model is hyperbolic almost surely (in a certain sense) uses the geometry of the complex of curves.\n\n"}
{"id": "57995304", "url": "https://en.wikipedia.org/wiki?curid=57995304", "title": "Dmitri Burago", "text": "Dmitri Burago\n\nDmitri Yurievich Burago (Дмитрий Юрьевич Бураго, born 1964) is a Russian mathematician, specializing in geometry.\n\nHe is the son of the professor of mathematics in Leningrad Yuri Dmitrievich Burago, with whom he also published a book. Burago received his doctorate in 1994 at Saint Petersburg State University under the supervision of Anatoly Vershik. He was at the Steklov Institute in Saint Petersburg and is now a professor at Pennsylvania State University's Center for Dynamical Systems and Geometry.\n\nIn 1992 he was awarded the prize of the Saint Petersburg Mathematical Society. In 1998 he was an Invited Speaker at the International Congress of Mathematicians in Berlin. In 2014 he was awarded the Leroy P. Steele Prize with Yuri Burago and Sergei Vladimirovich Ivanov for their book \"A course in metric geometry\".\n\n\n\n"}
{"id": "11767545", "url": "https://en.wikipedia.org/wiki?curid=11767545", "title": "Ebenezer Cunningham", "text": "Ebenezer Cunningham\n\nEbenezer Cunningham (7 May 1881, Hackney, London – 12 February 1977)\nwas a British mathematician who is remembered for his research and exposition at the dawn of special relativity.\n\nCunningham went up to St John's College, Cambridge in 1899 and graduated Senior Wrangler in 1902, winning the Smith's Prize in 1904.\n\nIn 1904, as a lecturer at the University of Liverpool, he began work on a new theorem in relativity with fellow lecturer Harry Bateman. They brought the methods of inversive geometry into electromagnetic theory with their transformations (spherical wave transformation):\n\nHe worked with Karl Pearson in 1907 at University College London.\nCunningham married Ada Collins in 1908.\n\nIn August 1911 he returned to St John's College where he made his career. When drafted for the war in 1915 he did alternative service growing food and in an office at the YMCA. He held a university lectureship from 1926 to 1946.\n\nHis book \"The Principle of Relativity\" (1914) was one of the first treatises in English about special relativity, along with those by Alfred Robb and Ludwik Silberstein. He followed with \"Relativity and the Electron Theory\" (1915) and \"Relativity, Electron Theory and Gravitation\" (1921). McCrea writes that Cunningham had doubts whether general relativity produced \"physical results adequate return for mathematical elaboration.\"\n\nHe was an ardent pacifist, strongly religious, a member of Emmanuel United Reformed Church, Cambridge and chairman of the Congregational Union of England and Wales for 1953–54.\n\n\n\n\n"}
{"id": "57183494", "url": "https://en.wikipedia.org/wiki?curid=57183494", "title": "Elie Cartan Prize", "text": "Elie Cartan Prize\n\nThe Élie Cartan Prize (Prix Élie Cartan) is awarded every three years by the Institut de France, Academie des Sciences, Fondation Élie Cartan, to recognize a mathematician who has introduced new ideas or solved a difficult problem. The prize, named for mathematician Élie Cartan, was established in 1980 and carries a monetary award.\n\nThe recipients of the Élie Cartan Prize are:\n\n"}
{"id": "36854652", "url": "https://en.wikipedia.org/wiki?curid=36854652", "title": "Eulerian matroid", "text": "Eulerian matroid\n\nIn matroid theory, an Eulerian matroid is a matroid whose elements can be partitioned into a collection of disjoint circuits.\n\nIn a uniform matroid formula_1, the circuits are the sets of exactly formula_2 elements. Therefore, a uniform matroid is Eulerian if and only if formula_2 is a divisor of formula_4. For instance, the formula_4-point lines formula_6 are Eulerian if and only if formula_4 is divisible by three.\n\nThe Fano plane has two kinds of circuits: sets of three collinear points, and sets of four points that do not contain any line. The three-point circuits are the complements of the four-point circuits, so it is possible to partition the seven points of the plane into two circuits, one of each kind. Thus, the Fano plane is also Eulerian.\n\nEulerian matroids were defined by as a generalization of the Eulerian graphs, graphs in which every vertex has even degree. By Veblen's theorem the edges of every such graph may be partitioned into simple cycles, from which it follows that the graphic matroids of Eulerian graphs are examples of Eulerian matroids.\n\nThe definition of an Eulerian graph above allows graphs that are disconnected, so not every such graph has an Euler tour. observes that the graphs that have Euler tours can be characterized in an alternative way that generalizes to matroids: a graph formula_8 has an Euler tour if and only if it can be formed from some other graph formula_9, and a cycle formula_10 in formula_9, by contracting the edges of formula_9 that do not belong to formula_10. In the contracted graph, formula_10 generally stops being a simple cycle and becomes instead an Euler tour. Analogously, Wilde considers the matroids that can be formed from a larger matroid by contracting the elements that do not belong to some particular circuit. He shows that this property is trivial for general matroids (it implies only that each element belongs to at least one circuit) but can be used to characterize the Eulerian matroids among the binary matroids, matroids representable over GF(2):\na binary matroid is Eulerian if and only if it is the contraction of another binary matroid onto a circuit.\n\nFor planar graphs, the properties of being Eulerian and bipartite are dual: a planar graph is Eulerian if and only if its dual graph is bipartite. As Welsh showed, this duality extends to binary matroids: a binary matroid is Eulerian if and only if its dual matroid is a bipartite matroid, a matroid in which every circuit has even cardinality.\n\nFor matroids that are not binary, the duality between Eulerian and bipartite matroids may break down. For instance, the uniform matroid formula_15 is Eulerian but its dual formula_16 is not bipartite, as its circuits have size five. The self-dual uniform matroid formula_17 is bipartite but not Eulerian.\n\nBecause of the correspondence between Eulerian and bipartite matroids among the binary matroids, the binary matroids that are Eulerian may be characterized in alternative ways. The characterization of is one example; two more are that a binary matroid is Eulerian if and only if every element belongs to an odd number of circuits, if and only if the whole matroid has an odd number of partitions into circuits. collect several additional characterizations of Eulerian binary matroids, from which they derive a polynomial time algorithm for testing whether a binary matroid is Eulerian.\n\nAny algorithm that tests whether a given matroid is Eulerian, given access to the matroid via an independence oracle, must perform an exponential number of oracle queries, and therefore cannot take polynomial time.\n\nIf formula_18 is a binary matroid that is not Eulerian, then it has a unique Eulerian extension, a binary matroid formula_19 whose elements are the elements of formula_18 together with one additional element formula_21, such that the restriction of formula_19 to the elements of formula_18 is isomorphic to formula_18. The dual of formula_19 is a bipartite matroid formed from the dual of formula_18 by adding formula_21 to every odd circuit.\n"}
{"id": "9712439", "url": "https://en.wikipedia.org/wiki?curid=9712439", "title": "Filled Julia set", "text": "Filled Julia set\n\nThe filled-in Julia set formula_1 of a polynomial formula_2 is :\n\nThe filled-in Julia set formula_1 of a polynomial formula_2 is defined as the set of all points formula_5 of the dynamical plane that have bounded orbit with respect to formula_2\n\nformula_7\n<br>\nwhere :\n\nformula_8 is the set of complex numbers\n\nformula_9 is the formula_10 -fold composition of formula_11 with itself = iteration of function formula_11\n\nThe filled-in Julia set is the (absolute) complement of the attractive basin of infinity. \nformula_13\n\nThe attractive basin of infinity is one of the components of the Fatou set.\nformula_14\n\nIn other words, the filled-in Julia set is the complement of the unbounded Fatou component: \nformula_15\n\nThe Julia set is the common boundary of the filled-in Julia set and the attractive basin of infinity <br>\n\nformula_16\n\nwhere : \nformula_17 denotes the attractive basin of infinity = exterior of filled-in Julia set = set of escaping points for formula_18\n\nformula_19\n\nIf the filled-in Julia set has no interior then the Julia set coincides with the filled-in Julia set. This happens when all the critical points of formula_18 are pre-periodic. Such critical points are often called Misiurewicz points.\n\nThe most studied polynomials are probably those of the form formula_21, which are often denoted by formula_22, where formula_23 is any complex number. In this case, the spine formula_24 of the filled Julia set formula_25 is defined as arc between formula_26-fixed point and formula_27,\n\nformula_28\n\nwith such properties:\n\nAlgorithms for constructing the spine:\n\nCurve formula_41 :\n\nformula_42\n\ndivides dynamical plane into two components.\n\n\n"}
{"id": "1731731", "url": "https://en.wikipedia.org/wiki?curid=1731731", "title": "Frank Harary", "text": "Frank Harary\n\nFrank Harary (March 11, 1921 – January 4, 2005) was an American mathematician, who specialized in graph theory. He was widely recognized as one of the \"fathers\" of modern graph theory.\nHarary was a master of clear exposition and, together with his many doctoral students, he standardized the terminology of graphs. He broadened the reach of this field to include physics, psychology, sociology, and even anthropology. Gifted with a keen sense of humor, Harary challenged and entertained audiences at all levels of mathematical sophistication. A particular trick he employed was to turn theorems into games - for instance, students would try to add red edges to a graph on six vertices in order to create a red triangle, while another group of students tried to add edges to create a blue triangle (and each edge of the graph had to be either blue or red). Because of the theorem on friends and strangers, one team or the other would have to win.\n\nFrank Harary was born in New York City, the oldest child to a family of Jewish immigrants from Syria and Morocco. He earned his bachelor's and master's degrees from Brooklyn College in 1941 and 1945 respectively and his Ph.D., with supervisor Alfred L. Foster, from University of California at Berkeley in 1948.\n\nPrior to his teaching career he became a research assistant in the Institute of Social Research at the University of Michigan.\n\nHarary's first publication, \"Atomic Boolean-like rings with finite radical\", went through much effort to be put into the \"Duke Mathematical Journal\" in 1950. This article was first submitted to the American Mathematical Society in November 1948, then sent to the \"Duke Mathematical Journal\" where it was revised three times before it was finally published two years after its initial submission. Harary began his teaching career at the University of Michigan in 1953 where he was first an assistant professor, then in 1959 associate professor and in 1964 was appointed as a professor of mathematics, a position he held until 1986.\n\nFrom 1987 he was Professor (and Distinguished Professor Emeritus) in the Computer Science Department at New Mexico State University in Las Cruces. He was one of the founders of the \"Journal of Combinatorial Theory\" and the \"Journal of Graph Theory\".\n\nIn 1949 Harary published \"On the algebraic structure of knots\". Shortly after this publication in 1953 Harary published his first book (jointly with George Uhlenbeck) \"On the number of Husimi trees\". It was following this text that Harary began to build up a worldwide reputation for his work in graph theory. In 1965 Harary's first book \"Structural models: An introduction to the theory of directed graphs\" was published, and for the rest of his life Harary's interest would be in the field of Graph Theory.\n\nWhile beginning his work in graph theory around 1965, Harary began buying up property in Ann Arbor to supplement income for his family. Harary and his wife Jayne had six children together, Miriam, Natalie, Judith, Thomas, Joel and Chaya.\n\nFrom 1973 to 2007 Harary jointly wrote five more books, each in the field of graph theory. In the time before his death, Harary traveled the world researching and publishing over 800 papers (with some 300 different co-authors), in mathematical journals and other scientific publications, more than any mathematician other than George Pólya. Harary recorded that he lectured in 166 different cities around the United States and some 274 cities in over 80 different countries. Harary was particularly proud that he had given lectures in cities around the world beginning with every letter of the alphabet, even including \"X\" when he traveled to Xanten, Germany. Harary also played a curious role in the award-winning film Good Will Hunting. The film displayed formulas he had published on the enumeration of trees, which were supposed to be fiendishly difficult.\n\nIt was in 1986 at the age of 65 that Harary retired from his professorship at the University of Michigan. Harary did not take his retirement lightly however, following his retirement Harary was appointed as a \"Distinguished Professor of Computer Sciences\" at New Mexico State University in Las Cruces. He held this position until his death in 2005. The same year as his retirement Harary was made an honorary fellow of the National Academy of Sciences of India, he also served as an editor for about 20 different journals focusing primarily on graph theory and combinatorial theory. It was following his retirement that Harary was elected as an honorary lifetime member of the Calcutta Mathematical Society and of the South African Mathematical Society.\n\nHe died at Memorial Medical Center in Las Cruces, New Mexico. At the time of his death in Las Cruces other members of the department of Computer Science felt the loss for the great mind that once worked beside them. The head of the department of Computer Science at the time of Harary's death Desh Ranjan had this to say, \"Dr. Harary was a true scholar with a genuine love for graph theory which was an endless source of new discoveries, beauty, curiosity, surprises and joy for him till the very end of his life.\"\n\nHarary's work in graph theory was diverse. Some topics of great interest to him were:\n\n\nAmong over 700 scholarly articles Harary wrote, two were co-authored with Paul Erdős, giving Harary an Erdős number of 1. He lectured extensively and kept alphabetical lists of the cities where he spoke.\n\nHarary's most famous classic book \"Graph Theory\" was published in 1969 and offered a practical introduction to the field of graph theory. It is evident that Harary's focus in this book and amongst his other publications was towards the varied and diverse application of graph theory to other fields of mathematics, physics and many others. Taken from the preface of \"Graph Theory,\" Harary notes ...\n\n\"...there are applications of graph theory to some areas of physics, chemistry, communication science, computer technology, electrical and civil engineering, architecture, operational research, genetics, psychology, sociology, economics, anthropology, and linguistics.\"\n\nHarary quickly began promoting inquiry based learning through his texts, apparent by his reference to the tradition of the Moore method. Harary made many unique contributions to graph theory as he explored more and more different fields of study and successfully attempted to relate them to graph theory. Harary's classic book \"Graph Theory\" begins by providing the reader with much of the requisite knowledge of basic graphs and then dives right into proving the diversity of content that is held within graph theory. Some of the other mathematical fields that Harary directly relates to graph theory in his book begin to appear around chapter 13, these topics include linear algebra, and abstract algebra.\n\nOne motivation for the study of graph theory is its application to sociograms described by Jacob L. Moreno. For instance the adjacency matrix of a sociogram was used by Leon Festinger. Festinger identified the graph theory clique with the social clique and examined the diagonal of the cube of a groups’ adjacency matrix to detect cliques. Harary joined with Ian Ross to improve on Festinger's clique detection.\n\nThe admission of powers of an adjacency matrix led Harary and Ross to note that a complete graph can be obtained from the square of an adjacency matrix of a tree. Relying on their study of clique detection, they described a class of graphs for which the adjacency matrix is the square of the adjacency matrix of a tree.\n\n\n\nOnce we have the tree in question we can create an adjacency matrix for the tree T and check that it is indeed to correct tree which we sought. Squaring the adjacency matrix of T should yield an adjacency matrix for a graph which is isomorphic to the graph G which we started with. Probably the simplest way to observe this theorem in action is to observe the case which Harary mentions in \"The Square of a Tree.\" Specifically the example in question describes the tree corresponding the graph of K\n\n\"Consider the tree consisting of one point joined with all the others. When the tree is squared, the result is the complete graph. We wish to illustrate... Tformula_1K\"\n\nUpon squaring of the adjacency matrix of the previously mentioned tree, we can observe that this theorem does in fact hold true. We can also observe that this pattern of setting up a tree where \"one point joined with all the others\" will always indeed yield the correct tree for all complete graphs.\n\n\n"}
{"id": "6905757", "url": "https://en.wikipedia.org/wiki?curid=6905757", "title": "Generative systems", "text": "Generative systems\n\nGenerative systems are technologies with the overall capacity to produce unprompted change driven by large, varied, and uncoordinated audiences. When generative systems provide a common platform, changes may occur at varying layers (physical, network, application, content) and provide a means through which different firms and individuals may cooperate indirectly and contribute to innovation.\n\nDepending on the rules, the patterns can be extremely varied and unpredictable. One of the more well-known examples is Conway's Game of Life, a cellular automaton. Another example is Boids. More examples can be found in generative music, generative art, and, more recently, in video games such as Spore.\n\n\n"}
{"id": "40648385", "url": "https://en.wikipedia.org/wiki?curid=40648385", "title": "Geraldine Claudette Darden", "text": "Geraldine Claudette Darden\n\nGeraldine Claudette Darden (born July 22, 1936) is an American mathematician. She was the fourteenth African American woman to earn a Ph.D. in mathematics. She was born in Nansemond County, Virginia. Darden earned her degrees in mathematics from Hampton Institute (B.S., 1957) an historically black institute; an M.S. (1960) University of Illinois at Urbana–Champaign, and her M.S. (1965) and a Ph.D. (1967) from Syracuse University. Her dissertation was completed under the auspices of James Reid, Ph.D., entitled, \"On the Direct Sums of Cyclic Groups\".\n\nInitially Darden taught at S.H. Clarke Junior High School in Portsmouth, Virginia, immediately after receiving her undergraduate degree in 1957. In the summer of 1958, Darden saw an opportunity for aspiring mathematicians created by the launch of Russian satellite Sputnik and ensuing US interest in mathematics and science a year earlier, and she applied for and received a National Science Foundation grant to attend the Summer Institute in Mathematics held at North Carolina Central University. Here she met Marjorie Lee Browne, the mathematician who directed the Institute, who would encourage Darden to go on to graduate school at Syracuse.\n\nIn addition to teaching, Darden also co-wrote selected papers on pre-calculus, with textbook author Tom Apostol, Gulbank D. Chakerian, and John D. Neff. Reprinted from the \"American Mathematical Monthly\" (vols. 1--81) and from the \"Mathematics Magazine\" (vols. 1--49). The Raymond W. Brink Selected Mathematical Papers, Vol. 1. The Mathematical Association of America, Washington, D.C., 1977. pp. xvii+469, \n\n"}
{"id": "7203237", "url": "https://en.wikipedia.org/wiki?curid=7203237", "title": "H. F. Baker", "text": "H. F. Baker\n\nHenry Frederick Baker FRS FRSE (3 July 1866 – 17 March 1956) was a British mathematician, working mainly in algebraic geometry, but also remembered for contributions to partial differential equations (related to what would become known as solitons), and Lie groups.\n\nHe was born in Cambridge the son of Henty Baker, a butler, and Sarah Ann Britham.\n\nHe was educated at The Perse School before winning a scholarship to St John's College, Cambridge in October 1884. Baker graduated as Senior Wrangler in 1887, bracketed with 3 others.\n\nBaker was elected Fellow of St John's in 1888 where he remained for 68 years.\n\nIn June, 1898 he was elected a Fellow of the Royal Society. In 1911, he gave the presidential address to the London Mathematical Society.\n\nIn January 1914 he was appointed Lowndean Professor of Astronomy.\n\nGordon Welchman recalled that in the 1930s before the war Dennis Babbage and he were members of a group of geometers known as Professor Baker’s \"Tea Party\", who met once a week to discuss the areas of research in which we were all interested\". .\n\nHe married twice. Firstly in 1893 to Lilly Isabella Hamfield Klopp, who died in 1903, then he remarried in 1913, to Muriel Irene Woodyard.\n\nHe died in Cambridge and is buried at the Parish of the Ascension Burial Ground, with his second wife Muriel (1885 - 1956).\n\n\n"}
{"id": "30475329", "url": "https://en.wikipedia.org/wiki?curid=30475329", "title": "Hilbert's inequality", "text": "Hilbert's inequality\n\nIn analysis, a branch of mathematics, Hilbert's inequality states that\n\nfor any sequence \"u\",\"u\"... of complex numbers. It was first demonstrated by David Hilbert with the constant 2 instead of ; the sharp constant was found by Issai Schur. It implies that the discrete Hilbert transform is a bounded operator in \"ℓ\".\n\nLet (\"u\") be a sequence of complex numbers. If the sequence is infinite, assume that it is square-summable:\n\nHilbert's inequality (see ) asserts that\n\nIn 1973, Montgomery & Vaughan reported several generalizations of Hilbert's inequality, considering the bilinear forms\n\nand\n\nwhere \"x\",\"x\"...,\"x\" are distinct real numbers modulo 1 (i.e. they belong to distinct classes in the quotient group R/Z) and \"λ\"...,\"λ\" are distinct real numbers. Montgomery & Vaughan's generalizations of Hilbert's inequality are then given by\n\nand\n\nwhere\n\nis the distance from \"s\" to the nearest integer, and min denotes the smallest positive value. Moreover, if\n\nthen the following inequalities hold:\n\nand\n\n"}
{"id": "607286", "url": "https://en.wikipedia.org/wiki?curid=607286", "title": "Hilbert's program", "text": "Hilbert's program\n\nIn mathematics, Hilbert's program, formulated by German mathematician David Hilbert in the early part of the 20th century, was a proposed solution to the foundational crisis of mathematics, when early attempts to clarify the foundations of mathematics were found to suffer from paradoxes and inconsistencies. As a solution, Hilbert proposed to ground all existing theories to a finite, complete set of axioms, and provide a proof that these axioms were consistent. Hilbert proposed that the consistency of more complicated systems, such as real analysis, could be proven in terms of simpler systems. Ultimately, the consistency of all of mathematics could be reduced to basic arithmetic.\n\nGödel's incompleteness theorems, published in 1931, showed that Hilbert's program was unattainable for key areas of mathematics. In his first theorem, Gödel showed that any consistent system with a computable set of axioms which is capable of expressing arithmetic can never be complete: it is possible to construct a statement that can be shown to be true, but that cannot be derived from the formal rules of the system. In his second theorem, he showed that such a system could not prove its own consistency, so it certainly cannot be used to prove the consistency of anything stronger with certainty. This refuted Hilbert's assumption that a finitistic system could be used to prove the consistency of itself, and therefore anything else.\n\nThe main goal of Hilbert's program was to provide secure foundations for all mathematics. In particular this should include:\n\nKurt Gödel showed that most of the goals of Hilbert's program were impossible to achieve, at least if interpreted in the most obvious way. Gödel's second incompleteness theorem shows that any consistent theory powerful enough to encode addition and multiplication of integers cannot prove its own consistency. This presents a challenge to Hilbert's program:\n\n\nMany current lines of research in mathematical logic, such as proof theory and reverse mathematics, can be viewed as natural continuations of Hilbert's original program. Much of it can be salvaged by changing its goals slightly (Zach 2005), and with the following modifications some of it was successfully completed:\n\n\n\n"}
{"id": "250896", "url": "https://en.wikipedia.org/wiki?curid=250896", "title": "Implicit function", "text": "Implicit function\n\nIn mathematics, an implicit equation is a relation of the form formula_1, where formula_2 is a function of several variables (often a polynomial). For example, the implicit equation of the unit circle is formula_3.\n\nAn implicit function is a function that is defined implicitly by an implicit equation, by associating one of the variables (the value) with the others (the arguments). Thus, an implicit function for formula_4 in the context of the unit circle is defined implicitly by formula_5. This implicit equation defines formula_6 as a function of formula_7 only if formula_8 and one considers only non-negative (or non-positive) values for the values of the function.\n\nThe implicit function theorem provides conditions under which some kinds of relations define an implicit function, namely relations defined as the indicator function of the zero set of some continuously differentiable multivariate function.\n\nA common type of implicit function is an inverse function. If \"g\" is a function of \"x\", then the inverse function of \"g\", called \"g\" , is the function giving a solution of the equation\n\nfor \"x\" in terms of \"y\". This solution is\n\nHere formula_11 is an implicit function, and more specifically an inverse function. While for some functions \"g\", formula_11 could be written out explicitly — for instance, if formula_13 then formula_14 — this is often not the case (as in the product log example below).\n\nIntuitively, an inverse function is obtained from \"g\" by interchanging the roles of the dependent and independent variables. Stated another way, the inverse function gives the implicit solution for \"x\" of the equation\n\nExample\n\nThe product log is an implicit function giving the solution for \"x\" of the equation \"y\" − \"xe\" = 0.\n\nAn algebraic function is a function that satisfies a polynomial equation whose coefficients are themselves polynomials. For example, an algebraic function in one variable \"x\" gives a solution for \"y\" of an equation\n\nwhere the coefficients \"a\"(\"x\") are polynomial functions of \"x\". This algebraic function can be written as the right side of the solution equation \"y\" = \"f\" (\"x\"). Written like this, \"f\" is a multi-valued implicit function.\n\nAlgebraic functions play an important role in mathematical analysis and algebraic geometry. A simple example of an algebraic function is given by the left side of the unit circle equation:\n\nSolving for \"y\" gives an explicit solution:\n\nBut even without specifying this explicit solution, it is possible to refer to the implicit solution of the unit circle equation as formula_19 where \"f\" is the multi-valued implicit function.\n\nWhile explicit solutions can be found for equations that are quadratic, cubic, and quartic in \"y\", the same is not in general true for quintic and higher degree equations, such as\n\nNevertheless, one can still refer to the implicit solution \"y\" = \"f\"(\"x\") involving the multi-valued implicit function \"f\".\n\nNot every equation \"R\"(\"x\", \"y\") = 0 implies a graph of a single-valued function, the circle equation being one prominent example. Another example is an implicit function given by \"x\" − \"C\"(\"y\") = 0 where \"C\" is a cubic polynomial having a \"hump\" in its graph. Thus, for an implicit function to be a \"true\" (single-valued) function it might be necessary to use just part of the graph. An implicit function can sometimes be successfully defined as a true function only after \"zooming in\" on some part of the \"x\"-axis and \"cutting away\" some unwanted function branches. Then an equation expressing \"y\" as an implicit function of the other variable(s) can be written.\n\nThe defining equation \"R\"(\"x\", \"y\") = 0 can also have other pathologies. For example, the equation \"x\" = 0 does not imply a function \"f\"(\"x\") giving solutions for \"y\" at all; it is a vertical line. In order to avoid a problem like this, various constraints are frequently imposed on the allowable sorts of equations or on the domain. The implicit function theorem provides a uniform way of handling these sorts of pathologies.\n\nIn calculus, a method called implicit differentiation makes use of the chain rule to differentiate implicitly defined functions.\n\nTo differentiate an implicit function \"y\"(\"x\"), defined by an equation \"R\"(\"x\", \"y\") = 0, it is not generally possible to solve it explicitly for \"y\" and then differentiate. Instead, one can totally differentiate \"R\"(\"x\", \"y\") = 0 with respect to \"x\" and \"y\" and then solve the resulting linear equation for \"dy\" /\"dx\" to explicitly get the derivative in terms of \"x\" and \"y\". Even when it is possible to explicitly solve the original equation, the formula resulting from total differentiation is, in general, much simpler and easier to use.\n\n1. Consider for example\n\nThis equation is easy to solve for \"y\", giving\n\nwhere the right side is the explicit form of the function \"y\"(\"x\"). Differentiation then gives \"dy\"/\"dx\" = −1. Alternatively, one can totally differentiate the original equation:\n\nSolving for \"dy\"/\"dx\" gives\n\nthe same answer as obtained previously.\n\n2. An example of an implicit function for which implicit differentiation is easier than using explicit differentiation is the function \"y\"(\"x\") defined by the equation\n\nTo differentiate this explicitly with respect to \"x\", one has first to get\n\nand then differentiate this function. This creates two derivatives: one for \"y\" ≥ 0 and another for \"y\" < 0.\n\nIt is substantially easier to implicitly differentiate the original equation:\n\ngiving\n\n3. Often, it is difficult or impossible to solve explicitly for \"y\", and implicit differentiation is the only feasible method of differentiation. An example is the equation\n\nIt is impossible to algebraically express \"y\" explicitly as a function of \"x\", and therefore one cannot find \"dy\" /\"dx\" by explicit differentiation. Using the implicit method, \"dy\" /\"dx\" can be obtained by differentiating the equation to obtain\n\nwhere \"dx\" /\"dx\" = 1. Factoring out \"dy\" /\"dx\" shows that\n\nwhich yields the result\n\nwhich is defined for formula_34 and formula_35\n\nIf formula_36 the derivative of the implicit function \"y\"(\"x\") is given by\n\nwhere \"R\" and \"R\" indicate the partial derivatives of \"R\" with respect to \"x\" and \"y\".\n\nThe above formula comes from using the generalized chain rule to obtain the total derivative—with respect to \"x\"—of both sides of \"R\"(\"x\", \"y\") = 0:\n\nhence\n\nwhich, when solved for \"dy\"/\"dx\", gives the expression above.\n\nIt can be shown that if is given by a smooth submanifold in R, and is a point of this submanifold such that the tangent space there is not vertical (that is, ), then in some small enough neighbourhood of is given by a parametrization where is a smooth function.\n\nIn less technical language, implicit functions exist and can be differentiated, unless the tangent to the supposed graph would be vertical. In the standard case where we are given an equation\nthe condition on can be checked by means of partial derivatives.\n\nConsider a relation of the form \"R\"(\"x\"..., \"x\") = 0, where \"R\" is a multivariable polynomial. The set of the values of the variables that satisfy this relation is called an implicit curve if \"n\" = 2 and an implicit surface if \"n\"=3. The implicit equations are the basis of algebraic geometry, whose basic subjects of study are the simultaneous solutions of several implicit equations whose left-hand sides are polynomials. These sets of simultaneous solutions are called affine algebraic sets.\n\nThe solutions of differential equations generally appear expressed by an implicit function.\n\nIn economics, when the level set \"R\"(\"x\", \"y\") = 0 is an indifference curve for the quantities \"x\" and \"y\" consumed of two goods, the absolute value of the implicit derivative d\"y\"/d\"x\" is interpreted as the marginal rate of substitution of the two goods: how much more of \"y\" one must receive in order to be indifferent to a loss of one unit of \"x\".\n\nSimilarly, sometimes the level set \"R\"(\"L\", \"K\") is an isoquant showing various combinations of utilized quantities \"L\" of labor and \"K\" of physical capital each of which would result in the production of the same given quantity of output of some good. In this case the absolute value of the implicit derivative d\"K\"/d\"L\" is interpreted as the marginal rate of technical substitution between the two factors of production: how much more capital the firm must use to produce the same amount of output with one less unit of labor.\n\nOften in economic theory, some function such as a utility function or a profit function is to be maximized with respect to a choice vector \"x\" even though the objective function has not been restricted to any specific functional form. The implicit function theorem guarantees that the first-order conditions of the optimization define an implicit function for each element of the optimal vector \"x*\" of the choice vector \"x\". When profit is being maximized, typically the resulting implicit functions are the labor demand function and the supply functions of various goods. When utility is being maximized, typically the resulting implicit functions are the labor supply function and the demand functions for various goods.\n\nMoreover, the influence of the problem's parameters on \"x*\"—the partial derivatives of the implicit function—can be expressed as total derivatives of the system of first-order conditions found using total differentiation.\n\n\n"}
{"id": "33792013", "url": "https://en.wikipedia.org/wiki?curid=33792013", "title": "Joseph Sgro", "text": "Joseph Sgro\n\nJoseph A. Sgro (born September 20, 1949, San Diego, California) is a mathematician, neurologist / neurophysiologist, and an engineering technologist / entrepreneur in the field of frame grabbers, high-speed cameras, smart cameras, image processors, and related computer vision and machine vision technologies.\n\nSgro began his career as an academic researcher in advanced mathematics and logic. He received an AB in Mathematics in 1970 from UCLA followed by an MA in mathematics in 1973 and a PhD in mathematics in 1975 from the University of Wisconsin, where he studied mathematical logic under H. Jerome Keisler who along with Jon Barwise and Kenneth Kunen formed his doctoral committee.\n\nAfter serving as an instructor and post doctoral fellow at Yale and also holding a membership at the Institute for Advanced Studies at Princeton, New Jersey, Sgro returned to school to study neurology, and received his M.D. in 1980 from the Ph.D to M.D. Program of the Leonard M. Miller School of Medicine at the University of Miami, followed by an internal medicine internship at UNC Memorial Hospital, residency in neurology, a fellowship, and faculty position in clinical neurophysiology at the Neurological Institute of New York.\n\nAs an outgrowth of his work in neurophysiology, while still working as a post-doctoral fellow and an assistant professor of neurology, Sgro founded Alacron, Inc. (formerly Corteks, Inc.until 1990) in 1985 to manufacture technologies relevant to his neurological research. In 1989 he commercialized this technology and began developing array processors, frame grabbers, vision processors, and most recently supported advances in BSI sensor technology. Extending his work in machine vision technology, in 2002, Sgro founded FastVision, LLC, a maker of smart cameras, as a subsidiary of Alacron, Inc .\n\nDuring his first year as a PhD candidate at the University of Wisconsin, Sgro proved that a topological extension of first-order logic using the open set logic quantifier has logical completeness, which had previously been widely believed but had not been proven. Sgro’s proof drew attention throughout mathematical world, and, in 1974, a year before finishing his PhD, he was awarded an appointment as a Josiah Willard Gibbs Instructor in Mathematics at Yale University, received an NSF research grant to continue his work in topological model theory. Yale allowed him to accept this honor while remotely completing his thesis and dissertation at Wisconsin, which he did in 1975. His conclusions regarding the topological model theory formed the basis of his PhD thesis and dissertation. During the 1976-1977 academic year Sgro received a Centennial Fellowship from the AMS. His work also resulted in an invitation to speak at the Logica Colloquim ’77 European Meeting of the Association for Symbolic Logic. This event was held in Wrocław, Poland, which was then still part of the Eastern Bloc, making Sgro among the first mathematicians from the West to speak at an event “behind the Iron Curtain.” Sgro also spent 1977-1978 at the Institute for Advanced Study at Princeton University.\n\nPublished in 1977, Sgro’s thesis “Completeness Theorems for Topological Models” and extensions of this research including the axiomatization and completeness of continuous functions on product topology open set quantifiers was published in 1976 in the Israel Journal of Mathematics. Following these results, Sgro published a proof that an extension of the open set quantifier logic using interior operator quantifier logic has completeness and satisfies Craig interpolation. He further showed that the Souslin-Kleene closure of the open set quantifier logic fails Craig Interpolation which implies that it is strictly weaker than the interior operator logic. His later research concentrated on proving the existence of maximal extensions of first order logic which satisfy Łoś's theorem on ultraproducts and have the Souslin-Kleene property. Also this was extended to ultraproduct extensions of first order logic which satisfied both the Łoś's theorem and an extended form of the compactness theorem.\n\nWhile researching mathematical logic, Sgro became interested in investigating the logic systems that the brain uses to process motor and sensory information, and returned to school, intending to study clinical neurophysiology, the branch of neurology and physiology that examines the functioning of the peripheral and central nervous system. Neurophysiological research typically uses imaging tools for visualizing chemical and electrical activity in nerve pathways, and today includes fMRI, electroencephalography (EEG), evoked potentials (EPs), TMS and other technologies to visualize and evaluate brain activity.\n\nAfter Sgro completed his internship in internal medicine at the University of North Carolina in 1981 and his residency in neurology at Columbia-Presbyterian Medical Center in 1984. Sgro served as a post-doctoral fellow in clinical neurophysiology (1983–1985), as an Associate in Neurology (1985–1986) and then as an Assistant Professor of Neurology (1986–1987) at The College of Physicians and Surgeons at Columbia University in New York City. Sgro relocated to Richmond, Virginia where he was an Associate Professor of Neurology and the Head of Neurophysiology (1987–1991) and finally, as Chief of the Division of Clinical Neurophysiology (1991–1994) at the Virginia Commonwealth University Medical Center. He was also appointed as an adjunct associate professor of Neurology at Columbia-Presbyterian Medical Center in 1994.\n\nDuring his post-doctoral fellowship at Columbia-Presbyterian Medical Center, Sgro achieved recognition in the medical community for his research and findings on the theory of evoked potentials, with a particular focus on Somatosensory Evoked Potentials (SSEPs). A summary of Sgro's efforts to improve evoked potential recording recording technology is found in Keith Chiappa's book. This article covers many one and two dimensional, linear and non-linear digital filters. Two approaches to improve recording fidelity is by increasing the signal to noise ratio (SNR) by the reduction of coherent electrical noise and second the development of a two dimensional DFT digital filtering of evoked potentials which trades off the SNR improvement of the moving average technique with the detection of changes in the averaged waveform. Using this technology, Sgro proved that SSEPs were “state dependent,” varying depending on whether the patient was awake or asleep (anesthetized). Following these findings with funding from the Whitaker Foundation, Sgro developed technology and techniques to analyze evoked potentials based on stimulation run by an ultra fast (i.e. hundreds of hertz) pseudorandom m-sequences. This work was demonstrated to be a more effective method of identification and predictor of sub-clinical diseases or damage such as mortality from status epilepticus (diseases that otherwise went undetected until they become severe enough to qualify as clinically apparent when compared to conventional evoked potentials).\n\nWhile conducting research into the (afferent) sensory nervous system with evoked potentials, Sgro also began to investigate devices and techniques to determine the state of the (efferent) motor nervous system using TMS with the goal of more effective detection of sub-clinical diseases and increased sensitivity of the motor system during intra-operative patient monitoring. Sgro and his associates studied the theoretical and practical issues involved in the design of a high magnetic field strength and rapid transcranial magnetic stimulator which could exceed the historical safety limit of electrical brain stimulation (40 uC/cm2/phase at a stimulation rate of 20 to 50 Hertz over several hours). These studies resulted in the construction of a rapid high magnetic field strength device which was suitable for safety studies. The safety of TMS in rats with a maximal field strength of 3.4 Tesla at 8 Herz for 20 minutes or 10uC/cm2/phase was demonstrated in Sgro \n\nWhile working as a neurology researcher Sgro began work in biomedical engineering and machine vision, specifically the use of imaging and machine vision technologies, to assess the function and integrity of the nervous system in various states of consciousness, during medical procedures, and disease. The research was performed initially using computer programs written in Fortran running on a DEC PDP minicomputer. In the mid 1980s the widespread adoption of IBM PC compatible computers with the ISA bus enabled the development of PC based expansion cards to increase the functionality of the PC. To facilitate lower cost advanced hardware development, Sgro co-founded Alacron, Inc. to develop advanced medical research equipment and commercial PC based products.\n\nIn 1985, Sgro co-founded Alacron, Inc. in Nashua, New Hampshire. Sgro and the Alacron engineering team focused on the development and production of frame grabbers and high speed image processing computational subsystems. The product family currently includes frame grabbers, software, data recording devices and supporting peripherals. Despite initial focus on neurophysiology research and medical imaging, Alacron saw uses for its products expand outside the field of medicine into other applications, such as manufacturing, military, and other industries that use robotics extensively. Alacron is one of the largest frame grabber manufacturers in the Automated Imaging Association's annual market data report.\n\nExamples of broader machine vision uses of frame grabbers originally developed for use in medical imaging include AS&E, which incorporated Alacron technology in backscatter X-ray equipment used for border security, and as image capture used for Voyage Data Recorders, the maritime equivalent of aviation “black boxes.”\n\nIn addition to the commercial product lines offered by Alacron, Sgro continued to perform basic research in integrating frame grabber technology with specialized systems for various disciplines. The company received SBIR grants where Sgro acted as principal investigators, including:\n\n\nAcademic presentations of Alacron’s technology and research include: \n\nIn 2002, Sgro launched FastVision, LLC. FastVision builds high-speed megapixel-plus digital cameras, based on CMOS and CCD image sensors . The company's goal is to produce smart cameras, i.e. cameras with high-speed scalable integrated image processing capabilities built into the same package housing the opto-electronics. Like most smart camera vendors, FastVision’s suite includes FPGA processing and memory subsystems to enable in-camera image processing. When integrated with a high powered frame grabber or vision processor board (or a host subsystem), the resulting system capabilities can be expanded beyond simple image compression. The smart camera subsystem can be integrated with disk or non-volatile semiconductor storage inside or outside the camera to hold sustained real-time data acquisition, a valuable aid to system effectiveness when network connectivity is overloaded or is unavailable.\n\nApplications for smart cameras range from security and surveillance, to robotics in medicine and manufacturing, to military applications such as bots, drones and intelligent weaponry, to satellites and inner and outer space exploration.\n\n\n"}
{"id": "32787803", "url": "https://en.wikipedia.org/wiki?curid=32787803", "title": "Kampé de Fériet function", "text": "Kampé de Fériet function\n\nIn mathematics, the Kampé de Fériet function is a two-variable generalization of the generalized hypergeometric series, introduced by Joseph Kampé de Fériet.\n\nThe Kampé de Fériet function is given by\n\nThe general sextic equation can be solved in terms of Kampé de Fériet functions.\n\n"}
{"id": "2012564", "url": "https://en.wikipedia.org/wiki?curid=2012564", "title": "Karp's 21 NP-complete problems", "text": "Karp's 21 NP-complete problems\n\nIn computational complexity theory, Karp's 21 NP-complete problems are a set of computational problems which are NP-complete. In his 1972 paper, \"Reducibility Among Combinatorial Problems\", Richard Karp used Stephen Cook's 1971 theorem that the boolean satisfiability problem is NP-complete (also called the Cook-Levin theorem) to show that there is a polynomial time many-one reduction from the boolean satisfiability problem to each of 21 combinatorial and graph theoretical computational problems, thereby showing that they are all NP-complete. This was one of the first demonstrations that many natural computational problems occurring throughout computer science are computationally intractable, and it drove interest in the study of NP-completeness and the P versus NP problem.\n\nKarp's 21 problems are shown below, many with their original names. The nesting indicates the direction of the reductions used. For example, Knapsack was shown to be NP-complete by reducing Exact cover to Knapsack.\n\n\nAs time went on it was discovered that many of the problems can be solved efficiently if restricted to special cases, or can be solved within any fixed percentage of the optimal result. However, David Zuckerman showed in 1996 that every one of these 21 problems has a constrained optimization version that is impossible to approximate within any constant factor unless P = NP, by showing that Karp's approach to reduction generalizes to a specific type of approximability reduction. Note however that these may be different from the standard optimization versions of the problems, which may have approximation algorithms (as in the case of maximum cut).\n\n\n"}
{"id": "26248766", "url": "https://en.wikipedia.org/wiki?curid=26248766", "title": "List of Italian mathematicians", "text": "List of Italian mathematicians\n\nA list of notable mathematicians from Italy by century:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "5075766", "url": "https://en.wikipedia.org/wiki?curid=5075766", "title": "Locally Hausdorff space", "text": "Locally Hausdorff space\n\nIn mathematics, in the field of topology, a topological space is said to be locally Hausdorff if every point has an open neighbourhood that is a Hausdorff space under the subspace topology.\n\nHere are some facts:\n\n"}
{"id": "5765458", "url": "https://en.wikipedia.org/wiki?curid=5765458", "title": "Loève Prize", "text": "Loève Prize\n\nThe Line and Michel Loève International Prize in Probability (Loève Prize) was created in 1992 in honor of Michel Loève by his widow Line. The prize, awarded every two years, is intended to recognize outstanding contributions by researchers in mathematical probability who are under 45 years old. With a prize value of around $30,000 this is one of the most generous awards in any specific mathematical subdiscipline.\n\n\n"}
{"id": "29812913", "url": "https://en.wikipedia.org/wiki?curid=29812913", "title": "Min-plus matrix multiplication", "text": "Min-plus matrix multiplication\n\nMin-plus matrix multiplication, also known as the distance product, is an operation on matrices.\n\nGiven two formula_1 matrices formula_2 and formula_3, their distance product formula_4 is defined as an formula_1 matrix such that formula_6. This is standard matrix multiplication for the semi-ring of tropical numbers in the min convention.\n\nThis operation is closely related to the shortest path problem. If formula_7 is an formula_1 matrix containing the edge weights of a graph, then formula_9 gives the distances between vertices using paths of length at most formula_10 edges, and formula_11 is the distance matrix of the graph.\n\n\n"}
{"id": "43023424", "url": "https://en.wikipedia.org/wiki?curid=43023424", "title": "Monomial ideal", "text": "Monomial ideal\n\nIn algebra, a monomial ideal is an ideal generated by some monomials in a multivariate polynomial ring over a field.\n\nA toric ideal is an ideal generated by differences of monomials (provided the ideal is a prime ideal). An affine or projective algebraic variety defined by a toric ideal or a homogeneous toric ideal is an affine or projective toric variety, possibly non-normal.\n\nLet formula_1 be a field and formula_2 be the polynomial ring over formula_1 with formula_4 variables formula_5. \n\nA monomial in formula_6, is a product formula_7 for an formula_4 tuple formula_9 of nonnegative integers. An ideal formula_10 is called a monomial ideal if it is generated by monomials. \n\nFurthermore, an ideal formula_11 is called a \"monomial ideal\" whenever it satisfies any of the following three properties. \n\nGiven a monomial ideal formula_21, formula_22 if and only if every monomial ideal term formula_23 of formula_24 is a multiple of one the formula_25. \n\nProof:\nSuppose formula_21 and that formula_22. Then formula_28, for some formula_29. \n\nFor all formula_30, we can express each formula_23 as the sum of monomials, so that formula_24 can be written as a sum of multiples of the formula_33. Hence, formula_24 will be a sum of multiples of monomial terms for at least one of the formula_33.\n\nConversely, let formula_21 and let each monomial term in formula_37 be a multiple of one of the formula_33 in formula_12. Then each monomial term in formula_12 can be factored from each monomial in formula_24. Hence formula_24 is of the form formula_43 for some formula_44, as a result formula_45. \n\nThe following illustrates an example of monomial and polynomial ideals.\n\nLet formula_46 then the polynomial formula_47 is in since each term is a multiple of an element in i.e., they can be rewritten as formula_48 and formula_49 both in However, if formula_50, then this polynomial formula_47 is not in since its terms are not multiples of elements in \n\nA monomial ideal can be interpreted as a Young diagram. Suppose formula_52, then formula_12 can be interpreted in terms of the minimal monomials generators as formula_54, where formula_55 and formula_56. The minimal monomial generators of formula_12 can be seen as the inner corners of the Young diagram. The minimal generators would determine where we would draw the staircase diagram. \n\nThe monomials not in formula_12 lie inside the staircase, and these monomials form a vector space basis for the quotient ring formula_59.\n\nConsider the following example. \nLet formula_60 be a monomial ideal. Then the set of grid points formula_61 corresponds to the minimal monomial formula_62 generators in formula_12. Then as the figure shows, the pink Young diagram consists of the monomials that are not in formula_12. The points in the inner corners of the Young diagram, make us possible to identify the minimal monomials formula_65 in formula_12 as seen in the green boxes. Hence, formula_67.\nIn general, to any set of grid points, we can associate a Young diagram, so that the monomial ideal is constructed by determining the inner corners that make up the staircase diagram; likewise, given a monomial ideal, we can make up the Young diagram by looking at the formula_68 and representing them as the inner corners of the Young diagram. The coordinates of the inner corners would represent the powers of the minimal monomials in formula_12. Thus, monomial ideals can be described by Young diagrams of partitions.\n\nMoreover, the formula_70-action on the set of formula_71 such that formula_72 as a vector space over formula_73 has fixed points corresponding to monomial ideals only, which correspond to partitions of size formula_4, which are identify by Young diagrams with formula_4 boxes.\n\nA monomial ordering is a well ordering \"formula_76\" on the set of monomials such that if formula_77 are monomials, then formula_78.\n\nBy the monomial order, we can state the following definitions for a polynomial in formula_79. \n\n\"Definition\"\n\n\nFor example, if we choose the lexicographical order formula_94 on formula_95, then formula_96, but formula_97 if formula_98. Thus, formula_86 in general depends of the ordering used.\n\nIn addition, monomials are present on Gröbner basis and to define the division algorithm for polynomials with several variables. \n\nNotice that for a monomial ideal formula_100, the finite set of generators formula_89 is a Gröbner basis for formula_12. To see this, notice that for any polynomial formula_24 in the ideal, can be expressed as formula_104 for formula_105. Then the leading term of formula_24 is a multiple for some formula_107. As a result, formula_86 is generated by the formula_107 likewise.\n\n\n\n"}
{"id": "32753180", "url": "https://en.wikipedia.org/wiki?curid=32753180", "title": "Mott polynomials", "text": "Mott polynomials\n\nIn mathematics the Mott polynomials \"s\"(\"x\") are polynomials introduced by who applied them to a problem in the theory of electrons. \nThey are given by the exponential generating function \nBecause the factor in the exponential has the power series\nin terms of Catalan numbers formula_3, the coefficient in front of formula_4 of the polynomial can be written as\naccording to the general formula for generalized Appell polynomials,\nwhere the sum is over all compositions formula_6 of formula_7 into formula_8 positive odd integers. The empty product appearing for formula_9 equals 1. Special values, where all contributing Catalan numbers equal 1, are\n\nBy differentiation the recurrence for the first derivative becomes\n\nThe first few of them are \n\nThe polynomials \"s\"(\"x\") form the associated Sheffer sequence for –2\"t\"/(1–t) .\n\n"}
{"id": "587678", "url": "https://en.wikipedia.org/wiki?curid=587678", "title": "Nerve of a covering", "text": "Nerve of a covering\n\nIn topology, the nerve of an open covering is a construction of an abstract simplicial complex from an open covering of a topological space \"X\" that captures many of the interesting topological properties in an algorithmic or combinatorial way. It was introduced by Pavel Alexandrov and now has many variants and generalisations, among them the Čech nerve of a cover, which in turn is generalised by hypercoverings.\nGiven an index set \"I\", and open sets \"U contained in \"X\", the \"nerve\" \"N\" is the set of finite subsets of \"I\" defined as follows:\n\nObviously, if \"J\" belongs to \"N\", then any of its subsets is also in \"N\". Therefore \"N\" is an abstract simplicial complex.\n\nIn general, the complex \"N\" need not reflect the topology of \"X\" accurately. For example we can cover any \"n\"-sphere with two contractible sets \"U\" and \"V\", in such a way that \"N\" is an abstract 1-simplex. However, if we also insist that the open sets corresponding to every intersection indexed by a set in \"N\" are also contractible, the situation changes. In particular, the nerve lemma states that if formula_2 is a good cover, in that for each formula_3, the set formula_4 is contractible if it is nonempty, then the nerve formula_5 is homotopy equivalent to formula_6. \n\nThis means for instance that a circle covered by three open arcs, intersecting in pairs in one arc, is modelled by a homeomorphic complex, the geometrical realization of \"N\".\n\nGiven an open cover formula_7 of a topological space formula_8, or more generally a cover in a site, we can regard the pairwise fibre products formula_9, which in the case of a topological space is precisely the intersection formula_10. The collection of all such intersections can be referred to as formula_11 and the triple intersections as formula_12. By considering the natural maps formula_13 and formula_14, we can construct a simplicial object formula_15 defined by formula_16, n-fold fibre product. This is the Čech nerve, and by taking connected components we get a simplicial set, which we can realise topologically: formula_17. If the covering and the space are sufficiently nice, for instance if formula_8 is compact and all intersections of sets in the cover are contractible or empty, then this space is weakly equivalent to formula_8. This is known as \"the nerve theorem\".\n\n"}
{"id": "56561099", "url": "https://en.wikipedia.org/wiki?curid=56561099", "title": "Ortrud Oellermann", "text": "Ortrud Oellermann\n\nOrtrud R. Oellermann is a South African mathematician specializing in graph theory. She is a professor of mathematics at the University of Winnipeg.\n\nOellermann was born in Vryheid.\nShe earned a bachelor's degree, \"cum laude\" honours, and a master's degree at the University of Natal in 1981, 1982, and 1983 respectively,\nas a student of Henda Swart.\nShe completed her Ph.D. in 1986 at Western Michigan University.\nHer dissertation was \"Generalized Connectivity in Graphs\" and was supervised by Gary Chartrand.\n\nOellermann taught at the University of Durban-Westville, Western Michigan University, University of Natal, and Brandon University, before moving to Winnipeg in 1996. At Winnipeg, she was co-chair of mathematics and statistics for 2011–2013.\n\nWith Gary Chartrand, Oellermann is the author of the book \"Applied and Algorithmic Graph Theory\" (McGraw Hill, 1993).\n\nShe is also the author of well-cited research publications on metric dimension of graphs, on distance-based notions of convex hulls in graphs, and on highly irregular graphs in which every vertex has a neighborhood in which all degrees are distinct. The phrase \"highly irregular\" was a catchphrase of her co-author Yousef Alavi; because of this, Ronald Graham suggested that there should be a concept of highly irregular graphs, by analogy to the regular graphs, and Oellermann came up with the definition of these graphs.\n\nIn 1991, Oellermann was the winner of the annual Silver British Association Medal of the Southern Africa Association for the Advancement of Science.\nShe won the Meiring Naude Medal of the Royal Society of South Africa in 1994.\nShe was also one of three winners of the Hall Medal of the Institute of Combinatorics and its Applications in 1994, the first year the medal was awarded.\n\n"}
{"id": "1234251", "url": "https://en.wikipedia.org/wiki?curid=1234251", "title": "PCF theory", "text": "PCF theory\n\nPCF theory is the name of a mathematical theory, introduced by Saharon , that deals with the cofinality of the ultraproducts of ordered sets. It gives strong upper bounds on the cardinalities of power sets of singular cardinals, and has many more applications as well. The abbreviation \"PCF\" stands for \"possible cofinalities\".\n\nIf \"A\" is an infinite set of regular cardinals, \"D\" is an ultrafilter on \"A\", then \nwe let formula_1 denote the cofinality of the ordered set of functions \nformula_2 where the ordering is defined as follows. \nformula_3 if formula_4. \npcf(\"A\") is the set of cofinalities that occur if we consider all ultrafilters on \"A\", that is, \nformula_5\n\nObviously, pcf(\"A\") consists of regular cardinals. Considering ultrafilters concentrated on elements of \"A\", we get that \nformula_6. Shelah proved, that if formula_7, then pcf(\"A\") has a largest element, and there are subsets formula_8 of \"A\" such that for each ultrafilter \"D\" on \"A\", formula_1 is the least element θ of pcf(\"A\") such that formula_10. Consequently, formula_11. \nShelah also proved that if \"A\" is an interval of regular cardinals (i.e., \"A\" is the set of all regular cardinals between two cardinals), then pcf(\"A\") is also an interval of regular cardinals and |pcf(\"A\")|<|\"A\"|. \nThis implies the famous inequality \nformula_12\nassuming that ℵ is strong limit.\n\nIf λ is an infinite cardinal, then \"J\" is the following ideal on \"A\". \"B\"∈\"J\" if formula_13 holds for every ultrafilter \"D\" with \"B\"∈\"D\". Then \"J\" is the ideal generated by the sets formula_14. There exist \"scales\", i.e., for every λ∈pcf(\"A\") there is a sequence of length λ of elements of formula_15 which is both increasing and cofinal mod \"J\". This implies that the cofinality of formula_2 under pointwise dominance is max(pcf(\"A\")). \nAnother consequence is that if λ is singular and no regular cardinal less than λ is Jónsson, then also λ is not Jónsson. In particular, there is a Jónsson algebra on ℵ, which settles an old conjecture.\n\nThe most notorious conjecture in pcf theory states that |pcf(\"A\")|=|\"A\"| holds for every set \"A\" of regular cardinals with |\"A\"|<min(\"A\"). This would imply that if ℵ is strong limit, then the sharp bound \nformula_17\nholds. The analogous bound \nformula_18\nfollows from Chang's conjecture (Magidor) or even from the nonexistence of a Kurepa tree (Shelah).\n\nA weaker, still unsolved conjecture states that if |\"A\"|<min(\"A\"), then pcf(\"A\") has no inaccessible limit point. This is equivalent to the statement that pcf(pcf(\"A\"))=pcf(\"A\").\n\nThe theory has found a great deal of applications, besides cardinal arithmetic.\nThe original survey by Shelah, \"Cardinal arithmetic for skeptics\", includes the following topics: almost free abelian groups, partition problems, failure of preservation of chain conditions in Boolean algebras under products, existence of Jónsson algebras, existence of entangled linear orders, equivalently narrow Boolean algebras, and the existence of nonisomorphic models equivalent in certain infinitary logics.\n\nIn the meantime, many further applications have been found in Set Theory, Model Theory, Algebra and Topology.\n\nSaharon Shelah, Cardinal Arithmetic, Oxford Logic Guides, vol. 29. Oxford University Press, 1994.\n\n"}
{"id": "550701", "url": "https://en.wikipedia.org/wiki?curid=550701", "title": "Prime geodesic", "text": "Prime geodesic\n\nIn mathematics, a prime geodesic on a hyperbolic surface is a primitive closed geodesic, i.e. a geodesic which is a closed curve that traces out its image exactly once. Such geodesics are called prime geodesics because, among other things, they obey an asymptotic distribution law similar to the prime number theorem.\n\nWe briefly present some facts from hyperbolic geometry which are helpful in understanding prime geodesics.\n\nConsider the Poincaré half-plane model \"H\" of 2-dimensional hyperbolic geometry. Given a Fuchsian group, that is, a discrete subgroup Γ of PSL(2, R), Γ acts on \"H\" via linear fractional transformation. Each element of PSL(2, R) in fact defines an isometry of \"H\", so Γ is a group of isometries of \"H\".\n\nThere are then 3 types of transformation: hyperbolic, elliptic, and parabolic. (The loxodromic transformations are not present because we are working with real numbers.) Then an element γ of Γ has 2 distinct real fixed points if and only if γ is hyperbolic. See Classification of isometries and Fixed points of isometries for more details.\n\nNow consider the quotient surface \"M\"=Γ\\\"H\". The following description refers to the upper half-plane model of the hyperbolic plane. This is a hyperbolic surface, in fact, a Riemann surface. Each hyperbolic element \"h\" of Γ determines a closed geodesic of Γ\\\"H\": first, by connecting the geodesic semicircle joining the fixed points of \"h\", we get a geodesic on \"H\" called the axis of \"h\", and by projecting this geodesic to \"M\", we get a geodesic on Γ\\\"H\".\n\nThis geodesic is closed because 2 points which are in the same orbit under the action of Γ project to the same point on the quotient, by definition.\n\nIt can be shown that this gives a 1-1 correspondence between closed geodesics on Γ\\\"H\" and hyperbolic conjugacy classes in Γ. The prime geodesics are then those geodesics that trace out their image exactly once — algebraically, they correspond to primitive hyperbolic conjugacy classes, that is, conjugacy classes {γ} such that γ cannot be written as a nontrivial power of another element of Γ.\n\nThe importance of prime geodesics comes from their relationship to other branches of mathematics, especially dynamical systems, ergodic theory, and number theory, as well as Riemann surfaces themselves. These applications often overlap among several different research fields.\n\nIn dynamical systems, the closed geodesics represent the periodic orbits of the geodesic flow.\n\nIn number theory, various \"prime geodesic theorems\" have been proved which are very similar in spirit to the prime number theorem. To be specific, we let π(\"x\") denote the number of closed geodesics whose norm (a function related to length) is less than or equal to \"x\"; then π(\"x\") ∼ \"x\"/ln(\"x\"). This result is usually credited to Atle Selberg. In his 1970 Ph.D. thesis, Grigory Margulis proved a similar result for surfaces of variable negative curvature, while in his 1980 Ph.D. thesis, Peter Sarnak proved an analogue of Chebotarev's density theorem.\n\nThere are other similarities to number theory — error estimates are improved upon, in much the same way that error estimates of the prime number theorem are improved upon. Also, there is a Selberg zeta function which is formally similar to the usual Riemann zeta function and shares many of its properties.\n\nAlgebraically, prime geodesics can be lifted to higher surfaces in much the same way that prime ideals in the ring of integers of a number field can be split (factored) in a Galois extension. See Covering map and Splitting of prime ideals in Galois extensions for more details.\n\nClosed geodesics have been used to study Riemann surfaces; indeed, one of Riemann's original definitions of the genus of a surface was in terms of simple closed curves. Closed geodesics have been instrumental in studying the eigenvalues of Laplacian operators, arithmetic Fuchsian groups, and Teichmüller spaces.\n\n"}
{"id": "8214401", "url": "https://en.wikipedia.org/wiki?curid=8214401", "title": "Proof of Stein's example", "text": "Proof of Stein's example\n\nStein's example is an important result in decision theory which can be stated as\n\nThe following is an outline of its proof. The reader is referred to the main article for more information.\n\nThe risk function of the decision rule formula_1 is\n\nNow consider the decision rule\n\nwhere formula_6. We will show that formula_7 is a better decision rule than formula_8. The risk function is\n\n— a quadratic in formula_12. We may simplify the middle term by considering a general \"well-behaved\" function formula_13 and using integration by parts. For formula_14, for any continuously differentiable formula_15 growing sufficiently slowly for large formula_16 we have:\n\nTherefore,\n\nNow, we choose\n\nIf formula_15 met the \"well-behaved\" condition (it doesn't, but this can be remedied -- see below), we would have\n\nand so\n\nThen returning to the risk function of formula_7 :\n\nThis quadratic in formula_12 is minimized at\n\ngiving\n\nwhich of course satisfies:\n\nmaking formula_8 an inadmissible decision rule.\n\nIt remains to justify the use of\n\nThis function is not continuously differentiable since it is singular at formula_35. However the function\n\nis continuously differentiable, and after following the algebra through and letting formula_37 one obtains the same result.\n\nSamworth has published a particularly compact proof based on partial integration that does without lemma and without ε.\n"}
{"id": "12182350", "url": "https://en.wikipedia.org/wiki?curid=12182350", "title": "Remez inequality", "text": "Remez inequality\n\nIn mathematics, the Remez inequality, discovered by the Soviet mathematician Evgeny Yakovlevich Remez , gives a bound on the sup norms of certain polynomials, the bound being attained by the Chebyshev polynomials.\n\nLet σ be an arbitrary fixed positive number. Define the class of polynomials π(σ) to be those polynomials \"p\" of the \"n\"th degree for which\n\non some set of measure ≥ 2 contained in the closed interval [−1, 1+σ]. Then the Remez inequality states that\n\nwhere \"T\"(\"x\") is the Chebyshev polynomial of degree \"n\", and the supremum norm is taken over the interval [−1, 1+σ].\n\nObserve that \"T\" is increasing on formula_3, hence \n\nThe R.i., combined with an estimate on Chebyshev polynomials, implies the following\ncorollary: If \"J\" ⊂ R is a finite interval, and \"E\" ⊂ \"J\" is an arbitrary measurable set, then\nfor any polynomial \"p\" of degree \"n\".\n\nInequalities similar to () have been proved for different classes of functions, and are known as Remez-type inequalities. One important example is Nazarov's inequality for exponential sums :\n\nIn the special case when \"λ\" are pure imaginary and integer, and the subset \"E\" is itself an interval, the inequality was proved by Pál Turán and is known as Turán's lemma.\n\nThis inequality also extends to formula_8 in the following way\n\nfor some \"A\">0 independent of \"p\", \"E\", and \"n\". When\n\na similar inequality holds for \"p\" > 2. For \"p\"=∞ there is an extension to multidimensional polynomials.\n\nProof: Applying Nazarov's lemma to formula_11 leads to\n\nthus\n\nNow fix a set formula_14 and choose formula_15 such that formula_16, that is\n\nNote that this implies: \n\nNow\n\nwhich completes the proof.\n\nOne of the corollaries of the R.i. is the Pólya inequality, which was proved by George Pólya , and states that the Lebesgue measure of a sub-level set of a polynomial \"p\" of degree \"n\" is bounded in terms of the leading coefficient LC(\"p\") as follows:\n\n"}
{"id": "22666353", "url": "https://en.wikipedia.org/wiki?curid=22666353", "title": "Ringel–Hall algebra", "text": "Ringel–Hall algebra\n\nIn mathematics, a Ringel–Hall algebra is a generalization of the Hall algebra, studied by .\nIt has a basis of equivalence classes of objects of an abelian category, and the structure constants for this basis are related to the numbers of extensions of objects in the category.\n\n\n"}
{"id": "5173456", "url": "https://en.wikipedia.org/wiki?curid=5173456", "title": "Scalar field theory", "text": "Scalar field theory\n\nIn theoretical physics, scalar field theory can refer to a relativistically invariant classical or quantum theory of scalar fields. A scalar field is invariant under any Lorentz transformation.\n\nThe only fundamental scalar quantum field that has been observed in nature is the Higgs field. However, scalar quantum fields feature in the effective field theory descriptions of many physical phenomena. An example is the pion, which is actually a pseudoscalar.\n\nSince they do not involve polarization complications, scalar fields are often the easiest to appreciate second quantization through. For this reason, scalar field theories are often used for purposes of introduction of novel concepts and techniques.\n\nThe signature of the metric employed below is .\n\nA general reference for this section is Ramond, Pierre (2001-12-21). Field Theory: A Modern Primer (Second Edition). USA: Westview Press. , Ch 1.\n\nThe most basic scalar field theory is the linear theory. Through the Fourier decomposition of the fields, it represents the normal modes of an where the continuum limit of the oscillator index \"i\" is now denoted by . The action for the free relativistic scalar field theory is then\n\nwhere formula_2 is known as a Lagrangian density; for the three spatial coordinates; is the Kronecker delta function; and for the -th coordinate .\n\nThis is an example of a quadratic action, since each of the terms is quadratic in the field, . The term proportional to is sometimes known as a mass term, due to its subsequent interpretation, in the quantized version of this theory, in terms of particle mass.\n\nThe equation of motion for this theory is obtained by extremizing the action above. It takes the following form, linear in , \n\nwhere ∇ is the Laplace operator. This is the Klein–Gordon equation, with the interpretation as a classical field equation, rather than as a quantum-mechanical wave equation.\n\nThe most common generalization of the linear theory above is to add a scalar potential to the Lagrangian, where typically, in addition to a mass term, \"V\" is a polynomial in . Such a theory is sometimes said to be interacting, because the Euler-Lagrange equation is now nonlinear, implying a self-interaction. The action for the most general such theory is\n\nThe \"n\"! factors in the expansion are introduced because they are useful in the Feynman diagram expansion of the quantum theory, as described below.\n\nThe corresponding Euler-Lagrange equation of motion is now\n\nPhysical quantities in these scalar field theories may have dimensions of length, time or mass, or some combination of the three.\n\nHowever, in a relativistic theory, any quantity , with dimensions of time, can be readily converted into a \"length\", , by using the velocity of light, . Similarly, any length is equivalent to an inverse mass, /\"mc\", using Planck's constant, . In natural units, one thinks of a time as a length, or either time or length as an inverse mass.\n\nIn short, one can think of the dimensions of any physical quantity as defined in terms of \"just one\" independent dimension, rather than in terms of all three. This is most often termed the mass dimension of the quantity. Knowing the dimensions of each quantity, allows one to \"uniquely restore\" conventional dimensions from a natural units expression in terms of this mass dimension, by simply reinserting the requisite powers of and required for dimensional consistency.\n\nOne conceivable objection is that this theory is classical, and therefore it is not obvious how Planck's constant should be a part of the theory at all. If desired, one could indeed recast the theory without mass dimensions at all: However, this would be at the expense of slightly obscuring the connection with the quantum scalar field. Given that one has dimensions of mass, Planck's constant is thought of here as an essentially \"arbitrary fixed reference quantity of action\" (not necessarily connected to quantization), hence with dimensions appropriate to convert between mass and inverse length.\n\nThe classical scaling dimension, or mass dimension, , of describes the transformation of the field under a rescaling of coordinates:\n\nThe units of action are the same as the units of , and so the action itself has zero mass dimension. This fixes the scaling dimension of the field to be\n\nThere is a specific sense in which some scalar field theories are scale-invariant. While the actions above are all constructed to have zero mass dimension, not all actions are invariant under the scaling transformation\n\nThe reason that not all actions are invariant is that one usually thinks of the parameters \"m\" and as fixed quantities, which are not rescaled under the transformation above. The condition for a scalar field theory to be scale invariant is then quite obvious: all of the parameters appearing in the action should be dimensionless quantities. In other words, a scale invariant theory is one without any fixed length scale (or equivalently, mass scale) in the theory.\n\nFor a scalar field theory with spacetime dimensions, the only dimensionless parameter satisfies = . For example, in = 4, only is classically dimensionless, and so the only classically scale-invariant scalar field theory in = 4 is the massless theory.\n\nClassical scale invariance, however, normally does not imply quantum scale invariance, because of the renormalization group involved – see the discussion of the beta function below.\n\nA transformation\nis said to be conformal if the transformation satisfies\nfor some function .\n\nThe conformal group contains as subgroups the isometries of the metric formula_13 (the Poincaré group) and also the scaling transformations (or dilatations) considered above. In fact, the scale-invariant theories in the previous section are also conformally-invariant.\n\nMassive theory illustrates a number of interesting phenomena in scalar field theory.\n\nThe Lagrangian density is\n\nThis Lagrangian has a ℤ₂ symmetry under the transformation .\nThis is an example of an internal symmetry, in contrast to a space-time symmetry.\n\nIf is positive, the potential \nhas a single minimum, at the origin. The solution \"φ\"=0 is clearly invariant under the ℤ₂ symmetry.\n\nConversely, if is negative, then one can readily see that the potential \nhas two minima. This is known as a \"double well potential\", and the lowest energy states (known as the vacua, in quantum field theoretical language) in such a theory are invariant under the ℤ₂ symmetry of the action (in fact it maps each of the two vacua into the other). In this case, the ℤ₂ symmetry is said to be \"spontaneously broken\".\n\nThe theory with a negative also has a kink solution, which is a canonical example of a soliton. Such a solution is of the form\nwhere is one of the spatial variables ( is taken to be independent of , and the remaining spatial variables). The solution interpolates between the two different vacua of the double well potential. It is not possible to deform the kink into a constant solution without passing through a solution of infinite energy, and for this reason the kink is said to be stable. For \"D\">2 (i.e., theories with more than one spatial dimension), this solution is called a domain wall.\n\nAnother well-known example of a scalar field theory with kink solutions is the sine-Gordon theory.\n\nIn a complex scalar field theory, the scalar field takes values in the complex numbers,\nrather than the real numbers. The action considered normally takes the form\n\nThis has a U(1), equivalently O(2) symmetry, whose action on the space of fields rotates formula_19, for some real phase angle .\n\nAs for the real scalar field, spontaneous symmetry breaking is found if \"m\" is negative. This gives rise to Goldstone's Mexican hat potential which is a rotation of the double-well potential of a real scalar\nfield by 2π radians about the \"V\"formula_20 axis. The symmetry breaking takes place in one higher dimension, i.e. the choice of vacuum breaks a continuous \"U\"(1) symmetry instead of a discrete one.\nThe two components of the scalar field are reconfigured as a massive mode and a massless Goldstone boson.\n\nOne can express the complex scalar field theory in terms of two real fields, \"φ\" = Re \"φ\" and \"φ\" = Im \"φ\", which transform in the vector representation of the \"U\"(1) = \"O\"(2) internal symmetry. Although such fields transform as a vector under the \"internal symmetry\", they are still Lorentz scalars.\n\nThis can be generalised to a theory of N scalar fields transforming in the vector representation of the \"O\"(\"N\") symmetry. The Lagrangian for an \"O\"(\"N\")-invariant scalar field theory is typically of the form\nusing an appropriate \"O\"(\"N\")-invariant inner product.\n\nA general reference for this section is Ramond, Pierre (2001-12-21). Field Theory: A Modern Primer (Second Edition). USA: Westview Press. , Ch. 4\n\nIn quantum field theory, the fields, and all observables constructed from them, are replaced by quantum operators on a Hilbert space. This Hilbert space is built on a vacuum state, and dynamics are governed by a quantum Hamiltonian, a positive-definite operator which annihilates the vacuum. A construction of a quantum scalar field theory is detailed in the canonical quantization article, which relies on canonical commutation relations among the fields. Essentially, the infinity of classical oscillators repackaged in the scalar field as its (decoupled) normal modes, above, are now quantized in the standard manner, so the respective quantum operator field describes an infinity of quantum harmonic oscillators acting on a respective Fock space.\n\nIn brief, the basic variables are the quantum field and its canonical momentum . Both these operator-valued fields are Hermitian. At spatial points , , at equal times, their canonical commutation relations are given by\n\nwhile the free Hamiltonian is, similarly to above, \n\nA spatial Fourier transform leads to momentum space fields\nwhich resolve to annihilation and creation operators\nwhere formula_26 .\n\nThese operators satisfy the commutation relations\n\nThe state formula_28 annihilated by all of the operators \"a\" is identified as the \"bare vacuum\", and a particle with momentum is created by applying formula_29 to the vacuum.\n\nApplying all possible combinations of creation operators to the vacuum constructs the relevant Hilbert space: This construction is called Fock space. The vacuum is annihilated by the Hamiltonian\nwhere the zero-point energy has been removed by Wick ordering. (See canonical quantization.)\n\nInteractions can be included by adding an interaction Hamiltonian. For a \"φ\" theory, this corresponds to adding a Wick ordered term \"g\":\"φ\":/4! to the Hamiltonian, and integrating over \"x\". Scattering amplitudes may be calculated from this Hamiltonian in the interaction picture. These are constructed in perturbation theory by means of the Dyson series, which gives the time-ordered products, or \"n\"-particle Green's functions formula_31 as described in the Dyson series article. The Green's functions may also be obtained from a generating function that is constructed as a solution to the Schwinger–Dyson equation.\n\nThe Feynman diagram expansion may be obtained also from the Feynman path integral formulation. The time ordered vacuum expectation values of polynomials in , known as the \"n\"-particle Green's functions, are constructed by integrating over all possible fields, normalized by the vacuum expectation value with no external fields,\n\nAll of these Green's functions may be obtained by expanding the exponential in \"J\"(\"x\")φ(\"x\") in the generating function\n\nA Wick rotation may be applied to make time imaginary. Changing the signature to (++++) then turns the Feynman integral into a statistical mechanics partition function in Euclidean space,\n\nNormally, this is applied to the scattering of particles with fixed momenta, in which case, a Fourier transform is useful, giving instead\n\nThe standard trick to evaluate this functional integral is to write it as a product of exponential factors, schematically,\nThe second two exponential factors can be expanded as power series, and the combinatorics of this expansion can be represented graphically through Feynman diagrams.\n\nThe integral with = 0 can be treated as a product of infinitely many elementary Gaussian integrals: the result may be expressed as a sum of Feynman diagrams, calculated using the following Feynman rules:\n\nThe last rule takes into account the effect of dividing by [0]. The Minkowski-space Feynman rules are similar, except that each vertex is represented by \"−ig\", while each internal line is represented by a propagator \"i\"/(\"q\"−\"m\"+\"iε\"), where the term represents the small Wick rotation needed to make the Minkowski-space Gaussian integral converge.\n\nThe integrals over unconstrained momenta, called \"loop integrals\", in the Feynman graphs typically diverge. This is normally handled by renormalization, which is a procedure of adding divergent counter-terms to the Lagrangian in such a way that the diagrams constructed from the original Lagrangian and counter-terms is finite. A renormalization scale must be introduced in the process, and the coupling constant and mass become dependent upon it.\n\nThe dependence of a coupling constant on the scale is encoded by a beta function, , defined by\n\nThis dependence on the energy scale is known as \"the running of the coupling parameter\", and theory of this systematic scale-dependence in quantum field theory is described by the renormalization group.\n\nBeta-functions are usually computed in an approximation scheme, most commonly perturbation theory, where one assumes that the coupling constant is small. One can then make an expansion in powers of the coupling parameters and truncate the higher-order terms (also known as higher loop contributions, due to the number of loops in the corresponding Feynman graphs).\n\nThe -function at one loop (the first perturbative contribution) for the theory is\n\nThe fact that the sign in front of the lowest-order term is positive suggests that the coupling constant increases with energy. If this behavior persisted at large couplings, this would indicate the presence of a Landau pole at finite energy, arising from quantum triviality. However, the question can only be answered non-perturbatively, since it involves strong coupling.\n\nA quantum field theory is said to be \"trivial\" when the renormalized coupling, computed through its beta function, goes to zero when the ultraviolet cutoff is removed. Consequently, the propagator becomes that of a free particle and the field is no longer interacting.\n\nFor a interaction, Michael Aizenman proved that the theory is indeed trivial, for space-time dimension ≥ 5.\n\nFor = 4, the triviality has yet to be proven rigorously, but lattice computations have provided strong evidence for this. This fact is important as quantum triviality can be used to bound or even \"predict\" parameters such as the Higgs boson mass. This can also lead to a predictable Higgs mass in asymptotic safety scenarios.\n\n\n\n"}
{"id": "173997", "url": "https://en.wikipedia.org/wiki?curid=173997", "title": "Special unitary group", "text": "Special unitary group\n\nIn mathematics, the special unitary group of degree , denoted , is the Lie group of unitary matrices with determinant 1.\n\nThe group operation is matrix multiplication. The special unitary group is a subgroup of the unitary group , consisting of all unitary matrices. As a compact classical group, is the group that preserves the standard inner product on . It is itself a subgroup of the general linear group, .\n\nThe groups find wide application in the Standard Model of particle physics, especially in the electroweak interaction and in quantum chromodynamics.\n\nThe simplest case, , is the trivial group, having only a single element. The group is isomorphic to the group of quaternions of norm 1, and is thus diffeomorphic to the 3-sphere. Since unit quaternions can be used to represent rotations in 3-dimensional space (up to sign), there is a surjective homomorphism from to the rotation group whose kernel is }. is also identical to one of the symmetry groups of spinors, Spin(3), that enables a spinor presentation of rotations.\n\nThe special unitary group is a real Lie group (though not a complex Lie group). Its dimension as a real manifold is . Topologically, it is compact and simply connected. Algebraically, it is a simple Lie group (meaning its Lie algebra is simple; see below).\n\nThe center of is isomorphic to the cyclic group , and is composed of the diagonal matrices for an root of unity and the \"n\"×\"n\" identity matrix.\n\nIts outer automorphism group, for , is , while the outer automorphism group of is the trivial group.\n\nA maximal torus, of rank , is given by the set of diagonal matrices with determinant 1. The Weyl group is the symmetric group , which is represented by signed permutation matrices (the signs being necessary to ensure the determinant is 1).\n\nThe Lie algebra of , denoted by formula_1, can be identified with the set of traceless antiHermitian complex matrices, with the regular commutator as Lie bracket. Particle physicists often use a different, equivalent representation: the set of traceless Hermitian complex matrices with Lie bracket given by times the commutator.\n\nThe Lie algebra formula_2 of formula_3 consists of formula_4 skew-Hermitian matrices with trace zero. This (real) Lie algebra has dimension formula_5. More information about the structure of this Lie algebra can be found below in the section \"Lie algebra structure.\"\n\nIn the physics literature, it is common to identify the Lie algebra with the space of trace-zero \"Hermitian\" (rather than the skew-Hermitian) matrices. That is to say, the physicists' Lie algebra differs by a factor of formula_6 from the mathematicians'. With this convention, one can then choose generators that are traceless Hermitian complex matrices, where:\n\nwhere the are the structure constants and are antisymmetric in all indices, while the -coefficients are symmetric in all indices.\n\nAs a consequence, the anticommutator and commutator are:\n\nThe factor of formula_6 in the commutation relations arises from the physics convention and is not present when using the mathematicians' convention.\n\nWe may also take\n\nas a normalization convention.\n\nIn the -dimensional adjoint representation, the generators are represented by × matrices, whose elements are defined by the structure constants themselves:\n\n is the following group,\n\nwhere the overline denotes complex conjugation.\n\nThere is a 2:1 homomorphism from SU(2) to SO(3).\n\nIf we consider formula_13 as a pair in formula_14 where formula_15 and formula_16, then the equation formula_17 becomes\n\nThis is the equation of the 3-sphere S. This can also be seen using an embedding: the map\n\nwhere denotes the set of 2 by 2 complex matrices, is an injective real linear map (by considering diffeomorphic to and diffeomorphic to ). Hence, the restriction of to the 3-sphere (since modulus is 1), denoted , is an embedding of the 3-sphere onto a compact submanifold of , namely .\n\nTherefore, as a manifold, is diffeomorphic to , which shows that can be endowed with the structure of a compact, connected Lie group.\n\nThe complex matrix:\n\ncan be mapped to the quaternion:\n\nThis map is in fact an isomorphism. Additionally, the determinant of the matrix is the norm of the corresponding quaternion. Clearly any matrix in is of this form and, since it has determinant 1, the corresponding quaternion has norm 1. Thus is isomorphic to the unit quaternions.\n\nThe Lie algebra of consists of formula_22 skew-Hermitian matrices with trace zero. Explicitly, this means\n\nThe Lie algebra is then generated by the following matrices,\nwhich have the form of the general element specified above.\n\nThese satisfy the quaternion relationships formula_25 formula_26 and formula_27 The commutator bracket is therefore specified by\n\nThe above generators are related to the Pauli matrices by formula_29 and formula_30 This representation is routinely used in quantum mechanics to represent the spin of fundamental particles such as electrons. They also serve as unit vectors for the description of our 3 spatial dimensions in loop quantum gravity.\n\nThe Lie algebra serves to work out the representations of.\n\nThe group SU(3) is a simply-connected, compact Lie group. Its topological structure can be understood by noting that SU(3) acts transitively on the unit sphere formula_31 in formula_32. The stabilizer of an arbitrary point in the sphere is isomorphic to SU(2), which topologically is a 3-sphere. It then follows that SU(3) is a fiber bundle over the base formula_31 with fiber formula_34. Since the fibers and the base are simply connected, the simple connectedness of SU(3) then follows by means of a standard topological result (the long exact sequence of homotopy groups for fiber bundles).\n\nThe representation theory of SU(3) is well understood. Descriptions of these representations, from the point of view of its complexified Lie algebra formula_35, may be found in the articles on Lie algebra representations or the Clebsch–Gordan coefficients for SU(3).\n\nThe generators, , of the Lie algebra su(3) of SU(3) in the defining representation, are:\n\nwhere , the Gell-Mann matrices, are the analog of the Pauli matrices for :\n\nThese span all traceless Hermitian matrices of the Lie algebra, as required. Note that are antisymmetric.\n\nThey obey the relations\n\nThe are the structure constants of the Lie algebra, given by:\n\nwhile all other not related to these by permutation are zero. In general, they vanish, unless they contain an odd number of indices from the set {2, 5, 7}.\n\nThe symmetric coefficients take the values:\n\nThey vanish if the number of indices from the set {2, 5, 7} is odd.\n\nA generic group element generated by a traceless 3×3 Hermitian matrix , normalized as , can be expressed as a \"second order\" matrix polynomial in :\n\nwhere\n\nAs noted above, the Lie algebra formula_2 of formula_3 consists of formula_46 skew-Hermitian matrices with trace zero.\n\nThe complexification of the Lie algebra formula_2 is formula_48, the space of all formula_46 complex matrices with trace zero. A Cartan subalgebra then consists of the diagonal matrices with trace zero, which we identify with vectors in formula_50 whose entries sum to zero. The roots then consist of all the permutations of .\n\nA choice of simple roots is\n\nSo, is of rank and its Dynkin diagram is given by , a chain of nodes: ... Its Cartan matrix is\n\nIts Weyl group or Coxeter group is the symmetric group , the symmetry group of the -simplex.\n\nFor a field , the generalized special unitary group over \"F\", , is the group of all linear transformations of determinant 1 of a vector space of rank over which leave invariant a nondegenerate, Hermitian form of signature . This group is often referred to as the special unitary group of signature over . The field can be replaced by a commutative ring, in which case the vector space is replaced by a free module.\n\nSpecifically, fix a Hermitian matrix of signature in , then all\n\nsatisfy\n\nOften one will see the notation without reference to a ring or field; in this case, the ring or field being referred to is and this gives one of the classical Lie groups. The standard choice for when is\n\nHowever, there may be better choices for for certain dimensions which exhibit more behaviour under restriction to subrings of .\n\nAn important example of this type of group is the Picard modular group which acts (projectively) on complex hyperbolic space of degree two, in the same way that acts (projectively) on real hyperbolic space of dimension two. In 2005 Gábor Francsics and Peter Lax computed an explicit fundamental domain for the action of this group on .\n\nA further example is , which is isomorphic to .\n\nIn physics the special unitary group is used to represent bosonic symmetries. In theories of symmetry breaking it is important to be able to find the subgroups of the special unitary group. Subgroups of that are important in GUT physics are, for ,\n\nwhere × denotes the direct product and , known as the circle group, is the multiplicative group of all complex numbers with absolute value 1.\n\nFor completeness, there are also the orthogonal and symplectic subgroups,\n\nSince the rank of is and of is 1, a useful check is that the sum of the ranks of the subgroups is less than or equal to the rank of the original group. is a subgroup of various other Lie groups,\nSee spin group, and simple Lie groups for E, E, and G.\n\nThere are also the accidental isomorphisms: , , and .\n\nOne may finally mention that is the double covering group of , a relation that plays an important role in the theory of rotations of 2-spinors in non-relativistic quantum mechanics.\n\n\n"}
{"id": "45351614", "url": "https://en.wikipedia.org/wiki?curid=45351614", "title": "Summa de arithmetica", "text": "Summa de arithmetica\n\nSumma de arithmetica, geometria, proportioni et proportionalita (\"Summary of arithmetic, geometry, proportions and proportionality\") is a book on mathematics written by Luca Pacioli and first published in 1494. It contains a comprehensive summary of Renaissance mathematics, including practical arithmetic, basic algebra, basic geometry and accounting, written in Italian for use as a textbook.\n\nThe \"Summa\" is the first printed work on algebra in a vernacular language, and it contains the first published description of the double-entry bookkeeping system. It set a new standard for writing and argumentation about algebra, and its impact upon the subsequent development and standardization of professional accounting methods was so great that Pacioli is sometimes referred to as the \"father of accounting.\"\n\nThe \"Summa de arithmetica\" as originally printed consists of ten chapters on a series of mathematical topics, collectively covering essentially all of Renaissance mathematics. The first seven chapters form a summary of arithmetic in 222 pages. The eighth chapter explains contemporary algebra in 78 pages. The ninth chapter discusses various topics relevant to business and trade, including barter, bills of exchange, weights and measures and bookkeeping, in 150 pages. The tenth and final chapter describes practical geometry (including basic trigonometry) in 151 pages. The work was dedicated to Guidobaldo da Montefeltro, Duke of Urbino, a patron of the arts whom Pacioli had met in Rome some years earlier.\n\nThe book's mathematical content draws heavily on the traditions of the abacus schools of contemporary northern Italy, where the children of merchants and the middle class studied arithmetic on the model established by Fibonacci's \"Liber Abaci\". The emphasis of this tradition was on facility with computation, using the Hindu–Arabic numeral system, developed through exposure to numerous example problems and case studies drawn principally from business and trade. Pacioli's work likewise teaches through examples, but it also develops arguments for the validity of its solutions through reference to general principles, axioms and logical proof. In this way the \"Summa\" begins to reintegrate the logical methods of classical Greek geometry into the medieval discipline of algebra.\n\nWithin the chapter on business, a section entitled \"Particularis de computis et scripturis\" (\"Details of calculation and recording\") describes the accounting methods then in use among northern-Italian merchants, including double-entry bookkeeping, trial balances, balance sheets and various other tools still employed by professional accountants. The business chapter also introduces the rule of 72 for predicting an investment's future value, anticipating the development of the logarithm by more than century.\n\nThese techniques did not originate with Pacioli, who merely recorded and explained the established best practices of contemporary businesspeople in his region; still, the \"Summa\"'s role in standardizing and disseminating professional bookkeeping methods has earned Pacioli a reputation as the \"father of accounting.\" \n\n\"Summa de arithmetica\" was composed over a period of decades through Pacioli's work as a professor of mathematics, and was probably intended as a textbook and reference work for students of mathematics and business, especially among the mercantile middle class of northern Italy. The book was originally published in Venice in 1494 by Paganino Paganini, with an identical second edition printed in 1523 in Toscolano.\n\nWhile the \"Summa\" contained little or no original mathematical work by Pacioli, it was the most comprehensive mathematical text ever published at the time. Its thoroughness and clarity (and the lack of any other similar work available in print) made it a basic point of reference for European mathematicians through the sixteenth century and beyond. The reputation the \"Summa\" earned Pacioli as a mathematician and intellectual inspired Ludovico Sforza, Duke of Milan, to invite him to serve as a mathematical lecturer in the ducal court, where Pacioli befriended and collaborated with Leonardo da Vinci.\n\nThe book also marks the beginning of a movement in sixteenth-century algebra toward the use of logical argumentation and theorems in the study of algebra, following the model of classical Greek geometry established by Euclid. It includes the first printed example of a set of plus and minus signs that were to become standard in Italian Renaissance mathematics: 'p' with a tilde above (p̄) for \"plus\" and 'm' with a tilde (m̄) for minus. Pacioli's (incorrect) assertion in the \"Summa\" that there was no general solution to cubic equations helped to popularize the problem among contemporary mathematicians, contributing to its eventual solution by Scipione del Ferro.\n\nIn 1994 Italy issued a 750-lira postage stamp honoring the 500th anniversary of the \"Summa\"'s publication, depicting Pacioli surrounded by mathematical and geometric implements. The image on the stamp was inspired by the \"Portrait of Luca Pacioli\", and contains many of the same elements.\n\n\n"}
{"id": "34709900", "url": "https://en.wikipedia.org/wiki?curid=34709900", "title": "Séminaire Lotharingien de Combinatoire", "text": "Séminaire Lotharingien de Combinatoire\n\nThe Séminaire Lotharingien de Combinatoire (Lotharingian Seminar of Combinatorics) is a peer-reviewed academic journal specialising in combinatorial mathematics, named after Lotharingia.\n\nIt has existed since 1980 as a regular joint seminar in Combinatorics for the Universities of Bayreuth, Erlangen and Strasbourg. In 1994, it was decided to create a journal under the same name. The regular meetings also continue to this day.\n\n\n"}
{"id": "32593444", "url": "https://en.wikipedia.org/wiki?curid=32593444", "title": "Tikhonov's theorem (dynamical systems)", "text": "Tikhonov's theorem (dynamical systems)\n\nIn applied mathematics, Tikhonov's theorem on dynamical systems is a result on stability of solutions of systems of differential equations. It has applications to chemical kinetics. The theorem is named after Andrey Nikolayevich Tikhonov.\n\nConsider this system of differential equations:\n\nTaking the limit as formula_2, this becomes the \"degenerate system\":\n\nwhere the second equation is the solution of the algebraic equation\n\nNote that there may be more than one such function formula_5.\n\nTikhonov's theorem states that as formula_6 the solution of the system of two differential equations above approaches the solution of the degenerate system if formula_7 is a stable root of the \"adjoined system\"\n"}
{"id": "17736871", "url": "https://en.wikipedia.org/wiki?curid=17736871", "title": "Universal algebraic geometry", "text": "Universal algebraic geometry\n\nIn algebraic geometry, universal algebraic geometry is generalized from the geometry of rings to geometry of arbitrary varieties of algebras, so that every \"variety of algebra\" has its own algebraic geometry. The two terms algebraic variety and \"variety of algebra\" should not be confused.\n\n\n"}
{"id": "34212179", "url": "https://en.wikipedia.org/wiki?curid=34212179", "title": "Victor Schlegel", "text": "Victor Schlegel\n\nVictor Schlegel (1843–1905) was a German mathematician. He is remembered for promoting the geometric algebra of Hermann Grassmann and for a method of visualizing polytopes called Schlegel diagrams.\n\nIn the nineteenth century there were various expansions of the traditional field of geometry through the innovations of hyperbolic geometry, non-Euclidean geometry and algebraic geometry. Hermann Grassmann was one of the more advanced innovators with his anticipation of linear algebra and multilinear algebra that he called \"Extension theory\" (\"Ausdehnungslehre\"). As recounted by David E. Rowe in 2010:\nIn 1872 Schlegel published the first part of his \"System der Raumlehre\" which used Grassmann’s methods to develop plane geometry. Schlegel used his book to put forth Grassmann’s case, arguing that\n\"Grassmann’s ideas had been neglected because he had not held a university chair.\"\nContinuing his criticism of academics, Schlegel expressed the reactionary view that\nSchlegel’s attitude was that no basis for scientific method was shown in mathematics, and \"neglect of foundations had led to the widely acknowledged lack of interest in mathematics in the schools.\"\n\nThe mathematician Felix Klein addressed Schlegel’s book in a review criticizing him for neglect of cross ratio and failure to contextualize Grassmann in the flow of mathematical developments. Rowe indicates that Klein was most interested in developing his Erlangen program.\nIn 1875 Schlegel countered with the second part of his \"System der Raumlehre\", answering Klein in the preface. This part developed conic sections, harmonic ranges, projective geometry, and determinants.\n\nSchlegel published a biography of Hermann Grassmann in 1878. Both parts of his textbook, and the biography, are now available at the Internet Archive; see External links section below.\n\nAt the Summer meeting of the American Mathematical Society on August 15, 1894, Schlegel presented an essay on the problem of finding the place which is at a minimum total distance from given points.\n\nIn 1899 Schlegel became German national secretary for the international Quaternion Society and reported on it in Monatshefte für Mathematik.\n\n"}
{"id": "33302", "url": "https://en.wikipedia.org/wiki?curid=33302", "title": "William Thomson, 1st Baron Kelvin", "text": "William Thomson, 1st Baron Kelvin\n\nWilliam Thomson, 1st Baron Kelvin, (26 June 1824 – 17 December 1907) was a Scots-Irish mathematical physicist and engineer who was born in Belfast in 1824. At the University of Glasgow he did important work in the mathematical analysis of electricity and formulation of the first and second laws of thermodynamics, and did much to unify the emerging discipline of physics in its modern form. He worked closely with mathematics professor Hugh Blackburn in his work. He also had a career as an electric telegraph engineer and inventor, which propelled him into the public eye and ensured his wealth, fame and honour. For his work on the transatlantic telegraph project he was knighted in 1866 by Queen Victoria, becoming Sir William Thomson. He had extensive maritime interests and was most noted for his work on the mariner's compass, which previously had limited reliability.\n\nAbsolute temperatures are stated in units of kelvin in his honour. While the existence of a lower limit to temperature (absolute zero) was known prior to his work, Lord Kelvin is known for determining its correct value as approximately −273.15 degree Celsius or −459.67 degree Fahrenheit.\n\nHe was ennobled in 1892 in recognition of his achievements in thermodynamics, and of his opposition to Irish Home Rule, becoming Baron Kelvin, of Largs in the County of Ayr. He was the first British scientist to be elevated to the House of Lords. The title refers to the River Kelvin, which flows near his laboratory at the University of Glasgow. His home was the red sandstone mansion Netherhall, in Largs. Despite offers of elevated posts from several world-renowned universities, Kelvin refused to leave Glasgow, remaining professor of Natural Philosophy for over 50 years, until his eventual retirement from that post. The Hunterian Museum at the University of Glasgow has a permanent exhibition on the work of Lord Kelvin including many of his original papers, instruments, and other artifacts, such as his smoking pipe.\n\nActive in industrial research and development, he was recruited around 1899 by George Eastman to serve as vice-chairman of the board of the British company Kodak Limited, affiliated with Eastman Kodak.\n\nWilliam Thomson's father, James Thomson, was a teacher of mathematics and engineering at the Royal Belfast Academical Institution and the son of a farmer. James Thomson married Margaret Gardner in 1817 and, of their children, four boys and two girls survived infancy. Margaret Thomson died in 1830 when William was six years old.\n\nWilliam and his elder brother James were tutored at home by their father while the younger boys were tutored by their elder sisters. James was intended to benefit from the major share of his father's encouragement, affection and financial support and was prepared for a career in engineering.\n\nIn 1832, his father was appointed professor of mathematics at Glasgow and the family moved there in October 1833. The Thomson children were introduced to a broader cosmopolitan experience than their father's rural upbringing, spending mid-1839 in London and the boys were tutored in French in Paris. Mid-1840 was spent in Germany and the Netherlands. Language study was given a high priority.\n\nHis sister, Anna Thomson, was the mother of James Thomson Bottomley FRSE (1845-1926).\n\nThomson had heart problems and nearly died when he was 9 years old. He attended the Royal Belfast Academical Institution, where his father was a professor in the university department, before beginning study at Glasgow University in 1834 at the age of 10, not out of any precociousness; the University provided many of the facilities of an elementary school for able pupils, and this was a typical starting age.\n\nIn school, Thomson showed a keen interest in the classics along with his natural interest in the sciences. At the age of 12 he won a prize for translating Lucian of Samosata's \"Dialogues of the Gods\" from Latin to English.\n\nIn the academic year 1839/1840, Thomson won the class prize in astronomy for his \"Essay on the figure of the Earth\" which showed an early facility for mathematical analysis and creativity. Throughout his life, he would work on the problems raised in the essay as a coping strategy during times of personal stress. On the title page of this essay Thomson wrote the following lines from Alexander Pope's \"Essay on Man\". These lines inspired Thomson to understand the natural world using the power and method of science:\n\nThomson became intrigued with Fourier's \"Théorie analytique de la chaleur\" and committed himself to study the \"Continental\" mathematics resisted by a British establishment still working in the shadow of Sir Isaac Newton. Unsurprisingly, Fourier's work had been attacked by domestic mathematicians, Philip Kelland authoring a critical book. The book motivated Thomson to write his first published scientific paper under the pseudonym \"P.Q.R.\", defending Fourier, and submitted to the \"Cambridge Mathematical Journal\" by his father. A second P.Q.R. paper followed almost immediately.\n\nWhile on holiday with his family in Lamlash in 1841, he wrote a third, more substantial, P.Q.R. paper \"On the uniform motion of heat in homogeneous solid bodies, and its connection with the mathematical theory of electricity\". In the paper he made remarkable connections between the mathematical theories of heat conduction and electrostatics, an analogy that James Clerk Maxwell was ultimately to describe as one of the most valuable \"science-forming ideas.\"\n\nWilliam's father was able to make a generous provision for his favourite son's education and, in 1841, installed him, with extensive letters of introduction and ample accommodation, at Peterhouse, Cambridge. In 1845 Thomson graduated as Second Wrangler. He also won the First Smith's Prize,\nwhich, unlike the tripos, is a test of original research. Robert Leslie Ellis, one of the examiners, is said to have declared to another examiner \"You and I are just about fit to mend his pens.\"\n\nWhile at Cambridge, Thomson was active in sports, athletics and sculling, winning the Colquhoun Sculls in 1843. He also took a lively interest in the classics, music, and literature; but the real love of his intellectual life was the pursuit of science. The study of mathematics, physics, and in particular, of electricity, had captivated his imagination.\nIn 1845, he gave the first mathematical development of Faraday's idea that electric induction takes place through an intervening medium, or \"dielectric\", and not by some incomprehensible \"action at a distance\". He also devised the mathematical technique of electrical images, which became a powerful agent in solving problems of electrostatics, the science which deals with the forces between electrically charged bodies at rest. It was partly in response to his encouragement that Faraday undertook the research in September 1845 that led to the discovery of the Faraday effect, which established that light and magnetic (and thus electric) phenomena were related.\n\nHe was elected a fellow of St. Peter's (as Peterhouse was often called at the time) in June 1845. On gaining the fellowship, he spent some time in the laboratory of the celebrated Henri Victor Regnault, at Paris; but in 1846 he was appointed to the chair of natural philosophy in the University of Glasgow. At twenty-two he found himself wearing the gown of a professor in one of the oldest Universities in the country, and lecturing to the class of which he was a first year student a few years before.\n\nBy 1847, Thomson had already gained a reputation as a precocious and maverick scientist when he attended the British Association for the Advancement of Science annual meeting in Oxford. At that meeting, he heard James Prescott Joule making yet another of his, so far, ineffective attempts to discredit the caloric theory of heat and the theory of the heat engine built upon it by Sadi Carnot and Émile Clapeyron. Joule argued for the mutual convertibility of heat and mechanical work and for their mechanical equivalence.\n\nThomson was intrigued but sceptical. Though he felt that Joule's results demanded theoretical explanation, he retreated into an even deeper commitment to the Carnot–Clapeyron school. He predicted that the melting point of ice must fall with pressure, otherwise its expansion on freezing could be exploited in a \"perpetuum mobile\". Experimental confirmation in his laboratory did much to bolster his beliefs.\n\nIn 1848, he extended the Carnot–Clapeyron theory further through his dissatisfaction that the gas thermometer provided only an operational definition of temperature. He proposed an \"absolute temperature scale\" in which \"a unit of heat descending from a body A at the temperature \"T\"° of this scale, to a body B at the temperature (\"T\"−1)°, would give out the same mechanical effect \"[work]\", whatever be the number\" T\".\" Such a scale would be \"quite independent of the physical properties of any specific substance.\" By employing such a \"waterfall\", Thomson postulated that a point would be reached at which no further heat (caloric) could be transferred, the point of \"absolute zero\" about which Guillaume Amontons had speculated in 1702. \"Reflections on the Motive Power of Heat\", published by Carnot in French in 1824, the year of Lord Kelvin's birth, used −267 as an estimate of the absolute zero temperature. Thomson used data published by Regnault to calibrate his scale against established measurements.\n\nIn his publication, Thomson wrote:\n\n— But a footnote signalled his first doubts about the caloric theory, referring to Joule's \"very remarkable discoveries\". Surprisingly, Thomson did not send Joule a copy of his paper, but when Joule eventually read it he wrote to Thomson on 6 October, claiming that his studies had demonstrated conversion of heat into work but that he was planning further experiments. Thomson replied on 27 October, revealing that he was planning his own experiments and hoping for a reconciliation of their two views.\n\nThomson returned to critique Carnot's original publication and read his analysis to the Royal Society of Edinburgh in January 1849, still convinced that the theory was fundamentally sound. However, though Thomson conducted no new experiments, over the next two years he became increasingly dissatisfied with Carnot's theory and convinced of Joule's. In February 1851 he sat down to articulate his new thinking. He was uncertain of how to frame his theory and the paper went through several drafts before he settled on an attempt to reconcile Carnot and Joule. During his rewriting, he seems to have considered ideas that would subsequently give rise to the second law of thermodynamics. In Carnot's theory, lost heat was \"absolutely lost\" but Thomson contended that it was \"\"lost to man\" irrecoverably; but not lost in the material world\". Moreover, his theological beliefs led to speculation about the heat death of the universe.\n\nCompensation would require \"a creative act or an act possessing similar power\".\n\nIn final publication, Thomson retreated from a radical departure and declared \"the whole theory of the motive power of heat is founded on ... two ... propositions, due respectively to Joule, and to Carnot and Clausius.\" Thomson went on to state a form of the second law:\n\nIn the paper, Thomson supported the theory that heat was a form of motion but admitted that he had been influenced only by the thought of Sir Humphry Davy and the experiments of Joule and Julius Robert von Mayer, maintaining that experimental demonstration of the conversion of heat into work was still outstanding.\n\nAs soon as Joule read the paper he wrote to Thomson with his comments and questions. Thus began a fruitful, though largely epistolary, collaboration between the two men, Joule conducting experiments, Thomson analysing the results and suggesting further experiments. The collaboration lasted from 1852 to 1856, its discoveries including the Joule–Thomson effect, sometimes called the Kelvin–Joule effect, and the published results did much to bring about general acceptance of Joule's work and the kinetic theory.\n\nThomson published more than 650 scientific papers and applied for 70 patents (not all were issued). Regarding science, Thomson wrote the following.\n\nThough now eminent in the academic field, Thomson was obscure to the general public. In September 1852, he married childhood sweetheart Margaret Crum, daughter of Walter Crum; but her health broke down on their honeymoon, and over the next seventeen years, Thomson was distracted by her suffering. On 16 October 1854, George Gabriel Stokes wrote to Thomson to try to re-interest him in work by asking his opinion on some experiments of Michael Faraday on the proposed transatlantic telegraph cable.\n\nFaraday had demonstrated how the construction of a cable would limit the rate at which messages could be sent – in modern terms, the bandwidth. Thomson jumped at the problem and published his response that month. He expressed his results in terms of the data rate that could be achieved and the economic consequences in terms of the potential revenue of the transatlantic undertaking. In a further 1855 analysis, Thomson stressed the impact that the design of the cable would have on its profitability.\n\nThomson contended that the signalling speed through a given cable was inversely proportional to the square of the length of the cable. Thomson's results were disputed at a meeting of the British Association in 1856 by Wildman Whitehouse, the electrician of the Atlantic Telegraph Company. Whitehouse had possibly misinterpreted the results of his own experiments but was doubtless feeling financial pressure as plans for the cable were already well under way. He believed that Thomson's calculations implied that the cable must be \"abandoned as being practically and commercially impossible\".\n\nThomson attacked Whitehouse's contention in a letter to the popular \"Athenaeum\" magazine, pitching himself into the public eye. Thomson recommended a larger conductor with a larger cross section of insulation. He thought Whitehouse no fool, and suspected that he might have the practical skill to make the existing design work. Thomson's work had attracted the attention of the project's undertakers. In December 1856, he was elected to the board of directors of the Atlantic Telegraph Company.\n\nThomson became scientific adviser to a team with Whitehouse as chief electrician and Sir Charles Tilston Bright as chief engineer but Whitehouse had his way with the specification, supported by Faraday and Samuel F. B. Morse.\n\nThomson sailed on board the cable-laying ship in August 1857, with Whitehouse confined to land owing to illness, but the voyage ended after when the cable parted. Thomson contributed to the effort by publishing in the \"Engineer\" the whole theory of the stresses involved in the laying of a submarine cable, and showed that when the line is running out of the ship, at a constant speed, in a uniform depth of water, it sinks in a slant or straight incline from the point where it enters the water to that where it touches the bottom.\n\nThomson developed a complete system for operating a submarine telegraph that was capable of sending a character every 3.5 seconds. He patented the key elements of his system, the mirror galvanometer and the siphon recorder, in 1858.\n\nWhitehouse still felt able to ignore Thomson's many suggestions and proposals. It was not until Thomson convinced the board that using purer copper for replacing the lost section of cable would improve data capacity, that he first made a difference to the execution of the project.\n\nThe board insisted that Thomson join the 1858 cable-laying expedition, without any financial compensation, and take an active part in the project. In return, Thomson secured a trial for his mirror galvanometer, which the board had been unenthusiastic about, alongside Whitehouse's equipment. Thomson found the access he was given unsatisfactory and the \"Agamemnon\" had to return home following the disastrous storm of June 1858. In London, the board was about to abandon the project and mitigate their losses by selling the cable. Thomson, Cyrus West Field and Curtis M. Lampson argued for another attempt and prevailed, Thomson insisting that the technical problems were tractable. Though employed in an advisory capacity, Thomson had, during the voyages, developed a real engineer's instincts and skill at practical problem-solving under pressure, often taking the lead in dealing with emergencies and being unafraid to assist in manual work. A cable was completed on 5 August.\n\nThomson's fears were realized when Whitehouse's apparatus proved insufficiently sensitive and had to be replaced by Thomson's mirror galvanometer. Whitehouse continued to maintain that it was his equipment that was providing the service and started to engage in desperate measures to remedy some of the problems. He succeeded in fatally damaging the cable by applying 2,000 V. When the cable failed completely Whitehouse was dismissed, though Thomson objected and was reprimanded by the board for his interference. Thomson subsequently regretted that he had acquiesced too readily to many of Whitehouse's proposals and had not challenged him with sufficient vigor.\n\nA joint committee of inquiry was established by the Board of Trade and the Atlantic Telegraph Company. Most of the blame for the cable's failure was found to rest with Whitehouse. The committee found that, though underwater cables were notorious in their lack of reliability, most of the problems arose from known and avoidable causes. Thomson was appointed one of a five-member committee to recommend a specification for a new cable. The committee reported in October 1863.\n\nIn July 1865, Thomson sailed on the cable-laying expedition of the but the voyage was dogged by technical problems. The cable was lost after had been laid and the project was abandoned. A further attempt in 1866 laid a new cable in two weeks , and the recover and complete the 1865 cable. The enterprise was now feted as a triumph by the public and Thomson enjoyed a large share of the adulation. Thomson, along with the other principals of the project, was knighted on 10 November 1866.\n\nTo exploit his inventions for signalling on long submarine cables, Thomson now entered into a partnership with C.F. Varley and Fleeming Jenkin. In conjunction with the latter, he also devised an automatic curb sender, a kind of telegraph key for sending messages on a cable.\n\nThomson took part in the laying of the French Atlantic submarine communications cable of 1869, and with Jenkin was engineer of the Western and Brazilian and Platino-Brazilian cables, assisted by vacation student James Alfred Ewing. He was present at the laying of the Pará to Pernambuco section of the Brazilian coast cables in 1873.\n\nThomson's wife died on 17 June 1870, and he resolved to make changes in his life. Already addicted to seafaring, in September he purchased a 126 ton schooner, the \"Lalla Rookh\" and used it as a base for entertaining friends and scientific colleagues. His maritime interests continued in 1871 when he was appointed to the board of enquiry into the sinking of .\n\nIn June 1873, Thomson and Jenkin were on board the \"Hooper\", bound for Lisbon with of cable when the cable developed a fault. An unscheduled 16-day stop-over in Madeira followed and Thomson became good friends with Charles R. Blandy and his three daughters. On 2 May 1874 he set sail for Madeira on the \"Lalla Rookh\". As he approached the harbour, he signalled to the Blandy residence \"Will you marry me?\" and Fanny signalled back \"Yes\". Thomson married Fanny, 13 years his junior, on 24 June 1874.\n\nOver the period 1855 to 1867, Thomson collaborated with Peter Guthrie Tait on a text book that founded the study of mechanics first on the mathematics of kinematics, the description of motion without regard to force. The text developed dynamics in various areas but with constant attention to energy as a unifying principle.\n\nA second edition appeared in 1879, expanded to two separately bound parts. The textbook set a standard for early education in mathematical physics.\n\nBetween 1870 and 1890 a theory purporting that an atom was a vortex in the aether was popular among British physicists and mathematicians. About 60 scientific papers were written by approximately 25 scientists. Following the lead of Thomson and Tait, the branch of topology called knot theory was developed. Kelvin's initiative in this complex study that continues to inspire new mathematics has led to persistence of the topic in history of science.\n\nThomson was an enthusiastic yachtsman, his interest in all things relating to the sea perhaps arising from, or fostered by, his experiences on the \"Agamemnon\" and the \"Great Eastern\".\n\nThomson introduced a method of deep-sea depth sounding, in which a steel piano wire replaces the ordinary hand line. The wire glides so easily to the bottom that \"flying soundings\" can be taken while the ship is at full speed. A pressure gauge to register the depth of the sinker was added by Thomson.\n\nAbout the same time he revived the Sumner method of finding a ship's position, and calculated a set of tables for its ready application. In 1876, he constructed a harmonic analyzer, in which an assembly of disks were used to sum trigonometric series and thus to predict tides. Kelvin mentioned that a similar device could be built to solve differential equations.\n\nDuring the 1880s, Thomson worked to perfect the adjustable compass to correct errors arising from magnetic deviation owing to the increased use of iron in naval architecture. Thomson's design was a great improvement on the older instruments, being steadier and less hampered by friction. The deviation due to the ship's magnetism was corrected by movable iron masses at the binnacle. Thomson's innovations involved much detailed work to develop principles identified by George Biddell Airy and others, but contributed little in terms of novel physical thinking. Thomson's energetic lobbying and networking proved effective in gaining acceptance of his instrument by The Admiralty.\n\nCharles Babbage had been among the first to suggest that a lighthouse might be made to signal a distinctive number by occultations of its light, but Thomson pointed out the merits of the Morse code for the purpose, and urged that the signals should consist of short and long flashes of the light to represent the dots and dashes.\n\nThomson did more than any other electrician up to his time in introducing accurate methods and apparatus for measuring electricity. As early as 1845 he pointed out that the experimental results of William Snow Harris were in accordance with the laws of Coulomb. In the \"Memoirs of the Roman Academy of Sciences\" for 1857 he published a description of his new divided ring electrometer, based on the old electroscope of Johann Gottlieb Friedrich von Bohnenberger and he introduced a chain or series of effective instruments, including the quadrant electrometer, which cover the entire field of electrostatic measurement. He invented the current balance, also known as the \"Kelvin balance\" or \"Ampere balance\" (\"SiC\"), for the precise specification of the ampere, the standard unit of electric current. From around 1880 he was aided by the electrical engineer Magnus Maclean FRSE in his electrical experiments.\n\nIn 1893, Thomson headed an international commission to decide on the design of the Niagara Falls power station. Despite his belief in the superiority of direct current electric power transmission, he endorsed Westinghouse's alternating current system which had been demonstrated at the Chicago World's Fair of that year. Even after Niagara Falls Thomson still held to his belief that direct current was the superior system.\n\nAcknowledging his contribution to electrical standardisation, the International Electrotechnical Commission elected Thomson as its first President at its preliminary meeting, held in London on 26–27 June 1906. \"On the proposal of the President [Mr Alexander Siemens, Great Britain], secounded [sic] by Mr Mailloux [US Institute of Electrical Engineers] the Right Honorable Lord Kelvin, G.C.V.O., O.M., was unanimously elected first President of the Commission\", minutes of the Preliminary Meeting Report read.\n\nThomson remained a devout believer in Christianity throughout his life; attendance at chapel was part of his daily routine. He saw his Christian faith as supporting and informing his scientific work, as is evident from his address to the annual meeting of the Christian Evidence Society, 23 May 1889.\n\nOne of the clearest instances of this interaction is in his estimate of the age of the Earth. Given his youthful work on the figure of the Earth and his interest in heat conduction, it is no surprise that he chose to investigate the Earth's cooling and to make historical inferences of the Earth's age from his calculations. Thomson was a creationist in a broad sense, but he was not a 'flood geologist'. He contended that the laws of thermodynamics operated from the birth of the universe and envisaged a dynamic process that saw the organisation and evolution of the solar system and other structures, followed by a gradual \"heat death\". He developed the view that the Earth had once been too hot to support life and contrasted this view with that of uniformitarianism, that conditions had remained constant since the indefinite past. He contended that \"This earth, certainly a moderate number of millions of years ago, was a red-hot globe ... .\"\n\nAfter the publication of Charles Darwin's \"On the Origin of Species\" in 1859, Thomson saw evidence of the relatively short habitable age of the Earth as tending to contradict Darwin's gradualist explanation of slow natural selection bringing about biological diversity. Thomson's own views favoured a version of theistic evolution sped up by divine guidance. His calculations showed that the Sun could not have possibly existed long enough to allow the slow incremental development by evolution – unless some energy source beyond what he or any other Victorian era person knew of was found. He was soon drawn into public disagreement with geologists, and with Darwin's supporters John Tyndall and T.H. Huxley. In his response to Huxley's address to the Geological Society of London (1868) he presented his address \"Of Geological Dynamics\" (1869) which, among his other writings, challenged the geologists' acceptance that the earth must be of indefinite age.\n\nThomson's initial 1864 estimate of the Earth's age was from 20 to 400 million years old. These wide limits were due to his uncertainty about the melting temperature of rock, to which he equated the earth's interior temperature, as well as the uncertainty in thermal conductivities and specific heats of rocks. Over the years he refined his arguments and reduced the upper bound by a factor of ten, and in 1897 Thomson, now Lord Kelvin, ultimately settled on an estimate that the Earth was 20–40 million years old. In a letter published in Scientific American Supplement 1895 Kelvin criticized geologist’s estimates of the age of rocks and the age of the earth, including the views published by Darwin, as “vaguely vast age”.\n\nHis exploration of this estimate can be found in his 1897 address to the Victoria Institute, given at the request of the Institute's president George Stokes, as recorded in that Institute's journal \"Transactions\". Although his former assistant John Perry published a paper in 1895 challenging Kelvin's assumption of low thermal conductivity inside the Earth, and thus showing a much greater age, this had little immediate impact. The discovery in 1903 that radioactive decay releases heat led to Kelvin's estimate being challenged, and Ernest Rutherford famously made the argument in a lecture attended by Kelvin that this provided the unknown energy source Kelvin had suggested, but the estimate was not overturned until the development in 1907 of radiometric dating of rocks.\n\nIt was widely believed that the discovery of radioactivity had invalidated Thomson's estimate of the age of the Earth. Thomson himself never publicly acknowledged this because he thought he had a much stronger argument restricting the age of the Sun to no more than 20 million years. Without sunlight, there could be no explanation for the sediment record on the Earth's surface. At the time, the only known source for the solar power output was gravitational collapse. It was only when thermonuclear fusion was recognised in the 1930s that Thomson's age paradox was truly resolved.\n\nIn the winter of 1860–1861 Kelvin slipped on some ice and fractured his leg, causing him to limp thereafter. He remained something of a celebrity on both sides of the Atlantic until his death.\n\nIn the 1902 Coronation Honours list published on 26 June 1902 (the original day of King Edward VII´s coronation), Lord Kelvin was appointed a Privy Counsellor and one of the first members of the new Order of Merit (OM). He received the order from the King on 8 August 1902, and was sworn a member of the council at Buckingham Palace on 11 August 1902.\n\nIn November 1907 he caught a chill and his condition deteriorated until he died at his Scottish residence, Netherhall, in Largs on 17 December.\n\nAt the request of Westminster Abbey, the undertakers Wylie & Lochhead prepared an oak coffin, lined with lead. In the dark of the winter evening the cortege set off from Netherhall for Largs railway station, a distance of about a mile. Large crowds witnessed the passing of the cortege, and shopkeepers closed their premises and dimmed their lights. The coffin was placed in a special Midland and Glasgow and South-Western Railway van. The train set off at 8.30 pm for Kilmarnock, where the van was attached to the overnight express to St Pancras railway station in London.\n\nLord Kelvin’s coffin was then taken by hearse to Westminster Abbey where it rested overnight in St Faith’s Chapel. The following day the Abbey was crowded for the funeral, including representatives from the University of Glasgow and the University of Cambridge, along with representatives from France, Italy, Germany, Austria, Russia, the United States, Canada, Australia, Japan, and Monaco. Lord Kelvin’s grave is in the nave, near the choir screen, and close to the graves of Isaac Newton, John Herschel, and Charles Darwin. The pall-bearers included Darwin’s son, Sir George Darwin.\n\nBack in Scotland the University of Glasgow held a memorial service for Lord Kelvin in the Bute Hall. Lord Kelvin had been a member of the Scottish Episcopal Church, attached to St Columba’s Episcopal Church in Largs, and when in Glasgow to St Mary’s Episcopal Church (now, St Mary's Cathedral, Glasgow). At the same time as the funeral in Westminster Abbey, a service was held in St Columba’s Episcopal Church, Largs, attended by a large congregation including burgh dignitaries.\n\nWilliam Thomson is also memorialised on the Thomson family grave in Glasgow Necropolis. The family grave has a second modern memorial to William alongside, erected by the Royal Philosophical Society of Glasgow; a society that he was president of in the periods 1856-58 and 1874-77.\n\nIn 1884, Thomson led a master class on \"Molecular Dynamics and the Wave Theory of Light\" at Johns Hopkins University. Kelvin referred to the acoustic wave equation describing sound as waves of pressure in air and attempted to describe also an electromagnetic wave equation, presuming a luminiferous aether susceptible to vibration. The study group included Michelson and Morley who subsequently performed the Michelson-Morley experiment that undercut the aether theory. Thomson did not provide a text but A. S. Hathaway took notes and duplicated them with a Papyrograph. As the subject matter was under active development, Thomson amended that text and in 1904 it was typeset and published. Thomson's attempts to provide mechanical models ultimately failed in the electromagnetic regime.\n\nOn 27 April 1900 he gave a widely reported lecture titled \"Nineteenth-Century Clouds over the Dynamical Theory of Heat and Light\" to the Royal Institution. The two \"dark clouds\" he was alluding to were confusion surrounding how matter moves through the aether (including the puzzling results of the Michelson–Morley experiment) and indications that the Law of Equipartition in statistical mechanics might break down. Two major physical theories were developed during the twentieth century starting from these issues: for the former, the theory of relativity; for the second, quantum mechanics. Albert Einstein, in 1905, published the so-called \"Annus Mirabilis papers\", one of which explained the photoelectric effect and was a precursor of quantum mechanics, another of which described special relativity, and the last of which explained Brownian motion in terms of statistical mechanics, providing a strong argument for the existence of atoms.\n\nLike many scientists, Thomson made some mistakes in predicting the future of technology.\n\nHis biographer Silvanus P. Thompson writes that \"When Röntgen's discovery of the X-rays was announced at the end of 1895, Lord Kelvin was entirely skeptical, and regarded the announcement as a hoax. The papers had been full of the wonders of Röntgen's rays, about which Lord Kelvin was intensely skeptical until Röntgen himself sent him a copy of his Memoir\"; on 17 January 1896, having read the paper & seen the photographs, he wrote Röntgen a letter saying that \"I need not tell you that when I read the paper I was very much astonished and delighted. I can say no more now than to congratulate you warmly on the great discovery you have made\" He would have his own hand X-rayed in May 1896. (See also N rays.)\n\nHis forecast for practical aviation (i.e., heavier-than-air aircraft) was negative. In 1896 he refused an invitation to join the Aeronautical Society, writing that \"I have not the smallest molecule of faith in aerial navigation other than ballooning or of expectation of good results from any of the trials we hear of.\" And in a 1902 newspaper interview he predicted that \"No balloon and no aeroplane will ever be practically successful.\"\n\nThe statement \"There is nothing new to be discovered in physics now. All that remains is more and more precise measurement\" has been widely misattributed to Kelvin since the 1980s, either without citation or stating that it was made in an address to the British Association for the Advancement of Science (1900). There is no evidence that Kelvin said this, and the quote is instead a paraphrase of Albert A. Michelson, who in 1894 stated: \"… it seems probable that most of the grand underlying principles have been firmly established … An eminent physicist remarked that the future truths of physical science are to be looked for in the sixth place of decimals.\" Similar statements were given earlier by others, such as Philipp von Jolly. The attribution to Kelvin giving an address in 1900 is presumably a confusion with his \"Two clouds\" speech, delivered to the Royal Institution in 1900 (see above), and which on the contrary pointed out areas that would subsequently see revolutions.\n\nIn 1898, Kelvin predicted that only 400 years of oxygen supply remained on the planet, due to the rate of burning combustibles. In his calculation, Kelvin assumed that photosynthesis was the only source of free oxygen; he did not know all of the components of the oxygen cycle. He could not even have known all of the sources of photosynthesis: for example the cyanobacterium \"Prochlorococcus\"—which accounts for more than half of marine photosynthesis—was not discovered until 1986.\n\nA variety of physical phenomena and concepts with which Thomson is associated are named \"Kelvin\":\n\n\n\n\n\n\n \n"}
{"id": "2279544", "url": "https://en.wikipedia.org/wiki?curid=2279544", "title": "Women in computing", "text": "Women in computing\n\nWomen in computing have shaped the evolution of information technology. They were among the first programmers in the early-20th century, and contributed substantially to the industry. As technology and practices altered, the role of women as programmers has changed, and the recorded history of the field has downplayed their achievements.\n\nSince the 18th century, women have developed scientific computations, including Nicole-Reine Lepaute's prediction of Halley's Comet, and Maria Mitchell's computation of the motion of Venus. The first algorithm intended to be executed by a computer was designed by Ada Lovelace who was a pioneer in the field. Grace Hopper was the first person to design a compiler for a programming language. Throughout the 19th and early-20th century, and up to World War II, programming was predominantly done by women; significant examples include the Harvard Computers, codebreaking at Bletchley Park and engineering at NASA.\n\nAfter the 1960s, the \"soft work\" that had been dominated by women evolved into modern software, and the importance of women decreased. The gender disparity and the lack of women in computing from the late 20th century onward has been examined, but no firm explanations have been established. Nevertheless, many women continued to make significant and important contributions to the IT industry, and attempts were made to readdress the gender disparity in the industry. In the 21st century, women held leadership roles in multiple tech companies, such as Meg Whitman, president and chief executive officer of Hewlett Packard Enterprise, and Marissa Mayer, president and CEO of Yahoo! and key spokesperson at Google.\n\nNicole-Reine Etable de la Brière Lepaute was one of a team of human computers who worked with Alexis-Claude Clairaut and Joseph-Jérôme Le Français de Lalande to predict the date of the return of Halley's Comet. They began work on the calculations in 1757, working throughout the day and sometimes during mealtimes. Their methods were followed by successive human computers. They divided large calculations into \"independent pieces, assembled the results from each piece into a final product\" and then checked for errors. Lepaute continued to work on computing for the rest of her life, working for the \"Connaissance de Temps\" and publishing predictions of solar eclipses.\n\nOne of the first computers for the American \"Nautical Almanac\" was Maria Mitchel. Her work on the assignment was to compute the motion of the planet Venus. The \"Almanac\" never became a reality, but Mitchell became the first astronomy professor at Vassar.\n\nAda Lovelace was the first person to publish an algorithm intended to be executed by the first modern computer, the Analytical Engine created by Charles Babbage. As a result she is often regarded as the first computer programmer. Lovelace was introduced to Babbage's difference engine when she was 17. In 1840, she wrote to Babbage and asked if she could become involved with his first machine. By this time, Babbage had moved on to his idea for the Analytical Engine. A paper describing the Analytical Engine, \"Notions sur la machine analytique\", published by L.F. Menabrea, came to the attention of Lovelace, who not only translated it into English, but corrected mistakes made by Menabrea. Babbage suggested that she expand the translation of the paper with her own ideas, which, signed only with her initials, AAL, \"synthesized the vast scope of Babbage's vision.\" Lovelace imagined the kind of impact of the Analytical Engine might have on society. She drew up explanations of how the engine could handle inputs, outputs, processing and data storage. She also created several proofs to show how the engine would handle calculations of Bernoulli Numbers on its own. The proofs are considered the first examples of a computer program. Lovelace downplayed her role in her work during her life, for example, in signing her contributions with AAL so as not be \"accused of bragging.\"\n\nAfter the Civil War in the United States, more women were hired as human computers. Many were war widows looking for ways to support themselves. Others were hired when the government opened positions to women because of a shortage of men to fill the roles.\nAnna Winlock asked to become a computer for the Harvard Observatory in 1875 and was hired to work for 25 cents an hour. By 1880, Edward Charles Pickering had hired several women to work for him at Harvard because he felt that women could do the job as well as men and he could ask them to volunteer or work for less pay. The women, described as \"Pickering's harem\" and also as the Harvard Computers, performed clerical work that the male employees and scholars considered to be tedious at a fraction of the cost of hiring a man. The women working for Pickering cataloged around ten thousand stars, discovered the Horsehead Nebula and developed the system to describe stars. One of the \"computers,\" Annie Jump Cannon, could classify stars at a rate of three stars per minute. The work for Pickering became so popular that women volunteered to work for free even when the computers were being paid. Even though they performed an important role, the Harvard Computers were paid less than factory workers.\n\nBy the 1890s, women computers were college graduates looking for jobs where they could use their training in a useful way. Florence Tebb Weldon, was part of this group and provided computations relating to biology and evidence for evolution, working with her husband, W.F. Raphael Weldon. Florence Weldon's calculations demonstrated that statistics could be used to support Darwin's theory of evolution. Another human computer involved in biology was Alice Lee, who worked with Karl Pearson. Pearson hired two sisters to work as part-time computers at his Biometrics Lab, Beatrice and Frances Cave-Brown-Cave.\n\nDuring World War I, Karl Pearson and his Biometrics Lab helped produce ballistics calculations for the British Ministry of Munitions. Beatrice Cave-Brown-Cave helped calculate trajectories for bomb shells. In 1916, Cave-Brown-Cave left Pearson's employ and started working full-time for the Ministry. In the United States, women computers were hired to calculate ballistics in 1918, working in a building on the Washington Mall. One of the women, Elizabeth Webb Wilson, worked as the chief computer. After the war, women who worked as ballistics computers for the U.S. government had trouble finding jobs in computing and Wilson eventually taught high school math.\n\nIn the early 1920s, Iowa State College, professor George Snedecor worked to improve the school's science and engineering departments, experimenting with new punch-card machines and calculators. Snedecor also worked with human calculators most of them women, including Mary Clem. Clem coined the term \"zero check\" to help identify errors in calculations. The computing lab, run by Clem, became one of the most powerful computing facilities of the time. \n\nWomen computers also worked at the American Telephone and Telegraph company. These human computers worked with electrical engineers to help figure out how to boost signals with vacuum tube amplifiers. One of the computers, Clara Froelich, was eventually moved along with the other computers to their own division where they worked with a mathematician, Thornton Fry, to create new computational methods. Froelich studied IBM tabulating equipment and desk calculating machines to see if she could adapt the machine method to calculations.\n\nEdith Clarke was the first woman to earn a degree in electrical engineering and who worked as the first professionally employed electrical engineer in the United States. She was hired by General Electric as a full engineer in 1923. Clarke also filed a patent in 1921 for a graphical calculator to be used in solving problems in power lines. It was granted in 1925.\n\nThe National Advisory Committee for Aeronautics (NACA) which became NASA hired a group of five women in 1935 to work as a computer pool. The women worked on the data coming from wind tunnel and flight tests.\n\n\"Tedious\" computing and calculating was seen as \"women's work\" through the 1940s resulting in the term \"kilogirl\", invented by a member of the Applied Mathematics Panel in the early 1940s. A kilogirl of energy was \"equivalent to roughly a thousand hours of computing labor.\" While women's contributions to the United States war effort during World War II was championed in the media, their roles and the work they did was minimized. This included minimizing the complexity, skill and knowledge needed to work on computers or work as human computers. During WWII, women did most of the ballistics computing, seen by male engineers as being below their level of expertise. Black women computers worked as hard (or more often, twice as hard) as their white counterparts, but in segregated situations. By 1943, almost all people employed as computers were women.\n\nNACA expanded its a pool of women human computers in the 1940s. NACA recognized in 1942 that \"the engineers admit themselves that the girl computers do the work more rapidly and accurately than they could.\" In 1943 two groups, segregated by race, worked on the east and west side of Langley Air Force Base. The black women were the West Area Computers. Unlike their white counterparts, the black women were asked by NACA to re-do college courses they had already passed and many never received promotions.\n\nWomen were also working on ballistic missile calculations. In 1948, women such as Barbara Paulson were working on the WAC Corporal, determining trajectories the missiles would take after launch.\n\nWomen worked with cryptography and, after some initial resistance, many operated and worked on the Bombe machines. Joyce Aylard operated the Bombe machine testing different methods to break the Enigma code. Joan Clarke was a cryptographer who worked with her friend, Alan Turing, on the Enigma machine at Bletchley Park. When she was promoted to a higher salary grade, there were no positions in the civil service for a \"senior female cryptanalyst,\" and she was listed as a linguist instead. While Clarke developed a method of increasing the speed of double-encrypted messages, unlike many of the men, her decryption technique was not named after her. Other cryptographers at Bletchley included Margaret Rock, Mavis Lever (later Batey), Ruth Briggs and Kerry Howard. In 1941, Batey's work enabled the Allies to break the Italian's naval code before the Battle of Cape Matapan. In the United States, several faster Bombe machines were created. Women, like Louise Pearsall, were recruited from the WAVES to work on code breaking and operate the American Bombe machines.\n\nHedy Lamarr and co-inventor, George Antheil, worked on a frequency hopping method to help the Navy control torpedoes remotely. The Navy passed on their idea, but Lamarr and Antheil received a patent for the work on August 11, 1942. This technique would later be used again, first in the 1950s at Sylvania Electronic Systems Division and is used in everyday technology such as Bluetooth and Wi-Fi.\n\nThe programmers of the ENIAC computer in 1944, were six female mathematicians; Marlyn Meltzer, Betty Holberton, Kathleen Antonelli, Ruth Teitelbaum, Jean Bartik, and Frances Spence who were human computers at the Moore School's computation lab. Adele Goldstine was their teacher and trainer and they were known as the \"ENIAC girls.\" The women who worked on ENIAC were warned that they would not be promoted into professional ratings which were only for men. Designing the hardware was \"men's work\" and programming the software was \"women's work.\" Sometimes women were given blueprints and wiring diagrams to figure out how the machine worked and how to program it. They learned how the ENIAC worked by repairing it, sometimes crawling through the computer, and by fixing \"bugs\" in the machinery. Even though the programmers were supposed to be doing the \"soft\" work of programming, in reality, they did that and fully understood and worked with the hardware of the ENIAC. When the ENIAC was revealed in 1946, Goldstine and the other women prepared the machine and the demonstration programs it ran for the public. None of their work in preparing the demonstrations was mentioned in the official accounts of the public events. After the demonstration, the university hosted an expensive celebratory dinner to which none of the ENIAC six were invited.\n\nIn Canada, Beatrice Worsley started working at the National Research Council of Canada in 1947 where she was an aerodynamics research officer. A year later, she started working in the new Computational Centre at the University of Toronto. She built a differential analyzer in 1948 and also worked with IBM machines in order to do calculations for Atomic Energy of Canada Limited. She went to study the EDSAC at the University of Cambridge in 1949. She wrote the program that was run the first time EDSAC performed its first calculations on May 6, 1949. \n\nGrace Hopper was the first person to create a compiler for a programming language and one of the first programmers of the Harvard Mark I computer, an electro-mechanical computer based on Analytical Engine. Hopper's work with computers started in 1943, when she started working at the Bureau of Ordnance's Computation Project at Harvard where she programmed the Harvard Mark I. Hopper not only programmed the computer, but created a 500 page comprehensive manual for it. Even though Hopper created the manual which was widely cited and published, she was not specifically credited in the manual. Hopper is often credited with the coining of the term \"bug\" and \"debugging\" when a moth caused the Mark II to malfunction. While a moth was found and the process of removing it called \"debugging,\" the terms were already part of the language of programmers.\n\nGrace Hopper continued to contribute to computer science through the 1950s. She brought the idea of using compilers from her time at Harvard to UNIVAC which she joined in 1949. Other women who were hired to program UNIVAC included Adele Mildred Koss, Frances E. Holberton, Jean Bartik, Frances Morello and Lillian Jay. To program the UNIVAC, Hopper and her team used the FLOW-MATIC programming language, which she developed. Holberton wrote a code, C-10, that allowed for keyboard inputs into a general-purpose computer. Holberton also developed the Sort-Merge Generator in 1951 which was used on the UNIVAC I. The Sort-Merge Generator marked the first time a computer \"used a program to write a program.\" Holberton suggested that computer housing should be beige or oatmeal in color which became a long-lasting trend. Koss worked with Hopper on various algorithms and a program that was a precursor to a report generator.\n\nKlara Dan von Neumann was one of the main programmers of the MANIAC, a more advanced version of ENIAC. Her work helped the field of meteorology and weather prediction.\n\nThe NACA, and subsequently NASA, recruited women computers following World War II. By the 1950s, a team was performing mathematical calculations at the Lewis Research Center in Cleveland, Ohio, including Annie Easley, Katherine Johnson and Kathryn Peddrew. At the National Bureau of Standards, Margaret R. Fox was hired to work as part of the technical staff of the Electronic Computer Laboratory in 1951. \n\nAt Convair Aircraft Corporation, Joyce Currie Little was one of the original programmers for analyzing data received from the wind tunnels. She used punch cards on an IBM 650 which was located in a different building from the wind tunnel. To save time in the physical delivery of the punch cards, she and her colleague, Maggie DeCaro, put on roller skates to get to and from the building faster. \n\nIn Israel, Thelma Estrin worked on the design and development of WEIZAC, one of the world's first large-scale programmable electronic computers. In the Soviet Union the IT industry was dominated by women; a team of them designed the first digital computer in 1951. In the UK, Kathleen Booth worked with her husband, Andrew Booth on several computers at Birkbeck College. Kathleen Booth was the programmer and Andrew built the machines. Kathleen developed Assembly Language at this time.\n\nAdele Mildred Koss, who had worked at UNIVAC with Hopper, started work at Control Data Corporation (CDC) in 1965. There she developed algorithms for graphics, including graphic storage and retrieval.\n\nMary K. Hawes of Burroghs Corporation set up a meeting in 1959 to discuss the creation a computer language that would be shared between businesses. Six people, including Hopper, attended to discuss the philosophy of creating a common business language (CBL). Hopper became involved in developing COBOL (Common Business Oriented Language) where she innovated new symbolic ways to write computer code. Hopper developed programming language that was easier to read and \"self-documenting.\" After COBOL was submitted to the CODASYL Executive Committee, Betty Holberton did further editing on the language before it was submitted to the Government Printing Office in 1960. IBM were slow to adopt COBOL, which hindered its progress but it was accepted as a standard in 1962, after Hopper had demonstrated the compiler working both on UNIVAC and RCA computers. The development of COBOL led to the generation of compilers and generators, most of which were created or refined by women such as Koss, Nora Moser, Deborah Davidson, Sue Knapp, Gertrude Tierney and Jean E. Sammet.\n\nSammet, who worked at IBM starting in 1961 was responsible for developing the programming language, FORMAC. She published a book, \"Programming Languages: History and Fundamentals\" (1969), which was considered the \"standard work on programming languages,\" according to Denise Gürer It was \"one of the most used books in the field,\" according to \"The Times\" in 1972. \n\nBetween 1961 and 1963, Margaret Hamilton began to study software reliability while she was working at the US SAGE air defense system. In 1965, she was responsible for programming the software for the onboard flight software on the Apollo mission computers. After Hamilton had completed the program, the code was sent to Raytheon where \"expert seamstresses\" called the \"Little Old Ladies\" actually hardwired the code by threading copper wire through magnetic rings. Each system could store more than 12,000 words that were represented by the copper wires.\nIn 1964, the British Prime Minister Harold Wilson announced a \"White-Hot\" revolution in technology, that would give greater prominence to IT work. As women still held most computing and programming positions at this time, it was hoped that it would give them more positive career prospects. In 1965, Sister Mary Kenneth Keller became the first American woman to earn a doctorate in computer science. Keller helped develop BASIC while working as a graduate student at Dartmouth, where the university \"broke the 'men only' rule\" so she could use its computer science center.\n\nChristine Darden began working for NASA's computing pool in 1967 having graduated from the Hampton Institute. Women were involved in the development of Whirlwind, including Judy Clapp. She created the prototype for an air defense system for Whirlwind which used radar input to track planes in the air and could direct aircraft courses.\n\nIn 1969, Elizabeth \"Jake\" Feinler, who was working for Stanford, made the first Resource Handbook for ARPANET. This led to the creation of the ARPANET directory, which was built by Feinler with a staff of mostly women. Without the directory, \"it was nearly impossible to navigate the ARPANET.\" \n\nBy the end of the decade, the general demographics of programmers had shifted away from being predominantly women, as they had before the 1940s. Though women accounted for around 30 to 50 percent of computer programmers during the 1960s, few were promoted to leadership roles and women were paid significantly less than their male counterparts. \"Cosmopolitan\" ran an article in the April 1967 issue about women in programming called \"The Computer Girls.\" Even while magazines such as \"Cosmopolitan\" saw a bright future for women in computers and computer programming in the 1960s, the reality was that women were still being marginalized.\n\nIn the early 1970s, Pam Hardt-English led a group to create a computer network they named Resource One and which was part of a group called Project One. Her idea to connect Bay Area bookstores, libraries and Project One was an early prototype of the Internet. To work on the project, Hardt-English obtained an expensive SDS-940 computer as a donation from TransAmerica Leasing Corporation in April 1972. They created an electronic library and housed it in a record store called Leopold's in Berkeley. This became the Community Memory database and was maintained by hacker, Jude Milhon. After 1975, the SDS-940 computer was repurposed by Sherry Reson, Mya Shone, Chris Macie and Mary Janowitz to create a social services database and a Social Services Referral Directory. Hard copies of the directory, printed out as a subscription service, were kept at city buildings and libraries. The database was maintained and in use until 2009.\n\nIn the early 1970s, Elizabeth \"Jake\" Feinler, who worked on the Resource Directory for ARPANET, and her team created the first WHOIS directory. Feinler set up a server at the Network Information Center (NIC) at Stanford which would work as a directory that could retrieve relevant information about a person or entity. She and her team worked on the creation of domains, with Feinler suggesting that domains be divided by categories based on where the computers were kept. For example, military computers would have the domain of .mil, computers at educational institutions would have .edu. Feinler worked for NIC until 1989. \n\nJean E. Sammet served as the first woman president of the Association for Computing Machinery (ACM), holding the position between 1974 and 1976.\n\nAdele Goldberg was one of seven programmers that developed Smalltalk in the 1970s, and wrote the majority of the language's documentation. It was one of the first object-oriented programming languages the base of the current graphic user interface, that has its roots in the 1968 The Mother of All Demos by Douglas Engelbart. Smalltalk was used by Apple to launch Apple Lisa in 1983, the first personal computer with a GUI, and a year later its Macintosh. Windows 1.0, based on the same principles, was launched a few months later in 1985.\n\nIn the late 1970s, women such as Paulson and Sue Finley wrote programs for the Voyager mission. Voyager continues to carry their codes inside its own memory banks as it leaves the solar system. In 1979, Ruzena Bajcsy founded the General Robotics, Automation, Sensing and Perception (GRASP) Lab at the University of Pennsylvania.\n\nIn the mid-70s, Joan Margaret Winters began working at IBM as part of a \"human factors project,\" called SHARE. In 1978, Winters was the deputy manager of the project and went on to lead the project between 1983 and 1987. The SHARE group worked on researching how software should be designed to consider human factors.\n\nErna Schneider Hoover developed a computerized switching system for telephone calls that would replace switchboards. Her software patent for the system, issued in 1971, was one of the first software patents ever issued.\n\nGwen Bell developed the Computer Museum in 1980. The museum, which collected computer artifacts became a non-profit organization in 1982 and in 1984, Bell moved it to downtown Boston. Adele Goldberg served as president of ACM between 1984 and 1986. In 1986, Lixia Zhang was the only woman and graduate student to participate in the early Internet Engineering Task Force (IETF) meetings. Zhang was involved in early Internet development. \n\nSometimes known as the \"Betsy Ross of the personal computer,\" according to the \"New York Times\", Susan Kare worked with Steve Jobs to design the original icons for the Macintosh. Kare designed the moving watch, paintbrush and trash can elements that made using a Mac user-friendly. Kare worked for Apple until the mid 1980s, going on to work on icons for Windows 3.0. Other types of computer graphics were being developed by Nadia Magnenat Thalmann in Canada. Thalmann started working on computer animation to develop \"realistic virtual actors\" first at the University of Montréal in 1980 and later in 1988 at the École Polytechnique Fédérale de Lausanne.\n\nIn the field of human computer interaction (HCI), French computer scientist, Joëlle Coutaz developed the presentation-abstraction-control (PAC) model in 1987. She founded the User Interface group at the Laboratorire de Génie Informatique of IMAG where they worked on different problems relating to user interface and other software tools. \n\nAs Ethernet became the standard for networking computers locally, Radia Perlman, who worked at Digital Equipment Corporation (DEC), was asked to \"fix\" limitations that Ethernet imposed on large network traffic. In 1985, Perlman came up with a way to route information packets from one computer to another in an \"infinitely scalable\" way that allowed large networks like the Internet to function. Her solution took less than a few days to design and write up. The name of the algorithm she created is the Spanning Tree Protocol. In 1988, Stacy Horn, who had been introduced to bulletin board systems (BBS) through The WELL, decided to create her own online community in New York, which she called the East Coast Hang Out (ECHO). Horn invested her own money and pitched the idea for ECHO to others after bankers refused to hear her business plan. Horn built her BBS using UNIX, which she and her friends taught to one another. Eventually ECHO moved an office in Tribeca in the early 1990s and started getting press attention. ECHO's users could post about topics that interested them, chat with on another and were provided email accounts. Around half of ECHO's users were women. ECHO is still online as of 2018.\n\nEurope was somewhat behind other countries in developing an Internet infrastructure. A project was developed in the mid-1980s to create an academic network in Europe using the Open System Interconnection (OSI) standards. Borka Jerman Blažič, a Yugoslavian computer scientist was invited to work on the project. She was involved in establishing a Yugoslav Research and Academic Network (YUNAC) in 1989 and registered the domain of .yu for the country.\n\nComputer and video games became popular in the 1980s, but many were primarily action-oriented and not designed from a woman's point of view. Stereotypical characters such as the damsel in distress featured prominently and consequently were not inviting towards women. Dona Bailey designed \"Centipede\", where the player shoots insects, as a reaction to such games, later saying \"It didn't seem bad to shoot a bug\". Carol Shaw, considered to be the first modern female games designer, released a 3D version of Tic-tac-toe for the Atari 2600 in 1980. Roberta Williams and her husband Ken, founded Sierra Online and pioneered the graphic adventure game format in \"Mystery House\" and the \"King's Quest\" series. The games had a friendly graphical user interface and introduced humor and puzzles. Cited as an important game designer, her influenced spread from Sierra to other companies such as LucasArts and beyond. Brenda Laurel worked on porting games from arcade versions to the Atari 400 and Atari 800 computers in the late 1970s and early 1980s. She then went to work for Activision, writing the manual for \"Maniac Mansion\". \n\n1984 was the year of Women Into Science and Engineering (WISE). A 1984 report by Ebury Publishing reported that in a typical family, only 5% of mothers and 19% of daughters were using a computer at home, compared to 25% of fathers and 51% of sons. To counteract this, the company launched a series of software titles designed towards women and publicised in \"Good Housekeeping\". Anita Borg, who had been noticing that women were under-represented in computer science, founded an email support group, Systers, in 1987.\n\nBy the 1990s, computing was dominated by men. The proportion of female computer science graduates peaked in 1984 around 37 per cent, and then steadily declined. Although the end of the 20th century saw an increase in women scientists and engineers, this did not hold true for computing, which stagnated. Despite this, they were very involved in working on hypertext and hypermedia projects in the late 1980s and early 1990s. A team of women at Brown University, including Nicole Yankelovich and Karen Catlin, developed Intermedia and invented the anchor link. Apple partially funded their project and incorporated their concepts into Apple operating systems. Sun Microsystems Sun Link Service was developed by Amy Pearl. Janet Walker developed the first system to use bookmarks when she created the Symbolics Document Examiner. In 1989, Wendy Hall created a hypertext project called Microcosm, which was based on digitized multimedia material found in the Mountbatten archive. Cathy Marshall worked on the NoteCards system at Xerox PARC. NoteCards went on to influence Apple's HyperCard. As the Internet became the World Wide Web, developers like Hall adapted their programs to include Web viewers. Her Microcosm was especially adaptable to new technologies, including animation and 3-D models. In 1994, Hall helped organize the first conference for the Web. \n\nSarah Allen, the co-founder of After Effects, co-founded a commercial software company called CoSA in 1990. In 1995, she started working on the Shockwave team for Macromedia where she was the lead developer of the Shockwave Mulituser Server, the Flash Media Server and Flash video.\n\nFollowing the increased popularity of the Internet in the 1990s, online spaces were set up to cater for women, including the online community Women's WIRE and the technical and support forum LinuxChix. Women's WIRE, launched by Nancy Rhine and Ellen Pack in October 1993, was the first Internet company to specifically target this demographic. A conference for women in computer-related jobs, the Grace Hopper Celebration of Women in Computing, was first launched in 1994 by Anita Borg.\n\nGame designer Brenda Laurel started working at Interval Research in 1992, and began to think about the differences in the way girls and boys experienced playing video games. After interviewing around 1,000 children and 500 adults, she determined that games weren't designed with girls' interests in mind. The girls she spoke with wanted more games with open worlds and characters they could interact with. Her research led to Interval Research giving Laurel's research team their own company in 1996, Purple Moon. Also in 1996, Mattel's game, \"Barbie Fashion Designer\", became the first best-selling game for girls. Purple Moon's first two games based on a character called Rockett, made it to the 100 best-selling games in the years they were released. In 1999, Mattel bought out Purple Moon.\n\nJaime Levy created the one of the first e-Zines in the early 1990s, starting with \"CyberRag\", which included articles, games and animations loaded onto diskettes that anyone with a Mac could access. Later, she renamed the zine to \"Electronic Hollywood.\" Billy Idol commissioned Levy to create a disk for his album, \"Cyberpunk\". She was hired to be the creative director of the online magazine, \"Word\", in 1995.\n\nCyberfeminists, VNS Matrix, made up of Josephine Starrs, Juliane Pierce, Francesca da Rimini and Virginia Barratt, created art in the early 1990s linking computer technology and women's bodies. In 1997, there was a gathering of cyberfeminists in Kassel, called the First Cyberfeminist International.\n\nIn China, Hu Qiheng, was the leader of the team who installed the first TCP/IP connection for China, connecting to the Internet on April 20, 1994. In 1995, Rosemary Candlin went to write software for CERN in Geneva. In the early 1990s, Nancy Hafkin was an important figure in working with the Association for Progressive Communications (APC) in enabling email connections in 10 African countries. Starting in 1999, Anne-Marie Eklund Löwinder began to work with Domain Name System Security Extensions (DNSSEC) in Sweden. She later made sure that the domain, .se, was the world's first top level domain name to be signed with DNSSEC.\n\nIn the 21st century, several attempts have been made to reduce the gender disparity in IT and get more women involved in computing again. A 2001 survey found that while both sexes use computers and the internet in equal measure, women were still five times less likely to choose it as a career or study the subject beyond standard secondary education. Journalist Emily Chang said a key problem has been personality tests in job interviews and the belief that good programmers are introverts, which tends to self-select the stereotype of an antisocial white male nerd.\n\nIn 2004, the National Center for Women & Information Technology was established by Lucy Sanders to address the gender gap. Carnegie Mellon University has made a concerted attempt to increase gender diversity in the computer science field, by selecting students based on a wide criteria including leadership ability, a sense of \"giving back to the community\" and high attainment in maths and science, instead of traditional computer programming expertise. As well as increase the intake of women into CMU, the programme resulted in better quality students overall, as they found the increased diversity made for a stronger team.\n\nDespite the pioneering work of some designers, video games are still considered biased towards men. A 2013 survey by the International Game Developers Association revealed only 22% of game designers are women, although this is substantially higher than figures in previous decades. Working to bring inclusion to the world of open source project development, Coraline Ada Ehmke drafts the Contributor Covenant in 2014. By 2018, over 40,000 software projects have started using the Contributor Covenant, including TensorFlow, Vue and Linux.\n\nIn 2017, Michelle Simmons founded the first quantum computing company in Australia. The team, which has made \"great strides\" in 2018, plans to develop a 10-qubit prototype silicon quantum integrated circuit by 2022. Also in 2017, Doina Precup became the head of DeepMind Montreal, working on artificial intelligence.\n\nOne of the biggest problems facing women in computing in the modern era is that they often find themselves working in an environment that is largely unpleasant, so they don't stay on in the careers in programming and technology. In 2013, a National Public Radio report said 20% of computer programmers in the US are female. There is no general consensus for any key reason there are less women in computing. In 2017, James Damore was fired from Google after claiming there was a biological reason for a lack of female computer scientists. The following year, Wikipedia was criticised for not having an article about scientist Donna Strickland until shortly after she won the Nobel Prize for Physics, which was attributed to a severe gender disparity of the site's editors.\n\nIn 1991, Massachusetts Institute of Technology undergraduate Ellen Spertus wrote an essay \"Why Are There So Few Women in Computer Science?\", which complained about inherent sexism in IT, which was responsible for a lack of women in computing. She subsequently taught computer science at Mills College, Oakland in order to increase interest in IT for women. A key problem is a lack of female role models in the IT industry, alongside computer programmers in fiction and the media generally being male. The University of Southampton's Wendy Hall has said the attractiveness of computers to women decreased significantly in the 1980s when they \"were sold as toys for boys\", and believes the cultural stigma has remained ever since, and may even be getting worse. Kathleen Lehman, project manager of the BRAID Initiative at UCLA has said a problem is that typically women aim for perfection and feel disillusioned when code does not compile, whereas men may simply treat it as a learning experience. A report in the \"Daily Telegraph\" suggested that women generally prefer people-facing jobs, which many computing and IT positions do not have, while men prefer jobs geared towards objects and tasks.\n\nThe gender disparity in IT is not global. The ratio of female to male computer scientists is significantly higher in India compared to the West. In Europe, Bulgaria and Romania have the highest rates of women going into computer programming. In government universities in Saudi Arabia in 2014, Arab women made up 59% of students enrolled in computer science. However, the ratio of African American female computer scientists in the US is significantly lower than the national average. It has been suggested there is a greater gap in countries where people of both sexes are treated more equally, contradicting any theories that society in general is to blame for any disparity.\n\nThe Association for Computing Machinery Turing Award, sometimes referred to as the \"Nobel Prize\" of computing, was named in honor of Alan Turing. It award has been won by three women between 1966 and 2015.\n\nThe British Computer Society Information Retrieval Specialist Group (BCS IRSG) in conjunction with the British Computer Society created an award in 2008 to commemorate the achievements of Karen Spärck Jones, a Professor Emerita of Computers and Information at the University of Cambridge and one of the most remarkable women in computer science. The KSJ award has been won by four women between 2009 and 2017:\n\n\nSeveral important groups have been established to encourage women in the IT industry. The Association for Women in Computing was one of the first and is dedicated to promoting the advancement of women in computing professions. The established in 1991 focused on increasing the number of women in Computer Science and Engineering (CSE) research and education at all levels. AnitaB.org runs the Grace Hopper Celebration of Women in Computing yearly conference. The National Center for Women & Information Technology is a nonprofit that aims to increase the number of women in technology and computing. The Women in Technology International (WITI) is a global organization dedicated to the advancement of women in business and technology.\n\nSome major societies and groups have offshoots dedicated to women. The Association for Computing Machinery's Council on Women in Computing (ACM-W) has over 36,000 members. BCSWomen is a women-only specialist group of the British Computer Society, founded in 2001. In Ireland, the charity Teen Turn run after school training and work placements for girls, and Women in Technology and Science (WITS) advocate for the inclusion and promotion of women within STEM industries.\n\nThe Women's Technology Empowerment Centre (W.TEC) is a non-profit organisation focused on providing technology education and mentoring to Nigerian women and girls. Black Girls Code is a non-profit focused on providing technology education to young African-American women.\n\nOther organisations dedicated to women in IT include Girl Develop It, a nonprofit organization that provides affordable programs for adult women interested in learning web and software development in a judgment-free environment, Girl Geek Dinners, an International group for women of all ages, Girls Who Code: a national non-profit organization dedicated to closing the gender gap in technology, LinuxChix, a women-oriented community in the open source movement and Systers, a moderated listserv dedicated to mentoring women in the IT industry.\n\n\nCitations\nSources\n\n\n"}
{"id": "38542576", "url": "https://en.wikipedia.org/wiki?curid=38542576", "title": "Young's convolution inequality", "text": "Young's convolution inequality\n\nIn mathematics, Young's convolution inequality is a mathematical inequality about the convolution of two functions, named after William Henry Young.\n\nIn real analysis, the following result is called Young's convolution inequality:\n\nSuppose \"f\" is in \"L\"(R) and \"g\" is in \"L\"(R) and\n\nwith 1 ≤ \"p\", \"q\", \"r\" ≤ ∞. Then\n\nHere the star denotes convolution, \"L\" is Lebesgue space, and\n\ndenotes the usual \"L\" norm.\n\nEquivalently, if formula_4 and formula_5 then \n\nYoung's convolution inequality has a natural generalization in which we replace formula_7 by a unimodular group formula_8. If we let formula_9 be a bi-invariant Haar measure on formula_8 and we let formula_11 or formula_12 be integrable functions, then we define formula_13 by\nThen in this case, Young's inequality states that for formula_15 and formula_16 and formula_17 such that \nwe have a bound \nEquivalently, if formula_4 and formula_5 then \nSince formula_7 is in fact a locally compact abelian group (and therefore unimodular) with the Lebesgue measure the desired Haar measure, this is in fact a generalization.\n\nAn example application is that Young's inequality can be used to show that the heat semigroup is a contracting semigroup using the \"L\" norm (i.e. the Weierstrass transform does not enlarge the \"L\" norm).\n\nYoung's inequality has an elementary proof with the non-optimal constant 1.\n\nWe assume that the functions formula_24 are nonnegative and integrable, where formula_8 is a unimodular group endowed with a bi-invariant Haar measure formula_9. We use the fact that formula_27 for any measurable formula_28.\nSince formula_29\nBy the Hölder inequality for three functions we deduce that \nThe conclusion follows then by left-invariance of the Haar measure, the fact that integrals are preserved by inversion of the domain, and by Fubini's theorem.\n\nIn case \"p\", \"q\" > 1 Young's inequality can be strengthened to a sharp form, via\n\nwhere the constant \"c\" < 1. When this optimal constant is achieved, the function formula_33 and formula_34 are multidimensional Gaussian functions.\n\n"}
{"id": "4119746", "url": "https://en.wikipedia.org/wiki?curid=4119746", "title": "Zero-crossing rate", "text": "Zero-crossing rate\n\nThe zero-crossing rate is the rate of sign-changes along a signal, i.e., the rate at which the signal changes from positive to negative or back. This feature has been used heavily in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.\n\nZCR is defined formally as\n\nwhere formula_2 is a signal of length formula_3 and formula_4 is an indicator function.\n\nIn some cases only the \"positive-going\" or \"negative-going\" crossings are counted, rather than all the crossings - since, logically, between a pair of adjacent positive zero-crossings there must be one and only one negative zero-crossing.\n\nFor monophonic tonal signals, the zero-crossing rate can be used as a primitive pitch detection algorithm.\n\nZero crossing rates are used for Voice activity detection (VAD), i.e., finding whether human speech is present in an audio segment or not.\n\n"}
