{"id": "19321486", "url": "https://en.wikipedia.org/wiki?curid=19321486", "title": "Abstract and Applied Analysis", "text": "Abstract and Applied Analysis\n\nAbstract and Applied Analysis is a peer-reviewed mathematics journal covering the fields of abstract and applied analysis and traditional forms of analysis such as linear and nonlinear ordinary and partial differential equations, optimization theory, and control theory. It is published by Hindawi Publishing Corporation. It was established by Athanassios G. Kartsatos (University of South Florida) in 1996, who was editor-in-chief until 2005. Martin Bohner (Missouri S&T) was editor-in-chief from 2006 until 2011 when the journal converted to a model shared by all Hindawi journals of not having an editor-in-chief, with editorial decisions made by editorial board members. The journal has faced delisting from the \"Journal Citation Reports\" (thus not receive an impact factor), for anomalous citation patterns.\n"}
{"id": "1043036", "url": "https://en.wikipedia.org/wiki?curid=1043036", "title": "Bertrand's ballot theorem", "text": "Bertrand's ballot theorem\n\nIn combinatorics, Bertrand's ballot problem is the question: \"In an election where candidate A receives \"p\" votes and candidate B receives \"q\" votes with \"p\" > \"q\", what is the probability that A will be strictly ahead of B throughout the count?\" The answer is\n\nThe result was first published by W. A. Whitworth in 1878, but is named after Joseph Louis François Bertrand who rediscovered it in 1887.\n\nIn Bertrand's original paper, he sketches a proof based on a general formula for the number of favourable sequences using a recursion relation. He remarks that it seems probable that such a simple result could be proved by a more direct method. Such a proof was given by Désiré André, based on the observation that the unfavourable sequences can be divided into two equally probable cases, one of which (the case where B receives the first vote) is easily computed; he proves the equality by an explicit bijection. A variation of his method is popularly known as André's reflection method, although André did not use any reflections.\n\nSuppose there are 5 voters, of whom 3 vote for candidate \"A\" and 2 vote for candidate \"B\" (so \"p\" = 3 and \"q\" = 2). There are ten possibilities for the order of the votes cast:\nFor the order \"AABAB\", the tally of the votes as the election progresses is:\n\nFor each column the tally for \"A\" is always larger than the tally for \"B\" so the \"A\" is always strictly ahead of \"B\". For the order \"AABBA\" the tally of the votes as the election progresses is:\n\nFor this order, \"B\" is tied with \"A\" after the fourth vote, so \"A\" is not always strictly ahead of \"B\".\nOf the 10 possible orders, \"A\" is always ahead of \"B\" only for \"AAABB\" and \"AABAB\". So the probability that \"A\" will always be strictly ahead is\nand this is indeed equal to formula_3 as the theorem predicts.\n\nRather than computing the probability that a random vote counting order has the desired property, one can instead compute the number of favourable counting orders, then divide by the total number of ways in which the votes could have been counted. (This is the method used by Bertrand.) The total number of ways is the binomial coefficient formula_4; Bertrand's proof shows that the number of favourable orders in which to count the votes is formula_5 (though he does not give this number explicitly). And indeed after division this gives formula_6.\n\nAnother equivalent problem is to calculate the number of random walks on the integers that consist of \"n\" steps of unit length, beginning at the origin and ending at the point \"m\", that never become negative. Assuming \"n\" and \"m\" have the same parity and \"n\" ≥ \"m\" ≥ 0, this number is\nWhen \"m\" = 0 and \"n\" is even, this gives the Catalan number formula_8.\n\nFor A to be strictly ahead of B throughout the counting of the votes, there can be no ties. Separate the counting sequences according to the first vote. Any sequence that begins with a vote for B must reach a tie at some point, because A eventually wins. For any sequence that begins with A and reaches a tie, reflect the votes up to the point of the first tie (so any A becomes a B, and vice versa) to obtain a sequence that begins with B. Hence every sequence that begins with A and reaches a tie is in one-to-one correspondence with a sequence that begins with B, and the probability that a sequence begins with B is formula_9, so the probability that A always leads the vote is\n\nAnother method of proof is by mathematical induction:\n\nA simple proof is based on the beautiful Cycle Lemma of Dvoretzky and Motzkin.\nCall a ballot sequence \"dominating\" if A is strictly ahead of B throughout the counting of the votes. The Cycle Lemma asserts that any sequence of formula_18 A's and formula_19 B's, where formula_20, has precisely formula_21 dominating cyclic permutations. To see this, just arrange the given sequence of formula_22 A's and B's in a circle and repeatedly remove adjacent pairs AB until only formula_21 A's remain. Each of these A's was the start of a dominating cyclic permutation before anything was removed. So formula_21 out of the formula_22 cyclic permutations of any arrangement of formula_18 A votes and formula_19 B votes are dominating.\n\nBertrand expressed the solution as\nwhere formula_29 is the total number of voters and formula_30 is the number of voters for the first candidate. He states that the result follows from the formula\nwhere formula_32 is the number of favourable sequences, but \"it seems probable that such a simple result could be shown in a more direct way\". Indeed, a more direct proof was soon produced by Désiré André. His approach is often mistakenly labelled \"the reflection principle\" by modern authors but in fact uses a permutation. He shows that the \"unfavourable\" sequences (those that reach an intermediate tie) consist of an equal number of sequences that begin with A as those that begin with B. Every sequence that begins with B is unfavourable, and there are formula_33 such sequences with a B followed by an arbitrary sequence of (\"q\"-1) B's and \"p\" A's. Each unfavourable sequence that begins with A can be transformed to an arbitrary sequence of (\"q\"-1) B's and \"p\" A's by finding the first B that violates the rule (by causing the vote counts to tie) and deleting it, and interchanging the order of the remaining parts. To reverse the process, take any sequence of (\"q\"-1) B's and \"p\" A's and search from the end to find where the number of A's first exceeds the number of B's, and then interchange the order of the parts and place a B in between. For example, the unfavourable sequence AABBABAA corresponds uniquely to the arbitrary sequence ABAAAAB. From this, it follows that the number of favourable sequences of \"p\" A's and \"q\" B's is\nand thus the required probability is\nas expected.\n\nThe original problem is to find the probability that the first candidate is always strictly ahead in the vote count. One may instead consider the problem of finding the probability that the second candidate is never ahead (that is, with ties are allowed). In this case, the answer is\nThe variant problem can be solved by the reflection method in a similar way to the original problem. The number of possible vote sequences is formula_37. Call a sequence \"bad\" if the second candidate is ever ahead, and if the number of bad sequences can be enumerated then the number of \"good\" sequences can be found by subtraction and the probability can be computed.\n\nRepresent a voting sequence as a lattice path on the Cartesian plane as follows:\nEach such path corresponds to a unique sequence of votes and will end at (\"p\", \"q\"). A sequence is 'good' exactly when the corresponding path never goes above the diagonal line \"y\" = \"x\"; equivalently, a sequence is 'bad' exactly when the corresponding path touches the line \"y\" = \"x\" + 1.\n\nFor each 'bad' path \"P\", define a new path \"P\"′ by reflecting the part of \"P\" up to the first point it touches the line across it. \"P\"′ is a path from (−1, 1) to (\"p\", \"q\"). The same operation applied again restores the original \"P\". This produces a one-to-one correspondence between the 'bad' paths and the paths from (−1, 1) to (\"p\", \"q\"). The number of these paths is formula_38 and so that is the number of 'bad' sequences. This leaves the number of 'good' sequences as\nSince there are formula_37 altogether, the probability of a sequence being good is formula_41.\n\nIn fact, the solutions to the original problem and the variant problem are easily related. For candidate A to be strictly ahead throughout the vote count, they must receive the first vote and for the remaining votes (ignoring the first) they must be either strictly ahead or tied throughout the count. Hence the solution to the original problem is\nas required.\n\nConversely, the tie case can be derived from the non-tie case. Note that the \"number\" of non-tie sequences with p+1 votes for A is equal to the number of tie sequences with p votes for A. The number of non-tie votes with p + 1 votes for A votes is formula_43, which by algebraic manipulation is formula_44, so the \"fraction\" of sequences with p votes for A votes is formula_45.\n\n\n"}
{"id": "22649391", "url": "https://en.wikipedia.org/wiki?curid=22649391", "title": "Bose–Mesner algebra", "text": "Bose–Mesner algebra\n\nIn mathematics, a Bose–Mesner algebra is a special set of matrices which arise from a combinatorial structure known as an association scheme, together with the usual set of rules for combining (forming the products of) those matrices, such that they form an associative algebra, or, more precisely, a unitary commutative algebra. Among these rules are:\n\nBose–Mesner algebras have applications in physics to spin models, and in statistics to the design of experiments. They are named for R. C. Bose and Dale Marsh Mesner.\n\nLet \"X\" be a set of \"v\" elements. Consider a partition of the 2-element subsets of \"X\" into \"n\" non-empty subsets, \"R\", ..., \"R\" such that:\nThis structure is enhanced by adding all pairs of repeated elements of \"X\" and collecting them in a subset \"R\". This enhancement permits the parameters \"i\", \"j\", and \"k\" to take on the value of zero, and lets some of \"x\",\"y\" or \"z\" be equal.\n\nA set with such an enhanced partition is called an Association scheme. One may view an association scheme as a partition of the edges of a complete graph (with vertex set \"X\") into n classes, often thought of as color classes. In this representation, there is a loop at each vertex and all the loops receive the same 0th color.\n\nThe association scheme can also be represented algebraically. Consider the matrices \"D\" defined by:\n\nLet formula_11 be the vector space consisting of all matrices formula_12, with formula_13 complex.\n\nThe definition of an association scheme is equivalent to saying that the formula_14 are \"v\" × \"v\" (0,1)-matrices which satisfy\n\n\nThe (\"x\",\"y\")-th entry of the left side of 4. is the number of two colored paths of length two joining \"x\" and \"y\" (using \"colors\" \"i\" and \"j\") in the graph. Note that the rows and columns of formula_15 contain formula_20 1s:\n\nFrom 1., these matrices are symmetric. From 2., formula_22 are linearly independent, and the dimension of formula_11 is formula_24. From 4., formula_11 is closed under multiplication, and multiplication is always associative. This associative commutative algebra formula_11 is called the Bose–Mesner algebra of the association scheme. Since the matrices in formula_11 are symmetric and commute with each other, they can be simultaneously diagonalized. This means that there is a matrix formula_28 such that to each formula_29 there is a diagonal matrix formula_30 with formula_31. This means that formula_11 is semi-simple and has a unique basis of primitive idempotents formula_33. These are complex n × n matrices satisfying\n\nThe Bose–Mesner algebra has two distinguished bases: the basis consisting of the adjacency matrices formula_15, and the basis consisting of the irreducible idempotent matrices formula_38. By definition, there exist well-defined complex numbers such that\n\nand\n\nThe p-numbers formula_41, and the q-numbers formula_42, play a prominent role in the theory. They satisfy well-defined orthogonality relations. The p-numbers are the eigenvalues of the adjacency matrix formula_15.\n\nThe eigenvalues of formula_44 and formula_42, satisfy the orthogonality conditions:\n\nAlso\n\nIn matrix notation, these are\n\nwhere formula_51\n\nThe eigenvalues of formula_52 are formula_53 with multiplicities formula_54. This implies that\n\nwhich proves Equation formula_56 and Equation formula_57,\n\nwhich gives Equations formula_59, formula_60 and formula_61.formula_62\n\nThere is an analogy between extensions of association schemes and extensions of finite fields. The cases we are most interested in are those where the extended schemes are defined on the formula_63-th Cartesian power formula_64 of a set formula_65 on which a basic association scheme formula_66 is defined. A first association scheme defined on formula_64 is called the formula_63-th Kronecker power formula_69 of formula_66. Next the extension is defined on the same set formula_64 by gathering classes of formula_69. The Kronecker power corresponds to the polynomial ring formula_73 first defined on a field formula_74, while the extension scheme corresponds to the extension field obtained as a quotient. An example of such an extended scheme is the Hamming scheme.\n\nAssociation schemes may be merged, but merging them leads to non-symmetric association schemes, whereas all usual codes are subgroups in symmetric Abelian schemes.\n\n\n"}
{"id": "21850245", "url": "https://en.wikipedia.org/wiki?curid=21850245", "title": "Circular ensemble", "text": "Circular ensemble\n\nIn the theory of random matrices, the circular ensembles are measures on spaces of unitary matrices introduced by Freeman Dyson as modifications of the Gaussian matrix ensembles. The three main examples are the circular orthogonal ensemble (COE) on symmetric unitary matrices, the circular unitary ensemble (CUE) on unitary matrices, and the circular symplectic ensemble (CSE) on self dual unitary quaternionic matrices.\n\nThe distribution of the unitary circular ensemble CUE(\"n\") is the Haar measure on the unitary group \"U(n)\". If \"U\" is a random element of CUE(\"n\"), then \"UU\" is a random element of COE(\"n\"); if \"U\" is a random element of CUE(\"2n\"), then \"UU\" is a random element of CSE(\"n\"), where \n\nEach element of a circular ensemble is a unitary matrix, so it has eigenvalues on the unit circle: formula_2 with formula_3 for \"k=1,2... n\", where the formula_4 are also known as eigenangles or eigenphases. In the CSE each of these \"n\" eigenvalues appears twice. The distributions have densities with respect to the eigenangles, given by\non formula_6 (symmetrized version), where β=1 for COE, β=2 for CUE, and β=4 for CSE. The normalisation constant \"Z\" is given by\nas can be verified via Selberg's integral formula, or Weyl's integral formula for compact Lie groups.\n\nGeneralizations of the circular ensemble restrict the matrix elements of \"U\" to real numbers [so that \"U\" is in the orthogonal group \"O(n)\"] or to real quaternion numbers [so that \"U\" is in the symplectic group \"Sp(2n)\". The Haar measure on the orthogonal group produces the circular real ensemble (CRE) and the Haar measure on the symplectic group produces the circular quaternion ensemble (CQE).\n\nThe eigenvalues of orthogonal matrices come in complex conjugate pairs formula_8 and formula_9, possibly complemented by eigenvalues fixed at \"+1\" or \"-1\". For \"n=2m\" even and \"det U=1\", there are no fixed eigenvalues and the phases \"θ\" have probability distribution \nwith \"C\" an unspecified normalization constant. For \"n=2m+1\" odd there is one fixed eigenvalue \"σ=det U\" equal to ±1. The phases have distribution\nFor \"n=2m+2\" even and \"det U=-1\" there is a pair of eigenvalues fixed at \"+1\" and \"-1\", while the phases have distribution\nThis is also the distribution of the eigenvalues of a matrix in \"Sp(2m)\".\nThese probability density functions are referred to as Jacobi distributions in the theory of random matrices, because correlation functions can be expressed in terms of Jacobi polynomials.\n\nAverages of products of matrix elements in the circular ensembles can be calculated using Weingarten functions. For large dimension of the matrix these calculations become impractical, and a numerical method is advantageous. There exist efficient algorithms to generate random matrices in the circular ensembles, for example by performing a QR decomposition on a Ginibre matrix. \n\n\n"}
{"id": "18748850", "url": "https://en.wikipedia.org/wiki?curid=18748850", "title": "Complex analytic space", "text": "Complex analytic space\n\nIn mathematics, a complex analytic space is a generalization of a complex manifold which allows the presence of singularities. Complex analytic spaces are locally ringed spaces which are locally isomorphic to local model spaces, where a local model space is an open subset of the vanishing locus of a finite set of holomorphic functions.\n\nDenote the constant sheaf on a topological space with value formula_1 by formula_2. A formula_1-space is a locally ringed space formula_4 whose structure sheaf is an algebra over formula_2.\n\nChoose an open subset formula_6 of some complex affine space formula_7, and fix finitely many holomorphic functions formula_8 in formula_6. Let formula_10 be the common vanishing locus of these holomorphic functions, that is, formula_11. Define a sheaf of rings on formula_12 by letting formula_13 be the restriction to formula_12 of formula_15, where formula_16 is the sheaf of holomorphic functions on formula_6. Then the locally ringed formula_1-space formula_4 is a local model space.\n\nA complex analytic space is a locally ringed formula_1-space formula_4 which is locally isomorphic to a local model space.\n\nMorphisms of complex analytic spaces are defined to be morphisms of the underlying locally ringed spaces, they are also called holomorphic maps.\n\n\n"}
{"id": "305924", "url": "https://en.wikipedia.org/wiki?curid=305924", "title": "Computational fluid dynamics", "text": "Computational fluid dynamics\n\nComputational fluid dynamics (CFD) is a branch of fluid mechanics that uses numerical analysis and data structures to analyze and solve problems that involve fluid flows. Computers are used to perform the calculations required to simulate the free-stream flow of the fluid, and the interaction of the fluid (liquids and gases) with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved, and are often required to solve the largest and most complex problems. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as transonic or turbulent flows. Initial validation of such software is typically performed using experimental apparatus such as wind tunnels. In addition, previously performed analytical or empirical analysis of a particular problem can be used for comparison. A final validation is often performed using full-scale testing, such as flight tests.\n\nCFD is applied to a wide range of research and engineering problems in many fields of study and industries, including aerodynamics and aerospace analysis, weather simulation, natural science and environmental engineering, industrial system design and analysis, biological engineering and fluid flows, and engine and combustion analysis.\n\n The fundamental basis of almost all CFD problems is the Navier–Stokes equations, which define many single-phase (gas or liquid, but not both) fluid flows. These equations can be simplified by removing terms describing viscous actions to yield the Euler equations. Further simplification, by removing terms describing vorticity yields the full potential equations. Finally, for small perturbations in subsonic and supersonic flows (not transonic or hypersonic) these equations can be linearized to yield the linearized potential equations.\n\nHistorically, methods were first developed to solve the linearized potential equations. Two-dimensional (2D) methods, using conformal transformations of the flow about a cylinder to the flow about an airfoil were developed in the 1930s.\n\nOne of the earliest type of calculations resembling modern CFD are those by Lewis Fry Richardson, in the sense that these calculations used finite differences and divided the physical space in cells. Although they failed dramatically, these calculations, together with Richardson's book \"Weather prediction by numerical process\", set the basis for modern CFD and numerical meteorology. In fact, early CFD calculations during the 1940s using ENIAC used methods close to those in Richardson's 1922 book.\n\nThe computer power available paced development of three-dimensional methods. Probably the first work using computers to model fluid flow, as governed by the Navier-Stokes equations, was performed at Los Alamos National Lab, in the T3 group. This group was led by Francis H. Harlow, who is widely considered as one of the pioneers of CFD. From 1957 to late 1960s, this group developed a variety of numerical methods to simulate transient two-dimensional fluid flows, such as \nParticle-in-cell method (Harlow, 1957), \nFluid-in-cell method (Gentry, Martin and Daly, 1966),\nVorticity stream function method (Jake Fromm, 1963), and\nMarker-and-cell method (Harlow and Welch, 1965). Fromm's vorticity-stream-function method for 2D, transient, incompressible flow was the first treatment of strongly contorting incompressible flows in the world.\n\nThe first paper with three-dimensional model was published by John Hess and A.M.O. Smith of Douglas Aircraft in 1967. This method discretized the surface of the geometry with panels, giving rise to this class of programs being called Panel Methods. Their method itself was simplified, in that it did not include lifting flows and hence was mainly applied to ship hulls and aircraft fuselages. The first lifting Panel Code (A230) was described in a paper written by Paul Rubbert and Gary Saaris of Boeing Aircraft in 1968. In time, more advanced three-dimensional Panel Codes were developed at Boeing (PANAIR, A502), Lockheed (Quadpan), Douglas (HESS), McDonnell Aircraft (MACAERO), NASA (PMARC) and Analytical Methods (WBAERO, USAERO and VSAERO). Some (PANAIR, HESS and MACAERO) were higher order codes, using higher order distributions of surface singularities, while others (Quadpan, PMARC, USAERO and VSAERO) used single singularities on each surface panel. The advantage of the lower order codes was that they ran much faster on the computers of the time. Today, VSAERO has grown to be a multi-order code and is the most widely used program of this class. It has been used in the development of many submarines, surface ships, automobiles, helicopters, aircraft, and more recently wind turbines. Its sister code, USAERO is an unsteady panel method that has also been used for modeling such things as high speed trains and racing yachts. The NASA PMARC code from an early version of VSAERO and a derivative of PMARC, named CMARC, is also commercially available.\n\nIn the two-dimensional realm, a number of Panel Codes have been developed for airfoil analysis and design. The codes typically have a boundary layer analysis included, so that viscous effects can be modeled. Professor Richard Eppler of the University of Stuttgart developed the PROFILE code, partly with NASA funding, which became available in the early 1980s. This was soon followed by MIT Professor Mark Drela's XFOIL code. Both PROFILE and XFOIL incorporate two-dimensional panel codes, with coupled boundary layer codes for airfoil analysis work. PROFILE uses a conformal transformation method for inverse airfoil design, while XFOIL has both a conformal transformation and an inverse panel method for airfoil design.\n\nAn intermediate step between Panel Codes and Full Potential codes were codes that used the Transonic Small Disturbance equations. In particular, the three-dimensional WIBCO code, developed by Charlie Boppe of Grumman Aircraft in the early 1980s has seen heavy use.\n\nDevelopers turned to Full Potential codes, as panel methods could not calculate the non-linear flow present at transonic speeds. The first description of a means of using the Full Potential equations was published by Earll Murman and Julian Cole of Boeing in 1970. Frances Bauer, Paul Garabedian and David Korn of the Courant Institute at New York University (NYU) wrote a series of two-dimensional Full Potential airfoil codes that were widely used, the most important being named Program H. A further growth of Program H was developed by Bob Melnik and his group at Grumman Aerospace as Grumfoil. Antony Jameson, originally at Grumman Aircraft and the Courant Institute of NYU, worked with David Caughey to develop the important three-dimensional Full Potential code FLO22 in 1975. Many Full Potential codes emerged after this, culminating in Boeing's Tranair (A633) code, which still sees heavy use.\n\nThe next step was the Euler equations, which promised to provide more accurate solutions of transonic flows. The methodology used by Jameson in his three-dimensional FLO57 code (1981) was used by others to produce such programs as Lockheed's TEAM program and IAI/Analytical Methods' MGAERO program. MGAERO is unique in being a structured cartesian mesh code, while most other such codes use structured body-fitted grids (with the exception of NASA's highly successful CART3D code, Lockheed's SPLITFLOW code and Georgia Tech's NASCART-GT). Antony Jameson also developed the three-dimensional AIRPLANE code which made use of unstructured tetrahedral grids.\n\nIn the two-dimensional realm, Mark Drela and Michael Giles, then graduate students at MIT, developed the ISES Euler program (actually a suite of programs) for airfoil design and analysis. This code first became available in 1986 and has been further developed to design, analyze and optimize single or multi-element airfoils, as the MSES program. MSES sees wide use throughout the world. A derivative of MSES, for the design and analysis of airfoils in a cascade, is MISES, developed by Harold \"Guppy\" Youngren while he was a graduate student at MIT.\n\nThe Navier–Stokes equations were the ultimate target of development. Two-dimensional codes, such as NASA Ames' ARC2D code first emerged. A number of three-dimensional codes were developed (ARC3D, OVERFLOW, CFL3D are three successful NASA contributions), leading to numerous commercial packages.\n\nIn all of these approaches the same basic procedure is followed.\n\nThe stability of the selected discretisation is generally established numerically rather than analytically as with simple linear problems. Special care must also be taken to ensure that the discretisation handles discontinuous solutions gracefully. The Euler equations and Navier–Stokes equations both admit shocks, and contact surfaces.\n\nSome of the discretization methods being used are:\n\nThe finite volume method (FVM) is a common approach used in CFD codes, as it has an advantage in memory usage and solution speed, especially for large problems, high Reynolds number turbulent flows, and source term dominated flows (like combustion).\n\nIn the finite volume method, the governing partial differential equations (typically the Navier-Stokes equations, the mass and energy conservation equations, and the turbulence equations) are recast in a conservative form, and then solved over discrete control volumes. This discretization guarantees the conservation of fluxes through a particular control volume. The finite volume equation yields governing equations in the form,\nwhere formula_2 is the vector of conserved variables, formula_3 is the vector of fluxes (see Euler equations or Navier–Stokes equations), formula_4 is the volume of the control volume element, and formula_5 is the surface area of the control volume element.\n\nThe finite element method (FEM) is used in structural analysis of solids, but is also applicable to fluids. However, the FEM formulation requires special care to ensure a conservative solution. The FEM formulation has been adapted for use with fluid dynamics governing equations. Although FEM must be carefully formulated to be conservative, it is much more stable than the finite volume approach. However, FEM can require more memory and has slower solution times than the FVM.\n\nIn this method, a weighted residual equation is formed:\n\nwhere formula_7 is the equation residual at an element vertex formula_8, formula_2 is the conservation equation expressed on an element basis, formula_10 is the weight factor, and formula_11 is the volume of the element.\n\nThe finite difference method (FDM) has historical importance and is simple to program. It is currently only used in few specialized codes, which handle complex geometry with high accuracy and efficiency by using embedded boundaries or overlapping grids (with the solution interpolated across each grid). \nwhere formula_2 is the vector of conserved variables, and formula_3, formula_15, and formula_16 are the fluxes in the formula_17, formula_18, and formula_19 directions respectively.\n\nSpectral element method is a finite element type method. It requires the mathematical problem (the partial differential equation) to be cast in a weak formulation. This is typically done by multiplying the differential equation by an arbitrary test function and integrating over the whole domain. Purely mathematically, the test functions are completely arbitrary - they belong to an infinite-dimensional function space. Clearly an infinite-dimensional function space cannot be represented on a discrete spectral element mesh; this is where the spectral element discretization begins. The most crucial thing is the choice of interpolating and testing functions. In a standard, low order FEM in 2D, for quadrilateral elements the most typical choice is the bilinear test or interpolating function of the form formula_20. In a spectral element method however, the interpolating and test functions are chosen to be polynomials of a very high order (typically e.g. of the 10th order in CFD applications). This guarantees the rapid convergence of the method. Furthermore, very efficient integration procedures must be used, since the number of integrations to be performed in numerical codes is big. Thus, high order Gauss integration quadratures are employed, since they achieve the highest accuracy with the smallest number of computations to be carried out.\nAt the time there are some academic CFD codes based on the spectral element method and some more are currently under development, since the new time-stepping schemes arise in the scientific world.\n\nIn the boundary element method, the boundary occupied by the fluid is divided into a surface mesh.\n\nHigh-resolution schemes are used where shocks or discontinuities are present. Capturing sharp changes in the solution requires the use of second or higher-order numerical schemes that do not introduce spurious oscillations. This usually necessitates the application of flux limiters to ensure that the solution is total variation diminishing.\n\nIn computational modeling of turbulent flows, one common objective is to obtain a model that can predict quantities of interest, such as fluid velocity, for use in engineering designs of the system being modeled. For turbulent flows, the range of length scales and complexity of phenomena involved in turbulence make most modeling approaches prohibitively expensive; the resolution required to resolve all scales involved in turbulence is beyond what is computationally possible. The primary approach in such cases is to create numerical models to approximate unresolved phenomena. This section lists some commonly used computational models for turbulent flows.\n\nTurbulence models can be classified based on computational expense, which corresponds to the range of scales that are modeled versus resolved (the more turbulent scales that are resolved, the finer the resolution of the simulation, and therefore the higher the computational cost). If a majority or all of the turbulent scales are not modeled, the computational cost is very low, but the tradeoff comes in the form of decreased accuracy.\n\nIn addition to the wide range of length and time scales and the associated computational cost, the governing equations of fluid dynamics contain a non-linear convection term and a non-linear and non-local pressure gradient term. These nonlinear equations must be solved numerically with the appropriate boundary and initial conditions.\n\nReynolds-averaged Navier–Stokes (RANS) equations are the oldest approach to turbulence modeling. An ensemble version of the governing equations is solved, which introduces new \"apparent stresses\" known as Reynolds stresses. This adds a second order tensor of unknowns for which various models can provide different levels of closure. It is a common misconception that the RANS equations do not apply to flows with a time-varying mean flow because these equations are 'time-averaged'. In fact, statistically unsteady (or non-stationary) flows can equally be treated. This is sometimes referred to as URANS. There is nothing inherent in Reynolds averaging to preclude this, but the turbulence models used to close the equations are valid only as long as the time over which these changes in the mean occur is large compared to the time scales of the turbulent motion containing most of the energy.\n\nRANS models can be divided into two broad approaches:\n\n\nLarge eddy simulation (LES) is a technique in which the smallest scales of the flow are removed through a filtering operation, and their effect modeled using subgrid scale models. This allows the largest and most important scales of the turbulence to be resolved, while greatly reducing the computational cost incurred by the smallest scales. This method requires greater computational resources than RANS methods, but is far cheaper than DNS.\n\nDetached eddy simulations (DES) is a modification of a RANS model in which the model switches to a subgrid scale formulation in regions fine enough for LES calculations. Regions near solid boundaries and where the turbulent length scale is less than the maximum grid dimension are assigned the RANS mode of solution. As the turbulent length scale exceeds the grid dimension, the regions are solved using the LES mode. Therefore, the grid resolution for DES is not as demanding as pure LES, thereby considerably cutting down the cost of the computation. Though DES was initially formulated for the Spalart-Allmaras model (Spalart et al., 1997), it can be implemented with other RANS models (Strelets, 2001), by appropriately modifying the length scale which is explicitly or implicitly involved in the RANS model. So while Spalart–Allmaras model based DES acts as LES with a wall model, DES based on other models (like two equation models) behave as a hybrid RANS-LES model. Grid generation is more complicated than for a simple RANS or LES case due to the RANS-LES switch. DES is a non-zonal approach and provides a single smooth velocity field across the RANS and the LES regions of the solutions.\n\nDirect numerical simulation (DNS) resolves the entire range of turbulent length scales. This marginalizes the effect of models, but is extremely expensive. The computational cost is proportional to formula_24. DNS is intractable for flows with complex geometries or flow configurations.\n\nThe coherent vortex simulation approach decomposes the turbulent flow field into a coherent part, consisting of organized vortical motion, and the incoherent part, which is the random background flow. This decomposition is done using wavelet filtering. The approach has much in common with LES, since it uses decomposition and resolves only the filtered portion, but different in that it does not use a linear, low-pass filter. Instead, the filtering operation is based on wavelets, and the filter can be adapted as the flow field evolves. Farge and Schneider tested the CVS method with two flow configurations and showed that the coherent portion of the flow exhibited the formula_25 energy spectrum exhibited by the total flow, and corresponded to coherent structures (vortex tubes), while the incoherent parts of the flow composed homogeneous background noise, which exhibited no organized structures. Goldstein and Vasilyev applied the FDV model to large eddy simulation, but did not assume that the wavelet filter completely eliminated all coherent motions from the subfilter scales. By employing both LES and CVS filtering, they showed that the SFS dissipation was dominated by the SFS flow field's coherent portion.\n\nProbability density function (PDF) methods for turbulence, first introduced by Lundgren, are based on tracking the one-point PDF of the velocity, formula_26, which gives the probability of the velocity at point formula_27 being between formula_28 and formula_29. This approach is analogous to the kinetic theory of gases, in which the macroscopic properties of a gas are described by a large number of particles. PDF methods are unique in that they can be applied in the framework of a number of different turbulence models; the main differences occur in the form of the PDF transport equation. For example, in the context of large eddy simulation, the PDF becomes the filtered PDF. PDF methods can also be used to describe chemical reactions, and are particularly useful for simulating chemically reacting flows because the chemical source term is closed and does not require a model. The PDF is commonly tracked by using Lagrangian particle methods; when combined with large eddy simulation, this leads to a Langevin equation for subfilter particle evolution.\n\nThe vortex method is a grid-free technique for the simulation of turbulent flows. It uses vortices as the computational elements, mimicking the physical structures in turbulence. Vortex methods were developed as a grid-free methodology that would not be limited by the fundamental smoothing effects associated with grid-based methods. To be practical, however, vortex methods require means for rapidly computing velocities from the vortex elements – in other words they require the solution to a particular form of the N-body problem (in which the motion of N objects is tied to their mutual influences). A breakthrough came in the late 1980s with the development of the fast multipole method (FMM), an algorithm by V. Rokhlin (Yale) and L. Greengard (Courant Institute). This breakthrough paved the way to practical computation of the velocities from the vortex elements and is the basis of successful algorithms. They are especially well-suited to simulating filamentary motion, such as wisps of smoke, in real-time simulations such as video games, because of the fine detail achieved using minimal computation.\n\nSoftware based on the vortex method offer a new means for solving tough fluid dynamics problems with minimal user intervention. All that is required is specification of problem geometry and setting of boundary and initial conditions. Among the significant advantages of this modern technology;\n\nThe vorticity confinement (VC) method is an Eulerian technique used in the simulation of turbulent wakes. It uses a solitary-wave like approach to produce a stable solution with no numerical spreading. VC can capture the small-scale features to within as few as 2 grid cells. Within these features, a nonlinear difference equation is solved as opposed to the finite difference equation. VC is similar to shock capturing methods, where conservation laws are satisfied, so that the essential integral quantities are accurately computed.\n\nThe Linear eddy model is a technique used to simulate the convective mixing that takes place in turbulent flow. Specifically, it provides a mathematical way to describe the interactions of a scalar variable within the vector flow field. It is primarily used in one-dimensional representations of turbulent flow, since it can be applied across a wide range of length scales and Reynolds numbers. This model is generally used as a building block for more complicated flow representations, as it provides high resolution predictions that hold across a large range of flow conditions.\n\nThe modeling of two-phase flow is still under development. Different methods have been proposed, including the Volume of fluid method, the level-set method and front tracking. These methods often involve a tradeoff between maintaining a sharp interface or conserving mass . This is crucial since the evaluation of the density, viscosity and surface tension is based on the values averaged over the interface. Lagrangian multiphase models, which are used for dispersed media, are based on solving the Lagrangian equation of motion for the dispersed phase.\n\nDiscretization in the space produces a system of ordinary differential equations for unsteady problems and algebraic equations for steady problems. Implicit or semi-implicit methods are generally used to integrate the ordinary differential equations, producing a system of (usually) nonlinear algebraic equations. Applying a Newton or Picard iteration produces a system of linear equations which is nonsymmetric in the presence of advection and indefinite in the presence of incompressibility. Such systems, particularly in 3D, are frequently too large for direct solvers, so iterative methods are used, either stationary methods such as successive overrelaxation or Krylov subspace methods. Krylov methods such as GMRES, typically used with preconditioning, operate by minimizing the residual over successive subspaces generated by the preconditioned operator.\n\nMultigrid has the advantage of asymptotically optimal performance on many problems. Traditional solvers and preconditioners are effective at reducing high-frequency components of the residual, but low-frequency components typically require many iterations to reduce. By operating on multiple scales, multigrid reduces all components of the residual by similar factors, leading to a mesh-independent number of iterations.\n\nFor indefinite systems, preconditioners such as incomplete LU factorization, additive Schwarz, and multigrid perform poorly or fail entirely, so the problem structure must be used for effective preconditioning. Methods commonly used in CFD are the SIMPLE and Uzawa algorithms which exhibit mesh-dependent convergence rates, but recent advances based on block LU factorization combined with multigrid for the resulting definite systems have led to preconditioners that deliver mesh-independent convergence rates.\n\nCFD made a major break through in late 70s with the introduction of LTRAN2, a 2-D code to model oscillating airfoils based on transonic small perturbation theory by Ballhaus and associates. It uses a Murman-Cole switch algorithm for modeling the moving shock-waves. Later it was extended to 3-D with use of a rotated difference scheme by AFWAL/Boeing that resulted in LTRAN3.\n\nCFD investigations are used to clarify the characteristics of aortic flow in detail that are otherwise invisible to experimental measurements. To analyze these conditions, CAD models of the human vascular system are extracted employing modern imaging techniques. A 3D model is reconstructed from this data and the fluid flow can be computed. Blood properties like Non-Newtonian behavior and realistic boundary conditions (e.g. systemic pressure) have to be taken into consideration. Therefore, making it possible to analyze and optimize the flow in the cardiovascular system for different applications.\n\nTraditionally, CFD simulations are performed on CPU's. In a more recent trend, simulations are also performed on GPU's. These typically contain slower but more processors. For CFD algorithms that feature good parallellisation performance (i.e. good speed-up by adding more cores) this can greatly reduce simulation times. Lattice-Boltzmann methods are a typical example of codes that scale well on GPU's.\n\n\n\n"}
{"id": "8884790", "url": "https://en.wikipedia.org/wiki?curid=8884790", "title": "Cutler's bar notation", "text": "Cutler's bar notation\n\nIn mathematics, Cutler's bar notation is a notation system for large numbers, introduced by Mark Cutler in 2004. The idea is based on iterated exponentiation in much the same way that exponentiation is iterated multiplication.\n\nA regular exponential can be expressed as such:\n\nHowever, these expressions become arbitrarily large when dealing with systems such as Knuth's up-arrow notation. Take the following:\n\nCutler's bar notation shifts these exponentials counterclockwise, forming formula_3. A bar is placed above the variable to denote this change. As such:\n\nThis system becomes effective with multiple exponents, when regular denotation becomes too cumbersome.\n\nThe same pattern could be iterated a fourth time, becoming formula_6. For this reason, it is sometimes referred to as Cutler's circular notation.\n\nThe Cutler bar notation can be used to easily express other notation systems in exponent form. It also allows for a flexible summarization of multiple copies of the same exponents, where any number of stacked exponents can be shifted counterclockwise and shortened to a single variable. The bar notation also allows for fairly rapid composure of very large numbers. For instance, the number formula_7 would contain more than a googolplex digits, while remaining fairly simple to write with and remember.\n\nHowever, the system reaches a problem when dealing with different exponents in a single expression. For instance, the expression formula_8 could not be summarized in bar notation. Additionally, the exponent can only be shifted thrice before it returns to its original position, making a five degree shift indistinguishable from a one degree shift. Some have suggested using a double and triple bar in subsequent rotations, though this presents problems when dealing with ten- and twenty-degree shifts.\n\n\n"}
{"id": "5194894", "url": "https://en.wikipedia.org/wiki?curid=5194894", "title": "Darboux derivative", "text": "Darboux derivative\n\nThe Darboux derivative of a map between a manifold and a Lie group is a variant of the standard derivative. In a certain sense, it is arguably a more natural generalization of the single-variable derivative. It allows a generalization of the single-variable fundamental theorem of calculus to higher dimensions, in a different vein than the generalization that is Stokes' theorem.\n\nLet formula_1 be a Lie group, and let formula_2 be its Lie algebra. The Maurer-Cartan form, formula_3, is the smooth formula_2-valued formula_5-form on formula_1 (cf. Lie algebra valued form) defined by\nfor all formula_8 and formula_9. Here formula_10 denotes left multiplication by the element formula_8 and formula_12 is its derivative at formula_13.\n\nLet formula_14 be a smooth function between a smooth manifold formula_15 and formula_1. Then the Darboux derivative of formula_17 is the smooth formula_2-valued formula_5-form \nthe pullback of formula_3 by formula_17. The map formula_17 is called an integral or primitive of formula_24.\n\nThe reason that one might call the Darboux derivative a more natural generalization of the derivative of single-variable calculus is this. In single-variable calculus, the derivative formula_25 of a function formula_26 assigns to each point in the domain a single number. According to the more general manifold ideas of derivatives, the derivative assigns to each point in the domain a linear map from the tangent space at the domain point to the tangent space at the image point. This derivative encapsulates two pieces of data: the image of the domain point \"and\" the linear map. In single-variable calculus, we drop some information. We retain only the linear map, in the form of a scalar multiplying agent (i.e. a number).\n\nOne way to justify this convention of retaining only the linear map aspect of the derivative is to appeal to the (very simple) Lie group structure of formula_27 under addition. The tangent bundle of any Lie group can be trivialized via left (or right) multiplication. This means that every tangent space in formula_27 may be identified with the tangent space at the identity, formula_29, which is the Lie algebra of formula_27. In this case, left and right multiplication are simply translation. By post-composing the manifold-type derivative with the tangent space trivialization, for each point in the domain we obtain a linear map from the tangent space at the domain point to the Lie algebra of formula_27. In symbols, for each formula_32 we look at the map\nSince the tangent spaces involved are one-dimensional, this linear map is just multiplication by some scalar. (This scalar can change depending on what basis we use for the vector spaces, but the canonical unit vector field formula_34 on formula_27 gives a canonical choice of basis, and hence a canonical choice of scalar.) This scalar is what we usually denote by formula_36.\n\nIf the manifold formula_15 is connected, and formula_38 are both primitives of formula_24, i.e. formula_40, then there exists some constant formula_41 such that \n\nThis constant formula_44 is of course the analogue of the constant that appears when taking an indefinite integral.\n\nRecall the structural equation for the Maurer-Cartan form:\nThis means that for all vector fields formula_46 and formula_47 on formula_1 and all formula_49, we have\nFor any Lie algebra-valued formula_5-form on any smooth manifold, all the terms in this equation make sense, so for any such form we can ask whether or not it satisfies this structural equation.\n\nThe usual fundamental theorem of calculus for single-variable calculus has the following local generalization.\n\nIf a formula_2-valued formula_5-form formula_54 on formula_15 satisfies the structural equation, then every point formula_56 has an open neighborhood formula_57 and a smooth map formula_58 such that\ni.e. formula_54 has a primitive defined in a neighborhood of every point of formula_15.\n\nFor a global generalization of the fundamental theorem, one needs to study certain monodromy questions in formula_15 and formula_1.\n\n"}
{"id": "330603", "url": "https://en.wikipedia.org/wiki?curid=330603", "title": "Disc integration", "text": "Disc integration\n\nDisc integration, also known in integral calculus as the disc method, is a means of calculating the volume of a solid of revolution of a solid-state material when integrating along an axis \"parallel\" to the axis of revolution. This method models the resulting three-dimensional shape as a stack of an infinite number of discs of varying radius and infinitesimal thickness. It is also possible to use the same principles with rings instead of discs (the \"washer method\") to obtain hollow solids of revolutions. This is in contrast to shell integration which integrates along an axis \"perpendicular\" to the axis of revolution.\n\nIf the function to be revolved is a function of , the following integral represents the volume of the solid of revolution:\n\nwhere is the distance between the function and the axis of rotation. This works only if the axis of rotation is horizontal (example: or some other constant).\n\nIf the function to be revolved is a function of , the following integral will obtain the volume of the solid of revolution:\n\nwhere is the distance between the function and the axis of rotation. This works only if the axis of rotation is vertical (example: or some other constant).\n\nTo obtain a hollow solid of revolution (the “washer method”), the procedure would be to take the volume of the inner solid of revolution and subtract it from the volume of the outer solid of revolution. This can be calculated in a single integral similar to the following: \n\nwhere is the function that is farthest from the axis of rotation and is the function that is closest to the axis of rotation. One should take caution not to evaluate the square of the difference of the two functions, but to evaluate the difference of the squares of the two functions.\n\nTo rotate about any horizontal axis, simply subtract from that axis each formula. If is the value of a horizontal axis, then the volume equals\n\nFor example, to rotate the region between and along the axis , one would integrate as follows:\n\nThe bounds of integration are the zeros of the first equation minus the second. Note that when integrating along an axis other than the , the graph of the function that is farthest from the axis of rotation may not be that obvious. In the previous example, even though the graph of is, with respect to the x-axis, further up than the graph of , with respect to the axis of rotation the function is the inner function: its graph is closer to or the equation of the axis of rotation in the example.\n\nThe same idea can be applied to both the -axis and any other vertical axis. One simply must solve each equation for before one inserts them into the integration formula.\n\n\n"}
{"id": "8328", "url": "https://en.wikipedia.org/wiki?curid=8328", "title": "Divergence", "text": "Divergence\n\nIn vector calculus, divergence is a vector operator that produces a scalar field, giving the quantity of a vector field's source at each point. More technically, the divergence represents the volume density of the outward flux of a vector field from an infinitesimal volume around a given point.\n\nAs an example, consider air as it is heated or cooled. The velocity of the air at each point defines a vector field. While air is heated in a region, it expands in all directions, and thus the velocity field points outward from that region. The divergence of the velocity field in that region would thus have a positive value. While the air is cooled and thus contracting, the divergence of the velocity has a negative value.\n\nIn physical terms, the divergence of a three-dimensional vector field is the extent to which the vector field flow behaves like a source at a given point. It is a local measure of its \"outgoingness\" – the extent to which there is more of some quantity exiting an infinitesimal region of space than entering it. If the divergence is nonzero at some point then there is compression or expansion at that point. (Note that we are imagining the vector field to be like the velocity vector field of a fluid (in motion) when we use the terms \"flow\" and so on.)\n\nMore rigorously, the divergence of a vector field at a point can be defined as the limit of the net flow of across the smooth boundary of a three-dimensional region divided by the volume of as shrinks to . Formally,\n\nwhere is the volume of , is the boundary of , and the integral is a surface integral with being the outward unit normal to that surface. The result, , is a function of . From this definition it also becomes obvious that can be seen as the \"source density\" of the flux of .\n\nIn light of the physical interpretation, a vector field with zero divergence everywhere is called \"incompressible\" or \"solenoidal\" – in which case any closed surface has no net flow across it.\n\nThe intuition that the sum of all sources minus the sum of all sinks should give the net flow outwards of a region is made precise by the divergence theorem.\n\nLet , , be a system of Cartesian coordinates in 3-dimensional Euclidean space, and let , , be the corresponding basis of unit vectors. The divergence of a continuously differentiable vector field is defined as the scalar-valued function:\n\nAlthough expressed in terms of coordinates, the result is invariant under rotations, as the physical interpretation suggests. This is because the trace of the Jacobian matrix of an -dimensional vector field in -dimensional space is invariant under any invertible linear transformation.\n\nThe common notation for the divergence is a convenient mnemonic, where the dot denotes an operation reminiscent of the dot product: take the components of the operator (see del), apply them to the corresponding components of , and sum the results. Because applying an operator is different from multiplying the components, this is considered an abuse of notation.\n\nThe divergence of a continuously differentiable second-order tensor field is a first-order tensor field:\n\nFor a vector expressed in local unit cylindrical coordinates as\nwhere is the unit vector in direction , the divergence is\n\nThe use of local coordinates is vital for the validity of the expression. If we consider the position vector and the functions formula_6, formula_7, and formula_8, which assign the corresponding global cylindrical coordinate to a vector, in general formula_9, formula_10, and formula_11. In particular, if we consider the identity function formula_12, we find that:\n\nIn spherical coordinates, with the angle with the axis and the rotation around the axis, and formula_14 again written in local unit coordinates, the divergence is\n\nUsing Einstein notation we can consider the divergence in general coordinates, which we write as , where is the number of dimensions of the domain. Here, the upper index refers to the number of the coordinate or component, so refers to the second component, and not the quantity squared. The index variable is used to refer to an arbitrary element, such as . The divergence can then be written via the Voss-Weyl formula, as:\n\nwhere formula_17 is the local coefficient of the volume element and are the components of with respect to the local unnormalized covariant basis (sometimes written as formula_18). The Einstein notation implies summation over , since it appears as both an upper and lower index.\n\nThe volume coefficient formula_17 is a function of position which depends on the coordinate system. In Cartesian, cylindrical and polar coordinates, formula_20 and formula_21 respectively, using the same conventions as above. It can also be expressed as formula_22, where formula_23 is the metric tensor. Since the determinant is a scalar quantity which doesn't depend on the indices, we can suppress them and simply write formula_24. Another expression comes from computing the determinant of the Jacobian for transforming from Cartesian coordinates, which for gives formula_25\n\nSome conventions expect all local basis elements to be normalized to unit length, as was done in the previous sections. If we write formula_26 for the normalized basis, and formula_27 for the components of with respect to it, we have that \nusing one of the properties of the metric tensor. By dotting both sides of the last equality with the contravariant element formula_29, we can conclude that formula_30. After substituting, the formula becomes:\n\nSee \"\" for further discussion.\n\nIt can be shown that any stationary flux that is at least twice continuously differentiable in and vanishes sufficiently fast for can be decomposed into an \"irrotational part\" and a \"source-free part\" . Moreover, these parts are explicitly determined by the respective \"source densities\" (see above) and \"circulation densities\" (see the article Curl):\n\nFor the irrotational part one has\n\nwith\n\nThe source-free part, , can be similarly written: one only has to replace the \"scalar potential\" by a \"vector potential\" and the terms by , and the source density \nby the circulation density .\n\nThis \"decomposition theorem\" is a by-product of the stationary case of electrodynamics. It is a special case of the more general Helmholtz decomposition which works in dimensions greater than three as well.\n\nThe following properties can all be derived from the ordinary differentiation rules of calculus. Most importantly, the divergence is a linear operator, i.e.,\n\nfor all vector fields and and all real numbers and .\n\nThere is a product rule of the following type: if is a scalar-valued function and is a vector field, then\n\nor in more suggestive notation\n\nAnother product rule for the cross product of two vector fields and in three dimensions involves the curl and reads as follows:\n\nor\n\nThe Laplacian of a scalar field is the divergence of the field's gradient:\n\nThe divergence of the curl of any vector field (in three dimensions) is equal to zero: \n\nIf a vector field with zero divergence is defined on a ball in , then there exists some vector field on the ball with . For regions in more topologically complicated than this, the latter statement might be false (see Poincaré lemma). The degree of \"failure\" of the truth of the statement, measured by the homology of the chain complex\n\nserves as a nice quantification of the complicatedness of the underlying region . These are the beginnings and main motivations of de Rham cohomology.\n\nOne can express the divergence as a particular case of the exterior derivative, which takes a 2-form to a 3-form in . Define the current two-form as\nIt measures the amount of \"stuff\" flowing through a surface per unit time in a \"stuff fluid\" of density moving with local velocity . Its exterior derivative is then given by\n\nThus, the divergence of the vector field can be expressed as:\nHere the superscript is one of the two musical isomorphisms, and is the Hodge star operator. Working with the current two-form and the exterior derivative is usually easier than working with the vector field and divergence, because unlike the divergence, the exterior derivative commutes with a change of (curvilinear) coordinate system.\n\nThe divergence of a vector field can be defined in any number of dimensions. If \n\nin a Euclidean coordinate system with coordinates , define\n\nThe appropriate expression is more complicated in curvilinear coordinates.\n\nIn the case of one dimension, reduces to a regular function, and the divergence reduces to the derivative.\n\nFor any , the divergence is a linear operator, and it satisfies the \"product rule\"\n\nfor any scalar-valued function .\n\nThe divergence of a vector field extends naturally to any differentiable manifold of dimension that has a volume form (or density) , e.g. a Riemannian or Lorentzian manifold. Generalising the construction of a two-form for a vector field on , on such a manifold a vector field defines an -form obtained by contracting with . The divergence is then the function defined by\n\nStandard formulas for the Lie derivative allow us to reformulate this as\n\nThis means that the divergence measures the rate of expansion of a volume element as we let it flow with the vector field.\n\nOn a pseudo-Riemannian manifold, the divergence with respect to the metric volume form can be computed in terms of the Levi-Civita connection :\n\nwhere the second expression is the contraction of the vector field valued 1-form with itself and the last expression is the traditional coordinate expression from Ricci calculus.\n\nAn equivalent expression without using connection is\n\nwhere is the metric and denotes the partial derivative with respect to coordinate .\n\nDivergence can also be generalised to tensors. In Einstein notation, the divergence of a contravariant vector is given by\n\nwhere denotes the covariant derivative.\n\nEquivalently, some authors define the divergence of a mixed tensor by using the musical isomorphism : if is a -tensor ( for the contravariant vector and for the covariant one), then we define the \"divergence of \" to be the -tensor\n\nthat is, we take the trace over the \"first two\" covariant indices of the covariant derivative\n\n\n\n"}
{"id": "2619735", "url": "https://en.wikipedia.org/wiki?curid=2619735", "title": "Flow (mathematics)", "text": "Flow (mathematics)\n\nIn mathematics, a flow formalizes the idea of the motion of particles in a fluid. Flows are ubiquitous in science, including engineering and physics. The notion of flow is basic to the study of ordinary differential equations. Informally, a flow may be viewed as a continuous motion of points over time. More formally, a flow is a group action of the real numbers on a set.\n\nThe idea of a vector flow, that is, the flow determined by a vector field, occurs in the areas of differential topology, Riemannian geometry and Lie groups. Specific examples of vector flows include the geodesic flow, the Hamiltonian flow, the Ricci flow, the mean curvature flow, and the Anosov flow. Flows may also be defined for systems of random variables and stochastic processes, and occur in the study of ergodic dynamical systems. The most celebrated of these is perhaps the Bernoulli flow.\n\nA flow on a set is a group action of the additive group of real numbers on . More explicitly, a flow is a mapping\nsuch that, for all ∈ and all real numbers and ,\n\nIt is customary to write instead of , so that the equations above can be expressed as (identity function) and (group law). Then, for all , the mapping is a bijection with inverse . This follows from the above definition, and the real parameter may be taken as a generalized functional power, as in function iteration.\n\nFlows are usually required to be compatible with structures furnished on the set . In particular, if is equipped with a topology, then is usually required to be continuous. If is equipped with a differentiable structure, then is usually required to be differentiable. In these cases the flow forms a one parameter subgroup of homeomorphisms and diffeomorphisms, respectively.\n\nIn certain situations one might also consider local flows, which are defined only in some subset\ncalled the flow domain of . This is often the case with the flows of vector fields.\n\nIt is very common in many fields, including engineering, physics and the study of differential equations, to use a notation that makes the flow implicit. Thus, is written for , and one might say that the \"variable depends on the time and the initial condition \". Examples are given below.\n\nIn the case of a flow of a vector field on a smooth manifold , the flow is often denoted in such a way that its generator is made explicit. For example,\n\nGiven in , the set formula_6 is called the orbit of under . Informally, it may be regarded as the trajectory of a particle that was initially positioned at . If the flow is generated by a vector field, then its orbits are the images of its integral curves.\n\nLet be a (time-independent) vector field\nand the solution of the initial value problem\n\nThen is the flow of the vector field \"F\". It is a well-defined local flow provided that the vector field \n\nIn the case of time-dependent vector fields , one denotes , where is the solution of\nThen is the time-dependent flow of \"F\". It is not a \"flow\" by the definition above, but it can easily be seen as one by rearranging its arguments. Namely, the mapping\nindeed satisfies the group law for the last variable:\nOne can see time-dependent flows of vector fields as special cases of time-independent ones by the following trick. Define\nThen y(\"t\") is the solution of the \"time-independent\" initial value problem\nif and only if is the solution of the original time-dependent initial value problem. Furthermore, then the mapping is exactly the flow of the \"time-independent\" vector field G.\n\nThe flows of time-independent and time-dependent vector fields are defined on smooth manifolds exactly as they are defined on the Euclidean space and their local behavior is the same. However, the global topological structure of a smooth manifold is strongly manifest in what kind of global vector fields it can support, and flows of vector fields on smooth manifolds are indeed an important tool in differential topology. The bulk of studies in dynamical systems are conducted on smooth manifolds, which are thought of as \"parameter spaces\" in applications.\n\nLet be a subdomain (bounded or not) of ℝ (with an integer). Denote by its boundary (assumed smooth). \nConsider the following Heat Equation on × (0,), for > 0,\nwith the following initial boundary condition in .\n\nThe equation = 0 on corresponds to the Homogeneous Dirichlet boundary condition. The mathematical setting for this problem can be the semigroup approach. To use this tool, we introduce the unbounded operator defined on formula_14 by its domain\n(see the classical Sobolev spaces with formula_16 \nand \nis the closure of the infinitely differentiable functions with compact support in for the formula_18norm). \n\nFor any formula_19, we have\n\nWith this operator, the heat equation becomes formula_21 and . Thus, the flow corresponding to this equation is (see notations above)\nwhere is the (analytic) semigroup generated by .\n\nAgain, let be a subdomain (bounded or not) of ℝ (with an integer). We denote by its boundary (assumed smooth). \nConsider the following Wave Equation on formula_23 (for > 0),\nwith the following initial condition in formula_25 and formula_26.\n\nUsing the same semigroup approach as in the case of the Heat Equation above. We write the wave equation as a first order in time partial differential equation by introducing the following unbounded operator,\nwith domain formula_28 on formula_29 (the operator formula_30 is defined in the previous example). \n\nWe introduce the column vectors \n(where formula_32 and formula_33) and \n\nWith these notions, the Wave Equation becomes formula_35 and formula_36. \n\nThus, the flow corresponding to this equation is \nformula_37\nwhere formula_38 is the (unitary) semigroup generated by formula_39.\n\nErgodic dynamical systems, that is, systems exhibiting randomness, exhibit flows as well. The most celebrated of these is perhaps the Bernoulli flow. The Ornstein isomorphism theorem states that, for any given entropy , there exists a flow , called the Bernoulli flow, such that the flow at time =1, \"i.e.\" , is a Bernoulli shift.\n\nFurthermore, this flow is unique, up to a constant rescaling of time. That is, if , is another flow with the same entropy, then \n, for some constant . The notion of uniqueness and isomorphism here is that of the isomorphism of dynamical systems. Many dynamical systems, including Sinai's billiards and Anosov flows are isomorphic to Bernoulli shifts.\n\n\n"}
{"id": "1488195", "url": "https://en.wikipedia.org/wiki?curid=1488195", "title": "Fundamental theorems of welfare economics", "text": "Fundamental theorems of welfare economics\n\nThere are two fundamental theorems of welfare economics. The First Theorem states that a market will tend toward a competitive equilibrium that is weakly Pareto optimal when the market maintains the following three attributes: \n\n1. Complete markets as no transaction costs and because of this each actor also has perfect information.\n\n2. Price-taking behavior as no monopolists and easy entry and exit from a market.\n\nFurthermore, the First Theorem states that the equilibrium will be fully Pareto optimal with the additional condition of:\n\n3. Local nonsatiation of preferences as for any original bundle of goods there is another bundle of goods arbitrarily close to the original bundle, but that is preferred.\n\nThe Second Theorem states that out of all possible Pareto optimal outcomes one can achieve any particular one by enacting a lump-sum wealth redistribution and then letting the market take over.\n\nThe First Theorem is often taken to be an analytical confirmation of Adam Smith's \"invisible hand\" hypothesis, namely that \"competitive markets tend toward an efficient allocation of resources\". The theorem supports a case for non-intervention in ideal conditions: let the markets do the work and the outcome will be Pareto efficient. However, Pareto efficiency is not necessarily the same thing as desirability; it merely indicates that no one can be made better off without someone being made worse off. There can be many possible Pareto efficient allocations of resources and not all of them may be equally desirable by society.\n\nThis appears to make the case that intervention has a legitimate place in policy – redistributions can allow us to select from all efficient outcomes for one that has other desired features, such as distributional equity. The shortcoming is that for the theorem to hold, the transfers have to be lump-sum and the government needs to have perfect information on individual consumers' tastes as well as the production possibilities of firms. An additional mathematical condition is that preferences and production technologies have to be convex.\n\nThe first fundamental theorem was first demonstrated graphically by economist Abba Lerner and mathematically by economists Harold Hotelling, Oskar Lange, Maurice Allais, Lionel McKenzie, Kenneth Arrow and Gérard Debreu. The theorem holds under general conditions.\n\nThe formal statement of the theorem is as follows: \"If preferences are locally nonsatiated, and if formula_1 is a price equilibrium with transfers, then the allocation formula_2is Pareto optimal.\" An equilibrium in this sense either relates to an exchange economy only or presupposes that firms are allocatively and productively efficient, which can be shown to follow from perfectly competitive factor and production markets.\n\nGiven a set formula_3 of types of goods we work in the real vector space over formula_3, formula_5 and use boldface for vector valued variables. For instance, if formula_6 then formula_5 would be a three dimensional vector space and the vector formula_8 would represent the bundle of goods containing one unit of butter, 2 units of cookies and 3 units of milk.\n\nSuppose that consumer \"i\" has wealth formula_9 such that formula_10 where formula_11 is the aggregate endowment of goods (i.e. the sum of all consumer and producer endowments) and formula_12 is the production of firm \"j\".\n\nPreference maximization (from the definition of price equilibrium with transfers) implies (using formula_13 to denote the preference relation for consumer \"i\"):\n\nIn other words, if a bundle of goods is strictly preferred to formula_16 it must be unaffordable at price formula_17. Local nonsatiation additionally implies:\n\nTo see why, imagine that formula_18 but formula_21. Then by local nonsatiation we could find formula_22 arbitrarily close to formula_23 (and so still affordable) but which is strictly preferred to formula_16. But formula_16 is the result of preference maximization, so this is a contradiction.\n\nAn allocation is a pair formula_26 where formula_27 and formula_28, i.e. formula_29 is the 'matrix' (allowing potentially infinite rows/columns) whose \"i\"th column is the bundle of goods allocated to consumer \"i\" and formula_30 is the 'matrix' whose \"j\"th column is the production of firm \"j\". We restrict our attention to feasible allocations which are those allocations in which no consumer sells or producer consumes goods which they lack, i.e.,for every good and every consumer that consumers initial endowment plus their net demand must be positive similarly for producers.\n\nNow consider an allocation formula_26 that Pareto dominates formula_32. This means that formula_18 for all \"i\" and formula_14 for some \"i\". By the above, we know formula_35 for all \"i\" and formula_36 for some \"i\". Summing, we find:\n\nBecause formula_38 is profit maximizing, we know formula_39, so formula_40. But goods must be conserved so formula_41. Hence, formula_26 is not feasible. Since all Pareto-dominating allocations are not feasible, formula_2 must itself be Pareto optimal.\n\nNote that while the fact that formula_38 is profit maximizing is simply assumed in the statement of the theorem the result is only useful/interesting to the extent such a profit maximizing allocation of production is possible. Fortunately, for any restriction of the production allocation formula_38 and price to a closed subset on which the marginal price is bounded away from 0, e.g., any reasonable choice of continuous functions to parameterize possible productions, such a maximum exists. This follows from the fact that the minimal marginal price and finite wealth limits the maximum feasible production (0 limits the minimum) and Tychonoff's theorem ensures the product of these compacts spaces is compact ensuring us a maximum of whatever continuous function we desire exists.\n\nThe Second Theorem formally states that, under the assumptions that every production set formula_46 is convex and every preference relation formula_47 is convex and locally nonsatiated, any desired Pareto-efficient allocation can be supported as a price \"quasi\"-equilibrium with transfers. Further assumptions are needed to prove this statement for price equilibria with transfers.\n\nThe proof proceeds in two steps: first, we prove that any Pareto-efficient allocation can be supported as a price quasi-equilibrium with transfers; then, we give conditions under which a price quasi-equilibrium is also a price equilibrium.\n\nLet us define a price quasi-equilibrium with transfers as an allocation formula_48, a price vector \"p\", and a vector of wealth levels \"w\" (achieved by lump-sum transfers) with formula_49 (where formula_50 is the aggregate endowment of goods and formula_51 is the production of firm \"j\") such that:\n\nThe only difference between this definition and the standard definition of a price equilibrium with transfers is in statement (\"ii\"). The inequality is weak here (formula_56) making it a price quasi-equilibrium. Later we will strengthen this to make a price equilibrium.\nDefine formula_62 to be the set of all consumption bundles strictly preferred to formula_58 by consumer \"i\", and let \"V\" be the sum of all formula_62. formula_62 is convex due to the convexity of the preference relation formula_47. \"V\" is convex because every formula_62 is convex. Similarly formula_68, the union of all production sets formula_69 plus the aggregate endowment, is convex because every formula_69 is convex. We also know that the intersection of \"V\" and formula_68 must be empty, because if it were not it would imply there existed a bundle that is strictly preferred to formula_48 by everyone and is also affordable. This is ruled out by the Pareto-optimality of formula_48.\n\nThese two convex, non-intersecting sets allow us to apply the separating hyperplane theorem. This theorem states that there exists a price vector formula_74 and a number \"r\" such that formula_75 for every formula_76 and formula_77 for every formula_78. In other words, there exists a price vector that defines a hyperplane that perfectly separates the two convex sets.\n\nNext we argue that if formula_79 for all \"i\" then formula_80. This is due to local nonsatiation: there must be a bundle formula_81 arbitrarily close to formula_57 that is strictly preferred to formula_58 and hence part of formula_62, so formula_85. Taking the limit as formula_86 does not change the weak inequality, so formula_80 as well. In other words, formula_57 is in the closure of \"V\".\n\nUsing this relation we see that for formula_58 itself formula_90. We also know that formula_91, so formula_92 as well. Combining these we find that formula_93. We can use this equation to show that formula_94 fits the definition of a price quasi-equilibrium with transfers.\n\nBecause formula_93 and formula_96 we know that for any firm j:\n\nwhich implies formula_52. Similarly we know:\n\nwhich implies formula_102. These two statements, along with the feasibility of the allocation at the Pareto optimum, satisfy the three conditions for a price quasi-equilibrium with transfers supported by wealth levels formula_103 for all \"i\".\n\nWe now turn to conditions under which a price quasi-equilibrium is also a price equilibrium, in other words, conditions under which the statement \"if formula_55 then formula_56\" imples \"if formula_55 then formula_107\". For this to be true we need now to assume that the consumption set formula_108 is convex and the preference relation formula_47 is continuous. Then, if there exists a consumption vector formula_81 such that formula_111 and formula_112, a price quasi-equilibrium is a price equilibrium.\n\nTo see why, assume to the contrary formula_55 and formula_114, and formula_57 exists. Then by the convexity of formula_108 we have a bundle formula_117 with formula_118. By the continuity of formula_47 for formula_120 close to 1 we have formula_121. This is a contradiction, because this bundle is preferred to formula_58 and costs less than formula_9.\n\nHence, for price quasi-equilibria to be price equilibria it is sufficient that the consumption set be convex, the preference relation to be continuous, and for there always to exist a \"cheaper\" consumption bundle formula_81. One way to ensure the existence of such a bundle is to require wealth levels formula_9 to be strictly positive for all consumers \"i\".\n\nBecause of welfare economics' close ties to social choice theory, Arrow's impossibility theorem is sometimes listed as a third fundamental theorem.\n\nThe ideal conditions of the theorems, however are an abstraction. The Greenwald-Stiglitz theorem, for example, states that in the presence of either imperfect information, or incomplete markets, markets are not Pareto efficient. Thus, in real world economies, the degree of these variations from ideal conditions must factor into policy choices. Further, even if these ideal conditions hold, the First Welfare Theorem fails in an overlapping generations model.\n\n"}
{"id": "35687516", "url": "https://en.wikipedia.org/wiki?curid=35687516", "title": "Guruswami–Sudan list decoding algorithm", "text": "Guruswami–Sudan list decoding algorithm\n\nIn coding theory, list decoding is an alternative to unique decoding of error-correcting codes for large error rates. Using unique decoder one can correct up to formula_1fraction of errors. But when error rate is greater than formula_1, unique decoder will not able to output the correct result. List decoding overcomes that issue. List decoding can correct more than formula_1 fraction of errors.\n\nThere are many efficient algorithms that can perform List decoding. In this article, we first present an algorithm for Reed–Solomon (RS) codes which corrects up to formula_4 errors and is due to Madhu Sudan. Subsequently we describe the improved Guruswami–Sudan list decoding algorithm, which can correct up to formula_5 errors.\n\nHere is a plot of the rate R and distance formula_6 for different algorithms.\n\nhttps://wiki.cse.buffalo.edu/cse545/sites/wiki.cse.buffalo.edu.cse545/files/81/Graph.jpg\n\nInput : A field formula_7; n distinct pairs of elements formula_8 in formula_9; and integers formula_10 and formula_11.\n\nOutput: A list of all functions formula_12 satisfying\n\nformula_13 is a polynomial in formula_14 of degree at most formula_15\n\nTo understand Sudan's Algorithm better, one may want to first know another algorithm which can be considered as the earlier version or the fundamental version of the algorithms for list decoding RS codes - the Berlekamp–Welch algorithm.\nWelch and Berlekamp initially came with an algorithm which can solve the problem in polynomial time with best threshold on formula_11 to be formula_17. \nThe mechanism of Sudan's Algorithm is almost the same as the algorithm of Berlekamp–Welch Algorithm, except in the step 1, one wants to compute a bivariate polynomial of bounded formula_18 degree. Sudan's list decoding algorithm for Reed–Solomon code which is an improvement on Berlekamp and Welch algorithm, can solve the problem with formula_19.This bound is better than the unique decoding bound formula_20 for formula_21.\n\nDefinition 1 (weighted degree)\n\nFor weights formula_22, the formula_23 – weighted degree of monomial formula_24 is formula_25. The formula_23 – weighted degree of a polynomial formula_27 is the maximum, over the monomials with non-zero coefficients, of the formula_23 – weighted degree of the monomial.\n\nFor example, formula_29 has formula_30-degree 7\n\nAlgorithm:\n\nInputs: formula_31; {formula_32} /* Parameters l,m to be set later. */\n\nStep 1: Find a non-zero bivariate polynomial formula_33 satisfying\n\nStep 2. Factor Q into irreducible factors.\n\nStep 3. Output all the polynomials formula_38 such that formula_39 is a factor of Q and formula_40 for at least t values of formula_37\n\nOne has to prove that the above algorithm runs in polynomial time and outputs the correct result. That can be done by proving following set of claims.\n\nClaim 1:\n\nIf a function formula_42 satisfying (2) exists, then one can find it in polynomial time.\n\nProof:\n\nNote that a bivariate polynomial formula_43 of formula_44-weighted degree at most formula_45 can be uniquely written as formula_46. Then one has to find the coefficients formula_47 satisfying the constraints formula_48, for every formula_49. This is a linear set of equations in the unknowns {formula_47}. One can find a solution using Gaussian elimination in polynomial time.\n\nClaim 2:\n\nIf formula_51 then there exists a function formula_52 satisfying (2)\n\nProof:\n\nTo ensure a non zero solution exists, the number of coefficients in formula_34 should be greater than the number of constraints. Assume that the maximum degree formula_54 of formula_55 in formula_56 is m and the maximum degree formula_57 of formula_58 in formula_56 is formula_60. Then the degree of formula_34 will be at most formula_62. One has to see that the linear system is homogenous. The setting formula_63 satisfies all linear constraints. However this does not satisfy (2), since the solution can be identically zero. To ensure that a non-zero solution exists, one has to make sure that number of unknowns in the linear system to be formula_51, so that one can have a non zero formula_52. Since this value is greater than n, there are more variables than constraints and therefore a non-zero solution exists.\n\nClaim 3:\n\nIf formula_34 is a function satisfying (2) and formula_67 is function satisfying (1) and formula_68, then formula_69 divides formula_34\n\nProof:\n\nConsider a function formula_71. This is a polynomial in formula_72, and argue that it has degree at most formula_73. Consider any monomial formula_74 of formula_75. Since formula_76 has formula_35-weighted degree at most formula_73, one can say that formula_79. Thus the term formula_80 is a polynomial in formula_72 of degree at most formula_79. Thus formula_83 has degree at most formula_73\n\nNext argue that formula_85 is identically zero. Since formula_86 is zero whenever formula_87, one can say that formula_88 is zero for strictly greater than formula_62 points. Thus formula_90has more zeroes than its degree and hence is identically zero, implying formula_91\n\nFinding optimal values for formula_92 and formula_60.\nNote that formula_94 and formula_95\nFor a given value formula_60, one can compute the smallest formula_92 for which the second condition holds\nBy interchanging the second condition one can get formula_92 to be at most formula_99\nSubstituting this value into first condition one can get formula_11 to be at least formula_101\nNext minimize the above equation of unknown parameter formula_60. One can do that by taking derivative of the equation and equating that to zero\nBy doing that one will get, formula_103\nSubstituting back the formula_60 value into formula_92 and formula_11 one will get\nformula_107\nformula_108\nformula_109\n\nConsider a formula_110 Reed–Solomon code over the finite field formula_111 with evaluation set formula_112 and a positive integer formula_113, the Guruswami-Sudan List Decoder accepts a vector formula_114 formula_115 formula_116 as input, and outputs a list of polynomials of degree formula_117 which are in 1 to 1 correspondence with codewords.\n\nThe idea is to add more restrictions on the bi-variate polynomial formula_34 which results in the increment of constraints along with the number of roots.\n\nA bi-variate polynomial formula_34 has a zero of multiplicity formula_113 at formula_121 means that formula_34 has no term of degree formula_123, where the \"x\"-degree of formula_67 is defined as the maximum degree of any x term in formula_67\nformula_126 formula_127 formula_128 formula_129\n\nFor example: \nLet formula_130.\n\nhttps://wiki.cse.buffalo.edu/cse545/sites/wiki.cse.buffalo.edu.cse545/files/76/Fig1.jpg\n\nHence, formula_34 has a zero of multiplicity 1 at (0,0).\n\nLet formula_132.\n\nhttps://wiki.cse.buffalo.edu/cse545/sites/wiki.cse.buffalo.edu.cse545/files/76/Fig2.jpg\n\nHence, formula_34 has a zero of multiplicity 1 at (0,0).\n\nLet formula_134\n\nhttps://wiki.cse.buffalo.edu/cse545/sites/wiki.cse.buffalo.edu.cse545/files/76/Fig3.jpg\n\nHence, formula_34 has a zero of multiplicity 2 at (0,0).\n\nSimilarly, if formula_136\nThen, formula_34 has a zero of multiplicity 2 at formula_138.\n\nformula_34 has formula_113 roots at formula_138 if formula_34 has a zero of multiplicity formula_113 at formula_138 when formula_145.\n\nLet the transmitted codeword be formula_146,formula_112 be the support set of the transmitted codeword & the received word be formula_148\n\nThe algorithm is as follows:\n\n• Interpolation step\n\nFor a received vector formula_148, construct a non-zero bi-variate polynomial formula_34 with formula_151weighted degree of at most formula_10 such that formula_76 has a zero of multiplicity formula_113 at each of the points formula_155 where formula_156\n\n• Factorization step\n\nFind all the factors of formula_34 of the form formula_159 and formula_160 for at least formula_11 values of formula_162\n\nwhere formula_163 & formula_85 is a polynomial of degree formula_117\n\nRecall that polynomials of degree formula_117 are in 1 to 1 correspondence with codewords. Hence, this step outputs the list of codewords.\n\nLemma:\nInterpolation step implies formula_167 constraints on the coefficients of formula_168\n\nLet formula_169\nwhere formula_170 and formula_171\n\nThen, formula_172 formula_128 formula_174 formula_175 formula_176 formula_177 formula_178 ...(Equation 1)\n\nwhere formula_175 formula_180 formula_128 formula_182 formula_183 formula_184 formula_185 formula_186 formula_187\n\nProof of Equation 1:\n\nProof of Lemma:\n\nThe polynomial formula_193 has a zero of multiplicity formula_113 at formula_138 if\n\nformula_204 formula_128 formula_167\n\nThus, formula_167 number of selections can be made for formula_208 and each selection implies constraints on the coefficients of formula_168\n\nProposition:\n\nformula_210 if formula_159 is a factor of formula_34\n\nProof:\n\nSince, formula_159 is a factor of formula_34, formula_34 can be represented as\n\nformula_216 formula_217 formula_218\n\nwhere, formula_219 is the quotient obtained when formula_34 is divided by formula_159\nformula_218 is the remainder\n\nNow, if formula_223 is replaced by formula_85, \nformula_225 formula_198 formula_199, only if formula_218 formula_198 formula_199\n\nTheorem:\n\nIf formula_231, then formula_232 is a factor of formula_233\n\nProof:\n\nformula_193 formula_128 formula_236 formula_175 formula_176 formula_239 formula_240...From Equation 2\n\nformula_225 formula_128 formula_236 formula_175 formula_176 formula_239 formula_247\n\nGiven, formula_248 formula_128 formula_250\nformula_251 mod formula_252 formula_128 formula_199\n\nHence, formula_239 formula_247 mod formula_257 formula_128 formula_199\n\nThus, formula_232 is a factor of formula_233.\n\nAs proved above,\n\nformula_262\n\nformula_263\n\nformula_264\nwhere LHS is the upper bound on the number of coefficients of formula_34 and RHS is the earlier proved Lemma.\n\nTherefore, formula_267\n\nSubstitute formula_268,\n\nHence proved, that Guruswami–Sudan List Decoding Algorithm can list decode Reed-Solomon(RS) codes up to formula_270 errors.\n\n"}
{"id": "37542163", "url": "https://en.wikipedia.org/wiki?curid=37542163", "title": "Horrocks construction", "text": "Horrocks construction\n\nIn mathematics, the Horrocks construction is a method for constructing vector bundles, especially over projective spaces, introduced by . His original construction gave an example of an indecomposable rank 2 vector bundle over 3-dimensional projective space, and generalizes to give examples of vector bundles of higher ranks over other projective spaces. The Horrocks construction is used in the ADHM construction to construct instantons over the 4-sphere.\n\n"}
{"id": "55997815", "url": "https://en.wikipedia.org/wiki?curid=55997815", "title": "Hyper-Wiener index", "text": "Hyper-Wiener index\n\nIn chemical graph theory, the hyper-Wiener index or hyper-Wiener number is a topological index of a molecule, used in biochemistry. The hyper-Wiener index is a generalization introduced by Milan Randić\n\nwhere \"d\"(\"u\",\"v\") is the distance between vertex \"u\" and \"v\".\nHyper-Wiener index as topological index assigned to \"G\" = (\"V\",\"E\") is based on the distance function which is invariant under the action of the automorphism group of \"G\".\n\nOne-pentagonal carbon nanocone which is an infinite symmetric graph, consists of one pentagon as its core surrounded by layers of hexagons. If there are \"n\" layers, then the graph of the molecules is denoted by \"G\".\nwe have the following explicit formula for hyper-Wiener index of one-pentagonal carbon nanocone,\n"}
{"id": "40086179", "url": "https://en.wikipedia.org/wiki?curid=40086179", "title": "Ian Munro (computer scientist)", "text": "Ian Munro (computer scientist)\n\nJames Ian Munro (born July 10, 1947) is a Canadian computer scientist. He is known for his fundamental contributions to algorithms and data structures (including optimal binary search trees, priority queues, hashing, and space-efficient data structures).\n\nAfter earning a bachelor's degree in 1968 from the University of New Brunswick and a master's in 1969 from the University of British Columbia,\nMunro finished his doctorate in 1971 from the University of Toronto, under the supervision of Allan Borodin. In , he formalized the notion of an implicit data structure, and has continued work in this area. He is currently a University Professor in the David R. Cheriton School of Computer Science at the University of Waterloo.\n\nMunro was elected as a member of the Royal Society of Canada in 2003. He became an ACM Fellow in 2008 for his contributions to algorithms and data structures.\n\nIn 2013 a conference was held at Waterloo in his honor, and a festschrift was published as its proceedings.\n"}
{"id": "45196", "url": "https://en.wikipedia.org/wiki?curid=45196", "title": "Injective function", "text": "Injective function\n\nIn mathematics, an injective function or injection or one-to-one function is a function that preserves distinctness: it never maps distinct elements of its domain to the same element of its codomain. In other words, every element of the function's codomain is the image of \"at most\" one element of its domain. The term \"one-to-one function\" must not be confused with \"one-to-one correspondence\" (a.k.a. bijective function), which uniquely maps all elements in both domain and codomain to each other (see figures).\n\nOccasionally, an injective function from \"X\" to \"Y\" is denoted , using an arrow with a barbed tail (). The set of injective functions from \"X\" to \"Y\" may be denoted \"Y\" using a notation derived from that used for falling factorial powers, since if \"X\" and \"Y\" are finite sets with respectively \"m\" and \"n\" elements, the number of injections from \"X\" to \"Y\" is \"n\" (see the twelvefold way).\n\nA function \"f\" that is not injective is sometimes called many-to-one. However, the injective terminology is also sometimes used to mean \"single-valued\", i.e., each argument is mapped to at most one value.\n\nA monomorphism is a generalization of an injective function in category theory.\n\nLet \"f\" be a function whose domain is a set \"X\". The function \"f\" is said to be injective provided that for all \"a\" and \"b\" in \"X\", whenever , then ; that is, implies .  Equivalently, if , then .\n\nSymbolically,\n\nwhich is logically equivalent to the contrapositive,\n\n\nMore generally, when \"X\" and \"Y\" are both the real line R, then an injective function is one whose graph is never intersected by any horizontal line more than once. This principle is referred to as the \"horizontal line test\".\n\nFunctions with left inverses are always injections. That is, given , if there is a function such that, for every ,\n\nthen \"f\" is injective. In this case, \"g\" is called a retraction of \"f\". Conversely, \"f\" is called a section of \"g\".\n\nConversely, every injection \"f\" with non-empty domain has a left inverse \"g\", which can be defined by fixing an element \"a\" in the domain of \"f\" so that \"g\"(\"x\") equals the unique preimage of \"x\" under \"f\" if it exists and \"g\"(\"x\") = \"a\" otherwise. \n\nThe left inverse \"g\" is not necessarily an inverse of \"f\" because the composition in the other order, , may differ from the identity on \"Y\". In other words, an injective function can be \"reversed\" by a left inverses, but is not necessarily invertible, which requires that the function is bijective.\n\nIn fact, to turn an injective function into a bijective (hence invertible) function, it suffices to replace its codomain \"Y\" by its actual range . That is, let such that for all \"x\" in \"X\"; then \"g\" is bijective. Indeed, \"f\" can be factored as , where is the inclusion function from \"J\" into \"Y\".\n\nMore generally, injective partial functions are called partial bijections.\n\n\n\nA proof that a function \"f\" is injective depends on how the function is presented and what properties the function holds.\nFor functions that are given by some formula there is a basic idea.\nWe use the contrapositive of the definition of injectivity, namely that if , then .\n\nHere is an example:\n\nProof: Let . Suppose . So ⇒ ⇒ . Therefore, it follows from the definition that \"f\" is injective.\n\nThere are multiple other methods of proving that a function is injective. For example, in calculus if \"f\" is a differentiable function defined on some interval, then it is sufficient to show that the derivative is always positive or always negative on that interval. In linear algebra, if \"f\" is a linear transformation it is sufficient to show that the kernel of \"f\" contains only the zero vector. If \"f\" is a function with finite domain it is sufficient to look through the list of images of each domain element and check that no image occurs twice on the list.\n\n\n\n"}
{"id": "1034227", "url": "https://en.wikipedia.org/wiki?curid=1034227", "title": "Insolubilia", "text": "Insolubilia\n\nIn the Middle Ages, variations on the liar paradox were studied under the name of insolubilia (\"insolubles\").\n\nAlthough the liar paradox was well known in antiquity, interest seems to have lapsed until the twelfth century, when it appears to have been reinvented independently of ancient authors. Medieval interest may have been inspired by a passage in the \"Sophistical Refutations\" of Aristotle. Although the \"Sophistical Refutations\" are consistently cited by medieval logicians from the earliest \"insolubilia\" literature, medieval studies of \"insolubilia\" go well beyond Aristotle. Other ancient sources which could suggest the liar paradox, including Saint Augustine, Cicero, and the quotation of Epimenides appearing in the Epistle to Titus, were not cited in discussions of \"insolubilia\".\n\nAdam of Balsham mentioned, in passing, some paradoxical statements (dated to 1132), but he did not dwell on the difficulties raised by these statements. Alexander Neckham, writing later in the twelfth century, explicitly recognized the paradoxical nature of \"insolubilia\", but did not attempt to resolve the inconsistent implications of the paradox. The first resolution was given by an anonymous author at the end of the twelfth or beginning of the thirteenth century. There was an established literature on the topic by about 1320, when Thomas Bradwardine prefaced his own discussion of \"insolubilia\" with nine views then current. Interest in \"insolubilia\" continued throughout the fourteenth century, especially by Jean Buridan.\n\nThe medieval \"insolubilia\" literature seems to treat these paradoxes as difficult but not truly \"insoluble\", and, though interesting and meriting investigation, not central to the study of logic. This may be contrasted with modern studies of self-referential paradoxes such as Russell's paradox, in which the problems are seen as fundamentally insoluble, and central to the foundations of logic.\n\nThomas Bradwardine, \"Insolubilia\" (Insolubles), Latin text and English translation by Stephen Read, Leuven, Peeters Editions (Dallas Medieval Texts and Translations, 10), 2010.\n"}
{"id": "50901898", "url": "https://en.wikipedia.org/wiki?curid=50901898", "title": "Invasion percolation", "text": "Invasion percolation\n\nInvasion percolation is a mathematical model of realistic fluid distributions for slow immiscible fluid invasion in porous media, in percolation theory.\nIt was introduced by Wilkinson and Willemsen (1983).\n"}
{"id": "5005005", "url": "https://en.wikipedia.org/wiki?curid=5005005", "title": "Israel Nathan Herstein", "text": "Israel Nathan Herstein\n\nIsrael Nathan Herstein (March 28, 1923 – February 9, 1988) was a mathematician, appointed as professor at the University of Chicago in 1951. He worked on a variety of areas of algebra, including ring theory, with over 100 research papers and over a dozen books.\n\nHerstein was born in Lublin, Poland, in 1923. His family emigrated to Canada in 1926, and he grew up in a harsh and underprivileged environment where, according to him, \"you either became a gangster or a college professor.\" During his school years he played football, ice hockey, golf, tennis, and pool. He also worked as a steeplejack and as a barker at a fair. He received his B.S. degree from the University of Manitoba and his M.A. from the University of Toronto. He received his Ph.D from Indiana University in 1948. His advisor was Max Zorn. He held positions at the University of Kansas, Ohio State University, University of Pennsylvania, and Cornell University before permanently settling at the University of Chicago in 1962. He was a Guggenheim Fellow for the academic year 1960–1961.\n\nHe is known for his lucid style of writing, as exemplified by his \"Topics in Algebra\", an undergraduate introduction to abstract algebra that was first published in 1964, with a second edition in 1975. A more advanced text is his \"Noncommutative Rings\" in the Carus Mathematical Monographs series. His primary interest was in noncommutative ring theory, but he also wrote papers on finite groups, linear algebra, and mathematical economics.\n\nHe had 30 Ph.D. students, traveled and lectured widely, and spoke Italian, Hebrew, Polish, and Portuguese. He died from cancer in Chicago, Illinois, in 1988. His doctoral students include Miriam Cohen, Susan Montgomery, Karen Parshall and Claudio Procesi.\n\n\n"}
{"id": "18381", "url": "https://en.wikipedia.org/wiki?curid=18381", "title": "John William Strutt, 3rd Baron Rayleigh", "text": "John William Strutt, 3rd Baron Rayleigh\n\nJohn William Strutt, 3rd Baron Rayleigh, (; 12 November 1842 – 30 June 1919), was a British scientist who made extensive contributions to both theoretical and experimental physics. He spent all of his academic career at the University of Cambridge. Among many honours, he received the 1904 Nobel Prize in Physics \"for his investigations of the densities of the most important gases and for his discovery of argon in connection with these studies.\" He served as President of the Royal Society from 1905 to 1908 and as Chancellor of the University of Cambridge from 1908 to 1919.\n\nRayleigh provided the first theoretical treatment of the elastic scattering of light by particles much smaller than the light's wavelength, a phenomenon now known as \"Rayleigh scattering\", which notably explains why the sky is blue. He studied and described transverse surface waves in solids, now known as \"Rayleigh waves\". He contributed extensively to fluid dynamics, with concepts such as the Rayleigh number (a dimensionless number associated with natural convection), Rayleigh flow, the Rayleigh–Taylor instability, and Rayleigh's criterion for the stability of Taylor–Couette flow. He also formulated the circulation theory of aerodynamic lift. In optics, Rayleigh proposed a well known criterion for angular resolution. His derivation of the Rayleigh–Jeans law for classical black-body radiation later played an important role in birth of quantum mechanics (see Ultraviolet catastrophe). Rayleigh's textbook \"The Theory of Sound\" (1877) is still used today by acousticians and engineers.\n\nStrutt was born on 12 November 1842 at Langford Grove in Maldon, Essex. In his early years he suffered from frailty and poor health. He attended Eton College and Harrow School (each for only a short period), before going on to the University of Cambridge in 1861 where he studied mathematics at Trinity College, Cambridge. He obtained a Bachelor of Arts degree (Senior Wrangler and 1st Smith's Prize) in 1865, and a Master of Arts in 1868. He was subsequently elected to a Fellowship of Trinity. He held the post until his marriage to Evelyn Balfour, daughter of James Maitland Balfour, in 1871. He had three sons with her. In 1873, on the death of his father, John Strutt, 2nd Baron Rayleigh, he inherited the Barony of Rayleigh.\n\nHe was the second Cavendish Professor of Physics at the University of Cambridge (following James Clerk Maxwell), from 1879 to 1884. He first described dynamic soaring by seabirds in 1883, in the British journal \"Nature\". From 1887 to 1905 he was Professor of Natural Philosophy at the Royal Institution.\n\nAround the year 1900 Rayleigh developed the \"duplex\" (combination of two) theory of human sound localisation using two binaural cues, interaural phase difference (IPD) and interaural level difference (ILD) (based on analysis of a spherical head with no external pinnae). The theory posits that we use two primary cues for sound lateralisation, using the difference in the phases of sinusoidal components of the sound and the difference in amplitude (level) between the two ears.\n\nIn 1919, Rayleigh served as President of the Society for Psychical Research. As an advocate that simplicity and theory be part of the scientific method, Rayleigh argued for the principle of similitude.\n\nRayleigh was elected Fellow of the Royal Society on 12 June 1873, and served as president of the Royal Society from 1905 to 1908. From time to time Rayleigh participated in the House of Lords; however, he spoke up only if politics attempted to become involved in science.\n\nHe died on 30 June 1919, in Witham, Essex. He was succeeded, as the 4th Lord Rayleigh, by his son Robert John Strutt, another well-known physicist. Lord Rayleigh was buried in the graveyard of All Saints' Church in Terling in Essex.\n\nThe rayl unit of acoustic impedance is named after him.\n\nRayleigh was an Anglican. Though he did not write about the relationship of science and religion, he retained a personal interest in spiritual matters. When his scientific papers were to be published in a collection by the Cambridge University Press, Strutt wanted to include a religious quotation from the Bible, but he was discouraged from doing so, as he later reported:\n\nStill, he had his wish and the quotation was printed in the five-volume collection of scientific papers.\n\nIn a letter to a family member, he wrote about his rejection of materialism and spoke of Jesus Christ as a moral teacher:\n\nHe held an interest in parapsychology and was an early member of the Society for Psychical Research (SPR). He was not convinced of spiritualism but remained open to the possibility of supernatural phenomena. Rayleigh was the president of the SPR in 1919. He gave a presidential address the year of his death but did not come to any definite conclusions.\n\nThe lunar crater \"Rayleigh\" as well as the Martian crater \"Rayleigh\" were named in his honour. The asteroid 22740 Rayleigh was named after him on 1 June 2007. A type of surface waves are known as Rayleigh waves. The rayl, a unit of specific acoustic impedance, is also named for him. Rayleigh was also awarded with (in chronological order):\n\nLord Rayleigh was among the original recipients of the Order of Merit (OM) in the 1902 Coronation Honours list published on 26 June 1902, and received the order from King Edward VII at Buckingham Palace on 8 August 1902.\n\nHe received the degree of \"Doctor mathematicae (honoris causa)\" from the Royal Frederick University on 6 September 1902, when they celebrated the centennial of the birth of mathematician Niels Henrik Abel.\n\nSir William Ramsay, his co-worker in the investigation to discover Argon described Rayleigh as \"the greatest man alive\" while speaking to Lady Ramsay during his last illness.\n\nH. M. Hyndmann said of Rayleigh that \"no man ever showed less consciousness of great genius\".\n\n\n"}
{"id": "4556261", "url": "https://en.wikipedia.org/wiki?curid=4556261", "title": "KL-51", "text": "KL-51\n\nThe KL-51 is an off-line keyboard encryption system that read and punched paper tape for use with teleprinters. In NATO it was called RACE (Rapid Automatic Cryptographic Equipment).\n\nIt was developed in the 1970s by a Norwegian company, Standard Telefon og Kabelfabrik (STK). It used digital electronics for encryption instead of rotors, and it may have been the first machine to use software based crypto algorithms. KL-51 is a very robust machine made to military specifications.\n\nU.S. National Security Agency bought it in the 1980s to replace the earlier KL-7. As of 2006, the U.S. Navy was developing plans to replace KL-51 units still in use with a unit based on a more modern Universal Crypto Device.\n\n"}
{"id": "46630283", "url": "https://en.wikipedia.org/wiki?curid=46630283", "title": "Kenneth I. Gross", "text": "Kenneth I. Gross\n\nKenneth Irwin Gross (14 October 1938 – 10 September 2017) was an American mathematician.\n\nBorn in Malden, Massachusetts in 1938, Gross received from Brandeis University his bachelor's degree in 1960 and his master's degree in 1962. He received his Ph.D. in 1966 from Washington University in St. Louis under Ray Kunze with thesis \"Plancherel Transform of the Nilpotent Part of formula_1 and Some Applications to the Representation Theory of formula_1\". He was an assistant professor from 1966 to 1968 at Tulane University and an assistant professor from 1968 to 1973 at Dartmouth College. He became in 1973 an associate professor and eventually a full professor at the University of North Carolina before resigning in 1981. From 1981 to 1985 he was the chair of the mathematics department of the University of Wyoming. In 1988 Gross became a professor at the University of Vermont, where he served as chair of the department of mathematics and statistics from 1988 to 1992. On a leave of absence he was for two years (2003–2005) at Lesley University, where he developed the mathematics program.\n\nGross has been a visiting professor at the University of California, Irvine, the University of Utah, the Academia Sinica in Taiwan, Drexel University, Macquarie University, and Australia's University of Newcastle. He has twice served as a divisional program director for the National Science Foundation. He was the director of the Vermont Mathematics Initiative.\n\nHe did research on harmonic analysis, group representation theory, analysis on Lie groups and homogeneous spaces, special functions, Fourier analysis, and mathematical applications to physics and multivariate statistics.\n\nIn 1979 he received the Lester Randolph Ford Award. In 1981 he received the Chauvenet Prize from the Mathematical Association of America. In 2012 he was elected a Fellow of the American Mathematical Society. He died on September 10, 2017 at the age of 78.\n\n\n"}
{"id": "40944268", "url": "https://en.wikipedia.org/wiki?curid=40944268", "title": "Leo Sario", "text": "Leo Sario\n\nLeo Reino Sario (18 May 1916 – 15 August 2009) was a Finnish-born mathematician who worked on complex analysis and Riemann surfaces.\n\nAfter service as a Finnish artillery officer in the Winter War and World War II, he received his PhD in 1948 under Rolf Nevanlinna at the University of Helsinki. Nevanlinna and Sario were founding members of the Academy of Finland, and there is a statue on the Academy grounds named after Sario. Sario moved to the United States in 1950 and obtained temporary positions at the Institute for Advanced Study, MIT, Stanford University, and Harvard University. In 1954 he became a professor at UCLA, remaining there until his retirement in 1986. He was the author or co-author of five major books on complex analysis and over 130 papers. He supervised 36 doctoral students, including Kōtarō Oikawa and Burton Rodin. In 1957 he was awarded the Cross of the Commander of Finland's Order of Knighthood.\n"}
{"id": "473486", "url": "https://en.wikipedia.org/wiki?curid=473486", "title": "List of variational topics", "text": "List of variational topics\n\nThis is a list of variational topics in from mathematics and physics. See calculus of variations for a general introduction.\n"}
{"id": "33975742", "url": "https://en.wikipedia.org/wiki?curid=33975742", "title": "László Filep", "text": "László Filep\n\n"}
{"id": "14753970", "url": "https://en.wikipedia.org/wiki?curid=14753970", "title": "Multiplicative partition", "text": "Multiplicative partition\n\nIn number theory, a multiplicative partition or unordered factorization of an integer \"n\" that is greater than 1 is a way of writing \"n\" as a product of integers greater than 1, treating two products as equivalent if they differ only in the ordering of the factors. The number \"n\" is itself considered one of these products. Multiplicative partitions closely parallel the study of multipartite partitions, discussed in , which are additive partitions of finite sequences of positive integers, with the addition made pointwise. Although the study of multiplicative partitions has been ongoing since at least 1923, the name \"multiplicative partition\" appears to have been introduced by . The Latin name \"factorisatio numerorum\" had been used previously. MathWorld uses the term unordered factorization.\n\n\n describe an application of multiplicative partitions in classifying integers with a given number of divisors. For example, the integers with exactly 12 divisors take the forms \"p\", \"p\"×\"q\", \"p\"×\"q\", and \"p\"×\"q\"×\"r\", where \"p\", \"q\", and \"r\" are distinct prime numbers; these forms correspond to the multiplicative partitions 12, 2×6, 3×4, and 2×2×3 respectively. More generally, for each multiplicative partition\nof the integer \"k\", there corresponds a class of integers having exactly \"k\" divisors, of the form\nwhere each \"p\" is a distinct prime. This correspondence follows from the multiplicative property of the divisor function.\n\n credits with the problem of counting the number of multiplicative partitions of \"n\"; this problem has since been studied by other others under the Latin name of \"factorisatio numerorum\". If the number of multiplicative partitions of \"n\" is \"a\", McMahon and Oppenheim observed that its Dirichlet series generating function \"f\"(\"s\") has the product representation\n\nThe sequence of numbers \"a\" begins\n\nOppenheim also claimed an upper bound on \"a\", of the form\nbut as showed, this bound is erroneous and the true bound is\n\nBoth of these bounds are not far from linear in \"n\": they are of the form \"n\".\nHowever, the typical value of \"a\" is much smaller: the average value of \"a\", averaged over an interval \"x\" ≤ \"n\" ≤ \"x\"+\"N\", is\na bound that is of the form \"n\" .\n\n observe, and prove, that most numbers cannot arise as the number \"a\" of multiplicative partitions of some \"n\": the number of values less than \"N\" which arise in this way is \"N\". Additionally, show that most values of \"n\" are not multiples of \"a\": the number of values \"n\" ≤ \"N\" such that \"a\" divides \"n\" is O(\"N\" / log \"N\").\n\n\n"}
{"id": "36202984", "url": "https://en.wikipedia.org/wiki?curid=36202984", "title": "Non-wellfounded mereology", "text": "Non-wellfounded mereology\n\nIn philosophy, specifically metaphysics, mereology is the study of parthood relationships. In mathematics and formal logic, wellfoundedness prohibits formula_1 for any \"x\".\n\nThus non-wellfounded mereology treats topologically circular, cyclical, repetitive, or other eventual self-containment.\n\nMore formally, non-wellfounded partial orders may exhibit formula_1 for some \"x\" whereas well-founded orders prohibit that.\n\n"}
{"id": "52992235", "url": "https://en.wikipedia.org/wiki?curid=52992235", "title": "Numberblocks", "text": "Numberblocks\n\nNumberblocks is a Bafta nominated British children's CGI-animated TV series. It debuted on CBeebies on 23 January 2017. It is produced by Blue-Zoo Productions and Alphablocks, the studios that also produce \"Alphablocks\", which is related and created by the same companies. The show teaches children how to count and do simple arithmetic.\n\n\"Numberblocks\" follows the adventures of anthropomorphic block characters in Number Land, with the number of blocks determining which numeral they stand for and a tiny black floating number above them to show how many blocks they are made of, which they call a Numberling. When one of the blocks hops on top of another, they transform into a different character to make a new number. The show helps toddlers and young kids learn numeracy skills, especially how to count and do simple math.\n\nThree,Four,Five,Six!\n\n\n\n"}
{"id": "2864280", "url": "https://en.wikipedia.org/wiki?curid=2864280", "title": "Oblique reflection", "text": "Oblique reflection\n\nIn Euclidean geometry, oblique reflections generalize ordinary reflections by not requiring that reflection be done using perpendiculars. If two points are oblique reflections of each other, they will still stay so under affine transformations. \n\nConsider a plane \"P\" in the three-dimensional Euclidean space. The usual reflection of a point \"A\" in space in respect to the plane \"P\" is another point \"B\" in space, such that the midpoint of the segment \"AB\" is in the plane, and \"AB\" is perpendicular to the plane. For an \"oblique reflection\", one requires instead of perpendicularity that \"AB\" be parallel to a given reference line.\n\nFormally, let there be a plane \"P\" in the three-dimensional space, and a line \"L\" in space not parallel to \"P\". To obtain the oblique reflection of a point \"A\" in space in respect to the plane \"P\", one draws through \"A\" a line parallel to \"L\", and lets the oblique reflection of \"A\" be the point \"B\" on that line on the other side of the plane such that the midpoint of \"AB\" is in \"P\". If the reference line \"L\" is perpendicular to the plane, one obtains the usual reflection. \n\nFor example, consider the plane \"P\" to be the \"xy\" plane, that is, the plane given by the equation \"z\"=0 in Cartesian coordinates. Let the direction of the reference line \"L\" be given by the vector (\"a\", \"b\", \"c\"), with \"c\"≠0 (that is, \"L\" is not parallel to \"P\"). The oblique reflection of a point (\"x\", \"y\", \"z\") will then be\n\nThe concept of oblique reflection is easily generalizable to oblique reflection in respect to an affine hyperplane in R with a line again serving as a reference, or even more generally, oblique reflection in respect to a \"k\"-dimensional affine subspace, with a \"n\"−\"k\"-dimensional affine subspace serving as a reference. Back to three dimensions, one can then define oblique reflection in respect to a line, with a plane serving as a reference. \n\nAn oblique reflection is an affine transformation, and it is an involution, meaning that the reflection of the reflection of a point is the point itself.\n"}
{"id": "7006917", "url": "https://en.wikipedia.org/wiki?curid=7006917", "title": "Prefix order", "text": "Prefix order\n\nIn mathematics, especially order theory, a prefix ordered set generalizes the intuitive concept of a tree by introducing the possibility of continuous progress and continuous branching. Natural prefix orders often occur when considering dynamical systems as a set of functions from \"time\" (a totally ordered set) to some phase space. In this case, the elements of the set are usually referred to as \"executions\" of the system.\n\nThe name \"prefix order\" stems from the prefix order on words, which is a special kind of substring relation and, because of its discrete character, a tree.\n\nA prefix order is a binary relation \"≤\" over a set \"P\" which is antisymmetric, transitive, reflexive, and downward total, i.e., for all \"a\", \"b\", and \"c\" in \"P\", we have that:\n\n\nWhile between partial orders it is usual to consider order-preserving functions, the most important type of functions between prefix orders are so-called history preserving functions. Given a prefix ordered set \"P\", a history of a point \"p∈P\" is the (by definition totally ordered) set \"p- ≜ {q | q ≤ p}\". A function \"f : P → Q\" between prefix orders P and Q is then history preserving if and only if for every \"p∈P\" we find \"f(p-) = f(p)-\". Similarly, a future of a point \"p∈P\" is the (prefix ordered) set \"p+ ≜ {q | p ≤ q}\" and \"f\" is future preserving if for all \"p∈P\" we find \"f(p+) = f(p)+\".\n\nEvery history preserving function and every future preserving function is also order preserving, but not vice versa.\nIn the theory of dynamical systems, history preserving maps capture the intuition that the behavior in one system is a \"refinement\" of the behavior in another. Furthermore, functions that are history and future preserving surjections capture the notion of bisimulation between systems, and thus the intuition that a given refinement is \"correct\" with respect to a specification.\n\nThe range of a history preserving function is always a prefix closed subset, where a subset \"S ⊆ P\" is prefix closed if for all \"s,t ∈ P\" with \"t∈S\" and \"s≤t\" we find \"s∈S\".\n\nTaking history preserving maps as \"morphisms\" in the category of prefix orders leads to a notion of product that is \"not\" the Cartesian product of the two orders since the Cartesian product is not always a prefix order. Instead, it leads to an \"arbitrary interleaving\" of the original prefix orders. The union of two prefix orders is the disjoint union, as it is with partial orders.\n\nAny bijective history preserving function is an order isomorphism. Furthermore, if for a given prefix ordered set \"P\" we construct the set \"P- ≜ { p- | p∈ P}\" we find that this set is prefix ordered by the subset relation ⊆, and furthermore, that the function \"max : P- → P\" is an isomorphism, where \"max(S)\" returns for each set \"S∈P-\" the maximum element in terms of the order on P (i.e. \"max(p-) ≜ p\").\n\n"}
{"id": "5007518", "url": "https://en.wikipedia.org/wiki?curid=5007518", "title": "Prolate spheroidal coordinates", "text": "Prolate spheroidal coordinates\n\nProlate spheroidal coordinates are a three-dimensional orthogonal coordinate system that results from rotating the two-dimensional elliptic coordinate system about the focal axis of the ellipse, i.e., the symmetry axis on which the foci are located. Rotation about the other axis produces oblate spheroidal coordinates. Prolate spheroidal coordinates can also be considered as a limiting case of ellipsoidal coordinates in which the two smallest principal axes are equal in length.\n\nProlate spheroidal coordinates can be used to solve various partial differential equations in which the boundary conditions match its symmetry and shape, such as solving for a field produced by two centers, which are taken as the foci on the \"z\"-axis. One example is solving for the wavefunction of an electron moving in the electromagnetic field of two positively charged nuclei, as in the hydrogen molecular ion, H. Another example is solving for the electric field generated by two small electrode tips. Other limiting cases include areas generated by a line segment (\"μ\" = 0) or a line with a missing segment (ν=0).\n\nThe most common definition of prolate spheroidal coordinates formula_1 is\n\nwhere formula_5 is a nonnegative real number and formula_6. The azimuthal angle formula_7 belongs to the interval formula_8.\n\nThe trigonometric identity\n\nshows that surfaces of constant formula_5 form prolate spheroids, since they are ellipses rotated about the axis \njoining their foci. Similarly, the hyperbolic trigonometric identity\n\nshows that surfaces of constant formula_12 form \nhyperboloids of revolution.\n\nThe distances from the foci located at formula_13 are\n\nThe scale factors for the elliptic coordinates formula_15 are equal\n\nwhereas the azimuthal scale factor equals\n\nConsequently, an infinitesimal volume element equals\n\nand the Laplacian can be written\n\nOther differential operators such as formula_20 and formula_21 can be expressed in the coordinates formula_1 by substituting the scale factors into the general formulae found in orthogonal coordinates.\n\nAn alternative and geometrically intuitive set of prolate spheroidal coordinates formula_23 are sometimes used, \nwhere formula_24 and formula_25. Hence, the curves of constant formula_26 are prolate spheroids, whereas the curves of constant formula_27 are hyperboloids of revolution. The coordinate formula_27 belongs to the interval [−1, 1], whereas the formula_26 coordinate must be greater than or equal to one.\nThe coordinates formula_26 and formula_27 have a simple relation to the distances to the foci formula_32 and formula_33. For any point in the plane, the \"sum\" formula_34 of its distances to the foci equals formula_35, whereas their \"difference\" formula_36 equals formula_37. Thus, the distance to formula_32 is formula_39, whereas the distance to formula_33 is formula_41. (Recall that formula_32 and formula_33 are located at formula_44 and formula_45, respectively.) This gives the following expressions for formula_26, formula_27, and formula_7:\n\nUnlike the analogous oblate spheroidal coordinates, the prolate spheroid coordinates (σ, τ, φ) are \"not\" degenerate; in other words, there is a unique, reversible correspondence between them and the Cartesian coordinates\n\nThe scale factors for the alternative elliptic coordinates formula_55 are\n\nwhile the azimuthal scale factor is now\n\nHence, the infinitesimal volume element becomes\n\nand the Laplacian equals\n\nOther differential operators such as formula_20 and formula_21 can be expressed in the coordinates formula_63 by substituting the scale factors into the general formulae found in orthogonal coordinates.\n\nAs is the case with spherical coordinates, Laplace's equation may be solved by the method of separation of variables to yield solutions in the form of prolate spheroidal harmonics, which are convenient to use when boundary conditions are defined on a surface with a constant prolate spheroidal coordinate (See Smythe, 1968).\n\n\n\n\n"}
{"id": "10784887", "url": "https://en.wikipedia.org/wiki?curid=10784887", "title": "Quantum cellular automaton", "text": "Quantum cellular automaton\n\nA quantum cellular automaton (QCA) is an abstract model of quantum computation, devised in analogy to conventional models of cellular automata introduced by von Neumann. The same name may also refer to quantum dot cellular automata, which are a proposed physical implementation of \"classical\" cellular automata by exploiting quantum mechanical phenomena. QCA have attracted a lot of attention as a result of its extremely small feature size (at the molecular or even atomic scale) and its ultra-low power consumption, making it one candidate for replacing CMOS technology.\n\nIn the context of models of computation or of physical systems, \"quantum cellular automaton\" refers to the merger of elements of both (1) the study of cellular automata in conventional computer science and (2) the study of quantum information processing. In particular, the following are features of models of quantum cellular automata:\nAnother feature that is often considered important for a model of quantum cellular automata is that it should be universal for quantum computation (i.e. that it can efficiently simulate quantum Turing machines, some arbitrary quantum circuit or simply all other quantum cellular automata).\n\nModels which have been proposed recently impose further conditions, e.g. that quantum cellular automata should be reversible and/or locally unitary, and have an easily determined global transition function from the rule for updating individual cells. Recent results show that these properties can be derived axiomatically, from the symmetries of the global evolution.\n\nIn 1982, Richard Feynman suggested an initial approach to quantizing a model of cellular automata. In 1985, David Deutsch presented a formal development of the subject. Later, Gerhard Grössing and Anton Zeilinger introduced the term \"quantum cellular automata\" to refer to a model they defined in 1988, although their model had very little in common with the concepts developed by Deutsch and so has not been developed significantly as a model of computation.\n\nThe first formal model of quantum cellular automata to be researched in depth was that introduced by John Watrous. This model was developed further by Wim van Dam, as well as Christoph Dürr, Huong LêThanh, and Miklos Santha, Jozef Gruska. and Pablo Arrighi. However it was later realised that this definition was too loose, in the sense that some instances of it allow superluminal signalling. A second wave of models includes those of Susanne Richter and Reinhard Werner, of Benjamin Schumacher and Reinhard Werner, of Carlos Pérez-Delgado and Donny Cheung, and of Pablo Arrighi, Vincent Nesme and Reinhard Werner. These are all closely related, and do not suffer any such locality issue. In the end one can say that they all agree to picture quantum cellular automata as just some large quantum circuit, infinitely repeating across time and space.\n\nModels of quantum cellular automata have been proposed by David Meyer, Bruce Boghosian and Washington Taylor, and Peter Love and Bruce Boghosian as a means of simulating quantum lattice gases, motivated by the use of \"classical\" cellular automata to model classical physical phenomena such as gas dispersion. Criteria determining when a quantum cellular automaton (QCA) can be described as quantum lattice gas automaton (QLGA) were given by Asif Shakeel and Peter Love.\n\nA proposal for implementing \"classical\" cellular automata by systems designed with quantum dots has been proposed under the name \"quantum cellular automata\" by Doug Tougaw and Craig Lent, as a replacement for classical computation using CMOS technology. In order to better differentiate between this proposal and models of cellular automata which perform quantum computation, many authors working on this subject now refer to this as a quantum dot cellular automaton.\n\n"}
{"id": "41138014", "url": "https://en.wikipedia.org/wiki?curid=41138014", "title": "Radiation law for human mobility", "text": "Radiation law for human mobility\n\nThe radiation law is way of modeling human mobility (geographic mobility, human migration) and it gives better empirical predictions than the gravity model of migration which is widely used in this subject.\n\nWaves of migration due to displacement by war, or exploitation in the hope of geographical discoveries could be observed in the past, however with new technological advancements in transportation keep making it easier and cheaper to get to one place from another. With intercontinental flights we even can travel to another continent, on a business trip for instance, and come back within a few hours. Not only time but road networks and flight networks are being used more and more intensively also, and there is an increasing need to describe the patterns of human peoples' mobility and their effect on network usage, whether the network is a transportation, communication or some other type of network.\n\nRadiation models appeared first in physics to study the process of energetic particles or waves travel through vacuum. The model in the social science describes the flows of people between different locations. Daily commuting is the major part of the flows, so modeling job seeking has to be an important part of the model and so it is in the radiation model. People look for jobs in every county starting with their own home county. The number of open jobs formula_1 depends on the size of the resident population formula_2. The potential employment opportunity (e.g. conditions, income, working hour, etc.) is formula_3 with the distribution of formula_4. Then, for each county formula_5 job opportunities are assigned, which are random draws from the formula_4 distribution. \nIndividuals then chooses the job which is closest to their home county and provides the highest formula_3. Thus, they take into account the proximity to their home county and the benefits it can provide. This optimization gives the migration flows (called commuting fluxes) between counties across the country. This is analogous to the model in physics that describes the radiation and absorption process, that's why it's called the radiation model. An important feature of the model is that the average flux between two counties does not depend on the benefit distribution, the number of job opportunities and the total number of commuters. The fundamental equation of the radiation model gives the average flux between two counties,\n\nwhere formula_9 is the total number of commuters from county formula_10, formula_11 and formula_12 are the population in county formula_10 and formula_14 respectively, and formula_15 is the total population in the circle centered at formula_10 and touching formula_14 excluding the source and the destination population. The model is not static as the Gravity model, and has clear implications which can be empirically verified.\n\nThe population density around Utah is much lower than around Alabama and so are the job opportunities, given that the population of the two states is the same. Thus, the fundamental equation implies that people from Utah have to travel further to find suitable jobs on average than people from Alabama, and indeed, this is what the data shows. The Gravity model gives bad predictions both on short and long distance commuting, while the prediction of the Radiation model is close to the census data. Further empirical testing shows that the Radiation model underestimates the flow in case of big cities, but generalizing the fundamental equation the model can give at least as good predictions as the Gravity model.\n\nIn 1971 famed economist William Alonso produced a working paper that describes a mathematical model of human mobility. In that manuscript Alonso remarks: \"It is almost as if an urban area were a radioactive body, emitting particles at a steady rate[.]\" In addition to many of the same mathematical terms used by Simini et al., Alonso's radiation model includes measures of climate (degree days) and wealth (per capita income) for both the emitting and receiving locales, but only includes the distance between these urban areas as opposed to a radial measure of intervening population density.\n\nThe most influential model to describe trade patterns, and in a similar way, describe human mobility is the gravity model of trade. The model predicts, that the migration flow is proportional to the population of the cities/countries, and it is reciprocal in a quadratic order in the distance between them. Although, it is an intuitive description of the flows, and it is used to describe gravitational forces in physics, in terms of migration it does not perform well empirically. Moreover, the model just simply assumes the given functional form without any theoretical background.\n"}
{"id": "632374", "url": "https://en.wikipedia.org/wiki?curid=632374", "title": "Reinsurance", "text": "Reinsurance\n\nReinsurance is insurance that is purchased by an insurance company. In the classic case, reinsurance allows insurance companies to remain solvent after major claims events, such as major disasters like hurricanes and wildfires. In addition to its basic role in risk management, reinsurance is sometimes used for tax mitigation and other reasons. The company that purchases the reinsurance policy is called a \"ceding company\" or \"cedent\" or \"cedant\" under most arrangements. The company issuing the reinsurance policy is referred simply as the \"reinsurer\".\n\nA company that purchases reinsurance pays a premium to the reinsurance company, who in exchange would pay a share of the claims incurred by the purchasing company. The reinsurer may be either a specialist reinsurance company, which only undertakes reinsurance business, or another insurance company. Insurance companies that sell reinsurance refer to the business as 'assumed reinsurance'.\n\nThere are two basic methods of reinsurance:\n\nThere are two main types of treaty reinsurance, proportional and non-proportional, which are detailed below. Under proportional reinsurance, the reinsurer's share of the risk is defined for each separate policy, while under non-proportional reinsurance the reinsurer's liability is based on the aggregate claims incurred by the ceding office. In the past 30 years there has been a major shift from proportional to non-proportional reinsurance in the property and casualty fields.\n\nAlmost all insurance companies have a reinsurance program. The ultimate goal of that program is to reduce their exposure to loss by passing part of the risk of loss to a reinsurer or a group of reinsurers.\n\nWith reinsurance, the \"insurer\" can issue policies with higher limits than would otherwise be allowed, thus being able to take on more risk because some of that risk is now transferred to the re-insurer.\n\nReinsurance can make an insurance company's results more predictable by absorbing larger losses and reducing the amount of capital needed to provide coverage. The risks are diversified, with the reinsurer bearing some of the loss incurred by the insurance company. The income smoothing comes forward as the losses of the cedant are essentially limited. This fosters stability in claim payouts and caps indemnification costs.\n\nProportional Treaties (or “pro-rata” treaties) provide the cedent with “surplus relief”; surplus relief being the capacity to write more business and/or at larger limits.\n\nThe insurance company may be motivated by arbitrage in purchasing reinsurance coverage at a lower rate than they charge the insured for the underlying risk, whatever the class of insurance.\n\nIn general, the reinsurer may be able to cover the risk at a lower premium than the insurer because:\n\nThe insurance company may want to avail itself of the expertise of a \"reinsurer\", or the reinsurer's ability to set an appropriate premium, in regard to a specific (specialised) risk. The reinsurer will also wish to apply this expertise to the underwriting in order to protect their own interests.\n\nBy choosing a particular type of reinsurance method, the insurance company may be able to create a more balanced and homogeneous portfolio of insured risks. This would lend greater predictability to the portfolio results on net basis (after reinsurance) and would be reflected in income smoothing. While income smoothing is one of the objectives of reinsurance arrangements, the mechanism is by way of balancing the portfolio.\n\nUnder proportional reinsurance, one or more reinsurers take a stated percentage share of each policy that an insurer issues (\"writes\"). The reinsurer will then receive that stated percentage of the premiums and will pay the stated percentage of claims. In addition, the reinsurer will allow a \"ceding commission\" to the insurer to cover the costs incurred by the insurer (mainly acquisition and administration).\n\nThe arrangement may be \"quota share\" or \"surplus reinsurance\" (also known as surplus of line or variable quota share treaty) or a combination of the two. Under a quota share arrangement, a fixed percentage (say 75%) of each insurance policy is reinsured. Under a surplus share arrangement, the ceding company decides on a \"retention limit\" - say $100,000. The ceding company retains the full amount of each risk, with a maximum of $100,000 per policy or per risk, and the balance of the risk is reinsured.\n\nThe ceding company may seek a quota share arrangement for several reasons. First, it may not have sufficient capital to prudently retain all of the business that it can sell. For example, it may only be able to offer a total of $100 million in coverage, but by reinsuring 75% of it, it can sell four times as much.\n\nThe ceding company may seek surplus reinsurance to limit the losses it might incur from a small number of large claims as a result of random fluctuations in experience. In a 9 line surplus treaty the reinsurer would then accept up to $900,000 (9 lines). So if the insurance company issues a policy for $100,000, they would keep all of the premiums and losses from that policy. If they issue a $200,000 policy, they would give (cede) half of the premiums and losses to the \"reinsurer\" (1 line each). The maximum automatic underwriting capacity of the cedant would be $1,000,000 in this example. Any policy larger than this would require facultative reinsurance.\n\nUnder non-proportional reinsurance the reinsurer only pays out if the total claims suffered by the insurer in a given period exceed a stated amount, which is called the \"retention\" or \"priority\". For instance the insurer may be prepared to accept a total loss up to $1 million, and purchases a layer of reinsurance of $4 million in excess of this $1 million. If a loss of $3 million were then to occur, the insurer would bear $1 million of the loss and would recover $2 million from its reinsurer. In this example, the insurer also retains any excess of loss over $5 million unless it has purchased a further excess layer of reinsurance.\n\nThe main forms of non-proportional reinsurance are excess of loss and stop loss.\n\nExcess of loss reinsurance can have three forms - \"Per Risk XL\" (Working XL), \"Per Occurrence or Per Event XL\" (Catastrophe or Cat XL), and \"Aggregate XL\".\n\nIn per risk, the cedant's insurance policy limits are greater than the reinsurance retention. For example, an insurance company might insure commercial property risks with policy limits up to $10 million, and then buy per risk reinsurance of $5 million in excess of $5 million. In this case a loss of $6 million on that policy will result in the recovery of $1 million from the reinsurer. These contracts usually contain event limits to prevent their misuse as a substitute for Catastrophe XLs.\n\nIn catastrophe excess of loss, the cedant's retention is usually a multiple of the underlying policy limits, and the reinsurance contract usually contains a two risk warranty (i.e. they are designed to protect the cedant against catastrophic events that involve more than one policy, usually very many policies). For example, an insurance company issues homeowners' policies with limits of up to $500,000 and then buys catastrophe reinsurance of $22,000,000 in excess of $3,000,000. In that case, the insurance company would only recover from reinsurers in the event of multiple policy losses in one event (e.g., hurricane, earthquake, flood).\n\nAggregate XL affords a frequency protection to the reinsured. For instance if the company retains $1 million net any one vessel, $5 million annual aggregate limit in excess of $5m annual aggregate deductible, the cover would equate to 5 total losses (or more partial losses) in excess of 5 total losses (or more partial losses). Aggregate covers can also be linked to the cedant's gross premium income during a 12-month period, with limit and deductible expressed as percentages and amounts. Such covers are then known as \"stop loss\" contracts.\n\nA basis under which reinsurance is provided for claims arising from policies commencing during the period to which the reinsurance\nrelates. The \"insurer\" knows there is coverage during the whole policy period even if claims are only discovered or made later on.\n\nAll claims from cedant underlying policies incepting during the period of the reinsurance contract are covered even if they occur after the expiration date of the reinsurance contract. Any claims from cedant underlying policies incepting outside the period of the reinsurance contract are not covered even if they occur during the period of the reinsurance contract.\n\nA Reinsurance treaty under which all claims occurring during the period of the contract, irrespective of when the underlying policies incepted, are covered. Any losses occurring after the contract expiration date are not covered.\n\nAs opposed to claims-made or risks attaching contracts. Insurance coverage is provided for losses occurring in the defined period. This is the usual basis of cover for short tail business.\n\nA policy which covers all claims reported to an \"insurer\" within the policy period irrespective of when they occurred.\n\nMost of the above examples concern reinsurance contracts that cover more than one policy (treaty). Reinsurance can also be purchased on a per policy basis, in which case it is known as facultative reinsurance. Facultative reinsurance can be written on either a quota share or excess of loss basis. Facultative reinsurance contracts are commonly memorialized in relatively brief contracts known as facultative certificates and often are used for large or unusual risks that do not fit within standard reinsurance treaties due to their exclusions. The term of a facultative agreement coincides with the term of the policy. Facultative reinsurance is usually purchased by the insurance underwriter who underwrote the original insurance policy, whereas treaty reinsurance is typically purchased by a senior executive at the insurance company.\n\nReinsurance treaties can either be written on a \"continuous\" or \"term\" basis. A continuous contract has no predetermined end date, but generally either party can give 90 days notice to cancel or amend the treaty. A term agreement has a built-in expiration date. It is common for insurers and reinsurers to have long term relationships that span many years. Reinsurance treaties are typically longer documents than facultative certificates, containing many of their own terms that are distinct from the terms of the direct insurance policies that they reinsure. However, even most reinsurance treaties are relatively short documents considering the number and variety of risks and lines of business that the treaties reinsure and the dollars involved in the transactions. There are not \"standard\" reinsurance contracts. However, many reinsurance contracts do include some commonly used provisions and provisions imbued with considerable industry common and practice.\n\nSometimes insurance companies wish to offer insurance in jurisdictions where they are not licensed: for example, an insurer may wish to offer an insurance programme to a multinational company, to cover property and liability risks in many countries around the world. In such situations, the insurance company may find a local insurance company which is authorised in the relevant country, arrange for the local insurer to issue an insurance policy covering the risks in that country, and enter into a reinsurance contract with the local insurer to transfer the risks. In the event of a loss, the policyholder would claim against the local insurer under the local insurance policy, the local insurer would pay the claim and would claim reimbursement under the reinsurance contract. Such an arrangement is called \"fronting\". Fronting is also sometimes used where an insurance buyer requires its insurers to have a certain financial strength rating and the prospective insurer does not satisfy that requirement: the prospective insurer may be able to persuade another insurer, with the requisite credit rating, to provide the coverage to the insurance buyer, and to take out reinsurance in respect of the risk. An insurer which acts as a \"fronting insurer\" receives a fronting fee for this service to cover administration and the potential default of the reinsurer. The fronting insurer is taking a risk in such transactions, because it has an obligation to pay its insurance claims even if the reinsurer becomes insolvent and fails to reimburse the claims.\n\nMany reinsurance placements are not placed with a single reinsurer but are shared between a number of reinsurers. For example, a $30,000,000 excess of $20,000,000 layer may be shared by 30 or more reinsurers. The reinsurer who sets the terms (premium and contract conditions) for the reinsurance contract is called the lead reinsurer; the other companies subscribing to the contract are called following reinsurers. Alternatively, one reinsurer can accept the whole of the reinsurance and then retrocede it (pass it on in a further reinsurance arrangement) to other companies.\n\nUsing game-theoretic modeling, Professors Michael R. Powers (Temple University) and Martin Shubik (Yale University) have argued that the number of active reinsurers in a given national market should be approximately equal to the square-root of the number of primary insurers active in the same market. Econometric analysis has provided empirical support for the Powers-Shubik rule.\n\nCeding companies often choose their reinsurers with great care as they are exchanging insurance risk for credit risk. Risk managers monitor reinsurers' financial ratings (S&P, A.M. Best, etc.) and aggregated exposures regularly.\n\nBecause of the governance effect insurance/cedent companies can have on society, re insurers can indirectly have societal impact as well, due to reinsurer underwriting and claims philosophies imposed on those underlying carriers which affects how the cedents offer coverage in the market. However, reinsurer governance is voluntarily accepted by cedents via contract to allow cedents the opportunity to rent reinsurer capital to expand cedent market share or limit their risk.\n\n"}
{"id": "14272194", "url": "https://en.wikipedia.org/wiki?curid=14272194", "title": "Saint-Venant's theorem", "text": "Saint-Venant's theorem\n\nIn solid mechanics, it is common to analyze the properties of beams with constant cross section. Saint-Venant's theorem states that the simply connected cross section with maximal torsional rigidity is a circle. It is named after the French mathematician Adhémar Jean Claude Barré de Saint-Venant.\n\nGiven a simply connected domain \"D\" in the plane with area \"A\", formula_1 the radius and formula_2 the area of its greatest inscribed circle, the torsional rigidity \"P\" \nof \"D\" is defined by\n\nHere the supremum is taken over all the continuously differentiable functions vanishing on the boundary of \"D\". The existence of this supremum is a consequence of Poincaré inequality.\n\nSaint-Venant conjectured in 1856 that\nof all domains \"D\" of equal area \"A\" the circular one has the greatest torsional rigidity, that is\n\nA rigorous proof of this inequality was not given until 1948 by Pólya. Another proof was given by Davenport and reported in. A more general proof and an estimate \n\nis given by Makai.\n"}
{"id": "47439851", "url": "https://en.wikipedia.org/wiki?curid=47439851", "title": "Selberg's identity", "text": "Selberg's identity\n\nIn number theory, Selberg's identity is an approximate identity involving logarithms of primes found by . Selberg and Erdős both used this identity to give elementary proofs of the prime number theorem.\n\nThere are several different but equivalent forms of Selberg's identity. One form is\nwhere the sums are over primes \"p\" and \"q\".\n\nThe strange-looking expression on the left side of Selberg's identity is (up to smaller terms) the sum \nwhere the numbers \nare the coefficients of the Dirichlet series\n\nThis function has a pole of order 2 at \"s\"=1 with coefficient 2, which gives the dominant term 2\"x\" log(\"x\") in the asymptotic expansion of formula_2.\n\nSelberg's identity sometimes also refers to the following divisor sum identity involving the von Mangoldt function and the Möbius function when formula_6:\n\nThis variant of Selberg's identity is proved using the concept of taking derivatives of arithmetic functions defined by formula_8 in Section 2.18 of Apostol's book (see also this link).\n\n"}
{"id": "3989092", "url": "https://en.wikipedia.org/wiki?curid=3989092", "title": "Siegel's theorem on integral points", "text": "Siegel's theorem on integral points\n\nIn mathematics, Siegel's theorem on integral points is the 1929 result of Carl Ludwig Siegel, that for a smooth algebraic curve \"C\" of genus \"g\" defined over a number field \"K\", presented in affine space in a given coordinate system, there are only finitely many points on \"C\" with coordinates in the ring of integers \"O\" of \"K\", provided \"g\" > 0. This result covers the Mordell curve, for example.\n\nThis was proved by combining a version of the Thue–Siegel–Roth theorem, from diophantine approximation, with the Mordell–Weil theorem from diophantine geometry (required in Weil's version, to apply to the Jacobian variety of \"C\"). It was the first major result on diophantine equations that depended only on the genus, not any special algebraic form of the equations. For \"g\" > 1 it was in the end superseded by Faltings's theorem. \n\nSiegel's result was ineffective (see effective results in number theory), since Thue's method in diophantine approximation also is ineffective in describing possible very good rational approximations to algebraic numbers. Effective results in some cases derive from Baker's method.\n\n"}
{"id": "6928351", "url": "https://en.wikipedia.org/wiki?curid=6928351", "title": "Sigma-ring", "text": "Sigma-ring\n\nIn mathematics, a nonempty collection of sets is called a σ-ring (pronounced \"sigma-ring\") if it is closed under countable union and relative complementation.\n\nLet formula_1 be a nonempty collection of sets. Then formula_1 is a σ-ring if:\n\nFrom these two properties we immediately see that\n\nThis is simply because formula_11.\n\nIf the first property is weakened to closure under finite union (i.e., formula_12 whenever formula_7) but not countable union, then formula_1 is a ring but not a σ-ring.\n\nσ-rings can be used instead of σ-fields (σ-algebras) in the development of measure and integration theory, if one does not wish to require that the universal set be measurable. Every σ-field is also a σ-ring, but a σ-ring need not be a σ-field.\n\nA σ-ring formula_1 that is a collection of subsets of formula_16 induces a σ-field for formula_16. Define formula_18 to be the collection of all subsets of formula_16 that are elements of formula_1 or whose complements are elements of formula_1. Then formula_18 is a σ-field over the set formula_16. In fact formula_18 is the minimal σ-field containing formula_1 since it must be contained in every σ-field containing formula_1.\n\n\n"}
{"id": "7788156", "url": "https://en.wikipedia.org/wiki?curid=7788156", "title": "Signature (logic)", "text": "Signature (logic)\n\nIn logic, especially mathematical logic, a signature lists and describes the non-logical symbols of a formal language. In universal algebra, a signature lists the operations that characterize an algebraic structure. In model theory, signatures are used for both purposes.\n\nSignatures play the same role in mathematics as type signatures in computer programming. They are rarely made explicit in more philosophical treatments of logic.\n\nFormally, a (single-sorted) signature can be defined as a triple σ = (\"S\", \"S\", ar), where \"S\" and \"S\" are disjoint sets not containing any other basic logical symbols, called respectively\nand a function ar: \"S\" formula_1 \"S\" → formula_2 which assigns a natural number called \"arity\" to every function or relation symbol. A function or relation symbol is called \"n\"-ary if its arity is \"n\". A nullary (\"0\"-ary) function symbol is called a \"constant symbol\".\nA signature with no function symbols is called a relational signature, and a signature with no relation symbols is called an algebraic signature. A finite signature is a signature such that \"S\" and \"S\" are finite. More generally, the cardinality of a signature σ = (\"S\", \"S\", ar) is defined as |σ| = |\"S\"| + |\"S\"|.\n\nThe language of a signature is the set of all well formed sentences built from the symbols in that signature together with the symbols in the logical system.\n\nIn universal algebra the word type or similarity type is often used as a synonym for \"signature\". In model theory, a signature σ is often called a vocabulary, or identified with the (first-order) language \"L\" to which it provides the non-logical symbols. However, the cardinality of the language \"L\" will always be infinite; if σ is finite then |L| will be ℵ.\n\nAs the formal definition is inconvenient for everyday use, the definition of a specific signature is often abbreviated in an informal way, as in:\n\nSometimes an algebraic signature is regarded as just a list of arities, as in:\n\nFormally this would define the function symbols of the signature as something like \"f\" (nullary), \"f\" (unary) and \"f\" (binary), but in reality the usual names are used even in connection with this convention.\n\nIn mathematical logic, very often symbols are not allowed to be nullary, so that constant symbols must be treated separately rather than as nullary function symbols. They form a set \"S\" disjoint from \"S\", on which the arity function \"ar\" is not defined. However, this only serves to complicate matters, especially in proofs by induction over the structure of a formula, where an additional case must be considered. Any nullary relation symbol, which is also not allowed under such a definition, can be emulated by a unary relation symbol together with a sentence expressing that its value is the same for all elements. This translation fails only for empty structures (which are often excluded by convention). If nullary symbols are allowed, then every formula of propositional logic is also a formula of first-order logic.\n\nIn the context of first-order logic, the symbols in a signature are also known as the non-logical symbols, because together with the logical symbols they form the underlying alphabet over which two formal languages are inductively defined: The set of \"terms\" over the signature and the set of (well-formed) \"formulas\" over the signature.\n\nIn a structure, an \"interpretation\" ties the function and relation symbols to mathematical objects that justify their names: The interpretation of an \"n\"-ary function symbol \"f\" in a structure A with \"domain\" \"A\" is a function \"f\": \"A\" → \"A\", and the interpretation of an \"n\"-ary relation symbol is a relation \"R\" ⊆ \"A\". Here \"A\" = \"A\" × \"A\" × ... × \"A\" denotes the \"n\"-fold cartesian product of the domain \"A\" with itself, and so \"f\" is in fact an \"n\"-ary function, and \"R\" an \"n\"-ary relation.\n\nFor many-sorted logic and for many-sorted structures signatures must encode information about the sorts. The most straightforward way of doing this is via symbol types that play the role of generalized arities.\n\nLet \"S\" be a set (of sorts) not containing the symbols × or →.\n\nThe symbol types over \"S\" are certain words over the alphabet \"S\" formula_1 {×, →}: the relational symbol types \"s\" × … × \"s\", and the functional symbol types \"s\" × … × \"s\"→\"s<nowiki>'</nowiki>\", for non-negative integers \"n\" and \"s\",\"s\",…,\"s\",\"s<nowiki>'</nowiki>\" formula_4 \"S\". (For \"n\" = 0, the expression \"s\" × … × \"s\" denotes the empty word.)\n\nA (many-sorted) signature is a triple (\"S\", \"P\", type) consisting of\n\n\n"}
{"id": "33175921", "url": "https://en.wikipedia.org/wiki?curid=33175921", "title": "Spherically complete field", "text": "Spherically complete field\n\nIn mathematics, a field \"K\" with an absolute value is called spherically complete if the intersection of every decreasing sequence of balls (in the sense of the metric induced by the absolute value) is nonempty:\nThe definition can be adapted also to a field \"K\" with a valuation \"v\" taking values in an arbitrary ordered abelian group: (\"K\",\"v\") is spherically complete if every collection of balls that is totally ordered by inclusion has a nonempty intersection.\n\nSpherically complete fields are important in nonarchimedean functional analysis, since many results analogous to theorems of classical functional analysis require the base field to be spherically complete.\n\n"}
{"id": "21394895", "url": "https://en.wikipedia.org/wiki?curid=21394895", "title": "Tarski's high school algebra problem", "text": "Tarski's high school algebra problem\n\nIn mathematical logic, Tarski's high school algebra problem was a question posed by Alfred Tarski. It asks whether there are identities involving addition, multiplication, and exponentiation over the positive integers that cannot be proved using eleven axioms about these operations that are taught in high-school-level mathematics. The question was solved in 1980 by Alex Wilkie, who showed that such unprovable identities do exist.\n\nTarski considered the following eleven axioms about addition ('+'), multiplication ('·'), and exponentiation to be standard axioms taught in high school:\n\nThese eleven axioms, sometimes called the \"high school identities\", are related to the axioms of an exponential ring. Tarski's problem then becomes: are there identities involving only addition, multiplication, and exponentiation, that are true for all positive integers, but that cannot be proved using only the axioms 1–11?\n\nSince the axioms seem to list all the basic facts about the operations in question it is not immediately obvious that there should be anything one can state using only the three operations that is not provably true. However, proving seemingly innocuous statements can require long proofs using only the above eleven axioms. Consider the following proof that (\"x\" + 1) = \"x\" + 2 · \"x\" + 1:\n\nHere brackets are omitted when axiom 2. tells us that there is no confusion about grouping.\n\nThe length of proofs is not an issue; proofs of similar identities to that above for things like (\"x\" + \"y\") would take a lot of lines, but would really involve little more than the above proof.\n\nThe list of eleven axioms can be found explicitly written down in the works of Richard Dedekind, although they were obviously known and used by mathematicians long before then. Dedekind was the first, though, who seemed to be asking if these axioms were somehow sufficient to tell us everything we could want to know about the integers. The question was put on a firm footing as a problem in logic and model theory sometime in the 1960s by Alfred Tarski, and by the 1980s it had become known as Tarski's high school algebra problem.\n\nIn 1980 Alex Wilkie proved that not every identity in question can be proved using the axioms above. He did this by explicitly finding such an identity. By introducing new function symbols corresponding to polynomials that map positive numbers to positive numbers he proved this identity, and showed that these functions together with the eleven axioms above were both sufficient and necessary to prove it. The identity in question is\nThis identity is usually denoted \"W\"(\"x\",\"y\") and is true for all positive integers \"x\" and \"y\", as can be seen by factoring formula_2 out of the second terms; yet it cannot be proved true using the eleven high school axioms.\n\nIntuitively, the identity cannot be proved because the high school axioms can't be used to discuss the polynomial formula_3. Reasoning about that polynomial and the subterm formula_4 requires a concept of negation or subtraction, and these are not present in the high school axioms. Lacking this, it is then impossible to use the axioms to manipulate the polynomial and prove true properties about it. Wilkie's results from his paper show, in more formal language, that the \"only gap\" in the high school axioms is the inability to manipulate polynomials with negative coefficients.\n\nWilkie proved that there are statements about the positive integers that cannot be proved using the eleven axioms above and showed what extra information is needed before such statements can be proved. Using Nevanlinna theory it has also been proved that if one restricts the kinds of exponential one takes then the above eleven axioms are sufficient to prove every true statement.\n\nAnother problem stemming from Wilkie's result, which remains open, is that which asks what the smallest algebra is for which \"W\"(\"x\", \"y\") is not true but the eleven axioms above are. In 1985 an algebra with 59 elements was found that satisfied the axioms but for which \"W\"(\"x\", \"y\") was false. Smaller such algebras have since been found, and it is now known that the smallest such one must have either 11 or 12 elements.\n\n"}
{"id": "26406313", "url": "https://en.wikipedia.org/wiki?curid=26406313", "title": "Three subgroups lemma", "text": "Three subgroups lemma\n\nIn mathematics, more specifically group theory, the three subgroups lemma is a result concerning commutators. It is a consequence of the Philip Hall and Ernst Witt's eponymous identity.\n\nIn that which follows, the following notation will be employed:\n\n\nLet \"X\", \"Y\" and \"Z\" be subgroups of a group \"G\", and assume\n\nThen formula_4.\n\nMore generally, if formula_5, then if formula_6 and formula_7, then formula_8.\n\nHall–Witt identity\n\nIf formula_9, then\n\nProof of the three subgroups lemma\n\nLet formula_11, formula_12, and formula_13. Then formula_14, and by the Hall–Witt identity above, it follows that formula_15 and so formula_16. Therefore, formula_17 for all formula_13 and formula_11. Since these elements generate formula_20, we conclude that formula_21 and hence formula_4.\n\n\n[[Category:Theorems in group theory]]\n[[Category:Articles containing proofs]]\n[[Category:Lemmas]]"}
{"id": "39776727", "url": "https://en.wikipedia.org/wiki?curid=39776727", "title": "Traveling tournament problem", "text": "Traveling tournament problem\n\nThe traveling tournament problem (TTP) is an mathematical optimization problem. The question involves scheduling a series of teams such that:\n\nA matrix is provided of the travel distances between each team's home city. All teams start and end at their own home city, and the goal is to minimize the total travel distance for every team over the course of the whole season.\n\nThere have been many papers published on the subject, and a contest exists to find the best solutions for certain specific schedules.\n"}
{"id": "7185671", "url": "https://en.wikipedia.org/wiki?curid=7185671", "title": "Uniformly Cauchy sequence", "text": "Uniformly Cauchy sequence\n\nIn mathematics, a sequence of functions formula_1 from a set \"S\" to a metric space \"M\" is said to be uniformly Cauchy if:\n\n\nAnother way of saying this is that formula_7 as formula_8, where the uniform distance formula_9 between two functions is defined by\n\nA sequence of functions {\"f\"} from \"S\" to \"M\" is pointwise Cauchy if, for each \"x\" ∈ \"S\", the sequence {\"f\"(\"x\")} is a Cauchy sequence in \"M\". This is a weaker condition than being uniformly Cauchy. \n\nIn general a sequence can be pointwise Cauchy and not pointwise convergent, or it can be uniformly Cauchy and not uniformly convergent. Nevertheless, if the metric space \"M\" is complete, then any pointwise Cauchy sequence converges pointwise to a function from \"S\" to \"M\". Similarly, any uniformly Cauchy sequence will tend uniformly to such a function.\n\nThe uniform Cauchy property is frequently used when the \"S\" is not just a set, but a topological space, and \"M\" is a complete metric space. The following theorem holds:\n\n\nA sequence of functions formula_1 from a set \"S\" to a metric space \"U\" is said to be uniformly Cauchy if:\n\n\n"}
{"id": "10458716", "url": "https://en.wikipedia.org/wiki?curid=10458716", "title": "Visual calculus", "text": "Visual calculus\n\nVisual calculus, invented by Mamikon Mnatsakanian (known as Mamikon), is an approach to solving a variety of integral calculus problems. Many problems that would otherwise seem quite difficult yield to the method with hardly a line of calculation, often reminiscent of what Martin Gardner calls \"aha! solutions\" or Roger Nelsen a proof without words.\n\nMamikon devised his method in 1959 while an undergraduate, first applying it to a well-known geometry problem: Find the area of a ring (annulus), given the length of a chord tangent to the inner circumference. (Perhaps surprisingly, no additional information is needed; the solution does not depend on the ring's inner and outer dimensions.)\n\nThe traditional approach involves algebra and application of the Pythagorean theorem. Mamikon's method, however, envisions an alternate construction of the ring: First the inner circle alone is drawn, then a constant-length tangent is made to travel along its circumference, \"sweeping out\" the ring as it goes.\n\nNow if all the (constant-length) tangents used in constructing the ring are translated so that their points of tangency coincide, the result is a circular disk of known radius (and easily computed area). Indeed, since the inner circle's radius is irrelevant, one could just as well have started with a circle of radius zero (a point)—and sweeping out a ring around a circle of zero radius is indistinguishable from simply rotating a line segment about one of its endpoints and sweeping out a disk.\n\nMamikon's insight was to recognize the equivalence of the two constructions; and because they are equivalent, they yield equal areas. Moreover, so long as it is given that the tangent length is constant, the two starting curves need not be circular—a finding not easily proven by more traditional geometric methods. This yields Mamikon's theorem:\n\nTom Apostol has produced a very readable introduction to the subject. In it he shows that the problems of finding the area of a cycloid and tractrix can be solved by very young students. \"Moreover, the new method also solves some problems \"unsolvable by calculus, and allows many incredible generalizations yet unknown in mathematics\".\" He also mentions that combining Mamikon's method with the geometric solution yields a new proof of the Pythagorean Theorem. Solutions to many other problems appear on Mamikon's Visual Calculus site.\n\nThe area of a cycloid can be calculated by considering the area between it and an enclosing rectangle. These tangents can all be clustered to form a circle. If the circle generating the cycloid has radius \"r\" then this circle also has radius \"r\" and area π\"r\". The area of the rectangle is formula_1. Therefore the area of the cycloid is formula_2: it is 3 times the area of the generating circle.\n\nThe tangent cluster can be seen to be a circle because the cycloid is generated by a circle and the tangent to the cycloid will be at right angle to the line from the generating point to the rolling point. Thus the tangent and the line to the contact point form a right-angled triangle in the generating circle. This means that clustered together the tangents will describe the shape of the generating circle.\n\n\n"}
{"id": "1436668", "url": "https://en.wikipedia.org/wiki?curid=1436668", "title": "Voigt notation", "text": "Voigt notation\n\nIn mathematics, Voigt notation or Voigt form in multilinear algebra is a way to represent a symmetric tensor by reducing its order. There are a few variants and associated names for this idea: Mandel notation, Mandel–Voigt notation and Nye notation are others found. Kelvin notation is a revival by Helbig of old ideas of Lord Kelvin. The differences here lie in certain weights attached to the selected entries of the tensor. Nomenclature may vary according to what is traditional in the field of application.\n\nFor example, a 2×2 symmetric tensor X has only three distinct elements, the two on the diagonal and the other being off-diagonal. Thus it can be expressed as the vector\n\nAs another example:\n\nThe stress tensor (in matrix notation) is given as\n\nIn Voigt notation it is simplified to a 6-dimensional vector:\n\nThe strain tensor, similar in nature to the stress tensor—both are symmetric second-order tensors --, is given in matrix form as\n\nIts representation in Voigt notation is\nwhere formula_6, formula_7, and formula_8 are engineering shear strains.\n\nThe benefit of using different representations for stress and strain is that the scalar invariance\nis preserved.\n\nLikewise, a three-dimensional symmetric fourth-order tensor can be reduced to a 6×6 matrix.\n\nA simple mnemonic rule for memorizing Voigt notation is as follows:\n\n\nVoigt indexes are numbered consecutively from the starting point to the end (in the example, the numbers in blue).\n\nFor a symmetric tensor of second rank\n\nonly six components are distinct, the three on the diagonal and the others being off-diagonal. \nThus it can be expressed, in Mandel notation, as the vector\n\nThe main advantage of Mandel notation is to allow the use of the same conventional operations used with vectors,\nfor example:\n\nA symmetric tensor of rank four satisfying formula_13 and formula_14 has 81 components in three-dimensional space, but only 36 \ncomponents are distinct. Thus, in Mandel notation, it can be expressed as\n\nThe notation is named after physicist Woldemar Voigt. It is useful, for example, in calculations involving constitutive models to simulate materials, such as the generalized Hooke's law, as well as finite element analysis, and Diffusion MRI.\n\nHooke's law has a symmetric fourth-order stiffness tensor with 81 components (3×3×3×3), but because the application of such a rank-4 tensor to a symmetric rank-2 tensor must yield another symmetric rank-2 tensor, not all of the 81 elements are independent. Voigt notation enables such a rank-4 tensor to be represented by a 6×6 matrix. However, Voigt's form does not preserve the sum of the squares, which in the case of Hooke's law has geometric significance. This explains why weights are introduced (to make the mapping an isometry).\n\nA discussion of invariance of Voigt's notation and Mandel's notation be found in Helnwein (2001).\n\n"}
{"id": "20625645", "url": "https://en.wikipedia.org/wiki?curid=20625645", "title": "Von Neumann stability analysis", "text": "Von Neumann stability analysis\n\nIn numerical analysis, von Neumann stability analysis (also known as Fourier stability analysis) is a procedure used to check the stability of finite difference schemes as applied to linear partial differential equations. The analysis is based on the Fourier decomposition of numerical error and was developed at Los Alamos National Laboratory after having been briefly described in a 1947 article by British researchers Crank and Nicolson.\nThis method is an example of explicit time integration where the function that defines governing equation is evaluated at the current time.\nLater, the method was given a more rigorous treatment in an article co-authored by John von Neumann.\n\nThe stability of numerical schemes is closely associated with numerical error. A finite difference scheme is stable if the errors made at one time step of the calculation do not cause the errors to be magnified as the computations are continued. A \"neutrally stable scheme\" is one in which errors remain constant as the computations are carried forward. If the errors decay and eventually damp out, the numerical scheme is said to be stable. If, on the contrary, the errors grow with time the numerical scheme is said to be unstable. The stability of numerical schemes can be investigated by performing von Neumann stability analysis. For time-dependent problems, stability guarantees that the numerical method produces a bounded solution whenever the solution of the exact differential equation is bounded. Stability, in general, can be difficult to investigate, especially when the equation under consideration is nonlinear.\n\nIn certain cases, von Neumann stability is necessary and sufficient for stability in the sense of Lax–Richtmyer (as used in the Lax equivalence theorem): The PDE and the finite difference scheme models are linear; the PDE is constant-coefficient with periodic boundary conditions and has only two independent variables; and the scheme uses no more than two time levels. Von Neumann stability is necessary in a much wider variety of cases. It is often used in place of a more detailed stability analysis to provide a good guess at the restrictions (if any) on the step sizes used in the scheme because of its relative simplicity.\n\nThe von Neumann method is based on the decomposition of the errors into Fourier series. To illustrate the procedure, consider the one-dimensional heat equation \ndefined on the spatial interval formula_2, which can be discretized as\nwhere\nand the solution formula_5 of the discrete equation approximates the analytical solution formula_6 of the PDE on the grid.\n\nDefine the round-off error formula_7 as\nwhere formula_9 is the solution of the discretized equation (1) that would be computed in the absence of round-off error, and formula_10 is the numerical solution obtained in finite precision arithmetic. Since the exact solution formula_9 must satisfy the discretized equation exactly, the error formula_7 must also satisfy the discretized equation. Here we assumed that formula_10 satisfies the equation, too (this is only true in machine precision).\nThus\nis a recurrence relation for the error. Equations (1) and (2) show that both the error and the numerical solution have the same growth or decay behavior with respect to time. For linear differential equations with periodic boundary condition, the spatial variation of error may be expanded in a finite Fourier series, in the interval formula_2, as\nwhere the wavenumber formula_17 with formula_18 and formula_19. The time dependence of the error is included by assuming that the amplitude of error formula_20 is a function of time. Since the error tends to grow or decay exponentially with time, it is reasonable to assume that the amplitude varies exponentially with time; hence\nwhere formula_22 is a constant.\n\nSince the difference equation for error is linear (the behavior of each term of the series is the same as series itself), it is enough to consider the growth of error of a typical term:\nThe stability characteristics can be studied using just this form for the error with no loss in generality. To find out how error varies in steps of time, substitute equation (5) into equation (2), after noting that\nto yield (after simplification)\n\nUsing the identities\nequation (6) may be written as\nDefine the amplification factor\nThe necessary and sufficient condition for the error to remain bounded is that formula_29 \nHowever,\nThus, from equations (7) and (8), the condition for stability is given by\nNote that the term formula_32 is always positive. Thus, to satisfy Equation (9):\nFor the above condition to hold for all formula_34 (and therefore all formula_35), we have\nEquation (11) gives the stability requirement for the FTCS scheme as applied to one-dimensional heat equation. It says that for a given formula_37, the allowed value of formula_38 must be small enough to satisfy equation (10).\n"}
{"id": "52021", "url": "https://en.wikipedia.org/wiki?curid=52021", "title": "W. T. Tutte", "text": "W. T. Tutte\n\nWilliam Thomas \"Bill\" Tutte (; 14 May 1917 – 2 May 2002) was a British codebreaker and mathematician. During the Second World War, he made a brilliant and fundamental advance in cryptanalysis of the Lorenz cipher, a major Nazi German cipher system which was used for top-secret communications within the Wehrmacht High Command. The high-level, strategic nature of the intelligence obtained from Tutte's crucial breakthrough, in the bulk decrypting of Lorenz-enciphered messages specifically, contributed greatly, and perhaps even decisively, to the defeat of Nazi Germany. He also had a number of significant mathematical accomplishments, including foundation work in the fields of graph theory and matroid theory.\n\nTutte's research in the field of graph theory proved to be of remarkable importance. At a time when graph theory was still a primitive subject, Tutte commenced the study of matroids and developed them into a theory by expanding from the work that Hassler Whitney had first developed around the mid 1930s. Even though Tutte's contributions to graph theory have been influential to modern graph theory and many of his theorems have been used to keep making advances in the field, most of his terminology was not in agreement with their conventional usage and thus his terminology is not used by graph theorists today. \"Tutte advanced graph theory from a subject with one text (D. Kőnig's) toward its present extremely active state.\"\n\nTutte was born in Newmarket in Suffolk. He was the younger son of William John Tutte (1873–1944), an estate gardener, and Annie (\"née\" Newell; 1881–1956), a housekeeper. Both parents worked at Fitzroy House stables where Tutte was born. The family spent some time in Buckinghamshire, County Durham and Yorkshire before returning to Newmarket, where Tutte attended Cheveley Church of England primary school. In 1927, when he was ten, Tutte won a scholarship to the Cambridge and County High School for Boys. He took up his place there in 1928. In 1935 he won a scholarship to study natural sciences at Trinity College, Cambridge, where he specialized in chemistry and graduated with first class honours in 1938. He continued with physical chemistry as a graduate student, but transferred to mathematics at the end of 1940. As a student, he (along with three of his friends) became one of the first to solve the problem of squaring the square, and the first to solve the problem without a squared subrectangle. Together the four created the pseudonym Blanche Descartes, under which Tutte published occasionally for years.\n\nSoon after the outbreak of the Second World War, Tutte's tutor, Patrick Duff, suggested him for war work at the Government Code and Cypher School at Bletchley Park (BP). He was interviewed and sent on a training course in London before going to Bletchley Park, where he joined the Research Section. At first, he worked on the Hagelin cipher that was being used by the Italian Navy. This was a rotor cipher machine that was available commercially, so the mechanics of enciphering was known, and decrypting messages only required working out how the machine was set up.\n\nIn the summer of 1941, Tutte was transferred to work on a project called Fish. Intelligence information had revealed that the Germans called the wireless teleprinter transmission systems \"Sägefisch\" (sawfish). This led the British to use the code Fish for the German teleprinter cipher system. The nickname Tunny (tunafish) was used for the first non-Morse link, and it was subsequently used for the Lorenz SZ machines and the traffic that they enciphered.\n\nTelegraphy used the 5-bit International Telegraphy Alphabet No. 2 (ITA2). Nothing was known about the mechanism of enciphering other than that messages were preceded by a 12-letter indicator, which implied a 12-wheel rotor cipher machine. The first step, therefore, had to be to diagnose the machine by establishing the logical structure and hence the functioning of the machine. Tutte played a pivotal role in achieving this, and it was not until shortly before the Allied victory in Europe in 1945, that Bletchley Park acquired a Tunny Lorenz cipher machine. Tutte's breakthroughs led eventually to bulk decrypting of Tunny-enciphered messages between the German High Command (OKW) in Berlin and their army commands throughout occupied Europe and contributed—perhaps decisively—to the defeat of Germany.\n\nOn 31 August 1941, two versions of the same message were sent using identical keys, which constituted a \"depth\". This allowed John Tiltman, Bletchley Park's veteran and remarkably gifted cryptanalyst, to deduce that it was a Vernam cipher which uses the Exclusive Or (XOR) function (symbolised by \"⊕\"), and to extract the two messages and hence obtain the obscuring key. After a fruitless period during which Research Section cryptanalysts tried to work out how the Tunny machine worked, this and some other keys were handed to Tutte, who was asked to \"see what you can make of these\".\n\nAt his training course, Tutte had been taught the Kasiski examination technique of writing out a key on squared paper, starting a new row after a defined number of characters that was suspected of being the frequency of repetition of the key. If this number was correct, the columns of the matrix would show more repetitions of sequences of characters than chance alone. Tutte knew that the Tunny indicators used 25 letters (excluding J) for 11 of the positions, but only 23 letters for the other. He therefore tried Kasiski's technique on the first impulse of the key characters, using a repetition of 25 × 23 = 575. He did not observe a large number of column repetitions with this period, but he did observe the phenomenon on a diagonal. He therefore tried again with 574, which showed up repeats in the columns. Recognising that the prime factors of this number are 2, 7 and 41, he tried again with a period of 41 and \"got a rectangle of dots and crosses that was replete with repetitions\".\n\nIt was clear, however, that the first impulse of the key was more complicated than that produced by a single wheel of 41 key impulses. Tutte called this component of the key formula_1 (\"chi\"). He figured that there was another component, which was XOR-ed with this, that did not always change with each new character, and that this was the product of a wheel that he called formula_2 (\"psi\"). The same applied for each of the five impulses (formula_1formula_1formula_1formula_1formula_1 and formula_2formula_2formula_2formula_2formula_2). So for a single character, the whole key K consisted of two components:\n\nAt Bletchley Park, mark impulses were signified by x and space impulses by •. For example, the letter \"H\" would be coded as ••x•x. Tutte's derivation of the \"chi\" and \"psi\" components was made possible by the fact that dots were more likely than not to be followed by dots, and crosses more likely than not to be followed by crosses. This was a product of a weakness in the German key setting, which they later eliminated. Once Tutte had made this breakthrough, the rest of the Research Section joined in to study the other impulses, and it was established that the five \"chi\" wheels all advanced with each new character and that the five \"psi\" wheels all moved together under the control of two \"mu\" or \"motor\" wheels. Over the following two months, Tutte and other members of the Research Section worked out the complete logical structure of the machine, with its set of wheels bearing cams that could either be in a position (raised) that added x to the stream of key characters, or in the alternative position that added in •.\n\nDiagnosing the functioning of the Tunny machine in this way was a truly remarkable cryptanalytical achievement which, in the citation for Tutte's induction as an Officer of the Order of Canada, was described as \"one of the greatest intellectual feats of World War II\".\n\nTo decrypt a Tunny message required knowledge not only of the logical functioning of the machine, but also the start positions of each rotor for the particular message. The search was on for a process that would manipulate the ciphertext or key to produce a frequency distribution of characters that departed from the uniformity that the enciphering process aimed to achieve. While on secondment to the Research Section in July 1942, Alan Turing worked out that the XOR combination of the values of successive characters in a stream of ciphertext and key emphasised any departures from a uniform distribution. The resultant stream (symbolised by the Greek letter \"delta\" Δ) was called the difference because XOR is the same as modulo 2 subtraction.\n\nThe reason that this provided a way into Tunny was that although the frequency distribution of characters in the ciphertext could not be distinguished from a random stream, the same was not true for a version of the ciphertext from which the \"chi\" element of the key had been removed. This was the case because where the plaintext contained a repeated character and the \"psi\" wheels did not move on, the differenced \"psi\" character (Δformula_2) would be the null character ('/ ' at Bletchley Park). When XOR-ed with any character, this character has no effect. Repeated characters in the plaintext were more frequent both because of the characteristics of German (EE, TT, LL and SS are relatively common), and because telegraphists frequently repeated the figures-shift and letters-shift characters as their loss in an ordinary telegraph message could lead to gibberish.\n\nTo quote the General Report on Tunny:Turingery introduced the principle that the key differenced at one, now called ΔΚ, could yield information unobtainable from ordinary key. This Δ principle was to be the fundamental basis of nearly all statistical methods of wheel-breaking and setting.\n\nTutte exploited this amplification of non-uniformity in the differenced values and by November 1942 had produced a way of discovering wheel starting points of the Tunny machine which became known as the \"Statistical Method\". The essence of this method was to find the initial settings of the \"chi\" component of the key by exhaustively trying all positions of its combination with the ciphertext, and looking for evidence of the non-uniformity that reflected the characteristics of the original plaintext. Because any repeated characters in the plaintext would always generate •, and similarly ∆formula_2 ⊕ ∆formula_2 would generate • whenever the \"psi\" wheels did not move on, and about half of the time when they did – some 70% overall.\n\nAs well as applying differencing to the full 5-bit characters of the ITA2 code, Tutte applied it to the individual impulses (bits). The current \"chi\" wheel cam settings needed to have been established to allow the relevant sequence of characters of the \"chi\" wheels to be generated. It was totally impracticable to generate the 22 million characters from all five of the \"chi\" wheels, so it was initially limited to 41 × 31 = 1271 from the first two. After explaining his findings to Max Newman, Newman was given the job of developing an automated approach to comparing ciphertext and key to look for departures from randomness. The first machine was dubbed Heath Robinson, but the much faster Colossus computer, developed by Tommy Flowers and using algorithms written by Tutte and his colleagues, soon took over for breaking codes.\n\nTutte completed a doctorate in mathematics from Cambridge in 1948 under the supervision of Shaun Wylie, who had also worked at Bletchley Park on Tunny. In late 1945, Tutte resumed his studies at Cambridge, now as a graduate student in mathematics. He published some work begun earlier, one a now famous paper that characterises which graphs have a perfect matching, and another that constructs a non-Hamiltonian graph. He went on to create a ground-breaking PhD thesis, \"An algebraic theory of graphs\", about the subject later known as matroid theory.\n\nThe same year, invited by Harold Scott MacDonald Coxeter, he accepted a position at the University of Toronto. In 1962, he moved to the University of Waterloo in Waterloo, Ontario, where he stayed for the rest of his academic career. He officially retired in 1985, but remained active as an emeritus professor. Tutte was instrumental in helping to found the Department of Combinatorics and Optimization at the University of Waterloo.\n\nHis mathematical career concentrated on combinatorics, especially graph theory, which he is credited as having helped create in its modern form, and matroid theory, to which he made profound contributions; one colleague described him as \"the leading mathematician in combinatorics for three decades\". He was editor in chief of the \"Journal of Combinatorial Theory\" until retiring from Waterloo in 1985. He also served on the editorial boards of several other mathematical research journals.\n\nTutte's work in graph theory includes the structure of cycle spaces and cut spaces, the size of maximum matchings and existence of \"k\"-factors in graphs, and Hamiltonian and non-Hamiltonian graphs. He disproved Tait's conjecture, on the Hamiltonicity of polyhedral graphs, by using the construction known as Tutte's fragment. The eventual proof of the four colour theorem made use of his earlier work. The graph polynomial he called the \"dichromate\" has become famous and influential under the name of the Tutte polynomial and serves as the prototype of combinatorial invariants that are universal for all invariants that satisfy a specified reduction law.\n\nThe first major advances in matroid theory were made by Tutte in his 1948 Cambridge PhD thesis which formed the basis of an important sequence of papers published over the next two decades. Tutte's work in graph theory and matroid theory has been profoundly influential on the development of both the content and direction of these two fields. In matroid theory, he discovered the highly sophisticated homotopy theorem and founded the studies of chain groups and regular matroids, about which he proved deep results.\n\nIn addition, Tutte developed an algorithm for determining whether a given binary matroid is a graphic matroid. The algorithm makes use of the fact that a planar graph is simply a graph whose circuit-matroid, the dual of its bond-matroid, is graphic.\n\nTutte wrote a paper entitled \"How to Draw a Graph\" in which he proved that any face in a 3-connected graph is enclosed by a peripheral cycle. Using this fact, Tutte developed an alternative proof to show that every Kuratowski graph is non-planar by showing that \"K\" and \"K\" each have three distinct peripheral cycles with a common edge. In addition to using peripheral cycles to prove that the Kuratowski graphs are non-planar, Tutte proved that every simple 3-connected graph can be drawn with all its faces convex, and devised an algorithm which constructs the plane drawing by solving a linear system. The resulting drawing is known as the Tutte embedding.\nTutte's algorithm makes use of the barycentric mappings of the peripheral circuits of a simple 3-connected graph.\n\nThe findings published in this paper have proved to be of much significance because the algorithms that Tutte developed have become popular planar graph drawing methods.\nOne of the reasons for which Tutte's embedding is popular is that the necessary computations that are carried out by his algorithms are simple and guarantee a one-to-one correspondence of a graph and its embedding onto the Euclidean plane, which is of importance when parameterising a three-dimensional mesh to the plane in geometric modelling. \"Tutte's theorem is the basis for solutions to other computer graphics problems, such as morphing.\"\n\nTutte was mainly responsible for developing the theory of enumeration of planar graphs, which has close links with chromatic and dichromatic polynomials. This work involved some highly innovative techniques of his own invention, requiring considerable manipulative dexterity in handling power series (whose coefficients count appropriate kinds of graphs) and the functions arising as their sums, as well as geometrical dexterity in extracting these power series from the graph-theoretic situation.\n\nTutte summarised his work in the \"Selected Papers of W.T. Tutte\", 1979, and in \"Graph Theory as I have known it\", 1998.\n\nTutte's work in World War II and subsequently in combinatorics brought him various positions, honours and awards:\nTutte served as Librarian for the Royal Astronomical Society of Canada in 1959–1960, and asteroid 14989 Tutte (1997 UB7) was named after him.\n\nBecause of Tutte's work at Bletchley Park, Canada's Communications Security Establishment named an internal organisation aimed at promoting research into cryptology, the Tutte Institute for Mathematics and Computing (TIMC), in his honour in 2011.\n\nIn September 2014, Tutte was celebrated in his hometown of Newmarket, England, with the unveiling of a sculpture, after a local newspaper started a campaign to honour his memory.\n\nBletchley Park in Milton Keynes celebrates Tuttes' work with a new exhibition Bill Tutte: Mathematician + Codebreaker scheduled to be available 15 May 2017 to 2019, preceded on 14 May 2017 by lectures about his life and work during the Bill Tutte Centenary Symposium.\n\nIn addition to the career benefits of working at the new University of Waterloo, the more rural setting of Waterloo County appealed to Bill and his wife Dorothea. They bought a house in the nearby village of West Montrose, Ontario where they enjoyed hiking, spending time in their garden on the Grand River and allowing others to enjoy the beautiful scenery of their property.\n\nThey also had an extensive knowledge of all the birds in their garden. Dorothea, an avid potter, was also a keen hiker and Bill organised hiking trips. Even near the end of his life Bill still was an avid walker. After his wife died in 1994, he moved back to Newmarket (Suffolk), but then returned to Waterloo in 2000, where he died two years later. He is buried in West Montrose United Cemetery.\n\n\n\n\n"}
