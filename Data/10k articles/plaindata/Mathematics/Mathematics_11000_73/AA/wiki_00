{"id": "12952238", "url": "https://en.wikipedia.org/wiki?curid=12952238", "title": "Algebraic topology (object)", "text": "Algebraic topology (object)\n\nIn mathematics, the algebraic topology on the set of group representations from \"G\" to a topological group \"H\" is the topology of pointwise convergence, i.e. \"p\" converges to \"p\" if the limit of \"p\"(\"g\") = \"p\"(\"g\") for every \"g\" in \"G\".\n\nThis terminology is often used in the case of the algebraic topology on the set of discrete, faithful representations of a Kleinian group into PSL(2,C). Another topology, the geometric topology (also called the Chabauty topology), can be put on the set of images of the representations, and its closure can include extra Kleinian groups that are not images of points in the closure in the algebraic topology. This fundamental distinction is behind the phenomenon of hyperbolic Dehn surgery and plays an important role in the general theory of hyperbolic 3-manifolds.\n\n"}
{"id": "775", "url": "https://en.wikipedia.org/wiki?curid=775", "title": "Algorithm", "text": "Algorithm\n\nIn mathematics and computer science, an algorithm () is an unambiguous specification of how to solve a class of problems. Algorithms can perform calculation, data processing and automated reasoning tasks.\n\nAs an effective method, an algorithm can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing \"output\" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input. \n\nThe concept of algorithm has existed for centuries. Greek mathematicians used algorithms in, for example, the sieve of Eratosthenes for finding prime numbers and the Euclidean algorithm for finding the greatest common divisor of two numbers. \n\nThe word \"algorithm\" itself derives from the 9th century mathematician Muḥammad ibn Mūsā al-Khwārizmī, Latinized \"Algoritmi\". A partial formalization of what would become the modern concept of algorithm began with attempts to solve the Entscheidungsproblem (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define \"effective calculability\" or \"effective method\". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.\n\nThe word 'algorithm' has its roots in Latinizing the name of Muhammad ibn Musa al-Khwarizmi in a first step to \"algorismus\". Al-Khwārizmī (, c. 780–850) was a Persian mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarezm', a region that was part of Greater Iran and is now in Uzbekistan.\n\nAbout 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century under the title \"Algoritmi de numero Indorum\". This title means \"Algoritmi on the numbers of the Indians\", where \"Algoritmi\" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, \"algorismus\", English 'algorism', the corruption of his name, simply meant the \"decimal number system\". In the 15th century, under the influence of the Greek word ἀριθμός 'number' (\"cf.\" 'arithmetic'), the Latin word was altered to \"algorithmus\", and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.\n\nIn English, it was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it wasn't until the late 19th century that \"algorithm\" took on the meaning that it has in modern English.\n\nAnother early use of the word is from 1240, in a manual titled \"Carmen de Algorismo\" composed by Alexandre de Villedieu. It begins thus:\nwhich translates as:\nThe poem is a few hundred lines long and summarizes the art of calculating with the new style of Indian dice, or Talibus Indorum, or Hindu numerals.\n\nAn informal definition could be \"a set of rules that precisely defines a sequence of operations.\" which would include all computer programs, including programs that do not perform numeric calculations. Generally, a program is only an algorithm if it stops eventually.\n\nA prototypical example of an algorithm is the Euclidean algorithm to determine the maximum common divisor of two integers; an example (there are others) is described by the flowchart above and as an example in a later section.\n\nNo human being can write fast enough, or long enough, or small enough† ( †\"smaller and smaller without limit ...you'd be trying to write on molecules, on atoms, on electrons\") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give \"explicit instructions for determining the nth member of the set\", for arbitrary finite \"n\". Such instructions are to be given quite explicitly, in a form in which \"they could be followed by a computing machine\", or by a \"human who is capable of carrying out only very elementary operations on symbols.\"\n\nAn \"enumerably infinite set\" is one whose elements can be put into one-to-one correspondence with the integers. Thus, Boolos and Jeffrey are saying that an algorithm implies instructions for a process that \"creates\" output integers from an \"arbitrary\" \"input\" integer or integers that, in theory, can be arbitrarily large. Thus an algorithm can be an algebraic equation such as \"y = m + n\" – two arbitrary \"input variables\" \"m\" and \"n\" that produce an output \"y\". But various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):\n\nThe concept of \"algorithm\" is also used to define the notion of decidability. That notion is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to our customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of \"algorithm\" that suits both concrete (in some sense) and abstract usage of the term.\n\nAlgorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform (in a specific order) to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):\n\nTypically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.\n\nFor some such computational process, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. That is, any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).\n\nBecause an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting \"from the top\" and going \"down to the bottom\", an idea that is described more formally by \"flow of control\".\n\nSo far, this discussion of the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception, and it attempts to describe a task in discrete, \"mechanical\" means. Unique to this conception of formalized algorithms is the assignment operation, setting the value of a variable. It derives from the intuition of \"memory\" as a scratchpad. There is an example below of such an assignment.\n\nFor some alternate conceptions of what constitutes an algorithm see functional programming and logic programming.\n\nAlgorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in natural language statements. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer but are often used as a way to define or document algorithms.\n\nThere is a wide variety of representations possible and one can express a given Turing machine program as a sequence of machine tables (see more at finite-state machine, state transition table and control table), as flowcharts and drakon-charts (see more at state diagram), or as a form of rudimentary machine code or assembly code called \"sets of quadruples\" (see more at Turing machine).\n\nRepresentations of algorithms can be classed into three accepted levels of Turing machine description:\n\nFor an example of the simple algorithm \"Add m+n\" described in all three levels, see Algorithm#Examples.\n\nAlgorithm design refers to a method or mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, such as the template method pattern and decorator pattern.\n\nOne of the most important aspects of algorithm design is creating an algorithm that has an efficient run-time, also known as its Big O.\n\nTypical steps in the development of algorithms:\n\nMost algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.\n\nIn computer systems, an algorithm is basically an instance of logic written in software by software developers, to be effective for the intended \"target\" computer(s) to produce \"output\" from given (perhaps null) \"input\". An optimal algorithm, even running in old hardware, would produce faster results than a non-optimal (higher time complexity) algorithm for the same purpose, running in more efficient hardware; that is why algorithms, like computer hardware, are considered technology.\n\n\"\"Elegant\" (compact) programs, \"good\" (fast) programs \": The notion of \"simplicity and elegance\" appears informally in Knuth and precisely in Chaitin:\n\nChaitin prefaces his definition with: \"I'll show you can't prove that a program is 'elegant'\"—such proof would solve the Halting problem (ibid).\n\n\"Algorithm versus function computable by an algorithm\": For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that \"It is . . . important to distinguish between the notion of \"algorithm\", i.e. procedure and the notion of \"function computable by algorithm\", i.e. mapping yielded by the procedure. The same function may have several different algorithms\".\n\nUnfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.\n\n\"Computers (and computors), models of computation\": A computer (or human \"computor\") is a restricted type of machine, a \"discrete deterministic mechanical device\" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable \"locations\", (ii) discrete, indistinguishable \"counters\" (iii) an agent, and (iv) a list of instructions that are \"effective\" relative to the capability of the agent.\n\nMinsky describes a more congenial variation of Lambek's \"abacus\" model in his \"Very Simple Bases for Computability\". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions, unless either a conditional IF–THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three \"assignment\" (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write \"code\" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general \"types\" of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.\n\n\"Simulation of an algorithm: computer (computor) language\": Knuth advises the reader that \"the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example\". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can \"effectively\" execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.\n\nThis means that the programmer must know a \"language\" that is effective relative to the target computing agent (computer/computor).\n\nBut what model should be used for the simulation? Van Emde Boas observes \"even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of \"simulation\" enters\". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a \"modulus\" instruction available rather than just subtraction (or worse: just Minsky's \"decrement\").\n\n\"Structured programming, canonical structures\": Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while \"undisciplined\" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in \"spaghetti code\", a programmer can write structured programs using only these instructions; on the other hand \"it is also possible, and not too hard, to write badly structured programs in a structured language\". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.\n\n\"Canonical flowchart symbols\": The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can \"nest\" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.\n\nOne of the simplest algorithms is to find the largest number in a list of numbers of random order. Finding the solution requires looking at every number in the list. From this follows a simple algorithm, which can be stated in a high-level description of English prose, as:\n\n\"High-level description:\"\n\n\"(Quasi-)formal description:\"\nWritten in prose but much closer to the high-level language of a computer program, the following is the more formal coding of the algorithm in pseudocode or pidgin code:\n\nEuclid's algorithm to compute the greatest common divisor (GCD) to two numbers appears as Proposition II in Book VII (\"Elementary Number Theory\") of his \"Elements\". Euclid poses the problem thus: \"Given two numbers not prime to one another, to find their greatest common measure\". He defines \"A number [to be] a multitude composed of units\": a counting number, a positive integer not including zero. To \"measure\" is to place a shorter measuring length \"s\" successively (\"q\" times) along longer length \"l\" until the remaining portion \"r\" is less than the shorter length \"s\". In modern words, remainder \"r\" = \"l\" − \"q\"×\"s\", \"q\" being the quotient, or remainder \"r\" is the \"modulus\", the integer-fractional part left over after the division.\n\nFor Euclid's method to succeed, the starting lengths must satisfy two requirements: (i) the lengths must not be zero, AND (ii) the subtraction must be “proper”; i.e., a test must guarantee that the smaller of the two numbers is subtracted from the larger (alternately, the two can be equal so their subtraction yields zero).\n\nEuclid's original proof adds a third requirement: the two lengths must not be prime to one another. Euclid stipulated this so that he could construct a reductio ad absurdum proof that the two numbers' common measure is in fact the \"greatest\". While Nicomachus' algorithm is the same as Euclid's, when the numbers are prime to one another, it yields the number \"1\" for their common measure. So, to be precise, the following is really Nicomachus' algorithm.\n\nOnly a few instruction \"types\" are required to execute Euclid's algorithm—some logical tests (conditional GOTO), unconditional GOTO, assignment (replacement), and subtraction.\n\nThe following algorithm is framed as Knuth's four-step version of Euclid's and Nicomachus', but, rather than using division to find the remainder, it uses successive subtractions of the shorter length \"s\" from the remaining length \"r\" until \"r\" is less than \"s\". The high-level description, shown in boldface, is adapted from Knuth 1973:2–4:\n\nINPUT:\n\nE0: [Ensure \"r\" ≥ \"s\".]\n\nE1: [Find remainder]: Until the remaining length \"r\" in R is less than the shorter length \"s\" in S, repeatedly subtract the measuring number \"s\" in S from the remaining length \"r\" in R.\n\nE2: [Is the remainder zero?]: EITHER (i) the last measure was exact, the remainder in R is zero, and the program can halt, OR (ii) the algorithm must continue: the last measure left a remainder in R less than measuring number in S.\n\nE3: [Interchange \"s\" and \"r\"]: The nut of Euclid's algorithm. Use remainder \"r\" to measure what was previously smaller number \"s\"; L serves as a temporary location.\n\nOUTPUT:\n\nDONE:\n\nThe following version of Euclid's algorithm requires only six core instructions to do what thirteen are required to do by \"Inelegant\"; worse, \"Inelegant\" requires more \"types\" of instructions. The flowchart of \"Elegant\" can be found at the top of this article. In the (unstructured) Basic language, the steps are numbered, and the instruction is the assignment instruction symbolized by ←.\n\nThe following version can be used with Object Oriented languages:\n\n\"How \"Elegant\" works\": In place of an outer \"Euclid loop\", \"Elegant\" shifts back and forth between two \"co-loops\", an A > B loop that computes A ← A − B, and a B ≤ A loop that computes B ← B − A. This works because, when at last the minuend M is less than or equal to the subtrahend S ( Difference = Minuend − Subtrahend), the minuend can become \"s\" (the new measuring length) and the subtrahend can become the new \"r\" (the length to be measured); in other words the \"sense\" of the subtraction reverses.\n\nDoes an algorithm do what its author wants it to do? A few test cases usually suffice to confirm core functionality. One source uses 3009 and 884. Knuth suggested 40902, 24140. Another interesting case is the two relatively prime numbers 14157 and 5950.\n\nBut exceptional cases must be identified and tested. Will \"Inelegant\" perform properly when R > S, S > R, R = S? Ditto for \"Elegant\": B > A, A > B, A = B? (Yes to all). What happens when one number is zero, both numbers are zero? (\"Inelegant\" computes forever in all cases; \"Elegant\" computes forever when A = 0.) What happens if \"negative\" numbers are entered? Fractional numbers? If the input numbers, i.e. the domain of the function computed by the algorithm/program, is to include only positive integers including zero, then the failures at zero indicate that the algorithm (and the program that instantiates it) is a partial function rather than a total function. A notable failure due to exceptions is the Ariane 5 Flight 501 rocket failure (June 4, 1996).\n\n\"Proof of program correctness by use of mathematical induction\": Knuth demonstrates the application of mathematical induction to an \"extended\" version of Euclid's algorithm, and he proposes \"a general method applicable to proving the validity of any algorithm\". Tausworthe proposes that a measure of the complexity of a program be the length of its correctness proof.\n\n\"Elegance (compactness) versus goodness (speed)\": With only six core instructions, \"Elegant\" is the clear winner, compared to \"Inelegant\" at thirteen instructions. However, \"Inelegant\" is \"faster\" (it arrives at HALT in fewer steps). Algorithm analysis indicates why this is the case: \"Elegant\" does \"two\" conditional tests in every subtraction loop, whereas \"Inelegant\" only does one. As the algorithm (usually) requires many loop-throughs, \"on average\" much time is wasted doing a \"B = 0?\" test that is needed only after the remainder is computed.\n\n\"Can the algorithms be improved?\": Once the programmer judges a program \"fit\" and \"effective\"—that is, it computes the function intended by its author—then the question becomes, can it be improved?\n\nThe compactness of \"Inelegant\" can be improved by the elimination of five steps. But Chaitin proved that compacting an algorithm cannot be automated by a generalized algorithm; rather, it can only be done heuristically; i.e., by exhaustive search (examples to be found at Busy beaver), trial and error, cleverness, insight, application of inductive reasoning, etc. Observe that steps 4, 5 and 6 are repeated in steps 11, 12 and 13. Comparison with \"Elegant\" provides a hint that these steps, together with steps 2 and 3, can be eliminated. This reduces the number of core instructions from thirteen to eight, which makes it \"more elegant\" than \"Elegant\", at nine steps.\n\nThe speed of \"Elegant\" can be improved by moving the \"B=0?\" test outside of the two subtraction loops. This change calls for the addition of three instructions (B = 0?, A = 0?, GOTO). Now \"Elegant\" computes the example-numbers faster; whether this is always the case for any given A, B, and R, S would require a detailed analysis.\n\nIt is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, the sorting algorithm above has a time requirement of O(\"n\"), using the big O notation with \"n\" as the length of the list. At all times the algorithm only needs to remember two values: the largest number found so far, and its current position in the input list. Therefore, it is said to have a space requirement of \"O(1)\", if the space required to store the input numbers is not counted, or O(\"n\") if it is counted.\n\nDifferent algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n) ) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.\n\nThe analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. Usually pseudocode is used for analysis as it is the simplest and most general representation. However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. For the solution of a \"one off\" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.\n\nEmpirical testing is useful because it may uncover unexpected interactions that affect performance. Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization.\nEmpirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.\n\nTo illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. In general, speed improvements depend on special properties of the problem, which are very common in practical applications. Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.\n\nThere are various ways to classify algorithms, each with its own merits.\n\nOne way to classify algorithms is by implementation means.\n\n\nAnother way of classifying algorithms is by their design methodology or paradigm. There is a certain number of paradigms, each different from the other. Furthermore, each of these categories includes many different types of algorithms. Some common paradigms are:\n\n\nFor optimization problems there is a more specific classification of algorithms; an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:\n\n\nEvery field of science has its own problems and needs efficient algorithms. Related problems in one field are often studied together. Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.\n\nFields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.\n\nAlgorithms can be classified by the amount of time they need to complete compared to their input size:\n\n\nSome problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. There are also mappings from some problems to other problems. Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.\n\nThe adjective \"continuous\" when applied to the word \"algorithm\" can mean:\n\nAlgorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute \"processes\" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.\n\nAdditionally, some cryptographic algorithms have export restrictions (see export of cryptography).\n\nAlgorithms were used in ancient Greece. Two examples are the Sieve of Eratosthenes, which was described in Introduction to Arithmetic by Nicomachus, and the Euclidean algorithm, which was first described in Euclid's Elements (c. 300 BC). Babylonian clay tablets describe and employ algorithmic procedures to compute the time and place of significant astronomical events.\n\nTally-marks: To keep track of their flocks, their sacks of grain and their money the ancients used tallying: accumulating stones or marks scratched on sticks or making discrete symbols in clay. Through the Babylonian and Egyptian use of marks and symbols, eventually Roman numerals and the abacus evolved (Dilson, p. 16–41). Tally marks appear prominently in unary numeral system arithmetic used in Turing machine and Post–Turing machine computations.\n\nThe work of the ancient Greek geometers (Euclidean algorithm), the Indian mathematician Brahmagupta, and the Persian mathematician Al-Khwarizmi (from whose name the terms \"algorism\" and \"algorithm\" are derived), and Western European mathematicians culminated in Leibniz's notion of the calculus ratiocinator (ca 1680):\n\"The clock\": Bolter credits the invention of the weight-driven clock as \"The key invention [of Europe in the Middle Ages]\", in particular, the verge escapement that provides us with the tick and tock of a mechanical clock. \"The accurate automatic machine\" led immediately to \"mechanical automata\" beginning in the 13th century and finally to \"computational machines\"—the difference engine and analytical engines of Charles Babbage and Countess Ada Lovelace, mid-19th century. Lovelace is credited with the first creation of an algorithm intended for processing on a computer – Babbage's analytical engine, the first device considered a real Turing-complete computer instead of just a calculator – and is sometimes called \"history's first programmer\" as a result, though a full implementation of Babbage's second device would not be realized until decades after her lifetime.\n\n\"Logical machines 1870—Stanley Jevons' \"logical abacus\" and \"logical machine\"\": The technical problem was to reduce Boolean equations when presented in a form similar to what is now known as Karnaugh maps. Jevons (1880) describes first a simple \"abacus\" of \"slips of wood furnished with pins, contrived so that any part or class of the [logical] combinations can be picked out mechanically . . . More recently, however, I have reduced the system to a completely mechanical form, and have thus embodied the whole of the indirect process of inference in what may be called a \"Logical Machine\"\" His machine came equipped with \"certain moveable wooden rods\" and \"at the foot are 21 keys like those of a piano [etc] . . .\". With this machine he could analyze a \"syllogism or any other simple logical argument\".\n\nThis machine he displayed in 1870 before the Fellows of the Royal Society. Another logician John Venn, however, in his 1881 \"Symbolic Logic\", turned a jaundiced eye to this effort: \"I have no high estimate myself of the interest or importance of what are sometimes called logical machines ... it does not seem to me that any contrivances at present known or likely to be discovered really deserve the name of logical machines\"; see more at Algorithm characterizations. But not to be outdone he too presented \"a plan somewhat analogous, I apprehend, to Prof. Jevon's \"abacus\" ... [And] [a]gain, corresponding to Prof. Jevons's logical machine, the following contrivance may be described. I prefer to call it merely a logical-diagram machine ... but I suppose that it could do very completely all that can be rationally expected of any logical machine\".\n\n\"Jacquard loom, Hollerith punch cards, telegraphy and telephony—the electromechanical relay\": Bell and Newell (1971) indicate that the Jacquard loom (1801), precursor to Hollerith cards (punch cards, 1887), and \"telephone switching technologies\" were the roots of a tree leading to the development of the first computers. By the mid-19th century the telegraph, the precursor of the telephone, was in use throughout the world, it's discrete and distinguishable encoding of letters as \"dots and dashes\" a common sound. By the late 19th century the ticker tape (ca 1870s) was in use, as was the use of Hollerith cards in the 1890 U.S. census. Then came the teleprinter (ca. 1910) with its punched-paper use of Baudot code on tape.\n\n\"Telephone-switching networks\" of electromechanical relays (invented 1835) was behind the work of George Stibitz (1937), the inventor of the digital adding device. As he worked in Bell Laboratories, he observed the \"burdensome' use of mechanical calculators with gears. \"He went home one evening in 1937 intending to test his idea... When the tinkering was over, Stibitz had constructed a binary adding device\".\n\nDavis (2000) observes the particular importance of the electromechanical relay (with its two \"binary states\" \"open\" and \"closed\"):\n\n\"Symbols and rules\": In rapid succession, the mathematics of George Boole (1847, 1854), Gottlob Frege (1879), and Giuseppe Peano (1888–1889) reduced arithmetic to a sequence of symbols manipulated by rules. Peano's \"The principles of arithmetic, presented by a new method\" (1888) was \"the first attempt at an axiomatization of mathematics in a symbolic language\".\n\nBut Heijenoort gives Frege (1879) this kudos: Frege's is \"perhaps the most important single work ever written in logic. ... in which we see a \" 'formula language', that is a \"lingua characterica\", a language written with special symbols, \"for pure thought\", that is, free from rhetorical embellishments ... constructed from specific symbols that are manipulated according to definite rules\". The work of Frege was further simplified and amplified by Alfred North Whitehead and Bertrand Russell in their Principia Mathematica (1910–1913).\n\n\"The paradoxes\": At the same time a number of disturbing paradoxes appeared in the literature, in particular, the Burali-Forti paradox (1897), the Russell paradox (1902–03), and the Richard Paradox. The resultant considerations led to Kurt Gödel's paper (1931)—he specifically cites the paradox of the liar—that completely reduces rules of recursion to numbers.\n\n\"Effective calculability\": In an effort to solve the Entscheidungsproblem defined precisely by Hilbert in 1928, mathematicians first set about to define what was meant by an \"effective method\" or \"effective calculation\" or \"effective calculability\" (i.e., a calculation that would succeed). In rapid succession the following appeared: Alonzo Church, Stephen Kleene and J. B. Rosser's λ-calculus a finely honed definition of \"general recursion\" from the work of Gödel acting on suggestions of Jacques Herbrand (cf. Gödel's Princeton lectures of 1934) and subsequent simplifications by Kleene. Church's proof that the Entscheidungsproblem was unsolvable, Emil Post's definition of effective calculability as a worker mindlessly following a list of instructions to move left or right through a sequence of rooms and while there either mark or erase a paper or observe the paper and make a yes-no decision about the next instruction. Alan Turing's proof of that the Entscheidungsproblem was unsolvable by use of his \"a- [automatic-] machine\"—in effect almost identical to Post's \"formulation\", J. Barkley Rosser's definition of \"effective method\" in terms of \"a machine\". S. C. Kleene's proposal of a precursor to \"Church thesis\" that he called \"Thesis I\", and a few years later Kleene's renaming his Thesis \"Church's Thesis\" and proposing \"Turing's Thesis\".\n\nHere is a remarkable coincidence of two men not knowing each other but describing a process of men-as-computers working on computations—and they yield virtually identical definitions.\n\nEmil Post (1936) described the actions of a \"computer\" (human being) as follows:\n\nHis symbol space would be\n\nAlan Turing's work preceded that of Stibitz (1937); it is unknown whether Stibitz knew of the work of Turing. Turing's biographer believed that Turing's use of a typewriter-like model derived from a youthful interest: \"Alan had dreamt of inventing typewriters as a boy; Mrs. Turing had a typewriter, and he could well have begun by asking himself what was meant by calling a typewriter 'mechanical'\". Given the prevalence of Morse code and telegraphy, ticker tape machines, and teletypewriters we might conjecture that all were influences.\n\nTuring—his model of computation is now called a Turing machine—begins, as did Post, with an analysis of a human computer that he whittles down to a simple set of basic motions and \"states of mind\". But he continues a step further and creates a machine as a model of computation of numbers.\n\nTuring's reduction yields the following:\n\"It may be that some of these change necessarily invoke a change of state of mind. The most general single operation must, therefore, be taken to be one of the following:\n\nA few years later, Turing expanded his analysis (thesis, definition) with this forceful expression of it:\n\nJ. Barkley Rosser defined an 'effective [mathematical] method' in the following manner (italicization added):\n\nRosser's footnote No. 5 references the work of (1) Church and Kleene and their definition of λ-definability, in particular Church's use of it in his \"An Unsolvable Problem of Elementary Number Theory\" (1936); (2) Herbrand and Gödel and their use of recursion in particular Gödel's use in his famous paper \"On Formally Undecidable Propositions of Principia Mathematica and Related Systems I\" (1931); and (3) Post (1936) and Turing (1936–37) in their mechanism-models of computation.\n\nStephen C. Kleene defined as his now-famous \"Thesis I\" known as the Church–Turing thesis. But he did this in the following context (boldface in original):\n\nA number of efforts have been directed toward further refinement of the definition of \"algorithm\", and activity is on-going because of issues surrounding, in particular, foundations of mathematics (especially the Church–Turing thesis) and philosophy of mind (especially arguments about artificial intelligence). For more, see Algorithm characterizations.\n\n\n"}
{"id": "3005202", "url": "https://en.wikipedia.org/wiki?curid=3005202", "title": "Atomic model (mathematical logic)", "text": "Atomic model (mathematical logic)\n\nIn model theory, an atomic model is a model such that the complete type of every tuple is axiomatized by a single formula. Such types are called principal types, and the formulas that axiomatize them are called complete formulas.\n\nA complete type \"p\"(\"x\", ..., \"x\") is called principal (or atomic) if it is axiomatized by a single formula φ(\"x\", ..., \"x\") ∈ \"p\"(\"x\", ..., \"x\").\n\nA formula φ in a complete theory \"T\" is called complete if for every other formula ψ(\"x\", ..., \"x\"), the formula φ implies exactly one of ψ and ¬ψ in \"T\".\nIt follows that a complete type is principal if and only if it contains a complete formula.\n\nA model \"M\" of the theory is called atomic if every \"n\"-tuple of elements of \"M\" satisfies a complete formula.\n\n\nThe back-and-forth method can be used to show that any two countable atomic models of a theory that are elementarily equivalent are isomorphic.\n\n"}
{"id": "22180498", "url": "https://en.wikipedia.org/wiki?curid=22180498", "title": "Birman–Wenzl algebra", "text": "Birman–Wenzl algebra\n\nIn mathematics, the Birman-Murakami-Wenzl (BMW) algebra, introduced by and , is a two-parameter family of algebras C(\"ℓ\", \"m\") of dimension 1·3·5 ··· (2\"n\" − 1) having the Hecke algebra of the symmetric group as a quotient. It is related to the Kauffman polynomial of a link. It is a deformation of the Brauer algebra in much the same way that Hecke algebras are deformations of the group algebra of the symmetric group.\n\nFor each natural number \"n\", the BMW algebra C(\"ℓ\", \"m\") is generated by G,G...,G,E,E...,E and relations:\n\nThese relations imply the further relations: <br>\n\nThis is the original definition given by Birman & Wenzl. However a slight change by the introduction of some minus signs is sometimes made, in accordance with Kauffman's 'Dubrovnik' version of his link invariant. In that way, the fourth relation in Birman & Wenzl's original version is changed to <br>\n(1) (Kauffman skein relation) \n\n(2) (Idempotent relation) \n(3) (Braid relations) \n(4) (Tangle relations) \n(5) (Delooping relations) \n\n\nIt is proved by that the BMW algebra C(\"ℓ\", \"m\") is isomorphic to the Kauffman's tangle algebra KT, the isomorphism formula_22 is defined by <br>\n\nDefine the face operator as\nwhere formula_24 and formula_25 are determined by \nand \n\nThen the face operator satisfies the Yang-Baxter equation. \nNow formula_29 with \nIn the limits formula_31, the braids formula_32 can be recovered up to a scale factor.\n\nIn 1984, Vaughan Jones introduced a new polynomial invariant of link isotopy types which is called the Jones polynomial. The invariants are related to the traces of irreducible representations of Hecke algebras associated with the symmetric groups. In 1986, showed that the Kauffman polynomial can also be interpreted as a function formula_33 on a certain associative algebra. In 1989, constructed a two-parameter family of algebras C(\"ℓ\", \"m\") with the Kauffman polynomial K(\"ℓ\", \"m\") as trace after appropriate renormalization.\n\n"}
{"id": "44987", "url": "https://en.wikipedia.org/wiki?curid=44987", "title": "Borel–Cantelli lemma", "text": "Borel–Cantelli lemma\n\nIn probability theory, the Borel–Cantelli lemma is a theorem about sequences of events. In general, it is a result in measure theory. It is named after Émile Borel and Francesco Paolo Cantelli, who gave statement to the lemma in the first decades of the 20th century. A related result, sometimes called the second Borel–Cantelli lemma, is a partial converse of the first Borel–Cantelli lemma. The lemma states that, under certain conditions, an event will have probability of either zero or one. Accordingly, it is the best-known of a class of similar theorems, known as zero-one laws. Other examples include Kolmogorov's zero–one law and the Hewitt–Savage zero–one law.\n\nLet \"E\",\"E\"... be a sequence of events in some probability space.\nThe Borel–Cantelli lemma states:\n\nHere, \"lim sup\" denotes limit supremum of the sequence of events, and each event is a set of outcomes. That is, lim sup \"E\" is the set of outcomes that occur infinitely many times within the infinite sequence of events (\"E\"). Explicitly,\n\nThe theorem therefore asserts that if the sum of the probabilities of the events \"E\" is finite, then the set of all outcomes that are \"repeated\" infinitely many times must occur with probability zero. Note that no assumption of independence is required.\n\nSuppose (\"X\") is a sequence of random variables with Pr(\"X\" = 0) = 1/\"n\" for each \"n\". The probability that \"X\" = 0 occurs for infinitely many \"n\" is equivalent to the probability of the intersection of infinitely many [\"X\" = 0] events. The intersection of infinitely many such events is a set of outcomes common to all of them. However, the sum ∑Pr(\"X\" = 0) converges to /6 ≈ 1.645 < ∞, and so the Borel–Cantelli Lemma states that the set of outcomes that are common to infinitely many such events occurs with probability zero. Hence, the probability of \"X\" = 0 occurring for infinitely many \"n\" is 0. Almost surely (i.e., with probability 1), \"X\" is nonzero for all but finitely many \"n\".\n\nLet formula_4 denote the indicator function of the event formula_5 (using Iverson bracket notation). Then, by the monotone convergence theorem\nby hypothesis. This directly implies that\nbecause otherwise\n\nLet (\"E\") be a sequence of events in some probability space and suppose that the sum of the probabilities of the \"E\" is finite. That is suppose:\n\nAs the series converges, we must have that\n\nTherefore :\n\nTherefore it follows that\n\nFor general measure spaces, the Borel–Cantelli lemma takes the following form:\n\nA related result, sometimes called the second Borel–Cantelli lemma, is a partial converse of the first Borel–Cantelli lemma. The lemma states: If the events \"E\" are independent and the sum of the probabilities of the \"E\" diverges to infinity, then the probability that infinitely many of them occur is 1. That is:\n\nThe assumption of independence can be weakened to pairwise independence, but in that case the proof is more difficult.\n\nThe infinite monkey theorem is a special case of this lemma.\n\nThe lemma can be applied to give a covering theorem in R. Specifically , if \"E\" is a collection of Lebesgue measurable subsets of a compact set in R such that\n\nthen there is a sequence \"F\" of translates\n\nsuch that\n\napart from a set of measure zero.\n\nSuppose that formula_21 and the events formula_22 are independent. It is sufficient to show the event that the \"E\"'s did not occur for infinitely many values of \"n\" has probability 0. This is just to say that it is sufficient to show that\n\nNoting that:\n\nit is enough to show: formula_25. Since the formula_16 are independent:\n\nThis completes the proof. Alternatively, we can see formula_28 by taking negative the logarithm of both sides to get:\n\nSince −log(1 − \"x\") ≥ \"x\" for all \"x\" > 0, the result similarly follows from our assumption that formula_30\n\nAnother related result is the so-called counterpart of the Borel–Cantelli lemma. It is a counterpart of the\nLemma in the sense that it gives a necessary and sufficient condition for the limsup to be 1 by replacing the independence assumption by the completely different assumption that formula_31 is monotone increasing for sufficiently large indices. This Lemma says:\n\nLet formula_31 be such that formula_33,\nand let formula_34 denote the complement of formula_35. Then the probability of infinitely many formula_36 occur (that is, at least one formula_36 occurs) is one if and only if there exists a strictly increasing sequence of positive integers formula_38 such that\n\nThis simple result can be useful in problems such as for instance those involving hitting probabilities for stochastic process with the choice of the sequence formula_40 usually being the essence.\n\n\n\n"}
{"id": "22791155", "url": "https://en.wikipedia.org/wiki?curid=22791155", "title": "Bruce Lee Rothschild", "text": "Bruce Lee Rothschild\n\nBruce Lee Rothschild (born August 26, 1941) is a professor of mathematics at the University of California, Los Angeles specializing in combinatorial mathematics.\n\nRothschild was born in 1941 in Los Angeles. He got his Ph.D. from Yale University in 1967 under the supervision of Øystein Ore. Rothschild wrote several papers with Paul Erdős, giving him an Erdős number of 1.\n\nRothschild, together with Ronald Graham, formulated one of the most monumental results in Ramsey Theory, the Graham-Rothschild Parameter Sets Theorem. He has collaborated with American mathematicians Joel Spencer and Ronald Graham on key texts related to Ramsey Theory.\n\nIn 1971, he shared the Pólya Prize (SIAM) with four other mathematicians for his work on Ramsey theory. In 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "275015", "url": "https://en.wikipedia.org/wiki?curid=275015", "title": "Building (mathematics)", "text": "Building (mathematics)\n\nIn mathematics, a building (also Tits building, Bruhat–Tits building, named after François Bruhat and Jacques Tits) is a combinatorial and geometric structure which simultaneously generalizes certain aspects of flag manifolds, finite projective planes, and Riemannian symmetric spaces. Initially introduced by Jacques Tits as a means to understand the structure of exceptional groups of Lie type, the theory has also been used to study the geometry and topology of homogeneous spaces of p-adic Lie groups and their discrete subgroups of symmetries, in the same way that trees have been used to study free groups.\n\nThe notion of a building was invented by Jacques Tits as a means of describing simple algebraic groups over an arbitrary field. Tits demonstrated how to every such group \"G\" one can associate a simplicial complex Δ = Δ(\"G\") with an action of \"G\", called the spherical building of \"G\". The group \"G\" imposes very strong combinatorial regularity conditions on the complexes Δ that can arise in this fashion. By treating these conditions as axioms for a class of simplicial complexes, Tits arrived at his first definition of a building. A part of the data defining a building Δ is a Coxeter group \"W\", which determines a highly symmetrical simplicial complex \"Σ\" = \"Σ\"(\"W\",\"S\"), called the \"Coxeter complex\". A building Δ is glued together from multiple copies of Σ, called its \"apartments\", in a certain regular fashion. When \"W\" is a finite Coxeter group, the Coxeter complex is a topological sphere, and the corresponding buildings are said to be of spherical type. When \"W\" is an affine Weyl group, the Coxeter complex is a subdivision of the affine plane and one speaks of affine, or Euclidean, buildings. An affine building of type formula_1 is the same as an infinite tree without terminal vertices.\n\nAlthough the theory of semisimple algebraic groups provided the initial motivation for the notion of a building, not all buildings arise from a group. In particular, projective planes and generalized quadrangles form two classes of graphs studied in incidence geometry which satisfy the axioms of a building, but may not be connected with any group. This phenomenon turns out to be related to the low rank of the corresponding Coxeter system (namely, two). Tits proved a remarkable theorem: all spherical buildings of rank at least three are connected with a group; moreover, if a building of rank at least two is connected with a group then the group is essentially determined by the building.\n\nIwahori–Matsumoto, Borel–Tits and Bruhat–Tits demonstrated that in analogy with Tits' construction of spherical buildings, affine buildings can also be constructed from certain groups, namely, reductive algebraic groups over a local non-Archimedean field. Furthermore, if the split rank of the group is at least three, it is essentially determined by its building. Tits later reworked the foundational aspects of the theory of buildings using the notion of a chamber system, encoding the building solely in terms of adjacency properties of simplices of maximal dimension; this leads to simplifications in both spherical and affine cases. He proved that, in analogy with the spherical case, every building of affine type and rank at least four arises from a group.\n\nAn \"n\"-dimensional building \"X\" is an abstract simplicial complex which is a union of subcomplexes \"A\" called apartments such that\n\n\nAn \"n\"-simplex in \"A\" is called a chamber (originally \"chambre\", i.e. \"room\" in French).\n\nThe rank of the building is defined to be \"n\" + 1.\n\nEvery apartment \"A\" in a building is a Coxeter complex. In fact, for every two \"n\"-simplices intersecting in an (\"n\" – 1)-simplex or \"panel\", there is a unique period two simplicial automorphism of \"A\", called a \"reflection\", carrying one \"n\"-simplex onto the other and fixing their common points. These reflections generate a Coxeter group \"W\", called the Weyl group of \"A\", and the simplicial complex \"A\" corresponds to the standard geometric realization of \"W\". Standard generators of the Coxeter group are given by the reflections in the walls of a fixed chamber in \"A\". Since\nthe apartment \"A\" is determined up to isomorphism by the building, the same is true of any two simplices in \"X\" lying in some common apartment \"A\". When \"W\" is finite, the building is said to be spherical. When it is an affine Weyl group, the building is said to be affine or euclidean.\n\nThe chamber system is given by the adjacency graph formed by the chambers; each pair of adjacent chambers can in addition be labelled by one of the standard\ngenerators of the Coxeter group (see ).\n\nEvery building has a canonical length metric inherited from the geometric realisation obtained by identifying the vertices with an orthonormal basis of a Hilbert space. For affine buildings, this metric satisfies the CAT(0) comparison inequality of Alexandrov, known in this setting as the Bruhat-Tits \"non-positive curvature condition\" for geodesic triangles: the distance from a vertex to the midpoint of the opposite side is no greater than the distance in the corresponding Euclidean triangle with the same side-lengths (see ).\n\nIf a group \"G\" acts simplicially on a building \"X\", transitively on pairs (C,A) of chambers \"C\" and apartments \"A\" containing them, then the stabilisers of such a pair define a BN pair or Tits system. In fact the pair of subgroups\n\nsatisfies the axioms of a BN pair and the Weyl group can be identified with \"N\" / \"N\" formula_2 \"B\".\nConversely the building can be recovered from the BN pair, so that every BN pair canonically defines a building.\nIn fact, using the terminology of BN pairs and calling any conjugate of \"B\" a Borel subgroup and any group containing a Borel subgroup a parabolic subgroup,\n\n\nThe same building can often be described by different BN pairs. Moreover, not every building comes from a BN pair: this corresponds to the failure of classification results in low rank and dimension (see below).\n\nThe simplicial structure of the affine and spherical buildings associated to \"SL\"(Q), as well as their interconnections, are easy to explain directly using only concepts from elementary algebra and geometry (see ). In this case there are three different buildings, two spherical and one affine. Each is a union of \"apartments\", themselves simplicial complexes. For the affine building, an apartment is a simplicial complex tessellating Euclidean space E by (\"n\"-1)-dimensional simplices; while for a spherical building it is the finite simplicial complex formed by all \"(n-1)\"! simplices with a given common vertex in the analogous tessellation in E.\n\nEach building is a simplicial complex \"X\" which has to satisfy the following axioms:\n\nLet \"F\" be a field and let \"X\" be the simplicial complex with vertices the non-trivial vector subspaces of \"V\"=\"F\". Two subspaces \"U\" and \"U\" are connected if one of them is a subset of the other. The \"k\"-simplices of \"X\" are formed by sets of \"k\" + 1 \nmutually connected subspaces. Maximal connectivity is obtained by taking \"n\" - 1 proper non-trivial subspaces and the corresponding (\"n\"-1)-simplex corresponds to a \"complete flag\"\n\nLower dimensional simplices correspond to partial flags with fewer intermediary subspaces \"U\".\n\nTo define the apartments in \"X\", it is convenient to define a \"frame\" in \"V\" as a basis (\"v\") determined up to scalar multiplication of each of its vectors \"v\"; in other words a frame is a set of one-dimensional subspaces \"L\" = \"F\"·\"v\" such that any \"k\" of them generate a \"k\"-dimensional subspace. Now an ordered frame \"L\", ..., \"L\" defines a complete flag via\n\nSince reorderings of the \"L\"'s also give a frame, it is straightforward to see that the subspaces, obtained as sums of the \"L\"'s,\nform a simplicial complex of the type required for an apartment of a spherical building. The axioms for a building can easily be verified using the classical Schreier refinement argument used to prove the uniqueness of the Jordan-Hölder decomposition.\n\nLet \"K\" be a field lying between Q and its p-adic completion Q with respect to the usual non-Archimedean p-adic norm \n\nWhen \"K\" = Q, \"R\" is the localization of Z at \"p\" and, when \"K\" = Q, \"R\" = Z, the p-adic integers, i.e. the closure of Z in Q.\n\nThe vertices of the building \"X\" are the \"R\"-lattices in \"V\" = \"K\", i.e. \"R\"-submodules of the form\n\nwhere (\"v\") is a basis of \"V\" over \"K\". Two lattices are said to be \"equivalent\" if one is a scalar multiple of the other by an element of \nthe multiplicative group \"K\"* of \"K\" (in fact only integer powers of \"p\" need be used). Two lattice \"L\" and \"L\" are said to be \"adjacent\" if some lattice equivalent to \"L\" lies between \"L\" and its sublattice \"p\"·\"L\": this relation is symmetric. The \"k\"-simplices of \"X\" are equivalence classes of \"k\" + 1 mutually adjacent lattices, The (\"n\" - 1)- simplices correspond, after relabelling, to chains\n\nwhere each successive quotient has order \"p\". Apartments are defined by fixing a basis (\"v\") of \"V\" and taking all lattices with basis\n(\"p\" \"v\") where (\"a\") lies in Z and is uniquely determined up to addition\nof the same integer to each entry.\n\nBy definition each apartment has the required form and their union is the whole of \"X\". The second axiom follows by a variant of the Schreier refinement argument. The last\naxiom follows by a simple counting argument based on the orders of finite Abelian groups of the form\n\nA standard compactness argument shows that \"X\" is in fact independent of the choice of \"K\". In particular taking \"K\" = Q, it follows that \"X\" is countable. On the other hand, taking \"K\" = Q, the definition shows that \"GL\"(Q) admits a natural simplicial action on the building.\n\nThe building comes equipped with a \"labelling\" of its vertices with values in Z / \"n\" Z. Indeed, fixing a reference lattice \"L\", the label of \"M\" is given by\n\nfor \"k\" sufficiently large. The vertices of any (\"n\" – 1)-simplex in \"X\" have distinct labels, running through the whole of Z / \"n\" Z. Any simplicial automorphism φ of \"X\" defines a permutation π of Z / \"n\" Z such that label (φ(\"M\")) = π(label (\"M\")). In particular for \"g\" in \"GL\" (Q),\n\nThus \"g\" preserves labels if \"g\" lies in \"SL\"(Q).\n\nTits proved that any label-preserving automorphism of the affine building arises from an element of \"SL\"(Q). Since automorphisms of the building permute the labels, there is a natural homomorphism\n\nThe action of \"GL\"(Q) gives rise to an n-cycle τ. Other automorphisms of the building arise from outer automorphisms of \"SL\"(Q) associated with automorphisms of the Dynkin diagram. Taking the standard\nsymmetric bilinear form with orthonormal basis \"v\", the map sending a lattice to its dual lattice gives an automorphism whose square is the identity, giving the permutation σ that sends each label to its negative modulo \"n\". The image of the above homomorphism is generated by σ and τ and is isomorphic to the dihedral group \"D\" of order \"2n\"; when \"n\" = 3, it gives the whole of \"S\".\n\nIf \"E\" is a finite Galois extension of Q and the building is constructed from \"SL\"(\"E\") instead of \"SL\"(Q), the Galois group Gal (\"E\"/Q) will also act by automorphisms on the building.\n\nSpherical buildings arise in two quite different ways in connection with the affine building \"X\" for \"SL\"(Q):\n\nWhen \"L\" is an archimedean local field then on the building for the group \"SL\"(\"L\") an additional structure can be imposed of a building with complex multiplication. These were first introduced by Martin L. Brown (). These buildings arise when a quadratic extension of \"L\" acts on the vector space \"L\". These building with complex multiplication can be extended to any global field. They describe the action of the Hecke operators on Heegner points on the classical modular curve \"X\"(\"N\") as well as on the Drinfeld modular curve \"X\"(\"I\"). These buildings with complex multiplication are completely classified for the case of \"SL\"(\"L\") in \n\nTits proved that all irreducible spherical buildings (i.e. with finite Weyl group) of rank greater than 2 are associated to simple algebraic or classical groups.\nA similar result holds for irreducible affine buildings of dimension greater than two (their buildings \"at infinity\" are spherical of rank greater than two). In lower rank or dimension, there is no such classification. Indeed, each incidence structure gives a spherical building of rank 2 (see ); and Ballmann and Brin proved that every 2-dimensional simplicial complex in which the links of vertices are isomorphic to the flag complex of a finite projective plane has the structure of a building, not necessarily classical. Many 2-dimensional affine buildings have been constructed using hyperbolic reflection groups or other more exotic constructions connected with orbifolds.\n\nTits also proved that every time a building is described by a BN pair in a group, then in almost all cases the automorphisms of the building correspond to automorphisms of the group (see ).\n\nThe theory of buildings has important applications in several rather disparate fields. Besides the already mentioned connections with the structure of reductive algebraic groups over general and local fields, buildings are used to study their representations. The results of Tits on determination of a group by its building have deep connections with rigidity theorems of George Mostow and Grigory Margulis, and with Margulis arithmeticity.\n\nSpecial types of buildings are studied in discrete mathematics, and the idea of a geometric approach to characterizing simple groups proved very fruitful in the classification of finite simple groups. The theory of buildings of type more general than spherical or affine is still relatively undeveloped, but these generalized buildings have already found applications to construction of Kac–Moody groups in algebra, and to nonpositively curved manifolds and hyperbolic groups in topology and geometric group theory.\n\n\n"}
{"id": "17387312", "url": "https://en.wikipedia.org/wiki?curid=17387312", "title": "Cation-anion radius ratio", "text": "Cation-anion radius ratio\n\nIn condensed matter physics and inorganic chemistry the cation-anion radius ratio (also: radius ratio rule ) is the ratio of the ionic radius of the cation to the ionic radius of the anion in a cation-anion compound. This is simply given by formula_1.\n\nAccording to Pauling's rules for crystal structures, the allowed size of the cation for a given structure is determined by the critical radius ratio. If the cation is too small, then it will attract the anions into each other and they will collide hence the compound will be unstable due to anion-anion repulsion; this occurs when the radius ratio drops below 0.155.\n\nAt the stability limit the cation is touching all the anions and the anions are just touching at their edges (radius ratio = 0.155). For radius ratios greater than 0.155, the compound may be stable.\n\nThe table below gives the relation between radius ratio and coordination number, which may be obtained from a simple geometrical proof.\nThe radius ratio rule was first proposed by Gustav F. Hüttig in 1920. In 1926 Victor Goldschmidt extended the use to ionic lattices. \n\nFace-centered cubic\n"}
{"id": "28148660", "url": "https://en.wikipedia.org/wiki?curid=28148660", "title": "Circular prime", "text": "Circular prime\n\nA circular prime is a prime number with the property that the number generated at each intermediate step when cyclically permuting its (base 10) digits will be prime. For example, 1193 is a circular prime, since 1931, 9311 and 3119 all are also prime. A circular prime with at least two digits can only consist of combinations of the digits 1, 3, 7 or 9, because having 0, 2, 4, 6 or 8 as the last digit makes the number divisible by 2, and having 0 or 5 as the last digit makes it divisible by 5. The complete listing of the smallest representative prime from all known cycles of circular primes (The single-digit primes and repunits are the only members of their respective cycles) is 2, 3, 5, 7, R, 13, 17, 37, 79, 113, 197, 199, 337, 1193, 3779, 11939, 19937, 193939, 199933, R, R, R, R, R, R, R, and R, where R is a repunit prime with \"n\" digits. There are no other circular primes up to 10. A type of prime related to the circular primes are the permutable primes, which are a subset of the circular primes (every permutable prime is also a circular prime, but not necessarily vice versa).\n\nThe complete listing of the smallest representative prime from all known cycles of circular primes in base 12 is (using inverted two and three for ten and eleven, respectively)\n\nwhere R is a repunit prime in base 12 with \"n\" digits. There are no other circular primes in base 12 up to 12.\n\nIn base 2, only Mersenne primes can be circular primes, since any 0 permuted to the one's place results in an even number.\n\n"}
{"id": "987014", "url": "https://en.wikipedia.org/wiki?curid=987014", "title": "Combinatorial class", "text": "Combinatorial class\n\nIn mathematics, a combinatorial class is a countable set of mathematical objects, together with a size function mapping each object to a non-negative integer, such that there are finitely many objects of each size.\n\nThe \"counting sequence\" of a combinatorial class is the sequence of the numbers of elements of size \"i\" for \"i\" = 0, 1, 2, ...; it may also be described as a generating function that has these numbers as its coefficients. The counting sequences of combinatorial classes are the main subject of study of enumerative combinatorics. Two combinatorial classes are said to be isomorphic if they have the same numbers of objects of each size, or equivalently, if their counting sequences are the same. Frequently, once two combinatorial classes are known to be isomorphic, a bijective proof of this equivalence is sought; such a proof may be interpreted as showing that the objects in the two isomorphic classes are cryptomorphic to each other.\n\nFor instance, the triangulations of regular polygons (with size given by the number of sides of the polygon, and a fixed choice of polygon to triangulate for each size) and the set of unrooted binary plane trees (up to graph isomorphism, with a fixed ordering of the leaves, and with size given by the number of leaves) are both counted by the Catalan numbers, so they form isomorphic combinatorial classes. A bijective isomorphism in this case is given by planar graph duality: a triangulation can be transformed bijectively into a tree with a leaf for each polygon edge, an internal node for each triangle, and an edge for each two polygon edges or triangles that are adjacent to each other.\n\nThe theory of combinatorial species and its extension to analytic combinatorics provide a language for describing many important combinatorial classes, constructing new classes from combinations of previously defined ones, and automatically deriving their counting sequences. For example, two combinatorial classes may be combined by disjoint union, or by a Cartesian product construction in which the objects are ordered pairs of one object from each of two classes, and the size function is the sum of the sizes of each object in the pair. These operations respectively form the addition and multiplication operations of a semiring on the family of (isomorphism equivalence classes of) combinatorial classes, in which the zero object is the empty combinatorial class, and the unit is the class whose only object is the empty set.\n\nIn the study of permutation patterns, a combinatorial class of permutation classes, enumerated by permutation length, is called a Wilf class. The study of enumerations of specific permutation classes has turned up unexpected equivalences in counting sequences of seemingly unrelated permutation classes.\n"}
{"id": "5842560", "url": "https://en.wikipedia.org/wiki?curid=5842560", "title": "Conical function", "text": "Conical function\n\nIn mathematics, conical functions or Mehler functions are functions which can be expressed in terms of Legendre functions of the first and second kind,\nformula_1 and formula_2\n\nThe functions formula_1 were introduced by Gustav Ferdinand Mehler, in 1868, when expanding in series the distance of a point on the axis of a cone to a point located on the surface of the cone. Mehler used the notation formula_4 to represent these functions. He obtained integral representation and series of functions representations for them. He also established an addition theorem\nfor the conical functions. Carl Neumann obtained an expansion of the functions formula_4 in terms\nof the Legendre polynomials in 1881. Leonhardt introduced for the conical functions the equivalent of the spherical harmonics in 1882.\n\n"}
{"id": "2845570", "url": "https://en.wikipedia.org/wiki?curid=2845570", "title": "Data Authentication Algorithm", "text": "Data Authentication Algorithm\n\nThe Data Authentication Algorithm (DAA) is a former U.S. government standard for producing cryptographic message authentication codes. DAA is defined in FIPS PUB 113, which was withdrawn on September 1, 2008. The algorithm is not considered secure by today's standards.\n\nAccording to the standard, a code produced by the DAA is called a Data Authentication Code (DAC). The algorithm chain encrypts the data, with the last cipher block truncated and used as the DAC.\n\nThe DAA is equivalent to ISO/IEC 9797-1 MAC algorithm 1, or CBC-MAC, with DES as the underlying cipher, truncated to between 24 and 56 bits (inclusive).\n"}
{"id": "577983", "url": "https://en.wikipedia.org/wiki?curid=577983", "title": "David S. Johnson", "text": "David S. Johnson\n\nDavid Stifler Johnson (December 9, 1945 – March 8, 2016) was an American computer scientist specializing in algorithms and optimization. He was the head of the Algorithms and Optimization Department of AT&T Labs Research from 1988 to 2013, and was a visiting professor at Columbia University from 2014 to 2016. He was awarded the 2010 Knuth Prize.\n\nJohnson was born in 1945 in Washington, D.C.. He graduated summa cum laude from Amherst College in 1967, then earned his S.M. from MIT in 1968 and his Ph.D. from MIT in 1973. All three of his degrees are in mathematics. He was inducted as a Fellow of the Association for Computing Machinery in 1995, and as a member of the National Academy of Engineering in 2016.\n\nHe was the coauthor of \"Computers and Intractability: A Guide to the Theory of NP-Completeness\" () along with Michael Garey. As of March 9, 2016, his publications have been cited over 96,000 times, and he has an h-index of 78. Johnson died on March 8, 2016 at the age of 70.\n\n"}
{"id": "16667323", "url": "https://en.wikipedia.org/wiki?curid=16667323", "title": "Dedekind–MacNeille completion", "text": "Dedekind–MacNeille completion\n\nIn order-theoretic mathematics, the Dedekind–MacNeille completion of a partially ordered set (also called the completion by cuts or normal completion) is the smallest complete lattice that contains the given partial order. It is named after Holbrook Mann MacNeille whose 1937 paper first defined and constructed it, and after Richard Dedekind because its construction generalizes the Dedekind cuts used by Dedekind to construct the real numbers from the rational numbers.\n\nA partially ordered set consists of a set of elements together with a binary relation on pairs of elements that is reflexive ( for every \"x\"), transitive (if and then ), and antisymmetric (if both and hold, then ). The usual numeric orderings on the integers or real numbers satisfy these properties; however, unlike the orderings on the numbers, a partial order may have two elements that are \"incomparable\": neither nor holds. Another familiar example of a partial ordering is the inclusion ordering ⊆ on pairs of sets.\n\nIf is a partially ordered set, a \"completion\" of means a complete lattice with an order-embedding of into . The notion of a complete lattice means that every subset of elements of has infimum and supremum; this generalizes the analogous properties of the real numbers. The notion of an order-embedding enforces the requirements that distinct elements of must be mapped to distinct elements of , and that each pair of elements in has the same ordering in as they do in . The extended real number line (real numbers together with +∞ and −∞) is a completion in this sense of the rational numbers: the set of rational numbers {3, 3.1, 3.14, 3.141, 3.1415, 3.14159, ...} does not have a rational least upper bound, but in the real numbers it has the least upper bound .\n\nA given partially ordered set may have several different completions. For instance, one completion of any partially ordered set is the set of its downwardly closed subsets ordered by inclusion. is embedded in this (complete) lattice by mapping each element to the lower set of elements that are less than or equal to . The result is a distributive lattice and is used in Birkhoff's representation theorem. However, it may have many more elements than are needed to form a completion of . Among all possible lattice completions, the Dedekind–MacNeille completion is the smallest complete lattice with embedded in it.\n\nFor each subset of a partially ordered set , let denote the set of upper bounds of ; that is, an element of belongs to whenever is greater than or equal to every element in . Symmetrically, let denote the set of lower bounds of , the elements that are less than or equal to every element in . Then the Dedekind–MacNeille completion of consists of all subsets for which \nit is ordered by inclusion: in the completion if and only if as sets.\n\nAn element of embeds into the completion as its principal ideal, the set of elements less than or equal to . Then is the set of elements greater than or equal to , and , showing that is indeed a member of the completion. It is straightforward to verify that the mapping from to is an order-embedding.\n\nAn alternative definition of the Dedekind–MacNeille completion that more closely resembles the definition of a Dedekind cut is sometimes used. In a partially ordered set , define a \"cut\" to be a pair of sets for which and . If is a cut then \"A\" satisfies the equation , and conversely if then is a cut. Therefore, the set of cuts, partially ordered by inclusion on the lower set of the cut (or the reverse of the inclusion relation on the upper set), gives an equivalent definition of the Dedekind–MacNeille completion.\n\nWith the alternative definition, both the join and the meet operations of the complete lattice have symmetric descriptions: if are the cuts in any family of cuts, then the meet of these cuts is the cut where , and the join is the cut where .\n\nIf Q is the set of rational numbers, viewed as a totally ordered set with the usual numerical order, then each element of the Dedekind–MacNeille completion of Q may be viewed as a Dedekind cut, and the Dedekind–MacNeille completion of Q is the total ordering on the real numbers, together with the two additional values ±∞. The construction of the real numbers from the rational numbers is an example of the Dedekind completion of a totally ordered set, and the Dedekind–MacNeille completion generalizes this concept from total orders to partial orders.\n\nIf is an antichain (a set of elements no two of which are comparable) then the Dedekind–MacNeille completion of consists of itself together with two additional elements, a bottom element that is below every element in and a top element that is above every element in .\n\nIf is any finite set of objects, and is any finite set of unary attributes for the objects in , then one may form a partial order of height two in which the elements of the partial order are the objects and attributes, and in which when is an object that has attribute . For a partial order defined in this way, the Dedekind–MacNeille completion of is known as a concept lattice, and it plays a central role in the field of formal concept analysis.\n\nThe Dedekind–MacNeille completion is the smallest complete lattice with embedded in it, in the sense that, if is any lattice completion of , then the Dedekind–MacNeille completion is a partially ordered subset of . When is finite, its completion is also finite, and has the smallest number of elements among all finite complete lattices containing .\n\nThe partially ordered set is join-dense and meet-dense in the Dedekind–MacNeille completion; that is, every element of the completion is a join of some set of elements of , and is also the meet of some set of elements in . The Dedekind–MacNeille completion is characterized among completions of by this property.\n\nThe Dedekind–MacNeille completion of a Boolean algebra is a complete Boolean algebra; this result is known as the Glivenko–Stone theorem, after Valery Ivanovich Glivenko and Marshall Stone. Similarly, the Dedekind–MacNeille completion of a residuated lattice is a complete residuated lattice. However, the completion of a distributive lattice need not itself be distributive, and the completion of a modular lattice may not remain modular.\n\nThe Dedekind–MacNeille completion is self-dual: the completion of the dual of a partial order is the same as the dual of the completion.\n\nThe Dedekind–MacNeille completion of has the same order dimension as does itself.\n\nIn the category of partially ordered sets and monotonic functions between partially ordered sets, the complete lattices form the injective objects for order-embeddings, and the Dedekind–MacNeille completion of is the injective hull of .\n\nSeveral researchers have investigated algorithms for constructing the Dedekind–MacNeille completion of a finite partially ordered set. Because the Dedekind–MacNeille completion may be exponentially larger than the partial order it comes from, it is necessary to measure the time bounds for such algorithms both in terms of the number of elements of the input partial order, but also in terms of the number of elements of its completion, and sometimes also in terms of additional measures of the complexity of the input and output. The format in which the output lattice is represented may also affect the running time of its construction algorithms; for instance, if it is represented as a logical matrix specifying the result of a comparison between each pair of lattice elements, the output size is and this will be a lower bound on the time for a construction algorithm.\n\n describe an incremental algorithm, in which the input partial order is built up by adding one element at a time; at each step, the completion of the smaller partial order is expanded to form the completion of the larger partial order. In their method, the completion is represented by an explicit list of cuts. Each cut of the augmented partial order, except for the one whose two sets intersect in the new element, is either a cut from the previous partial order or is formed by adding the new element to one or the other side of a cut from the previous partial order, so their algorithm need only test pairs of sets of this form to determine which ones are cuts. The time for using their method to add a single element to the completion of a partial order is where is the width of the partial order, that is, the size of its largest antichain. Therefore, the time to compute the completion of a given partial order is .\n\nAs observe, the problem of listing all cuts in a partially ordered set can be formulated as a special case of a simpler problem, of listing all maximal antichains in a different partially ordered set. If is any partially ordered set, let be a partial order whose elements contain two copies of : for each element of , contains two elements and , with if and only if and . Then the cuts in correspond one-for-one with the maximal antichains in : the elements in the lower set of a cut correspond to the elements with subscript 0 in an antichain, and the elements in the upper set of a cut correspond to the elements with subscript 1 in an antichain. Jourdan et al. describe an algorithm for finding maximal antichains that, when applied to the problem of listing all cuts in , takes time , an improvement on the algorithm of when the width is small. Alternatively, a maximal antichain in is the same as a maximal independent set in the comparability graph of , or a maximal clique in the complement of the comparability graph, so algorithms for the clique problem or the independent set problem can also be applied to this version of the Dedekind–MacNeille completion problem.\n\nThe transitive reduction or covering graph of the Dedekind–MacNeille completion describes the order relation between its elements in a concise way:\neach neighbor of a cut must remove an element of the original partial order from either the upper or lower set of the cut, so each vertex has at most neighbors. Thus, the covering graph has vertices and at most neighbors, a number that may be much smaller than the entries in a matrix that specifies all pairwise comparisons between elements. show how to compute this covering graph efficiently; more generally, if is any family of sets, they show how to compute the covering graph of the lattice of unions of subsets of . In the case of the Dedekind–MacNeille lattice, may be taken as the family of complement sets of principal ideals, and the unions of subsets of are complements of the lower sets of cuts. The main idea for their algorithm is to generate unions of subsets of incrementally (for each set in , forming its union with all previously generated unions), represent the resulting family of sets in a trie, and use the trie representation to test certain candidate pairs of sets for adjacency in the covering relation; it takes time . In later work, the same authors showed that the algorithm could be made fully incremental (capable of adding elements to the partial order one at a time) with the same total time bound.\n\n"}
{"id": "9260", "url": "https://en.wikipedia.org/wiki?curid=9260", "title": "Equivalence class", "text": "Equivalence class\n\nIn mathematics, when the elements of some set have a notion of equivalence (formalized as an equivalence relation) defined on them, then one may naturally split the set into equivalence classes. These equivalence classes are constructed so that elements and belong to the same equivalence class if and only if and are equivalent.\n\nFormally, given a set and an equivalence relation on , the \"equivalence class\" of an element in is the set\n\nof elements which are equivalent to . It may be proven from the defining properties of \"equivalence relations\" that the equivalence classes form a partition of . This partition – the set of equivalence classes – is sometimes called the quotient set or the quotient space of by and is denoted by .\n\nWhen the set has some structure (such as a group operation or a topology) and the equivalence relation is defined in a manner suitably compatible with this structure, then the quotient set often inherits a similar structure from its parent set. Examples include quotient spaces in linear algebra, quotient spaces in topology, quotient groups, homogeneous spaces, quotient rings, quotient monoids, and quotient categories.\n\n\nAn equivalence relation is a binary relation satisfying three properties:\n\nThe equivalence class of an element is denoted and is defined as the set\n\nof elements that are related to by . An alternative notation can be used to denote the equivalence class of the element , specifically with respect to the equivalence relation . This is said to be the -equivalence class of .\n\nThe set of all equivalence classes in with respect to an equivalence relation is denoted as and called modulo (or the quotient set of by ). The surjective map formula_3 from onto , which maps each element to its equivalence class, is called the canonical surjection or the canonical projection map.\n\nWhen an element is chosen (often implicitly) in each equivalence class, this defines an injective map called a \"section\". If this section is denoted by , one has for every equivalence class . The element is called a representative of . Any element of a class may be chosen as a representative of the class, by choosing the section appropriately.\n\nSometimes, there is a section that is more \"natural\" than the other ones. In this case, the representatives are called \"canonical representatives\". For example, in modular arithmetic, consider the equivalence relation on the integers defined by if is a multiple of a given integer , called the \"modulus\". Each class contains a unique non-negative integer smaller than , and these integers are the canonical representatives. The class and its representative are more or less identified, as is witnessed by the fact that the notation may denote either the class or its canonical representative (which is the remainder of the division of by ).\n\nEvery element of is a member of the equivalence class . Every two equivalence classes and are either equal or disjoint. Therefore, the set of all equivalence classes of forms a partition of : every element of belongs to one and only one equivalence class. Conversely every partition of comes from an equivalence relation in this way, according to which if and only if and belong to the same set of the partition.\n\nIt follows from the properties of an equivalence relation that\n\nIn other words, if is an equivalence relation on a set , and and are two elements of , then these statements are equivalent:\n\n\nAn undirected graph may be associated to any symmetric relation on a set , where the vertices are the elements of , and two vertices and are joined if and only if . Among these graphs are the graphs of equivalence relations; they are characterized as the graphs such that the connected components are cliques.\n\nIf is an equivalence relation on , and is a property of elements of such that whenever , is true if is true, then the property is said to be an invariant of , or well-defined under the relation .\n\nA frequent particular case occurs when is a function from to another set ; if whenever , then is said to be \"class invariant under\" , or simply \"invariant under\" . This occurs, e.g. in the character theory of finite groups. Some authors use \"compatible with \" or just \"respects \" instead of \"invariant under \".\n\nAny function itself defines an equivalence relation on according to which if and only if . The equivalence class of is the set of all elements in which get mapped to , i.e. the class is the inverse image of . This equivalence relation is known as the kernel of .\n\nMore generally, a function may map equivalent arguments (under an equivalence relation on ) to equivalent values (under an equivalence relation on ). Such a function is a morphism of sets equipped with an equivalence relation.\n\nIn topology, a quotient space is a topological space formed on the set of equivalence classes of an equivalence relation on a topological space using the original space's topology to create the topology on the set of equivalence classes.\n\nIn abstract algebra, congruence relations on the underlying set of an algebra allow the algebra to induce an algebra on the equivalence classes of the relation, called a quotient algebra. In linear algebra, a quotient space is a vector space formed by taking a quotient group where the quotient homomorphism is a linear map. By extension, in abstract algebra, the term quotient space may be used for quotient modules, quotient rings, quotient groups, or any quotient algebra. However, the use of the term for the more general cases can as often be by analogy with the orbits of a group action.\n\nThe orbits of a group action on a set may be called the quotient space of the action on the set, particularly when the orbits of the group action are the right cosets of a subgroup of a group, which arise from the action of the subgroup on the group by left translations, or respectively the left cosets as orbits under right translation.\n\nA normal subgroup of a topological group, acting on the group by translation action, is a quotient space in the senses of topology, abstract algebra, and group actions simultaneously.\n\nAlthough the term can be used for any equivalence relation's set of equivalence classes, possibly with further structure, the intent of using the term is generally to compare that type of equivalence relation on a set either to an equivalence relation that induces some structure on the set of equivalence classes from a structure of the same kind on , or to the orbits of a group action. Both the sense of a structure preserved by an equivalence relation and the study of invariants under group actions lead to the definition of invariants of equivalence relations given above.\n\n\n\nThis material is basic and can be found in any text dealing with the fundamentals of proof technique, such as any of the following:\n"}
{"id": "9637", "url": "https://en.wikipedia.org/wiki?curid=9637", "title": "Euler–Maclaurin formula", "text": "Euler–Maclaurin formula\n\nIn mathematics, the Euler–Maclaurin formula provides a powerful connection between integrals and sums. It can be used to approximate integrals by finite sums, or conversely to evaluate finite sums and infinite series using integrals and the machinery of calculus. For example, many asymptotic expansions are derived from the formula, and Faulhaber's formula for the sum of powers is an immediate consequence.\n\nThe formula was discovered independently by Leonhard Euler and Colin Maclaurin around 1735 (and later generalized as Darboux's formula). Euler needed it to compute slowly converging infinite series while Maclaurin used it to calculate integrals.\n\nIf formula_1 and formula_2 are natural numbers and formula_3 is a complex or real valued continuous function for real numbers formula_4 in the interval formula_5 then the integral\n\ncan be approximated by the sum (or vice versa)\n\n(see rectangle method). The Euler–Maclaurin formula provides expressions for the difference between the sum and the integral in terms of the higher derivatives formula_8 evaluated at the end points of the interval, that is to say when formula_9 and formula_10\n\nExplicitly, for formula_11 a positive integer and a function formula_3 that is formula_11 times continuously differentiable in the interval formula_14 we have\n\nwhere formula_16 is the formula_17th Bernoulli number (with formula_18) and formula_19 is an error term which is normally small for suitable values of formula_11 and depends on formula_21 and formula_22\n\nThe formula is often written with the subscript taking only even values, since the odd Bernoulli numbers are zero except for formula_23 in which case we have\n\nor alternatively\n\nThe first few Bernoulli numbers of even index are:\nWe may write the Euler-Maclaurin formula then as\n\nThe formula is derived below using repeated integration by parts applied to successive intervals formula_28 for integers formula_29 The derivation uses the periodic Bernoulli functions, formula_30 which are defined in terms of the Bernoulli polynomials formula_31 for formula_32\n\nThe Bernoulli polynomials may be defined recursively by\n\nand the periodic Bernoulli functions are defined as\n\nwhere formula_35 denotes the largest integer that is not greater than formula_4 so that formula_37 always lies in the interval formula_38\n\nIt can be shown that formula_39 for all formula_40 so that except for formula_41 all the periodic Bernoulli functions are continuous. The functions formula_42 are sometimes written as formula_43\n\nThe remainder term formula_19 can be written as\n\nWhen formula_46 it can be shown that\n\nwhere formula_48 denotes the Riemann zeta function; one approach to prove this inequality is to obtain the Fourier series for the polynomials formula_49 The bound is achieved for even formula_17 when formula_4 is zero. The term formula_52 may be omitted for odd formula_17 but the proof in this case is more complex (see Lehmer). Using this inequality, the size of the remainder term can be estimated using\n\nWe can use the formula as a means of approximating a finite integral, with the following simple formula:\n\nwhere formula_56 is the number of points in the interval of integration from formula_57 to formula_58 and formula_59 is the distance between points so that formula_60 Note the series above is usually not convergent; indeed, often the terms will increase rapidly after a number of iterations. Thus, attention generally needs to be paid to the remainder term.\nThis may be viewed as an extension of the trapezoid rule by the inclusion of correction terms.\n\nThe Basel problem asks to determine the sum\n\nEuler computed this sum to 20 decimal places with only a few terms of the Euler–Maclaurin formula in 1735. This probably convinced him that the sum equals formula_62 which he proved in the same year. Parseval's identity for the Fourier series of formula_63 gives the same result.\n\nIf formula_64 is a polynomial and formula_11 is big enough, then the remainder term vanishes. For instance, if formula_66 we can choose formula_67 to obtain after simplification\n\n(see Faulhaber's formula).\n\nThe Euler–Maclaurin formula is also used for detailed error analysis in numerical quadrature. It explains the superior performance of the trapezoidal rule on smooth periodic functions and is used in certain extrapolation methods. Clenshaw–Curtis quadrature is essentially a change of variables to cast an arbitrary integral in terms of integrals of periodic functions where the Euler–Maclaurin approach is very accurate (in that particular case the Euler–Maclaurin formula takes the form of a discrete cosine transform). This technique is known as a periodizing transformation.\n\nIn the context of computing asymptotic expansions of sums and series, usually the most useful form of the Euler–Maclaurin formula is\n\nwhere formula_70 and formula_71 are integers. Often the expansion remains valid even after taking the limits formula_72 or formula_73 or both. In many cases the integral on the right-hand side can be evaluated in closed form in terms of elementary functions even though the sum on the left-hand side cannot. Then all the terms in the asymptotic series can be expressed in terms of elementary functions. For example,\n\nHere the left-hand side is equal to formula_75 namely the first-order polygamma function defined through formula_76 the gamma function formula_77 is equal to formula_78 if formula_79 is a positive integer. This results in an asymptotic expansion for formula_80 That expansion, in turn, serves as the starting point for one of the derivations of precise error estimates for Stirling's approximation of the factorial function.\n\nIf is an integer greater than 1 we have:\n\nCollecting the constants into a value of the Riemann zeta function, we can write an asymptotic expansion:\n\nFor equal to 2 this simplifies to\nor\n\nWe can also derive (from Equation 1 below) the perhaps not-so-useful formula:\n\nWhen is 1, we get the following for the so-called harmonic numbers:\n\nwhere formula_87 is the Euler constant,\n\nWe follow the argument given in Apostol.\n\nThe Bernoulli polynomials and the periodic Bernoulli functions for were introduced above.\n\nThe first several Bernoulli polynomials are\n\nThe values are the Bernoulli numbers. Notice that for we have\n\nFor ,\n\nThe functions agree with the Bernoulli polynomials on the interval and are periodic with period 1. Furthermore, except when , they are also continuous. Thus,\n\nLet be an integer, and consider the integral\n\nwhere\n\nIntegrating by parts, we get\n\nUsing formula_95, formula_96, and summing the above from \"k\" = 0 to \"k\" = \"n\" − 1, we get\n\nAdding (\"f\"(\"n\") − \"f\"(0))/2 to both sides and rearranging, we have\n\nThe last two terms therefore give the error when the integral is taken to approximate the sum.\n\nNext, consider\n\nwhere\n\nIntegrating by parts again, we get\n\nThen summing from \"k\" = 0 to \"k\" = \"n\" − 1, and then replacing the last integral in (1) with what we have thus shown to be equal to it, we have\n\nBy now the reader will have guessed that this process can be iterated. In this way we get a proof of the Euler–Maclaurin summation formula which can be formalized by mathematical induction, in which the induction step relies on integration by parts and on the identities for periodic Bernoulli functions.\n\n"}
{"id": "60012", "url": "https://en.wikipedia.org/wiki?curid=60012", "title": "Formal power series", "text": "Formal power series\n\nIn mathematics, a formal power series is a generalization of a polynomial, where the number of terms is allowed to be infinite; this implies giving up the possibility of replacing the variable in the polynomial with an arbitrary number. Thus a formal power series differs from a polynomial in that it may have infinitely many terms, and differs from a power series, whose variables can take on numerical values. One way to view a formal power series is as an infinite ordered sequence of numbers. In this case, the powers of the variable are used only to indicate the order of the coefficients, so that the coefficient of formula_1 is the fifth term in the sequence. In combinatorics, formal power series provide representations of numerical sequences and of multisets, and for instance allow concise expressions for recursively defined sequences regardless of whether the recursion can be explicitly solved; this is known as the method of generating functions. More generally, formal power series can include series with any finite number of variables, and with coefficients in an arbitrary ring.\n\nA formal power series can be loosely thought of as an object that is like a polynomial, but with infinitely many terms. Alternatively, for those familiar with power series (or Taylor series), one may think of a formal power series as a power series in which we ignore questions of convergence by not assuming that the variable \"X\" denotes any numerical value (not even an unknown value). For example, consider the series\n\nIf we studied this as a power series, its properties would include, for example, that its radius of convergence is 1. However, as a formal power series, we may ignore this completely; all that is relevant is the sequence of coefficients [1, −3, 5, −7, 9, −11, ...]. In other words, a formal power series is an object that just records a sequence of coefficients. It is perfectly acceptable to consider a formal power series with the factorials [1, 1, 2, 6, 24, 120, 720, 5040, … ] as coefficients, even though the corresponding power series diverges for any nonzero value of \"X\".\n\nArithmetic on formal power series is carried out by simply pretending that the series are polynomials. For example, if\n\nthen we add \"A\" and \"B\" term by term:\n\nWe can multiply formal power series, again just by treating them as polynomials (see in particular Cauchy product):\n\nNotice that each coefficient in the product \"AB\" only depends on a \"finite\" number of coefficients of \"A\" and \"B\". For example, the \"X\" term is given by\n\nFor this reason, one may multiply formal power series without worrying about the usual questions of absolute, conditional and uniform convergence which arise in dealing with power series in the setting of analysis.\n\nOnce we have defined multiplication for formal power series, we can define multiplicative inverses as follows. The multiplicative inverse of a formal power series \"A\" is a formal power series \"C\" such that \"AC\" = 1, provided that such a formal power series exists. It turns out that if \"A\" has a multiplicative inverse, it is unique, and we denote it by \"A\". Now we can define division of formal power series by defining \"B\"/\"A\" to be the product \"BA\", provided that the inverse of \"A\" exists. For example, one can use the definition of multiplication above to verify the familiar formula\n\nAn important operation on formal power series is coefficient extraction. In its most basic form, the coefficient extraction operator formula_8 applied to a formal power series formula_9 in one variable extracts the coefficient of the formula_10th power of the variable, so that formula_11 and formula_12. Other examples include\n\nSimilarly, many other operations that are carried out on polynomials can be extended to the formal power series setting, as explained below.\n\nThe set of all formal power series in \"X\" with coefficients in a commutative ring \"R\" form another ring that is written formula_14 and called the ring of formal power series in the variable \"X\" over \"R\".\n\nOne can characterize formula_15 abstractly as the completion of the polynomial ring formula_16 equipped with a particular metric. This automatically gives formula_15 the structure of a topological ring (and even of a complete metric space). But the general construction of a completion of a metric space is more involved than what is needed here, and would make formal power series seem more complicated than they are. \nIt is possible to describe formula_15 more explicitly, and define the ring structure and topological structure separately, as follows.\n\nAs a set, formula_15 can be constructed as the set formula_20 of all infinite sequences of elements of formula_21, indexed by the natural numbers (taken to include 0). Designating a sequence whose term at index formula_10 is formula_23 by formula_24, one defines addition of two such sequences by\n\nand multiplication by\n\nThis type of product is called the Cauchy product of the two sequences of coefficients, and is a sort of discrete convolution. With these operations, formula_20 becomes a commutative ring with zero element formula_28 and multiplicative identity formula_29.\n\nThe product is in fact the same one used to define the product of polynomials in one indeterminate, which suggests using a similar notation. One embeds formula_21 into formula_15 by sending any (constant) formula_32 to the sequence formula_33 and designates the sequence formula_34 by formula_35; then using the above definitions every sequence with only finitely many nonzero terms can be expressed in terms of these special elements as\n\nthese are precisely the polynomials in formula_35. Given this, it is quite natural and convenient to designate a general sequence formula_38 by the formal expression formula_39, even though the latter \"is not\" an expression formed by the operations of addition and multiplication defined above (from which only finite sums can be constructed). This notational convention allows reformulation of the above definitions as\n\nand\n\nwhich is quite convenient, but one must be aware of the distinction between formal summation (a mere convention) and actual addition.\n\nHaving stipulated conventionally that\n\none would like to interpret the right hand side as a well-defined infinite summation. To that end, a notion of convergence in formula_20 is defined and a topology on formula_20 is constructed. There are several equivalent ways to define the desired topology.\n\n\n\n\nInformally, two sequences formula_55 and formula_56 become closer and closer if and only if more and more of their terms agree exactly. Formally, the sequence of partial sums of some infinite summation converges if for every fixed power of formula_35 the coefficient stabilizes: there is a point beyond which all further partial sums have the same coefficient. This is clearly the case for the right hand side of (1), regardless of the values formula_23, since inclusion of the term for formula_59 gives the last (and in fact only) change to the coefficient of formula_60. It is also obvious that the limit of the sequence of partial sums is equal to the left hand side.\n\nThis topological structure, together with the ring operations described above, form a topological ring. This is called the ring of formal power series over formula_21 and is denoted by formula_15. The topology has the useful property that an infinite summation converges if and only if the sequence of its terms converges to 0, which just means that any fixed power of formula_35 occurs in only finitely many terms.\n\nThe topological structure allows much more flexible use of infinite summations. For instance the rule for multiplication can be restated simply as\n\nsince only finitely many terms on the right affect any fixed formula_60. Infinite products are also defined by the topological structure; it can be seen that an infinite product converges if and only if the sequence of its factors converges to 1.\n\nThe above topology is the finest topology for which \n\nalways converges as a summation to the formal power series designated by the same expression, and it often suffices to give a meaning to infinite sums and products, or other kinds of limits that one wishes to use to designate particular formal power series. It can however happen occasionally that one wishes to use a coarser topology, so that certain expressions become convergent that would otherwise diverge. This applies in particular when the base ring formula_21 already comes with a topology other than the discrete one, for instance if it is also a ring of formal power series.\n\nConsider the ring of formal power series: formula_68 then the topology of above construction only relates to the indeterminate formula_69, since the topology that was put on formula_70 has been replaced by the discrete topology when defining the topology of the whole ring. So\n\nconverges to the power series suggested, which can be written as formula_72; however the summation\n\nwould be considered to be divergent, since every term affects the coefficient of formula_69 (which coefficient is itself a power series in formula_35). This asymmetry disappears if the power series ring in formula_69 is given the product topology where each copy of formula_70 is given its topology as a ring of formal power series rather than the discrete topology. As a consequence, for convergence of a sequence of elements of formula_78 it then suffices that the coefficient of each power of formula_69 converges to a formal power series in formula_35, a weaker condition that stabilizing entirely; for instance in the second example given here the coefficient of formula_69converges to formula_82, so the whole summation converges to formula_83.\n\nThis way of defining the topology is in fact the standard one for repeated constructions of rings of formal power series, and gives the same topology as one would get by taking formal power series in all indeterminates at once. In the above example that would mean constructing formula_84 and here a sequence converges if and only if the coefficient of every monomial formula_85 stabilizes. This topology, which is also the formula_86-adic topology, where formula_87 is the ideal generated by formula_35 and formula_69, still enjoys the property that a summation converges if and only if its terms tend to 0.\n\nThe same principle could be used to make other divergent limits converge. For instance in formula_90 the limit\n\ndoes not exist, so in particular it does not converge to \n\nThis is because for formula_93 the coefficient formula_94 of formula_95 does not stabilize as formula_96. It does however converge in the usual topology of formula_97, and in fact to the coefficient formula_98 of formula_99. Therefore, if one would give formula_90 the product topology of formula_101 where the topology of formula_97 is the usual topology rather than the discrete one, then the above limit would converge to formula_99. This more permissive approach is not however the standard when considering formal power series, as it would lead to convergence considerations that are as subtle as they are in analysis, while the philosophy of formal power series is on the contrary to make convergence questions as trivial as they can possibly be. With this topology it would \"not\" be the case that a summation converges if and only if its terms tend to 0.\n\nThe ring formula_15 may be characterized by the following universal property. If formula_105 is a commutative associative algebra over formula_21, if formula_86 is an ideal of formula_105 such that the formula_86-adic topology on formula_105 is complete, and if formula_111 is an element of formula_86, then there is a \"unique\" formula_113 with the following properties:\n\n\n\n\nOne can perform algebraic operations on power series to generate new power series. Besides the ring structure operations defined above, we have the following.\n\nIf \"n\" is a natural number we have\n\nwhere:\n\nIn the case of formal power series with complex coefficients, the complex powers are well defined at least for series \"f\" with constant term equal to 1. In this case, formula_120 can be defined either by composition with the binomial series (1+\"x\"), or by composition with the exponential and the logarithmic series, formula_121 or as the solution of the differential equation formula_122 with constant term 1, the three definitions being equivalent. The rules of calculus formula_123 and formula_124 easily follow.\n\nThe series\n\nis invertible in formula_15 if and only if its constant coefficient formula_50 is invertible in formula_21. This condition is necessary, for the following reason: if we suppose that formula_9 has an inverse formula_130 then the constant term formula_131 of formula_132 is the constant term of the identity series, i.e., it is 1. This condition is also sufficient; we may compute the coefficients of the inverse series formula_133 via the explicit recursive formula\n\nAn important special case is that the geometric series formula is valid in formula_135:\n\nIf formula_137 is a field, then a series is invertible if and only if the constant term is non-zero, i.e., if and only if the series is not divisible by formula_35. This says that formula_135 is a discrete valuation ring with uniformizing parameter formula_35.\n\nThe computation of a quotient formula_141\n\nassuming the denominator is invertible (that is, formula_50 is invertible in the ring of scalars), can be performed as a product formula_144 and the inverse of formula_145, or directly equating the coefficients in formula_146:\n\nThe coefficient extraction operator applied to a formal power series \n\nin \"X\" is written\n\nand extracts the coefficient of \"X\", so that\n\nGiven formal power series\n\none may form the \"composition\"\n\nwhere the coefficients \"c\" are determined by \"expanding out\" the powers of \"f\"(\"X\"):\n\nHere the sum is extended over all (\"k\", \"j\") with formula_155 and formula_156 with formula_157\n\nA more explicit description of these coefficients is provided by Faà di Bruno's formula, at least in the case where the coefficient ring is a field of characteristic 0.\n\nA point here is that this operation is only valid when formula_158 has \"no constant term\", so that each formula_159 depends on only a finite number of coefficients of formula_158 and formula_161. In other word the series for formula_162 converges in the topology of formula_15.\n\nAssume that the ring formula_21 has characteristic 0. If we denote by formula_99 the formal power series\n\nthen the expression\n\nmakes perfect sense as a formal power series. However, the statement\n\nis not a valid application of the composition operation for formal power series. Rather, it is confusing the notions of convergence in formula_15 and convergence in formula_21; indeed, the ring formula_21 may not even contain any number formula_172 with the appropriate properties.\n\nWhenever a formal series \n\nhas \"f\" = 0 and \"f\" being an invertible element of \"R\", there exists a series \n\nthat is the composition inverse of formula_144, meaning that composing formula_144 with formula_145 gives the series representing the identity function (whose first coefficient is 1 and all other coefficients are zero). The coefficients of formula_145 may be found recursively by using the above formula for the coefficients of a composition, equating them with those of the composition identity \"X\" (that is 1 at degree 1 and 0 at every degree greater than 1). In the case when the coefficient ring is a field of characteristic 0, the Lagrange inversion formula provides a powerful tool to compute the coefficients of \"g\", as well as the coefficients of the (multiplicative) powers of \"g\".\n\nGiven a formal power series\n\nwe define its formal derivative, denoted \"Df\" or \"f\"′, by\n\nThe symbol \"D\" is called the formal differentiation operator. The motivation behind this definition is that it simply mimics term-by-term differentiation of a polynomial.\n\nThis operation is \"R\"-linear:\n\nfor any \"a\", \"b\" in \"R\" and any \"f\", \"g\" in formula_182 Additionally, the formal derivative has many of the properties of the usual derivative of calculus. For example, the product rule is valid:\n\nand the chain rule works as well:\n\nwhenever the appropriate compositions of series are defined (see above under composition of series).\n\nThus, in these respects formal power series behave like Taylor series. Indeed, for the \"f\" defined above, we find that\n\nwhere \"D\" denotes the \"k\"th formal derivative (that is, the result of formally differentiating \"k\" times).\n\nformula_15 is an associative algebra over formula_21 which contains the ring formula_16 of polynomials over formula_21; the polynomials correspond to the sequences which end in zeros.\n\nThe Jacobson radical of formula_15 is the ideal generated by formula_35 and the Jacobson radical of formula_21; this is implied by the element invertibility criterion discussed above.\n\nThe maximal ideals of formula_15 all arise from those in formula_21 in the following manner: an ideal formula_195 of formula_15 is maximal if and only if formula_197 is a maximal ideal of formula_21 and formula_195 is generated as an ideal by formula_35 and formula_197.\n\nSeveral algebraic properties of formula_21 are inherited by formula_15:\n\n\n\n\n\nThe metric space formula_212 is complete.\n\nThe ring formula_15 is compact if and only if \"R\" is finite. This follows from Tychonoff's theorem and the characterisation of the topology on formula_15 as a product topology.\n\nThe ring of formal power series with coefficients in a complete local ring satisfies the Weierstrass preparation theorem.\n\nFormal power series can be used to solve recurrences occurring in number theory and combinatorics. For an example involving finding a closed form expression for the Fibonacci numbers, see the article on Examples of generating functions.\n\nOne can use formal power series to prove several relations familiar from analysis in a purely algebraic setting. Consider for instance the following elements of formula_215:\n\nThen one can show that\n\nThe last one being valid in the ring formula_221\n\nFor \"K\" a field, the ring formula_222 is often used as the \"standard, most general\" complete local ring over \"K\" in algebra.\n\nIn mathematical analysis, every convergent power series defines a function with values in the real or complex numbers. Formal power series can also be interpreted as functions, but one has to be careful with the domain and codomain. Let \n\nand suppose \"S\" is a commutative associative algebra over \"R\", \"I\" is an ideal in \"S\" such that the I-adic topology on \"S\" is complete, and \"x\" is an element of \"I\". Define:\n\nThis series is guaranteed to converge in \"S\" given the above assumptions on \"x\". Furthermore, we have\n\nand\n\nUnlike in the case of bona fide functions, these formulas are not definitions but have to be proved.\n\nSince the topology on formula_15 is the (\"X\")-adic topology and formula_15 is complete, we can in particular apply power series to other power series, provided that the arguments don't have constant coefficients (so that they belong to the ideal (\"X\")): \"f\"(0), \"f\"(\"X\"−\"X\") and \"f\"((1−\"X\") − 1) are all well defined for any formal power series formula_229\n\nWith this formalism, we can give an explicit formula for the multiplicative inverse of a power series \"f\" whose constant coefficient \"a\" = \"f\"(0) is invertible in \"R\":\n\nIf the formal power series \"g\" with \"g\"(0) = 0 is given implicitly by the equation\n\nwhere \"f\" is a known power series with \"f\"(0) = 0, then the coefficients of \"g\" can be explicitly computed using the Lagrange inversion formula.\n\nA formal Laurent series over a ring formula_21 is defined in a similar way to a formal power series, except that we also allow finitely many terms of negative degree (this is different from the classical Laurent series), that is series of the form\n\nwhere formula_234 for all but finitely many negative indices formula_10. Multiplication of such series can be defined. Indeed, similarly to the definition for formal power series, the coefficient of \"X\" of two series with respective sequences of coefficients formula_55 and formula_56 is\n\nwhich sum is effectively finite because of the assumed vanishing of coefficients at sufficiently negative indices, and which sum zero for sufficiently negative formula_53 for the same reason.\n\nFor a non-zero formal Laurent series, the minimal integer formula_10 such that formula_241 is called the order of formula_144, denoted formula_243 (The order of the zero series is formula_244.) The formal Laurent series form the ring of formal Laurent series over formula_21, denoted by formula_246. It is equal to the localization of formula_15 with respect to the set of positive powers of formula_35. It is a topological ring with the metric:\n\nIf formula_137 is a field, then formula_251 is in fact a field, which may alternatively be obtained as the field of fractions of the integral domain formula_135.\n\nOne may define formal differentiation for formal Laurent series in a natural way (term-by-term). Precisely, the formal derivative of the formal Laurent series formula_144 above is\n\nwhich is again an element of formula_251. Notice that if formula_144 is a non-constant formal Laurent series, and K is a field of characteristic 0, then one has\n\nHowever, in general this is not the case since the factor \"n\" for the lowest order term could be equal to 0 in \"R\".\n\nAssume that formula_210 is a field of characteristic 0. Then the map\n\nis a formula_210-derivation that satisfies\n\nThe latter shows that the coefficient of formula_263 in formula_144 is of particular interest; it is called \"formal residue of formula_144\" and denoted formula_266. The map\n\nis formula_210-linear, and by the above observation one has an exact sequence\n\nSome rules of calculus. As a quite direct consequence of the above definition, and of the rules of formal derivation, one has, for any formula_270\n\nProperty (i) is part of the exact sequence above. Property (ii) follows from (i) as applied to formula_277. Property (iii): any formula_144 can be written in the form formula_279, with formula_280 and formula_281: then formula_282 formula_281 implies formula_145 is invertible in formula_285 whence formula_286 Property (iv): Since formula_287 we can write formula_288 with formula_289. Consequently, formula_290 and (iv) follows from (i) and (iii). Property (v) is clear from the definition.\n\nAs mentioned above, any formal series formula_291 with \"f\" = 0 and \"f\" ≠ 0 has a composition inverse formula_292 The following relation between the coefficients \"g\" and \"f\" holds (\"\"):\n\nIn particular, for \"n\" = 1 and all \"k\" ≥ 1,\n\nSince the proof of the Lagrange inversion formula is a very short computation, it is worth reporting it here. Since formula_295, by the above rules of calculus,\n\nGeneralizations. One may observe that the above computation can be repeated plainly in more general settings than \"K\"((\"X\")): a generalization of the Lagrange inversion formula is already available working in the formula_297-modules formula_298 where α is a complex exponent. As a consequence, if \"f\" and \"g\" are as above, with formula_299, we can relate the complex powers of \"f/X\" and \"g/X\": precisely, if α and β are non-zero complex numbers with negative integer sum, formula_300 then\n\nFor instance, this way one finds the power series for complex powers of the Lambert function.\n\nFormal power series in any number of indeterminates (even infinitely many) can be defined. If \"I\" is an index set and \"X\" is the set of indeterminates \"X\" for \"i\"∈\"I\", then a monomial \"X\" is any finite product of elements of \"X\" (repetitions allowed); a formal power series in \"X\" with coefficients in a ring \"R\" is determined by any mapping from the set of monomials \"X\" to a corresponding coefficient \"c\", and is denoted formula_302. The set of all such formal power series is denoted formula_303 and it is given a ring structure by defining\n\nand\n\nThe topology on formula_306 is such that a sequence of its elements converges only if for each monomial \"X\" the corresponding coefficient stabilizes. If \"I\" is finite, then this the \"J\"-adic topology, where \"J\" is the ideal of formula_306 generated by all the indeterminates in \"X\". This does not hold if \"I\" is infinite. For example, if formula_308 then the sequence formula_309 with formula_310 does not converge with respect to any \"J\"-adic topology on \"R\", but clearly for each monomial the corresponding coefficient stabilizes.\n\nAs remarked above, the topology on a repeated formal power series ring like formula_311 is usually chosen in such a way that it becomes isomorphic as a topological ring to formula_312\n\nAll of the operations defined for series in one variable may be extended to the several variables case.\n\n\n\nIn the case of the formal derivative, there are now separate partial derivative operators, which differentiate with respect to each of the indeterminates. They all commute with each other.\n\nIn the several variables case, the universal property characterizing formula_313 becomes the following. If \"S\" is a commutative associative algebra over \"R\", if \"I\" is an ideal of \"S\" such that the \"I\"-adic topology on \"S\" is complete, and if \"x\", ..., \"x\" are elements of \"I\", then there is a \"unique\" map formula_314 with the following properties:\n\n\n\n\nThe several variable case can be further generalised by taking \"non-commuting variables\" \"X\" for \"i\" ∈ \"I\", where \"I\" is an index set and then a monomial \"X\" is any word in the \"X\"; a formal power series in \"X\" with coefficients in a ring \"R\" is determined by any mapping from the set of monomials \"X\" to a corresponding coefficient \"c\", and is denoted formula_315. The set of all such formal power series is denoted \"R\"«\"X\"», and it is given a ring structure by defining addition pointwise\n\nand multiplication by\n\nwhere · denotes concatenation of words. These formal power series over \"R\" form the Magnus ring over \"R\".\n\nIn theoretical computer science, the following definition of a formal power series is given: let Σ be an alphabet (finite set) and \"S\" be a semiring. In this context, a formal power series is any mapping \"r\" from the set of strings generated by Σ (denoted as Σ) to the semiring \"S\". The values of such a mapping \"r\" are (somewhat idiosyncratically) denoted as (\"r\", \"w\") where \"w\" ∈ Σ. Then the mapping \"r\" itself is conventionally written as \n\nGiven this notation, the values (\"r\", \"w\") are also called the coefficients of the series. Similarly to the non-commuting [ring] case discussed in the section above this, the notation for the collection of all power series given a fixed alphabet and semiring is \"S\"《Σ》.\n\nSuppose formula_319 is an ordered abelian group, meaning an abelian group with a total ordering formula_320 respecting the group's addition, so that formula_321. Let I be a well-ordered subset of formula_319, meaning I contains no infinite descending chain. Consider the set consisting of\n\nfor all such I, with formula_324 in a commutative ring formula_21, where we assume that for any index set, if all of the formula_324 are zero then the sum is zero. Then formula_327 is the ring of formal power series on formula_319; because of the condition that the indexing set be well-ordered the product is well-defined, and we of course assume that two elements which differ by zero are the same. Sometimes the notation formula_329 is used to denote formula_327.\n\nVarious properties of formula_21 transfer to formula_327. If formula_21 is a field, then so is formula_327. If formula_21 is an ordered field, we can order formula_327 by setting any element to have the same sign as its leading coefficient, defined as the least element of the index set I associated to a non-zero coefficient. Finally if formula_319 is a divisible group and formula_21 is a real closed field, then formula_327 is a real closed field, and if formula_21 is algebraically closed, then so is formula_327.\n\nThis theory is due to Hans Hahn, who also showed that one obtains subfields when the number of (non-zero) terms is bounded by some fixed infinite cardinality.\n\n\n\n"}
{"id": "1955708", "url": "https://en.wikipedia.org/wiki?curid=1955708", "title": "Fredholm's theorem", "text": "Fredholm's theorem\n\nIn mathematics, Fredholm's theorems are a set of celebrated results of Ivar Fredholm in the Fredholm theory of integral equations. There are several closely related theorems, which may be stated in terms of integral equations, in terms of linear algebra, or in terms of the Fredholm operator on Banach spaces.\n\nThe Fredholm alternative is one of the Fredholm theorems.\n\nFredholm's theorem in linear algebra is as follows: if \"M\" is a matrix, then the orthogonal complement of the row space of \"M\" is the null space of \"M\":\n\nSimilarly, the orthogonal complement of the column space of \"M\" is the null space of the adjoint:\n\nFredholm's theorem for integral equations is expressed as follows. Let formula_3 be an integral kernel, and consider the homogeneous equations\n\nand its complex adjoint\n\nHere, formula_6 denotes the complex conjugate of the complex number formula_7, and similarly for formula_8. Then, Fredholm's theorem is that, for any fixed value of formula_7, these equations have either the trivial solution formula_10 or have the same number of linearly independent solutions formula_11, formula_12.\n\nA sufficient condition for this theorem to hold is for formula_3 to be square integrable on the rectangle formula_14 (where \"a\" and/or \"b\" may be minus or plus infinity).\n\nHere, the integral is expressed as a one-dimensional integral on the real number line. In Fredholm theory, this result generalizes to integral operators on multi-dimensional spaces, including, for example, Riemannian manifolds.\n\nOne of Fredholm's theorems, closely related to the Fredholm alternative, concerns the existence of solutions to the inhomogeneous Fredholm equation\n\nSolutions to this equation exist if and only if the function formula_16 is orthogonal to the complete set of solutions formula_17 of the corresponding homogeneous adjoint equation:\n\nwhere formula_19 is the complex conjugate of formula_20 and the former is one of the complete set of solutions to\n\nA sufficient condition for this theorem to hold is for formula_3 to be square integrable on the rectangle formula_14.\n\n"}
{"id": "10125619", "url": "https://en.wikipedia.org/wiki?curid=10125619", "title": "Fundamental matrix (linear differential equation)", "text": "Fundamental matrix (linear differential equation)\n\nIn mathematics, a fundamental matrix of a system of \"n\" homogeneous linear ordinary differential equations\n\nis a matrix-valued function formula_2 whose columns are linearly independent solutions of the system.\nThen every solution to the system can be written as formula_3, for some constant vector formula_4 (written as a column vector of height \"n\").\n\nOne can show that a matrix-valued function formula_5 is a fundamental matrix of formula_1 if and only if formula_7 and formula_5 is a non-singular matrix for all formula_9.\n\nThe fundamental matrix is used to express the state-transition matrix, an essential component in the solution of a system of linear ordinary differential equations.\n\n"}
{"id": "4027364", "url": "https://en.wikipedia.org/wiki?curid=4027364", "title": "Generalized polygon", "text": "Generalized polygon\n\nIn mathematics, a generalized polygon is an incidence structure introduced by Jacques Tits in 1959. Generalized \"n\"-gons encompass as special cases projective planes (generalized triangles, \"n\" = 3) and generalized quadrangles (\"n\" = 4). Many generalized polygons arise from groups of Lie type, but there are also exotic ones that cannot be obtained in this way. Generalized polygons satisfying a technical condition known as the \"Moufang property\" have been completely classified by Tits and Weiss. Every generalized \"n\"-gon with \"n\" even is also a near polygon.\n\nA generalized \"2\"-gon (or a digon) is an incidence structure with at least 2 points and 2 lines where each point is incident to each line.\n\nFor \"formula_1\" a generalized \"n\"-gon is an incidence structure (formula_2), where formula_3 is the set of points, formula_4 is the set of lines and formula_5 is the incidence relation, such that:\n\nAn equivalent but sometimes simpler way to express these conditions is: consider the bipartite \"incidence graph\" with the vertex set formula_10 and the edges connecting the incident pairs of points and lines.\n\nFrom this it should be clear that the incidence graphs of generalized polygons are Moore graphs.\n\nA generalized polygon is of order \"(s,t)\" if:\n\nWe say a generalized polygon is thick if every point (line) is incident with at least three lines (points). All thick generalized polygons have an order.\n\nThe dual of a generalized \"n\"-gon (formula_2), is the incidence structure with notion of points and lines reversed and the incidence relation taken to be the converse relation of formula_14. It can easily be shown that this is again a generalized \"n\"-gon.\n\n\nWalter Feit and Graham Higman proved that \"finite\" generalized \"n\"-gons of order (\"s\", \"t\") with\n\"s\" ≥ 2, \"t\" ≥ 2 can exist only for the following values of \"n\":\n\nGeneralized \"n\"-gons for these values are referred to as generalized digons, triangles, quadrangles, hexagons and octagons.\n\nWhen Feit-Higman theorem is combined with the Haemers-Roos inequalities, we get the following restrictions,\n\n\nEvery known finite generalized hexagon of order (\"s\", \"t\") for \"s\", \"t\" > 1 has order \nwhere \"q\" is a prime power.\n\nEvery known finite generalized octagon of order (\"s\", \"t\") for \"s\", \"t\" > 1 has order\nwhere \"q\" is an odd power of 2.\n\nIf \"s\" and \"t\" are both infinite then generalized polygons exist for each \"n\" greater or equal to 2. It is unknown whether or not there exist generalized polygons with one of the parameters finite (and bigger than \"1\") while the other infinite (these cases are called \"semi-finite\"). Peter Cameron proved the non-existence of semi-finite generalized quadrangles with three points on each line, while Andries Brouwer and Bill Kantor independently proved the case of four points on each line. The non-existence result for five points on each line was proved by G. Cherlin using Model Theory. No such results are known without making any further assumptions for generalized hexagons or octagons, even for the smallest case of three points on each line.\n\nAs noted before the incidence graphs of generalized polygons have important properties. For example, every generalized \"n\"-gon of order \"(s,s)\" is a \"(s+1,2n)\" cage. They are also related to expander graphs as they have nice expansion properties. Several classes of extremal expander graphs are obtained from generalized polygons. In Ramsey theory, graphs constructed using generalized polygons give us some of the best known constructive lower bounds on offdiagonal Ramsey numbers.\n\n\n\n"}
{"id": "33222076", "url": "https://en.wikipedia.org/wiki?curid=33222076", "title": "Hidden algebra", "text": "Hidden algebra\n\nHidden algebra provides a formal semantics for use in the field of software engineering, especially for concurrent distributed object systems. It supports correctness proofs.\n\nHidden algebra was studied by Joseph Goguen. It handles features of large software-based systems, including concurrency, distribution, nondeterminism, and local states. It also handled object-oriented features like classes, subclasses (inheritance), attributes, and methods. Hidden algebra generalizes process algebra and transition system approaches.\n\n"}
{"id": "44408374", "url": "https://en.wikipedia.org/wiki?curid=44408374", "title": "Human disease network", "text": "Human disease network\n\nA human disease network is a network of human disorders and diseases with reference to their genetic origins or other features. More specifically, it is the map of human disease associations referring mostly to disease genes. For example, in a human disease network, two diseases are linked if they share at least one associated gene. A typical human disease network usually derives from bipartite networks which consist of both diseases and genes information. Additionally, some human disease networks use other features such as symptoms and proteins to associate diseases.\n\nIn 2007, Goh et al. constructed a disease-gene bipartite graph using information from OMIM database and termed human disease network. In 2009, Barrenas et al. derived complex disease-gene network using GWAs (Genome Wide Association studies). In the same year, Hidalgo et al. published a novel way of building human phenotypic disease networks in which diseases were connected according to their calculated distance. In 2011, Cusick et al. summarized studies on genotype-phenotype associations in cellular context. In 2014, Zhou, et al. built a symptom-based human disease network by mining biomedical literature database.\n\nA large-scale human disease network shows scale-free property. The degree distribution follows a power law suggesting that only a few diseases connect to a large number of diseases, whereas most diseases have few links to others. \nSuch network also shows a clustering tendency by disease classes.\n\nIn a symptom-based disease network, disease are also clustered according to their categories. Moreover, diseases sharing the same symptom are more likely to share the same genes and protein interactions.\n\n\n"}
{"id": "3459767", "url": "https://en.wikipedia.org/wiki?curid=3459767", "title": "Hyperplane section", "text": "Hyperplane section\n\nIn mathematics, a hyperplane section of a subset \"X\" of projective space P is the intersection of \"X\" with some hyperplane \"H\". In other words, we look at the subset \"X\" of those elements \"x\" of \"X\" that satisfy the single linear condition \"L\" = 0 defining \"H\" as a linear subspace. Here \"L\" or \"H\" can range over the dual projective space of non-zero linear forms in the homogeneous coordinates, up to scalar multiplication.\n\nFrom a geometrical point of view, the most interesting case is when \"X\" is an algebraic subvariety; for more general cases, in mathematical analysis, some analogue of the Radon transform applies. In algebraic geometry, assuming therefore that \"X\" is \"V\", a subvariety not lying completely in any \"H\", the hyperplane sections are algebraic sets with irreducible components all of dimension dim(\"V\") − 1. What more can be said is addressed by a collection of results known collectively as Bertini's theorem. The topology of hyperplane sections is studied in the topic of the Lefschetz hyperplane theorem and its refinements. Because the dimension drops by one in taking hyperplane sections, the process is potentially an inductive method for understanding varieties of higher dimension. A basic tool for that is the Lefschetz pencil.\n"}
{"id": "12972835", "url": "https://en.wikipedia.org/wiki?curid=12972835", "title": "Irmgard Flügge-Lotz", "text": "Irmgard Flügge-Lotz\n\nIrmgard Flügge-Lotz, née Lotz (16 July 1903 – 22 May 1974) was a German-American mathematician, aerospace engineer, and control theorist. She was a pioneer in the development of the theory of discontinuous automatic control, which has found wide application in hysteresis control systems; such applications include guidance systems, electronics, fire-control systems, and temperature regulation. She became the first female engineering professor at Stanford University in 1961 and the first female engineer elected a Fellow of the American Institute of Aeronautics and Astronautics.\n\nLotz was born in Hamelin, Germany on 16 July 1903. She was encouraged at an early age to pursue technical subjects by her mother, whose family had been involved in construction for several generations. She often visited construction sites with her uncle and attended half-price matinee shows for technical films. After her father, Osark, a travelling journalist, was drafted for military service in World War I, the young Irmgard helped the family by becoming a math tutor while studying at a girls' Gymnasium in Hanover. Several years later, when Osark returned to Hanover he was in poor health and Irmgard continued to work to bring in extra money for the family. She graduated from the Gymnasium in 1923 and entered the University of Hanover to study mathematics and engineering. Later in life, she explained why she decided to study engineering:\nIn college she studied applied mathematics and fluid dynamics and was often the only woman in her classes. In 1927, she received her Diplom-Ingenieur and remained in Hanover for her doctorate. In 1929 she earned her doctorate in engineering, publishing her thesis on the mathematical theory of circular cylinders and heat conduction.\n\nLotz went to work for the Aerodynamische Versuchsanstalt (AVA) in Göttingen, one of the most prominent aeronautical research institutions in Europe. She joined as a junior research engineer and worked closely with Ludwig Prandtl and Albert Betz, two of the leading German aerodynamicists of the time. Prior to her arrival at the AVA, Prandtl had been unsuccessfully working on solving a differential equation for his lifting-line theory for the spanwise lift distribution of an airplane wing. Lotz was able to overcome his difficulties and solve the equation, and additionally developed a relatively simple method for practical use. She published what is now known as the \"Lotz method\" in 1931 for calculating the lift on a three-dimensional wing, and it became a standard technique used internationally. Following this achievement, she was promoted to team leader and built the theoretical department at the AVA by establishing her own research program and assisting other research groups.\n\nIn 1932, she met Wilhelm Flügge, who was a civil engineer and privatdozent at the University of Göttingen. As they prepared to marry, Lotz's career progressed well and by the time they married in 1938 she had been appointed Head of the Department of Theoretical Aerodynamics. However, Flügge was branded \"politically unreliable\" and denied promotion at Göttingen for his anti-Nazi views. Flügge later recalled that while he was denied because of his political views, Lotz was \"blocked from any possibility of ever getting into a university career, just because of being a woman\". The escalation and increasing influence of Nazi policies on academia led to their departure from the AVA and they moved to the Deutsche Versuchsanstalt für Luftfahrt (DVL) in Berlin where Flügge-Lotz (her married name) was a consultant in aerodynamics and flight dynamics and Flügge was appointed Chief of Structures Research. Although banned from academic positions due to Nazi policies, they were permitted to continue their research activities under the protection of Hermann Göring, who was more concerned with technical expertise than ideological purity.\n\nAt DVL Flügge-Lotz began her career in automatic control theory and pioneered the theory of discontinuous control systems. These control systems, also known as \"on-off\" and \"bang-bang\" systems, have only two or three input settings and are simple to manufacture and very reliable in practical application. She was mainly interested in the implications these systems had for the development of simple automatic flight control equipment. However, the theory describing their performance needed to be developed before they could be reliably implemented in physical systems. Flügge-Lotz began developing the theory while at DVL, but wartime priorities limited her time for heavily theoretical projects so she focused mainly on aerodynamics during this time.\n\nAs World War II progressed, Berlin was increasingly subject to bombing raids by the Allies. In the spring of 1944, the destruction of Berlin had progressed so far that Flügge-Lotz and Flügge moved with their departments to the small town Saulgau in the hills of southern Germany. After the end of the war, Saulgau was in the French zone of Allied-occupied Germany. The French relaunched their aeronautical research activity and were eager to hire German scientists, so in 1947 Flügge-Lotz and Flügge moved with many of their colleagues to Paris to join the Office National d'Etudes et de Recherches Aerospatiales (ONERA). Flügge-Lotz served as Chief of a research group in aerodynamics until 1948 and published papers in both automatic control theory and aerodynamics, in which she discussed the problems arising from the increased speed of aircraft.\n\nAlthough Flügge-Lotz and her husband were happy living in Paris, the positions they held there provided limited opportunity for advancement. They wrote to Stephen Timoshenko at Stanford University casually asking about working in the United States and in 1948, and both received offers to teach there. However, at the time Stanford held a university policy that husband and wife could not hold professional rank in the same department, and despite Flügge-Lotz's reputation in research, she had to accept the relatively minor position of \"lecturer\" as her husband became professor.\n\nDespite lacking of a professorial title, she immediately began accepting students for PhD dissertation research in aerodynamic theory, and in the spring of 1949, taught her first Stanford course in boundary layer theory. At Stanford, Flügge-Lotz conducted research in numerical methods to solve boundary layer problems in fluid dynamics, making pioneering contributions with finite difference methods and the use of computers. In 1951 she set up a weekly fluid mechanics seminar for first-year graduate students to provide a forum for discussing the latest ideas and developments.\n\nIn addition to fluid mechanics, Flügge-Lotz returned to her work on automatic control theory initially started at DVL. She developed new courses and began advising student theses on the subject. She published the first textbook on discontinuous automatic control in 1953. A reviewer of her textbook wrote that:\nSince automatic control devices often found application in electronics, she also began collaborating with faculty and students in the Department of Electrical Engineering. Over time, her primary research efforts went increasingly into control theory, and in 1968, the year of her retirement, she published her second book, \"Discontinuous and Optimal Control\".\n\nBy the mid-1950s, it became evident that Flügge-Lotz was performing all the duties of a full Professor but without official recognition. In fact, it was hard for students to understand why she was a Lecturer rather than a Professor, or even what the difference meant. The disparity of her status as a Lecturer became more apparent when she was the only female delegate from the United States at the first Congress of the International Federation of Automatic Control in Moscow. To address the issue before school opened for the fall quarter, she was appointed a full Professor in both Engineering Mechanics and in Aeronautics and Astronautics in 1961.\n\nFlügge-Lotz retired in 1968 at age 65, but continued to conduct research on satellite control systems, heat transfer, and high-speed vehicle drag. During her lifetime, she received many honors for her work. In 1970, she was elected a Fellow of the American Institute of Aeronautics and Astronautics (AIAA) and chosen to give the von Kármán lecture to the AIAA in 1971. She received the Achievement Award by the Society of Women Engineers in 1970, and received an honorary doctorate from the University of Maryland in 1973. The citation for her honorary degree stated:\n\nShe was also a senior member of the Institute of Electrical and Electronic Engineers (IEEE), a member of Sigma Xi, and a member of the advisory boards of several scientific journals.\n\nIn honor of her contributions, the “Wilhelm Flügge and Irmgard Flügge-Lotz Memorial Award\" was established by the Applied Mechanics Devision at Stanford University for outstanding graduate students.\n\nFlügge-Lotz's health deteriorated after her retirement and she suffered increasingly severe pain from arthritis that spread over her body. On May 22, 1974, Flügge-Lotz died in Stanford Hospital after a long illness.\n\n\n\n\n"}
{"id": "58419310", "url": "https://en.wikipedia.org/wiki?curid=58419310", "title": "Joe P. Buhler", "text": "Joe P. Buhler\n\nJoe Peter Buhler (born 1950 in Vancouver, Washington) is an American mathematician.\n\nBuhler received his undergraduate degree from Reed College in 1972, and his Ph.D. from Harvard University in 1977 with thesis \"Icosahedral Galois Representations\" and thesis advisor John Tate. Buhler was a professor at Reed College in Portland, Oregon from 1980 until his retirement in 2005.\n\nIn 1997, he, Zinovy Reichstein, introduced the concept of essential dimension.\n\nBuhler is involved in a project to numerically verify the Kummer–Vandiver conjecture of Harry Vandiver and Ernst Eduard Kummer concerning the class number of cyclotomic fields. Vandiver proved it with a desk calculator up to class number 600, Derrick Lehmer (in the late 1940s) to about 5000, and Buhler with colleagues (in 2001) to 12 million. He continues the project with David Harvey and others.\n\nBuhler's research deals with algorithmic algebraic number theory, algebra, and cryptography.\n\nHe was elected a Fellow of the American Mathematical Society in 2012.\n\n"}
{"id": "1030427", "url": "https://en.wikipedia.org/wiki?curid=1030427", "title": "Key generator", "text": "Key generator\n\nA key generator is used in many cryptographic protocols to generate a sequence with many pseudo-random characteristics. This sequence is used as an encryption key at one end of communication, and as a decryption key at the other.\n\nExamples of key generators include linear feedback shift registers (LFSR) and the Solitaire (or Pontifex) cipher.\n"}
{"id": "2506864", "url": "https://en.wikipedia.org/wiki?curid=2506864", "title": "List of fractals by Hausdorff dimension", "text": "List of fractals by Hausdorff dimension\n\nBenoit Mandelbrot has stated that \"A fractal is by definition a set for which the Hausdorff-Besicovitch dimension strictly exceeds the topological dimension.\"\nPresented here is a list of fractals ordered by increasing Hausdorff dimension, with the purpose of visualizing what it means for a fractal to have a low or a high dimension.\n\n\n\n"}
{"id": "341482", "url": "https://en.wikipedia.org/wiki?curid=341482", "title": "Löwenheim–Skolem theorem", "text": "Löwenheim–Skolem theorem\n\nIn mathematical logic, the Löwenheim–Skolem theorem, named for Leopold Löwenheim and Thoralf Skolem, states that if a countable first-order theory has an infinite model, then for every infinite cardinal number κ it has a model of size κ. The result implies that first-order theories are unable to control the cardinality of their infinite models, and that no first-order theory with an infinite model can have a unique model up to isomorphism.\n\nThe (downward) Löwenheim–Skolem theorem is one of the two key properties, along with the compactness theorem, that are used in Lindström's theorem to characterize first-order logic. In general, the Löwenheim–Skolem theorem does not hold in stronger logics such as second-order logic.\n\nA signature consists of a set of function symbols \"S\", a set of relation symbols \"S\", and a function formula_1 representing the arity of function and relation symbols. (A nullary function symbol is called a constant symbol.) In the context of first-order logic, a signature is sometimes called a language. It is called countable if the set of function and relation symbols in it is countable, and in general the cardinality of a signature is the cardinality of the set of all the symbols it contains.\n\nA first-order theory consists of a fixed signature and a fixed set of sentences (formulas with no free variables) in that signature. Theories are often specified by giving a list of axioms that generate the theory, or by giving a structure and taking the theory to consist of the sentences satisfied by the structure.\n\nGiven a signature σ, a σ-structure \"M\"\nis a concrete interpretation of the symbols in σ. It consists of an underlying set (often also denoted by \"M\") together with an interpretation of the function and relation symbols of σ. An interpretation of a constant symbol of σ in \"M\" is simply an element of \"M\". More generally, an interpretation of an \"n\"-ary function symbol \"f\" is a function from \"M\" to \"M\". Similarly, an interpretation of a relation symbol \"R\" is an \"n\"-ary relation on \"M\", i.e. a subset of \"M\".\n\nA substructure of a σ-structure \"M\" is obtained by taking a subset \"N\" of \"M\" which is closed under the interpretations of all the function symbols in σ (hence includes the interpretations of all constant symbols in σ), and then restricting the interpretations of the relation symbols to \"N\". An elementary substructure is a very special case of this; in particular an elementary substructure satisfies exactly the same first-order sentences as the original structure (its elementary extension).\n\nThe modern statement of the theorem is both more general and stronger than the version for countable signatures stated in the introduction.\n\nIn its general form, the Löwenheim–Skolem Theorem states that for every signature σ, every infinite σ-structure \"M\" and every infinite cardinal number κ ≥ |σ|, there is a σ-structure \"N\" such that |\"N\"| = κ and\n\nThe theorem is often divided into two parts corresponding to the two bullets above. The part of the theorem asserting that a structure has elementary substructures of all smaller infinite cardinalities is known as the downward Löwenheim–Skolem Theorem. The part of the theorem asserting that a structure has elementary extensions of all larger cardinalities is known as the upward Löwenheim–Skolem Theorem.\n\nThe statement given in the introduction follows immediately by taking \"M\" to be an infinite model of the theory. The proof of the upward part of the theorem also shows that a theory with arbitrarily large finite models must have an infinite model; sometimes this is considered to be part of the theorem. For historical variants of the theorem, see the notes below.\n\nLet N denote the natural numbers and R the reals. It follows from the theorem that the theory of (N, +, ×, 0, 1) (the theory of true first-order arithmetic) has uncountable models, and that the theory of (R, +, ×, 0, 1) (the theory of real closed fields) has a countable model. There are, of course, axiomatizations characterizing (N, +, ×, 0, 1) and (R, +, ×, 0, 1) up to isomorphism. The Löwenheim–Skolem theorem shows that these axiomatizations cannot be first-order. For example, the completeness of a linear order, which is used to characterize the real numbers as a complete ordered field, is a non-first-order property.\n\nA theory is called categorical if it has only one model, up to isomorphism. This term was introduced by , and for some time thereafter mathematicians hoped they could put mathematics on a solid foundation by describing a categorical first-order theory of some version of set theory. The Löwenheim–Skolem theorem dealt a first blow to this hope, as it implies that a first-order theory which has an infinite model cannot be categorical. Later, in 1931, the hope was shattered completely by Gödel's incompleteness theorem.\n\nMany consequences of the Löwenheim–Skolem theorem seemed counterintuitive to logicians in the early 20th century, as the distinction between first-order and non-first-order properties was not yet understood. One such consequence is the existence of uncountable models of true arithmetic, which satisfy every first-order induction axiom but have non-inductive subsets. Another consequence that was considered particularly troubling is the existence of a countable model of set theory, which nevertheless must satisfy the sentence saying the real numbers are uncountable. This counterintuitive situation came to be known as Skolem's paradox; it shows that the notion of countability is not absolute.\n\nFor each first-order formula_2-formula formula_3 the axiom of choice implies the existence of a function\n\nsuch that, for all formula_5, either\n\nor\n\nApplying the axiom of choice again we get a function from the first order formulas formula_8 to such functions formula_9\n\nThe family of functions formula_10 gives rise to a preclosure operator formula_11 on the power set of formula_12\n\nfor formula_14\n\nIterating formula_11 countably many times results in a closure operator formula_16 Taking an arbitrary subset formula_17 such that formula_18, and having defined formula_19 one can see that also formula_20 formula_21 is an elementary substructure of formula_12 by the Tarski–Vaught test.\n\nThe trick used in this proof is essentially due to Skolem, who introduced function symbols for the Skolem functions formula_10 into the language. One could also define the formula_10 as partial functions such that formula_10 is defined if and only if formula_26 The only important point is that formula_11 is a preclosure operator such that formula_28 contains a solution for every formula with parameters in formula_29 which has a solution in formula_12 and that \n\nFirst, one extends the signature by adding a new constant symbol for every element of \"M\". The complete theory of \"M\" for the extended signature σ' is called the \"elementary diagram\" of \"M\". In the next step one adds κ many new constant symbols to the signature and adds to the elementary diagram of \"M\" the sentences \"c\" ≠ \"c<nowiki>'</nowiki>\" for any two distinct new constant symbols \"c\" and \"c<nowiki>'</nowiki>\". Using the compactness theorem, the resulting theory is easily seen to be consistent. Since its models must have cardinality at least κ, the downward part of this theorem guarantees the existence of a model \"N\" which has cardinality exactly κ. It contains an isomorphic copy of \"M\" as an elementary substructure.\n\nThis account is based mainly on . To understand the early history of model theory one must distinguish between \"syntactical consistency\" (no contradiction can be derived using the deduction rules for first-order logic) and \"satisfiability\" (there is a model). Somewhat surprisingly, even before the completeness theorem made the distinction unnecessary, the term \"consistent\" was used sometimes in one sense and sometimes in the other.\n\nThe first significant result in what later became model theory was \"Löwenheim's theorem\" in Leopold Löwenheim's publication \"Über Möglichkeiten im Relativkalkül\" (1915):\n\nLöwenheim's paper was actually concerned with the more general Peirce–Schröder \"calculus of relatives\" (relation algebra with quantifiers). He also used the now antiquated notations of Ernst Schröder. For a summary of the paper in English and using modern notations see .\n\nAccording to the received historical view, Löwenheim's proof was faulty because it implicitly used Kőnig's lemma without proving it, although the lemma was not yet a published result at the time. In a revisionist account, considers that Löwenheim's proof was complete.\n\nIt is somewhat ironic that Skolem's name is connected with the upward direction of the theorem as well as with the downward direction:\n\nThe Löwenheim–Skolem theorem is treated in all introductory texts on model theory or mathematical logic.\n\n\n\n"}
{"id": "30320703", "url": "https://en.wikipedia.org/wiki?curid=30320703", "title": "Mathematics Made Difficult", "text": "Mathematics Made Difficult\n\nMathematics Made Difficult is a book by Carl E. Linderholm that uses advanced mathematical methods to prove results normally shown using elementary proofs. Although the aim is largely satirical, it also shows the non-trivial mathematics behind operations normally considered obvious, such as numbering, counting, and factoring integers.\n\nAs an example, the proof that two is a prime number starts:\n\nIt is easily seen that the only numbers between 0 and 2, including 0 but excluding 2, are 0 and 1. Thus the remainder left by any number on division by 2 is either 0 or 1. Hence the quotient ring \"Z\"/2\"Z\", where 2\"Z\" is the ideal in Z generated by 2, has only the elements [0] and [1], where these are the images of 0 and 1 under the canonical quotient map. Since [1] must be the unit of this ring, every element of this ring except [0] is a unit, and the ring is a field ...\n"}
{"id": "27567102", "url": "https://en.wikipedia.org/wiki?curid=27567102", "title": "Matrix difference equation", "text": "Matrix difference equation\n\nA matrix difference equation is a difference equation in which the value of a vector (or sometimes, a matrix) of variables at one point in time is related to its own value at one or more previous points in time, using matrices. The order of the equation is the maximum time gap between any two indicated values of the variable vector. For example,\n\nis an example of a second-order matrix difference equation, in which \"x\" is an \"n\" × 1 vector of variables and \"A\" and \"B\" are \"n×n\" matrices. This equation is homogeneous because there is no vector constant term added to the end of the equation. The same equation might also be written as \n\nor as \n\nThe most commonly encountered matrix difference equations are first-order.\n\nAn example of a non-homogeneous first-order matrix difference equation is\n\nwith additive constant vector \"b\". The steady state of this system is a value \"x*\" of the vector \"x\" which, if reached, would not be deviated from subsequently. \"x*\" is found by setting formula_5 in the difference equation and solving for \"x*\" to obtain\n\nwhere formula_7 is the \"n×n\" identity matrix, and where it is assumed that formula_8 is invertible. Then the non-homogeneous equation can be rewritten in homogeneous form in terms of deviations from the steady state:\n\nThe first-order matrix difference equation [\"x\" - \"x\"*] = \"A\"[\"x\"-\"x\"*] is stable—that is, formula_10 converges asymptotically to the steady state \"x*\"—if and only if all eigenvalues of the transition matrix \"A\" (whether real or complex) have an absolute value which is less than 1.\n\nAssume that the equation has been put in the homogeneous form formula_11. Then we can iterate and substitute repeatedly from the initial condition formula_12, which is the initial value of the vector \"y\" and which must be known in order to find the solution:\n\nand so forth, so that by mathematical induction the solution in terms of \"t\" is\n\nFurther, if A is diagonalizable, we can rewrite \"A\" in terms of its eigenvalues and eigenvectors, giving the solution as\n\nwhere \"P\" is an \"n\" × \"n\" matrix whose columns are the eigenvectors of \"A\" (assuming the eigenvalues are all distinct) and \"D\" is an \"n\" × \"n\" diagonal matrix whose diagonal elements are the eigenvalues of \"A\". This solution motivates the above stability result: formula_18 shrinks to the zero matrix over time if and only if the eigenvalues of \"A\" are all less than unity in absolute value.\n\nStarting from the \"n\"-dimensional system formula_19 we can extract the dynamics of one of the state variables, say formula_20 The above solution equation for formula_21 shows that the solution for formula_22 is in terms of the \"n\" eigenvalues of \"A\". Therefore the equation describing the evolution of formula_23 by itself must have a solution involving those same eigenvalues. This description intuitively motivates the equation of evolution of formula_24 which is\n\nwhere the parameters formula_26 are from the characteristic equation of the matrix \"A\":\n\nThus each individual scalar variable of an \"n\"-dimensional first-order linear system evolves according to a univariate \"n\" degree difference equation, which has the same stability property (stable or unstable) as does the matrix difference equation.\n\nMatrix difference equations of higher order—that is, with a time lag longer than one period—can be solved, and their stability analyzed, by converting them into first-order form using a block matrix. For example, suppose we have the second-order equation\n\nwith the variable vector \"x\" being \"n\"×1 and \"A\" and \"B\" being \"n\"×\"n\". This can be stacked in the form\n\nwhere formula_7 is the \"n\"×\"n\" identity matrix and 0 is the \"n\"×\"n\" zero matrix. Then denoting the 2\"n\"×1 stacked vector of current and once-lagged variables as formula_31 and the 2\"n\"×2\"n\" block matrix as \"L\", we have as before the solution \n\nAlso as before, this stacked equation and thus the original second-order equation are stable if and only if all eigenvalues of the matrix \"L\" are smaller than unity in absolute value.\n\nIn linear-quadratic-Gaussian control, there arises a nonlinear matrix equation for the reverse evolution of a current-and-future-cost \"matrix\", denoted below as \"H\". This equation is called a discrete dynamic Riccati equation, and it arises when a variable vector evolving according to a linear matrix difference equation is controlled by manipulating an exogenous vector in order to optimize a quadratic cost function. This Riccati equation assumes the following, or a similar, form:\n\nwhere \"H\", \"K\", and \"A\" are \"n\"×\"n\", \"C\" is \"n\"×\"k\", \"R\" is \"k\"×\"k\", \"n\" is the number of elements in the vector to be controlled, and \"k\" is the number of elements in the control vector. The parameter matrices \"A\" and \"C\" are from the linear equation, and the parameter matrices \"K\" and \"R\" are from the quadratic cost function. See here for details.\n\nIn general this equation cannot be solved analytically for formula_34 in terms of \"t\" ; rather, the sequence of values for formula_34 is found by iterating the Riccati equation. However, it was shown in that this Riccati equation can be solved analytically if \"R\" is the zero matrix and \"n\"=\"k\"+1, by reducing it to a scalar rational difference equation; moreover, for any \"k\" and \"n\" if the transition matrix \"A\" is nonsingular then the Riccati equation can be solved analytically in terms of the eigenvalues of a matrix, although these may need to be found numerically. \n\nIn most contexts the evolution of \"H\" backwards through time is stable, meaning that \"H\" converges to a particular fixed matrix \"H\"* which may be irrational even if all the other matrices are rational. See also Stochastic control#Discrete time.\n\nA related Riccati equation is\n\nin which the matrices \"X\", \"A\", \"B\", \"C\", and \"E\" are all \"n\"×\"n\". This equation can be solved explicitly. Suppose formula_37, which certainly holds for \"t\"=0 with \"N\" = \"X\" and with \"D\" equal to the identity matrix. Then using this in the difference equation yields \n\nso by induction the form formula_42 holds for all \"t\". Then the evolution of \"N\" and \"D\" can be written as \n\nThus \n\n"}
{"id": "36950801", "url": "https://en.wikipedia.org/wiki?curid=36950801", "title": "Matroid partitioning", "text": "Matroid partitioning\n\nThe matroid partitioning problem is a problem arising in the mathematical study of matroids and in the design and analysis of algorithms, in which the goal is to partition the elements of a matroid into as few independent sets as possible. An example is the problem of computing the arboricity of an undirected graph, the minimum number of forests needed to cover all of its edges. Matroid partitioning may be solved in polynomial time, given an independence oracle for the matroid. It may be generalized to show that a matroid sum is itself a matroid, to provide an algorithm for computing ranks and independent sets in matroid sums, and to compute the largest common independent set in the intersection of two given matroids. \n\nThe arboricity of an undirected graph is the minimum number of forests into which its edges can be partitioned, or equivalently (by adding overlapping edges to each forest as necessary) the minimum number of spanning forests whose union is the whole graph. A formula proved by Crispin Nash-Williams characterizes the arboricity exactly: it is the maximum, over all subgraphs formula_1 of the given graph formula_2, of the quantity formula_3,\nso in this case the algorithm may find an optimal partition by placing formula_4 into its own new independent set and leaving the other independent sets unchanged.\n\nThe overall algorithm, then, considers each element formula_4 of the given matroid in turn, constructs the graph formula_6, tests which nodes can reach formula_4, and uses this information to update the current partition so that it includes formula_4. At each step, the partition of the elements considered so far is optimal, so when the algorithm terminates it will have found an optimal partition for the whole matroid.\nProving that this algorithm is correct requires showing that a shorcut-free path in the auxiliary graph always describes a sequence of operations that, when performed simultaneously, correctly preserves the independence of the sets in the partition; a proof of this fact was given by Edmonds.\nBecause the algorithm only increases the number of sets in the partition when the matroid partitioning formula shows that a larger number is needed, the correctness of this algorithm also shows the correctness of the formula.\n\nAlthough this algorithm depends only on the existence of an independence oracle for its correctness, faster algorithms can be found in many cases by taking advantage of the more specialized structure of specific types of matroids (such as graphic matroids) from which a particular partitioning problem has been defined. \n\nA matroid sum formula_9 (where each formula_10 is a matroid) is itself a matroid, having as its elements the union of the elements of the summands. A set is independent in the sum if it can be partitioned into sets that are independent within each summand. The matroid partitioning algorithm generalizes to the problem of testing whether a set is independent in a matroid sum, and its correctness can be used to prove that a matroid sum is necessarily a matroid.\n\nThe matroid intersection problem of finding the largest set that is independent in two matroids formula_11 and formula_12 may be solved by turning it into an equivalent matroid sum problem: if formula_13 is a basis of the sum formula_14, where formula_15 is the dual of formula_12, then formula_13 must have full rank in formula_15 and removing a maximal independent set of formula_15 from formula_13 leaves a maximum intersection.\n\nMatroid partitioning is a form of set cover problem, and the corresponding set packing problem (find a maximum number of disjoint spanning sets within a given matroid) is also of interest. It can be solved by algorithms similar to those for matroid partitioning. The fractional set packing and set covering problems associated with a matroid (that is, assign a weight to each independent set in such a way that for every element the total weight of the sets containing it is at most one or at least one, maximizing or minimizing the total weight of all the sets, respectively) can also be solved in polynomial time using matroid partitioning methods.\n\nAs well as its use in calculating the arboricity of a graph, matroid partitioning can be used with other matroids to find a subgraph of a given graph whose average degree is maximum, and to find the edge toughness of a graph (a variant of graph toughness involving the deletion of edges in place of vertices).\n"}
{"id": "689895", "url": "https://en.wikipedia.org/wiki?curid=689895", "title": "Mechanism design", "text": "Mechanism design\n\nMechanism design is a field in economics and game theory that takes an engineering approach to designing economic mechanisms or incentives, toward desired objectives, in strategic settings, where players act rationally. Because it starts at the end of the game, then goes backwards, it is also called reverse game theory. It has broad applications, from economics and politics (markets, auctions, voting procedures) to networked-systems (internet interdomain routing, sponsored search auctions).\n\nMechanism design studies solution concepts for a class of private-information games. Leonid Hurwicz explains that 'in a design problem, the goal function is the main \"given\", while the\nmechanism is the unknown. Therefore, the design problem is the \"inverse\" of traditional economic theory, which is typically devoted to the analysis of the performance of a given mechanism.' So, two distinguishing features of these games are:\n\n\nThe 2007 Nobel Memorial Prize in Economic Sciences was awarded to Leonid Hurwicz, Eric Maskin, and Roger Myerson \"for having laid the foundations of mechanism design theory\".\n\nIn an interesting class of Bayesian games, one player, called the \"principal\", would like to condition his behavior on information privately known to other players. For example, the principal would like to know the true quality of a used car a salesman is pitching. He cannot learn anything simply by asking the salesman, because it is in his interest to distort the truth. However, in mechanism design the principal does have one advantage: He may design a game whose rules can influence others to act the way he would like.\n\nWithout mechanism design theory, the principal's problem would be difficult to solve. He would have to consider all the possible games and choose the one that best influences other players' tactics. In addition, the principal would have to draw conclusions from agents who may lie to him. Thanks to mechanism design, and particularly the revelation principle, the principal only needs to consider games in which agents truthfully report their private information.\n\nA game of mechanism design is a game of private information in which one of the agents, called the principal, chooses the payoff structure. Following , the agents receive secret \"messages\" from nature containing information relevant to payoffs. For example, a message may contain information about their preferences or the quality of a good for sale. We call this information the agent's \"type\" (usually noted formula_1 and accordingly the space of types formula_2). Agents then report a type to the principal (usually noted with a hat formula_3) that can be a strategic lie. After the report, the principal and the agents are paid according to the payoff structure the principal chose.\n\nThe timing of the game is:\n\n\nIn order to understand who gets what, it is common to divide the outcome formula_5 into a goods allocation and a money transfer, formula_9 where formula_10 stands for an allocation of goods rendered or received as a function of type, and formula_11 stands for a monetary transfer as a function of type.\n\nAs a benchmark the designer often defines what would happen under full information. Define a formula_12 mapping the (true) type profile directly to the allocation of goods received or rendered,\n\nIn contrast a mechanism maps the \"reported\" type profile to an \"outcome\" (again, both a goods allocation formula_10 and a money transfer formula_11)\n\nA proposed mechanism constitutes a Bayesian game (a game of private information), and if it is well-behaved the game has a Bayesian Nash equilibrium. At equilibrium agents choose their reports strategically as a function of type\n\nIt is difficult to solve for Bayesian equilibria in such a setting because it involves solving for agents' best-response strategies and for the best inference from a possible strategic lie. Thanks to a sweeping result called the revelation principle, no matter the mechanism a designer can confine attention to equilibria in which agents truthfully report type. The revelation principle states: \"To every Bayesian Nash equilibrium there corresponds a Bayesian game with the same equilibrium outcome but in which players truthfully report type.\"\n\nThis is extremely useful. The principle allows one to solve for a Bayesian equilibrium by assuming all players truthfully report type (subject to an incentive compatibility constraint). In one blow it eliminates the need to consider either strategic behavior or lying.\n\nIts proof is quite direct. Assume a Bayesian game in which the agent's strategy and payoff are functions of its type and what others do, formula_18. By definition agent \"i\"'s equilibrium strategy formula_19 is Nash in expected utility:\n\nSimply define a mechanism that would induce agents to choose the same equilibrium. The easiest one to define is for the mechanism to commit to playing the agents' equilibrium strategies \"for\" them.\n\nUnder such a mechanism the agents of course find it optimal to reveal type since the mechanism plays the strategies they found optimal anyway. Formally, choose formula_22 such that\n\nThe designer of a mechanism generally hopes either\n\nTo implement a social choice function formula_12 is to find some formula_27 transfer function that motivates agents to pick outcome formula_28. Formally, if the equilibrium strategy profile under the mechanism maps to the same goods allocation as a social choice function,\nwe say the mechanism implements the social choice function.\n\nThanks to the revelation principle, the designer can usually find a transfer function formula_27 to implement a social choice by solving an associated truthtelling game. If agents find it optimal to truthfully report type,\nwe say such a mechanism is truthfully implementable (or just \"implementable\"). The task is then to solve for a truthfully implementable formula_27 and impute this transfer function to the original game. An allocation formula_28 is truthfully implementable if there exists a transfer function formula_27 such that\nwhich is also called the incentive compatibility (IC) constraint.\n\nIn applications, the IC condition is the key to describing the shape of formula_27 in any useful way. Under certain conditions it can even isolate the transfer function analytically. Additionally, a participation (individual rationality) constraint is sometimes added if agents have the option of not playing.\n\nConsider a setting in which all agents have a type-contingent utility function formula_37. Consider also a goods allocation formula_28 that is vector-valued and size formula_39 (which permits formula_39 number of goods) and assume it is piecewise continuous with respect to its arguments.\n\nThe function formula_28 is implementable only if\nwhenever formula_43 and formula_44 and \"x\" is continuous at formula_1. This is a necessary condition and is derived from the first- and second-order conditions of the agent's optimization problem assuming truth-telling.\n\nIts meaning can be understood in two pieces. The first piece says the agent's marginal rate of substitution (MRS) increases as a function of the type,\nIn short, agents will not tell the truth if the mechanism does not offer higher agent types a better deal. Otherwise, higher types facing any mechanism that punishes high types for reporting will lie and declare they are lower types, violating the truthtelling IC constraint. The second piece is a monotonicity condition waiting to happen,\nwhich, to be positive, means higher types must be given more of the good.\n\nThere is potential for the two pieces to interact. If for some type range the contract offered less quantity to higher types formula_48, it is possible the mechanism could compensate by giving higher types a discount. But such a contract already exists for low-type agents, so this solution is pathological. Such a solution sometimes occurs in the process of solving for a mechanism. In these cases it must be \"ironed.\" In a multiple-good environment it is also possible for the designer to reward the agent with more of one good to substitute for less of another (e.g. butter for margarine). Multiple-good mechanisms are an ongoing problem in mechanism design theory.\n\nMechanism design papers usually make two assumptions to ensure implementability:\nThis is known by several names: the single-crossing condition, the sorting condition and the Spence–Mirrlees condition. It means the utility function is of such a shape that the agent's MRS is increasing in type.\nThis is a technical condition bounding the rate of growth of the MRS.\n\nThese assumptions are sufficient to provide that any monotonic formula_28 is implementable (a formula_27 exists that can implement it). In addition, in the single-good setting the single-crossing condition is sufficient to provide that only a monotonic formula_28 is implementable, so the designer can confine his search to a monotonic formula_28.\n\n gives a celebrated result that any member of a large class of auctions assures the seller of the same expected revenue and that the expected revenue is the best the seller can do. This is the case if\n\nThe last condition is crucial to the theorem. An implication is that for the seller to achieve higher revenue he must take a chance on giving the item to an agent with a lower valuation. Usually this means he must risk not selling the item at all.\n\nThe Vickrey (1961) auction model was later expanded by and Groves to treat a public choice problem in which a public project's cost is borne by all agents, e.g. whether to build a municipal bridge. The resulting \"Vickrey–Clarke–Groves\" mechanism can motivate agents to choose the socially efficient allocation of the public good even if agents have privately known valuations. In other words, it can solve the \"tragedy of the commons\"—under certain conditions, in particular quasilinear utility or if budget balance is not required.\n\nConsider a setting in which formula_55 number of agents have quasilinear utility with private valuations formula_56 where the currency formula_11 is valued linearly. The VCG designer designs an incentive compatible (hence truthfully implementable) mechanism to obtain the true type profile, from which the designer implements the socially optimal allocation\n\nThe cleverness of the VCG mechanism is the way it motivates truthful revelation. It eliminates incentives to misreport by penalizing any agent by the cost of the distortion he causes. Among the reports the agent may make, the VCG mechanism permits a \"null\" report saying he is indifferent to the public good and cares only about the money transfer. This effectively removes the agent from the game. If an agent does choose to report a type, the VCG mechanism charges the agent a fee if his report is pivotal, that is if his report changes the optimal allocation \"x\" so as to harm other agents. The payment is calculated\nwhich sums the distortion in the utilities of the other agents (and not his own) caused by one agent reporting.\n\n and give an impossibility result similar in spirit to Arrow's impossibility theorem. For a very general class of games, only \"dictatorial\" social choice functions can be implemented.\n\nA social choice function \"f\"() is dictatorial if one agent always receives his most-favored goods allocation,\n\nThe theorem states that under general conditions any truthfully implementable social choice function must be dictatorial,\n\n show there is no efficient way for two parties to trade a good when they each have secret and probabilistically varying valuations for it, without the risk of forcing one party to trade at a loss. It is among the most remarkable negative results in economics—a kind of negative mirror to the fundamental theorems of welfare economics.\n\n introduces a setting in which the transfer function \"t\"() is easy to solve for. Due to its relevance and tractability it is a common setting in the literature. Consider a single-good, single-agent setting in which the agent has quasilinear utility with an unknown type parameter formula_1\nand in which the principal has a prior CDF over the agent's type formula_64. The principal can produce goods at a convex marginal cost \"c\"(\"x\") and wants to maximize the expected profit from the transaction\nsubject to IC and IR conditions\nThe principal here is a monopolist trying to set a profit-maximizing price scheme in which it cannot identify the type of the customer. A common example is an airline setting fares for business, leisure and student travelers. Due to the IR condition it has to give every type a good enough deal to induce participation. Due to the IC condition it has to give every type a good enough deal that the type prefers its deal to that of any other.\n\nA trick given by Mirrlees (1971) is to use the envelope theorem to eliminate the transfer function from the expectation to be maximized,\nIntegrating,\nwhere formula_71 is some index type. Replacing the incentive-compatible formula_72 in the maximand,\nafter an integration by parts. This function can be maximized pointwise.\n\nBecause formula_75 is incentive-compatible already the designer can drop the IC constraint. If the utility function satisfies the Spence–Mirrlees condition then a monotonic formula_28 function exists. The IR constraint can be checked at equilibrium and the fee schedule raised or lowered accordingly. Additionally, note the presence of a hazard rate in the expression. If the type distribution bears the monotone hazard ratio property, the FOC is sufficient to solve for \"t\"(). If not, then it is necessary to check whether the monotonicity constraint (see sufficiency, above) is satisfied everywhere along the allocation and fee schedules. If not, then the designer must use Myerson ironing.\n\nIn some applications the designer may solve the first-order conditions for the price and allocation schedules yet find they are not monotonic. For example, in the quasilinear setting this often happens when the hazard ratio is itself not monotone. By the Spence–Mirrlees condition the optimal price and allocation schedules must be monotonic, so the designer must eliminate any interval over which the schedule changes direction by flattening it.\n\nIntuitively, what is going on is the designer finds it optimal to bunch certain types together and give them the same contract. Normally the designer motivates higher types to distinguish themselves by giving them a better deal. If there are insufficiently few higher types on the margin the designer does not find it worthwhile to grant lower types a concession (called their information rent) in order to charge higher types a type-specific contract.\n\nConsider a monopolist principal selling to agents with quasilinear utility, the example above. Suppose the allocation schedule formula_28 satisfying the first-order conditions has a single interior peak at formula_78 and a single interior trough at formula_79, illustrated at right.\n\n\n\nThe proof uses the theory of optimal control. It considers the set of intervals formula_92 in the nonmonotonic region of formula_28 over which it might flatten the schedule. It then writes a Hamiltonian to obtain necessary conditions for a formula_28 within the intervals\nCondition two ensures that the formula_28 satisfying the optimal control problem reconnects to the schedule in the original problem at the interval boundaries (no jumps). Any formula_28 satisfying the necessary conditions must be flat because it must be monotonic and yet reconnect at the boundaries.\n\nAs before maximize the principal's expected payoff, but this time subject to the monotonicity constraint\nand use a Hamiltonian to do it, with shadow price formula_98\nwhere formula_10 is a state variable and formula_101 the control. As usual in optimal control the costate evolution equation must satisfy\nTaking advantage of condition 2, note the monotonicity constraint is not binding at the boundaries of the formula_1 interval,\nmeaning the costate variable condition can be integrated and also equals 0\nThe average distortion of the principal's surplus must be 0. To flatten the schedule, find an formula_10 such that its inverse image maps to a formula_1 interval satisfying the condition above.\n\n\n\n"}
{"id": "46671693", "url": "https://en.wikipedia.org/wiki?curid=46671693", "title": "Message authentication", "text": "Message authentication\n\nIn information security, message authentication or data origin authentication is a property that a message has not been modified while in transit (data integrity) and that the receiving party can verify the source of the message. Message authentication does \"not\" necessarily include the property of non-repudiation.\n\nMessage authentication is typically achieved by using message authentication codes (MACs), authenticated encryption (AE) or digital signatures. The message authentication code, also known as digital authenticator, is used as an integrity check based on a secret key shared by two parties to authenticate information transmitted between them. It is based on using a cryptographic hash or symmetric encryption algorithm. The authentication key is only shared by at least two parties or two communicating devices but it will fail in the existence of a third party since the algorithm will no longer be effective in detecting forgeries. In addition, the key must also be randomly generated to avoid its recovery through brute force searches and related key attacks designed to identify it from the messages transiting the medium.\n\nSome cryptographers distinguish between \"message authentication without secrecy\" systems -- which allow the intended receiver to verify the source of the message, but don't bother hiding the plaintext contents of the message -- from authenticated encryption systems.\nSome cryptographers have researched subliminal channel systems that send messages that appear to use a \"message authentication without secrecy\" system, but in fact also transmit a secret message.\n\n"}
{"id": "29921522", "url": "https://en.wikipedia.org/wiki?curid=29921522", "title": "PRF advantage", "text": "PRF advantage\n\nThe pseudorandom-function advantage (PRF advantage) of an algorithm on a pseudorandom function family is a measure of how effectively the algorithm can distinguish between a member of the family and a random oracle. Consequently, the maximum pseudorandom advantage attainable by any algorithm with a fixed amount of computational resources is a measure of how well such a function family emulates a random oracle. \n\nSay that an adversary algorithm has access to an oracle that will apply a function to inputs that are sent to it. The algorithm sends the oracle a number of queries before deciding whether the oracle is a random oracle or simply an instance of the pseudorandom function family. Say also that there is a 50% chance that the oracle is a random oracle and a 50% chance that it is a member of the function family. The pseudorandom advantage of the algorithm is defined as two times the probability that the algorithm guesses correctly minus one.\n\n"}
{"id": "39711379", "url": "https://en.wikipedia.org/wiki?curid=39711379", "title": "Physics applications of asymptotically safe gravity", "text": "Physics applications of asymptotically safe gravity\n\nThe asymptotic safety approach to quantum gravity provides a nonperturbative notion of renormalization in order to find a consistent and predictive quantum field theory of the gravitational interaction and spacetime geometry. It is based upon a nontrivial fixed point of the corresponding renormalization group (RG) flow such that the running coupling constants approach this fixed point in the ultraviolet (UV) limit. This suffices to avoid divergences in physical observables. Moreover, it has predictive power: Generically an arbitrary starting configuration of coupling constants given at some RG scale does not run into the fixed point for increasing scale, but a subset of configurations might have the desired UV properties. For this reason it is possible that — assuming a particular set of couplings has been measured in an experiment — the requirement of asymptotic safety fixes all remaining couplings in such a way that the UV fixed point is approached.\n\nAsymptotic safety, if realized in Nature, has far reaching consequences in all areas where quantum effects of gravity are to be expected. Their exploration, however, is still in its infancy. By now there are some phenomenological studies concerning the implications of asymptotic safety in particle physics, astrophysics and cosmology, for instance.\n\nThe Standard Model in combination with asymptotic safety might be valid up to arbitrarily high energies. Based on the assumption that this is indeed correct it is possible to make a statement about the Higgs boson mass. The first concrete results were obtained by Shaposhnikov and Wetterich.\nDepending on the sign of the gravity induced anomalous dimension formula_1 there are two possibilities: For formula_2 the Higgs mass formula_3 is restricted to the window formula_4. If, on the other hand, formula_5 which is the favored possibility, formula_3 must take the value\nwith an uncertainty of a few GeV only. In this spirit one can consider formula_3 a prediction of asymptotic safety. The result is in surprisingly good agreement with the latest experimental data measured at CERN by the ATLAS and CMS collaborations.\n\nBy taking into account the gravitational correction to the running of the fine structure constant formula_9 of quantum electrodynamics, Harst and Reuter were able to study the impacts of asymptotic safety on the infrared (renormalized) value of formula_9.\nThey found two fixed points suitable for the asymptotic safety construction both of which imply a well-behaved UV limit, without running into a Landau pole type singularity. The first one is characterized by a vanishing formula_9, and the infrared value formula_12 is a free parameter. In the second case, however, the fixed point value of formula_9 is non-zero, and its infrared value is a computable prediction of the theory.\n\nIn a more recent study, Christiansen and Eichhorn showed that quantum fluctuations of gravity generically generate self-interactions for gauge theories, which have to be included in a discussion of a potential ultraviolet completion. Depending on the gravitational and gauge parameters, they conclude that the fine structure constant formula_9 might be asymptotically free and not run into a Landau pole, while the induced coupling for the gauge self-interaction is irrelevant and thus its value can be predicted. This is an explicit example where Asymptotic Safety solves a problem of the Standard Model - the triviality of the U(1) sector - without introducing new free parameters.\n\nPhenomenological consequences of asymptotic safety can be expected also for astrophysics and cosmology. Bonanno and Reuter investigated the horizon structure of \"renormalization group improved\" black holes and computed quantum gravity corrections to the Hawking temperature and the corresponding thermodynamical entropy.\nBy means of an RG improvement of the Einstein–Hilbert action, Reuter and Weyer obtained a modified version of the Einstein equations which in turn results in a modification of the Newtonian limit, providing a possible explanation for the observed flat galaxy rotation curves without having to postulate the presence of dark matter.\n\nAs for cosmology, Bonanno and Reuter argued that asymptotic safety modifies the very early Universe, possibly leading to a resolution to the horizon and flatness problem of standard cosmology. Furthermore, asymptotic safety provides the possibility of inflation without the need of an inflaton field (while driven by the cosmological constant).\nIt was reasoned that the scale invariance related to the non-Gaussian fixed point underlying asymptotic safety is responsible for the near scale invariance of the primordial density perturbations. Using different methods, asymptotically safe inflation was analyzed further by Weinberg.\n\n"}
{"id": "23798", "url": "https://en.wikipedia.org/wiki?curid=23798", "title": "Poincaré conjecture", "text": "Poincaré conjecture\n\nIn mathematics, the Poincaré conjecture (; ) is a theorem about the characterization of the 3-sphere, which is the hypersphere that bounds the unit ball in four-dimensional space. The conjecture states: An equivalent form of the conjecture involves a coarser form of equivalence than homeomorphism called homotopy equivalence: if a 3-manifold is \"homotopy equivalent\" to the 3-sphere, then it is necessarily \"homeomorphic\" to it.\n\nOriginally conjectured by Henri Poincaré, the theorem concerns a space that locally looks like ordinary three-dimensional space but is connected, finite in size, and lacks any boundary (a closed 3-manifold). The Poincaré conjecture claims that if such a space has the additional property that each loop in the space can be continuously tightened to a point, then it is necessarily a three-dimensional sphere. The analogous conjectures for all higher dimensions had already been proved.\n\nAfter nearly a century of effort by mathematicians, Grigori Perelman presented a proof of the conjecture in three papers made available in 2002 and 2003 on arXiv. The proof built upon the program of Richard S. Hamilton to use the Ricci flow to attempt to solve the problem. Hamilton later introduced a modification of the standard Ricci flow, called \"Ricci flow with surgery\" to systematically excise singular regions as they develop, in a controlled way, but was unable to prove this method \"converged\" in three dimensions. Perelman completed this portion of the proof. Several teams of mathematicians verified that Perelman's proof was correct.\n\nThe Poincaré conjecture, before being proved, was one of the most important open questions in topology. In 2000, it was named one of the seven Millennium Prize Problems, for which the Clay Mathematics Institute offered a $1,000,000 prize for the first correct solution. Perelman's work survived review and was confirmed in 2006, leading to his being offered a Fields Medal, which he declined. Perelman was awarded the Millennium Prize on March 18, 2010. On July 1, 2010, he turned down the prize saying that he believed his contribution in proving the Poincaré conjecture was no greater than Hamilton's. , the Poincaré conjecture is the only solved Millennium problem.\n\nOn December 22, 2006, the journal \"Science\" honored Perelman's proof of the Poincaré conjecture as the scientific \"Breakthrough of the Year\", the first time this honor was bestowed in the area of mathematics.\n\nAt the beginning of the 20th century, Henri Poincaré was working on the foundations of topology—what would later be called combinatorial topology and then algebraic topology. He was particularly interested in what topological properties characterized a sphere.\n\nPoincaré claimed in 1900 that homology, a tool he had devised based on prior work by Enrico Betti, was sufficient to tell if a 3-manifold was a 3-sphere. However, in a 1904 paper he described a counterexample to this claim, a space now called the Poincaré homology sphere. The Poincaré sphere was the first example of a homology sphere, a manifold that had the same homology as a sphere, of which many others have since been constructed. To establish that the Poincaré sphere was different from the 3-sphere, Poincaré introduced a new topological invariant, the fundamental group, and showed that the Poincaré sphere had a fundamental group of order 120, while the 3-sphere had a trivial fundamental group. In this way he was able to conclude that these two spaces were, indeed, different.\n\nIn the same paper, Poincaré wondered whether a 3-manifold with the homology of a 3-sphere and also trivial fundamental group had to be a 3-sphere. Poincaré's new condition—i.e., \"trivial fundamental group\"—can be restated as \"every loop can be shrunk to a point.\"\n\nThe original phrasing was as follows:\n\nPoincaré never declared whether he believed this additional condition would characterize the 3-sphere, but nonetheless, the statement that it does is known as the Poincaré conjecture. Here is the standard form of the conjecture:\n\nThis problem seemed to lay dormant until J. H. C. Whitehead revived interest in the conjecture, when in the 1930s he first claimed a proof and then retracted it. In the process, he discovered some interesting examples of simply-connected (indeed contractible, i.e homotopically equivalent to a point) non-compact 3-manifolds not homeomorphic to R, the prototype of which is now called the Whitehead manifold.\n\nIn the 1950s and 1960s, other mathematicians attempted proofs of the conjecture only to discover that they contained flaws. Influential mathematicians such as G. de Rham, Bing, Haken, Moise, and Papakyriakopoulos attempted to prove the conjecture. In 1958 Bing proved a weak version of the Poincaré conjecture: if every simple closed curve of a compact 3-manifold is contained in a 3-ball, then the manifold is homeomorphic to the 3-sphere. Bing also described some of the pitfalls in trying to prove the Poincaré conjecture.\n\nWłodzimierz Jakobsche showed in 1978 that, if the Bing–Borsuk conjecture is true in dimension 3, then the Poincaré conjecture must also be true.\n\nOver time, the conjecture gained the reputation of being particularly tricky to tackle. John Milnor commented that sometimes the errors in false proofs can be \"rather subtle and difficult to detect.\" Work on the conjecture improved understanding of 3-manifolds. Experts in the field were often reluctant to announce proofs, and tended to view any such announcement with skepticism. The 1980s and 1990s witnessed some well-publicized fallacious proofs (which were not actually published in peer-reviewed form).\n\nAn exposition of attempts to prove this conjecture can be found in the non-technical book \"Poincaré's Prize\" by George Szpiro.\n\nThe classification of closed surfaces gives an affirmative answer to the analogous question in two dimensions. For dimensions greater than three, one can pose the Generalized Poincaré conjecture: is a homotopy \"n\"-sphere homeomorphic to the \"n\"-sphere? A stronger assumption is necessary; in dimensions four and higher there are simply-connected, closed manifolds which are not homotopy equivalent to an \"n\"-sphere.\n\nHistorically, while the conjecture in dimension three seemed plausible, the generalized conjecture was thought to be false. In 1961 Stephen Smale shocked mathematicians by proving the Generalized Poincaré conjecture for dimensions greater than four and extended his techniques to prove the fundamental h-cobordism theorem. In 1982 Michael Freedman proved the Poincaré conjecture in four dimensions. Freedman's work left open the possibility that there is a smooth four-manifold homeomorphic to the four-sphere which is not diffeomorphic to the four-sphere. This so-called smooth Poincaré conjecture, in dimension four, remains open and is thought to be very difficult. Milnor's exotic spheres show that the smooth Poincaré conjecture is false in dimension seven, for example.\n\nThese earlier successes in higher dimensions left the case of three dimensions in limbo. The Poincaré conjecture was essentially true in both dimension four and all higher dimensions for substantially different reasons. In dimension three, the conjecture had an uncertain reputation until the geometrization conjecture put it into a framework governing all 3-manifolds. John Morgan wrote:\n\nHamilton's program was started in his 1982 paper in which he introduced the Ricci flow on a manifold and showed how to use it to prove some special cases of the Poincaré conjecture. In the following years he extended this work, but was unable to prove the conjecture. The actual solution was not found until Grigori Perelman published his papers.\n\nIn late 2002 and 2003 Perelman posted three papers on the arXiv. In these papers he sketched a proof of the Poincaré conjecture and a more general conjecture, Thurston's geometrization conjecture, completing the Ricci flow program outlined earlier by Richard S. Hamilton.\n\nFrom May to July 2006, several groups presented papers that filled in the details of Perelman's proof of the Poincaré conjecture, as follows:\n\nAll three groups found that the gaps in Perelman's papers were minor and could be filled in using his own techniques.\n\nOn August 22, 2006, the ICM awarded Perelman the Fields Medal for his work on the conjecture, but Perelman refused the medal.\nJohn Morgan spoke at the ICM on the Poincaré conjecture on August 24, 2006, declaring that \"in 2003, Perelman solved the Poincaré Conjecture.\"\n\nIn December 2006, the journal \"Science\" honored the proof of Poincaré conjecture as the Breakthrough of the Year and featured it on its cover.\n\nHamilton's program for proving the Poincaré conjecture involves first putting a Riemannian metric on the unknown simply connected closed 3-manifold. The idea is to try to improve this metric; for example, if the metric can be improved enough so that it has constant curvature, then it must be the 3-sphere. The metric is improved using the Ricci flow equations;\n\nwhere \"g\" is the metric and \"R\" its Ricci curvature, and one hopes that as the time \"t\" increases the manifold becomes easier to understand. Ricci flow expands the negative curvature part of the manifold and contracts the positive curvature part.\n\nIn some cases Hamilton was able to show that this works; for example, if the manifold has positive Ricci curvature everywhere he showed that the manifold becomes extinct in finite time under Ricci flow without any other singularities. (In other words, the manifold collapses to a point in finite time; it is easy to describe the structure just before the manifold collapses.) This easily implies the Poincaré conjecture in the case of positive Ricci curvature. However, in general the Ricci flow equations lead to singularities of the metric after a finite time. Perelman showed how to continue past these singularities: very roughly, he cuts the manifold along the singularities, splitting the manifold into several pieces, and then continues with the Ricci flow on each of these pieces. This procedure is known as Ricci flow with surgery.\n\nA special case of Perelman's theorems about Ricci flow with surgery is given as follows.\nThis result implies the Poincaré conjecture because it is easy to check it for the possible manifolds listed in the conclusion.\n\nThe condition on the fundamental group turns out to be necessary (and sufficient) for finite time extinction, and in particular includes the case of trivial fundamental group. It is equivalent to saying that the prime decomposition of the manifold has no acyclic components, and turns out to be equivalent to the condition that all geometric pieces of the manifold have geometries based on the two Thurston geometries \"S\"×R and \"S\". By studying the limit of the manifold for large time, Perelman proved Thurston's geometrization conjecture for any fundamental group: at large times the manifold has a thick-thin decomposition, whose thick piece has a hyperbolic structure, and whose thin piece is a graph manifold, but this extra complication is not necessary for proving just the Poincaré conjecture.\n\nOn November 13, 2002, Russian mathematician Grigori Perelman posted the first of a series of three eprints on arXiv outlining a solution of the Poincaré conjecture. Perelman's proof uses a modified version of a Ricci flow program developed by Richard S. Hamilton. In August 2006, Perelman was awarded, but declined, the Fields Medal (worth $15,000 CAD) for his proof. On March 18, 2010, the Clay Mathematics Institute awarded Perelman the $1 million Millennium Prize in recognition of his proof. Perelman rejected that prize as well.\n\nPerelman proved the conjecture by deforming the manifold using the Ricci flow (which behaves similarly to the heat equation that describes the diffusion of heat through an object). The Ricci flow usually deforms the manifold towards a rounder shape, except for some cases where it stretches the manifold apart from itself towards what are known as singularities. Perelman and Hamilton then chop the manifold at the singularities (a process called \"surgery\") causing the separate pieces to form into ball-like shapes. Major steps in the proof involve showing how manifolds behave when they are deformed by the Ricci flow, examining what sort of singularities develop, determining whether this surgery process can be completed and establishing that the surgery need not be repeated infinitely many times.\n\nThe first step is to deform the manifold using the Ricci flow. The Ricci flow was defined by Richard S. Hamilton as a way to deform manifolds. The formula for the Ricci flow is an imitation of the heat equation which describes the way heat flows in a solid. Like the heat flow, Ricci flow tends towards uniform behavior. Unlike the heat flow, the Ricci flow could run into singularities and stop functioning. A singularity in a manifold is a place where it is not differentiable: like a corner or a cusp or a pinching. The Ricci flow was only defined for smooth differentiable manifolds. Hamilton used the Ricci flow to prove that some compact manifolds were diffeomorphic to spheres and he hoped to apply it to prove the Poincaré Conjecture. He needed to understand the singularities.\n\nHamilton created a list of possible singularities that could form but he was concerned that some singularities might lead to difficulties. He wanted to cut the manifold at the singularities and paste in caps, and then run the Ricci flow again, so he needed to understand the singularities and show that certain kinds of singularities do not occur. Perelman discovered the singularities were all very simple: essentially three-dimensional cylinders made out of spheres stretched out along a line. An ordinary cylinder is made by taking circles stretched along a line. Perelman proved this using something called the \"Reduced Volume\" which is closely related to an eigenvalue of a certain elliptic equation.\n\nSometimes an otherwise complicated operation reduces to multiplication by a scalar (a number). Such numbers are called eigenvalues of that operation. Eigenvalues are closely related to vibration frequencies and are used in analyzing a famous problem: can you hear the shape of a drum? Essentially an eigenvalue is like a note being played by the manifold. Perelman proved this note goes up as the manifold is deformed by the Ricci flow. This helped him eliminate some of the more troublesome singularities that had concerned Hamilton, particularly the cigar soliton solution, which looked like a strand sticking out of a manifold with nothing on the other side. In essence Perelman showed that all the strands that form can be cut and capped and none stick out on one side only.\n\nCompleting the proof, Perelman takes any compact, simply connected, three-dimensional manifold without boundary and starts to run the Ricci flow. This deforms the manifold into round pieces with strands running between them. He cuts the strands and continues deforming the manifold until eventually he is left with a collection of round three-dimensional spheres. Then he rebuilds the original manifold by connecting the spheres together with three-dimensional cylinders, morphs them into a round shape and sees that, despite all the initial confusion, the manifold was in fact homeomorphic to a sphere.\n\nOne immediate question posed was how one could be sure that infinitely many cuts weren't necessary. This was raised due to the cutting potentially progressing forever. Perelman proved this can't happen by using minimal surfaces on the manifold. A minimal surface is essentially a soap film. Hamilton had shown that the area of a minimal surface decreases as the manifold undergoes Ricci flow. Perelman verified what happened to the area of the minimal surface when the manifold was sliced. He proved that eventually the area is so small that any cut after the area is that small can only be chopping off three-dimensional spheres and not more complicated pieces. This is described as a battle with a Hydra by Sormani in Szpiro's book cited below. This last part of the proof appeared in Perelman's third and final paper on the subject.\n\nFriedmann–Lemaitre–Robertson–Walker universe corresponds to a time-evolving radius of an S3 space. It argues that if this universe is modified in M3, at the end the acceleration it may produce a phase transition changing M3 to a space of constant curvature which corresponds precisely to a de Sitter phase associated with S3. Another point of view is that since the geometrization conjecture (a generalization of the Poincaré conjecture) requires one to understand all locally homogeneous geometries on closed three manifolds, using Ricci flow one may consider Bianchi classification used to study cosmological models. What one may add to this scenario is that such a transition may require a torsion in order to make S3 (or other Bianchi cosmological models) parallelizable.\n\n\n"}
{"id": "3644492", "url": "https://en.wikipedia.org/wiki?curid=3644492", "title": "Probabilistic number theory", "text": "Probabilistic number theory\n\nProbabilistic number theory is a subfield of number theory, which explicitly uses probability to answer questions of number theory. One basic idea underlying it is that different prime numbers are, in some serious sense, like independent random variables. This however is not an idea that has a unique useful formal expression.\n\nThe founders of the theory were Paul Erdős, Aurel Wintner and Mark Kac during the 1930s, one of the periods of investigation in analytic number theory. The Erdős–Wintner theorem and the Erdős–Kac theorem on additive functions were foundational results.\n\n"}
{"id": "14484306", "url": "https://en.wikipedia.org/wiki?curid=14484306", "title": "Proof mining", "text": "Proof mining\n\nIn proof theory, a branch of mathematical logic, proof mining (or unwinding) is a research program that analyzes formalized proofs, especially in analysis, to obtain explicit bounds or rates of convergence from proofs that, when expressed in natural language, appear to be nonconstructive.\nThis research has led to improved results in analysis obtained from the analysis of classical proofs.\n\n"}
{"id": "32848705", "url": "https://en.wikipedia.org/wiki?curid=32848705", "title": "Q-Meixner–Pollaczek polynomials", "text": "Q-Meixner–Pollaczek polynomials\n\nIn mathematics, the \"q\"-Meixner–Pollaczek polynomials are a family of basic hypergeometric orthogonal polynomials in the basic Askey scheme. give a detailed list of their properties.\n\nThe polynomials are given in terms of basic hypergeometric functions and the Pochhammer symbol by ：\n\n"}
{"id": "43112064", "url": "https://en.wikipedia.org/wiki?curid=43112064", "title": "Rainbow coloring", "text": "Rainbow coloring\n\nIn graph theory, a path in an edge-colored graph is said to be rainbow if no color repeats on it. A graph is said to be rainbow-connected (or rainbow colored) if there is a rainbow path between each pair of its vertices. If there is a rainbow shortest path between each pair of vertices, the graph is said to be strongly rainbow-connected (or strongly rainbow colored).\n\nThe rainbow connection number of a graph formula_1 is the minimum number of colors needed to rainbow-connect formula_1, and is denoted by formula_3. Similarly, the strong rainbow connection number of a graph formula_1 is the minimum number of colors needed to strongly rainbow-connect formula_1, and is denoted by formula_6. Clearly, each strong rainbow coloring is also a rainbow coloring, while the converse is not true in general.\n\nIt is easy to observe that to rainbow-connect any connected graph formula_1, we need at least formula_8 colors, where formula_8 is the diameter of formula_1 (i.e. the length of the longest shortest path). On the other hand, we can never use more than formula_11 colors, where formula_11 denotes the number of edges in formula_1. Finally, because each strongly rainbow-connected graph is rainbow-connected, we have that formula_14.\n\nThe following are the extremal cases:\n\nThe above shows that in terms of the number of vertices, the upper bound formula_19 is the best possible in general. In fact, a rainbow coloring using formula_20 colors can be constructed by coloring the edges of a spanning tree of formula_1 in distinct colors. The remaining uncolored edges are colored arbitrarily, without introducing new colors. When formula_1 is 2-connected, we have that formula_23. Moreover, this is tight as witnessed by e.g. odd cycles.\n\nThe rainbow or the strong rainbow connection number has been determined for some structured graph classes:\n\nThe problem of deciding whether formula_32 for a given graph formula_1 is NP-complete. Because formula_32 if and only if formula_35, it follows that deciding if formula_35 is NP-complete for a given graph formula_1.\n\nChartrand, Okamoto and Zhang generalized the rainbow connection number as follows. Let formula_1 be an edge-colored nontrivial connected graph of order formula_39. A tree formula_40 is a \"rainbow tree\" if no two edges of formula_40 are assigned the same color. Let formula_42 be a fixed integer with formula_43. An edge coloring of formula_1 is called a \"formula_42-rainbow coloring\" if for every set formula_46 of formula_42 vertices of formula_1, there is a rainbow tree in formula_1 containing the vertices of formula_46. The formula_42-rainbow index formula_52 of formula_1 is the minimum number of colors needed in a formula_42-rainbow coloring of formula_1. A formula_42-rainbow coloring using formula_52 colors is called a \"minimum formula_42-rainbow coloring\". Thus formula_59 is the rainbow connection number of formula_1.\n\nRainbow connection has also been studied in vertex-colored graphs. This concept was introduced by Krivelevich and Yuster.\nHere, the rainbow vertex-connection number of a graph formula_1, denoted by formula_62, is the minimum number of colors needed to color formula_1 such that for each pair of vertices, there is a path connecting them whose internal vertices are assigned distinct colors.\n\n\n"}
{"id": "4025909", "url": "https://en.wikipedia.org/wiki?curid=4025909", "title": "Relation construction", "text": "Relation construction\n\nIn logic and mathematics, relation construction and relational constructibility have to do with the ways that one relation is determined by an indexed family or a sequence of other relations, called the \"relation dataset\". The relation in the focus of consideration is called the \"faciendum\". The relation dataset typically consists of a specified relation over sets of relations, called the \"constructor\", the \"factor\", or the \"method of construction\", plus a specified set of other relations, called the \"faciens\", the \"ingredients\", or the \"makings\".\n\nRelation composition and relation reduction are special cases of relation constructions.\n\n"}
{"id": "172564", "url": "https://en.wikipedia.org/wiki?curid=172564", "title": "Route inspection problem", "text": "Route inspection problem\n\nIn graph theory, a branch of mathematics and computer science, the Chinese postman problem (CPP), postman tour or route inspection problem is to find a shortest closed path or circuit that visits every edge of a (connected) undirected graph. When the graph has an Eulerian circuit (a closed walk that covers every edge once), that circuit is an optimal solution. Otherwise, the optimization problem is to find the smallest number of graph edges to duplicate (or the subset of edges with the minimum possible total weight) so that the resulting multigraph does have an Eulerian circuit. It may be solved in polynomial time.\n\nThe problem was originally studied by the Chinese mathematician Kwan Mei-Ko in 1960, whose Chinese paper was translated into English in 1962. The alternative name \"Chinese postman problem\" was coined in his honor; different sources credit the coinage either to Alan J. Goldman or Jack Edmonds, both of whom were at the U.S. National Bureau of Standards at the time.\n\nThe undirected route inspection problem may be solved in polynomial time by an algorithm based on the concept of a \"T\"-join.\nLet \"T\" be a subset of the vertex set of a graph. An edge set \"J\" is called a \"T-join if the collection of vertices that have an odd number of neighboring edges in \"J\" is exactly the set \"T\". A \"T\"-join exists whenever every connected component of the graph contains an even number of vertices in \"T\". The \"T-join problem is to find a \"T\"-join with the minimum possible number of edges or the minimum possible total weight.\nFor any \"T\", a smallest \"T\"-join (when it exists) necessarily consists of formula_1 paths that join the vertices of \"T\" in pairs. The paths will be such that the total length or total weight of all of them is as small as possible. In an optimal solution, no two of these paths will share any edge, but they may have shared vertices. A minimum \"T\"-join can be obtained by constructing a complete graph on the vertices of \"T\", with edges that represent shortest paths in the given input graph, and then finding a minimum weight perfect matching in this complete graph. The edges of this matching represent paths in the original graph, whose union forms the desired \"T\"-join.\nBoth constructing the complete graph, and then finding a matching in it, can be done in O(\"n\") computational steps.\n\nFor the route inspection problem, \"T\" should be chosen as the set of all odd-degree vertices. By the assumptions of the problem, the whole graph is connected (otherwise no tour exists), and by the handshaking lemma it has an even number of odd vertices, so a \"T\"-join always exists. Doubling the edges of a \"T\"-join causes the given graph to become an Eulerian multigraph (a connected graph in which every vertex has even degree), from which it follows that it has an Euler tour, a tour that visits each edge of the multigraph exactly once. This tour will be an optimal solution to the route inspection problem.\n\nOn a directed graph, the same general ideas apply, but different techniques must be used. If the graph is Eulerian, one need only find an Euler cycle. If it is not, one must find \"T\"-joins, which in this case entails finding paths from vertices with an in-degree greater than their out-degree to those with an out-degree greater than their in-degree such that they would make in-degree of every vertex equal to its out-degree. This can be solved as an instance of the minimum-cost flow problem in which there is one unit of supply for every unit of excess in-degree, and one unit of demand for every unit of excess out-degree. As such it is solvable in O(|\"V\"||\"E\"|) time. A solution exists if and only if the given graph is strongly connected.\n\nThe windy postman problem is a variant of the route inspection problem in which the input is an undirected graph, but where each edge may have a different cost for traversing it in one direction than for traversing it in the other direction.\nIn contrast to the solutions for directed and undirected graphs, it is NP-complete.\n\nVarious combinatorial problems are reduced to the Chinese Postman Problem, including finding a maximum cut in a planar graph \nand a minimum-mean length circuit in an undirected graph.\n\nA few variants of the Chinese Postman Problem have been studied and shown to be NP-complete.\n\n\n"}
{"id": "56073671", "url": "https://en.wikipedia.org/wiki?curid=56073671", "title": "Solovay–Kitaev theorem", "text": "Solovay–Kitaev theorem\n\nIn the mathematical theory of computation, the Solovay–Kitaev theorem says, roughly, that if a set of single-qubit quantum gates generates a dense subset of SU(2) then that set is guaranteed to fill SU(2) quickly, which means good approximations to any desired gate can be created using fairly short sequences of gates from the generating set. It is one of the most important fundamental results in the field of quantum computation. Robert M. Solovay and Alexei Kitaev jointly came up with and proved the theorem.\n"}
{"id": "28016", "url": "https://en.wikipedia.org/wiki?curid=28016", "title": "Steiner system", "text": "Steiner system\n\nIn combinatorial mathematics, a Steiner system (named after Jakob Steiner) is a type of block design, specifically a with λ = 1 and \"t\" ≥ 2.\n\nA Steiner system with parameters \"t\", \"k\", \"n\", written S(\"t\",\"k\",\"n\"), is an \"n\"-element set \"S\" together with a set of \"k\"-element subsets of \"S\" (called blocks) with the property that each \"t\"-element subset of \"S\" is contained in exactly one block. In an alternate notation for block designs, an S(\"t\",\"k\",\"n\") would be a \"t\"-(\"n\",\"k\",1) design.\n\nThis definition is relatively modern, generalizing the \"classical\" definition of Steiner systems which in addition required that \"k\" = \"t\" + 1. An S(2,3,\"n\") was (and still is) called a \"Steiner triple\" (or \"triad\") \"system\", while an S(3,4,\"n\") was called a \"Steiner quadruple system\", and so on. With the generalization of the definition, this naming system is no longer strictly adhered to.\n\nA long-standing problem in design theory was if any nontrivial (\"t\" < \"k\" < \"n\") Steiner systems have \"t\" ≥ 6; also if infinitely many have \"t\" = 4 or 5. This was solved in the affirmative by Peter Keevash in 2014.\n\nA finite projective plane of order , with the lines as blocks, is an , since it has points, each line passes through points, and each pair of distinct points lies on exactly one line.\n\nA finite affine plane of order , with the lines as blocks, is an . An affine plane of order can be obtained from a projective plane of the same order by removing one block and all of the points in that block from the projective plane. Choosing different blocks to remove in this way can lead to non-isomorphic affine planes.\n\nAn S(2,3,\"n\") is called a Steiner triple system, and its blocks are called triples. It is common to see the abbreviation STS(\"n\") for a Steiner triple system of order \"n\". \nThe total number of pairs is \"n(n-1)/2\", of which three appear in a triple, and so the total number of triples is \"n\"(\"n\"−1)/6. This shows that \"n\" must be of the form \"6k+1\" or \"6k + 3\" for some \"k\". The fact that this condition on \"n\" is sufficient for the existence of an S(2,3,\"n\") was proved by Raj Chandra Bose and T. Skolem. The projective plane of order 2 (the Fano plane) is an STS(7) and the affine plane of order 3 is an STS(9).\n\nUp to isomorphism, the STS(7) and STS(9) are unique, there are two STS(13)s, 80 STS(15)s, and 11,084,874,829 STS(19)s.\n\nWe can define a multiplication on the set \"S\" using the Steiner triple system by setting \"aa\" = \"a\" for all \"a\" in \"S\", and \"ab\" = \"c\" if {\"a\",\"b\",\"c\"} is a triple. This makes \"S\" an idempotent, commutative quasigroup. It has the additional property that \"ab\" = \"c\" implies \"bc\" = \"a\" and \"ca\" = \"b\". Conversely, any (finite) quasigroup with these properties arises from a Steiner triple system. Commutative idempotent quasigroups satisfying this additional property are called \"Steiner quasigroups\".\n\nAn S(3,4,\"n\") is called a Steiner quadruple system. A necessary and sufficient condition for the existence of an S(3,4,\"n\") is that \"n\" formula_1 2 or 4 (mod 6). The abbreviation SQS(\"n\") is often used for these systems.\n\nUp to isomorphism, SQS(8) and SQS(10) are unique, there are 4 SQS(14)s and 1,054,163 SQS(16)s.\n\nAn S(4,5,\"n\") is called a \"Steiner quintuple system\". A necessary condition for the existence of such a system is that \"n\" formula_1 3 or 5 (mod 6) which comes from considerations that apply to all the classical Steiner systems. An additional necessary condition is that \"n\" formula_3 4 (mod 5), which comes from the fact that the number of blocks must be an integer. Sufficient conditions are not known.\n\nThere is a unique Steiner quintuple system of order 11, but none of order 15 or order 17. Systems are known for orders 23, 35, 47, 71, 83, 107, 131, 167 and 243. The smallest order for which the existence is not known (as of 2011) is 21.\n\nIt is clear from the definition of that formula_4. (Equalities, while technically possible, lead to trivial systems.)\n\nIf exists, then taking all blocks containing a specific element and discarding that element gives a \"derived system\" . Therefore, the existence of is a necessary condition for the existence of .\n\nThe number of -element subsets in is formula_5, while the number of -element subsets in each block is formula_6. Since every -element subset is contained in exactly one block, we have formula_7, or \nwhere is the number of blocks. Similar reasoning about -element subsets containing a particular element gives us formula_9, or \nwhere is the number of blocks containing any given element. From these definitions follows the equation formula_12. It is a necessary condition for the existence of that and are integers. As with any block design, Fisher's inequality formula_13 is true in Steiner systems.\n\nGiven the parameters of a Steiner system and a subset of size formula_14, contained in at least one block, one can compute the number of blocks intersecting that subset in a fixed number of elements by constructing a Pascal triangle. In particular, the number of blocks intersecting a fixed block in any number of elements is independent of the chosen block.\n\nThe number of blocks that contain any \"i\"-element set of points is:\n\nIt can be shown that if there is a Steiner system , where is a prime power greater than 1, then formula_1 1 or . In particular, a Steiner triple system must have . And as we have already mentioned, this is the only restriction on Steiner triple systems, that is, for each natural number , systems and exist.\n\nSteiner triple systems were defined for the first time by W.S.B. Woolhouse in 1844 in the Prize question #1733 of Lady's and Gentlemen's Diary. The posed problem was solved by . In 1850 Kirkman posed a variation of the problem known as Kirkman's schoolgirl problem, which asks for triple systems having an additional property (resolvability). Unaware of Kirkman's work, reintroduced triple systems, and as this work was more widely known, the systems were named in his honor.\n\nSeveral examples of Steiner systems are closely related to group theory. In particular, the finite simple groups called Mathieu groups arise as automorphism groups of Steiner systems:\n\n\nThere is a unique S(5,6,12) Steiner system; its automorphism group is the Mathieu group M, and in that context it is denoted by W.\n\nThere are different ways to construct an S(5,6,12) system.\n\nThis construction is due to Carmichael (1937).\n\nAdd a new element, call it , to the 11 elements of the finite field (that is, the integers mod 11). This set, , of 12 elements can be formally identified with the points of the projective line over . Call the following specific subset of size 6,\na \"block\" (it contains together with the 5 nonzero squares in ). From this block, we obtain the other blocks of the (5,6,12) system by repeatedly applying the linear fractional transformations:\nwhere are in and .\nWith the usual conventions of defining and , these functions map the set onto itself. In geometric language, they are projectivities of the projective line. They form a group under composition which is the projective special linear group (2,11) of order 660. There are exactly five elements of this group that leave the starting block fixed setwise, namely those such that and so that . So there will be 660/5 = 132 images of that block. As a consequence of the multiply transitive property of this group acting on this set, any subset of five elements of will appear in exactly one of these 132 images of size six.\n\nAn alternative construction of W is obtained by use of the 'kitten' of R.T. Curtis, which was intended as a \"hand calculator\" to write down blocks one at a time. The kitten method is based on completing patterns in a 3x3 grid of numbers, which represent an affine geometry on the vector space FxF, an S(2,3,9) system.\n\nThe relations between the graph factors of the complete graph K generate an S(5,6,12). A K graph has six different 1-factorizations (ways to partition the edges into disjoint perfect matchings), and also six vertices. The set of vertices (labeled 123456) and the set of factorizations (labeled ABCDEF) provide one block each. Every pair of factorizations has exactly one perfect matching in common. Suppose factorizations A and B have the common matching with edges 12, 34 and 56. Add three new blocks AB3456, 12AB56, and 1234AB, replacing each edge in the common matching with the factorization labels in turn. Similarly add three more blocks 12CDEF, 34CDEF, and 56CDEF, replacing the factorization labels by the corresponding edge labels of the common matching. Do this for all 15 pairs of factorizations to add 90 new blocks. Finally take the full set of 12C6 = 924 combinations of 6 objects out of 12, and discard any combination that has 5 or more objects in common with any of the 92 blocks generated so far. Exactly 40 blocks remain, resulting in 2+90+40 = 132 blocks of the S(5,6,12).\n\nThe Steiner system S(5, 8, 24), also known as the Witt design or Witt geometry, was first described by and rediscovered by . This system is connected with many of the sporadic simple groups and with the exceptional 24-dimensional lattice known as the Leech lattice.\n\nThe automorphism group of S(5, 8, 24) is the Mathieu group M, and in that context the design is denoted W (\"W\" for \"Witt\")\n\nThere are many ways to construct the S(5,8,24). Two methods are described here:\n\nAll 8-element subsets of a 24-element set are generated in lexicographic order, and any such subset which differs from some subset already found in fewer than four positions is discarded.\n\nThe list of octads for the elements 01, 02, 03, ..., 22, 23, 24 is then:\n\nEach single element occurs 253 times somewhere in some octad. Each pair occurs 77 times. Each triple occurs 21 times. Each quadruple (tetrad) occurs 5 times. Each quintuple (pentad) occurs once. Not every hexad, heptad or octad occurs.\n\nAll 24-bit binary strings are generated in lexicographic order, and any such string that differs from some earlier one in fewer than 8 positions is discarded. The result looks like this:\n\nThe list contains 4096 items, which are each code words of the extended binary Golay code. They form a group under the XOR operation. One of them has zero 1-bits, 759 of them have eight 1-bits, 2576 of them have twelve 1-bits, 759 of them have sixteen 1-bits, and one has twenty-four 1-bits. The 759 8-element blocks of the S(5,8,24) (called octads) are given by the patterns of 1's in the code words with eight 1-bits.\n\n\n\n"}
{"id": "44774993", "url": "https://en.wikipedia.org/wiki?curid=44774993", "title": "Susan Friedlander", "text": "Susan Friedlander\n\nSusan Jean Friedlander (née Poate; born January 26, 1946) is an American mathematician. Her research concerns mathematical fluid dynamics, the Euler equations and the Navier-Stokes equations.\n\nFriedlander graduated from University College, London with a BS in Mathematics in 1967. She was awarded a Kennedy Scholarship to study at MIT, where she earned an MS in 1970. She completed her doctorate in 1972 from Princeton University under the supervision of Louis Norberg Howard.\n\nFrom 1972–1974, Friedlander was a Visiting Member at the Courant Institute of Mathematical Sciences, followed by a year as an instructor at Princeton University. In 1975, she joined the faculty in the Mathematics department at the University of Illinois at Chicago. In 2007, she moved to the University of Southern California where she is Professor of Mathematics and the Director of the Center for Applied Mathematical Sciences.\n\nFrom 1996–2010, Friedlander served as an officer of the American Mathematical Society in the role of Associate Secretary. In 2005, she was appointed the first female Editor-in-Chief of the \"Bulletin of the American Mathematical Society\". Her other leadership activities include membership of the Scientific Advisory Committee of the Mathematical Sciences Research Institute (2001–2006), the Board of Mathematical Sciences and their Applications (2008–2011), the Section A Steering Committee of the American Association for the Advancement of Science (2013–2015), and the MIT Mathematics Department Visiting Committee (2013–2021). She is currently the Chair of the Mathematical Council of the Americas.\n\n\nFriedlander is married to mathematician Eric Friedlander.\n\n"}
{"id": "57341498", "url": "https://en.wikipedia.org/wiki?curid=57341498", "title": "Thyra Eibe", "text": "Thyra Eibe\n\nThyra Eibe (3 November 1866 – 4 January 1955) was a Danish mathematician and translator, the first woman to earn a mathematics degree from the University of Copenhagen. She is known for her translation of Euclid's \"Elements\" into the Danish Language. In undertaking this translation, she was motivated by the earlier work of Danish historian Johan Ludvig Heiberg, who published an edition of Euclid's \"Elements\" in its original Greek, with translations into Latin.\n\nEibe was one of ten children of a Copenhagen bookseller. After completing a degree in historical linguistics in 1889 from N. Zahle's School (then a girls' school), Eibe studied mathematics at the University of Copenhagen, and earned a cand.mag. there in 1895. She returned to Zahle's School as a teacher, also teaching boys at Slomann's School and becoming the first woman to become an advanced mathematics teacher for boys in Denmark. In 1898 she moved to H. Adler Community College, later to become the , where she remained until 1934, serving as principal for a year in 1929–1930.\n\nAs well as her translations, Eibe wrote several widely-used Danish mathematics textbooks.\n\nIn 1942, she was given the Tagea Brandt Rejselegat, an award for Danish woman who have made a significant contribution in science, literature or art.\n"}
{"id": "9748518", "url": "https://en.wikipedia.org/wiki?curid=9748518", "title": "Toda oscillator", "text": "Toda oscillator\n\nIn physics, the Toda oscillator is a special kind of nonlinear oscillator. It represents a chain of particles with exponential potential interaction between neighbors. These concepts are named after Morikazu Toda. The Toda oscillator is used as a simple model to understand the phenomenon of self-pulsation, which is a quasi-periodic pulsation of the output intensity of a solid-state laser in the transient regime.\n\nThe Toda oscillator is a dynamical system of any origin, which can be described with dependent coordinate formula_1 and independent coordinate formula_2, characterized in that the evolution along independent coordinate formula_2 can be approximated with equation\n\\fracx}+\nD(x)\\frac\n"}
{"id": "15894465", "url": "https://en.wikipedia.org/wiki?curid=15894465", "title": "William Farish (chemist)", "text": "William Farish (chemist)\n\nWilliam Farish (1759–1837) was a British scientist who was a professor of Chemistry and Natural Philosophy at the University of Cambridge, known for the development of the method of isometric projection and development of the first written university examination.\n\nFarish was probably born around mid-April, as he was baptized on 21 April 1759. \nFarish's father was the Reverend James Farish (1714–1783), vicar of Stanwix near Carlisle. Farish himself was educated at Carlisle Grammar School, entered Magdalene College, Cambridge, as a sizar in 1774, and graduated Senior Wrangler and first in Smith's Prize in 1778. As tutor in 1792, Farish developed the concept of grading students' work quantitatively.\n\nHe was Professor of Chemistry at Cambridge from 1794 to 1813, lecturing on chemistry's practical application. Farish's lectures as professor of chemistry, which were oriented towards natural philosophy while the professor of natural and experimental philosophy F. J. H. Wollaston (1762–1828) gave very chemically oriented lectures. From 1813 to 1837 Farish was Jacksonian Professor of Natural Philosophy. In 1819 Professor Farish became the first president of the Cambridge Philosophical Society.\n\nFarish was also Vicar of St Giles' and St Peter from 1800 to 1837. He extensively remodelled St Giles' Church, Cambridge, increasing the accommodation from 100 to 600 seats.\n\nAt Cambridge University, according to Hilkens (1967), Farish was \"the first man to teach the construction of machines as a subject in its own right instead of merely using mechanisms as examples to illustrate the principles of theoretical physics or applied mathematics.\" He further became \"famous for his work in applying chemistry and mechanical science to arts and manufactures\".\n\nIn his lectures on the mechanical principles of machinery used in manufacturing industries, Farish often used models to illustrated particular principles. This models were often especially assembled for these lectures and disassembled for storage afterwards. In order to explain how these models were to be assembled he had developed a drawing technique, which he called \"Isometrical Perspective\".\n\nAlthough the concept of an isometric had existed in a rough way for centuries, William Farish is generally regarded as the first to provide rules for isometric drawing. In the 1822 paper \"On Isometrical Perspective\" Farish recognized the \"need for accurate technical working drawings free of optical distortion. This would lead him to formulate isometry. Isometry means \"equal measures\" because the same scale is used for height, width, and depth\".\n\nFrom the middle of the 19th century, according to Jan Krikke (2006) isometry became an \"invaluable tool for engineers, and soon thereafter axonometry and isometry were incorporated in the curriculum of architectural training courses in Europe and the U.S. The popular acceptance of axonometry came in the 1920s, when modernist architects from the Bauhaus and De Stijl embraced it\". De Stijl architects like Theo van Doesburg used \"axonometry for their architectural designs, which caused a sensation when exhibited in Paris in 1923\".\n\n"}
{"id": "614947", "url": "https://en.wikipedia.org/wiki?curid=614947", "title": "Yousef Alavi", "text": "Yousef Alavi\n\nYousef Alavi (March 19, 1928 – May 21, 2013) was an Iranian born American mathematician who specialized in combinatorics and graph theory. He received his Ph.D. from Michigan State University in 1958. He was a professor of mathematics at Western Michigan University from 1958 until his retirement in 1996; he chaired the department from 1989 to 1992.\n\nIn 1987 he received the first Distinguished Service Award of the Michigan Section of the Mathematical Association of America due to his 30 years of service to the MAA; at that time, the Michigan House and Senate issued a special resolution honoring him.\n"}
