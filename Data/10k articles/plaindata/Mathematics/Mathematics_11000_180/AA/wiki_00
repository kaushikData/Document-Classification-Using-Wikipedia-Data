{"id": "10361630", "url": "https://en.wikipedia.org/wiki?curid=10361630", "title": "Affine Hecke algebra", "text": "Affine Hecke algebra\n\nIn mathematics, an affine Hecke algebra is the algebra associated to an affine Weyl group, and can be used to prove Macdonald's constant term conjecture for Macdonald polynomials.\n\nLet formula_1 be a Euclidean space of a finite dimension and formula_2 an affine root system on formula_1. An affine Hecke algebra is a certain associative algebra that deforms the group algebra formula_4 of the Weyl group formula_5 of formula_2 (the affine Weyl group). It is usually denoted by formula_7, where formula_8 is multiplicity function that plays the role of deformation parameter. For formula_9 the affine Hecke algebra formula_7 indeed reduces to formula_4.\n\nIvan Cherednik introduced generalizations of affine Hecke algebras, the so-called double affine Hecke algebra (usually referred to as DAHA). Using this he was able to give a proof of Macdonald's constant term conjecture for Macdonald polynomials (building on work of Eric Opdam). Another main inspiration for Cherednik to consider the double affine Hecke algebra was the quantum KZ equations.\n\n"}
{"id": "45431688", "url": "https://en.wikipedia.org/wiki?curid=45431688", "title": "Alexander Meduna", "text": "Alexander Meduna\n\nAlexander Meduna (born 1957 in Olomouc, Czech Republic) is a theoretical computer scientist and expert on compiler design, formal languages and automata. He is a professor of Computer Science at the Brno University of Technology. Formerly, he taught theoretical computer science at various European and American universities, including the University of Missouri, where he spent a decade teaching advanced topics of formal language theory. He is the author of several books and over sixty papers related to the subject matter.\n\nMeduna is also an artist, who is primarily interested in visual art. He had several exhibitions in the USA and Europe. He often performs poetry reading as well.\n\n\n"}
{"id": "11176721", "url": "https://en.wikipedia.org/wiki?curid=11176721", "title": "Alfred George Greenhill", "text": "Alfred George Greenhill\n\nSir (Alfred) George Greenhill, F.R.S. (29 November 1847 in London – 10 February 1927 in London), was a British mathematician.\n\nGeorge Greenhill was educated at Christ's Hospital School and from there he went up to St John's College, Cambridge in 1866. In 1876, Greenhill was appointed professor of mathematics at the Royal Military Academy (RMA) at Woolwich, London, UK. He held this chair until his retirement in 1908. His 1892 textbook on applications of elliptic functions is of acknowledged excellence. He was one of the world's leading experts on applications of elliptic integrals in electromagnetic theory. He was a Plenary Speaker of the ICM in 1904 at Heidelberg and an Invited Speaker of the ICM in 1908 at Rome, in 1920 at Strasbourg, and in 1924 at Toronto.\n\nIn 1879, Greenhill developed a rule of thumb for calculating the optimal twist rate for lead-core bullets. This shortcut uses the bullet's length, needing no allowances for weight or nose shape. Greenhill applied this theory to account for the steadiness of flight conferred upon an elongated projectile by rifling. The eponymous \"Greenhill Formula\", still used today, is:\nformula_1\n\nwhere:\n\nThe original value of C was 150, which yields a twist rate in inches per turn, when given the diameter D and the length L of the bullet in inches. This works to velocities of about 840 m/s (2800 ft/s); above those velocities, a C of 180 should be used. For instance, with a velocity of 600 m/s (2000 ft/s), a diameter of and a length of , the Greenhill formula would give a value of 25, which means 1 turn in .\n\n\n"}
{"id": "11826062", "url": "https://en.wikipedia.org/wiki?curid=11826062", "title": "Auxiliary function", "text": "Auxiliary function\n\nIn mathematics, auxiliary functions are an important construction in transcendental number theory. They are functions that appear in most proofs in this area of mathematics and that have specific, desirable properties, such as taking the value zero for many arguments, or having a zero of high order at some point.\n\nAuxiliary functions are not a rigorously defined kind of function, rather they are functions which are either explicitly constructed or at least shown to exist and which provide a contradiction to some assumed hypothesis, or otherwise prove the result in question. Creating a function during the course of a proof in order to prove the result is not a technique exclusive to transcendence theory, but the term \"auxiliary function\" usually refers to the functions created in this area.\n\nBecause of the naming convention mentioned above, auxiliary functions can be dated back to their source simply by looking at the earliest results in transcendence theory. One of these first results was Liouville's proof that transcendental numbers exist when he showed that the so called Liouville numbers were transcendental. He did this by discovering a transcendence criterion which these numbers satisfied. To derive this criterion he started with a general algebraic number α and found some property that this number would necessarily satisfy. The auxiliary function he used in the course of proving this criterion was simply the minimal polynomial of α, which is the irreducible polynomial \"f\" with integer coefficients such that \"f\"(α) = 0. This function can be used to estimate how well the algebraic number α can be estimated by rational numbers \"p\"/\"q\". Specifically if α has degree \"d\" at least two then he showed that\n\nand also, using the mean value theorem, that there is some constant depending on α, say \"c\"(α), such that\n\nCombining these results gives a property that the algebraic number must satisfy; therefore any number not satisfying this criterion must be transcendental.\n\nThe auxiliary function in Liouville's work is very simple, merely a polynomial that vanishes at a given algebraic number. This kind of property is usually the one that auxiliary functions satisfy. They either vanish or become very small at particular points, which is usually combined with the assumption that they do not vanish or can't be too small to derive a result.\n\nAnother simple, early occurrence is in Fourier's proof of the irrationality of \"e\", though the notation used usually disguises this fact. Fourier's proof used the power series of the exponential function:\nBy truncating this power series after, say, \"N\" + 1 terms we get a polynomial with rational coefficients of degree \"N\" which is in some sense \"close\" to the function \"e\". Specifically if we look at the auxiliary function defined by the remainder:\nthen this function—an exponential polynomial—should take small values for \"x\" close to zero. If \"e\" is a rational number then by letting \"x\" = 1 in the above formula we see that \"R\"(1) is also a rational number. However, Fourier proved that \"R\"(1) could not be rational by eliminating every possible denominator. Thus \"e\" cannot be rational.\n\nHermite extended the work of Fourier by approximating the function \"e\" not with a polynomial but with a rational function, that is a quotient of two polynomials. In particular he chose polynomials \"A\"(\"x\") and \"B\"(\"x\") such that the auxiliary function \"R\" defined by\n\ncould be made as small as he wanted around \"x\" = 0. But if \"e\" were rational then \"R\"(\"r\") would have to be rational with a particular denominator, yet Hermite could make \"R\"(\"r\") too small to have such a denominator, hence a contradiction.\n\nTo prove that \"e\" was in fact transcendental, Hermite took his work one step further by approximating not just the function \"e\", but also the functions \"e\" for integers \"k\" = 1...,\"m\", where he assumed \"e\" was algebraic with degree \"m\". By approximating \"e\" by rational functions with integer coefficients and with the same denominator, say \"A\"(\"x\") / \"B\"(\"x\"), he could define auxiliary functions \"R\"(\"x\") by\n\nFor his contradiction Hermite supposed that \"e\" satisfied the polynomial equation with integer coefficients \"a\" + \"a\"\"e\" + ... + \"a\"\"e\" = 0. Multiplying this expression through by \"B\"(1) he noticed that it implied\n\nThe right hand side is an integer and so, by estimating the auxiliary functions and proving that 0 < |\"R\"| < 1 he derived the necessary contradiction.\n\nThe auxiliary functions sketched above can all be explicitly calculated and worked with. A breakthrough by Axel Thue and Carl Ludwig Siegel in the twentieth century was the realisation that these functions don't necessarily need to be explicitly known – it can be enough to know they exist and have certain properties. Using the Pigeonhole Principle Thue, and later Siegel, managed to prove the existence of auxiliary functions which, for example, took the value zero at many different points, or took high order zeros at a smaller collection of points. Moreover they proved it was possible to construct such functions without making the functions too large. Their auxiliary functions were not explicit functions, then, but by knowing that a certain function with certain properties existed, they used its properties to simplify the transcendence proofs of the nineteenth century and give several new results.\n\nThis method was picked up on and used by several other mathematicians, including Alexander Gelfond and Theodor Schneider who used it independently to prove the Gelfond–Schneider theorem. Alan Baker also used the method in the 1960s for his work on linear forms in logarithms and ultimately Baker's theorem. Another example of the use of this method from the 1960s is outlined below.\n\nLet β equal the cube root of \"b/a\" in the equation \"ax\" + \"bx\" = \"c\" and assume \"m\" is an integer that satisfies \"m\" + 1 > 2\"n\"/3 ≥ \"m\" ≥ 3 where \"n\" is a positive integer.\n\nThen there exists\n\nsuch that\n\nThe auxiliary polynomial theorem states\n\nIn the 1960s Serge Lang proved a result using this non-explicit form of auxiliary functions. The theorem implies both the Hermite–Lindemann and Gelfond–Schneider theorems. The theorem deals with a number field \"K\" and meromorphic functions \"f\"...,\"f\" of order at most \"ρ\", at least two of which are algebraically independent, and such that if we differentiate any of these functions then the result is a polynomial in all of the functions. Under these hypotheses the theorem states that if there are \"m\" distinct complex numbers ω...,ω such that \"f\" (ω) is in \"K\" for all combinations of \"i\" and \"j\", then \"m\" is bounded by\n\nTo prove the result Lang took two algebraically independent functions from \"f\"...,\"f\", say \"f\" and \"g\", and then created an auxiliary function which was simply a polynomial \"F\" in \"f\" and \"g\". This auxiliary function could not be explicitly stated since \"f\" and \"g\" are not explicitly known. But using Siegel's lemma Lang showed how to make \"F\" in such a way that it vanished to a high order at the \"m\" complex numbers\nω...,ω. Because of this high order vanishing it can be shown that a high-order derivative of \"F\" takes a value of small size one of the ωs, \"size\" here referring to an algebraic property of a number. Using the maximum modulus principle Lang also found a separate way to estimate the absolute values of derivatives of \"F\", and using standard results comparing the size of a number and its absolute value he showed that these estimates were contradicted unless the claimed bound on \"m\" holds.\n\nAfter the myriad of successes gleaned from using existent but not explicit auxiliary functions, in the 1990s Michel Laurent introduced the idea of interpolation determinants. These are alternants – determinants of matrices of the form\nwhere φ are a set of functions interpolated at a set of points ζ. Since a determinant is just a polynomial in the entries of a matrix, these auxiliary functions succumb to study by analytic means. A problem with the method was the need to choose a basis before the matrix could be worked with. A development by Jean-Benoît Bost removed this problem with the use of Arakelov theory, and research in this area is ongoing. The example below gives an idea of the flavour of this approach.\n\nOne of the simpler applications of this method is a proof of the real version of the Hermite–Lindemann theorem. That is, if α is a non-zero, real algebraic number, then \"e\" is transcendental. First we let \"k\" be some natural number and \"n\" be a large multiple of \"k\". The interpolation determinant considered is the determinant Δ of the \"n\"×\"n\" matrix\nThe rows of this matrix are indexed by 1 ≤ \"i\" ≤ \"n\"/\"k\" and 1 ≤ \"i\" ≤ \"k\", while the columns are indexed by 1 ≤ \"j\" ≤ \"n\" and 1 ≤ \"j\" ≤ \"n\". So the functions in our matrix are monomials in \"x\" and \"e\" and their derivatives, and we are interpolating at the \"k\" points 0,α,2α...,(\"k\" − 1)α. Assuming that \"e\" is algebraic we can form the number field Q(α,\"e\") of degree \"m\" over Q, and then multiply Δ by a suitable denominator as well as all its images under the embeddings of the field Q(α,\"e\") into C. For algebraic reasons this product is necessarily an integer, and using arguments relating to Wronskians it can be shown that it is non-zero, so its absolute value is an integer Ω ≥ 1.\n\nUsing a version of the mean value theorem for matrices it is possible to get an analytic bound on Ω as well, and in fact using big-O notation we have\nThe number \"m\" is fixed by the degree of the field Q(α,\"e\"), but \"k\" is the number of points we are interpolating at, and so we can increase it at will. And once \"k\" > 2(\"m\" + 1)/3 we will have Ω → 0, eventually contradicting the established condition Ω ≥ 1. Thus \"e\" cannot be algebraic after all.\n\n"}
{"id": "2908018", "url": "https://en.wikipedia.org/wiki?curid=2908018", "title": "Behrens–Fisher problem", "text": "Behrens–Fisher problem\n\nIn statistics, the Behrens–Fisher problem, named after Walter Behrens and Ronald Fisher, is the problem of interval estimation and hypothesis testing concerning the difference between the means of two normally distributed populations when the variances of the two populations are not assumed to be equal, based on two independent samples.\n\nOne difficulty with discussing the Behrens–Fisher problem and proposed solutions, is that there are many different interpretations of what is meant by \"the Behrens–Fisher problem\". These differences involve not only what is counted as being a relevant solution, but even the basic statement of the context being considered.\n\nLet \"X\", ..., \"X\" and \"Y\", ..., \"Y\" be i.i.d. samples from two populations which both come from the same location-scale family of distributions. The scale parameters are assumed to be unknown and not necessarily equal, and the problem is to assess whether the location parameters can reasonably be treated as equal. Lehmann states that \"the Behrens–Fisher problem\" is used both for this general form of model when the family of distributions is arbitrary and for when the restriction to a normal distribution is made. While Lehmann discusses a number of approaches to the more general problem, mainly based on nonparametrics, most other sources appear to use \"the Behrens–Fisher problem\" to refer only to the case where the distribution is assumed to be normal: most of this article makes this assumption.\n\nSolutions to the Behrens–Fisher problem have been presented that make use of either a classical or a Bayesian inference point of view and either solution would be notionally invalid judged from the other point of view. If consideration is restricted to classical statistical inference only, it is possible to seek solutions to the inference problem that are simple to apply in a practical sense, giving preference to this simplicity over any inaccuracy in the corresponding probability statements. Where exactness of the significance levels of statistical tests is required, there may be an additional requirement that the procedure should make maximum use of the statistical information in the dataset. It is well known that an exact test can be gained by randomly discarding data from the larger dataset until the sample sizes are equal, assembling data in pairs and taking differences, and then using an ordinary t-test to test for the mean-difference being zero: clearly this would not be \"optimal\" in any sense.\n\nThe task of specifying interval estimates for this problem is one where a frequentist approach fails to provide an exact solution, although some approximations are available. Standard Bayesian approaches also fail to provide an answer that can be expressed as straightforward simple formulae, but modern computational methods of Bayesian analysis do allow essentially exact solutions to be found. Thus study of the problem can be used to elucidate the differences between the frequentist and Bayesian approaches to interval estimation.\n\nRonald Fisher in 1935 introduced fiducial inference in order to apply it to this problem. He referred to an earlier paper by Walter Ulrich Behrens from 1929. Behrens and Fisher proposed to find the probability distribution of\n\nwhere formula_2 and formula_3 are the two sample means, and \"s\" and \"s\" are their standard deviations. See Behrens–Fisher distribution. Fisher approximated the distribution of this by ignoring the random variation of the relative sizes of the standard deviations,\n\nFisher's solution provoked controversy because it did not have the property that the hypothesis of equal means would be rejected with probability α if the means were in fact equal. Many other methods of treating the problem have been proposed since, and the effect on the resulting confidence intervals have been investigated.\n\nA widely used method is that of B. L. Welch, who, like Fisher, was at University College London. The variance of the mean difference\n\nresults in\n\nWelch (1938) approximated the distribution of formula_7 by the Type III Pearson distribution (a scaled chi-squared distribution) whose first two moments agree with that of formula_7. This applies to the following number of degrees of freedom (d.f.), which is generally non-integer:\n\nUnder the null hypothesis of equal expectations, , the distribution of the Behrens-Fisher statistic \"T\", which also depends on the variance ratio \"σ\"/\"σ\", could now be approximated by Student's t distribution with these \"ν\" degrees of freedom. But this \"ν\" contains the population variances \"σ\", and these are unknown. The following estimate only replaces the population variances by the sample variances:\n\nThis formula_11 is a random variable. A t distribution with a random number of degrees of freedom does not exist. Nevertheless, the Behrens-Fisher \"T\" can be compared with a corresponding quantile of Student's t distribution with these estimated number of degrees of freedom, formula_11, which is generally non-integer. In this way, the boundary between acceptance and rejection region of the test statistic \"T\" is calculated based on the empirical variances \"s\", in a way that is a smooth function of these.\n\nThis method also does not give exactly the nominal rate, but is generally not too far off. However, if the population variances are equal, or if the samples are rather small and the population variances can be assumed to be approximately equal, it is more accurate to use Student's t-test.\n\nA number of different approaches to the general problem have been proposed, some of which claim to \"solve\" some version of the problem. Among these are,\nIn Dudewicz’s comparison of selected methods, it was found that the Dudewicz–Ahmed procedure is recommended for practical use.\n\nA minor variant of the Behrens–Fisher problem has been studied. In this instance the problem is, assuming that the two population-means are in fact the same, to make inferences about the common mean: for example, one could require a confidence interval for the common mean.\n\nThe immediate generalisation of the problem involves multivariate normal distributions with unknown covariance matrices, and is known as the multivariate Behrens–Fisher problem. The nonparametric Behrens-Fisher problem does not assume that the distributions are normal.\n\n\n"}
{"id": "13953265", "url": "https://en.wikipedia.org/wiki?curid=13953265", "title": "Bicone", "text": "Bicone\n\nA bicone or dicone (\"bi- \"comes from Latin,\" di-\" from Greek) is the three-dimensional surface of revolution of a rhombus around one of its axes of symmetry. Equivalently, a bicone is the surface created by joining two congruent right circular cones base-to-base.\n\nA bicone has circular symmetry and orthogonal bilateral symmetry.\n\nFor a circular bicone with radius \"R\" and height center-to-top \"H\", the formula for volume becomes\n\nFor a right circular cone, the surface area is \n\nA \"bicone\" can be seen as a polyhedral limiting case of an n-gonal bipyramid where \"n\" approaches infinity. It can also be seen as a dual of a cylinder as an infinite-side prism.\n\n"}
{"id": "1830142", "url": "https://en.wikipedia.org/wiki?curid=1830142", "title": "Bijection, injection and surjection", "text": "Bijection, injection and surjection\n\nIn mathematics, injections, surjections and bijections are classes of functions distinguished by the manner in which \"arguments\" (input expressions from the domain) and \"images\" (output expressions from the codomain) are related or \"mapped to\" each other.\n\nA function maps elements from its domain to elements in its codomain. Given a function formula_1\n\n\n\n\nAn injective function need not be surjective (not all elements of the codomain may be associated with arguments), and a surjective function need not be injective (some images may be associated with \"more than one\" argument). The four possible combinations of injective and surjective features are illustrated in the adjacent diagrams.\n\nA function is injective (one-to-one) if each possible element of the codomain is mapped to by at most one argument. Equivalently, a function is injective if it maps distinct arguments to distinct images. An injective function is an injection. The formal definition is the following.\n\n\nA function is surjective (onto) if each possible image is mapped to by at least one argument. In other words, each element in the codomain has non-empty preimage. Equivalently, a function is surjective if its image is equal to its codomain. A surjective function is a surjection. The formal definition is the following.\n\n\nA function is bijective if it is both injective and surjective. A bijective function is a bijection (one-to-one correspondence). A function is bijective if and only if every possible image is mapped to by exactly one argument. This equivalent condition is formally expressed as follow.\n\n\nSuppose you want to define what it means for two sets to \"have the same number of elements\". One way to do this is to say that two sets \"have the same number of elements\" if and only if all the elements of one set can be paired with the elements of the other, in such a way that each element is paired with exactly one element. Accordingly, we can define two sets to \"have the same number of elements\" if there is a bijection between them. We say that the two sets have the same cardinality.\n\nLikewise, we can say that set formula_16 \"has fewer than or the same number of elements\" as set formula_17 if there is an injection from formula_16 to formula_17. We can also say that set formula_16 \"has fewer than the number of elements\" in set formula_17 if there is an injection from formula_16 to formula_17 but not a bijection between formula_16 and formula_17.\n\nIt is important to specify the domain and codomain of each function since by changing these, functions which we think of as the same may have different \"jectivity\". \n\n\n\n\n\n\nIn the category of sets, injections, surjections, and bijections correspond precisely to monomorphisms, epimorphisms, and isomorphisms, respectively.\n\nThis terminology was originally coined by the Bourbaki group.\n\n\n"}
{"id": "7185509", "url": "https://en.wikipedia.org/wiki?curid=7185509", "title": "Category of finite-dimensional Hilbert spaces", "text": "Category of finite-dimensional Hilbert spaces\n\nIn mathematics, the category FdHilb has all finite-dimensional Hilbert spaces for objects and the linear transformations between them as morphisms.\n\nThis category\n\nAccording to a theorem of Selinger, the category of finite-dimensional Hilbert spaces is complete in the dagger compact category. Many ideas from Hilbert spaces, such as the no-cloning theorem, hold in general for dagger compact categories. See that article for additional details.\n"}
{"id": "42392462", "url": "https://en.wikipedia.org/wiki?curid=42392462", "title": "Chasles' theorem (kinematics)", "text": "Chasles' theorem (kinematics)\n\nIn kinematics, Chasles' theorem, or Mozzi–Chasles' theorem, says that the most general rigid body displacement can be produced by a translation along a line (called its screw axis or Mozzi axis) followed (or preceded) by a rotation about an axis parallel to that line.\n\nThe proof that a spatial displacement can be decomposed into a rotation and slide around and along a line is attributed to the astronomer and mathematician Giulio Mozzi (1763), in fact the screw axis is traditionally called asse di Mozzi in Italy. However, most textbooks refer to a subsequent similar work by Michel Chasles dating 1830. Several other scholars contemporaries of M. Chasles obtained the same or similar results around that time, including G. Giorgini, Cauchy, Poinsot, Poisson and Rodrigues. An account of the 1763 proof by Giulio Mozzi and some of its history can be found here.\n\nMozzi considers a rigid body undergoing first a rotation about an axis passing through the center of mass and then a translation of displacement D in an arbitrary direction. Any rigid motion can be accomplished in this way due to a theorem by Euler on the existence of an axis of rotation. \nThe displacement D of the center of mass can be decomposed into components parallel and perpendicular to the axis. The perpendicular (and parallel) component acts on all points of the rigid body but Mozzi shows that for some points the previous rotation acted exactly with an opposite displacement, so those points are translated parallel to the axis of rotation. These points lie on the Mozzi axis through which the rigid motion can be accomplished through a screw motion.\n\nAnother elementary proof of Mozzi–Chasles' theorem was given by E. T. Whittaker in 1904. Suppose \"A\" is to be transformed into \"B\". Whittaker suggests that line \"AK\" be selected parallel to the axis of the given rotation, with \"K\" the foot of a perpendicular from \"B\". The appropriate screw displacement is about an axis parallel to \"AK\" such that \"K\" is moved to \"B\". The method corresponds to Euclidean plane isometry where a composition of rotation and translation can be replaced by rotation about an appropriate center. In Whittaker's terms, \"A rotation about any axis is equivalent to a rotation through the same angle about any axis parallel to it, together with a simple translation in a direction perpendicular to the axis.\"\n\n"}
{"id": "43222035", "url": "https://en.wikipedia.org/wiki?curid=43222035", "title": "Collapsing algebra", "text": "Collapsing algebra\n\nIn mathematics, a collapsing algebra is a type of Boolean algebra sometimes used in forcing to reduce (\"collapse\") the size of cardinals. The posets used to generate collapsing algebras were introduced by .\n\nThe collapsing algebra of λ is a complete Boolean algebra with at least λ elements but generated by a countable number of elements. As the size of countably generated complete Boolean algebras is unbounded, this shows that there is no free complete Boolean algebra on a countable number of elements.\n\nThere are several slightly different sorts of collapsing algebras. \n\nIf κ and λ are cardinals, then the Boolean algebra of regular open sets of the product space κ is a collapsing algebra. Here κ and λ are both given the discrete topology. There are several different options for the topology of \nκ. The simplest option is to take the usual product topology. Another option is to take the topology generated by open sets consisting of functions whose value is specified on less than λ elements of λ.\n\n"}
{"id": "2574552", "url": "https://en.wikipedia.org/wiki?curid=2574552", "title": "Compactly-supported homology", "text": "Compactly-supported homology\n\nIn mathematics, a homology theory in algebraic topology is compactly supported if, in every degree \"n\", the relative homology group H(\"X\", \"A\") of every pair of spaces\n\nis naturally isomorphic to the direct limit of the \"n\"th relative homology groups of pairs (\"Y\", \"B\"), where \"Y\" varies over compact subspaces of \"X\" and \"B\" varies over compact subspaces of \"A\".\n\nSingular homology is compactly supported, since each singular chain is a finite sum of simplices, which are compactly supported. Strong homology is not compactly supported.\n\nIf one has defined a homology theory over compact pairs, it is possible to extend it into a compactly supported homology theory in the wider category of Hausdorff pairs (\"X\", \"A\") with \"A\" closed in \"X\", by defining that the homology of a Hausdorff pair (\"X\", \"A\") is the direct limit over pairs (\"Y\", \"B\"), where \"Y\", \"B\" are compact, \"Y\" is a subset of \"X\", and \"B\" is a subset of \"A\".\n"}
{"id": "499429", "url": "https://en.wikipedia.org/wiki?curid=499429", "title": "D'Alembert's principle", "text": "D'Alembert's principle\n\nD'Alembert's principle, also known as the Lagrange–d'Alembert principle, is a statement of the fundamental classical laws of motion. It is named after its discoverer, the French physicist and mathematician Jean le Rond d'Alembert. It is the dynamic analogue to the \"principle of virtual work for applied forces\" in a static system and in fact is more general than Hamilton's principle, avoiding restriction to holonomic systems. A holonomic constraint depends only on the coordinates and time. It does not depend on the velocities. If the negative terms in accelerations are recognized as \"inertial forces\", the statement of d'Alembert's principle becomes \"The total virtual work of the impressed forces plus the inertial forces vanishes for reversible displacements\". The principle does not apply for irreversible displacements, such as sliding friction, and more general specification of the irreversibility is required.\n\nThe principle states that the sum of the differences between the forces acting on a system of mass particles and the time derivatives of the momenta of the system itself projected onto any virtual displacement consistent with the constraints of the system is zero. Thus, in symbols d'Alembert's principle is written as following,\n\nwhere :\n\nThis above equation is often called d'Alembert's principle, but it was first written in this variational form by Joseph Louis Lagrange. D'Alembert's contribution was to demonstrate that in the totality of a dynamic system the forces of constraint vanish. That is to say that the generalized forces formula_2 need not include constraint forces. It is equivalent to the somewhat more cumbersome Gauss's principle of least constraint.\n\nThe general statement of d'Alembert's principle mentions \"the time derivatives of the momenta of the system\". The momentum of the \"i\"-th mass is the product of its mass and velocity:\n\nand its time derivative is\n\nIn many applications, the masses are constant and this equation reduces to\n\nwhich appears in the formula given above. However, some applications involve changing masses (for example, chains being rolled up or being unrolled) and in those cases both terms formula_6 and formula_7 have to remain present, giving\n\nTo date, nobody has shown that D'Alembert's principle is equivalent to Newton's Second Law. D'Alembert's principle is a more general case . And it is true only for some very special cases e.g. rigid body constraints. However, an approximate solution to this problem does exist.\n\nConsider Newton's law for a system of particles, i. The total force on each particle is\n\nwhere\n\nMoving the inertial forces to the left gives an expression that can be considered to represent quasi-static equilibrium, but which is really just a small algebraic manipulation of Newton's law:\n\nConsidering the virtual work, formula_11, done by the total and inertial forces together through an arbitrary virtual displacement, formula_12, of the system leads to a zero identity, since the forces involved sum to zero for each particle.\n\nThe original vector equation could be recovered by recognizing that the work expression must hold for arbitrary displacements. Separating the total forces into applied forces, formula_14, and constraint forces, formula_15, yields\n\nIf arbitrary virtual displacements are assumed to be in directions that are orthogonal to the constraint forces (which is not usually the case, so this derivation works only for special cases), the constraint forces do no work. Such displacements are said to be \"consistent\" with the constraints. This leads to the formulation of \"d'Alembert's principle\", which states that the difference of applied forces and inertial forces for a dynamic system does no virtual work:.\n\nThere is also a corresponding principle for static systems called the principle of virtual work for applied forces.\n\nD'Alembert showed that one can transform an accelerating rigid body into an equivalent static system by adding the so-called \"inertial force\" and \"inertial torque\" or moment. The inertial force must act through the center of mass and the inertial torque can act anywhere. The system can then be analyzed exactly as a static system subjected to this \"inertial force and moment\" and the external forces. The advantage is that, in the equivalent static system one can take moments about any point (not just the center of mass). This often leads to simpler calculations because any force (in turn) can be eliminated from the moment equations by choosing the appropriate point about which to apply the moment equation (sum of moments = zero). Even in the course of Fundamentals of Dynamics and Kinematics of machines, this principle helps in analyzing the forces that act on a link of a mechanism when it is in motion. In textbooks of engineering dynamics this is sometimes referred to as \"d'Alembert's principle\".\n\nTo illustrate the concept of \"d'Alembert's principle\", let's use a simple model with a weight formula_18, suspended from a wire. The weight is subjected to a gravitational force, formula_19, and a tension force formula_20 in the wire. The mass accelerates upward with an acceleration formula_21. Newton's Second Law becomes formula_22 or formula_23. As an observer with feet planted firmly on the ground, we see that the force formula_20 accelerates the weight, formula_18, but, if we are moving with the wire we don’t see the acceleration, we feel it. The tension in the wire seems to counteract an acceleration “force” formula_26 or formula_27.\nFor a planar rigid body, moving in the plane of the body (the \"x\"–\"y\" plane), and subjected to forces and torques causing rotation only in this plane, the inertial force is\n\nwhere formula_29 is the position vector of the centre of mass of the body, and formula_30 is the mass of the body. The inertial torque (or moment) is\n\nwhere formula_32 is the moment of inertia of the body. If, in addition to the external forces and torques acting on the body, the inertia force acting through the center of mass is added and the inertial torque is added (acting around the centre of mass is as good as anywhere) the system is equivalent to one in static equilibrium. Thus the equations of static equilibrium\n\nhold. The important thing is that formula_34 is the sum of torques (or moments, including the inertial moment and the moment of the inertial force) taken about \"any\" point. The direct application of Newton's laws requires that the angular acceleration equation be applied \"only\" about the center of mass.\n\nD'Alembert's form of the principle of virtual work states that a system of rigid bodies is in dynamic equilibrium when the virtual work of the sum of the applied forces and the inertial forces is zero for any virtual displacement of the system. Thus, dynamic equilibrium of a system of n rigid bodies with m generalized coordinates requires that is to be\nfor any set of virtual displacements δq. This condition yields m equations,\nwhich can also be written as\nThe result is a set of m equations of motion that define the dynamics of the rigid body system.\n"}
{"id": "742352", "url": "https://en.wikipedia.org/wiki?curid=742352", "title": "Derivative test", "text": "Derivative test\n\nIn calculus, a derivative test uses the derivatives of a function to locate the critical points of a function and determine whether each point is a local maximum, a local minimum, or a saddle point. Derivative tests can also give information about the concavity of a function.\n\nThe usefulness of derivatives to find extrema is proved mathematically by Fermat's theorem of stationary points.\n\nThe first derivative test examines a function's monotonic properties (where the function is increasing or decreasing) focusing on a particular point in its domain. If the function \"switches\" from increasing to decreasing at the point, then the function will achieve a highest value at that point. Similarly, if the function \"switches\" from decreasing to increasing at the point, then it will achieve a least value at that point. If the function fails to \"switch\", and remains increasing or remains decreasing, then no highest or least value is achieved.\n\nOne can examine a function's monotonicity without calculus. However, calculus is usually helpful because there are sufficient conditions that guarantee the monotonicity properties above, and these conditions apply to the vast majority of functions one would encounter.\n\nStated precisely, suppose \"f\" is a real-valued function of a real variable, defined on some interval containing the point \"x\".\n\n\nNote that in the first two cases, \"f\" is not required to be strictly increasing or strictly decreasing to the left or right of \"x\", while in the last two cases, \"f\" is required to be strictly increasing or strictly decreasing. The reason is that in the definition of local maximum and minimum, the inequality is not required to be strict: e.g. every value of a constant function is considered both a local maximum and a local minimum.\n\nThe first derivative test depends on the \"increasing-decreasing test\", which is itself ultimately a consequence of the mean value theorem.\n\nSuppose \"f\" is a real-valued function of a real variable defined on some interval containing the critical point \"a\". Further suppose that \"f\" is continuous at \"a\" and differentiable on some open interval containing \"a\", except possibly at \"a\" itself.\n\n\nAgain, corresponding to the comments in the section on monotonicity properties, note that in the first two cases, the inequality is not required to be strict, while in the next two, strict inequality is required.\n\nThe first derivative test is helpful in solving optimization problems in physics, economics, and engineering. In conjunction with the extreme value theorem, it can be used to find the absolute maximum and minimum of a real-valued function defined on a closed, bounded interval. In conjunction with other information such as concavity, inflection points, and asymptotes, it can be used to sketch the graph of a function.\n\nAfter establishing the critical points of a function, the \"second derivative test\" uses the value of the second derivative at those points to determine whether such points are a local maximum or a local minimum. If the function \"f\" is twice differentiable at a critical point \"x\" (i.e. \"(x) = 0\"), then:\n\n\nIn the last case, Taylor's Theorem may be used to determine the behavior of \"f\" near \"x\" using higher derivatives.\n\nSuppose we have formula_8 (the proof for formula_9 is analogous). By assumption, formula_10. Then\n\nThus, for \"h\" sufficiently small we get\n\nwhich means that\nformula_13 if \"h\" < 0 (intuitively, f is decreasing as it approaches \"x\" from the left), and that formula_14 if \"h\" > 0 (intuitively, f is increasing as we go right from \"x\"). Now, by the first derivative test, formula_2 has a local minimum at formula_3.\n\nA related but distinct use of second derivatives is to determine whether a function is concave up or concave down at a point. It does not, however, provide information about inflection points. Specifically, a twice-differentiable function \"f\" is concave up if formula_8 and concave down if formula_9. Note that if formula_19, then formula_20 has zero second derivative, yet is not an inflection point, so the second derivative alone does not give enough information to determine if a given point is an inflection point.\n\nThe \"higher-order derivative test\" or \"general derivative test\" is able to determine whether a function's critical points are maxima, minima, or points of inflection for a wider variety of functions than the second-order derivative test. As shown below, the second derivative test is mathematically identical to the special case of n=1 in the higher-order derivative test.\n\nLet \"f\" be a real-valued, sufficiently differentiable function on the interval formula_21 and formula_22 an integer. Also let all the derivatives of \"f\" at \"c\" be zero up to and including the \"n\"th derivative, but with the (\"n\"+1)th derivative being non-zero:\n\nformula_23.\n\nThere are four possibilities, the first two cases where \"c\" is an extremum, the second two where c is a (local) saddle point:\n\nSince \"n\" must be either odd or even, this analytical test classifies any stationary point of \"f\", so long as a nonzero derivative shows up eventually.\n\nSay we want to perform the general derivative test on the function formula_28 at the point formula_20. To do this, we calculate the derivatives of the function and then evaluate them at the point of interest until the result is nonzero.\n\nAs shown above, at the point formula_20, the function formula_43 has all of its derivatives at 0 equal to 0 except for the 6th derivative, which is positive. Thus n=5, and by the test, there is a local minimum at 0.\n\nFor a function of more than one variable, the second derivative test generalizes to a test based on the eigenvalues of the function's Hessian matrix at the critical point. In particular, assuming that all second order partial derivatives of \"f\" are continuous on a neighbourhood of a critical point \"x\", then if the eigenvalues of the Hessian at \"x\" are all positive, then \"x\" is a local minimum. If the eigenvalues are all negative, then \"x\" is a local maximum, and if some are positive and some negative, then the point is a saddle point. If the Hessian matrix is singular, then the second derivative test is inconclusive.\n\n"}
{"id": "43854082", "url": "https://en.wikipedia.org/wiki?curid=43854082", "title": "Direct sum of topological groups", "text": "Direct sum of topological groups\n\nIn mathematics, a topological group \"G\" is called the topological direct sum of two subgroups \"H\" and \"H\" if\nthe map\nis a topological isomorphism.\n\nMore generally, \"G\" is called the direct sum of a finite set of subgroups formula_2 of the map\n\nNote that if a topological group \"G\" is the topological direct sum of the family of subgroups formula_4 then in particular, as an abstract group (without topology) it is also the direct sum (in the usual way) of the family formula_4.\n\nGiven a topological group \"G\", we say that a subgroup \"H\" is a topological direct summand of \"G\" (or that splits topologically from \"G\") if and only if there exist another subgroup \"K\" ≤ \"G\" such that \"G\" is the direct sum of the subgroups \"H\" and \"K\".\n\nA the subgroup \"H\" is a topological direct summand if and only if the extension of topological groups\n\nsplits, where formula_7 is the natural inclusion and formula_8 is the natural projection.\n\n"}
{"id": "1216914", "url": "https://en.wikipedia.org/wiki?curid=1216914", "title": "Discrete-time Fourier transform", "text": "Discrete-time Fourier transform\n\nIn mathematics, the discrete-time Fourier transform (DTFT) is a form of Fourier analysis that is applicable to the uniformly-spaced samples of a continuous function. The term \"discrete-time\" refers to the fact that the transform operates on discrete data (samples) whose interval often has units of time. From only the samples, it produces a function of frequency that is a periodic summation of the continuous Fourier transform of the original continuous function. Under certain theoretical conditions, described by the sampling theorem, the original continuous function can be recovered perfectly from the DTFT and thus from the original discrete samples. The DTFT itself is a continuous function of frequency, but discrete samples of it can be readily calculated via the discrete Fourier transform (DFT) (see Sampling the DTFT), which is by far the most common method of modern Fourier analysis.\n\nBoth transforms are invertible. The inverse DTFT is the original sampled data sequence. The inverse DFT is a periodic summation of the original sequence. The fast Fourier transform (FFT) is an algorithm for computing one cycle of the DFT, and its inverse produces one cycle of the inverse DFT.\n\nThe discrete-time Fourier transform of a discrete set of real or complex numbers , for all integers , is a Fourier series, which produces a periodic function of a frequency variable. When the frequency variable, ω, has normalized units of \"radians/sample\", the periodicity is , and the Fourier series is:\n\nThe utility of this frequency domain function is rooted in the Poisson summation formula. Let be the Fourier transform of any function, , whose samples at some interval (\"seconds\") are equal (or proportional to) the sequence, i.e. . Then the periodic function represented by the Fourier series is a periodic summation of In terms of frequency in hertz (\"cycles/sec\"):\n\nThe integer has units of \"cycles/sample\", and is the sample-rate, (\"samples/sec\"). So comprises exact copies of that are shifted by multiples of hertz and combined by addition. For sufficiently large the term can be observed in the region with little or no distortion (aliasing) from the other terms. In Fig.1, the extremities of the distribution in the upper left corner are masked by aliasing in the periodic summation (lower left).\n\nWe also note that is the Fourier transform of . Therefore, an alternative definition of DTFT is:\n\nThe modulated Dirac comb function is a mathematical abstraction sometimes referred to as \"impulse sampling\".\n\nAn operation that recovers the discrete data sequence from the DTFT function is called an \"inverse DTFT\". For instance, the inverse continuous Fourier transform of both sides of produces the sequence in the form of a modulated Dirac comb function:\n\nHowever, noting that is periodic, all the necessary information is contained within any interval of length . In both and , the summations over n are a Fourier series, with coefficients . The standard formulas for the Fourier coefficients are also the inverse transforms:\n\nWhen the input data sequence is -periodic, can be computationally reduced to a discrete Fourier transform (DFT), because:\n\nThe kernel is -periodic at the harmonic frequencies, . Introducing the notation formula_3 to represent a sum over any -sequence of length , we can write:\n\nTherefore, the DTFT diverges at the harmonic frequencies, but at different frequency-dependent rates. And those rates are given by the DFT of one cycle of the sequence. In terms of a Dirac comb function, this is represented by:\n\nWhen the DTFT is continuous, a common practice is to compute an arbitrary number of samples () of one cycle of the periodic function :\n\nwhere is a periodic summation:\n\nThe sequence is the inverse DFT. Thus, our sampling of the DTFT causes the inverse transform to become periodic. The array of is known as a periodogram, where parameter is known to Matlab users as NFFT.\n\nIn order to evaluate one cycle of numerically, we require a finite-length sequence. For instance, a long sequence might be truncated by a window function of length resulting in two cases worthy of special mention: and , for some integer (typically 6 or 8). For notational simplicity, consider the values below to represent the modified values.\n\nWhen , a cycle of reduces to a summation of \"blocks\" of length . This goes by various names, such as:\n\nAn interesting way to understand/motivate the technique is to recall that decimation of sampled data in one domain (time or frequency) produces aliasing in the other, and vice versa. The \"x\" summation is mathematically equivalent to aliasing, leading to decimation in frequency, leaving only DTFT samples least affected by spectral leakage. That is usually a priority when implementing an FFT filter-bank (channelizer). With a conventional window function of length \"L\", scalloping loss would be unacceptable. So multi-block windows are created using FIR filter design tools.  Their frequency profile is flat at the highest point and falls off quickly at the midpoint between the remaining DTFT samples. The larger the value of parameter , the better the potential performance. We note that the same results can be obtained by computing and decimating an -length DFT, but that is not computationally efficient.\n\nWhen the DFT is usually written in this more familiar form:\n\nIn order to take advantage of a fast Fourier transform algorithm for computing the DFT, the summation is usually performed over all terms, even though of them are zeros. Therefore, the case is often referred to as \"zero-padding\".\n\nSpectral leakage, which increases as decreases, is detrimental to certain important performance metrics, such as resolution of multiple frequency components and the amount of noise measured by each DTFT sample. But those things don't always matter, for instance when the sequence is a noiseless sinusoid (or a constant), shaped by a window function. Then it is a common practice to use \"zero-padding\" to graphically display and compare the detailed leakage patterns of window functions. To illustrate that for a rectangular window, consider the sequence:\n\nFigures 2 and 3 are plots of the magnitude of two different sized DFTs, as indicated in their labels. In both cases, the dominant component is at the signal frequency: . Also visible in Fig 2 is the spectral leakage pattern of the rectangular window. The illusion in Fig 3 is a result of sampling the DTFT at just its zero-crossings. Rather than the DTFT of a finite-length sequence, it gives the impression of an infinitely long sinusoidal sequence. Contributing factors to the illusion are the use of a rectangular window, and the choice of a frequency (1/8 = 8/64) with exactly 8 (an integer) cycles per 64 samples. A Hann window would produce a similar result, except the peak would be widened to 3 samples (see DFT-even Hann window).\n\nThe convolution theorem for sequences is:\n\nAn important special case is the circular convolution of sequences and defined by where is a periodic summation. The discrete-frequency nature of } \"selects\" only discrete values from the continuous function }, which results in considerable simplification of the inverse transform. As shown at Convolution theorem#Functions of discrete variable sequences:\n\nFor and sequences whose non-zero duration is less than or equal to , a final simplification is:\n\nThe significance of this result is expounded at Circular convolution and Fast convolution algorithms.\n\n\n\n\n\nformula_22 is a Fourier series that can also be expressed in terms of the bilateral Z-transform.  I.e.:\n\nwhere the formula_24 notation distinguishes the Z-transform from the Fourier transform. Therefore, we can also express a portion of the Z-transform in terms of the Fourier transform:\n\nNote that when parameter changes, the terms of formula_22 remain a constant separation formula_27 apart, and their width scales up or down. The terms of remain a constant width and their separation scales up or down.\n\nSome common transform pairs are shown in the table below. The following notation applies:\n\n\n\nThis table shows some mathematical operations in the time domain and the corresponding effects in the frequency domain.\n\n\n"}
{"id": "316826", "url": "https://en.wikipedia.org/wiki?curid=316826", "title": "Fibration", "text": "Fibration\n\nIn topology, a branch of mathematics, a fibration is a generalization of the notion of a fiber bundle. A fiber bundle makes precise the idea of one topological space (called a fiber) being \"parameterized\" by another topological space (called a base). A fibration is like a fiber bundle, except that the fibers need not be the same space, nor even homeomorphic; rather, they are just homotopy equivalent. Weak fibrations discard even this equivalence for a more technical property.\n\nFibrations do not necessarily have the local Cartesian product structure that defines the more restricted fiber bundle case, but something weaker that still allows \"sideways\" movement from fiber to fiber. Fiber bundles have a particularly simple homotopy theory that allows topological information about the bundle to be inferred from information about one or both of these constituent spaces. A fibration satisfies an additional condition (the homotopy lifting property) guaranteeing that it will behave like a fiber bundle from the point of view of homotopy theory.\n\nFibrations are dual to cofibrations, with a correspondingly dual notion of the homotopy extension property; this is loosely known as Eckmann–Hilton duality.\n\nA fibration (or Hurewicz fibration or Hurewicz fiber space, so named after Witold Hurewicz) is a continuous mapping formula_1 satisfying the homotopy lifting property with respect to any space. Fiber bundles (over paracompact bases) constitute important examples. In homotopy theory, any mapping is 'as good as' a fibration—i.e. any map can be decomposed as a homotopy equivalence into a \"mapping path space\" followed by a fibration into homotopy fibers.\n\nThe \"fibers\" are by definition the subspaces of that are the inverse images of points of . If the base space is path connected, it is a consequence of the definition that the fibers of two different points formula_2 and formula_3 in are homotopy equivalent. Therefore, one usually speaks of \"the fiber\" .\n\nA continuous mapping with the homotopy lifting property for CW complexes (or equivalently, just cubes formula_4) is called a Serre fibration or a weak fibration, in honor of the part played by the concept in the thesis of Jean-Pierre Serre. This thesis firmly established in algebraic topology the use of spectral sequences, and clearly separated the notions of fiber bundles and fibrations from the notion of sheaf (both concepts together having been implicit in the pioneer treatment of Jean Leray). Because a sheaf (thought of as an étalé space) can be considered a local homeomorphism, the notions were closely interlinked at the time. One of the main desirable properties of the Serre spectral sequence is to account for the action of the fundamental group of the base on the homology of the \"total space\" .\n\nNote that Serre fibrations are strictly weaker than fibrations in general: the homotopy lifting property need only hold on cubes (or CW complexes), and not on all spaces in general. As a result, the fibers might not even be homotopy equivalent; an explicit example is given below.\n\nIn the following examples, a fibration is denoted\nwhere the first map is the inclusion of \"the\" fiber into the total space and the second map is the fibration onto the basis . This is also referred to as a fibration sequence.\n\n\nThe previous examples all have fibers that are homotopy equivalent. This must be the case for fibrations in general, but not necessarily for weak fibrations. The notion of a weak fibration is strictly weaker than a fibration, as the following example illustrates: the fibers might not even have the same homotopy type.\n\nConsider the subset of the real plane formula_5 given by\nand the base space given by the unit interval formula_7, the projection by formula_8. One can easily see that this is a Serre fibration. However, the fiber formula_9 and the fiber at formula_10 are not homotopy equivalent. The space formula_11 has an obvious injection into the total space formula_12 and has an obvious homotopy (the constant function) in the base space formula_13; however, it cannot be lifted, and thus the example cannot be a fibration in general.\n\nChoose a base point . Let refer to the fiber over , i.e. ; and let be the inclusion . Choose a base point and let . In terms of these base points, the Puppe sequence can be used to show that there is a long exact sequence\nIt is constructed from the homotopy groups of the fiber , total space , and base space . The homomorphisms and are just the induced homomorphisms from and , respectively. The maps involving π are not group homomorphisms because the π are not groups, but they are exact in the sense that the image equals the kernel (here the \"neutral element\" is the connected component containing the base point).\n\nThis sequence holds for both fibrations, and for weak fibrations, although the proof of the two cases is slightly different.\n\nOne possible way to demonstrate that the sequence above is well-defined and exact, while avoiding contact with the Puppe sequence, is to proceed directly, as follows.\nThe third set of homomorphisms (called the \"connecting homomorphisms\" (in reference to the snake lemma) or the \"boundary maps\") is not an induced map and is defined directly in the corresponding homotopy groups with the following steps.\nThe above is summarized in the following commutative diagram:\n\nRepeated application of the homotopy lifting property is used to prove that is well-defined (does not depend on a particular lift), depends only on the homotopy class of its argument, it is a homomorphism and that the long sequence is exact.\n\nAlternatively, one can use relative homotopy groups to obtain the long exact sequence on homotopy of a fibration from the long exact sequence on relative homotopy of the pair formula_15. One uses that the n-th homotopy group of formula_12 relative to formula_17 is isomorphic to the n-th homotopy group of the base formula_13.\n\nOne may also proceed in the reverse direction. When the fibration is the mapping fibre (dual to the mapping cone, a cofibration), then one obtains the exact Puppe sequence. In essence, the long exact sequence of homotopy groups follows from the fact that the homotopy groups can be obtained as suspensions, or dually, loop spaces.\n\nThe Euler characteristic is multiplicative for fibrations with certain conditions.\n\nIf is a fibration with fiber , with the base path-connected, and the fibration is orientable over a field , then the Euler characteristic with coefficients in the field satisfies the product property:\nThis includes product spaces and covering spaces as special cases,\nand can be proven by the Serre spectral sequence on homology of a fibration.\n\nFor fiber bundles, this can also be understood in terms of a transfer map —note that this is a lifting and goes \"the wrong way\"—whose composition with the projection map is multiplication by the Euler characteristic of the fiber:\n\nFibrations of topological spaces fit into a more general framework, the so-called closed model categories, following from the acyclic models theorem. In such categories, there are distinguished classes of morphisms, the so-called \"fibrations\", \"cofibrations\" and \"weak equivalences\". Certain axioms, such as stability of fibrations under composition and pullbacks, factorization of every morphism into the composition of an acyclic cofibration followed by a fibration or a cofibration followed by an acyclic fibration, where the word \"acyclic\" indicates that the corresponding arrow is also a weak equivalence, and other requirements are set up to allow the abstract treatment of homotopy theory. (In the original treatment, due to Daniel Quillen, the word \"trivial\" was used instead of \"acyclic.\")\n\nIt can be shown that the category of topological spaces is in fact a model category, where (abstract) fibrations are just the Serre fibrations introduced above and weak equivalences are weak homotopy equivalences.\n\n"}
{"id": "1364622", "url": "https://en.wikipedia.org/wiki?curid=1364622", "title": "Four-dimensional space", "text": "Four-dimensional space\n\nA four-dimensional space or 4D space is a mathematical extension of the concept of three-dimensional or 3D space. Three-dimensional space is the simplest possible generalization of the observation that one only needs three numbers, called \"dimensions\", to describe the sizes or locations of objects in the everyday world. For example, the volume of a rectangular box is found by measuring its length (often labeled \"x\"), width (\"y\"), and depth (\"z\").\n\nThe idea of adding a fourth dimension began with Joseph-Louis Lagrange in the mid-1700s and culminated in a precise formalization of the concept in 1854 by Bernhard Riemann. In 1880 Charles Howard Hinton popularized these insights in an essay titled \"\", which explained the concept of a four-dimensional cube with a step-by-step generalization of the properties of lines, squares, and cubes. The simplest form of Hinton's method is to draw two ordinary cubes separated by an \"unseen\" distance, and then draw lines between their equivalent vertices. This can be seen in the accompanying animation, whenever it shows a smaller inner cube inside a larger outer cube. The eight lines connecting the vertices of the two cubes in that case represent a single direction in the \"unseen\" fourth dimension.\n\nHigher dimensional spaces have since become one of the foundations for formally expressing modern mathematics and physics. Large parts of these topics could not exist in their current forms without the use of such spaces. Einstein's concept of spacetime uses such a 4D space, though it has a Minkowski structure that is a bit more complicated than Euclidean 4D space.\n\nSingle locations in 4D space can be given as vectors or \"n-tuples\", i.e. as ordered lists of numbers such as \"(t,x,y,z)\". It is only when such locations are linked together into more complicated shapes that the full richness and geometric complexity of 4D and higher dimensional spaces emerge. A hint to that complexity can be seen in the accompanying animation of one of simplest possible 4D objects, the 4D cube or tesseract.\n\nLagrange wrote in his \"Mécanique analytique\" (published 1788, based on work done around 1755) that mechanics can be viewed as operating in a four-dimensional space — three dimensions of space, and one of time. In 1827 Möbius realized that a fourth dimension would allow a three-dimensional form to be rotated onto its mirror-image, and by 1853 Ludwig Schläfli had discovered many polytopes in higher dimensions, although his work was not published until after his death. Higher dimensions were soon put on firm footing by Bernhard Riemann's 1854 thesis, \"Über die Hypothesen welche der Geometrie zu Grunde liegen\", in which he considered a \"point\" to be any sequence of coordinates (\"x\", ..., \"x\"). The possibility of geometry in higher dimensions, including four dimensions in particular, was thus established.\n\nAn arithmetic of four dimensions called quaternions was defined by William Rowan Hamilton in 1843. This associative algebra was the source of the science of vector analysis in three dimensions as recounted in \"A History of Vector Analysis\". Soon after tessarines and coquaternions were introduced as other four-dimensional algebras over R.\n\nOne of the first major expositors of the fourth dimension was Charles Howard Hinton, starting in 1880 with his essay \"What is the Fourth Dimension?\"; published in the Dublin University magazine. He coined the terms \"tesseract\", \"ana\" and \"kata\" in his book \"A New Era of Thought\", and introduced a method for visualising the fourth dimension using cubes in the book \"Fourth Dimension\".\n\nHinton's ideas inspired a fantasy about a \"Church of the Fourth Dimension\" featured by Martin Gardner in his January 1962 \"Mathematical Games column\" in \"Scientific American\". In 1886 Victor Schlegel described his method of visualizing four-dimensional objects with Schlegel diagrams.\n\nIn 1908, Hermann Minkowski presented a paper consolidating the role of time as the fourth dimension of spacetime, the basis for Einstein's theories of special and general relativity. But the geometry of spacetime, being non-Euclidean, is profoundly different from that popularised by Hinton. The study of Minkowski space required new mathematics quite different from that of four-dimensional Euclidean space, and so developed along quite different lines. This separation was less clear in the popular imagination, with works of fiction and philosophy blurring the distinction, so in 1973 H. S. M. Coxeter felt compelled to write:\nMathematically four-dimensional space is simply a space with four spatial dimensions, that is a space that needs four parameters to specify a point in it. For example, a general point might have position vector a, equal to\nThis can be written in terms of the four standard basis vectors (e, e, e, e), given by\nso the general vector a is\nVectors add, subtract and scale as in three dimensions.\n\nThe dot product of Euclidean three-dimensional space generalizes to four dimensions as\nIt can be used to calculate the norm or length of a vector,\nand calculate or define the angle between two non-zero vectors as\n\nMinkowski spacetime is four-dimensional space with geometry defined by a non-degenerate pairing different from the dot product:\nAs an example, the distance squared between the points (0,0,0,0) and (1,1,1,0) is 3 in both the Euclidean and Minkowskian 4-spaces, while the distance squared between (0,0,0,0) and (1,1,1,1) is 4 in Euclidean space and 2 in Minkowski space; increasing formula_8 actually decreases the metric distance. This leads to many of the well-known apparent \"paradoxes\" of relativity.\n\nThe cross product is not defined in four dimensions. Instead the exterior product is used for some applications, and is defined as follows:\nThis is bivector valued, with bivectors in four dimensions forming a six-dimensional linear space with basis (e, e, e, e, e, e). They can be used to generate rotations in four dimensions.\n\nIn the familiar three-dimensional space in which we live there are three coordinate axes—usually labeled \"x\", \"y\", and \"z\"—with each axis orthogonal (i.e. perpendicular) to the other two. The six cardinal directions in this space can be called \"up\", \"down\", \"east\", \"west\", \"north\", and \"south\". Positions along these axes can be called \"altitude\", \"longitude\", and \"latitude\". Lengths measured along these axes can be called \"height\", \"width\", and \"depth\".\n\nComparatively, four-dimensional space has an extra coordinate axis, orthogonal to the other three, which is usually labeled \"w\". To describe the two additional cardinal directions, Charles Howard Hinton coined the terms \"ana\" and \"kata\", from the Greek words meaning \"up toward\" and \"down from\", respectively. A position along the \"w\" axis can be called \"spissitude\", as coined by Henry More.\n\nThe geometry of four-dimensional space is much more complex than that of three-dimensional space, due to the extra degree of freedom.\n\nJust as in three dimensions there are polyhedra made of two dimensional polygons, in four dimensions there are 4-polytopes made of polyhedra. In three dimensions, there are 5 regular polyhedra known as the Platonic solids. In four dimensions, there are 6 convex regular 4-polytopes, the analogues of the Platonic solids. Relaxing the conditions for regularity generates a further 58 convex uniform 4-polytopes, analogous to the 13 semi-regular Archimedean solids in three dimensions. Relaxing the conditions for convexity generates a further 10 nonconvex regular 4-polytopes.\n\nIn three dimensions, a circle may be extruded to form a cylinder. In four dimensions, there are several different cylinder-like objects. A sphere may be extruded to obtain a spherical cylinder (a cylinder with spherical \"caps\", known as a spherinder), and a cylinder may be extruded to obtain a cylindrical prism (a cubinder). The Cartesian product of two circles may be taken to obtain a duocylinder. All three can \"roll\" in four-dimensional space, each with its own properties.\n\nIn three dimensions, curves can form knots but surfaces cannot (unless they are self-intersecting). In four dimensions, however, knots made using curves can be trivially untied by displacing them in the fourth direction—but 2D surfaces can form non-trivial, non-self-intersecting knots in 4D space. Because these surfaces are two-dimensional, they can form much more complex knots than strings in 3D space can. The Klein bottle is an example of such a knotted surface. Another such surface is the real projective plane. \n\nThe set of points in Euclidean 4-space having the same distance R from a fixed point P forms a hypersurface known as a 3-sphere. The hyper-volume of the enclosed space is:\n\nThis is part of the Friedmann–Lemaître–Robertson–Walker metric in General relativity where \"R\" is substituted by function \"R(t)\" with \"t\" meaning the cosmological age of the universe. Growing or shrinking \"R\" with time means expanding or collapsing universe, depending on the mass density inside.\n\nResearch using virtual reality finds that humans, in spite of living in a three-dimensional world, can, without special practice, make spatial judgments about line segments, embedded in four-dimensional space, based on their length (one dimensional) and the angle (two dimensional) between them. The researchers noted that \"the participants in our study had minimal practice in these tasks, and it remains an open question whether it is possible to obtain more sustainable, definitive, and richer 4D representations with increased perceptual experience in 4D virtual environments.\" In another study, the ability of humans to orient themselves in 2D, 3D and 4D mazes has been tested. Each maze consisted of four path segments of random length and connected with orthogonal random bends, but without branches or loops (i.e. actually labyrinths). The graphical interface was based on John McIntosh's free 4D Maze game. The participating persons had to navigate through the path and finally estimate the linear direction back to the starting point. The researchers found that some of the participants were able to mentally integrate their path after some practice in 4D (the lower-dimensional cases were for comparison and for the participants to learn the method).\n\nTo understand the nature of four-dimensional space, a device called \"dimensional analogy\" is commonly employed. Dimensional analogy is the study of how (\"n\" − 1) dimensions relate to \"n\" dimensions, and then inferring how \"n\" dimensions would relate to (\"n\" + 1) dimensions.\n\nDimensional analogy was used by Edwin Abbott Abbott in the book \"Flatland\", which narrates a story about a square that lives in a two-dimensional world, like the surface of a piece of paper. From the perspective of this square, a three-dimensional being has seemingly god-like powers, such as ability to remove objects from a safe without breaking it open (by moving them across the third dimension), to see everything that from the two-dimensional perspective is enclosed behind walls, and to remain completely invisible by standing a few inches away in the third dimension.\n\nBy applying dimensional analogy, one can infer that a four-dimensional being would be capable of similar feats from our three-dimensional perspective. Rudy Rucker illustrates this in his novel \"Spaceland\", in which the protagonist encounters four-dimensional beings who demonstrate such powers.\n\nAs a three-dimensional object passes through a two-dimensional plane, a two-dimensional being would only see a cross-section of the three-dimensional object. For example, if a spherical balloon passed through a sheet of paper, a being on the paper would see first a single point, then a circle gradually growing larger, then smaller again until it shrank to a point and then disappeared. Similarly, if a four-dimensional object passed through three dimensions, we would see a three-dimensional cross-section of the four-dimensional object—for example, a hypersphere would appear first as a point, then as a growing sphere, with the sphere then shrinking to a single point and then disappearing. This means of visualizing aspects of the fourth dimension was used in the novel \"Flatland\" and also in several works of Charles Howard Hinton.\n\nA useful application of dimensional analogy in visualizing higher dimensions is in projection. A projection is a way for representing an \"n\"-dimensional object in dimensions. For instance, computer screens are two-dimensional, and all the photographs of three-dimensional people, places and things are represented in two dimensions by projecting the objects onto a flat surface. By doing this, the dimension orthogonal to the screen (\"depth\") is removed and replaced with indirect information. The retina of the eye is also a two-dimensional array of receptors but the brain is able to perceive the nature of three-dimensional objects by inference from indirect information (such as shading, foreshortening, binocular vision, etc.). Artists often use perspective to give an illusion of three-dimensional depth to two-dimensional pictures. The \"shadow\", cast by a fictitious grid model of a rotating tesseract on a plane surface, as shown in the figures, is also the result of projections.\n\nSimilarly, objects in the fourth dimension can be mathematically projected to the familiar three dimensions, where they can be more conveniently examined. In this case, the 'retina' of the four-dimensional eye is a three-dimensional array of receptors. A hypothetical being with such an eye would perceive the nature of four-dimensional objects by inferring four-dimensional depth from indirect information in the three-dimensional images in its retina.\n\nThe perspective projection of three-dimensional objects into the retina of the eye introduces artifacts such as foreshortening, which the brain interprets as depth in the third dimension. In the same way, perspective projection from four dimensions produces similar foreshortening effects. By applying dimensional analogy, one may infer four-dimensional \"depth\" from these effects.\n\nAs an illustration of this principle, the following sequence of images compares various views of the three-dimensional cube with analogous projections of the four-dimensional tesseract into three-dimensional space.\n\nA concept closely related to projection is the casting of shadows.\nIf a light is shone on a three-dimensional object, a two-dimensional shadow is cast. By dimensional analogy, light shone on a two-dimensional object in a two-dimensional world would cast a one-dimensional shadow, and light on a one-dimensional object in a one-dimensional world would cast a zero-dimensional shadow, that is, a point of non-light. Going the other way, one may infer that light shone on a four-dimensional object in a four-dimensional world would cast a three-dimensional shadow.\n\nIf the wireframe of a cube is lit from above, the resulting shadow is a square within a square with the corresponding corners connected. Similarly, if the wireframe of a tesseract were lit from “above” (in the fourth dimension), its shadow would be that of a three-dimensional cube within another three-dimensional cube. (Note that, technically, the visual representation shown here is actually a two-dimensional image of the three-dimensional shadow of the four-dimensional wireframe figure.)\n\nDimensional analogy also helps in inferring basic properties of objects in higher dimensions. For example, two-dimensional objects are bounded by one-dimensional boundaries: a square is bounded by four edges. Three-dimensional objects are bounded by two-dimensional surfaces: a cube is bounded by 6 square faces. By applying dimensional analogy, one may infer that a four-dimensional cube, known as a tesseract, is bounded by three-dimensional volumes. And indeed, this is the case: mathematics shows that the tesseract is bounded by 8 cubes. Knowing this is key to understanding how to interpret a three-dimensional projection of the tesseract. The boundaries of the tesseract project to \"volumes\" in the image, not merely two-dimensional surfaces.\n\nBeing three-dimensional, we are only able to see the world with our eyes in two dimensions. A four-dimensional being would be able to see the world in three dimensions. For example, it would be able to see all six sides of an opaque box simultaneously, and in fact, what is inside the box at the same time, just as we can see the interior of a square on a piece of paper. It would be able to see all points in 3-dimensional space simultaneously, including the inner structure of solid objects and things obscured from our three-dimensional viewpoint. Our brains receive images in two dimensions and use reasoning to help us \"picture\" three-dimensional objects.\n\nReasoning by analogy from familiar lower dimensions can be an excellent intuitive guide, but care must be exercised not to accept results that are not more rigorously tested. For example, consider the formulas for the circumference of a circle\nformula_11\nand the surface area of a sphere:\nformula_12.\nOne might be tempted to suppose that the surface volume of a hypersphere is formula_13, or perhaps formula_14, but either of these would be wrong. The correct formula is formula_15.\n\n\n"}
{"id": "385339", "url": "https://en.wikipedia.org/wiki?curid=385339", "title": "Generalized flag variety", "text": "Generalized flag variety\n\nIn mathematics, a generalized flag variety (or simply flag variety) is a homogeneous space whose points are flags in a finite-dimensional vector space \"V\" over a field F. When F is the real or complex numbers, a generalized flag variety is a smooth or complex manifold, called a real or complex flag manifold. Flag varieties are naturally projective varieties.\n\nFlag varieties can be defined in various degrees of generality. A prototype is the variety of complete flags in a vector space \"V\" over a field F, which is a flag variety for the special linear group over F. Other flag varieties arise by considering partial flags, or by restriction from the special linear group to subgroups such as the symplectic group. For partial flags, one needs to specify the sequence of dimensions of the flags under consideration. For subgroups of the linear group, additional conditions must be imposed on the flags.\n\nThe most general concept of a generalized flag variety is a conjugacy class of parabolic subgroups of a semisimple algebraic or Lie group \"G\": \"G\" acts transitively on such a conjugacy class by conjugation, and the stabilizer of a parabolic \"P\" is \"P\" itself, so that the generalized flag variety is isomorphic to \"G\"/\"P\". It may also be realised as the orbit of a highest weight space in a projectivized representation of \"G\". In the algebraic setting, generalized flag varieties are precisely the homogeneous spaces for \"G\" which are complete as algebraic varieties. In the smooth setting, generalized flag manifolds are the compact flat model spaces for Cartan geometries of parabolic type, and are homogeneous Riemannian manifolds under any maximal compact subgroup of \"G\".\n\nFlag manifolds can be symmetric spaces. Over the complex numbers, the corresponding flag manifolds are the Hermitian symmetric spaces. Over the real numbers, an \"R\"-space is a synonym for a real flag manifold and the corresponding symmetric spaces are called symmetric \"R\"-spaces.\n\nA flag in a finite dimensional vector space \"V\" over a field F is an increasing sequence of subspaces, where \"increasing\" means each is a proper subspace of the next (see filtration):\nIf we write the dim \"V\" = \"d\" then we have\nwhere \"n\" is the dimension of \"V\". Hence, we must have \"k\" ≤ \"n\". A flag is called a \"complete flag\" if \"d\" = \"i\" for all \"i\", otherwise it is called a \"partial flag\". The \"signature\" of the flag is the sequence (\"d\", …, \"d\").\n\nA partial flag can be obtained from a complete flag by deleting some of the subspaces. Conversely, any partial flag can be completed (in many different ways) by inserting suitable subspaces.\n\nAccording to basic results of linear algebra, any two complete flags in an \"n\"-dimensional vector space \"V\" over a field F are no different from each other from a geometric point of view. That is to say, the general linear group acts transitively on the set of all complete flags.\n\nFix an ordered basis for \"V\", identifying it with F, whose general linear group is the group GL(\"n\",F) of \"n\" × \"n\" invertible matrices. The standard flag associated with this basis is the one where the \"i\" th subspace is spanned by the first \"i\" vectors of the basis. Relative to this basis, the stabilizer of the standard flag is the group of nonsingular upper triangular matrices, which we denote by \"B\". The complete flag variety can therefore be written as a homogeneous space GL(\"n\",F) / \"B\", which shows in particular that it has dimension \"n\"(\"n\"−1)/2 over F.\n\nNote that the multiples of the identity act trivially on all flags, and so one can restrict attention to the special linear group SL(\"n\",F) of matrices with determinant one, which is a semisimple algebraic group; the set of upper triangular matrices of determinant one is a Borel subgroup.\n\nIf the field F is the real or complex numbers we can introduce an inner product on \"V\" such that the chosen basis is orthonormal. Any complete flag then splits into a direct sum of one-dimensional subspaces by taking orthogonal complements. It follows that the complete flag manifold over the complex numbers is the homogeneous space\nwhere U(\"n\") is the unitary group and T is the \"n\"-torus of diagonal unitary matrices. There is a similar description over the real numbers with U(\"n\") replaced by the orthogonal group O(\"n\"), and T by the diagonal orthogonal matrices (which have diagonal entries ±1).\n\nThe partial flag variety\nis the space of all flags of signature (\"d\", \"d\", … \"d\") in a vector space \"V\" of dimension \"n\" = \"d\" over F. The complete flag variety is the special case that \"d\" = \"i\" for all \"i\". When \"k\"=2, this is a Grassmannian of \"d\"-dimensional subspaces of \"V\".\n\nThis is a homogeneous space for the general linear group \"G\" of \"V\" over F. To be explicit, take \"V\" = F so that \"G\" = GL(\"n\",F). The stabilizer of a flag of nested subspaces \"V\" of dimension \"d\" can be taken to be the group of nonsingular block upper triangular matrices, where the dimensions of the blocks are \"n\" := \"d\" − \"d\" (with \"d\" = 0).\n\nRestricting to matrices of determinant one, this is a parabolic subgroup \"P\" of SL(\"n\",F), and thus the partial flag variety is isomorphic to the homogeneous space SL(\"n\",F)/\"P\".\n\nIf F is the real or complex numbers, then an inner product can be used to split any flag into a direct sum, and so the partial flag variety is also isomorphic to the homogeneous space\nin the complex case, or\nin the real case.\n\nThe upper triangular matrices of determinant one are a Borel subgroup of SL(\"n\",F), and hence the stabilizers of partial flags are parabolic subgroups. Furthermore, a partial flag is determined by the parabolic subgroup which stabilizes it.\n\nHence, more generally, if \"G\" is a semisimple algebraic or Lie group, then the (generalized) flag variety for \"G\" is \"G\"/\"P\" where \"P\" is a parabolic subgroup of \"G\". The correspondence between parabolic subgroups and generalized flag varieties allows each to be understood in terms of the other.\n\nThe extension of the terminology \"flag variety\" is reasonable, because points of \"G\"/\"P\" can still be described using flags. When \"G\" is a classical group, such as a symplectic group or orthogonal group, this is particularly transparent. If (\"V\", \"ω\") is a symplectic vector space then a partial flag in \"V\" is \"isotropic\" if the symplectic form vanishes on proper subspaces of \"V\" in the flag. The stabilizer of an isotropic flag is a parabolic subgroup of the symplectic group Sp(\"V\",\"ω\"). For orthogonal groups there is a similar picture, with a couple of complications. First, if F is not algebraically closed, then isotropic subspaces may not exist: for a general theory, one needs to use the split orthogonal groups. Second, for vector spaces of even dimension 2\"m\", isotropic subspaces of dimension \"m\" come in two flavours (\"self-dual\" and \"anti-self-dual\") and one needs to distinguish these to obtain a homogeneous space.\n\nIf \"G\" is a compact, connected Lie group, it contains a maximal torus \"T\" and the space \"G\"/\"T\" of left cosets with the quotient topology is a compact real manifold. If \"H\" is any other closed, connected subgroup of \"G\" containing \"T\", then \"G\"/\"H\" is another compact real manifold. (Both are actually complex homogeneous spaces in a canonical way through complexification.) \n\nThe presence of a complex structure and cellular (co)homology make it easy to see that the cohomology ring of \"G\"/\"H\" is concentrated in even degrees, but in fact, something much stronger can be said. Because \"G\" → \"G/H\" is a principal \"H\"-bundle, there exists a classifying map \"G\"/\"H\" → \"BH\" with target the classifying space \"BH\". If we replace \"G\"/\"H\" with the homotopy quotient \"G\" in the sequence \"G\" → \"G/H\" → \"BH\", we obtain a principal \"G\"-bundle called the Borel fibration of the right multiplication action of \"H\" on \"G\", and we can use the cohomological Serre spectral sequence of this bundle to understand the fiber-restriction homomorphism \"H\"*(\"G\"/\"H\") → \"H\"*(\"G\")\nand the characteristic map \"H\"*(\"BH\") → \"H\"*(\"G\"/\"H\"), so called because its image, the \"characteristic subring\" of \"H\"*(\"G\"/\"H\"), carries the characteristic classes of the original bundle \"H\" → \"G\" → \"G\"/\"H\".\n\nLet us now restrict our coefficient ring to be a field \"k\" of characteristic zero, so that,\nby Hopf's theorem, \"H\"*(\"G\") is an exterior algebra on generators of odd degree (the subspace of primitive elements). It follows that the edge homomorphisms \n\nof the spectral sequence must eventually take the space of primitive elements in the left column \"H\"*(\"G\") of the page \"E\" bijectively into the bottom row \"H\"*(\"BH\"): we know \"G\" and \"H\" have the same rank,\nso if the collection of edge homomorphisms were \"not\" full rank on the primitive subspace, then the image of the bottom row \"H\"*(\"BH\") in the final page \"H\"*(\"G\"/\"H\") of the sequence would be infinite-dimensional as a \"k\"-vector space, which is impossible, for instance by cellular cohomology again, because a compact homogeneous space admits a finite CW structure.\n\nThus the ring map \"H\"*(\"G\"/\"H\") → \"H\"*(\"G\") is trivial in this case, and the characteristic map is surjective, so that \"H\"*(\"G\"/\"H\") is a quotient of \"H\"*(\"BH\"). The kernel of the map is the ideal generated by the images of primitive elements under the edge homomorphisms, which is also the ideal generated by positive-degree elements in the image of the canonical map \"H\"*(\"BG\") → \"H\"*(\"BH\") induced by the inclusion of \"H\" in \"G\". \n\nThe map \"H\"*(\"BG\") → \"H\"*(\"BT\") is injective, and likewise for \"H\", with image the subring \"H\"*(\"BT\") of elements invariant under the action of the Weyl group, so one finally obtains the concise description\n\nwhere formula_9 denotes positive-degree elements and the parentheses the generation of an ideal. For example, for the complete complex flag manifold \"U\"(\"n\")/\"T\", one has \n\nwhere the \"t\" are of degree 2 and the σ are the first \"n\" elementary symmetric polynomials in the variables \"t\". For a more concrete example, take \"n\" = 2, so that \"U\"(\"2\")/[\"U\"(1) × \"U\"(1)] is the complex Grassmannian Gr(1,ℂ) ≈ ℂ\"P\" ≈ \"S\". Then we expect the cohomology ring to be an exterior algebra on a generator of degree two (the fundamental class), and indeed,\n\nas hoped.\n\nIf \"G\" is a semisimple algebraic group (or Lie group) and \"V\" is a (finite dimensional) highest weight representation of \"G\", then the highest weight space is a point in the projective space P(\"V\") and its orbit under the action of \"G\" is a projective algebraic variety. This variety is a (generalized) flag variety, and furthermore, every (generalized) flag variety for \"G\" arises in this way.\n\nArmand Borel showed that this characterizes the flag varieties of a general semisimple algebraic group \"G\": they are precisely the complete homogeneous spaces of \"G\", or equivalently (in this context), the projective \"G\"-varieties.\n\nLet \"G\" be a semisimple Lie group with maximal compact subgroup \"K\". Then \"K\" acts transitively on any conjugacy class of parabolic subgroups, and hence the generalized flag variety \"G\"/\"P\" is a compact homogeneous Riemannian manifold \"K\"/(\"K\"∩\"P\") with isometry group \"K\". Furthermore, if \"G\" is a complex Lie group, \"G\"/\"P\" is a homogeneous Kähler manifold.\n\nTurning this around, the Riemannian homogeneous spaces\n\nadmit a strictly larger Lie group of transformations, namely \"G\". Specializing to the case that \"M\" is a symmetric space, this observation yields all symmetric spaces admitting such a larger symmetry group, and these spaces have been classified by Kobayashi and Nagano.\n\nIf \"G\" is a complex Lie group, the symmetric spaces \"M\" arising in this way are the compact Hermitian symmetric spaces: \"K\" is the isometry group, and \"G\" is the biholomorphism group of \"M\".\n\nOver the real numbers, a real flag manifold is also called an R-space, and the R-spaces which are Riemannian symmetric spaces under \"K\" are known as symmetric R-spaces. The symmetric R-spaces which are not Hermitian symmetric are obtained by taking \"G\" to be a real form of the biholomorphism group \"G\" of a Hermitian symmetric space \"G\"/\"P\" such that \"P\" := \"P\"∩\"G\" is a parabolic subgroup of \"G\". Examples include projective spaces (with \"G\" the group of projective transformations) and spheres (with \"G\" the group of conformal transformations).\n\n\n"}
{"id": "5678057", "url": "https://en.wikipedia.org/wiki?curid=5678057", "title": "Godunov's theorem", "text": "Godunov's theorem\n\nIn numerical analysis and computational fluid dynamics, Godunov's theorem — also known as Godunov's order barrier theorem — is a mathematical theorem important in the development of the theory of high resolution schemes for the numerical solution of partial differential equations.\n\nThe theorem states that: \n\nProfessor Sergei K. Godunov originally proved the theorem as a Ph.D. student at Moscow State University. It is his most influential work in the area of applied and numerical mathematics and has had a major impact on science and engineering, particularly in the development of methods used in computational fluid dynamics (CFD) and other computational fields. One of his major contributions was to prove the theorem (Godunov, 1954; Godunov, 1959), that bears his name.\n\nWe generally follow Wesseling (2001).\n\nAside\n\nAssume a continuum problem described by a PDE is to be computed using a numerical scheme based upon a uniform computational grid and a one-step, constant step-size, \"M\" grid point, integration algorithm, either implicit or explicit. Then if formula_1 and formula_2, such a scheme can be described by\n\nIn other words, the solution formula_4 at time formula_5 and location formula_6 is a linear function of the solution at the previous time step formula_7. We assume that formula_8 determines formula_4 uniquely. Now, since the above equation represents a linear relationship between formula_10 and formula_11 we can perform a linear transformation to obtain the following equivalent form,\n\nTheorem 1: \"Monotonicity preserving\"\n\nThe above scheme of equation (2) is monotonicity preserving if and only if\n\n\"Proof\" - Godunov (1959)\n\nCase 1: (sufficient condition) \n\nAssume (3) applies and that formula_14 is monotonically increasing with formula_15.\n\nThen, because formula_16 it therefore follows that formula_17 because\n\nThis means that monotonicity is preserved for this case.\n\nCase 2: (necessary condition) \n\nWe prove the necessary condition by contradiction. Assume that formula_19 for some formula_20 and choose the following monotonically increasing formula_21,\n\nThen from equation (2) we get\n\nNow choose formula_24, to give\n\nwhich implies that formula_4 is NOT increasing, and we have a contradiction. Thus, monotonicity is NOT preserved for formula_27, which completes the proof.\n\nTheorem 2: \"Godunov’s Order Barrier Theorem\"\n\nLinear one-step second-order accurate numerical schemes for the convection equation \n\ncannot be monotonicity preserving unless\n\nwhere formula_30 is the signed Courant–Friedrichs–Lewy condition (CFL) number.\n\n\"Proof\" - Godunov (1959)\n\nAssume a numerical scheme of the form described by equation (2) and choose\n\nThe exact solution is\n\nIf we assume the scheme to be at least second-order accurate, it should produce the following solution exactly\n\nSubstituting into equation (2) gives:\n\nSuppose that the scheme IS monotonicity preserving, then according to the theorem 1 above, formula_35. \n\nNow, it is clear from equation (15) that\n\nAssume formula_37 and choose formula_15 such that formula_39 . This implies that formula_40 and formula_41 .\n\nIt therefore follows that,\n\nwhich contradicts equation (16) and completes the proof.\n\nThe exceptional situation whereby formula_43 is only of theoretical interest, since this cannot be realised with variable coefficients. Also, integer CFL numbers greater than unity would not be feasible for practical problems.\n\n\n\n"}
{"id": "1943892", "url": "https://en.wikipedia.org/wiki?curid=1943892", "title": "Greek letters used in mathematics, science, and engineering", "text": "Greek letters used in mathematics, science, and engineering\n\nGreek letters are used in mathematics, science, engineering, and other areas where mathematical notation is used as symbols for constants, special functions, and also conventionally for variables representing certain quantities. In these contexts, the capital letters and the small letters represent distinct and unrelated entities. Those Greek letters which have the same form as Latin letters are rarely used: capital A, B, E, Z, H, I, K, M, N, O, P, T, Y, X. Small ι, ο and υ are also rarely used, since they closely resemble the Latin letters i, o and u. Sometimes font variants of Greek letters are used as distinct symbols in mathematics, in particular for ε/ϵ and π/ϖ. The archaic letter digamma (Ϝ/ϝ/ϛ) is sometimes used.\n\nThe Bayer designation naming scheme for stars typically uses the first Greek letter, α, for the brightest star in each constellation, and runs through the alphabet before switching to Latin letters.\n\nIn mathematical finance, the Greeks are the variables denoted by Greek letters used to describe the risk of certain investments.\n\nThe Greek letter forms used in mathematics are often different from those used in Greek-language text: they are designed to be used in isolation, not connected to other letters, and some use variant forms which are not normally used in current Greek typography.\n\nThe OpenType font format has the feature tag 'mgrk' \"Mathematical Greek\" to identify a glyph as representing a Greek letter to be used in mathematical (as opposed to Greek language) contexts.\n\nThe table below shows a comparison of Greek letters rendered in TeX and HTML.\nThe font used in the TeX rendering is an italic style. This is in line with the convention that variables should be italicized. As Greek letters are more often than not used as variables in mathematical formulas, a Greek letter appearing similar to the TeX rendering is more likely to be encountered in works involving mathematics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnote: a symbol for the empty set, formula_23, resembles Φ but is not Φ\n\n\n\n\n\n"}
{"id": "1139981", "url": "https://en.wikipedia.org/wiki?curid=1139981", "title": "Hyperbolic angle", "text": "Hyperbolic angle\n\nIn mathematics, a hyperbolic angle is a geometric figure that defines a section of a hyperbola. The relationship of a hyperbolic angle to a hyperbola parallels the relationship of an \"ordinary\" angle to a circle.\n\nHyperbolic angle is defined in terms of the \"standard\" rectangular hyperbola: \"y\" = 1/\"x\", for positive \"x\". It can be described as the angle subtended at the origin by a given interval on the hyperbola, while its magnitude is defined as the magnitude of the area so subtended - that is, the area bounded on two sides by rays from the origin to the endpoints of the interval and on the third side by the curve of the interval itself.\n\nHyperbolic angle is used as the independent variable for the hyperbolic functions sinh, cosh, and tanh, because these functions may be premised on hyperbolic analogies to the corresponding circular trigonometric functions by regarding a hyperbolic angle as defining a hyperbolic triangle.\nThe parameter thus becomes one of the most useful in the calculus of real variables.\n\nWe consider the rectangular hyperbola formula_1, and (by convention) pay particular attention to the \"branch\" formula_2.\n\nWe first define:\n\nNote that, because of the role played by the natural logarithm:\n\nFinally, we extend the definition of \"hyperbolic angle\" to that subtended by any interval on the hyperbola. Suppose formula_10 are real numbers such that formula_11 and formula_12, so that formula_13 and formula_14 are points on the hyperbola formula_15 and determine an interval on it. Then the squeeze mapping formula_16 maps the angle formula_17 to the \"standard position\" angle formula_18. By the result of Gregoire de Saint-Vincent, the hyperbolic sectors determined by these angles have the same area, which is taken to be the magnitude of the angle. This magnitude is formula_19.\n\nA unit circle formula_20 has a circular sector with an area half of the circular angle in radians. Analogously, a unit hyperbola formula_21 has a hyperbolic sector with an area half of the hyperbolic angle.\n\nThere is also a projective resolution between circular and hyperbolic cases: both curves are conic sections, and hence are treated as projective ranges in projective geometry. Given an origin point on one of these ranges, other points correspond to angles. The idea of addition of angles, basic to science, corresponds to addition of points on one of these ranges as follows:\n\nCircular angles can be characterised geometrically by the property that if two chords \"P\"\"P\" and \"P\"\"P\" subtend angles \"L\" and \"L\" at the centre of a circle, their sum is the angle subtended by a chord \"PQ\", where \"PQ\" is required to be parallel to \"P\"\"P\".\n\nThe same construction can also be applied to the hyperbola. If \"P\" is taken to be the point , \"P\" the point , and \"P\" the point , then the parallel condition requires that \"Q\" be the point . It thus makes sense to define the hyperbolic angle from \"P\" to an arbitrary point on the curve as a logarithmic function of the point's value of \"x\".\n\nWhereas in Euclidean geometry moving steadily in an orthogonal direction to a ray from the origin traces out a circle, in a pseudo-Euclidean plane steadily moving orthogonally to a ray from the origin traces out a hyperbola. In Euclidean space, the multiple of a given angle traces equal distances around a circle while it traces exponential distances upon the hyperbolic line.\n\nBoth circular and hyperbolic angle provide instances of an invariant measure. Arcs with an angular magnitude on a circle generate a measure on certain measurable sets on the circle whose magnitude does not vary as the circle turns or rotates. For the hyperbola the turning is by squeeze mapping, and the hyperbolic angle magnitudes stay the same when the plane is squeezed by a mapping\n\nThe quadrature of the hyperbola is the evaluation of the area of a hyperbolic sector. It can be shown to be equal to the corresponding area against an asymptote. The quadrature was first accomplished by Gregoire de Saint-Vincent in 1647 in his momentous \"Opus geometricum quadrature circuli et sectionum coni\". As expressed by a historian,\n\nA. A. de Sarasa interpreted the quadrature as a logarithm and thus the geometrically defined natural logarithm (or \"hyperbolic logarithm\") is understood as the area under to the right of . As an example of a transcendental function, the logarithm is more familiar than its motivator, the hyperbolic angle. Nevertheless, the hyperbolic angle plays a role when the theorem of Saint-Vincent is advanced with squeeze mapping.\n\nCircular trigonometry was extended to the hyperbola by Augustus De Morgan in his textbook \"Trigonometry and Double Algebra\". In 1878 W.K. Clifford used the hyperbolic angle to parametrize a unit hyperbola, describing it as \"quasi-harmonic motion\".\n\nIn 1894 Alexander Macfarlane circulated his essay \"The Imaginary of Algebra\", which used hyperbolic angles to generate hyperbolic versors, in his book \"Papers on Space Analysis\". The following year Bulletin of the American Mathematical Society published Mellen W. Haskell's outline of the hyperbolic functions.\n\nWhen Ludwik Silberstein penned his popular 1914 textbook on the new theory of relativity, he used the rapidity concept based on hyperbolic angle \"a\", where , the ratio of velocity \"v\" to the speed of light. He wrote:\n\nSilberstein also uses Lobachevsky's concept of angle of parallelism Π(\"a\") to obtain .\n\nThe hyperbolic angle is often presented as if it were an imaginary number. Thus, if \"x\" is a real number and , then\nso that the hyperbolic functions cosh and sinh can be presented through the circular functions. But these identities do not arise from a circle or rotation, rather they can be understood in terms of infinite series. In particular, the one expressing the exponential function (formula_23 ) consists of even and odd terms, the former comprise the cosh function (formula_24), the latter the sinh function (formula_25). The infinite series for cosine is derived from cosh by turning it into an alternating series, and the series for sine comes from making sinh into an alternating series. The above identities use the number \"i\" to remove the alternating factor (−1) from terms of the series to restore the full halves of the exponential series. Nevertheless, in the theory of holomorphic functions, the hyperbolic sine and cosine functions are incorporated into the complex sine and cosine functions.\n\n\n"}
{"id": "13623185", "url": "https://en.wikipedia.org/wiki?curid=13623185", "title": "Injection locking", "text": "Injection locking\n\nInjection locking and injection pulling are the frequency effects that can occur when a harmonic oscillator is disturbed by a second oscillator operating at a nearby frequency. When the coupling is strong enough and the frequencies near enough, the second oscillator can capture the first oscillator, causing it to have essentially identical frequency as the second. This is injection locking. When the second oscillator merely disturbs the first but does not capture it, the effect is called injection pulling. Injection locking and pulling effects are observed in numerous types of physical systems, however the terms are most often associated with electronic oscillators or laser resonators.\n\nInjection locking has been used in beneficial and clever ways in the design of early television sets and oscilloscopes, allowing the equipment to be synchronized to external signals at a relatively low cost. Injection locking has also been used in high performance frequency doubling circuits. However, injection locking and pulling, when unintended, can degrade the performance of phase locked loops and RF integrated circuits.\n\nInjection pulling and injection locking can be observed in numerous physical systems where pairs of oscillators are coupled together. Perhaps the first to document these effects was Christiaan Huygens, the inventor of the pendulum clock, who was surprised to note that two pendulum clocks which normally would keep slightly different time nonetheless became perfectly synchronized when hung from a common beam. Modern researchers have confirmed his suspicion that the pendulums were coupled by tiny back-and-forth vibrations in the wooden beam. The two clocks became injection locked to a common frequency.\nIn a modern-day voltage-controlled oscillator an injection-locking signal may override its low-frequency control voltage, resulting in loss of control. When intentionally employed, injection locking provides a means to significantly reduce power consumption and possibly reduce phase noise in comparison to other frequency synthesizer and PLL design techniques. In similar fashion, the frequency output of large lasers can be purified by injection locking them with high accuracy reference lasers (see injection seeder).\n\nAn injection-locked oscillator (ILO) is usually based on cross-coupled LC oscillator. It has been employed for frequency division or jitter reduction in PLL, with the input of pure sinusoidal waveform. It was employed in continuous mode clock and data recovery (CDR) or clock recovery to perform clock restoration from the aid of either preceding pulse generation circuit to convert non-return-to-zero (NRZ) data to pseudo-return-to-zero (PRZ) format or nonideal retiming circuit residing at the transmitter side to couple the clock signal into the data. Recently, the ILO was employed for burst mode clock recovery scheme.\n\nThe operation of ILO is based on the fact that the local oscillation can be locked to the frequency and phase of external injection signal under proper conditions.\n\nHigh-speed logic signals and their harmonics are potential threats to an oscillator. The leakage of these and other high frequency signals into an oscillator through a substrate concomitant with an unintended lock is unwanted injection locking.\n\nInjection locking can also provide a means of gain at a low power cost in certain applications.\n\nInjection (aka frequency) pulling occurs when an interfering frequency source disturbs an oscillator but is unable to injection lock it. The frequency of the oscillator is pulled towards the frequency source as can be seen in the spectrogram. The failure to lock may be due to insufficient coupling, or because the injection source frequency lies outside the locking window of the oscillator.\nEntrainment has been used to refer to the process of mode locking of coupled driven oscillators, which is the process whereby two interacting oscillating systems, which have different periods when they function independently, assume a common period. The two oscillators may fall into synchrony, but other phase relationships are also possible. The system with the greater frequency slows down, and the other speeds up.\n\nDutch physicist Christiaan Huygens, the inventor of the pendulum clock, introduced the concept after he noticed, in 1666, that the pendulums of two clocks mounted on a common board had synchronized, and subsequent experiments duplicated this phenomenon. He described this effect as \"odd sympathy\". The two pendulum clocks synchronized with their pendulums swinging in opposite directions, 180° out of phase, but in-phase states can also result. Entrainment occurs because small amounts of energy are transferred between the two systems when they are out of phase in such a way as to produce negative feedback. As they assume a more stable phase relationship, the amount of energy gradually reduces to zero. In the realm of physics, Huygens' observations are related to resonance and the resonant coupling of harmonic oscillators, which also gives rise to sympathetic vibrations.\n\nA 2002 study of Huygens' observations show that an antiphase stable oscillation was somewhat fortuitous, and that there are other possible stable solutions, including a \"death state\" where a clock stops running, depending on the strength of the coupling between the clocks.\n\nMode locking between driven oscillators can be easily demonstrated using mechanical metronomes on a common, easily movable surface. Such mode locking is important for many biological systems including the proper operation of pacemakers.\n\nThe use of the word entrainment in the modern Physics literature most often refers to the movement of one fluid, or collection of particulates, by another (see Entrainment (hydrodynamics)). The use of the word to refer to mode locking of non-linear coupled oscillators appears mostly after about 1980, and remains relatively rare in comparison.\n\nA similar coupling phenomenon was characterized in hearing aids when the adaptive feedback cancellation is used. This chaotic artifact (entrainment) is observed when correlated input signals are presented to an adaptive feedback canceller.\n\nIn recent years, aperiodic entrainment has been identified as an alternative form of entrainment that is of interest in biological rhythms.\n\n\n\n\n"}
{"id": "7426218", "url": "https://en.wikipedia.org/wiki?curid=7426218", "title": "Integrated mathematics", "text": "Integrated mathematics\n\nIntegrated mathematics is the term used in the United States to describe the style of mathematics education which integrates many topics or strands of mathematics throughout each year of secondary school. Each math course in secondary school covers topics in algebra, geometry, trigonometry and analysis. Nearly all countries throughout the world, except the United States, follow this type of curriculum.\n\nIn the United States, topics are usually integrated throughout elementary school up to the eighth grade. Beginning with high school level courses, topics are usually separated so that one year a student focuses entirely on algebra, the next year entirely on geometry, and then another year of algebra and later an optional fifth year of analysis (calculus). The one exception in the American high school curriculum is the fourth year of math, typically referred to as precalculus, which usually integrates algebra, analysis, trigonometry, and geometry topics. Statistics may be integrated into all the courses or presented as a separate course.\n\nNew York State began using integrated math curricula in the 1980s, but recently returned to a traditional curriculum. A few other localities in the United States have also tried such integrated curricula, including Georgia, which mandated them in 2008 but subsequently made them optional. More recently, a few other states have mandated that all districts change to integrated curricula, including North Carolina, West Virginia and Utah. Some districts in other states, including California, have either switched or are considering switching to an integrated curriculum.\n\nUnder the Common Core Standards adopted by most states in 2012, high school mathematics may be taught using either a traditional American approach or an integrated curriculum. The only difference would be the order in which the topics are taught. Supporters of using integrated curricula in the United States believe that students will be able to see the connections between algebra and geometry better in an integrated curriculum.\n\n\"General mathematics\" is another term for a mathematics course organized around different branches of mathematics, with topics arranged according to the main objective of the course.\nWhen applied to primary education, the term \"general mathematics\" may encompass mathematical concepts more complex than basic arithmetic, like number notation, addition and multiplication tables, fractions and related operations, measurement units.\nWhen used in context of higher education, the term may encompass mathematical terminology and concepts, finding and applying appropriate techniques to solve routine problems, interpreting and representing practical information given in various forms, interpreting and using mathematical models, and constructing mathematical arguments to solve familiar and unfamiliar problems.\n"}
{"id": "5626232", "url": "https://en.wikipedia.org/wiki?curid=5626232", "title": "Intersection theorem", "text": "Intersection theorem\n\nIn projective geometry, an intersection theorem or incidence theorem is a statement concerning an incidence structure – consisting of points, lines, and possibly higher-dimensional objects and their incidences – together with a pair of objects and (for instance, a point and a line). The \"theorem\" states that, whenever a set of objects satisfies the incidences (\"i.e.\" can be identified with the objects of the incidence structure in such a way that incidence is preserved), then the objects and must also be incident. An intersection theorem is not necessarily true in all projective geometries; it is a property that some geometries satisfy but others don't.\n\nFor example, Desargues' theorem can be stated using the following incidence structure:\nThe implication is then formula_5—that point is incident with line .\n\nDesargues' theorem holds in a projective plane if and only if is the projective plane over some division ring (skewfield} — formula_6. The projective plane is then called \"desarguesian\".\nA theorem of Amitsur and Bergman states that, in the context of desarguesian projective planes, for every intersection theorem there is a rational identity such that the plane satisfies the intersection theorem if and only if the division ring satisfies the rational identity.\n\n"}
{"id": "210731", "url": "https://en.wikipedia.org/wiki?curid=210731", "title": "Invariance of domain", "text": "Invariance of domain\n\nInvariance of domain is a theorem in topology about homeomorphic subsets of Euclidean space R. It states: \n\nThe theorem and its proof are due to L. E. J. Brouwer, published in 1912. The proof uses tools of algebraic topology, notably the Brouwer fixed point theorem.\n\nThe conclusion of the theorem can equivalently be formulated as: \"\"f\" is an open map\".\n\nNormally, to check that \"f\" is a homeomorphism, one would have to verify that both \"f\" and its inverse function \"f\" are continuous; the theorem says that if the domain is an \"open\" subset of R and the image is also in R, then continuity of \"f\" is automatic. Furthermore, the theorem says that if two subsets \"U\" and \"V\" of R are homeomorphic, and \"U\" is open, then \"V\" must be open as well. (Note that V is\nopen as a subset of R, and not just in the subspace topology. Openness of V in the subspace topology is automatic.\n) Both of these statements are not at all obvious and are not generally true if one leaves Euclidean space.\nIt is of crucial importance that both domain and range of \"f\" are contained in Euclidean space \"of the same dimension\". Consider for instance the map \"f\" : (0,1) → R with \"f\"(\"t\") = (\"t\",0). This map is injective and continuous, the domain is an open subset of R, but the image is not open in R. A more extreme example is \"g\" : (−1.1,1) → R with \"g\"(\"t\") = (\"t\" − 1, \"t\" − \"t\") because here \"g\" is injective and continuous but does not even yield a homeomorphism onto its image.\n\nThe theorem is also not generally true in infinite dimensions. Consider for instance the Banach space \"l\" of all bounded real sequences. Define \"f\" : \"l\" → \"l\" as the shift \"f\"(\"x\",\"x\"...) = (0, \"x\", \"x\"...). Then \"f\" is injective and continuous, the domain is open in \"l\", but the image is not.\n\nAn important consequence of the domain invariance theorem is that R cannot be homeomorphic to R if \"m\" ≠ \"n\". Indeed, no non-empty open subset of R can be homeomorphic to any open subset of R in this case.\n\nThe domain invariance theorem may be generalized to manifolds: if \"M\" and \"N\" are topological \"n\"-manifolds without boundary and \"f\" : \"M\" → \"N\" is a continuous map which is locally one-to-one (meaning that every point in \"M\" has a neighborhood such that \"f\" restricted to this neighborhood is injective), then \"f\" is an open map (meaning that \"f\"(\"U\") is open in \"N\" whenever \"U\" is an open subset of \"M\") and a local homeomorphism.\n\nThere are also generalizations to certain types of continuous maps from a Banach space to itself.\n\n"}
{"id": "6528528", "url": "https://en.wikipedia.org/wiki?curid=6528528", "title": "Journal of Symbolic Computation", "text": "Journal of Symbolic Computation\n\nThe Journal of Symbolic Computation is a peer-reviewed monthly scientific journal covering all aspects of symbolic computation published by Academic Press and then by Elsevier. It is targeted to both mathematicians and computer scientists. It was established in 1985 by Bruno Buchberger, who served as its editor until 1994.\n\nThe journal covers a wide variety of topics, including:\n\nAccording to the \"Journal Citation Reports\", its 2009 impact factor is 0.853. The journal is abstracted and indexed by Scopus and the Science Citation Index.\n\n"}
{"id": "41317", "url": "https://en.wikipedia.org/wiki?curid=41317", "title": "Line code", "text": "Line code\n\nIn telecommunication, a line code is a pattern of voltage, current, or photons used to represent digital data transmitted down a transmission line. This repertoire of signals is usually called a constrained code in data storage systems. Some signals are more prone to error than others when conveyed over a communication channel as the physics of the communication or storage medium constrains the repertoire of signals that can be used reliably. \n\nCommon line encodings are unipolar, polar, bipolar, and Manchester code.\n\nAfter line coding, the signal is put through a physical communication channel, either a transmission medium or data storage medium. The most common physical channels are:\n\nSome of the more common binary line codes include:\n\nEach line code has advantages and disadvantages. The particular line code used is chosen to meet one or more of the following criteria:\n\nThe disparity of a bit pattern is the difference in the number of one bits vs the number of zero bits. The running disparity is the running total of the disparity of all previously transmitted words.\n\nUnfortunately, most long-distance communication channels cannot reliably transport a DC component. The DC component is also called the disparity, the bias, or the DC coefficient. The simplest possible line code, unipolar, gives too many errors on such systems, because it has an unbounded DC component.\n\nMost line codes eliminate the DC component such codes are called DC-balanced, zero-DC, or DC-free.\nThere are three ways of eliminating the DC component:\n\n\nBipolar line codes have two polarities, are generally implemented as RZ, and have a radix of three since there are three distinct output levels. One of the principle advantages of this type of code is that it can completely eliminate any DC component. This is important if the signal must pass through a transformer or a long transmission line.\n\nUnfortunately, several long-distance communication channels have polarity ambiguity.\nTo compensate, several people have designed polarity-insensitive transmission systems.\nThere are three ways of providing unambiguous reception of \"0\" bits or \"1\" bits over such channels:\n\nFor reliable clock recovery at the receiver, a maximum run length constraint may be imposed on the generated channel sequence, i.e., the maximum number of consecutive ones or zeros is bounded to a reasonable number. A clock period is recovered by observing transitions in the received sequence, so that a maximum run length guarantees such clock recovery, while sequences without such a constraint could seriously hamper the detection quality.\n\nRun-length limited or RLL coding is a line coding technique that is used to send arbitrary data over a communications channel with bandwidth limits. RLL codes are defined by four main parameters: \"m\", \"n\", \"d\", \"k\". The first two, \"m\"/\"n\", refer to the rate of the code, while the remaining two specify the minimal \"d\" and maximal \"k\" number of zeroes between consecutive ones. This is used in both telecommunication and storage systems that move a medium past a fixed recording head.\n\nSpecifically, RLL bounds the length of stretches (runs) of repeated bits during which the signal does not change. If the runs are too long, clock recovery is difficult; if they are too short, the high frequencies might be attenuated by the communications channel. By modulating the data, RLL reduces the timing uncertainty in decoding the stored data, which would lead to the possible erroneous insertion or removal of bits when reading the data back. This mechanism ensures that the boundaries between bits can always be accurately found (preventing bit slip), while efficiently using the media to reliably store the maximal amount of data in a given space.\n\nEarly disk drives used very simple encoding schemes, such as RLL (0,1) FM code, followed by RLL (1,3) MFM code which were widely used in hard disk drives until the mid-1980s and are still used in digital optical discs such as CD, DVD, MD, Hi-MD and Blu-ray using EFM and EFMPLus codes. Higher density RLL (2,7) and RLL (1,7) codes became the \"de facto\" industry standard for hard disks by the early 1990s.\n\nLine coding should make it possible for the receiver to synchronize itself to the phase of the received signal. If the synchronization is not ideal, then the signal to be decoded will not have optimal differences (in amplitude) between the various digits or symbols used in the line code. This will increase the error probability in the received data.\nBiphase line codes require at least one transition per bit time. This makes it easier to synchronize the transceivers and detect errors, however, the baud rate is greater than that of NRZ codes.\n\nIt is also preferred for the line code to have a structure that will enable error detection.\nNote that the line-coded signal and a signal produced at a terminal may differ, thus requiring translation.\n\nA line code will typically reflect technical requirements of the transmission medium, such as optical fiber or shielded twisted pair. These requirements are unique for each medium, because each one has different behavior related to interference, distortion, capacitance and loss of amplitude.\n\nOptical line codes:\n\n\n"}
{"id": "30654226", "url": "https://en.wikipedia.org/wiki?curid=30654226", "title": "List of Chinese mathematicians", "text": "List of Chinese mathematicians\n\nThis is a list of Chinese mathematicians. With a history spanning over three millennia, Chinese mathematics is believed to have initially developed largely independently of other cultures.\n\n\n\n"}
{"id": "527369", "url": "https://en.wikipedia.org/wiki?curid=527369", "title": "Lists of mathematics topics", "text": "Lists of mathematics topics\n\nThis article itemizes the various lists of mathematics topics. Some of these lists link to hundreds of articles; some link only to a few. The template to the right includes links to alphabetical lists of all mathematical articles. This article brings together the same content organized in a manner better suited for browsing.\n\nThe purpose of this list is \"not\" similar to that of the Mathematics Subject Classification formulated by the American Mathematical Society. Many mathematics journals ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The subject codes so listed are used by the two major reviewing databases, \"Mathematical Reviews\" and \"Zentralblatt MATH\". This list has some items that would not fit in such a classification, such as list of exponential topics and list of factorial and binomial topics, which may surprise the reader with the diversity of their coverage.\n\nThis branch is typically taught in secondary education or in the first year of university.\n\nSee also Areas of mathematics and Glossary of areas of mathematics.\n\nAs a rough guide this list is divided into pure and applied sections although in reality these branches are overlapping and intertwined.\n\nAlgebra includes the study of algebraic structures, which are sets and operations defined on these sets satisfying certain axioms. The field of algebra is further divided according to which structure is studied; for instance, group theory concerns an algebraic structure called \"group\".\n\nCalculus studies the computation of limits, derivatives, and integrals of functions of real numbers, and in particular studies instantaneous rates of change. Analysis evolved from calculus.\n\nGeometry is initially the study of spatial figures like circles and cubes, though it has been generalized considerably. Topology developed from geometry; it looks at those properties that do not change even when the figures are deformed by stretching and bending, like dimension.\n\nCombinatorics concerns the study of discrete (and usually finite) objects. Aspects include \"counting\" the objects satisfying certain criteria (\"enumerative combinatorics\"), deciding when the criteria can be met, and constructing and analyzing objects meeting the criteria (as in \"combinatorial designs and matroid theory\"), finding \"largest\", \"smallest\", or \"optimal\" objects (\"extremal combinatorics\" and \"combinatorial optimization\"), and finding algebraic structures these objects may have (\"algebraic combinatorics\").\n\nLogic is the foundation which underlies mathematical logic and the rest of mathematics. It tries to formalize valid reasoning. In particular, it attempts to define what constitutes a proof.\n\nNumber theory studies the natural, or whole, numbers. One of the central concepts in number theory is that of the prime number, and there are many questions about primes that appear simple but whose resolution continues to elude mathematicians.\n\nA differential equation is an equation involving an unknown function and its derivatives.\n\nIn a dynamical system, a fixed rule describes the time dependence of a point in a geometrical space. The mathematical models used to describe the swinging of a clock pendulum, the flow of water in a pipe, or the number of fish each spring in a lake are examples of dynamical systems.\n\nMathematical physics is concerned with \"the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories\".\n\nThe fields of mathematics and computing intersect both in computer science, the study of algorithms and data structures, and in scientific computing, the study of algorithmic methods for solving problems in mathematics, science and engineering.\n\nInformation theory is a branch of applied mathematics and electrical engineering involving the quantification of information. Historically, information theory was developed to find fundamental limits on compressing and reliably communicating data.\n\nSignal processing is the analysis, interpretation, and manipulation of signals. Signals of interest include sound, images, biological signals such as ECG, radar signals, and many others. Processing of such signals includes filtering, storage and reconstruction, separation of information from noise, compression, and feature extraction.\n\nProbability theory is the formalization and study of the mathematics of uncertain events or knowledge. The related field of mathematical statistics develops statistical theory with mathematics. Statistics, the science concerned with collecting and analyzing data, is an autonomous discipline (and not a subdiscipline of applied mathematics).\n\nGame theory is a branch of mathematics that uses models to study interactions with formalized incentive structures (\"games\"). It has applications in a variety of fields, including economics, evolutionary biology, political science, social psychology and military strategy.\n\nOperations research is the study and use of mathematical models, statistics and algorithms to aid in decision-making, typically with the goal of improving or optimizing performance of real-world systems.\n\n\nA mathematical statement amounts to a proposition or assertion of some mathematical fact, formula, or construction. Such statements include axioms and the theorems that may be proved from them, conjectures that may be unproven or even unprovable, and also algorithms for computing the answers to questions that can be expressed mathematically.\n\n\nAmong mathematical objects are numbers, functions, sets, a great variety of things called \"spaces\" of one kind or another, algebraic structures such as rings, groups, or fields, and many other things.\n\n\n\nMathematicians study and research in all the different areas of mathematics. The publication of new discoveries in mathematics continues at an immense rate in hundreds of scientific journals, many of them devoted to mathematics and many devoted to subjects to which mathematics is applied (such as theoretical computer science and theoretical physics).\n\n\n\nIn calculus, the integral of a function is a generalization of area, mass, volume, sum, and total. The following pages list the integrals of many different functions.\n\n\n\n\n\n"}
{"id": "38246589", "url": "https://en.wikipedia.org/wiki?curid=38246589", "title": "Loss network", "text": "Loss network\n\nIn queueing theory, a loss network is a stochastic model of a telephony network in which calls are routed around a network between nodes. The links between nodes have finite capacity and thus some calls arriving may find no route available to their destination. These calls are lost from the network, hence the name loss networks.\n\nThe loss network was first studied by Erlang for a single telephone link. Frank Kelly was awarded the Frederick W. Lanchester Prize for his 1991 paper \"Loss Networks\" where he demonstrated the behaviour of loss networks can exhibit hysteresis.\n\nConsider a network with \"J\" links labelled 1, 2, …, \"J\" and that each link \"j\" has \"C\" circuits. Let \"R\" be the set of all possible routes in the network (combinations of links a call might use) and each route \"r\", write \"A\" for the number of circuits route \"r\" uses on link \"j\" (\"A\" is therefore a \"J\" x |\"R\"| matrix). Consider the case where all elements of \"A\" are either 0 or 1 and for each route \"r\" calls requiring use of the route arrive according to a Poisson process of rate \"v\". When a call arrives if there is sufficient capacity remaining on all the required links the call is accepted and occupies the network for an exponentially distributed length of time with parameter 1. If there is insufficient capacity on any individual link to accept the call it is rejected (lost) from the network.\n\nWrite \"n\"(\"t\") for the number of calls on route \"r\" in progress at time \"t\", \"n\"(\"t\") for the vector (\"n\"(\"t\") : \"r\" in \"R\") and \"C\" = (\"C\", \"C\", ... , \"C\"). Then the continuous-time Markov process \"n\"(\"t\") has unique stationary distribution\nwhere\nand\n\nFrom this result loss probabilities for calls arriving on different routes can be calculated by summing over appropriate states.\n\nThere are common algorithms for computing the loss probabilities in loss networks\n"}
{"id": "7972057", "url": "https://en.wikipedia.org/wiki?curid=7972057", "title": "MESH (cipher)", "text": "MESH (cipher)\n\nIn cryptography, MESH is a block cipher designed in 2002 by Jorge Nakahara, Jr., Vincent Rijmen, Bart Preneel, and Joos Vandewalle. MESH is based directly on IDEA and uses the same basic operations.\n\nMESH is actually a family of 3 variant ciphers with block sizes of 64, 96, and 128 bits. The key size is twice the block size. The number of rounds is 8.5, 10.5, or 12.5, depending on the block size. The algorithm uses a Lai-Massey scheme based on IDEA's, but with a larger round structure, or \"MA-box\". MESH also has a more complex key schedule than IDEA, intended to prevent weak keys and other insecure patterns in subkeys.\n\n"}
{"id": "11916611", "url": "https://en.wikipedia.org/wiki?curid=11916611", "title": "Mary P. Dolciani", "text": "Mary P. Dolciani\n\nMary P. Dolciani (1923–1985) was an American mathematician, known for her work with secondary-school mathematics teachers.\n\nDolciani earned her Bachelor of Arts degree (B.A.) at Hunter College in New York City, and she completed her doctor of philosophy (Ph.D.) at Cornell University in 1947 with B. W. Jones as thesis advisor. She taught briefly at Vassar College before returning to Hunter, where she spent the next forty years. Dolciani taught mathematics there, and at times, she also served as a Dean or the Provost.\n\nBeginning in the 1960s Mary Dolciani wrote a series of high school mathematics textbook, \"Structure and Method\", which in 2000 - 2010 has experienced a resurgence of popularity.\n\nShortly before her death in 1985, Dolciani also co-wrote (along with two other mathematics educators) \"Pre-Algebra: An Accelerated Course\". This textbook was widely used in the later 1980s through the 1990s. In addition to teaching the pure mathematics, it emphasized the usefulness of algebra in various practical applications.\n\nThe Mathematical Association of America publishes a series of mathematical books named for her: \"The Dolciani Mathematical Expositions\". Also, the Association's headquarters building in Washington D.C. is named The Dolciani Mathematical Center in her honor.\n\nAlthough Dolciani is not well known by the general public, she was influential in developing the basic modern method used for teaching basic algebra in the United States (called \"Dolciani algebra\", which teaches it on the basis of drill like arithmetic, rather than on the basis of proofs as in Euclidean geometry). Dolciani also popularized the short-form names of the Properties that are familiar to many high school algebra students, e.g. the \"Zero Property\".\n\nIn 1982, Dr. Mary P. Dolciani Halloran, with her husband James J. Halloran and Eugene J. Callahan as Trustees, established the Mary P. Dolciani Halloran Foundation to further the study of mathematics and mathematics education.\n\n"}
{"id": "27210822", "url": "https://en.wikipedia.org/wiki?curid=27210822", "title": "Michel Broué", "text": "Michel Broué\n\nMichel Broué (born October 28, 1946) is a French mathematician. He holds a chair at Paris Diderot University. Broué has made contributions to algebraic geometry and representation theory.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\nHe is the son of French historian Pierre Broué and the father of French director and screenwriter Isabelle Broué and of French journalist and radio producer Caroline Broué.\n\n"}
{"id": "7309022", "url": "https://en.wikipedia.org/wiki?curid=7309022", "title": "Nearest neighbor search", "text": "Nearest neighbor search\n\nNearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values. Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set \"S\" of points in a space \"M\" and a query point \"q\" ∈ \"M\", find the closest point in \"S\" to \"q\". Donald Knuth in vol. 3 of \"The Art of Computer Programming\" (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a \"k\"-NN search, where we need to find the \"k\" closest points.\n\nMost commonly \"M\" is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, \"M\" is taken to be the \"d\"-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example are asymmetric Bregman divergences, for which the triangle inequality does not hold.\n\nThe nearest neighbor search problem arises in numerous fields of application, including:\n\nVarious solutions to the NNS problem have been proposed. The quality and usefulness of the algorithms are determined by the time complexity of queries as well as the space complexity of any search data structures that must be maintained. The informal observation usually referred to as the curse of dimensionality states that there is no general-purpose exact solution for NNS in high-dimensional Euclidean space using polynomial preprocessing and polylogarithmic search time.\n\nThe simplest solution to the NNS problem is to compute the distance from the query point to every other point in the database, keeping track of the \"best so far\". This algorithm, sometimes referred to as the naive approach, has a running time of \"O\"(\"dN\") where \"N\" is the cardinality of \"S\" and \"d\" is the dimensionality of \"M\". There are no search data structures to maintain, so linear search has no space complexity beyond the storage of the database. Naive search can, on average, outperform space partitioning approaches on higher dimensional spaces.\n\nSince the 1970s, branch and bound methodology has been applied to the problem. In the case of Euclidean space this approach is known as spatial index or spatial access methods. Several space-partitioning methods have been developed for solving the NNS problem. Perhaps the simplest is the k-d tree, which iteratively bisects the search space into two regions containing half of the points of the parent region. Queries are performed via traversal of the tree from the root to a leaf by evaluating the query point at each split. Depending on the distance specified in the query, neighboring branches that might contain hits may also need to be evaluated. For constant dimension query time, average complexity is \"O\"(log \"N\") in the case of randomly distributed points, worst case complexity is \"O\"(\"kN\"^(1-1/\"k\"))\nAlternatively the R-tree data structure was designed to support nearest neighbor search in dynamic context, as it has efficient algorithms for insertions and deletions such as the R* tree. R-trees can yield nearest neighbors not only for Euclidean distance, but can also be used with other distances.\n\nIn case of general metric space branch and bound approach is known under the name of metric trees. Particular examples include vp-tree and BK-tree.\n\nUsing a set of points taken from a 3-dimensional space and put into a BSP tree, and given a query point taken from the same space, a possible solution to the problem of finding the nearest point-cloud point to the query point is given in the following description of an algorithm. (Strictly speaking, no such point may exist, because it may not be unique. But in practice, usually we only care about finding any one of the subset of all point-cloud points that exist at the shortest distance to a given query point.) The idea is, for each branching of the tree, guess that the closest point in the cloud resides in the half-space containing the query point. This may not be the case, but it is a good heuristic. After having recursively gone through all the trouble of solving the problem for the guessed half-space, now compare the distance returned by this result with the shortest distance from the query point to the partitioning plane. This latter distance is that between the query point and the closest possible point that could exist in the half-space not searched. If this distance is greater than that returned in the earlier result, then clearly there is no need to search the other half-space. If there is such a need, then you must go through the trouble of solving the problem for the other half space, and then compare its result to the former result, and then return the proper result. The performance of this algorithm is nearer to logarithmic time than linear time when the query point is near the cloud, because as the distance between the query point and the closest point-cloud point nears zero, the algorithm needs only perform a look-up using the query point as a key to get the correct result.\n\nAn approximate nearest neighbor search algorithm is allowed to return points, whose distance from the query is at most formula_1 times the distance from the query to its nearest points. The appeal of this approach is that, in many cases, an approximate nearest neighbor is almost as good as the exact one. In particular, if the distance measure accurately captures the notion of user quality, then small differences in the distance should not matter.\n\nProximity graph methods (such as HNSW) are considered the current state-of-the-art for the approximate nearest neighbors search.\n\nThe methods are based on greedy traversing in proximity neighborhood graphs formula_2in which every point formula_3 is uniquely associated with vertex formula_4. The search for the nearest neighbors to a query \"q\" in the set \"S\" takes the form of searching for the vertex in the graph formula_2.\nThe basic algorithm - greedy search, works as follows: search starts from an enter-point vertex formula_4 by computing the distances from the query q to each vertex of its the neighborhood formula_7, and then finds a vertex with the minimal distance value. If the distance value between the query and the selected vertex is smaller than the one between the query and the current element, then the algorithm moves to the selected vertex, and it becomes new enter-point. The algorithm stops when it reaches a local minimum: a vertex whose neighborhood does not contain a vertex that is closer to the query than the vertex itself.\n\nThe idea of proximity neighborhood graphs was exploited in multiple publications, including the seminal paper by Arya and Mount, in the VoroNet system for the plane, in the RayNet system for the formula_8, in the Metrized Small World and HNSW algorithms for the general case of spaces with a distance function. These works were preceded by a pioneering paper by Toussaint, where he introduced a concept of a \"relative neighborhood\" graph.\n\nLocality sensitive hashing (LSH) is a technique for grouping points in space into 'buckets' based on some distance metric operating on the points. Points that are close to each other under the chosen metric are mapped to the same bucket with high probability.\n\nThe cover tree has a theoretical bound that is based on the dataset's doubling constant. The bound on search time is \"O\"(\"c\" log \"n\") where \"c\" is the expansion constant of the dataset.\n\nIn the special case where the data is a dense 3D map of geometric points, the projection geometry of the sensing technique can be used to dramatically simplify the search problem.\nThis approach requires that the 3D data is organized by a projection to a two dimensional grid and assumes that the data is spatially smooth across neighboring grid cells with the exception of object boundaries.\nThese assumptions are valid when dealing with 3D sensor data in applications such as surveying, robotics and stereo vision but may not hold for unorganized data in general.\nIn practice this technique has an average search time of \"O\"(\"1\") or \"O\"(\"K\") for the \"k\"-nearest neighbor problem when applied to real world stereo vision data.\n\nIn high dimensional spaces, tree indexing structures become useless because an increasing percentage of the nodes need to be examined anyway. To speed up linear search, a compressed version of the feature vectors stored in RAM is used to prefilter the datasets in a first run. The final candidates are determined in a second stage using the uncompressed data from the disk for distance calculation.\n\nThe VA-file approach is a special case of a compression based search, where each feature component is compressed uniformly and independently. The optimal compression technique in multidimensional spaces is Vector Quantization (VQ), implemented through clustering. The database is clustered and the most \"promising\" clusters are retrieved. Huge gains over VA-File, tree-based indexes and sequential scan have been observed. Also note the parallels between clustering and LSH.\n\nThere are numerous variants of the NNS problem and the two most well-known are the \"k\"-nearest neighbor search and the ε-approximate nearest neighbor search.\n\n\"k\"-nearest neighbor search identifies the top \"k\" nearest neighbors to the query. This technique is commonly used in predictive analytics to estimate or classify a point based on the consensus of its neighbors. \"k\"-nearest neighbor graphs are graphs in which every point is connected to its \"k\" nearest neighbors.\n\nIn some applications it may be acceptable to retrieve a \"good guess\" of the nearest neighbor. In those cases, we can use an algorithm which doesn't guarantee to return the actual nearest neighbor in every case, in return for improved speed or memory savings. Often such an algorithm will find the nearest neighbor in a majority of cases, but this depends strongly on the dataset being queried.\n\nAlgorithms that support the approximate nearest neighbor search include locality-sensitive hashing, best bin first and balanced box-decomposition tree based search.\n\nNearest neighbor distance ratio do not apply the threshold on the direct distance from the original point to the challenger neighbor but on a ratio of it depending on the distance to the previous neighbor. It is used in CBIR to retrieve pictures through a \"query by example\" using the similarity between local features. More generally it is involved in several matching problems.\n\nFixed-radius near neighbors is the problem where one wants to efficiently find all points given in Euclidean space within a given fixed distance from a specified point. The data structure should work on a distance which is fixed however the query point is arbitrary.\n\nFor some applications (e.g. entropy estimation), we may have \"N\" data-points and wish to know which is the nearest neighbor \"for every one of those N points\". This could of course be achieved by running a nearest-neighbor search once for every point, but an improved strategy would be an algorithm that exploits the information redundancy between these \"N\" queries to produce a more efficient search. As a simple example: when we find the distance from point \"X\" to point \"Y\", that also tells us the distance from point \"Y\" to point \"X\", so the same calculation can be reused in two different queries.\n\nGiven a fixed dimension, a semi-definite positive norm (thereby including every L norm), and \"n\" points in this space, the nearest neighbour of every point can be found in \"O\"(\"n\" log \"n\") time and the \"m\" nearest neighbours of every point can be found in \"O\"(\"mn\" log \"n\") time.\n\n\n\n\n"}
{"id": "33572770", "url": "https://en.wikipedia.org/wiki?curid=33572770", "title": "Network theory in risk assessment", "text": "Network theory in risk assessment\n\nA network is an abstract structure capturing only the basics of connection patterns and little else. Because it is a generalized pattern, tools developed for analyzing, modeling and understanding networks can theoretically be implemented across disciplines. As long as a system can be represented by a network, there is an extensive set of tools – mathematical, computational, and statistical – that are well-developed and if understood can be applied to the analysis of the system of interest. \nTools that are currently employed in risk assessment are often sufficient, but model complexity and limitations of computational power can tether risk assessors to involve more causal connections and account for more Black Swan event outcomes. By applying network theory tools to risk assessment, computational limitations may be overcome and result in broader coverage of events with a narrowed range of uncertainties.\n\nDecision-making processes are not incorporated into routine risk assessments; however, they play a critical role in such processes. It is therefore very important for risk assessors to minimize confirmation bias by carrying out their analysis and publishing their results with minimal involvement of external factors such as politics, media, and advocates. In reality, however, it is nearly impossible to break the iron triangle among politicians, scientists (in this case, risk assessors), and advocates and media. Risk assessors need to be sensitive to the difference between risk studies and risk perceptions. One way to bring the two closer is to provide decision-makers with data they can easily rely on and understand. Employing networks in the risk analysis process can visualize causal relationships and identify heavily-weighted or important contributors to the probability of the critical event.\n\nA \"bow-tie\" diagram, cause-and-effect diagram, Bayesian network (a \"directed acyclic\" network) and fault trees are few examples of how network theories can be applied in risk assessment.\n\nIn epidemiology risk assessments (Figure 7 and 9), once a network model was constructed, we can visually see then quantify and evaluate the potential exposure or infection risk of people related to the well-connected patients (Patient 1, 6, 35, 130 and 127 in Figure 7) or high-traffic places (Hotel M in Figure 9). In ecological risk assessments (Figure 8), through a network model we can identify the keystone species and determine how widespread the impacts will extend from the potential hazards being investigated.\n\nRisk assessment is a method for dealing with uncertainty. For it to be beneficial to the overall risk management and decision making process, it must be able to capture extreme and catastrophic events. Risk assessment involves two parts: risk analysis and risk evaluation, although the term “\"risk assessment\"” can be seen used indistinguishable with “\"risk analysis\"”. In general, risk assessment can be divided into these steps:\n\n\nNaturally, the number of steps required varies with each assessment. It depends on the scope of the analysis and the complexity of the study object. Because these is always varies degrees of uncertainty involved in any risk analysis process, sensitivity and uncertainty analysis are usually carried out to mitigate the level of uncertainty and therefore improve the overall risk assessment result.\n\nA network is a simplified representation that reduces a system to an abstract structure. Simply put, it is a collection of points linked together by lines. Each point is known as a “vertex\"” (multiple: “\"vertices\"”) or “\"nodes\"”, and each line as “\"edges\"” or “\"links\"”. Network modeling and studying have already been applied in many areas, including computer, physical, biological, ecological, logistical and social science. Through the studying of these models, we gain insights into the nature of individual components (i.e. vertices), connections or interactions between those components (i.e. edges), as well as the pattern of connections (i.e. network).\n\nUndoubtedly, modifications of the structure (or pattern) of any given network can have a big effect on the behavior of the system it depicts. For example, connections in a social network affect how people communicate, exchange news, travel, and, less obviously, spread diseases. In order to gain better understanding of how each of these systems functions, some knowledge of the structure of the network is necessary.\n\nSmall-World Effect \n\nDegree, Hubs, and Paths\n\nCentrality\n\nComponents\n\nDirected Networks \n\nWeighted Network\n\nTrees\n\nEarly social network studies can be traced back to the end of the nineteenth century. However well-documented studies and foundation of this field are usually attributed to a psychiatrist named Jacob Moreno. He published a book entitled \"Who Whall Survive?\" in 1934 which laid out the foundation for \"sociometry\" (later known as \"social network analysis\").\n\nAnother famous contributor to the early development of social network analysis is a perimental psychologist known as Stanley Milgram. His \"small-world\" experiments gave rise to concepts such as six degrees of separation and well-connected acquaintances (also known as \"sociometric superstars\"). This experiment was recently repeated by Dodds \"et al.\" by means of email messages, and the basic results were similar to Milgram's. The estimated true average path length (that is, the number of edges the email message has to pass from one unique individual to the intended targets in different countries) for the experiment was around five to seven, which is not much deviated from the original six degree of separation.\n\nA food web, or food chain, is an example of directed network which describes the prey-predator relationship in a given ecosystem. Vertices in this type of network represent species, and the edges the prey-predator relationship. A collection of species may be represented by a single vertex if all members in that collection prey upon and are preyed on by the same organisms. A food web is often acyclic, with few exceptions such as adults preys on juveniles and parasitism.\n\nEpidemiology is closely related to social network. Contagious diseases can spread through connection networks such as work space, transportation, intimate body contacts and water system (see Figure 7 and 9). Though it only exists virtually, a computer viruses spread across internet networks are not much different from their physical counterparts. Therefore, understanding each of these network patterns can no doubt aid us in more precise prediction of the outcomes of epidemics and preparing better disease prevention protocols.\n\nThe simplest model of infection is presented as a \"SI\" (\"susceptible - infected\") model. Most diseases, however, do not behave in such simple manner. Therefore, many modifications to this model were made such as the \"SIR\" (\"susceptible – infected – recovered\"), the \"SIS\" (the second \"S\" denotes \"reinfection\") and \"SIRS\" models. The idea of latency is taken into accounts in models such as \"SEIR\" (where \"E\" stands for \"exposed\"). The SIR model is also known as the Reed-Frost model.\n\nTo factor these into an outbreak network model, one must consider the degree distributions of vertices in the giant component of the network (outbreaks in small components are isolation and die out quickly, which does not allow the outbreaks to become epidemics). Theoretically, weighted network can provide more accurate information on exposure probability of vertices but more proofs are needed. Pastor-Satorras \"et al.\" pioneered much work in this area, which began with the simplest form (the \"SI\" model) and applied to networks drawn from the configuration model.\n\nThe biology of how an infection causes disease in an individual is complicated and is another type of disease pattern specialists are interested in (a process known as pathogenesis which involves immunology of the host and virulence factors of the pathogen).\n\n"}
{"id": "30666710", "url": "https://en.wikipedia.org/wiki?curid=30666710", "title": "Numerische Mathematik", "text": "Numerische Mathematik\n\nNumerische Mathematik is a peer-reviewed mathematics journal on numerical analysis. It was established in 1959 and is published by Springer Science+Business Media. The journal is indexed by \"Mathematical Reviews\" and Zentralblatt MATH. Its 2009 MCQ was 1.06, and its 2016 impact factor was 2.152.\n"}
{"id": "1441528", "url": "https://en.wikipedia.org/wiki?curid=1441528", "title": "Object theory", "text": "Object theory\n\nObject theory is a theory in philosophy and mathematical logic concerning objects and the statements that can be made about objects.\n\nIn some cases \"objects\" can be concretely thought of as symbols and strings of symbols, here illustrated by a string of four symbols \" ←←↑↓←→←↓\" as composed from the 4-symbol alphabet { ←, ↑, →, ↓ } . When they are \"known only through the relationships of the system [in which they appear], the system is [said to be] \"abstract\" ... what the objects are, in any respect other than how they fit into the structure, is left unspecified.\" (Kleene 1952:25) A further specification of the objects results in a model or representation of the abstract system, \"i.e. a system of objects which satisfy the relationships of the abstract system and have some further status as well\" (ibid).\n\nA system, in its general sense, is a collection of objects O = {o, o, ... o, ... } and (a specification of) the relationship \"r\" or relationships r, r, ... r between the objects: \n\nA model of this system would occur when we assign, for example the familiar natural numbers { 0, 1, 2, 3 }, to the symbols { ←, ↑, →, ↓ }, i.e. in this manner: → = 0, ↑ = 1, ← = 2, ↓ = 3 . Here, the symbol ∫ indicates the \"successor function\" (often written as an apostrophe ' to distinguish it from +) operating on a collection of only 4 objects, thus 0' = 1, 1' = 2, 2' = 3, 3' = 0. \n\nThe following is an example of the genetic or constructive method of making objects in a system, the other being the axiomatic or postulational method. Kleene states that a genetic method is intended to \"generate\" all the objects of the system and thereby \"determine the abstract structure of the system completely\" and uniquely (and thus define the system categorically). If axioms rather than a genetic method is used, such axiom-sets are said to be categorical.\n\nUnlike the ∫ example above, the following creates an unbounded number of objects. The fact that O is a set, and □ is an element of O, and ■ is an operation, must be specified at the outset; this is being done in the language of the metatheory (see below):\n\nThe object ■□ demonstrates the use of \"abbreviation\", a way to simplify the denoting of objects, and consequently discussions about them, once they have been created \"officially\". Done correctly the definition would proceed as follows:\n\nKurt Gödel 1931 virtually constructed the entire proof of his incompleteness theorems (actually he proved Theorem IV and sketched a proof of Theorem XI) by use of this tactic, proceeding from his axioms using substitution, concatenation and deduction of \"modus ponens\" to produce a collection of 45 \"definitions\" (derivations or theorems more accurately) from the axioms.\n\nA more familiar tactic is perhaps the design of subroutines that are given names, e.g. in Excel the subroutine \" =INT(A1)\" that returns to the cell where it is typed (e.g. cell B1) the integer it finds in cell A1.\n\nA model of the above example is a left-ended Post–Turing machine tape with its fixed \"head\" located on the left-end square; the system's relation is equivalent to: \"To the left end, tack on a new square □, right-shift the tape, then print ■ on the new square\". Another model is the natural numbers as created by the \"successor\" function. Because the objects in the two systems e.g. ( □, ■□, ■■□, ■■■□ ... ) and (0, 0′, 0′′, 0′′′, ...) can be put into a 1-1 correspondence, the systems are said to be (simply) isomorphic (meaning \"same shape\"). Yet another isomorphic model is the little sequence of instructions for a counter machine e.g. \"Do the following in sequence: (1) Dig a hole. (2) Into the hole, throw a pebble. (3) Go to step 2.\"\n\nAs long as their objects can be placed in one-to-one correspondence (\"while preserving the relationships\") models can be considered \"equivalent\" no matter how their objects are generated (e.g. genetically or axiomatically):\n\nAn alert reader may have noticed that writing symbols □, ■□, ■■□, ■■■□, etc. by concatenating a marked square, i.e. ■, to an existing string is different from writing the completed symbols one after another on a Turing-machine tape. Another entirely possible scenario would be to generate the symbol-strings one after another on different sections of tape e.g. after three symbols: ■■■□■■□■□□. The proof that these two possibilities are different is easy: they require different \"programs\". But in a sense both versions create the same objects; in the second case the objects are preserved on the tape. In the same way, if a person were to write 0, then erase it, write 1 in the same place, then erase it, write 2, erase it, ad infinitum, the person is generating the same objects as if they were writing down 0 1 2 3 ... writing one symbol after another to the right on the paper. \n\nOnce the step has been taken to write down the symbols 3 2 1 0 one after another on a piece of paper (writing the new symbol on the left this time), or writing ∫∫∫※∫∫※∫※※ in a similar manner, then putting them in 1-1 correspondence with the Turing-tape symbols seems obvious. Digging holes one after the other, starting with a hole at \"the origin\", then a hole to its left with one pebble in it, then a hole to \"its\" left with two pebbles in it, ad infinitum, raises practical questions, but in the abstract it too can be seen to be conducive to the same 1-1 correspondence. \n\nHowever, nothing in particular in the definition of genetic versus axiomatic methods clears this up—these are issues to be discussed in the metatheory. The mathematician or scientist is to be held responsible for sloppy specifications. Breger cautions that axiomatic methods are susceptible to tacit knowledge, in particular, the sort that involves \"know-how of a human being\" (Breger 2000:227).\n\nIn general, in mathematics a formal system or \"formal theory\" consists of \"objects\" in a structure: \n\nA metatheory exists outside the formalized object theory—the meaningless symbols and relations and (well-formed-) strings of symbols. The metatheory comments on (describes, interprets, illustrates) these meaningless objects using \"intuitive\" notions and \"ordinary language\". Like the object theory, the metatheory should be disciplined, perhaps even quasi-formal itself, but in general the interpretations of objects and rules are intuitive rather than formal. Kleene requires that the methods of a metatheory (at least for the purposes of metamathematics) be finite, conceivable, and performable; these methods cannot appeal to the completed infinite. \"Proofs of existence shall give, at least implicitly, a method for constructing the object which is being proved to exist.\" (p. 64)\n\nKleene summarizes this as follows: \"In the full picture there will be three separate and distinct \"theories\"\"\n\nHe goes on to say that object theory (b) is not a \"theory\" in the conventional sense, but rather is \"a system of symbols and of objects built from symbols (described from (c))\". \n\nIf a collection of objects (symbols and symbol-sequences) is to be considered \"well-formed\", an algorithm must exist to determine, by halting with a \"yes\" or \"no\" answer, whether or not the object is well-formed (in mathematics a wff abbreviates well-formed formula). This algorithm, in the extreme, might require (or be) a Turing machine or Turing-equivalent machine that \"parses\" the symbol-string as presented as \"data\" on its tape; before a universal Turing machine can execute an instruction on its tape, it must parse the symbols to determine the exact nature of the instruction and/or datum encoded there. In simpler cases a finite state machine or a pushdown automaton can do the job. Enderton describes the use of \"trees\" to determine whether or not a logic formula (in particular a string of symbols with parentheses) is well formed. Alonzo Church 1934 describes the construction of \"formulas\" (again: sequences of symbols) as written in his λ-calculus by use of a recursive description of how to start a formula and then build on the starting-symbol using concatenation and substitution.\n\nExample: Church specified his λ-calculus as follows (the following is simplified version leaving out notions of free- and bound-variable). This example shows how an object theory begins with a specification of an \"object system\" of symbols and relations (in particular by use of concatenation of symbols):\n"}
{"id": "56352872", "url": "https://en.wikipedia.org/wiki?curid=56352872", "title": "Paul Jean Joseph Barbarin", "text": "Paul Jean Joseph Barbarin\n\nPaul Jean Joseph Barbarin (20 October 1855, Tarbes – 28 September 1931) was a French mathematician, specializing in geometry.\n\nBarbarin studied mathematics for a brief time at the École Polytechnique but changed, at the age of 19, to the École Normale Supérieure, where he studied mathematics under Briot, Bouquet, Tannery, and Darboux. After graduation, Barbarin became a professor of mathematics at the Lyceum of Nice and then at the School of St.-Cyr of the Lyceum of Toulon. In 1891 he became a professor at the Lyceum of Bordeaux, where he taught for many years. At the time of his death he was a professor at the École Spéciale des Travaux Publics in Paris.\n\nIn 1903 the Kazan Physical and Mathematical Society of Kazan State University awarded the Lobachevsky Prize to Hilbert but the Society cited Barbarin as the second choice among the nominees considered. When Hilbert received the Society's award, Henri Poincaré contributed a report on the work of Hilbert, and Professor Mansion of Ghent contributed a report on the work of Barbarin. In a 1904 article published in the journal \"Science\", G. B. Halsted gave an English summary of the two French reports.\n\nAthanase Papadopoulos edited and translated Lobachevsky's \"Pangéométrie ou Précis de géométrie fondée sur une théorie générale et rigoureuse des parallèles\" (\"Pangeometry\") and provided a footnote concerning Barbarin:\n\nBarbarin was an Invited Speaker of the ICM in 1928 in Bologna.\n\n\n"}
{"id": "666177", "url": "https://en.wikipedia.org/wiki?curid=666177", "title": "Picard–Lindelöf theorem", "text": "Picard–Lindelöf theorem\n\nIn mathematics – specifically, in differential equations – the Picard–Lindelöf theorem, Picard's existence theorem, Cauchy–Lipschitz theorem, or existence and uniqueness theorem gives a set of conditions under which an initial value problem has a unique solution.\n\nThe theorem is named after Émile Picard, Ernst Lindelöf, Rudolf Lipschitz and Augustin-Louis Cauchy.\n\nConsider the initial value problem\n\nSuppose is uniformly Lipschitz continuous in (meaning the Lipschitz constant can be taken independent of ) and continuous in , then for some value , there exists a unique solution to the initial value problem on the interval formula_2.\n\nThe proof relies on transforming the differential equation, and applying fixed-point theory. By integrating both sides, any function satisfying the differential equation must also satisfy the integral equation\n\nA simple proof of existence of the solution is obtained by successive approximations. In this context, the method is known as Picard iteration.\n\nSet\n\nand\n\nIt can then be shown, by using the Banach fixed point theorem, that the sequence of \"Picard iterates\" is convergent and that the limit is a solution to the problem. An application of Grönwall's lemma to , where and are two solutions, shows that , thus proving the global uniqueness (the local uniqueness is a consequence of the uniqueness of the Banach fixed point).\n\nLet formula_6 and formula_7, so formula_8 and formula_9\n\nformula_10\n\nformula_11\n\nformula_12\n\nand so on\n\nThis is calculating the series expansion of formula_13, an alternative method is to use Bernoulli's numbers\n\nThe idea behind the theorem is the following. A differential equation can possess a stationary point. For example, for the equation the stationary solution is , which is obtained for the initial condition . Beginning with another initial condition , the stationary solution is reached after an infinite time and therefore the uniqueness of solution is guaranteed. However, if the stationary solution is reached after a \"finite\" time, the uniqueness is violated. This happens for example for the equation , the solution corresponding to the initial condition can be either or\n\nOne can note that the function has an infinite slope at and therefore is not Lipschitz continuous. The Lipschitz continuity condition rules out this type of differential equation.\n\nLet\n\nwhere:\n\nThis is the compact cylinder where is defined. Let\n\nthis is, the maximum slope of the function in modulus. Finally, let \"L\" be the Lipschitz constant of with respect to the second variable.\n\nWe will proceed to apply Banach fixed point theorem using the metric on formula_18 induced by the uniform norm\n\nWe define an operator between two functional spaces of continuous functions, Picard's operator, as follows:\n\ndefined by:\n\nWe must show that this operator maps a complete non-empty metric space X into itself and also is a contraction mapping.\n\nWe first show that, given certain restrictions on formula_22, formula_23 takes formula_24 into itself in the space of continuous functions with uniform norm. Here, formula_24 is a closed ball in the space of continuous (and bounded) functions \"centered\" at the constant function formula_26. Hence we need to show that \n\nimplies\n\nwhere formula_29 is some number in formula_30 where the maximum is achieved. \nThe last step is true if we impose the requirement .\n\nNow let's try to prove that this operator is a contraction.\n\nGiven two functions formula_31, in order to apply the Banach fixed point theorem we want\n\nfor some \"q\" < 1. So let \"t\" be such that\n\nthen using the definition of Γ\n\nThis is a contraction if .\n\nWe have established that the Picard's operator is a contraction on the Banach spaces with the metric induced by the uniform norm. This allows us to apply the Banach fixed point theorem to conclude that the operator has a unique fixed point. In particular, there is a unique function\n\nsuch that . This function is the unique solution of the initial value problem, valid on the interval \"I\" where \"a\" satisfies the condition \n\nNevertheless, there is a corollary of the Banach fixed point theorem that states that if an operator \"T\" is a contraction for some \"n\" in N then \"T\" has a unique fixed point. We will try to apply this theorem to the Picard's operator. But before doing that, let us recall a lemma that will be very useful to apply the aforementioned corollary.\nLemma:   formula_37\n\"Proof.\" We will prove this by induction. For the base of the induction we have already seen this, so suppose the inequality holds for , then we have: \n\nTherefore, taking into account this inequality we can assure that for some \"m\" large enough, \n\nand hence Γ will be a contraction. So by the previous corollary Γ will have a unique fixed point. So, finally, we have been able to optimize the interval of the solution by taking \n\nThe importance of this result is that the interval of definition of the solution does eventually not depend on the Lipschitz constant of the field, but essentially depends on the interval of definition of the field and its maximum absolute value of it.\n\nThe Picard–Lindelöf theorem shows that the solution exists and that it is unique. The Peano existence theorem shows only existence, not uniqueness, but it assumes only that is continuous in , instead of Lipschitz continuous. For example, the right-hand side of the equation with initial condition is continuous but not Lipschitz continuous. Indeed, rather than being unique, this equation has three families of solutions:\n\nEven more general is Carathéodory's existence theorem, which proves existence (in a more general sense) under weaker conditions on . Although these conditions are only sufficient, there also exist necessary and sufficient conditions for the solution of an initial value problem to be unique, such as Okamura's theorem.\n\n\n\n"}
{"id": "6022750", "url": "https://en.wikipedia.org/wiki?curid=6022750", "title": "Pieter Hendrik Schoute", "text": "Pieter Hendrik Schoute\n\nPieter Hendrik Schoute (21 January 1846, Wormerveer – 18 April 1923, Groningen) was a Dutch mathematician known for his work on regular polytopes and Euclidean geometry.\n\nIn 1886, he became member of the Royal Netherlands Academy of Arts and Sciences.\n\n"}
{"id": "29309766", "url": "https://en.wikipedia.org/wiki?curid=29309766", "title": "RegularChains", "text": "RegularChains\n\nThe RegularChains package in the computer algebra software package Maple is a collection of commands for solving systems of polynomial equations, inequations and inequalities symbolically. This package also allows the user to manipulate and study the solutions of such systems. \n\nThe main two commands are 'Triangularize' and 'RealTriangularize'. Each of them computes from a system of polynomials formula_1 a set of simpler systems formula_2 such that a point is a solution of formula_1 if and only if it is a solution of one of the systems formula_2. Each of these simpler systems is called a regular chain in the case of Triangularize and a regular semi-algebraic system in the case of RealTriangularize. In both cases, each of these simpler systems has a triangular shape and remarkable properties. For this reason, the set formula_2 is called a triangular decomposition of the system formula_1.\n\nIn addition to its main functions 'Triangularize' and 'RealTriangularize', the package has six subpackages and other commands.\n\nThe 'MatrixTools' subpackage provides commands for solving linear systems of equations modulo the saturated ideal of a regular chain. Among other operations are computations of matrix inverses and lower echelon forms. These commands are considered here in a non-standard context. Indeed, the coefficients of these matrices are polynomials and the computations are performed modulo (the saturated ideal of) a regular chain. Since this latter is not required to be a prime ideal, the commands of this subpackage allows you to do linear algebra computations over non-integral domains.\n\nThe 'ConstructibleSetTools' subpackage provides a large set of commands for manipulating constructible sets. Constructible sets are the fundamental objects of Algebraic Geometry, and they play there the role that ideals play in Polynomial Algebra. In broad terms, a constructible set is the solution set of a system of polynomial equations and inequations. Constructible sets appear naturally in many questions, from high-school problems to advanced research topics.\n\nThe 'SemiAlgebraicSetTools' subpackage contains a collection of commands for isolating and counting real roots of zero-dimensional semi-algebraic systems or regular chains (that is regular chains with a finite number of complex solutions). It also offers various commands for studying the real solutions of polynomial systems of positive dimension or with parameters. In particular, commands for real root classification, cylindrical algebraic decomposition and partial cylindrical algebraic decomposition sampling are available. Several inspection functions on semi-algebraic systems and their solution sets (namely, semi-algebraic sets) are also provided. They are intended to support the command 'RealRootClassification', 'RealTriangularize', 'LazyRealTriangularize' and 'SamplePoints'.\n\nThe 'ParametricSystemTools' subpackage provides commands for solving systems of equations that depend on parameters. Given a parametric polynomial system formula_7, this subpackage can be used to answer questions such as: for which values of the parameters does formula_7 have solutions? finitely many solutions? formula_9 real solutions, for a given formula_9?\n\nThe 'ChainTools' subpackage provides advanced operations on regular chains. Most of these commands allow you to inspect, construct and transform regular chains, or to check the properties of a polynomial with respect to a regular chain. Some commands operate transformations on a set of regular chains. They can be used to analyze the results computed by the command 'Triangularize'.\n\nThe 'FastArithmeticTools' subpackage contains a collection of commands for computing with regular chains in prime characteristic using asymptotically fast algorithms. Most of the underlying polynomial arithmetic is performed at C level and relies on (multi-dimensional) Fast Fourier Transform (FFT). This imposes some constraints on the characteristic. One of the main purposes of this subpackage is to offer efficient basic routines in order to suppor the implementation of modular algorithms for computing with regular chains and algebraic numbers.\n\n\n"}
{"id": "18066908", "url": "https://en.wikipedia.org/wiki?curid=18066908", "title": "S. B. Rao", "text": "S. B. Rao\n\nSiddani Bhaskara Rao a Graph theorist is a Professor Emeritus and director of the Indian Statistical Institute (ISI), Calcutta. Rao is the first director of the CR Rao Advanced Institute of Mathematics, Statistics and Computer Science. S. B. Rao is known for his work on line graphs, frequency partitions and degree sequences.\n\nRao hails from Andhra Pradesh and completed his M.A. (1965) in mathematics from Andhra University. He received his Ph.D. (1971) from the Indian Statistical Institute, Calcutta under the supervision of renowned Statistician CR Rao. After completing his Ph.D., he moved to the University of Mumbai to work with S. S. Shrikhande. At the same time, he visited King's College, Aberdeen to work with Crispin St. J. A. Nash-Williams. From the University of Mumbai, Rao went back to the Indian Statistical Institute (ISI). While at ISI, he visited Ohio State University. Rao has guided students for their Ph.D.s in graph theory. He was the director of ISI Calcutta from 1995 to 2000. After retirement from ISI, he went to University of Hyderabad to work as the first director of the C. R. Rao Advanced Institute of Mathematics, Statistics and Computer Science, Hyderabad.\n"}
{"id": "19355609", "url": "https://en.wikipedia.org/wiki?curid=19355609", "title": "Senior Wrangler (University of Cambridge)", "text": "Senior Wrangler (University of Cambridge)\n\nThe Senior Wrangler is the top mathematics undergraduate at Cambridge University in England, a position which has been described as \"the greatest intellectual achievement attainable in Britain.\"\n\nSpecifically, it is the person who achieves the highest overall mark among the Wranglers – the students at Cambridge who gain first-class degrees in mathematics. The Cambridge undergraduate mathematics course, or Mathematical Tripos, is famously difficult.\n\nMany Senior Wranglers have become world-leading figures in mathematics, physics, and other fields. They include George Airy, John Herschel, Arthur Cayley, James Inman, George Stokes, Isaac Todhunter, Morris Pell, Lord Rayleigh, Arthur Eddington, J. E. Littlewood, Jayant Narlikar, Frank Ramsey, Donald Coxeter, Jacob Bronowski, Lee Hsien Loong, Kevin Buzzard, Christopher Budd, Ben Green, and John Polkinghorne.\n\nSenior Wranglers were once fêted with torchlit processions and took pride of place in the University's graduation ceremony. Years in Cambridge were often remembered by who had been Senior Wrangler that year.\n\nThe annual ceremony in which the Senior Wrangler becomes known was first held in the 18th century. Standing on the balcony of the University's Senate House, the examiner reads out the class results for mathematics, and printed copies of the results are then thrown to the audience below. The examiner no longer announces the students' exact rankings, but they still identify the Senior Wrangler, nowadays by tipping their academic hat when reading out the person's name.\n\nThe difficulty of the examinations is illustrated by the identities of some of those who have performed well, but less well than the Senior Wrangler.\n\nThose who have achieved second place, known as Second Wranglers, include Alfred Marshall, James Clerk Maxwell, J. J. Thomson, Lord Kelvin, William Clifford, and William Whewell.\n\nThose who have finished between third and 12th include Karl Pearson and William Henry Bragg (third), George Green and G. H. Hardy (fourth), Adam Sedgwick (fifth), John Venn (sixth), Bertrand Russell and Nevil Maskelyne (seventh), Thomas Malthus (ninth), and John Maynard Keynes (12th).\n\nBetween 1748 and 1909, the University publicly announced the ranking, which was then reported in newspapers such as \"The Times\". The examination was considered to be by far the most important in Britain and the Empire. The prestige of being a high Wrangler was great; the respect accorded to the Senior Wrangler was immense. Andrew Warwick, author of \"Masters of Theory\", describes the term 'Senior Wrangler' as \"synonymous with academic supremacy\".\n\nSince 1910, successful students in the examinations have been told their rankings privately, and not all Senior Wranglers have become publicly known as such. In recent years, the custom of discretion regarding ranking has progressively vanished, and all Senior Wranglers since 2010 have announced their identity publicly.\n\nThe youngest person to be Senior Wrangler is probably Arran Fernandez, who came top in 2013, aged 18 years and 0 months. The previous youngest was probably James Wilkinson in 1939, aged 19 years and 9 months. The youngest up to 1909 were Alfred Flux in 1887, aged 20 years and 2 months and Peter Tait in 1852, aged 20 years and 8 months.\n\nTwo individuals have placed first without becoming known as Senior Wrangler. One was the student Philippa Fawcett in 1890. At that time, although the University allowed women to take the examinations, it did not allow them to be members of the University, nor to receive degrees. Therefore they could not be known as 'Wranglers', and were merely told how they had performed compared to the male candidates, for example, \"equal to the Third Wrangler\", or \"between the Seventh and Eighth Wranglers\". Having gained the highest mark, Fawcett was declared to have finished \"above the Senior Wrangler\".\n\nThe other was the mathematics professor George Pólya. As he had contributed to reforming the Tripos with the aim that an excellent performance would be less dependent on solving hard problems and more so on showing a broad mathematical understanding and knowledge, G.H. Hardy asked Pólya to sit the examinations himself, unofficially, during his stay in England in 1924–5. Pólya did so, and to Hardy's surprise, received the highest mark, an achievement which, had he been a student, would have made him the Senior Wrangler.\n\nSenior Wrangler's Walk is a path in Cambridge, the walk to and along which was considered to be sufficient constitutional exercise for a student aspiring to become the Senior Wrangler. The route was shorter than other walks, such as Wranglers' Walk and the Grantchester Grind, undertaken by undergraduates whose aspirations were lower.\n\nSenior Wrangler sauce is a Cambridge term for brandy butter, a type of hard sauce made from brandy, butter, and sugar, traditionally served in Britain with Christmas pudding and warm mince pies.\n\nSenior Wrangler is also the name of a solitaire card game, alternatively known as Mathematics and Double Calculation, played with two decks of cards and involving elementary modular arithmetic.\n\nFictional Senior Wranglers appearing in novels include Roger Hamley, a character in Elizabeth Gaskell's \"Wives and Daughters\", and Tom Jericho, the cryptanalyst in Robert Harris's novel \"Enigma\", who is described as having been Senior Wrangler in 1938.\n\nIn George Bernard Shaw's play \"Mrs. Warren's Profession\", the title character's daughter Vivie is praised for \"tieing with the third wrangler,\" and she comments that \"the mathematical tripos\" means \"grind, grind, grind for six to eight hours a day at mathematics, and nothing but mathematics.\"\n\nIn Ford Madox Ford's \"Parade's End\", the character Christopher Tietjens is described as having settled deliberately for only being Second Wrangler, in order to avoid the weight of expectation that the title would create.\n\nIn his Discworld series of novels, Terry Pratchett has a character called the Senior Wrangler, a faculty member at the Unseen University, whose first name is Horace.\n\nThe compiler of crosswords for \"The Leader\" in the 1930s used 'Senior Wrangler' as a pseudonym.\n\nThe two most successful 19th-century coaches of Senior Wranglers were William Hopkins and Edward Routh. Hopkins, the 'Senior Wrangler Maker', who himself was the 7th Wrangler, coached 17 Senior Wranglers. Routh, who had himself been the Senior Wrangler, coached 27. Another, described by his student (and Senior Wrangler) J.E. Littlewood as “the last of the great coaches”, was another Senior Wrangler, Robert Alfred Herman.\n\nDuring 1748–1909, the top two colleges in terms of number of Senior Wranglers were Trinity and St John's with 56 and 54 respectively. Gonville and Caius was third with 13.\n\nSenior Wranglers since 1910 also include:\n\n\n"}
{"id": "873839", "url": "https://en.wikipedia.org/wiki?curid=873839", "title": "Series-parallel networks problem", "text": "Series-parallel networks problem\n\nIn combinatorial mathematics, the series-parallel networks problem asks for the number of series-parallel networks that can be formed using a given number of edges. The edges can be distinguishable or indistinguishable.\n\nWhen the edges are indistinguishable, we look for the number of topologically different networks on \"n\" edges.\n\nThe idea is to break-down the problem by classifying the networks as essentially series and essentially parallel networks.\n\n\nBy the duality of networks, it can be proved that the number of essentially series networks is equal to the number of essentially parallel networks. Thus for all \"n\" > 1, the number of networks in \"n\" edges is twice the number of essentially series networks. For \"n\" = 1, the number of networks is 1.\n\nDefine\n\nThen\n\nformula_2 can be found out by enumerating the partitions of formula_5.\n\nConsider a partition, formula_6 of \"n\":\n\nConsider the essentially series networks whose components correspond to the partition above. That is the number of components with \"i\" edges is formula_8. The number of such networks can be computed as\n\nHence\n\nwhere the summation is over all partitions, formula_8 of \"n\" except for the trivial partition consisting of only \"n\".\n\nThis gives a recurrence for computing formula_2. Now formula_1 can be computed as above.\n\n[\"TODO: Generating function for formula_1 and formula_2 are explained in the external links.\"]\n\n"}
{"id": "552466", "url": "https://en.wikipedia.org/wiki?curid=552466", "title": "System identification", "text": "System identification\n\nThe field of system identification uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models as well as model reduction.\n\nA dynamical mathematical model in this context is a mathematical description of the dynamic behavior of a system or process in either the time or frequency domain. Examples include:\n\n\nOne of the many possible applications of system identification is in control systems. For example, it is the basis for modern data-driven control systems, in which concepts of system identification are integrated into the controller design, and lay the foundations for formal controller optimality proofs.\n\nSystem identification techniques can utilize both input and output data (e.g. eigensystem realization algorithm) or can include only the output data (e.g. frequency domain decomposition). Typically an input-output technique would be more accurate, but the input data is not always available.\n\nThe quality of system identification depends on the quality of the inputs, which are under the control of the systems engineer. Therefore, systems engineers have long used the principles of the design of experiments. In recent decades, engineers have increasingly used the theory of optimal experimental design to specify inputs that yield maximally precise estimators.\n\nOne could build a so-called white-box model based on first principles, e.g. a model for a physical process from the Newton equations, but in many cases such models will be overly complex and possibly even impossible to obtain in reasonable time due to the complex nature of many systems and processes.\n\nA much more common approach is therefore to start from measurements of the behavior of the system and the external influences (inputs to the system) and try to determine a mathematical relation between them without going into the details of what is actually happening inside the system. This approach is called system identification. Two types of models are common in the field of system identification:\n\n\n\nIn the context of nonlinear system identification Jin et al. describe greybox modeling by assuming a model structure a priori and then estimating the model parameters. Parameter estimation is relatively easy if the model form is known but this is rarely the case. Alternatively the structure or model terms for both linear and highly complex nonlinear models can be identified using NARMAX methods. This approach is completely flexible and can be used with grey box models where the algorithms are primed with the known terms, or with completely black box models where the model terms are selected as part of the identification procedure. Another advantage of this approach is that the algorithms will just select linear terms if the system under study is linear, and nonlinear terms if the system is nonlinear, which allows a great deal of flexibility in the identification.\n\nIn control systems applications, the objective of engineers is to obtain a good performance of the closed-loop system, which is the one comprising the physical system, the feedback loop and the controller. This performance is typically achieved by designing the control law relying on a model of the system, which needs to be identified starting from experimental data. If the model identification procedure is aimed at control purposes, what really matters is not to obtain the best possible model that fits the data, as in the classical system identification approach, but to obtain a model satisfying enough for the closed-loop performance. This more recent approach is called identification for control, or I4C in short.\n\nThe idea behind I4C can be better understood by considering the following simple example. Consider a system with \"true\" transfer function formula_1:\nand an identified model formula_3:\nFrom a classical system identification perspective, formula_3 is \"not\", in general, a \"good\" model for formula_1. In fact, modulus and phase of formula_3 are different from those of formula_1 at low frequency. What is more, while formula_1 is an asymptotically stable system, formula_3 is a simply stable system. However, formula_3 may still be a model good enough for control purposes. In fact, if one wants to apply a purely proportional negative feedback controller with high gain formula_12, the closed-loop transfer function from the reference to the output is, for formula_1\nand for formula_3\nSince formula_12 is very large, one has that formula_18. Thus, the two closed-loop transfer functions are indistinguishable. In conclusion, formula_3 is a \"perfectly acceptable\" identified model for the \"true\" system if such feedback control law has to be applied.\n\nIn conclusion, whether or not a model is \"appropriate\" for control design depends not only on the plant/model mismatch, but also on the controller that will be implemented. As such, in the I4C framework, given a control performance objective, the control engineer has to design the identification phase in such a way that the performance achieved by the model-based controller on the \"true\" system is as high as possible.\n\nSometimes, it is even convenient to design a controller without explicitly identifying a model of the system, but directly working on experimental data. This is the case of \"direct\" data-driven control systems.\n\n\n"}
{"id": "47331006", "url": "https://en.wikipedia.org/wiki?curid=47331006", "title": "Translation of axes", "text": "Translation of axes\n\nIn mathematics, a translation of axes in two dimensions is a mapping from an \"xy\"-Cartesian coordinate system to an \"x'y<nowiki>'</nowiki>\"-Cartesian coordinate system in which the \"x<nowiki>'</nowiki>\" axis is parallel to the \"x\" axis and \"k\" units away, and the \"y<nowiki>'</nowiki>\" axis is parallel to the \"y\" axis and \"h\" units away. This means that the origin \"O<nowiki>'</nowiki>\" of the new coordinate system has coordinates (\"h\", \"k\") in the original system. The positive \"x<nowiki>'</nowiki>\" and \"y<nowiki>'</nowiki>\" directions are taken to be the same as the positive \"x\" and \"y\" directions. A point \"P\" has coordinates (\"x\", \"y\") with respect to the original system and coordinates (\"x<nowiki>'</nowiki>\", \"y<nowiki>'</nowiki>\") with respect to the new system, where\n\nor equivalently\n\nIn the new coordinate system, the point \"P\" will appear to have been translated in the opposite direction. For example, if the \"xy\"-system is translated a distance \"h\" to the right and a distance \"k\" upward, then \"P\" will appear to have been translated a distance \"h\" to the left and a distance \"k\" downward in the \"x'y<nowiki>'</nowiki>\"-system . A translation of axes in more than two dimensions is defined similarly. A translation of axes is a rigid transformation, but not a linear map. (See Affine transformation.)\n\nCoordinate systems are essential for studying the equations of curves using the methods of analytic geometry. To use the method of coordinate geometry, the axes are placed at a convenient position with respect to the curve under consideration. For example, to study the equations of ellipses and hyperbolas, the foci are usually located on one of the axes and are situated symmetrically with respect to the origin. If the curve (hyperbola, parabola, ellipse, etc.) is \"not\" situated conveniently with respect to the axes, the coordinate system should be changed to place the curve at a convenient and familiar location and orientation. The process of making this change is called a transformation of coordinates.\n\nThe solutions to many problems can be simplified by translating the coordinate axes to obtain new axes parallel to the original ones.\n\nThrough a change of coordinates, the equation of a conic section can be put into a standard form, which is usually easier to work with. For the most general equation of the second degree, it is always possible to perform a rotation of axes in such a way that in the new system the equation takes the form\n\nthat is, there is no \"xy\" term. Next, a translation of axes can reduce an equation of the form () to an equation of the same form but with new variables (\"x<nowiki>'</nowiki>\", \"y<nowiki>'</nowiki>\") as coordinates, and with \"D\" and \"E\" both equal to zero (with certain exceptions—for example, parabolas). The principal tool in this process is \"completing the square.\" In the examples that follow, it is assumed that a rotation of axes has already been performed.\n\nGiven the equation\n\nby using a translation of axes, determine whether the locus of the equation is a parabola, ellipse, or hyperbola. Determine foci (or focus), vertices (or vertex), and eccentricity.\n\nSolution: To complete the square in \"x\" and \"y\", write the equation in the form\n\nComplete the squares and obtain\n\nDefine\n\nThat is, the translation in equations () is made with formula_8 The equation in the new coordinate system is\n\nDivide equation () by 225 to obtain\n\nwhich is recognizable as an ellipse with formula_10 In the \"x'y<nowiki>'</nowiki>\"-system, we have: center formula_11; vertices formula_12; foci formula_13\n\nIn the \"xy\"-system, use the relations formula_14 to obtain: center formula_15; vertices formula_16; foci formula_17; eccentricity formula_18\n\nFor an \"xyz\"-Cartesian coordinate system in three dimensions, suppose that a second Cartesian coordinate system is introduced, with axes \"x<nowiki>'</nowiki>\", \"y<nowiki>'</nowiki>\" and \"z<nowiki>'</nowiki>\" so located that the \"x<nowiki>'</nowiki>\" axis is parallel to the \"x\" axis and \"h\" units from it, the \"y<nowiki>'</nowiki>\" axis is parallel to the \"y\" axis and \"k\" units from it, and the \"z<nowiki>'</nowiki>\" axis is parallel to the \"z\" axis and \"l\" units from it. A point \"P\" in space will have coordinates in both systems. If its coordinates are (\"x\", \"y\", \"z\") in the original system and (\"x<nowiki>'</nowiki>\", \"y<nowiki>'</nowiki>\", \"z<nowiki>'</nowiki>\") in the second system, the equations\n\nhold. Equations () define a translation of axes in three dimensions where (\"h\", \"k\", \"l\") are the \"xyz\"-coordinates of the new origin. A translation of axes in any finite number of dimensions is defined similarly.\n\nIn three-space, the most general equation of the second degree in \"x\", \"y\" and \"z\" has the form\n\nwhere the quantities formula_19 are positive or negative numbers or zero. The points in space satisfying such an equation all lie on a surface. Any second-degree equation which does not reduce to a cylinder, plane, line, or point corresponds to a surface which is called quadric.\n\nAs in the case of plane analytic geometry, the method of translation of axes may be used to simplify second-degree equations, thereby making evident the nature of certain quadric surfaces. The principal tool in this process is \"completing the square.\"\n\nUse a translation of coordinates to identify the quadric surface\n\nSolution: Write the equation in the form\n\nComplete the square to obtain\n\nIntroduce the translation of coordinates\n\nThe equation of the surface takes the form\n\nwhich is recognizable as the equation of an ellipsoid.\n\n\n"}
{"id": "18717261", "url": "https://en.wikipedia.org/wiki?curid=18717261", "title": "Trigonometry", "text": "Trigonometry\n\nTrigonometry (from Greek \"trigōnon\", \"triangle\" and \"metron\", \"measure\") is a branch of mathematics that studies relationships involving lengths and angles of triangles. The field emerged in the Hellenistic world during the 3rd century BC from applications of geometry to astronomical studies.\n\nThe 3rd-century astronomers first noted that the lengths of the sides of a right-angle triangle and the angles between those sides have fixed relationships: that is, if at least the length of one side and the value of one angle is known, then all other angles and lengths can be determined algorithmically. These calculations soon came to be defined as the trigonometric functions and today are pervasive in both pure and applied mathematics: fundamental methods of analysis such as the Fourier transform, for example, or the wave equation, use trigonometric functions to understand cyclical phenomena across many applications in fields as diverse as physics, mechanical and electrical engineering, music and acoustics, astronomy, ecology, and biology. Trigonometry is also the foundation of surveying.\n\nTrigonometry is most simply associated with planar right-angle triangles (each of which is a two-dimensional triangle with one angle equal to 90 degrees). The applicability to non-right-angle triangles exists, but, since any non-right-angle triangle (on a flat plane) can be bisected to create two right-angle triangles, most problems can be reduced to calculations on right-angle triangles. Thus the majority of applications relate to right-angle triangles. One exception to this is spherical trigonometry, the study of triangles on spheres, surfaces of constant positive curvature, in elliptic geometry (a fundamental part of astronomy and navigation). Trigonometry on surfaces of negative curvature is part of hyperbolic geometry.\n\nTrigonometry basics are often taught in schools, either as a separate course or as a part of a precalculus course.\n\nSumerian astronomers studied angle measure, using a division of circles into 360 degrees. They, and later the Babylonians, studied the ratios of the sides of similar triangles and discovered some properties of these ratios but did not turn that into a systematic method for finding sides and angles of triangles. The ancient Nubians used a similar method.\n\nIn the 3rd century BC, Hellenistic mathematicians such as Euclid and Archimedes studied the properties of chords and inscribed angles in circles, and they proved theorems that are equivalent to modern trigonometric formulae, although they presented them geometrically rather than algebraically. In 140 BC, Hipparchus (from Nicaea, Asia Minor) gave the first tables of chords, analogous to modern tables of sine values, and used them to solve problems in trigonometry and spherical trigonometry. In the 2nd century AD, the Greco-Egyptian astronomer Ptolemy (from Alexandria, Egypt) constructed detailed trigonometric tables (Ptolemy's table of chords) in Book 1, chapter 11 of his \"Almagest\". Ptolemy used chord length to define his trigonometric functions, a minor difference from the sine convention we use today. (The value we call sin(θ) can be found by looking up the chord length for twice the angle of interest (2θ) in Ptolemy's table, and then dividing that value by two.) Centuries passed before more detailed tables were produced, and Ptolemy's treatise remained in use for performing trigonometric calculations in astronomy throughout the next 1200 years in the medieval Byzantine, Islamic, and, later, Western European worlds.\n\nThe modern sine convention is first attested in the \"Surya Siddhanta\", and its properties were further documented by the 5th century (AD) Indian mathematician and astronomer Aryabhata. These Greek and Indian works were translated and expanded by medieval Islamic mathematicians. By the 10th century, Islamic mathematicians were using all six trigonometric functions, had tabulated their values, and were applying them to problems in spherical geometry. At about the same time, Chinese mathematicians developed trigonometry independently, although it was not a major field of study for them. The Persian polymath Nasir al-Din al-Tusi has been described as the creator of trigonometry as a mathematical discipline in its own right. Knowledge of trigonometric functions and methods reached Western Europe via Latin translations of Ptolemy's Greek \"Almagest\" as well as the works of Persian and Arabic astronomers such as Al Battani and Nasir al-Din al-Tusi. One of the earliest works on trigonometry by a northern European mathematician is \"De Triangulis\" by the 15th century German mathematician Regiomontanus, who was encouraged to write, and provided with a copy of the \"Almagest\", by the Byzantine Greek scholar cardinal Basilios Bessarion with whom he lived for several years. At the same time, another translation of the \"Almagest\" from Greek into Latin was completed by the Cretan George of Trebizond. Trigonometry was still so little known in 16th-century northern Europe that Nicolaus Copernicus devoted two chapters of \"De revolutionibus orbium coelestium\" to explain its basic concepts.\n\nDriven by the demands of navigation and the growing need for accurate maps of large geographic areas, trigonometry grew into a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his \"Trigonometria\" in 1595. Gemma Frisius described for the first time the method of triangulation still used today in surveying. It was Leonhard Euler who fully incorporated complex numbers into trigonometry. The works of the Scottish mathematicians James Gregory in the 17th century and Colin Maclaurin in the 18th century were influential in the development of trigonometric series. Also in the 18th century, Brook Taylor defined the general Taylor series.\n\nIf one angle of a triangle is 90 degrees and one of the other angles is known, the third is thereby fixed, because the three angles of any triangle add up to 180 degrees. The two acute angles therefore add up to 90 degrees: they are complementary angles. The shape of a triangle is completely determined, except for similarity, by the angles. Once the angles are known, the ratios of the sides are determined, regardless of the overall size of the triangle. If the length of one of the sides is known, the other two are determined. These ratios are given by the following trigonometric functions of the known angle \"A\", where \"a\", \" b\" and \"c\" refer to the lengths of the sides in the accompanying figure:\n\n\nThe hypotenuse is the side opposite to the 90 degree angle in a right triangle; it is the longest side of the triangle and one of the two sides adjacent to angle \"A\". The adjacent leg is the other side that is adjacent to angle \"A\". The opposite side is the side that is opposite to angle \"A\". The terms perpendicular and base are sometimes used for the opposite and adjacent sides respectively.(see below under Mnemonics).\n\nThe reciprocals of these functions are named the cosecant (csc), secant (sec), and cotangent (cot), respectively:\n\nThe inverse functions are called the arcsine, arccosine, and arctangent, respectively. There are arithmetic relations between these functions, which are known as trigonometric identities. The cosine, cotangent, and cosecant are so named because they are respectively the sine, tangent, and secant of the complementary angle abbreviated to \"co-\".\n\nWith these functions, one can answer virtually all questions about arbitrary triangles by using the law of sines and the law of cosines. These laws can be used to compute the remaining angles and sides of any triangle as soon as two sides and their included angle or two angles and a side or three sides are known. These laws are useful in all branches of geometry, since every polygon may be described as a finite combination of triangles.\n\nThe above definitions only apply to angles between 0 and 90 degrees (0 and π/2 radians). Using the unit circle, one can extend them to all positive and negative arguments (see trigonometric function). The trigonometric functions are periodic, with a period of 360 degrees or 2π radians. That means their values repeat at those intervals. The tangent and cotangent functions also have a shorter period, of 180 degrees or π radians.\n\nThe trigonometric functions can be defined in other ways besides the geometrical definitions above, using tools from calculus and infinite series. With these definitions the trigonometric functions can be defined for complex numbers. The complex exponential function is particularly useful.\n\nSee Euler's and De Moivre's formulas.\n\nA common use of mnemonics is to remember facts and relationships in trigonometry. For example, the \"sine\", \"cosine\", and \"tangent\" ratios in a right triangle can be remembered by representing them and their corresponding sides as strings of letters. For instance, a mnemonic is SOH-CAH-TOA:\n\nOne way to remember the letters is to sound them out phonetically (i.e., \"SOH-CAH-TOA\", which is pronounced 'so-kə-toe-uh' ). Another method is to expand the letters into a sentence, such as \"Some Old Hippie Caught Another Hippie Trippin' On Acid\".\n\nTrigonometric functions were among the earliest uses for mathematical tables. Such tables were incorporated into mathematics textbooks and students were taught to look up values and how to interpolate between the values listed to get higher accuracy. Slide rules had special scales for trigonometric functions.\n\nToday, scientific calculators have buttons for calculating the main trigonometric functions (sin, cos, tan, and sometimes cis and their inverses). Most allow a choice of angle measurement methods: degrees, radians, and sometimes gradians. Most computer programming languages provide function libraries that include the trigonometric functions. The floating point unit hardware incorporated into the microprocessor chips used in most personal computers has built-in instructions for calculating trigonometric functions.\n\nThere is an enormous number of uses of trigonometry and trigonometric functions. For instance, the technique of triangulation is used in astronomy to measure the distance to nearby stars, in geography to measure distances between landmarks, and in satellite navigation systems. The sine and cosine functions are fundamental to the theory of periodic functions, such as those that describe sound and light waves.\n\nFields that use trigonometry or trigonometric functions include astronomy (especially for locating apparent positions of celestial objects, in which spherical trigonometry is essential) and hence navigation (on the oceans, in aircraft, and in space), music theory, audio synthesis, acoustics, optics, electronics, biology, medical imaging (CT scans and ultrasound), pharmacy, chemistry, number theory (and hence cryptology), seismology, meteorology, oceanography, many physical sciences, land surveying and geodesy, architecture, image compression, phonetics, economics, electrical engineering, mechanical engineering, civil engineering, computer graphics, cartography, crystallography and game development.\n\nThe following identities are related to the Pythagorean theorem and hold for any value:\n\nCertain equations involving trigonometric functions are true for all angles and are known as \"trigonometric identities\". Some identities equate an expression to a different expression involving the same angles. These are listed in List of trigonometric identities. Triangle identities that relate the sides and angles of a given triangle are listed below.\n\nIn the following identities, \"A\", \"B\" and \"C\" are the angles of a triangle and \"a\", \"b\" and \"c\" are the lengths of sides of the triangle opposite the respective angles (as shown in the diagram).\n\nThe law of sines (also known as the \"sine rule\") for an arbitrary triangle states:\n\nwhere formula_16 is the area of the triangle and \"R\" is the radius of the circumscribed circle of the triangle:\n\nAnother law involving sines can be used to calculate the area of a triangle. Given two sides \"a\" and \"b\" and the angle between the sides \"C\", the area of the triangle is given by half the product of the lengths of two sides and the sine of the angle between the two sides:\n\nThe law of cosines (known as the cosine formula, or the \"cos rule\") is an extension of the Pythagorean theorem to arbitrary triangles:\n\nor equivalently:\n\nThe law of cosines may be used to prove Heron's formula, which is another method that may be used to calculate the area of a triangle. This formula states that if a triangle has sides of lengths \"a\", \"b\", and \"c\", and if the semiperimeter is\n\nthen the area of the triangle is:\n\nwhere R is the radius of the circumcircle of the triangle.\n\nThe law of tangents:\n\nEuler's formula, which states that formula_24, produces the following analytical identities for sine, cosine, and tangent in terms of \"e\" and the imaginary unit \"i\":\n\n"}
{"id": "52167339", "url": "https://en.wikipedia.org/wiki?curid=52167339", "title": "Universal chord theorem", "text": "Universal chord theorem\n\nIn mathematical analysis, the universal chord theorem states that if a function \"f\" is continuous on [\"a\",\"b\"] and satisfies formula_1, then for every natural number formula_2, there exists some formula_3 such that formula_4.\n\nThe theorem was published by Paul Lévy in 1934 as a generalization of Rolle's Theorem.\n\nLet formula_5 denote the chord set of the function \"f\". If \"f\" is a continuous function and formula_6, then formula_7\nfor all natural numbers \"n\".\nThe case when \"n\" = 2 can be considered an application of the Borsuk–Ulam theorem to the real line. It says that if formula_8 is continuous on some\ninterval formula_9 with the condition that formula_1, then there exists some formula_3 such that formula_12. \n\nIn less generality, if formula_13 is continuous and formula_14, then there exists formula_15 that satisfies formula_16.\n\n"}
{"id": "28810270", "url": "https://en.wikipedia.org/wiki?curid=28810270", "title": "Vector algebra relations", "text": "Vector algebra relations\n\nThe relations below apply to vectors in a three-dimensional Euclidean space. Some, but not all of them, extend to vectors of higher dimensions. In particular, the cross product of two vectors is not available in all dimensions. See Seven-dimensional cross product.\n\nThe magnitude of a vector A is determined by its three components along three orthogonal directions using Pythagoras' theorem:\n\nThe magnitude also can be expressed using the dot product:\n\nHere the notation (A · B) denotes the dot product of vectors A and B.\n\nThe vector product and the scalar product of two vectors define the angle between them, say θ:\n\nTo satisfy the right-hand rule, for positive θ, vector B is counter-clockwise from A, and for negative θ it is clockwise.\nHere the notation A × B denotes the vector cross product of vectors A and B.\nThe Pythagorean trigonometric identity then provides:\n\nIf a vector A = (\"A, A, A\") makes angles α, β, γ with an orthogonal set of \"x-\", \"y-\" and \"z-\"axes, then:\n\nand analogously for angles β, γ. Consequently:\nwith formula_11 unit vectors along the axis directions.\n\nThe area Σ of a parallelogram with sides \"A\" and \"B\" containing the angle θ is:\nwhich will be recognized as the magnitude of the vector cross product of the vectors A and B lying along the sides of the parallelogram. That is:\nThe square of this expression is:\nwhere Γ(A, B) is the Gram determinant of A and B defined by:\n\nIn a similar fashion, the squared volume \"V\" of a parallelepiped spanned by the three vectors A, B and C is given by the Gram determinant of the three vectors:\nThis process can be extended to \"n\"-dimensions.\n\nSome of the following algebraic relations refer to the dot product and the cross product of vectors. These relations can be found in a variety of sources, for example, see Albright.\n\n"}
{"id": "9501159", "url": "https://en.wikipedia.org/wiki?curid=9501159", "title": "Well-pointed category", "text": "Well-pointed category\n\nIn category theory, a category with a terminal object formula_1 is well-pointed if for every pair of arrows formula_2 such that formula_3, there is an arrow formula_4 such that formula_5. (The arrows formula_6 are called the global elements or \"points\" of the category; a well-pointed category is thus one that has \"enough points\" to distinguish non-equal arrows.)\n\n"}
