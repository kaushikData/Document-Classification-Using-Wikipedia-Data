{"id": "2274", "url": "https://en.wikipedia.org/wiki?curid=2274", "title": "Arthur Eddington", "text": "Arthur Eddington\n\nSir Arthur Stanley Eddington (28 December 1882 – 22 November 1944) was an English astronomer, physicist, and mathematician of the early 20th century who did his greatest work in astrophysics. He was also a philosopher of science and a populariser of science. The Eddington limit, the natural limit to the luminosity of stars, or the radiation generated by accretion onto a compact object, is named in his honour.\n\nAround 1920, he anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper \"The Internal Constitution of the Stars\". At that time, the source of stellar energy was a complete mystery; Eddington was the first to correctly speculate that the source was fusion of hydrogen into helium.\n\nHe is famous for his work concerning the theory of relativity. Eddington wrote a number of articles that announced and explained Einstein's theory of general relativity to the English-speaking world. World War I severed many lines of scientific communication and new developments in German science were not well known in England. He also conducted an expedition to observe the solar eclipse of 29 May 1919 that provided one of the earliest confirmations of general relativity, and he became known for his popular expositions and interpretations of the theory.\n\nEddington was born 28 December 1882 in Kendal, Westmorland (now Cumbria), England, the son of Quaker parents, Arthur Henry Eddington, headmaster of the Quaker School, and Sarah Ann Shout.\n\nHis father taught at a Quaker training college in Lancashire before moving to Kendal to become headmaster of Stramongate School. He died in the typhoid epidemic which swept England in 1884. His mother was left to bring up her two children with relatively little income. The family moved to Weston-super-Mare where at first Stanley (as his mother and sister always called Eddington) was educated at home before spending three years at a preparatory school. The family lived at a house called Varzin, 42 Walliscote Road, Weston-super-Mare. There is a commemorative plaque on the building explaining Sir Arthur's contribution to science.\n\nIn 1893 Eddington entered Brynmelyn School. He proved to be a most capable scholar, particularly in mathematics and English literature. His performance earned him a scholarship to Owens College, Manchester (what was later to become the University of Manchester) in 1898, which he was able to attend, having turned 16 that year. He spent the first year in a general course, but turned to physics for the next three years. Eddington was greatly influenced by his physics and mathematics teachers, Arthur Schuster and Horace Lamb. At Manchester, Eddington lived at Dalton Hall, where he came under the lasting influence of the Quaker mathematician J. W. Graham. His progress was rapid, winning him several scholarships and he graduated with a B.Sc. in physics with First Class Honours in 1902.\n\nBased on his performance at Owens College, he was awarded a scholarship to Trinity College, Cambridge in 1902. His tutor at Cambridge was Robert Alfred Herman and in 1904 Eddington became the first ever second-year student to be placed as Senior Wrangler. After receiving his M.A. in 1905, he began research on thermionic emission in the Cavendish Laboratory. This did not go well, and meanwhile he spent time teaching mathematics to first year engineering students. This hiatus was brief. Through a recommendation by E. T. Whittaker, his senior colleague at Trinity College, he secured a position at the Royal Observatory in Greenwich where he was to embark on his career in astronomy, a career whose seeds had been sown even as a young child when he would often \"try to count the stars\".\n\nIn January 1906, Eddington was nominated to the post of chief assistant to the Astronomer Royal at the Royal Greenwich Observatory. He left Cambridge for Greenwich the following month. He was put to work on a detailed analysis of the parallax of 433 Eros on photographic plates that had started in 1900. He developed a new statistical method based on the apparent drift of two background stars, winning him the Smith's Prize in 1907. The prize won him a Fellowship of Trinity College, Cambridge. In December 1912 George Darwin, son of Charles Darwin, died suddenly and Eddington was promoted to his chair as the Plumian Professor of Astronomy and Experimental Philosophy in early 1913. Later that year, Robert Ball, holder of the theoretical Lowndean chair also died, and Eddington was named the director of the entire Cambridge Observatory the next year. In May 1914 he was elected a Fellow of the Royal Society: he was awarded the Royal Medal in 1928 and delivered the Bakerian Lecture in 1926.\n\nEddington also investigated the interior of stars through theory, and developed the first true understanding of stellar processes. He began this in 1916 with investigations of possible physical explanations for Cepheid variable stars. He began by extending Karl Schwarzschild's earlier work on radiation pressure in Emden polytropic models. These models treated a star as a sphere of gas held up against gravity by internal thermal pressure, and one of Eddington's chief additions was to show that radiation pressure was necessary to prevent collapse of the sphere. He developed his model despite knowingly lacking firm foundations for understanding opacity and energy generation in the stellar interior. However, his results allowed for calculation of temperature, density and pressure at all points inside a star, and Eddington argued that his theory was so useful for further astrophysical investigation that it should be retained despite not being based on completely accepted physics. James Jeans contributed the important suggestion that stellar matter would certainly be ionized, but that was the end of any collaboration between the pair, who became famous for their lively debates.\n\nEddington defended his method by pointing to the utility of his results, particularly his important mass-luminosity relation. This had the unexpected result of showing that virtually all stars, including giants and dwarfs, behaved as ideal gases. In the process of developing his stellar models, he sought to overturn current thinking about the sources of stellar energy. Jeans and others defended the Kelvin–Helmholtz mechanism, which was based on classical mechanics, while Eddington speculated broadly about the qualitative and quantitative consequences of possible proton-electron annihilation and nuclear fusion processes.\n\nAround 1920, he anticipated the discovery and mechanism of nuclear fusion processes in stars, in his paper \"The Internal Constitution of the Stars\". At that time, the source of stellar energy was a complete mystery; Eddington correctly speculated that the source was fusion of hydrogen into helium, liberating enormous energy according to Einstein's equation \"E = mc\". This was a particularly remarkable development since at that time fusion and thermonuclear energy, and even that stars are largely composed of hydrogen (see metallicity), had not yet been discovered. Eddington's paper, based on knowledge at the time, reasoned that:\n\nAll of these speculations were proven correct in the following decades.\n\nWith these assumptions, he demonstrated that the interior temperature of stars must be millions of degrees. In 1924, he discovered the mass-luminosity relation for stars (see Lecchini in #External links and references ). Despite some disagreement, Eddington's models were eventually accepted as a powerful tool for further investigation, particularly in issues of stellar evolution. The confirmation of his estimated stellar diameters by Michelson in 1920 proved crucial in convincing astronomers unused to Eddington's intuitive, exploratory style. Eddington's theory appeared in mature form in 1926 as \"The Internal Constitution of the Stars\", which became an important text for training an entire generation of astrophysicists.\n\nEddington's work in astrophysics in the late 1920s and the 1930s continued his work in stellar structure, and precipitated further clashes with Jeans and Edward Arthur Milne. An important topic was the extension of his models to take advantage of developments in quantum physics, including the use of degeneracy physics in describing dwarf stars.\n\nThe topic of extension of his models precipitated his famous dispute with Subrahmanyan Chandrasekhar, who was then a student at Cambridge. Chandrasekhar's work presaged the discovery of black holes, which at the time seemed so absurdly non-physical that Eddington refused to believe that Chandrasekhar's purely mathematical derivation had consequences for the real world. History clearly proved Eddington wrong, but his motivation remains a matter of some controversy. Chandrasekhar's narrative of this incident, in which his work is harshly rejected, portrays Eddington as rather cruel, dogmatic, and racist. Eddington's criticism seems to have been based on a suspicion that a purely mathematical derivation from relativity theory was not enough to explain away the seemingly daunting physical paradoxes that were inherent to degenerate stars.\n\nDuring World War I, Eddington was Secretary of the Royal Astronomical Society, which meant he was the first to receive a series of letters and papers from Willem de Sitter regarding Einstein's theory of general relativity. Eddington was fortunate in being not only one of the few astronomers with the mathematical skills to understand general relativity, but owing to his internationalist and pacifist views inspired by his Quaker religious beliefs, one of the few at the time who was still interested in pursuing a theory developed by a German physicist. He quickly became the chief supporter and expositor of relativity in Britain. He and Astronomer Royal Frank Watson Dyson organized two expeditions to observe a solar eclipse in 1919 to make the first empirical test of Einstein's theory: the measurement of the deflection of light by the sun's gravitational field. In fact, Dyson's argument for the indispensability of Eddington's expertise in this test was what prevented Eddington from eventually having to enter military service.\n\nWhen conscription was introduced in Britain on 2 March 1916, Eddington intended to apply for an exemption as a conscientious objector. Cambridge University authorities instead requested and were granted an exemption on the ground of Eddington's work being of national interest. In 1918, this was appealed against by the Ministry of National Service. Before the appeal tribunal in June, Eddington claimed conscientious objector status, which was not recognized and would have ended his exemption in August 1918. A further two hearings took place in June and July, respectively. Eddington's personal statement at the June hearing about his objection to war based on religious grounds is on record. Astronomer Royal, Sir Frank Dyson, supported Eddington at the July hearing with a written statement, emphasising Eddington's essential role in the solar eclipse expedition to Principe in May 1919. Eddington made clear his willingness to serve in the Friends' Ambulance Unit, under the jurisdiction of the British Red Cross, or as a harvest labourer. However, the tribunal's decision to grant a further twelve months exemption from military service was on condition of Eddington continuing his astronomy work, in particular in preparation for the Principe expedition. The war ended before the end of his exemption.\nAfter the war, Eddington travelled to the island of Príncipe off the west coast of Africa to watch the solar eclipse of 29 May 1919. During the eclipse, he took pictures of the stars (several stars in the Hyades cluster include Kappa Tauri of the constellation Taurus) in the region around the Sun. According to the theory of general relativity, stars with light rays that passed near the Sun would appear to have been slightly shifted because their light had been curved by its gravitational field. This effect is noticeable only during eclipses, since otherwise the Sun's brightness obscures the affected stars. Eddington showed that Newtonian gravitation could be interpreted to predict half the shift predicted by Einstein.\n\nEddington's observations published the next year confirmed Einstein's theory, and were hailed at the time as evidence of general relativity over the Newtonian model. The news was reported in newspapers all over the world as a major story. Afterward, Eddington embarked on a campaign to popularize relativity and the expedition as landmarks both in scientific development and international scientific relations.\n\nIt has been claimed that Eddington's observations were of poor quality, and he had unjustly discounted simultaneous observations at Sobral, Brazil, which appeared closer to the Newtonian model, but a 1979 re-analysis with modern measuring equipment and contemporary software validated Eddington's results and conclusions. The quality of the 1919 results was indeed poor compared to later observations, but was sufficient to persuade contemporary astronomers. The rejection of the results from the Brazil expedition was due to a defect in the telescopes used which, again, was completely accepted and well understood by contemporary astronomers.\n\nThroughout this period, Eddington lectured on relativity, and was particularly well known for his ability to explain the concepts in lay terms as well as scientific. He collected many of these into the \"Mathematical Theory of Relativity\" in 1923, which Albert Einstein suggested was \"the finest presentation of the subject in any language.\" He was an early advocate of Einstein's General Relativity, and an interesting anecdote well illustrates his humour and personal intellectual investment: Ludwik Silberstein, a physicist who thought of himself as an expert on relativity, approached Eddington at the Royal Society's (6 November) 1919 meeting where he had defended Einstein's Relativity with his Brazil-Principe Solar Eclipse calculations with some degree of skepticism, and ruefully charged Arthur as one who claimed to be one of three men who actually understood the theory (Silberstein, of course, was including himself and Einstein as the other). When Eddington refrained from replying, he insisted Arthur not be \"so shy\", whereupon Eddington replied, \"Oh, no! I was wondering who the third one might be!\"\n\nEddington was also heavily involved with the development of the first generation of general relativistic cosmological models. He had been investigating the instability of the Einstein universe when he learned of both Lemaître's 1927 paper postulating an expanding or contracting universe and Hubble's work on the recession of the spiral nebulae. He felt the cosmological constant must have played the crucial role in the universe's evolution from an Einsteinian steady state to its current expanding state, and most of his cosmological investigations focused on the constant's significance and characteristics. In \"The Mathematical Theory of Relativity,\" Eddington interpreted the cosmological constant to mean that the universe is \"self-gauging\".\n\nDuring the 1920s until his death, Eddington increasingly concentrated on what he called \"fundamental theory\" which was intended to be a unification of quantum theory, relativity, cosmology, and gravitation. At first he progressed along \"traditional\" lines, but turned increasingly to an almost numerological analysis of the dimensionless ratios of fundamental constants.\n\nHis basic approach was to combine several fundamental constants in order to produce a dimensionless number. In many cases these would result in numbers close to 10, its square, or its square root. He was convinced that the mass of the proton and the charge of the electron were a \"natural and complete specification for constructing a Universe\" and that their values were not accidental. One of the discoverers of quantum mechanics, Paul Dirac, also pursued this line of investigation, which has become known as the Dirac large numbers hypothesis, and some scientists even today believe it has something to it.\n\nA somewhat damaging statement in his defence of these concepts involved the fine structure constant, α. At the time it was measured to be very close to 1/136, and he argued that the value should in fact be exactly 1/136 for epistemological reasons. Later measurements placed the value much closer to 1/137, at which point he switched his line of reasoning to argue that one more should be added to the degrees of freedom, so that the value should in fact be exactly 1/137, the Eddington number. Wags at the time started calling him \"Arthur Adding-one\". This change of stance detracted from Eddington's credibility in the physics community. The current measured value is estimated at 1/137.035 999 074(44).\n\nEddington believed he had identified an algebraic basis for fundamental physics, which he termed \"E-numbers\" (representing a certain group – a Clifford algebra). These in effect incorporated spacetime into a higher-dimensional structure. While his theory has long been neglected by the general physics community, similar algebraic notions underlie many modern attempts at a grand unified theory. Moreover, Eddington's emphasis on the values of the fundamental constants, and specifically upon dimensionless numbers derived from them, is nowadays a central concern of physics. In particular, he predicted a number of hydrogen atoms in the Universe 136 × 2, or equivalently the half of the total number of particles protons + electrons. When equalized with the non-dark energy equivalent number of hydrogen atoms (3/10) × Rc/GmH, this corresponds to a Universe radius R = 13.8 Giga light year, a value predicted for years from universal constants using an atomic-cosmic symmetry, and compatible with \"c\" times the so-called age of the Universe, 13.80(4) Gyr, as determined by the Planck mission in March 2003.\n\nHe did not complete this line of research before his death in 1944; his book \"Fundamental Theory\" was published posthumously in 1948.\n\nEddington is credited with devising a measure of a cyclist's long-distance riding achievements. The Eddington number in the context of cycling is defined as the maximum number E such that the cyclist has cycled E miles on E days. For example, an Eddington number of 70 would imply that the cyclist has cycled at least 70 miles in a day on 70 occasions. Achieving a high Eddington number is difficult since moving from, say, 70 to 75 will probably require more than five new long distance rides since any rides shorter than 75 miles will no longer be included in the reckoning. Eddington's own E-number was 84.\n\nThe Eddington number for cycling is analogous to the \"h\"-index that quantifies both the actual scientific productivity and the apparent scientific impact of a scientist.\n\nThe Eddington Number for cycling has units (indeed applying it to any physical property will result in E having units). For example, an E of 62 miles means a cyclist has covered 62 or more miles on 62 or more days. However, in units of kilometers the 62 miles becomes 100 km. It is possible that the cyclist, while having covered 100 km on 62 days or more, may not have covered 100 km on 100 days or more. Thus the order of bicyclists may change depending on units used. Using the original miles, one cyclist may have an Eddington number of 60 – 60 miles (97 km) in 60 days, another of 50 (corresponding to 80 km). However, the latter may be a regular on a distance like this and get a km-Eddington of 80, while the former only had those 60 days riding, and thus stays at a km-Eddington of 60.\n\nEddington wrote in his book \"The Nature of the Physical World\" that \"The stuff of the world is mind-stuff.\"\nThe idealist conclusion was not integral to his epistemology but was based on two main arguments.\n\nThe first derives directly from current physical theory. Briefly, mechanical theories of the ether and of the behaviour of fundamental particles have been discarded in both relativity and quantum physics. From this, Eddington inferred that a materialistic metaphysics was outmoded and that, in consequence, since the disjunction of materialism or idealism are assumed to be exhaustive, an idealistic metaphysics is required. The second, and more interesting argument, was based on Eddington's epistemology, and may be regarded as consisting of two parts. First, all we know of the objective world is its structure, and the structure of the objective world is precisely mirrored in our own consciousness. We therefore have no reason to doubt that the objective world too is \"mind-stuff\". Dualistic metaphysics, then, cannot be evidentially supported.\n\nBut, second, not only can we not know that the objective world is nonmentalistic, we also cannot intelligibly suppose that it could be material. To conceive of a dualism entails attributing material properties to the objective world. However, this presupposes that we could observe that the objective world has material properties. But this is absurd, for whatever is observed must ultimately be the content of our own consciousness, and consequently, nonmaterial.\n\nIan Barbour, in his book \"Issues in Science and Religion\" (1966), p. 133, cites Eddington's \"The Nature of the Physical World\" (1928) for a text that argues the Heisenberg Uncertainty Principles provides a scientific basis for \"the defense of the idea of human freedom\" and his \"Science and the Unseen World\" (1929) for support of philosophical idealism \"the thesis that reality is basically mental\".\n\nCharles De Koninck points out that Eddington believed in objective reality existing apart from our minds, but was using the phrase \"mind-stuff\" to highlight the inherent intelligibility of the world: that our minds and the physical world are made of the same \"stuff\" and that our minds are the inescapable connection to the world. As De Koninck quotes Eddington,\n\nAgainst Albert Einstein and others who advocated determinism, indeterminism—championed by Eddington—says that a physical object has an ontologically undetermined component that is not due to the epistemological limitations of physicists' understanding. The uncertainty principle in quantum mechanics, then, would not necessarily be due to hidden variables but to an indeterminism in nature itself.\n\nEddington wrote a clever parody of \"The Rubaiyat of Omar Khayyam\", recounting his 1919 solar eclipse experiment. It contained the following quatrain:\n\nDuring the 1920s and 30s, Eddington gave numerous lectures, interviews, and radio broadcasts on relativity, in addition to his textbook \"The Mathematical Theory of Relativity\", and later, quantum mechanics. Many of these were gathered into books, including \"The Nature of the Physical World\" and \"New Pathways in Science\". His skillful use of literary allusions and humour helped make these famously difficult subjects quite accessible.\n\nEddington's books and lectures were immensely popular with the public, not only because of Eddington's clear and entertaining exposition, but also for his willingness to discuss the philosophical and religious implications of the new physics. He argued for a deeply rooted philosophical harmony between scientific investigation and religious mysticism, and also that the positivist nature of modern physics (i.e., relativity and quantum physics) provided new room for personal religious experience and free will. Unlike many other spiritual scientists, he rejected the idea that science could provide proof of religious propositions.\n\nHe is sometimes misunderstood as having promoted the infinite monkey theorem in his 1928 book \"The Nature of the Physical World\", with the phrase \"If an army of monkeys were strumming on typewriters, they might write all the books in the British Museum\". It is clear from the context that Eddington is not suggesting that the probability of this happening is worthy of serious consideration. On the contrary, it was a rhetorical illustration of the fact that below certain levels of probability, the term \"improbable\" is functionally equivalent to \"impossible\".\n\nHis popular writings made him a household name in Great Britain between the world wars.\n\nEddington died of cancer in the Evelyn Nursing Home, Cambridge, on 22 November 1944. He was unmarried. His body was cremated at Cambridge Crematorium (Cambridgeshire) on 27 November 1944; the cremated remains were buried in the grave of his mother in the Ascension Parish Burial Ground in Cambridge.\n\nCambridge University's North West Cambridge Development has been named \"Eddington\" in his honour.\n\n\nAwards\n\nNamed after him\n\nService\n\n\n\n\n\n\n\n\n"}
{"id": "39430307", "url": "https://en.wikipedia.org/wiki?curid=39430307", "title": "Benedikt Löwe", "text": "Benedikt Löwe\n\nBenedikt Löwe (born 1972) is a German mathematician and logician, and Professor at the University of Hamburg, known for initiating the interdisciplinary conference \"Foundations of the Formal Sciences\" (FotFS) in 1999.\n\nLöwe received his BA in mathematics and philosophy at the University of Hamburg and continued his studies at the University of Tübingen, the University of Berlin and Berkeley. In 2001 he completed his PhD entitled \"Blackwell Determinacy\" about determinacy under supervision of Donald A. Martin and Ronald Björn Jensen. Since early 2000 he is Assistant Professor at the University of Amsterdam, and since late 2000s also Professor at the University of Hamburg. Furthermore, he was a Fellow at the University of Cambridge for some time. Currently, he is managing editor of the journal \"Mathematical Logic Quarterly\". and the journal \"Computability\", and editor at the journals \"Journal of Logic, Language and Information\" and \"Tbilisi Mathematical Journal\".\n\nBooks, a selection:\n\n"}
{"id": "35740250", "url": "https://en.wikipedia.org/wiki?curid=35740250", "title": "Capped square antiprismatic molecular geometry", "text": "Capped square antiprismatic molecular geometry\n\nIn chemistry, the capped square antiprismatic molecular geometry describes the shape of compounds where nine atoms, groups of atoms, or ligands are arranged around a central atom, defining the vertices of a gyroelongated square pyramid.\n\nThe gyroelongated square pyramid is a square pyramid with a square antiprism connected to the square base. In this respect, it can be seen as a \"capped\" square antiprism (a square antiprism with a pyramid erected on one of the square faces).\n\nIt is very similar to the tricapped trigonal prismatic molecular geometry, and there is some dispute over the specific geometry exhibited by certain molecules.\n\n"}
{"id": "44514254", "url": "https://en.wikipedia.org/wiki?curid=44514254", "title": "Compact finite difference", "text": "Compact finite difference\n\nThe compact finite difference (CTFD) formulation, or Hermitian formulation, is a numerical method to solve the compressible Navier–Stokes equation. This method is both accurate and numerically very stable (especially for high-order derivatives).\n\nThe expression for partial derivatives is developed and expressed mainly on dependent variables. An approach to increase accuracy of the estimates of the derivatives, in particular a problem involving shorter length scales or equivalently high frequencies, is to include the influence of the neighboring points in the calculations. This approach is analogous to the solution of a partial differential equation by an Implicit scheme to an explicit scheme. The resulting approximation is called a compact finite difference (CTFD) formulation or a Hermitian formulation.\n\nForward difference formulae and backward difference formulae are first order accurate, and central difference formula are second order accurate; compact finite difference formulae provide a more accurate method to solve equations.\n\nConsider a three-point Hermitian formula involving the first derivative:\n\nSubstituting the Taylor series expansion of the terms f and f results in:\n\nIn the above expression, only the first few terms are to be considered zero, and the rest of the higher order terms will be considered as the truncation error (TE). To obtain a third-order scheme, the coefficients of \"f\" , \"f\"<nowiki>'</nowiki>, \"f\"<nowiki>\"</nowiki> , \"f\"<nowiki>\"'</nowiki> will be zero. If it is a fourth-order scheme, the coefficient of f’’’’ will also be zero.\n\nFrom the above equations, one can solve for a , a , a and b in the form of b and b\n\nThe Truncation error will be:\n\nSubstituting (4) in the above equation results in: \n\nThe standard compact finite difference formula for first order derivatives of f(x) has a 3-point formulation \n[Δx/3] [f’ + 4f’ + f’] = -f + f \nwhere f’ = df(x)/dx\nSimilarly, for a fifth point formulation, take m=2 in the beginning.\n\nOne-parameter family of compact finite difference schemes for first order derivatives Scheme\nSimilar to the first order derivatives equation, i.e., eq. (5) the equation for a 2nd order derivative will be:\nCompact finite difference formulation is very frequently used to solve problems based on the Navier–Stokes equation, and is used extensively in solving hyperbolic equations.\n\n"}
{"id": "451445", "url": "https://en.wikipedia.org/wiki?curid=451445", "title": "Cube (algebra)", "text": "Cube (algebra)\n\nIn arithmetic and algebra, the cube of a number is its third power: the result of the number multiplied by itself twice:\n\nIt is also the number multiplied by its square:\n\nThis is also the volume formula for a geometric cube with sides of length , giving rise to the name. The inverse operation of finding a number whose cube is is called extracting the cube root of . It determines the side of the cube of a given volume. It is also raised to the one-third power.\n\nBoth cube and cube root are odd functions:\n\nThe cube of a number or any other mathematical expression is denoted by a superscript 3, for example or .\n\nThe graph of the cube function f: \"x\" → \"x\" (or the equation \"y\" = \"x\") is known as the cubic parabola. Because cube is an odd function, this curve has a point of symmetry in the origin, but no axis of symmetry.\n\nA cube number, or a perfect cube, or sometimes just a cube, is a number which is the cube of an integer.\nThe perfect cubes up to 60 are :\n\nGeometrically speaking, a positive integer is a perfect cube if and only if one can arrange solid unit cubes into a larger, solid cube. For example, 27 small cubes can be arranged into one larger one with the appearance of a Rubik's Cube, since .\n\nThe difference between the cubes of consecutive integers can be expressed as follows:\n\nor\n\nThere is no minimum perfect cube, since the cube of a negative integer is negative. For example, .\n\nUnlike perfect squares, perfect cubes do not have a small number of possibilities for the last two digits. Except for cubes divisible by 5, where only 25, 75 and 00 can be the last two digits, \"any\" pair of digits with the last digit odd can be a perfect cube. With even cubes, there is considerable restriction, for only 00, 2, 4, 6 and 8 can be the last two digits of a perfect cube (where stands for any odd digit and for any even digit). Some cube numbers are also square numbers; for example, 64 is a square number and a cube number . This happens if and only if the number is a perfect sixth power (in this case 2).\n\nThe last digits of each 3rd power are:\n\nIt is, however, easy to show that most numbers are not perfect cubes because \"all\" perfect cubes must have digital root 1, 8 or 9. That is their values modulo 9 may be only −1, 1 and 0. Moreover, the digital root of any number's cube can be determined by the remainder the number gives when divided by 3:\n\nEvery positive integer can be written as the sum of nine (or fewer) positive cubes. This upper limit of nine cubes cannot be reduced because, for example, 23 cannot be written as the sum of fewer than nine positive cubes:\n\nThe equation has no non-trivial (i.e. ) solutions in integers. In fact, it has none in Eisenstein integers.\n\nBoth of these statements are also true for the equation .\n\nThe sum of the first cubes is the th triangle number squared:\n\nProofs.\n\nThat identity is related to triangular numbers formula_6 in the following way:\n\nand thus the summands forming formula_8 start off just after those forming all previous values formula_9 up to formula_10.\nApplying this property, along with another well-known identity:\n\nwe obtain the following derivation:\n\nIn the more recent mathematical literature, uses the rectangle-counting interpretation of these numbers to form a geometric proof of the identity (see also ); he observes that it may also be proved easily (but uninformatively) by induction, and states that provides \"an interesting old Arabic proof\". provides a purely visual proof, provide two additional proofs, and gives seven geometric proofs.\n\nFor example, the sum of the first 5 cubes is the square of the 5th triangular number,\n\nA similar result can be given for the sum of the first odd cubes,\n\nbut , must satisfy the negative Pell equation . For example, for and , then,\n\nand so on. Also, every even perfect number, except the lowest, is the sum of the first odd cubes (\"p\" = 3, 5, 7, ...):\n\nThere are examples of cubes of numbers in arithmetic progression whose sum is a cube:\n\nwith the first one also known as Plato's number. The formula for finding the sum of \ncubes of numbers in arithmetic progression with common difference and initial cube ,\n\nis given by\n\nA parametric solution to\n\nis known for the special case of , or consecutive cubes, but only sporadic solutions are known for integer , such as  = 2, 3, 5, 7, 11, 13, 37, 39, etc.\n\nIn the sequence of odd integers 1, 3, 5, 7, 9, 11, 13, 15, 17, 19, …, the first \"one\" is a cube (); the sum of the next \"two\" is the next cube (); the sum of the next \"three\" is the next cube (); and so forth.\n\nEvery positive rational number is the sum of three positive rational cubes, and there are rationals that are not the sum of two rational cubes.\n\nIn real numbers, the cube function preserves the order: larger numbers have larger cubes. In other words, cubes (strictly) monotonically increase. Also, its codomain is the entire real line: the function is a surjection (takes all possible values). Only three numbers are equal to their own cubes: , , and . If or , then . If or , then . All aforementioned properties pertain also to any higher odd power (, , …) of real numbers. Equalities and inequalities are also true in any ordered ring.\n\nVolumes of similar Euclidean solids are related as cubes of their linear sizes.\n\nIn complex numbers, the cube of a purely imaginary number is also purely imaginary. For example, .\n\nThe derivative of equals .\n\nCubes \"occasionally\" have the surjective property in other fields, such as in for such prime that , but not necessarily: see the counterexample with rationals above. Also in only three elements 0, ±1 are perfect cubes, of seven total. −1, 0, and 1 are perfect cubes \"anywhere\" and the only elements of a field equal to the own cubes: .\n\nDetermination of the cubes of large numbers was very common in many ancient civilizations. Mesopotamian mathematicians created cuneiform tablets with tables for calculating cubes and cube roots by the Old Babylonian period (20th to 16th centuries BC). Cubic equations were known to the ancient Greek mathematician Diophantus. Hero of Alexandria devised a method for calculating cube roots in the 1st century CE. Methods for solving cubic equations and extracting cube roots appear in \"The Nine Chapters on the Mathematical Art\", a Chinese mathematical text compiled around the 2nd century BCE and commented on by Liu Hui in the 3rd century CE. The Indian mathematician Aryabhata wrote an explanation of cubes in his work Aryabhatiya. In 2010 Alberto Zanoni found a new algorithm to compute the cube of a long integer in a certain range, faster than squaring-and-multiplying.\n\n"}
{"id": "1372580", "url": "https://en.wikipedia.org/wiki?curid=1372580", "title": "Discrete symmetry", "text": "Discrete symmetry\n\nIn mathematics, a discrete symmetry is a symmetry that describes non-continuous changes in a system. For example, a square possesses discrete rotational symmetry, as only rotations by multiples of right angles will preserve the square's original appearance. Discrete symmetries sometimes involve some type of 'swapping', these swaps usually being called \"reflections\" or \"interchanges\". In mathematics and theoretical physics, a discrete symmetry is a symmetry under the transformations of a discrete group—e.g. a topological group with a discrete topology whose elements form a finite or a countable set.\n\nOne of the most prominent discrete symmetries in physics is parity symmetry. It manifests itself in various elementary physical quantum systems, such as quantum harmonic oscillator, electron orbitals of Hydrogen-like atoms by forcing wavefunctions to be even or odd. This in turn gives rise to selection rules that determine which transition lines are visible in atomic absorption spectra.\n\n"}
{"id": "26068475", "url": "https://en.wikipedia.org/wiki?curid=26068475", "title": "Dominique Foata", "text": "Dominique Foata\n\nDominique Foata (born October 12, 1934) is a mathematician who works in enumerative combinatorics. With Pierre Cartier and Marcel-Paul Schützenberger he pioneered the modern approach to classical combinatorics, that lead, in part, to the current blossoming of algebraic combinatorics. His pioneering work on permutation statistics, and his combinatorial approach to special functions, are especially notable.\n\nFoata gave an invited talk at the International Congress of Mathematicians in Warsaw (1983). Among his honors are the Scientific Prize of the\nUnion des Assurances de Paris (September 1985). With Adalbert Kerber and Volker Strehl he founded the mathematics journal \"Séminaire Lotharingien de Combinatoire\". He is also one of the contributors of the pseudonymous collective M. Lothaire.\n\nHe was born in Damascus while it was under French mandate.\n\n\n\n"}
{"id": "609125", "url": "https://en.wikipedia.org/wiki?curid=609125", "title": "Expression (mathematics)", "text": "Expression (mathematics)\n\nIn mathematics, an expression or mathematical expression is a finite combination of symbols that is well-formed according to rules that depend on the context. Mathematical symbols can designate numbers (constants), variables, operations, functions, brackets, punctuation, and grouping to help determine order of operations, and other aspects of logical syntax.\n\nThe use of expressions ranges from the simple:\n\nto the complex: \n\nAn expression is a syntactic construct, it must be well-formed: the allowed operators must have the correct number of inputs in the correct places, the characters that make up these inputs must be valid, have a clear order of operations, etc. Strings of symbols that violate the rules of syntax are not well-formed and are not valid mathematical expressions.\n\nFor example, in the usual notation of arithmetic, the expression \"1 + 2 × 3\" is well-formed, but the following expression is not:\n\nSemantics is the study of meaning. Formal semantics is about attaching meaning to expressions.\n\nIn algebra, an expression may be used to designate a value, which might depend on values assigned to variables occurring in the expression. The determination of this value depends on the semantics attached to the symbols of the expression. The choice of semantics depends on the context of the expression. The same syntactic expression \"1 + 2 × 3\" can have different values (mathematically 7, but also 9), depending on the order of operations implied by the context (See Order of operations:Calculators).\n\nThe semantic rules may declare that certain expressions do not designate any value (for instance when they involve division by 0); such expressions are said to have an undefined value, but they are well-formed expressions nonetheless. In general the meaning of expressions is not limited to designating values; for instance, an expression might designate a condition, or an equation that is to be solved, or it can be viewed as an object in its own right that can be manipulated according to certain rules. Certain expressions that designate a value simultaneously express a condition that is assumed to hold, for instance those involving the operator formula_7 to designate an internal direct sum.\n\nFormal languages allow formalizing the concept of well-formed expressions.\n\nIn the 1930s, a new type of expressions, called lambda expressions, were introduced by Alonzo Church and Stephen Kleene for formalizing functions and their evaluation. They form the basis for lambda calculus, a formal system used in mathematical logic and the theory of programming languages.\n\nThe equivalence of two lambda expressions is undecidable. This is also the case for the expressions representing real numbers, which are built from the integers by using the arithmetical operations, the logarithm and the exponential (Richardson's theorem).\n\nMany mathematical expressions include variables. Any variable can be classified as being either a free variable or a bound variable.\n\nFor a given combination of values for the free variables, an expression may be evaluated, although for some combinations of values of the free variables, the value of the expression may be undefined. Thus an expression represents a function whose inputs are the values assigned to the free variables and whose output is the resulting value of the expression.\n\nFor example, the expression\n\nevaluated for \"x\" = 10, \"y\" = 5, will give 2; but it is undefined for \"y\" = 0.\n\nThe evaluation of an expression is dependent on the definition of the mathematical operators and on the system of values that is its context.\n\nTwo expressions are said to be equivalent if, for each combination of values for the free variables, they have the same output, i.e., they represent the same function. Example:\n\nThe expression\nhas free variable \"x\", bound variable \"n\", constants 1, 2, and 3, two occurrences of an implicit multiplication operator, and a summation operator. The expression is equivalent to the simpler expression 12\"x\". The value for \"x\" = 3 is 36.\n\n"}
{"id": "2666205", "url": "https://en.wikipedia.org/wiki?curid=2666205", "title": "Fermat point", "text": "Fermat point\n\nIn geometry, the Fermat point of a triangle, also called the Torricelli point or Fermat–Torricelli point, is a point such that the total distance from the three vertices of the triangle to the point is the minimum possible. It is so named because this problem is first raised by Fermat in a private letter to Evangelista Torricelli, who solved it.\n\nThe Fermat point gives a solution to the geometric median and Steiner tree problems for three points.\n\nThe Fermat point of a triangle with largest angle at most 120° is simply its first isogonic center or X(13), which is constructed as follows:\n\nAn alternate method is the following:\n\nWhen a triangle has an angle greater than 120°, the Fermat point is sited at the obtuse-angled vertex.\n\nIn what follows \"Case 1\" means the triangle has an angle exceeding 120°. \"Case 2\" means no angle of the triangle exceeds 120°.\n\nFig. 2 shows the equilateral triangles ARB, AQC and CPB attached to the sides of the arbitrary triangle ABC.\nHere is a proof using properties of concyclic points to show that the three lines RC, BQ and AP in Fig 2 all intersect at the point F and cut one another at angles of 60°.\n\nThe triangles RAC and BAQ are congruent because the second is a 60° rotation of the first about A. Hence ∠ARF = ∠ABF and ∠AQF = ∠ACF. By the converse of the inscribed angle theorem applied to the segment AF, the points ARBF are concyclic (they lie on a circle). Similarly, the points AFCQ are concyclic.\n\n∠ARB = 60°, so ∠AFB = 120°, using the inscribed angle theorem. Similarly, ∠AFC = 120°.\n\nSo ∠BFC = 120°. So, ∠BFC and ∠BPC add up to 180°. Using the inscribed angle theorem, this implies that the points BPCF are concyclic. So, using the inscribed angle theorem applied to the segment BP, ∠BFP = ∠BCP = 60°. Because ∠BFP + ∠BFA = 180°, the point F lies on the line segment AP. So, the lines RC, BQ and AP are concurrent (they intersect at a single point). Q.E.D.\n\nThis proof only applies in Case 2 since if ∠BAC > 120°, point A lies inside the circumcircle of BPC which switches the relative positions of A and F. However it is easily modified to cover Case 1. Then ∠AFB = ∠AFC = 60° hence ∠BFC = ∠AFB = ∠AFC = 120° which means BPCF is concyclic so ∠BFP = ∠BCP = 60° = ∠BFA. Therefore, A lies on FP.\n\nThe lines joining the centers of the circles in Fig. 2 are perpendicular to the line segments AP, BQ and CR. For example, the line joining the center of the circle containing ARB and the center of the circle containing AQC, is perpendicular to the segment AP. So, the lines joining the centers of the circles also intersect at 60° angles. Therefore, the centers of the circles form an equilateral triangle. This is known as Napoleon's Theorem.\n\nGiven any Euclidean triangle ABC and an arbitrary point P let d(P) = PA+PB+PC, with PA denoting the distance between P and A. The aim of this section is to identify a point P such that d(P) < d(P) for all P ≠ P. If such a point exists then it will be the Fermat point. In what follows Δ will denote the points inside the triangle and will be taken to include its boundary Ω.\n\nA key result that will be used is the dogleg rule which asserts that if a triangle and a polygon have one side in common and the rest of the triangle lies inside the polygon then the triangle has a shorter perimeter than the polygon. [If AB is the common side extend AC to cut the polygon at X. Then by the triangle inequality the polygon perimeter > AB + AX + XB = AB + AC + CX + XB ≥ AB + AC + BC.]\n\nLet P be any point outside Δ. Associate each vertex with its remote zone; that is, the half-plane beyond the (extended) opposite side. These 3 zones cover the entire plane except for Δ itself and P clearly lies in either one or two of them. If P is in two (say the B and C zones intersection) then setting P' = A implies d(P') = d(A) < d(P) by the dogleg rule. Alternatively if P is in only one zone, say the A-zone, then d(P') < d(P) where P' is the intersection of AP and BC. So for every point P outside Δ there exists a point P' in Ω such that d(P') < d(P).\n\nCase 1. The triangle has an angle ≥ 120°.\n\nWithout loss of generality suppose that the angle at A is ≥ 120°. Construct the equilateral triangle AFB and for any point P in Δ (except A itself) construct Q so that the triangle AQP is equilateral and has the orientation shown. Then the triangle ABP is a 60° rotation of the triangle AFQ about A so these two triangles are congruent and it follows that d(P) = CP+PQ+QF which is simply the length of the path CPQF. As P is constrained to lie within ABC, by the dogleg rule the length of this path exceeds AC+AF = d(A). Therefore, d(A) < d(P) for all P є Δ, P ≠ A. Now allow P to range outside Δ. From above a point P' є Ω exists such that d(P') < d(P) and as d(A) ≤ d (P') it follows that d(A) < d(P) for all P outside Δ. Thus d(A) < d(P) for all P ≠ A which means that A is the Fermat point of Δ. In other words, the Fermat point lies at the obtuse angled vertex.\n\nCase 2. The triangle has no angle ≥ 120°.\n\nConstruct the equilateral triangle BCD and let P be any point inside Δ and construct the equilateral triangle CPQ. Then CQD is a 60° rotation of CPB about C so d(P) = PA+PB+PC = AP+PQ+QD which is simply the length of the path APQD. Let P be the point where AD and CF intersect. This point is commonly called the first isogonic center. By the angular restriction P lies inside Δ moreover BCF is a 60° rotation of BDA about B so Q must lie somewhere on AD. Since CDB = 60° it follows that Q lies between P and D which means APQD is a straight line so d(P) = AD. Moreover, if P ≠ P then either P or Q won't lie on AD which means d(P) = AD < d(P). Now allow P to range outside Δ. From above a point P' є Ω exists such that d(P') < d(P) and as d(P) ≤ d(P') it follows that d(P) < d(P) for all P outside Δ. That means P is the Fermat point of Δ. In other words, the Fermat point is coincident with the first isogonic center.\n\nLet \"O\", \"A\", \"B\", \"C\", \"X\" be any five points in a plane. Denote the vectors formula_1 by a, b, c, x respectively, and let i, j, k be the unit vectors from \"O\" along a, b, c. \nNow |a| = a⋅i = (a − x)⋅i + x⋅i ≤ |a − x| + x⋅i and similarly |b| ≤ |b − x| + x⋅j and |c| ≤ |c − x| + x⋅k. \nAdding gives |a| + |b| + |c| ≤ |a − x| + |b − x| + |c − x| + x⋅(i + j + k). \nIf a, b, c meet at \"O\" at angles of 120° then i + j + k = 0 so |a| + |b| + |c| ≤ |a − x| + |b − x| + |c − x| for all x. \nIn other words, \"OA\" + \"OB\" + \"OC\" ≤ \"XA\" + \"XB\" + \"XC\" and hence \"O\" is the Fermat point of \"ABC\". \nThis argument fails when the triangle has an angle \"∠C\" > 120° because there is no point \"O\" where a, b, c meet at angles of 120°. Nevertheless, it is easily fixed by redefining k = − (i + j) and placing \"O\" at \"C\" so that c = 0. Note that |k| ≤ 1 because the angle between the unit vectors i and j is \"∠C\" which exceeds 120°. Since |0| ≤ |0 − x| + x⋅k the third inequality still holds, the other two inequalities are unchanged. The proof now continues as above (adding the three inequalities and using i + j + k = 0) to reach the same conclusion that \"O\" (or in this case \"C\") must be the Fermat point of \"ABC\".\n\nAnother approach to find a point within the triangle, from where sum of the distances to the vertices of triangle is minimum, is to use one of the optimization (mathematics) methods. In particular, method of the Lagrange multipliers and the law of cosines.\n\nWe draw lines from the point within the triangle to its vertices and call them X, Y and Z. Also, let the lengths of these lines be x, y, and z, respectively. Let the angle between X and Y be α, Y and Z be β. Then the angle between X and Z is (2π − α − β). Using the method of Lagrange multipliers we have to find the minimum of the Lagrangian \"L\", which is expressed as:\n\nwhere \"a\", \"b\" and \"c\" are the lengths of the sides of the triangle.\n\nEquating each of the five partial derivatives δ\"L\"/δx, δ\"L\"/δy, δ\"L\"/δz, δ\"L\"/δα, δ\"L\"/δβ to zero and eliminating \"λ\", \"λ\", \"λ\" eventually gives sin(α) = sin(β) and sin(α + β) = − sin(β) so α = β = 120°. However the elimination is a long tedious business and the end result only covers Case 2.\n\n\nThe isogonic centers \"X\"(13) and \"X\"(14) are also known as the first Fermat point and the second Fermat point respectively. Alternatives are the positive Fermat point and the negative Fermat point. However these different names can be confusing and are perhaps best avoided. The problem is that much of the literature blurs the distinction between the Fermat point and the first Fermat point whereas it is only in Case 2 above that they are actually the same.\n\nThis question was proposed by Fermat, as a challenge to Evangelista Torricelli. He solved the problem in a similar way to Fermat's, albeit using intersection of the circumcircles of the three regular triangles instead. His pupil, Viviani, published the solution in 1659.\n\n\n"}
{"id": "57033959", "url": "https://en.wikipedia.org/wiki?curid=57033959", "title": "Finitist set theory", "text": "Finitist set theory\n\nFinitist set theory (FST) is a collection theory designed for modeling finite nested structures of individuals and a variety of transitive and antitransitive chains of relations between individuals. Unlike classical set theories such as ZFC and KPU, FST is not intended to function as a foundation mathematics, but only as a tool in ontological modeling. FST functions as the logical foundation of the classical layer-cake interpretation, and manages to incorporate a large portion of the functionality of discrete mereology.\n\nFST models are of type formula_1, which is abbreviated as formula_2. formula_3 is the collection of ur-elements of model formula_2. Ur-elements (urs) are indivisible\nprimitives. By assigning a finite integer such as 2 as the value of formula_5, it is determined that formula_6 contains exactly 2 urs. formula_7 is a collection whose elements will be called sets. formula_8 is a finite integer which denotes the maximum rank (nesting level) of sets in formula_7. Every set in formula_7 has one or more sets or urs or both as members. The assigned formula_5 and formula_12 and the applied axioms fix the contents of formula_6 and formula_7. To facilitate the use of language, expressions such as \"sets that are elements of formula_7 of model formula_2 and urs that are elements of formula_6 of model formula_18\" are abbreviated as \"sets and urs that are elements of formula_2\".\n\nFST’s formal development conforms to its intended function as a tool in ontological modeling. The goal of an engineer who applies FST is to select axioms which yield a model that is one-one correlated with a target domain that is to be modeled by FST, such as a range of chemical compounds or social constructions that are found in nature. The target domain gives the engineer an intuition about the contents of the FST model that ought to be one-one correlated with it. FST provides a framework that facilitates selecting specific axioms that yield the one-one correlation. The axioms of extensionality and restriction are postulated in all versions of FST, but set construction axioms (nesting- axioms and union-axioms) vary; the assignment of finite integer values to formula_5 and formula_12 is implicit in the selected set construction axioms.\n\nFST is thereby not a single theory, but a name for a family of theories or versions of FST, where each version has its own set construction\naxioms and a unique model formula_22, which has a finite cardinality and all its sets have a finite rank and cardinality. FST axioms are formulated by first-order logic complemented by the member of relation formula_23. All versions of FST are first-order theories. In the axioms and definitions, symbols formula_24 are variables for sets, formula_25 are variables for both sets and urs, formula_26 is a variable for urs, and formula_27 denote individual urs of a model. The symbols for urs may appear only on the left side of formula_23. The symbols for sets may appear on both formula_29.\n\nAn applied FST model is always the minimal model which satisfies the applied axioms. This guarantees that those and only those elements exist in the applied model which are explicitly constructed by the selected axioms: only those urs exist which are stated to exist by assigning their number, and only those sets exist which are constructed by the selected axioms; no other elements exist in addition to these. This interpretation is needed, for typical FST axioms which generate e.g. exactly one set formula_30 do not otherwise exclude sets such as formula_31\n\nComplete FST models contain all permutations of sets and urs within the limits of formula_5 and formula_12. The axioms for complete FST models are extensionality, restriction, singleton sets and union of sets. Extensionality and restriction are axioms of all versions of FST, whereas the axiom for singleton sets is a provisional nesting-axiom (formula_34-axiom) and the axiom of union of sets is a provisional union-axiom (formula_35-axiom).\n\n\nComplete FST models formula_2 contain all permutations of sets and urs within limits of the assigned formula_5 and formula_12. The cardinality of formula_2 is its number of sets and urs formula_81.\nConsider some examples.\n\nThe recursive formula formula_104 gives the number of sets in formula_22:\n\nIn formula_109 there are formula_110 sets. \n\nIn formula_111 there are formula_112 sets.\n\nFST definitions should be understood as practical naming conventions which are used in stating that the elements of an applied FST model are or are not interrelated in specific ways. The definitions ought not be seen as axioms: only axioms entail existence of elements of an FST model, not definitions. In order to avoid conflicts (especially with axioms for incomplete FST models), the definitions must be subjugated to the applied axioms with the given formula_5 and formula_12. To illustrate a seeming conflict, suppose that formula_59 and formula_72 are the only sets of the applied model. The definition of intersection states that formula_117. As formula_56 does not exist in the applied model, the definition of intersection may appear to be an axiom. However, this is only apparent, for formula_56 does not have to exist in order to state that the only common element of formula_59 and formula_72 is formula_53, which is the function of the definition of intersection. Similarly with all definitions.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs transitive theories, Mereology and Boolean algebra are incapable of modeling nested structures. It is therefore intelligible to take FST or another intransitive theory as primary in modeling nested structures. However, also the functionality of transitive theories finds application in modeling nested structures. A large portion of the functionality of discrete mereology (DM) can be incorporated in FST, in terms of relations which mimic DM's relations. \n\nDM operates with structureless aggregates such as formula_344 that consists of urs formula_86, and formula_346 that consists of urs formula_347. DM's formula_348 and other relations defined in terms of formula_348 characterize relations between aggregates such as in formula_350 and formula_351. An axiomatization of DM and some definitions are given; some definitions are prefixed by formula_320 to distinguish them from FST's definitions with the same names.\n\n\nA large portion of the functionality of DM can be incorporated in FST by defining a relation analogous to DM's primitive formula_348 in terms of FST's membership. Although the identical symbol 'formula_348' is used with DM and the goal is to mimic DM functionality, FST's formula_348 may hold only between elements of a FST model, i.e., nothing is added to the applied FST models. As always, variables formula_375 denote FST sets and formula_26 denotes an ur-element. \n\nThe basic idea is that membership and FST's basic relations defined in terms of membership are structural, whereas FST's formula_348 and relations defined in terms of formula_348 are \"structure-independent\" or \"structure-neutral\". That formula_23 and formula_380 are structural means that they are sensitive to nested structures of sets: when it is known that formula_29 holds it is known that formula_26 is a member of formula_38 and exists in the first partition level of formula_38, and when it is known that formula_152 holds it is known that all members of formula_37 are members of formula_38 and exists in the first partition level of formula_38. In contrast, when it is known e.g. that formula_389 holds, it is not known on which specific level of formula_38 does formula_26 exist. formula_348 is characterized as structure-neutral because it allows formula_26 existing in whatever partition level of formula_38. formula_348 is applied in talking about structural FST sets in a structure-neutral way. Similarly as with formula_23, symbols for urs may appear only the left side of formula_348. Consider the definitions:\n\n\nWhen formula_389 holds, formula_26 exists in some level of set formula_38. For instance, formula_407 holds. When formula_401 holds, every ur in any level of formula_37 exists in some level of formula_38. For instance, formula_411 holds. Accordingly, formula_412 means that there is an ur in some level of formula_38 that is not in any level of formula_37. By the definition of proper part, e.g. formula_415 and formula_416 hold. Given any kind of a membership hierarchy whatsoever, such as formula_417, also formula_418 holds; given any kind of a subset hierarchy such as formula_419, also formula_420 holds; given any kind of a hierarchy which is a combination of membership and subset relations such as formula_421, also formula_418 holds. Note that formula_423 holds whereas formula_424 does not hold in all FST models, such as in the case where formula_425 and formula_426. Fine (2010, p. 579) notes that also chains of relations such as formula_427 may be used; such chains have now been given an axiomatic base.\n\nThe following translations of DM axioms into the terminology of FST show that FST's formula_348 is congenial with DM's axioms of reflexivity, transitivity and discreteness, but that DM extensionality must be modified by changing one of its equivalence relations into an implication. This reminds that FST sets are structural whereas DM aggretates are structureless.\n\n\nTo illustrate how FST's formula_348 can be applied as a structure-neutral relation in talking about structural sets, consider translations of examples (1-2) where only mereology is applied, into (1'-2') where FST's formula_348 is be applied together with membership.\n\n1. A handle is a part of a door; a door is a part of a house; but the handle is not a part of the house.:\n1'. A handle is a part of a door and a member of a door: handle formula_348 door; handle formula_23 door. The door is a part of a house and a member of the house: door formula_348 house; door formula_23 house. The handle is a part of the house but not a member of the house: handle formula_348 house; handle formula_459 house.:\nformula_460:\nformula_461\n\n2. A platoon is part of a company; a company is part of a battalion; but a platoon is not a part of a battalion.:\n2'. A platoon is part of a company and a member of a company; a company is a part of a battalion and a member of the battalion; a platoon is a part of a battalion but not a member of a battalion.:\n\nAs formula_348 has been defined, all DM relations that are defined in terms of formula_348 can be considered as FST definitions, including m-overlap, m-disjointness, m-intersection, m-union and m-difference.\n\n\nRegarding the definitions of formula_320-intersection, formula_320-union and formula_320-difference, in complete FST models all sets formula_63 exist. In some incomplete FST models some formula_63 do not exist. For instance, when formula_59 and formula_72 are the only sets in the applied model, the definition of formula_320-intersection states that formula_496, which makes the definition appear as an axiom. As indicated above, the definition is not interpreted as an axiom, but only as a formula which states that formula_53 is found in some level of both formula_59 and formula_72.\n"}
{"id": "3058220", "url": "https://en.wikipedia.org/wiki?curid=3058220", "title": "Free Boolean algebra", "text": "Free Boolean algebra\n\nIn mathematics, a free Boolean algebra is a Boolean algebra with a distinguished set of elements, called generators, such that:\n\nThe generators of a free Boolean algebra can represent independent propositions. Consider, for example, the propositions \"John is tall\" and \"Mary is rich\". These generate a Boolean algebra with four atoms, namely:\n\nOther elements of the Boolean algebra are then logical disjunctions of the atoms, such as \"John is tall and Mary is not rich, or John is not tall and Mary is rich\". In addition there is one more element, FALSE, which can be thought of as the empty disjunction; that is, the disjunction of no atoms.\n\nThis example yields a Boolean algebra with 16 elements; in general, for finite \"n\", the free Boolean algebra with \"n\" generators has 2 atoms, and therefore formula_1 elements.\n\nIf there are infinitely many generators, a similar situation prevails except that now there are no atoms. Each element of the Boolean algebra is a combination of finitely many of the generating propositions, with two such elements deemed identical if they are logically equivalent.\n\nIn the language of category theory, free Boolean algebras can be defined simply in terms of an adjunction between the category of sets and functions, Set, and the category of Boolean algebras and Boolean algebra homomorphisms, BA. In fact, this approach generalizes to any algebraic structure definable in the framework of universal algebra.\n\nAbove, we said that a free Boolean algebra is a Boolean algebra with a set of generators that behave a certain way; alternatively, one might start with a set and ask which algebra it generates. Every set \"X\" generates a free Boolean algebra \"FX\" defined as the algebra such that for every algebra \"B\" and function \"f\" : \"X\" → \"B\", there is a unique Boolean algebra homomorphism \"f\"′ : \"FX\" → \"B\" that extends \"f\". Diagrammatically,\n\nwhere \"i\" is the inclusion, and the dashed arrow denotes uniqueness. The idea is that once one chooses where to send the elements of \"X\", the laws for Boolean algebra homomorphisms determine where to send everything else in the free algebra \"FX\". If \"FX\" contained elements inexpressible as combinations of elements of \"X\", then \"f\"′ wouldn't be unique, and if the elements of \"X\" weren't sufficiently independent, then \"f\"′ wouldn't be well defined! It is easily shown that \"FX\" is unique (up to isomorphism), so this definition makes sense. It is also easily shown that a free Boolean algebra with generating set X, as defined originally, is isomorphic to \"FX\", so the two definitions agree.\n\nOne shortcoming of the above definition is that the diagram doesn't capture that \"f\"′ is a homomorphism; since it is a diagram in Set each arrow denotes a mere function. We can fix this by separating it into two diagrams, one in BA and one in Set. To relate the two, we introduce a functor \"U\" : BA → Set that \"forgets\" the algebraic structure, mapping algebras and homomorphisms to their underlying sets and functions.\n\nIf we interpret the top arrow as a diagram in BA and the bottom triangle as a diagram in Set, then this diagram properly expresses that every function \"f\" : \"X\" → \"UB\" extends to a unique Boolean algebra homomorphism \"f\"′ : \"FX\" → \"B\". The functor \"U\" can be thought of as a device to pull the homomorphism \"f\"′ back into Set so it can be related to \"f\".\n\nThe remarkable aspect of this is that the latter diagram is one of the various (equivalent) definitions of when two functors are adjoint. Our \"F\" easily extends to a functor Set → BA, and our definition of \"X\" generating a free Boolean algebra \"FX\" is precisely that \"U\" has a left adjoint \"F\".\n\nThe free Boolean algebra with κ generators, where κ is a finite or infinite cardinal number, may be realized as the collection of all clopen subsets of {0,1}, given the product topology assuming that {0,1} has the discrete topology. For each α<κ, the α\"th\" generator is the set of all elements of {0,1} whose α\"th\" coordinate is 1. In particular, the free Boolean algebra with formula_2 generators is the collection of all clopen subsets of a Cantor space, sometimes called the Cantor algebra. Surprisingly, this collection is countable. In fact, while the free Boolean algebra with \"n\" generators, \"n\" finite, has cardinality formula_1, the free Boolean algebra with formula_2 generators, as for any free algebra with formula_2 generators and countably many finitary operations, has cardinality formula_2.\n\nFor more on this topological approach to free Boolean algebra, see Stone's representation theorem for Boolean algebras.\n\n\n"}
{"id": "2229523", "url": "https://en.wikipedia.org/wiki?curid=2229523", "title": "Frölicher space", "text": "Frölicher space\n\nIn mathematics, Frölicher spaces extend the notions of calculus and smooth manifolds. They were introduced in 1982 by the mathematician Alfred Frölicher.\n\nA Frölicher space consists of a non-empty set \"X\" together with a subset \"C\" of Hom(R, \"X\") called the set of smooth curves, and a subset \"F\" of Hom(\"X\", R) called the set of smooth real functions, such that for each real function\n\nin \"F\" and each curve\n\nin \"C\", the following axioms are satisfied:\n\n\nLet \"A\" and \"B\" be two Frölicher spaces. A map\n\nis called \"smooth\" if for each smooth curve \"c\" in \"C\", \"m\".\"c\" is in \"C\". Furthermore, the space of all such smooth maps has itself the structure of a Frölicher space. The smooth functions on \n\nare the images of\n\n"}
{"id": "25525269", "url": "https://en.wikipedia.org/wiki?curid=25525269", "title": "GJMS operator", "text": "GJMS operator\n\nIn the mathematical field of differential geometry, the GJMS operators are a family of differential operators, that are defined on a Riemannian manifold. In an appropriate sense, they depend only on the conformal structure of the manifold. The GJMS operators generalize the Paneitz operator and the conformal Laplacian. The initials GJMS are for its discoverers Graham, Jenne, Mason & Sparling (1992).\n\nProperly, the GJMS operator on a conformal manifold of dimension \"n\" is a conformally invariant operator between the line bundle of conformal densities of weight for \"k\" a positive integer\nThe operators have leading symbol given by a power of the Laplace–Beltrami operator, and have lower order correction terms that ensure conformal invariance.\n\nThe original construction of the GJMS operators used the ambient construction of Charles Fefferman and Robin Graham. A conformal density defines, in a natural way, a function on the null cone in the ambient space. The GJMS operator is defined by taking density \"ƒ\" of the appropriate weight and extending it arbitrarily to a function \"F\" off the null cone so that it still retains the same homogeneity. The function Δ\"F\", where Δ is the ambient Laplace–Beltrami operator, is then homogeneous of degree , and its restriction to the null cone does not depend on how the original function \"ƒ\" was extended to begin with, and so is independent of choices. The GJMS operator also represents the obstruction term to a formal asymptotic solution of the Cauchy problem for extending a weight function off the null cone in the ambient space to a harmonic function in the full ambient space.\n\nThe most important GJMS operators are the \"critical\" GJMS operators. In even dimension \"n\", these are the operators \"L\" that take a true function on the manifold and produce a multiple of the volume form.\n"}
{"id": "2433632", "url": "https://en.wikipedia.org/wiki?curid=2433632", "title": "Gigantic prime", "text": "Gigantic prime\n\nA gigantic prime is a prime number with at least 10,000 decimal digits. \n\nThe term appeared in \"Journal of Recreational Mathematics\" in the article \"Collecting gigantic and titanic primes\" (1992) by Samuel Yates. Chris Caldwell, who continued Yates' collection in The Prime Pages, reports that he changed the requirement from Yates' original 5,000 digits to 10,000 digits, when he was asked to revise the article after the death of Yates. Few primes of that size were known then, but a modern personal computer can find many in a day.\n\nThe first discovered gigantic prime was the Mersenne prime 2 − 1. It has 13,395 digits and was found in 1979 by Harry L. Nelson and David Slowinski.\n\nThe smallest gigantic prime is 10 + 33603. It was proved prime in 2003 by Jens Franke, Thorsten Kleinjung and Tobias Wirth with their own distributed ECPP program. It was the largest ECPP proof at the time.\n\n\n"}
{"id": "690728", "url": "https://en.wikipedia.org/wiki?curid=690728", "title": "Harmonious coloring", "text": "Harmonious coloring\n\nIn graph theory, a harmonious coloring is a (proper) vertex coloring in which every pair of colors appears on at most one pair of adjacent vertices. The harmonious chromatic number χ(\"G\") of a graph \"G\" is the minimum number of colors needed for any harmonious coloring of \"G\".\n\nEvery graph has a harmonious coloring, since it suffices to assign every vertex a distinct color; thus χ(\"G\") ≤ |V(\"G\")|. There trivially exist graphs \"G\" with χ(\"G\") > χ(\"G\") (where χ is the chromatic number); one example is any path of length>2, which can be 2-colored but has no harmonious coloring with 2 colors.\n\nSome properties of χ(\"G\"):\n\nHarmonious coloring was first proposed by Harary and Plantholt (1982).\nStill very little is known about it.\n\n\n\n"}
{"id": "34371378", "url": "https://en.wikipedia.org/wiki?curid=34371378", "title": "Holomorphic curve", "text": "Holomorphic curve\n\nIn mathematics, in the field of complex geometry, a holomorphic curve in a complex manifold \"M\" is a non-constant holomorphic map \"f\" from the complex plane to \"M\".\n\nNevanlinna theory addresses the question of the distribution of values of a holomorphic curve in the complex projective line.\n\n"}
{"id": "23624339", "url": "https://en.wikipedia.org/wiki?curid=23624339", "title": "Indexed family", "text": "Indexed family\n\nIn mathematics, an indexed family is informally a collection of objects, each associated with an index from some index set. For example, a \"family of real numbers, indexed by the set of integers\" is a collection of real numbers, where a given function selects for each integer one real number (possibly the same).\n\nMore formally, an indexed family is a mathematical function together with its domain \"I\" and image \"X\". Often the elements of the set \"X\" are referred to as making up the family. In this view indexed families are interpreted as collections instead of as functions. The set \"I\" is called the \"index (set)\" of the family, and \"X\" is the \"indexed set\".\n\nDefinition. Let \"I\" and \"X\" be sets and formula_1 a surjective function, such that\nthen this establishes a family of elements in \"X\" indexed by \"I\" , which is denoted by (\"x\") or simply (\"x\"), when the index set is assumed to be known. Sometimes angle brackets or braces are used instead of parentheses, the latter with the risk of mixing-up families with sets.\n\nAn indexed family can be turned into a set by considering the set formula_3, that is, the image of \"I\" under \"x\". Since the mapping x is not required to be injective, there may exist formula_4 with formula_5 such that formula_6. Thus, formula_7 where |\"A\"| denotes the cardinality of the set \"A\".\n\nThe index set is not restricted to be countable, and, of course, a subset of a powerset may be indexed, resulting in an indexed family of sets. For the important differences in sets and families see below.\n\nWhenever index notation is used the indexed objects form a family. For example, consider the following sentence.\nHere (\"v\") denotes a family of vectors. The \"i\"-th vector \"v\" only makes sense with respect to this family, as sets are unordered and there is no \"i\"-th vector of a set. Furthermore, linear independence is only defined as the property of a collection; it therefore is important if those vectors are linearly independent as a set or as a family. \n\nIf we consider \"n\" = 2 and \"v\" = \"v\" = (1, 0), the \"set\" of them consists of only one element and is linearly independent, but the family contains the same element twice and is linearly dependent.\n\nSuppose a text states the following:\n\nAs in the previous example it is important that the rows of \"A\" are linearly independent as a family, not as a set. For Example, consider the matrix\nThe \"set\" of rows only consists of a single element (1, 1) and is linearly independent, but the matrix is not invertible. The \"family\" of rows contains two elements and is linearly dependent. The statement is therefore correct if it refers to the family of rows, but wrong if it refers to the set of rows. (The statement is also correct when \"the rows\" is interpreted as referring to a multiset, in which the elements are also kept distinct but which lacks some of the structure of an indexed family.)\n\nSurjective functions and families are formally equivalent, as any function \"f\" with domain \"I\" induces a family (\"f\"(\"i\")). In practice, however, a family is viewed as a collection, not as a function: being \"an element of a family\" is equivalent with being in the range of the corresponding function. A family contains any element exactly once, if and only if the corresponding function is injective. \n\nLike a set, a family is a container and any set \"X\" gives rise to a family (\"x\"). Thus any set naturally becomes a family. For any family (\"A\") there is the set of all elements {\"A\" | \"i\"∈\"I\"}, but this does not carry any information on multiple containment or the structure given by \"I\". Hence, by using a set instead of the family, some information might be lost.\n\nLet n be the finite set {1, 2, …, \"n\"}, where \"n\" is a positive integer.\n\nIndex sets are often used in sums and other similar operations. For example, if (\"a\") is a family of numbers, the sum of all those numbers is denoted by\n\nWhen (\"A\") is a family of sets, the union of all those sets is denoted by\n\nLikewise for intersections and cartesian products.\n\nA family (\"B\") is a subfamily of a family (\"A\"), if and only if \"J\" is a subset of \"I\" and for all \"i\" in \"J\" \n\nThe analogous concept in category theory is called a diagram. A diagram is a functor giving rise to an indexed family of objects in a category C, indexed by another category J, and related by morphisms depending on two indices.\n\n\n"}
{"id": "6555321", "url": "https://en.wikipedia.org/wiki?curid=6555321", "title": "International Conference on Rewriting Techniques and Applications", "text": "International Conference on Rewriting Techniques and Applications\n\nRewriting Techniques and Applications (RTA) is an annual international academic conference on the topic of rewriting. It covers all aspects of rewriting, including termination, equational reasoning, theorem proving, higher-order rewriting, unification and the lambda calculus. The conference consists of peer-reviewed papers with the proceedings published by Springer in the LNCS series until 2009, and since then in the LIPIcs series published by the Leibniz-Zentrum für Informatik. Several rewriting-related workshops are also affiliated with RTA.\n\nThe first RTA was held in Dijon, France in September 1983. RTA takes part in the federated conferences Federated Logic Conference (FLoC), and Rewriting, Deduction, and Programming (RDP).\n\n"}
{"id": "28878050", "url": "https://en.wikipedia.org/wiki?curid=28878050", "title": "Isoelastic function", "text": "Isoelastic function\n\nIn mathematical economics, an isoelastic function, sometimes constant elasticity function, is a function that exhibits a constant elasticity, i.e. has a constant elasticity coefficient. The elasticity is the ratio of the percentage change in the dependent variable to the percentage causative change in the independent variable, in the limit as the changes approach zero in magnitude. \n\nFor an elasticity coefficient formula_1 (which can take on any real value), the function's general form is given by\n\nwhere formula_3 and formula_1 are constants. The elasticity is by definition\n\nwhich for this function simply equals \"r\".\n\nAn example in microeconomics is the constant elasticity demand function, in which \"p\" is the price of a product and \"D\"(\"p\") is the resulting quantity demanded by consumers. For most goods the elasticity \"r\" (the responsiveness of quantity demanded to price) is negative, so it can be convenient to write the constant elasticity demand function with a negative sign on the exponent, in order for the coefficient formula_1 to take on a positive value:\nwhere formula_8 is now interpreted as the unsigned magnitude of the responsiveness.\n\nThe constant elasticity function is also used in the theory of choice under risk aversion, which usually assumes that risk-averse decision-makers maximize the expected value of a concave von Neumann-Morgenstern utility function. In this context, with a constant elasticity of utility with respect to, say, wealth, optimal decisions on such things as shares of stocks in a portfolio are independent of the scale of the decision-maker's wealth. The constant elasticity utility function in this context is generally written as\n\nwhere \"x\" is wealth and formula_10 is the elasticity, with formula_11 , formula_12 ≠ 1 referred to as the constant coefficient of relative risk aversion (with risk aversion approaching infinity as formula_13 → ∞).\n\n\n"}
{"id": "6395589", "url": "https://en.wikipedia.org/wiki?curid=6395589", "title": "Karatsuba algorithm", "text": "Karatsuba algorithm\n\nThe Karatsuba algorithm is a fast multiplication algorithm. It was discovered by Anatoly Karatsuba in 1960 and published in 1962. It reduces the multiplication of two \"n\"-digit numbers to at most formula_1 single-digit multiplications in general (and exactly formula_2 when \"n\" is a power of 2). It is therefore faster than the classical algorithm, which requires \"n\" single-digit products. For example, the Karatsuba algorithm requires 3 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (\"n\" = 1024 = 2), whereas the classical algorithm requires (2) = 1,048,576 (a speedup of 17.75 times).\n\nThe Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic \"grade school\" algorithm.\nThe Toom–Cook algorithm is a faster generalization of Karatsuba's method, and the Schönhage–Strassen algorithm is even faster, for sufficiently large \"n\".\n\nThe standard procedure for multiplication of two \"n\"-digit numbers requires a number of elementary operations proportional to formula_3, or formula_4 in big-O notation. Andrey Kolmogorov conjectured that the classical algorithm was \"asymptotically optimal,\" meaning that any algorithm for that task would require formula_5 elementary operations.\n\nIn 1960, Kolmogorov organized a seminar on mathematical problems in cybernetics at the Moscow State University, where he stated the formula_5 conjecture and other problems in the complexity of computation. Within a week, Karatsuba, then a 23-year-old student, found an algorithm (later it was called \"divide and conquer\") that multiplies two \"n\"-digit numbers in formula_7 elementary steps, thus disproving the conjecture. Kolmogorov was very agitated about the discovery; he communicated it at the next meeting of the seminar, which was then terminated. Kolmogorov did some lectures on the Karatsuba result at the conferences all over the world (see, for example, \"Proceedings of the international congress of mathematicians 1962\", pp. 351–356, and also\n\"6 Lectures delivered at the International Congress of Mathematicians in Stockholm, 1962\") and published the method in 1962, in the Proceedings of the USSR Academy of Sciences. The article had been written by Kolmogorov and contained two results on multiplication, Karatsuba's algorithm and a separate result by Yuri Ofman; it listed \"A. Karatsuba and Yu. Ofman\" as the authors. Karatsuba only became aware of the paper when he received the reprints from the publisher.\n\nThe basic step of Karatsuba's algorithm is a formula that allows one to compute the product of two large numbers formula_8 and formula_9 using three multiplications of smaller numbers, each with about half as many digits as formula_8 or formula_9, plus some additions and digit shifts. This basic step is, in fact, a generalization of Gauss's complex multiplication algorithm, where the imaginary unit is replaced by a power of the base.\n\nLet formula_8 and formula_9 be represented as formula_14-digit strings in some base formula_15. For any positive integer formula_16 less than formula_14, one can write the two given numbers as\n\nwhere formula_20 and formula_21 are less than formula_22. The product is then\n\nwhere\n\nThese formulae require four multiplications and were known to Charles Babbage. Karatsuba observed that formula_28 can be computed in only three multiplications, at the cost of a few extra additions. With formula_29 and formula_30 as before one can calculate\n\nwhich holds, since\n\nAn issue that occurs, however, when computing formula_34 is that the above computation of formula_35 and formula_36 may result in overflow (will produce a result in the range formula_37), which require a multiplier having one extra bit. This can be avoided by noting that\n\nThis computation of formula_44 and formula_45 will produce a result in the range of formula_46. This method may produce negative numbers, which require one extra bit to encode signedness, and would still require one extra bit for the multiplier. However, one way to avoid this is to record the sign and then use the absolute value of formula_44 and formula_45 to perform an unsigned multiplication, after which the result may be negated when both signs originally differed. Another advantage is that even though formula_49 may be negative, the final computation of formula_34 only involve additions.\n\nTo compute the product of 12345 and 6789, where \"B\" = 10, choose \"m\" = 3. Then we decompose the input operands using the resulting base (\"B\" = \"1000\"), as:\n\nOnly three multiplications, which operate on smaller integers, are used to compute three partial results:\n\nWe get the result by just adding these three partial results, shifted accordingly (and then taking carries into account by decomposing these three inputs in base \"1000\" like for the input operands):\n\nNote that the intermediate third multiplication operates on an input domain which is less than two times larger than for the two first multiplications, its output domain is less than four times larger, and base-\"1000\" carries computed from the first two multiplications must be taken into account when computing these two subtractions.\n\nIf \"n\" is four or more, the three multiplications in Karatsuba's basic step involve operands with fewer than \"n\" digits. Therefore, those products can be computed by recursive calls of the Karatsuba algorithm. The recursion can be applied until the numbers are so small that they can (or must) be computed directly.\n\nIn a computer with a full 32-bit by 32-bit multiplier, for example, one could choose \"B\" = 2 = , and store each digit as a separate 32-bit binary word. Then the sums \"x\" + \"x\" and \"y\" + \"y\" will not need an extra binary word for storing the carry-over digit (as in carry-save adder), and the Karatsuba recursion can be applied until the numbers to multiply are only one-digit long.\n\nKaratsuba's original formula and other generalizations are themselves symmetric. For example, \nthe following formula computes \nwith 6 multiplications in formula_52, where formula_53 is the Galois field with two elements 0 and 1.\n\nwhere formula_55 and formula_56.\nWe note that addition and subtraction are the same in fields of characteristic 2.\n\nThis formula is symmetrical, namely, it does not change if we exchange formula_57 and formula_58 in formula_59 and formula_60.\n\nBased on the second Generalized division algorithms\n, Fan et al. found the following asymmetric formula:\n\nwhere \nformula_62 and \nformula_63.\n\nIt is asymmetric because we can obtain the following new formula by exchanging formula_57 and formula_58 in \nformula_66 and formula_67.\n\nwhere \nformula_69 and formula_70.\n\nKaratsuba's basic step works for any base \"B\" and any \"m\", but the recursive algorithm is most efficient when \"m\" is equal to \"n\"/2, rounded up. In particular, if \"n\" is 2, for some integer \"k\", and the recursion stops only when \"n\" is 1, then the number of single-digit multiplications is 3, which is \"n\" where \"c\" = log3.\n\nSince one can extend any inputs with zero digits until their length is a power of two, it follows that the number of elementary multiplications, for any \"n\", is at most formula_71.\n\nSince the additions, subtractions, and digit shifts (multiplications by powers of \"B\") in Karatsuba's basic step take time proportional to \"n\", their cost becomes negligible as \"n\" increases. More precisely, if \"t\"(\"n\") denotes the total number of elementary operations that the algorithm performs when multiplying two \"n\"-digit numbers, then\n\nfor some constants \"c\" and \"d\". For this recurrence relation, the master theorem for divide-and-conquer recurrences gives the asymptotic bound formula_73.\n\nIt follows that, for sufficiently large \"n\", Karatsuba's algorithm will perform fewer shifts and single-digit additions than longhand multiplication, even though its basic step uses more additions and shifts than the straightforward formula. For small values of \"n\", however, the extra shift and add operations may make it run slower than the longhand method. The point of positive return depends on the computer platform and context. As a rule of thumb, Karatsuba's method is usually faster when the multiplicands are longer than 320–640 bits.\n\nIn practice, the choice of the optimal base case is machine specific.\n\n"}
{"id": "31821456", "url": "https://en.wikipedia.org/wiki?curid=31821456", "title": "Kurt Leichtweiss", "text": "Kurt Leichtweiss\n\nKurt Leichtweiss (March 2, 1927 in Villingen, Baden – June 23, 2013) was a mathematician specializing in convex and differential geometry.\n\nLeichtweiss was born in Villingen-Schwenningen. In 1944, while still in high school he traveled to the Mathematical Institute where his mathematical interests were encouraged. He studied at the University of Freiburg and the ETH Zurich. He was a student of Emanuel Sperner and Wilhelm Süss. He was then a lecturer in Freiburg and in 1963 became a professor at TU Berlin. From 1970 until his retirement in 1995, he was a professor at the University of Stuttgart.\n\n\n"}
{"id": "4734121", "url": "https://en.wikipedia.org/wiki?curid=4734121", "title": "LOGCFL", "text": "LOGCFL\n\nIn computational complexity theory, LOGCFL is the complexity class that contains all decision problems that can be reduced in logarithmic space to a context-free language. This class is situated between NL and AC1, in the sense that it contains the former and is contained in the latter. Problems that are complete for LOGCFL include many problems whose instances can be characterized by acyclic hypergraphs:\n\n"}
{"id": "757745", "url": "https://en.wikipedia.org/wiki?curid=757745", "title": "List of network theory topics", "text": "List of network theory topics\n\nNetwork theory is an area of applied mathematics. \n\nThis page is a list of network theory topics.\nSee also List of graph theory topics.\n\n\n\n\n\n"}
{"id": "58389748", "url": "https://en.wikipedia.org/wiki?curid=58389748", "title": "List of things named after Alan Turing", "text": "List of things named after Alan Turing\n\nThis is a list of things named after Alan Turing. Alan Turing (1912–1954), a pioneer computer scientist, mathematician, and philosopher, is the eponym of all of the things (and topics) listed below.\n"}
{"id": "43333", "url": "https://en.wikipedia.org/wiki?curid=43333", "title": "Measurable space", "text": "Measurable space\n\nIn mathematics, a measurable space or Borel space is a basic object in measure theory. It consists of a set and a formula_1-algebra on this set and provides information about the sets that will be measured.\nConsider a nonempty set formula_2 and a formula_1-algebra formula_4 on formula_2. Then the tuple formula_6 is called a measurable space.\n\nNote that in contrast to a measure space, no measure is needed for a measurable space.\n\nLook at the set\nOne possible formula_1-Algebra would be\nThen formula_10 is a measurable space. Another possible formula_11-algebra would be the power set on formula_2:\nWith this, a second measurable space on the set formula_2 is given by formula_15.\n\nIf formula_2 is finite or countable infinite, the formula_11-algebra is most of the times the power set on formula_2, so formula_19. This leads to the measurable space formula_20.\n\nIf formula_2 is a topological space, the formula_11-algebra is most commonly the Borel formula_1-algebra formula_24, so formula_25. This leads to the measurable space formula_26 that is common for all topological spaces such as the real numbers formula_27.\n\nThe term Borel space is used for different types of measurable spaces. It can refer to\n"}
{"id": "5456797", "url": "https://en.wikipedia.org/wiki?curid=5456797", "title": "Method of moments (probability theory)", "text": "Method of moments (probability theory)\n\nIn probability theory, the method of moments is a way of proving convergence in distribution by proving convergence of a sequence of moment sequences. Suppose \"X\" is a random variable and that all of the moments\n\nexist. Further suppose the probability distribution of \"X\" is completely determined by its moments, i.e., there is no other probability distribution with the same sequence of moments\n(cf. the problem of moments). If\n\nfor all values of \"k\", then the sequence {\"X\"} converges to \"X\" in distribution.\n\nThe method of moments was introduced by Pafnuty Chebyshev for proving the central limit theorem; Chebyshev cited earlier contributions by Irénée-Jules Bienaymé. More recently, it has been applied by Eugene Wigner to prove Wigner's semicircle law, and has since found numerous applications in the theory of random matrices.\n"}
{"id": "39536649", "url": "https://en.wikipedia.org/wiki?curid=39536649", "title": "Miklós Bóna", "text": "Miklós Bóna\n\nMiklós Bóna (born October 6, 1967, in Székesfehérvár) is an American mathematician of Hungarian origin.\n\nBóna completed his undergraduate studies in Budapest and Paris, then obtained his Ph.D. at MIT in 1997 as a student of Richard P. Stanley. Since 1999, he has taught at the University of Florida, where in 2010 he was inducted to the Academy of Distinguished Teaching Scholars.\n\nBóna's main fields of research include the combinatorics of permutations, as well as enumerative and analytic combinatorics. Since 2010, he has been one of the editors-in-chief of the Electronic Journal of Combinatorics.\n\n\n"}
{"id": "7315901", "url": "https://en.wikipedia.org/wiki?curid=7315901", "title": "Milü", "text": "Milü\n\nThe name Milü (; \"close ratio\"), also known as Zulü (Zu's ratio), is given to an approximation to (pi) found by Chinese mathematician and astronomer, Zǔ Chōngzhī (祖沖之). Using Liu Hui's algorithm (which is based on the areas of regular polygons approximating a circle), Zu famously computed to be between 3.1415926 and 3.1415927 and gave two rational approximations of , and , naming them respectively Yuelü 约率 (approximate ratio) and Milü.\n\nThe accuracy of Milü to the true value of can be explained using the continued fraction expansion of, the first few terms of which are formula_2. A property of continued fractions is that truncating the expansion of a given number at any point will give the \"best rational approximation\" to the number. To obtain Milü, truncate the continued fraction expansion of immediately before the term 292; that is, is approximated by the finite continued fraction formula_3, which is equivalent to Milü. Since 292 is an unusually large term in a continued fraction expansion, this convergent will be very close to the true value of .\n\nAn easy mnemonic helps memorize this useful fraction by writing down each of the first three odd numbers twice: 1 1 3 3 5 5, then dividing the decimal number represented by the last 3 digits by the decimal number given by the first three digits. Alternatively, ≈ .\n\nZu's contemporary calendarist and mathematician He Chengtian () invented a fraction interpolation method called \"harmonization of the divisor of the day\" to obtain a closer approximation by iteratively adding the numerators and denominators of a \"weak\" fraction and a \"strong\" fraction. Zu Chongzhi's approximation ≈ can be obtained with He Chengtian's method.\n\n\n"}
{"id": "3105466", "url": "https://en.wikipedia.org/wiki?curid=3105466", "title": "Petr Vopěnka", "text": "Petr Vopěnka\n\nPetr Vopěnka (16 May 1935 – 20 March 2015) was a Czech mathematician. In the early seventies, he developed alternative set theory (i.e. alternative to the classical Cantor theory), which he subsequently developed in a series of articles and monographs. Vopěnka’s name is associated with many mathematical achievements, including Vopěnka's principle.\nSince the mid-eighties he concerned himself with philosophical questions of mathematics (particularly vis-à-vis Husserlian phenomenology).\n\nVopěnka served as the Minister of Education of the Czech Republic (then part of Czechoslovakia) from 1990 to 1992 within the government of Prime Minister Petr Pithart.\n\nPetr Vopěnka grew up in small town of Dolní Kralovice. After finishing gymnasium in Ledeč nad Sázavou in 1953 he went to study mathematics at the Mathematics and Physics Faculty of Charles University in Prague, graduating in 1958. In 1962 he was made Candidate of Sciences (CSc) and in 1967 Doctor of Science (DrSc). His advisors were Eduard Čech and Ladislav Rieger.\n\nStarting in 1958 Vopěnka taught at the Mathematics and Physics Faculty, since 1964 as lecturer, since 1965 as senior lecturer. In 1968 he was made professor but was prevented to take this title until 1990 due to political reasons. Between 1966 and 69 Vopěnka served as Vice Dean of the faculty.\n\nIn 1967 Vopěnka became head of the newly established Department of Mathematical Logic. The department was abolished in 1970 and Vopěnka, though allowed to stay at the university, fell into disfavour with the regime which limited his contacts with foreign mathematicians. During the 1970s and 1980s he concentrated on philosophy and history of mathematics and on phenomenology of infinity.\n\nAfter the Velvet Revolution, in January 1990, Vopěnka became Deputy Rector of the Charles University. Between June 1990 - July 1992 he served as Minister of Education of the Czech Republic (then part of Czechoslovakia). In this position he, without much of success and facing protests from the teachers, attempted to institute school reforms.\n\nIn 1992 the Department of Mathematical Logic was reopened and Vopěnka became its head. In 2000 he retired from the Charles University and the department was closed. Until 2009 Vopěnka worked as a professor at the Jan Evangelista Purkyně University in Ústí nad Labem, in the Department of Mathematics of the Faculty of Science.\n\nPetr Vopěnka also participated in translation and publishing of early mathematical texts (such as works of Euclid and Al-Khwarezmi) into Czech language and then he worked at the Department of Philosophy and Department of Interdisciplinary Activities, University of West Bohemia in Pilsen.\n\n\n\n\n\n"}
{"id": "11081803", "url": "https://en.wikipedia.org/wiki?curid=11081803", "title": "Phase retrieval", "text": "Phase retrieval\n\nPhase retrieval is the process of algorithmically finding solutions to the phase problem. Given a complex signal formula_1, of amplitude formula_2, and phase formula_3:\n\nwhere \"x\" is an \"M\"-dimensional spatial coordinate and \"k\" is an \"M\"-dimensional spatial frequency coordinate, phase retrieval consists in finding the phase that for a measured amplitude satisfies a set of constraints. Important applications of phase retrieval include X-ray crystallography, transmission electron microscopy and coherent diffractive imaging, for which formula_5. (Fienup 1982:2759) Uniqueness theorems for both 1-D and 2-D cases of the phase retrieval problem, including the phaseless 1-D inverse scattering problem, were proved by Klibanov and his collaborators (see References).\n\nThe error reduction is a generalisation of the Gerchberg–Saxton algorithm. It solves for formula_6 from measurements of formula_7. It uses iteration of a four-step process. For the formula_8'th iteration the steps are as follows:\n\nStep (1): formula_9, formula_10, and formula_11 are estimates of, respectively, formula_12, formula_13 and formula_6. In the first step formula_11 undergoes Fourier transformation:\n\nStep (2): The experimental value of formula_17, calculated from the diffraction pattern via the signal equation, is then substituted for formula_18, giving an estimate of the Fourier transformation:\n\nwhere the ' denotes that the object is temporary, for further calculations.\n\nStep (3): the estimate of the Fourier transformation formula_20 is inverse Fourier transformed:\n\nStep (4): formula_22 then must be changed so that the new estimate of the object, formula_23 satisfies the object constraints. formula_23 is therefore defined piecewise as:\nwhere formula_26 is the domain in which formula_22 does not satisfy the object constraints. A new estimate formula_23 is obtained and the four step process can be repeated iteratively.\n\nThis process is continued until both the Fourier constraint and object constraint are satisfied. Theoretically, the process will always lead to a convergence (Fienup 1982:2761), but the large number of iterations needed to produce a satisfactory image (generally >2000) results in the error-reduction algorithm being unsuitably inefficient for sole use in practical applications.\n\nThe hybrid input-output algorithm is a modification of the error-reduction algorithm - the first three stages are identical. However, formula_11 no longer acts as an estimate of formula_6, but the input function corresponding to the output function formula_22, which is an estimate of formula_6 (Fienup 1982:2762). In the fourth step, when the function formula_22 violates the object constraints, the value of formula_34 is forced towards zero, but optimally not to zero. The chief advantage of the hybrid input-output algorithm is that the function formula_11 contains feedback information concerning previous iterations, reducing the probability of stagnation.\n\nHere formula_37 is a feedback parameter which can take a value between 0 and 1. For most applications, formula_38 gives optimal results.\n\nFor a two dimensional phase retrieval problem, there is a degeneracy of solutions as formula_6 and its conjugate formula_40 have the same Fourier modulus. This leads to \"image twinning\" in which the phase retrieval algorithm stagnates producing an image with features of both the object and its conjugate (Fienup and Wackerman, 1986:1900). The shrinkwrap technique periodically updates the estimate of the support by low-pass filtering the current estimate of the object amplitude (by convolution with a Gaussian) and applying a threshold, leading to a reduction in the image ambiguity (Marchesini et al., 2003).\n\n\n"}
{"id": "47366237", "url": "https://en.wikipedia.org/wiki?curid=47366237", "title": "Pierre de Carcavi", "text": "Pierre de Carcavi\n\nPierre de Carcavi was born in about 1603, in Lyon, France and died in Paris in April 1684. He was a secretary of the National Library of France under Louis XIV. Carcavi was a French mathematician.\n\nCarcavi is known for his correspondence with Pierre de Fermat, Blaise Pascal, Christian Huygens, Galileo Galilei, Marin Mersenne, Evangelista Torricelli and René Descartes.\n"}
{"id": "47172571", "url": "https://en.wikipedia.org/wiki?curid=47172571", "title": "Positive real numbers", "text": "Positive real numbers\n\nIn mathematics, the set of positive real numbers, formula_1, is the subset of those real numbers that are greater than zero. The non-negative real numbers, formula_2, also include zero. The symbols formula_3 and formula_4 are ambiguously used for either of these, so it safer to always specify which.\n\nIn a complex plane, formula_5 is identified with the positive real axis and is usually drawn as a horizontal ray. This ray is used as reference in the polar form of a complex number. The real positive axis corresponds to complex numbers formula_6 with argument formula_7.\n\nThe non-negative reals serve as the range for metrics, norms, and measures in mathematics.\n\nThe set formula_5 is closed under addition, multiplication, and division. It inherits a topology from the real line and, thus, has the structure of a multiplicative topological group or of an additive topological semigroup. Including 0, the set formula_9 has a semiring structure (0 is the additive identity), known as the probability semiring; taking logarithms (with a choice of base giving a logarithmic unit) gives an isomorphism with the log semiring (with 0 corresponding to −∞), and its units (the finite numbers, excluding −∞) correspond to the positive real numbers.\n\nFor a given positive real number \"x\", the sequence {\"x\"} of its integral powers has three different fates: When \"x\" ∈ (0, 1) the limit is zero and when \"x\" ∈ (1, ∞) the limit is infinity, while the sequence is constant for \"x\" = 1. The case \"x\" > 1 thus leads to an unbounded sequence.\n\nformula_10 and the multiplicative inverse function exchanges the intervals. The functions floor, formula_11, and excess, formula_12, have been used to describe an element formula_13 as a continued fraction formula_14 which is a sequence of integers obtained from the floor function after the excess has been reciprocated. For formula_15 the sequence terminates with an exact fractional expression of formula_16, and for quadratic irrational formula_16 the sequence becomes a periodic continued fraction.\n\nIn the study of classical groups, for every formula_18, the determinant gives a map from matrices over the reals to the real numbers formula_19 Restricting to invertible matrices gives a map from the general linear group to non-zero real numbers: formula_20. Restricting to matrices with a positive determinant gives the map formula_21; interpreting the image as a quotient group by the normal subgroup relation expresses the positive reals as a Lie group.\n\nIf formula_22 is an interval, then formula_23 determines a measure on certain subsets of formula_5, corresponding to the pullback of the usual Lebesgue measure on the real numbers under the logarithm: it is the length on the logarithmic scale. In fact, it is an invariant measure with respect to multiplication formula_25 by a formula_26, just as the Lebesgue measure is invariant under addition. In the context of topological groups, this measure is an example of a Haar measure.\n\nThe utility of this measure is shown in its use for describing stellar magnitudes and noise levels in decibels, among other applications of the logarithmic scale. For purposes of international standards ISO 80000-3 the dimensionless quantities are referred to as levels.\n"}
{"id": "2070635", "url": "https://en.wikipedia.org/wiki?curid=2070635", "title": "Q-theta function", "text": "Q-theta function\n\nIn mathematics, the \"q\"-theta function is a type of \"q\"-series. It is given by\n\nwhere one takes 0 ≤ |\"q\"| < 1. It obeys the identities\n\nIt may also be expressed as:\n\nwhere formula_4 is the q-Pochhammer symbol.\n\n"}
{"id": "29360130", "url": "https://en.wikipedia.org/wiki?curid=29360130", "title": "Robert James Marsh", "text": "Robert James Marsh\n\nRobert James Marsh is a mathematician working in the areas of cluster algebras, representation theory of finite-dimensional algebras, homological algebra, tilting theory, quantum groups, algebraic groups, Lie algebras and Coxeter groups. Marsh currently works at the University of Leeds as a Professor of pure mathematics. He also holds the position of EPSRC Leadership Fellow. In addition to his duties at the University of Leeds, Marsh is also an editor of the Glasgow Mathematical Journal.\n\nIn July 2009, Marsh was awarded the Whitehead Prize by the London Mathematical Society for his work on representation theory, and especially for his research on cluster categories and cluster algebras.\n\n"}
{"id": "563620", "url": "https://en.wikipedia.org/wiki?curid=563620", "title": "Septentrional", "text": "Septentrional\n\nSeptentrional, meaning \"of the north\", is a word rarely used in English, but is commonly used in Latin and in the Romance languages. The term \"septentrional\" usually is found on maps, mostly those made before 1700. Early maps of North America often refer to the northern- and northwestern-most unexplored areas of the continent as at the \"Septentrional\" and as \"America Septentrionalis\", sometimes with slightly varying spellings.\n\nThe term \"septentrional\" is the adjectival form of the Latin noun \"septentriones\", which refers to the seven stars of the \"Big Dipper\" asterism, the \"Septentrion\".\n\nThe \"Oxford English Dictionary\" gives the etymology of \"septentrional\" as:\n\n\"Septentrional\" is more or less synonymous with the term \"boreal\". The constellation Ursa Major, containing the Big Dipper, or Plough, dominates the skies of the North. There does not appear to be a comparable term linking the regions of the South with some prominent astral feature of the Southern sky. The usual antonym for \"septentrional\" is the term \"meridional\", which refers to the noonday sun, not to a celestial feature in the Southern sky.\n\nThe novelist Gene Wolfe used the word \"septentrional\" in \"The Book of the New Sun\", as the name of a praetorian guard, who are especially close to the ruler, hence are part of the palace inner-circle; such stars are close to the polar star. \nThe term, sometimes abbreviated to \"Sep.\", was used in historical astronomy to indicate the northern direction on the celestial globe, together with Meridional (\"Mer.\") for southern, Oriental (\"Ori.\") for eastern and Occidental (\"Occ.\") for western.\nIn France, the term septentrional refers to the Northern stretch of the Côtes du Rhône AOC winemaking region. The Northern Rhône, or septentrional, runs along the Rhône river from Vienne in the north, to Montélimar in the south. It includes the eight crus: Côte Rôtie, Condrieu, Château-Grillet, Hermitage, Saint-Joseph, Crozes-Hermitage, Cornas and Saint-Péray. The Southern Rhône is referred to as the meridional (\"Rhône méridionale\"), and extends from Montélimar in the north, to Avignon in the south.\n\n"}
{"id": "56722423", "url": "https://en.wikipedia.org/wiki?curid=56722423", "title": "Sigma (operations research)", "text": "Sigma (operations research)\n\nSigma (Science in General Management) was a limited company established by Henry Novy, Stafford Beer and Roger Eddison in 1961. It sold operational research OR in the United Kingdom and overseas. Beer was responsible for the cybernetic models of organisation and corporate planning which were the firm's specially. Eddison was the Operations Director.\n\nThe consultancy arose following discussions between the Société d’Économie et de Mathématiques Appliquées (SEMA) and Martech Consultants Limited. Novy, who ran Maretch, was given the job of selecting the people to run the company, inviting several OR practitioners to come to Paris for an interview. After he selected Beer, Beer only agreed if Eddison would also join him. This was arranged and the company was launched with a reception at the Connaught Hotel in October 1961.\nOver the next five years Sigma developed into a substantial organisation providing services to several leading companies across the UK as well as for six government departments. It also took on contracts in the United States and several developing countries. Amongst the diverse areas it worked in were energy, for the Gas Council, transportation, for British Rail and the Port of London Authority, shipbuilding, for the Geddes Committee, education for Yugoslavia, tourism, for Israel, nationalised industry in South America, and distribution for a variety of industrial firms. \n\nSigma, Martech, and Proplan were amalgamated to form what afterwards became the Metra Consulting Group in the United Kingdom.\n"}
{"id": "885651", "url": "https://en.wikipedia.org/wiki?curid=885651", "title": "Stationary point", "text": "Stationary point\n\nIn mathematics, particularly in calculus, a stationary point of a differentiable function of one variable is a point on the graph of the function where the function's derivative is zero. Informally, it is a point where the function \"stops\" increasing or decreasing (hence the name). Some authors treat this term as synonymous with \"critical point,\" but for other authors the latter is a broader term.\n\nFor a differentiable function of several real variables, a stationary (critical) point is a point on the surface of the graph where all its partial derivatives are zero (equivalently, the gradient is zero).\n\nStationary points are easy to visualize on the graph of a function of one variable: they correspond to the points on the graph where the tangent is horizontal (i.e., parallel to the -axis). For a function of two variables, they correspond to the points on the graph where the tangent plane is parallel to the plane.\n\nA turning point is a point at which the derivative changes sign. A turning point may be either a relative maximum or a relative minimum (also known as local minimum and maximum). If the function is differentiable, then a turning point is a stationary point; however not all stationary points are turning points. If the function is twice differentiable, the stationary points that are not turning points are horizontal inflection points. For example, the function formula_1 has a stationary point at x=0, which is also an inflection point, but is not a turning point.\n\nIsolated stationary points of a formula_2 real valued function formula_3 are classified into four kinds, by the first derivative test:\n\n\n\nThe first two options are collectively known as \"local extrema\". Similarly a point that is either a global (or absolute) maximum or a global (or absolute) minimum is called a global (or absolute) extremum. The last two options—stationary points that are \"not\" local extremum—are known as saddle points.\n\nBy Fermat's theorem, global extrema must occur (for a formula_2 function) on the boundary or at stationary points.\n\nDetermining the position and nature of stationary points aids in curve sketching of differentiable functions. Solving the equation \"f'\"(\"x\") = 0 returns the \"x\"-coordinates of all stationary points; the \"y\"-coordinates are trivially the function values at those \"x\"-coordinates.\nThe specific nature of a stationary point at \"x\" can in some cases be determined by examining the second derivative \"f''\"(\"x\"):\n\nA more straightforward way of determining the nature of a stationary point is by examining the function values between the stationary points (if the function is defined and continuous between them).\n\nA simple example of a point of inflection is the function \"f\"(\"x\") = \"x\". There is a clear change of concavity about the point \"x\" = 0, and we can prove this by means of calculus. The second derivative of \"f\" is the everywhere-continuous 6\"x\", and at \"x\" = 0, \"f\"′′ = 0, and the sign changes about this point. So \"x\" = 0 is a point of inflection.\n\nMore generally, the stationary points of a real valued function formula_5 are those\npoints x where the derivative in every direction equals zero, or equivalently, the gradient is zero.\n\nFor the function \"f\"(\"x\") = \"x\" we have \"f<nowiki>'</nowiki>\"(0) = 0 and \"f<nowiki>\"</nowiki>\"(0) = 0. Even though \"f<nowiki>\"</nowiki>\"(0) = 0, this point is not a point of inflection. The reason is that the sign of \"f\"'(\"x\") changes from negative to positive.\n\nFor the function \"f\"(\"x\") = sin(\"x\") we have \"f<nowiki>'</nowiki>\"(0) ≠ 0 and \"f<nowiki>\"</nowiki>\"(0) = 0. But this is not a stationary point, rather it is a point of inflection. This is because the concavity changes from concave downwards to concave upwards and the sign of \"f\"'(\"x\") does not change; it stays positive.\n\nFor the function \"f\"(\"x\") = x we have \"f<nowiki>'</nowiki>\"(0) = 0 and \"f<nowiki>\"</nowiki>\"(0) = 0. This is both a stationary point and a point of inflection. This is because the concavity changes from concave downwards to concave upwards and the sign of \"f<nowiki>'</nowiki>\"(\"x\") does not change; it stays positive.\n\n\n"}
{"id": "7961420", "url": "https://en.wikipedia.org/wiki?curid=7961420", "title": "Streamline diffusion", "text": "Streamline diffusion\n\nStreamline diffusion, given an advection-diffusion equation, refers to all diffusion going on along the advection direction.\n\nIn an advection equation, for simplicity assume: formula_1, and formula_2\n\nAdding a diffusion term, again for simplicity, assume the diffusion to be constant over the entire field.\n\nGiving an equation of the form:\n\nThe equation may now be rewritten in the following form:\n\nThe term below is called streamline diffusion.\n\nAny diffusion orthogonal to the streamline diffusion is called crosswind diffusion, for us this becomes the term:\n"}
{"id": "299847", "url": "https://en.wikipedia.org/wiki?curid=299847", "title": "Structural information theory", "text": "Structural information theory\n\nStructural information theory (SIT) is a theory about human perception and in particular about visual perceptual organization, which is the neuro-cognitive process that enables us to perceive scenes as structured wholes consisting of objects arranged in space. It has been applied to a wide range of research topics, mostly in visual form perception but also in, for instance, visual ergonomics, data visualization, and music perception.\n\nSIT began as a quantitative model of visual pattern classification. Nowadays, it includes quantitative models of symmetry perception and amodal completion, and is theoretically sustained by a perceptually adequate formalization of visual regularity, a quantitative account of viewpoint dependencies, and a powerful form of neurocomputation. SIT has been argued to be the best defined and most successful extension of Gestalt ideas. It is the only Gestalt approach providing a formal calculus that generates plausible perceptual interpretations.\n\nAlthough visual stimuli are fundamentally multi-interpretable, the human visual system usually has a clear preference for only one interpretation. To explain this preference, SIT introduced a formal coding model starting from the assumption that the perceptually preferred interpretation of a stimulus is the one with the simplest code. A simplest code is a code with minimum information load, that is, a code that enables a reconstruction of the stimulus using a minimum number of descriptive parameters. Such a code is obtained by capturing a maximum amount of visual regularity and yields a hierarchical organization of the stimulus in terms of wholes and parts.\n\nThe assumption that the visual system prefers simplest interpretations is called the simplicity principle. Historically, the simplicity principle is an information-theoretical translation of the Gestalt law of Prägnanz, which was inspired by the natural tendency of physical systems to settle into relatively stable states defined by a minimum of free-energy. Furthermore, just as the later-proposed minimum description length principle in algorithmic information theory (AIT), a.k.a. the theory of Kolmogorov complexity, it can be seen as a formalization of Occam's Razor, according to which the simplest interpretation of data is the best one.\n\nSince the 1960s, SIT (in psychology) and AIT (in computer science) evolved independently as viable alternatives for Shannon's classical information theory which had been developed in communication theory. In Shannon's approach, things are assigned codes with lengths based on their probability in terms of frequencies of occurrence (as, e.g., in the Morse code). However, in many domains, including perception, such probabilities are hardly quantifiable, if at all. Both SIT and AIT circumvent this problem by turning to descriptive complexities of individual things.\n\nAlthough SIT and AIT share many starting points and objectives, there are also several relevant differences:\n\nIn visual perception research, the simplicity principle contrasts with the Helmholtzian likelihood principle, which assumes that the preferred interpretation of a stimulus is the one most likely to be true in this world. As shown within a Bayesian framework and using AIT findings, the simplicity principle would imply that perceptual interpretations are fairly veridical (i.e., truthful) in many worlds rather than, as assumed by the likelihood principle, highly veridical in only one world. In other words, whereas the likelihood principle suggests that the visual system is a special-purpose system (i.e., adapted to one specific world), the simplicity principle suggests that it is a general-purpose system (i.e., adaptive to many different worlds).\n\nCrucial to the latter finding is the distinction between, and integration of, viewpoint-independent and viewpoint-dependent factors in vision, as proposed in SIT's empirically successful model of amodal completion. In the Bayesian framework, these factors correspond to prior probabilities and conditional probabilities, respectively. In SIT's model, however, both factors are quantified in terms of complexities, that is, complexities of objects and of their spatial relationships, respectively. This approach is consistent with neuroscientific ideas about the distinction and interaction between the ventral (\"what\") and dorsal (\"where\") streams in the brain.\n\nA representational theory like SIT seems opposite to dynamic systems theory (DST), while connectionism can be seen as something in between. That is, connectionism flirts with DST when it comes to the usage of differential equations and flirts with theories like SIT when it comes to the representation of information. In fact, the different operating bases of SIT, connectionism, and DST, correspond to what Marr called the computational, the algorithmic, and the implementational levels of description, respectively. According to Marr, these levels of description are complementary rather than opposite, thus reflecting epistemological pluralism.\n\nWhat SIT, connectionism, and DST have in common is that they describe nonlinear system behavior, that is, a minor change in the input may yield a major change in the output. Their complementarity expresses itself in that they focus on different aspects:\n\nIn SIT's formal coding model, candidate interpretations of a stimulus are represented by symbol strings, in which identical symbols refer to identical perceptual primitives (e.g., blobs or edges). Every substring of such a string represents a spatially contiguous part of an interpretation, so that the entire string can be read as a reconstruction recipe for the interpretation and, thereby, for the stimulus. These strings then are encoded (i.e., they are searched for visual regularities) to find the interpretation with the simplest code.\n\nThis encoding is performed by way of symbol manipulation, which, in psychology, has led to critical statements of the sort of \"SIT assumes that the brain performs symbol manipulation\". Such statements, however, fall in the same category as statements such as \"physics assumes that nature applies formulas such as Einstein's \"E=mc\" or Newton's \"F=ma\"\" and \"DST models assume that dynamic systems apply differential equations\". That is, these statements ignore that the very concept of formalization means that potentially relevant things are represented by symbols—not as a goal in itself but as a means to capture potentially relevant relationships between these things.\n\nTo obtain simplest codes, SIT applies coding rules that capture the kinds of regularity called iteration, symmetry, and alternation. These have been shown to be the only regularities that satisfy the formal criteria of\n(a) being holographic regularities that (b) allow for hierarchically transparent codes.\n\nA crucial difference with respect to the traditionally considered transformational formalization of visual regularity is that, holographically, mirror symmetry is composed of many relationships between symmetry pairs rather than one relationship between symmetry halves. Whereas the transformational characterization may be suited better for object recognition, the holographic characterization seems more consistent with the buildup of mental representations in object perception.\n\nThe perceptual relevance of the criteria of holography and transparency has been verified in the holographic approach to visual regularity. This approach provides an empirically successful model of the detectability of single and combined visual regularities, whether or not perturbed by noise. For instance, it explains that mirror symmetries and Glass pattens are about equally detectable and usually better detectable than repetitions. It also explains that the detectability of mirror symmetries and Glass pattens in the presence of noise follows a psychophysical law that improves on Weber's law.\n\n"}
{"id": "3797882", "url": "https://en.wikipedia.org/wiki?curid=3797882", "title": "Tetrahedral molecular geometry", "text": "Tetrahedral molecular geometry\n\nIn a tetrahedral molecular geometry, a central atom is located at the center with four substituents that are located at the corners of a tetrahedron. The bond angles are cos(−⅓) = 109.4712206...° ≈ 109.5° when all four substituents are the same, as in methane (CH) as well as its heavier analogues. Methane and other perfectly symmetrical tetrahedral molecules belong to point group T, but most tetrahedral molecules have lower symmetry. Tetrahedral molecules can be chiral.\n\nAside from virtually all saturated organic compounds, most compounds of Si, Ge, and Sn are tetrahedral. Often tetrahedral molecules feature multiple bonding to the outer ligands, as in xenon tetroxide (XeO), the perchlorate ion (), the sulfate ion (), the phosphate ion (). Thiazyl trifluoride (SNF) is tetrahedral, featuring a sulfur-to-nitrogen triple bond.\n\nOther molecules have a tetrahedral arrangement of electron pairs around a central atom; for example ammonia (NH) with the nitrogen atom surrounded by three hydrogens and one lone pair. However the usual classification considers only the bonded atoms and not the lone pair, so that ammonia is actually considered as pyramidal. The H–N–H angles are 107°, contracted from 109.5. This difference is attributed to the influence of the lone pair which exerts a greater repulsive influence than a bonded atom.\n\nAgain the geometry is widespread, particularly so for complexes where the metal has d or d configuration. Illustrative examples include tetrakis(triphenylphosphine)palladium(0) (Pd[P(CH)]), nickel carbonyl (Ni(CO)), and titanium tetrachloride (TiCl). Many complexes with incompletely filled d-shells are often tetrahedral, e.g. the tetrahalides of iron(II), cobalt(II), and nickel(II).\n\nIn the gas phase, a single water molecule has an oxygen atom surrounded by two hydrogens and two lone pairs, and the HO geometry is simply described as bent without considering the nonbonded lone pairs. \n\nHowever in liquid water or in ice, the lone pairs form hydrogen bonds with neighboring water molecules. The most common arrangement of hydrogen atoms around an oxygen is tetrahedral with two hydrogen atoms covalently bonded to oxygen and two attached by hydrogen bonds. Since the hydrogen bonds vary in length many of these water molecules are not symmetrical and form transient irregular tetrahedra between their four associated hydrogen atoms.\n\nMany compounds and complexes adopt bitetrahedral structures. In this motif, the two tetrahedra share a common edge. The inorganic polymer silicon disulfide features an infinite chain of edge-shared tetrahedra.\nInversion of tetrahedral occurs widely in organic and main group chemistry. The so-called Walden inversion illustrates the stereochemical consequences of inversion at carbon. Nitrogen inversion in ammonia also entails transient formation of planar NH.\n\nGeometrical constraints in a molecule can cause a severe distortion of idealized tetrahedral geometry. In compounds featuring \"inverted\" tetrahedral geometry at a carbon atom, all four groups attached to this carbon are on one side of a plane. The carbon atom lies at or near the apex of a square pyramid with the other four groups at the corners.\n\nThe simplest examples of organic molecules displaying inverted tetrahedral geometry are the smallest propellanes, such as [1.1.1]propellane; or more generally the paddlanes, and pyramidane ([3.3.3.3]fenestrane). Such molecules are typically strained, resulting in increased reactivity.\n\nA tetrahedron can also be distorted by increasing the angle between two of the bonds. In the extreme case, flattening results. For carbon this phenomenon can be observed in a class of compounds called the fenestranes.\n\nA few molecules have a tetrahedral geometry with no central atom. An inorganic example is tetraphosphorus (P) which has four phosphorus atoms at the vertices of a tetrahedron and each bonded to the other three. An organic example is tetrahedrane (CH) with four carbon atoms each bonded to one hydrogen and the other three carbons.\n\n\n"}
{"id": "3573165", "url": "https://en.wikipedia.org/wiki?curid=3573165", "title": "Tsallis entropy", "text": "Tsallis entropy\n\nIn physics, the Tsallis entropy is a generalization of the standard Boltzmann–Gibbs entropy.\n\nThe concept was introduced in 1988 by Constantino Tsallis as a basis for generalizing the standard statistical mechanics, and is identical in form to Havrda–Charvát structural α-entropy, introduced in 1967 within information theory. In the scientific literature, the physical relevance of the Tsallis entropy has been debated. However, from the years 2000 on, an increasingly wide spectrum of natural, artificial and social complex systems have been identified which confirm the predictions and consequences that are derived from this nonadditive entropy, such as nonextensive statistical mechanics, which generalizes the Boltzmann–Gibbs theory.\n\nAmong the various experimental verifications and applications presently available in the literature, the following ones deserve a special mention:\n\n\nAmong the various available theoretical results which clarify the physical conditions under which Tsallis entropy and associated statistics apply, the following ones can be selected: \n\nFor further details a bibliography is available at http://tsallis.cat.cbpf.br/biblio.htm\n\nGiven a discrete set of probabilities formula_1 with the condition formula_2, and formula_3 any real number, the Tsallis entropy is defined as\n\nwhere formula_3 is a real parameter sometimes called \"entropic-index\".\nIn the limit as formula_6, the usual Boltzmann–Gibbs entropy is recovered, namely\n\nFor continuous probability distributions, we define the entropy as\n\nwhere formula_9 is a probability density function.\n\nThe Tsallis Entropy has been used along with the Principle of maximum entropy to derive the Tsallis distribution.\n\nThe discrete Tsallis entropy satisfies\n\nwhere \"D\" is the q-derivative with respect to \"x\". This may be compared to the standard entropy formula:\n\nGiven two independent systems \"A\" and \"B\", for which the joint probability density satisfies\n\nthe Tsallis entropy of this system satisfies\n\nFrom this result, it is evident that the parameter formula_14 is a measure of the departure from additivity. In the limit when \"q\" = 1,\n\nwhich is what is expected for an additive system. This property is sometimes referred to as \"pseudo-additivity\".\n\nMany common distributions like the normal distribution belongs to the statistical exponential families.\nTsallis entropy for an exponential family can be written as\n\nwhere \"F\" is log-normalizer and \"k\" the term indicating the carrier measure.\nFor multivariate normal, term \"k\" is zero, and therefore the Tsallis entropy is in closed-form.\n\nA number of interesting physical systems abide to entropic functionals that are more general than the standard Tsallis entropy. Therefore, several physically meaningful generalizations have been introduced. The two most general of those are notably: Superstatistics, introduced by C. Beck and E. G. D. Cohen in 2003 and Spectral Statistics, introduced by G. A. Tsekouras and Constantino Tsallis in 2005. Both these entropic forms have Tsallis and Boltzmann–Gibbs statistics as special cases; Spectral Statistics has been proven to at least contain Superstatistics and it has been conjectured to also cover some additional cases.\n\n\n\n"}
{"id": "279701", "url": "https://en.wikipedia.org/wiki?curid=279701", "title": "Type variable", "text": "Type variable\n\nIn type theory and programming languages, a type variable is a mathematical variable ranging over types. Even in programming languages that allow mutable variables, a type variable remains an abstraction, in the sense that it does not correspond to some memory locations.\n\nProgramming languages that support parametric polymorphism make use of universally quantified type variables. Languages that support existential types make use of existentially quantified type variables. For example, the following OCaml code defines a polymorphic identity function that has a universally quantified type, which is printed by the interpreter on the second line:\n\nIn mathematical notation, the type of the function codice_1 is formula_1, where formula_2 is a type variable.\n\n"}
{"id": "42061249", "url": "https://en.wikipedia.org/wiki?curid=42061249", "title": "Variance function", "text": "Variance function\n\nIn statistics, the variance function is a smooth function which depicts the variance of a random quantity as a function of its mean. The variance function plays a large role in many settings of statistical modelling. It is a main ingredient in the generalized linear model framework and a tool used in non-parametric regression, semiparametric regression and functional data analysis. In parametric modeling, variance functions take on a parametric form and explicitly describe the relationship between the variance and the mean of a random quantity. In a non-parametric setting, the variance function is assumed to be a smooth function.\n\nIn a regression model setting, the goal is to establish whether or not a relationship exists between a response variable and a set of predictor variables. Further, if a relationship does exist, the goal is then to be able to describe this relationship as best as possible. A main assumption in linear regression is constant variance or (homoscedasticity), meaning that different response variables have the same variance in their errors, at every predictor level. This assumption works well when the response variable and the predictor variable are jointly Normal, see Normal distribution. As we will see later, the variance function in the Normal setting, is constant, however, we must find a way to quantify heteroscedasticity (non-constant variance) in the absence of joint Normality.\n\nWhen it is likely that the response follows a distribution that is a member of the exponential family, a generalized linear model may be more appropriate to use, and moreover, when we wish not to force a parametric model onto our data, a non-parametric regression approach can be useful. The importance of being able to model the variance as a function of the mean lies in improved inference (in a parametric setting), and estimation of the regression function in general, for any setting.\n\nVariance functions play a very important role in parameter estimation and inference. In general, maximum likelihood estimation requires that a likelihood function be defined. This requirement then implies that one must first specify the distribution of the response variables observed. However, to define a quasi-likelihood, one need only specify a relationship between the mean and the variance of the observations to then be able to use the quasi-likelihood function for estimation. Quasi-likelihood estimation is particularly useful when there is overdispersion. Overdispersion occurs when there is more variability in the data than there should otherwise be expected according to the assumed distribution of the data.\n\nIn summary, to ensure efficient inference of the regression parameters and the regression function, the heteroscedasticity must be accounted for. Variance functions quantify the relationship between the variance and the mean of the observed data and hence play a significant role in regression estimation and inference.\n\nThe variance function and its applications come up in many areas of statistical analysis. A very important use of this function is in the framework of generalized linear models and non-parametric regression.\n\nWhen a member of the exponential family has been specified, the variance function can easily be derived. The general form of the variance function is presented under the exponential family context, as well as specific forms for Normal, Bernoulli, Poisson, and Gamma. In addition, we describe the applications and use of variance functions in maximum likelihood estimation and quasi-likelihood estimation.\n\nThe generalized linear model (GLM), is a generalization of ordinary regression analysis that extends to any member of the exponential family. It is particularly useful when the response variable is categorical, binary or subject to a constraint (e.g. only positive responses make sense). A quick summary of the components of a GLM are summarized on this page, but for more details and information see the page on generalized linear models.\n\nA GLM consists of three main ingredients:\n\nFirst it is important to derive a couple key properties of the exponential family.\n\nAny random variable formula_4 in the exponential family has a probability density function of the form,\n\nwith loglikelihood,\nHere, formula_7 is the canonical parameter and the parameter of interest, and formula_8 is a nuisance parameter which plays a role in the variance.\nWe use the Bartlett's Identities to derive a general expression for the variance function.\nThe first and second Bartlett results ensures that under suitable conditions ((see Leibniz integral rule)), for a density function dependent on formula_9,\n\nThese identities lead to simple calculations of the expected value and variance of any random variable formula_4 in the exponential family formula_13.\n\nExpected value of \"Y\":\nTaking the first derivative with respect to formula_7 of the log of the density in the exponential family form described above, we have\nThen taking the expected value and setting it equal to zero leads to, \n\nVariance of Y:\nTo compute the variance we use the second Bartlett identity,\n\nWe have now a relationship between formula_21 and formula_7, namely\n\nNote that because formula_28, then formula_29 is invertible.\nWe derive the variance function for a few common distributions.\n\nThe Normal distribution is a special case where the variance function is a constant. Let formula_30 then we put the density function of y in the form of the exponential family described above:\n\nwhere\n\nTo calculate the variance function formula_36, we first express formula_7 as a function of formula_21. Then we transform formula_39 into a function of formula_21\n\nTherefore, the variance function is constant.\n\nLet formula_44, then we express the density of the Bernoulli distribution in exponential family form, \n\nThis give us \n\nLet formula_54, then we express the density of the Poisson distribution in exponential family form, \n\nThis give us \n\nHere we see the central property of Poisson data, that the variance is equal to the mean.\n\nThe Gamma distribution and density function can be expressed under different parametrizations. We will use the form of the gamma with parameters formula_62\n\nThen in exponential family form we have \n\nAnd we have formula_70\n\nA very important application of the variance function is its use in parameter estimation and inference when the response variable is of the required exponential family form as well as in some cases when it is not (which we will discuss in quasi-likelihood). Weighted least squares (WLS) is a special case of generalized least squares. Each term in the WLS criterion includes a weight that determines that the influence each observation has on the final parameter estimates. As in regular least squares, the goal is to estimate the unknown parameters in the regression function by finding values for parameter estimates that minimize the sum of the squared deviations between the observed responses and the functional portion of the model.\n\nWhile WLS assumes independence of observations it does not assume equal variance and is therefore a solution for parameter estimation in the presence of heteroscedasticity. The Gauss–Markov theorem and Aitken demonstrate that the best linear unbiased estimator (BLUE), the unbiased estimator with minimum variance, has each weight equal to the reciprocal of the variance of the measurement.\n\nIn the GLM framework, our goal is to estimate parameters formula_71, where formula_72. Therefore, we would like to minimize formula_73 and if we define the weight matrix W as\n\nwhere formula_75 are defined in the previous section, it allows for iteratively reweighted least squares (IRLS) estimation of the parameters. See the section on iteratively reweighted least squares for more derivation and information.\n\nAlso, important to note is that when the weight matrix is of the form described here, minimizing the expression formula_73 also minimizes the Pearson distance. See Distance correlation for more.\n\nThe matrix W falls right out of the estimating equations for estimation of formula_71. Maximum likelihood estimation for each parameter formula_78, requires\nLooking at a single observation we have,\n\nThis gives us\n\nThe Hessian matrix is determined in a similar manner and can be shown to be,\n\nNoticing that the Fisher Information (FI), \n\nBecause most features of GLMs only depend on the first two moments of the distribution, rather than then entire distribution, the quasi-likelihood can be developed by just specifying a link function and a variance function. That is, we need to specify\nWith a specified variance function and link function we can develop, as alternatives to the log-likelihood function, the score function, and the Fisher information, a quasi-likelihood, a quasi-score, and the quasi-information. This allows for full inference of formula_71.\n\nQuasi-likelihood (QL)\n\nThough called a quasi-likelihood, this is in fact a quasi-log-likelihood. The QL for one observation is\n\nAnd therefore the QL for all n observations is\n\nFrom the QL we have the quasi-score\n\nQuasi-score (QS)\n\nRecall the score function, U, for data with log-likelihood formula_97 is\n\nWe obtain the quasi-score in an identical manner,\n\nNoting that, for one observation the score is\n\nThe first two Bartlett equations are satisfied for the quasi-score, namely\n\nand\n\nIn addition, the quasi-score is linear in y.\n\nUltimately the goal is to find information about the parameters of interest formula_71. Both the QS and the QL are actually functions of formula_71. Recall, formula_105, and formula_106, therefore,\n\nQuasi-information (QI)\n\nThe quasi-information, is similar to the Fisher information,\n\nQL,QS,QI as functions of formula_71\n\nThe QL, QS and QI all provide the building blocks for inference about the parameters of interest and therefore it is important to express the QL, QS and QI all as functions of formula_71.\n\nRecalling again that formula_111, we derive the expressions for QL,QS and QI parametrized under formula_71.\n\nQuasi-likelihood in formula_71, \n\nThe QS as a function of formula_71 is therefore\n\nWhere, \n\nThe quasi-information matrix in formula_71 is,\n\nObtaining the score function and the information of formula_71 allows for parameter estimation and inference in a similar manner as described in Application – weighted least squares.\n\nNon-parametric estimation of the variance function and its importance, has been discussed widely in the literature\nIn non-parametric regression analysis, the goal is to express the expected value of your response variable(y) as a function of your predictors (X). That is we are looking to estimate a mean function, formula_122 without assuming a parametric form. There are many forms of non-parametric smoothing methods to help estimate the function formula_123. An interesting approach is to also look at a non-parametric variance function, formula_124. A non-parametric variance function allows one to look at the mean function as it relates to the variance function and notice patterns in the data.\n\nAn example is detailed in the pictures to the left. The goal of the project was to determine (among other things) whether or not the predictor, number of years in the major leagues (baseball,) had an effect on the response, salary, a player made. An initial scatter plot of the data indicates that there is heteroscedasticity in the data as the variance is not constant at each level of the predictor. Because we can visually detect the non-constant variance, it useful now to plot formula_125, and look to see if the shape is indicative of any known distribution. One can estimate formula_127 and formula_128 using a general smoothing method. The plot of the non-parametric smoothed variance function can give the researcher an idea of the relationship between the variance and the mean. The picture to the right indicates a quadratic relationship between the mean and the variance. As we saw above, the Gamma variance function is quadratic in the mean.\n\n"}
{"id": "18994043", "url": "https://en.wikipedia.org/wiki?curid=18994043", "title": "Walter Benz", "text": "Walter Benz\n\nWalter Benz (May 2, 1931 Lahnstein – January 13, 2017 Ratzeburg) was a German mathematician, an expert in geometry.\n\nBenz studied at the Johannes Gutenberg University of Mainz and received his doctoral degree in 1954, with Robert Furch as his advisor.\nAfter a position at the Johann Wolfgang Goethe University Frankfurt am Main, he served as a professor at the Universities of Bochum, Waterloo and Hamburg. Benz was honoured with the degree of a Dr. h.c.\n\nBased on his book \"Vorlesungen über Geometrie der Algebren\" (Springer 1973), certain geometric objects are called Benz planes.\n\nInner product spaces over the real numbers provide the basis of a 2007 book by Benz: \"Classical Geometries in Modern Contexts\".\n\n\n"}
{"id": "28291162", "url": "https://en.wikipedia.org/wiki?curid=28291162", "title": "Whitney topologies", "text": "Whitney topologies\n\nIn mathematics, and especially differential topology, functional analysis and singularity theory, the Whitney topologies are a countably infinite family of topologies defined on the set of smooth mappings between two smooth manifolds. They are named after the American mathematician Hassler Whitney.\n\nLet \"M\" and \"N\" be two real, smooth manifolds. Furthermore, let C(\"M\",\"N\") denote the space of smooth mappings between \"M\" and \"N\". The notation C means that the mappings are infinitely differentiable, i.e. partial derivatives of all orders exist and are continuous.\n\nFor some integer , let J(\"M\",\"N\") denote the \"k\"-jet space of mappings between \"M\" and \"N\". The jet space can be endowed with a smooth structure (i.e. a structure as a C manifold) which make it into a topological space. This topology is used to define a topology on C(\"M\",\"N\").\n\nFor a fixed integer consider an open subset and denote by \"S\"(\"U\") the following:\nThe sets \"S\"(\"U\") form a basis for the Whitney C-topology on C(\"M\",\"N\").\n\nFor each choice of , the Whitney C-topology gives a topology for C(\"M\",\"N\"); in other words the Whitney C-topology tells us which subsets of C(\"M\",\"N\") are open sets. Let us denote by W the set of open subsets of C(\"M\",\"N\") with respect to the Whitney C-topology. Then the Whitney C-topology is defined to be the topology whose basis is given by \"W\", where:\n\nNotice that C(\"M\",\"N\") has infinite dimension, whereas J(\"M\",\"N\") has finite dimension. In fact, J(\"M\",\"N\") is a real, finite-dimensional manifold. To see this, let denote the space of polynomials, with real coefficients, in \"m\" variables of order at most \"k\" and with zero as the constant term. This is a real vector space with dimension\nWriting } then, by the standard theory of vector spaces and so is a real, finite-dimensional manifold. Next, define:\nUsing \"b\" to denote the dimension \"B\", we see that , and so is a real, finite-dimensional manifold.\n\nIn fact, if \"M\" and \"N\" have dimension \"m\" and \"n\" respectively then:\n\nConsider the surjective mapping from the space of smooth maps between smooth manifolds and the \"k\"-jet space:\nIn the Whitney C-topology the open sets in C(\"M\",\"N\") are, by definition, the preimages of open sets in J(\"M\",\"N\"). It follows that the map π between C(\"M\",\"N\") given the Whitney C-topology and J(\"M\",\"N\") given the Euclidean topology is continuous.\n\nGiven the Whitney C-topology, the space C(\"M\",\"N\") is a Baire space, i.e. every residual set is dense.\n"}
{"id": "58756838", "url": "https://en.wikipedia.org/wiki?curid=58756838", "title": "Wigner surmise", "text": "Wigner surmise\n\nIn mathematical physics, the Wigner surmise is a statement about the probability distribution of the spaces between points in the spectra of nuclei of heavy atoms. It was proposed by Eugene Wigner in probability theory. The surmise was a result of Wigner's introduction of random matrices in the field of nuclear physics. The surmise consists of two postulates:\n"}
{"id": "20315566", "url": "https://en.wikipedia.org/wiki?curid=20315566", "title": "Willis Dysart", "text": "Willis Dysart\n\nWillis Nelson Dysart (March 15, 1923 in Omega, Georgia - November 8, 2011) was an American mental calculator. His talent for arithmetic emerged at the age of three after his mother taught him to count. He quit school in the third grade (age 9) and pursued a career as a lightning calculator.\n\nIn 1938 Robert Ripley featured Dysart in his 'Believe It Or Not' newspaper column and introduced what would become Dysart's stage name - 'Willie the Wizard'.\n\nIn 1940 Dysart was recruited by a local radio station to tally votes in the US presidential election. Dysart would very quickly (much more quickly than rival radio stations aided by calculating machines) give \"the exact standing of any candidate on the board, including his current total, the percentage of votes counted at that point and the probable outcome of the contest on the basis of existing information\". Not content with that, Dysart would provide a little entertainment by, for instance, asking for the birth dates of the candidates and immediately giving the years, months, hours, minutes and seconds they had lived to that moment\n\nDysart has given many live demonstrations of his skill at a range of venues. He has also appeared on numerous television shows, including I've Got a Secret, You Asked For It, The Art Linkletter Show and The Joe Pyne Show, which made him famous in the United States. He has also been the subject of psychological studies.\n\nAlthough excelling at all kinds of arithmetic, Dysart's most startling demonstrations have been in addition and multiplication. Multiplying a pair of three-digit numbers is for Dysart a trivial task, which is why he breaks larger numbers into groups of three digits before multiplying them (many of the multiplications reported to have been made by Dysart involve six or nine-digit numbers). In this respect Dysart is unique among the documented calculators, some of whom – most notably Dutchman Wim Klein – multiplied large numbers by breaking them up into groups of two-digit numbers but never three-digit numbers. It took Dysart less than 10 seconds to multiply a pair of nine-digit numbers.\n\nDysart died on November 8, 2011 in Long Beach, California.\n"}
{"id": "34052752", "url": "https://en.wikipedia.org/wiki?curid=34052752", "title": "Émile Picard Medal", "text": "Émile Picard Medal\n\nThe Émile Picard Medal (or Médaille Émile Picard) is a medal named for Émile Picard awarded every 6 years to an outstanding mathematician by the Institut de France, Académie des sciences.\n\nThe Émile Picard Medal recipients are \n"}
{"id": "27151056", "url": "https://en.wikipedia.org/wiki?curid=27151056", "title": "Łoś–Tarski preservation theorem", "text": "Łoś–Tarski preservation theorem\n\nThe Łoś–Tarski theorem is a theorem in model theory, a branch of mathematics, that states that the set of formulas preserved under taking substructures is exactly the set of \"universal\" formulas (Hodges 1997). The theorem was discovered by Jerzy Łoś and Alfred Tarski.\n\nLet formula_1 be a theory in a first-order language formula_2 and \nformula_3 a set of formulas of formula_2.\n(The set of sequence of variables formula_5 need not be\nfinite.) Then the following are equivalent:\n\nA formula is formula_19 if and only if it is of the form formula_22 where formula_23 is quantifier-free.\n\nNote that this property fails for finite models.\n\n"}
