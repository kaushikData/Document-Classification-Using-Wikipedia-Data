{"id": "35238282", "url": "https://en.wikipedia.org/wiki?curid=35238282", "title": "Abstract cell complex", "text": "Abstract cell complex\n\nIn mathematics, an abstract cell complex is an abstract set with Alexandrov topology in which a non-negative integer number called dimension is assigned to each point. The complex is called “abstract” since its points, which are called “cells”, are not subsets of a Hausdorff space as it is the case in Euclidean and CW complex. Abstract cell complexes play an important role in image analysis and computer graphics.\n\nThe idea of abstract cell complexes (also named abstract cellular complexes) relates to J. Listing (1862) und E. Steinitz (1908). Also A.W Tucker (1933), K. Reidemeister (1938), P.S. Aleksandrov (1956) as well as R. Klette und A. Rosenfeld (2004) have described abstract cell complexes. E. Steinitz has defined an abstract cell complex as formula_1 where \"E\" is an abstract set, \"B\" is an asymmetric, irreflexive and transitive binary relation called the bounding relation among the elements of \"E\" and \"dim\" is a function assigning a non-negative integer to each element of \"E\" in such a way that if formula_2, then formula_3. \nV. Kovalevsky (1989) described abstract cell complexes for 3D and higher dimensions. He also suggested numerous applications to image analysis. In his book (2008) he has suggested an axiomatic theory of locally finite topological spaces which are generalization of abstract cell complexes. The book contains among others new definitions of topological balls and spheres independent of metric, a new definition of combinatorial manifolds and many algorithms useful for image analysis.\n\nThe topology of abstract cell complexes is based on a partial order in the set of its points or cells. \n\nThe notion of the abstract cell complex defined by E. Steinitz is related to the notion of an abstract simplicial complex and it differs from a simplicial complex by the property that its elements are no simplices: An \"n\"-dimensional element of an abstract complexes must not have \"n\"+1 zero-dimensional sides, and not each subset of the set of zero-dimensional sides of a cell is a cell. This is important since the notion of an abstract cell complexes can be applied to the two- and three-dimensional grids used in image processing, which is not true for simplicial complexes. A non-simplicial complex is a generalization which makes the introduction of cell coordinates possible: There are non-simplicial complexes which are Cartesian products of such \"linear\" one-dimensional complexes where each zero-dimensional cell, besides two of them, bounds exactly two one-dimensional cells. Only such Cartesian complexes make it possible to introduce such coordinates that each cell has a set of coordinates and any two different cells have different coordinate sets. The coordinate set can serve as a name of each cell of the complex which is important for processing complexes. \n\nAbstract complexes allow the introduction of classical topology (Alexandrov-topology) in grids being the basis of digital image processing. This possibility defines the great advantage of abstract cell complexes: It becomes possible to exactly define the notions of connectivity and of the boundary of subsets. The definition of dimension of cells and of complexes is in the general case different from that of simplicial complexes (see below).\n\nThe notion of an abstract cell complex differs essentially from that of a CW-complex because an abstract cell complex is no Hausdorff space. This is important from the point of view of computer science since it is impossible to explicitly represent a non-discrete Hausdorff space in a computer. (The neighborhood of each point in such a space must have infinitely many points). \n\nThe book by V. Kovalevsky contains the description of the theory of locally finite spaces which are a generalization of abstract cell complexes. A locally finite space \"S\" is a set of points where a subset of \"S\" is defined for each point \"P\" of \"S\". This subset containing a limited number of points is called the smallest neighborhood of \"P\". A binary neighborhood relation is defined in the set of points of the locally finite space \"S\": The element (point) \"b\" is in the neighborhood relation with the element \"a\" if \"b\" belongs to the smallest neighborhood of the element \"a\". New axioms of a locally finite space have been formulated, and it was proven that the space \"S\" is in accordance with the axioms only if the neighborhood relation is anti-symmetric and transitive. The neighborhood relation is the reflexive hull of the inverse bounding relation. It was shown that classical axioms of the topology can be deduced as theorems from the new axioms. Therefore, a locally finite space satisfying the new axioms is a particular case of a classical topological space. Its topology is a poset topology or Alexandrov topology.\nAn abstract cell complex is a particular case of a locally finite space in which the dimension is defined for each point. It was demonstrated that the dimension of a cell \"c\" of an abstract cell complex is equal to the length (number of cells minus 1) of the maximum bounding path leading from any cell of the complex to the cell \"c\". The bounding path is a sequence of cells in which each cell bounds the next one. The book contains the theory of digital straight segments in 2D complexes, numerous algorithms for tracing boundaries in 2D and 3D, for economically encoding the boundaries and for exactly reconstructing a subset from the code of its boundary.\n\nA digital image may be represented by a 2D Abstract Cell Complex (ACC) by decomposing the image into its ACC dimensional constituents: points (0-cell), cracks/edges (1-cell), and pixels/faces (2-cell).\nThis decomposition together with a coordinate assignment rule to unambiguously assign coordinates from the image pixels to the dimensional constituents permit certain image analysis operations to be carried out on the image with elegant algorithms such as crack boundary tracing, digital straight segment subdivision, etc. One such rule maps the points, cracks, and faces to the top left coordinate of the pixel. It should be noted that these dimensional constituents require no explicit translation into their own data structures but may be implicitly understood and related to the 2D array which is the usual data structure representation of a digital image. This coordinate assignment rule and the renderings of each cell incident to this image is depicted in the image at right.\n\n"}
{"id": "2479369", "url": "https://en.wikipedia.org/wiki?curid=2479369", "title": "Abū Kāmil Shujāʿ ibn Aslam", "text": "Abū Kāmil Shujāʿ ibn Aslam\n\nAbu Kamil made important contributions to algebra and geometry. He was the first Islamic mathematician to work easily with algebraic equations with powers higher than formula_1 (up to formula_2), and solved sets of non-linear simultaneous equations with three unknown variables. He wrote all problems rhetorically, and some of his books lacked any mathematical notation beside those of integers. For example, he uses the Arabic expression \"māl māl shayʾ\" (\"square-square-thing\") for formula_3 (i.e., formula_4).\n\nThe muslim encyclopedist Ibn Khaldūn classified Abū Kāmil as the second greatest algebraist chronologically after al-Khwarizmi.\n\nAlmost nothing is known about the life and career of Abu Kamil except that he was a successor of al-Khwarizmi, whom he never personally met.\n\nThe \"Algebra\" is perhaps Abu Kamil's most influential work, which he intended to supersede and expand upon that of Al-Khwarizmi. Whereas the \"Algebra\" of al-Khwarizmi was geared towards the general public, Abu Kamil was addressing other mathematicians, or readers familiar with Euclid's \"Elements\". In this book Abu Kamil solves systems of equations whose solutions are whole numbers and fractions, and accepted irrational numbers (in the form of a square root or fourth root) as solutions and coefficients to quadratic equations.\n\nThe first chapter teaches algebra by solving problems of application to geometry, often involving an unknown variable and square roots. The second chapter deals with the six types of problems found in Al-Khwarizmi's book, but some of which, especially those of formula_1, were now worked out directly instead of first solving for formula_6 and accompanied with geometrical illustrations and proofs. The third chapter contains examples of quadratic irrationalities as solutions and coefficients. The fourth chapter shows how these irrationalities are used to solve problems involving polygons. The rest of the book contains solutions for sets of indeterminate equations, problems of application in realistic situations, and problems involving unrealistic situations intended for recreational mathematics.\n\nA number of Islamic mathematicians wrote commentaries on this work, including al-Iṣṭakhrī al-Ḥāsib and ʿAli ibn Aḥmad al-ʿImrānī (d. 955-6), but both commentaries are now lost.\n\nIn Europe, similar material to this book is found in the writings of Fibonacci, and some sections were incorporated and improved upon in the Latin work of John of Seville, \"Liber mahameleth\". A partial translation to Latin was done in the 14th century by William of Luna, and in the 15th century the whole work also appeared in a Hebrew translation by Mordekhai Finzi.\n\nAbu Kamil describes a number of systematic procedures for finding integral solutions for indeterminate equations. It is also the earliest known Arabic work where solutions are sought to the type of indeterminate equations found in Diophantus's \"Arithmetica\". However, Abu Kamil explains certain methods not found in any extant copy of the \"Arithmetica\". He also describes one problem for which he found 2,678 solutions.\n\nIn this treatise algebraic methods are used to solve geometrical problems. Abu Kamil uses the equation formula_7 to calculate a numerical approximation for the side of a regular pentagon in a circle of diameter 10. He also uses the golden ratio in some of his calculations. Fibonacci knew about this treatise and made extensive use of it in his \"Practica geometriae\".\n\nA small treatise teaching how to solve indeterminate linear systems with positive integral solutions. The title is derived from a type of problems known in the east which involve the purchase of different species of birds. Abu Kamil wrote in the introduction:\n\nI found myself before a problem that I solved and for which I discovered a great many solutions; looking deeper for its solutions, I obtained two thousand six hundred and seventy-six correct ones. My astonishment about that was great, but I found out that, when I recounted this discovery, those who did not know me were arrogant, shocked, and suspicious of me. I thus decided to write a book on this kind of calculations, with the purpose of facilitating its treatment and making it more accessible.\n\nAccording to Jacques Sesiano, Abu Kamil remained seemingly unparalleled throughout the Middle Ages in trying to find all the possible solutions to some of his problems.\n\nA manual of geometry for non-mathematicians, like land surveyors and other government officials, which presents a set of rules for calculating the volume and surface area of solids (mainly rectangular parallelepipeds, right circular prisms, square pyramids, and circular cones). The first few chapters contain rules for determining the area, diagonal, perimeter, and other parameters for different types of triangles, rectangles and squares.\n\nSome of Abu Kamil's lost works include:\n\n\nIbn al-Nadim in his \"Fihrist\" listed the following additional titles: \"Book of Fortune\" (\"Kitāb al-falāḥ\"), \"Book of the Key to Fortune\" (\"Kitāb miftāḥ al-falāḥ\"), \"Book of the Adequate\" (\"Kitāb al-kifāya\"), and \"Book of the Kernel\" (\"Kitāb al-ʿasīr\").\n\nThe works of Abu Kamil influenced other mathematicians, like al-Karaji and Fibonacci, and as such had a lasting impact on the development of algebra. Many of his examples and algebraic techniques were later copied by Fibonacci in his \"Practica geometriae\" and other works. Unmistakable borrowings, but without Abu Kamil being explicitly mentioned and perhaps mediated by lost treatises, are also found in Fibonacci's \"Liber Abaci\".\n\nAbu Kamil was one of the earliest mathematicians to recognize al-Khwarizmi's contributions to algebra, defending him against Ibn Barza who attributed the authority and precedent in algebra to his grandfather, 'Abd al-Hamīd ibn Turk. Abu Kamil wrote in the introduction of his \"Algebra\":\n\nI have studied with great attention the writings of the mathematicians, examined their assertions, and scrutinized what they explain in their works; I thus observed that the book by Muḥammad ibn Mūsā al-Khwārizmī known as \"Algebra\" is superior in the accuracy of its principle and the exactness of its argumentation. It thus behooves us, the community of mathematicians, to recognize his priority and to admit his knowledge and his superiority, as in writing his book on algebra he was an initiator and the discoverer of its principles, ...\n\n\n"}
{"id": "7869295", "url": "https://en.wikipedia.org/wiki?curid=7869295", "title": "Adjoint filter", "text": "Adjoint filter\n\nIn signal processing, the adjoint filter mask formula_1 of a filter mask formula_2 is reversed in time and the elements are complex conjugated.\n\nIts name is derived from the fact that the convolution with the adjoint filter is the adjoint operator of the original filter, with respect to the Hilbert space formula_4 of the sequences in which the inner product is the Euclidean norm.\n\nThe autocorrelation of a signal formula_6 can be written as formula_7.\n\n"}
{"id": "212117", "url": "https://en.wikipedia.org/wiki?curid=212117", "title": "Algebraic enumeration", "text": "Algebraic enumeration\n\nAlgebraic enumeration is a subfield of enumeration that deals with finding exact formulas for the number of combinatorial objects of a given type, rather than estimating this number asymptotically. Methods of finding these formulas include generating functions and the solution of recurrence relations.\n"}
{"id": "21600196", "url": "https://en.wikipedia.org/wiki?curid=21600196", "title": "B-convex space", "text": "B-convex space\n\nIn functional analysis, the class of \"B\"-convex spaces is a class of Banach space. The concept of \"B\"-convexity was defined and used to characterize Banach spaces that have the strong law of large numbers by Anatole Beck in 1962; accordingly, \"B-convexity\" is understood as an abbreviation of Beck convexity. Beck proved the following theorem: A Banach space is \"B\"-convex if and only if every sequence of independent, symmetric, uniformly bounded and Radon random variables in that space satisfies the strong law of large numbers.\n\nLet \"X\" be a Banach space with norm || ||. \"X\" is said to be \"B\"-convex if for some \"ε\" > 0 and some natural number \"n\", it holds true that whenever \"x\", ..., \"x\" are elements of the closed unit ball of \"X\", there is a choice of signs \"α\", ..., \"α\" ∈ {−1, +1} such that\n\nLater authors have shown that B-convexity is equivalent to a number of other important properties in the theory of Banach spaces. Being B-convex and having Rademacher type formula_2 were shown to be equivalent Banach-space properties by Gilles Pisier.\n\n"}
{"id": "13502744", "url": "https://en.wikipedia.org/wiki?curid=13502744", "title": "Babuška–Lax–Milgram theorem", "text": "Babuška–Lax–Milgram theorem\n\nIn mathematics, the Babuška–Lax–Milgram theorem is a generalization of the famous Lax–Milgram theorem, which gives conditions under which a bilinear form can be \"inverted\" to show the existence and uniqueness of a weak solution to a given boundary value problem. The result is named after the mathematicians Ivo Babuška, Peter Lax and Arthur Milgram.\n\nIn the modern, functional-analytic approach to the study of partial differential equations, one does not attempt to solve a given partial differential equation directly, but by using the structure of the vector space of possible solutions, e.g. a Sobolev space \"W\". Abstractly, consider two real normed spaces \"U\" and \"V\" with their continuous dual spaces \"U\" and \"V\" respectively. In many applications, \"U\" is the space of possible solutions; given some partial differential operator Λ : \"U\" → \"V\" and a specified element \"f\" ∈ \"V\", the objective is to find a \"u\" ∈ \"U\" such that\n\nHowever, in the weak formulation, this equation is only required to hold when \"tested\" against all other possible elements of \"V\". This \"testing\" is accomplished by means of a bilinear function \"B\" : \"U\" × \"V\" → R which encodes the differential operator Λ; a \"weak solution\" to the problem is to find a \"u\" ∈ \"U\" such that\n\nThe achievement of Lax and Milgram in their 1954 result was to specify sufficient conditions for this weak formulation to have a unique solution that depends continuously upon the specified datum \"f\" ∈ \"V\": it suffices that \"U\" = \"V\" is a Hilbert space, that \"B\" is continuous, and that \"B\" is strongly coercive, i.e.\n\nfor some constant \"c\" > 0 and all \"u\" ∈ \"U\".\n\nFor example, in the solution of the Poisson equation on a bounded, open domain Ω ⊂ R,\n\nthe space \"U\" could be taken to be the Sobolev space \"H\"(Ω) with dual \"H\"(Ω); the former is a subspace of the \"L\" space \"V\" = \"L\"(Ω); the bilinear form \"B\" associated to −Δ is the \"L\"(Ω) inner product of the derivatives:\n\nHence, the weak formulation of the Poisson equation, given \"f\" ∈ \"L\"(Ω), is to find \"u\" such that\n\nIn 1971, Babuška provided the following generalization of Lax and Milgram's earlier result, which begins by dispensing with the requirement that \"U\" and \"V\" be the same space. Let \"U\" and \"V\" be two real Hilbert spaces and let \"B\" : \"U\" × \"V\" → R be a continuous bilinear functional. Suppose also that \"B\" is weakly coercive: for some constant \"c\" > 0 and all \"u\" ∈ \"U\",\n\nand, for all 0 ≠ \"v\" ∈ \"V\", \n\nThen, for all \"f\" ∈ \"V\", there exists a unique solution \"u\" = \"u\" ∈ \"U\" to the weak problem\n\nMoreover, the solution depends continuously on the given datum:\n\n\n"}
{"id": "41144473", "url": "https://en.wikipedia.org/wiki?curid=41144473", "title": "Burnside category", "text": "Burnside category\n\nIn category theory and homotopy theory the Burnside category of a finite group \"G\" is a category whose objects are finite \"G\"-sets and whose morphisms are (equivalence classes of) spans of \"G\"-equivariant maps. It is a categorification of the Burnside ring of \"G\".\n\nLet \"G\" be a finite group (in fact everything will work verbatim for a profinite group). Then for any two finite \"G\"-sets \"X\" and \"Y\" we can define an equivalence relation among spans of \"G\"-sets of the form formula_1 where two spans formula_1 and formula_3are equivalent if and only if there is a \"G\"-equivariant bijection of \"U\" and \"W\" commuting with the projection maps to \"X\" and \"Y\". This set of equivalence classes form naturally a monoid under disjoint union; we indicate with formula_4 the group completion of that monoid. Taking pullbacks induces natural maps formula_5.\n\nFinally we can define the Burnside category \"A(G)\" of \"G\" as the category whose objects are finite \"G\"-sets and the morphisms spaces are the groups formula_4.\n\n\nIf \"C\" is an additive category, then a \"C\"-valued Mackey functor is an additive functor from \"A(G)\" to \"C\". Mackey functors are important in representation theory and stable equivariant homotopy theory.\n\n"}
{"id": "6946171", "url": "https://en.wikipedia.org/wiki?curid=6946171", "title": "Canonical Huffman code", "text": "Canonical Huffman code\n\nA canonical Huffman code is a particular type of Huffman code with unique properties which allow it to be described in a very compact manner.\n\nData compressors generally work in one of two ways. Either the decompressor can infer what codebook the compressor has used from previous context, or the compressor must tell the decompressor what the codebook is. Since a canonical Huffman codebook can be stored especially efficiently, most compressors start by generating a \"normal\" Huffman codebook, and then convert it to canonical Huffman before using it.\n\nIn order for a symbol code scheme such as the Huffman code to be decompressed, the same model that the encoding algorithm used to compress the source data must be provided to the decoding algorithm so that it can use it to decompress the encoded data. In standard Huffman coding this model takes the form of a tree of variable-length codes, with the most frequent symbols located at the top of the structure and being represented by the fewest number of bits.\n\nHowever, this code tree introduces two critical inefficiencies into an implementation of the coding scheme. Firstly, each node of the tree must store either references to its child nodes or the symbol that it represents. This is expensive in memory usage and if there is a high proportion of unique symbols in the source data then the size of the code tree can account for a significant amount of the overall encoded data. Secondly, traversing the tree is computationally costly, since it requires the algorithm to jump randomly through the structure in memory as each bit in the encoded data is read in.\n\nCanonical Huffman codes address these two issues by generating the codes in a clear standardized format; all the codes for a given length are assigned their values sequentially. This means that instead of storing the structure of the code tree for decompression only the lengths of the codes are required, reducing the size of the encoded data. Additionally, because the codes are sequential, the decoding algorithm can be dramatically simplified so that it is computationally efficient.\n\nThe normal Huffman coding algorithm assigns a variable length code to every symbol in the alphabet. More frequently used symbols will be assigned a shorter code. For example, suppose we have the following \"non\"-canonical codebook:\n\nHere the letter A has been assigned 2 bits, B has 1 bit, and C and D both have 3 bits. To make the code a \"canonical\" Huffman code, the codes are renumbered. The bit lengths stay the same with the code book being sorted \"first\" by codeword length and \"secondly\" by alphabetical value:\n\nEach of the existing codes are replaced with a new one of the same length, using the following algorithm:\n\n\nBy following these three rules, the \"canonical\" version of the code book produced will be:\n\nAnother perspective on the canonical codewords is that they are the digits past the radix point (binary decimal point) in a binary representation of a certain series. Specifically, suppose the lengths of the codewords are \"l\" ... \"l\". Then the canonical codeword for symbol \"i\" is the first \"l\" binary digits past the radix point in the binary representation of\n\nformula_1\n\nThis perspective is particularly useful in light of Kraft's inequality, which says that the sum above will always be less than or equal to 1 (since the lengths come from a prefix free code). This shows that adding one in the algorithm above never overflows and creates a codeword that is longer than intended.\n\nThe whole advantage of a canonical Huffman tree is that one can encode the description (the codebook) in fewer bits than a fully described tree.\n\nLet us take our original Huffman codebook:\n\nThere are several ways we could encode this Huffman tree. For example, we could write each symbol followed by the number of bits and code:\n\nSince we are listing the symbols in sequential alphabetical order, we can omit the symbols themselves, listing just the number of bits and code:\n\nWith our \"canonical\" version we have the knowledge that the symbols are in sequential alphabetical order \"and\" that a later code will always be higher in value than an earlier one. The only parts left to transmit are the bit-lengths (number of bits) for each symbol. Note that our canonical Huffman tree always has higher values for longer bit lengths and that any symbols of the same bit length (\"C\" and \"D\") have higher code values for higher symbols:\n\nSince two-thirds of the constraints are known, only the number of bits for each symbol need be transmitted:\n\nWith knowledge of the canonical Huffman algorithm, it is then possible to recreate the entire table (symbol and code values) from just the bit-lengths. Unused symbols are normally transmitted as having zero bit length.\n\nAnother efficient way representing the codebook is to list all symbols in increasing order by their bit-lengths, and record the number of symbols for each bit-length. For the example mentioned above, the encoding becomes:\n\nThis means that the first symbol \"B\" is of length 1, then the \"A\" of length 2, and remains of 3. Since the symbols are sorted by bit-length, we can efficiently reconstruct the codebook. A pseudo code describing the reconstruction is introduced on the next section.\n\nThis type of encoding is advantageous when only a few symbols in the alphabet are being compressed. For example, suppose the codebook contains only 4 letters \"C\", \"O\", \"D\" and \"E\", each of length 2. To represent the letter \"O\" using the previous method, we need to either add a lot of zeros:\n\nor record which 4 letters we have used. Each way makes the description longer than:\n\nThe JPEG File Interchange Format uses this method of encoding, because at most only 162 symbols out of the 8-bit alphabet, which has size 256, will be in the codebook.\n\nGiven a list of symbols sorted by bit-length, the following pseudo code will print a canonical Huffman code book:\n\nThe algorithm described in:\n\"A Method for the Construction of Minimum-Redundancy Codes\"\nDavid A. Huffman, Proceedings of the I.R.E.\nis:\n\nReferences: \n1. Managing Gigabytes: book with an implementation of canonical huffman codes for word dictionaries. \n"}
{"id": "483120", "url": "https://en.wikipedia.org/wiki?curid=483120", "title": "Closure operator", "text": "Closure operator\n\nIn mathematics, a closure operator on a set \"S\" is a function formula_1 from the power set of \"S\" to itself which satisfies the following conditions for all sets formula_2\n\nClosure operators are determined by their closed sets, i.e., by the sets of the form cl(\"X\"), since the closure cl(\"X\") of a set \"X\" is the smallest closed set containing \"X\". Such families of \"closed sets\" are sometimes called \"Moore families\", in honor of E. H. Moore who studied closure operators in his 1910 \"Introduction to a form of general analysis\", whereas the concept of the closure of a subset originated in the work of Frigyes Riesz in connection with topological spaces.\n\nClosure operators are also called \"hull operators\", which prevents confusion with the \"closure operators\" studied in topology. A set together with a closure operator on it is sometimes called a closure space.\n\nClosure operators have many applications: \n\nIn topology, the closure operators are \"topological\" closure operators, which must satisfy\n\nfor all formula_4 (Note that for formula_5 this gives formula_6). \n\nIn algebra and logic, many closure operators are finitary closure operators, i.e. they satisfy\n\nIn universal logic, closure operators are also known as consequence operators. \n\nIn the theory of partially ordered sets, which are important in theoretical computer science, closure operators have an alternative definition.\n\nThe topological closure of a subset \"X\" of a topological space consists of all points \"y\" of the space, such that every neighbourhood of \"y\" contains a point of \"X\". The function that associates to every subset \"X\" its closure is a topological closure operator. Conversely, every topological closure operator on a set gives rise to a topological space whose closed sets are exactly the closed sets with respect to the closure operator.\n\nFor topological closure operators the second closure axiom (being increasing) is redundant.\n\nFinitary closure operators play a relatively prominent role in universal algebra, and in this context they are traditionally called \"algebraic closure operators\". Every subset of an algebra \"generates\" a subalgebra: the smallest subalgebra containing the set. This gives rise to a finitary closure operator.\n\nPerhaps the best known example for this is the function that associates to every subset of a given vector space its linear span. Similarly, the function that associates to every subset of a given group the subgroup generated by it, and similarly for fields and all other types of algebraic structures.\n\nThe linear span in a vector space and the similar algebraic closure in a field both satisfy the \"exchange property:\" If \"x\" is in the closure of the union of \"A\" and {\"y\"} but not in the closure of \"A\", then \"y\" is in the closure of the union of \"A\" and {\"x\"}. A finitary closure operator with this property is called a matroid. The dimension of a vector space, or the transcendence degree of a field (over its prime field) is exactly the rank of the corresponding matroid.\n\nThe function that maps every subset of a given field to its algebraic closure is also a finitary closure operator, and in general it is different from the operator mentioned before. Finitary closure operators that generalize these two operators are studied in model theory as dcl (for \"definable closure\") and acl (for \"algebraic closure\").\n\nThe convex hull in \"n\"-dimensional Euclidean space is another example of a finitary closure operator. It satisfies the \"anti-exchange property:\" If \"x\" is not contained in the union of \"A\" and {\"y\"}, but in its closure, then \"y\" is not contained in the closure of the union of \"A\" and {\"x\"}. Finitary closure operators with this property give rise to antimatroids.\n\nSuppose you have some logical formalism that contains certain rules allowing you to derive new formulas from given ones. Consider the set \"F\" of all possible formulas, and let \"P\" be the power set of \"F\", ordered by ⊆. For a set \"X\" of formulas, let cl(\"X\") be the set of all formulas that can be derived from \"X\". Then cl is a closure operator on \"P\". More precisely, we can obtain cl as follows. Call \"continuous\" an operator \"J\" such that, for every directed class \"T\",\n\nThis continuity condition is on the basis of a fixed point theorem for \"J\". Consider the one-step operator \"J\" of a monotone logic. This is the operator associating any set \"X\" of formulas with the set \"J(X)\" of formulas which are either logical axioms or are obtained by an inference rule from formulas in \"X\" or are in \"X\". Then such an operator is continuous and we can define cl(\"X\") as the least fixed point for \"J\" greater or equal to \"X\". In accordance with such a point of view, Tarski, Brown, Suszko and other authors proposed a general approach to logic based on closure operator theory. Also, such an idea is proposed in programming logic (see Lloyd 1987) and in fuzzy logic (see Gerla 2000).\n\nAround 1930, Alfred Tarski developed an abstract theory of logical deductions which models some properties of logical calculi. Mathematically, what he described is just a finitary closure operator on a set (the set of \"sentences\"). In universal logic, finitary closure operators are still studied under the name \"consequence operator\", which was coined by Tarski. The set \"S\" represents a set of sentences, a subset \"T\" of \"S\" a theory, and cl(\"T\") is the set of all sentences that follow from the theory. Nowadays the term can refer to closure operators which need not be finitary; finitary closure operators are then sometimes called finite consequence operators.\n\nThe closed sets with respect to a closure operator on \"S\" form a subset \"C\" of the power set P(\"S\"). Any intersection of sets in \"C\" is again in \"C\". In other words, \"C\" is a complete meet-subsemilattice of P(\"S\"). Conversely, if \"C\" ⊆ P(\"S\") is closed under arbitrary intersections, then the function that associates to every subset \"X\" of \"S\" the smallest set \"Y\" ∈ \"C\" such that \"X\" ⊆ \"Y\" is a closure operator.\n\nThere is a simple and fast algorithm for generating all closed sets of a given closure operator.\n\nA closure operator on a set is topological if and only if the set of closed sets is closed under finite unions, i.e., \"C\" is a meet-complete sublattice of P(\"S\"). Even for non-topological closure operators, \"C\" can be seen as having the structure of a lattice. (The join of two sets \"X\",\"Y\" ⊆ P(\"S\") being cl(\"X\" formula_8 \"Y\").) But then \"C\" is not a sublattice of the lattice P(\"S\").\n\nGiven a finitary closure operator on a set, the closures of finite sets are exactly the compact elements of the set \"C\" of closed sets. It follows that \"C\" is an algebraic poset.\nSince \"C\" is also a lattice, it is often referred to as an algebraic lattice in this context. Conversely, if \"C\" is an algebraic poset, then the closure operator is finitary.\n\nEach closure operator on a finite set \"S\" is uniquely determined by its images of its \"pseudo-closed\" sets.\nThese are recursively defined: A set is pseudo-closed if it is not closed and contains the closure of each of its pseudo-closed proper subsets. Formally: \"P\"⊆\"S\" is pseudo-closed if and only if\n\nA partially ordered set (poset) is a set together with a \"partial order\" ≤, i.e. a binary relation which is reflexive (), transitive ( implies ) and antisymmetric ( implies \"a\" = \"b\"). Every power set P(\"S\") together with inclusion ⊆ is a partially ordered set.\n\nA function cl: \"P\" → \"P\" from a partial order \"P\" to itself is called a closure operator if it satisfies the following axioms for all elements \"x\", \"y\" in \"P\".\n\nMore succinct alternatives are available: the definition above is equivalent to the single axiom\n\nfor all \"x\", \"y\" in \"P\".\n\nUsing the pointwise order on functions between posets, one may alternatively write the extensiveness property as id ≤ cl, where id is the identity function. A self-map \"k\" that is increasing and idempotent, but satisfies the dual of the extensiveness property, i.e. \"k\" ≤ id is called a kernel operator, interior operator, or dual closure. As examples, if \"A\" is a subset of a set \"B\", then the self-map on the powerset of \"B\" given by \"μ\"(\"X\") = \"A\" ∪ \"X\" is a closure operator, whereas \"λ\"(\"X\") = \"A\" ∩ \"X\" is a kernel operator. The ceiling function from the real numbers to the real numbers, which assigns to every real \"x\" the smallest integer not smaller than \"x\", is another example of a closure operator.\n\nA fixpoint of the function cl, i.e. an element \"c\" of \"P\" that satisfies cl(\"c\") = \"c\", is called a closed element. A closure operator on a partially ordered set is determined by its closed elements. If \"c\" is a closed element, then \"x\" ≤ \"c\" and cl(\"x\") ≤ \"c\" are equivalent conditions.\n\nEvery Galois connection (or residuated mapping) gives rise to a closure operator (as is explained in that article). In fact, \"every\" closure operator arises in this way from a suitable Galois connection. The Galois connection is not uniquely determined by the closure operator. One Galois connection that gives rise to the closure operator cl can be described as follows: if \"A\" is the set of closed elements with respect to cl, then cl: \"P\" → \"A\" is the lower adjoint of a Galois connection between \"P\" and \"A\", with the upper adjoint being the embedding of \"A\" into \"P\". Furthermore, every lower adjoint of an embedding of some subset into \"P\" is a closure operator. \"Closure operators are lower adjoints of embeddings.\" Note however that not every embedding has a lower adjoint.\n\nAny partially ordered set \"P\" can be viewed as a category, with a single morphism from \"x\" to \"y\" if and only if \"x\" ≤ \"y\". The closure operators on the partially ordered set \"P\" are then nothing but the monads on the category \"P\". Equivalently, a closure operator can be viewed as an endofunctor on the category of partially ordered sets that has the additional \"idempotent\" and \"extensive\" properties.\n\nIf \"P\" is a complete lattice, then a subset \"A\" of \"P\" is the set of closed elements for some closure operator on \"P\" if and only if \"A\" is a Moore family on \"P\", i.e. the largest element of \"P\" is in \"A\", and the infimum (meet) of any non-empty subset of \"A\" is again in \"A\". Any such set \"A\" is itself a complete lattice with the order inherited from \"P\" (but the supremum (join) operation might differ from that of \"P\"). When \"P\" is the powerset Boolean algebra of a set \"X\", then a Moore family on \"P\" is called a closure system on \"X\".\n\nThe closure operators on \"P\" form themselves a complete lattice; the order on closure operators is defined by cl ≤ cl iff cl(\"x\") ≤ cl(\"x\") for all \"x\" in \"P\".\n\n\n\n"}
{"id": "15690807", "url": "https://en.wikipedia.org/wiki?curid=15690807", "title": "Count data", "text": "Count data\n\nIn statistics, count data is a statistical data type, a type of data in which the observations can take only the non-negative integer values {0, 1, 2, 3, ...}, and where these integers arise from counting rather than ranking. The statistical treatment of count data is distinct from that of binary data, in which the observations can take only two values, usually represented by 0 and 1, and from ordinal data, which may also consist of integers but where the individual values fall on an arbitrary scale and only the relative ranking is important.\n\nStatistical analyses involving count data includes simple counts, such as the number of occurrences of thunderstorms in a calendar year, and categorical data in which the counts represent the numbers of items falling into each of several categories.\n\nAn individual piece of count data is often termed a count variable. When such a variable is treated as a random variable, the Poisson, binomial and negative binomial distributions are commonly used to represent its distribution.\n\nGraphical examination of count data may be aided by the use of data transformations chosen to have the property of stabilising the sample variance. In particular, the square root transformation might be used when data can be approximated by a Poisson distribution (although other transformation have modestly improved properties), while an inverse sine transformation is available when a binomial distribution is preferred.\n\nHere the count variable would be treated as a dependent variable. Statistical methods such as least squares and analysis of variance are designed to deal with continuous dependent variables. These can be adapted to deal with count data by using data transformations such as the square root transformation, but such methods have several drawbacks; they are approximate at best and estimate parameters that are often hard to interpret.\n\nThe Poisson distribution can form the basis for some analyses of count data and in this case Poisson regression may be used. This is a special case of the class of generalized linear models which also contains specific forms of model capable of using the binomial distribution (binomial regression, logistic regression) or the negative binomial distribution where the assumptions of the Poisson model are violated, in particular when the range of count values is limited or when overdispersion is present.\n\n\n"}
{"id": "1038753", "url": "https://en.wikipedia.org/wiki?curid=1038753", "title": "Cut rule", "text": "Cut rule\n\nIn mathematical logic, the cut rule is an inference rule of sequent calculus. It is a generalisation of the classical modus ponens inference rule. Its meaning is that, if a formula \"A\" appears as a conclusion in one proof and a hypothesis in another, then another proof in which the formula \"A\" does not appear can be deduced. In the particular case of the modus ponens, for example occurrences of \"man\" are eliminated of \"Every man is mortal, Socrates is a man\" to deduce \"Socrates is mortal\".\n\nFormal notation in sequent calculus notation :\n\nThe cut rule is the subject of an important theorem, the cut elimination theorem. It states that any judgement that possesses a proof in the sequent calculus that makes use of the cut rule also possesses a cut-free proof, that is, a proof that does not make use of the cut rule.\n"}
{"id": "33509133", "url": "https://en.wikipedia.org/wiki?curid=33509133", "title": "Cutting sequence", "text": "Cutting sequence\n\nIn digital geometry, a cutting sequence is a sequence of symbols whose elements correspond to the individual grid lines crossed (\"cut\") as a curve crosses a square grid.\n\nSturmian words are a special case of cutting sequences where the curves are straight lines of irrational slope.\n"}
{"id": "4135768", "url": "https://en.wikipedia.org/wiki?curid=4135768", "title": "Deviant logic", "text": "Deviant logic\n\nPhilosopher Susan Haack uses the term \"deviant logic\" to describe certain non-classical systems of logic. In these logics,\n\n\nThe set of theorems of a deviant logic can differ in any possible way from classical logic's set of theorems: as a proper subset, superset, or fully exclusive set. A notable example of this is the trivalent logic developed by Polish logician and mathematician Jan Łukasiewicz. Under this system, any theorem necessarily dependent on classical logic's principle of bivalence would fail to be valid. The term first appears in Chapter 6 of Willard Van Orman Quine's \"Philosophy of Logic\", New Jersey: Prentice Hall (1970), which is cited by Haack on p. 15 of her book.\n\nHaack also described what she calls a \"quasi\"-deviant logic. These logics are different from pure deviant logics in that:\n\n\nFinally, Haack defined a class of merely \"extended\" logics. In these,\n\n\nSome systems of modal logic meet this definition. In such systems, any novel theorem would not parse in classical logic due to modal operators. While deviant and quasi-deviant logics are typically proposed as rivals to classical logic, the impetus behind extended logics is normally only to provide a supplement to it.\n\nAchille Varzi in his review of the 1996 edition of Haack's book writes that the survey did not stand well the test of time, particularly with the \"extraordinary proliferation of nonclassical logics in the past two decades—paraconsistent logics, linear logics, substructural logics, nonmonotonic logics, innumerable other logics for AI and computer science.\" He also finds that Haack's account of vagueness \"is now seriously defective.\" He concedes however that \"as a defense of a philosophical position, \"Deviant Logic\" retains its significance.\"\n"}
{"id": "30721658", "url": "https://en.wikipedia.org/wiki?curid=30721658", "title": "Euler Book Prize", "text": "Euler Book Prize\n\nThe Euler Book Prize is an award named after Swiss mathematician and physicist Leonhard Euler (1707-1783) and given annually at the Joint Mathematics Meetings by the Mathematical Association of America to an outstanding book in mathematics that is likely to improve the public view of the field.\n\nThe prize was founded in 2005 with funds provided by mathematician Paul Halmos (1916–2006) and his wife Virginia. It was first given in 2007; this date was chosen to honor the 300th anniversary of Euler's birth, as part of the MAA \"Year of Euler\" celebration.\n\n"}
{"id": "1977119", "url": "https://en.wikipedia.org/wiki?curid=1977119", "title": "FastICA", "text": "FastICA\n\nFastICA is an efficient and popular algorithm for independent component analysis invented by Aapo Hyvärinen at Helsinki University of Technology. Like most ICA algorithms, FastICA seeks an orthogonal rotation of prewhitened data, through a fixed-point iteration scheme, that maximizes a measure of non-Gaussianity of the rotated components. Non-gaussianity serves as a proxy for statistical independence, which is a very strong condition and requires infinite data to verify. FastICA can also be alternatively derived as an approximative Newton iteration.\n\nLet the formula_1 denote the input data matrix, formula_2 the number of columns corresponding with the number of samples of mixed signals and formula_3 the number of rows corresponding with the number of independent source signals. The input data matrix formula_4 must be \"prewhitened\", or centered and whitened, before applying the FastICA algorithm to it.\n\nformula_6 \nformula_16\nformula_21\n\nThe iterative algorithm finds the direction for the weight vector formula_22\nthat maximizes a measure of non-Gaussianity of the projection formula_23, \nwith formula_24 denoting a prewhitened data matrix as described above.\nNote that formula_25 is a column vector. To measure non-Gaussianity, FastICA relies on a nonquadratic nonlinear function formula_26, its first derivative formula_27, and its second derivative formula_28. Hyvärinen states that the functions \nformula_29\nare useful for general purposes, while \nformula_30 \nmay be highly robust. The steps for extracting the weight vector formula_25 for single component in FastICA are the following: \n\nThe single unit iterative algorithm estimates only one weight vector which extracts a single component. Estimating additional components that are mutually \"independent\" requires repeating the algorithm to obtain linearly independent projection vectors - note that the notion of independence here refers to maximizing non-Gaussianity in the estimated components. Hyvärinen provides several ways of extracting multiple components with the simplest being the following. Here, formula_37 is a column vector of 1's of dimension formula_2.\n\nAlgorithm FastICA\n\n\n"}
{"id": "1461517", "url": "https://en.wikipedia.org/wiki?curid=1461517", "title": "Fixed-point space", "text": "Fixed-point space\n\nIn mathematics, a Hausdorff space \"X\" is called a fixed-point space if every continuous function formula_1 has a fixed point.\n\nFor example, any closed interval [a,b] in formula_2 is a fixed point space, and it can be proved from the intermediate value property of real continuous function. The open interval (\"a\", \"b\"), however, is not a fixed point space. To see it, consider the function \nformula_3, for example. \n\nAny linearly ordered space that is connected and has a top and a bottom element is a fixed point space. \n\nNote that, in the definition, we could easily have disposed of the condition that the space is Hausdorff.\n\n"}
{"id": "43198751", "url": "https://en.wikipedia.org/wiki?curid=43198751", "title": "Fulkerson–Chen–Anstee theorem", "text": "Fulkerson–Chen–Anstee theorem\n\nThe Fulkerson–Chen–Anstee theorem is a result in graph theory, a branch of combinatorics. It provides one of two known approaches solving the digraph realization problem, i.e. it gives a necessary and sufficient condition for pairs of nonnegative integers formula_1 to be the indegree-outdegree pairs of a simple directed graph; a sequence obeying these conditions is called \"digraphic\". D. R. Fulkerson (1960) obtained a characterization analogous to the classical Erdős–Gallai theorem for graphs, but in contrast to this solution with exponentially many inequalities. In 1966 Chen improved this result in demanding the additional constraint that the integer pairs must be sorted in non-increasing lexicographical order leading to n inequalities. Anstee (1982) observed in a different context that it is suffient to have formula_2. Berger reinvented this result and gives a direct proof.\n\nA sequence formula_3 of nonnegative integer pairs with formula_2 is digraphic if and only if formula_5 and the following inequality holds for \"k\" such that formula_6:\n\nBerger proved that it suffices to consider the formula_8th inequality such that formula_9 with formula_10 and for formula_11.\n\nThe theorem can also be stated in terms of zero-one matrices. The connection can be seen if one realizes that each directed graph has an adjacency matrix where the column sums and row sums correspond to formula_12 and formula_13. Note that the diagonal of the matrix only contains zeros. There is a connection to the relation majorization. We define a sequence formula_14 with formula_15. Sequence formula_14 can also be determined by a corrected Ferrers diagram. Consider sequences formula_12, formula_13 and formula_14 as formula_20-dimensional vectors formula_21, formula_22 and formula_23. Since formula_24 by applying the principle of double counting, the theorem above states that a pair of nonnegative integer sequences formula_25 with nonincreasing formula_21 is digraphic if and only if vector formula_23 majorizes formula_21.\n\nA sequence formula_3 of nonnegative integer pairs with formula_2 is digraphic if and only if formula_5 and there exists a sequence formula_32 such that the pair formula_33 is digraphic and formula_32 majorizes formula_21.\n\nSimilar theorems describe the degree sequences of simple graphs, simple directed graphs with loops, and simple bipartite graphs. The first problem is characterized by the Erdős–Gallai theorem. The latter two cases, which are equivalent see Berger, are characterized by the Gale–Ryser theorem.\n\n"}
{"id": "22955929", "url": "https://en.wikipedia.org/wiki?curid=22955929", "title": "General elephant", "text": "General elephant\n\nIn algebraic geometry, general elephant is an idiosyncratic name for a general element of the anticanonical system of a variety, introduced by . For 3-folds the general elephant problem (or conjecture) asks whether general elephants have at most du Val singularities; this has been proved in several cases.\n"}
{"id": "31628660", "url": "https://en.wikipedia.org/wiki?curid=31628660", "title": "Generality of algebra", "text": "Generality of algebra\n\nIn the history of mathematics, the generality of algebra was a phrase used by Augustin-Louis Cauchy to describe a method of argument that was used in the 18th century by mathematicians such as Leonhard Euler and Joseph-Louis Lagrange, particularly in manipulating infinite series. According to Koetsier, the generality of algebra principle assumed, roughly, that the algebraic rules that hold for a certain class of expressions can be extended to hold more generally on a larger class of objects, even if the rules are no longer obviously valid. As a consequence, 18th century mathematicians believed that they could derive meaningful results by applying the usual rules of algebra and calculus that hold for finite expansions even when manipulating infinite expansions. In works such as \"Cours d'Analyse\", Cauchy rejected the use of \"generality of algebra\" methods and sought a more rigorous foundation for mathematical analysis.\n\nAn example is Euler's derivation of the series\n\nfor formula_1. He first evaluated the identity\n\nat formula_2 to obtain\n\nThe infinite series on the right hand side of () diverges for all real formula_3. But nevertheless integrating this term-by-term gives (), an identity which is known to be true by modern methods.\n"}
{"id": "10782668", "url": "https://en.wikipedia.org/wiki?curid=10782668", "title": "Irregularity of distributions", "text": "Irregularity of distributions\n\nThe irregularity of distributions problem, stated first by Hugo Steinhaus, is a numerical problem with a surprising result. The problem is to find \"N\" numbers, formula_1, all between 0 and 1, for which the following conditions hold:\n\n\nMathematically, we are looking for a sequence of real numbers\n\nsuch that for every \"n\" ∈ {1, ..., \"N\"} and every \"k\" ∈ {1, ..., \"n\"} there is some \"i\" ∈ {1, ..., \"n\"} such that\n\nThe surprising result is that there is a solution up to \"N\" = 17, but starting at \"N\" = 18 and above it is impossible. A possible solution for \"N\" ≤ 17 is shown diagrammatically on the right; numerically it is as follows:\n\nIn this example, considering for instance the first 5 numbers, we have\n\n"}
{"id": "3956618", "url": "https://en.wikipedia.org/wiki?curid=3956618", "title": "Join and meet", "text": "Join and meet\n\nIn a partially ordered set \"P\", the join and meet of a subset \"S\" are respectively the supremum (least upper bound) of \"S\", denoted ⋁\"S\", and infimum (greatest lower bound) of \"S\", denoted ⋀\"S\". In general, the join and meet of a subset of a partially ordered set need not exist; when they do exist, they are elements of \"P\". \n\nJoin and meet can also be defined as a commutative, associative and idempotent partial binary operation on pairs of elements from \"P\". If a and b are elements from \"P\", the join is denoted as a ∨ b and the meet is denoted a ∧ b. \n\nJoin and meet are symmetric duals with respect to order inversion. The join/meet of a subset of a totally ordered set is simply its maximal/minimal element, if such an element exists.\n\nA partially ordered set in which all pairs have a join is a join-semilattice. Dually, a partially ordered set in which all pairs have a meet is a meet-semilattice. A partially ordered set that is both a join-semilattice and a meet-semilattice is a lattice. A lattice in which every subset, not just every pair, possesses a meet and a join is a complete lattice. It is also possible to define a partial lattice, in which not all pairs have a meet or join but the operations (when defined) satisfy certain axioms.\n\nLet \"A\" be a set with a partial order ≤, and let \"x\" and \"y\" be two elements in \"A\". An element \"z\" of \"A\" is the meet (or greatest lower bound or infimum) of \"x\" and \"y\", if the following two conditions are satisfied:\nIf there is a meet of \"x\" and \"y\", then it is unique, since if both \"z\" and \"z\"′ are greatest lower bounds of \"x\" and \"y\", then and , and thus \"z\" = \"z\"′. If the meet does exist, it is denoted .\nSome pairs of elements in \"A\" may lack a meet, either since they have no lower bound at all, or since none of their lower bounds is greater than all the others. If all pairs of elements have meets, then the meet is a binary operation on \"A\", and it is easy to see that this operation fulfills the following three conditions: For any elements \"x\", \"y\", and \"z\" in \"A\",\n\nBy definition, a binary operation ∧ on a set \"A\" is a \"meet\", if it satisfies the three conditions a, b, and c. The pair (\"A\",∧) then is a meet-semilattice. Moreover, we then may define a binary relation ≤ on \"A\", by stating that if and only if \"x\" ∧ \"y\" = \"x\". In fact, this relation is a partial order on \"A\". Indeed, for any elements \"x\", \"y\", and \"z\" in \"A\",\n\nNote that both meets and joins equally satisfy this definition: a couple of associated meet and join operations yield partial orders which are the reverse of each other. When choosing one of these orders as the main ones, one also fixes which operation is considered a meet (the one giving the same order) and which is considered a join (the other one).\n\nIf (\"A\",≤) is a partially ordered set, such that each pair of elements in \"A\" has a meet, then indeed \"x\" ∧ \"y\" = \"x\" if and only if , since in the latter case indeed \"x\" is a lower bound of \"x\" and \"y\", and since clearly \"x\" is the \"greatest\" lower bound if and only if it is a lower bound. Thus, the partial order defined by the meet in the universal algebra approach coincides with the original partial order.\n\nConversely, if (\"A\",∧) is a meet-semilattice, and the partial order ≤ is defined as in the universal algebra approach, and \"z\" = \"x\" ∧ \"y\" for some elements \"x\" and \"y\" in \"A\", then \"z\" is the greatest lower bound of \"x\" and \"y\" with respect to ≤, since\nand therefore . Similarly, , and if \"w\" is another lower bound of \"x\" and \"y\", then \"w\" ∧ \"x\" = \"w\" ∧ \"y\" = w, whence\nThus, there is a meet defined by the partial order defined by the original meet, and the two meets coincide.\n\nIn other words, the two approaches yield essentially equivalent concepts, a set equipped with both a binary relation and a binary operation, such that each one of these structures determines the other, and fulfil the conditions for partial orders or meets, respectively.\n\nIf (\"A\",∧) is a meet-semilattice, then the meet may be extended to a well-defined meet of any non-empty finite set, by the technique described in iterated binary operations. Alternatively, if the meet defines or is defined by a partial order, some subsets of \"A\" indeed have infima with respect to this, and it is reasonable to consider such an infimum as the meet of the subset. For non-empty finite subsets, the two approaches yield the same result, whence either may be taken as a definition of meet. In the case where \"each\" subset of \"A\" has a meet, in fact (\"A\",≤) is a complete lattice; for details, see completeness (order theory).\n\n"}
{"id": "5647048", "url": "https://en.wikipedia.org/wiki?curid=5647048", "title": "Karl Küpfmüller", "text": "Karl Küpfmüller\n\nKarl Küpfmüller (6 October 1897 – 26 December 1977) was a German electrical engineer, who was prolific in the areas of communications technology, measurement and control engineering, acoustics, communication theory and theoretical electro-technology.\n\nKüpfmüller was born in Nuremberg, where he studied at the Ohm-Polytechnikum. After returning from military service in World War I, he worked at the telegraph research division of the German Post in Berlin as a co-worker of Karl Willy Wagner, and, from 1921, he was lead engineer at the central laboratory of Siemens & Halske AG in the same city.\n\nIn 1928 he became full professor of general and theoretical electrical engineering at the \"Technische Hochschule\" in Danzig, and later held the same position in Berlin. Küpfmüller joined the National Socialist Motor Corps in 1933. In the following year he also joined the SA. In 1937 Küpfmüller joined the NSDAP and became a member of the SS, where he reached the rank of Obersturmbannführer.\n\nKüpfmüller was appointed as director of communication technology Research & Development at the Siemens-Wernerwerk for telegraphy. In 1941–1945 he was director of the central R&D division at Siemens & Halske in 1937.\n\nLater he was honorary professor at the \"Technische Hochschule Berlin\". In 1968, he received the Werner von Siemens Ring for his contributions to the theory of telecommunications and other electro-technology.\n\nHe died at Darmstadt.\n\nAbout 1928, he did the same analysis that Harry Nyquist did, to show that not more than 2B independent pulses per second could be put through a channel of bandwidth B. He did this by quantifying the time-bandwidth product \"k\" of various communication signal types, and showing that \"k\" could never be less than 1/2. From his 1931 paper (rough translation from Swedish):\n\n\n"}
{"id": "43278241", "url": "https://en.wikipedia.org/wiki?curid=43278241", "title": "Knuth's Simpath algorithm", "text": "Knuth's Simpath algorithm\n\nSimpath is an algorithm introduced by Donald Knuth that constructs a zero-suppressed decision diagram (ZDD) representing all simple paths between two vertices in a given graph.\n\n"}
{"id": "18610", "url": "https://en.wikipedia.org/wiki?curid=18610", "title": "Laplace transform", "text": "Laplace transform\n\nIn mathematics, the Laplace transform is an integral transform named after its discoverer Pierre-Simon Laplace (). It takes a function of a real variable (often time) to a function of a complex variable (complex frequency).\n\nThe Laplace transform is very similar to the Fourier transform. While the Fourier transform of a function is a complex function of a \"real\" variable (frequency), the Laplace transform of a function is a complex function of a \"complex variable\". Laplace transforms are usually restricted to functions of with . A consequence of this restriction is that the Laplace transform of a function is a holomorphic function of the variable . Unlike the Fourier transform, the Laplace transform of a distribution is generally a well-behaved function. Techniques of complex variables can also be used to directly study Laplace transforms. As a holomorphic function, the Laplace transform has a power series representation. This power series expresses a function as a linear superposition of moments of the function. This perspective has applications in probability theory.\n\nThe Laplace transform is invertible on a large class of functions. The inverse Laplace transform takes a function of a complex variable \"s\" (often frequency) and yields a function of a real variable \"t\" (time). Given a simple mathematical or functional description of an input or output to a system, the Laplace transform provides an alternative functional description that often simplifies the process of analyzing the behavior of the system, or in synthesizing a new system based on a set of specifications. So, for example, Laplace transformation from the time domain to the frequency domain transforms differential equations into algebraic equations and convolution into multiplication. It has many applications in the sciences and technology.\n\nThe Laplace transform is named after mathematician and astronomer Pierre-Simon Laplace, who used a similar transform in his work on probability theory. Laplace's use of generating functions was similar to what is now known as the z-transform and he gave little attention to the continuous variable case which was discussed by Abel. The theory was further developed in the 19th and early 20th centuries by Lerch, Heaviside, and Bromwich. The current widespread use of the transform (mainly in engineering) came about during and soon after World War II replacing the earlier Heaviside operational calculus. The advantages of the Laplace transform had been emphasized by Doetsch to whom the name Laplace Transform is apparently due. \n\nThe early history of methods having some similarity to Laplace transform is as follows. From 1744, Leonhard Euler investigated integrals of the form\nas solutions of differential equations but did not pursue the matter very far.\n\nJoseph Louis Lagrange was an admirer of Euler and, in his work on integrating probability density functions, investigated expressions of the form\nwhich some modern historians have interpreted within modern Laplace transform theory.\n\nThese types of integrals seem first to have attracted Laplace's attention in 1782 where he was following in the spirit of Euler in using the integrals themselves as solutions of equations. However, in 1785, Laplace took the critical step forward when, rather than just looking for a solution in the form of an integral, he started to apply the transforms in the sense that was later to become popular. He used an integral of the form\nakin to a Mellin transform, to transform the whole of a difference equation, in order to look for solutions of the transformed equation. He then went on to apply the Laplace transform in the same way and started to derive some of its properties, beginning to appreciate its potential power.\n\nLaplace also recognised that Joseph Fourier's method of Fourier series for solving the diffusion equation could only apply to a limited region of space because those solutions were periodic. In 1809, Laplace applied his transform to find solutions that diffused indefinitely in space.\n\nThe Laplace transform of a function , defined for all real numbers , is the function , which is a unilateral transform defined by\nwhere \"s\" is a complex number frequency parameter\n\nAn alternate notation for the Laplace transform is formula_6 instead of .\n\nThe meaning of the integral depends on types of functions of interest. A necessary condition for existence of the integral is that must be locally integrable on . For locally integrable functions that decay at infinity or are of exponential type, the integral can be understood to be a (proper) Lebesgue integral. However, for many applications it is necessary to regard it as a conditionally convergent improper integral at . Still more generally, the integral can be understood in a weak sense, and this is dealt with below.\n\nOne can define the Laplace transform of a finite Borel measure by the Lebesgue integral\n\nAn important special case is where is a probability measure, for example, the Dirac delta function. In operational calculus, the Laplace transform of a measure is often treated as though the measure came from a probability density function . In that case, to avoid potential confusion, one often writes\nwhere the lower limit of is shorthand notation for\n\nThis limit emphasizes that any point mass located at is entirely captured by the Laplace transform. Although with the Lebesgue integral, it is not necessary to take such a limit, it does appear more naturally in connection with the Laplace–Stieltjes transform.\n\nIn pure and applied probability, the Laplace transform is defined as an expected value. If is a random variable with probability density function , then the Laplace transform of is given by the expectation\n\nBy convention, this is referred to as the Laplace transform of the random variable itself. Replacing by gives the moment generating function of . The Laplace transform has applications throughout probability theory, including first passage times of stochastic processes such as Markov chains, and renewal theory.\n\nOf particular use is the ability to recover the cumulative distribution function of a continuous random variable by means of the Laplace transform as follows\n\nWhen one says \"the Laplace transform\" without qualification, the unilateral or one-sided transform is normally intended. The Laplace transform can be alternatively defined as the \"bilateral Laplace transform\" or two-sided Laplace transform by extending the limits of integration to be the entire real axis. If that is done the common unilateral transform simply becomes a special case of the bilateral transform where the definition of the function being transformed is multiplied by the Heaviside step function.\n\nThe bilateral Laplace transform is defined as follows,\n\nTwo integrable functions have the same Laplace transform only if they differ on a set of Lebesgue measure zero. This means that, on the range of the transform, there is an inverse transform. In fact, besides integrable functions, the Laplace transform is a one-to-one mapping from one function space into another in many other function spaces as well, although there is usually no easy characterization of the range. Typical function spaces in which this is true include the spaces of bounded continuous functions, the space , or more generally tempered distributions (that is, functions of at worst polynomial growth) on . The Laplace transform is also defined and injective for suitable spaces of tempered distributions.\n\nIn these cases, the image of the Laplace transform lives in a space of analytic functions in the region of convergence. The inverse Laplace transform is given by the following complex integral, which is known by various names (the Bromwich integral, the Fourier–Mellin integral, and Mellin's inverse formula):\nwhere is a real number so that the contour path of integration is in the region of convergence of . An alternative formula for the inverse Laplace transform is given by Post's inversion formula. The limit here is interpreted in the weak-* topology.\n\nIn practice, it is typically more convenient to decompose a Laplace transform into known transforms of functions obtained from a table, and construct the inverse by inspection.\n\nIf is a locally integrable function (or more generally a Borel measure locally of bounded variation), then the Laplace transform of converges provided that the limit\nexists.\n\nThe Laplace transform converges absolutely if the integral\nexists (as a proper Lebesgue integral). The Laplace transform is usually understood as conditionally convergent, meaning that it converges in the former instead of the latter sense.\n\nThe set of values for which converges absolutely is either of the form or else , where is an extended real constant, . (This follows from the dominated convergence theorem.) The constant is known as the abscissa of absolute convergence, and depends on the growth behavior of . Analogously, the two-sided transform converges absolutely in a strip of the form , and possibly including the lines or . The subset of values of for which the Laplace transform converges absolutely is called the region of absolute convergence or the domain of absolute convergence. In the two-sided case, it is sometimes called the strip of absolute convergence. The Laplace transform is analytic in the region of absolute convergence: this is a consequence of Fubini's theorem and Morera's theorem. \n\nSimilarly, the set of values for which converges (conditionally or absolutely) is known as the region of conditional convergence, or simply the region of convergence (ROC). If the Laplace transform converges (conditionally) at , then it automatically converges for all with . Therefore, the region of convergence is a half-plane of the form , possibly including some points of the boundary line .\n\nIn the region of convergence , the Laplace transform of can be expressed by integrating by parts as the integral\n\nThat is, in the region of convergence can effectively be expressed as the absolutely convergent Laplace transform of some other function. In particular, it is analytic.\n\nThere are several Paley–Wiener theorems concerning the relationship between the decay properties of and the properties of the Laplace transform within the region of convergence.\n\nIn engineering applications, a function corresponding to a linear time-invariant (LTI) system is \"stable\" if every bounded input produces a bounded output. This is equivalent to the absolute convergence of the Laplace transform of the impulse response function in the region . As a result, LTI systems are stable provided the poles of the Laplace transform of the impulse response function have negative real part.\n\nThis ROC is used in knowing about the causality and stability of a system.\n\nThe Laplace transform has a number of properties that make it useful for analyzing linear dynamical systems. The most significant advantage is that differentiation and integration become multiplication and division, respectively, by (similarly to logarithms changing multiplication of numbers to addition of their logarithms).\n\nBecause of this property, the Laplace variable is also known as \"operator variable\" in the domain: either \"derivative operator\" or (for \"integration operator\". The transform turns integral equations and differential equations to polynomial equations, which are much easier to solve. Once solved, use of the inverse Laplace transform reverts to the time domain.\n\nGiven the functions and , and their respective Laplace transforms and ,\n\nThe following table is a list of properties of unilateral Laplace transform:\n\n\nThe Laplace transform can be viewed as a continuous analogue of a power series. If is a discrete function of a positive integer , then the power series associated to is the series\nwhere is a real variable (see Z transform). Replacing summation over with integration over , a continuous version of the power series becomes\nwhere the discrete function is replaced by the continuous one . \n\nChanging the base of the power from to gives\n\nFor this to converge for, say, all bounded functions , it is necessary to require that . Making the substitution gives just the Laplace transform:\n\nIn other words, the Laplace transform is a continuous analog of a power series in which the discrete parameter is replaced by the continuous parameter , and is replaced by .\n\nThe quantities\n\nare the \"moments\" of the function . If the first moments of converge absolutely, then by repeated differentiation under the integral, \nThis is of special significance in probability theory, where the moments of a random variable are given by the expectation values formula_28. Then, the relation holds\n\nIt is often convenient to use the differentiation property of the Laplace transform to find the transform of a function's derivative. This can be derived from the basic expression for a Laplace transform as follows:\n\nyielding\n\nand in the bilateral case,\n\nThe general result\n\nwhere denotes the th derivative of , can then be established with an inductive argument.\n\nA useful property of the Laplace transform is the following:\n\nunder suitable assumptions on the behaviour of formula_35 in a right neighbourhood of formula_36 and on the decay rate of formula_35 in a left neighbourhood of formula_38. The above formula is a variation of integration by parts, with the operators \nformula_39 and formula_40 being replaced by formula_41 and formula_42. Let us prove the equivalent formulation:\n\nBy plugging in formula_44 the left-hand side turns into:\n\nbut assuming Fubini's theorem holds, by reversing the order of integration we get the wanted right-hand side.\n\nLet formula_46, then (see the table above)\n\nor\n\nLetting , gives one the identity\n\nprovided that the interchange of limits can be justified. Even when the interchange cannot be justified the calculation can be suggestive. For example, proceeding formally one has\n\nThe validity of this identity can be proved by other means. It is an example of a Frullani integral.\n\nAnother example is Dirichlet integral.\n\nThe (unilateral) Laplace–Stieltjes transform of a function is defined by the Lebesgue–Stieltjes integral\n\nThe function is assumed to be of bounded variation. If is the antiderivative of :\n\nthen the Laplace–Stieltjes transform of and the Laplace transform of coincide. In general, the Laplace–Stieltjes transform is the Laplace transform of the Stieltjes measure associated to . So in practice, the only distinction between the two transforms is that the Laplace transform is thought of as operating on the density function of the measure, whereas the Laplace–Stieltjes transform is thought of as operating on its cumulative distribution function.\n\nThe continuous Fourier transform is equivalent to evaluating the bilateral Laplace transform with imaginary argument or when the condition explained below is fulfilled, \n\nThis definition of the Fourier transform requires a prefactor of 1/2 on the reverse Fourier transform. This relationship between the Laplace and Fourier transforms is often used to determine the frequency spectrum of a signal or dynamical system.\n\nThe above relation is valid as stated if and only if the region of convergence (ROC) of contains the imaginary axis, .\n\nFor example, the function has a Laplace transform whose ROC is . As is a pole of , substituting in does not yield the Fourier transform of , which is proportional to the Dirac delta-function .\n\nHowever, a relation of the form\nholds under much weaker conditions. For instance, this holds for the above example provided that the limit is understood as a weak limit of measures (see vague topology). General conditions relating the limit of the Laplace transform of a function on the boundary to the Fourier transform take the form of Paley–Wiener theorems.\n\nThe Mellin transform and its inverse are related to the two-sided Laplace transform by a simple change of variables.\n\nIf in the Mellin transform\nwe set we get a two-sided Laplace transform.\n\nThe unilateral or one-sided Z-transform is simply the Laplace transform of an ideally sampled signal with the substitution of\nwhere is the sampling period (in units of time e.g., seconds) and is the sampling rate (in samples per second or hertz).\n\nLet\nbe a sampling impulse train (also called a Dirac comb) and\nbe the sampled representation of the continuous-time \n\nThe Laplace transform of the sampled signal is\n\nThis is the precise definition of the unilateral Z-transform of the discrete function \n\nwith the substitution of .\n\nComparing the last two equations, we find the relationship between the unilateral Z-transform and the Laplace transform of the sampled signal,\n\nThe similarity between the and Laplace transforms is expanded upon in the theory of time scale calculus.\n\nThe integral form of the Borel transform\n\nis a special case of the Laplace transform for an entire function of exponential type, meaning that\n\nfor some constants and . The generalized Borel transform allows a different weighting function to be used, rather than the exponential function, to transform functions not of exponential type. Nachbin's theorem gives necessary and sufficient conditions for the Borel transform to be well defined.\n\nSince an ordinary Laplace transform can be written as a special case of a two-sided transform, and since the two-sided transform can be written as the sum of two one-sided transforms, the theory of the Laplace-, Fourier-, Mellin-, and Z-transforms are at bottom the same subject. However, a different point of view and different characteristic problems are associated with each of these four major integral transforms.\n\nThe following table provides Laplace transforms for many common functions of a single variable. For definitions and explanations, see the \"Explanatory Notes\" at the end of the table.\n\nBecause the Laplace transform is a linear operator,\n\n\n\nUsing this linearity, and various trigonometric, hyperbolic, and complex number (etc.) properties and/or identities, some Laplace transforms can be obtained from others more quickly than by using the definition directly.\n\nThe unilateral Laplace transform takes as input a function whose time domain is the non-negative reals, which is why all of the time domain functions in the table below are multiples of the Heaviside step function, .\n\nThe entries of the table that involve a time delay are required to be causal (meaning that ). A causal system is a system where the impulse response is zero for all time prior to . In general, the region of convergence for causal systems is not the same as that of anticausal systems.\n\n\n|}\n\nThe Laplace transform is often used in circuit analysis, and simple conversions to the -domain of circuit elements can be made. Circuit elements can be transformed into impedances, very similar to phasor impedances.\n\nHere is a summary of equivalents:\n\nNote that the resistor is exactly the same in the time domain and the -domain. The sources are put in if there are initial conditions on the circuit elements. For example, if a capacitor has an initial voltage across it, or if the inductor has an initial current through it, the sources inserted in the -domain account for that.\n\nThe equivalents for current and voltage sources are simply derived from the transformations in the table above.\n\nThe Laplace transform is used frequently in engineering and physics; the output of a linear time-invariant system can be calculated by convolving its unit impulse response with the input signal. Performing this calculation in Laplace space turns the convolution into a multiplication; the latter being easier to solve because of its algebraic form. For more information, see control theory.\n\nThe Laplace transform can also be used to solve differential equations and is used extensively in mechanical engineering and electrical engineering. The Laplace transform reduces a linear differential equation to an algebraic equation, which can then be solved by the formal rules of algebra. The original differential equation can then be solved by applying the inverse Laplace transform. The English electrical engineer Oliver Heaviside first proposed a similar scheme, although without using the Laplace transform; and the resulting operational calculus is credited as the Heaviside calculus.\n\nIn nuclear physics, the following fundamental relationship governs radioactive decay: the number of radioactive atoms in a sample of a radioactive isotope decays at a rate proportional to . This leads to the first order linear differential equation\n\nwhere is the decay constant. The Laplace transform can be used to solve this equation.\n\nRearranging the equation to one side, we have\n\nNext, we take the Laplace transform of both sides of the equation:\n\nwhere\n\nand\n\nSolving, we find\n\nFinally, we take the inverse Laplace transform to find the general solution\n\nwhich is indeed the correct form for radioactive decay.\n\nIn the theory of electrical circuits, the current flow in a capacitor is proportional to the capacitance and rate of change in the electrical potential (in SI units). Symbolically, this is expressed by the differential equation\n\nwhere is the capacitance (in farads) of the capacitor, is the electric current (in amperes) through the capacitor as a function of time, and is the voltage (in volts) across the terminals of the capacitor, also as a function of time.\n\nTaking the Laplace transform of this equation, we obtain\n\nwhere\n\nand\n\nSolving for we have\n\nThe definition of the complex impedance (in ohms) is the ratio of the complex voltage divided by the complex current while holding the initial state at zero:\n\nUsing this definition and the previous equation, we find:\n\nwhich is the correct expression for the complex impedance of a capacitor. \nIn addition, the Laplace transform has large applications in control theory.\n\nConsider a linear time-invariant system with transfer function\n\nThe impulse response is simply the inverse Laplace transform of this transfer function:\n\nTo evaluate this inverse transform, we begin by expanding using the method of partial fraction expansion,\n\nThe unknown constants and are the residues located at the corresponding poles of the transfer function. Each residue represents the relative contribution of that singularity to the transfer function's overall shape.\n\nBy the residue theorem, the inverse Laplace transform depends only upon the poles and their residues. To find the residue , we multiply both sides of the equation by to get\n\nThen by letting , the contribution from vanishes and all that is left is\n\nSimilarly, the residue is given by\n\nNote that\nand so the substitution of and into the expanded expression for gives\n\nFinally, using the linearity property and the known transform for exponential decay (see \"Item\" #\"3\" in the \"Table of Laplace Transforms\", above), we can take the inverse Laplace transform of to obtain\nwhich is the impulse response of the system.\n\nThe same result can be achieved using the convolution property as if the system is a series of filters with transfer functions of and . That is, the inverse of\n\nis\n\nStarting with the Laplace transform,\n\nwe find the inverse by first rearranging terms in the fraction:\n\nWe are now able to take the inverse Laplace transform of our terms:\n\nThis is just the sine of the sum of the arguments, yielding:\n\nWe can apply similar logic to find that\n\nThe wide and general applicability of the Laplace transform and its inverse is illustrated by an application in astronomy which provides some information on the \"spatial distribution\" of matter of an astronomical source of radio-frequency thermal radiation too distant to resolve as more than a point, given its flux density spectrum, rather than relating the \"time\" domain with the spectrum (frequency domain).\n\nAssuming certain properties of the object, e.g. spherical shape and constant temperature, calculations based on carrying out an inverse Laplace transformation on the spectrum of the object can produce the only possible model of the distribution of matter in it (density as a function of distance from the center) consistent with the spectrum. When independent information on the structure of an object is available, the inverse Laplace transform method has been found to be in good agreement.\n\nIn statistical mechanics, the Laplace transform of the density of states formula_118 defines the partition function. That is, the partition function formula_119 is given by\nand the inverse is given by\n\n\n\n\n"}
{"id": "2236984", "url": "https://en.wikipedia.org/wiki?curid=2236984", "title": "Late fee", "text": "Late fee\n\nA late fee, also known as an \"overdue fine\", \"late fine\", or \"past due fee\", is a charge fined against a client by a company or organization for not paying a bill or returning a rented or borrowed item by its due date. Its use is most commonly associated with businesses like creditors, video rental outlets and libraries. Late fees are generally calculated on a per day, per item basis.\n\nOrganizations encourage the payment of late fees by suspending a client's borrowing or rental privileges until accumulated fees are paid, sometimes after these fees have exceeded a certain level.\n\nLate fees are widely regarded as an annoyance. In 2005, video rental chain Blockbuster Video capitalized on this perception with a major advertising campaign that touted a revision of its rental policy as \"The End of Late Fees\".\n\nMore recently, in 2006, Rogers Video has used the same technique, except only for movies, and without any restocking fee (due to movies costing much less than video games).\n\nLate fees charged by banks, landlords, and utilities have been heavily criticized as a penalty against the poor, and attempts have been made in some places to outlaw them completely or place caps on them. The argument against them is that the poor will inevitably be forced to pay them as they cannot earn the money to pay their bills by the due date. These people will be forced to pay even higher fees for the same services, and will find making future timely payments to their creditors even more difficult.\n\nLate fees are issued to people who do not pay on time and don't honor a lease or obligation that they are responsible for.\n\nA special use of the term late fee is additional postage that was once required by Post Offices to allow inclusion of a letter or package in the outgoing mail dispatch although having been posted later than the normal closing time for mail. Often a special Late Fee Box was provided.\n\n"}
{"id": "33278970", "url": "https://en.wikipedia.org/wiki?curid=33278970", "title": "Linear transport theory", "text": "Linear transport theory\n\nIn mathematical physics Linear transport theory is the study of equations describing the migration of particles or energy within a host medium when such migration involves random absorption, emission and scattering events. Subject to certain simplifying assumptions, this is a common and useful framework for describing the scattering of light (radiative transfer) or neutrons (neutron transport).\n\nGiven the laws of individual collision events (in the form of absorption coefficients and scattering kernels/phase functions) the problem of linear transport theory is then to determine the result of a large number of random collisions governed by these laws. This involves computing exact or approximate solutions of the transport equation, and there are various forms of the transport equation that have been studied. Common varieties include steady-state vs time-dependent, scalar vs vector (the latter including polarization), and monoenergetic vs multi-energy (multi-group).\n\n\n"}
{"id": "7042478", "url": "https://en.wikipedia.org/wiki?curid=7042478", "title": "Main effect", "text": "Main effect\n\nIn the design of experiments and analysis of variance, a main effect is the effect of an independent variable on a dependent variable averaging across the levels of any other independent variables. The term is frequently used in the context of factorial designs and regression models to distinguish main effects from interaction effects.\n\nRelative to a factorial design, under an analysis of variance, a main effect test will test the hypotheses expected such as H, the null hypothesis. Running a hypothesis for a main effect will test whether there is evidence of an effect of different treatments. However a main effect test is nonspecific and will not allow for a localization of specific mean pairwise comparisons (simple effects). A main effect test will merely look at whether overall there is something about a particular factor that is making a difference. In other words a test examining differences amongst the levels of a single factor (averaging over the other factor and/or factors). Main effects are essentially the overall effect of a factor.\n\n"}
{"id": "45270", "url": "https://en.wikipedia.org/wiki?curid=45270", "title": "Measure space", "text": "Measure space\n\nA measure space is a basic object of measure theory, a branch of mathematics that studies generalized notions of volumes. Measure spaces contain information about the underlying set, the subsets of said set that are feasible for measuring (the formula_1-algebra) and the method that is used for measuring (the measure). One important example of a measure space is a probability space.\n\nMeasure space should not be confused with the related measurable spaces.\n\nA measure space is a triple formula_2 , where\n\nSet\n\nThe formula_10-algebra on finite sets such as the one above is usually the power set, which is the set of all subsets (of a given set) and is denoted by formula_11. Sticking with this convention, we set\n\nIn this simple case, the power set can be written down explicitly:\n\nAs measure, define formula_14 by \n\nso formula_16 (by additivity of measures) and formula_17 (by definition of measures).\n\nThis leads to the measure space formula_18. It is a probability space, since formula_16. The measure formula_14 corresponds to the Bernoulli distribution with formula_21, which is for example used to model a fair coin flip.\n\nMost important classes of measure spaces are defined by the properties of their associated measures. This includes\n\nAnother class of measure spaces are the complete measure spaces.\n"}
{"id": "31812917", "url": "https://en.wikipedia.org/wiki?curid=31812917", "title": "Method of Four Russians", "text": "Method of Four Russians\n\nIn computer science, the Method of Four Russians is a technique for speeding up algorithms involving Boolean matrices, or more generally algorithms involving matrices in which each cell may take on only a bounded number of possible values.\n\nThe main idea of the method is to partition the matrix into small square blocks of size for some parameter , and to use a lookup table to perform the algorithm quickly within each block. The index into the lookup table encodes the values of the matrix cells on the upper left of the block boundary prior to some operation of the algorithm, and the result of the lookup table encodes the values of the boundary cells on the lower right of the block after the operation. Thus, the overall algorithm may be performed by operating on only blocks instead of on matrix cells, where is the side length of the matrix. In order to keep the size of the lookup tables (and the time needed to initialize them) sufficiently small, is typically chosen to be .\n\nAlgorithms to which the Method of Four Russians may be applied include:\nIn each of these cases it speeds up the algorithm by one or two logarithmic factors.\n\nThe Method of Four Russians matrix inversion algorithm published by Bard is implemented in M4RI library for fast arithmetic with dense matrices over \"F\". M4RI is used by SageMath and the PolyBoRi library.\n\nThe algorithm was introduced by V. L. Arlazarov, E. A. Dinic, M. A. Kronrod, and I. A. Faradžev in 1970. The origin of the name is unknown; explain:\nAll four authors worked in Moscow, Russia at the time.\n\n"}
{"id": "212453", "url": "https://en.wikipedia.org/wiki?curid=212453", "title": "Misère", "text": "Misère\n\nMisere, misère, bettel or bettler (French for \"destitution\"; equivalent terms in other languages include \"contrabola\", \"devole\", \"pobre\") is a bid in various card games, and the player who bids misere undertakes to win no tricks or as few as possible, usually at no trump, in the round to be played. This does not allow sufficient variety to constitute a game in its own right, but it is the basis of such trick-avoidance games as Hearts, and provides an optional contract for most games involving an auction.\n\nA misere bid usually indicates an extremely poor hand, hence the name. An open or lay down misere, or misere ouvert is a 500 bid where the player is so sure of losing every trick that they undertake to do so with their cards placed face-up on the table. Consequently, 'lay down misere' is Australian gambling slang for a predicted easy victory. \n\nIn Skat, the bidding can result in a null game, where the bidder wins only if they lose every trick. (Conversely, the opponents win by forcing the bidder to take a trick.)\n\nThe word is first recorded in this sense in the rules for the game \"Boston\" in the late 18th century. Cannot be played in 6 hand 500.\n\nA misère game or bettel game is a game that is played according to its conventional rules, except that it is \"played to lose\"; that is, the winner is the one who loses according to the normal game rules. Such games generally have rulesets that normally encourage players to win; for example, most variations of draughts (known as \"checkers\" in the United States) require players to make a capture move if it is available; thus, in the misère variation, players can force their opponents to take numerous checkers through intentionally \"poor\" play.\n\nIn combinatorial game theory, a misère game is one played according to the \"misère play condition\"; that is, a player unable to move wins. (This is opposed to the \"normal play condition\" in which a player unable to move loses.) For most games this is the same as the ordinary use of the word, but a very few games are actually misère games according to their standard rules, for example Sylver coinage.\n\n\n"}
{"id": "314730", "url": "https://en.wikipedia.org/wiki?curid=314730", "title": "Monic polynomial", "text": "Monic polynomial\n\nIn algebra, a monic polynomial is a single-variable polynomial (that is, a univariate polynomial) in which the leading coefficient (the nonzero coefficient of highest degree) is equal to 1. Therefore, a monic polynomial has the form\n\nIf a polynomial has only one indeterminate (univariate polynomial), then the terms are usually written either from highest degree to lowest degree (\"descending powers\") or from lowest degree to highest degree (\"ascending powers\"). A univariate polynomial in \"x\" of degree \"n\" then takes the general form displayed above, where\n\nare constants, the coefficients of the polynomial.\n\nHere the term \"c\"\"x\" is called the \"leading term\", and its coefficient \"c\" the \"leading coefficient\"; if the leading coefficient , the univariate polynomial is called monic.\n\n\nThe set of all monic polynomials (over a given (unitary) ring \"A\" and for a given variable \"x\") is closed under multiplication, since the product of the leading terms of two monic polynomials is the leading term of their product. Thus, the monic polynomials form a multiplicative semigroup of the polynomial ring \"A\"[\"x\"]. Actually, since the constant polynomial 1 is monic, this semigroup is even a monoid.\n\nThe restriction of the divisibility relation to the set of all monic polynomials (over the given ring) is a partial order, and thus makes this set to a poset. The reason is that if \"p\"(\"x\") divides \"q\"(\"x\") and \"q\"(\"x\") divides \"p\"(\"x\") for two monic polynomials \"p\" and \"q\", then \"p\" and \"q\" must be equal. The corresponding property is not true for polynomials in general, if the ring contains invertible elements other than 1.\n\nIn other respects, the properties of monic polynomials and of their corresponding monic polynomial equations depend crucially on the coefficient ring \"A\". If \"A\" is a field, then every non-zero polynomial \"p\" has exactly one associated monic polynomial \"q\"; actually, \"q\" is \"p\" divided with its leading coefficient. In this manner, then, any non-trivial polynomial equation \"p\"(\"x\") = 0 may be replaced by an equivalent monic equation \"q\"(\"x\") = 0. E.g., the general real second degree equation\nmay be replaced by\nby putting  \"p\" = \"b\"/\"a\"  and  \"q\" = \"c\"/\"a\". Thus, the equation\nis equivalent to the monic equation\n\nThe general quadratic solution formula is then the slightly more simplified form of:\n\nOn the other hand, if the coefficient ring is not a field, there are more essential differences. E.g., a monic polynomial equation with integer coefficients cannot have other rational solutions than integer solutions. Thus, the equation\npossibly might have some rational root, which is not an integer, (and incidentally it does have \"inter alia\" the root −1/2); while the equations\nand\nonly may have integer solutions or irrational solutions.\n\nThe roots of monic polynomial with integer coefficients are called algebraic integers.\n\nThe solutions to monic polynomial equations over an integral domain are important in the theory of integral extensions and integrally closed domains, and hence for algebraic number theory. In general, assume that \"A\" is an integral domain, and also a subring of the integral domain \"B\". Consider the subset \"C\" of \"B\", consisting of those \"B\" elements, which satisfy monic polynomial equations over \"A\":\nThe set \"C\" contains \"A\", since any \"a\" ∈ \"A\" satisfies the equation \"x\" − \"a\" = 0. Moreover, it is possible to prove that \"C\" is closed under addition and multiplication. Thus, \"C\" is a subring of \"B\". The ring \"C\" is called the \"integral closure\" of \"A\" in \"B\"; or just the integral closure of \"A\", if \"B\" is the fraction field of \"A\"; and the elements of \"C\" are said to be \"integral\" over \"A\". If here formula_12 (the ring of integers) and formula_13 (the field of complex numbers), then \"C\" is the ring of \"algebraic integers\".\n\nIf is a prime number, the number of monic irreducible polynomials of degree over a finite field formula_14 with elements is equal to the necklace counting function . \n\nIf one removes the constraint of being monic, this number becomes . \n\nThe total number of roots of these monic irreducible polynomials is . This is the number of elements of the field (with elements) that do not belong to any smaller field.\n\nFor , such polynomials are commonly used to generate pseudorandom binary sequences.\n\nOrdinarily, the term \"monic\" is not employed for polynomials of several variables. However, a polynomial in several variables may be regarded as a polynomial in only \"the last\" variable, but with coefficients being polynomials in the others. This may be done in several ways, depending on which one of the variables is chosen as \"the last one\". E.g., the real polynomial\nis monic, considered as an element in R[\"y\"][\"x\"], i.e., as a univariate polynomial in the variable \"x\", with coefficients which themselves are univariate polynomials in \"y\":\nbut \"p\"(\"x\",\"y\") is not monic as an element in R[\"x\"][\"y\"], since then the highest degree coefficient (i.e., the \"y\" coefficient) is  2\"x\" − 1.\n\nThere is an alternative convention, which may be useful e.g. in Gröbner basis contexts: a polynomial is called monic, if its leading coefficient (as a multivariate polynomial) is 1. In other words, assume that \"p = p\"(\"x\"\"...,x\") is a non-zero polynomial in \"n\" variables, and that there is a given monomial order on the set of all (\"monic\") monomials in these variables, i.e., a total order of the free commutative monoid generated by \"x\"\"...,x\", with the unit as lowest element, and respecting multiplication. In that case, this order defines a highest non-vanishing term in \"p\", and \"p\" may be called monic, if that term has coefficient one.\n\n\"Monic multivariate polynomials\" according to either definition share some properties with the \"ordinary\" (univariate) monic polynomials. Notably, the product of monic polynomials again is monic.\n"}
{"id": "4637146", "url": "https://en.wikipedia.org/wiki?curid=4637146", "title": "Neugebauer equations", "text": "Neugebauer equations\n\nThe Neugebauer equations are a set of equations used to model color printing systems, developed by Hans E. J. Neugebauer. They were intended to predict the color produced by a combination of halftones printed in cyan, magenta, and yellow inks.\n\nThe equations estimate the reflectance (in CIE XYZ coordinates or as a function of wavelength) as a function of the reflectance of the 8 possible combinations of CMY inks (or the 16 combinations of CMYK inks), weighted by the area they take up on the paper. In wavelength form:\nwhere \"R\"(\"λ\") is the reflectance of ink combination \"i\", and \"w\" is the relative proportions of the 16 colors in a uniformly colored patch. The weights are dependent on the halftone pattern and possibly subject to various forms of dot gain.\n\nLight can interact with the paper and ink in more complex ways. The Yule–Nielsen correction takes into account light entering through blank regions and re-emerging through ink:\nThe factor \"n\" would be 2 for a perfectly-diffusing Lambertian paper substrate, but can be adjusted based on empirical measurements. Further considerations of the optics, such as multiple internal reflections, can be added at the price of additional complexity. \n\nIn order to achieve a desired reflectance these equations have to be inverted to produce the actual dot areas or digital values sent to the printer, a nontrivial operation that may have multiple solutions.\n\n"}
{"id": "29224388", "url": "https://en.wikipedia.org/wiki?curid=29224388", "title": "Newton's theorem about ovals", "text": "Newton's theorem about ovals\n\nIn mathematics, Newton's theorem about ovals states that the area cut off by a secant of a smooth convex oval is not an algebraic function of the secant.\n\nIsaac Newton stated it as lemma 28 of section VI of book 1 of Newton's \"Principia\", and used it to show that the position of a planet moving in an orbit is not an algebraic function of time. There has been some controversy about whether or not this theorem is correct because Newton did not state exactly what he meant by an oval, and for some interpretations of the word oval the theorem is correct, while for others it is false. If \"oval\" means \"continuous convex curve\", then there are counterexamples, such as triangles or one of the lobes of Huygens lemniscate \"y\" = \"x\" − \"x\", while pointed that if \"oval\" means \"infinitely differentiable convex curve\" then Newton's claim is correct and his argument has the essential steps of a rigorous proof.\n\nAn English translation Newton's original statement is:\n\nIn modern mathematical language, Newton essentially proved the following theorem:\n\nIn other words, \"oval\" in Newton's statement should mean \"convex smooth curve\". The infinite differentiability at all points is necessary: For any positive integer \"n\" there are algebraic curves that are smooth at all but one point and differentiable \"n\" times at the remaining point for which the area cut off by a secant is algebraic.\n\nNewton observed that a similar argument shows that the arclength of a (smooth convex) oval between two points is not given by an algebraic function of the points.\n\nNewton took the origin \"P\" inside the oval, and considered the spiral of points (\"r\", \"θ\") in polar coordinates whose distance \"r\" from \"P\" is the area cut off by the lines from \"P\" with angles 0 and \"θ\". He then observed that this spiral cannot be algebraic as it has an infinite number of intersections with a line through \"P\", so the area cut off by a secant cannot be an algebraic function of the secant.\n\nThis proof requires that the oval and therefore the spiral be smooth; otherwise the spiral might be an infinite union of pieces of different algebraic curves. This is what happens in the various \"counterexamples\" to Newton's theorem for non-smooth ovals.\n"}
{"id": "35252798", "url": "https://en.wikipedia.org/wiki?curid=35252798", "title": "Nicolai V. Krylov", "text": "Nicolai V. Krylov\n\nNicolai Vladimirovich Krylov (; born 5 June 1941) is a Russian mathematician specializing in partial differential equations, particularly stochastic partial differential equations and diffusion processes. Krylov studied at Lomonosov University, where he in 1966 under E. B. Dynkin attained a doctoral candidate title (similar to a PhD) and in 1973 a Russian doctoral degree (somewhat more prestigious than a PhD). He taught from 1966 to 1990 at the Lomonosov University and is since 1990 a professor at the University of Minnesota. At the beginning of his career (starting from 1963) he, in collaboration with Dynkin, worked on nonlinear stochastic control theory, making advances in the study of convex, nonlinear partial equations of 2nd order (\"i.e.\" Bellman equations), which were examined with stochastic methods. This led to the Evans-Krylov theory, for which he received with Lawrence C. Evans in 2004 the Leroy P. Steele Prize of the American Mathematical Society (for work done simultaneously and independently by both Krylov and Evans). They proved the second order differentiability (Hölder continuity of the second derivative) of the solutions of convex, completely nonlinear, second order elliptical partial differential equations and thus the existence of \"classical solutions\" (Theorem of Evans-Krylov). He was in 1978 at Helsinki and in 1986 at Berkeley an Invited Speaker for the ICM. He received the Humboldt Research Award in 2001. In 1993 he was elected a member of the American Academy of Arts and Sciences (1993). He should not be confused with the mathematician Nikolay M. Krylov.\n\n\n"}
{"id": "5644212", "url": "https://en.wikipedia.org/wiki?curid=5644212", "title": "North east down", "text": "North east down\n\nNorth east down (NED), also known as local tangent plane (LTP), is a geographical coordinate system for representing state vectors that is commonly used in aviation. It consists of three numbers: one represents the position along the northern axis, one along the eastern axis, and one represents vertical position. Down is chosen as opposed to up in order to comply with the right-hand rule. The origin of this coordinate system is usually chosen to be a point on the surface of the geoid below the aircraft's center of gravity. However, care must be taken since, if the aircraft is accelerating (turning or accelerating linearly), then the NED coordinates are no longer inertial coordinates.\n\nNED coordinates are similar to ECEF in that they're Cartesian, however they can be more convenient due to the relatively small numbers involved, and also because of the intuitive axes. NED and ECEF coordinates can be related with the following formula:\n\nwhere formula_2 is a 3D position in a NED system, formula_3 is the corresponding ECEF position, formula_4 is the reference ECEF position (where the local tangent plane originates), and formula_5 is a rotation matrix whose columns are the north, east, and down axes. formula_5 may be defined conveniently from the latitude formula_7 and longitude formula_8 corresponding to formula_4:\n\n"}
{"id": "13294412", "url": "https://en.wikipedia.org/wiki?curid=13294412", "title": "Orientation of churches", "text": "Orientation of churches\n\nWithin church architecture, orientation is an arrangement by which the point of main interest in the interior is towards the east (). The east end is where the altar is placed, often within an apse. The façade and main entrance are accordingly at the west end.\n\nThe opposite arrangement, in which the church is entered from the east and the sanctuary is at the other end, is called occidentation.\n\nSince the eighth century most churches are oriented. Hence, even in the many churches where the altar end is not actually to the east, terms such as \"east end\", \"west door\", \"north aisle\" are commonly used as if the church were oriented, treating the altar end as the liturgical east. \n\nThe first Christians faced east when praying, for which various explanations have been offered. In a tradition well established by the time of Christ, Jews in the diaspora would pray facing Jerusalem, which in most of the Roman Empire would have been to the east. Another explanation is that Christ's Second Coming would be on the clouds coming from the East: \"For as the lightning cometh out of the east, and shineth even unto the west; so shall also the coming of the Son of man be.\" (Matthew 24:27). Due to this eastward posture of prayer, Tertullian (c. 160 — c. 220) says that some non-Christians thought they worshipped the sun. Origen (c. 185 — 253) says: \"The fact that [...] of all the quarters of the heavens, the east is the only direction we turn to when we pour out prayer, the reasons for this, I think, are not easily discovered by anyone.\" Later on, various Fathers of the Church advanced mystical reasons for the custom.\n\nAt first, the orientation of the building in which Christians met was unimportant, but after the legalization of the religion in the fourth century, customs developed in this regard. These differed in eastern and western Christianity.\n\nThe \"Apostolic Constitutions\", a work of eastern Christianity written between 375 and 380 AD, gave it as a rule that churches should have the sanctuary (with apse and sacristies) at the east end, to enable Christians to pray eastward in church as in private or in small groups. In the middle of the sanctuary was the altar, behind which was the bishop's throne, flanked by the seats of the presbyters, while the laity were on the opposite side. However, even in the East there were churches (for example, in Tyre, Lebanon) that had the entrance at the east end, and the sanctuary at the west end. During the readings all looked towards the readers, the bishop and presbyters looking westward, the people eastward. The \"Apostolic Constitutions\", like the other documents that speak of the custom of praying towards the east, do not indicate on which side of the altar the bishop stood for \"the sacrifice\".\n\nThe earliest Christian churches in Rome were all built with the entrance to the east, like the Jewish temple in Jerusalem. Only in the 8th or 9th century did Rome accept the orientation that had become obligatory in the Byzantine Empire and was also generally adopted in the Frankish Empire and elsewhere in northern Europe. The original Constantinian Church of the Holy Sepulchre in Jerusalem also had the altar in the west end.\n\nThe old Roman custom of having the altar at the west end and the entrance at the east was sometimes followed as late as the 11th century even in areas under Frankish rule, as seen in Petershausen (Constance), Bamberg Cathedral, Augsburg Cathedral, Regensburg Cathedral, and Hildesheim Cathedral (all in present-day Germany).\n\nThe importance attached to orientation of churches declined after the 15th century. In his instructions on the building and arrangement of churches, Charles Borromeo, archbishop of Milan from 1560 to 1584, expressed a preference for having the apse point exactly east, but accepted that, where that is impractical, a church could be built even on a north-south axis, preferably with the façade at the southern end. He stated that the altar can also be at the west end, where \"in accordance with the rite of the Church it is customary for Mass to be celebrated at the main altar by a priest facing the people\".\n\nThe medieval mendicant orders generally built their churches inside towns and had to fit them into the town plans, regardless of orientation. Later, in the Spanish and Portuguese colonial empires they made no attempt to observe orientation, as is seen in San Francisco de Asis Mission Church near Taos, New Mexico. Today in the West, orientation is little observed in building churches.\n\nCharles Borromeo stated that churches ought to be oriented exactly east, in line with the rising sun at the equinoxes, not at the solstices, but some churches seem to be oriented to sunrise on the feast day of their patron saint. Thus St. Stephen's Cathedral, Vienna is oriented in line with sunrise on St. Stephen's Day, 26 December, in Julian calendar 1137, when it began to be built. However, a survey of old English churches published in 2006 showed practically no relationship with the feast days of the saints to whom they are dedicated. The results also did not conform to a theory that compass readings could have caused the variants. Taken as a body, those churches can only be said to have been oriented approximately but not exactly to the geographical east.\n\nAnother survey of a smaller number of English churches examined other possible alignments also and found that, if sunset as well as sunrise is taken into account, the saint's day hypothesis covered 43% of the cases considered, and that there was a significant correspondence also with sunrise on Easter morning of the year of foundation. The results provided no support for the compass readings hypothesis.\n\nYet another study of English churches found that a significant proportion of churches that showed a considerable deviation from true east were constrained by neighbouring buildings in town and perhaps by site topography in rural areas.\n\nSimilarly, a survey of a total of 32 medieval churches with reliable metadata in Lower Austria and northern Germany discovered only a few aligned in accordance with the saint's feast, with no general trend. There was no evidence of the use of compasses; and there was a preferred alignment towards true east, with variations due to town and natural topography.\n\nA notable example of an (approximately) oriented church building that – to match the contours of its location and to avoid an area that was swampy at the time of its construction – bends slightly in the middle is Quimper Cathedral in Brittany.\n\n"}
{"id": "4902017", "url": "https://en.wikipedia.org/wiki?curid=4902017", "title": "Parametric oscillator", "text": "Parametric oscillator\n\nA parametric oscillator is a driven harmonic oscillator in which the oscillations are driven by varying some parameter of the system at some frequency, typically different from the natural frequency of the oscillator. A simple example of a parametric oscillator is a child pumping a swing by periodically standing and squatting to increase the size of the swing's oscillations. The child's motions vary the moment of inertia of the swing as a pendulum. The \"pump\" motions of the child must be at twice the frequency of the swing's oscillations. Examples of parameters that may be varied are the oscillator's resonance frequency formula_1 and damping formula_2.\n\nParametric oscillators are used in several areas of physics. The classical varactor parametric oscillator consists of a semiconductor varactor diode connected to a resonant circuit or cavity resonator. It is driven by varying the diode's capacitance by applying a varying bias voltage. The circuit that varies the diode's capacitance is called the \"pump\" or \"driver\". In microwave electronics, waveguide/YAG based parametric oscillators operate in the same fashion. Another important example is the optical parametric oscillator, which converts an input laser light wave into two output waves of lower frequency (formula_3)\n\nWhen operated at pump levels below oscillation, the parametric oscillator can amplify a signal, becoming a parametric amplifier (paramp). Varactor parametric amplifiers have been developed as low-noise amplifiers in the radio and microwave frequency range. The advantage of a parametric amplifier is that it has much lower noise than an ordinary amplifier based on a gain device like a transistor or vacuum tube. This is because in the parametric amplifier a reactance is varied instead of a (noise-producing) resistance. They have been used in very low noise radio receivers in radio telescopes and spacecraft communication antennas.\n\nParametric resonance occurs in a mechanical system when a system is parametrically excited and oscillates at one of its resonant frequencies. Parametric excitation differs from forcing since the action appears as a time varying modification on a system parameter.\n\nParametric oscillations were first noticed in mechanics. Michael Faraday (1831) was the first to notice oscillations of one frequency being excited by forces of double the frequency, in the crispations (ruffled surface waves) observed in a wine glass excited to \"sing\". Franz Melde (1860) generated parametric oscillations in a string by employing a tuning fork to periodically vary the tension at twice the resonance frequency of the string. Parametric oscillation was first treated as a general phenomenon by Rayleigh (1883,1887).\n\nOne of the first to apply the concept to electric circuits was George Francis FitzGerald, who in 1892 tried to excite oscillations in an LC circuit by pumping it with a varying inductance provided by a dynamo.\n\nIn 1948 Aldert van der Ziel pointed out a major advantage of the parametric amplifier: because it used a variable reactance instead of a resistance for amplification it had inherently low noise. A parametric amplifier used as the front end of a radio receiver could amplify a weak signal while introducing very little noise. In 1952 Harrison Rowe at Bell Labs extended some 1934 mathematical work on pumped oscillations by Jack Manley and published the modern mathematical theory of parametric oscillations, the Manley-Rowe relations. \n\nThe varactor diode invented in 1956 had a nonlinear capacitance that was usable into microwave frequencies. The varactor parametric amplifier was developed by Marion Hines in 1956 at Western Electric. At the time it was invented microwaves were just being exploited, and the varactor amplifier was the first semiconductor amplifier at microwave frequencies. It was applied to low noise radio receivers in many areas, and has been widely used in radio telescopes, satellite ground stations, and long range radar. It is the main type of parametric amplifier used today. Since that time parametric amplifiers have been built with other nonlinear active devices such as Josephson junctions. The technique has been extended to optical frequencies in optical parametric oscillators and amplifiers which use nonlinear crystals as the active element.\n\nA parametric oscillator is a harmonic oscillator whose physical properties vary with time. The equation of such an oscillator is\nThis equation is linear in formula_5. By assumption, the parameters \nformula_6 and formula_2 depend only on time and do \"not\" depend on the state of the oscillator. In general, formula_8 and/or formula_9 are assumed to vary periodically, with the same period formula_10.\n\nIf the parameters vary at roughly \"twice\" the natural frequency of the oscillator (defined below), the oscillator phase-locks to the parametric variation and absorbs energy at a rate proportional to the energy it already has. Without a compensating energy-loss mechanism provided by formula_2, the oscillation amplitude grows exponentially. (This phenomenon is called parametric excitation, parametric resonance or parametric pumping.) However, if the initial amplitude is zero, it will remain so; this distinguishes it from the non-parametric resonance of driven simple harmonic oscillators, in which the amplitude grows linearly in time regardless of the initial state.\n\nA familiar experience of both parametric and driven oscillation is playing on a swing. Rocking back and forth pumps the swing as a driven harmonic oscillator, but once moving, the swing can also be parametrically driven by alternately standing and squatting at key points in the swing arc. This changes moment of inertia of the swing and hence the resonance frequency, and children can quickly reach large amplitudes provided that they have some amplitude to start with (e.g., get a push). Standing and squatting at rest, however, leads nowhere.\n\nWe begin by making a change of variables\n\nwhere formula_13 is a time integral of the damping\n\nThis change of variables eliminates the damping term\n\nwhere the transformed frequency is defined\n\nIn general, the variations in damping and frequency are relatively small perturbations\n\nwhere formula_19 and formula_20 are constants, namely, the time-averaged oscillator frequency and damping, respectively. The transformed frequency can be written in a similar way:\n\nwhere formula_22 is the natural frequency of the damped harmonic oscillator\n\nand\n\nThus, our transformed equation can be written\n\nThe independent variations formula_26 and formula_27 in the oscillator damping and resonance frequency, respectively, can be combined into a single pumping function formula_28. The converse conclusion is that any form of parametric excitation can be accomplished by varying either the resonance frequency or the damping, or both.\n\nLet us assume that formula_28 is sinusoidal, specifically\n\nwhere the pumping frequency formula_31 but need not equal formula_22 exactly. The solution formula_33 of our transformed equation may be written\n\nwhere the rapidly varying components have been factored out(formula_35 and formula_36) to isolate the slowly varying amplitudes formula_37 and formula_38. This corresponds to Laplace's variation of parameters method.\n\nSubstituting this solution into the transformed equation and retaining only the terms first-order in formula_39 yields two coupled equations\n\nThese equations may be decoupled and solved by making another change of variables\n\nwhich yields the equations\n\nwhere for brevity the following are defined\n\nand the detuning\n\nThe formula_49 equation does not depend on formula_50, and linearization near its equilibrium position formula_51 shows that formula_49 decays exponentially to its equilibrium\n\nwhere the decay constant\n\nformula_54.\n\nIn other words, the parametric oscillator phase-locks to the pumping signal formula_28.\n\nTaking formula_56 (i.e., assuming that the phase has locked), the formula_50 equation becomes\n\nwhose solution is formula_59; the amplitude of the formula_33 oscillation diverges exponentially. However, the corresponding amplitude formula_61 of the \"untransformed\" variable formula_62 need not diverge\n\nThe amplitude formula_61 diverges, decays or stays constant, depending on whether formula_65 is greater than, less than, or equal to formula_13, respectively.\n\nThe maximum growth rate of the amplitude occurs when formula_67. At that frequency, the equilibrium phase formula_51 is zero, implying that formula_69 and formula_70. As formula_71 is varied from formula_22, formula_51 moves away from zero and formula_74, i.e., the amplitude grows more slowly. For sufficiently large deviations of formula_71, the decay constant formula_76 can become purely imaginary since\n\nIf the detuning formula_78 exceeds formula_79, formula_76 becomes purely imaginary and formula_33 varies sinusoidally. Using the definition of the detuning formula_78, the pumping frequency formula_83 must lie between formula_84 and formula_85 in order to achieve exponential growth in formula_86. Expanding the square roots in a binomial series shows that the spread in pumping frequencies that result in exponentially growing formula_86 is approximately formula_88.\n\nThe above derivation may seem like a mathematical sleight-of-hand, so it may be helpful to give an intuitive derivation. The formula_86 equation may be written in the form\n\nwhich represents a simple harmonic oscillator (or, alternatively, a bandpass filter) being driven by a signal formula_91 that is proportional to its response formula_86.\n\nAssume that formula_93 already has an oscillation at frequency formula_71 and that the pumping formula_95 has double the frequency and a small amplitude formula_39. Applying a trigonometric identity for products of sinusoids, their product formula_97 produces two driving signals,\none at frequency formula_71 and the other at frequency formula_99\n\nBeing off-resonance, the formula_101 signal is attenuated and can be neglected initially. By contrast, the formula_71 signal is on resonance, serves to amplify formula_86, and is proportional to the amplitude \nformula_104. Hence, the amplitude of formula_86 grows exponentially unless it is initially zero.\n\nExpressed in Fourier space, the multiplication formula_106 is a convolution of their Fourier transforms formula_107 and formula_108. The positive feedback arises because the formula_109 component of formula_28 converts the formula_111 component of formula_33 into a driving signal at \nformula_113, and vice versa (reverse the signs). This explains why the pumping frequency must be near formula_114, twice the natural frequency of the oscillator. Pumping at a grossly different frequency would not couple (i.e., provide mutual positive feedback) between the formula_111 and formula_113 components of formula_33.\n\nParametric resonance is the parametrical resonance phenomenon of mechanical perturbation and oscillation at certain frequencies (and the associated harmonics). This effect is different from regular resonance because it exhibits the instability phenomenon.\n\nParametric resonance occurs in a mechanical system when a system is parametrically excited and oscillates at one of its resonant frequencies. Parametric resonance takes place when the external excitation frequency equals twice the natural frequency of the system. Parametric excitation differs from forcing since the action appears as a time varying modification on a system parameter. The classical example of parametric resonance is that of the vertically forced pendulum.\n\nFor small amplitudes and by linearising, the stability of the periodic solution is given by Mathieu's equation:\n\nformula_118\n\nwhere formula_119 is some perturbation from the periodic solution. Here the formula_120 term acts as an ‘energy’ source and is said to parametrically excite the system. The Mathieu equation describes many other physical systems to a sinusoidal parametric excitation such as an LC Circuit where the capacitor plates move sinusoidally.\n\nA parametric amplifier is implemented as a mixer. The mixer's gain shows up in the output as amplifier gain. The input weak signal is mixed with a strong local oscillator signal, and the resultant strong output is used in the ensuing receiver stages.\n\nParametric amplifiers also operate by changing a parameter of the amplifier. \nIntuitively, this can be understood as follows, for a variable capacitor based amplifier.\n\nQ [charge in a capacitor] = C x V<br>\ntherefore <br>\nV [voltage across a capacitor] = Q/C\n\nKnowing the above, if a capacitor is charged until its voltage equals the sampled voltage of an incoming weak signal, and if the capacitor's capacitance is then reduced (say, by manually moving the plates further apart), then the voltage across the capacitor will increase. In this way, the voltage of the weak signal is amplified.\n\nIf the capacitor is a varicap diode, then the 'moving the plates' can be done simply by applying time-varying DC voltage to the varicap diode. This driving voltage usually comes from another oscillator — sometimes called a \"pump\".\n\nThe resulting output signal contains frequencies that are the sum and difference of the input signal (f1) and the pump signal (f2): (f1 + f2) and (f1 - f2).\n\nA practical parametric oscillator needs the following connections: one for the \"common\" or \"ground\", one to feed the pump, one to retrieve the output, and maybe a fourth one for biasing. A parametric amplifier needs a fifth port to input the signal being amplified. Since a varactor diode has only two connections, it can only be a part of an LC network with four eigenvectors with nodes at the connections. This can be implemented as a transimpedance amplifier, a traveling wave amplifier or by means of a circulator.\n\nThe parametric oscillator equation can be extended by adding an external driving force formula_121:\n\nWe assume that the damping formula_123 is sufficiently strong that, in the absence of the driving force formula_124, the amplitude of the parametric oscillations does not diverge, i.e., that formula_125. In this situation, the parametric pumping acts to lower the effective damping in the system. For illustration, let the damping be constant formula_126 and assume that the external driving force is at the mean resonance frequency formula_19, i.e., formula_128. The equation becomes\n\nwhose solution is roughly\n\nAs formula_131 approaches the threshold formula_132, the amplitude diverges. When formula_133, the system enters parametric resonance and the amplitude begins to grow exponentially, even in the absence of a driving force formula_121.\n\n1:It is highly sensitive\n\n2:low noise level amplifier for ultra high frequency and microwave radio signal\n\n3:The unique capability to operate as a wireless powered amplifier that doesn't require internal power source\n\nIf the parameters of any second-order linear differential equation are varied periodically, Floquet analysis shows that the solutions must vary either sinusoidally or exponentially.\n\nThe formula_86 equation above with periodically varying formula_28 is an example of a Hill equation. If formula_28 is a simple sinusoid, the equation is called a Mathieu equation.\n\n\n\n"}
{"id": "237147", "url": "https://en.wikipedia.org/wiki?curid=237147", "title": "Polyphase quadrature filter", "text": "Polyphase quadrature filter\n\nA polyphase quadrature filter, or PQF, is a filter bank which splits an input signal into a given number N (mostly a power of 2) of equidistant sub-bands. These sub-bands are subsampled by a factor of N, so they are critically sampled.\n\nThis critical sampling introduces aliasing. Similar to the MDCT time domain alias cancellation the aliasing of polyphase quadrature filters is canceled by neighbouring sub-bands, i.e. signals are typically stored in two sub-bands.\n\nNote that signal in odd subbands is stored frequency inverted.\n\nPQF filters are used in MPEG-1 Audio Layer I and II, Musepack (which was based on MPEG-1 layer II), in MPEG-1 Layer III with an additional MDCT, in MPEG-4 AAC-SSR for the 4 band PQF bank, in MPEG-4 V3 SBR\nfor the analysis of the upper spectral replicated band, and in DTS.\n\nPQF has an advantage over the very similar stacked quadrature mirror filter (QMF). Delay and computational effort are much lower.\n\nA PQF filter bank is constructed using a base filter, which is a low-pass at fs/4N. This lowpass is modulated by N cosine functions and converted to N band-passes with a bandwidth of fs/2N.\n\nThe base lowpass is typically a FIR filter with a length of 10*N ... 24*N taps. Note that it is also possible to build PQF filters using recursive IIR filters.\n\nThere are different formulas possible. Most of them are based on the MDCT but are slightly modified.\n"}
{"id": "3929020", "url": "https://en.wikipedia.org/wiki?curid=3929020", "title": "Proof-carrying code", "text": "Proof-carrying code\n\nProof-carrying code (PCC) is a software mechanism that allows a host system to verify properties about an application via a formal proof that accompanies the application's executable code. The host system can quickly verify the validity of the proof, and it can compare the conclusions of the proof to its own security policy to determine whether the application is safe to execute. This can be particularly useful in ensuring memory safety (i.e. preventing issues like buffer overflows).\n\nProof-carrying code was originally described in 1996 by George Necula and Peter Lee.\n\nThe original publication on proof-carrying code in 1996 used packet filters as an example: a user-mode application hands a function written in machine code to the kernel that determines whether or not an application is interested in processing a particular network packet. Because the packet filter runs in kernel mode, it could compromise the integrity of the system if it contains malicious code that writes to kernel data structures. Traditional approaches to this problem include interpreting a domain specific language for packet filtering, inserting checks on each memory access (software fault isolation), and writing the filter in a high-level language which is compiled by the kernel before it is run. These approaches all have severe performance disadvantages for code as frequently run as a packet filter.\n\nWith proof-carrying code, the kernel publishes a security policy specifying properties that any packet filter must obey: for example, will not access memory outside of the packet and its scratch memory area. A theorem prover is used to show that the machine code satisfies this policy. The steps of this proof are recorded and attached to the machine code which is given to the kernel program loader. The program loader can then rapidly validate the proof, allowing it to thereafter run the machine code without any additional checks. If a malicious party modifies either the machine code or the proof, the resulting proof-carrying code is either invalid or harmless (still satisfies the security policy).\n\n\n"}
{"id": "910505", "url": "https://en.wikipedia.org/wiki?curid=910505", "title": "Proof-theoretic semantics", "text": "Proof-theoretic semantics\n\nProof-theoretic semantics is an approach to the semantics of logic that attempts to locate the meaning of propositions and logical connectives not in terms of interpretations, as in Tarskian approaches to semantics, but in the role that the proposition or logical connective plays within the system of inference.\n\nGerhard Gentzen is the founder of proof-theoretic semantics, providing the formal basis for it in his account of cut-elimination for the sequent calculus, and some provocative philosophical remarks about locating the meaning of logical connectives in their introduction rules within natural deduction. The history of proof-theoretic semantics since then has been devoted to exploring the consequences of these ideas.\n\nDag Prawitz extended Gentzen's notion of analytic proof to natural deduction, and suggested that the value of a proof in natural deduction may be understood as its normal form. This idea lies at the basis of the Curry–Howard isomorphism, and of intuitionistic type theory. His inversion principle lies at the heart of most modern accounts of proof-theoretic semantics.\n\nMichael Dummett introduced the very fundamental idea of logical harmony, building on a suggestion of Nuel Belnap. In brief, a language, which is understood to be associated with certain patterns of inference, has logical harmony if it is always possible to recover analytic proofs from arbitrary demonstrations, as can be shown for the sequent calculus by means of cut-elimination theorems and for natural deduction by means of normalisation theorems. A language that lacks logical harmony will suffer from the existence of incoherent forms of inference: it will likely be inconsistent.\n\n\n\n"}
{"id": "1531781", "url": "https://en.wikipedia.org/wiki?curid=1531781", "title": "Rarita–Schwinger equation", "text": "Rarita–Schwinger equation\n\nIn theoretical physics, the Rarita–Schwinger equation is the\nrelativistic field equation of spin-3/2 fermions. It is similar to the Dirac equation for spin-1/2 fermions. This equation was first introduced by William Rarita and Julian Schwinger in 1941. \n\nIn modern notation it can be written as:\nwhere formula_2 is the Levi-Civita symbol,\nformula_3 and formula_4 are Dirac matrices,\nformula_5 is the mass,\nformula_6,\nand formula_7 is a vector-valued spinor with additional components compared to the four component spinor in the Dirac equation. It corresponds to the representation of the Lorentz group, or rather, its part.\nThis field equation can be derived as the Euler–Lagrange equation corresponding to the Rarita–Schwinger Lagrangian:\nwhere the bar above formula_9 denotes the Dirac adjoint.\n\nThis equation controls the propagation of the wave function of composite objects such as the delta baryons () or for the conjectural gravitino. So far, no elementary particle with spin 3/2 has been found experimentally. \n\nThe massless Rarita–Schwinger equation has a fermionic gauge symmetry: is invariant under the gauge transformation formula_10, where formula_11 is an arbitrary spinor field. This is simply the local supersymmetry of supergravity, and the field must be a gravitino.\n\n\"Weyl\" and \"Majorana\" versions of the Rarita–Schwinger equation also exist.\n\nConsider a massless Rarita-Schwinger field described by the Lagrangian density\nwhere the sum over spin indices is implicit, formula_9 are Majorana spinors, and\n\nTo obtain the equations of motion we vary the Lagrangian with respect to the fields formula_9, obtaining:\nusing the Majorana flip properties\nwe see that the second and first terms on the RHS are equal, concluding that\nplus unimportant boundary terms.\nImposing formula_18 we thus see that the equation of motion for a massless Majorana Rarita-Schwinger spinor reads:\n\nThe current description of massive, higher spin fields through either Rarita–Schwinger or Fierz–Pauli formalisms is afflicted with several maladies.\n\nAs in the case of the Dirac equation, electromagnetic interaction can be added by promoting the partial derivative to gauge covariant derivative:\nIn 1969, Velo and Zwanziger showed that the Rarita–Schwinger Lagrangian coupled to electromagnetism leads to equation with solutions representing wavefronts, some of which propagate faster than light. In other words, \nthe field then suffers from acausal, superluminal propagation; consequently, the quantization in interaction with electromagnetism is essentially flawed. In extended supergravity, though, Das and Freedman have shown that local supersymmetry solves this problem.\n\n"}
{"id": "976516", "url": "https://en.wikipedia.org/wiki?curid=976516", "title": "Sergei Natanovich Bernstein", "text": "Sergei Natanovich Bernstein\n\nSergei Natanovich Bernstein (, sometimes Romanized as ; 5 March 1880 – 26 October 1968) was a Russian and Soviet mathematician of Jewish origin known for contributions to partial differential equations, differential geometry, probability theory, and approximation theory.\n\nIn his doctoral dissertation, submitted in 1904 to the Sorbonne, Bernstein solved Hilbert's nineteenth problem on the analytic solution of elliptic differential equations. His later work was devoted to Dirichlet's boundary problem for non-linear equations of elliptic type, where, in particular, he introduced a priori estimates.\n\nIn 1917, Bernstein suggested the first axiomatic foundation of probability theory, based on the underlying algebraic structure. It was later superseded by the measure-theoretic approach of Kolmogorov.\n\nIn the 1920s, he introduced a method for proving limit theorems for sums of dependent random variables.\n\nThrough his application of Bernstein polynomials, he laid the foundations of constructive function theory, a field studying the connection between smoothness properties of a function and its approximations by polynomials. In particular, he proved \nthe Weierstrass approximation theorem and Bernstein's theorem (approximation theory).\n\n\n\n"}
{"id": "47945620", "url": "https://en.wikipedia.org/wiki?curid=47945620", "title": "Snedecor Award", "text": "Snedecor Award\n\nThe Snedecor Award, named after George W. Snedecor, is given by the Committee of Presidents of Statistical Societies to a statistician for contribution to biometry.\n\n"}
{"id": "23594537", "url": "https://en.wikipedia.org/wiki?curid=23594537", "title": "Time delay neural network", "text": "Time delay neural network\n\nTime delay neural network (TDNN) is a multilayer artificial neural network architecture whose purpose is to 1) classify patterns with shift-invariance, and 2) model context at each layer of the network.\n\nShift-invariant classification means that the classifier does not require explicit segmentation prior to classification. For the classification of a temporal pattern (such as speech), the TDNN thus avoids having to determine the beginning and end points of sounds before classifying them.\n\nFor contextual modelling in a TDNN, each neural unit at each layer receives input not only from activations/features at the layer below, but from a pattern of unit output and its context. For time signals each unit receives as input the activation patterns over time from units below. Applied to two-dimensional classification (images, time-frequency patterns), the TDNN can be trained with shift-invariance in the coordinate space and avoids precise segmentation in the coordinate space.\n\nThe TDNN was first proposed to classify phonemes in speech signals for automatic speech recognition, where the automatic determination of precise segments or feature boundaries is difficult or impossible. Because the TDNN recognizes phonemes and their underlying acoustic/phonetic features, independent of position in time, it improved performance over static classification. It was also applied to two-dimensional signals (time-frequency patterns in speech, and coordinate space pattern in OCR.\n\nThe Time Delay Neural Network, like other neural networks, operates with multiple interconnected layers of perceptrons, and is implemented as a feedforward neural network. All neurons (at each layer) of a TDNN receive inputs from the outputs of neurons at the layer below but with two differences:\n\n\nIn the case of a speech signal, inputs are spectral coefficients over time.\n\nIn order to learn critical acoustic-phonetic features (for example formant transitions, bursts, frication, etc.) without first requiring precise localization, the TDNN is trained time-shift-invariantly. Time-shift invariance is achieved through weight sharing across time during training: Time shifted copies of the TDNN are made over the input range (from left to right in Fig.1). Backpropagation is then performed from an overall classification target vector (see TDNN diagram, three phoneme class targets (/b/, /d/, /g/) are shown in the output layer), resulting in gradients that will generally vary for each of the time-shifted network copies. Since such time-shifted networks are only copies, however, the position dependence is removed by weight sharing. In this example, this is done by averaging the gradients from each time-shifted copy before performing the weight update. In speech, time-shift invariant training was shown to learn weight matrices that are independent of precise positioning of the input. The weight matrices could also be shown to detect important acoustic-phonetic features that are known to be important for human speech perception, such as formant transitions, bursts, etc. TDNN’s could also be combined or grown by way of pre-training.\n\nThe precise architecture of TDNNs (time-delays, number of layers) is mostly determined by the designer depending on the classification problem and the most useful context sizes. The delays or context windows are chosen specific to each application. Work has also been done to create adaptable time-delay TDNNs where this manual tuning is eliminated.\n\nTDNN based phoneme recognizers compared favourably in early comparisons with HMM based phone models. Modern deep TDNN architectures include many more hidden layers and sub-sample or pool connections over broader contexts at higher layers. They achieve up to 50% word error reduction over GMM based acoustic models. While the different layers of TDNN’s are intended to learn features of increasing context width, they do model local contexts. When longer distance relationships and pattern sequences have to be processed, learning states and state-sequences is important and TDNNs can be combined with other modelling techniques \n\nTDNNs used to solve problems in speech recognition that were introduced in 1987 and initially focused on shift-invariant phoneme recognition. Speech lends itself nicely to TDNNs as spoken sounds are rarely of uniform length and precise segmentation is difficult or impossible. By scanning a sound over past and future, the TDNN is able to construct a model for the key elements of that sound in a time-shift invariant manner. This is particularly useful as sounds are smeared out through reverberation. Large phonetic TDNN’s can be constructed modularly through pre-training and combining smaller networks.\n\nLarge vocabulary speech recognition requires recognizing sequences of phonemes that make up words subject to the constraints of a large pronunciation vocabulary. Integration of TDNNs into large vocabulary speech recognizers is possible by introducing state transitions and search between phonemes that make up a word. The resulting Multi-State Time-Delay Neural Network (MS-TDNN) can be trained discriminative from the word level, thereby optimizing the entire arrangement toward word recognition instead of phoneme classification.\n\nTwo-dimensional variants of the TDNN’s were proposed for speaker independence. Here, shift-invariance is applied to the time \"as well as\" to the frequency axis in order to learn hidden features that are independent of precise location in time and in frequency (the latter being due to speaker variability).\n\nOne of the persistent problems in speech recognition is recognizing speech when it is corrupted by echo and reverberation (as is the case in large rooms and distant microphones). Reverberation can be viewed as corrupting speech with delayed versions of itself. In general, it is difficult, however, to de-reverberate a signal as the impulse response function (and thus the convolutional noise experienced by the signal) is not known for any arbitrary space. The TDNN was shown to be effective to recognize speech robustly despite different levels of reverberation.\n\nTDNNs were also successfully used in early demonstrations of audio-visual speech, where the sounds of speech are complemented by visually reading lip movement. Here, TDNN based recognizers used visual and acoustic features jointly to achieve improved recognition accuracy, particularly in the presence of noise, where complementary information from an alternate modality could be fused nicely in a neural net.\n\nTDNNs have been used effectively in compact and high-performance handwriting recognition systems. Shift-invariance was also adapted to spatial patterns (x/y-axes) in image offline handwriting recognition.\n\nVideo has a temporal dimension that makes a TDNN an ideal solution to analysing motion patterns. An example of this analysis is a combination of vehicle detection and recognizing pedestrians. When examining videos, subsequent images are fed into the TDNN as input where each image is the next frame in the video. The strength of the TDNN comes from its ability to examine objects shifted in time forward and backward to define an object detectable as the time is altered. If an object can be recognized in this manner, an application can plan on that object to be found in the future and perform an optimal action.\n\nTwo-dimensional TDNNs were later applied to other image recognition tasks under the name of “Convolutional Neural Networks”, where shift-invariant training is applied to the x/y axes of an image.\n\n\n"}
{"id": "33178845", "url": "https://en.wikipedia.org/wiki?curid=33178845", "title": "TopFIND", "text": "TopFIND\n\nTopFIND is the Termini oriented protein Function Inferred Database (TopFIND) is an integrated knowledgebase focused on protein termini, their formation by proteases and functional implications. It contains information about the processing and the processing state of proteins and functional implications thereof derived from research literature, contributions by the scientific community and biological databases.\n\nAmong the most fundamental characteristics of a protein are the N- and C-termini defining the start and end of the polypeptide chain. While genetically encoded, protein termini isoforms are also often generated during translation, following which, termini are highly dynamic, being frequently trimmed at their ends by a large array of exopeptidases. Neo-termini can also be generated by endopeptidases after precise and limited proteolysis, termed processing. Necessary for the maturation of many proteins, processing can also occur afterwards, often resulting in dramatic functional consequences. Aberrant proteolysis can cause wide range of diseases like arthritis or cancer. Hence, proteolytic generation of pleiotrophic stable forms of proteins, the universal susceptibility of proteins to proteolysis, and its irreversibility, distinguishes proteolysis from many highly studied posttranslational modifications. Proteases are tightly interconnected in the protease web and their aberrant activity in disease can lead to diagnostic fragment profiles with characteristic protein termini. Following proteolysis, the newly formed protein termini can be further modified, a process that affects protein function and stability.\n\nTopFIND is a resource for comprehensive coverage of protein N- and C-termini discovered by all available in silico, in vitro as well as in vivo methodologies. It makes use of existing knowledge by seamless integration of data from UniProt and MEROPS and provides access to new data from community submission and manual literature curating. It renders modifications of protein termini, such as acetylation and citrullination, easily accessible and searchable and provides the means to identify and analyse extend and distribution of terminal modifications across a protein. Since its inception TopFIND has been expanded to further species.\n\nThe data is presented to the user with a strong emphasis on the relation to curated background information and underlying evidence that led to the observation of a terminus, its modification or proteolytic cleavage. In brief the protein information, its domain structure, protein termini, terminus modifications and proteolytic processing of and by other proteins is listed. All information is accompanied by metadata like its original source, method of identification, confidence measurement or related publication. A positional cross correlation evaluation matches termini and cleavage sites with protein features (such as amino acid variants) and domains to highlight potential effects and dependencies in a unique way. Also, a network view of all proteins showing their functional dependency as protease, substrate or protease inhibitor tied in with protein interactions is provided for the easy evaluation of network wide effects. A powerful yet user friendly filtering mechanism allows the presented data to be filtered based on parameters like methodology used, in vivo relevance, confidence or data source (e.g. limited to a single laboratory or publication). This provides means to assess physiological relevant data and to deduce functional information and hypotheses relevant to the bench scientist. In a later release analysis tools for the evaluation of proteolytic pathways in experimental data have been added.\n\n\n"}
{"id": "1190670", "url": "https://en.wikipedia.org/wiki?curid=1190670", "title": "Transparency (data compression)", "text": "Transparency (data compression)\n\nIn data compression and psychoacoustics, transparency is the result of lossy data compression accurate enough that the compressed result is perceptually indistinguishable from the uncompressed input. In other words, transparent compression has no perceptible compression artifacts.\n\nA transparency threshold is a given value at which transparency is reached. It is commonly used to describe compressed data bitrates. For example, the transparency threshold for MP3 to Linear PCM audio is said to be between 175 and 245 kbit/s, at 44.1 kHz, when encoded as VBR MP3 (corresponding to the -V3 and -V0 settings of the highly popular LAME MP3 encoder). This means that when an MP3 that was encoded at those bitrates is being played back, it is indistinguishable from the original PCM, and the compression is transparent to the listener.\n\nTransparency, like sound or video quality, is subjective. It depends most on the listener's familiarity with digital artifacts, their awareness that artifacts may in fact be present, and to a lesser extent, the compression method, bit-rate used, input characteristics, and the listening/viewing conditions and equipment. Despite this, sometimes general consensus is formed for what compression options \"should\" provide transparent results for most people on most equipment. Due to the subjectivity and the changing nature of compression, recording, and playback technology, such opinions should be considered only as rough estimates rather than established fact.\n\nJudging transparency can be difficult, due to observer bias, in which subjective like/dislike of a certain compression methodology emotionally influences their judgment. This bias is commonly referred to as \"placebo\", although this use is slightly different from the medical use of the term.\n\nTo scientifically prove that a compression method is \"not\" transparent, double-blind tests may be useful. The ABX method is normally used, with a null hypothesis that the samples tested are the same and with an alternative hypothesis that the samples are in fact different.\n\nAll lossless data compression methods are transparent, by nature. However, a double-blind comparison could still yield claims of perceived differences and thus lack of transparency, even though such claims would be in error.\n\n\n\n"}
{"id": "8272539", "url": "https://en.wikipedia.org/wiki?curid=8272539", "title": "Treviso Arithmetic", "text": "Treviso Arithmetic\n\nThe Treviso Arithmetic, or Arte dell'Abbaco, is an anonymous textbook in commercial arithmetic written in vernacular Venetian and published in Treviso, Italy in 1478.\n\nThe author explains the motivation for writing this textbook:\nThe \"Treviso Arithmetic\" is the earliest known printed mathematics book in the West, and one of the first printed European textbooks dealing with a science.\n\nThere appears to have been only one edition of the work. David Eugene Smith translated parts of the\" Treviso Arithmetic\" for educational purposes in 1907. Frank J. Swetz translated the complete work using Smith's notes in 1987 in his \"Capitalism & Arithmetic: The New Math of the 15th Century\". Swetz used a copy of the \"Treviso\" housed in the Manuscript Library at Columbia University. The volume found its way to this collection via a curious route. Maffeo Pinelli (1785), an Italian bibliophile, is the first known owner. After his death his library was purchased by a London book dealer and sold at auction on February 6, 1790. The book was obtained for \"three shillings\" by Mr. Wodhull. About 100 years later the \"Arithmetic\" appeared in the library of Brayton Ives, a New York lawyer. When Ives sold the collection of books at auction, George Arthur Plimpton, a New York publisher, acquired the \"Treviso\" and made it an acquisition to his extensive collection of early scientific texts. Plimpton donated his library to Columbia University in 1936. Original copies of the \"Treviso Arithmetic\" are extremely rare.\n\nThere are 123 pages of text with 32 lines of print to a page. The pages are unnumbered, untrimmed and have wide margins. Some of the margins contain written notes. The size of the book is 14.5 cm by 20.6 cm.\n\nThe book included information taken from the 1202 \"Liber Abaci\", such as lattice multiplication. George G. Joseph, \"Crest of the Peacock' suggests that Napier read this book to create Napier's bones, or Napier's rods.\n\nThe \"Treviso Arithmetic\" is a practical book intended for self study and for use in Venetian trade. It is written in vernacular Venetian and communicated knowledge to a large population.\n\nIt helped to end the monopoly on mathematical knowledge and gave important information to the middle class. It was not written for a large audience, but was intended to teach mathematics of everyday currency.\n\nThe \"Treviso\" became one of the first mathematics books written for the expansion of human knowledge. It provided an opportunity for the common person, rather than only a privileged few, to learn the art of computation. The \"Treviso Arithmetic\" provided an early example of the Hindu–Arabic numeral system computational algorithms.\n\n\n\n"}
{"id": "38693595", "url": "https://en.wikipedia.org/wiki?curid=38693595", "title": "Unified Modeling Language for Interactive Systems", "text": "Unified Modeling Language for Interactive Systems\n\nThe Unified Modeling Language for Interactive Systems (UMLi) is a conservative extension of the Unified Modeling Language for user interface design. UMLi was developed in the period between 1998 and 2002 as part of Paulo Pinheiro's Ph.D. Thesis at the University of Manchester. UMLi is based on model-based user interface development environments (MB-UIDEs), which provide the capability to design and implement user interfaces in a declarative and systematic way.\n\n"}
{"id": "38237389", "url": "https://en.wikipedia.org/wiki?curid=38237389", "title": "Wojciech Rytter", "text": "Wojciech Rytter\n\nWojciech Rytter is a Polish computer scientist, a professor of computer science at the University of Warsaw. His research focuses on the design and analysis of algorithms, and in particular on stringology.\n\nRytter earned a master's degree in 1971 and a Ph.D. in 1975 from Warsaw University, and earned his habilitation in 1985. He has been on the faculty of Warsaw University since 1971, and is now a full professor there. He has also held long-term visiting positions at the New Jersey Institute of Technology, Liverpool University, Bonn University, the University of California, Riverside, Warwick University, and the University of Mexico.\n\nRytter is the author or co-author of:\n\nRytter is a member of the Academia Europaea.\n\n"}
{"id": "8396078", "url": "https://en.wikipedia.org/wiki?curid=8396078", "title": "Z-channel (information theory)", "text": "Z-channel (information theory)\n\nA Z-channel is a communications channel used in coding theory and information theory to model the behaviour of some data storage systems.\n\nA \"Z-channel\" (or a \"binary asymmetric channel\") is a channel with binary input and binary output where the crossover 1 → 0 occurs with nonnegative probability \"p\", whereas the crossover 0 → 1 never occurs. In other words, if \"X\" and \"Y\" are the random variables describing the probability distributions of the input and the output of the channel, respectively, then the crossovers of the channel are characterized by the conditional probabilities\n\nThe capacity formula_1 of the Z-channel formula_2 with the crossover 1 → 0 probability \"p\", when the input random variable \"X\" is distributed according to the Bernoulli distribution with probability \"formula_3\" for the occurrence of 0, is calculated as follows.\nwhere formula_7 is the binary entropy function.\n\nThe maximum is attained for\nyielding the following value of formula_1 as a function of \"p\"\n\nFor small \"p\", the capacity is approximated by\n\nas compared to the capacity formula_12 of the binary symmetric channel with crossover probability \"p\".\n\nDefine the following distance function formula_13 on the words formula_14 of length \"n\" transmitted via a Z-channel\nDefine the sphere formula_16 of radius \"t\" around a word formula_17 of length \"n\" as the set of all the words at distance \"t\" or less from formula_18, in other words,\nA code formula_20 of length \"n\" is said to be \"t\"-asymmetric-error-correcting if for any two codewords formula_21, one has formula_22. Denote by formula_23 the maximum number of codewords in a \"t\"-asymmetric-error-correcting code of length \"n\".\n\nThe Varshamov bound.\nFor \"n\"≥1 and \"t\"≥1,\n\nThe constant-weight code bound.\nFor \"n > 2t ≥ 2\", let the sequence \"B, B, ..., B\" be defined as\nThen formula_27\n\n"}
