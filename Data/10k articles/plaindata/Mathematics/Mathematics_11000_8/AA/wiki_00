{"id": "46808043", "url": "https://en.wikipedia.org/wiki?curid=46808043", "title": "Alicia Dickenstein", "text": "Alicia Dickenstein\n\nAlicia Dickenstein (born 17 January 1955, in Buenos Aires) is an Argentine mathematician known for her work on algebraic geometry, particularly toric geometry and tropical geometry. Currently, she is full professor at the University of Buenos Aires and serves as vice-president of the International Mathematical Union (2015–2018 term). Her book \"Mate max: la matemática en todas partes\" presents mathematical problems destined to young children.\n\nDickenstein obtained her Ph.D. from the Universidad de Buenos Aires in 1982. She received the TWAS Prize in 2015.\nShe is editor-in-chief of the journal \"Revista de la Unión Matemática Argentina\". In 2018, Dickenstein was elected as a Fellow of the American Mathematical Society for \"contributions to computational algebra and its applications, especially in systems biology, and for global leadership in supporting underrepresented groups in mathematics.\" \n\n"}
{"id": "1395328", "url": "https://en.wikipedia.org/wiki?curid=1395328", "title": "Andrew Beal", "text": "Andrew Beal\n\nDaniel Andrew \"Andy\" Beal (born November 29, 1952) is an American banker, businessman, investor, poker player, and amateur mathematician. He is a Dallas-based businessman who accumulated wealth in real estate and banking. Born and reared in Lansing, Michigan, Beal is founder and chairman of Beal Bank and Beal Bank USA, as well as other affiliated companies. Beal has an estimated worth of US$11.6 billion as of April 2018.\n\nA number theorist, Beal is also known for the Beal conjecture, a mathematical generalization of Fermat's Last Theorem. He has funded a $1 million standing prize for its proof or disproof. His banks sponsor two annual science and technology fairs affiliated with the International Science & Engineering Fair. Beal participated in some high-stakes poker games in the mid-2000s that were the subject of a book.\n\nBeal grew up in Lansing, Michigan, where his mother worked in state government and his father was a mechanical engineer. His siblings include an older brother and a younger sister. As a teenager, Beal began earning money by fixing and reselling used televisions with the help of his uncle. While attending high school he also installed apartment security systems. He also started a business moving houses and managed rental properties.\n\nBeal excelled on his high school debate team at Lansing Sexton High School and went on to enroll at Michigan State University, where he studied mathematics.\n\nAt age 19, Beal became a real estate investor, where he bought a house in Lansing for $6,500 and started leasing it for $119 per month. Beal became known for buying properties, renovating them and selling them. In 1976, he attended an auction of federal properties in Washington, DC and bid on an apartment building in Waco, TX. His winning bid was $217,500. Three years later he sold the building for more than $1 million. Also in 1976, he enrolled at Baylor University in Waco, TX, but left school to focus on business endeavors.\n\nIn 1981, Beal and a partner bought two housing project buildings in disrepair, the Brick Towers in Newark, New Jersey, for $25,000. Two years later they sold the repaired buildings for $3.2 million to a private investor.\n\nIn 1988, Beal opened a bank in Dallas, and in 2004 another in Las Vegas. Since then, the banks have purchased financial assets and held them as the market improved. The Banks’ purchases have included:\n\n\nBased on the Uniform Bank Performance Report from the Federal Financial Institutions Examination Council, Beal Bank’s return on assets (ROA) was 8.1 in 2008, several times in excess of its peer group (insured savings banks with assets greater than $1 billion). From 2009-2012 Beal Bank generally exceeded its peer group.\n\nBeal Bank USA's ROA is generally several times in excess of its peer group (insured commercial banks with assets greater than $3 billion).\n\nBeal Bank and Beal Bank USA report combined total capital in excess of $2.9 billion and combined total assets in excess of $9.5 billion as of September 2012.\n\nThey have a total of 37 branch locations and offer online banking also. Both banks are members of and are insured by the Federal Deposit Insurance Corporation (FDIC). They offer deposit products to the public including CDs, money market accounts, statement savings accounts, and IRA CD accounts that are insured by the FDIC. Because they do not offer consumer loans or checking accounts, the banks are considered wholesale banks. Both banks purchase pools of non-agency residential first liens and commercial real estate-secured loans in order to fund commercial loans and participations in loans and syndications, through affiliates.\n\nBeal’s major businesses as of 2013 include:\n\nIn 1997, as part of a space privatization trend encouraged by the federal government, Beal started an aerospace company to build rockets with the goal of placing communications satellites in orbit. Operating with more than 200 employees from a 163,000-square-foot space in Frisco, Texas, Beal Aerospace focused on a three-stage, 200-foot-tall rocket. Powered by hydrogen peroxide and kerosene, the engine eliminated the need for a separate ignition system because, as the hydrogen peroxide oxidized, it ignited the kerosene.\n\nFacing competition from new NASA-funded group initiatives, Beal closed the company and ceased operations on Oct. 23, 2000, citing the difficulty private companies face when competing with the governmental subsidies of NASA.\n\nThrough his banks, Beal is an annual title sponsor of:\n\nBeal has funded more than $1 million in prizes for the events. Both fairs are sanctioned events of the International Science & Engineering Fair, and open to students in grades 6 through 12 with winners moving on to national and international competitions.\n\nThrough Beal Bank, Beal also donated $1 million to the Perot Museum of Nature and Science in Dallas, which opened in December 2012. Beal’s companies donated more than 200 computers to the Dallas Independent School District for student use.\n\nBeal is self-taught in number theory in mathematics. In 1993, he publicly stated a new mathematical hypothesis that implies Fermat's Last Theorem as a corollary. His hypothesis has become known as the Beal Conjecture. No counterexample has been found to the conjecture.\n\nTo encourage research on the conjecture, Beal has personally funded a standing prize of $1 million for its proof or disproof. The funds are held in trust by the American Mathematical Society, and an informational website on is hosted by the University of North Texas.\n\nAs of November 2017, the Beal Conjecture prize remains unclaimed.\n\nDuring visits to Las Vegas between 2001 and 2004, Beal participated in high-stakes poker games against professional players. The games included $100,000 to $200,000 limit Texas Hold 'Em poker. On May 13, 2004, at the Las Vegas Bellagio, Beal won one of the largest single hands in poker history, $11.7 million. The games have been chronicled in the Michael Craig book, “The Professor, the Banker, and the Suicide King: Inside the Richest Poker Game of All Time.”\n\nWhile the games outlined in Craig's book ended in 2004, Beal returned to Las Vegas from February 1–5, 2006 to again take on \"The Corporation\" in a $50,000/100,000 Limit Hold 'Em match at the Wynn Las Vegas Casino. Opponents included Todd Brunson, Jennifer Harman, Ted Forrest, and others.\n\nOn February 5, 2006, Beal was down $3.3 million. He then returned to the Wynn Casino a week later, and won approximately $13.6 million from the Corporation during daily poker sessions from February 12–15. The games resumed February 21–23, with world champion poker player Phil Ivey representing the Corporation against Beal at limits of $30,000/60,000 and $50,000/100,000. During these three days, Beal lost $16.6 million to Ivey. According to Jennifer Harman, during an interview on Poker Podcast with Daniel Negreanu on 26 October 2016, she stated that the games went as high as $100,000/200,000.\n\nBeal has been married twice. He has two children with his first wife. In 1996, he married Estonian immigrant Simona Beal. They have four children. Simona filed for divorce in 2010.\n\nBeal endorsed Donald Trump for President of the United States in 2016. Beal serves as one of the top economic advisers to Trump's campaign. Beal donated $2 million to a Trump super PAC in September 2016, and another $1 million for the inaugural festivities according to Forbes.\n"}
{"id": "7475344", "url": "https://en.wikipedia.org/wiki?curid=7475344", "title": "CWC mode", "text": "CWC mode\n\nIn cryptography, CWC Mode (Carter–Wegman + CTR mode) is an AEAD block cipher mode of operation that provides both encryption and built-in message integrity, similar to CCM and OCB modes. It combines the use of CTR mode for encryption with an efficient polynomial Carter–Wegman MAC and is designed by Tadayoshi Kohno, John Viega and Doug Whiting. NIST previously considered CWC mode for standardization, but opted for the similar GCM mode instead.\n\n"}
{"id": "36703918", "url": "https://en.wikipedia.org/wiki?curid=36703918", "title": "Cantelli's inequality", "text": "Cantelli's inequality\n\nIn probability theory, Cantelli's inequality, named after Francesco Paolo Cantelli, is a generalization of Chebyshev's inequality in the case of a single \"tail\". The inequality states that\n\nwhere\n\nCombining the cases of formula_8 and formula_9 gives, for formula_10\n\nThe inequality is due to Francesco Paolo Cantelli. The Chebyshev inequality implies that in any data sample or probability distribution, \"nearly all\" values are close to the mean in terms of the absolute value of the difference between the points of the data sample and the weighted average of the data sample. The Cantelli inequality (sometimes called the \"Chebyshev–Cantelli inequality\" or the \"one-sided Chebyshev inequality\") gives a way of estimating how the points of the data sample are bigger than or smaller than their weighted average without the two tails of the absolute value estimate. The Chebyshev inequality has \"higher moments versions\" and \"vector versions\", and so does the Cantelli inequality.\n\n\nLet formula_2 be a real-valued random variable with finite variance formula_6 and expectation formula_15, and define formula_16 (so that formula_17 and formula_18).\n\nThen, for any formula_19, we have\nthe last inequality being a consequence of Markov's inequality. As the above holds for any choice of formula_21, we can choose to apply it with the value that minimizes the function formula_22. By differentiating, this can be seen to be formula_23, leading to\n\nusing the previous derivation on formula_29. By taking the complement, we obtain\n"}
{"id": "1841654", "url": "https://en.wikipedia.org/wiki?curid=1841654", "title": "Cartan's equivalence method", "text": "Cartan's equivalence method\n\nIn mathematics, Cartan's equivalence method is a technique in differential geometry for determining whether two geometrical structures are the same up to a diffeomorphism. For example, if \"M\" and \"N\" are two Riemannian manifolds with metrics \"g\" and \"h\", respectively, \nwhen is there a diffeomorphism\n\nsuch that\n\nAlthough the answer to this particular question was known in dimension 2 to Gauss and in higher dimensions to Christoffel and perhaps Riemann as well, Élie Cartan and his intellectual heirs developed a technique for answering similar questions for radically different geometric structures. (For example see the Cartan–Karlhede algorithm.)\n\nCartan successfully applied his equivalence method to many such structures, including projective structures, CR structures, and complex structures, as well as ostensibly non-geometrical structures such as the equivalence of Lagrangians and ordinary differential equations. (His techniques were later developed more fully by many others, such as D. C. Spencer and Shiing-Shen Chern.)\n\nThe equivalence method is an essentially algorithmic procedure for determining when two geometric structures are identical. For Cartan, the primary geometrical information was expressed in a coframe or collection of coframes on a differentiable manifold. See method of moving frames.\n\nSpecifically, suppose that \"M\" and \"N\" are a pair of manifolds each carrying a G-structure for a structure group \"G\". This amounts to giving a special class of coframes on \"M\" and \"N\". Cartan's method addresses the question of whether there exists a local diffeomorphism φ:\"M\"→\"N\" under which the \"G\"-structure on \"N\" pulls back to the given \"G\"-structure on \"M\". An equivalence problem has been \"solved\" if one can give a complete set of structural invariants for the \"G\"-structure: meaning that such a diffeomorphism exists if and only if all of the structural invariants agree in a suitably defined sense.\n\nExplicitly, local systems of one-forms θ and γ are given on \"M\" and \"N\", respectively, which span the respective cotangent bundles (i.e., are coframes). The question is whether there is a local diffeomorphism φ:\"M\"→\"N\" such that the pullback of the coframe on \"N\" satisfies\nwhere the coefficient \"g\" is a function on \"M\" taking values in the Lie group \"G\". For example, if \"M\" and \"N\" are Riemannian manifolds, then \"G\"=\"O\"(\"n\") is the orthogonal group and θ and γ are orthonormal coframes of \"M\" and \"N\" respectively. The question of whether two Riemannian manifolds are isometric is then a question of whether there exists a diffeomorphism φ satisfying (1).\n\nThe first step in the Cartan method is to express the pullback relation (1) in as invariant a way as possible through the use of a \"prolongation\". The most economical way to do this is to use a \"G\"-subbundle \"PM\" of the principal bundle of linear coframes \"LM\", although this approach can lead to unnecessary complications when performing actual calculations. In particular, later on this article uses a different approach. But for the purposes of an overview, it is convenient to stick with the principal bundle viewpoint.\n\nThe second step is to use the diffeomorphism invariance of the exterior derivative to try to isolate any other higher-order invariants of the \"G\"-structure. Basically one obtains a connection in the principal bundle \"PM\", with some torsion. The components of the connection and of the torsion are regarded as invariants of the problem.\n\nThe third step is that if the remaining torsion coefficients are not constant in the fibres of the principal bundle \"PM\", it is often possible (although sometimes difficult), to normalize them by setting them equal to a convenient constant value and solving these normalization equations, thereby reducing the effective dimension of the Lie group \"G\". If this occurs, one goes back to step one, now having a Lie group of one lower dimension to work with.\n\nThe main purpose of the first three steps was to reduce the structure group itself as much as possible. Suppose that the equivalence problem has been through the loop enough times that no further reduction is possible. At this point, there are various possible directions in which the equivalence method leads. For most equivalence problems, there are only four cases: complete reduction, involution, prolongation, and degeneracy.\n\nComplete reduction. Here the structure group has been reduced completely to the trivial group. The problem can now be handled by methods such as the Frobenius theorem. In other words, the algorithm has successfully terminated.\n\nOn the other hand, it is possible that the torsion coefficients are constant on the fibres of \"PM\". Equivalently, they no longer depend on the Lie group \"G\" because there is nothing left to normalize, although there may still be some torsion. The three remaining cases assume this.\n\nInvolution. The equivalence problem is said to be involutive (or \"in involution\") if it passes Cartan's test. This is essentially a rank condition on the connection obtained in the first three steps of the procedure. The Cartan test generalizes the Frobenius theorem on the solubility of first-order linear systems of partial differential equations. If the coframes on \"M\" and \"N\" (obtained by a thorough application of the first three steps of the algorithm) agree and satisfy the Cartan test, then the two \"G\"-structures are equivalent. (Actually, to the best of the author's knowledge, the coframes must be real analytic in order for this to hold, because the Cartan-Kähler theorem requires analyticity.)\n\nProlongation. This is the most intricate case. In fact there are two sub-cases. In the first sub-case, all of the torsion can be uniquely absorbed into the connection form. (Riemannian manifolds are an example, since the Levi-Civita connection absorbs all of the torsion). The connection coefficients and their invariant derivatives form a complete set of invariants of the structure, and the equivalence problem is solved. In the second subcase, however, it is either impossible to absorb all of the torsion, or there is some ambiguity (as is often the case in Gaussian elimination, for example). Here, just as in Gaussian elimination, there are additional parameters which appear in attempting to absorb the torsion. These parameters themselves turn out to be additional invariants of the problem, so the structure group \"G\" must be \"prolonged\" into a subgroup of a jet group. Once this is done, one obtains a new coframe on the prolonged space and has to return to the first step of the equivalence method. (See also prolongation of G-structures.)\n\nDegeneracy. Because of a non-uniformity of some rank condition, the equivalence method is unsuccessful in handling this particular equivalence problem. For example, consider the equivalence problem of mapping a manifold \"M\" with a single one-form θ to another manifold with a single one-form γ such that φ*γ=θ. The zeros of these one forms, as well as the rank of their exterior derivatives at each point need to be taken into account. The equivalence method can handle such problems if all of the ranks are uniform, but it is not always suitable if the rank changes. Of course, depending on the particular application, a great deal of information can still be obtained with the equivalence method.\n"}
{"id": "25029425", "url": "https://en.wikipedia.org/wiki?curid=25029425", "title": "Concolic testing", "text": "Concolic testing\n\nConcolic testing (a portmanteau of \"concrete\" and \"symbolic\") is a hybrid software verification technique that performs symbolic execution, a classical technique that treats program variables as symbolic variables, along a \"concrete execution\" (testing on particular inputs) path. Symbolic execution is used in conjunction with an automated theorem prover or constraint solver based on constraint logic programming to generate new concrete inputs (test cases) with the aim of maximizing code coverage. Its main focus is finding bugs in real-world software, rather than demonstrating program correctness.\n\nA description and discussion of the concept was introduced in \"DART: Directed Automated Random Testing\" by Patrice Godefroid, Nils Klarlund, and Koushik Sen. The paper \"CUTE: A concolic unit testing engine for C\", by Koushik Sen, Darko Marinov, and Gul Agha, further extended the idea to data structures, and first coined the term \"concolic testing\". Another tool, called EGT (renamed to EXE and later improved and renamed to KLEE), based on similar ideas was independently developed by Cristian Cadar and Dawson Engler in 2005, and published in 2005 and 2006. PathCrawler first proposed to perform symbolic execution along a concrete execution path, but unlike concolic testing PathCrawler does not simplify complex symbolic constraints using concrete values. These tools (DART and CUTE, EXE) applied concolic testing to unit testing of C programs and concolic testing was originally conceived as a white box improvement upon established random testing methodologies. The technique was later generalized to testing multithreaded Java programs with jCUTE, and unit testing programs from their executable codes (tool OSMOSE). It was also combined with fuzz testing and extended to detect exploitable security issues in large-scale x86 binaries by Microsoft Research's SAGE.\n\nThe concolic approach is also applicable to model checking. In a concolic model checker, the model checker traverses states of the model representing the software being checked, while storing both a concrete state and a symbolic state. The symbolic state is used for checking properties on the software, while the concrete state is used to avoid reaching unreachable state. One such tool is ExpliSAT by Sharon Barner, Cindy Eisner, Ziv Glazberg, Daniel Kroening and Ishai Rabinovitz\n\nImplementation of traditional symbolic execution based testing requires the implementation of a full-fledged symbolic interpreter for a programming language. Concolic testing implementors noticed that implementation of full-fledged symbolic execution can be avoided if symbolic execution can be piggy-backed with the normal execution of a program through instrumentation. This idea of simplifying implementation of symbolic execution gave birth to concolic testing.\n\nAn important reason for the rise of concolic testing (and more generally, symbolic-execution based analysis of programs) in the decade since it was introduced in 2005 is the dramatic improvement in the efficiency and expressive power of SMT Solvers. The key technical developments that lead to the rapid development of SMT solvers include combination of theories, lazy solving, DPLL(T) and the huge improvements in the speed of SAT solvers. SMT solvers that are particularly tuned for concolic testing include Z3, STP, Z3str2, and Boolector.\n\nConsider the following simple example, written in C:\n\nSimple random testing, trying random values of \"x\" and \"y\", would require an impractically large number of tests to reproduce the failure.\n\nWe begin with an arbitrary choice for \"x\" and \"y\", for example \"x\" = \"y\" = 1. In the concrete execution, line 2 sets \"z\" to 2, and the test in line 3 fails since 1 ≠ 100000. Concurrently, the symbolic execution follows the same path but treats \"x\" and \"y\" as symbolic variables. It sets \"z\" to the expression 2\"y\" and notes that, because the test in line 3 failed, \"x\" ≠ 100000. This inequality is called a \"path condition\" and must be true for all executions following the same execution path as the current one.\n\nSince we'd like the program to follow a different execution path on the next run, we take the last path condition encountered, \"x\" ≠ 100000, and negate it, giving \"x\" = 100000. An automated theorem prover is then invoked to find values for the input variables \"x\" and \"y\" given the complete set of symbolic variable values and path conditions constructed during symbolic execution. In this case, a valid response from the theorem prover might be \"x\" = 100000, \"y\" = 0.\n\nRunning the program on this input allows it to reach the inner branch on line 4, which is not taken since 100000 (\"x\") is not less than 0 (\"z\" = 2\"y\"). The path conditions are \"x\" = 100000 and \"x\" ≥ \"z\". The latter is negated, giving \"x\" < \"z\". The theorem prover then looks for \"x\", \"y\" satisfying \"x\" = 100000, \"x\" < \"z\", and \"z\" = 2\"y\"; for example, \"x\" = 100000, \"y\" = 50001. This input reaches the error.\n\nEssentially, a concolic testing algorithm operates as follows:\n\n\nThere are a few complications to the above procedure:\n\nSymbolic-execution based analysis and testing, in general, has witnessed a significant level of interest from industry . Perhaps the most famous commercial tool that uses dynamic symbolic execution (aka concolic testing) is the SAGE tool from Microsoft. The KLEE and S2E tools (both of which are open-source tools, and use the STP constraint solver) are widely used in many companies including Micro Focus Fortify, NVIDIA, and IBM . Increasingly these technologies are being used by many security companies and hackers alike to find security vulnerabilities.\n\nConcolic testing has a number of limitations:\n\n\n\nMany tools, notably DART and SAGE, have not been made available to the public at large. Note however that for instance SAGE is \"used daily\" for internal security testing at Microsoft.\n"}
{"id": "48611088", "url": "https://en.wikipedia.org/wiki?curid=48611088", "title": "Conglomerate (set theory)", "text": "Conglomerate (set theory)\n\nIn mathematics, a conglomerate is a collection of classes, just as a class is a collection of sets. A quasi-category is like a category except that its objects and morphisms form conglomerates instead of classes. The subclasses of any class, and in particular, the collection of all classes (every class is a subclass of the class of all sets), form a conglomerate.\n"}
{"id": "57574496", "url": "https://en.wikipedia.org/wiki?curid=57574496", "title": "Contracted Bianchi identities", "text": "Contracted Bianchi identities\n\nIn general relativity and tensor calculus, the contracted Bianchi identities are:\n\nwhere formula_2 is the Ricci tensor, formula_3 the scalar curvature, and formula_4 indicates covariant differentiation.\n\nA proof can be found in the entry Proofs involving covariant derivatives.\n\nThese identities are named after Luigi Bianchi, although they had been already derived by Aurel Voss in 1880.\n\n\n"}
{"id": "6338535", "url": "https://en.wikipedia.org/wiki?curid=6338535", "title": "DICING", "text": "DICING\n\nIn cryptography, DICING is a stream cypher algorithm developed by Li An-Ping. It has been submitted to the eSTREAM Project of the eCRYPT network.\n"}
{"id": "31316540", "url": "https://en.wikipedia.org/wiki?curid=31316540", "title": "DataScene", "text": "DataScene\n\nDataScene is a scientific graphing, animation, data analysis, and real-time data monitoring software package. It was developed with the Common Language Infrastructure technology and the GDI+ graphics library. With the two Common Language Runtime engines - the .Net and Mono frameworks - DataScene runs on all major operating systems.\n\nWith DataScene, the user can plot 39 types 2D & 3D graphs (e.g., Area graph, Bar graph, Boxplot graph, Pie graph, Line graph, Histogram graph, Surface graph, Polar graph, Water Fall graph, etc.), manipulate, print, and export graphs to various formats (e.g., Bitmap, WMF/EMF, JPEG, PNG, GIF, TIFF, PostScript, and PDF), analyze data with different mathematical methods (fitting curves, calculating statics, FFT, etc.), create chart animations for presentations (e.g. with Powerpoint), classes, and web pages, and monitor and chart real-time data.\n\nDataScene was first released (version 1.0) in March 2009 for the Windows platform and the .Net 2.0 framework. Since version 2.0, DataScene has been ported to the Mono framework 2.6 and all Linux and Unix/X11 operating systems. The latest version of DataScene is version 3.0.\n\nCyberwit offers free licensing for the Express edition of DataScene.\n\n"}
{"id": "4257408", "url": "https://en.wikipedia.org/wiki?curid=4257408", "title": "Denjoy's theorem on rotation number", "text": "Denjoy's theorem on rotation number\n\nIn mathematics, the Denjoy theorem gives a sufficient condition for a diffeomorphism of the circle to be topologically conjugate to a diffeomorphism of a special kind, namely an irrational rotation. proved the theorem in the course of his topological classification of homeomorphisms of the circle. He also gave an example of a \"C\" diffeomorphism with an irrational rotation number that is not conjugate to a rotation.\n\nLet \"ƒ\": \"S\" → \"S\" be an orientation-preserving diffeomorphism of the circle whose rotation number \"θ\" = \"ρ\"(\"ƒ\") is irrational. Assume that it has positive derivative \"ƒ\"(\"x\") > 0 that is a continuous function with bounded variation on the interval [0,1). Then \"ƒ\" is topologically conjugate to the irrational rotation by \"θ\". Moreover, every orbit is dense and every nontrivial interval \"I\" of the circle intersects its forward image \"ƒ\"°(\"I\"), for some \"q\" > 0 (this means that the non-wandering set of \"ƒ\" is the whole circle).\n\nIf \"ƒ\" is a \"C\" map, then the hypothesis on the derivative holds; however, for any irrational rotation number Denjoy constructed an example showing that this condition cannot be relaxed to \"C\", continuous differentiability of \"ƒ\".\n\nVladimir Arnold showed that the conjugating map need not be smooth, even for an analytic diffeomorphism of the circle. Later Michel Herman proved that nonetheless, the conjugating map of an analytic diffeomorphism is itself analytic for \"most\" rotation numbers, forming a set of full Lebesgue measure, namely, for those that are badly approximable by rational numbers. His results are even more general and specify differentiability class of the conjugating map for \"C\" diffeomorphisms with any \"r\" ≥ 3.\n\n\n\n"}
{"id": "50665165", "url": "https://en.wikipedia.org/wiki?curid=50665165", "title": "Dupin hypersurface", "text": "Dupin hypersurface\n\nIn differential geometry, a Dupin hypersurface is a submanifold in a space form, whose principal curvatures have globally constant multiplicities.\n\nA hypersurface is called a Dupin hypersurface if the multiplicity of each principal curvature is constant on hypersurface and each principal curvature is constant along its associated principal directions. All proper Dupin submanifolds arise as focal submanifolds of proper Dupin hypersurfaces.\n"}
{"id": "35777836", "url": "https://en.wikipedia.org/wiki?curid=35777836", "title": "Extinction probability", "text": "Extinction probability\n\nExtinction probability is the chance of an inherited trait becoming extinct as a function of time \"t\". If \"t\" = ∞ this may be the complement of the chance of becoming a universal trait.\n"}
{"id": "22965204", "url": "https://en.wikipedia.org/wiki?curid=22965204", "title": "Fano fibration", "text": "Fano fibration\n\nIn algebraic geometry, a Fano fibration or Fano fiber space, named after Gino Fano, is a morphism of varieties whose general fiber is a Fano variety (in other words has ample anticanonical bundle) of positive dimension. The ones arising from extremal contractions in the minimal model program are called Mori fibrations or Mori fiber spaces (for Shigefumi Mori). They appear as standard forms for varieties without a minimal model.\n\n"}
{"id": "3855752", "url": "https://en.wikipedia.org/wiki?curid=3855752", "title": "Giulio Ascoli", "text": "Giulio Ascoli\n\nGiulio Ascoli (20 January 1843, Trieste – 12 July 1896, Milan) was a Jewish-Italian mathematician. He was a student of the Scuola Normale di Pisa, where he graduated in 1868.\n\nIn 1872 he became Professor of Algebra and Calculus of the Politecnico di Milano University. From 1879 he was professor of mathematics at the Reale Istituto Tecnico Superiore, where, in 1901, was affixed a plaque that remembers him.\n\nHe was also corresponding member of Istituto Lombardo.\n\nHe made contributions to the theory of functions of a real variable and to Fourier series. For example, Ascoli introduced equicontinuity in 1884, a topic regarded as one of the fundamental concepts in the theory of real functions. In 1889, Italian mathematician Cesare Arzelà generalized Ascoli's Theorem into the Arzelà–Ascoli theorem, a practical sequential compactness criterion of functions.\n\n\n\n\n"}
{"id": "147164", "url": "https://en.wikipedia.org/wiki?curid=147164", "title": "Goldbach's weak conjecture", "text": "Goldbach's weak conjecture\n\nIn number theory, Goldbach's weak conjecture, also known as the odd Goldbach conjecture, the ternary Goldbach problem, or the 3-primes problem, states that\n\nThis conjecture is called \"weak\" because if Goldbach's strong conjecture (concerning sums of two primes) is proven, it would be true. This is because if every even number greater than 4 is the sum of two odd primes, merely adding 3 to each even number greater than 4 will produce the odd numbers greater than 7 (7 itself is equal to 2+2+3).\n\nIn 2013, Harald Helfgott published a proof of Goldbach's weak conjecture. As of 2018, the proof is widely accepted in the mathematics community, but it has not yet been published in a peer-reviewed journal.\n\nSome state the conjecture as\nThis version excludes 7 = 2+2+3 because this requires the even prime 2. On odd numbers larger than 7 it is slightly stronger as it also excludes sums like 17 = 2+2+13, which are allowed in the other formulation. Helfgott's proof covers both versions of the conjecture. Like the other formulation, this one also immediately follows from Goldbach's strong conjecture.\n\nIn 1923, Hardy and Littlewood showed that, assuming the generalized Riemann hypothesis, the weak Goldbach conjecture is true for all sufficiently large odd numbers. In 1937, Ivan Matveevich Vinogradov eliminated the dependency on the generalised Riemann hypothesis and proved directly (see Vinogradov's theorem) that all sufficiently large odd numbers can be expressed as the sum of three primes. Vinogradov's original proof, as it used the ineffective Siegel–Walfisz theorem, did not give a bound for \"sufficiently large\"; his student K. Borozdin proved that 3 is large enough. This number has 6,846,169 decimal digits, so checking every number under this figure would be completely infeasible.\n\nIn 1997, Deshouillers, Effinger, te Riele and Zinoviev published a result showing that the generalized Riemann hypothesis implies Goldbach's weak conjecture for all numbers. This result combines a general statement valid for numbers greater than 10 with an extensive computer search of the small cases. Saouter also conducted a computer search covering the same cases at approximately the same time.\n\nOlivier Ramaré in 1995 showed that every even number \"n\" ≥ 4 is in fact the sum of at most six primes, from which it follows that every odd number \"n\" ≥ 5 is the sum of at most seven primes. Leszek Kaniecki showed every odd integer is a sum of at most five primes, under the Riemann Hypothesis. In 2012, Terence Tao proved this without the Riemann Hypothesis; this improves both results.\n\nIn 2002, Liu Ming-Chit (University of Hong Kong) and Wang Tian-Ze lowered this threshold to approximately formula_1. The exponent is still much too large to admit checking all smaller numbers by computer. (Computer searches have only reached as far as 10 for the strong Goldbach conjecture, and not much further than that for the weak Goldbach conjecture.)\n\nIn 2012 and 2013, Peruvian mathematician Harald Helfgott released a pair of papers improving major and minor arc estimates sufficiently to unconditionally prove the weak Goldbach conjecture. Here, the major arcs formula_2 is the union of intervals formula_3 around the rationals formula_4 where formula_5 is a constant. Minor arcs formula_6 are defined to be formula_7.\n"}
{"id": "25264191", "url": "https://en.wikipedia.org/wiki?curid=25264191", "title": "Hardy hierarchy", "text": "Hardy hierarchy\n\nIn computability theory, computational complexity theory and proof theory, the Hardy hierarchy, named after G. H. Hardy, is an ordinal-indexed family of functions \"h\": N → N (where N is the set of natural numbers, {0, 1, ...}). It is related to the fast-growing hierarchy and slow-growing hierarchy. The hierarchy was first described in Hardy's 1904 paper, \"A theorem concerning the infinite cardinal numbers\".\n\nLet μ be a large countable ordinal such that a fundamental sequence is assigned to every limit ordinal less than μ. The Hardy hierarchy of functions \"h\": N → N, for \"α\" < \"μ\", is then defined as follows:\n\n\nHere α[\"n\"] denotes the \"n\" element of the fundamental sequence assigned to the limit ordinal \"α\". A standardized choice of fundamental sequence for all \"α\" ≤ \"ε\" is described in the article on the fast-growing hierarchy.\n\nCaicedo (2007) defines a modified Hardy hierarchy of functions formula_4 by using the standard fundamental sequences, but with α[\"n\"+1] (instead of α[\"n\"]) in the third line of the above definition.\n\nThe Wainer hierarchy of functions \"f\" and the Hardy hierarchy of functions \"h\" are related by \"f\" = \"h\" for all α < ε. Thus, for any α < ε, \"h\" grows much more slowly than does \"f\". However, the Hardy hierarchy \"catches up\" to the Wainer hierarchy at α = ε, such that \"f\" and \"h\" have the same growth rate, in the sense that \"f\"(\"n\"-1) ≤ \"h\"(\"n\") ≤ \"f\"(\"n\"+1) for all \"n\" ≥ 1. \n\n"}
{"id": "44700778", "url": "https://en.wikipedia.org/wiki?curid=44700778", "title": "Hattendorf's theorem", "text": "Hattendorf's theorem\n\nHattendorff's Theorem, attributed to K. Hattendorff (1868), is a theorem in actuarial science that describes the allocation of the variance or risk of the loss random variable over the lifetime of an actuarial reserve. In other words, Hattendorff's theorem demonstrates that the variation in the present value of the loss of an issued insurance policy can be allocated to the future years during which the insured is still alive. This, in turn, facilitates the management of risk prevalent in such insurance contracts over short periods of time.\n\nThe main result of the theorem has three equivalent formulations:\n\nwhere:\n\nIn its above formulation, and in particular the first result, Hattendorff's theorem states that the variance of formula_1, the insurer's total loss over the remaining life of the policy at time h, can be calculated by discounting the variances of the yearly net losses (cash losses plus changes in net liabilities) formula_2 in future years.\n\nIn the most general stochastic setting in which the analysis of reserves is carried out, consider an insurance policy written at time zero, over which the insured pays yearly premiums formula_3 at the beginning of each year starting today until the year of death of the insured. Furthermore, the insured receives a benefit of formula_4, at the end of the year of death, equal to formula_5. No other payments are received nor paid over the lifetime of the policy.\n\nSuppose an insurance company is interested to know the cash loss from this policy over the year (h, h+1). Of course, if the death of the insured happens prior to time h, or when formula_6, then there is no remaining loss and formula_7. If the death of the insured occurs exactly at time h, or when formula_8, then the loss on the policy is equal to the present value of the benefit paid in the following year, formula_9, less the premium paid at time h. Hence in this case formula_10 Lastly, if the death of the insured occurs after time h, or when formula_11, then the cash loss in the year (h, h+1) is just the negative of the premium received at time h (cash inflows are treated as negative losses). Hence we summarize this result as\n\nFurthermore, the actuarial present value of the future cash losses in each year has the explicit formula\n\nIn the analysis of reserves, a central quantity of interest is the benefit reserve formula_14 at time h, which is the expected loss on the policy at time h given that status x has survived to age h\n\nwhich admits to the closed form expression\n\nLastly, the present value of the net cash loss at time h over the year (h, h+1), denoted formula_17, is equal to the present value of the cash loss in year h, formula_18 (see above), plus the present value of the change in liabilites formula_19 at time h. In other words, formula_20. If formula_11, then formula_22. Similarly, if formula_8, then formula_24 since there is no reserve after the year of death. Finally, if formula_6, then there is no loss in the future and formula_26. Summarizing, this yields the following result, which is important in the formulation of Hattendorff's theorem\n\nThe proof of the first equality is written as follows. First, by writing the present value of future net losses at time h,\n\nfrom which it is easy to see that\n\nIt is known that the individual net cash flows in different years are uncorrelated, or formula_30 when formula_31 (see Bowers et al., 1997, for a proof of this result). Using these two results, we conclude that\n\nformula_32\n\nwhich proves the first part of the theorem. The reader is referred to (Bowers et al., pg 241) for the proof of the other equalities.\n\n"}
{"id": "6158260", "url": "https://en.wikipedia.org/wiki?curid=6158260", "title": "Higher residuosity problem", "text": "Higher residuosity problem\n\nIn cryptography, most public key cryptosystems are founded on problems that are believed to be intractable. The higher residuosity problem (also called the n th-residuosity problem) is one such problem. This problem is \"easier\" to solve than integer factorization, so the assumption that this problem is hard to solve is \"stronger\" than the assumption that integer factorization is hard.\n\nIf \"n\" is an integer, then the integers modulo \"n\" form a ring. If \"n\"=\"pq\" where \"p\" and \"q\" are primes, then the Chinese remainder theorem tells us that\n\nThe group of units of any ring form a group, and the group of units in formula_2 is traditionally denoted formula_3.\n\nFrom the isomorphism above, we have\n\nas an isomorphism of \"groups\". Since \"p\" and \"q\" were assumed to be prime, the groups formula_5 and formula_6 are cyclic of orders \"p\"-1 and \"q\"-1 respectively. If \"d\" is a divisor of \"p\"-1, then the set of \"d\"th powers in formula_5 form a subgroup of index \"d\". If gcd(\"d\",\"q\"-1) = 1, then \"every\" element in formula_6 is a \"d\"th power, so the set of \"d\"th powers in formula_9 is also a subgroup of index \"d\". In general, if gcd(\"d\",\"q\"-1) = \"g\", then there are (\"q\"-1)/(\"g\") \"d\"th powers in formula_6, so the set of \"d\"th powers in formula_9 has index \"dg\".\nThis is most commonly seen when \"d\"=2, and we are considering the subgroup of quadratic residues, it is well known that exactly one quarter of the elements in formula_9 are\nquadratic residues (when \"n\" is the product of exactly two primes, as it is here).\n\nThe important point is that for any divisor \"d\" of \"p\"-1 (or \"q\"-1) the set of \"d\"th powers forms a subgroup of formula_13\n\nGiven an integer \"n\" = \"pq\" where \"p\" and \"q\" are unknown, an integer \"d\" such that \"d\" divides \"p\"-1, and an integer \"x\" < \"n\", it is infeasible to determine whether \"x\" is a \"d\"th power (equivalently \"d\"th residue) modulo \"n\".\n\nNotice that if \"p\" and \"q\" are known it is easy to determine whether \"x\" is a \"d\"th residue modulo \"n\" because \"x\" will be a \"d\"th residue modulo \"p\" if and only if\n\nWhen \"d\"=2, this is called the quadratic residuosity problem.\n\nThe semantic security of the Benaloh cryptosystem and the Naccache-Stern cryptosystem rests on the intractability of this problem.\n"}
{"id": "25360121", "url": "https://en.wikipedia.org/wiki?curid=25360121", "title": "Hilbert–Samuel function", "text": "Hilbert–Samuel function\n\nIn commutative algebra the Hilbert–Samuel function, named after David Hilbert and Pierre Samuel, of a nonzero finitely generated module formula_1 over a commutative Noetherian local ring formula_2 and a primary ideal formula_3 of formula_2 is the map formula_5 such that, for all formula_6,\n\nwhere formula_8 denotes the length over formula_2. It is related to the Hilbert function of the associated graded module formula_10 by the identity\n\nFor sufficiently large formula_12, it coincides with a polynomial function of degree equal to formula_13.\n\nFor the ring of formal power series in two variables formula_14 taken as a module over itself and the ideal formula_3 generated by the monomials \"x\" and \"y\" we have\n\nUnlike the Hilbert function, the Hilbert–Samuel function is not additive on an exact sequence. However, it is still reasonably close to being additive, as a consequence of the Artin–Rees lemma. We denote by formula_17 the Hilbert-Samuel polynomial; i.e., it coincides with the Hilbert–Samuel function for large integers.\n\nProof: Tensoring the given exact sequence with formula_18 and computing the kernel we get the exact sequence:\nwhich gives us:\nThe third term on the right can be estimated by Artin-Rees. Indeed, by the lemma, for large \"n\" and some \"k\",\nThus,\nThis gives the desired degree bound.\n\n"}
{"id": "320026", "url": "https://en.wikipedia.org/wiki?curid=320026", "title": "Index notation", "text": "Index notation\n\nIn mathematics and computer programming, index notation is used to specify the elements of an array of numbers. The formalism of how indices are used varies according to the subject. In particular, there are different methods for referring to the elements of a list, a vector, or a matrix, depending on whether one is writing a formal mathematical paper for publication, or when one is writing a computer program.\n\nIt is frequently helpful in mathematics to refer to the elements of an array using subscripts. The subscripts can be integers or variables. The array takes the form of tensors in general, since these can be treated as multi-dimensional arrays. Special (and more familiar) cases are vectors (1d arrays) and matrices (2d arrays).\n\nThe following is only an introduction to the concept: index notation is used in more detail in mathematics (particularly in the representation and manipulation of tensor operations). See the main article for further details.\n\nA vector treated as an array of numbers by writing as a row vector or column vector (whichever is used depends on convenience or context):\n\nIndex notation allows indication of the elements of the array by simply writing \"a\", where the index \"i\" is known to run from 1 to \"n\".\nFor example, given the vector:\n\nthen some entries are\n\nThe notation can be applied to vectors in mathematics and physics. The following vector equation\n\ncan also be written in terms of the elements of the vector (aka components), that is\n\nwhere the indices take a given range of values. This expression represents a set of equations, one for each index. If the vectors each have \"n\" elements, meaning \"i\" = 1,2...\"n\", then the equations are explicitly\n\nHence, index notation serves as an efficient shorthand for\n\nMore than one index is used to describe arrays of numbers, in two or more dimensions, such as the elements of a matrix, (see also image to right);\n\nThe entry of a matrix A is written using two indices, say \"i\" and \"j\", with or without commas to separate the indices: \"a\" or \"a\", where the first subscript is the row number and the second is the column number. Juxtaposition is also used as notation for multiplication; this may be a source of confusion. For example, if\n\nthen some entries are\n\nFor indices larger than 9, the comma-based notation may be superior (e.g., \"a\" instead of \"a\").\n\nMatrix equations are written similarly to vector equations, such as\n\nin terms of the elements of the matrices (aka components)\n\nfor all values of \"i\" and \"j\". Again this expression represents a set of equations, one for each index. If the matrices each have \"m\" rows and \"n\" columns, meaning and , then there are \"mn\" equations.\n\nThe notation allows a clear generalization to multi-dimensional arrays of elements: tensors. For example,\n\nrepresenting a set of many equations.\n\nIn tensor analysis, superscripts are used instead of subscripts to distinguish covariant from contravariant entities, see covariance and contravariance of vectors and raising and lowering indices.\n\nIn several programming languages, index notation is a way of addressing elements of an array. This method is used since it is closest to how it is implemented in assembly language whereby the address of the first element is used as a base, and a multiple (the index) of the element size is used to address inside the array.\n\nFor example, if an array of integers is stored in a region of the computer's memory starting at the memory cell with address 3000 (the base address), and each integer occupies four cells (bytes), then the elements of this array are at memory locations 0x3000, 0x3004, 0x3008, …, 0x3000 + 4(\"n\" − 1). In general, the address of the \"i\"th element of an array with base address \"b\" and element size \"s\" is .\n\nIn the C programming language, we can write the above as (pointer form) or (array indexing form), which is exactly equivalent because the C standard defines the array indexing form as a transformation to pointer form. Coincidentally, since pointer addition is commutative, this allows for obscure expressions such as which is equivalent to .\n\nThings become more interesting when we consider arrays with more than one index, for example, a two-dimensional table. We have three possibilities:\nIn C, all three methods can be used. When the first method is used, the programmer decides how the elements of the array are laid out in the computer's memory, and provides the formulas to compute the location of each element. The second method is used when the number of elements in each row is the same and known at the time the program is written. The programmer declares the array to have, say, three columns by writing e.g. . One then refers to a particular element of the array by writing . The compiler computes the total number of memory cells occupied by each row, uses the first index to find the address of the desired row, and then uses the second index to find the address of the desired element in the row. When the third method is used, the programmer declares the table to be an array of pointers, like in . When the programmer subsequently specifies a particular element , the compiler generates instructions to look up the address of the row specified by the first index, and use this address as the base when computing the address of the element specified by the second index.\n\nThis function multiplies two 3x3 floating point matrices together.\nIn other programming languages such as Pascal, indices may start at 1, so indexing in a block of memory can be changed to fit a start-at-1 addressing scheme by a simple linear transformation - in this scheme, the memory location of the \"i\"th element with base address \"b\" and element size \"s\" is .\n\n"}
{"id": "22778065", "url": "https://en.wikipedia.org/wiki?curid=22778065", "title": "Inductive type", "text": "Inductive type\n\nIn type theory, a system has inductive types if it has facilities for creating a new type along with constants and functions that create terms of that type. The feature serves a role similar to data structures in a programming language and allows a type theory to add concepts like numbers, relations, and trees. As the name suggests, inductive types can be self-referential, but usually only in a way that permits structural recursion.\n\nThe standard example is encoding the natural numbers using Peano's encoding.\n\nHere, a natural number is created either from the constant \"0\" or by applying the function \"S\" to another natural number. \"S\" is the successor function which represents adding 1 to a number. Thus, \"0\" is zero, \"S 0\" is one, \"S (S 0)\" is two, \"S (S (S 0))\" is three, and so on.\n\nSince their introduction, inductive types have been extended to encode more and more structures, while still being predicative and supporting structural recursion.\n\nInductive types usually come with a function to prove properties about them. Thus, \"nat\" may come with:\n\nThis is the expected function for structural recursion for the type \"nat\".\n\nW-types are well-founded types in intuitionistic type theory (ITT). They generalize natural numbers, lists, binary trees, and other \"tree-shaped\" data types. Let be a universe of types. Given a type : and a dependent family : → , one can form a W-type formula_1. The type may be thought of as \"labels\" for the (potentially infinitely many) constructors of the inductive type being defined, whereas indicates the (potentially infinite) arity of each constructor. W-types (resp. M-types) may also be understood as well-founded (resp. non-well-founded) trees with nodes labeled by elements : and where the node labeled by has ()-many subtrees.\n\nLet 0, 1, 2, etc. be finite types with inhabitants 1 : 1, 1, 2:2, etc. One may define the natural numbers as the W-type\n\nwith : 2 → is defined by (1) = 0 (representing the constructor for zero, which takes no arguments), and (2) = 1 (representing the successor function, which takes one argument).\n\nOne may define lists over a type : as formula_3 where\nand 1 is the sole inhabitant of 1. The value of formula_5 corresponds to the constructor for the empty list, whereas the value of formula_6 corresponds to the constructor that appends to the beginning of another list.\n\nThe constructor for elements of a generic W-type formula_1 has type\nWe can also write this rule in the style of a natural deduction proof,\n\nThe elimination rule for W-types works similarly to structural induction on trees. If, whenever a property (under the propositions-as-types interpretation) : → holds for all subtrees of a given tree it also holds for that tree, then it holds for all trees.\n\nIn extensional type theories, W-types (resp. M-types) can be defined up to isomorphism as initial algebras (resp. final coalgebras) for polynomial functors. In this case, the property of initiality (res. finality) corresponds directly to the appropriate induction principle. In intensional type theories with the univalence axiom, this correspondence holds up to homotopy (propositional equality).\n\nM-types are dual to W-types, they represent coinductive (potentially infinite) data such as streams. M-types can be derived from W-types.\n\nThis technique allows \"some\" definitions of multiple types that depend on each other. For example, defining two parity predicates on natural numbers using two mutually inductive types in Coq:\n\nInduction-recursion started as a study into the limits of ITT. Once found, the limits were turned into rules that allowed defining new inductive types. These types could depend upon a function and the function on the type, as long as both were defined simultaneously.\n\nUniverse types can be defined using induction-recursion.\n\nInduction-induction allows definition of a type and a family of types at the same time. So, a type and a family of types formula_11.\n\nThis is a current research area in Homotopy Type Theory (HoTT). HoTT differs from ITT by its identity type (equality). Higher inductive types not only define a new type with constants and functions that create the type, but also new instances of the identity type that relate them.\n\nA simple example is the type, which is defined with two constructors, a basepoint;\n\nand a loop;\n\nThe existence of a new constructor for the identity type makes a higher inductive type.\n\n\n\n"}
{"id": "3976717", "url": "https://en.wikipedia.org/wiki?curid=3976717", "title": "Inverse demand function", "text": "Inverse demand function\n\nIn economics, an 'inverse demand function', P = f(Q), is a function that maps the quantity of output demanded to the market price (dependent variable) for that output. Quantity demanded, Q, is a function of price; the inverse demand function treats price as a function of quantity demanded, and is also called the price function. Note that the inverse demand function is not the reciprocal of the demand function—the word \"inverse\" refers to the mathematical concept of an inverse function.\n\nIn mathematical terms, if the demand function is f(P), then the inverse demand function is f(Q), whose value is the highest price that could be charged and still generate the quantity demanded Q. This is to say that the inverse demand function is the demand function with the axes switched. This is useful because economists typically place price (P) on the vertical axis and quantity (Q) on the horizontal axis.\n\nThe inverse demand function is the same as the average revenue function, since P = AR.\n\nTo compute the inverse demand function, simply solve for P from the demand function. For example, if the demand function has the form Q = 240 - 2P then the inverse demand function would be P = 120 - 0.5Q.\n\nThe inverse demand function can be used to derive the total and marginal revenue functions. Total revenue equals price, P, times quantity, Q, or TR = P×Q. Multiply the inverse demand function by Q to derive the total revenue function: TR = (120 - .5Q) × Q = 120Q - 0.5Q². The marginal revenue function is the first derivative of the total revenue function or MR = 120 - Q. Note that in this linear example the MR function has the same y-intercept as the inverse demand function, the x-intercept of the MR function is one-half the value of the demand function, and the slope of the MR function is twice that of the inverse demand function. This relationship holds true for all linear demand equations. The importance of being able to quickly calculate MR is that the profit-maximizing condition for firms regardless of market structure is to produce where marginal revenue equals marginal cost (MC). To derive MC the first derivative of the total cost function is taken.\n\nFor example, assume cost, C, equals 420 + 60Q + Q. then MC = 60 + 2Q. Equating MR to MC and solving for Q gives Q = 20. So 20 is the profit maximizing quantity: to find the profit-maximizing price simply plug the value of Q into the inverse demand equation and solve for P.\n\nThe inverse demand function is the form of the demand function that appears in the famous Marshallian Scissors diagram. The function appears in this form because economists place the independent variable on the y-axis and the dependent variable on the x-axis. The slope of the inverse function is ∆P/∆Q. This fact should be kept in mind when calculating elasticity. The formula for elasticity is (∆Q/∆P) × (P/Q).\n\nThere is a close relationship between any inverse demand function for a linear demand equation and the marginal revenue function. For any linear demand function with an inverse demand equation of the form P = a - bQ, the marginal revenue function has the form MR = a - 2bQ. The marginal revenue function and inverse linear demand function have the following characteristics:\n\n"}
{"id": "294370", "url": "https://en.wikipedia.org/wiki?curid=294370", "title": "Jacobi identity", "text": "Jacobi identity\n\nIn mathematics the Jacobi identity is a property of a binary operation which describes how the order of evaluation (the placement of parentheses in a multiple product) affects the result of the operation. By contrast, for operations with the associative property, any order of evaluation gives the same result (parentheses in a multiple product are not needed). The identity is named after the German mathematician Carl Gustav Jakob Jacobi. The cross product formula_1 and the Lie bracket operation formula_2 both satisfy the Jacobi identity.\n\nA binary operation \"×\" on a set \"S\" possessing a binary operation \"+\" with an additive identity denoted by 0 satisfies the Jacobi identity if:\n\nThat is, if the sum of all even permutations of (\"a\",(\"b\",\"c\")) is zero (where the permutation is performed by leaving the parentheses fixed and interchanging letters an even number of times).\n\nThe simplest example of a Lie algebra is constructed from the (associative) ring of formula_4 matrices, which may be thought of as infinitesimal motions of an \"n\"-dimensional vector space. The Lie bracket operation is then defined as the commutator, which measures the failure of commutativity in matrix multiplication:\n\nIt is then easy to check the Jacobi identity:\n\nMore generally, suppose A is an associative algebra and \"V\" is a subspace of A with the property that for all \"A\" and \"B\" in A, the element formula_6 belongs to \"V\". Then the Jacobi identity holds on \"V\" for the bracket operator given by formula_7. Thus, if a binary operation satisfies the Jacobi identity, we may say that it \"behaves as if\" it were given by formula_6 in some associative algebra, even if it is not actually defined that way.\n\nUsing the antisymmetry property formula_9, the Jacobi identity can be rewritten as a modification of the associative property: \n\nConsidering formula_11 as the action of the infinitesimal motion \"A\" on \"C\", this can be stated as: \nThere is also a plethora of mixed analogs involving anticommutators, such as\netc. (Graded Jacobi identities)\nThe majority of common examples of the Jacobi identity come from the bracket multiplication on Lie algebras and Lie rings. Because of this the Jacobi identity is often expressed using Lie bracket notation:\n\nBecause the bracket multiplication is antisymmetric, the Jacobi identity admits two equivalent reformulations. Defining the adjoint operator\nformula_14, the identity becomes:\nThus, the Jacobi identity for Lie algebras simply states that the action of any element on the algebra is a derivation. This form of the Jacobi identity is also used to define the notion of Leibniz algebra.\n\nAnother rearrangement shows that the Jacobi identity is equivalent to the following identity between the operators of the adjoint representation:\nThis identity implies that the map sending each element to its adjoint action is a Lie algebra homomorphism of the original algebra into the Lie algebra of its derivations.\n\nThe Hall–Witt identity is the analogous identity for the commutator operation in a group.\n\nIn analytical mechanics, the Jacobi identity is satisfied by the Poisson brackets. In quantum mechanics, it is satisfied by operator commutators on a Hilbert space and, equivalently, in the phase space formulation of quantum mechanics by the Moyal bracket.\n\nThe following identitity follows from anticommutativity and Jacobi identity and holds in arbitrary Lie algebra:\n\n"}
{"id": "57340821", "url": "https://en.wikipedia.org/wiki?curid=57340821", "title": "Kittell graph", "text": "Kittell graph\n\nIn the mathematical field of graph theory, the Kittell graph is a planar graph with 23 vertices and 63 edges. Its unique planar embedding has 42 triangular faces. The Kittell graph is named after Irving Kittell, who used it as a counterexample to Alfred Kempe's flawed proof of the four-color theorem. Simpler counterexamples include the Errera graph and Poussin graph (both published earlier than Kittell) and the Fritsch graph and Soifer graph.\n"}
{"id": "19188374", "url": "https://en.wikipedia.org/wiki?curid=19188374", "title": "Konrad Osterwalder", "text": "Konrad Osterwalder\n\nKonrad Osterwalder (born June 3, 1942) is a Swiss mathematician and physicist, former Undersecretary-General of the United Nations, former Rector of the United Nations University (UNU), and Rector Emeritus of the Swiss Federal Institute of Technology Zurich (ETH Zurich). He is known for the Osterwalder–Schrader theorem.\n\nOsterwalder was appointed to the position of United Nations Under Secretary General and United Nations University Rector by United Nations Secretary-General Ban Ki-moon May 2007 and served until 28 February 2013. He succeeded Prof. Hans van Ginkel from the Netherlands to be the fifth Rector of the United Nations University.\n\nHe is credited with turning United Nations University into a world leading institution, ranked #5 & #6 in two categories according to the 2012 Global Go to Think Tank Rankings. He was responsible for ensuring that UNU's charter was amended by the United Nations General Assembly in 2009 allowing the United Nations University to grant degrees, introducing UNU's degree programmes and creating a new concept in education, research and development by introducing the twin institute programmes. A concept that is changing the way that development, aid and capacity building is approached both by developed countries and developing and least developed countries.\n\nIn March 2000, following the Bologna Declaration by 28 European Education Ministers, the European University Association and the Comite de Liaison within the National Rector's Conference convened the Convention of European Higher Education in salamanca Spain, hereinafter referred to as the \"Salamanca Process\"with the aim of discussing the Bologna Declaration and delivering an overall, univocal response to the Council of Ministers. Professor Osterwalder, Rector of ETH, was chosen by the conference as the Rapporteur of the Salamanca Process and the voice of Higher Education institutions. The meeting concluded with a declaration and a report that led to the basis of Higher Education reform within the Bologna process and the EU. In addition, the two conveners of the conference formed the European University Association.\n\nKonrad Osterwalder was born in Frauenfeld, Thurgau, Switzerland, in June 1942. He studied at the Swiss Federal Institute of Technology (Eidgenössische Technische Hochschule; ETH) in Zurich, where he earned a Diploma in theoretical physics in 1965 and a Doctorate in theoretical physics in 1970. He is married to Verena Osterwalder-Bollag, an analytical therapist. They have three kids.\n\nAfter one year with the Courant Institute of Mathematical Sciences, New York University, he accepted a research position at Harvard University in 1971. He remained on the faculty of Harvard for seven years, and was promoted to Assistant Professor for Mathematical Physics in 1973 and Associate Professor for Mathematical Physics in 1976. In 1977, he returned to Switzerland upon being appointed a full Professor for Mathematical Physics at ETH Zurich. His doctoral students include Felix Finster and Emil J. Straube.\n\nDuring his tenure at ETH Zurich, Osterwalder served as Head of the Department of Mathematics (1986–1990) and Head of the Planning Committee (1990–1995), and was founder of the Centro Stefano Franscini seminar center in Ascona. He was appointed Rector of ETH in 1995 and held that post for 12 years. From November 2006 through August 2007, he also served concurrently as ETH President pro tempore.\n\nOn 1 September 2007, Osterwalder joined the United Nations University as its fifth rector. In that role, he held the rank of Under-Secretary-General of the United Nations.\n\nOsterwalder's research focused on the mathematical structure of relativistic quantum field theory as well as on elementary particle physics and statistical mechanics. During his long and distinguished career, he has been a Visiting Fellow/Guest Professor at several prominent universities around the world, including the Institut des Hautes Études Scientifiques (IHES; Bures-sur-Yvette, France); Harvard University; University of Texas (Austin); Max Planck Institute for Physics and Astrophysics (Munich), Università La Sapienza (Rome); Università di Napoli; Waseda University; and Weizmann Institute of Science (Rehovot, Israel).\n\nSince 2014 - member of International Scientific Council of Tomsk Polytechnic University.\n\nOsterwalder career encompasses service on many advisory boards, committees and associations including as\n\n\nOsterwalder has been a recipient of many honours and prizes including:\n\n\n\n"}
{"id": "91591", "url": "https://en.wikipedia.org/wiki?curid=91591", "title": "Linearity", "text": "Linearity\n\nLinearity is the property of a mathematical relationship or function which means that it can be graphically represented as a straight line. Examples are the relationship of voltage and current across a resistor (Ohm's law), or the mass and weight of an object. Proportionality implies linearity, but linearity does not imply proportionality.\n\nIn mathematics, a linear map or linear function \"f\"(\"x\") is a function that satisfies the following two properties:\n\n\nThe homogeneity and additivity properties together are called the superposition principle. It can be shown that additivity implies homogeneity in all cases where α is rational; this is done by proving the case where α is a natural number by mathematical induction and then extending the result to arbitrary rational numbers. If \"f\" is assumed to be continuous as well, then this can be extended to show homogeneity for any real number α, using the fact that rationals form a dense subset of the reals.\n\nIn this definition, \"x\" is not necessarily a real number, but can in general be a member of any vector space. A more specific definition of linear function, not coinciding with the definition of linear map, is used in elementary mathematics.\n\nThe concept of linearity can be extended to linear operators. Important examples of linear operators include the derivative considered as a differential operator, and many constructed from it, such as del and the Laplacian. When a differential equation can be expressed in linear form, it is generally straightforward to solve by breaking the equation up into smaller pieces, solving each of those pieces, and summing the solutions.\n\nLinear algebra is the branch of mathematics concerned with the study of vectors, vector spaces (also called linear spaces), linear transformations (also called linear maps), and systems of linear equations.\n\nThe word linear comes from the Latin word \"linearis\", which means \"pertaining to or resembling a line\". For a description of linear and nonlinear equations, see \"linear equation\". Nonlinear equations and functions are of interest to physicists and mathematicians because they can be used to represent many natural phenomena, including chaos.\n\nIn a different usage to the above definition, a polynomial of degree 1 is said to be linear, because the graph of a function of that form is a line.\n\nOver the reals, a linear equation is one of the forms:\n\nwhere \"m\" is often called the slope or gradient; \"b\" the y-intercept, which gives the point of intersection between the graph of the function and the \"y\"-axis.\n\nNote that this usage of the term \"linear\" is not the same as in the section above, because linear polynomials over the real numbers do not in general satisfy either additivity or homogeneity. In fact, they do so if and only if . Hence, if , the function is often called an affine function (see in greater generality affine transformation).\n\nIn Boolean algebra, a linear function is a function formula_2 for which there exist formula_3 such that\n\nA Boolean function is linear if one of the following holds for the function's truth table:\n\nAnother way to express this is that each variable always makes a difference in the truth value of the operation or it never makes a difference.\n\nNegation, Logical biconditional, exclusive or, tautology, and contradiction are linear functions.\n\nIn physics, \"linearity\" is a property of the differential equations governing many systems; for instance, the Maxwell equations or the diffusion equation.\n\nLinearity of a differential equation means that if two functions \"f\" and \"g\" are solutions of the equation, then any linear combination is, too.\n\nIn instrumentation, linearity means that for every change in the variable you are observing, you get the same change in the output of the measurement apparatus - this is highly desirable in scientific work. In general, instruments are close to linear over a useful certain range, and most useful within that range. In contrast, human senses are highly nonlinear- for instance, the brain totally ignores incoming light unless it exceeds a certain absolute threshold number of photons.\n\nIn electronics, the linear operating region of a device, for example a transistor, is where a dependent variable (such as the transistor collector current) is directly proportional to an independent variable (such as the base current). This ensures that an analog output is an accurate representation of an input, typically with higher amplitude (amplified). A typical example of linear equipment is a high fidelity audio amplifier, which must amplify a signal without changing its waveform. Others are linear filters, linear regulators, and linear amplifiers in general.\n\nIn most scientific and technological, as distinct from mathematical, applications, something may be described as linear if the characteristic is approximately but not exactly a straight line; and linearity may be valid only within a certain operating region—for example, a high-fidelity amplifier may distort a small signal, but sufficiently little to be acceptable (acceptable but imperfect linearity); and may distort very badly if the input exceeds a certain value, taking it away from the approximately linear part of the transfer function.\n\nFor an electronic device (or other physical device) that converts a quantity to another quantity, Bertram S. Kolts writes:\n\nThere are three basic definitions for integral linearity in common use: independent linearity, zero-based linearity, and terminal, or end-point, linearity. In each case, linearity defines how well the device's actual performance across a specified operating range approximates a straight line. Linearity is usually measured in terms of a deviation, or non-linearity, from an ideal straight line and it is typically expressed in terms of percent of full scale, or in ppm (parts per million) of full scale. Typically, the straight line is obtained by performing a least-squares fit of the data. The three definitions vary in the manner in which the straight line is positioned relative to the actual device's performance. Also, all three of these definitions ignore any gain, or offset errors that may be present in the actual device's performance characteristics.\n\nMany times a device's specifications will simply refer to linearity, with no other explanation as to which type of linearity is intended. In cases where a specification is expressed simply as linearity, it is assumed to imply independent linearity.\n\nIndependent linearity is probably the most commonly used linearity definition and is often found in the specifications for DMMs and ADCs, as well as devices like potentiometers. Independent linearity is defined as the maximum deviation of actual performance relative to a straight line, located such that it minimizes the maximum deviation. In that case there are no constraints placed upon the positioning of the straight line and it may be wherever necessary to minimize the deviations between it and the device's actual performance characteristic.\n\nZero-based linearity forces the lower range value of the straight line to be equal to the actual lower range value of the device's characteristic, but it does allow the line to be rotated to minimize the maximum deviation. In this case, since the positioning of the straight line is constrained by the requirement that the lower range values of the line and the device's characteristic be coincident, the non-linearity based on this definition will generally be larger than for independent linearity.\n\nFor terminal linearity, there is no flexibility allowed in the placement of the straight line in order to minimize the deviations. The straight line must be located such that each of its end-points coincides with the device's actual upper and lower range values. This means that the non-linearity measured by this definition will typically be larger than that measured by the independent, or the zero-based linearity definitions. This definition of linearity is often associated with ADCs, DACs and various sensors.\n\nA fourth linearity definition, absolute linearity, is sometimes also encountered. Absolute linearity is a variation of terminal linearity, in that it allows no flexibility in the placement of the straight line, however in this case the gain and offset errors of the actual device are included in the linearity measurement, making this the most difficult measure of a device's performance. For absolute linearity the end points of the straight line are defined by the ideal upper and lower range values for the device, rather than the actual values. The linearity error in this instance is the maximum deviation of the actual device's performance from ideal.\n\nIn military tactical formations, \"linear formations\" were adapted from phalanx-like formations of pike protected by handgunners towards shallow formations of handgunners protected by progressively fewer pikes. This kind of formation would get thinner until its extreme in the age of Wellington with the 'Thin Red Line'. It would eventually be replaced by skirmish order at the time of the invention of the breech-loading rifle that allowed soldiers to move and fire independently of the large-scale formations and fight in small, mobile units.\n\nLinear is one of the five categories proposed by Swiss art historian Heinrich Wölfflin to distinguish \"Classic\", or Renaissance art, from the Baroque. According to Wölfflin, painters of the fifteenth and early sixteenth centuries (Leonardo da Vinci, Raphael or Albrecht Dürer) are more linear than \"painterly\" Baroque painters of the seventeenth century (Peter Paul Rubens, Rembrandt, and Velázquez) because they primarily use outline to create shape. Linearity in art can also be referenced in digital art. For example, hypertext fiction can be an example of nonlinear narrative, but there are also websites designed to go in a specified, organized manner, following a linear path.\n\nIn music the linear aspect is succession, either intervals or melody, as opposed to simultaneity or the vertical aspect.\n\nIn measurement, the term \"linear foot\" refers to the number of feet in a straight line of material (such as lumber or fabric) generally without regard to the width. It is sometimes incorrectly referred to as \"lineal feet\"; however, \"lineal\" is typically reserved for usage when referring to ancestry or heredity. The words \"linear\" & \"lineal\" \nboth descend from the same root meaning, the Latin word for line, which is \"linea\".\n\n"}
{"id": "888711", "url": "https://en.wikipedia.org/wiki?curid=888711", "title": "Mathematical statistics", "text": "Mathematical statistics\n\nMathematical statistics is the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data. Specific mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure theory.\n\nStatistical data collection is concerned with the planning of studies, especially with the design of randomized experiments and with the planning of surveys using random sampling. The initial analysis of the data often follows the study protocol specified prior to the study being conducted. The data from a study can also be analyzed to consider secondary hypotheses inspired by the initial results, or to suggest new studies. A secondary analysis of the data from a planned study uses tools from data analysis, and the process of doing this is mathematical statistics.\n\nData analysis is divided into:\n\n\nWhile the tools of data analysis work best on data from randomized studies, they are also applied to other kinds of data. For example, from natural experiments and observational studies, in which case the inference is dependent on the model chosen by the statistician, and so subjective.\n\nThe following are some of the important topics in mathematical statistics:\n\nA probability distribution is a function that assigns a probability to each measurable subset of the possible outcomes of a random experiment, survey, or procedure of statistical inference. Examples are found in experiments whose sample space is non-numerical, where the distribution would be a categorical distribution; experiments whose sample space is encoded by discrete random variables, where the distribution can be specified by a probability mass function; and experiments with sample spaces encoded by continuous random variables, where the distribution can be specified by a probability density function. More complex experiments, such as those involving stochastic processes defined in continuous time, may demand the use of more general probability measures.\n\nA probability distribution can either be univariate or multivariate. A univariate distribution gives the probabilities of a single random variable taking on various alternative values; a multivariate distribution (a joint probability distribution) gives the probabilities of a random vector—a set of two or more random variables—taking on various combinations of values. Important and commonly encountered univariate probability distributions include the binomial distribution, the hypergeometric distribution, and the normal distribution. The multivariate normal distribution is a commonly encountered multivariate distribution.\n\n\nStatistical inference is the process of drawing conclusions from data that are subject to random variation, for example, observational errors or sampling variation. Initial requirements of such a system of procedures for inference and induction are that the system should produce reasonable answers when applied to well-defined situations and that it should be general enough to be applied across a range of situations. Inferential statistics are used to test hypotheses and make estimations using sample data. Whereas descriptive statistics describe a sample, inferential statistics infer predictions about a larger population that the sample represents.\n\nThe outcome of statistical inference may be an answer to the question \"what should be done next?\", where this might be a decision about making further experiments or surveys, or about drawing a conclusion before implementing some organizational or governmental policy.\nFor the most part, statistical inference makes propositions about populations, using data drawn from the population of interest via some form of random sampling. More generally, data about a random process is obtained from its observed behavior during a finite period of time. Given a parameter or hypothesis about which one wishes to make inference, statistical inference most often uses:\n\nIn statistics, regression analysis is a statistical process for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables. More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed. Most commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, the estimation target is a function of the independent variables called the regression function. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the regression function which can be described by a probability distribution.\n\nMany techniques for carrying out regression analysis have been developed. Familiar methods, such as linear regression, are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data (e.g. using ordinary least squares). Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional.\n\nNonparametric statistics are values calculated from data in a way that is not based on parameterized families of probability distributions. They include both descriptive and inferential statistics. The typical parameters are the mean, variance, etc. Unlike parametric statistics, nonparametric statistics make no assumptions about the probability distributions of the variables being assessed.\n\nNon-parametric methods are widely used for studying populations that take on a ranked order (such as movie reviews receiving one to four stars). The use of non-parametric methods may be necessary when data have a ranking but no clear numerical interpretation, such as when assessing preferences. In terms of levels of measurement, non-parametric methods result in \"ordinal\" data.\n\nAs non-parametric methods make fewer assumptions, their applicability is much wider than the corresponding parametric methods. In particular, they may be applied in situations where less is known about the application in question. Also, due to the reliance on fewer assumptions, non-parametric methods are more robust.\n\nAnother justification for the use of non-parametric methods is simplicity. In certain cases, even when the use of parametric methods is justified, non-parametric methods may be easier to use. Due both to this simplicity and to their greater robustness, non-parametric methods are seen by some statisticians as leaving less room for improper use and misunderstanding.\n\nMathematical statistics is a key subset of the discipline of statistics. Statistical theorists study and improve statistical procedures with mathematics, and statistical research often raises mathematical questions. Statistical theory relies on probability and decision theory.\n\nMathematicians and statisticians like Gauss, Laplace, and C. S. Peirce used decision theory with probability distributions and loss functions (or utility functions). The decision-theoretic approach to statistical inference was reinvigorated by Abraham Wald and his successors, and makes extensive use of scientific computing, analysis, and optimization; for the design of experiments, statisticians use algebra and combinatorics.\n\n\n"}
{"id": "49403913", "url": "https://en.wikipedia.org/wiki?curid=49403913", "title": "Mathematics and the Search for Knowledge", "text": "Mathematics and the Search for Knowledge\n\nMathematics and the Search for Knowledge is a book by Morris Kline on the developing mathematics ideas, which are partially overlap with his previous book \"\", as a source of human knowledge about the physical world, starting from astronomical theories of Ancient Greek to the modern theories.\n\nIn contrast to the numerous theories, that have appeared since the ancient times up to Newton's theory of gravitation, which are describe different physical phenomena and were often intuitive and can be mechanically explained, but all modern theories, such as electromagnetism, theory of relativity, quantum mechanics, are the mathematical description of reality, which could not be granted with a clear interpretation, which would be available to human senses.\n\nAbout the concepts that appear and are used in these theories to describe the physical world, we are the only known - mathematical relationships that they are satisfy (for example, an electromagnetic radiation, wave-particle duality, spacetime, or an electron).\n\nDue to the limitations of our senses capability (for example, from the whole spectrum of electromagnetic radiation the human eye perceives only a small part) and the ability to mislead us (for example, optical illusion), human is forced to use the mathematics as a tool that allows us to not only to compensate the imperfection of our senses, but also to obtain new knowledge, which are not available to our sensory perception.\n\nThe author brings us to the idea that the physical world, is not what available to us in our sensation, but rather what human-made mathematical theories say.\n\n"}
{"id": "252953", "url": "https://en.wikipedia.org/wiki?curid=252953", "title": "Method of Fluxions", "text": "Method of Fluxions\n\nMethod of Fluxions is a book by Isaac Newton. The book was completed in 1671, and published in 1736. Fluxion is Newton's term for a derivative. He originally developed the method at Woolsthorpe Manor during the closing of Cambridge during the Great Plague of London from 1665 to 1667, but did not choose to make his findings known (similarly, his findings which eventually became the \"Philosophiae Naturalis Principia Mathematica\" were developed at this time and hidden from the world in Newton's notes for many years). Gottfried Leibniz developed his form of calculus independently around 1673, 7 years after Newton had developed the basis for differential calculus, as seen in surviving documents like “the method of fluxions and fluents...\" from 1666. Leibniz however published his discovery of differential calculus in 1684, nine years before Newton formally published his fluxion notation form of calculus in part during 1693. The calculus notation in use today is mostly that of Leibniz, although Newton's dot notation for differentiation formula_1 for denoting derivatives with respect to time is still in current use throughout mechanics and circuit analysis.\n\nNewton's \"Method of Fluxions\" was formally published posthumously, but following Leibniz's publication of the calculus a bitter rivalry erupted between the two mathematicians over who had developed the calculus first and so Newton no longer hid his knowledge of fluxions.\n\nFor a period of time encompassing Newton's working life, the discipline of analysis was a subject of controversy in the mathematical community. Although analytic techniques provided solutions to long-standing problems, including problems of quadrature and the finding of tangents, the proofs of these solutions were not known to be reducible to the synthetic rules of Euclidean geometry. Instead, analysts were often forced to invoke infinitesimal, or \"infinitely small\", quantities to justify their algebraic manipulations. Some of Newton's mathematical contemporaries, such as Isaac Barrow, were highly skeptical of such techniques, which had no clear geometric interpretation. Although in his early work Newton also used infinitesimals in his derivations without justifying them, he later developed something akin to the modern definition of limits in order to justify his work.\n\n"}
{"id": "41775284", "url": "https://en.wikipedia.org/wiki?curid=41775284", "title": "P-space", "text": "P-space\n\nIn the mathematical field of topology, there are various notions of a P\"-space and of a p\"-space.\n\nThe expression \"P-space\" might be used generically to denote a topological space satisfying some given and previously introduced topological invariant \"P\". This might apply also to spaces of a different kind, i.e. non-topological spaces with additional structure.\n\nA \"P-space\" in the sense of Gillman–Henriksen is a topological space in which every countable intersection of open sets is open. An equivalent condition is that countable unions of closed sets are closed. In other words, G sets are open and F sets are closed. The letter \"P\" stands for both \"pseudo-discrete\" and \"prime\". Gillman and Henriksen also define a \"P-point\" as a point at which any prime ideal of the ring of real-valued continuous functions is maximal, and a P-space is a space in which every point is a P-point.\n\nDifferent authors restrict their attention to topological spaces that satisfy various separation axioms. With the right axioms, one may characterize \"P\"-spaces in terms of their rings of continuous real-valued functions.\n\nSpecial kinds of \"P\"-spaces include Alexandrov-discrete spaces, in which arbitrary intersections of open sets are open. These in turn include locally finite spaces, which include finite spaces and discrete spaces.\n\nA different notion of a \"P-space\" has been introduced by Kiiti Morita in 1964, in connection with his (now solved) conjectures (see the relative entry for more information). Spaces satisfying the covering property introduced by Morita are sometimes also called \"Morita P-spaces\" or \"normal P-spaces\".\n\nA notion of a \"p-space\" has been introduced by Alexander Arhangelskii.\n\n\n"}
{"id": "8007266", "url": "https://en.wikipedia.org/wiki?curid=8007266", "title": "Peter Cameron (mathematician)", "text": "Peter Cameron (mathematician)\n\nPeter Jephson Cameron FRSE (born 23 January 1947) is an Australian mathematician who works in\ngroup theory, combinatorics, coding theory, and model theory. He is currently half-time Professor of Mathematics at the University of St Andrews, and Emeritus Professor at Queen Mary University of London.\n\nCameron received a B.Sc. from the University of Queensland and a D.Phil. in 1971 from University of Oxford, with Peter M. Neumann as his supervisor. Subsequently, he was a Junior Research Fellow and later a Tutorial Fellow at Merton College, Oxford, and also lecturer at Bedford College, London. \n\nCameron specialises in algebra and combinatorics; he has written books about combinatorics, algebra, permutation groups, and logic, and has produced over 250 academic papers. He posed the Cameron–Erdős conjecture with Paul Erdős.\n\nHe was awarded the London Mathematical Society's Whitehead Prize in 1979 and is joint winner of the 2003 Euler Medal.In 2018 he was elected a Fellow of the Royal Society of Edinburgh.\n\n\n\n"}
{"id": "1807504", "url": "https://en.wikipedia.org/wiki?curid=1807504", "title": "Proofs involving the Laplace–Beltrami operator", "text": "Proofs involving the Laplace–Beltrami operator\n\nThe claim is made that −div is adjoint to \"d\":\n\nProof of the above statement:\n\nIf \"f\" has compact support, then the last integral vanishes, and we have the desired result.\n\nOne may prove that the Laplace–de Rham operator is equivalent to the definition of the Laplace–Beltrami operator, when acting on a scalar function \"f\". This proof reads as:\n\nwhere vol; is the volume form and \"ε\" is the completely antisymmetric Levi-Civita symbol. Note that in the above, the italic lower-case index \"i\" is a single index, whereas the upper-case Roman \"J\" stands for all of the remaining indices. Notice that the Laplace–de Rham operator is actually the negative Laplace–Beltrami operator; this minus sign follows from the conventional definition of the properties of the codifferential. Unfortunately, Δ is used to denote both; reader beware.\n\nGiven scalar functions \"f\" and \"h\", and a real number \"a\", the Laplacian has the property:\n\n\\delta\\,\\mathrm{d}fh = \n\\delta(f\\,\\mathrm{d}h + h\\,\\mathrm{d}f) = \n"}
{"id": "24510992", "url": "https://en.wikipedia.org/wiki?curid=24510992", "title": "Proofs related to chi-squared distribution", "text": "Proofs related to chi-squared distribution\n\nThe following are proofs of several characteristics related to the chi-squared distribution.\n\nLet random variable \"Y\" be defined as \"Y\" = \"X\" where \"X\" has normal distribution with mean 0 and variance 1 (that is \"X\" ~ \"N\"(0,1)).\n\nThen,<br>\nformula_1\n\nWhere formula_3 and formula_4 are the cdf and pdf of the corresponding random variables.\n\nThen formula_5\n\nThe change of variable formula (implicitly derived above), for a monotonic transformation formula_6, is: \n\nIn this case the change is not monotonic, because every value of formula_8 has two corresponding values of formula_9 (one positive and negative). However, because of symmetry, both halves will transform identically, i.e.\n\nIn this case, the transformation is: formula_11, and its derivative is\nformula_12\n\nSo here:\n\nAnd one gets the chi-squared distribution, noting the property of the gamma function: formula_14.\n\nThere are several methods to derive chi-squared distribution with 2 degrees of freedom. Here is one based on the distribution with 1 degree of freedom.\n\nSuppose that formula_15 and formula_16 are two independent variables satisfying formula_17 and formula_18, so that the probability density functions of formula_15 and formula_16 are respectively:\n\nand\n\nSimply, we can derive the joint distribution of formula_15 and formula_16:\n\nwhere formula_26 is replaced by formula_27. Further, let formula_28 and formula_29, we can get that:\n\nand\n\nor, inversely\n\nand\n\nSince the two variable change policies are symmetric, we take the upper one and multiply the result by 2. The Jacobian determinant can be calculated as:\n\nNow we can change formula_35 to formula_36:\n\nwhere the leading constant 2 is to take both the two variable change policies into account. Finally, we integrate out formula_38 to get the distribution of formula_39, i.e. formula_40:\n\nLet formula_42, the equation can be changed to:\n\nSo the result is:\n\nConsider the \"k\" samples formula_45 to represent a single point in a \"k\"-dimensional space. The chi square distribution for \"k\" degrees of freedom will then be given by:\n\nwhere formula_47 is the standard normal distribution and formula_48 is that elemental shell volume at \"Q\"(\"x\"), which is proportional to the (\"k\" − 1)-dimensional surface in \"k\"-space for which\n\nIt can be seen that this surface is the surface of a \"k\"-dimensional ball or, alternatively, an n-sphere where \"n\" = \"k\" - 1 with radius formula_50, and that the term in the exponent is simply expressed in terms of \"Q\". Since it is a constant, it may be removed from inside the integral.\n\nThe integral is now simply the surface area \"A\" of the (\"k\" − 1)-sphere times the infinitesimal thickness of the sphere which is\n\nThe area of a (\"k\" − 1)-sphere is:\n\nSubstituting, realizing that formula_54, and cancelling terms yields:\n"}
{"id": "451286", "url": "https://en.wikipedia.org/wiki?curid=451286", "title": "Random oracle", "text": "Random oracle\n\nIn cryptography, a random oracle is an oracle (a theoretical black box) that responds to every \"unique query\" with a (truly) random response chosen uniformly from its output domain. If a query is repeated it responds the same way every time that query is submitted.\n\nStated differently, a random oracle is a mathematical function chosen uniformly at random, that is, a function mapping each possible query to a (fixed) random response from its output domain.\n\nRandom oracles as a mathematical abstraction were firstly used in rigorous cryptographic proofs in the 1993 publication by Mihir Bellare and Phillip Rogaway (1993). They are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function. A system that is proven secure when every hash function is replaced by a random oracle is described as being secure in the random oracle model, as opposed to secure in the standard model of cryptography.\n\nRandom oracles are typically used as an ideal replacement for cryptographic hash functions in schemes where strong randomness assumptions are needed of the hash function's output. Such a proof generally shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the oracle, or solve some mathematical problem believed hard in order to break it.\n\nNot all uses of cryptographic hash functions require random oracles: schemes that require only one or more properties having a definition in the standard model (such as collision resistance, preimage resistance, second preimage resistance, etc.) can often be proven secure in the standard model (e.g., the Cramer–Shoup cryptosystem).\n\nRandom oracles have long been considered in computational complexity theory, and many schemes have been proven secure in the random oracle model, for example Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme. In 1986, Amos Fiat and Adi Shamir showed a major application of random oracles – the removal of interaction from protocols for the creation of signatures.\n\nIn 1989, Russell Impagliazzo and Steven Rudich showed the limitation of random oracles – namely that their existence alone is not sufficient for secret-key exchange.\n\nIn 1993, Mihir Bellare and Phillip Rogaway were the first to advocate their use in cryptographic constructions. In their definition, the random oracle produces a bit-string of infinite length which can be truncated to the length desired.\n\nWhen a random oracle is used within a security proof, it is made available to all players, including the adversary or adversaries. A single oracle may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query (e.g., queries formatted as \"1|x\" or \"0|x\" can be considered as calls to two separate random oracles, similarly \"00|x\", \"01|x\", \"10|x\" and \"11|x\" can be used to represent calls to four separate random oracles).\n\nAccording to the Church–Turing thesis, no function computable by a finite algorithm can implement a true random oracle (which by definition requires an infinite description).\n\nIn fact, certain artificial signature and encryption schemes are known which are proven secure in the random oracle model, but which are trivially insecure when any real function is substituted for the random oracle. Nonetheless, for any more natural protocol a proof of security in the random oracle model gives very strong evidence of the \"practical\" security of the protocol.\n\nIn general, if a protocol is proven secure, attacks to that protocol must either be outside what was proven, or break one of the assumptions in the proof; for instance if the proof relies on the hardness of integer factorization, to break this assumption one must discover a fast integer factorization algorithm. Instead, to break the random oracle assumption, one must discover some unknown and undesirable property of the actual hash function; for good hash functions where such properties are believed unlikely, the considered protocol can be considered secure.\n\nAlthough the Baker–Gill–Solovay theorem showed that there exists an oracle A such that P = NP, subsequent work by Bennett and Gill, showed that for a \"random oracle\" B (a function from {0,1} to {0,1} such that each input element maps to each of 0 or 1 with probability 1/2, independently of the mapping of all other inputs), P ⊊ NP with probability 1. Similar separations, as well as the fact that random oracles separate classes with probability 0 or 1 (as a consequence of the Kolmogorov's zero–one law), led to the creation of the Random Oracle Hypothesis, that two \"acceptable\" complexity classes C and C are equal if and only if they are equal (with probability 1) under a random oracle (the acceptability of a complexity class is defined in BG81). This hypothesis was later shown to be false, as the two acceptable complexity classes IP and PSPACE were shown to be equal despite IP ⊊ PSPACE for a random oracle A with probability 1.\n\nAn \"ideal\" cipher is a random permutation oracle that is used to model an idealized block cipher.\nA random permutation decrypts each ciphertext block into one and only one plaintext block and vice versa, so there is a one-to-one correspondence.\nSome cryptographic proofs make not only the \"forward\" permutation available to all players, but also the \"reverse\" permutation.\n\nRecent works showed that an ideal cipher can be constructed from a random oracle using 10-round or even 8-round Feistel networks.\n\n"}
{"id": "52079701", "url": "https://en.wikipedia.org/wiki?curid=52079701", "title": "Regulus (geometry)", "text": "Regulus (geometry)\n\nIn three-dimensional space, a regulus \"R\" is a set of skew lines, every point of which is on a transversal which intersects an element of \"R\" only once, and such that every point on a transversal lies on a line of \"R\" \n\nThe set of transversals of \"R\" forms an opposite regulus \"S\". In ℝ the union \"R\" ∪ \"S\" is the ruled surface of a hyperboloid of one sheet.\n\nThree skew lines determine a regulus:\n\nAccording to Charlotte Scott, \"The regulus supplies extremely simple proofs of the properties of a conic...the theorems of Chasles, Brianchon, and Pascal ...\"\n\nIn a finite geometry PG(3, \"q\"), a regulus has \"q\" + 1 lines. For example, in 1954 William Edge described a pair of reguli of four lines each in PG(3,3).\n\n\n"}
{"id": "2715277", "url": "https://en.wikipedia.org/wiki?curid=2715277", "title": "Robin Wilson (mathematician)", "text": "Robin Wilson (mathematician)\n\nRobin James Wilson (born 5 December 1943) is an emeritus professor in the Department of Mathematics at the Open University, having previously been Head of the Pure Mathematics Department and Dean of the Faculty. He was a Stipendiary Lecturer at Pembroke College, Oxford and, , Professor of Geometry at Gresham College, London, where he has also been a visiting professor. On occasion, he guest-teaches at Colorado College in the United States.\n\nFrom January 1999 to September 2003, Robin Wilson was editor-in-chief of the European Mathematical Society Newsletter.\n\nRobin Wilson is the son of Harold Wilson, former Prime Minister of the United Kingdom, and his wife Mary Wilson. He is married; the couple have two daughters.\n\n\nWilson's academic interests lie in graph theory, particularly in colouring problems, e.g. the four colour problem, and algebraic properties of graphs.\n\nHe also researches the history of mathematics, particularly British mathematics and mathematics in the 17th century and the period 1860 to 1940 and the history of graph theory and combinatorics.\n\nIn 1974, he won the Lester R. Ford Award from the Mathematical Association of America for his expository article \"An introduction to matroid theory\".\n\nDue to his collaboration on a 1977 paper with the Hungarian mathematician Paul Erdős, Wilson has an Erdős number of 1.\n\nIn July 2008, he published a study of the mathematical work of Lewis Carroll, the creator of \"Alice's Adventures in Wonderland\" and \"Through the Looking-Glass\" — \"Lewis Carroll in Numberland: His Fantastical Mathematical Logical Life\" (Allen Lane, 2008. ).\n\nHe is President of the British Society for the History of Mathematics.\n\nHe has strong interests in music, including the operas of Gilbert and Sullivan, and is the co-author (with Frederic Lloyd) of \"Gilbert and Sullivan: The Official D'Oyly Carte Picture History\". In 2007, he was a guest on \"Private Passions\", the biographical music discussion programme on BBC Radio 3.\n\nWilson has written or edited about thirty books, including popular books on sudoku and the Four Color Theorem:\n\n\n"}
{"id": "2285574", "url": "https://en.wikipedia.org/wiki?curid=2285574", "title": "SLAM project", "text": "SLAM project\n\nThe SLAM project, which was started by Microsoft Research, aimed at verifying some software safety properties using model checking techniques. It is implemented in OCaml, and has been used to find many bugs in Windows Device Drivers. It is distributed as part of the Microsoft Windows Driver Foundation development kit as the Static Driver Verifier (SDV).\n\nSLAM uses a technique called counterexample-guided abstraction refinement, which uses progressively better models of the program under test.\n\n\"SLAM originally was an acronym but we found it too cumbersome to explain. We now prefer to think of 'slamming' the bugs in a program.\" It probably stood for \"Software, Languages, Analysis, and Modeling.\" Note that Microsoft has since re-used SLAM to stand for \"Social Location Annotation Mobile\".\n\n\n"}
{"id": "46754", "url": "https://en.wikipedia.org/wiki?curid=46754", "title": "Sacred geometry", "text": "Sacred geometry\n\nSacred geometry ascribes symbolic and sacred meanings to certain geometric shapes and certain geometric proportions. It is associated with the belief that a god is the geometer of the world. The geometry used in the design and construction of religious structures such as churches, temples, mosques, religious monuments, altars, and tabernacles has sometimes been considered sacred. The concept applies also to sacred spaces such as temenoi, sacred groves, village greens and holy wells, and the creation of religious art.\n\nThe belief that a god created the universe according to a geometric plan has ancient origins. Plutarch attributed the belief to Plato, writing that \"Plato said god geometrizes continually\" (\"Convivialium disputationum\", liber 8,2). In modern times, the mathematician Carl Friedrich Gauss adapted this quote, saying \"God arithmetizes\".\n\nAs late as Johannes Kepler (1571–1630), a belief in the geometric underpinnings of the cosmos persisted among some scientists.\n\nAccording to Stephen Skinner, the study of sacred geometry has its roots in the study of nature, and the mathematical principles at work therein. Many forms observed in nature can be related to geometry; for example, the chambered nautilus grows at a constant rate and so its shell forms a logarithmic spiral to accommodate that growth without changing shape. Also, honeybees construct hexagonal cells to hold their honey. These and other correspondences are sometimes interpreted in terms of sacred geometry and considered to be further proof of the natural significance of geometric forms.\n\nGeometric ratios, and geometric figures were often employed in the designs of ancient Egyptian, ancient Indian, Greek and Roman architecture. Medieval European cathedrals also incorporated symbolic geometry. Indian and Himalayan spiritual communities often constructed temples and fortifications on design plans of mandala and yantra.\n\nMany of the sacred geometry principles of the human body and of ancient architecture were compiled into the Vitruvian Man drawing by Leonardo da Vinci. The latter drawing was itself based on the much older writings of the Roman architect Vitruvius.\n\nIslamic geometric patterns are well known too, which are used in the Qu'ran, Mosques and even in the caligraphies of personal names.\n\nThe Agamas are a collection of Sanskrit, Tamil, and Grantha scriptures chiefly constituting the methods of temple construction and creation of idols, worship means of deities, philosophical doctrines, meditative practices, attainment of sixfold desires, and four kinds of yoga.\n\nElaborate rules are laid out in the Agamas for Shilpa (the art of sculpture) describing the quality requirements of such matters as the places where temples are to be built, the kinds of image to be installed, the materials from which they are to be made, their dimensions, proportions, air circulation, and lighting in the temple complex. The Manasara and Silpasara are works that deal with these rules. The rituals of daily worship at the temple also follow rules laid out in the Agamas.\n\nThe construction of Medieval European cathedrals was often based on geometries intended to make the viewer see the world through mathematics, and through this understanding, gain a better understanding of the divine. These churches frequently featured a Latin Cross floor-plan. \n\nAt the beginning of the Renaissance in Europe, views shifted to favor simple and regular geometries. The circle in particular became a central and symbolic shape for the base of buildings, as it represented the perfection of nature and the centrality of man's place in the universe. The use of the circle and other simple and symmetrical geometric shapes was solidified as a staple of Renaissance religious architecture in Leon Battista Alberti's architectural treatise, which described the ideal church in terms of spiritual geometry.\n\nStephen Skinner discusses the tendency of some writers to place a geometric diagram over virtually any image of a natural object or human created structure, find some lines intersecting the image and declare it based on sacred geometry. If the geometric diagram does not intersect major physical points in the image, the result is what Skinner calls \"unanchored geometry\".\n\n\n\n"}
{"id": "29205627", "url": "https://en.wikipedia.org/wiki?curid=29205627", "title": "Segal space", "text": "Segal space\n\nIn mathematics, a Segal space is a simplicial space satisfying some pullback conditions, making it look like a homotopical version of a category. More precisely, a simplicial set, considered as a simplicial discrete space, satisfies the Segal conditions iff it is the nerve of a category. The condition for Segal spaces is a homotopical version of this. \n\nComplete Segal spaces were introduced by as models for (∞, 1)-categories.\n\n"}
{"id": "44107834", "url": "https://en.wikipedia.org/wiki?curid=44107834", "title": "Signal magnitude area", "text": "Signal magnitude area\n\nIn mathematics, the signal magnitude area (abbreviated SMA or sma) is a statistical measure of the magnitude of a varying quantity.\n\nThe SMA value of a set of values (or a continuous-time waveform) is the normalized integral of the original values.\n\nIn the case of a set of \"n\" values formula_1 matching a time length \"T\", the SMA\n\nIn the continuous domain, we have for example, with a 3-axis signal with an offset correction \"a\" for each axis, the following equation:\n\n"}
{"id": "12024508", "url": "https://en.wikipedia.org/wiki?curid=12024508", "title": "Small-bias sample space", "text": "Small-bias sample space\n\nIn theoretical computer science, a small-bias sample space (also known as formula_1-biased sample space, formula_1-biased generator, or small-bias probability space) is a probability distribution that fools parity functions.\nIn other words, no parity function can distinguish between a small-bias sample space and the uniform distribution with high probability, and hence, small-bias sample spaces naturally give rise to pseudorandom generators for parity functions.\n\nThe main useful property of small-bias sample spaces is that they need far fewer truly random bits than the uniform distribution to fool parities. Efficient constructions of small-bias sample spaces have found many applications in computer science, some of which are derandomization, error-correcting codes, and probabilistically checkable proofs.\nThe connection with error-correcting codes is in fact very strong since formula_1-biased sample spaces are \"equivalent\" to formula_1-balanced error-correcting codes.\n\nLet formula_5 be a probability distribution over formula_6.\nThe \"bias\" of formula_5 with respect to a set of indices formula_8 is defined as\nwhere the sum is taken over formula_10, the finite field with two elements. In other words, the sum formula_11 equals formula_12 if the number of ones in the sample formula_13 at the positions defined by formula_14 is even, and otherwise, the sum equals formula_15.\nFor formula_16, the empty sum is defined to be zero, and hence formula_17.\n\nA probability distribution formula_5 over formula_6 is called an \"formula_1-biased sample space\" if\nformula_21\nholds for all non-empty subsets formula_22.\n\nAn formula_1-biased sample space formula_5 that is generated by picking a uniform element from a multiset formula_25 is called \"formula_1-biased set\".\nThe \"size\" formula_27 of an formula_1-biased set formula_5 is the size of the multiset that generates the sample space.\n\nAn formula_1-biased generator formula_31 is a function that maps strings of length formula_32 to strings of length formula_33 such that the multiset formula_34 is an formula_1-biased set. The \"seed length\" of the generator is the number formula_32 and is related to the size of the formula_1-biased set formula_38 via the equation formula_39.\n\nThere is a close connection between formula_1-biased sets and \"formula_1-balanced\" linear error-correcting codes.\nA linear code formula_42 of message length formula_33 and block length formula_27 is\n\"formula_1-balanced\" if the Hamming weight of every nonzero codeword formula_46 is between formula_47 and formula_48.\nSince formula_49 is a linear code, its generator matrix is an formula_50-matrix formula_51 over formula_10 with formula_53.\n\nThen it holds that a multiset formula_54 is formula_1-biased if and only if the linear code formula_56, whose columns are exactly elements of formula_5, is formula_1-balanced.\n\nUsually the goal is to find formula_1-biased sets that have a small size formula_27 relative to the parameters formula_33 and formula_1.\nThis is because a smaller size formula_27 means that the amount of randomness needed to pick a random element from the set is smaller, and so the set can be used to fool parities using few random bits.\n\nThe probabilistic method gives a non-explicit construction that achieves size formula_64.\nThe construction is non-explicit in the sense that finding the formula_1-biased set requires a lot of true randomness, which does not help towards the goal of reducing the overall randomness.\nHowever, this non-explicit construction is useful because it shows that these efficient codes exist.\nOn the other hand, the best known lower bound for the size of formula_1-biased sets is formula_67, that is, in order for a set to be formula_1-biased, it must be at least that big.\n\nThere are many explicit, i.e., deterministic constructions of formula_1-biased sets with various parameter settings:\nThese bounds are mutually incomparable. In particular, none of these constructions yields the smallest formula_1-biased sets for all settings of formula_1 and formula_33.\n\nAn important application of small-bias sets lies in the construction of almost k-wise independent sample spaces.\n\nA random variable formula_81 over formula_6 is a \"k-wise independent space\" if, for all index sets formula_83 of size formula_84, the marginal distribution formula_85 is exactly equal to the uniform distribution over formula_86.\nThat is, for all such formula_14 and all strings formula_88, the distribution formula_81 satisfies formula_90.\n\nk-wise independent spaces are fairly well understood.\n\n constructs a formula_84-wise independent space formula_81 over the finite field with some prime number formula_96 of elements, i.e., formula_81 is a distribution over formula_98. The initial formula_84 marginals of the distribution are drawn independently and uniformly at random:\nFor each formula_101 with formula_102, the marginal distribution of formula_103 is then defined as\nwhere the calculation is done in formula_105.\nThe distribution formula_81 is uniform on its support, and hence, the support of formula_81 forms a \"formula_84-wise independent set\".\nIt contains all formula_91 strings in formula_113 that have been extended to strings of length formula_33 using the deterministic rule above.\n\nA random variable formula_81 over formula_6 is a \"formula_117-almost k-wise independent space\" if, for all index sets formula_83 of size formula_84, the restricted distribution formula_85 and the uniform distribution formula_121 on formula_86 are formula_117-close in 1-norm, i.e., formula_124.\n\n give a general framework for combining small k-wise independent spaces with small formula_1-biased spaces to obtain formula_117-almost k-wise independent spaces of even smaller size.\nIn particular, let formula_127 be a linear mapping that generates a k-wise independent space and let formula_128 be a generator of an formula_1-biased set over formula_130.\nThat is, when given a uniformly random input, the output of formula_131 is a k-wise independent space, and the output of formula_132 is formula_1-biased.\nThen formula_134 with formula_135 is a generator of an formula_117-almost formula_84-wise independent space, where formula_138.\n\nAs mentioned above, construct a generator formula_131 with formula_140, and construct a generator formula_132 with formula_142.\nHence, the concatenation formula_143 of formula_131 and formula_132 has seed length formula_146.\nIn order for formula_143 to yield a formula_117-almost k-wise independent space, we need to set formula_149, which leads to a seed length of formula_150 and a sample space of total size formula_151.\n\n"}
{"id": "1240378", "url": "https://en.wikipedia.org/wiki?curid=1240378", "title": "Symmetry breaking", "text": "Symmetry breaking\n\nIn physics, symmetry breaking is a phenomenon in which (infinitesimally) small fluctuations acting on a system crossing a critical point decide the system's fate, by determining which branch of a bifurcation is taken. To an outside observer unaware of the fluctuations (or \"noise\"), the choice will appear arbitrary. This process is called symmetry \"breaking\", because such transitions usually bring the system from a symmetric but disorderly state into one or more definite states. Symmetry breaking is thought to play a major role in pattern formation.\n\nIn 1972, Nobel laureate P.W. Anderson used the idea of symmetry breaking to show some of the drawbacks of reductionism in his paper titled \"More is different\" in \"Science\".\n\nSymmetry breaking can be distinguished into two types, explicit symmetry breaking and spontaneous symmetry breaking, characterized by whether the equations of motion fail to be invariant or the ground state fails to be invariant.\n\nIn explicit symmetry breaking, the equations of motion describing a system are variant under the broken symmetry.\n\nIn spontaneous symmetry breaking, the equations of motion of the system are invariant, but the system is not because the background (spacetime) of the system, its vacuum, is non-invariant. Such a symmetry breaking is parametrized by an order parameter. A special case of this type of symmetry breaking is dynamical symmetry breaking.\n\nSymmetry breaking can cover any of the following scenarios:\n\nOne of the first cases of broken symmetry discussed in the physics literature is related to the form taken by a uniformly rotating body of incompressible fluid in gravitational and hydrostatic equilibrium. Jacobi and soon later Liouville, in 1834, discussed the fact that a tri-axial ellipsoid was an equilibrium solution for this problem when the kinetic energy compared to the gravitational energy of the rotating body exceeded a certain critical value. The axial symmetry presented by the McLaurin spheroids is broken at this bifurcation point. Furthermore, above this bifurcation point, and for constant angular momentum, the solutions that minimize the kinetic energy are the \"non\"-axially symmetric Jacobi ellipsoids instead of the Maclaurin spheroids.\n\n"}
{"id": "2382632", "url": "https://en.wikipedia.org/wiki?curid=2382632", "title": "Tanner graph", "text": "Tanner graph\n\nIn coding theory, a Tanner graph, named after Michael Tanner, is a bipartite graph used to state constraints or equations which specify error correcting codes. In coding theory, Tanner graphs are used to construct longer codes from smaller ones. Both encoders and decoders employ these graphs extensively.\n\nTanner graphs were proposed by Michael Tanner as a means to create larger error correcting codes from smaller ones using recursive techniques. He generalized the techniques of Elias for product codes.\n\nTanner discussed lower bounds on the codes obtained from these graphs irrespective of the specific characteristics of the codes which were being used to construct larger codes.\n\nTanner graphs are partitioned into subcode nodes and digit nodes. For linear block codes, the subcode nodes denote rows of the parity-check matrix H. The digit nodes represent the columns of the matrix H. An edge connects a subcode node to a digit node if a nonzero entry exists in the intersection of the corresponding row and column.\n\nTanner proved the following bounds\n\nLet formula_1 be the rate of the resulting linear code, let the degree of the digit nodes be formula_2 and the degree of the subcode nodes be formula_3. If each subcode node is associated with a linear code (n,k) with rate r = k/n, then the rate of the code is bounded by\n\nThe advantage of these recursive techniques is that they are computationally tractable. The coding\nalgorithm for Tanner graphs is extremely efficient in practice, although it is not\nguaranteed to converge except for cycle-free graphs, which are known not to admit asymptotically\ngood codes.\n\nZemor's decoding algorithm, which is a recursive low-complexity approach to code construction, is based on Tanner graphs.\n\n"}
{"id": "30448", "url": "https://en.wikipedia.org/wiki?curid=30448", "title": "Taylor series", "text": "Taylor series\n\nIn mathematics, a Taylor series is a representation of a function as an infinite sum of terms that are calculated from the values of the function's derivatives at a single point.\n\nThe concept of a Taylor series was formulated by the Scottish mathematician James Gregory and formally introduced by the English mathematician Brook Taylor in 1715. If the Taylor series is centered at zero, then that series is also called a Maclaurin series, named after the Scottish mathematician Colin Maclaurin, who made extensive use of this special case of Taylor series in the 18th century.\n\nA function can be approximated by using a finite number of terms of its Taylor series. Taylor's theorem gives quantitative estimates on the error introduced by the use of such an approximation. The polynomial formed by taking some initial terms of the Taylor series is called a Taylor polynomial. The Taylor series of a function is the limit of that function's Taylor polynomials as the degree increases, provided that the limit exists. A function may not be equal to its Taylor series, even if its Taylor series converges at every point. A function that is equal to its Taylor series in an open interval (or a disc in the complex plane) is known as an analytic function in that interval.\n\nThe Taylor series of a real or complex-valued function that is infinitely differentiable at a real or complex number is the power series\n\nwhich can be written in the more compact sigma notation as\n\nwhere denotes the factorial of and denotes the th derivative of evaluated at the point . The derivative of order zero of is defined to be itself and and are both defined to be 1. When , the series is also called a Maclaurin series.\n\nThe Taylor series for any polynomial is the polynomial itself.\n\nThe Maclaurin series for is the geometric series\n\nso the Taylor series for at is\n\nBy integrating the above Maclaurin series, we find the Maclaurin series for , where log denotes the natural logarithm:\n\nand the corresponding Taylor series for at is\n\nand more generally, the corresponding Taylor series for at some is:\n\nThe Taylor series for the exponential function at is\n\nThe above expansion holds because the derivative of with respect to is also and equals 1. This leaves the terms in the numerator and in the denominator for each term in the infinite sum.\n\nThe Greek philosopher Zeno considered the problem of summing an infinite series to achieve a finite result, but rejected it as an impossibility: the result was Zeno's paradox. Later, Aristotle proposed a philosophical resolution of the paradox, but the mathematical content was apparently unresolved until taken up by Archimedes, as it had been prior to Aristotle by the Presocratic Atomist Democritus. It was through Archimedes's method of exhaustion that an infinite number of progressive subdivisions could be performed to achieve a finite result. Liu Hui independently employed a similar method a few centuries later.\n\nIn the 14th century, the earliest examples of the use of Taylor series and closely related methods were given by Madhava of Sangamagrama. Though no record of his work survives, writings of later Indian mathematicians suggest that he found a number of special cases of the Taylor series, including those for the trigonometric functions of sine, cosine, tangent, and arctangent. The Kerala School of Astronomy and Mathematics further expanded his works with various series expansions and rational approximations until the 16th century.\n\nIn the 17th century, James Gregory also worked in this area and published several Maclaurin series. It was not until 1715 however that a general method for constructing these series for all functions for which they exist was finally provided by Brook Taylor, after whom the series are now named.\n\nThe Maclaurin series was named after Colin Maclaurin, a professor in Edinburgh, who published the special case of the Taylor result in the 18th century.\n\nIf is given by a convergent power series in an open disc (or interval in the real line) centered at in the complex plane, it is said to be analytic in this disc. Thus for in this disc, is given by a convergent power series\nDifferentiating by the above formula times, then setting gives:\nand so the power series expansion agrees with the Taylor series. Thus a function is analytic in an open disc centered at if and only if its Taylor series converges to the value of the function at each point of the disc.\n\nIf is equal to its Taylor series for all in the complex plane, it is called entire. The polynomials, exponential function , and the trigonometric functions sine and cosine, are examples of entire functions. Examples of functions that are not entire include the square root, the logarithm, the trigonometric function tangent, and its inverse, arctan. For these functions the Taylor series do not converge if is far from . That is, the Taylor series diverges at if the distance between and is larger than the radius of convergence. The Taylor series can be used to calculate the value of an entire function at every point, if the value of the function, and of all of its derivatives, are known at a single point.\n\nUses of the Taylor series for analytic functions include:\n\nPictured on the right is an accurate approximation of around the point . The pink curve is a polynomial of degree seven:\n\nThe error in this approximation is no more than . In particular, for , the error is less than 0.000003.\n\nIn contrast, also shown is a picture of the natural logarithm function and some of its Taylor polynomials around . These approximations converge to the function only in the region ; outside of this region the higher-degree Taylor polynomials are \"worse\" approximations for the function. This is similar to Runge's phenomenon.\n\nThe \"error\" incurred in approximating a function by its th-degree Taylor polynomial is called the \"remainder\" or \"residual\" and is denoted by the function . Taylor's theorem can be used to obtain a bound on the size of the remainder.\n\nIn general, Taylor series need not be convergent at all. And in fact the set of functions with a convergent Taylor series is a meager set in the Fréchet space of smooth functions. And even if the Taylor series of a function does converge, its limit need not in general be equal to the value of the function . For example, the function\nis infinitely differentiable at , and has all derivatives zero there. Consequently, the Taylor series of about is identically zero. However, is not the zero function, so does not equal its Taylor series around the origin. Thus, is an example of a non-analytic smooth function.\n\nIn real analysis, this example shows that there are infinitely differentiable functions whose Taylor series are \"not\" equal to even if they converge. By contrast, the holomorphic functions studied in complex analysis always possess a convergent Taylor series, and even the Taylor series of meromorphic functions, which might have singularities, never converge to a value different from the function itself. The complex function , however, does not approach 0 when approaches 0 along the imaginary axis, so it is not continuous in the complex plane and its Taylor series is undefined at 0.\n\nMore generally, every sequence of real or complex numbers can appear as coefficients in the Taylor series of an infinitely differentiable function defined on the real line, a consequence of Borel's lemma. As a result, the radius of convergence of a Taylor series can be zero. There are even infinitely differentiable functions defined on the real line whose Taylor series have a radius of convergence 0 everywhere.\n\nA function cannot be written as a Taylor series centered at a singularity; in these cases, one can often still achieve a series expansion if one allows also negative powers of the variable ; see Laurent series. For example, can be written as a Laurent series.\n\nThere is, however, a generalization of the Taylor series that does converge to the value of the function itself for any bounded continuous function on , using the calculus of finite differences. Specifically, one has the following theorem, due to Einar Hille, that for any ,\nHere is the th finite difference operator with step size . The series is precisely the Taylor series, except that divided differences appear in place of differentiation: the series is formally similar to the Newton series. When the function is analytic at , the terms in the series converge to the terms of the Taylor series, and in this sense generalizes the usual Taylor series.\n\nIn general, for any infinite sequence , the following power series identity holds:\nSo in particular,\nThe series on the right is the expectation value of , where is a Poisson-distributed random variable that takes the value with probability . Hence,\nThe law of large numbers implies that the identity holds.\n\nSeveral important Maclaurin series expansions follow. All these expansions are valid for complex arguments .\n\nThe exponential function formula_17 (with base ) has Maclaurin series\nIt converges for all .\n\nThe natural logarithm (with base ) has Maclaurin series\nThey converge for formula_20.\n\nThe geometric series and its derivatives have Maclaurin series\n\nAll are convergent for formula_20. These are special cases of the binomial series given in the next section.\n\nThe binomial series is the power series\n\nwhose coefficients are the generalized binomial coefficients\n\n(If , this product is an empty product and has value 1.) It converges for formula_20 for any real or complex number .\n\nWhen , this is essentially the infinite geometric series mentioned in the previous section. The special cases and give the square root function and its inverse:\n\nThe usual trigonometric functions and their inverses have the following Maclaurin series:\n\nAll angles are expressed in radians. The numbers appearing in the expansions of are the Bernoulli numbers. The in the expansion of are Euler numbers.\n\nThe hyperbolic functions have Maclaurin series closely related to the series for the corresponding trigonometric functions:\n\nThe numbers appearing in the series for are the Bernoulli numbers.\n\nSeveral methods exist for the calculation of Taylor series of a large number of functions. One can attempt to use the definition of the Taylor series, though this often requires generalizing the form of the coefficients according to a readily apparent pattern. Alternatively, one can use manipulations such as substitution, multiplication or division, addition or subtraction of standard Taylor series to construct the Taylor series of a function, by virtue of Taylor series being power series. In some cases, one can also derive the Taylor series by repeatedly applying integration by parts. Particularly convenient is the use of computer algebra systems to calculate Taylor series.\n\nIn order to compute the 7th degree Maclaurin polynomial for the function\none may first rewrite the function as\nThe Taylor series for the natural logarithm is (using the big O notation)\nand for the cosine function\nThe latter series expansion has a zero constant term, which enables us to substitute the second series into the first one and to easily omit terms of higher order than the 7th degree by using the big notation:\n\nSince the cosine is an even function, the coefficients for all the odd powers have to be zero.\n\nSuppose we want the Taylor series at 0 of the function\nWe have for the exponential function\nand, as in the first example,\nAssume the power series is\nThen multiplication with the denominator and substitution of the series of the cosine yields\nCollecting the terms up to fourth order yields\nThe values of formula_40 can be found by comparison of coefficients with the top expression for formula_17, yielding:\n\nHere we employ a method called \"indirect expansion\" to expand the given function. This method uses the known Taylor expansion of the exponential function. In order to expand as a Taylor series in , we use the known Taylor series of function :\nThus,\n\nClassically, algebraic functions are defined by an algebraic equation, and transcendental functions (including those discussed above) are defined by some property that holds for them, such as a differential equation. For example, the exponential function is the function which is equal to its own derivative everywhere, and assumes the value 1 at the origin. However, one may equally well define an analytic function by its Taylor series.\n\nTaylor series are used to define functions and \"operators\" in diverse areas of mathematics. In particular, this is true in areas where the classical definitions of functions break down. For example, using Taylor series, one may extend analytic functions to sets of matrices and operators, such as the matrix exponential or matrix logarithm.\n\nIn other areas, such as formal analysis, it is more convenient to work directly with the power series themselves. Thus one may define a solution of a differential equation \"as\" a power series which, one hopes to prove, is the Taylor series of the desired solution.\n\nThe Taylor series may also be generalized to functions of more than one variable with\n\nFor example, for a function formula_46 that depends on two variables, and , the Taylor series to second order about the point is\n\nwhere the subscripts denote the respective partial derivatives.\n\nA second-order Taylor series expansion of a scalar-valued function of more than one variable can be written compactly as\n\nwhere is the gradient of evaluated at and is the Hessian matrix. Applying the multi-index notation the Taylor series for several variables becomes\n\nwhich is to be understood as a still more abbreviated multi-index version of the first equation of this paragraph, again in full analogy to the single variable case.\n\nIn order to compute a second-order Taylor series expansion around point of the function\n\none first computes all the necessary partial derivatives:\n\nEvaluating these derivatives at the origin gives the Taylor coefficients\n\nSubstituting these values in to the general formula\n\nproduces\n\nSince is analytic in , we have\n\nThe trigonometric Fourier series enables one to express a periodic function (or a function defined on a closed interval ) as an infinite sum of trigonometric functions (sines and cosines). In this sense, the Fourier series is analogous to Taylor series, since the latter allows one to express a function as an infinite sum of powers. Nevertheless, the two series differ from each other in several relevant issues:\n\n\n\n"}
{"id": "58565905", "url": "https://en.wikipedia.org/wiki?curid=58565905", "title": "The Unicist Research Institute", "text": "The Unicist Research Institute\n\nThe Unicist Research Institute is a global decentralized research boutique, specialized in complexity science, focused on the research of complex adaptive systems and their applications in the fields of social, institutional and individual behavior. It was founded in 1976 by Peter Belohlavek as an unincorporated association that was later transformed into a privately held company.\n\nWith its holding in Delaware, USA and its Research Bunker in Adrogue, Argentina, The Unicist Research Institute has a business arm and an academic arm. Some of its core specialties are the development of cognitive systems, social behavior labs and educational programs for graduates.\n\nThe framework of the complexity science research methodology used in The Unicist Research Institute is based on pragmatism, structuralism and functionalism.\n\nThe four basic pillars of the applied research of The Unicist Research Institute were the ontogenesis of evolution, the anthropological invariables and their evolution, the functionality of the roots of human intelligence, and the double dialectical behavior.\n\nThe Future Research Laboratory of The Unicist Research Institute develops future scenarios as the core aspect for social and economic development plans. In this area, The Unicist Research Institute introduced evolutionary economics in the microeconomics driven development program for countries such as Argentina, Brazil, Spain, Ukraine and the United Kingdom based on its 40-year scenario building work in the field.\n\nSome of the main applied researches of the institute include:\n\n\nThe academic arm of The Unicist Research Institute uses the unicist reflection driven education approach for graduates, which is homologous to research-based learning programs, but focused on researching the root causes of complex adaptive environments.\n\nIn the field of Business Education this approach is based on learning to manage the concepts and fundamentals that underlie business functions and define the root causes of the problems to manage the root drivers of the solutions.\n\nSome of the main accomplishments of the institute include:\n\n\n"}
{"id": "48152888", "url": "https://en.wikipedia.org/wiki?curid=48152888", "title": "Thomas W. Hawkins Jr.", "text": "Thomas W. Hawkins Jr.\n\nThomas W. Hawkins Jr. (born 10 January 1938 in Flushing, New York) is an American historian of mathematics.\n\nHawkins defended his Ph.D. thesis on \"The Origins and Early Development of Lebesgue's Theory of Integration\" at the University of Wisconsin-Madison in 1968 under Robert Creighton Buck. Since 1972 he has been based at Boston University. Hawkins was an invited speaker at the International Congress of Mathematicians in 1974 at Vancouver and in 1986 at Berkeley.\n\nIn 1997 Hawkins was awarded the Chauvenet Prize for his article \"The birth of Lie's theory of groups\", published in the Mathematical Intelligencer in 1994. In fall 2012 Hawkins was elected a Fellow of the American Mathematical Society.\n\n"}
{"id": "321930", "url": "https://en.wikipedia.org/wiki?curid=321930", "title": "Weird number", "text": "Weird number\n\nIn number theory, a weird number is a natural number that is abundant but not semiperfect.\nIn other words, the sum of the proper divisors (divisors including 1 but not itself) of the number is greater than the number, but no subset of those divisors sums to the number itself.\n\nThe smallest weird number is 70. Its proper divisors are 1, 2, 5, 7, 10, 14, and 35; these sum to 74, but no subset of these sums to 70. The number 12, for example, is abundant but \"not\" weird, because the proper divisors of 12 are 1, 2, 3, 4, and 6, which sum to 16; but 2+4+6 = 12.\n\nThe first few weird numbers are \n\nInfinitely many weird numbers exist. For example, 70\"p\" is weird for all primes \"p\" ≥ 149. In fact, the set of weird numbers has positive asymptotic density.\n\nIt is not known if any odd weird numbers exist. If so, they must be greater than 10.\n\nSidney Kravitz has shown that for \"k\" a positive integer, \"Q\" a prime exceeding 2, and\nalso prime and greater than 2, then\nis a weird number.\nWith this formula, he found a large weird number\n\nA property of weird numbers is that if \"n\" is weird, and \"p\" is a prime greater than the sum of divisors σ(\"n\"), then \"pn\" is also weird. This leads to the definition of \"primitive weird numbers\", i.e. weird numbers that are not multiple of other weird numbers . There are only 24 primitive weird numbers smaller than a million, compared to 1765 weird numbers up to that limit. The construction of Kravitz yields primitive weird numbers, since all weird numbers of the form formula_4 are primitive, but the existence of infinitely many \"k\" and \"Q\" which yield a prime \"R\" is not guaranteed. It is conjectured that there exist infinitely many primitive numbers, and Melfi has shown that the infiniteness of primitive weird numbers is a consequence of Cramér's conjecture.\n\n"}
{"id": "9659484", "url": "https://en.wikipedia.org/wiki?curid=9659484", "title": "Young measure", "text": "Young measure\n\nIn mathematical analysis, a Young measure is a parameterized measure that is associated with certain subsequences of a given bounded sequence of measurable functions. Young measures have applications in the calculus of variations and the study of nonlinear partial differential equations, as well as in various optimization (or optimal control problems). They are named after Laurence Chisholm Young who invented them, however, in terms of linear functionals already in 1937 still before the measure theory has been developed.\n\nWe let formula_1 be a bounded sequence in formula_2, where formula_3 denotes an open bounded subset of formula_4. Then there exists a subsequence formula_5 and for almost every formula_6 a Borel probability measure formula_7 on formula_8 such that for each formula_9 we have formula_10 in formula_11. The measures formula_7 are called the Young measures generated by the sequence formula_1.\n\nFor every minimizing sequence formula_14 of formula_15 subject to formula_16 , the sequence of derivatives formula_17 generates the Young measures formula_18. This captures the essential features of all minimizing sequences to this problem, namely developing finer and finer slopes of formula_19 (or close to\nformula_19).\n\n"}
