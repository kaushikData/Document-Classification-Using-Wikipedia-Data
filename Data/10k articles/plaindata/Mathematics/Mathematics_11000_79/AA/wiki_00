{"id": "4682796", "url": "https://en.wikipedia.org/wiki?curid=4682796", "title": "Andrew Forsyth", "text": "Andrew Forsyth\n\nProf Andrew Russell Forsyth, FRS, FRSE (18 June 1858, Glasgow – 2 June 1942, South Kensington) was a British mathematician.\n\nForsyth was born in Glasgow on 18 June 1858, the son of John Forsyth, a marine engineer, and his wife Christina Glen.\n\nForsyth studied at Liverpool College and was tutored by Richard Pendlebury before entering Trinity College, Cambridge, graduating senior wrangler in 1881. He was elected a fellow of Trinity and then appointed to the chair of mathematics at the University of Liverpool at the age of 24. He returned to Cambridge as a lecturer in 1884 and became Sadleirian Professor of Pure Mathematics in 1895.\n\nForsyth was forced to resign his chair in 1910 as a result of a scandal caused by his affair with Marion Amelia Boys, \"née\" Pollock, the wife of physicist C. V. Boys. Boys was granted a divorce on the grounds of Marion's adultery with Forsyth. Marion and Andrew Forsyth were later married.\n\nForsyth became professor at the Imperial College of Science in 1913 and retired in 1923, remaining mathematically active into his seventies. He was elected a Fellow of the Royal Society in 1886 and won its Royal Medal in 1897. He was a Plenary Speaker of the ICM in 1908 at Rome.\n\nHe is now remembered much more as an author of treatises than as an original researcher. His books have, however, often been criticized (for example by J. E. Littlewood, in his \"A Mathematician's Miscellany\"). E. T. Whittaker was his only official student.\n\nHe died in London on 2 June 1942 and was cremated at Golders Green Crematorium.\n\nForsyth received the degree of \"Doctor mathematicae (honoris causa)\" from the Royal Frederick University on 6 September 1902, when they celebrated the centennial of the birth of mathematician Niels Henrik Abel.\n\nForsyth married Marion Amelia Pollock in 1910.\n\n"}
{"id": "4665", "url": "https://en.wikipedia.org/wiki?curid=4665", "title": "Banach algebra", "text": "Banach algebra\n\nIn mathematics, especially functional analysis, a Banach algebra, named after Stefan Banach, is an associative algebra \"A\" over the real or complex numbers (or over a non-Archimedean complete normed field) that at the same time is also a Banach space, i.e. a normed space and complete in the metric induced by the norm. The norm is required to satisfy\n\nThis ensures that the multiplication operation is continuous.\n\nA Banach algebra is called \"unital\" if it has an identity element for the multiplication whose norm is 1, and \"commutative\" if its multiplication is commutative.\nAny Banach algebra formula_2 (whether it has an identity element or not) can be embedded isometrically into a unital Banach algebra formula_3 so as to form a closed ideal of formula_3. Often one assumes \"a priori\" that the algebra under consideration is unital: for one can develop much of the theory by considering formula_3 and then applying the outcome in the original algebra. However, this is not the case all the time. For example, one cannot define all the trigonometric functions in a Banach algebra without identity.\n\nThe theory of real Banach algebras can be very different from the theory of complex Banach algebras. For example, the spectrum of an element of a nontrivial complex Banach algebra can never be empty, whereas in a real Banach algebra it could be empty for some elements.\n\nBanach algebras can also be defined over fields of p-adic numbers. This is part of p-adic analysis.\n\nThe prototypical example of a Banach algebra is formula_6, the space of (complex-valued) continuous functions on a locally compact (Hausdorff) space that vanish at infinity. formula_6 is unital if and only if \"X\" is compact. The complex conjugation being an involution, formula_6 is in fact a C*-algebra. More generally, every C*-algebra is a Banach algebra.\n\n\nThe algebra of the quaternions formula_9 is not a complex Banach algebra (for any norm on formula_9), for if formula_11 is a complex Banach algebra that is also a division algebra, then formula_12 (Gelfand–Mazur theorem), since if formula_13 is a point in the non-empty spectrum formula_14 of formula_15, formula_16 is not invertible, hence formula_17 since formula_11 is a division algebra, whence formula_19 (which also proves the Gelfand–Mazur theorem).\n\nSeveral elementary functions which are defined via power series may be defined in any unital Banach algebra; examples include the exponential function and the trigonometric functions, and more generally any entire function. (In particular, the exponential map can be used to define abstract index groups.) The formula for the geometric series remains valid in general unital Banach algebras. The binomial theorem also holds for two commuting elements of a Banach algebra.\n\nThe set of invertible elements in any unital Banach algebra is an open set, and the inversion operation on this set is continuous, (and hence is a homeomorphism) so that it forms a topological group under multiplication.\n\nIf a Banach algebra has unit 1, then 1 cannot be a commutator; i.e., formula_20  for any \"x\", \"y\" ∈ \"A\". This is because \"xy\" and \"yx\" have the same spectrum except possibly \"0\".\n\nThe various algebras of functions given in the examples above have very different properties from standard examples of algebras such as the reals. For example:\n\n\nUnital Banach algebras over the complex field provide a general setting to develop spectral theory. The \"spectrum\" of an element \"x\" ∈ \"A\", denoted by formula_21, consists of all those complex scalars \"λ\" such that \"x\" − \"λ\"1 is not invertible in \"A\". The spectrum of any element \"x\" is a closed subset of the closed disc in C with radius ||\"x\"|| and center 0, and thus is compact. Moreover, the spectrum formula_21 of an element \"x\" is non-empty and satisfies the spectral radius formula:\n\nGiven \"x\" ∈ \"A\", the holomorphic functional calculus allows to define \"ƒ\"(\"x\") ∈ \"A\" for any function \"ƒ\" holomorphic in a neighborhood of formula_24 Furthermore, the spectral mapping theorem holds:\n\nWhen the Banach algebra \"A\" is the algebra L(\"X\") of bounded linear operators on a complex Banach space \"X\"  (e.g., the algebra of square matrices), the notion of the spectrum in \"A\" coincides with the usual one in the operator theory. For \"ƒ\" ∈ \"C\"(\"X\") (with a compact Hausdorff space \"X\"), one sees that:\n\nThe norm of a normal element \"x\" of a C*-algebra coincides with its spectral radius. This generalizes an analogous fact for normal operators.\n\nLet \"A\"  be a complex unital Banach algebra in which every non-zero element \"x\" is invertible (a division algebra). For every \"a\" ∈ \"A\", there is \"λ\" ∈ C such that\n\"a\" − \"λ1 is not invertible (because the spectrum of \"a\" is not empty) hence \"a\" = \"λ1 : this algebra \"A\" is naturally isomorphic to C (the complex case of the Gelfand–Mazur theorem).\n\nLet \"A\"  be a unital \"commutative\" Banach algebra over C. Since \"A\" is then a commutative ring with unit, every non-invertible element of \"A\" belongs to some maximal ideal of \"A\". Since a maximal ideal formula_27 in \"A\" is closed, formula_28 is a Banach algebra that is a field, and it follows from the Gelfand–Mazur theorem that there is a bijection between the set of all maximal ideals of \"A\" and the set Δ(\"A\") of all nonzero homomorphisms from \"A\"  to C. The set Δ(\"A\") is called the \"structure space\" or \"character space\" of \"A\", and its members \"characters.\"\n\nA character χ is a linear functional on \"A\" which is at the same time multiplicative, χ(\"ab\") = χ(\"a\") χ(\"b\"), and satisfies \"χ\"(1) = 1. Every character is automatically continuous from \"A\"  to C, since the kernel of a character is a maximal ideal, which is closed. Moreover, the norm (\"i.e.\", operator norm) of a character is one. Equipped with the topology of pointwise convergence on \"A\" (\"i.e.\", the topology induced by the weak-* topology of \"A\"), the character space, Δ(\"A\"), is a Hausdorff compact space.\n\nFor any \"x\" ∈ \"A\",\n\nwhere formula_30 is the Gelfand representation of \"x\" defined as follows: formula_30 is the continuous function from Δ(\"A\") to C given by formula_32  The spectrum of formula_33 in the formula above, is the spectrum as element of the algebra \"C\"(Δ(\"A\")) of complex continuous functions on the compact space Δ(\"A\"). Explicitly,\n\nAs an algebra, a unital commutative Banach algebra is semisimple (i.e., its Jacobson radical is zero) if and only if its Gelfand representation has trivial kernel. An important example of such an algebra is a commutative C*-algebra. In fact, when \"A\" is a commutative unital C*-algebra, the Gelfand representation is then an isometric *-isomorphism between \"A\" and \"C\"(Δ(\"A\")) .\n\n\n"}
{"id": "24011805", "url": "https://en.wikipedia.org/wiki?curid=24011805", "title": "Baranyai's theorem", "text": "Baranyai's theorem\n\nIn combinatorial mathematics, Baranyai's theorem (proved by and named after Zsolt Baranyai) deals with the decompositions of complete hypergraphs.\n\nThe statement of the result is that if formula_1 are natural numbers and \"r\" divides \"k\", then the complete hypergraph formula_2 decomposes into 1-factors. formula_2 is a hypergraph with \"k\" vertices, in which every subset of \"r\" vertices forms a hyperedge; a 1-factor of this hypergraph is a set of hyperedges that touches each vertex exactly once, or equivalently a partition of the vertices into subsets of size \"r\". Thus, the theorem states that the \"k\" vertices of the hypergraph may be partitioned into subsets of \"r\" vertices in formula_4 different ways, in such a way that each \"r\"-element subset appears in exactly one of the partitions.\nIn the special case formula_5, we have a complete graph formula_7 on formula_8 vertices, and we wish to color the edges with formula_9 colors so that the edges of each color form a perfect matching. Baranyai's theorem says that we can do this whenever formula_8 is even.\n\nThe \"r\" = 2 case can be rephrased as stating that every complete graph with an even number of vertices has an edge coloring whose number of colors equals its degree, or equivalently that its edges may be partitioned into perfect matchings. It may be used to schedule round-robin tournaments, and its solution was already known in the 19th century. The case that \"k\" = 2\"r\" is also easy.\n\nThe \"r\" = 3 case was established by R. Peltesohn in 1936. The general case was proved by Zsolt Baranyai in 1975.\n\n \n"}
{"id": "3804402", "url": "https://en.wikipedia.org/wiki?curid=3804402", "title": "Bigraph", "text": "Bigraph\n\nA bigraph (often used in the plural bigraphs) can be modelled as the superposition of a graph (the \"link graph\") and a set of trees (the \"place graph\").\n\nEach node of the bigraph is part of a graph and also part of some tree that describes how the nodes are nested. Bigraphs can be conveniently and formally displayed as diagrams. They have applications in the modelling of distributed systems for ubiquitous computing and can be used to describe mobile interactions. They have also been used by Robin Milner in an attempt to subsume Calculus of Communicating Systems (CCS) and π-calculus. They have been studied in the context of category theory.\n\nAside from nodes and (hyper-)edges, a bigraph may have associated with it one or more \"regions\" which are roots in the place forest, and zero or more \"holes\" in the place graph, into which other bigraph regions may be inserted. Similarly, to nodes we may assign \"controls\" that define identities and an arity (the number of \"ports\" for a given node to which link-graph edges may connect). These controls are drawn from a bigraph \"signature\". In the link graph we define \"inner\" and \"outer\" names, which define the connection points at which coincident names may be fused to form a single link.\n\nA bigraph is a 5-tuple:\n\nformula_1\n\nwhere formula_2 is a set of nodes, formula_3 is a set of edges, formula_4 is the \"control map\" that assigns controls to nodes, formula_5 is the \"parent map\" that defines the nesting of nodes, and formula_6 is the \"link map\" that defines the link structure.\n\nThe notation formula_7 indicates that the bigraph has formula_8 \"holes\" (sites) and a set of inner names formula_9 and formula_10 \"regions\", with a set of \"outer names\" formula_11. These are respectively known as the \"inner\" and \"outer\" interfaces of the bigraph.\n\nFormally speaking, each bigraph is an arrow in a symmetric partial monoidal category (usually abbreviated \"spm-category\") in which the objects are these interfaces. As a result, the composition of bigraphs is definable in terms of the composition of arrows in the category.\n\nBigraphs with sharing are a generalisation of Milner's formalisation that allows for a straightforward representation of overlapping or intersecting spatial locations. In bigraphs with sharing, the place graph is defined as a directed acyclic graph (DAG), i.e. formula_5 is a binary relation instead of a map. The definition of link graph is unaffected by the introduction of sharing. Note that standard bigraphs are a sub-class of bigraphs with sharing.\n\nAreas of application of bigraphs with sharing include wireless networking protocols, real-time management of domestic wireless networks and mixed reality systems.\n\n\n\n\n"}
{"id": "55706943", "url": "https://en.wikipedia.org/wiki?curid=55706943", "title": "Catherine Yan", "text": "Catherine Yan\n\nCatherine Huafei Yan () is a professor of mathematics at Texas A&M University interested in algebraic combinatorics.\n\nYan earned a bachelor's degree from Peking University in 1993.\nShe was a student of Gian-Carlo Rota at the Massachusetts Institute of Technology, where she earned her Ph.D. in 1997 with a dissertation on \"The Theory of Commuting Boolean Algebras\".\nAfter working for two years as a Courant Instructor at New York University, she joined Texas A&M in 1999, with a three-year hiatus as Chern Professor at the Center of Combinatorics, Nankai University, from 2005 to 2008.\n\nWith her advisor and Joseph Kung, she is an author of \"Combinatorics: The Rota Way\" (Cambridge University Press, 2009). The book provides an exposition of the areas of combinatorics of interest to Rota, unified through an algebraic framework, and lists many open research problems in this area.\n\nShe was elected to the 2018 class of fellows of the American Mathematical Society \"for contributions to combinatorics and discrete geometry\".\n\n"}
{"id": "52199962", "url": "https://en.wikipedia.org/wiki?curid=52199962", "title": "Cathy Kessel", "text": "Cathy Kessel\n\nCathy Kessel is a U.S. researcher in mathematics education and consultant, past-president of Association for Women in Mathematics, winner of the Association for Women in Mathematics Louise Hay Award, and a blogger on Mathematics and Education. She is currently (2016) an editor for Illustrative Mathematics.\n\nKessel received her Ph.D. in mathematics from the University of Colorado Boulder, specializing in mathematical logic, and taught for three years after earning her Ph.D. She taught for a total of 13 years as a graduate and postgraduate until the 1990s when she made the switch to research in education. She began auditing courses and working on research projects at the School of Education at the University of California at Berkeley. This led to a career that included editing reports, books, articles, and curriculum and standards documents. She was the president of the Association for Women in Mathematics from 2007 to 2009 and worked as a mathematics education consultant until 2015.\n\nKessel has participated in multiple projects pertaining to mathematics education, including the following.\n\n\n\n\nIn 2017, she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n"}
{"id": "1179451", "url": "https://en.wikipedia.org/wiki?curid=1179451", "title": "Change of basis", "text": "Change of basis\n\nIn linear algebra, a basis for a vector space of dimension \"n\" is a set of \"n\" vectors , called basis vectors, with the property that every vector in the space can be expressed as a unique linear combination of the basis vectors. The matrix representations of operators are also determined by the chosen basis. Since it is often desirable to work with more than one basis for a vector space, it is of fundamental importance in linear algebra to be able to easily transform coordinate-wise representations of vectors and operators taken with respect to one basis to their equivalent representations with respect to another basis. Such a transformation is called a change of basis.\n\nAlthough the terminology of vector spaces is used below and the symbol R can be taken to mean the field of real numbers, the results discussed hold whenever R is a commutative ring and \"vector space\" is everywhere replaced with \"free R-module\".\n\nThe standard basis for formula_1 is the ordered sequence formula_2, where formula_3 is the element of formula_1 with formula_5 in the formula_6 place and formula_7s elsewhere. For example, the standard basis for formula_8 would be\n\nIf formula_10 is a linear transformation, the formula_11 matrix associated with formula_12 is the matrix formula_13 whose formula_14 column is formula_15, for formula_16, that is\n\nIn this case we have formula_18, formula_19, where we regard formula_20 as a column vector and the multiplication on the right side is matrix multiplication. It is a basic fact in linear algebra that the vector space Hom(formula_21) of all linear transformations from formula_1 to formula_23 is naturally isomorphic to the space formula_24of formula_11 matrices over formula_26; that is, a linear transformation formula_10 is for all intents and purposes equivalent to its matrix formula_13.\n\nWe will also make use of the following simple observation.\n\nLet formula_29 and formula_30 be vector spaces, let formula_31 be a basis for formula_29, and let formula_33 be any formula_34 vectors in formula_30. Then there exists a \"unique linear transformation\" formula_36 with \nformula_37, for formula_16.\n\nThis unique formula_12 is defined by\n\nOf course, if formula_33 happens to be a basis for formula_30, then formula_12 is bijective as well as linear; in other words, formula_12 is an isomorphism. If in this case we also have formula_45, then formula_12 is said to be an automorphism.\n\nNow let formula_29 be a vector space over formula_26 and suppose formula_31 is a basis for formula_29. By definition, if formula_51 is a vector in formula_29, then formula_53 for a unique choice of scalars formula_54 called the \"coordinates of formula_51 relative to the ordered basis formula_56\". The vector formula_57 is called the \"coordinate tuple of formula_51 relative to formula_56\".\n\nThe unique linear map formula_60 with formula_61 for formula_62 is called the coordinate isomorphism for formula_29 and the basis formula_31. Thus formula_65 if and only if formula_53.\n\nA set of vectors can be represented by a matrix of which each column consists of the components of the corresponding vector of the set. As a basis is a set of vectors, a basis can be given by a matrix of this kind. Later it will be shown that the change of basis of any object of the space is related to this matrix. For example, vectors change with its inverse (and they are therefore called contravariant objects).\n\nFirst we examine the question of how the coordinates of a vector formula_51 in the vector space formula_29 change when we select another basis.\n\nThis means that given a matrix formula_69 whose columns are the vectors of the new basis of the space (new basis matrix), the new coordinates for a column vector formula_70 are given by the matrix product formula_71. For this reason, it is said that ordinary vectors are contravariant objects.\n\nAny finite set of vectors can be represented by a matrix in which its columns are the coordinates of the given vectors. As an example in dimension 2, a pair of vectors obtained by rotating the standard basis counterclockwise for 45°. The matrix whose columns are the coordinates of these vectors is\n\nIf we want to change any vector of the space to this new basis, we only need to left-multiply its components by the inverse of this matrix.\n\nFor example, let R be a new basis given by its Euler angles. The matrix of the basis will have as columns the components of each vector. Therefore, this matrix will be (See Euler angles article):\n\nAgain, any vector of the space can be changed to this new basis by left-multiplying its components by the inverse of this matrix.\n\nSuppose formula_74 and formula_75 are two ordered bases for an \"n\"-dimensional vector space \"V\" over a field \"K\". Let \"φ\" and \"φ\" be the corresponding coordinate isomorphisms (linear maps) from \"K\" to \"V\", i.e. formula_76 and formula_77 for , where \"e\" denotes the \"n\"-tuple with \"i\" entry equal to 1, and all other entries equal to 0.\n\nIf formula_78 is the coordinate \"n\"-tuple of a vector \"v\" in \"V\" with respect to the basis \"A\", so that formula_79, then the coordinate tuple of \"v\" with respect to \"B\" is the tuple \"y\" such that formula_80, i.e. formula_81, so that for any vector in \"V\", the map formula_82 maps its coordinate tuple with respect to \"A\" to its coordinate tuple with respect to \"B\". Since this map is an automorphism on \"K\", it therefore has an associated square matrix \"C\". Moreover, the \"i\" column of \"C\" is formula_83, that is, the coordinate tuple of \"α\" with respect to \"B\".\n\nThus, for any vector \"v\" in \"V\", if \"x\" is the coordinate tuple of \"v\" with respect to \"A\", then the tuple formula_84 is the coordinate tuple of \"v\" with respect to \"B\". The matrix \"C\" is called the transition matrix from \"A\" to \"B\".\n\nNow suppose is a linear transformation, is a basis for \"V\" and is a basis for \"W\". Let φ and ψ be the coordinate isomorphisms for \"V\" and \"W\", respectively, relative to the given bases. Then the map is a linear transformation from R to R, and therefore has a matrix t; its \"j\"th column is for . This matrix is called the matrix of \"T\" with respect to the ordered bases and If and y and x are the coordinate tuples of η and ξ, then . Conversely, if ξ is in \"V\" and is the coordinate tuple of ξ with respect to and we set and , then . That is, if ξ is in \"V\" and η is in \"W\" and x and y are their coordinate tuples, then if and only if .\n\nTheorem Suppose \"U\", \"V\" and \"W\" are vector spaces of finite dimension and an ordered basis is chosen for each. If and are linear transformations with matrices s and t, then the matrix of the linear transformation (with respect to the given bases) is st.\n\nNow we ask what happens to the matrix of when we change bases in \"V\" and \"W\". Let and be ordered bases for \"V\" and \"W\" respectively, and suppose we are given a second pair of bases and Let φ and φ be the coordinate isomorphisms taking the usual basis in R to the first and second bases for \"V\", and let ψ and ψ be the isomorphisms taking the usual basis in R to the first and second bases for \"W\".\n\nLet , and (both maps taking R to R), and let t and t be their respective matrices. Let p and q be the matrices of the change-of-coordinates automorphisms on R and on R.\n\nThe relationships of these various maps to one another are illustrated in the following commutative diagram.\n\nSince we have , and since composition of linear maps corresponds to matrix multiplication, it follows that\n\nGiven that the change of basis has once the basis matrix and once its inverse, these objects are said to be 1-co, 1-contra-variant.\n\nAn important case of the matrix of a linear transformation is that of an endomorphism, that is,\na linear map from a vector space \"V\" to itself: that is, the case that .\nWe can naturally take and The matrix of the linear map \"T\" is necessarily square.\n\nWe apply the same change of basis, so that and the change of basis formula becomes\n\nIn this situation the invertible matrix p is called a change-of-basis matrix for the vector space \"V\", and the equation above says that the matrices t and t are similar.\n\nA \"bilinear form\" on a vector space \"V\" over a field R is a mapping which is linear in both arguments. That is, is bilinear if the maps\nare linear for each \"w\" in \"V\". This definition applies equally well to modules over a commutative ring with linear maps being module homomorphisms.\n\nThe Gram matrix \"G\" attached to a basis formula_87 is defined by\n\nIf formula_89 and formula_90 are the expressions of vectors \"v\", \"w\" with respect to this basis, then the bilinear form is given by\n\nThe matrix will be symmetric if the bilinear form \"B\" is a symmetric bilinear form.\n\nIf \"P\" is the invertible matrix representing a change of basis from\nformula_87 to formula_93\nthen the Gram matrix transforms by the matrix congruence\n\nIn abstract vector space theory the change of basis concept is innocuous; it seems to add little to science. Yet there are cases in associative algebras where a change of basis is sufficient to turn a caterpillar into a butterfly, figuratively speaking:\n\n\n"}
{"id": "40030306", "url": "https://en.wikipedia.org/wiki?curid=40030306", "title": "Characteristic variety", "text": "Characteristic variety\n\nIn mathematical analysis, the characteristic variety of a microdifferential operator \"P\" is an algebraic variety that is the zero set of the principal symbol of \"P\" in the cotangent bundle. It is invariant under a quantized contact transformation.\n\nThe notion is also defined more generally in commutative algebra. A basic theorem says a characteristic variety is involutive.\n\n"}
{"id": "411512", "url": "https://en.wikipedia.org/wiki?curid=411512", "title": "Cymatics", "text": "Cymatics\n\nCymatics, from , meaning \"wave\", is a subset of modal vibrational phenomena. The term was coined by Hans Jenny (1904-1972), a Swiss follower of the philosophical school known as anthroposophy. Typically the surface of a plate, diaphragm or membrane is vibrated, and regions of maximum and minimum displacement are made visible in a thin coating of particles, paste or liquid. Different patterns emerge in the excitatory medium depending on the geometry of the plate and the driving frequency.\n\nThe apparatus employed can be simple, such as the Chinese spouting bowl, in which copper handles are rubbed and cause the copper bottom elements to vibrate. Other examples include the Chladni Plate and the so-called cymascope.\n\nOn July 8, 1680, Robert Hooke was able to see the nodal patterns associated with the modes of vibration of glass plates. Hooke ran a bow along the edge of a glass plate covered with flour, and saw the nodal patterns emerge.\n\nThe German musician and physicist Ernst Chladni noticed in the eighteenth century that the modes of vibration of a membrane or a plate can be observed by sprinkling the vibrating surface with a fine dust (e.g., lycopodium powder, flour or fine sand). The powder moves due to the vibration and accumulates progressively in points of the surface corresponding to the sound vibration. The points form a pattern of lines, known as \"nodal lines of the vibration mode\". The normal modes of vibration, and the pattern of nodal lines associated with each of these, are completely determined, for a surface with homogeneous mechanical characteristics, from the geometric shape of the surface and by the way in which the surface is constrained.\n\nExperiments of this kind, similar to those carried out earlier by Galileo Galilei around 1630 and by Robert Hooke in 1680, were later perfected by Chladni, who introduced them systematically in 1787 in his book \"Entdeckungen über die Theorie des Klanges\" (Discoveries on the theory of sound). This provided an important contribution to the understanding of acoustic phenomena and the functioning of musical instruments. The figures thus obtained (with the aid of a violin bow that rubbed perpendicularly along the edge of smooth plates covered with fine sand) are still designated by the name of \"Chladni figures\".\n\nIn 1967 Hans Jenny, a follower of the anthroposophical doctrine of Rudolf Steiner, published two volumes entitled \"Kymatic\" (1967 and 1972), in which, repeating Chladni's experiments, he claimed the existence of a subtle power based on the normal, symmetrical images made by sound waves. Jenny put sand, dust and fluids on a metal plate connected to an oscillator which could produce a broad spectrum of frequencies. The sand or other substances were organized into different structures characterized by geometric shapes typical of the frequency of the vibration emitted by the oscillator. Modern analysts, including Michael Shermer, have termed anthroposophy's application in areas such as engineering, medicine, biology, and biodynamic agriculture as pseudoscience.\n\nAccording to Jenny, these structures, reminiscent of the mandala and other forms recurring in nature, would be a manifestation of an invisible force field of the vibrational energy that generated it. He was particularly impressed by an observation that imposing a vocalization in ancient Sanskrit of Om (regarded by Hindus and Buddhists as the sound of creation) the lycopodium powder formed a circle with a centre point, one of the ways in which Om had been represented. In fact, for a plate of circular shape, resting in the centre (or the border, or at least in a set of points with central symmetry), the nodal vibration modes all have central symmetry, so the observation of Jenny is entirely consistent with well known mathematical properties.\n\nFrom the physical-mathematical standpoint, the form of the nodal patterns is predetermined by the shape of the body set in vibration or, in the case of acoustic waves in a gas, the shape of the cavity in which the gas is contained. The sound wave, therefore, does not influence at all the shape of the vibrating body or the shape of the nodal patterns. The only thing that changes due to the vibration is the arrangement of the sand. The image formed by the sand, in turn, is influenced by the frequency spectrum of the vibration only because each vibration mode is characterized by a specific frequency. Therefore, the spectrum of the signal that excites the vibration determines which patterns are actually nodally displayed.\n\nThe physical phenomena involved in the formation of Chladni figures are best explained by classical physics.\n\nIt has been speculated by some researchers that application of ultrasound cause wounds to heal faster. However, other than select articles on the subject of low-amplitude high-frequency sound in bone fracture healing, there is no medical evidence of this phenomenon.\n\nDevices for displaying nodal images have influenced visual arts and contemporary music. Artist Björk created projections of cymatics patterns by using bass frequencies on tour for her album \"Biophilia\".\n\nHans Jenny's book on Chladni figures influenced Alvin Lucier and helped lead to Lucier's composition \"Queen of the South\". Jenny's work was also followed up by Center for Advanced Visual Studies (CAVS) founder György Kepes at MIT. His work in this area included an acoustically vibrated piece of sheet metal in which small holes had been drilled in a grid. Small flames of gas burned through these holes and thermodynamic patterns were made visible by this setup.\n\nIn the mid 1980s, visual artist Ron Rocco, who also developed his work at CAVS, employed mirrors mounted to tiny servo motors, driven by the audio signal of a synthesizer and amplified by a tube amp to reflect the beam of a laser. This created light patterns which corresponded to the audio's frequency and amplitude. Using this beam to generate video feedback and computers to process the feedback signal, Rocco created his \"Andro-media\" series of installations. Rocco later formed a collaboration with musician David Hykes, who practiced a form of Mongolian overtone chanting with The Harmonic Choir, to generate cymatic images from a pool of liquid mercury, which functioned as a liquid mirror to modulate the beam of a Helium-Neon laser from the sound thus generated. Photographs of this work can be found in the Ars Electronica catalog of 1987.\n\nContemporary German photographer and philosopher Alexander Lauterwasser has brought cymatics into the 21st century using finely crafted crystal oscillators to resonate steel plates covered with fine sand and to vibrate small samples of water in Petri dishes. His first book, \"Water Sound Images\", translated into English in 2006, features imagery of light reflecting off the surface of water set into motion by sound sources ranging from pure sine waves to music by Beethoven, Karlheinz Stockhausen, electroacoustic group \"Kymatik\" (who often record in ambisonic surround sound) and overtone singing. The resulting photographs of standing wave patterns are striking. Lauterwasser's book focused on creating detailed visual analogues of natural patterns ranging from the distribution of spots on a leopard to the geometric patterns found in plants and flowers, to the shapes of jellyfish and the intricate patterns found on the shell of a tortoise.\n\nComposer Stuart Mitchell and his father T.J. Mitchell claimed that Rosslyn Chapel's carvings supposedly contain references to cymatics patterns. In 2005 they created a work called \"The Rosslyn Motet\" realised by attempting to match various Chladni patterns to 13 geometric symbols carved onto the faces of cubes emanating from 14 arches.\n\nLike many claims in the cymatics community, the hypothesis that the carvings represent Chladni patterns is not supported by scientific or historical evidence. One of the problems is that many of the 'box' carvings are not original, having been replaced in the 19th century following damage by erosion.\n\nThe musical group, \"The Glitch Mob\" used cymatics to produce the music video \"Becoming Harmonious (ft. Metal Mother)\".\n\nInfluenced by Yantra diagrams and cymatics, artist and fashion designer Mandali Mendrilla created a sculpture dress called \"Kamadhenu (Wish Tree Dress III)\" the pattern of which is based \non a Yantra diagram depicting goddess Kamadhenu.\n\nAphex Twin suggests learning more about cymatics (linking to this article) in reference to 'master tuning of 440 Hz' in a conversation with synth-maker Tatsuya Takahashi.\n\nSince 2010, the art collective Analema Group creates participatory performances in which cymatic patterns are produced digitally in real-time by the audience.\n\nInspired by periodic and symmetrical patterns at the air-liquid interface created by sound vibration, P. Chen and coworkers developed a method to engineer diverse structures from microscale materials using liquid-based templates. This liquid-based template can be dynamically reconfigured by tuning vibration frequency and acceleration.\n\n"}
{"id": "7938", "url": "https://en.wikipedia.org/wiki?curid=7938", "title": "Diatomic molecule", "text": "Diatomic molecule\n\nDiatomic molecules are molecules composed of only two atoms, of the same or different chemical elements. The prefix \"di-\" is of Greek origin, meaning \"two\". If a diatomic molecule consists of two atoms of the same element, such as hydrogen (H) or oxygen (O), then it is said to be homonuclear. Otherwise, if a diatomic molecule consists of two different atoms, such as carbon monoxide (CO) or nitric oxide (NO), the molecule is said to be heteronuclear.\n\nThe only chemical elements that form stable homonuclear diatomic molecules at standard temperature and pressure (STP) (or typical laboratory conditions of 1 bar and 25 °C) are the gases hydrogen (H), nitrogen (N), oxygen (O), fluorine (F), and chlorine (Cl).\n\nThe noble gases (helium, neon, argon, krypton, xenon, and radon) are also gases at STP, but they are monatomic. The homonuclear diatomic gases and noble gases together are called \"elemental gases\" or \"molecular gases\", to distinguish them from other gases that are chemical compounds.\n\nAt slightly elevated temperatures, the halogens bromine (Br) and iodine (I) also form diatomic gases. All halogens have been observed as diatomic molecules, except for astatine, which is uncertain.\n\nThe mnemonics BrINClHOF, pronounced \"Brinklehof\", and HONClBrIF, pronounced \"Honkelbrif\", and HOFBrINCl (pronouced as Hofbrinkle) have been coined to aid recall of the list of diatomic elements.\n\nOther elements form diatomic molecules when evaporated, but these diatomic species repolymerize when cooled. Heating (\"cracking\") elemental phosphorus gives diphosphorus, P. Sulfur vapor is mostly disulfur (S). Dilithium (Li) is known in the gas phase. Ditungsten (W) and dimolybdenum (Mo) form with sextuple bonds in the gas phase. The bond in a homonuclear diatomic molecule is non-polar. Dirubidium (Rb) is diatomic.\n\nAll other diatomic molecules are chemical compounds of two different elements. Many elements can combine to form heteronuclear diatomic molecules, depending on temperature and pressure.\n\nSome examples include, gases carbon monoxide (CO), nitric oxide (NO), and hydrogen chloride (HCl).\n\nMany 1:1 binary compounds are not normally considered diatomic because they are polymeric at room temperature, but they form diatomic molecules when evaporated, for example gaseous MgO, SiO, and many others.\n\nHundreds of diatomic molecules have been identified in the environment of the Earth, in the laboratory, and in interstellar space. About 99% of the Earth's atmosphere is composed of two species of diatomic molecules: nitrogen (78%) and oxygen (21%). The natural abundance of hydrogen (H) in the Earth's atmosphere is only of the order of parts per million, but H is the most abundant diatomic molecule in the universe. The interstellar medium is, indeed, dominated by hydrogen atoms.\n\nDiatomic elements played an important role in the elucidation of the concepts of element, atom, and molecule in the 19th century, because some of the most common elements, such as hydrogen, oxygen, and nitrogen, occur as diatomic molecules. John Dalton's original atomic hypothesis assumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed water's formula to be HO, giving the atomic weight of oxygen as eight times that of hydrogen, instead of the modern value of about 16. As a consequence, confusion existed regarding atomic weights and molecular formulas for about half a century.\n\nAs early as 1805, Gay-Lussac and von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen, and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the assumption of diatomic elemental molecules. However, these results were mostly ignored until 1860, partly due to the belief that atoms of one element would have no chemical affinity toward atoms of the same element, and also partly due to apparent exceptions to Avogadro's law that were not explained until later in terms of dissociating molecules.\n\nAt the 1860 Karlsruhe Congress on atomic weights, Cannizzaro resurrected Avogadro's ideas and used them to produce a consistent table of atomic weights, which mostly agree with modern values. These weights were an important prerequisite for the discovery of the periodic law by Dmitri Mendeleev and Lothar Meyer.\n\nDiatomic molecules are normally in their lowest or ground state, which conventionally is also known as the formula_1 state. When a gas of diatomic molecules is bombarded by energetic electrons, some of the molecules may be excited to higher electronic states, as occurs, for example, in the natural aurora; high-altitude nuclear explosions; and rocket-borne electron gun experiments. Such excitation can also occur when the gas absorbs light or other electromagnetic radiation. The excited states are unstable and naturally relax back to the ground state. Over various short time scales after the excitation (typically a fraction of a second, or sometimes longer than a second if the excited state is metastable), transitions occur from higher to lower electronic states and ultimately to the ground state, and in each transition results a photon is emitted. This emission is known as fluorescence. Successively higher electronic states are conventionally named formula_2, formula_3, formula_4, etc. (but this convention is not always followed, and sometimes lower case letters and alphabetically out-of-sequence letters are used, as in the example given below). The excitation energy must be greater than or equal to the energy of the electronic state in order for the excitation to occur.\n\nIn quantum theory, an electronic state of a diatomic molecule is represented by\n\nwhere formula_6 is the total electronic spin quantum number, formula_7 is the total electronic angular momentum quantum number along the internuclear axis, and formula_8 is the vibrational quantum number. formula_7 takes on values 0, 1, 2, …, which are represented by the electronic state symbols formula_10, formula_11, formula_12,….\nFor example, the following table lists the common electronic states (without vibrational quantum numbers) along with the energy of the lowest vibrational level (formula_13) of diatomic nitrogen (N), the most abundant gas in the Earth's atmosphere. In the table, the subscripts and superscripts after formula_7 give additional quantum mechanical details about the electronic state.\n\nNote: The \"energy\" units in the above table are actually the reciprocal of the wavelength of a photon emitted in a transition to the lowest energy state. The actual energy can be found by multiplying the given statistic by the product of \"c\" (the speed of light) and \"h\" (Planck's constant), i.e., about 1.99 × 10 Joule metres, and then multiplying by a further factor of 100 to convert from cm to m.\n\nThe aforementioned fluorescence occurs in distinct regions of the electromagnetic spectrum, called \"emission bands\": each band corresponds to a particular transition from a higher electronic state and vibrational level to a lower electronic state and vibrational level (typically, many vibrational levels are involved in an excited gas of diatomic molecules). For example, N formula_2-formula_1 emission bands (a.k.a. Vegard-Kaplan bands) are present in the spectral range from 0.14 to 1.45 μm (micrometres). A given band can be spread out over several nanometers in electromagnetic wavelength space, owing to the various transitions that occur in the molecule's rotational quantum number, formula_17. These are classified into distinct sub-band branches, depending on the change in formula_17. The formula_19 branch corresponds to formula_20, the formula_21 branch to formula_22, and the formula_23 branch to formula_24. Bands are spread out even further by the limited spectral resolution of the spectrometer that is used to measure the spectrum. The spectral resolution depends on the instrument's point spread function.\n\nThe molecular term symbol is a shorthand expression of the angular momenta that characterize the electronic quantum states of a diatomic molecule, which are also \n\nThe translational energy of the molecule is given by the kinetic energy expression:\n\nwhere formula_26 is the mass of the molecule and formula_8 is its velocity.\n\nClassically, the kinetic energy of rotation is\n\nFor microscopic, atomic-level systems like a molecule, angular momentum can only have specific discrete values given by\n\nAlso, for a diatomic molecule the moment of inertia is\n\nSo, substituting the angular momentum and moment of inertia into E, the rotational energy levels of a diatomic molecule are given by:\n\nAnother type of motion of a diatomic molecule is for each atom to oscillate—or vibrate—along the line connecting the two atoms. The vibrational energy is approximately that of a quantum harmonic oscillator:\n\nThe spacing, and the energy of a typical spectroscopic transition, between vibrational energy levels is about 100 times greater than that of a typical transition between rotational energy levels.\n\nThe good quantum numbers for a diatomic molecule, as well as good approximations of rotational energy levels, can be obtained by modeling the molecule using Hund's cases.\n\n\n\n"}
{"id": "9838", "url": "https://en.wikipedia.org/wiki?curid=9838", "title": "Eiffel (programming language)", "text": "Eiffel (programming language)\n\nEiffel is an object-oriented programming language designed by Bertrand Meyer (an object-orientation proponent and author of \"Object-Oriented Software Construction\") and Eiffel Software. Meyer conceived the language in 1985 with the goal of increasing the reliability of commercial software development; the first version becoming available in 1986. In 2005, Eiffel became an ISO-standardized language.\n\nThe design of the language is closely connected with the Eiffel programming method. Both are based on a set of principles, including design by contract, command–query separation, the uniform-access principle, the single-choice principle, the open–closed principle, and option–operand separation.\n\nMany concepts initially introduced by Eiffel later found their way into Java, C#, and other languages. New language design ideas, particularly through the Ecma/ISO standardization process, continue to be incorporated into the Eiffel language.\n\nThe key characteristics of the Eiffel language include:\n\n\nEiffel emphasizes declarative statements over procedural code and attempts to eliminate the need for bookkeeping instructions.\n\nEiffel shuns coding tricks or coding techniques intended as optimization hints to the compiler. The aim is not only to make the code more readable, but also to allow programmers to concentrate on the important aspects of a program without getting bogged down in implementation details. Eiffel's simplicity is intended to promote simple, extensible, reusable, and reliable answers to computing problems. Compilers for computer programs written in Eiffel provide extensive optimization techniques, such as automatic in-lining, that relieve the programmer of part of the optimization burden.\n\nEiffel was originally developed by Eiffel Software, a company founded by Bertrand Meyer. \"Object-Oriented Software Construction\" contains a detailed treatment of the concepts and theory of the object technology that led to Eiffel's design.\n\nThe design goal behind the Eiffel language, libraries, and programming methods is to enable programmers to create reliable, reusable software modules. Eiffel supports multiple inheritance, genericity, polymorphism, encapsulation, type-safe conversions, and parameter covariance. Eiffel's most important contribution to software engineering is design by contract (DbC), in which assertions, preconditions, postconditions, and class invariants are employed to help ensure program correctness without sacrificing efficiency.\n\nEiffel's design is based on object-oriented programming theory, with only minor influence of other paradigms or concern for support of legacy code. Eiffel formally supports abstract data types. Under Eiffel's design, a software text should be able to reproduce its design documentation from the text itself, using a formalized implementation of the \"Abstract Data Type\".\n\nEiffelStudio is an integrated development environment available under either an open source or a commercial license. It offers an object-oriented environment for software engineering. EiffelEnvision is a plug-in for Microsoft Visual Studio that allows users to edit, compile, and debug Eiffel projects from within the Microsoft Visual Studio IDE. Five other open source implementations are available: \"The Eiffel Compiler\" tecomp; Gobo Eiffel; SmartEiffel, the GNU implementation, based on an older version of the language; LibertyEiffel, based on the SmartEiffel compiler; and Visual Eiffel.\n\nSeveral other programming languages incorporate elements first introduced in Eiffel. Sather, for example, was originally based on Eiffel but has since diverged, and now includes several functional programming features. The interactive-teaching language Blue, forerunner of BlueJ, is also Eiffel-based. The Apple Media Tool includes an Eiffel-based Apple Media Language.\n\nThe Eiffel language definition is an international standard of the ISO. The standard was developed by ECMA International, which first approved the standard on 21 June 2005 as Standard ECMA-367, Eiffel: Analysis, Design and Programming Language. In June 2006, ECMA and ISO adopted the second version. In November 2006, ISO first published that version. The standard can be found and used free of charge on the ECMA site. The ISO version is identical in all respects except formatting.\n\nEiffel Software, \"The Eiffel Compiler\" tecomp and Eiffel-library-developer Gobo have committed to implementing the standard; Eiffel Software's EiffelStudio 6.1 and \"The Eiffel Compiler\" tecomp implement some of the major new mechanisms—in particular, inline agents, assigner commands, bracket notation, non-conforming inheritance, and attached types. The SmartEiffel team has turned away from this standard to create its own version of the language, which they believe to be closer to the original style of Eiffel. Object Tools has not disclosed whether future versions of its Eiffel compiler will comply with the standard. LibertyEiffel implements a dialect somewhere in between the SmartEiffel language and the standard.\n\nThe standard cites the following, predecessor Eiffel-language specifications:\n\n\nThe current version of the standard from June 2006 contains some inconsistencies (e.g. covariant redefinitions). The ECMA committee has not yet announced any timeline and direction on how to resolve the inconsistencies.\n\nAn Eiffel \"system\" or \"program\" is a collection of \"classes\". Above the level of classes, Eiffel defines \"cluster\", which is essentially a group of classes, and possibly of \"subclusters\" (nested clusters). Clusters are not a syntactic language construct, but rather a standard organizational convention. Typically an Eiffel program will be organized with each class in a separate file, and each cluster in a directory containing class files. In this organization, subclusters are subdirectories. For example, under standard organizational and casing conventions, codice_1 might be the name of a file that defines a class called X.\n\nA class contains \"features\", which are similar to \"routines\", \"members\", \"attributes\" or \"methods\" in other object-oriented programming languages. A class also defines its invariants, and contains other properties, such as a \"notes\" section for documentation and metadata. Eiffel's standard data types, such as codice_2, codice_3 and codice_4, are all themselves classes.\n\nEvery system must have a class designated as \"root\", with one of its creation procedures designated as \"root procedure\". Executing a system consists of creating an instance of the root class and executing its root procedure. Generally, doing so creates new objects, calls new features, and so on.\n\nEiffel has five basic executable instructions: assignment, object creation, routine call, condition, and iteration. Eiffel's control structures are strict in enforcing structured programming: every block has exactly one entry and exactly one exit.\n\nUnlike many object-oriented languages, but like Smalltalk, Eiffel does not permit any assignment into attributes of objects, except within the features of an object, which is the practical application of the principle of information hiding or data abstraction, requiring formal interfaces for data mutation. To put it in the language of other object-oriented programming languages, all Eiffel attributes are \"protected\", and \"setters\" are needed for client objects to modify values. An upshot of this is that \"setters\" can, and normally do, implement the invariants for which Eiffel provides syntax.\n\nWhile Eiffel does not allow direct access to the features of a class by a client of the class, it does allow for the definition of an \"assigner command\", such as:\n\nWhile a slight bow to the overall developer community to allow something looking like direct access (e.g. thereby breaking the Information Hiding Principle), the practice is dangerous as it hides or obfuscates the reality of a \"setter\" being used. In practice, it is better to redirect the call to a setter rather than implying a direct access to a feature like codice_5 as in the example code above.\n\nUnlike other languages, having notions of \"public\", \"protected\", \"private\" and so on, Eiffel uses an exporting technology to more precisely control the scoping between client and supplier classes. Feature visibility is checked statically at compile-time. For example, (below), the \"{NONE}\" is similar to \"protected\" in other languages. Scope applied this way to a \"feature set\" (e.g. everything below the 'feature' keyword to either the next feature set keyword or the end of the class) can be changed in descendant classes using the \"export\" keyword.\nfeature {NONE} -- Initialization\nAlternatively, the lack of a {x} export declaration implies {ANY} and is similar to the \"public\" scoping of other languages.\nfeature -- Constants\nFinally, scoping can be selectively and precisely controlled to any class in the Eiffel project universe, such as:\nfeature {DECIMAL, DCM_MA_DECIMAL_PARSER, DCM_MA_DECIMAL_HANDLER} -- Access\nHere, the compiler will allow only the classes listed between the curly braces to access the features within the feature group (e.g. DECIMAL, DCM_MA_DECIMAL_PARSER, DCM_MA_DECIMAL_HANDLER).\n\nA programming language's look and feel is often conveyed using a \"Hello, world!\" program. Such a program written in Eiffel might be:\n\nclass\ncreate\nfeature\nend \nThis program contains the class codice_6. The constructor (create routine) for the class, named codice_7, invokes the codice_8 system library routine to write a codice_9 codice_10 message to the output.\n\nThe concept of Design by Contract is central to Eiffel. The contracts assert what must be true before a routine is executed (precondition) and what must hold to be true after the routine finishes (post-condition). Class Invariant contracts define what assertions must hold true both before and after any feature of a class is accessed (both routines and attributes). Moreover, contracts codify into executable code developer and designers assumptions about the operating environment of the features of a class or the class as a whole by means of the invariant.\n\nThe Eiffel compiler is designed to include the feature and class contracts in various levels. EiffelStudio, for example, executes all feature and class contracts during execution in the \"Workbench mode.\" When an executable is created, the compiler is instructed by way of the project settings file (e.g. ECF file) to either include or exclude any set of contracts. Thus, an executable file can be compiled to either include or exclude any level of contract, thereby bringing along continuous levels of unit and integration testing. Moreover, contracts can be continually and methodically exercised by way of the Auto-Test feature found in EiffelStudio.\n\nThe Design by Contract mechanisms are tightly integrated with the language and guide redefinition of features in inheritance:\n\n\nIn addition, the language supports a \"check instruction\" (a kind of \"assert\"), loop invariants, and loop variants (which guarantee loop termination).\n\nVoid-safety, like static typing, is another facility for improving software quality. Void-safe software is protected from run time errors caused by calls to void references, and therefore will be more reliable than software in which calls to void targets can occur. The analogy to static typing is a useful one. In fact, void-safe capability could be seen as an extension to the type system, or a step beyond static typing, because the mechanism for ensuring void-safety is integrated into the type system.\n\nThe guard against void target calls can be seen by way of the notion of attachment and (by extension) detachment (e.g. detachable keyword). The void-safe facility can be seen in a short re-work of the example code used above:\n\nThe code example above shows how the compiler can statically address the reliability of whether codice_5 will be attached or detached at the point it is used. Notably, the codice_12 keyword allows for an \"attachment local\" (e.g. codice_13), which is scoped to only the block of code enclosed by the if-statement construct. Thus, within this small block of code, the local variable (e.g. codice_13) can be statically guaranteed to be non-void (i.e. void-safe).\n\nThe primary characteristic of a class is that it defines a set of features: as a class represents a set of run-time objects, or \"instances\", a feature is an operation on these objects. There are two kinds of features: queries and commands. A query provides information about an instance. A command modifies an instance.\n\nThe command-query distinction is important to the Eiffel method. In particular:\n\n\nEiffel does not allow argument overloading. Each feature name within a class always maps to a specific feature within the class. One name, within one class, means one thing. This design choice helps the readability of classes, by avoiding a cause of ambiguity about which routine will be invoked by a call. It also simplifies the language mechanism; in particular, this is what makes Eiffel's multiple inheritance mechanism possible.\n\nNames can, of course, be reused in different classes. For example, the feature plus (along with its infix alias \"+\") is defined in several classes: INTEGER, REAL, STRING, etc.\n\nA generic class is a class that varies by type (e.g. LIST [PHONE], a list of phone numbers; ACCOUNT [G->ACCOUNT_TYPE], allowing for ACCOUNT [SAVINGS] and ACCOUNT [CHECKING], etc.). Classes can be generic, to express that they are parameterized by types. Generic parameters appear in square brackets:\nclass LIST [G] ...\nG is known as a \"formal generic parameter\". (Eiffel reserves \"argument\" for routines, and uses \"parameter\" only for generic classes.) With such a declaration G represents within the class an arbitrary type; so a function can return a value of type G, and a routine can take an argument of that type:\nitem: G do ... end\nput (x: G) do ... end\nThe codice_17 and codice_18 are \"generic derivations\" of this class. Permitted combinations (with codice_19, codice_20, codice_21, codice_22) are:\nn := il.item\nwl.put (w)\ncodice_2 and codice_24 are the \"actual generic parameters\" in these generic derivations.\n\nIt is also possible to have 'constrained' formal parameters, for which the actual parameter must inherit from a given class, the \"constraint\". For example, in\na derivation codice_25 is valid only if codice_3 inherits from codice_27 (as it indeed does in typical Eiffel libraries). Within the class, having codice_28 constrained by codice_27 means that for codice_30 it is possible to apply to codice_31 all the features of codice_27, as in codice_33.\n\nTo inherit from one or more others, a class will include an codice_34 clause at the beginning:\nclass C inherit\n\n-- ... Rest of class declaration ...\nThe class may redefine (override) some or all of the inherited features. This must be explicitly announced at the beginning of the class through a codice_35 subclause of the inheritance clause, as in\nclass C inherit\nSee for a complete discussion of Eiffel inheritance.\n\nClasses may be defined with codice_36 rather than with codice_37 to indicate that the class may not be directly instantiated. Non-instantiatable classes are called abstract classes in some other object-oriented programming languages. In Eiffel parlance, only an \"effective\" class can be instantiated (it may be a descendent of a deferred class). A feature can also be deferred by using the codice_38 keyword in place of a codice_39 clause. If a class has any deferred features it must be declared as deferred; however, a class with no deferred features may nonetheless itself be deferred.\n\nDeferred classes play some of the same role as interfaces in languages such as Java, though many object-oriented programming theorists believe interfaces are themselves largely an answer to Java's lack of multiple inheritance (which Eiffel has).\n\nA class that inherits from one or more others gets all its features, by default under their original names. It may, however, change their names through codice_40 clauses. This is required in the case of multiple inheritance if there are name clashes between inherited features; without renaming, the resulting class would violate the no-overloading principle noted above and hence would be invalid.\n\nTuples types may be viewed as a simple form of class, providing only attributes and the corresponding \"setter\" procedure. A typical tuple type reads\nand could be used to describe a simple notion of birth record if a class is not needed. An instance of such a tuple is simply a sequence of values with the given types, given in brackets, such as\nComponents of such a tuple can be accessed as if the tuple tags were attributes of a class, for example if codice_41 has been assigned the above tuple then codice_42 has value 3.5.\n\nThanks to the notion of assigner command (see below), dot notation can also be used to assign components of such a tuple, as in\n\nThe tuple tags are optional, so that it is also possible to write a tuple type as codice_43. (In some compilers this is the only form of tuple, as tags were introduced with the ECMA standard.)\n\nThe precise specification of e.g. codice_44 is that it describes sequences of \"at least\" three elements, the first three being of types codice_45, codice_46, codice_47 respectively. As a result, codice_44 conforms to (may be assigned to) codice_49, to codice_50 and to codice_51 (without parameters), the topmost tuple type to which all tuple types conform.\n\nEiffel's \"agent\" mechanism wraps operations into objects. This mechanism can be used for iteration, event-driven programming, and other contexts in which it is useful to pass operations around the program structure. Other programming languages, especially ones that emphasize functional programming, allow a similar pattern using continuations, closures, or generators; Eiffel's agents emphasize the language's object-oriented paradigm, and use a syntax and semantics similar to code blocks in Smalltalk and Ruby.\n\nFor example, to execute the codice_52 block for each element of codice_53, one would write:\nTo execute codice_52 only on elements satisfying codice_55, a limitation/filter can be added:\nIn these examples, codice_52 and codice_55 are routines. Prefixing them with codice_58 yields an object that represents the corresponding routine with all its properties, in particular the ability to be called with the appropriate arguments. So if codice_59 represents that object (for example because codice_59 is the argument to codice_61), the instruction\nwill call the original routine with the argument codice_31, as if we had directly called the original routine: codice_63. Arguments to codice_64 are passed as a tuple, here codice_65.\n\nIt is possible to keep some arguments to an agent open and make others closed. The open arguments are passed as arguments to codice_64: they are provided at the time of \"agent use\". The closed arguments are provided at the time of agent \"definition\". For example, if codice_67 has two arguments, the iteration\niterates codice_68 for successive values of codice_31, where the second argument remains set to codice_70. The question mark codice_71 indicates an open argument; codice_70 is a closed argument of the agent. Note that the basic syntax codice_73 is a shorthand for codice_74 with all arguments open. It is also possible to make the \"target\" of an agent open through the notation codice_75 where codice_76 is the type of the target.\n\nThe distinction between open and closed operands (operands = arguments + target) corresponds to the distinction between bound and free variables in lambda calculus. An agent expression such as codice_77 with some operands closed and some open corresponds to a version of the original operation \"curried\" on the closed operands.\n\nThe agent mechanism also allows defining an agent without reference to an existing routine (such as codice_52, codice_55, codice_67), through inline agents as in\nmy_list.do_all (agent (s: STRING)\nThe inline agent passed here can have all the trappings of a normal routine, including precondition, postcondition, rescue clause (not used here), and a full signature. This avoids defining routines when all that's needed is a computation to be wrapped in an agent. This is useful in particular for contracts, as in an invariant clause that expresses that all elements of a list are positive:\nThe current agent mechanism leaves a possibility of run-time type error (if a routine with \"n\" arguments is passed to an agent expecting \"m\" arguments with \"m\" < \"n\"). This can be avoided by a run-time check through the precondition codice_81 of codice_64. Several proposals for a purely static correction of this problem are available, including a language change proposal by Ribet et al.\n\nA routine's result can be cached using the codice_83 keyword in place of codice_39. Non-first calls to a routine require no additional computation or resource allocation, but simply return a previously computed result. A common pattern for \"once functions\" is to provide shared objects; the first call will create the object, subsequent ones will return the reference to that object. The typical scheme is:\nshared_object: SOME_TYPE\nThe returned object—codice_85 in the example—can itself be mutable, but its reference remains the same.\n\nOften \"once routines\" perform a required initialization: multiple calls to a library can include a call to the initialization procedure, but only the first such call will perform the required actions. Using this pattern initialization can be decentralized, avoiding the need for a special initialization module. \"Once routines\" are similar in purpose and effect to the singleton pattern in many programming languages, and to the used in Python.\n\nBy default, a \"once routine\" is called \"once per thread\". The semantics can be adjusted to \"once per process\" or \"once per object\" by qualifying it with a \"once key\", e.g. codice_86.\n\nEiffel provides a mechanism to allow conversions between various types. The mechanisms coexists with inheritance and complements it. To avoid any confusion between the two mechanisms, the design enforces the following principle:\n\nFor example, codice_87 may conform to codice_88, but codice_2 converts to codice_90 (and does not inherit from it).\n\nThe conversion mechanism simply generalizes the ad hoc conversion rules (such as indeed between codice_2 and codice_90) that exist in most programming languages, making them applicable to any type as long as the above principle is observed. For example, a codice_93 class may be declared to convert to codice_3; this makes it possible to create a string from a date simply through\n\nas a shortcut for using an explicit object creation with a conversion procedure:\n\nTo make the first form possible as a synonym for the second, it suffices to list the creation procedure (constructor) codice_95 in a codice_96 clause at the beginning of the class.\n\nAs another example, if there is such a conversion procedure listed from codice_97, then one can directly assign a tuple to a date, causing the appropriate conversion, as in\n\nException handling in Eiffel is based on the principles of design by contract. For example, an exception occurs when a routine's caller fails to satisfy a precondition, or when a routine cannot ensure a promised postcondition. In Eiffel, exception handling is not used for control flow or to correct data-input mistakes.\n\nAn Eiffel exception handler is defined using the rescue keyword. Within the rescue section, the retry keyword executes the routine again. For example, the following routine tracks the number of attempts at executing the routine, and only retries a certain number of times:\nconnect_to_server (server: SOCKET)\nThis example is arguably flawed for anything but the simplest programs, however, because connection failure is to be expected. For most programs a routine name like attempt_connecting_to_server would be better, and the postcondition would not promise a connection, leaving it up to the caller to take appropriate steps if the connection was not opened.\n\nA number of networking and threading libraries are available, such as EiffelNet and EiffelThreads. A concurrency model for Eiffel, based on the concepts of design by contract, is SCOOP, or \"Simple Concurrent Object-Oriented Programming\", not yet part of the official language definition but available in EiffelStudio.\nCAMEO is an (unimplemented) variation of SCOOP for Eiffel.\nConcurrency also interacts with exceptions. Asynchronous exceptions can be troublesome (where a routine raises an exception after its caller has itself finished).\n\nEiffel's view of computation is completely object-oriented in the sense that every operation is relative to an object, the \"target\". So for example an addition such as\na + b\nis conceptually understood as if it were the method call\na.plus (b)\nwith target codice_59, feature codice_99 and argument codice_100.\n\nOf course, the former is the conventional syntax and usually preferred. Operator syntax makes it possible to use either form by declaring the feature (for example in codice_2, but this applies to other basic classes and can be used in any other for which such an operator is appropriate):\nplus alias \"+\" (other: INTEGER): INTEGER\nThe range of operators that can be used as \"alias\" is quite broad; they include predefined operators such as \"+\" but also \"free operators\" made of non-alphanumeric symbols. This makes it possible to design special infix and prefix notations, for example in mathematics and physics applications.\n\nEvery class may in addition have \"one\" function aliased to \"[]\", the \"bracket\" operator, allowing the notation codice_102 as a synonym for codice_103 where codice_104 is the chosen function. This is particularly useful for container structures such as arrays, hash tables, lists etc. For example, access to an element of a hash table with string keys can be written\n\"Assigner commands\" are a companion mechanism designed in the same spirit of allowing well-established, convenient notation reinterpreted in the framework of object-oriented programming. Assigner commands allow assignment-like syntax to call \"setter\" procedures. An assignment proper can never be of the form codice_105 as this violates information hiding; you have to go for a setter command (procedure). For example, the hash table class can have the function and the procedure\nitem alias \"[]\" (key: STRING): ELEMENT [3]\n\nput (e: ELEMENT; key: STRING)\nThen to insert an element you have to use an explicit call to the setter command:\nIt is possible to write this equivalently as\n(in the same way that codice_106 is a synonym for codice_107), provided the declaration of codice_108 now starts (replacement for [3]) with\nThis declares codice_109 as the assigner command associated with codice_108 and, combined with the bracket alias, makes [5] legal and equivalent to [4]. (It could also be written, without taking advantage of the bracket, as codice_111.\n\nnote: The argument list of a's assigner is constrained to be: (a's return type;all of a's argument list...)\n\nEiffel is not case-sensitive. The tokens codice_7, codice_113 and codice_114 all denote the same identifier. See, however, the \"style rules\" below.\n\nComments are introduced by codice_115 (two consecutive dashes) and extend to the end of the line.\n\nThe semicolon, as instruction separator, is optional. Most of the time the semicolon is just omitted, except to separate multiple instructions on a line. This results in less clutter on the program page.\n\nThere is no nesting of feature and class declarations. As a result, the structure of an Eiffel class is simple: some class-level clauses (inheritance, invariant) and a succession of feature declarations, all at the same level.\n\nIt is customary to group features into separate \"feature clauses\" for more readability, with a standard set of basic feature tags appearing in a standard order, for example:\nclass HASH_TABLE [ELEMENT, KEY -> HASHABLE] inherit TABLE [ELEMENT]\n\nend\nIn contrast to most curly bracket programming languages, Eiffel makes a clear distinction between expressions and instructions. This is in line with the Command-Query Separation principle of the Eiffel method.\n\nMuch of the documentation of Eiffel uses distinctive style conventions, designed to enforce a consistent look-and-feel. Some of these conventions apply to the code format itself, and others to the standard typographic rendering of Eiffel code in formats and publications where these conventions are possible.\n\nWhile the language is case-insensitive, the style standards prescribe the use of all-capitals for class names (codice_116), all-lower-case for feature names (codice_7), and initial capitals for constants (codice_118). The recommended style also suggests underscore to separate components of a multi-word identifier, as in codice_119.\n\nThe specification of Eiffel includes guidelines for displaying software texts in typeset formats: keywords in bold, user-defined identifiers and constants are shown in \"codice_120\", comments, operators, and punctuation marks in codice_121, with program text in codice_122 as in the present article to distinguish it from explanatory text. For example, the \"Hello, world!\" program given above would be rendered as below in Eiffel documentation:\n\nEiffel is a purely object-oriented language but provides an open architecture for interfacing with \"external\" software in any other programming language.\n\nIt is possible for example to program machine- and operating-system level operations in C. Eiffel provides a straightforward interface to C routines, including support for \"inline C\" (writing the body of an Eiffel routine in C, typically for short machine-level operations).\n\nAlthough there is no direct connection between Eiffel and C, many Eiffel compilers (Visual Eiffel is one exception) output C source code as an intermediate language, to submit to a C compiler, for optimizing and portability. As such, they are examples of transcompilers. The Eiffel Compiler tecomp can execute Eiffel code directly (like an interpreter) without going via an intermediate C code or emit C code which will be passed to a C compiler in order to obtain optimized native code. On .NET, the EiffelStudio compiler directly generates CIL (Common Intermediate Language) code. The SmartEiffel compiler can also output Java bytecode.\n\n"}
{"id": "5007474", "url": "https://en.wikipedia.org/wiki?curid=5007474", "title": "Elliptic cylindrical coordinates", "text": "Elliptic cylindrical coordinates\n\nElliptic cylindrical coordinates are a three-dimensional orthogonal coordinate system that results from projecting the two-dimensional elliptic coordinate system in the\nperpendicular formula_1-direction. Hence, the coordinate surfaces are prisms of confocal ellipses and hyperbolae. The two foci \nformula_2 and formula_3 are generally taken to be fixed at formula_4 and\nformula_5, respectively, on the formula_6-axis of the Cartesian coordinate system.\n\nThe most common definition of elliptic cylindrical coordinates formula_7 is\n\nwhere formula_11 is a nonnegative real number and formula_12. \n\nThese definitions correspond to ellipses and hyperbolae. The trigonometric identity\n\nshows that curves of constant formula_11 form ellipses, whereas the hyperbolic trigonometric identity\n\nshows that curves of constant formula_16 form hyperbolae.\n\nThe scale factors for the elliptic cylindrical coordinates formula_11 and formula_16 are equal\n\nwhereas the remaining scale factor formula_20. \nConsequently, an infinitesimal volume element equals\n\nand the Laplacian equals \n\nOther differential operators such as formula_23 and formula_24 can be expressed in the coordinates formula_7 by substituting \nthe scale factors into the general formulae found in orthogonal coordinates.\n\nAn alternative and geometrically intuitive set of elliptic coordinates formula_26 are sometimes used, where formula_27 and formula_28. Hence, the curves of constant formula_29 are ellipses, whereas the curves of constant formula_30 are hyperbolae. The coordinate formula_30 must belong to the interval [-1, 1], whereas the formula_29 \ncoordinate must be greater than or equal to one.\nThe coordinates formula_26 have a simple relation to the distances to the foci formula_2 and formula_3. For any point in the (x,y) plane, the \"sum\" formula_36 of its distances to the foci equals formula_37, whereas their \"difference\" formula_38 equals formula_39.\nThus, the distance to formula_2 is formula_41, whereas the distance to formula_3 is formula_43. (Recall that formula_2 and formula_3 are located at formula_46 and formula_47, respectively.) \n\nA drawback of these coordinates is that they do not have a 1-to-1 transformation to the Cartesian coordinates\n\nThe scale factors for the alternative elliptic coordinates formula_26 are \n\nand, of course, formula_20. Hence, the infinitesimal volume element becomes \n\nand the Laplacian equals\n\nOther differential operators such as formula_23 \nand formula_24 can be expressed in the coordinates formula_58 by substituting \nthe scale factors into the general formulae \nfound in orthogonal coordinates.\n\nThe classic applications of elliptic cylindrical coordinates are in solving partial differential equations, \ne.g., Laplace's equation or the Helmholtz equation, for which elliptic cylindrical coordinates allow a \nseparation of variables. A typical example would be the electric field surrounding a \nflat conducting plate of width formula_59.\n\nThe three-dimensional wave equation, when expressed in elliptic cylindrical coordinates, may be solved by separation of variables, leading to the Mathieu differential equations.\n\nThe geometric properties of elliptic coordinates can also be useful. A typical example might involve \nan integration over all pairs of vectors formula_60 and formula_61 \nthat sum to a fixed vector formula_62, where the integrand \nwas a function of the vector lengths formula_63 and formula_64. (In such a case, one would position formula_65 between the two foci and aligned with the formula_6-axis, i.e., formula_67.) For concreteness, formula_65, formula_60 and formula_61 could represent the momenta of a particle and its decomposition products, respectively, and the integrand might involve the kinetic energies of the products (which are proportional to the squared lengths of the momenta).\n\n\n"}
{"id": "43350725", "url": "https://en.wikipedia.org/wiki?curid=43350725", "title": "Euclid–Euler theorem", "text": "Euclid–Euler theorem\n\nThe Euclid–Euler theorem is a theorem in mathematics that relates perfect numbers to Mersenne primes. It states that every even perfect number has the form 2(2 − 1), where 2 − 1 is a prime number. The prime numbers of the form 2 − 1 are known as Mersenne primes, and require \"n\" itself to be prime.\n\nIt has been conjectured that there are infinitely many Mersenne primes. Although the truth of this conjecture remains unknown, it is equivalent, by the Euclid–Euler theorem, to the conjecture that there are infinitely many even perfect numbers. However it is also unknown whether there exists even a single odd perfect number.\n\nA perfect number is a natural number that equals the sum of its proper divisors, the numbers that are less than it and divide it evenly. For instance, the proper divisors of 6 are 1, 2, and 3, which sum to 6, so 6 is perfect.\nA Mersenne prime is a prime number of the form \"M\" = 2 − 1; for a number of this form to be prime, \"p\" itself must also be prime.\nThe Euclid–Euler theorem states that an even natural number is perfect if and only if it has the form 2\"M\" where \"M\" is a Mersenne prime.\n\nEuclid proved that is an even perfect number whenever is prime (Euclid, Prop. IX.36). This is the final result on number theory in \"Euclid's Elements\"; the later books in the \"Elements\" instead concern irrational numbers, solid geometry, and the golden ratio. Euclid expresses the result by stating that if a finite geometric series beginning at 1 with ratio 2 has a prime sum \"P\", then this sum multiplied by the last term \"T\" in the series is perfect. Expressed in these terms, the sum \"P\" of the finite series is the Mersenne prime and the last term \"T\" in the series is the power of two Euclid proves that \"PT\" is perfect by observing that the geometric series with ratio 2 starting at \"P\", with the same number of terms, is proportional to the original series; therefore, since the original series sums to \"P\" = 2\"T\" − 1, the second series sums to \"P\"(2\"T\" − 1) = 2\"PT\" − \"P\", and both series together add to 2\"PT\", two times the supposed perfect number. However, these two series are disjoint from each other and (by the primality of \"P\") exhaust all the divisors of \"PT\", so \"PT\" has divisors that sum to 2\"PT\", showing that it is perfect.\n\nOver a millennium after Euclid, Alhazen conjectured that \"every\" even perfect number is of the form where is prime, but he was not able to prove this result.\n\nIt was not until the 18th century that Leonhard Euler proved that the formula will yield all the even perfect numbers. Thus, there is a one-to-one relationship between even perfect numbers and Mersenne primes; each Mersenne prime generates one even perfect number, and vice versa.\n\nEuler's proof is short and depends on the fact that the sum of divisors function σ is multiplicative; that is, if \"a\" and \"b\" are any two relatively prime integers, then For this formula to be valid, the sum of divisors of a number must include the number itself, not just the proper divisors. A number is perfect if and only if its sum of divisors is twice its value.\n\nOne direction of the theorem (the part already proved by Euclid) immediately follows from the multiplicative property: Every Mersenne prime gives rise to an even perfect number. When is prime, . The sum of divisors for is , e.g. for we get 16 () and the sum of its divisors is 31 (). being a prime implies that the sum of its divisors is since its constituents are the number itself and 1 so .\n\nTaking this into account leads to:\n\nIn the other direction, suppose that an even perfect number has been given, and partially factor it as 2\"x\", where \"x\" is odd. For to be perfect, its sum of divisors must be twice its value:\n\nThe odd factor on the right side of (∗) is at least 3, and it must divide or equal \"x\", the only odd factor on the left side, so\nFor this equality to be true, there can be no other divisors. Therefore, \"y\" must be 1, and \"x\" must be a prime of the form \n"}
{"id": "15462488", "url": "https://en.wikipedia.org/wiki?curid=15462488", "title": "Fermi–Ulam model", "text": "Fermi–Ulam model\n\nThe Fermi–Ulam model (FUM) is a dynamical system that was introduced by Polish mathematician Stanislaw Ulam in 1961.\n\nFUM is a variant of Enrico Fermi's primary work on acceleration of cosmic rays, namely Fermi acceleration. The system consists of a particle that collides elastically between a fixed wall and a moving one, each of infinite mass. The walls represent the magnetic mirrors with which the cosmic particles collide.\n\nA. J. Lichtenberg and M. A. Lieberman provided a simplified version of FUM (SFUM) that derives from the\nPoincaré surface of section formula_1 and writes\n\nwhere formula_4 is the velocity of the particle after the formula_5-th collision with the fixed wall, formula_6 is the corresponding phase of the moving wall, formula_7 is the velocity law of the moving wall and formula_8 is the stochasticity parameter of the system.\n\nIf the velocity law of the moving wall is differentiable enough, according to KAM theorem invariant curves in the phase space formula_9 exist. These invariant curves act as barriers that do not allow for a particle to further accelerate and the average velocity of a population of particles saturates after finite iterations of the map. For instance, for sinusoidal velocity law of the moving wall such curves exist, while they do not for sawtooth velocity law that is discontinuous. Consequently, at the first case particles cannot accelerate infinitely, reversely to what happens at the last one.\n\nFUM became over the years a prototype model for studying non-linear dynamics and coupled mappings.\n\nThe rigorous solution of the Fermi-Ulam problem (the velocity and energy of the particle are bounded) was given first by L. D. Pustyl'nikov in (see also and references therein).\n\nIn spite of these negative results, if one considers the Fermi–Ulam model in the framework of the special theory of relativity, then under some general conditions the energy of the particle tends to infinity for an open set of initial data.\n\nThough the 1D FUM does not lead to acceleration for smooth oscillations, unbounded energy growth has been observed in 2D billiards with oscillating boundaries, The growth rate of energy in chaotic billiards is found to be much larger than that in billiards that are integrable in the static limit.\n\nStrongly chaotic billiard with oscillating boundary can serve as a paradigm for driven chaotic systems. \nIn the experimental arena this topic arises in the theory of \"nuclear friction\" \nand more recently in the studies of cold atoms that are trapped in \"optical billiards\"\nThe driving induces diffusion in energy \nand consequently the absorption coefficient is determined by the Kubo formula\n\n"}
{"id": "1679022", "url": "https://en.wikipedia.org/wiki?curid=1679022", "title": "Fodor's lemma", "text": "Fodor's lemma\n\nIn mathematics, particularly in set theory, Fodor's lemma states the following:\n\nIf formula_1 is a regular, uncountable cardinal, formula_2 is a stationary subset of formula_1, and formula_4 is regressive (that is, formula_5 for any formula_6, formula_7) then there is some formula_8 and some stationary formula_9 such that formula_10 for any formula_11. In modern parlance, the nonstationary ideal is \"normal\".\n\nThe lemma was first proved by the Hungarian set theorist, Géza Fodor in 1956. It is sometimes also called \"The Pressing Down Lemma\".\n\nWe can assume that formula_12 (by removing 0, if necessary).\nIf Fodor's lemma is false, for every formula_13 there is some club set formula_14 such that formula_15. Let formula_16. The club sets are closed under diagonal intersection, so formula_17 is also club and therefore there is some formula_18. Then formula_19 for each formula_20, and so there can be no formula_20 such that formula_22, so formula_23, a contradiction.\n\nFodor's lemma also holds for Thomas Jech's notion of stationary sets as well as for the general notion of stationary set.\n\nAnother related statement, also known as Fodor's lemma (or Pressing-Down-lemma), is the following:\n\nFor every non-special tree formula_24 and regressive mapping formula_25 (that is, formula_26, with respect to the order on formula_24, for every formula_28), there is a non-special subtree formula_29 on which formula_30 is constant.\n\n"}
{"id": "3821872", "url": "https://en.wikipedia.org/wiki?curid=3821872", "title": "Friendly number", "text": "Friendly number\n\nIn number theory, friendly numbers are two or more natural numbers with a common abundancy index, the ratio between the sum of divisors of a number and the number itself. Two numbers with the same abundancy form a friendly pair; \"n\" numbers with the same abundancy form a friendly \"n\"-tuple.\n\nBeing mutually friendly is an equivalence relation, and thus induces a partition of the positive naturals into clubs (equivalence classes) of mutually friendly numbers.\n\nA number that is not part of any friendly pair is called solitary.\n\nThe abundancy index of \"n\" is the rational number σ(\"n\") / \"n\", in which σ denotes the sum of divisors function. A number \"n\" is a friendly number if there exists \"m\" ≠ \"n\" such that σ(\"m\") / \"m\" = σ(\"n\") / \"n\". Note that abundancy is not the same as abundance, which is defined as σ(\"n\") − 2\"n\".\n\nAbundancy may also be expressed as formula_1 where formula_2 denotes a divisor function with formula_3 equal to the sum of the \"k\"-th powers of the divisors of \"n\".\n\nThe numbers 1 through 5 are all solitary. The smallest friendly number is 6, forming for example the friendly pair 6 and 28 with abundancy σ(6) / 6 = (1+2+3+6) / 6 = 2, the same as σ(28) / 28 = (1+2+4+7+14+28) / 28 = 2. The shared value 2 is an integer in this case but not in many other cases. Numbers with abundancy 2 are also known as perfect numbers. There are several unsolved problems related to the friendly numbers.\n\nIn spite of the similarity in name, there is no specific relationship between the friendly numbers and the amicable numbers or the sociable numbers, although the definitions of the latter two also involve the divisor function.\n\nAs another example, 30 and 140 form a friendly pair, because 30 and 140 have the same abundancy:\n\nThe numbers 2480, 6200 and 40640 are also members of this club, as they each have an abundancy equal to 12/5.\n\nFor an example of odd numbers being friendly, consider 135 and 819 (abundancy 16/9). There are also cases of even being friendly to odd, like 42 and 544635 (abundancy 16/7).\n\nA square number can be friendly, for instance both 693479556 (the square of 26334) and 8640 have abundancy 127/36 (this example is due to Dean Hickerson).\n\n are \"proved\" friendly , are \"proved\" solitary , numbers \"n\" such that \"n\" and formula_6 are coprime are not coloured here, though they are known to be solitary. Other numbers have unknown status and are .\n\nA number that belongs to a singleton club, because no other number is friendly with it, is a solitary number. All prime numbers are known to be solitary, as are powers of prime numbers. More generally, if the numbers \"n\" and σ(\"n\") are coprime – meaning that the greatest common divisor of these numbers is 1, so that σ(\"n\")/\"n\" is an irreducible fraction – then the number \"n\" is solitary . For a prime number \"p\" we have σ(\"p\") = \"p\" + 1, which is coprime with \"p\".\n\nNo general method is known for determining whether a number is friendly or solitary. The smallest number whose classification is unknown (as of 2009) is 10; it is conjectured to be solitary; if not, its smallest friend is a fairly large number, like the status for the number 24, although 24 is friendly, its smallest friend is 91,963,648. There are no friendly numbers for 10 that are less than 2,000,000,000.\n\nIt is an open problem whether there are infinitely large clubs of mutually friendly numbers. The perfect numbers form a club, and it is conjectured that there are infinitely many perfect numbers (at least as many as there are Mersenne primes), but no proof is known. , 50 perfect numbers are known, the largest of which has more than 46 million digits in decimal notation. There are clubs with more known members, in particular those formed by multiply perfect numbers, which are numbers whose abundancy is an integer. As of early 2013, the club of friendly numbers with abundancy equal to 9 has 2094 known members. Although some are known to be quite large, clubs of multiply perfect numbers (excluding the perfect numbers themselves) are conjectured to be finite.\n\n"}
{"id": "442967", "url": "https://en.wikipedia.org/wiki?curid=442967", "title": "Fundamental theorem of asset pricing", "text": "Fundamental theorem of asset pricing\n\nThe fundamental theorems of asset pricing (also: of arbitrage, of finance) provide necessary and sufficient conditions for a market to be arbitrage free and for a market to be complete. An arbitrage opportunity is a way of making money with no initial investment without any possibility of loss. Though arbitrage opportunities do exist briefly in real life, it has been said that any sensible market model must avoid this type of profit. The first theorem is important in that it ensures a fundamental property of market models. Completeness is a common property of market models (for instance the Black–Scholes model). A complete market is one in which every contingent claim can be replicated. Though this property is common in models, it is not always considered desirable or realistic.\n\nIn a discrete (i.e. finite state) market, the following hold:\n\nWhen stock price returns follow a single Brownian motion, there is a unique risk neutral measure. When the stock price process is assumed to follow a more general sigma-martingale or semimartingale, then the concept of arbitrage is too narrow, and a stronger concept such as no free lunch with vanishing risk must be used to describe these opportunities in an infinite dimensional setting.\n\n\n\n"}
{"id": "6796998", "url": "https://en.wikipedia.org/wiki?curid=6796998", "title": "Gleason's theorem", "text": "Gleason's theorem\n\nGleason's theorem (named after Andrew M. Gleason) is a mathematical result which shows that the rule one uses to calculate probabilities in quantum physics follows logically from particular assumptions about how measurements are represented mathematically. More specifically, it proves that the Born rule for the probability of obtaining specific results for a given measurement follows naturally from the structure formed by the lattice of events in a real or complex Hilbert space. This result is of particular importance for the field of quantum logic. Furthermore, it was historically significant for the role it played in showing that local hidden variable theories are inconsistent with quantum physics. The theorem states:\n\nThe trace-class operator \"W\" can be interpreted as the density matrix of a quantum state. Effectively, the theorem says that any legitimate probability measure on the space of measurement outcomes is generated by some quantum state.\n\nConsider a quantum system with a Hilbert space of dimension 3 or larger, and suppose that there exists some function that assigns a probability to each outcome of any possible measurement upon that system. The probability of any such outcome must be a real number between 0 and 1 inclusive, and in order to be consistent, for any individual measurement the probabilities of the different possible outcomes must add up to 1. Gleason's theorem shows that any such function—that is, any consistent assignment of probabilities to measurement outcomes—must be expressible in terms of a quantum-mechanical density operator and the Born rule. In other words, given that each quantum system is associated with a Hilbert space, and given that measurements are described by particular mathematical entities defined on that Hilbert space, both the structure of quantum state space and the rule for calculating probabilities from a quantum state then follow.\n\nFor simplicity, we can assume that the dimension of the Hilbert space is finite. A quantum-mechanical observable is a self-adjoint operator on that Hilbert space. Equivalently, we can say that a measurement is defined by an orthonormal basis, with each possible outcome of that measurement corresponding to one of the vectors comprising the basis. A density operator is a positive-semidefinite operator whose trace is equal to 1. In the language of von Weizsäcker, a density operator is a \"catalogue of probabilities\": for each measurement that can be defined, we can compute the probability distribution over the outcomes of that measurement from the density operator. We do so by applying the Born rule, which states thatformula_5where formula_6 is the density operator and formula_7 is the projection operator onto the basis vector associated with the measurement outcome formula_8.\n\nLet formula_9 be a function from projection operators to the unit interval with the property that, if a set formula_10 of projection operators sum to the identity matrix—that is, if they correspond to an orthonormal basis—thenformula_11Such a function expresses an assignment of probability values to the outcomes of measurements, an assignment that is \"noncontextual\" in the sense that the probability for an outcome does not depend upon which measurement that outcome is embedded within, but only upon the mathematical representation of that specific outcome, i.e., its projection operator. Gleason's theorem states that for any such function formula_9, there exists a positive semidefinite operator with unit trace formula_6 such thatformula_14Both the Born rule and the fact that \"catalogues of probability\" are positive semidefinite operators of unit trace follow from the assumptions that measurements are represented by orthonormal bases, and that probability assignments are \"noncontextual\". In order for Gleason's theorem to be applicable, the space on which measurements are defined must be a real or complex Hilbert space, or a quaternionic module. (Gleason's argument is inapplicable if, for example, one tries to construct an analogue of quantum mechanics using \"p\"-adic numbers.)\n\nAnother way of phrasing the theorem uses the terminology of quantum logic, which makes heavy use of lattice theory. Quantum logic treats quantum events (or measurement outcomes) as logical propositions, and studies the relationships and structures formed by these events, with specific emphasis on quantum measurement. In quantum logic, the logical propositions that describe events are organized into a lattice in which the distributive law, valid in classical logic, is weakened, to reflect the fact that in quantum physics, not all pairs of quantities can be measured simultaneously. The \"representation theorem\" in quantum logic shows that such a lattice is isomorphic to the lattice of subspaces of a vector space with a scalar product. It remains an open problem in quantum logic to constrain the field \"K\" over which the vector space is defined. Solèr's theorem implies that, granting certain hypotheses, the field \"K\" must be either the real numbers, complex numbers, or the quaternions.\n\nWe let \"A\" represent an observable with finitely many potential outcomes: the eigenvalues of the Hermitian operator \"A\", i.e. formula_15. An \"event\", then, is a proposition formula_16, which in natural language can be rendered \"the outcome of measuring \"A\" on the system is formula_17\". Let \"H\" denote the Hilbert space associated with the physical system, and let \"L\" denote the lattice of subspaces of \"H.\" The events formula_16 generate a sublattice of \"L\" which is a finite Boolean algebra, and if \"n\" is the dimension of the Hilbert space, then each event is an atom of the lattice \"L.\"\n\nA \"quantum probability function\" over \"H\" is a real function \"P\" on the atoms in \"L\" that has the following properties:\n\nThis means for every lattice element \"y\", the probability of obtaining \"y\" as a measurement outcome is known, since it may be expressed as the union of the atoms under \"y\": formula_24\n\nIn this context, Gleason's theorem states:\n\nAs one consequence: if some formula_31 satisfies formula_32, then \"W\" is the projection onto the complex line spanned by formula_31 and formula_34 for all formula_27.\n\nGleason's theorem highlights a number of fundamental issues in quantum measurement theory. Fuchs argues that the theorem \"is an extremely powerful result,\" because \"it indicates the extent to which the Born probability rule and even the state-space structure of density operators are \"dependent\" upon the theory's other postulates.\" As a consequence, quantum theory is \"a tighter package than one might have first thought.\"\n\nThe theorem is often taken to rule out the possibility of hidden variables in quantum mechanics. This is because the theorem implies that there can be no bivalent probability measures, i.e. probability measures having only the values 1 and 0. To see this, note that the mapping formula_36 is continuous on the unit sphere of the Hilbert space for any density operator \"W\". Since this unit sphere is connected, no continuous function on it can take only the values of 0 and 1. But, a hidden variable theory which is deterministic implies that the probability of a given outcome is \"always\" either 0 or 1: either the electron's spin is up, or it isn't (which accords with classical intuitions). Gleason's theorem therefore seems to hint that quantum theory represents a deep and fundamental departure from the classical way of looking at the world. (This has been argued to support a variety of philosophical perspectivism.)\n\nGleason's theorem motivated later work by John Stuart Bell, Ernst Specker and Simon Kochen that led to the result often called the Kochen–Specker theorem, which rules out a broad class of hidden-variable models. As noted above, Gleason's theorem shows that there is no bivalent probability measure over the rays of a Hilbert space (as long as the dimension of that space exceeds 2). The Kochen–Specker theorem refines this statement by constructing a specific finite subset of rays on which no bivalent probability measure can be defined.\n\nA density operator that is a rank-1 projection is known as a \"pure\" quantum state, and all quantum states that are not pure are designated \"mixed.\" Assigning a pure state to a quantum system implies certainty about the outcome of some measurement on that system (i.e., formula_37 for some outcome \"x\"). Any mixed state can be written as a convex combination of pure states, though not in a unique way. Because Gleason's theorem yields the set of all quantum states, pure and mixed, it can be taken as an argument that pure and mixed states should be treated on the same conceptual footing, rather than viewing pure states as more fundamental conceptions.\n\nTo some researchers, such as Pitowsky, the result is convincing enough to conclude that quantum mechanics represents a new theory of probability. Alternatively, such approaches as relational quantum mechanics make use of Gleason's theorem as an essential step in deriving the quantum formalism from information-theoretic postulates.\n\nGleason's original proof proceeds in three stages. In Gleason's terminology, a frame function that is derived in the standard way—i.e., by the Born rule from a quantum state—is \"regular.\" Gleason derives a sequence of lemmas concerning when a frame function is necessarily regular, culminating in the final theorem. First, he establishes that every frame function on the Hilbert space formula_38 is continuous. Then, he proves the theorem for the special case of formula_38. Finally, he shows that the general problem can be reduced to this special case.\n\nGleason originally proved the theorem assuming that the measurements applied to the system are of the von Neumann type, i.e., that each possible measurement corresponds to an orthonormal basis of the Hilbert space. Later, Busch, and independently Caves \"et al.,\" proved an analogous result for a more general class of measurements, known as positive operator valued measures (POVMs). The proof of this result is simpler than that of Gleason's, and unlike the original theorem of Gleason, the generalized version using POVMs also applies to the case of a single qubit, for which the dimension of the Hilbert space equals 2. This has been interpreted as showing that the probabilities for outcomes of measurements upon a single qubit cannot be explained in terms of hidden variables, provided that the class of allowed measurements is sufficiently broad.\n\nGleason's theorem, in its original version, does not hold if the Hilbert space is defined over the rational numbers, i.e., if the components of vectors in the Hilbert space are restricted to be rational numbers, or complex numbers with rational parts. However, when the set of allowed measurements is the set of all POVMs, the theorem holds.\n\nThe original proof by Gleason was not constructive: one of the ideas on which it depends is the fact that every continuous function defined on a compact space obtains its minimum. Because one cannot in all cases explicitly show where the minimum occurs, a proof that relies upon this principle will not be a constructive proof. However, the theorem can be reformulated in such a way that a constructive proof can be found.\n\nGleason's theorem can be extended to some cases where the observables of the theory form a von Neumann algebra. Specifically, an analogue of Gleason's result can be shown to hold if the algebra of observables has no direct summand that is representable as the algebra of two-by-two matrices over a commutative von Neumann algebra (i.e., no direct summand of type I). In essence, the only barrier to proving the theorem is the fact that Gleason's original result does not hold when the Hilbert space is that of a qubit.\n\n"}
{"id": "8666496", "url": "https://en.wikipedia.org/wiki?curid=8666496", "title": "Hanno Rund", "text": "Hanno Rund\n\nHanno Rund (26 October 1925 in Schwerin – 5 January 1993 in Tucson, Arizona) was a German mathematician. He wrote numerous publications, including perhaps his most famous, \"The Hamilton-Jacobi theory in the calculus of variations. Its role in mathematics and physics\".\n\nRund received his Ph.D in 1950 from the University of Cape Town, South Africa. In 1952, he obtained his Habilitation\nat the University of Freiburg in Germany. His notable students include David Lovelock and Martin Sade.\n\n"}
{"id": "462698", "url": "https://en.wikipedia.org/wiki?curid=462698", "title": "Harmonic divisor number", "text": "Harmonic divisor number\n\nIn mathematics, a harmonic divisor number, or Ore number (named after Øystein Ore who defined it in 1948), is a positive integer whose divisors have a harmonic mean that is an integer. The first few harmonic divisor numbers are\n\nFor example, the harmonic divisor number 6 has the four divisors 1, 2, 3, and 6. Their harmonic mean is an integer:\n\nThe number 140 has divisors 1, 2, 4, 5, 7, 10, 14, 20, 28, 35, 70, and 140. Their harmonic mean is:\n\n5 is an integer, making 140 a harmonic divisor number.\n\nThe harmonic mean of the divisors of any number can be expressed as the formula\nwhere is the sum of th powers of the divisors of : is the number of divisors, and is the sum of divisors .\nAll of the terms in this formula are multiplicative, but not completely multiplicative.\nTherefore, the harmonic mean is also multiplicative.\nThis means that, for any positive integer , the harmonic mean can be expressed as the product of the harmonic means for the prime powers in the factorization of .\n\nFor instance, we have\nand\n\nFor any integer \"M\", as Ore observed, the product of the harmonic mean and arithmetic mean of its divisors equals \"M\" itself, as can be seen from the definitions. Therefore, \"M\" is harmonic, with harmonic mean of divisors \"k\", if and only if the average of its divisors is the product of \"M\" with a unit fraction 1/\"k\".\n\nOre showed that every perfect number is harmonic. To see this, observe that the sum of the divisors of a perfect number \"M\" is exactly \"2M\"; therefore, the average of the divisors is \"M\"(2/τ(\"M\")), where τ(\"M\") denotes the number of divisors of \"M\". For any \"M\", τ(\"M\") is odd if and only if \"M\" is a square number, for otherwise each divisor \"d\" of \"M\" can be paired with a different divisor \"M\"/\"d\". But, no perfect number can be a square: this follows from the known form of even perfect numbers and from the fact that odd perfect numbers (if they exist) must have a factor of the form \"q\" where α ≡ 1 (mod 4). Therefore, for a perfect number \"M\", τ(\"M\") is even and the average of the divisors is the product of \"M\" with the unit fraction 2/τ(\"M\"); thus, \"M\" is a harmonic divisor number.\n\nOre conjectured that no odd harmonic divisor numbers exist other than 1. If the conjecture is true, this would imply the nonexistence of odd perfect numbers.\n\nW. H. Mills (unpublished; see Muskat) showed that any odd harmonic divisor number above 1 must have a prime power factor greater than 10, and Cohen showed that any such number must have at least three different prime factors. showed that there are no odd harmonic divisor numbers smaller than 10.\n\nCohen, Goto, and others starting with Ore himself have performed computer searches listing all small harmonic divisor numbers. From these results, lists are known of all harmonic divisor numbers up to 2×10, and all harmonic divisor numbers for which the harmonic mean of the divisors is at most 300.\n\n"}
{"id": "1573991", "url": "https://en.wikipedia.org/wiki?curid=1573991", "title": "Hilbert's fourth problem", "text": "Hilbert's fourth problem\n\nIn mathematics, Hilbert's fourth problem in the 1900 Hilbert problems is a foundational question in geometry. In one statement derived from the original, it was to find geometries whose axioms are closest to those of Euclidean geometry if the ordering and incidence axioms are retained, the congruence axioms weakened, and the equivalent of the parallel postulate omitted. The original statement of Hilbert has been judged too vague to admit a definitive answer. Nevertheless, a solution was sought with the German mathematician Georg Hamel being the first who tried to solve the problem. A recognized solution for dimensions 2 and 3 was given by Armenian mathematician Rouben V. Ambartzumian.\n\nHilbert discusses the existence of non-Euclidean geometry and non-Archimedean geometry, as well as the idea that a 'straight line' is defined as the shortest path between two points. He mentions how congruence of triangles is necessary for Euclid's proof that a straight line in the plane is the shortest distance between two points. He summarizes as follows:\nThe theorem of the straight line as the shortest distance between two points and the essentially equivalent theorem of Euclid about the sides of a triangle, play an important part not only in number theory but also in the theory of surfaces and in the calculus of variations. For this reason, and because I believe that the thorough investigation of the conditions for the validity of this theorem will throw a new light upon the idea of distance, as well as upon other elementary ideas, e. g., upon the idea of the plane, and the possibility of its definition by means of the idea of the straight line, \"the construction and systematic treatment of the geometries here possible seem to me desirable.\"\nOne popular interpretation of this problem is that it is asking for all metrics on convex portions of the plane where the geodesics are straight Euclidean lines.\n\nThe solution of Hilbert's fourth problem in dimension 2 was obtained in 1976 by Rouben V. Ambartzumian in the framework of his theory of combinatorial integral geometry by application of measure continuation starting from \"Buffonic\" valuations in the space of lines in the plane. Recently (2014) an attempt was made by Ambartzumian to apply the same techniques starting from similar valuations that live in the space of planes in 3 dimensional Euclidean space. The paper puts forward the concept of \"wedge metrics\" and formulates some conditions for a wedge metric to generate a measure in the space of planes. The definition of a wedge metrics is based on certain tetrahedral inequalities of combinatorial nature. The latter inequalities replace the usual triangle inequality.\n\nA gnomonic map projection of the sphere displays all great circles as straight lines, resulting in any line segment on a gnomonic map showing the shortest route between the segment's two endpoints. This is achieved by casting surface points of the sphere onto a tangent plane, each landing where a ray from the center of the earth passes through the point on the surface and then on to the plane.\n\nThis projection allows one to give a spherical metric to the portion of the plane it maps onto.\n\nIn geometry, the Klein disk model is a model of 2-dimensional hyperbolic geometry in which points are represented by the points in the interior of the unit disk and lines are represented by the chords, straight line segments with endpoints on the boundary circle.\n\n"}
{"id": "1071730", "url": "https://en.wikipedia.org/wiki?curid=1071730", "title": "Hyperbolic quaternion", "text": "Hyperbolic quaternion\n\nIn abstract algebra, the algebra of hyperbolic quaternions is a nonassociative algebra over the real numbers with elements of the form\nwhere multiplication is determined with rules that are similar to multiplication in the quaternions.\n\nThe four-dimensional algebra of hyperbolic quaternions incorporates some of the features of the older and larger algebra of biquaternions. They both contain subalgebras isomorphic to the split-complex number plane. Furthermore, just as the quaternion algebra H can be viewed as a union of complex planes, so the hyperbolic quaternion algebra is a union of split-complex number planes sharing the same real line.\n\nIt was Alexander Macfarlane who promoted this concept in the 1890s as his \"Algebra of Physics\", first through the American Association for the Advancement of Science in 1891, then through his 1894 book of five \"Papers in Space Analysis\", and in a series of lectures at Lehigh University in 1900.\n\nLike the quaternions, the set of hyperbolic quaternions form a vector space over the real numbers of dimension 4. A linear combination\n\nis a hyperbolic quaternion when formula_3 and formula_4 are real numbers and the basis set formula_5 has these products:\n\nUsing the distributive property, these relations can be used to multiply any two hyperbolic quaternions.\n\nUnlike the ordinary quaternions, the hyperbolic quaternions are not associative. For example, formula_10, while formula_11. In fact, this example shows that the hyperbolic quaternions are not even an alternative algebra.\n\nThe first three relations show that products of the (non-real) basis elements are anti-commutative. Although this basis set does not form a group, the set\n\nforms a quasigroup. One also notes that any subplane of the set \"M\" of hyperbolic quaternions that contains the real axis forms a plane of split-complex numbers. If\n\nis the conjugate of formula_14, then the product\n\nis the quadratic form used in spacetime theory. In fact, for events \"p\" and \"q\", the bilinear form \narises as the negative of the real part of the hyperbolic quaternion product \"pq\"*, and is used in Minkowski space.\n\nNote that the set of units U = {\"q\" : \"qq\"* ≠ 0 } is \"not\" closed under multiplication. See the references (external link) for details.\n\nThe hyperbolic quaternions form a nonassociative ring; the failure of associativity in this algebra curtails the facility of this algebra in transformation theory. Nevertheless, \nthis algebra put a focus on analytical kinematics by suggesting a mathematical model:\nWhen one selects a unit vector \"r\" in the hyperbolic quaternions, then \"r\" = +1. The plane formula_17 with hyperbolic quaternion multiplication is a commutative and associative subalgebra isomorphic to the split-complex number plane.\nThe hyperbolic versor formula_18 transforms D by\nSince the direction \"r\" in space is arbitrary, this hyperbolic quaternion multiplication can express any Lorentz boost using the parameter \"a\" called rapidity. However, the hyperbolic quaternion algebra is deficient for representing the full Lorentz group (see biquaternion instead).\n\nWriting in 1967 about the dialogue on vector methods in the 1890s, a historian commented\n\nLater, Macfarlane published an article in the \"Proceedings of the Royal Society of Edinburgh\" in 1900. In it he treats a model for hyperbolic space H on the hyperboloid\n\nThis isotropic model is called the hyperboloid model and consists of all the hyperbolic versors in the ring of hyperbolic quaternions.\n\nThe 1890s felt the influence of the posthumous publications of W. K. Clifford and the \"continuous groups\" of Sophus Lie. An example of a one-parameter group is the hyperbolic versor with the hyperbolic angle parameter. This parameter is part of the polar decomposition of a split-complex number. But it is a startling aspect of finite mathematics that makes the hyperbolic quaternion ring different:\n\nThe basis formula_22 of the vector space of hyperbolic quaternions is not closed under multiplication: for example, formula_23. Nevertheless, the set formula_24 is closed under multiplication. It satisfies all the properties of an abstract group except the associativity property; being finite, it is a Latin square or quasigroup, a peripheral mathematical structure. Loss of the associativity property of multiplication as found in quasigroup theory is not consistent with linear algebra since all linear transformations compose in an associative manner. Yet physical scientists were calling in the 1890s for mutation of the squares of formula_25,formula_26, and formula_27 to be formula_28 instead of formula_29 :\nThe Yale University physicist Willard Gibbs had pamphlets with the plus one square in his three-dimensional vector system. Oliver Heaviside in England wrote columns in the \"Electrician\", a trade paper, advocating the positive square. In 1892 he brought his work together in \"Transactions of the Royal Society A\" where he says his vector system is\n\nSo the appearance of Macfarlane's hyperbolic quaternions had some motivation, but the disagreeable non-associativity precipitated a reaction. Cargill Gilston Knott was moved to offer the following:\n\nTheorem (Knott 1892)\n\nProof:\n\nThis theorem needed statement to justify resistance to the call of the physicists and the \"Electrician\". The quasigroup stimulated a considerable stir in the 1890s: the journal \"Nature\" was especially conducive to an exhibit of what was known by giving two digests of Knott's work as well as those of several other vector theorists. Michael J. Crowe devotes chapter six of his book \"A History of Vector Analysis\" to the various published views, and notes the hyperbolic quaternion:\n\nIn 1899 Charles Jasper Joly noted the hyperbolic quaternion and the non-associativity property while ascribing its origin to Oliver Heaviside.\n\nThe hyperbolic quaternions, as the \"Algebra of Physics\", undercut the claim that ordinary quaternions made on physics. As for mathematics, the hyperbolic quaternion is another hypercomplex number, as such structures were called at the time. By the 1890s Richard Dedekind had introduced the ring concept into commutative algebra, and the vector space concept was being abstracted by Giuseppe Peano. In 1899 Alfred North Whitehead promoted Universal algebra, advocating for inclusivity. The concepts of quasigroup and algebra over a field are examples of mathematical structures describing hyperbolic quaternions.\n\nThe \"Proceedings of the Royal Society of Edinburgh\" published \"Hyperbolic Quaternions\" \nin 1900, a paper in which Macfarlane regains associativity for multiplication by reverting \nto complexified quaternions. While there he used some expressions later \nmade famous by Wolfgang Pauli: where Macfarlane wrote\nthe Pauli matrices satisfy\nwhile referring to the same complexified quaternions.\n\nThe opening sentence of the paper is \"It is well known that quaternions are intimately connected with spherical trigonometry and in fact they reduce the subject to a branch of algebra.\" This statement may be verified by reference to the contemporary work \"Vector Analysis\" which works with a reduced quaternion system based on dot product and cross product. In Macfarlane's paper there is an effort to produce \"trigonometry on the surface of the equilateral hyperboloids\" through the algebra of hyperbolic quaternions, now re-identified in an associative ring of eight real dimensions. The effort is reinforced by a plate of nine figures on page 181. They illustrate the descriptive power of his \"space analysis\" method. For example, figure 7 is the \ncommon Minkowski diagram used today in special relativity to discuss change of velocity of a frame of reference and relativity of simultaneity.\n\nOn page 173 Macfarlane expands on his greater theory of quaternion variables. By way of contrast he notes that Felix Klein appears not to look beyond the theory of Quaternions and spatial rotation.\n\n"}
{"id": "259105", "url": "https://en.wikipedia.org/wiki?curid=259105", "title": "Independence of irrelevant alternatives", "text": "Independence of irrelevant alternatives\n\nThe independence of irrelevant alternatives (IIA), also known as binary independence or the independence axiom, is an axiom of decision theory and various social sciences. The term is used with different meanings in different contexts; although they all attempt to provide an account of rational individual behavior or aggregation of individual preferences, the exact formulations differ from context to context.\n\nIn individual choice theory, IIA sometimes refers to Chernoff's condition or Sen's property α (alpha):\nif an alternative \"x\" is chosen from a set \"T\", and \"x\" is also an element of a subset \"S\" of \"T\", then \"x\" must be chosen from \"S\". That is, eliminating some of the unchosen alternatives shouldn't affect the selection of \"x\" as the best option.\n\nIn social choice theory, Arrow's IIA is one of the conditions in Arrow's impossibility theorem, which states that it is impossible to aggregate individual rank-order preferences (\"votes\") satisfying IIA in addition to certain other reasonable conditions. Arrow defines IIA thus:\n\nAnother expression of the principle:\nIn other words, preferences for \"A\" or \"B\" should not be changed by the inclusion of \"X\", i.e., \"X\" is irrelevant to the choice between \"A\" and \"B\". This formulation appears in bargaining theory, theories of individual choice, and voting theory. Some theorists find it too strict an axiom; experiments have shown that human behavior rarely adheres to this axiom (see ). \n\nIn social choice theory, IIA is also defined as:\nIn other words, whether \"A\" or \"B\" is selected should not be affected by a change in the vote for an unavailable \"X\", which is irrelevant to the choice between \"A\" and \"B\".\n\nIn voting systems, independence from irrelevant alternatives is often interpreted as, if one candidate (\"X\") would win an election, and if a new candidate (\"Y\") were added to the ballot, then either \"X\" or \"Y\" would win the election.\n\nApproval voting, range voting, and majority judgment satisfy the IIA criterion if it is assumed that voters rate candidates individually and independently of knowing the available alternatives in the election, using their own absolute scale. This assumption implies that some voters having meaningful preferences in an election with only two alternatives will necessarily cast a vote which has little or no voting power, or necessarily abstain. If it is assumed to be at least possible that any voter having preferences might not abstain, or vote their favorite and least favorite candidates at the top and bottom ratings respectively, then these systems fail IIA. Allowing either of these conditions alone causes failure. Another cardinal system, cumulative voting, does not satisfy the criterion regardless of either assumption.\n\nAn anecdote that illustrates a violation of IIA has been attributed to Sidney Morgenbesser:\n\nAll voting systems have some degree of inherent susceptibility to strategic nomination considerations. Some regard these considerations as less serious unless the voting system fails the easier-to-satisfy independence of clones criterion.\n\nA criterion weaker than IIA proposed by H. Peyton Young and A. Levenglick is called local independence from irrelevant alternatives (LIIA).\nLIIA requires that both of the following conditions always hold:\n\nAn equivalent way to express LIIA is that if a subset of the options are in consecutive positions in the order of finish, then their relative order of finish must not change if all other options are deleted from the votes. For example, if all options except those in 3rd, 4th and 5th place are deleted, the option that finished 3rd must win, the 4th must finish second, and 5th must finish 3rd.\n\nAnother equivalent way to express LIIA is that if two options are consecutive in the order of finish, the one that finished higher must win if all options except those two are deleted from the votes.\n\nLIIA is weaker than IIA because satisfaction of IIA implies satisfaction of LIIA, but not vice versa.\n\nDespite being a weaker criterion (i.e. easier to satisfy) than IIA, LIIA is satisfied by very few voting methods. These include Kemeny-Young and ranked pairs, but not Schulze. Just as with IIA, LIIA compliance for rating methods such as approval voting, range voting, and majority judgment require the assumption that voters rate each alternative individually and independently of knowing any other alternatives, on an absolute scale (calibrated prior to the election), even when this assumption implies that voters having meaningful preferences in a two candidate election will necessarily abstain.\n\nIIA is too strong to be satisfied by any voting method capable of majority rule unless there are only two alternatives.\n\nConsider a scenario in which there are three candidates \"A\", \"B\", & \"C\", and the voters' preferences are as follows:\n\n75% prefer \"C\" over \"A\", 65% prefer \"B\" over \"C\", and 60% prefer \"A\" over \"B\". The presence of this societal intransitivity is the voting paradox. Regardless of the voting method and the actual votes, there are only three cases to consider:\n\nTo show failure, it is only assumed at least possible that enough voters in the majority might cast a minimally positive vote for their preferred candidate when there are only two candidates, rather than abstain. Most ranked ballot methods and Plurality voting satisfy the Majority Criterion, and therefore fail IIA automatically by the example above. Meanwhile, passage of IIA by Approval and Range voting requires in certain cases that voters in the majority are necessarily excluded from voting (they are assumed to necessarily abstain in a two candidate race, despite having a meaningful preference between the alternatives).\n\nSo even if IIA is desirable, requiring its satisfaction seems to allow only voting methods that are undesirable in some other way, such as treating one of the voters as a dictator. Thus the goal must be to find which voting methods are best, rather than which are perfect.\n\nAn argument can be made that IIA is itself undesirable. IIA assumes that when deciding whether \"A\" is likely to be better than \"B\", information about voters' preferences regarding \"C\" is irrelevant and should not make a difference. However, the heuristic that leads to majority rule when there are only two options is that the larger the number of people who think one option is better than the other, the greater the likelihood that it is better, all else being equal (see Condorcet's Jury Theorem). A majority is more likely than the opposing minority to be right about which of the two candidates is better, all else being equal, hence the use of majority rule.\n\nThe same heuristic implies that the larger the majority, the more likely it is that they are right. It would seem to also imply that when there is more than one majority, larger majorities are more likely to be right than smaller majorities. Assuming this is so, the 75% who prefer \"C\" over \"A\" and the 65% who prefer \"B\" over \"C\" are more likely to be right than the 60% who prefer \"A\" over \"B\", and since it is not possible for all three majorities to be right, the smaller majority (who prefer \"A\" over \"B\") are more likely to be wrong, and less likely than their opposing minority to be right. Rather than being irrelevant to whether \"A\" is better than \"B\", the additional information about the voters' preferences regarding \"C\" provides a strong hint that this is a situation where all else is not equal.\n\nFrom Kenneth Arrow, each \"voter\" \"i\" in the society has an ordering R that ranks the (conceivable) objects of social choice—\"x\", \"y\", and \"z\" in simplest case—from high to low.\nAn \"aggregation rule\" (\"voting rule\") in turn maps each \"profile\" or tuple (R, ...,R) of voter preferences (orderings)\nto a \"social ordering\" R that determines the social preference (ranking) of \"x\", \"y\", and \"z\".\n\nArrow's IIA requires that whenever a pair of alternatives is ranked the same way in two preference profiles (over the same choice set), then the aggregation rule must order these alternatives identically across the two profiles.\nFor example, suppose an aggregation rule ranks \"a\" above \"b\" at the profile given by \n(i.e., the first individual prefers \"a\" first, \"c\" second, \"b\" third, \"d\" last; the second individual prefers \"d\" first, ..., and \"c\" last). Then, if it satisfies IIA, it must rank \"a\" above \"b\" at the following three profiles:\nThe last two forms of profiles (placing the two at the top; and placing the two at the top and bottom) are especially useful\nin the proofs of theorems involving IIA.\n\nArrow's IIA does not imply an IIA similar to those different from this at the top of this article nor conversely.\n\nIn the first edition of his book, Arrow misinterpreted IIA by considering the removal of a choice from the consideration set. Among the objects of choice, he distinguished those that by hypothesis are specified as \"feasible\" and \"infeasible\". Consider two possible sets of voter orderings (\"formula_1, ...,formula_2 \") and (\"formula_3, ...,formula_4\") such that the ranking of \"X\" and \"Y\" for each voter \"i\" is the same for \"formula_5\" and \"formula_6\". The voting rule generates corresponding social orderings \"R\" and \"R'.\" Now suppose that \"X\" and \"Y\" are feasible but \"Z\" is infeasible (say, the candidate is not on the ballot or the social state is outside the production possibility curve). Arrow required that the voting rule that \"R\" and \"R' \"select the same (top-ranked) \"social choice\" from the feasible set (X, Y), and that this requirement holds no matter what the ranking is of infeasible \"Z\" relative to \"X\" and \"Y\" in the two sets of orderings. IIA does not allow \"removing\" an alternative from the available set (a candidate from the ballot), and it says nothing about what would happen in such a case: all options are assumed to be \"feasible.\"\n\nIn a Borda count election, 5 voters rank 5 alternatives [\"A\", \"B\", \"C\", \"D\", \"E\"].\n\n3 voters rank [\"A\">\"B\">\"C\">\"D\">\"E\"].\n1 voter ranks [\"C\">\"D\">\"E\">\"B\">\"A\"].\n1 voter ranks [\"E\">\"C\">\"D\">\"B\">\"A\"].\n\nBorda count (\"a\"=0, \"b\"=1): \"C\"=13, \"A\"=12, \"B\"=11, \"D\"=8, \"E\"=6. \"C\" wins.\n\nNow, the voter who ranks [\"C\">\"D\">\"E\">\"B\">\"A\"] instead ranks [\"C\">\"B\">\"E\">\"D\">\"A\"]; and the voter who ranks [\"E\">\"C\">\"D\">\"B\">\"A\"] instead ranks [\"E\">\"C\">\"B\">\"D\">\"A\"]. They change their preferences only over the pairs [\"B\", \"D\"], [\"B\", \"E\"] and [\"D\", \"E\"].\n\nThe new Borda count: \"B\"=14, \"C\"=13, \"A\"=12, \"E\"=6, \"D\"=5. \"B\" wins.\n\nThe social choice has changed the ranking of [\"B\", \"A\"] and [\"B\", \"C\"]. The changes in the social choice ranking are dependent on irrelevant changes in the preference profile. In particular, \"B\" now wins instead of \"C\", even though no voter changed their preference over [\"B\", \"C\"].\n\nConsider an election in which there are three candidates, \"A\", \"B\", and \"C\", and only two voters. Each voter ranks the candidates in order of preference. The highest ranked candidate in a voter's preference is given 2 points, the second highest 1, and the lowest ranked 0; the overall ranking of a candidate is determined by the total score it gets; the highest ranked candidate wins.\n\nWe consider two profiles:\n\nThus, if the second voter wishes \"A\" to be elected, he had better vote \"ACB\" regardless of his actual opinion of \"C\" and \"B\". This violates the idea of \"independence from irrelevant alternatives\" because the voter's comparative opinion of \"C\" and \"B\" affects whether \"A\" is elected or not. In both profiles, the rankings of \"A\" relative to \"B\" are the same for each voter, but the social rankings of \"A\" relative to \"B\" are different.\n\nThis example shows that Copeland's method violates IIA. Assume four candidates A, B, C and D with 6 voters with the following preferences:\nThe results would be tabulated as follows:\n\nResult: A has two wins and one defeat, while no other candidate has more wins than defeats. Thus, A is elected Copeland winner.\n\nNow, assume all voters would raise D over B and C without changing the order of A and D. The preferences of the voters would now be:\n\nThe results would be tabulated as follows:\nResult: D wins against all three opponents. Thus, D is elected Copeland winner.\n\nThe voters changed only their preference orders over B, C and D. As a result, the outcome order of D and A changed. A turned from winner to loser without any change of the voters' preferences regarding A. Thus, Copeland's method fails the IIA criterion.\n\nIn an instant-runoff election, 5 voters rank 3 alternatives [\"A\", \"B\", \"C\"].\n\n2 voters rank [\"A\">\"B\">\"C\"].\n2 voters rank [\"C\">\"B\">\"A\"].\n1 voter ranks [\"B\">\"A\">\"C\"].\n\nRound 1: \"A\"=2, \"B\"=1, \"C\"=2; \"B\" eliminated.\nRound 2: \"A\"=3, \"C\"=2; \"A\" wins.\n\nNow, the two voters who rank [\"C\">\"B\">\"A\"] instead rank [\"B\">\"C\">\"A\"]. They change only their preferences over \"B\" and \"C\".\n\nRound 1: \"A\"=2, \"B\"=3, \"C\"=0; \"B\" wins with a majority of the vote.\n\nThe social choice ranking of [\"A\", \"B\"] is dependent on preferences over the irrelevant alternatives [\"B\", \"C\"].\n\nThis example shows that the Kemeny–Young method violates the IIA criterion. Assume three candidates A, B and C with 7 voters and the following preferences:\nThe Kemeny–Young method arranges the pairwise comparison counts in the following tally table:\nThe ranking scores of all possible rankings are:\nResult: The ranking A > B > C has the highest ranking score. Thus, A wins ahead of B and C.\n\nNow, assume the two voters (marked bold) with preferences B > C > A would change their preferences over the pair B and C. The preferences of the voters would then be in total:\nThe Kemeny–Young method arranges the pairwise comparison counts in the following tally table:\nThe ranking scores of all possible rankings are:\nResult: The ranking C > A > B has the highest ranking score. Thus, C wins ahead of A and B.\n\nThe two voters changed only their preferences over B and C, but this resulted in a change of the order of A and C in the result, turning A from winner to loser without any change of the voters' preferences regarding A. Thus, the Kemeny-Young method fails the IIA criterion.\n\nThis example shows that the Minimax method violates the IIA criterion. Assume four candidates A, B and C and 13 voters with the following preferences:\n\nSince all preferences are strict rankings (no equals are present), all three Minimax methods (winning votes, margins and pairwise opposite) elect the same winners.\n\nThe results would be tabulated as follows:\n\nResult: A has the closest biggest defeat. Thus, A is elected Minimax winner.\n\nNow, assume the two voters (marked bold) with preferences B > A > C change the preferences over the pair A and C. The preferences of the voters would then be in total:\nThe results would be tabulated as follows:\nResult: Now, B has the closest biggest defeat. Thus, B is elected Minimax winner.\n\nSo, by changing the order of A and C in the preferences of some voters, the order of A and B in the result changed. B is turned from loser to winner without any change of the voters' preferences regarding B. Thus, the Minimax method fails the IIA criterion.\n\nIn a plurality voting system 7 voters rank 3 alternatives (\"A\", \"B\", \"C\").\n\n\nIn an election, initially only \"A\" and \"B\" run: \"B\" wins with 4 votes to \"A\"'s 3, but the entry of \"C\" into the race makes \"A\" the new winner.\n\nThe relative positions of \"A\" and \"B\" are reversed by the introduction of \"C\", an \"irrelevant\" alternative.\n\nThis example shows that the Ranked pairs method violates the IIA criterion. Assume three candidates A, B and C and 7 voters with the following preferences:\nThe results would be tabulated as follows:\nThe sorted list of victories would be:\nResult: A > B and B > C are locked in (and C > A cannot be locked in after that), so the full ranking is A > B > C. Thus, A is elected Ranked pairs winner.\n\nNow, assume the two voters (marked bold) with preferences B > C > A change their preferences over the pair B and C. The preferences of the voters would then be in total:\nThe results would be tabulated as follows:\nThe sorted list of victories would be:\nResult: All three duels are locked in, so the full ranking is C > A > B. Thus, the Condorcet winner C is elected Ranked pairs winner.\n\nSo, by changing their preferences over B and C, the two voters changed the order of A and C in the result, turning A from winner to loser without any change of the voters' preferences regarding A. Thus, the Ranked pairs method fails the IIA criterion.\n\nThis example shows that the Schulze method violates the IIA criterion. Assume four candidates A, B, C and D and 12 voters with the following preferences:\nThe pairwise preferences would be tabulated as follows:\nNow, the strongest paths have to be identified, e.g. the path D > A > B is stronger than the direct path D > B (which is nullified, since it is a tie).\nResult: The full ranking is C > D > A > B. Thus, C is elected Schulze winner and D is preferred over A.\n\nNow, assume the two voters (marked bold) with preferences C > B > D > A change their preferences over the pair B and C. The preferences of the voters would then be in total:\nHence, the pairwise preferences would be tabulated as follows:\nNow, the strongest paths have to be identified:\nResult: Now, the full ranking is A > B > C > D. Thus, A is elected Schulze winner and is preferred over D.\n\nSo, by changing their preferences over B and C, the two voters changed the order of A and D in the result, turning A from loser to winner without any change of the voters' preferences regarding A. Thus, the Schulze method fails the IIA criterion.\n\nA probable example of the two-round system failing this criterion was the 2002 French presidential election. Polls leading up to the election have suggested a runoff between centre-right candidate Jacques Chirac and centre-left candidate Lionel Jospin, in which Jospin has been expected to win. However, the first round was contested by an unprecedented 16 candidates, including left-wing candidates who intended to support Jospin in the runoff, eventually resulting in the far-right candidate, Jean-Marie Le Pen, finishing second and entering the runoff instead of Jospin, which Chirac won by a large margin. Thus, the presence of many candidates who did not intend to win in the election changed which of the candidates won.\n\nIIA implies that adding another option or changing the characteristics of a third option does not affect the relative odds between the two options considered. This implication is not realistic for applications with similar options. Many examples have been constructed to illustrate this problem.\n\nConsider the Red Bus/Blue Bus example. Commuters face a decision between car and red bus. Suppose that a commuter chooses between these two options with equal probability, 0.5, so that the odds ratio equals 1:1. Now suppose a third mode, blue bus, is added. Assuming bus commuters do not care about the color of the bus, they are expected to choose between bus and car still with equal probability, so the probability of car is still 0.5, while the probability of each of the two bus types is 0.25. But IIA implies that this is not the case: for the odds ratio between car and red bus to be preserved, and the odds of red and blue bus to be equal (in other words, the commuter is indifferent to color), the new probabilities must be car 0.33; red bus 0.33; blue bus 0.33. The blue bus is of course not irrelevant if it is chosen, but it must be treated as irrelevant when it is not chosen, leading to a decreased overall probability of car travel, which does not make sense for a commuter who does not care about colors. In intuitive terms, the problem with the IIA axiom is that it leads to a failure to take account of the fact that red bus and blue bus are very similar, and are \"perfect substitutes\".\n\nIIA is a property assumed by the multinomial logit and the conditional logit models in econometrics. If these models are used in situations which in fact violate independence (such as multicandidate elections in which preferences exhibit cycling or situations mimicking the Red Bus/Blue Bus example given above) then these estimators become invalid.\n\nMany modeling advances have been motivated by a desire to alleviate the concerns raised by IIA. Generalized extreme value, multinomial probit (also called conditional probit) and mixed logit are models for nominal outcomes that relax IIA, but they often have assumptions of their own that may be difficult to meet or are computationally infeasible. The multinomial probit model has as a disadvantage that it makes calculation of maximum likelihood infeasible for more than five options as it involves multiple integrals. IIA can be relaxed by specifying a hierarchical model, ranking the choice alternatives. The most popular of these is the nested logit model.\n\nGeneralized extreme value and multinomial probit models possess another property, the Invariant Proportion of Substitution, which suggests similarly counterintuitive individual choice behavior.\n\nIn the expected utility theory of von Neumann and Morgenstern, four axioms together imply that individuals act in situations of risk as if they maximize the expected value of a utility function. One of the axioms is an independence axiom analogous to the IIA axiom:\n\nwhere \"p\" is a probability, \"pL\"+(1-\"p\")\"N\" means a gamble with probability \"p\" of yielding \"L\" and probability (1-\"p\") of yielding \"N\", and formula_7 means that \"M\" is preferred over \"L\". This axiom says that if one outcome (or lottery ticket) \"L\" is considered to be not as good as another (\"M\"), then having a chance with probability \"p\" of receiving \"L\" rather than \"N\" is considered to be not as good as having a chance with probability \"p\" of receiving \"M\" rather than \"N\".\n\nNatural selection can favor animals' non-IIA-type choices, thought to be due to occasional availability of foodstuffs, according to a study published in January 2014.\n\n\n\n"}
{"id": "49172", "url": "https://en.wikipedia.org/wiki?curid=49172", "title": "Interval (mathematics)", "text": "Interval (mathematics)\n\nIn mathematics, a (real) interval is a set of real numbers with the property that any number that lies between two numbers in the set is also included in the set. For example, the set of all numbers satisfying is an interval which contains and , as well as all numbers between them. Other examples of intervals are the set of all real numbers formula_1, the set of all negative real numbers, and the empty set.\n\nReal intervals play an important role in the theory of integration, because they are the simplest sets whose \"size\" or \"measure\" or \"length\" is easy to define. The concept of measure can then be extended to more complicated sets of real numbers, leading to the Borel measure and eventually to the Lebesgue measure.\n\nIntervals are central to interval arithmetic, a general numerical computing technique that automatically provides guaranteed enclosures for arbitrary formulas, even in the presence of uncertainties, mathematical approximations, and arithmetic roundoff.\n\nIntervals are likewise defined on an arbitrary totally ordered set, such as integers or rational numbers. The notation of integer intervals is considered in the special section below.\n\nAn open interval does not include its endpoints, and is indicated with parentheses. For example, means greater than and less than . A closed interval is an interval which includes all its limit points, and is denoted with square brackets. For example, means greater than or equal to and less than or equal to . A half-open interval includes only one of its endpoints, and is denoted by mixing the notations for open and closed intervals. means greater than and less than or equal to , while means greater than or equal to and less than .\n\nA degenerate interval is any set consisting of a single real number. Some authors include the empty set in this definition. A real interval that is neither empty nor degenerate is said to be proper, and has infinitely many elements.\n\nAn interval is said to be left-bounded or right-bounded if there is some real number that is, respectively, smaller than or larger than all its elements. An interval is said to be bounded if it is both left- and right-bounded; and is said to be unbounded otherwise. Intervals that are bounded at only one end are said to be half-bounded. The empty set is bounded, and the set of all reals is the only interval that is unbounded at both ends. Bounded intervals are also commonly known as finite intervals.\n\nBounded intervals are bounded sets, in the sense that their diameter (which is equal to the absolute difference between the endpoints) is finite. The diameter may be called the length, width, measure, or size of the interval. The size of unbounded intervals is usually defined as , and the size of the empty interval may be defined as or left undefined.\n\nThe centre (midpoint) of bounded interval with endpoints and is , and its radius is the half-length . These concepts are undefined for empty or unbounded intervals.\n\nAn interval is said to be left-open if and only if it contains no minimum (an element that is smaller than all other elements); right-open if it contains no maximum; and open if it has both properties. The interval  = , for example, is left-closed and right-open. The empty set and the set of all reals are open intervals, while the set of non-negative reals, for example, is a right-open but not left-open interval. The open intervals coincide with the open sets of the real line in its standard topology.\n\nAn interval is said to be left-closed if it has a minimum element, right-closed if it has a maximum, and simply closed if it has both. These definitions are usually extended to include the empty set and to the (left- or right-) unbounded intervals, so that the closed intervals coincide with closed sets in that topology.\n\nThe interior of an interval is the largest open interval that is contained in ; it is also the set of points in which are not endpoints of . The closure of is the smallest closed interval that contains ; which is also the set augmented with its finite endpoints.\n\nFor any set of real numbers, the interval enclosure or interval span of is the unique interval that contains and does not properly contain any other interval that also contains .\n\nThe terms segment and interval have been employed in the literature in two essentially opposite ways, resulting in ambiguity when these terms are used. The \"Encyclopedia of Mathematics\" defines \"interval\" (without a qualifier) to exclude both endpoints (i.e., open interval) and \"segment\" to include both endpoints (i.e., closed interval), while Rudin's \"Principles of Mathematical Analysis\" calls sets of the form [\"a\", \"b\"] \"intervals\" and sets of the form (\"a\", \"b\") \"segments\" throughout. These terms tend to appear in older works; modern texts increasingly favor the term \"interval\" (qualified by \"open\", \"closed\", or \"half-open\"), regardless of whether endpoints are included.\n\nThe interval of numbers between and , including and , is often denoted . The two numbers are called the \"endpoints\" of the interval. In countries where numbers are written with a decimal comma, a semicolon may be used as a separator, to avoid ambiguity.\n\nTo indicate that one of the endpoints is to be excluded from the set, the corresponding square bracket can be either replaced with a parenthesis, or reversed. Both notations are described in International standard ISO 31-11. Thus, in set builder notation,\nNote that , , and each represents the empty set, whereas denotes the set . When , all four notations are usually taken to represent the empty set.\n\nBoth notations may overlap with other uses of parentheses and brackets in mathematics. For instance, the notation is often used to denote an ordered pair in set theory, the coordinates of a point or vector in analytic geometry and linear algebra, or (sometimes) a complex number in algebra. That is why Bourbaki introduced the notation to denote the open interval. The notation too is occasionally used for ordered pairs, especially in computer science.\n\nSome authors use to denote the complement of the interval ; namely, the set of all real numbers that are either less than or equal to , or greater than or equal to .\n\nIn some contexts, an interval may be defined as a subset of the extended real numbers, the set of all real numbers augmented with and .\n\nIn this interpretation, the notations  ,  ,  , and are all meaningful and distinct. In particular, denotes the set of all ordinary real numbers, while denotes the extended reals. \n\nEven in the context of the ordinary reals, one may use an infinite endpoint to indicate that there is no bound in that direction. For example, is the set of positive real numbers also written ℝ. The context affects some of the above definitions and terminology. For instance, the interval  = formula_1 is closed in the realm of ordinary reals, but not in the realm of the extended reals.\n\nThe notation when and are integers, or , or just is sometimes used to indicate the interval of all \"integers\" between and , including both. This notation is used in some programming languages; in Pascal, for example, it is used to formally define a subrange type, most frequently used to specify lower and upper bounds of valid indices of an array.\n\nAn integer interval that has a finite lower or upper endpoint always includes that endpoint. Therefore, the exclusion of endpoints can be explicitly denoted by writing  ,  , or . Alternate-bracket notations like or are rarely used for integer intervals.\n\nThe intervals of real numbers can be classified into the eleven different types listed below, where and are real numbers, and formula_4:\n\nThe intervals are precisely the connected subsets of formula_1. It follows that the image of an interval by any continuous function is also an interval. This is one formulation of the intermediate value theorem.\n\nThe intervals are also the convex subsets of formula_1. The interval enclosure of a subset formula_18 is also the convex hull of formula_19.\n\nThe intersection of any collection of intervals is always an interval. The union of two intervals is an interval if and only if they have a non-empty intersection or an open end-point of one interval is a closed end-point of the other (e.g., formula_20).\n\nIf formula_1 is viewed as a metric space, its open balls are the open bounded sets , and its closed balls are the closed bounded sets .\n\nAny element  of an interval  defines a partition of  into three disjoint intervals , , : respectively, the elements of  that are less than , the singleton formula_22, and the elements that are greater than . The parts and are both non-empty (and have non-empty interiors) if and only if is in the interior of . This is an interval version of the trichotomy principle.\n\nA \"dyadic interval\" is a bounded real interval whose endpoints are formula_23 and formula_24, where formula_25 and formula_26 are integers. Depending on the context, either endpoint may or may not be included in the interval.\n\nDyadic intervals have the following properties:\n\n\nThe dyadic intervals consequently have a structure that reflects that of an infinite binary tree.\n\nDyadic intervals are relevant to several areas of numerical analysis, including adaptive mesh refinement, multigrid methods and wavelet analysis. Another way to represent such a structure is p-adic analysis (for ).\n\nIn many contexts, an formula_27-dimensional interval is defined as a subset of formula_28 that is the Cartesian product of formula_27 intervals, formula_30, one on each coordinate axis.\n\nFor formula_31, this can be thought of as region bounded by a square or rectangle whose sides are parallel to the coordinate axes, depending on whether the width of the intervals are the same or not; likewise, for formula_32, this can be thought of as a region bounded by an axis-aligned cube or a rectangular cuboid. \nIn higher dimensions, the Cartesian product of formula_27 intervals is bounded by an n-dimensional hypercube or hyperrectangle.\n\nA facet of such an interval formula_34 is the result of replacing any non-degenerate interval factor formula_35 by a degenerate interval consisting of a finite endpoint of formula_35. The faces of formula_34 comprise formula_34 itself and all faces of its facets. The corners of formula_34 are the faces that consist of a single point of formula_28.\n\nIntervals of complex numbers can be defined as regions of the complex plane, either rectangular or circular.\n\nIntervals can be associated with points of the plane and hence regions of intervals can be associated with regions of the plane. Generally, an interval in mathematics corresponds to an ordered pair (\"x,y\") taken from the direct product R × R of real numbers with itself. Often it is assumed that \"y\" > \"x\". For purposes of mathematical structure, this restriction is discarded, and \"reversed intervals\" where \"y\" − \"x\" < 0 are allowed. Then the collection of all intervals [\"x,y\"] can be identified with the topological ring formed by the direct sum of R with itself where addition and multiplication are defined component-wise.\n\nThe direct sum algebra formula_41 has two ideals, { [\"x\",0] : \"x\" ∈ R } and { [0,\"y\"] : \"y\" ∈ R }. The identity element of this algebra is the condensed interval [1,1]. If interval [\"x,y\"] is not in one of the ideals, then it has multiplicative inverse [1/\"x\", 1/\"y\"]. Endowed with the usual topology, the algebra of intervals forms a topological ring. The group of units of this ring consists of four quadrants determined by the axes, or ideals in this case. The identity component of this group is quadrant I.\n\nEvery interval can be considered a symmetric interval around its midpoint. In a reconfiguration published in 1956 by M Warmus, the axis of \"balanced intervals\" [\"x\", −\"x\"] is used along with the axis of intervals [\"x,x\"] that reduce to a point.\nInstead of the direct sum formula_42, the ring of intervals has been identified with the split-complex number plane by M. Warmus and D. H. Lehmer through the identification\nThis linear mapping of the plane, which amounts of a ring isomorphism, provides the plane with a multiplicative structure having some analogies to ordinary complex arithmetic, such as polar decomposition.\n\n\n\n"}
{"id": "217122", "url": "https://en.wikipedia.org/wiki?curid=217122", "title": "Invertible matrix", "text": "Invertible matrix\n\nIn linear algebra, an \"n\"-by-\"n\" square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an \"n\"-by-\"n\" square matrix B such that\n\nwhere I denotes the \"n\"-by-\"n\" identity matrix and the multiplication used is ordinary matrix multiplication. If this is the case, then the matrix B is uniquely determined by A and is called the inverse of A, denoted by A.\n\nA square matrix that is not invertible is called singular or degenerate. A square matrix is singular if and only if its determinant is 0. Singular matrices are rare in the sense that a square matrix randomly selected from a continuous uniform distribution on its entries will almost never be singular.\n\nNon-square matrices (\"m\"-by-\"n\" matrices for which ) do not have an inverse. However, in some cases such a matrix may have a left inverse or right inverse. If A is \"m\"-by-\"n\" and the rank of A is equal to \"n\", then A has a left inverse: an \"n\"-by-\"m\" matrix B such that . If A has rank \"m\", then it has a right inverse: an \"n\"-by-\"m\" matrix B such that .\n\nMatrix inversion is the process of finding the matrix B that satisfies the prior equation for a given invertible matrix A.\n\nWhile the most common case is that of matrices over the real or complex numbers, all these definitions can be given for matrices over any ring. However, in the case of the ring being commutative, the condition for a square matrix to be invertible is that its determinant is invertible in the ring, which in general is a stricter requirement than being nonzero. For a noncommutative ring, the usual determinant is not defined. The conditions for existence of left-inverse or right-inverse are more complicated since a notion of rank does not exist over rings.\n\nThe set of invertible matrices together with the operation of matrix multiplication form a group, the general linear group of degree \"n\".\n\nLet A be a square \"n\" by \"n\" matrix over a field \"K\" (for example the field R of real numbers). The following statements are equivalent, i.e., for any given matrix they are either all true or all false:\n\nFurthermore, the following properties hold for an invertible matrix A:\n\n\nA matrix that is its own inverse, i.e. such that and , is called an involutory matrix.\n\nThe adjugate of a matrix formula_2 can be used to find the inverse of formula_2 as follows:\n\nIf formula_2 is an formula_5 invertible matrix, then\n\nIt follows from the associativity of matrix multiplication that if\n\nfor \"finite square\" matrices A and B, then also\n\nOver the field of real numbers, the set of singular \"n\"-by-\"n\" matrices, considered as a subset of R, is a null set, i.e., has Lebesgue measure zero. This is true because singular matrices are the roots of the polynomial function in the entries of the matrix given by the determinant. Thus in the language of measure theory, almost all \"n\"-by-\"n\" matrices are invertible.\n\nFurthermore, the \"n\"-by-\"n\" invertible matrices are a dense open set in the topological space of all \"n\"-by-\"n\" matrices. Equivalently, the set of singular matrices is closed and nowhere dense in the space of \"n\"-by-\"n\" matrices.\n\nIn practice however, one may encounter non-invertible matrices. And in numerical calculations, matrices which are invertible, but close to a non-invertible matrix, can still be problematic; such matrices are said to be ill-conditioned.\n\nConsider the following \"2\"-by-\"2\" matrix:\nThe matrix formula_10 is invertible. To check this, one can compute that formula_11, which is non-zero. \n\nAs an example of a non-invertible, or singular, matrix, consider the matrix\nThe determinant of formula_13 is 0, which is a necessary and sufficient condition for a matrix to be non-invertible.\n\nGauss-Jordan elimination is an algorithm that can be used to determine whether a given matrix is invertible and to find the inverse. An alternative is the LU decomposition which generates upper and lower triangular matrices which are easier to invert.\n\nA generalization of Newton's method as used for a multiplicative inverse algorithm may be convenient, if it is convenient to find a suitable starting seed:\n\nVictor Pan and John Reif have done work that includes ways of generating a starting seed. Byte magazine summarised one of their approaches.\n\nNewton's method is particularly useful when dealing with families of related matrices that behave enough like the sequence manufactured for the homotopy above: sometimes a good starting point for refining an approximation for the new inverse can be the already obtained inverse of a previous matrix that nearly matches the current matrix, e.g. the pair of sequences of inverse matrices used in obtaining matrix square roots by Denman-Beavers iteration; this may need more than one pass of the iteration at each new matrix, if they are not close enough together for just one to be enough. Newton's method is also useful for \"touch up\" corrections to the Gauss–Jordan algorithm which has been contaminated by small errors due to imperfect computer arithmetic.\n\nThe Cayley–Hamilton theorem allows the inverse of formula_2 to be expressed in terms of det(formula_2), traces and powers of formula_2 \n\nwhere formula_19 is dimension of formula_2, and formula_21 is the trace of matrix formula_2 given by the sum of the main diagonal. The sum is taken over formula_23 and the sets of all formula_24 satisfying the linear Diophantine equation \nThe formula can be rewritten in terms of complete Bell polynomials of arguments \nformula_26 as\n\nIf matrix A can be eigendecomposed and if none of its eigenvalues are zero, then A is invertible and its inverse is given by\nwhere Q is the square (\"N\"×\"N\") matrix whose \"i\" column is the eigenvector formula_29 of A and Λ is the diagonal matrix whose diagonal elements are the corresponding eigenvalues, \"i.e.\", formula_30.\nFurthermore, because Λ is a diagonal matrix, its inverse is easy to calculate:\n\nIf matrix A is positive definite, then its inverse can be obtained as\nwhere L is the lower triangular Cholesky decomposition of A, and L* denotes the conjugate transpose of L.\n\nWriting the transpose of the matrix of cofactors, known as an adjugate matrix, can also be an efficient way to calculate the inverse of \"small\" matrices, but this recursive method is inefficient for large matrices. To determine the inverse, we calculate a matrix of cofactors:\n\nso that\nwhere |A| is the determinant of A, C is the matrix of cofactors, and C represents the matrix transpose.\n\nThe \"cofactor equation\" listed above yields the following result for matrices. Inversion of these matrices can be done as follows:\nThis is possible because is the reciprocal of the determinant of the matrix in question, and the same strategy could be used for other matrix sizes.\n\nThe Cayley–Hamilton method gives\n\nA computationally efficient matrix inversion is given by\n(where the scalar \"A\" is not to be confused with the matrix A).\nIf the determinant is non-zero, the matrix is invertible, with the elements of the intermediary matrix on the right side above given by\nThe determinant of A can be computed by applying the rule of Sarrus as follows:\n\nThe Cayley–Hamilton decomposition gives\nThe general inverse can be expressed concisely in terms of the cross product and triple product. If a matrix formula_41 (consisting of three column vectors, formula_42, formula_43, and formula_44) is invertible, its inverse is given by \nNote that formula_46 is equal to the triple product of formula_47, formula_48, and formula_49—the volume of the parallelepiped formed by the rows or columns: \nThe correctness of the formula can be checked by using cross- and triple-product properties and by noting that for groups, left and right inverses always coincide. Intuitively, because of the cross products, each row of formula_51 is orthogonal to the non-corresponding two columns of formula_52 (causing the off-diagonal terms of formula_53 be zero). Dividing by \ncauses the diagonal elements of formula_53 to be unity. For example, the first diagonal is:\n\nWith increasing dimension, expressions for the inverse of A get complicated. For , the Cayley–Hamilton method leads to an expression that is still tractable:\n\nMatrices can also be \"inverted blockwise\" by using the following analytic inversion formula:\n\nwhere A, B, C and D are matrix sub-blocks of arbitrary size. (A must be square, so that it can be inverted. Furthermore, A and must be nonsingular.) This strategy is particularly advantageous if A is diagonal and (the Schur complement of A) is a small matrix, since they are the only matrices requiring inversion.\n\nThis technique was reinvented several times and is due to Hans Boltz (1923), who used it for the inversion of geodetic matrices, and Tadeusz Banachiewicz (1937), who generalized it and proved its correctness.\n\nThe nullity theorem says that the nullity of A equals the nullity of the sub-block in the lower right of the inverse matrix, and that the nullity of B equals the nullity of the sub-block in the upper right of the inverse matrix.\n\nThe inversion procedure that led to Equation () performed matrix block operations that operated on C and D first. Instead, if A and B are operated on first, and provided D and are nonsingular, the result is\n\nEquating Equations () and () leads to\n\nwhere Equation () is the Woodbury matrix identity, which is equivalent to the binomial inverse theorem.\n\nSince a blockwise inversion of an matrix requires inversion of two half-sized matrices and 6 multiplications between two half-sized matrices, it can be shown that a divide and conquer algorithm that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally. There exist matrix multiplication algorithms with a complexity of operations, while the best proven lower bound is .\n\nIf a matrix A has the property that\n\nthen A is nonsingular and its inverse may be expressed by a Neumann series:\n\nTruncating the sum results in an \"approximate\" inverse which may be useful as a preconditioner. Note that a truncated series can be accelerated exponentially by noting that the Neumann series is a geometric sum. As such, it satisfies \nTherefore, only formula_64 matrix multiplications are needed to compute formula_65 terms of the sum.\n\nMore generally, if A is \"near\" the invertible matrix X in the sense that\nthen A is nonsingular and its inverse is\nIf it is also the case that has rank 1 then this simplifies to\n\nIf \"A\" is a matrix with integer or rational coefficients and we seek a solution in arbitrary-precision rationals, then a p-adic approximation method converges to an exact solution in formula_69, assuming standard formula_70 matrix multiplication is used. The method relies on solving \"n\" linear systems via Dixon's method of \"p\"-adic approximation (each in formula_71) and is available as such in software specialized in arbitrary-precision matrix operations, e.g. in IML.\n\nSuppose that the invertible matrix A depends on a parameter \"t\". Then the derivative of the inverse of A with respect to \"t\" is given by\n\nTo derive the above expression for the derivative of the inverse of A, one can differentiate the definition of the matrix inverse formula_73 and then solve for the inverse of A:\nSubtracting formula_75 from both sides of the above and multiplying on the right by formula_51 gives the correct expression for the derivative of the inverse:\nSimilarly, if formula_78 is a small number then\n\nMore generally, if\n\nthen,\n\nGiven a positive integer formula_19,\n\nTherefore,\n\nSome of the properties of inverse matrices are shared by generalized inverses (e.g., the Moore–Penrose inverse), which can be defined for any \"m\"-by-\"n\" matrix.\n\nFor most practical applications, it is \"not\" necessary to invert a matrix to solve a system of linear equations; however, for a unique solution, it \"is\" necessary that the matrix involved be invertible.\n\nDecomposition techniques like LU decomposition are much faster than inversion, and various fast algorithms for special classes of linear systems have also been developed.\n\nAlthough an explicit inverse is not necessary to estimate the vector of unknowns, it is unavoidable to estimate their precision, found in the diagonal of the posterior covariance matrix of the vector of unknowns.\n\nMatrix inversion plays a significant role in computer graphics, particularly in 3D graphics rendering and 3D simulations. Examples include screen-to-world ray casting, world-to-subspace-to-world object transformations, and physical simulations.\n\nMatrix inversion also plays a significant role in the MIMO (Multiple-Input, Multiple-Output) technology in wireless communications. The MIMO system consists of N transmit and M receive antennas. Unique signals, occupying the same frequency band, are sent via N transmit antennas and are received via M receive antennas. The signal arriving at each receive antenna will be a linear combination of the N transmitted signals forming a NxM transmission matrix H. It is crucial for the matrix H to be invertible for the receiver to be able to figure out the transmitted information.\n\n\n\n"}
{"id": "13940585", "url": "https://en.wikipedia.org/wiki?curid=13940585", "title": "Japanese theorem for cyclic quadrilaterals", "text": "Japanese theorem for cyclic quadrilaterals\n\nIn geometry, the Japanese theorem states that the centers of the incircles of certain triangles inside a cyclic quadrilateral are vertices of a rectangle.\n\nTriangulating an arbitrary cyclic quadrilateral by its diagonals yields four overlapping triangles (each diagonal creates two triangles). The centers of the incircles of those triangles form a rectangle.\n\nSpecifically, let be an arbitrary cyclic quadrilateral and let , , , be the incenters of the triangles , , , . Then the quadrilateral formed by , , , is a rectangle.\n\nNote that this theorem is easily extended to prove the Japanese theorem for cyclic polygons. To prove the quadrilateral case, simply construct the parallelogram tangent to the corners of the constructed rectangle, with sides parallel to the diagonals of the quadrilateral. The construction shows that the parallelogram is a rhombus, which is equivalent to showing that the sums of the radii of the incircles tangent to each diagonal are equal.\n\nThe quadrilateral case immediately proves the general case by induction on the set of triangulating partitions of a general polygon.\n\n\n\n"}
{"id": "7756595", "url": "https://en.wikipedia.org/wiki?curid=7756595", "title": "John Rigby (politician)", "text": "John Rigby (politician)\n\nSir John Rigby, PC (8 January 1834 – 26 July 1903), was a British judge and Liberal politician who sat in the House of Commons between 1885 and 1894.\n\nRigby was born in Runcorn, Cheshire, the son of Thomas Rigby of Halton, Cheshire, and his wife Elizabeth Kendal. He attended Liverpool College before going to Trinity College, Cambridge in 1853. He graduated as Second Wrangler in 1856, also being placed second for the Smith's Prize. He became a fellow of Trinity in 1856 and was called to the Bar at Lincoln's Inn in 1860.\n\nThe story of how Rigby came to the Bar may be found on pg. 120 of the 1958 memoir “B-berry and I Look Back”, by Dornford Yates.\nIn 1875 Rigby was appointed junior counsel to the Treasury. In 1881 he \"took silk\", becoming a Queen's Counsel. He distinguished himself as an advocate, and was frequently involved in bringing appeals to the judicial committee of the House of Lords. Rigby was twice briefly a Liberal Party Member of Parliament. He was elected as MP for the Wisbech division of Cambridgeshire at the 1885 general election, but lost the seat when another election was held in 1886.\n\nIn 1892 Rigby returned to parliament, when he was among a number of Liberals who gained seats from the government parties in Scotland. He was elected at Forfarshire, unseating the Liberal Unionist, James Barclay. He was appointed Solicitor General for England and Wales in William Gladstone's new government, and received a knighthood on 26 November 1892. On 3 May 1894 Rigby became Attorney General for England and Wales.\n\nOn 19 October 1894 he vacated his Commons seat when he was appointed a Lord Justice of Appeal, in succession to Sir Horace Davey. He was sworn of the Privy Council at the same time. He served on the Court of Appeal until his retirement in 1901, when he was granted an annuity of £3,500.\n\n\nA few years before his retirement, Rigby had suffered a severe fall, and never fully recovered his health. He died aged 69, unmarried, at his home at Chelsea Embankment, London in July 1903.\n\n"}
{"id": "46930583", "url": "https://en.wikipedia.org/wiki?curid=46930583", "title": "José F. Escobar", "text": "José F. Escobar\n\nJosé Fernando \"Chepe\" Escobar (born 20 December 1954, in Manizales, Colombia) was a Colombian mathematician known for his work on differential geometry and partial differential equations. He was professor at Cornell University.\n\nHe completed his mathematical undergraduate program at Universidad del Valle, Colombia. He received a scholarship that permitted him to do a master in science studies in the Institute of Pure and Applied Mathematics (IMPA) in Rio de Janeiro, Brazil.\n\nEscobar obtained his Ph.D. from the University of California, Berkeley in 1986, under the supervision of Richard Schoen. In his thesis he solved the problem known as \"the boundary Yamabe problem\", that had been previously settled only for the case of manifolds without boundary.\n\nHe died from cancer on 3 January 2004, at the age 49.\n\nAmong the awards he received for his work were \"the Alfred Sloan Fellowship\" and \"the Presidential Faculty Fellowship\" (received at the White House directly from the hands of the President of the United States).\n\nMathematician Fernando Codá Marques was a student of him.\n\n\n"}
{"id": "9482601", "url": "https://en.wikipedia.org/wiki?curid=9482601", "title": "Liouville's theorem (differential algebra)", "text": "Liouville's theorem (differential algebra)\n\nIn mathematics, Liouville's theorem, originally formulated by Joseph Liouville in 1833 to 1841, places an important restriction on antiderivatives that can be expressed as elementary functions.\n\nThe antiderivatives of certain elementary functions cannot themselves be expressed as elementary functions. A standard example of such a function is formula_1 whose antiderivative is (with a multiplier of a constant) the error function, familiar from statistics. Other examples include the functions formula_2 and formula_3.\n\nLiouville's theorem states that elementary antiderivatives, if they exist, must be in the same differential field as the function, plus possibly a finite number of logarithms.\n\nFor any differential field \"F\", there is a subfield\n\ncalled the constants of \"F\". Given two differential fields \"F\" and \"G\", \"G\" is called a logarithmic extension of \"F\" if \"G\" is a simple transcendental extension of \"F\" (i.e. \"G\" = \"F\"(\"t\") for some transcendental \"t\") such that\n\nThis has the form of a logarithmic derivative. Intuitively, one may think of \"t\" as the logarithm of some element \"s\" of \"F\", in which case, this condition is analogous to the ordinary chain rule. But it must be remembered that \"F\" is not necessarily equipped with a unique logarithm; one might adjoin many \"logarithm-like\" extensions to \"F\". Similarly, an exponential extension is a simple transcendental extension that satisfies\n\nWith the above caveat in mind, this element may be thought of as an exponential of an element \"s\" of \"F\". Finally, \"G\" is called an elementary differential extension of \"F\" if there is a finite chain of subfields from \"F\" to \"G\" where each extension in the chain is either algebraic, logarithmic, or exponential.\n\nSuppose \"F\" and \"G\" are differential fields, with Con(\"F\") = Con(\"G\"), and that \"G\" is an elementary differential extension of \"F\". Let \"a\" be in \"F\", \"y\" in G, and suppose \"Dy\" = \"a\" (in words, suppose that \"G\" contains an antiderivative of \"a\"). Then there exist \"c\", ..., \"c\" in Con(\"F\"), \"u\", ..., \"u\", \"v\" in \"F\" such that\n\nIn other words, the only functions that have \"elementary antiderivatives\" (i.e. antiderivatives living in, at worst, an elementary differential extension of \"F\") are those with this form. Thus, on an intuitive level, the theorem states that the only elementary antiderivatives are the \"simple\" functions plus a finite number of logarithms of \"simple\" functions.\n\nA proof of Liouville's theorem can be found in section 12.4 of Geddes, et al.\n\nAs an example, the field C(\"x\") of rational functions in a single variable has a derivation given by the standard derivative with respect to that variable. The constants of this field are just the complex numbers C.\n\nThe function formula_5, which exists in C(\"x\"), does not have an antiderivative in C(\"x\"). Its antiderivatives ln \"x\" + \"C\" do, however, exist in the logarithmic extension C(\"x\", ln \"x\").\n\nLikewise, the function formula_6 does not have an antiderivative in C(\"x\"). Its antiderivatives tan(\"x\") + \"C\" do not seem to satisfy the requirements of the theorem, since they are not (apparently) sums of rational functions and logarithms of rational functions. However, a calculation with Euler's formula formula_7 shows that in fact the antiderivatives can be written in the required manner (as logarithms of rational functions).\n\nLiouville's theorem is sometimes presented as a theorem in differential Galois theory, but this is not strictly true. The theorem can be proved without any use of Galois theory. Furthermore, the Galois group of a simple antiderivative is either trivial (if no field extension is required to express it), or is simply the additive group of the constants (corresponding to the constant of integration). Thus, an antiderivative's differential Galois group does not encode enough information to determine if it can be expressed using elementary functions, the major condition of Liouville's theorem.\n\n"}
{"id": "709567", "url": "https://en.wikipedia.org/wiki?curid=709567", "title": "List of integration and measure theory topics", "text": "List of integration and measure theory topics\n\nThis is a list of integration and measure theory topics, by Wikipedia page.\n\n\n\n\n\n\n\n\"See also list of transforms, list of Fourier-related transforms\"\n\n\n\n\n"}
{"id": "5971801", "url": "https://en.wikipedia.org/wiki?curid=5971801", "title": "List of mathematicians (D)", "text": "List of mathematicians (D)\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "7541497", "url": "https://en.wikipedia.org/wiki?curid=7541497", "title": "Locally simply connected space", "text": "Locally simply connected space\n\nIn mathematics, a locally simply connected space is a topological space that admits a basis of simply connected sets. Every locally simply connected space is also locally path-connected and locally connected.\nThe circle is an example of a locally simply connected space which is not simply connected. The Hawaiian earring is a space which is neither locally simply connected nor simply connected. The cone on the Hawaiian earring is contractible and therefore simply connected, but still not locally simply connected.\n\nAll topological manifolds and CW complexes are locally simply connected. In fact, these satisfy the much stronger property of being locally contractible.\n\nA strictly weaker condition is that of being semi-locally simply connected. Both locally simply connected spaces and simply connected spaces are semi-locally simply connected, but neither converse holds.\n"}
{"id": "53153455", "url": "https://en.wikipedia.org/wiki?curid=53153455", "title": "Maximal entropy random walk", "text": "Maximal entropy random walk\n\nMaximal entropy random walk (MERW) is a popular type of biased random walk on a graph, in which transition probabilities are chosen accordingly to the principle of maximum entropy, which says that the probability distribution which best represents the current state of knowledge is the one with largest entropy. While standard random walk chooses for every vertex uniform probability distribution among its outgoing edges, locally maximizing entropy rate, MERW maximizes it globally (average entropy production) by assuming uniform probability distribution among all paths in a given graph.\n\nMERW is used in various fields of science. A direct application is choosing probabilities to maximize transmission rate through a constrained channel, analogously to Fibonacci coding. Its properties also made it useful for example in analysis of complex networks, like link prediction, community detection and centrality measures. Also in image analysis, for example for detecting visual saliency regions, object localization, tampering detection or tractography problem.\n\nAdditionally, it recreates some properties of quantum mechanics, suggesting a way to repair the discrepancy between diffusion models and quantum predictions, like Anderson localization.\n\nImagine there is a graph with formula_1 vertices, given by adjacency matrix formula_2: formula_3 if there is an edge from vertex formula_4 to formula_5, 0 otherwise. For simplicity assume it is an undirected graph, what corresponds to symmetric formula_6; however, MERW can be also generalized for directed and weighted graphs (getting Boltzmann distribution among paths instead of uniform).\n\nWe would like to choose a random walk as Markov process on this graph: for every vertex formula_4 and its outgoing edge to formula_5, choose probability formula_9 of the walker randomly using this edge after visiting formula_4. Formally, find a stochastic matrix formula_11 such that\nAssuming this graph is connected and not periodic, ergodic theory says that evolution of this stochastic process leads to some stationary probability distribution formula_16 such that formula_17.\n\nUsing Shannon entropy for every vertex and averaging over probability of visiting this vertex (to be able to use its entropy), we get the following formula for average entropy production (entropy rate) of a stochastic process:\n\nThis definition turns out to be equivalent with asymptotic average entropy (per length) of the probability distribution in the space of paths for this stochastic process.\n\nIn the standard random walk, referred here as generic random walk (GRW), we naturally choose that each outgoing edge is equally probable:\nFor a symmetric formula_6 it leads to a stationary probability distribution formula_16 with\nIt locally maximizes entropy production (uncertainty) for every vertex, but usually leads to a suboptimal averaged global entropy rate formula_23.\n\nMERW chooses the stochastic matrix which maximizes formula_23, or equivalently assumes uniform probability distribution among all paths in a given graph. Its formula is obtained by first calculating the dominant eigenvalue formula_25 and corresponding eigenvector formula_26 of the adjacency matrix, i.e. the largest formula_27 with corresponding formula_28 such that formula_29. Then stochastic matrix and stationary probability distribution are given by\nfor which every possible path of length formula_31 from the formula_4-th to formula_5-th vertex has probability\nIts entropy rate is formula_35 and the stationary probability distribution formula_16 is\n\nIn contrast to GRW, the MERW transition probabilities generally depend on the structure of the entire graph (are nonlocal). Hence, they rather should not be imagined as directly applied by the walker – if randomly looking decisions are performed based on local situation, like a person would do, the GRW approach is rather more appropriate. MERW is based on the principle of maximum entropy, making it the safest assumption when we don't have any additional knowledge about the system. For example, it would be appropriate for modelling our knowledge about an object performing some complex dynamics – not necessarily random, like a particle.\n\nAssume for simplicity that the considered graph is indirected, connected and aperiodic, allowing to conclude from the Perron-Frobenius theorem that the dominant eigenvector is unique. Hence formula_38 can be asymptotically (formula_39) approximated by formula_40 (or formula_41 in bra-ket notation).\n\nMERW requires uniform distribution along paths. The number formula_42 of paths with length formula_43 and vertex formula_4 in the center is\nhence for all formula_4,\n\nAnalogously calculating probability distribution for two succeeding vertices, one obtains that the probability of being at the formula_4-th vertex and next at the formula_5-th vertex is\nDividing by the probability of being at the formula_4-th vertex, i.e. formula_52, gives for the conditional probability formula_9 of the formula_5-th vertex being next after the formula_4-th vertex\n\nLet us first look at probably the simplest nontrivial situation: Fibonacci coding, where we want to transmit a message as a sequence of 0/1, but not using two successive 1s – after 1 there has to be used 0. To maximize the amount of information transmitted in such sequence, we should assume uniform probability distribution in the space of all possible sequences fulfilling this constraint. To practically use such long sequences, after 1 we have to use 0, but there remains a freedom of choosing the probability of 0 after 0. Let us denote this probability by formula_57, then entropy coding would allow to encode a message using this chosen probability distribution. The stationary probability distribution of symbols for a given formula_57 turns out to be formula_59. Hence, entropy production is formula_60, which is maximized for formula_61, known from golden ratio. In contrast, standard random walk would choose suboptimal formula_62. While choosing larger formula_57 reduces the amount of information produced after 0, it also reduces frequency of 1, after which we cannot write any information.\n\nA more complex example is defected one-dimensional cyclic lattice: let say 1000 nodes connected in a ring, for which all nodes but the defects have self-loop (edge to itself). In standard random walk (GRW) the stationary probability distribution would have defect probability being 2/3 of probability of the remaining vertices – there is nearly no localization, also analogously for standard diffusion, which is infinitesimal limit of GRW. For MERW we have to first find the dominant eigenvector of the adjacency matrix – maximizing formula_25 in:\n\nformula_65\n\nfor all positions formula_66, where formula_67 for defects, 0 otherwise. Substituting formula_68 and multiplying the equation by −1 we get:\n\nformula_69\n\nwhere formula_70 is minimized now, becoming the analog of energy. The formula inside the bracket is discrete Laplace operator, making this equation a discrete analogue of stationary Schrodinger equation. Like in quantum mechanics, MERW predicts that the probability distribution should lead exactly to the one of quantum ground state: formula_71 with its strongly localized density (in contrast to standard diffusion). Performing infinitesimal limit, we can get standard continuous stationary Schrodinger equation (formula_72 for formula_73) here.\n\n\n"}
{"id": "733653", "url": "https://en.wikipedia.org/wiki?curid=733653", "title": "Mean width", "text": "Mean width\n\nIn geometry, the mean width is a measure of the \"size\" of a body; see Hadwiger's theorem for more about the available measures of bodies. In formula_1 dimensions, one has to consider formula_2-dimensional hyperplanes perpendicular to a given direction formula_3 in formula_4, where formula_5 is the n-sphere (the surface of a formula_6-dimensional sphere).\nThe \"width\" of a body in a given direction formula_3 is the distance between the closest pair of such planes, such that the body is entirely in between the two hyper planes (the planes only intersect \nwith the boundary of the body). The mean width is the average of this \"width\" over all formula_3 in formula_4.\n\nMore formally, define a compact body B as being equivalent to set of points in its interior plus the points on the boundary (here, points denote elements of formula_10). The support function of body B is defined as\n\nwhere formula_1 is a direction and formula_13 denotes the usual inner product on formula_10. The mean width is then\n\nwhere formula_16 is the formula_2-dimensional volume of formula_4.\nNote, that the mean width can be defined for any body (that is compact), but it is most\nuseful for convex bodies (that is bodies, whose corresponding set is a convex set).\n\nThe mean width of a line segment \"L\" is the length (1-volume) of \"L\".\n\nThe mean width \"w\" of any compact shape \"S\" in two dimensions is \"p\"/π, where \"p\" is the perimeter of the convex hull of \"S\". So \"w\" is the diameter of a circle with the same perimeter as the convex hull.\n\nFor convex bodies \"K\" in three dimensions, the mean width of \"K\" is related to the average of the mean curvature, \"H\", over the whole surface of \"K\". In fact,\n\nwhere formula_20 is the boundary of the convex body formula_21 and formula_22\na surface integral element, formula_23 is the mean curvature at the corresponding position\non formula_20. Similar relations can be given between the other measures \nand the generalizations of the mean curvature, also for other dimensions \nAs the integral over the mean curvature is typically much easier to calculate\nthan the mean width, this is a very useful result.\n\n\nThe mean width is usually mentioned in any good reference on convex geometry, for instance, \"Selected topics in convex geometry\" by Maria Moszyńska (Birkhäuser, Boston 2006). The relation between the mean width and the mean curvature is also derived in that reference.\n\nThe application of the mean width as one of the measures featuring in Hadwiger's theorem\nis discussed in Beifang Chen in \"A simplified elementary proof of Hadwiger's volume theorem.\" \"Geom. Dedicata\" 105 (2004), 107—120.\n"}
{"id": "12169338", "url": "https://en.wikipedia.org/wiki?curid=12169338", "title": "Mitrofan Cioban", "text": "Mitrofan Cioban\n\nMitrofan Cioban (born 5 January 1942, Copceac, Ștefan Vodă) is a Moldovan mathematician, a member of the Academy of Sciences of Moldova (2000).\n"}
{"id": "853141", "url": "https://en.wikipedia.org/wiki?curid=853141", "title": "Motzkin number", "text": "Motzkin number\n\nIn mathematics, a Motzkin number for a given number is the number of different ways of drawing non-intersecting chords between points on a circle (not necessarily touching every point by a chord). The Motzkin numbers are named after Theodore Motzkin and have diverse applications in geometry, combinatorics and number theory.\n\nThe Motzkin numbers formula_1 for formula_2 form the sequence:\n\nThe following figure shows the 9 ways to draw non-intersecting chords between 4 points on a circle ():\n\nThe following figure shows the 21 ways to draw non-intersecting chords between 5 points on a circle ():\n\nThe Motzkin numbers satisfy the recurrence relations\n\nThe Motzkin numbers can be expressed in terms of binomial coefficients and Catalan numbers:\n\nA Motzkin prime is a Motzkin number that is prime. , four such primes are known:\n\nThe Motzkin number for is also the number of positive integer sequences of length in which the opening and ending elements are either 1 or 2, and the difference between any two consecutive elements is −1, 0 or 1. Equivalently, the Motzkin number for is the number of positive integer sequences of length in which the opening and ending elements are 1, and the difference between any two consecutive elements is −1, 0 or 1.\n\nAlso, the Motzkin number for gives the number of routes on the upper right quadrant of a grid from coordinate (0, 0) to coordinate (, 0) in steps if one is allowed to move only to the right (up, down or straight) at each step but forbidden from dipping below the = 0 axis.\n\nFor example, the following figure shows the 9 valid Motzkin paths from (0, 0) to (4, 0):\n\nThere are at least fourteen different manifestations of Motzkin numbers in different branches of mathematics, as enumerated by in their survey of Motzkin numbers.\n\n\n"}
{"id": "43010045", "url": "https://en.wikipedia.org/wiki?curid=43010045", "title": "Point to Point Encryption", "text": "Point to Point Encryption\n\nPoint-to-point encryption (P2PE) is a standard established by the PCI Security Standards Council. Payment solutions that offer similar encryption but do not meet the P2Pe standard are referred to as end-to-end encryption (E2Ee) solutions. The objective of P2Pe and E2Ee is to provide a payment security solution that instantaneously converts confidential payment card (credit and debit card) data and information into indecipherable code at the time the card is swiped to prevent hacking and fraud. It is designed to maximize the security of payment card transactions in an increasingly complex regulatory environment.\n\nThe P2PE Standard defines the requirements that a \"solution\" must meet in order to be accepted as a PCI validated P2PE solution. A \"solution\" is a complete set of hardware, software, gateway, decryption, device handling, etc. Only \"solutions\" can be validated; individual pieces of hardware such as card readers cannot be validated. It is also a common mistake to refer to P2PE validated solutions as \"certified\"; there is no such certification.\n\nThe determination of whether or not a solution meets the P2PE standard is the responsibility of a P2PE Qualified Security Assessor (P2PE-QSA). P2PE-QSA companies are independent third-party companies who employ assessors that have met the PCI Security Standards Council's requirements for education and experience, and have passed the requisite exam. The PCI Security Standards Council does not validate solutions.\n\nAs a payment card is swiped through a card reading device, referred to as a point of interaction (POI) device, at the merchant location or point of sale, the device immediately encrypts the card information. A device that is part of a PCI validated P2Pe solution uses an algorithmic calculation to encrypt the confidential payment card data. From the POI, the encrypted, indecipherable codes are sent to the payment gateway or processor for decryption. The keys for encryption and decryption are never available to the merchant, making card data entirely invisible to the retailer. Once the encrypted codes are within the secure data zone of the payment processor, the codes are decrypted to the original card numbers and then passed to the issuing bank for authorization. The bank either approves or rejects the transaction, depending upon the card holder's payment account status. The merchant is then notified if the payment is accepted or rejected to complete the process along with a token that the merchant can store. This token is a unique number reference to the original transaction that the merchant can use should they ever be needed to perform research or refund the customer without ever knowing the customer's card information (tokenization). There are also Qualified Integrator and Reseller (QIR) Companies, which are businesses authorized to \"implement, configure, and/or support validated\" PA-DSS Payment Applications, and perform qualified installations.\n\nAccording to the PCI Security Standards Council:The P2PE solution provider is a third-party entity (for example, a processor, acquirer, or payment gateway) that has overall responsibility for the design and implementation of a specific P2PE solution, and manages P2PE solutions for its merchant customers. The solution provider has overall responsibility for ensuring that all P2PE requirements are met, including any P2PE requirements performed by third-party organizations on behalf of the solution provider (for example, certification authorities and key-injection facilities).\n\nP2PE significantly reduces the risk of payment card fraud by instantaneously encrypting confidential cardholder data at the moment a payment card is swiped or 'dipped' if it is a chip card at the card reading device (payment terminal) or POI.\n\nP2PE significantly facilitates merchant responsibilities:\n\nA point-to-point connection directly links system 1 (the point of payment card acceptance) to system 2 (the point of payment processing). \nA true P2PE solution is determined with three main factors:\n\nEnd-to-end encryption as the name suggests has the advantage over P2PE that card details are not unencrypted between the two endpoints. If the endpoints are a PCI PED validated PIN pad and a POS acquirer, there is no opportunity for the card details to be intercepted. It is obviously important that the endpoints (the PED and gateway) are provided by PCI accredited organisations.\n\nThe requirements include: \n"}
{"id": "24109458", "url": "https://en.wikipedia.org/wiki?curid=24109458", "title": "Popoviciu's inequality", "text": "Popoviciu's inequality\n\nIn convex analysis, Popoviciu's inequality is an inequality about convex functions. It is similar to Jensen's inequality and was found in 1965 by Tiberiu Popoviciu, a Romanian mathematician. It states:\n\nLet \"f\" be a function from an interval formula_1 to formula_2. If \"f\" is convex, then for any three points \"x\", \"y\", \"z\" in \"I\",\n\nIf a function \"f\" is continuous, then it is convex if and only if the above inequality holds for all \"x\", \"y\", \"z\" from formula_4. When \"f\" is strictly convex, the inequality is strict except for \"x\" = \"y\" = \"z\".\n\nIt can be generalised to any finite number \"n\" of points instead of 3, taken on the right-hand side \"k\" at a time instead of 2 at a time:\n\nLet \"f\" be a continuous function from an interval formula_1 to formula_2. Then \"f\" is convex if and only if, for any integers \"n\" and \"k\" where \"n\" ≥ 3 and formula_7, and any \"n\" points formula_8 from \"I\",\n\nPopoviciu's inequality can also be generalised to a weighted inequality. Popoviciu's paper has been published in Romanian language, but the interested reader can find his results in the review . Page 1 Page 2\n"}
{"id": "40152955", "url": "https://en.wikipedia.org/wiki?curid=40152955", "title": "Poretsky's law of forms", "text": "Poretsky's law of forms\n\nIn Boolean algebra, Poretsky's law of forms shows that the single Boolean equation formula_1 is equivalent to formula_2 if and only if formula_3, where formula_4 represents exclusive or.\n\nThe law of forms was discovered by Platon Poretsky.\n\n\n"}
{"id": "38150904", "url": "https://en.wikipedia.org/wiki?curid=38150904", "title": "Primordial element (algebra)", "text": "Primordial element (algebra)\n\nIn algebra, a primordial element is a particular kind of a vector in a vector space. Let \"V\" be a vector space over a field \"k\" and fix a basis for \"V\" of vectors formula_1 for formula_2. By the definition of a basis, every vector \"v\" in \"V\" can be expressed uniquely as\nDefine formula_4, the set of indices for which the expression of \"v\" has a nonzero coefficient. Given a subspace \"W\" of \"V\", a nonzero vector \"w\" in \"W\" is said to be \"primordial\" if it has the following two properties:\n"}
{"id": "1882815", "url": "https://en.wikipedia.org/wiki?curid=1882815", "title": "Proofs of Fermat's theorem on sums of two squares", "text": "Proofs of Fermat's theorem on sums of two squares\n\nFermat's theorem on sums of two squares asserts that an odd prime number \"p\" can be expressed as\n\nwith integer \"x\" and \"y\" if and only if \"p\" is congruent to 1 (mod 4). The statement was announced by Girard in 1625, and again by Fermat in 1640, but neither supplied a proof.\n\nThe \"only if\" clause is easy: a perfect square is congruent to 0 or 1 modulo 4, hence a sum of two squares is congruent to 0, 1, or 2. An odd prime number is congruent to either 1 or 3 modulo 4, and the second possibility has just been ruled out. The first proof that such a representation exists was given by Leonhard Euler in 1747 and was complicated. Since then, many different proofs have been found. Among them, the proof using Minkowski's theorem about convex sets and Don Zagier's short proof based on involutions have appeared.\n\nEuler succeeded in proving Fermat's theorem on sums of two squares in 1749, when he was forty-two years old. He communicated this in a letter to Goldbach dated 12 April 1749. The proof relies on infinite descent, and is only briefly sketched in the letter. The full proof consists in five steps and is published in two papers. The first four steps are Propositions 1 to 4 of the first paper and do not correspond exactly to the four steps below. The fifth step below is from the second paper. \n\nFor the avoidance of ambiguity, zero will always be a valid possible constituent of \"sums of two squares\", so for example every square of an integer is trivially expressible as the sum of two squares by setting one of them to be zero. \n\n1. \"The product of two numbers, each of which is a sum of two squares, is itself a sum of two squares.\"\n\n2. \"If a number which is a sum of two squares is divisible by a prime which is a sum of two squares, then the quotient is a sum of two squares.\"\n(This is Euler's first Proposition).\n\n3. \"If a number which can be written as a sum of two squares is divisible by a number which is not a sum of two squares, then the quotient has a factor which is not a sum of two squares.\" (This is Euler's second Proposition).\n\n4. \"If formula_29 and formula_30 are relatively prime positive integers then every factor of formula_3 is a sum of two squares.\"\n(This is the step that uses step (3.) to produce an 'infinite descent' and was Euler's Proposition 4. The proof sketched below also includes the proof of his Proposition 3).\n\n5. \"Every prime of the form formula_79 is a sum of two squares.\"\n(This is the main result of Euler's second paper).\n\nLagrange completed a proof in 1775 based on his general theory of integral quadratic forms. The following presentation incorporates a slight simplification of his argument, due to Gauss, which appears in article 182 of the Disquisitiones Arithmeticae.\n\nAn (integral binary) quadratic form is an expression of the form formula_105 with formula_106 integers. A number formula_107 is said to be \"represented by the form\" if there exist integers formula_108 such that formula_109. Fermat's theorem on sums of two squares is then equivalent to the statement that a prime formula_82 is represented by the form formula_111 (i.e., formula_112, formula_113) exactly when formula_82 is congruent to formula_91 modulo formula_116.\n\nThe discriminant of the quadratic form is defined to be formula_117. The discriminant of formula_111 is then equal to formula_119.\n\nTwo forms formula_120 and formula_121 are \"equivalent\" if and only if there exist substitutions with integer coefficients\nwith formula_124 such that, when substituted into the first form, yield the second. Equivalent forms are readily seen to have the same discriminant, and hence also the same parity for the middle coefficient formula_125, which coincides with the parity of the discriminant. Moreover, it is clear that equivalent forms will represent exactly the same integers, because these kind of substitutions can be reversed by substitutions of the same kind.\n\nLagrange proved that all positive definite forms of discriminant −4 are equivalent. Thus, to prove Fermat's theorem it is enough to find \"any\" positive definite form of discriminant −4 that represents formula_82. For example, one can use a form\nwhere the first coefficient \"a\" = formula_82 was chosen so that the form represents formula_82 by setting \"x\" = 1, and \"y\" = 0, the coefficient \"b\" = 2\"m\" is an arbitrary even number (as it must be, to get an even discriminant), and finally formula_130 is chosen so that the discriminant formula_131 is equal to −4, which guarantees that the form is indeed equivalent to formula_132. Of course, the coefficient formula_130 must be an integer, so the problem is reduced to finding some integer \"m\" such that formula_82 divides formula_135: or in other words, a \" 'square root of -1 modulo formula_82' \". \n\nWe claim such a square root of formula_137 is given by formula_138. Firstly it follows from Euclid's Fundamental Theorem of Arithmetic that formula_139. Consequently formula_140: that is, formula_141 are their own inverses modulo formula_82 and this property is unique to them. It then follows from the validity of Euclidean division in the integers, and the fact that formula_82 is prime, that for every formula_144 the gcd of formula_29 and formula_82 may be expressed via the Euclidean algorithm yielding a unique and \"distinct\" inverse formula_147 of formula_29 modulo formula_82. In particular therefore the product of \"all\" non-zero residues modulo formula_82 is formula_137. Let formula_152: from what has just been observed, formula_153. But by definition, since each term in formula_154 may be paired with its negative in formula_155, formula_156, which since formula_82 is odd shows that formula_158, as required. \n\nRichard Dedekind gave at least two proofs of Fermat's theorem on sums of two squares, both using the arithmetical properties of the Gaussian integers, which are numbers of the form \"a\" + \"bi\", where \"a\" and \"b\" are integers, and \"i\" is the square root of −1. One appears in section 27 of his exposition of ideals published in 1877; the second appeared in Supplement XI to Peter Gustav Lejeune Dirichlet's \"Vorlesungen über Zahlentheorie\", and was published in 1894.\n\n1. First proof. If formula_82 is an odd prime number, then we have formula_160 in the Gaussian integers. Consequently, writing a Gaussian integer ω = \"x\" + \"iy\" with \"x,y\" ∈ Z and applying the Frobenius automorphism in Z[\"i\"]/(\"p\"), one finds\nsince the automorphism fixes the elements of Z/(\"p\"). In the current case, formula_80 for some integer n, and so in the above expression for ω, the exponent (p-1)/2 of -1 is even. Hence the right hand side equals ω, so in this case the Frobenius endomorphism of Z[\"i\"]/(\"p\") is the identity. \nKummer had already established that if } is the order of the Frobenius automorphism of Z[\"i\"]/(\"p\"), then the ideal formula_163 in Z[\"i\"] would be a product of 2/\"f\" distinct prime ideals. (In fact, Kummer had established a much more general result for any extension of Z obtained by adjoining a primitive \"m\"-th root of unity, where \"m\" was any positive integer; this is the case of that result.) Therefore, the ideal (\"p\") is the product of two different prime ideals in Z[\"i\"]. Since the Gaussian integers are a Euclidean domain for the norm function formula_164, every ideal is principal and generated by a nonzero element of the ideal of minimal norm. Since the norm is multiplicative, the norm of a generator formula_165 of one of the ideal factors of (\"p\") must be a strict divisor of formula_166, so that we must have formula_167, which gives Fermat's theorem.\n\n2. Second proof. This proof builds on Lagrange's result that if formula_80 is a prime number, then there must be an integer \"m\" such that formula_169 is divisible by \"p\" (we can also see this by Euler's criterion); it also uses the fact that the Gaussian integers are a unique factorization domain (because they are a Euclidean domain). Since does not divide either of the Gaussian integers formula_170 and formula_171 (as it does not divide their imaginary parts), but it does divide their product formula_169, it follows that formula_82 cannot be a prime element in the Gaussian integers. We must therefore have a nontrivial factorization of \"p\" in the Gaussian integers, which in view of the norm can have only two factors (since the norm is multiplicative, and formula_174, there can only be up to two factors of p), so it must be of the form formula_175 for some integers formula_176 and formula_177. This immediately yields that formula_1.\n\nFor formula_82 congruent to formula_91 mod formula_116 a prime, formula_137 is a quadratic residue mod formula_82 by Euler's criterion. Therefore, there exists an integer formula_184 such that formula_82 divides formula_135. Let formula_187 be the standard basis elements for the vector space formula_188 and set formula_189 and formula_190. Consider the lattice formula_191. If formula_192 then formula_193. Thus formula_82 divides formula_195 for any formula_196.\n\nThe area of the fundamental parallelogram (in this case a rectangle) of the lattice is formula_82. The area of the open disk, formula_198, of radius formula_199 centered around the origin is formula_200. Furthermore, formula_198 is convex and symmetrical about the origin. Therefore, by Minkowski's theorem there exists a nonzero vector formula_196 such that formula_203. Both formula_204 and formula_205 so formula_206. Hence formula_82 is the sum of the squares of the components of formula_208.\n\nLet formula_209 be prime, let formula_210 denote the natural numbers (with or without zero), and consider the finite set formula_211 of triples of numbers.\nThen formula_212 has two involutions: an obvious one formula_213 whose fixed points formula_214 correspond to representations of formula_82 as a sum of two squares, and a more complicated one,\n\nwhich has exactly one fixed point formula_217. Two involutions over the same finite set must have sets of fixed points with the same parity, and since the second involution has an odd number of fixed points, so does the first.\nZero is even, so the first involution has a nonzero number of fixed points, any one of which gives a representation of formula_82 as a sum of two squares.\n\nThis proof, due to Zagier, is a simplification of an earlier proof by Heath-Brown, which in turn was inspired by a proof of Liouville. The technique of the proof is a combinatorial analogue of the topological principle that the Euler characteristics of a topological space with an involution and of its fixed point set have the same parity and is reminiscent of the use of \"sign-reversing involutions\" in the proofs of combinatorial bijections.\n\nIn 2016, A. David Christopher gave a partition-theoretic proof by considering partitions of the odd prime formula_107 having exactly two sizes formula_220, each occurring exactly formula_221 times, and by showing that at least one such partition exists if formula_107 is congruent to 1 modulo 4.\n\n"}
{"id": "24860178", "url": "https://en.wikipedia.org/wiki?curid=24860178", "title": "Revolutions in Mathematics", "text": "Revolutions in Mathematics\n\nRevolutions in Mathematics is a collection of essays in the history and philosophy of mathematics.\n\n\nThe book was reviewed by Pierre Kerszberg for \"Mathematical Reviews\" and by Michael S. Mahoney for \"American Mathematical Monthly\". Mahoney says \"The title should have a question mark.\" He sets the context by referring to paradigm shifts that characterize scientific revolutions as described by Thomas Kuhn in his book \"The Structure of Scientific Revolutions\". According to Michael Crowe in chapter one, revolutions never occur in mathematics. Mahoney explains how mathematics grows upon itself and does not discard earlier gains in understanding with new ones, such as happens in biology, physics, or other sciences. A nuanced version of revolution in mathematics is described by Caroline Dunmore who sees change at the level of \"meta-mathematical values of the community that define the telos and methods of the subject, and encapsulate general beliefs about its value.\" On the other hand, reaction to innovation in mathematics is noted, resulting in \"clashes of intellectual and social values\".\n\n"}
{"id": "44613219", "url": "https://en.wikipedia.org/wiki?curid=44613219", "title": "Ruth Charney", "text": "Ruth Charney\n\nRuth Michele Charney (born 1950) is an American mathematician known for her work in geometric group theory and Artin groups. She became president of the Association for Women in Mathematics in 2013, and holds the Theodore and Evelyn G. Berenson Chair in Mathematics at Brandeis University. She was in the first group of mathematicians named Fellows of the American Mathematical Society.\n\nCharney attended Brandeis University, graduating in mathematics in 1972. She then attended Merce Cunningham Dance Studio for a year, studying modern dance. She received her Ph.D. from Princeton University in 1977 under Wu-Chung Hsiang.\n\nFollowing her graduation from Princeton, Charney took a postdoctoral position at University of California, Berkeley, followed by an NSF postdoctoral appointment/assistant professor position at Yale University. She worked for Ohio State University until 2003, when she returned to work at Brandeis University.\n\nCharney was elected as President of the Association for Women in Mathematics in 2013. She emphasized the importance of encouraging young women in mathematics through summer programs, mentorships, and parental involvement.\n\nShe has served as an editor of the journal \"Algebraic and Geometric Topology\".\n\nIn 2013 Charney was named a Fellow of the American Mathematical Society in the inaugural class.\n\nIn 2017 she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\n\n"}
{"id": "14056793", "url": "https://en.wikipedia.org/wiki?curid=14056793", "title": "Samuel Earnshaw", "text": "Samuel Earnshaw\n\nSamuel Earnshaw (1 February 1805, Sheffield, Yorkshire – 6 December 1888, Sheffield, Yorkshire) was an English clergyman and mathematician and physicist, noted for his contributions to theoretical physics, especially \"Earnshaw's theorem\".\n\nEarnshaw was born in Sheffield and entered St John's College, Cambridge, graduating Senior Wrangler and Smith's Prizeman in 1831.\n\nFrom 1831 to 1847 Earnshaw worked in Cambridge as tripos coach, and in 1846 was appointed to the parish church St. Michael, Cambridge. For a time he acted as curate to the Revd Charles Simeon. In 1847 his health broke down and he returned to Sheffield working as a chaplain and teacher.\n\nEarnshaw published several mathematical and physical articles and books. His most famous contribution, \"Earnshaw's theorem\", shows the impossibility of stable levitating permanent magnets: other topics included optics, waves, dynamics and acoustics in physics, calculus, trigonometry and partial differential equations in mathematics. As a clergyman, he published several sermons and treatises.\n\n"}
{"id": "439711", "url": "https://en.wikipedia.org/wiki?curid=439711", "title": "Scientific calculator", "text": "Scientific calculator\n\nA scientific calculator is a type of electronic calculator, usually but not always handheld, designed to calculate problems in science, engineering, and mathematics. They have almost completely replaced slide rules in traditional applications, and are widely used in both education and professional settings.\n\nIn certain contexts such as higher education, scientific calculators have been superseded by graphing calculators, which offer a superset of scientific calculator functionality along with the ability to graph input data and write and store programs for the device. There is also some overlap with the financial calculator market.\n\nModern scientific calculators generally have many more features than a standard four or five-function calculator, and the feature set differs between manufacturers and models; however, the defining features of a scientific calculator include:\n\n\nIn addition, high-end scientific calculators generally include:\n\n\nWhile most scientific models have traditionally used a single-line display similar to traditional pocket calculators, many of them have more digits (10 to 12), sometimes with extra digits for the floating point exponent. A few have multi-line displays, with some models from Hewlett-Packard, Texas Instruments, Casio, Sharp, and Canon using dot matrix displays similar to those found on graphing calculators.\n\nScientific calculators are used widely in situations that require quick access to certain mathematical functions, especially those that were once looked up in mathematical tables, such as trigonometric functions or logarithms. They are also used for calculations of very large or very small numbers, as in some aspects of astronomy, physics, and chemistry.\n\nThey are very often required for math classes from the junior high school level through college, and are generally either permitted or required on many standardized tests covering math and science subjects; as a result, many are sold into educational markets to cover this demand, and some high-end models include features making it easier to translate a problem on a textbook page into calculator input, e.g. by providing a method to enter an entire problem in as it is written on the page using simple formatting tools.\n\nThe first scientific calculator that included all of the basic ideas above was the programmable Hewlett-Packard HP-9100A, released in 1968, though the Wang LOCI-2 and the Mathatronics Mathatron had some features later identified with scientific calculator designs. The HP-9100 series was built entirely from discrete transistor logic with no integrated circuits, and was one of the first uses of the CORDIC algorithm for trigonometric computation in a personal computing device, as well as the first calculator based on Reverse Polish Notation (RPN) entry. HP became closely identified with RPN calculators from then on, and even today some of their high-end calculators (particularly the long-lived HP-12C financial calculator and the HP-48 series of graphing calculators) still offer RPN as their default input mode due to having garnered a very large following.\n\nThe HP-35, introduced on February 1, 1972, was Hewlett-Packard's first pocket calculator and the world's first handheld scientific calculator. Like some of HP's desktop calculators it used RPN. Introduced at US$395, the HP-35 was available from 1972 to 1975.\n\nTexas Instruments (TI), after the introduction of several units with scientific notation, came out with a handheld scientific calculator on January 15, 1974, in the form of the SR-50. TI continues to be a major player in the calculator market, with their long-running TI-30 series being one of the most widely used scientific calculators in classrooms.\n\nCasio, Canon and Sharp have also been major players, with Casio's fx series (beginning with the Casio fx-1 in 1972) being a very common brand, used particularly in schools. Casio is also a major player in the graphing calculator market, and was the first company to produce one (Casio fx-7000G).\n\n"}
{"id": "7154332", "url": "https://en.wikipedia.org/wiki?curid=7154332", "title": "Skewness risk", "text": "Skewness risk\n\nSkewness risk in financial modeling is the risk that results when observations are not spread symmetrically around an average value, but instead have a skewed distribution. As a result, the mean and the median can be different. Skewness risk can arise in any quantitative model that assumes a symmetric distribution (such as the normal distribution) but is applied to skewed data.\n\nIgnoring skewness risk, by assuming that variables are symmetrically distributed when they are not, will cause any model to understate the risk of variables with high skewness.\n\nSkewness risk plays an important role in hypothesis testing. The analysis of variance, the most common test used in hypothesis testing, assumes that the data is normally distributed. If the variables tested are not normally distributed because they are too skewed, the test cannot be used. Instead, nonparametric tests can be used, such as the Mann–Whitney test for unpaired situation or the sign test for paired situation.\n\nSkewness risk and kurtosis risk also have technical implications in calculation of value at risk. If either are ignored, the Value at Risk calculations will be flawed.\n\nBenoît Mandelbrot, a French mathematician, extensively researched this issue. He feels that the extensive reliance on the normal distribution for much of the body of modern finance and investment theory is a serious flaw of any related models (including the Black–Scholes model and CAPM). He explained his views and alternative finance theory in a book: \"The (Mis)Behavior of Markets: A Fractal View of Risk, Ruin and Reward\".\n\nIn options markets, the difference in implied volatility at different strike prices represents the market's view of skew, and is called volatility skew. (In pure Black–Scholes, implied volatility is constant with respect to strike and time to maturity.)\n\nBonds have a skewed return. A bond will either pay the full amount on time (very likely to much less likely depending on quality), or less than that. A normal bond does not ever pay \"more\" than the \"good\" case.\n\n\n"}
{"id": "465067", "url": "https://en.wikipedia.org/wiki?curid=465067", "title": "Sperner's lemma", "text": "Sperner's lemma\n\nIn mathematics, Sperner's lemma is a combinatorial analog of the Brouwer fixed point theorem, which is equivalent to it.\n\nSperner's lemma states that every Sperner coloring (described below) of a triangulation of an \"n\"-dimensional simplex contains a cell colored with a complete set of colors.\n\nThe initial result of this kind was proved by Emanuel Sperner, in relation with proofs of invariance of domain. Sperner colorings have been used for effective computation of fixed points and in root-finding algorithms, and are applied in fair division (cake cutting) algorithms. It is now believed to be an intractable computational problem to find a Brouwer fixed point or equivalently a Sperner coloring, even in the plane, in the general case. The problem is PPAD-complete, a complexity class invented by Christos Papadimitriou.\n\nAccording to the Soviet \"Mathematical Encyclopaedia\" (ed. I.M. Vinogradov), a related 1929 theorem (of Knaster, Borsuk and Mazurkiewicz) had also become known as the \"Sperner lemma\" – this point is discussed in the English translation (ed. M. Hazewinkel). It is now commonly known as the Knaster–Kuratowski–Mazurkiewicz lemma.\n\nIn one dimension, Sperner's Lemma can be regarded as a discrete version of the intermediate value theorem. In this case, it essentially says that if a discrete function takes only the values 0 and 1, begins at the value 0 and ends at the value 1, then it must switch values an odd number of times.\n\nThe two-dimensional case is the one referred to most frequently. It is stated as follows:\n\nGiven a triangle ABC, and a triangulation \"T\" of the triangle, the set \"S\" of vertices of \"T\" is colored with three colors in such a way that\n\nThen there exists a triangle from \"T\", whose vertices are colored with the three different colors. More precisely, there must be an odd number of such triangles.\n\nIn the general case the lemma refers to a \"n\"-dimensional simplex\n\nWe consider a triangulation \"T\" which is a disjoint division of formula_2 into smaller \"n\"-dimensional simplices. Denote the coloring function as \"f\" : \"S\" → {1,2,3...,\"n\",\"n\"+1}, where \"S\" is again the set of vertices of \"T\". The rules of coloring are:\n\nThen there exists an odd number of simplices from \"T\", whose vertices are colored with all \"n+1\" colors. In particular, there must be at least one.\n\nWe shall first address the two-dimensional case. Consider a graph \"G\" built from the triangulation \"T\" as follows:\n\nNote that on the interval AB there is an odd number of borders colored 1-2 (simply because A is colored 1, B is colored 2; and as we move along AB, there must be an odd number of color changes in order to get different colors at the beginning and at the end). Therefore, the vertex of \"G\" corresponding to the outer area has an odd degree. But it is known (the handshaking lemma) that in a finite graph there is an even number of vertices with odd degree. Therefore, the remaining graph, excluding the outer area, has an odd number of vertices with odd degree corresponding to members of \"T\".\n\nIt can be easily seen that the only possible degree of a triangle from \"T\" is 0, 1, or 2, and that the degree 1 corresponds to a triangle colored with the three colors 1, 2, and 3.\n\nThus we have obtained a slightly stronger conclusion, which says that in a triangulation \"T\" there is an odd number (and at least one) of full-colored triangles.\n\nA multidimensional case can be proved by induction on the dimension of a simplex. We apply the same reasoning, as in the two-dimensional case, to conclude that in a \"n\"-dimensional triangulation there is an odd number of full-colored simplices.\n\nHere is an elaboration of the proof given previously, for a reader new to graph theory: This diagram numbers the colors of the vertices of the example given previously. The small triangles whose vertices all have different numbers are shaded in the graph. Each small triangle becomes a node in the new graph derived from the triangulation. The small letters identify the areas, eight inside the figure, and area \"i\" designates the space outside of it. As described previously, those nodes that share an edge whose endpoints are numbered 1 and 2 are joined in the derived graph. For example, node \"d\" shares an edge with the outer area \"i\", and its vertices all have different numbers, so it is also shaded. Node \"b\" is not shaded because two vertices have the same number, but it is joined to the outer area.\nOne could add a new full-numbered triangle, say by inserting a node numbered 3 into the edge between 1 and 1 of node \"a\", and joining that node to the other vertex of \"a\". Doing so would have to create a pair of new nodes, like the situation with nodes \"f\" and \"g\".\n\nSuppose that, instead of an formula_5-dimensional simplex, we have a formula_6-dimensional polytope with formula_7 vertices.\n\nThen there are at least formula_8 fully labeled simplices, where \"fully labeled\" indicates that every label on the simplex has a different color. For example, if a (two-dimensional) polygon with \"n\" vertices is triangulated and colored according to the Sperner criterion, then there are at least formula_9 fully labeled triangles.\n\nThe general statement was conjectured by Atanassov in 1996, who proved it for the case formula_10. \nThe proof of the general case was first given by de Loera, Peterson, and Su in 2002.\n\nSuppose that, instead of a single labeling, we have formula_7 different Sperner labelings.\n\nWe consider pairs (simplex,permutation) such that, the label of each vertex of the simplex is chosen from a different labeling (so for each simplex, there are formula_12 different pairs).\n\nThen there are at least formula_12 fully labeled pairs. This was proved by Ravindra Bapat.\n\nAnother way to state this lemma is as follows. Suppose there are formula_7 people, each of whom produces a different Sperner labeling of the same triangulation. Then, there exists a simplex, and a matching of the people to its vertices, such that each vertex is labeled by its owner differently (one person labels its vertex by 1, another person labels its vertex by 2, etc.). Moreover, there are at least formula_12 such matchings. This can be used to find an envy-free cake-cutting with connected pieces.\n\nSuppose a triangle is triangulated and labeled with {1,2,3}. Consider the cyclic sequence of labels on the boundary of the triangle. Define the \"degree\" of the labeling as the difference between the number of switches from 1 to 2, and the number of switches from 2 to 1. See examples in the table at the right. Note that the degree is the same if we count switches from 2 to 3 minus 3 to 2, or from 3 to 1 minus 1 to 3.\n\nMusin proved that \"the number of fully labeled triangles is at least the degree of the labeling\". In particular, if the degree is nonzero, then there exists at least one fully labeled triangle.\n\nIf a labeling satisfies the Sperner condition, then its degree is exactly 1: there are 1-2 and 2-1 switches only in the side between vertices 1 and 2, and the number of 1-2 switches must be one more than the number of 2-1 switches (when walking from vertex 1 to vertex 2). Therefore, the original Sperner lemma follows from Musin's theorem.\n\nThere is a similar lemma about finite and infinite trees and cycles.\n\nA variant of Sperner's lemma on a cube (instead of a simplex) was proved by Harold W. Kuhn. It is related to the Poincaré–Miranda theorem.\n\nSperner colorings have been used for effective computation of fixed points. A Sperner coloring can be constructed such that fully labeled simplices correspond to fixed points of a given function. By making a triangulation smaller and smaller, one can show that the limit of the fully labeled simplices is exactly the fixed point. Hence, the technique provides a way to approximate fixed points.\n\nFor this reason, Sperner's lemma can also be used in root-finding algorithms and fair division algorithms; see Simmons–Su protocols.\n\nSperner's lemma is one of the key ingredients of the proof of Monsky's theorem, that a square cannot be cut into an odd number of equal-area triangles.\n\nFifty years after first publishing it, Sperner presented a survey on the development, influence and applications of his combinatorial lemma.\n\n\n"}
{"id": "52448137", "url": "https://en.wikipedia.org/wiki?curid=52448137", "title": "Twisted sheaf", "text": "Twisted sheaf\n\nIn mathematics, a twisted sheaf is a variant of a coherent sheaf. Precisely, it is specified by: an open covering in the étale topology \"U\", coherent sheaves \"F\" over \"U\", a Čech 2-cocycle \"θ\" on the covering \"U\" as well as the isomorphisms\nsatisfying\n\nThe notion of twisted sheaves was introduced by Giraud. The above definition due to Căldăraru is down-to-earth but is equivalent to a more sophisticated definition in terms of gerbe; see § 2.1.3 of .\n\n\n"}
{"id": "29204993", "url": "https://en.wikipedia.org/wiki?curid=29204993", "title": "VIPER microprocessor", "text": "VIPER microprocessor\n\nVIPER is a 32-bit microprocessor design created by Royal Signals and Radar Establishment in the 1980s, intended for use in safety-critical systems such as avionics. It was the first commercial microprocessor design to be formally proven correct, although there was some controversy surrounding this claim and the definition of proof.\n\n"}
{"id": "6704603", "url": "https://en.wikipedia.org/wiki?curid=6704603", "title": "Wasserstein metric", "text": "Wasserstein metric\n\nIn mathematics, the Wasserstein or Kantorovich-Rubinstein metric or distance is a distance function defined between probability distributions on a given metric space formula_1.\n\nIntuitively, if each distribution is viewed as a unit amount of \"dirt\" piled on \"formula_1\", the metric is the minimum \"cost\" of turning one pile into the other, which is assumed to be the amount of dirt that needs to be moved times the mean distance it has to be moved. Because of this analogy, the metric is known in computer science as the earth mover's distance.\n\nThe name \"Wasserstein distance\" was coined by R. L. Dobrushin in 1970, after the Russian mathematician Leonid Vaseršteĭn who introduced the concept in 1969. Most English-language publications use the German spelling \"Wasserstein\" (attributed to the name \"Vaseršteĭn\" being of German origin).\n\nLet formula_3 be a metric space for which every probability measure on \"formula_1\" is a Radon measure (a so-called Radon space). For formula_5, let formula_6 denote the collection of all probability measures formula_7 on \"formula_1\" with finite formula_9 moment for some formula_10 in \"formula_1\",\n\nThen the formula_9 Wasserstein distance between two probability measures \"formula_7\" and formula_15 in formula_6 is defined as\n\nwhere formula_18 denotes the collection of all measures on formula_19 with marginals \"formula_7\" and \"formula_15\" on the first and second factors respectively. (The set formula_18 is also called the set of all couplings of \"formula_7\" and \"formula_15\".)\n\nThe above distance is usually denoted formula_25 (typically among authors who prefer the \"Wasserstein\" spelling) or formula_26 (typically among authors who prefer the \"Vaserstein\" spelling). The remainder of this article will use the \"formula_27\" notation.\n\nThe Wasserstein metric may be equivalently defined by\n\nwhere formula_29 denotes the expected value of a random variable formula_30 and the infimum is taken over all joint distributions of the random variables formula_31 and formula_32 with marginals \"formula_7\" and formula_15 respectively.\n\nOne way to understand the motivation of the above definition is to consider the optimal transport problem. That is, for a distribution of mass formula_35 on a space formula_31, we wish to transport the mass in such a way that it is transformed into the distribution formula_37 on the same space; transforming the 'pile of earth' formula_7 to the pile formula_15. This problem only makes sense if the pile to be created has the same mass as the pile to be moved; therefore without loss of generality assume that formula_7 and formula_15 are probability distributions containing a total mass of 1. Assume also that there is given some cost function \n\nthat gives the cost of transporting a unit mass from the point formula_43 to the point formula_44.\nA transport plan to move formula_7 into formula_15 can be described by a function formula_47 which gives the amount of mass to move from formula_43 to formula_44. In order for this plan to be meaningful, it must satisfy the following properties\nThat is, that the total mass moved \"out of\" an infinitesimal region around formula_43 must be equal to formula_52 and the total mass moved \"into\" a region around formula_43 must be formula_54. This is equivalent to the requirement that formula_55 be a joint probability distribution with marginals formula_7 and formula_15. Thus, the infinitesimal mass transported from formula_43 to formula_44 is formula_60, and the cost of moving is formula_61, following the definition of the cost function. Therefore, the total cost of a transport plan formula_55 is\n\nThe plan formula_55 is not unique; the optimal transport plan is the plan with the minimal cost out of all possible transport plans. As mentioned, the requirement for a plan to be valid is that it is a joint distribution with marginals formula_7 and formula_15; letting formula_67 denote the set of all such measures as in the first section, the cost of the optimal plan is\nIf the cost of a move is simply the distance between the two points, then the optimal cost is identical to the definition of the formula_69 distance.\n\nLet formula_70 and formula_71 be two degenerate distributions (i.e. Dirac delta distributions) located at points formula_72 and formula_73 in formula_74. There is only one possible coupling of these two measures, namely the point mass formula_75 located at formula_76. Thus, using the usual absolute value function as the distance function on formula_74, for any formula_78, the formula_79-Wasserstein distance between formula_80 and formula_81 is\nBy similar reasoning, if formula_70 and formula_71 are point masses located at points formula_72 and formula_73 in formula_87, and we use the usual Euclidean norm on formula_87 as the distance function, then\n\nLet formula_90 and formula_91 be two non-degenerate Gaussian measures (i.e. normal distributions) on formula_87, with respective expected values formula_93 and formula_94 and symmetric positive semi-definite covariance matrices formula_95 and formula_96. Then, with respect to the usual Euclidean norm on formula_87, the 2-Wasserstein distance between formula_80 and formula_81 is\nThis result generalises the earlier example of the Wasserstein distance between two point masses (at least in the case formula_101), since a point mass can be regarded as a normal distribution with covariance matrix equal to zero, in which case the trace term disappears and only the term involving the Euclidean distance between the means remains.\n\nThe Wasserstein metric is a natural way to compare the probability distributions of two variables \"X\" and \"Y\", where one variable is derived from the other by small, non-uniform perturbations (random or deterministic).\n\nIn computer science, for example, the metric \"W\" is widely used to compare discrete distributions, \"e.g.\" the color histograms of two digital images; see earth mover's distance for more details.\n\nIt can be shown that \"W\" satisfies all the axioms of a metric on P(\"M\"). Furthermore, convergence with respect to \"W\" is equivalent to the usual weak convergence of measures plus convergence of the first \"p\"th moments.\n\nThe following dual representation of \"W\" is a special case of the duality theorem of Kantorovich and Rubinstein (1958): when \"μ\" and \"ν\" have bounded support,\n\nwhere Lip(\"f\") denotes the minimal Lipschitz constant for \"f\".\n\nCompare this with the definition of the Radon metric:\n\nIf the metric \"d\" is bounded by some constant \"C\", then\n\nand so convergence in the Radon metric (identical to total variation convergence when \"M\" is a Polish space) implies convergence in the Wasserstein metric, but not vice versa.\n\nFor any \"p\" ≥ 1, the metric space (P(\"M\"), \"W\") is separable, and is complete if (\"M\", \"d\") is separable and complete.\n\n\n"}
