{"id": "202840", "url": "https://en.wikipedia.org/wiki?curid=202840", "title": "Adjugate matrix", "text": "Adjugate matrix\n\nIn linear algebra, the adjugate, classical adjoint, or adjunct of a square matrix is the transpose of its cofactor matrix.\n\nThe adjugate has sometimes been called the \"adjoint\", but today the \"adjoint\" of a matrix normally refers to its corresponding adjoint operator, which is its conjugate transpose.\n\nThe adjugate of is the transpose of the cofactor matrix of ,\n\nIn more detail, suppose is a commutative ring and is an matrix with entries from . The -\"minor\" of , denoted , is the determinant of the matrix that results from deleting row and column of . The cofactor matrix of is the matrix whose entry is the \"cofactor\" of , which is the -minor times a sign factor:\nThe adjugate of is the transpose of , that is, the matrix whose entry is the cofactor of ,\n\nThe adjugate is defined as it is so that the product of with its adjugate yields a diagonal matrix whose diagonal entries are the determinant . That is,\nwhere is the identity matrix. This is a consequence of the Laplace expansion of the determinant.\n\nThe above formula implies one of the fundamental results in matrix algebra, that is invertible if and only if is an invertible element of . When this holds, the equation above yields\n\nThe adjugate of any 1×1 matrix is formula_6.\n\nThe adjugate of the 2×2 matrix\nis\nBy direct computation,\nIn this case, it is also true that det(adj(A)) = det(A) and hence that adj(adj(A)) = A.\n\nConsider a 3×3 matrix\nIts cofactor matrix is\nwhere\n\nIts adjugate is the transpose of its cofactor matrix,\n\nAs a specific example, we have\nIt is easy to check the adjugate is the inverse times the determinant, .\n\nThe in the second row, third column of the adjugate was computed as follows. The (2,3) entry of the adjugate is the (3,2) cofactor of A. This cofactor is computed using the submatrix obtained by deleting the third row and second column of the original matrix A,\nThe (3,2) cofactor is a sign times the determinant of this submatrix:\nand this is the (2,3) entry of the adjugate.\n\nFor any matrix , elementary computations show that adjugates enjoy the following properties.\n\nOver the complex numbers,\n\nSuppose that is another matrix. Then\nThis can be proved in two ways. One way, valid for any \ncommutative ring, is a direct computation using the \nCauchy–Binet formula. The other way, valid for the real \nor complex numbers, is to first observe that for invertible \nmatrices and ,\nBecause every non-invertible matrix is the limit of invertible matrices, continuity of the adjugate then implies that the formula remains true when one of or is not invertible.\n\nA corollary of the previous formula is that, for any non-negative integer ,\nIf is invertible, then the above formula also holds for negative .\n\nFrom the identity\nwe deduce\n\nSuppose that commutes with . Multiplying the identity on the left and right by proves that\nIf is invertible, this implies that also commutes with . Over the real or complex numbers, continuity implies that commutes with even when is not invertible.\n\nUsing the above properties and other elementary computations, it is straightforward to show that if has one of the following properties, then does as well:\n\nIf is invertible, then, as noted above, there is a formula for in terms of the determinant and inverse of . When is not invertible, the adjugate satisfies different but closely related formulas.\n\nPartition into column vectors:\nLet be a column vector of size . Fix and consider the matrix formed by replacing column of by :\nLaplace expand the determinant of this matrix along column . The result is entry of the product . Collecting these determinants for the different possible yields an equality of column vectors\n\nThis formula has the following concrete consequence. Consider the linear system of equations\nAssume that is non-singular. Multiplying this system on the left by and dividing by the determinant yields\nApplying the previous formula to this situation yields Cramer's rule,\nwhere is the th entry of .\n\nLet the characteristic polynomial of be\nThe first divided difference of is a symmetric polynomial of degree ,\nMultiply by its adjugate. Since by the Cayley–Hamilton theorem, some elementary manipulations reveal\n\nIn particular, the resolvent of is defined to be\nand by the above formula, this is equal to\n\nThe adjugate also appears in Jacobi's formula for the derivative of the determinant. If is continuously differentiable, then\nIt follows that the total derivative of the determinant is the transpose of the adjugate:\n\nLet be the characteristic polynomial of . The Cayley–Hamilton theorem states that\nSeparating the constant term and multiplying the equation by gives an expression for the adjugate that depends only on and the coefficients of . These coefficients can be explicitly represented in terms of traces of powers of using complete exponential Bell polynomials. The resulting formula is\nwhere is the dimension of , and the sum is taken over and all sequences of satisfying the linear Diophantine equation\n\nFor the 2×2 case, this gives\nFor the 3×3 case, this gives\nFor the 4×4 case, this gives\n\nThe same formula follows directly from the terminating step of the Faddeev–LeVerrier algorithm, which efficiently determines the characteristic polynomial of .\n\nThe adjugate can be viewed in abstract terms using exterior algebras. Let be an -dimensional vector space. The exterior product defines a bilinear pairing\nAbstractly, formula_53 is isomorphic to , and under any such isomorphism the exterior product is a perfect pairing. Therefore, it yields an isomorphism\nExplicitly, this pairing sends to formula_55, where\nSuppose that is a linear transformation. Pullback by the st exterior power of induces a morphism of spaces. The adjugate of is the composite\n\nIf is endowed with its coordinate basis , and if the matrix of in this basis is , then the adjugate of is the adjugate of . To see why, give formula_58 the basis\nFix a basis vector of . The image of under formula_60 is determined by where it sends basis vectors:\nOn basis vectors, the st exterior power of is\nEach of these terms maps to zero under formula_63 except the term. Therefore, the pullback of formula_63 is the linear transformation for which\nthat is, it equals\nApplying the inverse of formula_60 shows that the adjugate of is the linear transformation for which\nConsequently, its matrix representation is the adjugate of .\n\nIf is endowed with an inner product and a volume form, then the map can be decomposed further. In this case, can be understood as the composite of the Hodge star operator and dualization. Specifically, if is the volume form, then it, together with the inner product, determines an isomorphism\nThis induces an isomorphism\nA vector in corresponds to the linear functional\nBy the definition of the Hodge star operator, this linear functional is dual to . That is, equals .\n\nLet be an matrix, and fix . The th higher adjugate of is an formula_72 matrix, denoted , whose entries are indexed by size subsets and of . Let and denote the complements of and , respectively. Also let formula_73 denote the submatrix of containing those rows and columns whose indices are in and , respectively. Then the entry of is\nwhere and are the sum of the elements of and , respectively.\n\nBasic properties of higher adjugates include:\n\nHigher adjugates may be defined in abstract algebraic terms in a similar fashion to the usual adjugate, substituting formula_76 and formula_77 for formula_78 and formula_79, respectively.\n\nIteratively taking the adjugate of an invertible matrix A times yields\n\nFor example,\n\n\n\n"}
{"id": "614969", "url": "https://en.wikipedia.org/wiki?curid=614969", "title": "András Gyárfás", "text": "András Gyárfás\n\nAndrás Gyárfás (born 1945) is a Hungarian mathematician who specializes in the study of graph theory. Together with Paul Erdős he conjectured what is now called the Erdős–Gyárfás conjecture which states that any graph with minimum degree 3 contains a simple cycle whose length is a power of two. He and David Sumner independently formulated the Gyárfás–Sumner conjecture according to which, for every tree \"T\", the \"T\"-free graphs are χ-bounded.\n\nGyárfás began working as a researcher for the Computer and Automation Research Institute of the Hungarian Academy of Sciences in 1968. He earned a candidate degree in 1980, and a doctorate (Dr. Math. Sci.) in 1992. He won the Géza Grünwald Commemorative Prize for young researchers of the János Bolyai Mathematical Society in 1978.\n\n"}
{"id": "21431954", "url": "https://en.wikipedia.org/wiki?curid=21431954", "title": "Behavior of coupled DEVS", "text": "Behavior of coupled DEVS\n\n\"DEVS is closed under coupling\" [Zeigper84] [ZPK00]. In other words, given a coupled DEVS model formula_1, its behavior is described as an atomic DEVS model formula_2. For a given coupled DEVS formula_1, once we have an equivalent atomic DEVS formula_4, behavior of formula_4 can be referred to behavior of atomic DEVS which is based on Timed Event System.\n\nSimilar to behavior of atomic DEVS, behavior of the Coupled DEVS class is described depending on definition of the total state set and its handling as follows.\n\nGiven a coupled DEVS model formula_6, its behavior is described as an atomic DEVS model formula_7\n\nwhere\n \nwhere\n\nGiven the partial state formula_27, let formula_28 denote \"the set of imminent components\". The \"firing component\" formula_29 which triggers the internal state transition and an output event is determined by formula_30\n\nwhere\n\n\nGiven a coupled DEVS model formula_6, its behavior is described as an atomic DEVS model formula_7\n\nwhere\n \nwhere\nand\n\nGiven the partial state formula_58, let formula_59 denote \"the set of imminent components\". The \"firing component\" formula_29 which triggers the internal state transition and an output event is determined by formula_30\n\nwhere\n\n\nSince in a coupled DEVS model with non-empty sub-components, i.e., formula_69, the number of clocks which trace their elapsed times are multiple, so time passage of the model is noticeable. \nGiven a total state formula_70 where formula_71\n\nIf unit event segment formula_72 is the null event segment, i.e. formula_73, the state trajectory in terms of Timed Event System is \n\nGiven a total state formula_75 where formula_76\n\nIf unit event segment formula_72 is the null event segment, i.e. formula_73, the state trajectory in terms of Timed Event System is \n\n\n\n"}
{"id": "44578", "url": "https://en.wikipedia.org/wiki?curid=44578", "title": "Big O notation", "text": "Big O notation\n\nBig O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. It is a member of a family of notations invented by Paul Bachmann, Edmund Landau, and others, collectively called Bachmann–Landau notation or asymptotic notation.\n\nIn computer science, big O notation is used to classify algorithms according to how their running time or space requirements grow as the input size grows. In analytic number theory, big O notation is often used to express a bound on the difference between an arithmetical function and a better understood approximation; a famous example of such a difference is the remainder term in the prime number theorem.\n\nBig O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation.\n\nThe letter O is used because the growth rate of a function is also referred to as the order of the function. A description of a function in terms of big O notation usually only provides an upper bound on the growth rate of the function. Associated with big O notation are several related notations, using the symbols , to describe other kinds of bounds on asymptotic growth rates.\n\nBig O notation is also used in many other fields to provide similar estimates.\n\nLet \"f\" be a real or complex valued function and \"g\" a real valued function, both defined on some unbounded subset of the real positive numbers, such that \"g(x)\" is strictly positive for all large enough values of \"x\". One writes\n\nif and only if for all sufficiently large values of \"x\", the absolute value of \"f\"(\"x\") is at most a positive constant multiple of \"g\"(\"x\"). That is, \"f\"(\"x\") = \"O\"(\"g\"(\"x\")) if and only if there exists a positive real number \"M\" and a real number \"x\" such that\n\nIn many contexts, the assumption that we are interested in the growth rate as the variable \"x\" goes to infinity is left unstated, and one writes more simply that\n\nThe notation can also be used to describe the behavior of \"f\" near some real number \"a\" (often, \"a\" = 0): we say\n\nif and only if there exist positive numbers \"δ\" and \"M\" such that\n\nAs \"g\"(\"x\") is chosen to be non-zero for values of \"x\" sufficiently close to \"a\", both of these definitions can be unified using the limit superior:\n\nif and only if\n\nIn typical usage, the formal definition of \"O\" notation is not used directly; rather, the \"O\" notation for a function \"f\" is derived by the following simplification rules:\nFor example, let \"f\"(\"x\") = 6\"x\" − 2\"x\" + 5, and suppose we wish to simplify this function, using \"O\" notation, to describe its growth rate as \"x\" approaches infinity. This function is the sum of three terms: 6\"x\", −2\"x\", and 5. Of these three terms, the one with the highest growth rate is the one with the largest exponent as a function of \"x\", namely 6\"x\". Now one may apply the second rule: 6\"x\" is a product of 6 and \"x\" in which the first factor does not depend on \"x\". Omitting this factor results in the simplified form \"x\". Thus, we say that \"f\"(\"x\") is a \"big-oh\" of (\"x\"). Mathematically, we can write \"f\"(\"x\") = \"O\"(\"x\").\nOne may confirm this calculation using the formal definition: let \"f\"(\"x\") = 6\"x\" − 2\"x\" + 5 and \"g\"(\"x\") = \"x\". Applying the formal definition from above, the statement that \"f\"(\"x\") = \"O\"(\"x\") is equivalent to its expansion,\nfor some suitable choice of \"x\" and \"M\" and for all \"x\" > \"x\". To prove this, let \"x\" = 1 and \"M\" = 13. Then, for all \"x\" > \"x\":\nso\n\nBig O notation has two main areas of application:\n\nIn both applications, the function \"g\"(\"x\") appearing within the \"O\"(...) is typically chosen to be as simple as possible, omitting constant factors and lower order terms.\n\nThere are two formally close, but noticeably different, usages of this notation: \n\nThis distinction is only in application and not in principle, however—the formal definition for the \"big O\" is the same for both cases, only with different limits for the function argument.\n\nBig O notation is useful when analyzing algorithms for efficiency. For example, the time (or the number of steps) it takes to complete a problem of size \"n\" might be found to be \"T\"(\"n\") = 4\"n\" − 2\"n\" + 2.\nAs \"n\" grows large, the \"n\" term will come to dominate, so that all other terms can be neglected—for instance when \"n\" = 500, the term 4\"n\" is 1000 times as large as the 2\"n\" term. Ignoring the latter would have negligible effect on the expression's value for most purposes.\nFurther, the coefficients become irrelevant if we compare to any other order of expression, such as an expression containing a term \"n\" or \"n\". Even if \"T\"(\"n\") = 1,000,000\"n\", if \"U\"(\"n\") = \"n\", the latter will always exceed the former once \"n\" grows larger than 1,000,000 (\"T\"(1,000,000) = 1,000,000= \"U\"(1,000,000)). Additionally, the number of steps depends on the details of the machine model on which the algorithm runs, but different types of machines typically vary by only a constant factor in the number of steps needed to execute an algorithm.\nSo the big O notation captures what remains: we write either\nor\nand say that the algorithm has \"order of n\" time complexity.\nNote that \"=\" is not meant to express \"is equal to\" in its normal mathematical sense, but rather a more colloquial \"is\", so the second expression is sometimes considered more accurate (see the \"Equals sign\" discussion below) while the first is considered by some as an abuse of notation.\n\nBig O can also be used to describe the error term in an approximation to a mathematical function. The most significant terms are written explicitly, and then the least-significant terms are summarized in a single big O term. Consider, for example, the exponential series and two expressions of it that are valid when \"x\" is small:\nThe second expression (the one with \"O\"(\"x\")) means the absolute-value of the error \"e\" − (1 + \"x\" + \"x\"/2) is at most some constant times |\"x\"| when \"x\" is close enough to 0.\n\nIf the function \"f\" can be written as a finite sum of other functions, then the fastest growing one determines the order of\n\"f\"(\"n\"). For example, \nIn particular, if a function may be bounded by a polynomial in \"n\", then as \"n\" tends to \"infinity\", one may disregard \"lower-order\" terms of the polynomial. \nAnother thing to notice is the sets \"O\"(\"n\") and \"O\"(\"c\") are very different. If \"c\" is greater than one, then the latter grows much faster. A function that grows faster than \"n\" for any \"c\" is called \"superpolynomial\". One that grows more slowly than any exponential function of the form \"c\" is called \"subexponential\". An algorithm can require time that is both superpolynomial and subexponential; examples of this include the fastest known algorithms for integer factorization and the function \"n\".\n\nWe may ignore any powers of \"n\" inside of the logarithms. The set \"O\"(log \"n\") is exactly the same as \"O\"(log(\"n\")). The logarithms differ only by a constant factor (since\nlog(\"n\") = \"c\" log \"n\") and thus the big O notation ignores that. Similarly, logs with different constant bases are equivalent. On the other hand, exponentials with different bases are not of the same order. For example, 2 and 3 are not of the same order.\n\nChanging units may or may not affect the order of the resulting algorithm. Changing units is equivalent to multiplying the appropriate variable by a constant wherever it appears. For example, if an algorithm runs in the order of \"n\", replacing \"n\" by \"cn\" means the algorithm runs in the order of \"c\"\"n\", and the big O notation ignores the constant \"c\". This can be written as \"c\"\"n\" = O(\"n\"). If, however, an algorithm runs in the order of 2, replacing \"n\" with \"cn\" gives 2 = (2). This is not equivalent to 2 in general.\nChanging variables may also affect the order of the resulting algorithm. For example, if an algorithm's run time is \"O\"(\"n\") when measured in terms of the number \"n\" of \"digits\" of an input number \"x\", then its run time is \"O\"(log \"x\") when measured as a function of the input number \"x\" itself, because \"n\" = \"O\"(log \"x\").\n\nThis implies formula_18, which means that formula_19 is a convex cone.\n\nBig \"O\" (and little o, and Ω...) can also be used with multiple variables.\nTo define Big \"O\" formally for multiple variables, suppose formula_22 and formula_23 are two functions defined on some subset of formula_24. We say\nif and only if\nEquivalently, the condition that formula_27 for some formula_28 can be replaced with the condition that formula_29, where formula_30 denotes the Chebyshev norm. For example, the statement\nasserts that there exist constants \"C\" and \"M\" such that\nwhere \"g\"(\"n\",\"m\") is defined by\nNote that this definition allows all of the coordinates of formula_34 to increase to infinity. In particular, the statement\n(i.e., formula_36) is quite different from\n(i.e., formula_38).\n\nNote that under this definition, the subset on which a function is defined is significant when generalizing statements from the univariate setting to the multivariate setting. For example, if formula_39 and formula_40, then formula_41 if we restrict formula_22 and formula_23 to formula_44, but not if they are defined on formula_45.\n\nThis is not the only generalization of big O to multivariate functions, and in practice, there is some inconsistency in the choice of definition.\n\nThe statement \"\"f\"(\"x\") is \"O\"(\"g\"(\"x\"))\" as defined above is usually written as \"f\"(\"x\") = \"O\"(\"g\"(\"x\")). Some consider this to be an abuse of notation, since the use of the equals sign could be misleading as it suggests a symmetry that this statement does not have. As de Bruijn says, \"O\"(\"x\") = \"O\"(\"x\") is true but \"O\"(\"x\") = \"O\"(\"x\") is not. Knuth describes such statements as \"one-way equalities\", since if the sides could be reversed, \"we could deduce ridiculous things like \"n\" = \"n\" from the identities \"n\" = \"O\"(\"n\") and \"n\" = \"O\"(\"n\").\"\n\nFor these reasons, it would be more precise to use set notation and write \"f\"(\"x\") ∈ \"O\"(\"g\"(\"x\")), thinking of \"O\"(\"g\"(\"x\")) as the class of all functions \"h\"(\"x\") such that |\"h\"(\"x\")| ≤ \"C\"|\"g\"(\"x\")| for some constant \"C\". However, the use of the equals sign is customary. Knuth pointed out that \"mathematicians customarily use the = sign as they use the word 'is' in English: Aristotle is a man, but a man isn't necessarily Aristotle.\"\n\nBig O notation can also be used in conjunction with other arithmetic operators in more complicated equations. For example, \"h\"(\"x\") + \"O\"(\"f\"(\"x\")) denotes the collection of functions having the growth of \"h\"(\"x\") plus a part whose growth is limited to that of \"f\"(\"x\"). Thus,\nexpresses the same as\n\nSuppose an algorithm is being developed to operate on a set of \"n\" elements. Its developers are interested in finding a function \"T\"(\"n\") that will express how long the algorithm will take to run (in some arbitrary measurement of time) in terms of the number of elements in the input set. The algorithm works by first calling a subroutine to sort the elements in the set and then perform its own operations. The sort has a known time complexity of \"O\"(\"n\"), and after the subroutine runs the algorithm must take an additional 55\"n\" + 2\"n\" + 10 steps before it terminates. Thus the overall time complexity of the algorithm can be expressed as \"T\"(\"n\") = 55\"n\" + \"O\"(\"n\").\nHere the terms 2\"n\"+10 are subsumed within the faster-growing \"O\"(\"n\"). Again, this usage disregards some of the formal meaning of the \"=\" symbol, but it does allow one to use the big O notation as a kind of convenient placeholder.\n\nIn more complicated usage, \"O\"(...) can appear in different places in an equation, even several times on each side. For example, the following are true for formula_48\nThe meaning of such statements is as follows: for \"any\" functions which satisfy each \"O\"(...) on the left side, there are \"some\" functions satisfying each \"O\"(...) on the right side, such that substituting all these functions into the equation makes the two sides equal. For example, the third equation above means: \"For any function \"f\"(\"n\") = \"O\"(1), there is some function \"g\"(\"n\") = \"O\"(\"e\") such that \"n\" = \"g\"(\"n\").\" In terms of the \"set notation\" above, the meaning is that the class of functions represented by the left side is a subset of the class of functions represented by the right side. In this use the \"=\" is a formal symbol that unlike the usual use of \"=\" is not a symmetric relation. Thus for example \"n\" = \"O\"(\"e\") does not imply the false statement \"O\"(\"e\") = \"n\"\n\nBig O consists of just an uppercase \"O\". Unlike Greek-named Bachmann–Landau notations, it needs no special symbol. Yet, commonly used calligraphic variants, like formula_52, are available, in LaTeX and derived typesetting systems.\n\nHere is a list of classes of functions that are commonly encountered when analyzing the running time of an algorithm. In each case, \"c\" is a positive constant and \"n\" increases without bound. The slower-growing functions are generally listed first.\nThe statement formula_53 is sometimes weakened to formula_54 to derive simpler formulas for asymptotic complexity.\nFor any formula_55 and formula_56, formula_57 is a subset of formula_58 for any formula_59, so may be considered as a polynomial with some bigger order.\n\nBig \"O\" is the most commonly used asymptotic notation for comparing functions. Together with some other related notations it forms the family of Bachmann–Landau notations.\n\nIntuitively, the assertion \" is \" (read \" is little-o of \") means that grows much faster than . Let as before \"f\" be a real or complex valued function and \"g\" a real valued function, both defined on some unbounded subset of the real positive numbers, such that \"g(x)\" is strictly positive for all large enough values of \"x\". One writes\nIf for every positive constant there exists a constant such that\n\nThe difference between the earlier definition for the big-O notation and the present definition of little-o, is that while the former has to be true for \"at least one\" constant \"M\", the latter must hold for \"every\" positive constant , however small. In this way, little-o notation makes a \"stronger statement\" than the corresponding big-O notation: every function that is little-o of \"g\" is also big-O of \"g\", but not every function that is big-O of \"g\" is also little-o of \"g\". For example, formula_64 but formula_65\n\nAs \"g\"(\"x\") is nonzero, or at least becomes nonzero beyond a certain point, the relation is equivalent to\n\nLittle-o respects a number of arithmetic operations. For example,\nIt also satisfies a transitivity relation:\n\nThere are two very widespread and incompatible definitions of the statement\n\nwhere \"a\" is some real number, ∞, or −∞, where \"f\" and \"g\" are real functions defined in a neighbourhood of \"a\", and where \"g\" is positive in this neighbourhood.\n\nThe first one (chronologically) is used in analytic number theory, and the other one in computational complexity theory. When the two subjects meet, this situation is bound to generate confusion.\n\nIn 1914 Godfrey Harold Hardy and John Edensor Littlewood introduced the new symbol formula_76, which is defined as follows:\n\nThus formula_78 is the negation of formula_79.\n\nIn 1916 the same authors introduced the two new symbols formula_80 and formula_81, defined thusly:\n\nThese symbols were used by Edmund Landau, with the same meanings, in 1924. After Landau, the notations were never used again exactly thus; formula_80 became formula_85 and formula_81 became formula_87.\n\nThese three symbols formula_88, as well as formula_89 (meaning that formula_90 and formula_91 are both satisfied), are now currently used in analytic number theory.\n\nWe have\n\nand more precisely\n\nWe have\n\nand more precisely\n\nhowever\n\nIn 1976 Donald Knuth published a paper to justify his use of the formula_76-symbol to describe a stronger property. Knuth wrote: \"For all the applications I have seen so far in computer science, a stronger requirement […] is much more appropriate\". He defined\n\nwith the comment: \"Although I have changed Hardy and Littlewood's definition of formula_76, I feel justified in doing so because their definition is by no means in wide use, and because there are other ways to say what they want to say in the comparatively rare cases when their definition applies.\"\n\nThe limit definitions assume formula_118 for sufficiently large . The table is (partly) sorted from smallest to largest, in the sense that o, O, Θ, ∼, (Knuth's version of) Ω, ω on functions correspond to <, ≤, ≈, =, ≥, > on the real line (the Hardy-Littlewood version of Ω, however, doesn't correspond to any such description). \n\nComputer science uses the big \"O\", Big Theta Θ, little \"o\", little omega ω and Knuth's big Omega Ω notations. Analytic number theory often uses the big \"O\", small \"o\", Hardy–Littlewood's big Omega Ω (with or without the +, - or ± subscripts) and formula_119 notations. The small omega ω notation is not used as often in analysis.\n\nInformally, especially in computer science, the Big \"O\" notation often is permitted to be somewhat abused to describe an asymptotic tight bound where using Big Theta Θ notation might be more factually appropriate in a given context. For example, when considering a function \"T\"(\"n\") = 73\"n\" + 22\"n\" + 58, all of the following are generally acceptable, but tighter bounds (i.e., numbers 2 and 3 below) are usually strongly preferred over looser bounds (i.e., number 1 below).\nThe equivalent English statements are respectively:\nSo while all three statements are true, progressively more information is contained in each. In some fields, however, the big O notation (number 2 in the lists above) would be used more commonly than the Big Theta notation (bullets number 3 in the lists above). For example, if \"T\"(\"n\") represents the running time of a newly developed algorithm for input size \"n\", the inventors and users of the algorithm might be more inclined to put an upper asymptotic bound on how long it will take to run without making an explicit statement about the lower asymptotic bound.\n\nIn their book \"Introduction to Algorithms\", Cormen, Leiserson, Rivest and Stein consider the set of functions \"f\" which satisfy \n\nIn a correct notation this set can for instance be called \"O\"(\"g\"), where\n\nThe authors state that the use of equality operator (=) to denote set membership rather than the set membership operator (∈) is an abuse of notation, but that doing so has advantages. Inside an equation or inequality, the use of asymptotic notation stands for an anonymous function in the set \"O\"(\"g\"), which eliminates lower-order terms, and helps to reduce inessential clutter in equations, for example:\n\nAnother notation sometimes used in computer science is Õ (read \"soft-O\"): \"f\"(\"n\") = \"Õ\"(\"g\"(\"n\")) is shorthand\nfor \"f\"(\"n\") = \"O\"(\"g\"(\"n\") log \"g\"(\"n\")) for some \"k\". Essentially, it is big O notation, ignoring logarithmic factors because the growth-rate effects of some other super-logarithmic function indicate a growth-rate explosion for large-sized input parameters that is more important to predicting bad run-time performance than the finer-point effects contributed by the logarithmic-growth factor(s). This notation is often used to obviate the \"nitpicking\" within growth-rates that are stated as too tightly bounded for the matters at hand (since log \"n\" is always \"o\"(\"n\") for any constant \"k\" and any ε > 0).\n\nAlso the L notation, defined as\nis convenient for functions that are between polynomial and exponential in terms of formula_127.\n\nThe generalization to functions taking values in any normed vector space is straightforward (replacing absolute values by norms), where \"f\" and \"g\" need not take their values in the same space. A generalization to functions \"g\" taking values in any topological group is also possible.\nThe \"limiting process\" \"x\" → \"x\" can also be generalized by introducing an arbitrary filter base, i.e. to directed nets \"f\" and \"g\".\nThe \"o\" notation can be used to define derivatives and differentiability in quite general spaces, and also (asymptotical) equivalence of functions,\nwhich is an equivalence relation and a more restrictive notion than the relationship \"\"f\" is Θ(\"g\")\" from above. (It reduces to lim \"f\" / \"g\" = 1 if \"f\" and \"g\" are positive real valued functions.) For example, 2\"x\" is Θ(\"x\"), but 2\"x\" − \"x\" is not \"o\"(\"x\").\n\nThe symbol O was first introduced by number theorist Paul Bachmann in 1894, in the second volume of his book \"Analytische Zahlentheorie\" (\"analytic number theory\"), the first volume of which (not yet containing big O notation) was published in 1892. The number theorist Edmund Landau adopted it, and was thus inspired to introduce in 1909 the notation o; hence both are now called Landau symbols. These notations were used in applied mathematics during the 1950s for asymptotic analysis. \nThe symbol formula_76 (in the sense \"is not an \"o\" of\") was introduced in 1914 by Hardy and Littlewood. Hardy and Littlewood also introduced in 1918 the symbols formula_80 (\"right\") and formula_81 (\"left\"), precursors of the modern symbols formula_85 (\"is not smaller than a small o of\") and formula_87 (\"is not larger than a small o of\"). Thus the Omega symbols (with their original meanings) are sometimes also referred to as \"Landau symbols\". This notation formula_76 became commonly used in number theory at least since the 1950s.\nIn the 1970s the big O was popularized in computer science by Donald Knuth, who introduced the related Theta notation, and proposed a different definition for the Omega notation.\n\nLandau never used the Big Theta and small omega symbols.\n\nHardy's symbols were (in terms of the modern \"O\" notation)\n(Hardy however never defined or used the notation formula_137, nor formula_138, as it has been sometimes reported).\nIt should also be noted that Hardy introduces the symbols formula_139 and formula_140 (as well as some other symbols) in his 1910 tract \"Orders of Infinity\", and makes use of it only in three papers (1910–1913). In his nearly 400 remaining papers and books he consistently uses the Landau symbols O and o.\n\nHardy's notation is not used anymore. On the other hand, in the 1930s, the Russian number theorist Ivan Matveyevich Vinogradov introduced his notation formula_138, which has been increasingly used in number theory instead of the formula_142 notation. We have \nand frequently both notations are used in the same paper.\nThe big-O originally stands for \"order of\" (\"Ordnung\", Bachmann 1894), and is thus a Latin letter. Neither Bachmann nor Landau ever call it \"Omicron\". The symbol was much later on (1976) viewed by Knuth as a capital omicron, probably in reference to his definition of the symbol Omega. The digit zero should not be used.\n\n\n\n"}
{"id": "5300610", "url": "https://en.wikipedia.org/wiki?curid=5300610", "title": "Boolean delay equation", "text": "Boolean delay equation\n\nAs a novel type of semi-discrete dynamical systems, Boolean delay equations (BDEs) are models with Boolean-valued variables that evolve in continuous time. Since at the present time, most phenomena are too complex to be modeled by partial differential equations (as continuous infinite-dimensional systems), BDEs are intended as a (heuristic) first step on the challenging road to further understanding and modeling them. For instance, one can mention complex problems in fluid dynamics, climate dynamics, solid-earth geophysics, and many problems elsewhere in natural sciences where much of the discourse is still conceptual.\n\nAlthough in recent centuries, differential equations (both ordinary and partial) have extensively served as quantitative models of vast categories of problems, by the recent greedy and rapid burst of complexities everywhere, the gap between quantitative and qualitative modeling and reasoning techniques is widening. BDEs offer a formal mathematical language that is promising to help bridge that gap.\n\n"}
{"id": "710299", "url": "https://en.wikipedia.org/wiki?curid=710299", "title": "Bounded complete poset", "text": "Bounded complete poset\n\nIn the mathematical field of order theory, a partially ordered set is bounded complete if all of its subsets that have some upper bound also have a least upper bound. Such a partial order can also be called consistently or coherently complete (Visser 2004, p. 182), since any upper bound of a set can be interpreted as some consistent (non-contradictory) piece of information that extends all the information present in the set. Hence the presence of some upper bound in a way guarantees the consistency of a set. Bounded completeness then yields the existence of a least upper bound of any \"consistent\" subset, which can be regarded as the most general piece of information that captures all the knowledge present within this subset. This view closely relates to the idea of information ordering that one typically finds in domain theory. \n\nFormally, a partially ordered set (\"P\", ≤) is \"bounded complete\" if the following holds for any subset \"S\" of \"P\":\n\nBounded completeness has various relationships to other completeness properties, which are detailed in the article on completeness in order theory. Note also that the term \"bounded poset\" is sometimes used to refer to a partially ordered set that has both a least and a greatest element. Hence it is important to distinguish between a bounded-complete poset and a bounded complete partial order (cpo).\n\nFor a typical example of a bounded-complete poset, consider the set of all finite decimal numbers starting with \"0.\" (like 0.1, 0.234, 0.122) together with all infinite such numbers (like the decimal representation 0.1111... of 1/9). Now these elements can be ordered based on the prefix order of words: a decimal number \"n\" is below some other number \"m\" if there is some string of digits w such that \"nw\" = \"m\". For example, 0.2 is below 0.234, since one can obtain the latter by appending the string \"34\" to 0.2. The infinite decimal numbers are the maximal elements within this order. In general, subsets of this order do not have least upper bounds: just consider the set {0.1, 0.3}. Looking back at the above intuition, one might say that it is not consistent to assume that some number starts both with 0.1 and with 0.3. However, the order is still bounded complete. In fact, it is even an example of a more specialized class of structures, the Scott domains, which provide many other examples for bounded-complete posets.\n\n"}
{"id": "3875858", "url": "https://en.wikipedia.org/wiki?curid=3875858", "title": "Brauer algebra", "text": "Brauer algebra\n\nIn mathematics, a Brauer algebra is an algebra introduced by used in the representation theory of the orthogonal group. It plays the same role that the symmetric group does for the representation theory of the general linear group in Schur–Weyl duality.\n\nThe Brauer algebra depends on the choice of a positive integer \"n\" and a number \"d\" (which in practice is often the dimension of the fundamental representation of an orthogonal group \"O\"). The Brauer algebra has dimension (2\"n\")!/2\"n\"! = (2\"n\" − 1)(2\"n\" − 3) ··· 5·3·1 and has a basis consisting of all pairings on a set of 2\"n\" elements \"X\", ..., \"X\", \"Y\", ..., \"Y\" (that is, all perfect matchings of a complete graph \"K\": any two of the 2\"n\" elements may be matched to each other, regardless of their symbols). The elements \"X\" are usually written in a row, with the elements \"Y\" beneath them. The product of two basis elements \"A\" and \"B\" is obtained by first identifying the endpoints in the bottom row of \"A \" and the top row of \"B \" (Figure \"AB \" in the diagram), then deleting the endpoints in the middle row and joining endpoints in the remaining two rows if they are joined, directly or by a path, in \"AB \" (Figure \"AB=nn \" in the diagram).\n\nIf \"O\"(R) is the orthogonal group acting on \"V\" = R, then \nthe Brauer algebra has a natural action on the space of polynomials on \"V\" commuting with the action of the orthogonal group.\n\n\n"}
{"id": "14087640", "url": "https://en.wikipedia.org/wiki?curid=14087640", "title": "Centroidal Voronoi tessellation", "text": "Centroidal Voronoi tessellation\n\nIn geometry, a centroidal Voronoi tessellation (CVT) is a special type of Voronoi tessellation or Voronoi diagram. A Voronoi tessellation is called centroidal when the generating point of each Voronoi cell is also its mean (center of mass). It can be viewed as an optimal partition corresponding to an optimal distribution of generators. A number of algorithms can be used to generate centroidal Voronoi tessellations, including Lloyd's algorithm for K-means clustering.\n\nGersho's conjecture, proven for one and two dimensions, says that \"asymptotically speaking, all cells of the optimal CVT, while forming a tessellation, are congruent to a basic cell which depends on the dimension.\" In two dimensions, the basic cell for the optimal CVT is a regular hexagon.\n\nCentroidal Voronoi tessellations are useful in data compression, optimal quadrature, optimal quantization, clustering, and optimal mesh generation. Many patterns seen in nature are closely approximated by a Centroidal Voronoi tessellation. Examples of this include the Giant's Causeway, the cells of the cornea, and the breeding pits of the male tilapia.\n\nA weighted centroidal Voronoi diagrams is a CVT in which each centroid is weighted according to a certain function. For example, a grayscale image can be used as a density function to weight the points of a CVT, as a way to create digital stippling.\n"}
{"id": "32809583", "url": "https://en.wikipedia.org/wiki?curid=32809583", "title": "Chihara–Ismail polynomials", "text": "Chihara–Ismail polynomials\n\nIn mathematics, the Chihara–Ismail polynomials are a family of orthogonal polynomials introduced by , generalizing the van Doorn polynomials introduced by and the Karlin–McGregor polynomials. They have a rather unusual measure, which is discrete except for a single limit point at 0 with jump 0, and is non-symmetric, but whose support has an infinite number of both positive and negative points.\n\n"}
{"id": "57910969", "url": "https://en.wikipedia.org/wiki?curid=57910969", "title": "Christopher J. Bishop", "text": "Christopher J. Bishop\n\nChristopher Bishop is an American mathematician at Stony Brook University. He is known for his contributions to geometric function theory, Kleinian groups, complex dynamics, and computational geometry; and in particular for topics such as fractals, harmonic measure, conformal and quasiconformal mappings and Julia sets. He received his Ph.D. from the University of Chicago in 1987, under the supervision of Peter Jones.\n\nBishop was awarded the 1992 A. P. Sloan Foundation fellowship, was an invited speaker at the 2018 International Congress of Mathematicians. He was included in the 2019 class of fellows of the American Mathematical Society \"for contributions to the theory of harmonic measures, quasiconformal maps and transcendental dynamics\".\n\nWith Yuval Peres, Bishop is the author of the book \"Fractals in Probability and Analysis\" (Cambridge Studies in Advanced Mathematics 162, 2009).\n\n"}
{"id": "2189987", "url": "https://en.wikipedia.org/wiki?curid=2189987", "title": "Complete market", "text": "Complete market\n\nIn economics, a complete market (aka Arrow-Debreu market or complete system of markets) is a market with two conditions:\n\n\nIn such a market, the complete set of possible bets on future states-of-the-world can be constructed with existing assets without friction. Here goods are state-contingent; that is, a good includes the time and state of the world in which it is consumed. So for instance, an umbrella tomorrow if it rains is a distinct good from an umbrella tomorrow if it is clear. The study of complete markets is central to state-preference theory. The theory can be traced to the work of Kenneth Arrow (1964), Gérard Debreu (1959), Arrow & Debreu (1954) and Lionel McKenzie(1954). Arrow and Debreu were awarded the Nobel Memorial Prize in Economics (Arrow in 1972, Debreu in 1983), largely for their work in developing the theory of complete markets and applying it to the problem of general equilibrium.\n\nA state of the world is a complete specification of the values of all relevant variables over the relevant time horizon. A state-contingent claim, or state claim, is a contract whose future payoffs depend on future states of the world. For example, suppose you can bet on the outcome of a coin toss. If you guess the outcome correctly, you will win one dollar, and otherwise you will lose one dollar. A bet on heads is a state claim, with payoff of one dollar if heads is the outcome, and payoff of negative one dollar if tails is the outcome. \"Heads\" and \"tails\" are the states of the world in this example. A state-contingent claim can be represented as a payoff vector with one element for each state of the world, e.g. (payoff if heads, payoff if tails). So a bet on heads can be represented as ($1, −$1) and a bet on tails can be represented as (−$1, $1). Notice that by placing one bet on heads and one bet on tails, you have a state-contingent claim of ($0, $0); that is, the payoff is the same regardless of which state of the world occurs.\n\nThe bet on a coin toss is a simplistic example but illustrates widely applicable concepts, especially in finance. If markets are complete, it is possible to arrange a portfolio with any conceivable payoff vector. That is, the state claims available for purchase, represented as payoff vectors, span the payoff space. A \"pure security\" or simple contingent claim is a state claim that pays off in only one state. Any state-contingent claim can be regarded as a collection of pure securities. A system of markets is complete if and only if the number of attainable pure securities equals the number of possible states. Formally, a market is complete with respect to a trading strategy, formula_1, if there exists a self-financing trading strategy, formula_2 such that at any time formula_3, the returns of the two strategies, formula_1 and formula_2 are equal. This is equivalent to stating that for a complete market, all cash flows for a trading strategy can be replicated using a similar synthetic trading strategy. Because a trading strategy can be simplified into a set of simple contingent claims (strategies paying 1 in one state and 0 in every other state), a complete market can be generalized as the ability to replicate cash flows of all simple contingent claims.\n\nOften used to describe insurance markets the model of a complete market occurs if agents can buy insurance contracts to protect themselves against \"any\" future time and state-of-the-world.\n\nFor example, if a market is a finite state market with dimension \"N\", then a complete market would be one where there exist traded assets with payoffs that form a basis for R.\n\nIn order for a market to be complete, it must be possible to \"instantaneously\" enter into any position regarding any future state of the market. In contrast, a market is called dynamically complete if it is possible to construct a self-financing trading strategy that will have the same cash-flow. In other words, a complete market allows you to place all of your bet at once, while a dynamically complete market may require that you execute subsequent trades after making your initial investment. The requirement that the strategy be self-financing means that subsequent trades must be cash-flow neutral (you cannot contribute or withdraw any additional funds). Any complete market is also dynamically complete.\n\n\n"}
{"id": "52297304", "url": "https://en.wikipedia.org/wiki?curid=52297304", "title": "Convergence group", "text": "Convergence group\n\nIn mathematics, a convergence group or a discrete convergence group is a group formula_1 acting by homeomorphisms on a compact metrizable space formula_2 in a way that generalizes the properties of the action of Kleinian group by Möbius transformations on the ideal boundary formula_3 of the hyperbolic 3-space formula_4.\nThe notion of a convergence group was introduced by Gehring and Martin (1987) and has since found wide applications in geometric topology, quasiconformal analysis, and geometric group theory.\n\nLet formula_1 be a group acting by homeomorphisms on a compact metrizable space formula_2. This action is called a \"convergence action\" or a \"discrete convergence action\" (and then formula_1 is called a \"convergence group\" or a \"discrete convergence group\" for this action) if for every infinite distinct sequence of elements formula_8 there exist a subsequence formula_9 and points formula_10 such that the maps formula_11 converge uniformly on compact subsets to the constant map sending formula_12 to formula_13. Here converging uniformly on compact subsets means that for every open neighborhood formula_14 of formula_13 in formula_2 and every compact formula_17 there exists an index formula_18 such that for every formula_19 formula_20. Note that the \"poles\" formula_21 associated with the subsequence formula_22 are not required to be distinct.\n\nThe above definition of convergence group admits a useful equivalent reformulation in terms of the action of formula_1 on the \"space of distinct triples\" of formula_2.\nFor a set formula_2 denote formula_26, where formula_27. The set formula_28 is called the \"space of distinct triples\" for formula_2.\n\nThen the following equivalence is known to hold:\n\nLet formula_1 be a group acting by homeomorphisms on a compact metrizable space formula_2 with at least two points. Then this action is a discrete convergence action if and only if the inducted action of formula_1 on formula_28 is properly discontinuous.\n\n\nLet formula_1 be a group acting by homeomorphisms on a compact metrizable space formula_2with at least three points, and let formula_47. Then it is known (Lemma 3.1 in or Lemma 6.2 in ) that exactly one of the following occurs:\n\n(1) The element formula_48 has finite order in formula_49; in this case formula_48 is called \"elliptic\".\n\n(2) The element formula_48 has infinite order in formula_49 and the fixed set formula_53 is a single point; in this case formula_48 is called \"parabolic\".\n\n(3) The element formula_48 has infinite order in formula_1 and the fixed set formula_53 consists of two distinct points; in this case formula_48 is called \"loxodromic\".\n\nMoreover, for every formula_59 the elements formula_48 and formula_61have the same type. Also in cases (2) and (3) formula_62 (where formula_59) and the group formula_64 acts properly discontinuously on formula_65. Additionally, if formula_48 is loxodromic, then formula_64 acts properly discontinuously and cocompactly on formula_68.\n\nIf formula_69 is parabolic with a fixed point formula_70 then for every formula_71 one has formula_72\nIf formula_69 is loxodromic, then formula_53 can be written as formula_75 so that for every formula_76 one has formula_77 and for every formula_78 one has formula_79, and these convergences are uniform on compact subsets of formula_80.\n\nA discrete convergence action of a group formula_1 on a compact metrizable space formula_2 is called \"uniform\" (in which case formula_1 is called a \"uniform convergence group\") if the action of formula_1 on formula_28 is co-compact. Thus formula_1 is a uniform convergence group if and only if its action on formula_28 is both properly discontinuous and co-compact.\n\nLet formula_1 act on a compact metrizable space formula_2 as a discrete convergence group. A point formula_71 is called a \"conical limit point\" (sometimes also called a \"radial limit point\" or a \"point of approximation\") if there exist an infinite sequence of distinct elements formula_91 and distinct points formula_10 such that formula_93 and for every formula_94 one has formula_95.\n\nAn important result of Tukia, also independently obtained by Bowditch, states:\n\nA discrete convergence group action of a group formula_1 on a compact metrizable space formula_2 is uniform if and only if every non-isolated point of formula_2 is a conical limit point.\n\nIt was already observed by Gromov that the natural action by translations of a word-hyperbolic group formula_36 on its boundary formula_37 is a uniform convergence action (see for a formal proof). Bowditch proved an important converse, thus obtaining a topological characterization of word-hyperbolic groups:\n\nTheorem. Let formula_101 act as a discrete uniform convergence group on a compact metrizable space formula_2 with no isolated points. Then the group formula_36 is word-hyperbolic and there exists a formula_36-equivariant homeomorphism formula_105.\n\nAn isometric action of a group formula_36 on the hyperbolic plane formula_107 is called \"geometric\" if this action is properly discontinuous and cocompact. Every geometric action of formula_36 on formula_107 induces a uniform convergence action of formula_36 on formula_111.\nAn important result of Tukia (1986), Gabai (1992), Casson–Jungreis (1994), and Freden (1995) shows that the converse also holds:\n\nTheorem. If formula_36 is a group acting as a discrete uniform convergence group on formula_113 then this action is topologically conjugate to an action induced by a geometric action of formula_36 on formula_115 by isometries.\n\nNote that whenever formula_36 acts geometrically on formula_107, the group formula_36 is virtually a hyperbolic surface group, that is, formula_36 contains a finite index subgroup isomorphic to the fundamental group of a closed hyperbolic surface.\n\nOne of the equivalent reformulations of Cannon's conjecture, originally posed by James W. Cannon in terms of word-hyperbolic groups with boundaries homeomorphic to formula_3, says that if formula_36 is a group acting as a discrete uniform convergence group on formula_3 then this action is topologically conjugate to an action induced by a geometric action of formula_36 on formula_124 by isometries. This conjecture still remains open.\n\n"}
{"id": "7961605", "url": "https://en.wikipedia.org/wiki?curid=7961605", "title": "CoreASM", "text": "CoreASM\n\nCoreASM is an open source project (licensed under Academic Free License version 3.0) that focuses on the design of a lean executable ASM (Abstract State Machines) language, in combination with a supporting tool environment for high-level design, experimental validation, and formal verification (where appropriate) of abstract system models.\n\nAbstract state machines are known for their versatility in modeling of algorithms, architectures, languages, protocols, and virtually all kinds of sequential, parallel, and distributed systems. The ASM formalism has been studied extensively by researchers in academia and industry for more than 15 years with the intention to bridge the gap between formal and pragmatic approaches. \n\nModel-based systems engineering can benefit from abstract executable specifications as a tool for design exploration and experimental validation through simulation and testing. Building on experiences with two generations of ASM tools, a novel executable ASM language, called CoreASM, is being developed (see CoreASM homepage). \n\nThe CoreASM language emphasizes freedom of experimentation, and supports the evolutionary nature of design as a product of creativity. It is particularly suited to Exploring the problem space for the purpose of writing an initial specification. The CoreASM language allows writing of highly abstract and concise specifications by minimizing the need for encoding in mapping the problem space to a formal model, and by allowing explicit declaration of the parts of the specification that are purposely left abstract. The principle of minimality, in combination with robustness of the underlying mathematical framework, improves modifiability of specifications, while effectively supporting the highly iterative nature of specification and design.\n\n\n"}
{"id": "6778984", "url": "https://en.wikipedia.org/wiki?curid=6778984", "title": "Discrete tomography", "text": "Discrete tomography\n\nDiscrete tomography focuses on the problem of reconstruction of binary images (or finite subsets of the integer lattice) from a small number of their projections.\n\nIn general, tomography deals with the problem of determining shape and dimensional information of an object from a set of projections. From the mathematical point of view, the object corresponds to a function and the problem posed is to reconstruct this function from its integrals or sums over subsets of its domain. In general, the tomographic inversion problem may be continuous or discrete. In continuous tomography both the\ndomain and the range of the function are continuous and line integrals are used. In discrete tomography the domain of the function may be either discrete or continuous, and the range of the function is a finite set of real, usually nonnegative numbers. In continuous tomography when a large number of projections is available, accurate reconstructions can be made by many different algorithms.\nIt is typical for discrete tomography that only a few projections (line sums) are used. In this case, conventional techniques all fail. A special case of discrete tomography deals with the problem of the reconstruction of\na binary image from a small number of projections. The name \"discrete tomography\" is due to Larry Shepp, who organized the first meeting devoted to this topic (DIMACS Mini-Symposium on Discrete Tomography, September 19, 1994, Rutgers University).\n\nDiscrete tomography has strong connections with other mathematical fields, such as number theory, discrete mathematics, complexity theory and combinatorics. In fact, a number of discrete tomography problems were first discussed as combinatorial problems. In 1957, H. J. Ryser found a necessary and sufficient condition for a pair of vectors being the two orthogonal projections of a discrete set. In the proof of his theorem, Ryser also described a reconstruction algorithm, the very first reconstruction algorithm for a general discrete set from two orthogonal projections. In the same year, David Gale found the same consistency conditions, but in connection with the network flow problem. Another result of Ryser's is the definition of the switching operation by which discrete sets having the same projections can be transformed into each other.\n\nThe problem of reconstructing a binary image from a small number of projections generally leads to a large number of solutions. It is desirable to limit the class of possible solutions to only those that are typical of the class of the images which contains the image being reconstructed by using a priori information, such as convexity or connectedness.\n\n\nFor further results see.\n\nAmong the reconstruction methods one can find algebraic reconstruction techniques (e.g., DART \n\nVarious algorithms have been applied in image processing\n, medicine, \nthree-dimensional statistical data security problems, computer\ntomograph assisted engineering and design, electron microscopy\n\nA form of discrete tomography also forms the basis of nonograms, a type of logic puzzle in which information about the rows and columns of a digital image is used to reconstruct the image.\n\n\n"}
{"id": "58795079", "url": "https://en.wikipedia.org/wiki?curid=58795079", "title": "Edith Cohen", "text": "Edith Cohen\n\nEdith Cohen (born May 21, 1966) is an Israeli and American computer scientist specializing in data mining and algorithms for big data. She is also known for her research on peer-to-peer networks. She works for Google in Mountain View, California, and as a visiting professor at Tel Aviv University in Israel.\n\nCohen is originally from Tel Aviv, where her father was a banker.\nShe earned a bachelor's degree in 1985 and a master's degree in 1986 from Tel Aviv University; her master's thesis was supervised by Michael Tarsi. She moved to Stanford University for her doctoral studies, and completed her Ph.D. in 1991 with Andrew V. Goldberg as her doctoral advisor and Nimrod Megiddo as an unofficial mentor. Her dissertation was \"Combinatorial Algorithms for Optimization Problems\".\n\nCohen was a student researcher at IBM Research - Almaden from 1987 to 1991, and a researcher at Bell Labs and its successor AT&T Labs from 1991 to 2012. In 2012, she became a visiting professor at Tel Aviv University, and began working for Microsoft Research, as a visitor for one year and then as a principal researcher. She has been associated with Google since 2015.\n\nCohen won the William R. Bennett prize of the IEEE Communications Society in 2007 with David Applegate, for their work on robust network routing.\nShe was nominated an ACM Fellow in 2017 \"for contributions to the design of efficient algorithms for networking and big data\".\n"}
{"id": "32446743", "url": "https://en.wikipedia.org/wiki?curid=32446743", "title": "Equioscillation theorem", "text": "Equioscillation theorem\n\nThe equioscillation theorem concerns the approximation of continuous functions using polynomials when the merit function is the maximum difference (uniform norm). Its discovery is attributed to Chebyshev.\n\nLet formula_1 be a continuous function from formula_2 to formula_3. Among all the polynomials of degree formula_4, the polynomial formula_5 minimizes the uniform norm of the difference formula_6 if and only if there are formula_7 points formula_8 such that formula_9 where formula_10.\n\nSeveral minimax approximation algorithms are available, the most common being the Remez algorithm.\n\n"}
{"id": "243382", "url": "https://en.wikipedia.org/wiki?curid=243382", "title": "Erlangen program", "text": "Erlangen program\n\nThe Erlangen program is a method of characterizing geometries based on group theory and projective geometry. It was published by Felix Klein in 1872 as \"Vergleichende Betrachtungen über neuere geometrische Forschungen.\" It is named after the University Erlangen-Nürnberg, where Klein worked.\n\nBy 1872, non-Euclidean geometries had emerged, but without a way to determine their hierarchy and relationships. Klein's method was fundamentally innovative in three ways:\n\nLater, Élie Cartan generalized Klein's homogeneous model spaces to Cartan connections on certain principal bundles, which generalized Riemannian geometry.\n\nSince Euclid, geometry had meant the geometry of Euclidean space of two dimensions (plane geometry) or of three dimensions (solid geometry). In the first half of the nineteenth century there had been several developments complicating the picture. Mathematical applications required geometry of four or more dimensions; the close scrutiny of the foundations of the traditional Euclidean geometry had revealed the independence of the parallel postulate from the others, and non-Euclidean geometry had been born. Klein proposed an idea that all these new geometries are just special cases of the projective geometry, as already developed by Poncelet, Möbius, Cayley and others. Klein also strongly suggested to mathematical \"physicists\" that even a moderate cultivation of the projective purview might bring substantial benefits to them.\n\nWith every geometry, Klein associated an underlying group of symmetries. The hierarchy of geometries is thus mathematically represented as a hierarchy of these groups, and hierarchy of their invariants. For example, lengths, angles and areas are preserved with respect to the Euclidean group of symmetries, while only the incidence structure and the cross-ratio are preserved under the most general projective transformations. A concept of parallelism, which is preserved in affine geometry, is not meaningful in projective geometry. Then, by abstracting the underlying groups of symmetries from the geometries, the relationships between them can be re-established at the group level. Since the group of affine geometry is a subgroup of the group of projective geometry, any notion invariant in projective geometry is \"a priori\" meaningful in affine geometry; but not the other way round. If you add required symmetries, you have a more powerful theory but fewer concepts and theorems (which will be deeper and more general).\n\nIn other words, the \"traditional spaces\" are homogeneous spaces; but not for a uniquely determined group. Changing the group changes the appropriate geometric language.\n\nIn today's language, the groups concerned in classical geometry are all very well known as Lie groups: the classical groups. The specific relationships are quite simply described, using technical language.\n\nFor example, the group of projective geometry in \"n\" real-valued dimensions is the symmetry group of \"n\"-dimensional real projective space (the general linear group of degree , quotiented by scalar matrices). The affine group will be the subgroup respecting (mapping to itself, not fixing pointwise) the chosen hyperplane at infinity. This subgroup has a known structure (semidirect product of the general linear group of degree \"n\" with the subgroup of translations). This description then tells us which properties are 'affine'. In Euclidean plane geometry terms, being a parallelogram is affine since affine transformations always take one parallelogram to another one. Being a circle is not affine since an affine shear will take a circle into an ellipse.\n\nTo explain accurately the relationship between affine and Euclidean geometry, we now need to pin down the group of Euclidean geometry within the affine group. The Euclidean group is in fact (using the previous description of the affine group) the semi-direct product of the orthogonal (rotation and reflection) group with the translations. (See Klein geometry for more details.)\n\nThe long-term effects of the Erlangen program can be seen all over pure mathematics (see tacit use at congruence (geometry), for example); and the idea of transformations and of synthesis using groups of symmetry has become standard in physics.\n\nWhen topology is routinely described in terms of properties invariant under homeomorphism, one can see the underlying idea in operation. The groups involved will be infinite-dimensional in almost all cases – and not Lie groups – but the philosophy is the same. Of course this mostly speaks to the pedagogical influence of Klein. Books such as those by H.S.M. Coxeter routinely used the Erlangen program approach to help 'place' geometries. In pedagogic terms, the program became transformation geometry, a mixed blessing in the sense that it builds on stronger intuitions than the style of Euclid, but is less easily converted into a logical system.\n\nIn his book \"Structuralism\" (1970) Jean Piaget says, \"In the eyes of contemporary structuralist mathematicians, like Bourbaki, the Erlangen Program amounts to only a partial victory for structuralism, since they want to subordinate all mathematics, not just geometry, to the idea of structure.\"\n\nFor a geometry and its group, an element of the group is sometimes called a motion of the geometry. For example, one can learn about the Poincaré half-plane model of hyperbolic geometry through a development based on hyperbolic motions. Such a development enables one to methodically prove the ultraparallel theorem by successive motions.\n\nQuite often, it appears there are two or more distinct geometries with isomorphic automorphism groups. There arises the question of reading the Erlangen program from the \"abstract\" group, to the geometry.\n\nOne example: oriented (i.e., reflections not included) elliptic geometry (i.e., the surface of an \"n\"-sphere with opposite points identified) and oriented spherical geometry (the same non-Euclidean geometry, but with opposite points not identified) have isomorphic automorphism group, SO(\"n\"+1) for even \"n\". These may appear to be distinct. It turns out, however, that the geometries are very closely related, in a way that can be made precise.\n\nTo take another example, elliptic geometries with different radii of curvature have isomorphic automorphism groups. That does not really count as a critique as all such geometries are isomorphic. General Riemannian geometry falls outside the boundaries of the program.\n\nComplex, dual and double (aka split-complex) numbers appear as homogeneous spaces SL(2,R)/H for the group SL(2,R) and its subgroups H=A, N, K . The group SL(2,R) acts on these homogeneous spaces by linear fractional transformations and a large portion of the respective geometries can be obtained in a uniform way from the Erlangen programme.\n\nSome further notable examples have come up in physics.\n\nFirstly, \"n\"-dimensional hyperbolic geometry, \"n\"-dimensional de Sitter space and (\"n\"−1)-dimensional inversive geometry all have isomorphic automorphism groups,\n\nthe orthochronous Lorentz group, for . But these are apparently distinct geometries. Here some interesting results enter, from the physics. It has been shown that physics models in each of the three geometries are \"dual\" for some models.\n\nAgain, \"n\"-dimensional anti-de Sitter space and (\"n\"−1)-dimensional conformal space with \"Lorentzian\" signature (in contrast with conformal space with \"Euclidean\" signature, which is identical to inversive geometry, for three dimensions or greater) have isomorphic automorphism groups, but are distinct geometries. Once again, there are models in physics with \"dualities\" between both spaces. See AdS/CFT for more details.\n\nThe covering group of SU(2,2) is isomorphic to the covering group of SO(4,2), which is the symmetry group of a 4D conformal Minkowski space and a 5D anti-de Sitter space and a complex four-dimensional twistor space.\n\nThe Erlangen program can therefore still be considered fertile, in relation with dualities in physics.\n\nIn the seminal paper which introduced categories, Saunders Mac Lane and S. Eilenberg stated: \"This may be regarded as a continuation of the Klein Erlanger Program, in the sense that a geometrical space with its group of transformations is generalized to a category with its algebra of mappings\"\n\nRelations of the Erlangen program with work of C. Ehresmann on groupoids in geometry is considered in the article below by Pradines.\n\nIn mathematical logic, the Erlangen Program also served as an inspiration for Alfred Tarski in his analysis of logical notions.\n\n\n\n"}
{"id": "10949", "url": "https://en.wikipedia.org/wiki?curid=10949", "title": "Four color theorem", "text": "Four color theorem\n\nIn mathematics, the four color theorem, or the four color map theorem, states that, given any separation of a plane into contiguous regions, producing a figure called a \"map\", no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. \"Adjacent\" means that two regions share a common boundary curve segment, not merely a corner where three or more regions meet.\n\nUnlike the five color theorem, a theorem that states that five colors are enough to color a map, which was proved in the 1800s, the four color theorem was proved in 1976 by Kenneth Appel and Wolfgang Haken, but only after many false proofs and counterexamples. It was the first major theorem to be proved using a computer. Initially, their proof was not accepted by all mathematicians because the computer-assisted proof was infeasible for a human to check by hand. Since then the proof has gained wide acceptance, although some doubters remain.\n\nTo dispel any remaining doubts about the Appel–Haken proof, a simpler proof using the same ideas and still relying on computers was published in 1997 by Robertson, Sanders, Seymour, and Thomas. Additionally, in 2005, the theorem was proved by Georges Gonthier with general-purpose theorem-proving software.\n\nThe intuitive statement of the four color theorem, i.e. \"that given any separation of a plane into contiguous regions, called a map, the regions can be colored using at most four colors so that no two adjacent regions have the same color\", needs to be interpreted appropriately to be correct.\n\nFirst, all corners, points that belong to (technically, are in the closure of) three or more countries, must be ignored. In addition, bizarre maps (using regions of finite area but infinite perimeter) can require more than four colors. Second, for the purpose of the theorem, every \"country\" has to be a connected region, or contiguous. In the real world, this is not true (e.g., the Upper and Lower Peninsula of Michigan, Nakhchivan as part of Azerbaijan, and Kaliningrad as part of Russia are not contiguous). Because all the territory of a particular country must be the same color, four colors may not be sufficient. For instance, consider a simplified map:\n\nIn this map, the two regions labeled \"A\" belong to the same country, and must be the same color. This map then requires five colors, since the two \"A\" regions together are contiguous with four other regions, each of which is contiguous with all the others. A similar construction also applies if a single color is used for all bodies of water, as is usual on real maps. For maps in which more than one country may have multiple disconnected regions, six or more colors might be required.\n\nA simpler statement of the theorem uses graph theory. The set of regions of a map can be represented more abstractly as an undirected graph that has a vertex for each region and an edge for every pair of regions that share a boundary segment. This graph is planar (it is important to note that we are talking about the graphs that have some limitations according to the map they are transformed from only): it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves that lead without crossing within each region from the vertex location to each shared boundary point of the region. Conversely any planar graph can be formed from a map in this way. In graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be colored with at most four colors so that no two adjacent vertices receive the same color, or for short, \"every planar graph is four-colorable\".\n\nAs far as is known, the conjecture was first proposed on October 23, 1852 when Francis Guthrie, while trying to color the map of counties of England, noticed that only four different colors were needed. At the time, Guthrie's brother, Frederick, was a student of Augustus De Morgan (the former advisor of Francis) at University College London. Francis inquired with Frederick regarding it, who then took it to De Morgan (Francis Guthrie graduated later in 1852, and later became a professor of mathematics in South Africa). According to De Morgan:\n\n\"A student of mine [Guthrie] asked me to day to give him a reason for a fact which I did not know was a fact—and do not yet. He says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary \"line\" are differently colored—four colors may be wanted but not more—the following is his case in which four colors \"are\" wanted. Query cannot a necessity for five or more be invented…\" \n\n\"F.G.\", perhaps one of the two Guthries, published the question in \"The Athenaeum\" in 1854, and De Morgan posed the question again in the same magazine in 1860. Another early published reference by in turn credits the conjecture to De Morgan.\n\nThere were several early failed attempts at proving the theorem. De Morgan believed that it followed from a simple fact about four regions, though he didn't believe that fact could be derived from more elementary facts.\nThis arises in the following way. We never need four colors in a neighborhood unless there be four counties, each of which has boundary lines in common with each of the other three. Such a thing cannot happen with four areas unless one or more of them be inclosed by the rest; and the color used for the inclosed county is thus set free to go on with. Now this principle, that four areas cannot each have common boundary with all the other three without inclosure, is not, we fully believe, capable of demonstration upon anything more evident and more elementary; it must stand as a postulate.\n\nOne alleged proof was given by Alfred Kempe in 1879, which was widely acclaimed; another was given by Peter Guthrie Tait in 1880. It was not until 1890 that Kempe's proof was shown incorrect by Percy Heawood, and in 1891, Tait's proof was shown incorrect by Julius Petersen—each false proof stood unchallenged for 11 years.\n\nIn 1890, in addition to exposing the flaw in Kempe's proof, Heawood proved the five color theorem and generalized the four color conjecture to surfaces of arbitrary genus.\n\nTait, in 1880, showed that the four color theorem is equivalent to the statement that a certain type of graph (called a snark in modern terminology) must be non-planar.\n\nIn 1943, Hugo Hadwiger formulated the Hadwiger conjecture, a far-reaching generalization of the four-color problem that still remains unsolved.\n\nDuring the 1960s and 1970s German mathematician Heinrich Heesch developed methods of using computers to search for a proof. Notably he was the first to use discharging for proving the theorem, which turned out to be important in the unavoidability portion of the subsequent Appel–Haken proof. He also expanded on the concept of reducibility and, along with Ken Durre, developed a computer test for it. Unfortunately, at this critical juncture, he was unable to procure the necessary supercomputer time to continue his work.\n\nOthers took up his methods and his computer-assisted approach. While other teams of mathematicians were racing to complete proofs, Kenneth Appel and Wolfgang Haken at the University of Illinois announced, on June 21, 1976, that they had proved the theorem. They were assisted in some algorithmic work by John A. Koch.\n\nIf the four-color conjecture were false, there would be at least one map with the smallest possible number of regions that requires five colors. The proof showed that such a minimal counterexample cannot exist, through the use of two technical concepts:\n\n\nUsing mathematical rules and procedures based on properties of reducible configurations, Appel and Haken found an unavoidable set of reducible configurations, thus proving that a minimal counterexample to the four-color conjecture could not exist. Their proof reduced the infinitude of possible maps to 1,936 reducible configurations (later reduced to 1,476) which had to be checked one by one by computer and took over a thousand hours. This reducibility part of the work was independently double checked with different programs and computers. However, the unavoidability part of the proof was verified in over 400 pages of microfiche, which had to be checked by hand with the assistance of Haken's daughter Dorothea Blostein .\n\nAppel and Haken's announcement was widely reported by the news media around the world, and the math department at the University of Illinois used a postmark stating \"Four colors suffice.\" At the same time the unusual nature of the proof—it was the first major theorem to be proved with extensive computer assistance—and the complexity of the human-verifiable portion aroused considerable controversy .\n\nIn the early 1980s, rumors spread of a flaw in the Appel–Haken proof. Ulrich Schmidt at RWTH Aachen examined Appel and Haken's proof for his master's thesis . He had checked about 40% of the unavoidability portion and found a significant error in the discharging procedure . In 1986, Appel and Haken were asked by the editor of \"Mathematical Intelligencer\" to write an article addressing the rumors of flaws in their proof. They responded that the rumors were due to a \"misinterpretation of [Schmidt's] results\" and obliged with a detailed article . Their magnum opus, \"Every Planar Map is Four-Colorable\", a book claiming a complete and detailed proof (with a microfiche supplement of over 400 pages), appeared in 1989 and explained Schmidt's discovery and several further errors found by others .\n\nSince the proving of the theorem, efficient algorithms have been found for 4-coloring maps requiring only O(\"n\") time, where \"n\" is the number of vertices. In 1996, Neil Robertson, Daniel P. Sanders, Paul Seymour, and Robin Thomas created a quadratic-time algorithm, improving on a quartic-time algorithm based on Appel and Haken’s proof This new proof is similar to Appel and Haken's but more efficient because it reduces the complexity of the problem and requires checking only 633 reducible configurations. Both the unavoidability and reducibility parts of this new proof must be executed by computer and are impractical to check by hand. In 2001, the same authors announced an alternative proof, by proving the snark theorem.\n\nIn 2005, Benjamin Werner and Georges Gonthier formalized a proof of the theorem inside the Coq proof assistant. This removed the need to trust the various computer programs used to verify particular cases; it is only necessary to trust the Coq kernel.\n\nThe following discussion is a summary based on the introduction to \"Every Planar Map is Four Colorable\" . Although flawed, Kempe's original purported proof of the four color theorem provided some of the basic tools later used to prove it. The explanation here is reworded in terms of the modern graph theory formulation above.\n\nKempe's argument goes as follows. First, if planar regions separated by the graph are not \"triangulated\", i.e. do not have exactly three edges in their boundaries, we can add edges without introducing new vertices in order to make every region triangular, including the unbounded outer region. If this triangulated graph is colorable using four colors or fewer, so is the original graph since the same coloring is valid if edges are removed. So it suffices to prove the four color theorem for triangulated graphs to prove it for all planar graphs, and without loss of generality we assume the graph is triangulated.\n\nSuppose \"v\", \"e\", and \"f\" are the number of vertices, edges, and regions (faces). Since each region is triangular and each edge is shared by two regions, we have that 2\"e\" = 3\"f\". This together with Euler's formula, \"v\" − \"e\" + \"f\" = 2, can be used to show that 6\"v\" − 2\"e\" = 12. Now, the \"degree\" of a vertex is the number of edges abutting it. If \"v\" is the number of vertices of degree \"n\" and \"D\" is the maximum degree of any vertex,\nBut since 12 > 0 and 6 − \"i\" ≤ 0 for all \"i\" ≥ 6, this demonstrates that there is at least one vertex of degree 5 or less.\n\nIf there is a graph requiring 5 colors, then there is a \"minimal\" such graph, where removing any vertex makes it four-colorable. Call this graph \"G\". Then \"G\" cannot have a vertex of degree 3 or less, because if \"d\"(\"v\") ≤ 3, we can remove \"v\" from \"G\", four-color the smaller graph, then add back \"v\" and extend the four-coloring to it by choosing a color different from its neighbors.\n\nKempe also showed correctly that \"G\" can have no vertex of degree 4. As before we remove the vertex \"v\" and four-color the remaining vertices. If all four neighbors of \"v\" are different colors, say red, green, blue, and yellow in clockwise order, we look for an alternating path of vertices colored red and blue joining the red and blue neighbors. Such a path is called a Kempe chain. There may be a Kempe chain joining the red and blue neighbors, and there may be a Kempe chain joining the green and yellow neighbors, but not both, since these two paths would necessarily intersect, and the vertex where they intersect cannot be colored. Suppose it is the red and blue neighbors that are not chained together. Explore all vertices attached to the red neighbor by red-blue alternating paths, and then reverse the colors red and blue on all these vertices. The result is still a valid four-coloring, and \"v\" can now be added back and colored red.\n\nThis leaves only the case where \"G\" has a vertex of degree 5; but Kempe's argument was flawed for this case. Heawood noticed Kempe's mistake and also observed that if one was satisfied with proving only five colors are needed, one could run through the above argument (changing only that the minimal counterexample requires 6 colors) and use Kempe chains in the degree 5 situation to prove the five color theorem.\n\nIn any case, to deal with this degree 5 vertex case requires a more complicated notion than removing a vertex. Rather the form of the argument is generalized to considering \"configurations\", which are connected subgraphs of \"G\" with the degree of each vertex (in G) specified. For example, the case described in degree 4 vertex situation is the configuration consisting of a single vertex labelled as having degree 4 in \"G\". As above, it suffices to demonstrate that if the configuration is removed and the remaining graph four-colored, then the coloring can be modified in such a way that when the configuration is re-added, the four-coloring can be extended to it as well. A configuration for which this is possible is called a \"reducible configuration\". If at least one of a set of configurations must occur somewhere in G, that set is called \"unavoidable\". The argument above began by giving an unavoidable set of five configurations (a single vertex with degree 1, a single vertex with degree 2, ..., a single vertex with degree 5) and then proceeded to show that the first 4 are reducible; to exhibit an unavoidable set of configurations where every configuration in the set is reducible would prove the theorem.\n\nBecause \"G\" is triangular, the degree of each vertex in a configuration is known, and all edges internal to the configuration are known, the number of vertices in \"G\" adjacent to a given configuration is fixed, and they are joined in a cycle. These vertices form the \"ring\" of the configuration; a configuration with \"k\" vertices in its ring is a \"k\"-ring configuration, and the configuration together with its ring is called the \"ringed configuration\". As in the simple cases above, one may enumerate all distinct four-colorings of the ring; any coloring that can be extended without modification to a coloring of the configuration is called \"initially good\". For example, the single-vertex configuration above with 3 or less neighbors were initially good. In general, the surrounding graph must be systematically recolored to turn the ring's coloring into a good one, as was done in the case above where there were 4 neighbors; for a general configuration with a larger ring, this requires more complex techniques. Because of the large number of distinct four-colorings of the ring, this is the primary step requiring computer assistance.\n\nFinally, it remains to identify an unavoidable set of configurations amenable to reduction by this procedure. The primary method used to discover such a set is the method of discharging. The intuitive idea underlying discharging is to consider the planar graph as an electrical network. Initially positive and negative \"electrical charge\" is distributed amongst the vertices so that the total is positive.\n\nRecall the formula above:\n\nEach vertex is assigned an initial charge of 6-deg(\"v\"). Then one \"flows\" the charge by systematically redistributing the charge from a vertex to its neighboring vertices according to a set of rules, the \"discharging procedure\". Since charge is preserved, some vertices still have positive charge. The rules restrict the possibilities for configurations of positively charged vertices, so enumerating all such possible configurations gives an unavoidable set.\n\nAs long as some member of the unavoidable set is not reducible, the discharging procedure is modified to eliminate it (while introducing other configurations). Appel and Haken's final discharging procedure was extremely complex and, together with a description of the resulting unavoidable configuration set, filled a 400-page volume, but the configurations it generated could be checked mechanically to be reducible. Verifying the volume describing the unavoidable configuration set itself was done by peer review over a period of several years.\n\nA technical detail not discussed here but required to complete the proof is \"immersion reducibility\".\n\nThe four color theorem has been notorious for attracting a large number of false proofs and disproofs in its long history. At first, \"The New York Times\" refused as a matter of policy to report on the Appel–Haken proof, fearing that the proof would be shown false like the ones before it . Some alleged proofs, like Kempe's and Tait's mentioned above, stood under public scrutiny for over a decade before they were refuted. But many more, authored by amateurs, were never published at all.\n\nGenerally, the simplest, though invalid, counterexamples attempt to create one region which touches all other regions. This forces the remaining regions to be colored with only three colors. Because the four color theorem is true, this is always possible; however, because the person drawing the map is focused on the one large region, they fail to notice that the remaining regions can in fact be colored with three colors.\n\nThis trick can be generalized: there are many maps where if the colors of some regions are selected beforehand, it becomes impossible to color the remaining regions without exceeding four colors. A casual verifier of the counterexample may not think to change the colors of these regions, so that the counterexample will appear as though it is valid.\n\nPerhaps one effect underlying this common misconception is the fact that the color restriction is not transitive: a region only has to be colored differently from regions it touches directly, not regions touching regions that it touches. If this were the restriction, planar graphs would require arbitrarily large numbers of colors.\n\nOther false disproofs violate the assumptions of the theorem in unexpected ways, such as using a region that consists of multiple disconnected parts, or disallowing regions of the same color from touching at a point.\n\nWhile every planar map can be colored with four colors, it is NP-complete in complexity to decide whether an arbitrary planar map can be colored with just three colors.\n\nThe four-color theorem applies not only to finite planar graphs, but also to infinite graphs that can be drawn without crossings in the plane, and even more generally to infinite graphs (possibly with an uncountable number of vertices) for which every finite subgraph is planar. To prove this, one can combine a proof of the theorem for finite planar graphs with the De Bruijn–Erdős theorem stating that, if every finite subgraph of an infinite graph is \"k\"-colorable, then the whole graph is also \"k\"-colorable . This can also be seen as an immediate consequence of Kurt Gödel's compactness theorem for first-order logic, simply by expressing the colorability of an infinite graph with a set of logical formulae.\n\nOne can also consider the coloring problem on surfaces other than the plane (Weisstein). The problem on the sphere or cylinder is equivalent to that on the plane. For closed (orientable or non-orientable) surfaces with positive genus, the maximum number \"p\" of colors needed depends on the surface's Euler characteristic χ according to the formula\nwhere the outermost brackets denote the floor function.\n\nAlternatively, for an orientable surface the formula can be given in terms of the genus of a surface, \"g\":\n\nThis formula, the Heawood conjecture, was conjectured by P. J. Heawood in 1890 and proved by Gerhard Ringel and J. W. T. Youngs in 1968. The only exception to the formula is the Klein bottle, which has Euler characteristic 0 (hence the formula gives p = 7) and requires only 6 colors, as shown by P. Franklin in 1934 (Weisstein).\n\nFor example, the torus has Euler characteristic χ = 0 (and genus \"g\" = 1) and thus \"p\" = 7, so no more than 7 colors are required to color any map on a torus. This upper bound of 7 is sharp: certain toroidal polyhedra such as the Szilassi polyhedron require seven colors.\nA Möbius strip requires six colors as do 1-planar graphs (graphs drawn with at most one simple crossing per edge) . If both the vertices and the faces of a planar graph are colored, in such a way that no two adjacent vertices, faces, or vertex-face pair have the same color, then again at most six colors are needed .\n\nThere is no obvious extension of the coloring result to three-dimensional solid regions. By using a set of \"n\" flexible rods, one can arrange that every rod touches every other rod. The set would then require \"n\" colors, or \"n\"+1 if you consider the empty space that also touches every rod. The number \"n\" can be taken to be any integer, as large as desired. Such examples were known to Fredrick Guthrie in 1880 . Even for axis-parallel cuboids (considered to be adjacent when two cuboids share a two-dimensional boundary area) an unbounded number of colors may be necessary (; ).\n\nDror Bar-Natan gave a statement concerning Lie algebras and Vassiliev invariants which is equivalent to the four color theorem.\n\nDespite the motivation from coloring political maps of countries, the theorem is not of particular interest to cartographers. According to an article by the math historian Kenneth May, \"Maps utilizing only four colors are rare, and those that do usually require only three. Books on cartography and the history of mapmaking do not mention the four-color property\" . The theorem also does not guarantee the usual cartographic requirement that non-contiguous regions of the same country (such as Great Britain and Northern Ireland) be colored identically.\n\n\n\n"}
{"id": "174908", "url": "https://en.wikipedia.org/wiki?curid=174908", "title": "Functional predicate", "text": "Functional predicate\n\nIn formal logic and related branches of mathematics, a functional predicate, or function symbol, is a logical symbol that may be applied to an object term to produce another object term.\nFunctional predicates are also sometimes called \"mappings\", but that term has other meanings as well.\nIn a model, a function symbol will be modelled by a function.\n\nSpecifically, the symbol \"F\" in a formal language is a functional symbol if, given any symbol \"X\" representing an object in the language, \"F\"(\"X\") is again a symbol representing an object in that language.\nIn typed logic, \"F\" is a functional symbol with \"domain\" type T and \"codomain\" type U if, given any symbol \"X\" representing an object of type T, \"F\"(\"X\") is a symbol representing an object of type U.\nOne can similarly define function symbols of more than one variable, analogous to functions of more than one variable; a function symbol in zero variables is simply a constant symbol.\n\nNow consider a model of the formal language, with the types T and U modelled by sets [T] and [U] and each symbol \"X\" of type T modelled by an element [\"X\"] in [T].\nThen \"F\" can be modelled by the set\nwhich is simply a function with domain [T] and codomain [U].\nIt is a requirement of a consistent model that [\"F\"(\"X\")] = [\"F\"(\"Y\")] whenever [\"X\"] = [\"Y\"].\n\nIn a treatment of predicate logic that allows one to introduce new predicate symbols, one will also want to be able to introduce new function symbols. Given the function symbols \"F\" and \"G\", one can introduce a new function symbol \"F\" ∘ \"G\", the \"composition\" of \"F\" and \"G\", satisfying (\"F\" ∘ \"G\")(\"X\") = \"F\"(\"G\"(\"X\")), for all \"X\".\nOf course, the right side of this equation doesn't make sense in typed logic unless the domain type of \"F\" matches the codomain type of \"G\", so this is required for the composition to be defined.\n\nOne also gets certain function symbols automatically.\nIn untyped logic, there is an \"identity predicate\" id that satisfies id(\"X\") = \"X\" for all \"X\".\nIn typed logic, given any type T, there is an identity predicate id with domain and codomain type T; it satisfies id(\"X\") = \"X\" for all \"X\" of type T.\nSimilarly, if T is a subtype of U, then there is an inclusion predicate of domain type T and codomain type U that satisfies the same equation; there are additional function symbols associated with other ways of constructing new types out of old ones.\n\nAdditionally, one can define functional predicates after proving an appropriate theorem.\nSpecifically, if you can prove that for every \"X\" (or every \"X\" of a certain type), there exists a unique \"Y\" satisfying some condition \"P\", then you can introduce a function symbol \"F\" to indicate this.\nNote that \"P\" will itself be a relational predicate involving both \"X\" and \"Y\".\nSo if there is such a predicate \"P\" and a theorem:\nthen you can introduce a function symbol \"F\" of domain type T and codomain type U that satisfies:\n\nMany treatments of predicate logic don't allow functional predicates, only relational predicates.\nThis is useful, for example, in the context of proving metalogical theorems (such as Gödel's incompleteness theorems), where one doesn't want to allow the introduction of new functional symbols (nor any other new symbols, for that matter).\nBut there is a method of replacing functional symbols with relational symbols wherever the former may occur; furthermore, this is algorithmic and thus suitable for applying most metalogical theorems to the result.\n\nSpecifically, if \"F\" has domain type T and codomain type U, then it can be replaced with a predicate \"P\" of type (T,U).\nIntuitively, \"P\"(\"X\",\"Y\") means \"F\"(\"X\") = \"Y\".\nThen whenever \"F\"(\"X\") would appear in a statement, you can replace it with a new symbol \"Y\" of type U and include another statement \"P\"(\"X\",\"Y\").\nTo be able to make the same deductions, you need an additional proposition:\n\nBecause the elimination of functional predicates is both convenient for some purposes and possible, many treatments of formal logic do not deal explicitly with function symbols but instead use only relation symbols; another way to think of this is that a functional predicate is a \"special kind of\" predicate, specifically one that satisfies the proposition above.\nThis may seem to be a problem if you wish to specify a proposition schema that applies only to functional predicates \"F\"; how do you know ahead of time whether it satisfies that condition?\nTo get an equivalent formulation of the schema, first replace anything of the form \"F\"(\"X\") with a new variable \"Y\".\nThen universally quantify over each \"Y\" immediately after the corresponding \"X\" is introduced (that is, after \"X\" is quantified over, or at the beginning of the statement if \"X\" is free), and guard the quantification with \"P\"(\"X\",\"Y\").\nFinally, make the entire statement a material consequence of the uniqueness condition for a functional predicate above.\n\nLet us take as an example the axiom schema of replacement in Zermelo–Fraenkel set theory.\nThis schema states (in one form), for any functional predicate \"F\" in one variable:\nFirst, we must replace \"F\"(\"C\") with some other variable \"D\": \nOf course, this statement isn't correct; \"D\" must be quantified over just after \"C\": \nWe still must introduce \"P\" to guard this quantification: \nThis is almost correct, but it applies to too many predicates; what we actually want is: \nThis version of the axiom schema of replacement is now suitable for use in a formal language that doesn't allow the introduction of new function symbols. Alternatively, one may interpret the original statement as a statement in such a formal language; it was merely an abbreviation for the statement produced at the end.\n\n"}
{"id": "22697171", "url": "https://en.wikipedia.org/wiki?curid=22697171", "title": "Georg Cantor's first set theory article", "text": "Georg Cantor's first set theory article\n\nGeorg Cantor published his first set theory article in 1874, and it contains the first theorems of transfinite set theory, which studies infinite sets and their properties. One of these theorems is \"Cantor's revolutionary discovery\" that the set of all real numbers is uncountably, rather than countably, infinite. This theorem is proved using Cantor's first uncountability proof, which differs from the more familiar proof using his diagonal argument. The title of the article, \"On a Property of the Collection of All Real Algebraic Numbers\" (\"Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen\"), refers to its first theorem: the set of real algebraic numbers is countable. In 1879, Cantor modified his uncountability proof by using the topological notion of a set being dense in an interval.\n\nCantor's 1874 article also contains a proof of the existence of transcendental numbers. As early as 1930, mathematicians have disagreed on whether this proof is constructive or non-constructive. Books as recent as 2014 and 2015 indicate that this disagreement has not been resolved. Since Cantor's proof either constructs transcendental numbers or does not, an analysis of his article can determine whether his proof is constructive or non-constructive. Cantor's correspondence with Richard Dedekind shows the development of his ideas and reveals that he had a choice between two proofs, one that uses the uncountability of the real numbers and one that does not.\n\nHistorians of mathematics have examined Cantor's article and the circumstances in which it was written. For example, they have discovered that Cantor was advised to leave out his uncountability theorem in the article he submitted; he added it during proofreading. They have traced this and other facts about the article to the influence of Karl Weierstrass and Leopold Kronecker. Historians have also studied Dedekind's contributions to the article, including his contributions to the theorem on the countability of the real algebraic numbers. In addition, they have looked at the article's legacy, which includes the impact that the uncountability theorem and the concept of countability have had on mathematics.\n\nCantor's article is short, less than four and a half pages. It begins with a discussion of the real algebraic numbers and a statement of his first theorem: The set of real algebraic numbers can be put into one-to-one correspondence with the set of positive integers. Cantor restates this theorem in terms more familiar to mathematicians of his time: The set of real algebraic numbers can be written as an infinite sequence in which each number appears only once.\n\nCantor's second theorem works with a closed interval [\"a\", \"b\"], which is the set of real numbers ≥ \"a\" and ≤ \"b\". The theorem states: Given any sequence of real numbers \"x\", \"x\", \"x\", … and any interval [\"a\", \"b\"], there is a number in [\"a\", \"b\"] that is not contained in the given sequence. Hence, there are infinitely many such numbers.\n\nThe first part of this theorem implies the \"Hence\" part. For example, let [0, 1] be the interval, and consider its pairwise disjoint subintervals …. Applying the first part of the theorem to each subinterval produces infinitely many numbers in [0, 1] that are not contained in the given sequence.\n\nCantor observes that combining his two theorems yields a new proof of Liouville's theorem that every interval [\"a\", \"b\"] contains infinitely many transcendental numbers.\n\nCantor then remarks that his second theorem is:\nThis remark contains Cantor's uncountability theorem, which only states that an interval [\"a\", \"b\"] cannot be put into one-to-one correspondence with the set of positive integers. It does not state that this interval is an infinite set of larger cardinality than the set of positive integers. Cardinality is defined in Cantor's next article, which was published in 1878.\n\nCantor only states his uncountability theorem. He does not use it in any proofs.\n\nTo prove that the set of real algebraic numbers is countable, define the \"height\" of a polynomial of degree \"n\" with integer coefficients as: \"n\" − 1 + |\"a\"| + |\"a\"| + … + |\"a\"|, where \"a\", \"a\", …, \"a\" are the coefficients of the polynomial. Order the polynomials by their height, and order the real roots of polynomials of the same height by numeric order. Since there are only a finite number of roots of polynomials of a given height, these orderings put the real algebraic numbers into a sequence. Cantor went a step further and produced a sequence in which each real algebraic number appears just once. He did this by only using polynomials that are irreducible over the integers. The table below contains the beginning of Cantor's enumeration.\n\nOnly the first part of Cantor's second theorem needs to be proved. It states: Given any sequence of real numbers \"x\", \"x\", \"x\", … and any interval [\"a\", \"b\"], there is a number in [\"a\", \"b\"] that is not contained in the given sequence. We simplify Cantor's proof by using open intervals. The open interval (\"a\", \"b\") is the set of real numbers greater than \"a\" and less than \"b\".\n\nTo find a number in [\"a\", \"b\"] that is not contained in the given sequence, construct two sequences of real numbers as follows: Find the first two numbers of the given sequence that are in (\"a\", \"b\"). Denote the smaller of these two numbers by \"a\" and the larger by \"b\". Similarly, find the first two numbers of the given sequence that are in (\"a\", \"b\"). Denote the smaller by \"a\" and the larger by \"b\". Continuing this procedure generates a sequence of intervals (\"a\", \"b\"), (\"a\", \"b\"), (\"a\", \"b\"), … such that each interval in the sequence contains all succeeding intervals—that is, it generates a sequence of nested intervals. This implies that the sequence \"a\", \"a\", \"a\", … is increasing and the sequence \"b\", \"b\", \"b\", … is decreasing.\n\nEither the number of intervals generated is finite or infinite. If finite, let (\"a\", \"b\") be the last interval. If infinite, take the limits \"a\" = lim \"a\" and \"b\" = lim \"b\". Since \"a\" < \"b\" for all \"n\", either \"a\" = \"b\" or \"a\" < \"b\". Thus, there are three cases to consider:\n\nThe proof is complete since, in all cases, at least one real number in [\"a\", \"b\"] has been found that is not contained in the given sequence.\n\nCantor's proofs are constructive and have been used to write a computer program that generates the digits of a transcendental number. This program applies Cantor's construction to a sequence containing all the real algebraic numbers between 0 and 1. The article that discusses this program gives some of its output, which shows how the construction generates a transcendental.\n\nAn example illustrates how Cantor's construction works. Consider the sequence: , , , , , , , , , … This sequence is obtained by ordering the rational numbers in (0, 1) by increasing denominators, ordering those with the same denominator by increasing numerators, and omitting reducible fractions. The table below shows the first five steps of the construction. The table's first column contains the intervals (\"a\", \"b\"). The second column lists the terms visited during the search for the first two terms in (\"a\", \"b\"). These two terms are in red.\n\nSince the sequence contains all the rational numbers in (0, 1), the construction generates an irrational number, which turns out to be  − 1.\n\nIn 1879, Cantor published a new uncountability proof that modifies his 1874 proof. He first defines the topological notion of a point set \"P\" being \"everywhere dense in an interval\" (which is quite often shortened to \"dense in an interval\"):\n\nWe will use \"a\", \"b\", \"c\", \"d\" rather than α, β, γ, δ. Cantor assumes that an interval [\"c\", \"d\"] satisfies \"d\" > \"c\".\n\nSince our discussion of Cantor's 1874 proof was simplified using by open intervals rather than closed intervals, the same simplification is used here. This requires an equivalent definition of everywhere dense: A set \"P\" is everywhere dense in the interval [\"a\", \"b\"] if and only if every subinterval (\"c\", \"d\") of [\"a\", \"b\"] contains at least one point of \"P\".\n\nCantor did not specify how many points of \"P\" a subinterval (\"c\", \"d\") must contain. He did not need to specify this because assuming that every subinterval contains at least one point of \"P\" implies that they contain infinitely many points of \"P\". This is proved by generating a sequence of points belonging to both \"P\" and (\"c\", \"d\"). Since \"P\" is dense in [\"a\", \"b\"], the subinterval (\"c\", \"d\") contains at least one point \"x\" of \"P\". Now consider the subinterval (\"x\", \"d\"). It contains at least one point \"x\" of \"P\", which satisfies \"x\" > x. In general, after generating \"x\", the subinterval (x, \"d\") is used to obtain the point \"x\", which satisfies \"x\" > \"x\". The points \"x\" are all unique and belong to both \"P\" and (\"c\", \"d\").\n\nCantor's 1879 proof is the same as his 1874 proof except for a new proof of first part of his second theorem: Given any sequence \"P\" of real numbers \"x\", \"x\", \"x\", … and any interval [\"a\", \"b\"], there is a number in [\"a\", \"b\"] that is not contained in the sequence \"P\". The new proof has only two cases.\n\nIn the first case, \"P\" is not dense in [\"a\", \"b\"]. By definition, \"P\" is dense if and only if for all , there is an \"x\" ∈ \"P\" such that . Taking the negation of each side of the \"if and only if\" produces: \"P\" is not dense in [\"a\", \"b\"] if and only if there exists a such that for all \"x\" ∈ \"P\", we have . Thus, every number in (\"c\", \"d\") is not contained in the sequence \"P\". This case handles cases 1 and 3 of Cantor's 1874 proof.\n\nIn the second case, \"P\" is dense in [\"a\", \"b\"]. The denseness of \"P\" is used to recursively define a nested sequence of intervals that excludes all elements of \"P\". The definition begins with\n\"a\" = \"a\" and \"b\" = \"b\". The definition's inductive case starts with the interval (\"a\", \"b\"), which because of the denseness of \"P\" contains infinitely many elements of \"P\". From these elements of \"P\", we take the two with smallest indices and denote the least of these two numbers by \"a\" and the greatest by \"b\".\nCantor proved that for all \"n\": . We proved this in a previous section.\n\nThe sequence \"a\" is increasing and bounded above by \"b\", so it has a limit \"A\", which satisfies \"a\" < \"A\". The sequence \"b\" is decreasing and bounded below by \"a\", so it has a limit \"B\", which satisfies \"B\" < \"b\". Also, \"a\" < \"b\" implies \"A\" ≤ \"B\". Therefore, . If \"A\" < \"B\", then for every \"n\": \"x\" ∉ (\"A\", \"B\") because \"x\" is not in the larger interval (\"a\", \"b\"). This contradicts \"P\" being dense in [\"a\", \"b\"]. Therefore, \"A\" = \"B\". Since for all \"n\": but , the limit \"A\" is a real number that is not contained in the sequence \"P\". This case handles case 2 of Cantor's 1874 proof.\n\nCantor's new proof first takes care of the easy case of the sequence \"P\" not being dense in the interval. Then it deals with the more difficult case of \"P\" being dense. This division into cases not only indicates which sequences are most difficult to handle, but it also reveals the important role denseness plays in the proof.\n\nIn the Example of Cantor's construction, each successive nested interval excludes rational numbers for two different reasons. It will exclude the finitely many rationals visited in the search for the first two rationals within the interval (these two rationals will have the least indices). These rationals are then used to form an interval that excludes the rationals visited in the search along with infinitely many more rationals. However, it still contains infinitely many rationals since our sequence of rationals is dense in [0, 1]. Forming this interval from the two rationals with the least indices guarantees that this interval excludes an initial segment of our sequence that contains at least two more elements than the preceding initial segment. Since the denseness of our sequence guarantees that this process never ends, all rationals will be excluded. Because of the ordering of the rationals in our sequence, the intersection of the nested intervals is the set  − 1}.\n\nThe development leading to Cantor's 1874 article appears in the correspondence between Cantor and Richard Dedekind. On November 29, 1873, Cantor asked Dedekind whether the collection of positive integers and the collection of positive real numbers \"can be corresponded so that each individual of one collection corresponds to one and only one individual of the other?\" Cantor added that collections having such a correspondence include the collection of positive rational numbers, and collections of the form (\"a\") where \"n\", \"n\", . . . , \"n\", and \"ν\" are positive integers.\n\nDedekind replied that he was unable to answer Cantor's question, and said that it \"did not deserve too much effort because it has no particular practical interest.\" Dedekind also sent Cantor a proof that the set of algebraic numbers is countable.\nOn December 2, Cantor responded that his question does have interest: \"It would be nice if it could be answered; for example, provided that it could be answered \"no\", one would have a new proof of Liouville's theorem that there are transcendental numbers.\"\n\nOn December 7, Cantor sent Dedekind a proof by contradiction that the set of real numbers is uncountable. Cantor starts by assuming the real numbers can be written as a sequence. Then he applies a construction to this sequence to produce a real number not in the sequence, thus contradicting his assumption. The letters of December 2 and 7 lead to a non-constructive proof of the existence of transcendental numbers.\n\nOn December 9, Cantor announced the theorem that allowed him to construct transcendental numbers as well as prove the uncountability of the set of real numbers:\n\nThis is the second theorem in Cantor's article. It comes from realizing that his construction can be applied to any sequence, not just to sequences that supposedly enumerate the real numbers. So Cantor had a choice between two proofs that demonstrate the existence of transcendental numbers: one proof is constructive, but the other is not. We now compare the proofs assuming that we have a sequence consisting of all the real algebraic numbers.\n\nThe constructive proof applies Cantor's construction to this sequence and the interval [\"a\", \"b\"] to produce a transcendental number in this interval.\n\nThe non-constructive proof uses two proofs by contradiction:\n\nCantor chose to publish the constructive proof, which not only produces a transcendental number but is also shorter and avoids two proofs by contradiction. The non-constructive proof from Cantor's correspondence is simpler than the one above because it works with all the real numbers rather than the interval [\"a\", \"b\"]. This eliminates the subsequence step and all occurrences of [\"a\", \"b\"] in the second proof by contradiction.\n\nThe correspondence containing Cantor's non-constructive reasoning was published in 1937. By then, other mathematicians had rediscovered its non-constructive proof. As early as 1921, this proof was attributed to Cantor and criticized for not producing any transcendental numbers. In that year, Oskar Perron stated: \"… Cantor's proof for the existence of transcendental numbers has, along with its simplicity and elegance, the great disadvantage that it is only an existence proof; it does not enable us to actually specify even a single transcendental number.\"\nSome mathematicians have attempted to correct this misunderstanding of Cantor's work. In 1930, the set theorist Abraham Fraenkel stated that Cantor's method is \"… a method that incidentally, contrary to a widespread interpretation, is fundamentally constructive and not merely existential.\" In 1972, Irving Kaplansky wrote: \"It is often said that Cantor's proof is not 'constructive,' and so does not yield a tangible transcendental number. This remark is not justified. If we set up a definite listing of all algebraic numbers … and then apply the diagonal procedure …, we get a perfectly definite transcendental number (it could be computed to any number of decimal places).\"\n\nThe disagreement about Cantor's proof occurs because two groups of mathematicians are talking about different proofs: the constructive one that Cantor published and the non-constructive one that was later rediscovered. The opinion that Cantor's proof is non-constructive appears in some books that were quite successful as measured by the length of time new editions or reprints appeared—for example: Eric Temple Bell's \"Men of Mathematics\" (1937; still being reprinted), Godfrey Hardy and E. M. Wright's \"An Introduction to the Theory of Numbers\" (1938; 2008 6th edition), Garrett Birkhoff and Saunders Mac Lane's \"A Survey of Modern Algebra\" (1941; 1997 5th edition), and Michael Spivak's \"Calculus\" (1967; 2008 4th edition). Since these books view Cantor's proof as non-constructive, they do not mention his constructive proof. On the other hand, the quotations above from Fraenkel and Kaplansky show that they knew Cantor's work can be used non-constructively. The disagreement about Cantor's proof shows no sign of being resolved: since 2014, at least two books have appeared stating that Cantor's proof is constructive, and at least four have appeared stating that his proof does not construct any (or a single) transcendental.\n\nAsserting that Cantor gave a non-constructive proof can lead to erroneous statements about the history of mathematics. In \"A Survey of Modern Algebra,\" Birkhoff and Mac Lane state: \"Cantor's argument for this result [Not every real number is algebraic] was at first rejected by many mathematicians, since it did not exhibit any specific transcendental number.\" Birkhoff and Mac Lane are talking about the non-constructive proof. Cantor's proof produces transcendental numbers, and there appears to be no evidence that his argument was rejected. Even Leopold Kronecker, who had strict views on what is acceptable in mathematics and who could have delayed publication of Cantor's article, did not delay it. In fact, applying Cantor's construction to the sequence of real algebraic numbers produces a limiting process that Kronecker accepted—namely, it determines a number to any required degree of accuracy.\n\nHistorians of mathematics have discovered the following facts about Cantor's article \"On a Property of the Collection of All Real Algebraic Numbers\":\n\n\nTo explain these facts, historians have pointed to the influence of Cantor's former professors, Karl Weierstrass and Leopold Kronecker. Cantor discussed his results with Weierstrass on December 23, 1873. Weierstrass was first amazed by the concept of countability, but then found the countability of the set of real algebraic numbers useful. Cantor did not want to publish yet, but Weierstrass felt that he must publish at least his results concerning the algebraic numbers.\n\nFrom his correspondence, it appears that Cantor only discussed his article with Weierstrass. However, Cantor told Dedekind: \"The restriction which I have imposed on the published version of my investigations is caused in part by local circumstances …\" Cantor biographer Joseph Dauben believes that \"local circumstances\" refers to Kronecker who, as a member of the editorial board of \"Crelle's Journal\", had delayed publication of an 1870 article by Eduard Heine, one of Cantor's colleagues. Cantor would submit his article to \"Crelle's Journal\".\n\nWeierstrass advised Cantor to leave his uncountability theorem out of the article he submitted, but Weierstrass also told Cantor that he could add it as a marginal note during proofreading, which he did. It appears in a remark at the end of the article's introduction. The opinions of Kronecker and Weierstrass both played a role here. Kronecker did not accept infinite sets, and it seems that Weierstrass did not accept that two infinite sets could be so different, with one being countable and the other not. Weierstrass changed his opinion later. Without the uncountability theorem, the article needed a title that did not refer to this theorem. Cantor chose \"Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen\" (\"On a Property of the Collection of All Real Algebraic Numbers\"), which refers to the countability of the set of real algebraic numbers, the result that Weierstrass found useful.\n\nKronecker's influence appears in the proof of Cantor's second theorem. Cantor used Dedekind's version of the proof except he left out why the limits \"a\" = lim \"a\" and \n\"b\" = lim \"b\" exist. Dedekind had used his \"principle of continuity\" to prove they exist. This principle (which is equivalent to the least upper bound property of the real numbers) comes from Dedekind's construction of the real numbers, a construction Kronecker did not accept.\n\nCantor restricted his first theorem to the set of real algebraic numbers even though Dedekind had sent him a proof that handled all algebraic numbers. Cantor did this for expository reasons and because of \"local circumstances.\" This restriction simplifies the article because the second theorem works with real sequences. Hence, the construction in the second theorem can be applied directly to the enumeration of the real algebraic numbers to produce \"an effective procedure for the calculation of transcendental numbers.\" This procedure would be acceptable to Weierstrass.\n\nSince 1856, Dedekind had developed theories involving infinitely many infinite sets—for example: ideals, which he used in algebraic number theory, and Dedekind cuts, which he used to construct the real numbers. This work enabled him to understand and contribute to Cantor's work.\n\nDedekind's first contribution concerns the theorem that the set of real algebraic numbers is countable. Cantor is usually given credit for this theorem, but the mathematical historian José Ferreirós calls it \"Dedekind's theorem.\" Their correspondence reveals what each mathematician contributed to the theorem.\n\nIn his letter introducing the concept of countability, Cantor stated without proof that the set of positive rational numbers is countable, as are sets of the form (\"a\") where \"n\", \"n\", …, \"n, and \"ν\" are positive integers. Cantor's second result uses indexed numbers: a set of the form (\"a\") is the range of a function from the \"ν\" indices to the set of real numbers. His second result implies his first: let \"ν\" = 2 and \"a\" = . The function can be quite general—for example, \"a\" = (where R is the set of real numbers) and the set of irrational numbers have the same cardinality as R.\n\nIn 1883, Cantor extended the natural numbers with his infinite ordinals. This extension was necessary for his work on the Cantor–Bendixson theorem. Cantor discovered other uses for the ordinals—for example, he used sets of ordinals to produce an infinity of sets having different infinite cardinalities. His work on infinite sets together with Dedekind's set-theoretical work created set theory.\n\nThe concept of countability led to countable operations and objects that are used in various areas of mathematics. For example, in 1878, Cantor introduced countable unions of sets. In the 1890s, Émile Borel used countable unions in his theory of measure, and René Baire used countable ordinals to define his classes of functions. Building on the work of Borel and Baire, Henri Lebesgue created his theories of measure and integration, which were published from 1899 to 1901.\n\nCountable models are used in set theory. In 1922, Thoralf Skolem proved that if conventional axioms of set theory are consistent, then they have a countable model. Since this model is countable, its set of real numbers is countable. This consequence is called Skolem's paradox, and Skolem explained why it does not contradict Cantor's uncountability theorem: although there is a one-to-one correspondence between this set and the set of positive integers, no such one-to-one correspondence is a member of the model. Thus the model considers its set of real numbers to be uncountable, or more precisely, the first-order sentence that says the set of real numbers is uncountable is true within the model. In 1963, Paul Cohen used countable models to prove his independence theorems.\n\n\n"}
{"id": "9626709", "url": "https://en.wikipedia.org/wiki?curid=9626709", "title": "HAS-V", "text": "HAS-V\n\nHAS-V is a cryptographic hash function with a variable output length. HAS-V is a hash function based on a block cipher. The hash function can produce hash values with lengths from 16 to 40 bytes.\n\nDigest Size: 128-320 bits\nMax message length: <2 bits\nCompression Function: 1024-bit message block, 320-bit chaining variable\n\nThe hash function was developed by Nan Kyoung Park, Joon Ho Hwang and Pil Joong Lee, and was released in 2000.\n\n\n"}
{"id": "20383103", "url": "https://en.wikipedia.org/wiki?curid=20383103", "title": "Henry Coddington", "text": "Henry Coddington\n\nHenry Coddington (1798/9, Oldbridge, County Meath — 3 March 1845, Rome) was an English natural philosopher, fellow and tutor of Trinity College, Cambridge and Church of England clergyman.\n\nHenry Coddington was the son of Latham Coddington, Rector of Timolin, Kildare. \nAdmitted to Trinity College, Cambridge in 1816, Coddingtion graduated BA as Senior Wrangler in 1820, and first Smith's prizeman; proceeded M.A. in 1823, and obtained a fellowship and sub-tutorship in his college. He retired to the college living of Ware in Hertfordshire, and in the discharge of his clerical duties burst a blood-vessel, thereby fatally injuring his health.\n\nCoddington was vicar of Ware, Hertfordshire from 1832 to 1845.\nAdvised to try a southern climate, he travelled abroad, and died at Rome 3 March 1845.\n\nHe married a daughter of Dr. Batten, principal of Haileybury College, and left seven children.\n\nHe wrote chiefly on optics, in particular \"An Elementary Treatise on Optics\". He also made the Coddington magnifier popular. He was elected a Fellow of the Royal Society in February, 1829.\n\nHis name occurs on the first list of members of the British Association. He was one of the earliest members of the Royal Astronomical Society, was a fellow of the Royal Geographical Society and Royal Society, and sat on the council of the latter body in 1831-2.\n\n"}
{"id": "6595367", "url": "https://en.wikipedia.org/wiki?curid=6595367", "title": "Hewitt–Savage zero–one law", "text": "Hewitt–Savage zero–one law\n\nThe Hewitt–Savage zero–one law is a theorem in probability theory, similar to Kolmogorov's zero–one law and the Borel–Cantelli lemma, that specifies that a certain type of event will either almost surely happen or almost surely not happen. It is sometimes known as the Hewitt–Savage law for symmetric events. It is named after Edwin Hewitt and Leonard Jimmie Savage.\n\nLet formula_1 be a sequence of independent and identically-distributed random variables taking values in a set formula_2. The Hewitt–Savage zero–one law says that any event whose occurrence or non-occurrence is determined by the values of these random variables and whose occurrence or non-occurrence is unchanged by finite permutations of the indices, has probability either 0 or 1 (a \"finite\" permutation is one that leaves all but finitely many of the indices fixed).\n\nSomewhat more abstractly, define the \"exchangeable sigma algebra\" or \"sigma algebra of symmetric events\" formula_3 to be the set of events (depending on the sequence of variables formula_1) which are invariant under permutations of the indices in the sequence formula_1. Then formula_6.\n\nSince any finite permutation can be written as a product of transpositions, if we wish to check whether or not an event formula_7 is symmetric (lies in formula_3), it is enough to check if its occurrence is unchanged by an arbitrary transposition formula_9, formula_10.\n\nLet the sequence formula_1 take values in formula_12. Then the event that the series formula_13 converges (to a finite value) is a symmetric event in formula_3, since its occurrence is unchanged under transpositions (for a finite re-ordering, the convergence or divergence of the series—and, indeed, the numerical value of the sum itself—is independent of the order in which we add up the terms). Thus, the series either converges almost surely or diverges almost surely. If we assume in addition that the common expected value formula_15 (which essentially means that formula_16 because of the random variables' non-negativity), we may conclude that\n\ni.e. the series diverges almost surely. This is a particularly simple application of the Hewitt–Savage zero–one law. In many situations, it can be easy to apply the Hewitt–Savage zero–one law to show that some event has probability 0 or 1, but surprisingly hard to determine \"which\" of these two extreme values is the correct one.\n\nContinuing with the previous example, define\n\nwhich is the position at step \"N\" of a random walk with the iid increments \"X\". The event { \"S\" = 0 infinitely often } is invariant under finite permutations. Therefore, the zero–one law is applicable and one infers that the probability of a random walk with real iid increments visiting the origin infinitely often is either one or zero. Visiting the origin infinitely often is a tail event with respect to the sequence (\"S\"), but \"S\" are not independent and therefore the Kolmogorov's zero–one law is not directly applicable here.\n"}
{"id": "21310186", "url": "https://en.wikipedia.org/wiki?curid=21310186", "title": "History of compiler construction", "text": "History of compiler construction\n\nIn computing, a compiler is a computer program that transforms source code written in a programming language or computer language (the \"source language\"), into another computer language (the \"target language\", often having a binary form known as \"object code\" or \"machine code\"). The most common reason for transforming source code is to create an executable program.\n\nAny program written in a high-level programming language must be translated to object code before it can be executed, so all programmers using such a language use a compiler or an interpreter. Thus, compilers are very important to programmers. Improvements to a compiler may lead to a large number of improved executable programs.\n\nThe Production Quality Compiler-Compiler, in the late 1970s, introduced the principles of compiler organization that are still widely used today (e.g., a front-end handling syntax and sematics and a back-end generating machine code).\n\nSoftware for early computers was primarily written in assembly language. It is usually more productive for a programmer to use a high-level language, and programs written in a high-level language can be reused on different kinds of computers. Even so, it took a while for compilers to become established, because they generated code that did not perform as well as hand-written assembler, they were daunting development projects in their own right, and the very limited memory capacity of early computers created many technical problems for practical compiler implementations.\n\nThe first compiler was written by Corrado Böhm, in 1951, for his PhD thesis. The term \"compiler\" was coined by Grace Hopper., referring to her A-0 system which functioned as a loader or linker, not the modern notion of a compiler. The FORTRAN team led by John W. Backus at IBM introduced the first commercially available compiler, in 1957, which took 18 person-years to create.\n\nThe first ALGOL 58 compiler was completed by the end of 1958 by Friedrich L. Bauer, Hermann Bottenbruch, Heinz Rutishauser, and Klaus Samelson for the Z22 computer. Bauer et al. had been working on compiler technology for the \"Sequentielle Formelübersetzung\" (i.e. \"sequential formula translation\") in the previous years.\n\nBy 1960, an extended Fortran compiler, ALTAC, was available on the Philco 2000, so it is probable that a Fortran program was compiled for both IBM and Philco computer architectures in mid-1960. The first known demonstrated cross-platform high-level language was COBOL. In a demonstration in December 1960, a COBOL program was compiled and executed on both the UNIVAC II and the RCA 501.\n\nLike any other software, there are benefits from implementing a compiler in a high-level language. In particular, a compiler can be self-hosted – that is, written in the programming language it compiles. Building a self-hosting compiler is a bootstrapping problem, i.e. the first such compiler for a language must be either hand written machine code or compiled by a compiler written in another language, or compiled by running the compiler in an interpreter.\n\nCorrado Böhm developed a language, a machine, and a translation method for compiling that language on the machine in his PhD dissertation dated 1951. He not only described a complete compiler, but also defined for the first time that compiler in its own language. The language was interesting in itself, because every statement (including input statements, output statements and control statements) was a special case of an assignment statement.\n\nThe Navy Electronics Laboratory International ALGOL Compiler or NELIAC was a dialect and compiler implementation of the ALGOL 58 programming language developed by the Naval Electronics Laboratory in 1958.\n\nNELIAC was the brainchild of Harry Huskey — then Chairman of the ACM and a well known computer scientist (and later academic supervisor of Niklaus Wirth), and supported by Maury Halstead, the head of the computational center at NEL. The earliest version was implemented on the prototype USQ-17 computer (called the Countess) at the laboratory. It was the world's first self-compiling compiler - the compiler was first coded in simplified form in assembly language (the \"bootstrap\"), then re-written in its own language and compiled by the bootstrap, and finally re-compiled by itself, making the bootstrap obsolete.\n\nAnother early self-hosting compiler was written for Lisp by Tim Hart and Mike Levin at MIT in 1962. They wrote a Lisp compiler in Lisp, testing it inside an existing Lisp interpreter. Once they had improved the compiler to the point where it could compile its own source code, it was self-hosting.\n\nThis technique is only possible when an interpreter already exists for the very same language that is to be compiled. It borrows directly from the notion of running a program on itself as input, which is also used in various proofs in theoretical computer science, such as the proof that the halting problem is undecidable.\n\nForth is an example of a self-hosting compiler. The self compilation and cross compilation features of Forth are commonly confused with metacompilation and metacompilers. Like Lisp, Forth is an extensible programming language. It is the extensible programming language features of Forth and Lisp that enable them to generate new versions of themselves or port themselves to new environments.\n\nA parser is an important component of a compiler. It parses the source code of a computer programming language to create some form of internal representation. Programming languages tend to be specified in terms of a context-free grammar because fast and efficient parsers can be written for them. Parsers can be written by hand or generated by a parser generator. A context-free grammar provides a simple and precise mechanism for describing how programming language constructs are built from smaller blocks. The formalism of context-free grammars was developed in the mid-1950s by Noam Chomsky.\n\nBlock structure was introduced into computer programming languages by the ALGOL project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting ALGOL syntax.\n\nContext-free grammars are simple enough to allow the construction of efficient parsing algorithms which, for a given string, determine whether and how it can be generated from the grammar. If a programming language designer is willing to work within some limited subsets of context-free grammars, more efficient parsers are possible.\n\nThe LR parser (left to right) was invented by Donald Knuth in 1965 in a paper, \"On the Translation of Languages from Left to Right\". An LR parser is a parser that reads input from Left to right (as it would appear if visually displayed) and produces a Rightmost derivation. The term LR(\"k\") parser is also used, where \"k\" refers to the number of unconsumed lookahead input symbols that are used in making parsing decisions.\n\nKnuth proved that LR(\"k\") grammars can be parsed with an execution time essentially proportional to the length of the program, and that every LR(\"k\") grammar for \"k\" > 1 can be mechanically transformed into an LR(1) grammar for the same language. In other words, it is only necessary to have one symbol lookahead to parse any deterministic context-free grammar(DCFG).\n\nKorenjak (1969) was the first to show parsers for programming languages could be produced using these techniques. Frank DeRemer devised the more practical Simple LR (SLR) and Look-ahead LR (LALR) techniques, published in his PhD dissertation at MIT in 1969. This was an important breakthrough, because LR(k) translators, as defined by Donald Knuth, were much too large for implementation on computer systems in the 1960s and 1970s.\n\nIn practice, LALR offers a good solution; the added power of LALR(1) parsers over SLR(1) parsers (that is, LALR(1) can parse more complex grammars than SLR(1)) is useful, and, though LALR(1) is not comparable with LL(1) (LALR(1) cannot parse all LL(1) grammars), most LL(1) grammars encountered in practice can be parsed by LALR(1). LR(1) grammars are more powerful again than LALR(1); however, an LR(1) grammar requires a canonical LR parser which would be extremely large in size and is not considered practical. The syntax of many programming languages are defined by grammars that can be parsed with an LALR(1) parser, and for this reason LALR parsers are often used by compilers to perform syntax analysis of source code.\n\nA recursive ascent parser implements an LALR parser using mutually-recursive functions rather than tables. Thus, the parser is \"directly encoded\" in the host language similar to recursive descent. Direct encoding usually yields a parser which is faster than its table-driven equivalent for the same reason that compilation is faster than interpretation. It is also (in principle) possible to hand edit a recursive ascent parser, whereas a tabular implementation is nigh unreadable to the average human.\n\nRecursive ascent was first described by Thomas Pennello in his article \"Very fast LR parsing\" in 1986. The technique was later expounded upon by G.H. Roberts in 1988 as well as in an article by Leermakers, Augusteijn, Kruseman Aretz in 1992 in the journal \"Theoretical Computer Science\".\n\nAn LL parser parses the input from Left to right, and constructs a Leftmost derivation of the sentence (hence LL, as opposed to LR). The class of grammars which are parsable in this way is known as the \"LL grammars\". LL grammars are an even more restricted class of context-free grammars than LR grammars. Nevertheless, they are of great interest to compiler writers, because such a parser is simple and efficient to implement.\n\nLL(k) grammars can be parsed by a recursive descent parser which is usually coded by hand, although a notation such as META II might alternatively be used.\n\nThe design of ALGOL sparked investigation of recursive descent, since the ALGOL language itself is recursive. The concept of recursive descent parsing was discussed in the January 1961 issue of CACM in separate papers by A.A. Grau and Edgar T. \"Ned\" Irons.\nRichard Waychoff and colleagues also implemented recursive descent in the Burroughs ALGOL compiler in March 1961, the two groups used different approaches but were in at least informal contact.\n\nThe idea of LL(1) grammars was introduced by Lewis and Stearns (1968).\n\nRecursive descent was popularised by Niklaus Wirth with PL/0, an educational programming language used to teach compiler construction in the 1970s.\n\nLR parsing can handle a larger range of languages than LL parsing, and is also better at error reporting, i.e. it detects syntactic errors when the input does not conform to the grammar as soon as possible.\n\nIn 1970, Jay Earley invented what came to be known as the Earley parser. Earley parsers are appealing because they can parse all context-free languages reasonably efficiently.\n\nJohn Backus proposed \"metalinguistic formulas\"\nto describe the syntax of the new programming language IAL, known today as ALGOL 58 (1959). Backus's work was based on the Post canonical system devised by Emil Post.\n\nFurther development of ALGOL led to ALGOL 60; in its report (1963), Peter Naur named Backus's notation Backus normal form (BNF), and simplified it to minimize the character set used. However, Donald Knuth argued that BNF should rather be read as Backus–Naur form, and that has become the commonly accepted usage.\n\nNiklaus Wirth defined extended Backus–Naur form (EBNF), a refined version of BNF, in the early 1970s for PL/0. Augmented Backus–Naur form (ABNF) is another variant. Both EBNF and ABNF are widely used to specify the grammar of programming languages, as the inputs to parser generators, and in other fields such as defining communication protocols.\n\nA parser generator generates the lexical-analyser portion of a compiler. It is a program that takes a description of a formal grammar of a specific programming language and produces a parser for that language. That parser can be used in a compiler for that specific language. The parser detects and identifies the reserved words and symbols of the specific language from a stream of text and returns these as tokens to the code which implements the syntactic validation and translation into object code. This second part of the compiler can also be created by a \"compiler-compiler\" using a formal rules-of-precedence syntax-description as input.\n\nThe first \"compiler-compiler\" to use that name was written by Tony Brooker in 1960 and was used to create compilers for the Atlas computer at the University of Manchester, including the Atlas Autocode compiler. However it was rather different from modern compiler-compilers, and today would probably be described as being somewhere between a highly customisable generic compiler and an extensible-syntax language. The name 'compiler-compiler' was far more appropriate for Brooker's system than it is for most modern compiler-compilers, which are more accurately described as parser generators. It is almost certain that the \"Compiler Compiler\" name has entered common use due to Yacc rather than Brooker's work being remembered.\n\nIn the early 1960s, Robert McClure at Texas Instruments invented a compiler-compiler called TMG, the name taken from \"transmogrification\". In the following years TMG was ported to several UNIVAC and IBM mainframe computers.\n\nThe Multics project, a joint venture between MIT and Bell Labs, was one of the first to develop an operating system in a high level language. PL/I was chosen as the language, but an external supplier could not supply a working compiler. The Multics team developed their own subset dialect of PL/I known as Early PL/I (EPL) as their implementation language in 1964. TMG was ported to GE-600 series and used to develop EPL by Douglas McIlroy, Robert Morris, and others.\n\nNot long after Ken Thompson wrote the first version of Unix for the PDP-7 in 1969, Doug McIlroy created the new system's first higher-level language: an implementation of McClure's TMG. TMG was also the compiler definition tool used by Ken Thompson to write the compiler for the B language on his PDP-7 in 1970. B was the immediate ancestor of C.\n\nAn early LALR parser generator was called \"TWS\", created by Frank DeRemer and Tom Pennello.\n\nXPL is a dialect of the PL/I programming language, used for the development of compilers for computer languages. It was designed and implemented in 1967 by a team with William M. McKeeman, James J. Horning, and David B. Wortman at Stanford University and the University of California, Santa Cruz. It was first announced at the 1968 Fall Joint Computer Conference in San Francisco.\n\nXPL featured a relatively simple translator writing system dubbed ANALYZER, based upon a bottom-up compiler precedence parsing technique called MSP (mixed strategy precedence). XPL was bootstrapped through Burroughs Algol onto the IBM System/360 computer. (Some subsequent versions of XPL used on University of Toronto internal projects utilized an SLR(1) parser, but those implementations have never been distributed).\n\nYacc is a parser generator (loosely, compiler-compiler), not to be confused with lex, which is a lexical analyzer frequently used as a first stage by Yacc. Yacc was developed by Stephen C. Johnson at AT&T for the Unix operating system. The name is an acronym for \"Yet Another Compiler Compiler.\" It generates an LALR(1) compiler based on a grammar written in a notation similar to Backus–Naur form.\n\nJohnson worked on Yacc in the early 1970s at Bell Labs. He was familiar with TMG and its influence can be seen in Yacc and the design of the C programming language. Because Yacc was the default compiler generator on most Unix systems, it was widely distributed and used. Derivatives such as GNU Bison are still in use.\n\nThe compiler generated by Yacc requires a lexical analyzer. Lexical analyzer generators, such as lex or flex are widely available. The IEEE POSIX P1003.2 standard defines the functionality and requirements for both Lex and Yacc.\n\nMetacompilers differ from parser generators, taking as input a program written in a metalanguage. Their input consists grammar analyzing formula and code production transforms that output executable code. Many can be programmed in their own metalanguage enabling them to compile themselves, making them self-hosting extensible language compilers.\n\nMany metacompilers build on the work of Dewey Val Schorre. His META II compiler, first released in 1964, was the first documented metacompiler. Able to define its own language and others, META II accepted syntax formula having imbedded output (code production)s. It also translated to one of the earliest instances of a virtual machine. Lexical analysis was performed by built token recognizing functions: .ID, .STRING, and .NUMBER. Quoted strings in syntax formula recognize lexemes that are not kept.\n\nTREE-META, a second generation Schorre metacompiler, appeared around 1968. It extended the capabilities of META II, adding unparse rules separating code production from the grammar analysis. Tree transform operations in the syntax formula produce abstract syntax trees that the unparse rules operate on. The unparse tree pattern matching provided peephole optimization ability.\n\nCWIC, described in a 1970 ACM publication is a third generation Schorre metacompiler that added lexing rules and backtracking operators to the grammar analysis. LISP 2 was married with the unparse rules of TREEMETA in the CWIC generator language. With LISP 2 processing, CWIC can generate fully optimized code. CWIC also provided binary code generation into named code sections. Single and multipass compiles could be implemented using CWIC.\n\nCWIC compiled to 8 bit byte addressable machine code instructions primarily designed to produce IBM System/360 code.\n\nLater generations are not publicly documented. One important feature would be the abstraction of the target processor instruction set, generating to a pseudo machine instruction set, macros, that could be separately defined or mapped to a real machine's instructions. Optimizations applying to sequential instructions could then be applied to the pseudo instruction before their expansion to target machine code.\n\nA cross compiler runs in one environment but produces object code for another. Cross compilers are used for embedded development, where the target computer has limited capabilities.\n\nAn early example of cross compilation was AIMICO, where a FLOW-MATIC program on a UNIVAC II was used to generate assembly language for the IBM 705, which was then assembled on the IBM computer.\n\nThe ALGOL 68C compiler generated \"ZCODE\" output, that could then be either compiled into the local machine code by a \"ZCODE\" translator or run interpreted. \"ZCODE\" is a register-based intermediate language. This ability to interpret or compile \"ZCODE\" encouraged the porting of ALGOL 68C to numerous different computer platforms.\n\nCompiler optimization is the process of improving the quality of object code without changing the results it produces.\n\nThe developers of the first FORTRAN compiler aimed to generate code that was \"better\" than the average\nhand-coded assembler, so that customers would actually use their product. In one of the first\nreal compilers, they often succeeded.\n\nLater compilers, like IBM's Fortran IV compiler, placed more priority on good diagnostics and executing more quickly, at the expense of object code optimization. It wasn't until the IBM System/360 series that IBM provided two separate compilers: a fast executing code checker, and a slower optimizing one.\n\nFrances E. Allen, working alone and jointly with John Cocke, introduced many of the concepts for optimization. Allen's 1966 paper, \"Program Optimization,\" introduced the use of graph data structures to encode program content for optimization. Her 1970 papers, \"Control Flow Analysis\" and \"A Basis for Program Optimization\" established \"intervals\" as the context for efficient and effective data flow analysis and optimization. Her 1971 paper with Cocke, \"A Catalogue of Optimizing Transformations\", provided the first description and systematization of optimizing transformations. Her 1973 and 1974 papers on interprocedural data flow analysis extended the analysis to whole programs. Her 1976 paper with Cocke describes one of the two main analysis strategies used in optimizing compilers today.\n\nAllen developed and implemented her methods as part of compilers for the IBM 7030 Stretch-Harvest and the experimental Advanced Computing System. This work established the feasibility and structure of modern machine- and language-independent optimizers. She went on to establish and lead the PTRAN project on the automatic parallel execution of FORTRAN programs. Her PTRAN team developed new parallelism detection schemes and created the concept of the program dependence graph, the primary structuring method used by most parallelizing compilers.\n\n\"Programming Languages and their Compilers\" by John Cocke and Jacob T. Schwartz, published early in 1970, devoted more than 200 pages to optimization algorithms. It included many of the now familiar techniques such as redundant code elimination and strength reduction.\n\nPeephole optimization is a very simple but effective optimization technique. It was invented by William M. McKeeman and published in 1965 in CACM. It was used in the XPL compiler that McKeeman helped develop.\n\nCapex Corporation developed the \"COBOL Optimizer\" in the mid 1970s for COBOL. This type of optimizer depended, in this case, upon knowledge of 'weaknesses' in the standard IBM COBOL compiler, and actually replaced (or patched) sections of the object code with more efficient code. The replacement code might replace a linear table lookup with a binary search for example or sometimes simply replace a relatively 'slow' instruction with a known faster one that was otherwise functionally equivalent within its context. This technique is now known as \"Strength reduction\". For example, on the IBM System/360 hardware the CLI instruction was, depending on the particular model, between twice and 5 times as fast as a CLC instruction for single byte comparisons.\n\nModern compilers typically provide optimization options, so programmers can choose whether or not to execute an optimization pass.\n\nWhen a compiler is given a syntactically incorrect program, a good, clear error message is helpful. From the perspective of the compiler writer, it is often difficult to achieve.\n\nThe WATFIV Fortran compiler was developed at the University of Waterloo, Canada in the late 1960s. It was designed to give better error messages than IBM's Fortran compilers of the time. In addition, WATFIV was far more usable, because it combined compiling, linking and execution into one step, whereas IBM's compilers had three separate components to run.\n\nPL/C was a computer programming language developed at Cornell University in the early 1970s. While PL/C was a subset of IBM's PL/I language, it was designed with the specific goal of being used for teaching programming. The two researchers and academic teachers who designed PL/C were Richard W. Conway and Thomas R. Wilcox. They submitted the famous article \"Design and implementation of a diagnostic compiler for PL/I\" published in the Communications of ACM in March 1973.\n\nPL/C eliminated some of the more complex features of PL/I, and added extensive debugging and error recovery facilities. The PL/C compiler had the unusual capability of never failing to compile any program, through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements.\n\nJust in time compilation (JIT) is the generation of executable code on-the-fly or as close as possible to its actual execution, to take advantage of run time metrics or other performance enhancing options.\n\nMost modern compilers have a lexer and parser that produce an intermediate representation of the program. The intermediate representation is a simple sequence of operations which can be used by an optimizer and a code generator which produces instructions in the machine language of the target processor. Because the code generator uses an intermediate representation, the same code generator can be used for many different high level languages.\n\nThere are many possibilities for the intermediate representation. Three-address code, also known as a \"quadruple\" or \"quad\" is a common form, where there is an operator, two operands, and a result. Two-address code or \"triples\" have a stack to which results are written, in contrast to the explicit variables of three-address code.\n\nStatic Single Assignment (SSA) was developed by Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck, researchers at IBM in the 1980s. In SSA, a variable is given a value only once. A new variable is created rather than modifying an existing one. SSA simplifies optimization and code generation.\n\nA code generator generates machine language instructions for the target processor.\n\nSethi–Ullman algorithm or Sethi-Ullman numbering is a method to minimise the number of registers needed to hold variables.\n\n\n\n"}
{"id": "46897719", "url": "https://en.wikipedia.org/wiki?curid=46897719", "title": "Hub (network science)", "text": "Hub (network science)\n\nIn network science, a hub is a node with a number of links that greatly exceeds the average. Emergence of hubs is a consequence of a scale-free property of networks. While hubs cannot be observed in a random network, they are expected to emerge in scale-free networks. The uprise of hubs in scale-free networks is associated with power-law distribution. Hubs have a significant impact on the network topology. Hubs can be found in many real networks, such as Brain Network or Internet.\n\nA hub is a component of a network with a high-degree node. Hubs have a significantly larger number of links in comparison with other nodes in the network. The number of links (degrees) for a hub in a scale-free network is much higher than for the biggest node in a random network, keeping the size \"N\" of the network and average degree \"<k>\" constant. The existence of hubs is the biggest difference between random networks and scale-free networks. In random networks, the degree \"k\" is comparable for every node; it is therefore not possible for hubs to emerge. In scale-free networks, a few nodes (hubs) have a high degree \"k\" while the other nodes have a small number of links.\n\nEmergence of hubs can be explained by the difference between scale-free networks and random networks. Scale-free networks (Barabási–Albert model) are different from random networks (Erdős–Rényi model) in two aspects: (a) growth, (b) preferential attachment.\n\nThe mathematical explanation for Barabási–Albert model:\nThe network begins with an initial connected network of formula_1 nodes.\n\nNew nodes are added to the network one at a time. Each new node is connected to formula_2 existing nodes with a probability that is proportional to the number of links that the existing nodes already have. Formally, the probability formula_3 that the new node is connected to node formula_4 is\n\nwhere formula_6 is the degree of the node formula_4 and the sum is taken over all pre-existing nodes formula_8 (i.e. the denominator results in twice the current number of edges in the network).\n\nEmergence of hubs in networks is also related to time. In scale-free networks, nodes which emerged earlier have a higher chance of becoming a hub than latecomers. This phenomenon is called first-mover advantage and it explains why some nodes become hubs and some do not. However, in a real network, the time of emergence is not the only factor that influences the size of the hub. For example, Facebook emerged 8 years later after Google became the largest hub on the World Wide Web and yet in 2011 Facebook became the largest hub of WWW. Therefore, in real networks the growth and the size of a hub depends also on various attributes such as popularity, quality or the aging of a node.\n\nThere are several attributes of Hubs in a scale-free networks\n\nThe more observable hubs are in a network, the more they shrink a distances between nodes. In a scale-free networks hubs serve as bridges between the small degree nodes. Since the distance of two random nodes in a scale-free networks is small, we refer to scale-free networks as \"small\" or \"ultra small\". While a difference between path distance in a various small networks may not be noticeable, the difference in a path distance between large random network and scale-free network is remarkable.\n\nAverage path length in scale-free networks:\nformula_9\n\nThe phenomenon present in a real networks, when older hubs are shadowed in a network. This phenomenon is responsible for changes in evolution and topology of networks. The example of aging phenomenon may be the case of Facebook overtaking the position of the largest hub on the Web where Google was the largest node since 2000.\n\nDuring the random failure of nodes or targeted attack hubs are key components of the network. During the random failure of nodes in network hubs are responsible for exceptional robustness of network. The chance that a random failure would delete the hub is very small, because hubs coexists with a large number of small degree nodes. The removal of small degree nodes does not have a large effect on integrity of network. Even though the random removal would hit the hub, the chance of fragmantation of network is very small because the remaining hubs would hold the network together. In this case, hubs are the strength of a scale-free networks.\n\nDuring a targeted attack on hubs, the integrity of a network will fall apart relatively fast. Since small nodes are predominantly linked to hubs, the targeted attack on the largest hubs results in destroys the network in a short period of time. The financial market meltdown in 2008 is an example of such a network failure, when bankruptcy of the largest players (hubs) led to a continuous breakdown of the whole system. On the other hand, it may have a positive effect when removing hubs in a terrorist network; targeted node deletion may destroy the whole terrorist group. The attack tolerance of a network may be increased by connecting its peripheral nodes, however it requires to double the number of links.\n\nThe perfect degree correlation means that each degree-k node is connected only to the same degree-k nodes. Such connectivity of nodes decide the topology of networks, which has an effect on robustness of network, the attribute discussed above. If the number of links between the hubs is the same as would be expected by chance, we refer to this network as Neutral Network. If hubs tend to connected to each other while avoiding linking to small-degree nodes we refer to this network as Assortative Network. This network is relatively resistant against attacks, because hubs form a core group, which is more reduntant against hub removal. If hubs avoid connecting to each other while linking to small-degree nodes, we refer to this network as Disassortative Network. This network has a hub-and-spoke character. Therefore, if we remove the hub in this type of network, it may damage or destroy the whole network.\n\nThe hubs are also responsible for effective spreading of material on network. In an analysis of disease spreading or information flow, hubs are referred to as super-spreaders. Super-spreaders may have a positive impact, such as effective information flow, but also devastating in a case of epidemic spreading such as H1N1 or AIDS. The mathematical models such as model of H1H1 Epidemic prediction may allow us to predict the spread of diseases based on human mobility networks, infectiousness, or social interactions among humans. Hubs are also important in the eradication of disease. In a scale-free network hubs are most likely to be infected, because of the large number of connections they have. After the hub is infected, it broadcasts the disease to the nodes it is linked to. Therefore, the selective immunization of hubs may be the cost-effective strategy in eradication of spreading disease.\n"}
{"id": "59158118", "url": "https://en.wikipedia.org/wiki?curid=59158118", "title": "Idempotent analysis", "text": "Idempotent analysis\n\nIn mathematical analysis, idempotent analysis is the study of idempotent semirings, such as the tropical semiring. The lack of an additive inverse in the semiring is compensated somewhat by the idempotent rule \"A\" + \"A\" = \"A\".\n\n"}
{"id": "2969494", "url": "https://en.wikipedia.org/wiki?curid=2969494", "title": "Joseph Wedderburn", "text": "Joseph Wedderburn\n\nJoseph Henry Maclagan Wedderburn FRSE FRS (2 February 1882, Forfar, Angus, Scotland – 9 October 1948, Princeton, New Jersey) was a Scottish mathematician, who taught at Princeton University for most of his career. A significant algebraist, he proved that a finite division algebra is a field, and part of the Artin–Wedderburn theorem on simple algebras. He also worked on group theory and matrix algebra.\nHis younger brother was the lawyer Ernest Wedderburn.\n\nJoseph Wedderburn was the tenth of fourteen children of Alexander Wedderburn of Pearsie, a physician, and Anne Ogilvie. Educated at Forfar Academy and George Watson's College, Edinburgh, in 1898 he entered the University of Edinburgh. In 1903, he published his first three papers, worked as an assistant in the Physical Laboratory of the University, obtained an MA degree with First Class Honours in mathematics, and was elected a Fellow of the Royal Society of Edinburgh, upon the proposal of George Chrystal, James Gordon MacGregor, Cargill Gilston Knott and William Peddie.\n\nHe then studied briefly at the University of Leipzig and the University of Berlin, where he met the algebraists Frobenius and Schur. A Carnegie Scholarship allowed him to spend the 1904–1905 academic year at the University of Chicago where he worked with Oswald Veblen, E. H. Moore, and most importantly, Leonard Dickson, who was to become the most important American algebraist of his day.\n\nReturning to Scotland in 1905, Wedderburn worked for four years at the University of Edinburgh as an assistant to George Chrystal, who supervised his D.Sc, awarded in 1908 for a thesis titled \"On Hypercomplex Numbers\". He gained a PhD in algebra from the University of Edinburgh in 1908. From 1906 to 1908, Wedderburn edited the \"Proceedings of the Edinburgh Mathematical Society\". In 1909, he returned to the United States to become a Preceptor in Mathematics at Princeton University; his colleagues included Luther P. Eisenhart, Oswald Veblen, Gilbert Ames Bliss, and George Birkhoff.\n\nUpon the outbreak of the First World War, Wedderburn enlisted in the British Army as a private. He was the first person at Princeton to volunteer for that war, and had the longest war service of anyone on the staff. He served with the Seaforth Highlanders in France, as Lieutenant (1914), then as Captain of the 10th Battalion (1915–18). While a Captain in the Fourth Field Survey Battalion of the Royal Engineers in France, he devised sound-ranging equipment to locate enemy artillery.\n\nHe returned to Princeton after the war, becoming Associate Professor in 1921 and editing the \"Annals of Mathematics\" until 1928. While at Princeton, he supervised only three PhDs, one of them being Nathan Jacobson. In his later years, Wedderburn became an increasingly solitary figure and may even have suffered from depression. His isolation after his 1945 early retirement was such that his death from a heart attack was not noticed for several days. His Nachlass was destroyed, as per his instructions.\n\nWedderburn received the MacDougall-Brisbane Gold Medal and Prize from the Royal Society of Edinburgh in 1921, and was elected to the Royal Society of London in 1933.\n\nAs to why Wedderburn never married:\n\nIn all, Wedderburn published about 40 books and papers, making important advances in the theory of rings, algebras and matrix theory.\n\nIn 1905, Wedderburn published a paper that included three claimed proofs of a theorem stating that a noncommutative finite division ring could not exist. The proofs all made clever use of the interplay between the additive group of a finite division algebra \"A\", and the multiplicative group \"A\"* = \"A\"-{0}. Parshall (1983) notes that the first of these three proofs had a gap not noticed at the time. Meanwhile, Wedderburn's Chicago colleague Dickson also found a proof of this result but, believing Wedderburn's first proof to be correct, Dickson acknowledged Wedderburn's priority. But Dickson also noted that Wedderburn constructed his second and third proofs only after having seen Dickson's proof. Parshall concludes that Dickson should be credited with the first correct proof.\n\nA corollary to this theorem yields the complete structure of all finite projective geometry. In their paper on \"Non-Desarguesian and non-Pascalian geometries\" in the 1907 \"Transactions of the American Mathematical Society\", Wedderburn and Veblen showed that in these geometries, Pascal's theorem is a consequence of Desargues' theorem. They also constructed finite projective geometries which are neither \"Desarguesian\" nor \"Pascalian\" (the terminology is Hilbert's).\n\nWedderburn's best-known paper was his sole-authored \"On hypercomplex numbers,\" published in the 1907 Proceedings of the London Mathematical Society, and for which he was awarded the D.Sc. the following year. This paper gives a complete classification of simple and semisimple algebras. He then showed that every semisimple algebra finite-dimensional can be constructed as a direct sum of simple algebras and that every simple algebra is isomorphic to a matrix algebra for some division ring. The Artin–Wedderburn theorem generalises this result, with the ascending chain condition.\n\nHis best known book is his \"Lectures on Matrices\" (1934), which Jacobson praised as follows:\n\nAbout Wedderburn's teaching:\n\n"}
{"id": "9753326", "url": "https://en.wikipedia.org/wiki?curid=9753326", "title": "Ken Ono", "text": "Ken Ono\n\nKen Ono (born 20 March 1968) is a Japanese-American mathematician who specializes in number theory, especially in integer partitions, modular forms, Umbral moonshine, and the fields of interest to Srinivasa Ramanujan. He was the Manasse Professor of Letters and Science and the Hilldale Professor of Mathematics at the University of Wisconsin–Madison.\n\nHe is currently the Asa Griggs Candler Professor of Mathematics at Emory University and the Vice President of the American Mathematical Society.\n\nOno is the son of mathematician Takashi Ono, who emigrated from Japan to the United States after World War II. His older brother, immunologist and university president Santa J. Ono, was born while Takashi Ono was in Canada working at the University of British Columbia, but by the time Ken Ono was born the family had returned to the US for a position at the University of Pennsylvania. In the 1980s, Ono attended Towson High School, but he dropped out. He later enrolled at the University of Chicago without a high school diploma. There he raced bicycles, and he was a member of the Pepsi–Miyata Cycling Team.\n\nHe received his BA from the University of Chicago in 1989, where he was a member of the Psi Upsilon fraternity. He earned his PhD in 1993 at UCLA where his advisor was Basil Gordon. Initially he planned to study medicine, but later switched to mathematics. He attributes his interest in mathematics to his father.\n\nOno's contributions include several monographs and over 160 research and popular articles in number theory, combinatorics, and algebra. He is considered to be an expert in the theory of integer partitions and modular forms. In 2000 he greatly expanded Ramanujan's theory of partition congruences, and in work with Kathrin Bringmann he has made important contributions to the theory of Maass forms, functions which include Ramanujan's mock theta functions as examples. In 2007 Don Zagier gave a Seminar Bourbaki address on the work of Bringmann, Ono, and Zwegers on the mock theta functions. The 2009 SASTRA Ramanujan Prize, awarded to a young mathematician under the age of 32, was awarded to Kathrin Bringmann for this joint work with Ono. Recently he and his collaborators have announced a proof of the famous Umbral Moonshine Conjecture.\n\nOno has received many awards for his research. In April 2000 he received the Presidential Career Award (PECASE) from Bill Clinton in a ceremony at the White House, and in June 2005 he received the National Science Foundation Director's Distinguished Teaching Scholar Award at the National Academy of Science. He has also won a Sloan Fellowship, a Packard Fellowship, and a Guggenheim Fellowship. In 2012 he became a fellow of the American Mathematical Society.\n\nIn 2011 and 2015 Ono gave TED talks.\n\nIn a joint work with Jan Bruinier, he recently discovered a finite formula for computing partition numbers.\n\nHe stars in the 2013 docudrama \"The genius of Srinivasa Ramanujan\".\n\nHe is profiled in the May 2014 issue of Scientific American. He was an Associate Producer and the mathematical consultant for the movie \"The Man Who Knew Infinity\" based on Ramanujan's biography written by Robert Kanigel.\n\nIn April 2014 Ono announced that he and two others had found a framework for the Rogers–Ramanujan identities and their arithmetic properties, solving a long-standing mystery stemming from the work of Ramanujan. The findings yield a treasure trove of algebraic numbers and formulas to access them. Ono's co-authors for this work were S. Ole Warnaar of the University of Queensland and Michael Griffin, an Emory University graduate student. Their work made world news that year and was ranked 15th among the top 100 stories of 2014 in science in \"Discover\" magazine.\n\nAfter 15 years of focusing on the Rogers–Ramanujan identities, Warnaar had found a way to embed them into a much larger class of similar identities using representation theory. When Ono saw Warnaar's work, \"It just clicked,\" Ono recalls. \"Now we can extract infinitely many functions whose values are algebraic numbers.\" \n\nIn recent years, Ono has resumed athletic training as a runner, swimmer and cyclist; since 2012, he has competed in triathlons as a member of Team USA. Ken Ono now lives in Atlanta, Georgia, with his wife Erika, daughter Aspen, and son Sage.\n\n\nOno is on the editorial board of several journals:\n\n\n"}
{"id": "29973465", "url": "https://en.wikipedia.org/wiki?curid=29973465", "title": "Kolmogorov equations", "text": "Kolmogorov equations\n\nIn probability theory, Kolmogorov equations, including Kolmogorov forward equations and Kolmogorov backward equations, characterize \"stochastic processes\". In particular, they describe how the probability that a stochastic process is in a certain state changes over time.\n\nWriting in 1931, Andrei Kolmogorov started from the theory of discrete time Markov processes, which are described by the Chapman-Kolmogorov equation, and sought to derive a theory of continuous time Markov processes by extending this equation. He found that there are two kinds of continuous time Markov processes, depending on the assumed behavior over small intervals of time:\n\nIf you assume that \"in a small time interval there is an overwhelming probability that the state will remain unchanged; however, if it changes, the change may be radical\", then you are led to what are called jump processes.\n\nThe other case leads to processes such as those \"represented by diffusion and by Brownian motion; there it is certain that some change will occur in any time interval, however small; only, here it is certain that the changes during small time intervals will be also small\".\n\nFor each of these two kinds of processes, Kolmogorov derived a forward and a backward system of equations (four in all).\n\nThe equations are named after Andrei Kolmogorov since they were highlighted in his 1931 foundational work.\n\nWilliam Feller, in 1949, used the names \"forward equation\" and \"backward equation\" for his more general version of the Kolmogorov's pair,\nin both jump and diffusion processes. Much later, in 1956, he referred to the equations for the jump process as \"Kolmogorov forward equations\" and \"Kolmogorov backward equations\".\n\nOther authors, such as Motoo Kimura referred to the diffusion (Fokker–Planck) equation as Kolmogorov forward equation, a name that has persisted.\n\n\nOne example from biology is given below:\n\nThis equation is applied to model population growth with birth. Where formula_2 is the population index, with reference the initial population, formula_3 is the birth rate, and finally formula_4, i.e. the probability of achieving a certain population size.\n\nThe analytical solution is:\n\nThis is a formula for the density formula_6 in terms of the preceding ones, i.e. formula_7.\n"}
{"id": "18102", "url": "https://en.wikipedia.org/wiki?curid=18102", "title": "Linear map", "text": "Linear map\n\nIn mathematics, a linear map (also called a linear mapping, linear transformation or, in some contexts, linear function) is a mapping between two modules (including vector spaces) that preserves (in the sense defined below) the operations of addition and scalar multiplication. \n\nAn important special case is when , in which case the map is called a linear operator, or an endomorphism of . Sometimes the term \"linear function\" has the same meaning as \"linear map\", while in analytic geometry it does not.\n\nA linear map always maps linear subspaces onto linear subspaces (possibly of a lower dimension); for instance it maps a plane through the origin to a plane, straight line or point. Linear maps can often be represented as matrices, and simple examples include rotation and reflection linear transformations.\n\nIn the language of abstract algebra, a linear map is a module homomorphism. In the language of category theory it is a morphism in the category of modules over a given ring.\n\nLet formula_1 and formula_2 be vector spaces over the same field formula_3 A function formula_4 is said to be a \"linear map\" if for any two vectors formula_5 and any scalar formula_6 the following two conditions are satisfied:\n\nThus, a linear map is said to be \"operation preserving\". In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.\n\nThis is equivalent to requiring the same for any linear combination of vectors, i.e. that for any vectors formula_7 and scalars formula_8 the following equality holds:\n\nDenoting the zero elements of the vector spaces formula_1 and formula_2 by formula_12 and formula_13 respectively, it follows that formula_14 Let formula_15 and formula_16 in the equation for homogeneity of degree 1:\n\nOccasionally, formula_1 and formula_2 can be considered to be vector spaces over different fields. It is then necessary to specify which of these ground fields is being used in the definition of \"linear\". If formula_1 and formula_2 are considered as spaces over the field formula_22 as above, we talk about formula_22-linear maps. For example, the conjugation of complex numbers is an formula_24-linear map formula_25, but it is not formula_26-linear.\n\nA linear map formula_27 with formula_22 viewed as a vector space over itself is called a linear functional.\n\nThese statements generalize to any left-module formula_29 over a ring formula_30 without modification, and to any right-module upon reversing of the scalar multiplication.\n\n\nIf \"V\" and \"W\" are finite-dimensional vector spaces and a basis is defined for each vector space, then every linear map from \"V\" to \"W\" can be represented by a matrix. This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if \"A\" is a real matrix, then describes a linear map (see Euclidean space).\n\nLet {v, …, v} be a basis for \"V\". Then every vector v in \"V\" is uniquely determined by the coefficients \"c\", …, \"c\" in the field R:\n\nIf is a linear map,\n\nwhich implies that the function \"f\" is entirely determined by the vectors \"f\"(v), …, \"f\"(v). Now let be a basis for \"W\". Then we can represent each vector \"f\"(v) as\n\nThus, the function \"f\" is entirely determined by the values of \"a\". If we put these values into an matrix \"M\", then we can conveniently use it to compute the vector output of \"f\" for any vector in \"V\". To get \"M\", every column \"j\" of \"M\" is a vector\n\ncorresponding to \"f\"(v) as defined above. To define it more clearly, for some column \"j\" that corresponds to the mapping \"f\"(v),\n\nwhere M is the matrix of \"f\". In other words, every column has a corresponding vector \"f\"(v) whose coordinates \"a\", …, \"a\" are the elements of column \"j\". A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.\n\nThe matrices of a linear transformation can be represented visually:\n\n\nSuch that starting in the bottom left corner formula_50 and looking for the bottom right corner formula_51, one would left-multiply—that is, formula_52. The equivalent method would be the \"longer\" method going clockwise from the same point such that formula_50 is left-multiplied with formula_54, or formula_55.\n\nIn two-dimensional space R linear maps are described by 2 × 2 real matrices. These are some examples:\n\n\nThe composition of linear maps is linear: if and are linear, then so is their composition . It follows from this that the class of all vector spaces over a given field \"K\", together with \"K\"-linear maps as morphisms, forms a category.\n\nThe inverse of a linear map, when defined, is again a linear map.\n\nIf and are linear, then so is their pointwise sum (which is defined by .\n\nIf is linear and \"a\" is an element of the ground field \"K\", then the map \"af\", defined by , is also linear.\n\nThus the set of linear maps from \"V\" to \"W\" itself forms a vector space over \"K\", sometimes denoted . Furthermore, in the case that , this vector space (denoted End(\"V\")) is an associative algebra under composition of maps, since the composition of two linear maps is again a linear map, and the composition of maps is always associative. This case is discussed in more detail below.\n\nGiven again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the matrix multiplication, the addition of linear maps corresponds to the matrix addition, and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.\n\nA linear transformation \"f\": \"V\" → \"V\" is an endomorphism of \"V\"; the set of all such endomorphisms End(\"V\") together with addition, composition and scalar multiplication as defined above forms an associative algebra with identity element over the field \"K\" (and in particular a ring). The multiplicative identity element of this algebra is the identity map id: \"V\" → \"V\".\n\nAn endomorphism of \"V\" that is also an isomorphism is called an automorphism of \"V\". The composition of two automorphisms is again an automorphism, and the set of all automorphisms of \"V\" forms a group, the automorphism group of \"V\" which is denoted by Aut(\"V\") or GL(\"V\"). Since the automorphisms are precisely those endomorphisms which possess inverses under composition, Aut(\"V\") is the group of units in the ring End(\"V\").\n\nIf \"V\" has finite dimension \"n\", then End(\"V\") is isomorphic to the associative algebra of all \"n\" × \"n\" matrices with entries in \"K\". The automorphism group of \"V\" is isomorphic to the general linear group GL(\"n\", \"K\") of all \"n\" × \"n\" invertible matrices with entries in \"K\".\n\nIf \"f\" : \"V\" → \"W\" is linear, we define the kernel and the image or range of \"f\" by\n\nker(\"f\") is a subspace of \"V\" and im(\"f\") is a subspace of \"W\". The following dimension formula is known as the rank–nullity theorem:\n\nThe number dim(im(\"f\")) is also called the \"rank of f\" and written as rank(\"f\"), or sometimes, ρ(\"f\"); the number dim(ker(\"f\")) is called the \"nullity of f\" and written as null(\"f\") or ν(\"f\"). If \"V\" and \"W\" are finite-dimensional, bases have been chosen and \"f\" is represented by the matrix \"A\", then the rank and nullity of \"f\" are equal to the rank and nullity of the matrix \"A\", respectively.\n\nA subtler invariant of a linear transformation formula_66 is the \"co\"kernel, which is defined as\n\nThis is the \"dual\" notion to the kernel: just as the kernel is a \"sub\"space of the \"domain,\" the co-kernel is a \"quotient\" space of the \"target.\"\nFormally, one has the exact sequence\n\nThese can be interpreted thus: given a linear equation \"f\"(v) = w to solve,\n\n\nThe dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space \"W\"/\"f\"(\"V\") is the dimension of the target space minus the dimension of the image.\n\nAs a simple example, consider the map \"f\": R → R, given by \"f\"(\"x\", \"y\") = (0, \"y\"). Then for an equation \"f\"(\"x\", \"y\") = (\"a\", \"b\") to have a solution, we must have \"a\" = 0 (one constraint), and in that case the solution space is (\"x\", \"b\") or equivalently stated, (0, \"b\") + (\"x\", 0), (one degree of freedom). The kernel may be expressed as the subspace (\"x\", 0) < \"V\": the value of \"x\" is the freedom in a solution – while the cokernel may be expressed via the map \"W\" → R, formula_69 given a vector (\"a\", \"b\"), the value of \"a\" is the \"obstruction\" to there being a solution.\n\nAn example illustrating the infinite-dimensional case is afforded by the map \"f\": R → R, formula_70 with \"b\" = 0 and \"b\" = \"a\" for \"n\" > 0. Its image consists of all sequences with first element 0, and thus its cokernel consists of the classes of sequences with identical first element. Thus, whereas its kernel has dimension 0 (it maps only the zero sequence to the zero sequence), its co-kernel has dimension 1. Since the domain and the target space are the same, the rank and the dimension of the kernel add up to the same sum as the rank and the dimension of the co-kernel ( formula_71 ), but in the infinite-dimensional case it cannot be inferred that the kernel and the co-kernel of an endomorphism have the same dimension (0 ≠ 1). The reverse situation obtains for the map \"h\": R → R, formula_72 with \"c\" = \"a\". Its image is the entire target space, and hence its co-kernel has dimension 0, but since it maps all sequences in which only the first element is non-zero to the zero sequence, its kernel has dimension 1.\n\nFor a linear operator with finite-dimensional kernel and co-kernel, one may define \"index\" as:\n\nnamely the degrees of freedom minus the number of constraints.\n\nFor a transformation between finite-dimensional vector spaces, this is just the difference dim(\"V\") − dim(\"W\"), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.\n\nThe index of an operator is precisely the Euler characteristic of the 2-term complex 0 → \"V\" → \"W\" → 0. In operator theory, the index of Fredholm operators is an object of study, with a major result being the Atiyah–Singer index theorem.\n\nNo classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.\n\nLet \"V\" and \"W\" denote vector spaces over a field, \"F\". Let \"T\": \"V\" → \"W\" be a linear map.\n\n\nGiven a linear map which is an endomorphism whose matrix is \"A\", in the basis \"B\" of the space it transforms vector coordinates [u] as [v] = \"A\"[u]. As vectors change with the inverse of \"B\" (vectors are contravariant) its inverse transformation is [v] = \"B\"[v'].\n\nSubstituting this in the first expression\n\nhence\n\nTherefore, the matrix in the new basis is \"A′\" = \"B\"\"AB\", being \"B\" the matrix of the given basis.\n\nTherefore, linear maps are said to be 1-co- 1-contra-variant objects, or type (1, 1) tensors.\n\nA \"linear transformation\" between topological vector spaces, for example normed spaces, may be continuous. If its domain and codomain are the same, it will then be a continuous linear operator. A linear operator on a normed linear space is continuous if and only if it is bounded, for example, when the domain is finite-dimensional. An infinite-dimensional domain may have discontinuous linear operators.\n\nAn example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0). For a specific example, sin(\"nx\")/\"n\" converges to 0, but its derivative cos(\"nx\") does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).\n\nA specific application of linear maps is for geometric transformations, such as those performed in computer graphics, where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a transformation matrix. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.\n\nAnother application of these transformations is in compiler optimizations of nested-loop code, and in parallelizing compiler techniques.\n\n\n"}
{"id": "40658915", "url": "https://en.wikipedia.org/wiki?curid=40658915", "title": "List of works by Nicolas Minorsky", "text": "List of works by Nicolas Minorsky\n\nList of works by Nicolas Minorsky.\n\n\n\n\n\n\n"}
{"id": "45209715", "url": "https://en.wikipedia.org/wiki?curid=45209715", "title": "Livingstone graph", "text": "Livingstone graph\n\nIn the mathematical field of graph theory, the Livingstone graph is a distance-transitive graph with 266 vertices and 1463 edges. It is the largest distance-transitive graph with degree 11.\n\nThe automorphism group of the Livingstone graph is the sporadic simple group J, and the stabiliser of a point is PSL(2,11). As the stabiliser is maximal in J, it acts primitively on the graph.\n\nAs the Livingstone graph is distance-transitive, PSL(2,11) acts transitively on the set of 11 vertices adjacent to a reference vertex \"v\", and also on the set of 12 vertices at distance 4 from \"v\". The second action is equivalent to the standard action of PSL(2,11) on the projective line over F; the first is equivalent to an exceptional action on 11 points, related to the Paley biplane.\n"}
{"id": "17950868", "url": "https://en.wikipedia.org/wiki?curid=17950868", "title": "Locally discrete collection", "text": "Locally discrete collection\n\nIn mathematics, particularly topology, collections of subsets are said to be locally discrete if they look like they have precisely one element from a local point of view. The study of locally discrete collections is worthwhile as Bing's metrization theorem shows.\n\nLet \"X\" be a topological space. A collection {G} of subsets of \"X\" is said to be locally discrete, if each point of the space has a neighbourhood intersecting at most one element of the collection. A collection of subsets of \"X\" is said to be countably locally discrete, if it is the countable union of locally discrete collections.\n\n1. Locally discrete collections are always locally finite. See the page on local finiteness.\n\n2. If a collection of subsets of a topological space X is locally discrete, it must satisfy the property that each point of the space belongs to at most one element of the collection. This means that only collections of pairwise disjoint sets can be locally discrete. \n\n3. A Hausdorff space cannot have a locally discrete basis unless it is itself discrete. The same property holds for a T space.\n\n4. The following is known as Bing's metrization theorem:\n\nA space \"X\" is metrizable iff it is regular and has a basis that is countably locally discrete.\n\n5. A countable collection of sets is necessarily countably locally discrete. Therefore, if X is a metrizable space with a countable basis, one implication of Bing's metrization theorem holds. In fact, Bing's metrization theorem is almost a corollary of the Nagata-Smirnov theorem.\n\n\n"}
{"id": "4258398", "url": "https://en.wikipedia.org/wiki?curid=4258398", "title": "Locally integrable function", "text": "Locally integrable function\n\nIn mathematics, a locally integrable function (sometimes also called locally summable function) is a function which is integrable (so its integral is finite) on every compact subset of its domain of definition. The importance of such functions lies in the fact that their function space is similar to spaces, but its members are not required to satisfy any growth restriction on their behavior at infinity: in other words, locally integrable functions can grow arbitrarily fast at infinity, but are still manageable in a way similar to ordinary integrable functions.\n\n. Let be an open set in the Euclidean space and be a Lebesgue measurable function. If on is such that\n\ni.e. its Lebesgue integral is finite on all compact subsets of , then   is called \"locally integrable\". The set of all such functions is denoted by :\n\nwhere denotes the restriction of   to the set .\n\nThe classical definition of a locally integrable function involves only measure theoretic and topological concepts and can be carried over abstract to complex-valued functions on a topological measure space : however, since the most common application of such functions is to distribution theory on Euclidean spaces, all the definitions in this and the following sections deal explicitly only with this important case.\n\n. Let be an open set in the Euclidean space . Then a function such that\n\nfor each test function is called \"locally integrable\", and the set of such functions is denoted by . Here denotes the set of all infinitely differentiable functions with compact support contained in .\n\nThis definition has its roots in the approach to measure and integration theory based on the concept of continuous linear functional on a topological vector space, developed by Nicolas Bourbaki and his school: it is also the one adopted by and by . This \"distribution theoretic\" definition is equivalent to the standard one, as the following lemma proves:\n\n. A given function is locally integrable according to if and only if it is locally integrable according to , i.e.\n\n. Let be an open set in the Euclidean space ℝ\"\" and ℂ be a Lebesgue measurable function. If, for a given with , satisfies\n\ni.e., it belongs to for all compact subsets of , then is called \"locally\" -\"integrable\" or also -\"locally integrable\". The set of all such functions is denoted by :\n\nAn alternative definition, completely analogous to the one given for locally integrable functions, can also be given for locally -integrable functions: it can also be and proven equivalent to the one in this section. Despite their apparent higher generality, locally -integrable functions form a subset of locally integrable functions for every such that .\n\nApart from the different glyphs which may be used for the uppercase \"L\", there are few variants for the notation of the set of locally integrable functions\n\n. is a complete metrizable space: its topology can be generated by the following metric:\nwhere is a family of non empty open sets such that\n\nIn references , , and , this theorem is stated but not proved on a formal basis: a complete proof of a more general result, which includes it, is found in .\n\n. Every function belonging to , , where is an open subset of ℝ\"\", is locally integrable.\n\nProof. The case is trivial, therefore in the sequel of the proof it is assumed that . Consider the characteristic function of a compact subset of : then, for ,\n\nwhere\nThen by Hölder's inequality, the product is integrable i.e. belongs to and\n\ntherefore\n\nNote that since the following inequality is true\n\nthe theorem is true also for functions belonging only to the space of locally -integrable functions, therefore the theorem implies also the following result.\n\n. Every function formula_17 in formula_18, formula_19.\n\nNote: If formula_20 is an open subset of formula_21 that is also bounded, then one has the standard inclusion formula_22 which makes sense given the above inclusion formula_23. But the first of these statements is not true if formula_20 is not bounded; then it is still true that formula_25 for any formula_26, but not that formula_27. To see this, one typically considers the function formula_28, which is in formula_29 but not in formula_30 for any finite formula_26.\n\n. A function is the density of an absolutely continuous measure if and only if formula_32.\n\nThe proof of this result is sketched by . Rephrasing its statement, this theorem asserts that every locally integrable function defines an absolutely continuous measure and conversely that every absolutely continuous measures defines a locally integrable function: this is also, in the abstract measure theory framework, the form of the important Radon–Nikodym theorem given by Stanisław Saks in his treatise.\n\n\nLocally integrable functions play a prominent role in distribution theory and they occur in the definition of various classes of functions and function spaces, like functions of bounded variation. Moreover, they appear in the Radon–Nikodym theorem by characterizing the absolutely continuous part of every measure.\n\n\n\n"}
{"id": "34233472", "url": "https://en.wikipedia.org/wiki?curid=34233472", "title": "Minimal ideal", "text": "Minimal ideal\n\nIn the branch of abstract algebra known as ring theory, a minimal right ideal of a ring \"R\" is a nonzero right ideal which contains no other nonzero right ideal. Likewise a minimal left ideal is a nonzero left ideal of \"R\" containing no other nonzero left ideals of \"R\", and a minimal ideal of \"R\" is a nonzero ideal containing no other nonzero two-sided ideal of \"R\". \n\nSaid another way, minimal right ideals are minimal elements of the poset of nonzero right ideals of \"R\" ordered by inclusion. The reader is cautioned that outside of this context, some posets of ideals may admit the zero ideal, and so zero could potentially be a minimal element in that poset. This is the case for the poset of prime ideals of a ring, which may include the zero ideal as a minimal prime ideal.\n\nThe definition of a minimal right ideal \"N\" of a module \"R\" is equivalent to the following conditions:\n\nMinimal right ideals are the dual notion to the idea of maximal right ideals.\n\nMany standard facts on minimal ideals can be found in standard texts such as , , , and .\n\n\nA nonzero submodule \"N\" of a right module \"M\" is called a minimal submodule if it contains no other nonzero submodules of \"M\". Equivalently, \"N\" is a nonzero submodule of \"M\" which is a simple module. This can also be extended to bimodules by calling a nonzero sub-bimodule \"N\" a minimal sub-bimodule of \"M\" if \"N\" contains no other nonzero sub-bimodules.\n\nIf the module \"M\" is taken to be the right \"R\" module \"R\", then clearly the minimal submodules are exactly the minimal right ideals of \"R\". Likewise, the minimal left ideals of \"R\" are precisely the minimal submodules of the left module \"R\". In the case of two-sided ideals, we see that the minimal ideals of \"R\" are exactly the minimal sub-bimodules of the bimodule \"R\".\n\nJust as with rings, there is no guarantee that minimal submodules exist in a module. Minimal submodules can be used to define the socle of a module.\n\n\n"}
{"id": "24087239", "url": "https://en.wikipedia.org/wiki?curid=24087239", "title": "Noncommutative unique factorization domain", "text": "Noncommutative unique factorization domain\n\nIn mathematics, the noncommutative unique factorization domain is the noncommutative counterpart of the commutative or classical unique factorization domain (UFD).\n\n\n"}
{"id": "27157933", "url": "https://en.wikipedia.org/wiki?curid=27157933", "title": "Nucleic acid secondary structure", "text": "Nucleic acid secondary structure\n\nNucleic acid secondary structure is the basepairing interactions within a single nucleic acid polymer or between two polymers. It can be represented as a list of bases which are paired in a nucleic acid molecule.\nThe secondary structures of biological DNA's and RNA's tend to be different: biological DNA mostly exists as fully base paired double helices, while biological RNA is single stranded and often forms complex and intricate base-pairing interactions due to its increased ability to form hydrogen bonds stemming from the extra hydroxyl group in the ribose sugar.\n\nIn a non-biological context, secondary structure is a vital consideration in the nucleic acid design of nucleic acid structures for DNA nanotechnology and DNA computing, since the pattern of basepairing ultimately determines the overall structure of the molecules.\n\nIn molecular biology, two nucleotides on opposite complementary DNA or RNA strands that are connected via hydrogen bonds are called a base pair (often abbreviated bp). In the canonical Watson-Crick base pairing, adenine (A) forms a base pair with thymine (T) and guanine (G) forms one with cytosine (C) in DNA. In RNA, thymine is replaced by uracil (U). Alternate hydrogen bonding patterns, such as the wobble base pair and Hoogsteen base pair, also occur—particularly in RNA—giving rise to complex and functional tertiary structures. Importantly, pairing is the mechanism by which codons on messenger RNA molecules are recognized by anticodons on transfer RNA during protein translation. Some DNA- or RNA-binding enzymes can recognize specific base pairing patterns that identify particular regulatory regions of genes.\nHydrogen bonding is the chemical mechanism that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the \"right\" pairs to form stably. DNA with high GC-content is more stable than DNA with low GC-content, but contrary to popular belief, the hydrogen bonds do not stabilize the DNA significantly and stabilization is mainly due to stacking interactions.\n\nThe larger nucleobases, adenine and guanine, are members of a class of doubly ringed chemical structures called purines; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of singly ringed chemical structures called pyrimidines. Purines are only complementary with pyrimidines: pyrimidine-pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine-purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. The only other possible pairings are GT and AC; these pairings are mismatches because the pattern of hydrogen donors and acceptors do not correspond. The GU wobble base pair, with two hydrogen bonds, does occur fairly often in RNA.\n\nHybridization is the process of complementary base pairs binding to form a double helix. Melting is the process by which the interactions between the strands of the double helix are broken, separating the two nucleic acid strands. These bonds are weak, easily separated by gentle heating, enzymes, or physical force. Melting occurs preferentially at certain points in the nucleic acid. T and A rich sequences are more easily melted than C and G rich regions. Particular base steps are also susceptible to DNA melting, particularly T A and T G base steps. These mechanical features are reflected by the use of sequences such as TATAA at the start of many genes to assist RNA polymerase in melting the DNA for transcription.\n\nStrand separation by gentle heating, as used in PCR, is simple providing the molecules have fewer than about 10,000 base pairs (10 kilobase pairs, or 10 kbp). The intertwining of the DNA strands makes long segments difficult to separate. The cell avoids this problem by allowing its DNA-melting enzymes (helicases) to work concurrently with topoisomerases, which can chemically cleave the phosphate backbone of one of the strands so that it can swivel around the other. Helicases unwind the strands to facilitate the advance of sequence-reading enzymes such as DNA polymerase.\n\nNucleic acid secondary structure is generally divided into helices (contiguous base pairs), and various kinds of loops (unpaired nucleotides surrounded by helices). Frequently these elements, or combinations of them, are further classified into additional categories including, for example, tetraloops, pseudoknots, and stem-loops.\n\nThe double helix is an important tertiary structure in nucleic acid molecules which is intimately connected with the molecule's secondary structure. A double helix is formed by regions of many consecutive base pairs.\n\nThe nucleic acid double helix is a spiral polymer, usually right-handed, containing two nucleotide strands which base pair together. A single turn of the helix constitutes about ten nucleotides, and contains a major groove and minor groove, the major groove being wider than the minor groove. Given the difference in widths of the major groove and minor groove, many proteins which bind to DNA do so through the wider major groove. Many double-helical forms are possible; for DNA the three biologically relevant forms are A-DNA, B-DNA, and Z-DNA, while RNA double helices have structures similar to the A form of DNA.\n\nThe secondary structure of nucleic acid molecules can often be uniquely decomposed into stems and loops. The stem-loop structure (also often referred to as an \"hairpin\"), in which a base-paired helix ends in a short unpaired loop, is extremely common and is a building block for larger structural motifs such as cloverleaf structures, which are four-helix junctions such as those found in transfer RNA. Internal loops (a short series of unpaired bases in a longer paired helix) and bulges (regions in which one strand of a helix has \"extra\" inserted bases with no counterparts in the opposite strand) are also frequent.\n\nThere are many secondary structure elements of functional importance to biological RNA's; some famous examples are the Rho-independent terminator stem-loops and the tRNA cloverleaf. Active research is on-going to determine the secondary structure of RNA molecules, with approaches including both experimental and computational methods (see also the List of RNA structure prediction software).\n\nA pseudoknot is a nucleic acid secondary structure containing at least two stem-loop structures in which half of one stem is intercalated between the two halves of another stem. Pseudoknots fold into knot-shaped three-dimensional conformations but are not true topological knots. The base pairing in pseudoknots is not well nested; that is, base pairs occur that \"overlap\" one another in sequence position. This makes the presence of general pseudoknots in nucleic acid sequences impossible to predict by the standard method of dynamic programming, which uses a recursive scoring system to identify paired stems and consequently cannot detect non-nested base pairs with common algorithms. However, limited subclasses of pseudoknots can be predicted using modified dynamic programs.\nNewer structure prediction techniques such as stochastic context-free grammars are also unable to consider pseudoknots.\n\nPseudoknots can form a variety of structures with catalytic activity and several important biological processes rely on RNA molecules that form pseudoknots. For example, the RNA component of the human telomerase contains a pseudoknot that is critical for its activity. The hepatitis delta virus ribozyme is a well known example of a catalytic RNA with a pseudoknot in its active site. Though DNA can also form pseudoknots, they are generally not present in standard physiological conditions.\n\nMost methods for nucleic acid secondary structure prediction rely on a nearest neighbor thermodynamic model. A common method to determine the most probable structures given a sequence of nucleotides makes use of a dynamic programming algorithm that seeks to find structures with low free energy. Dynamic programming algorithms often forbid pseudoknots, or other cases in which base pairs are not fully nested, as considering these structures becomes computationally very expensive for even small nucleic acid molecules. Other methods, such as stochastic context-free grammars can also be used to predict nucleic acid secondary structure.\n\nFor many RNA molecules, the secondary structure is highly important to the correct function of the RNA — often more so than the actual sequence. This fact aids in the analysis of non-coding RNA sometimes termed \"RNA genes\". One application of bioinformatics uses predicted RNA secondary structures in searching a genome for noncoding but functional forms of RNA. For example, microRNAs have canonical long stem-loop structures interrupted by small internal loops.\n\nRNA secondary structure applies in RNA splicing in certain species. In humans and other tetrapods, it has been shown that without the U2AF2 protein, the splicing process is inhibited. However, in zebrafish and other teleosts the RNA splicing process can still occur on certain genes in the absence of U2AF2. This may be because 10% of genes in zebrafish have alternating TG and AC base pairs at the 3' splice site (3'ss) and 5' splice site (5'ss) respectively on each intron, which alters the secondary structure of the RNA. This suggests that secondary structure of RNA can influence splicing, potentially without the use of proteins like U2AF2 that have been thought to be required for splicing to occur.\n\n\n"}
{"id": "17593652", "url": "https://en.wikipedia.org/wiki?curid=17593652", "title": "Optional stopping theorem", "text": "Optional stopping theorem\n\nIn probability theory, the optional stopping theorem (or Doob's optional sampling theorem) says that, under certain conditions, the expected value of a martingale at a stopping time is equal to its initial expected value. Since martingales can be used to model the wealth of a gambler participating in a fair game, the optional stopping theorem says that, on average, nothing can be gained by stopping play based on the information obtainable so far (i.e., without looking into the future). Certain conditions are necessary for this result to hold true. In particular, the theorem applies to doubling strategies.\n\nThe optional stopping theorem is an important tool of mathematical finance in the context of the fundamental theorem of asset pricing.\n\nA discrete-time version of the theorem is given below:\n\nLet be a discrete-time martingale and a stopping time with values in }, both with respect to a filtration . Assume that one of the following three conditions holds:\nThen is an almost surely well defined random variable and formula_3\n\nSimilarly, if the stochastic process is a submartingale or a supermartingale and one of the above conditions holds, then\nfor a submartingale, and\nfor a supermartingale.\n\nUnder condition () it is possible that happens with positive probability. On this event is defined as the almost surely existing pointwise limit of , see the proof below for details.\n\n\nLet denote the stopped process, it is also a martingale (or a submartingale or supermartingale, respectively). Under condition () or (), the random variable is well defined. Under condition () the stopped process is bounded, hence by Doob's martingale convergence theorem it converges a.s. pointwise to a random variable which we call .\n\nIf condition () holds, then the stopped process is bounded by the constant random variable . Otherwise, writing the stopped process as\n\ngives for all , where\n\nBy the monotone convergence theorem\n\nIf condition () holds, then this series only has a finite number of non-zero terms, hence is integrable.\n\nIf condition () holds, then we continue by inserting a conditional expectation and using that the event } is known at time (note that is assumed to be a stopping time with respect to the filtration), hence\n\nwhere a representation of the expected value of non-negative integer-valued random variables is used for the last equality.\n\nTherefore, under any one of the three conditions in the theorem, the stopped process is dominated by an integrable random variable . Since the stopped process converges almost surely to  , the dominated convergence theorem implies\n\nBy the martingale property of the stopped process,\n\nhence\n\nSimilarly, if is a submartingale or supermartingale, respectively, change the equality in the last two formulas to the appropriate inequality.\n\n\n"}
{"id": "31260972", "url": "https://en.wikipedia.org/wiki?curid=31260972", "title": "Otto Schilling", "text": "Otto Schilling\n\nOtto Franz Georg Schilling (3 November 1911 – 20 June 1973) was a German-American mathematician known as one of the leading algebraists of his time.\n\nHe was born in Apolda and studied in the 1930s with the Universität Jena and the Universität Göttingen under Emmy Noether. After Noether was forced to leave Germany by the Nazis, he found a new advisor in Helmut Hasse, and obtained his Ph.D. from Marburg University in 1934 on the thesis \"Über gewisse Beziehungen zwischen der Arithmetik hyperkomplexer Zahlsysteme und algebraischer Zahlkörper\". He then was post doc at Trinity College, Cambridge before moving to Institute for Advanced Study 1935–37 and the Johns Hopkins University 1937–39. He became an instructor with the University of Chicago in 1939, promoted to assistant professor 1943, associate 1945 and full professor in 1958. In 1961 he moved to Purdue University. He died in Highland Park, Illinois. His students were, among others, the game theorist Anatol Rapoport and the mathematician Harley Flanders.\n\n\n"}
{"id": "428111", "url": "https://en.wikipedia.org/wiki?curid=428111", "title": "Parabolic coordinates", "text": "Parabolic coordinates\n\nParabolic coordinates are a two-dimensional orthogonal coordinate system in which the coordinate lines are confocal parabolas. A three-dimensional version of parabolic coordinates is obtained by rotating the two-dimensional system about the symmetry axis of the parabolas. \n\nParabolic coordinates have found many applications, e.g., the treatment of the Stark effect and the potential theory of the edges.\n\nTwo-dimensional parabolic coordinates formula_1 are defined by the equations, in terms of cartesian coordinates:\n\nThe curves of constant formula_4 form confocal parabolae\n\nthat open upwards (i.e., towards formula_6), whereas the curves of constant formula_7 form confocal parabolae\n\nthat open downwards (i.e., towards formula_9). The foci of all these parabolae are located at the origin.\n\nThe scale factors for the parabolic coordinates formula_1 are equal\n\nHence, the infinitesimal element of area is\n\nand the Laplacian equals\n\nOther differential operators such as formula_14 \nand formula_15 can be expressed in the coordinates formula_1 by substituting \nthe scale factors into the general formulae \nfound in orthogonal coordinates.\n\nThe two-dimensional parabolic coordinates form the basis for two sets of three-dimensional orthogonal coordinates. The parabolic cylindrical coordinates are produced by projecting in the formula_17-direction.\nRotation about the symmetry axis of the parabolae produces a set of \nconfocal paraboloids, the coordinate system of tridimensional parabolic coordinates. Expressed in terms of cartesian coordinates:\n\nwhere the parabolae are now aligned with the formula_17-axis,\nabout which the rotation was carried out. Hence, the azimuthal angle formula_22 is defined\n\nThe surfaces of constant formula_4 form confocal paraboloids\n\nthat open upwards (i.e., towards formula_26) whereas the surfaces of constant formula_7 form confocal paraboloids \n\nthat open downwards (i.e., towards formula_29). The foci of all these paraboloids are located at the origin.\n\nThe Riemannian metric tensor associated with this coordinate system is\n\nThe three dimensional scale factors are:\n\nIt is seen that the scale factors formula_34 and formula_35 are the same as in the two-dimensional case. The infinitesimal volume element is then\n\nand the Laplacian is given by\n\nOther differential operators such as formula_14 \nand formula_15 can be expressed in the coordinates formula_40 by substituting \nthe scale factors into the general formulae \nfound in orthogonal coordinates.\n\n\n\n"}
{"id": "25056", "url": "https://en.wikipedia.org/wiki?curid=25056", "title": "Polish notation", "text": "Polish notation\n\nPolish notation (PN), also known as normal Polish notation (NPN), Łukasiewicz notation, Warsaw notation, Polish prefix notation or simply prefix notation, is a mathematical notation in which operators \"precede\" their operands, in contrast to (the more common) infix notation (in which operators are placed \"between\" operands), as well as to reverse Polish notation (RPN, in which operators \"follow\" their operands). It does not need any parentheses as long as each operator has a fixed number of operands. The description \"Polish\" refers to the nationality of logician Jan Łukasiewicz, who invented Polish notation in 1924.\n\nThe term \"Polish notation\" is sometimes taken (as the opposite of \"infix notation\") to also include reverse Polish notation.\n\nWhen Polish notation is used as a syntax for mathematical expressions by programming language interpreters, it is readily parsed into abstract syntax trees and can, in fact, define a one-to-one representation for the same. Because of this, Lisp (see below) and related programming languages define their entire syntax in terms of prefix notation (and others use postfix notation).\n\nA quotation from a paper by Jan Łukasiewicz, \"Remarks on Nicod's Axiom and on \"Generalizing Deduction\"\", page 180, states how the notation was invented:\nI came upon the idea of a parenthesis-free notation in 1924. I used that notation for the first time in my article Łukasiewicz(1), p. 610, footnote.\n\nThe reference cited by Łukasiewicz is apparently a lithographed report in Polish. The referring paper by Łukasiewicz \"Remarks on Nicod's Axiom and on \"Generalizing Deduction\"\" was reviewed by Henry A. Pogorzelski in the \"Journal of Symbolic Logic\" in 1965. Heinrich Behmann, editor in 1924 of the article of Moses Schönfinkel already had the idea of eliminating parentheses in logic formulas.\n\nAlonzo Church mentions this notation in his classic book on mathematical logic as worthy of remark in notational systems even contrasted to Alfred Whitehead and Bertrand Russell's logical notational exposition and work in Principia Mathematica.\n\nIn Łukasiewicz's 1951 book, \"Aristotle's Syllogistic from the Standpoint of Modern Formal Logic\", he mentions that the principle of his notation was to write the functors before the arguments to avoid brackets and that he had employed his notation in his logical papers since 1929. He then goes on to cite, as an example, a 1930 paper he wrote with Alfred Tarski on the sentential calculus.\n\nWhile no longer used much in logic, Polish notation has since found a place in computer science.\n\nThe expression for adding the numbers 1 and 2 is written in Polish notation as (pre-fix), rather than as (in-fix). In more complex expressions, the operators still precede their operands, but the operands may themselves be expressions including again operators and their operands. For instance, the expression that would be written in conventional infix notation as\ncan be written in Polish notation as\nAssuming a given arity of all involved operators (here the \"−\" denotes the binary operation of subtraction, not the unary function of sign-change), any well formed prefix representation thereof is unambiguous, and brackets within the prefix expression are unnecessary. As such, the above expression can be further simplified to\n\nThe processing of the product is deferred until its two operands are available (i.e., 5 minus 6, and 7). As with \"any\" notation, the innermost expressions are evaluated first, but in Polish notation this \"innermost-ness\" can be conveyed by the sequence of operators and operands rather than by bracketing.\n\nIn the conventional infix notation parentheses are required to override the standard precedence rules, since, referring to the above example, moving them\nor removing them\nchanges the meaning and the result of the expression. This version is written in Polish notation as\n\nWhen dealing with non-commutative operations, like division or subtraction, it is necessary to coordinate the sequential arrangement of the operands with the definition of how the operator takes its arguments, i.e., from left to right. For example, , with 10 left to 5, has the meaning of 10 ÷ 5 (read as \"divide 10 by 5\"), or , with 7 left to 6, has the meaning of 7 - 6 (read as \"subtract from 7 the operand 6\").\n\nPrefix/postfix notation is especially popular for its innate ability to express the intended order of operations without the need for parentheses and other precedence rules, as are usually employed with infix notation. Instead, the notation uniquely indicates which operator to evaluate first. The operators are assumed to have a fixed arity each, and all necessary operands are assumed to be explicitly given. A valid prefix expression always starts with an operator and ends with an operand. Evaluation can either proceed from left to right, or in the opposite direction. Starting at the left, the input string, consisting of tokens denoting operators or operands, is pushed token for token on a stack, until the top entries of the stack contain the number of operands that fits to the top most operator (immediately beneath). This group of tokens at the stacktop (the last stacked operator and the according number of operands) is replaced by the result of executing the operator on these/this operand(s). Then the processing of the input continues in this manner. The rightmost operand in a valid prefix expression thus empties the stack, except for the result of evaluating the whole expression. When starting at the right, the pushing of tokens is performed similarly, just the evaluation is triggered by an operator, finding the appropriate number of operands that fits its arity already at the stacktop. Now the leftmost token of a valid prefix expression must be an operator, fitting to the number of operands in the stack, which again yields the result. As can be seen from the description, a push-down store with no capability of arbitrary stack inspection suffices to implement this parsing.\n\nThe above sketched stack manipulation works –with mirrored input– also for expressions in reverse Polish notation.\n\nThe table below shows the core of Jan Łukasiewicz's notation for sentential logic. Some letters in the Polish notation table stand for particular words in Polish, as shown:\n\nNote that the quantifiers ranged over propositional values in Łukasiewicz's work on many-valued logics.\n\nBocheński introduced a system of Polish notation that names all 16 binary connectives of classical propositional logic. For classical propositional logic, it is a compatible extension of the notation of Łukasiewicz. But the notations are incompatible in the sense that Bocheński uses L and M (for nonimplication and converse nonimplication) in propositional logic and Łukasiewicz uses L and M in modal logic.\n\nPrefix notation has seen wide application in Lisp s-expressions, where the brackets are required since the operators in the language are themselves data (first-class functions). Lisp functions may also be variadic. The Tcl programming language, much like Lisp also uses Polish notation through the mathop library. The Ambi programming language uses Polish notation for arithmetic operations and program construction.\n\nPostfix notation is used in many stack-oriented programming languages like PostScript and Forth. CoffeeScript syntax also allows functions to be called using prefix notation, while still supporting the unary postfix syntax common in other languages.\n\nThe number of return values of an expression equals the difference between the number of operands in an expression and the total arity of the operators minus the total number of return values of the operators.\n\nPolish notation, usually in postfix form, is the chosen notation of certain calculators, notably from Hewlett-Packard. At a lower level, postfix operators are used by some stack machines such as the Burroughs large systems.\n\n\n"}
{"id": "4587226", "url": "https://en.wikipedia.org/wiki?curid=4587226", "title": "Product category", "text": "Product category\n\nIn the mathematical field of category theory, the product of two categories \"C\" and \"D\", denoted and called a product category, is an extension of the concept of the Cartesian product of two sets. Product categories are used to define bifunctors and multifunctors.\n\nThe product category has:\n\nFor small categories, this is the same as the action on objects of the categorical product in the category Cat. A functor whose domain is a product category is known as a bifunctor. An important example is the Hom functor, which has the product of the opposite of some category with the original category as domain:\n\nJust as the binary Cartesian product is readily generalized to an \"n\"-ary Cartesian product, binary product of two categories can be generalized, completely analogously, to a product of \"n\" categories. The product operation on categories is commutative and associative, up to isomorphism, and so this generalization brings nothing new from a theoretical point of view.\n\n"}
{"id": "2219011", "url": "https://en.wikipedia.org/wiki?curid=2219011", "title": "Proofs involving the addition of natural numbers", "text": "Proofs involving the addition of natural numbers\n\nThis article contains mathematical proofs for some properties of addition of the natural numbers: the additive identity, commutativity, and associativity. These proofs are used in the article Addition of natural numbers.\n\nThis article will use the Peano axioms for the definitions of addition of the natural numbers, and the successor function S(a). In particular:\n\nFor the proof of commutativity, it is useful to define another natural number closely related to the successor function, namely \"1\". We define 1 to be the successor of 0, in other words,\n\nNote that for all natural numbers \"a\",\n\nWe prove associativity by first fixing natural numbers \"a\" and \"b\" and applying induction on the natural number \"c\".\n\nFor the base case \"c\" = 0,\n\nEach equation follows by definition [A1]; the first with \"a\" + \"b\", the second with \"b\".\n\nNow, for the induction. We assume the induction hypothesis, namely we assume that for some natural number \"c\",\n\nThen it follows,\n\nIn other words, the induction hypothesis holds for \"S\"(\"c\"). Therefore, the induction on \"c\" is complete.\n\nDefinition [A1] states directly that 0 is a right identity.\nWe prove that 0 is a left identity by induction on the natural number \"a\".\n\nFor the base case \"a\" = 0, 0 + 0 = 0 by definition [A1].\nNow we assume the induction hypothesis, that 0 + \"a\" = \"a\".\nThen\n\nThis completes the induction on \"a\".\n\nWe prove commutativity (\"a\" + \"b\" = \"b\" + \"a\") by applying induction on the natural number \"b\". First we prove the base cases \"b\" = 0 and \"b\" = \"S\"(0) = 1 (i.e. we prove that 0 and 1 commute with everything).\n\nThe base case \"b\" = 0 follows immediately from the identity element property (0 is an additive identity), which has been proved above:\n\"a\" + 0 = \"a\" = 0 + \"a\".\n\nNext we will prove the base case \"b\" = 1, that 1 commutes with everything, i.e. for all natural numbers \"a\", we have \"a\" + 1 = 1 + \"a\". We will prove this by induction on \"a\" (an induction proof within an induction proof). We have proved that 0 commutes with everything, so in particular, 0 commutes with 1: for \"a\" = 0, we have 0 + 1 = 1 + 0. Now, suppose \"a\" + 1 = 1 + \"a\". Then\n\nThis completes the induction on \"a\", and so we have proved the base case \"b\" = 1. Now, suppose that for all natural numbers \"a\", we have \"a\" + \"b\" = \"b\" + \"a\". We must show that for all natural numbers \"a\", we have \"a\" + \"S\"(\"b\") = \"S\"(\"b\") + \"a\". We have\n\nThis completes the induction on \"b\".\n\n\n"}
{"id": "826997", "url": "https://en.wikipedia.org/wiki?curid=826997", "title": "Regression analysis", "text": "Regression analysis\n\nIn statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.\n\nMost commonly, regression analysis estimates the conditional expectation of the dependent variable given the independent variables – that is, the average value of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a quantile, or other location parameter of the conditional distribution of the dependent variable given the independent variables. In all cases, a function of the independent variables called the regression function is to be estimated. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the prediction of the regression function using a probability distribution. A related but distinct approach is Necessary Condition Analysis (NCA), which estimates the maximum (rather than average) value of the dependent variable for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is necessary but not sufficient for a given value of the dependent variable.\n\nRegression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer causal relationships between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable.\n\nMany techniques for carrying out regression analysis have been developed. Familiar methods such as linear regression and ordinary least squares regression are parametric, in that the regression function is defined in terms of a finite number of unknown parameters that are estimated from the data. Nonparametric regression refers to techniques that allow the regression function to lie in a specified set of functions, which may be infinite-dimensional.\n\nThe performance of regression analysis methods in practice depends on the form of the data generating process, and how it relates to the regression approach being used. Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small effects or questions of causality based on observational data, regression methods can give misleading results.\n\nIn a narrower sense, regression may refer specifically to the estimation of continuous response (dependent) variables, as opposed to the discrete response variables used in classification. The case of a continuous dependent variable may be more specifically referred to as \"metric regression\" to distinguish it from related problems.\n\nThe earliest form of regression was the method of least squares, which was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss–Markov theorem.\n\nThe term \"regression\" was coined by Francis Galton in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as regression toward the mean).\nFor Galton, regression had only this biological meaning, but his work was later extended by Udny Yule and Karl Pearson to a more general statistical context. In the work of Yule and Pearson, the joint distribution of the response and explanatory variables is assumed to be Gaussian. This assumption was weakened by R.A. Fisher in his works of 1922 and 1925. Fisher assumed that the conditional distribution of the response variable is Gaussian, but the joint distribution need not be. In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.\n\nIn the 1950s and 1960s, economists used electromechanical desk \"calculators\" to calculate regressions. Before 1970, it sometimes took up to 24 hours to receive the result from one regression.\n\nRegression methods continue to be an area of active research. In recent decades, new methods have been developed for robust regression, regression involving correlated responses such as time series and growth curves, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, nonparametric regression, Bayesian methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression.\n\nRegression models involve the following parameters and variables:\n\nIn various fields of application, different terminologies are used in place of dependent and independent variables.\n\nA regression model relates formula_3 to a function of formula_2 and formula_6.\n\nThe approximation is usually formalized as formula_8. To carry out regression analysis, the form of the function formula_9 must be specified. Sometimes the form of this function is based on knowledge about the relationship between formula_3 and formula_2 that does not rely on the data. If no such knowledge is available, a flexible or convenient form for formula_9 is chosen.\n\nAssume now that the vector of unknown parameters formula_1 is of length formula_14. In order to perform a regression analysis the user must provide information about the dependent variable formula_3:\n\nIn the last case, the regression analysis provides the tools for:\n\nConsider a regression model which has three unknown parameters, formula_35, formula_36, and formula_37. Suppose an experimenter performs 10 measurements all at exactly the same value of independent variable vector formula_2 (which contains the independent variables formula_39, formula_40, and formula_41). In this case, regression analysis fails to give a unique set of estimated values for the three unknown parameters; the experimenter did not provide enough information. The best one can do is to estimate the average value and the standard deviation of the dependent variable formula_3. Similarly, measuring at two different values of formula_2 would give enough data for a regression with two unknowns, but not for three or more unknowns.\n\nIf the experimenter had performed measurements at three different values of the independent variable vector formula_44, then regression analysis would provide a unique set of estimates for the three unknown parameters in formula_6.\n\nIn the case of general linear regression, the above statement is equivalent to the requirement that the matrix formula_46 is invertible.\n\nWhen the number of measurements, formula_16, is larger than the number of unknown parameters, formula_14, and the measurement errors formula_49 are normally distributed then \"the excess of information\" contained in formula_50 measurements is used to make statistical predictions about the unknown parameters. This excess of information is referred to as the degrees of freedom of the regression.\n\nClassical assumptions for regression analysis include:\nThese are sufficient conditions for the least-squares estimator to possess desirable properties; in particular, these assumptions imply that the parameter estimates will be unbiased, consistent, and efficient in the class of linear unbiased estimators. It is important to note that actual data rarely satisfies the assumptions. That is, the method is used even though the assumptions are not true. Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful. Many of these assumptions may be relaxed in more advanced treatments. Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model.\n\nIndependent and dependent variables often refer to values measured at point locations. There may be spatial trends and spatial autocorrelation in the variables that violate statistical assumptions of regression. Geographic weighted regression is one technique to deal with such data. Also, variables may include values aggregated by areas. With aggregated data the modifiable areal unit problem can cause extreme variation in regression parameters. When analyzing data aggregated by political boundaries, postal codes or census areas results may be very distinct with a different choice of units.\n\nIn linear regression, the model specification is that the dependent variable, formula_51 is a linear combination of the \"parameters\" (but need not be linear in the \"independent variables\"). For example, in simple linear regression for modeling formula_52 data points there is one independent variable: formula_53, and two parameters, formula_35 and formula_36:\n\nIn multiple linear regression, there are several independent variables or functions of independent variables.\n\nAdding a term in formula_57 to the preceding regression gives:\n\nThis is still linear regression; although the expression on the right hand side is quadratic in the independent variable formula_59, it is linear in the parameters formula_35, formula_36 and formula_62\n\nIn both cases, formula_63 is an error term and the subscript formula_64 indexes a particular observation.\n\nReturning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:\n\nThe residual, formula_66, is the difference between the value of the dependent variable predicted by the model, formula_67, and the true value of the dependent variable, formula_68. One method of estimation is ordinary least squares. This method obtains parameter estimates that minimize the sum of squared residuals, SSR:\n\nMinimization of this function results in a set of normal equations, a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, formula_70.\n\nIn the case of simple regression, the formulas for the least squares estimates are\n\nwhere formula_72 is the mean (average) of the formula_73 values and formula_74 is the mean of the formula_75 values.\n\nUnder the assumption that the population error term has a constant variance, the estimate of that variance is given by:\n\nThis is called the mean square error (MSE) of the regression. The denominator is the sample size reduced by the number of model parameters estimated from the same data, formula_77 for formula_78 regressors or formula_79 if an intercept is used. In this case, formula_80 so the denominator is formula_81.\n\nThe standard errors of the parameter estimates are given by\n\nUnder the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create confidence intervals and conduct hypothesis tests about the population parameters.\n\nIn the more general multiple regression model, there are formula_78 independent variables:\n\nwhere formula_86 is the formula_64-th observation on the formula_88-th independent variable.\nIf the first independent variable takes the value 1 for all formula_64, formula_90, then formula_36 is called the regression intercept.\n\nThe least squares parameter estimates are obtained from formula_78 normal equations. The residual can be written as\n\nThe normal equations are\n\nIn matrix notation, the normal equations are written as\n\nwhere the formula_96 element of formula_97 is formula_86, the formula_64 element of the column vector formula_3 is formula_68, and the formula_88 element of formula_103 is formula_104. Thus formula_97 is formula_106, formula_3 is formula_108, and formula_103 is formula_110. The solution is\n\nOnce a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.\n\nInterpretations of these diagnostic tests rest heavily on the model assumptions. Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.\n\nLimited dependent variables, which are response variables that are categorical variables or are variables constrained to fall only in a certain range, often arise in econometrics.\n\nThe response variable may be non-continuous (\"limited\" to lie on some subset of the real line). For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the linear probability model. Nonlinear models for binary dependent variables include the probit and logit model. The multivariate probit model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables. For categorical variables with more than two values there is the multinomial logit. For ordinal variables with more than two values, there are the ordered logit and ordered probit models. Censored regression models may be used when the dependent variable is only sometimes observed, and Heckman correction type models may be used when the sample is not randomly selected from the population of interest. An alternative to such procedures is linear regression based on polychoric correlation (or polyserial correlations) between the categorical variables. Such procedures differ in the assumptions made about the distribution of the variables in the population. If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the Poisson regression or the negative binomial model may be used.\n\nWhen the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. This introduces many complications which are summarized in Differences between linear and non-linear least squares.\n\nRegression models predict a value of the \"Y\" variable given known values of the \"X\" variables. Prediction \"within\" the range of values in the dataset used for model-fitting is known informally as interpolation. Prediction \"outside\" this range of the data is known as extrapolation. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.\n\nIt is generally advised that when performing extrapolation, one should accompany the estimated value of the dependent variable with a prediction interval that represents the uncertainty. Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.\n\nFor such reasons and others, some tend to say that it might be unwise to undertake extrapolation.\n\nHowever, this does not cover the full set of modeling errors that may be made: in particular, the assumption of a particular form for the relation between \"Y\" and \"X\". A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available. This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. Best-practice advice here is that a linear-in-variables and linear-in-parameters relationship should not be chosen simply for computational convenience, but that all available knowledge should be deployed in constructing a regression model. If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model – even if the observed dataset has no values particularly near such bounds. The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. At a minimum, it can ensure that any extrapolation arising from a fitted model is \"realistic\" (or in accord with what is known).\n\nThere are no generally agreed methods for relating the number of observations versus the number of independent variables in the model. One rule of thumb suggested by Good and Hardin is formula_112, where formula_16 is the sample size, formula_114 is the number of independent variables and formula_115 is the number of observations needed to reach the desired precision if the model had only one independent variable. For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (formula_16). If the researcher decides that five observations are needed to precisely define a straight line (formula_115), then the maximum number of independent variables the model can support is 4, because\n\nformula_118.\n\nAlthough the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:\n\nAll major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.\n\n\n"}
{"id": "12435391", "url": "https://en.wikipedia.org/wiki?curid=12435391", "title": "Residuated Boolean algebra", "text": "Residuated Boolean algebra\n\nIn mathematics, a residuated Boolean algebra is a residuated lattice whose lattice structure is that of a Boolean algebra. Examples include Boolean algebras with the monoid taken to be conjunction, the set of all formal languages over a given alphabet Σ under concatenation, the set of all binary relations on a given set \"X\" under relational composition, and more generally the power set of any equivalence relation, again under relational composition. The original application was to relation algebras as a finitely axiomatized generalization of the binary relation example, but there exist interesting examples of residuated Boolean algebras that are not relation algebras, such as the language example.\n\nA residuated Boolean algebra is an algebraic structure (\"L\", ∧, ∨, ¬, 0, 1, •, I, \\, /) such that\n\nAn equivalent signature better suited to the relation algebra application is (\"L\", ∧, ∨, ¬, 0, 1, •, I, ▷, ◁) where the unary operations \"x\"\\ and \"x\"▷ are intertranslatable in the manner of De Morgan's laws via\n\nwith the residuation axioms in the residuated lattice article reorganized accordingly (replacing \"z\" by ¬\"z\") to read\n\nThis De Morgan dual reformulation is motivated and discussed in more detail in the section below on conjugacy.\n\nSince residuated lattices and Boolean algebras are each definable with finitely many equations, so are residuated Boolean algebras, whence they form a finitely axiomatizable variety.\n\n\nThe De Morgan duals ▷ and ◁ of residuation arise as follows. Among residuated lattices, Boolean algebras are special by virtue of having a complementation operation ¬. This permits an alternative expression of the three inequalities\n\nin the axiomatization of the two residuals in terms of disjointness, via the equivalence \"x\" ≤ \"y\" ⇔ \"x\"∧¬\"y\" = 0. Abbreviating \"x\"∧\"y\" = 0 to \"x\" # \"y\" as the expression of their disjointness, and substituting ¬\"z\" for \"z\" in the axioms, they become with a little Boolean manipulation\n\nNow ¬(\"x\"\\¬\"z\") is reminiscent of De Morgan duality, suggesting that \"x\"\\ be thought of as a unary operation \"f\", defined by \"f\"(y) = \"x\"\\\"y\", that has a De Morgan dual ¬\"f\"(¬\"y\"), analogous to ∀\"x\"φ(\"x\") = ¬∃\"x\"¬φ(\"x\"). Denoting this dual operation as \"x\"▷, we define \"x\"▷\"z\" as ¬(\"x\"\\¬\"z\"). Similarly we define another operation \"z\"◁\"y\" as ¬(¬\"z\"/\"y\"). By analogy with \"x\"\\ as the residual operation associated with the operation \"x\"•, we refer to \"x\"▷ as the conjugate operation, or simply conjugate, of \"x\"•. Likewise ◁\"y\" is the conjugate of •\"y\". Unlike residuals, conjugacy is an equivalence relation between operations: if \"f\" is the conjugate of \"g\" then \"g\" is also the conjugate of \"f\", i.e. the conjugate of the conjugate of \"f\" is \"f\". Another advantage of conjugacy is that it becomes unnecessary to speak of right and left conjugates, that distinction now being inherited from the difference between \"x\"• and •\"x\", which have as their respective conjugates \"x\"▷ and ◁\"x\". (But this advantage accrues also to residuals when \"x\"\\ is taken to be the residual operation to \"x\"•.)\n\nAll this yields (along with the Boolean algebra and monoid axioms) the following equivalent axiomatization of a residuated Boolean algebra.\n\nWith this signature it remains the case that this axiomatization can be expressed as finitely many equations.\n\nIn examples 2 and 3 it can be shown that \"x\"▷I = I◁\"x\". In example 2 both sides equal the converse \"x\"˘ of \"x\", while in example 3 both sides are I when \"x\" contains the empty word and 0 otherwise. In the former case \"x\"˘ = \"x\". This is impossible for the latter because \"x\"▷I retains hardly any information about \"x\". Hence in example 2 we can substitute \"x\"˘ for \"x\" in \"x\"▷I = \"x\"˘ = I◁\"x\" and cancel (soundly) to give\n\n\"x\"˘ = \"x\" can be proved from these two equations. Tarski's notion of a relation algebra can be defined as a residuated Boolean algebra having an operation \"x\"˘ satisfying these two equations.\n\nThe cancellation step in the above is not possible for example 3, which therefore is not a relation algebra, \"x\"˘ being uniquely determined as \"x\"▷I.\n\nConsequences of this axiomatization of converse include \"x\"˘ = \"x\", ¬(\"x\"˘) = (¬\"x\")˘, (\"x\"∨\"y\")˘ = \"x\"˘∨\"y\"˘, and (\"x\"•\"y\")˘ = \"y\"˘•\"x\"˘.\n\n"}
{"id": "35068788", "url": "https://en.wikipedia.org/wiki?curid=35068788", "title": "Reversed compound agent theorem", "text": "Reversed compound agent theorem\n\nIn probability theory, the reversed compound agent theorem (RCAT) is a set of sufficient conditions for a stochastic process expressed in the PEPA language to have a product form stationary distribution (assuming that the process is stationary). The theorem shows that product form solutions in Jackson's theorem, the BCMP theorem and G-networks are based on the same fundamental mechanisms.\n\nThe theorem identifies a reversed process using Kelly's lemma, from which the stationary distribution can be computed.\n\n"}
{"id": "1525933", "url": "https://en.wikipedia.org/wiki?curid=1525933", "title": "Rodrigues' rotation formula", "text": "Rodrigues' rotation formula\n\nIn the theory of three-dimensional rotation, Rodrigues' rotation formula, named after Olinde Rodrigues, is an efficient algorithm for rotating a vector in space, given an axis and angle of rotation. By extension, this can be used to transform all three basis vectors to compute a rotation matrix in , the group of all rotation matrices, from an axis–angle representation. In other words, the Rodrigues' formula provides an algorithm to compute the exponential map from , the Lie algebra of , to without actually computing the full matrix exponential.\n\nIf is a vector in and is a unit vector describing an axis of rotation about which rotates by an angle according to the right hand rule, the Rodrigues formula for the rotated vector is\nAn alternative statement is to write the axis vector as a cross product of any two nonzero vectors and which define the plane of rotation, and the sense of the angle is measured away from and towards . Letting denote the angle between these vectors, the two angles and are not necessarily equal, but they are measured in the same sense. Then the unit axis vector can be written\n\nThis form may be more useful when two vectors defining a plane are involved. An example in physics is the Thomas precession which includes the rotation given by Rodrigues' formula, in terms of two non-collinear boost velocities, and the axis of rotation is perpendicular to their plane.\n\nLet be a unit vector defining a rotation axis, and let be any vector to rotate about by angle (right hand rule, anticlockwise in the figure).\n\nUsing the dot and cross products, the vector can be decomposed into components parallel and perpendicular to the axis ,\n\nwhere the component parallel to is\n\ncalled the vector projection of on , and the component perpendicular to is\n\ncalled the vector rejection of from . \n\nThe vector can be viewed as a copy of rotated anticlockwise by 90° about , so their magnitudes are equal but directions are perpendicular. Likewise the vector a copy of rotated anticlockwise through about , so that and are equal in magnitude but in opposite directions (i.e. they are negatives of each other, hence the minus sign). Expanding the vector triple product establishes the connection between the parallel and perpendicular components, for reference the formula is given any three vectors , , .\n\nThe component parallel to the axis will not change magnitude nor direction under the rotation,\n\nonly the perpendicular component will change direction but retain its magnitude, according to \n\nand since and are parallel, their cross product is zero , so that\n\nand it follows\n\nThis rotation is correct since the vectors and have the same length, and is rotated anticlockwise through about . An appropriate scaling of and using the trigonometric functions sine and cosine gives the rotated perpendicular component. The form of the rotated component is similar to the radial vector in 2D planar polar coordinates in the Cartesian basis \n\nwhere , are unit vectors in their indicated directions.\n\nNow the full rotated vector is\n\nBy substituting the definitions of and in the equation results in\n\nRepresenting and as column matrices, the cross product can be expressed as a matrix product\n\nLetting denote the \"cross-product matrix\" for the unit vector ,\nthe matrix equation is, symbolically,\nfor any vector . (In fact, is the unique matrix with this property. It has eigenvalues 0 and ). \n\nIterating the cross product on the right is equivalent to multiplying by the cross product matrix on the left, in particular\n\nMoreover, since is a unit vector, has unit 2-norm. The previous rotation formula in matrix language is therefore\n\nNote the coefficient of the leading term is \"now\" 1, in this notation.\n\nFactorizing the allows the compact expression\nwhere \nis the rotation matrix through an angle counterclockwise about the axis , and the identity matrix. This matrix is an element of the rotation group of , and is an element of the Lie algebra generating that Lie group (note that is skew-symmetric, which characterizes ). In terms of the matrix exponential, \nTo see that the last identity holds, one notes that\ncharacteristic of a one-parameter subgroup, i.e. exponential, and that the formulas match for infinitesimal .\n\nFor an alternative derivation based on this exponential relationship, see exponential map from to. For the inverse mapping, see log map from to.\n\n\n\n"}
{"id": "21666983", "url": "https://en.wikipedia.org/wiki?curid=21666983", "title": "Synchronous frame", "text": "Synchronous frame\n\nA reference frame in which the time coordinate defines proper time for all co-moving observers is called \"synchronous\". It is built by choosing some time-like hypersurface as an origin, such that has in every point a normal along the time line (lies inside the light cone with an apex in that point); all interval elements on this hypersurface are space-like. A family of geodesics normal to this hypersurface are drawn and defined as the time coordinates with a beginning at the hypersurface.\n\nSuch a construct, and hence, choice of synchronous frame, is always possible though it is not unique. It allows any transformation of space coordinates that does not depend on time and, additionally, a transformation brought about by the arbitrary choice of hypersurface used for this geometric construct.\n\nSynchronization of clocks located at different space points means that events happening at different places can be measured as simultaneous if those clocks show the same times. In the special relativity theory, the space distance element \"dl\" is defined as the intervals between two very close events that occur at the same moment of time. In the general relativity theory this cannot be done, that is, one cannot define \"dl\" by just substituting \"dx\" = 0 in the metric. The reason for this is the different dependence between proper time and time coordinate \"x\" in different points of space.\n\nTo find \"dl\" in this case, one can first synchronize time over the whole space in the following way (Fig. 1): Bob sends a light signal from some space point \"B\" with coordinates \"x\" + \"dx\" to Alice who is at a very close point \"A\" with coordinates \"x\" and then Alice immediately reflects the signal back to Bob. The time necessary for this operation (measured by Bob), multiplied by \"c\" is, obviously, the doubled distance between Alice and Bob.\n\nThe squared interval, with separated space and time coordinates, is:\n\nwhere a repeated Greek index within a term means summation by values 1, 2, 3. The interval between the events of signal arrival in point \"A\" and its immediate reflection back is zero (two events in the same time at the same point). Equation \"ds\" = 0 solved for \"dx\" gives two roots:\nwhich correspond to the propagation of the signal in both directions between Alice and Bob. If \"x\" is the moment of arrival/reflection of the signal to/from Alice, the moments of signal departure from Bob and its arrival back to Bob correspond, respectively, to \"x\" + \"dx\" and \"x\" + \"dx\". The thick lines on Fig. 1 are the world lines of Alice and Bob with coordinates \"x\" and \"x\" + \"dx\", respectively, while the red lines are the world lines of the signals. Fig. 1 supposes that \"dx\" is positive and \"dx\" is negative, which, however, is not necessarily the case: \"dx\" and \"dx\" may have the same sign. The fact that in the latter case the value \"x\" (Alice) in the moment of signal arrival at Alice's position may be less than the value \"x\" (Bob) in the moment of signal departure from Bob does not contain a contradiction because clocks in different points of space are not supposed to be synchronized. It is clear that the full \"time\" interval between departure and arrival of the signal in Bob's place is\n\nThe respective proper time interval is obtained from the above relationship by multiplication by formula_3, and the distance \"dl\" between the two points – by additional multiplication by \"c\"/2. As a result:\n\nThis is the required relationship that defines distance through the space coordinate elements.\n\nIt is obvious that such synchronization should be done by exchange of light signals between points. Consider again propagation of signals between infinitesimally close points \"A\" and \"B\" in Fig. 1. The clock reading in \"B\" which is simultaneous with the moment \"x\" in \"A\" lies in the middle between the moments of sending and receiving the signal in \"B\"; this is the moment\nSubstitute here to find the difference in \"time\" \"x\" between two simultaneous events occurring in infinitesimally close points as\nThis relationship allows clock synchronization in any infinitesimally small space volume. By continuing such synchronization further from point \"A\", one can synchronize clocks, that is, determine simultaneity of events along any open line. The synchronization condition can be written in another form by multiplying by \"g\" and bringing terms to the left hand side \n\nor, the \"covariant differential\" \"dx\" between two infinitesimally close points should be zero.\n\nHowever, it is impossible, in general, to synchronize clocks along a closed contour: starting out along the contour and returning to the starting point one would obtain a Δ\"x\" value different from zero. Thus, unambiguous synchronization of clocks over the whole space is impossible. An exception are reference frames in which all components \"g\" are zeros.\n\nNote the inability to synchronize all clocks is a property of the reference frame and not of the spacetime itself. It is always possible in infinitely many ways in any gravitational field to choose the reference frame so that the three \"g\" become zeros and thus enable a complete synchronization of clocks. To this class are assigned cases where \"g\" can be made zeros by a simple change in the time coordinate which does not involve a choice of a system of objects that define the space coordinates.\n\nIn the special relativity theory, too, proper time elapses differently for clocks moving relatively to each other. In general relativity, proper time is different even in the same reference frame at different points of space. This means that the interval of proper time between two events occurring at some space point and the time interval between the events simultaneous with those at another space point are, in general, different from one another.\n\nRewrite in the form\n\nwhere\n\nis the three-dimensional metric tensor that determines the metric, that is, the geometrical properties of space. Equations give the relationships between the metric of the three-dimensional space and the metric of the four-dimensional spacetime.\n\nIn general, however, the metric \"g\" depends on \"x\" so that the space metric changes with time. Therefore, it doesn't make sense to integrate \"dl\": this integral depends on the choice of world line between the two points on which it is taken. It follows that in general relativity the distance between two bodies cannot be determined in general; this distance is determined only for infinitesimally close points. Distance can be determined also for finite space regions only in such reference frames in which \"g\" does not depend on time and therefore the integral ∫\"dl\" along the space curve acquires some definite sense.\n\nThe tensor –γ is inverse to the contravariant 3-dimensional tensor \"g\". Indeed, writing equation \"gg\" = formula_5 in components, one has:\n\nDetermine \"g\" from the second equation and substitute in the first to obtain\n\nwhich was to be demonstrated. This result can be presented otherwise by saying that \"g\" are components of a contravariant 3-dimensional tensor corresponding to metric :\n\nThe determinants \"g\" and γ composed of elements \"g\" and γ, respectively, are related to each other by the simple relationship:\n\nIn many applications, it is convenient to define a 3-dimensional vector g with covariant components\n\nConsidering g as a vector in space with metric , its contravariant components can be written as \"g\" = γ\"g\". Using and the second of , it is easy to see that\n\nFrom the third of , it follows\n\nAs concluded from , the condition that allows clock synchronization in different space points is that metric tensor components \"g\" are zeros. If, in addition, \"g\" = 1, then the time coordinate \"x\" = \"t\" is the proper time in each space point (with \"c\" = 1). A reference frame that satisfies the conditions\n\nis called \"synchronous frame\". The interval element in this system is given by the expression\n\nwith the spatial metric tensor components identical (with opposite sign) to the components \"g\":\nIn synchronous frame time, time lines are normal to the hypersurfaces \"t\" = const. Indeed, the unit four-vector normal to such a hypersurface \"n\" = ∂\"t\"/∂\"x\" has covariant components \"n\" = 0, \"n\" = 1. The respective contravariant components with the conditions are again \"n\" = 0, \"n\" = 1.\n\nThe components of the unit normal coincide with those of the four-vector \"u\" = \"dx/ds\" which is tangent to the world line \"x\", \"x\", \"x\" = const. The \"u\" with components \"u\" = 0, \"u\" = 1 automatically satisfies the geodesic equations:\nsince, from the conditions , the Christoffel symbols formula_9 and formula_10 vanish identically. Therefore, in the synchronous reference system the time lines are geodesics in the spacetime.\n\nThese properties can be used to construct synchronous frame in any spacetime (Fig. 2). To this end, choose some spacelike hypersurface as an origin, such that has in every point a normal along the time line (lies inside the light cone with an apex in that point); all interval elements on this hypersurface are space-like. Then draw a family of geodesics normal to this hypersurface. Choose these lines as time coordinate lines and define the time coordinate \"t\" as the length \"s\" of the geodesic measured with a beginning at the hypersurface; the result is a synchronous frame.\n\nAn analytic transformation to synchronous frame can be done with the use of the Hamilton–Jacobi equation. The principle of this method is based on the fact that particle trajectories in gravitational fields are geodesics. The Hamilton–Jacobi equation for a particle (whose mass is set equal to unity) in a gravitational field is\n\nwhere \"S\" is the action. Its complete integral has the form:\n\nwhere \"f\" is a function of the four coordinates \"x\" and the three parameters ξ; the constant \"A\" is treated as an arbitrary function of the three ξ. With such a representation for \"S\" the equations for the trajectory of the particle can be obtained by equating the derivatives ∂\"S\"/∂ξ to zero, i.e. \nFor each set of assigned values of the parameters ξ, the right sides of equations have definite constant values, and the world line determined by these equations is one of the possible trajectories of the particle. Choosing the quantities ξ, which are constant along the trajectory, as new space coordinates, and the quantity \"S\" as the new time coordinate, one obtains a synchronous reference system; the transformation from the old coordinates to the new ones is given by equations . In fact, it is guaranteed that for such a transformation the time lines will be geodesics and will be normal to the hypersurfaces \"S\" = const. The latter point is obvious from the mechanical analogy: the four-vector ∂\"S\"/∂\"x\" which is normal to the hypersurface coincides in mechanics with the four-momentum of the particle, and therefore coincides in direction with its four-velocity \"u\" i.e. with the four-vector tangent to the trajectory. Finally the condition \"g\" = 1 is obviously satisfied, since the derivative −\"dS\"/\"ds\" of the action along the trajectory is the mass of the particle, which was set equal to 1; therefore |\"dS\"/\"ds\"| = 1.\n\nThe gauge conditions do not fix the coordinate system completely and therefore are not a fixed gauge, as the spacelike hypersurface at formula_11 can be chosen arbitrarily. One still have the freedom of performing some coordinate transformations containing four arbitrary functions depending on the three spatial variables \"x\", which are easily worked out in infinitesimal form:\n\nHere, the collections of the four old coordinates (\"t\", \"x\") and four new coordinates formula_12 are denoted by the symbols \"x\" and formula_13, respectively. The functions formula_14 together with their first derivatives are infinitesimally small quantities. After such a transformation, the four-dimensional interval takes the form:\n\nwhere\n\nIn the last formula, the formula_15 are the same functions \"g\"(\"x\") in which \"x\" should simply be replaced by formula_13. If one wishes to preserve the gauge also for the new metric tensor formula_17 in the new coordinates formula_13, it is necessary to impose the following restrictions on the functions formula_19:\nThe solutions of these equations are:\nwhere \"f\" and \"f\" are four arbitrary functions depending only on the spatial coordinates formula_20.\n\nFor a more elementary geometrical explanation, consider Fig. 2. First, the synchronous time line ξ = \"t\" can be chosen arbitrarily (Bob's, Carol's, Dana's or any of an infinitely many observers). This makes one arbitrarily chosen function: formula_21. Second, the initial hypersurface can be chosen in infinitely many ways. Each of these choices changes three functions: one function for each of the three spatial coordinates formula_22. Altogether, four (= 1 + 3) functions are arbitrary.\n\nWhen discussing general solutions \"g\" of the field equations in synchronous gauges, it is necessary to keep in mind that the gravitational potentials \"g\" contain, among all possible arbitrary functional parameters present in them, four arbitrary functions of 3-space just representing the gauge freedom and therefore of no direct physical significance.\n\nAnother problem with the reference system is that caustics can occur which cause the gauge choice to break down. These problems have caused some difficulties doing cosmological perturbation theory in this system, but the problems are now well understood. Synchronous coordinates are generally considered the most efficient reference system for doing calculations, and are used in many modern cosmology codes, such as CMBFAST. They are also useful for solving theoretical problems in which a spacelike hypersurface needs to be fixed, as with spacelike singularities.\n\nIntroduction of a synchronous frame allows one to separate the operations of space and time differentiation in the Einstein field equations. To make them more concise, the notation\nis introduced for the time derivatives of the three-dimensional metric tensor; these quantities also form a three-dimensional tensor. In the synchronous frame formula_23 is proportional to the second fundamental form (shape tensor). All operations of shifting indices and covariant differentiation of the tensor formula_23 are done in three-dimensional space with the metric γ. This does not apply to operations of shifting indices in the space components of the four-tensors \"R\", \"T\". Thus \"T\" must be understood to be \"g\"\"T\" + \"g\"\"T\", which reduces to \"g\"\"T\" and differs in sign from γ\"T\". The sum formula_25 is the logarithmic derivative of the determinant γ ≡ |γ| = − \"g\":\nThen for the complete set of Christoffel symbols formula_26 one obtains:\n\nwhere formula_27 are the three-dimensional Christoffel symbols constructed from γ:\n\nwhere the comma denotes partial derivative by the respective coordinate.\n\nWith the Christoffel symbols , the components \"R\" = \"gR\" of the Ricci tensor can be written in the form:\nDots on top denote time differentiation, semicolons (\";\") denote covariant differentiation which in this case is performed with respect to the three-dimensional metric γ with three-dimensional Christoffel symbols formula_27, formula_29, and \"P\" is a three-dimensional Ricci tensor constructed from formula_27:\n\nIt follows from that the Einstein equations formula_31 (with the components of the energy-momentum tensor \"T\" = −\"T\", \"T\" = −\"T\", \"T\" = γ\"T\") become in a synchronous coordinate system:\nA characteristic feature of synchronous reference systems is that they are not stationary: the gravitational field cannot be constant in such a system. In a constant field formula_23 would become zero. But in the presence of matter the disappearance of all formula_23 would contradict (which has a right side different from zero). In empty space from follows that all \"P\", and with them all the components of the three-dimensional curvature tensor \"P\" (Riemann tensor) vanish, i.e. the field vanishes entirely (in a synchronous system with a Euclidean spatial metric the space-time is flat).\n\nAt the same time the matter filling the space cannot in general be at rest relative to the synchronous reference frame. This is obvious from the fact that particles of matter within which there are pressures generally move along lines that are not geodesics; the world line of a particle at rest is a time line, and thus is a geodesic in the synchronous reference system. An exception is the case of dust (\"p\" = 0). Here the particles interacting with one another will move along geodesic lines; consequently, in this case the condition for a synchronous reference system does not contradict the condition that it be comoving with the matter. Even in this case, in order to be able to choose a synchronously comoving system of reference, it is still necessary that the matter move without rotation. In the comoving system the contravariant components of the velocity are \"u\" = 1, \"u\" = 0. If the reference system is also synchronous, the covariant components must satisfy \"u\" = 1, \"u\" = 0, so that its four-dimensional curl must vanish:\nBut this tensor equation must then also be valid in any other reference frame. Thus, in a synchronous but not comoving system the condition curl v = 0 for the three-dimensional velocity v is additionally needed. For other equations of state a similar situation can occur only in special cases when the pressure gradient vanishes in all or in certain directions.\n\n\n"}
{"id": "12480332", "url": "https://en.wikipedia.org/wiki?curid=12480332", "title": "Van Wijngaarden transformation", "text": "Van Wijngaarden transformation\n\nIn mathematics and numerical analysis, in order to accelerate convergence of an alternating series, Euler's transform can be computed as follows.\n\nCompute a row of partial sums :\nand form rows of averages between neighbors, \nThe first column formula_3 then contains the partial sums of the Euler transform.\n\nAdriaan van Wijngaarden's contribution was to point out that it is better not to carry this procedure through to the very end, but to stop two-thirds of the way. If formula_4 are available, then formula_5 is almost always a better approximation to the sum than formula_6\n\nLeibniz formula for pi, formula_7, gives the partial sum formula_8, the Euler transform partial sum formula_9 and the van Wijngaarden result formula_10 (relative errors are in round brackets).\nThis table results from the J formula 'b11.8'8!:2-:&(}:+}.)^:n+/\\(_1^n)*%1+2*n=.i.13 In many cases the diagonal terms do not converge in one cycle so process of averaging is to be repeated with diagonal terms by bringing them in a row. This will be needed in a geometric series with ratio -4. This process of successive averaging of the average of partial sum can be replaced by using formula to calculate the diagonal term.\n\nEuler summation\n"}
{"id": "655334", "url": "https://en.wikipedia.org/wiki?curid=655334", "title": "Weak interpretability", "text": "Weak interpretability\n\nIn mathematical logic, weak interpretability is a notion of translation of logical theories, introduced together with interpretability by Alfred Tarski in 1953.\n\nAssume T and S are formal theories. Slightly simplified, T is said to be weakly interpretable in S if, and only if, the language of T can be translated into the language of S in such a way that the translation of every theorem of T is consistent with S. Of course, there are some natural conditions on admissible translations here, such as the necessity for a translation to preserve the logical structure of formulas.\n\nA generalization of weak interpretability, tolerance, was introduced by Giorgi Japaridze in 1992. \n\n\n"}
