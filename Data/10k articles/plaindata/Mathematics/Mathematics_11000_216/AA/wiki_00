{"id": "454987", "url": "https://en.wikipedia.org/wiki?curid=454987", "title": "180 (number)", "text": "180 (number)\n\n180 (one hundred [and] eighty) is the natural number following 179 and preceding 181.\n\n180 is an abundant number, with its proper divisors summing up to 366. 180 is also a highly composite number, a positive integer with more divisors than any smaller positive integer. One of the consequences of 180 having so many divisors is that it is a practical number, meaning that any positive number smaller than 180 that is not a divisor of 180 can be expressed as the sum of some of 180's divisors. 180 is a refactorable number.\n\n180 is the sum of two square numbers: 12 + 6. It can be expressed as either the sum of six consecutive prime numbers: 19 + 23 + 29 + 31 + 37 + 41, or the sum of eight consecutive prime numbers: 11 + 13 + 17 + 19 + 23 + 29 + 31 + 37. 180 is an Ulam number, which can be expressed as a sum of earlier terms in the Ulam sequence only as 177 + 3.\n\n180 is a 61-gonal number.\n\nHalf a circle has 180 degrees.\n\nSumming Euler's totient function φ(\"x\") over the first + 24 integers gives 180.\n\n180 is a Harshad number in base 10, and in binary it is a digitally balanced number, since its binary representation has the same number of zeros as ones (10110100).\n\nThe Book of Genesis says that Isaac died at the age of 180.\n\n\n\n"}
{"id": "9036441", "url": "https://en.wikipedia.org/wiki?curid=9036441", "title": "Anatoly Vershik", "text": "Anatoly Vershik\n\nAnatoly Moiseevich Vershik (; born on 28 December 1933 in Leningrad) is a Soviet and Russian mathematician. He is most famous for his joint work with Sergey V. Kerov on representations of infinite symmetric groups and applications to the longest increasing subsequences.\n\nVershik studied at Leningrad State University, receiving his doctoral degree in 1974; his advisor was Vladimir Rokhlin.\n\nHe works at the Steklov Institute of Mathematics and at Saint Petersburg State University. In 1998–2008 he was the president of the St. Petersburg Mathematical Society.\n\nIn 2012 he became a fellow of the American Mathematical Society. His doctoral students include Alexander Barvinok, Anna Erschler and Sergey Fomin.\n\n\n\n"}
{"id": "800010", "url": "https://en.wikipedia.org/wiki?curid=800010", "title": "Belief propagation", "text": "Belief propagation\n\nBelief propagation, also known as sum-product message passing, is a message-passing algorithm for performing inference on graphical models, such as Bayesian networks and Markov random fields. It calculates the marginal distribution for each unobserved node, conditional on any observed nodes. Belief propagation is commonly used in artificial intelligence and information theory and has demonstrated empirical success in numerous applications including low-density parity-check codes, turbo codes, free energy approximation, and satisfiability.\n\nThe algorithm was first proposed by Judea Pearl in 1982, who formulated it as an exact inference algorithm on trees, which was later extended to polytrees. While it is not exact on general graphs anymore, it has been shown to be a useful approximate algorithm.\n\nIf \"X\"={\"X\"} is a set of discrete random variables with a joint mass function \"p\", the marginal distribution of a single \"X\" is simply the summation of \"p\" over all other variables:\n\nHowever, this quickly becomes computationally prohibitive: if there are 100 binary variables, then one needs to sum over 2 ≈ 6.338 × 10 possible values. By exploiting the polytree structure, belief propagation allows the marginals to be computed much more efficiently.\nVariants of the belief propagation algorithm exist for several types of graphical models (Bayesian networks and Markov random fields in particular). We describe here the variant that operates on a factor graph. A factor graph is a bipartite graph containing nodes corresponding to variables \"V\" and factors \"F\", with edges between variables and the factors in which they appear. We can write the joint mass function:\n\nwhere x is the vector of neighboring variable nodes to the factor node \"a\". Any Bayesian network or Markov random field can be represented as a factor graph.\n\nThe algorithm works by passing real valued functions called \"messages\" along with the edges between the hidden nodes. More precisely, if \"v\" is a variable node and \"a\" is a factor node connected to \"v\" in the factor graph, the messages from \"v\" to \"a\", (denoted by formula_3) and from \"a\" to \"v\" (formula_4), are real-valued functions whose domain is Dom(\"v\"), the set of values that can be taken by the random variable associated with \"v\". These messages contain the \"influence\" that one variable exerts on another. The messages are computed differently depending on whether the node receiving the message is a variable node or a factor node. Keeping the same notation:\n\n\nAs shown by the previous formula: the complete marginalization is reduced to a sum of products of simpler terms than the ones appearing in the full joint distribution. This is the reason why it is called the sum-product algorithm.\n\nIn a typical run, each message will be updated iteratively from the previous value of the neighboring messages. Different scheduling can be used for updating the messages. In the case where the graphical model is a tree, an optimal scheduling allows to reach convergence after computing each messages only once (see next sub-section). When the factor graph has cycles, such an optimal scheduling does not exist, and a typical choice is to update all messages simultaneously at each iteration.\n\nUpon convergence (if convergence happened), the estimated marginal distribution of each node is proportional to the product of all messages from adjoining factors (missing the normalization constant):\n\nLikewise, the estimated joint marginal distribution of the set of variables belonging to one factor is proportional to the product of the factor and the messages from the variables:\n\nIn the case where the factor graph is acyclic (i.e. is a tree or a forest), these estimated marginal actually converge to the true marginals in a finite number of iterations. This can be shown by mathematical induction.\n\nIn the case when the factor graph is a tree, the belief propagation algorithm will compute the exact marginals. Furthermore, with proper scheduling of the message updates, it will terminate after 2 steps. This optimal scheduling can be described as follows:\n\nBefore starting, the graph is oriented by designating one node as the \"root\"; any non-root node which is connected to only one other node is called a \"leaf\".\n\nIn the first step, messages are passed inwards: starting at the leaves, each node passes a message along the (unique) edge towards the root node. The tree structure guarantees that it is possible to obtain messages from all other adjoining nodes before passing the message on. This continues until the root has obtained messages from all of its adjoining nodes.\n\nThe second step involves passing the messages back out: starting at the root, messages are passed in the reverse direction. The algorithm is completed when all leaves have received their messages.\n\nCuriously, although it was originally designed for acyclic graphical models, it was found that the Belief Propagation algorithm can be used in general graphs. The algorithm is then sometimes called \"loopy\" belief propagation, because graphs typically contain cycles, or loops. The initialization and scheduling of message updates must be adjusted slightly (compared with the previously described schedule for acyclic graphs) because graphs might not contain any leaves. Instead, one initializes all variable messages to 1 and uses the same message definitions above, updating all messages at every iteration (although messages coming from known leaves or tree-structured subgraphs may no longer need updating after sufficient iterations). It is easy to show that in a tree, the message definitions of this modified procedure will converge to the set of message definitions given above within a number of iterations equal to the diameter of the tree.\n\nThe precise conditions under which loopy belief propagation will converge are still not well understood; it is known that on graphs containing a single loop it converges in most cases, but the probabilities obtained might be incorrect. Several sufficient (but not necessary) conditions for convergence of loopy belief propagation to a unique fixed point exist. There exist graphs which will fail to converge, or which will oscillate between multiple states over repeated iterations. Techniques like EXIT charts can provide an approximate visualization of the progress of belief propagation and an approximate test for convergence.\n\nThere are other approximate methods for marginalization including variational methods and Monte Carlo methods.\n\nOne method of exact marginalization in general graphs is called the junction tree algorithm, which is simply belief propagation on a modified graph guaranteed to be a tree. The basic premise is to eliminate cycles by clustering them into single nodes.\n\nA similar algorithm is commonly referred to as the Viterbi algorithm, but also known as a special case of the max-product or min-sum algorithm, which solves the related problem of maximization, or most probable explanation. Instead of attempting to solve the marginal, the goal here is to find the values formula_14 that maximizes the global function (i.e. most probable values in a probabilistic setting), and it can be defined using the arg max:\n\nAn algorithm that solves this problem is nearly identical to belief propagation, with the sums replaced by maxima in the definitions.\n\nIt is worth noting that inference problems like marginalization and maximization are NP-hard to solve exactly and approximately (at least for relative error) in a graphical model. More precisely, the marginalization problem defined above is #P-complete and maximization is NP-complete.\n\nThe memory usage of belief propagation can be reduced through the use of the Island algorithm (at a small cost in time complexity).\n\nThe sum-product algorithm is related to the calculation of free energy in thermodynamics. Let \"Z\" be the partition function. A probability distribution\n\n(as per the factor graph representation) can be viewed as a measure of the internal energy present in a system, computed as\n\nThe free energy of the system is then\n\nIt can then be shown that the points of convergence of the sum-product algorithm represent the points where the free energy in such a system is minimized. Similarly, it can be shown that a fixed point of the iterative belief propagation algorithm in graphs with cycles is a stationary point of a free energy approximation.\n\nBelief propagation algorithms are normally presented as message update equations on a factor graph, involving messages between variable nodes and their neighboring factor nodes and vice versa. Considering messages between \"regions\" in a graph is one way of generalizing the belief propagation algorithm. There are several ways of defining the set of regions in a graph that can exchange messages. One method uses ideas introduced by Kikuchi in the physics literature, and is known as Kikuchi's cluster variation method .\n\nImprovements in the performance of belief propagation algorithms are also achievable by breaking the replicas symmetry in the distributions of the fields (messages). This generalization leads to a new kind of algorithm called survey propagation (SP), which have proved to be very efficient in NP-complete problems like satisfiability\nand graph coloring.\n\nThe cluster variational method and the survey propagation algorithms are two different improvements to belief propagation. The name generalized survey propagation (GSP) is waiting to be assigned to the algorithm that merges both generalizations.\n\nGaussian belief propagation is a variant of the belief propagation algorithm when the underlying distributions are Gaussian. The first work analyzing this special model was the seminal work of Weiss and Freeman.\n\nThe GaBP algorithm solves the following marginalization problem:\n\nwhere Z is a normalization constant, \"A\" is a symmetric positive definite matrix (inverse covariance matrix a.k.a. precision matrix) and \"b\" is the shift vector.\n\nEquivalently, it can be shown that using the Gaussian model, the solution of the marginalization problem is equivalent to the MAP assignment problem:\n\nThis problem is also equivalent to the following minimization problem of the quadratic form:\n\nWhich is also equivalent to the linear system of equations\n\nConvergence of the GaBP algorithm is easier to analyze (relatively to the general BP case) and there are two known sufficient convergence conditions. The first one was formulated by Weiss et al. in the year 2000, when the information matrix A is diagonally dominant. The second convergence condition was formulated by Johnson et al. in 2006, when the spectral radius of the matrix\n\nwhere \"D\" = diag(\"A\"). Later, Su and Wu established the necessary and sufficient convergence conditions for synchronous GaBP and damped GaBP, as well as another sufficient convergence condition for asynchronous GaBP. For each case, the convergence condition involves verifying 1) a set (determined by A) being non-empty, 2) the spectral radius of a certain matrix being smaller than one, and 3) the singularity issue (when converting BP message into belief) does not occur.\n\nThe GaBP algorithm was linked to the linear algebra domain, and it was shown that the GaBP algorithm can be\nviewed as an iterative algorithm for solving the linear system of equations\n\"Ax\" = \"b\" where \"A\" is the information matrix and \"b\" is the shift vector. Empirically, the GaBP algorithm is shown to converge faster than classical iterative methods like the Jacobi method, the Gauss–Seidel method, successive over-relaxation, and others. Additionally, the GaBP algorithm is shown to be immune to numerical problems of the preconditioned conjugate gradient method \n\n"}
{"id": "58559351", "url": "https://en.wikipedia.org/wiki?curid=58559351", "title": "Brown measure", "text": "Brown measure\n\nIn mathematics, the Brown measure of an operator in a finite factor is a probability measure on the complex plane which may be viewed as an analog of the spectral counting measure (based on algebraic multiplicity) of matrices. \n\nLet formula_1 be a finite factor with the canonical normalized trace formula_2. For every operator formula_3, the function\n\nis subharmonic and its Laplacian in the distributional sense is a probability measure on formula_5 which is called the Brown measure of formula_6.\n\n"}
{"id": "14222708", "url": "https://en.wikipedia.org/wiki?curid=14222708", "title": "Chemical graph theory", "text": "Chemical graph theory\n\nChemical graph theory is the topology branch of mathematical chemistry which applies graph theory to mathematical modelling of chemical phenomena.\nThe pioneers of the chemical graph theory are Alexandru Balaban, Ante Graovac, Ivan Gutman, Haruo Hosoya, Milan Randić and Nenad Trinajstić (also Harry Wiener and others).\nIn 1988, it was reported that several hundred researchers worked in this area producing about 500 articles annually. A number of monographs have been written in the area, including the two-volume comprehensive text by Trinajstic, \"Chemical Graph Theory\", that summarized the field up to mid-1980s.\n\nThe adherents of the theory maintain that the properties of a chemical graph (i.e., a graph-theoretical representation of a molecule) give valuable insights into the chemical phenomena. The opponents contend that graphs play only a fringe role in chemical research. One variant of the theory is the representation of materials as infinite Euclidean graphs, particularly crystals by periodic graphs.\n\n"}
{"id": "8975663", "url": "https://en.wikipedia.org/wiki?curid=8975663", "title": "Coding gain", "text": "Coding gain\n\nIn coding theory and related engineering problems, coding gain is the measure in the difference between the signal-to-noise ratio (SNR) levels between the uncoded system and coded system required to reach the same bit error rate (BER) levels when used with the error correcting code (ECC).\n\nIf the uncoded BPSK system in AWGN environment has a bit error rate (BER) of 10 at the SNR level 4 dB, and the corresponding coded (e.g., BCH) system has the same BER at an SNR of 2.5 dB, then we say the \"coding gain\" = , due to the code used (in this case BCH).\n\nIn the \"power-limited regime\" (where the nominal spectral efficiency formula_1 [b/2D or b/s/Hz], \"i.e.\" the domain of binary signaling), the effective coding gain formula_2 of a signal set formula_3 at a given target error probability per bit formula_4 is defined as the difference in dB between the formula_5 required to achieve the target formula_4 with formula_3 and the formula_5 required to achieve the target formula_4 with 2-PAM or (2×2)-QAM (\"i.e.\" no coding). The nominal coding gain formula_10 is defined as\n\nThis definition is normalized so that formula_12 for 2-PAM or (2×2)-QAM. If the average number of nearest neighbors per transmitted bit formula_13 is equal to one, the effective coding gain formula_2 is approximately equal to the nominal coding gain formula_10. However, if formula_16, the effective coding gain formula_2 is less than the nominal coding gain formula_10 by an amount which depends on the steepness of the formula_4 \"vs.\" formula_5 curve at the target formula_4. This curve can be plotted using the union bound estimate (UBE)\n\nwhere \"Q\" is the Gaussian probability-of-error function.\n\nFor the special case of a binary linear block code formula_23 with parameters formula_24, the nominal spectral efficiency is formula_25 and the nominal coding gain is \"kd\"/\"n\".\n\nThe table below lists the nominal spectral efficiency, nominal coding gain and effective coding gain at formula_26 for Reed–Muller codes of length formula_27:\n\nIn the \"bandwidth-limited regime\" (formula_28, \"i.e.\" the domain of non-binary signaling), the effective coding gain formula_2 of a signal set formula_3 at a given target error rate formula_31 is defined as the difference in dB between the formula_32 required to achieve the target formula_31 with formula_3 and the formula_32 required to achieve the target formula_31 with M-PAM or (M×M)-QAM (\"i.e.\" no coding). The nominal coding gain formula_10 is defined as\n\nThis definition is normalized so that formula_12 for M-PAM or (\"M\"×\"M\")-QAM. The UBE becomes\n\nwhere formula_41 is the average number of nearest neighbors per two dimensions.\n\n\nMIT OpenCourseWare, 6.451 Principles of Digital Communication II, Lecture Notes sections 5.3, 5.5, 6.3, 6.4\n"}
{"id": "26358420", "url": "https://en.wikipedia.org/wiki?curid=26358420", "title": "Controlled grammar", "text": "Controlled grammar\n\nControlled grammars are a class of grammars that extend, usually, the context-free grammars with additional controls on the derivations of a sentence in the language. A number of different kinds of controlled grammars exist, the four main divisions being Indexed grammars, grammars with prescribed derivation sequences, grammars with contextual conditions on rule application, and grammars with parallelism in rule application. Because indexed grammars are so well established in the field, this article will address only the latter three kinds of controlled grammars.\n\nGrammars with prescribed sequences are grammars in which the sequence of rule application is constrained in some way. There are four different versions of prescribed sequence grammars: language controlled grammars (often called just controlled grammars), matrix grammars, vector grammars, and programmed grammars.\n\nIn the standard context-free grammar formalism, a grammar itself is viewed as a 4-tuple, formula_1, where \"N\" is a set of non-terminal/phrasal symbols, \"T\" is a disjoint set of terminal/word symbols, \"S\" is a specially designated start symbol chosen from \"N\", and \"P\" is a set of production rules like formula_2, where \"X\" is some member of \"N\", and formula_3 is some member of formula_4.\n\nProductions over such a grammar are sequences of rules in \"P\" that, when applied in order of the sequence, lead to a terminal string. That is, one can view the set of imaginable derivations in \"G\" as the set formula_5, and the language of \"G\" as being the set of terminal strings formula_6. Control grammars take seriously this definition of the language generated by a grammar, concretizing the set-of-derivations as an aspect of the grammar. Thus, a prescribed sequence controlled grammar is at least approximately a 5-tuple formula_7 where everything except \"R\" is the same as in a CFG, and \"R\" is an infinite set of valid derivation sequences formula_8.\n\nThe set \"R\", due to its infinitude, is almost always (though not necessarily) described via some more convenient mechanism, such as a grammar (as in language controlled grammars), or a set of matrices or vectors (as in matrix and vector grammars). The different variations of prescribed sequence grammars thus differ by how the sequence of derivations is defined on top of the context-free base. Because matrix grammars and vector grammars are essentially special cases of language controlled grammars, examples of the former two will not be provided below.\n\nLanguage controlled grammars are grammars in which the production sequences constitute a well-defined language of arbitrary nature, usually though not necessarily regular, over a set of (again usually though not necessarily) context-free production rules. They also often have a sixth set in the grammar tuple, making it formula_9, where \"F\" is a set of productions that are allowed to apply vacuously. This version of language controlled grammars, ones with what is called \"appearance checking\", is the one henceforth.\n\nWe let a regularly controlled context-free grammar with appearance checking be a 6-tuple formula_9 where \"N\", \"T\", \"S\", and \"P\" are defined as in CFGs, \"R\" is a subset of \"P*\" constituting a regular language over \"P\", and \"F\" is some subset of \"P\". We then define the immediately derives relation formula_11 as follows:\n\nGiven some strings \"x\" and \"y\", both in formula_4, and some rule formula_13,\n\nholds if either\n\nIntuitively, this simply spells out that a rule can apply to a string if the rule's left-hand-side appears in that string, or if the rule is in the set of \"vacuously applicable\" rules which can \"apply\" to a string without changing anything. This requirement that the non-vacuously applicable rules must apply is the appearance checking aspect of such a grammar. The language for this kind of grammar is then simply set of terminal strings formula_19.\n\nLet's consider a simple (though not the simplest) context-free grammar that generates the language formula_20:\n\nLet formula_21, where\n\nIn language controlled form, this grammar is simply formula_27 (where formula_28 is a regular expression denoting the set of all sequences of production rules). A simple modification to this grammar, changing is control sequence set \"R\" into the set formula_29, and changing its vacuous rule set \"F\" to formula_30, yields a grammar which generates the non-CF language formula_31. To see how, let's consider the general case of some string with \"n\" instances of \"S\" in it, i.e. formula_32 (the special case formula_33 trivially derives the string \"a\" which is formula_34, an uninteresting fact).\n\nIf we chose some arbitrary production sequence formula_35, we can consider three possibilities: formula_36, formula_37, and formula_38 When formula_36 we rewrite all \"n\" instances of \"S\" as \"AA\", by applying rule \"f\" to the string \"u\" times, and proceed to apply \"g\", which applies vacuously (by virtue of being in \"F\") . When formula_37, we rewrite all \"n\" instances of \"S\" as \"AA\", and then try to perform the \"n+1\" rewrite using rule \"f\", but this fails because there are no more \"S\"s to rewrite, and \"f\" is not in \"F\" and so cannot apply vacuously, thus when formula_37, the derivation fails. Lastly, then formula_38, we rewrite \"u\" instances of \"S\", leaving at least one instance of \"S\" to be rewritten by the subsequent application of \"g\", rewriting \"S\" as \"X\". Given that no rule of this grammar ever rewrites \"X\", such a derivation is destined to never produce a terminal string. Thus only derivations with formula_36 will ever successfully rewrite the string formula_32. Similar reasoning holds of the number of \"A\"s and \"v\". In general, then, we can say that the only valid derivations have the structure formula_45 will produce terminal strings of the grammar. The \"X\" rules, combined with the structure of the control, essentially force all \"S\"s to be rewritten as \"AA\"s prior to any \"A\"s being rewritten as \"S\"s, which again is forced to happen prior to all still later iterations over the \"S-to-AA\" cycle. Finally, the \"S\"s are rewritten as \"a\"s. In this way, the number of \"S\"s doubles each for each instantiation of formula_46 that appears in a terminal-deriving sequence.\n\nChoosing two random non-terminal deriving sequences, and one terminal-deriving one, we can see this in work:\n\nLet formula_47, then we get the failed derivation:\n\nLet formula_49, then we get the failed derivation:\n\nLet formula_51, then we get the successful derivation:\n\nSimilar derivations with a second cycle of formula_53 produce only \"SSSS\". Showing only the (continued) successful derivation:\n\nMatrix grammars (expanded on in their own article) are a special case of regular controlled context-free grammars, in which the production sequence language is of the form formula_57, where each \"matrix\" formula_58 is a single sequence. For convenience, such a grammar is not represented with a grammar over \"P\", but rather with just a set of the matrices in place of both the language and the production rules. Thus, a matrix grammar is the 5-tuple formula_59, where \"N\", \"T\", \"S\", and \"F\" are defined essentially as previously done (with \"F\" a subset of \"M\" this time), and \"M\" is a set of matrices formula_60 where each formula_61 is a context-free production rule.\n\nThe derives relation in a matrix grammar is thus defined simply as:\n\nGiven some strings \"x\" and \"y\", both in formula_4, and some matrix formula_63,\n\nholds if either\n\nInformally, a matrix grammar is simply a grammar in which during each rewriting cycle, a particular sequence of rewrite operations must be performed, rather than just a single rewrite operation, i.e. one rule \"triggers\" a cascade of other rules. Similar phenomena can be performed in the standard context-sensitive idiom, as done in rule-based phonology and earlier Transformational grammar, by what are known as \"feeding\" rules, which alter a derivation in such a way as to provide the environment for a non-optional rule that immediately follows it.\n\nVector grammars are closely related to matrix grammars, and in fact can be seen as a special class of matrix grammars, in which if formula_70, then so are all of its permutations formula_71. For convenience, however, we will define vector grammars as follows: a vector grammar is a 5-tuple formula_59, where \"N\", \"T\", and \"F\" are defined previously (\"F\" being a subset of \"M\" again), and where \"M\" is a set of vectors formula_73, each vector being a set of context free rules.\n\nThe derives relation in a vector grammar is then:\n\nGiven some strings \"x\" and \"y\", both in formula_4, and some matrix formula_75,\n\nholds if either\n\nNotice that the number of production rules used in the derivation sequence, \"n\", is the same as the number of production rules in the vector. Informally, then, a vector grammar is one in which a set of productions is applied, each production applied exactly once, in arbitrary order, to derive one string from another. Thus vector grammars are almost identical to matrix grammars, minus the restriction on the order in which the productions must occur during each cycle of rule application.\n\nProgrammed grammars are relatively simple extensions to context-free grammars with rule-by-rule control of the derivation. A programmed grammar is a 4-tuple formula_1, where \"N\", \"T\", and \"S\" are as in a context-free grammar, and \"P\" is a set of tuples formula_84, where \"p\" is a context-free production rule, formula_85 is a subset of \"N\" (called the success field), and formula_86 is a subset of \"N\" (called the failure field). If the failure field of every rule in \"P\" is empty, the grammar lacks appearance checking, and if at least one failure field is not empty, the grammar has appearance checking. The derivation relation on a programmed grammar is defined as follows:\n\nGiven two strings formula_87, and some rule formula_88,\n\nThe language of a programmed grammar \"G\" is defined by constraining the derivation rule-wise, as formula_92, where for each formula_93, either formula_94 or formula_95.\n\nIntuitively, when applying a rule \"p\" in a programmed grammar, the rule can either succeed at rewriting a symbol in the string, in which case the subsequent rule must be in \"p\"s success field, or the rule can fail to rewrite a symbol (thus applying vacuously), in which case the subsequent rule must be in \"p\"s failure field. The choice of which rule to apply to the start string is arbitrary, unlike in a language controlled grammar, but once a choice is made the rules that can be applied after it constrain the sequence of rules from that point on.\n\nAs with so many controlled grammars, programmed grammars can generate the language formula_31:\n\nLet formula_97, where\n\nThe derivation for the string \"\" is as follows:\n\nAs can be seen from the derivation and the rules, each time formula_105 and formula_106 succeed, they feed back to themselves, which forces each rule to continue to rewrite the string over and over until it can do so no more. Upon failing, the derivation can switch to a different rule. In the case of formula_105, that means rewriting all \"S\"s as \"AA\"s, then switching to formula_106. In the case of formula_106, it means rewriting all \"A\"s as \"S\"s, then switching either to formula_105, which will lead to doubling the number of \"S\"s produced, or to formula_111 which converts the \"S\"s to \"a\"s then halts the derivation. Each cycle through formula_105 then formula_106 therefore either doubles the initial number of \"S\"s, or converts the \"S\"s to \"a\"s. The trivial case of generating \"a\", in case it is difficult to see, simply involves vacuously applying formula_105, thus jumping straight to formula_106 which also vacuously applies, then jumping to formula_111 which produces \"a\".\n\nUnlike grammars controlled by prescribed sequences of production rules, which constrain the space of valid derivations but do not constrain the sorts of sentences that a production rule can apply to, grammars controlled by context conditions have no sequence constraints, but permit constraints of varying complexity on the sentences to which a production rule applies. Similar to grammars controlled by prescribed sequences, there are multiple different kinds of grammars controlled by context conditions: conditional grammars, semi-conditional grammars, random context grammars, and ordered grammars.\n\nConditional grammars are the simplest version of grammars controlled by context conditions. The structure of a conditional grammar is very similar to that of a normal rewrite grammar: formula_1, where \"N\", \"T\", and \"S\" are as defined in a context-free grammar, and \"P\" is a set of pairs of the form formula_118 where \"p\" is a production rule (usually context-free), and \"R\" is a language (usually regular) over formula_119. When \"R\" is regular, \"R\" can just be expressed as a regular expression.\n\nWith this definition of a conditional grammar, we can define the derives relation as follows:\n\nGiven two strings formula_87, and some production rule formula_121,\n\nInformally then, the production rule for some pair in \"P\" can apply only to strings that are in its context language. Thus, for example, if we had some pair formula_126, we can only apply this to strings consisting of any number of \"a\"s followed by exactly only \"S\" followed by any number of \"b\"s, i.e. to sentences in formula_127, such as the strings \"S\", \"aSb\", ', ', etc. It cannot apply to strings like \"xSy\", \", etc.\n\nConditional grammars can generate the context-sensitive language formula_31.\n\nLet formula_129, where\n\nWe can then generate the sentence \" with the following derivation:\n\nA semi-conditional grammar is very similar to a conditional grammar, and technically the class of semi-conditional grammars are a subset of the conditional grammars. Rather than specifying what the whole of the string must look like for a rule to apply, semi-conditional grammars specify that a string must have as substrings all of some set of strings, and none of another set, in order for a rule to apply. Formally, then, a semi-conditional grammar is a tuple formula_1, where, \"N\", \"T\", and \"S\" are defined as in a CFG, and \"P\" is a set of rules like formula_140 where \"p\" is a (usually context-free) production rule, and \"R\" and \"Q\" are finite sets of strings. The derives relation can then be defined as follows.\n\nFor two strings formula_141, and some rule formula_142,\n\nThe language of a semi-conditional grammar is then trivially the set of terminal strings formula_146.\n\nAn example of a semi-conditional grammar is given below also as an example of random context grammars.\n\nA random context grammar is a semi-conditional grammar in which the \"R\" and \"Q\" sets are all subsets of \"N\". Because subsets of \"N\" are finite sets over formula_4, it is clear that random context grammars are indeed kinds of semi-conditional grammars.\n\nLike conditional grammars, random context grammars (and thus semi-conditional grammars) can generate the language formula_31. One grammar which can do this is:\n\nLet formula_149, where\n\nConsider now the production for \"\":\n\nThe behavior of the \"R\" sets here is trivial: any string can be rewritten according to them, because they do not require any substrings to be present. The behavior of the \"Q\" sets, however, are more interesting. In formula_105, we are forced by the \"Q\" set to rewrite an \"S\", thus beginning an \"S\"-doubling process, only when no \"Y\"s or \"A\"s are present in the string, which means only when a prior \"S\"-doubling process has been fully initiated, eliminating the possibility of only doubling some of the \"S\"s. In formula_106, which moves the \"S\"-doubling process into its second stage, we cannot begin this process until the first stage is complete and there are no more \"S\"s to try to double, because the \"Q\" set prevents the rule from applying if there is an \"S\" symbol still in the string. In formula_111, we complete the doubling stage by introducing the \"S\"s back only when there are no more \"X\"s to rewrite, thus when the second stage is complete. We can cycle through these stages as many times as we want, rewriting all \"S\"s to \"XX\"s before then rewriting each \"X\" to a Y, and then each \"Y\" to an \"S\", finally ending by replacing each \"S\" with an \"A\" and then an \"a\". Because the rule for replacing \"S\" with \"A\" prohibits application to a string with an \"X\" in it, we cannot apply this in the middle of the first stage of the \"S\"-doubling process, thus again preventing us from only doubling some \"S\"s.\n\nOrdered grammars are perhaps one of the simpler extensions of grammars into the controlled grammar domain. An ordered grammar is simply a tuple formula_1 where \"N\", \"T\", and \"S\" are identical to those in a CFG, and \"P\" is a set of context-free rewrite rules with a partial ordering formula_164. The partial ordering is then used to determine which rule to apply to a string, when multiple rules are applicable. The derives relation is then:\n\nGiven some strings formula_141 and some rule formula_13,\n\nLike many other contextually controlled grammars, ordered grammars can enforce the application of rules in a particular order. Since this is the essential property of previous grammars that could generate the language formula_31, it should be no surprise that a grammar that explicitly uses rule ordering, rather than encoding it via string contexts, should similarly be able to capture that language. And as it turns out, just such an ordered grammar exists:\n\nLet formula_171, where \"P\" is the partially ordered set described by the Hasse diagram\n\nThe derivation for the string \"\" is simply:\n\nAt each step of the way, the derivation proceeds by rewriting in cycles. Notice that if at the fifth step \"SY\", we had four options: formula_178, the first two of which halt the derivation, as \"Z\" cannot be rewritten. In the example, we used formula_179 to derive \"SS\", but consider if we had chosen formula_180 instead. We would have produced the string \"AS\", the options for which are formula_181 and formula_182, both of which halt the derivation. Thus with the string \"SY\", and conversely with \"YS\", we must rewrite the \"Y\" to produce \"SS\". The same hold for other combinations, so that overall, the ordering forces the derivation to halt, or else proceed by rewriting all \"S\"s to \"XX\"s, then all \"X\"s to \"Y\"s, then all \"Y\"s to \"S\"s, and so on, then finally all \"S\"s to \"A\"s then all \"A\"s to \"a\"s. In this way, a string formula_32 can only ever be rewritten as formula_184 which produces \"a\"s, or as formula_185. Starting with \"n = 0\", it should be clear that this grammar only generates the language formula_31.\n\nA still further class of controlled grammars is the class of grammars with parallelism in the application of a rewrite operation, in which each rewrite step can (or must) rewrite more than one non-terminal simultaneously. These, too, come in several flavors: Indian parallel grammars, k-grammars, scattered context grammars, unordered scattered context grammars, and k-simple matrix grammars. Again, the variants differ in how the parallelism is defined.\n\nAn Indian parallel grammar is simply a CFG in which to use a rewrite rule, all instances of the rules non-terminal symbol must be rewritten simultaneously. Thus, for example, given the string \"aXbYcXd\", with two instances of \"X\", and some rule formula_187, the only way to rewrite this string with this rule is to rewrite it as \"awbYcwd\"; neither \"awbYcXd\" nor \"aXbYcwd\" are valid rewrites in an Indian parallel grammar, because they did not rewrite all instances of \"X\".\n\nIndian parallel grammars can easily produce the language formula_188:\n\nLet formula_189, where\n\nGenerating \"aabaab\" then is quite simple:\n\nThe language formula_31 is even simpler:\n\nLet formula_196, where \"P\" consists of\n\nIt should be obvious, just from the first rule, and the requirement that all instances of a non-terminal are rewritten simultaneously with the same rule, that the number of \"S\"s doubles on each rewrite step using the first rule, giving the derivation steps formula_199. Final application of the second rule replaces all the \"S\"s with \"a\"s, thus showing how this simple language can produce the language formula_31.\n\nA k-grammar is yet another kind of parallel grammar, very different from an Indian parallel grammar, but still with a level of parallelism. In a k-grammar, for some number \"k\", exactly \"k\" non-terminal symbols must be rewritten at every step (except the first step, where the only symbol in the string is the start symbol). If the string has less than \"k\" non-terminals, the derivation fails.\n\nA 3-grammar can produce the language formula_201, as can be seen below:\n\nLet formula_202, where \"P\" consists of:\n\nWith the following derivation for \"\":\n\nAt each step in the derivation except the first and last, we used the self-recursive rules formula_211. If we had not use the recursive rules, instead using, say, formula_212, where one of the rules is not self-recursive, the number of non-terminals would have decreased to 2, thus making the string unable to be derived further because it would have too few non-terminals to be rewritten.\n\nRussian parallel grammars are somewhere between Indian parallel grammars and k-grammars, defined as formula_213, where \"N\", \"T\", and \"S\" are as in a context-free grammar, and \"P\" is a set of pairs formula_214, where formula_215 is a context-free production rule, and \"k\" is either 1 or 2. Application of a rule formula_216 involves rewriting \"k\" occurrences of \"A\" to \"w\" simultaneously.\n\nA scattered context grammar is a 4-tuple formula_1 where \"N\", \"T\", and \"S\" are defined as in a context-free grammar, and \"P\" is a set of tuples called matrixes formula_218, where formula_219 can vary according to the matrix. The derives relation for such a grammar is\n\nIntuitively, then, the matrixes in a scattered context grammar provide a list of rules which must each be applied to non-terminals in a string, where those non-terminals appear in the same linear order as the rules that rewrite them.\n\nAn unordered scattered context grammar is a scattered context grammar in which, for every rule in \"P\", each of its permutations is also in \"P\". As such, a rule and its permutations can instead be represented as a set rather than as tuples.\n\nScattered context grammars are capable of describing the language formula_201 quite easily.\n\nLet formula_225, where\n\nDeriving \"\" then is trivial:\n"}
{"id": "37903530", "url": "https://en.wikipedia.org/wiki?curid=37903530", "title": "Diff-Text", "text": "Diff-Text\n\nDiff-Text is a free software tool which finds the differences between two blocks of plain text. It takes\nthe form of a collection of web-pages, each one with a slightly different layout. Text to be compared is\npasted directly into the web-page. It can be used from any operating system.\n\nDiff-Text was developed by DiffEngineX LLC and uses improved algorithms originally developed for the spreadsheet compare tool DiffEngineX\n\nIt allows the user to choose between comparing at the level of whole lines (or paragraphs), words or characters. If comparing whole lines only the fact that a line is not in the other block will be reported. Diff Text considers a paragraph to be any\nline ending with a Windows, Macintosh or Unix line terminator.\n\nThe website can combine the original and modified text blocks into one pane with all differences highlighted. Alternatively the marked-up original and modified text blocks can be displayed in individual panes.\n\nNavigation from one difference to the next is supported.\n\nAll of the above features are not unique and can be found in other text comparison tools.\n\nThe software can display just the differences, the differences with a variable amount of context on either side or the whole marked-up text.\n\nThe website supports the use of SSL (\"https\") so confidential text can be compared.\n\nThe algorithm used by Diff Text is used by Selection Diff Tool, which is an app for Microsoft Word and Excel 2013.\n\nThe unique feature of Diff-Text is its ability to spot text that has either been moved up or down in the document and placed into a new context. To avoid spurious similarities being flagged, the software allows the user to specify the minimum number of adjacent words or characters to be reported as a move. Text movements are reported such that the number of individual edits to transform the original text into the modified text are at a minimum.\n\nThe vast majority of text comparison software based on the longest common subsequence problem algorithm incorrectly report\nmoved text as unlinked additions and deletions. The algorithm only reports the longest in-order run of text between two documents. Text moved out of the longest run of similarities is missed.\n\nHeuristics are not used. Any similarity between the two documents above the specified minimum will be reported (if detecting moves is selected). This is the main difference between Diff Text and most other text comparison algorithms. Diff Text will always match up significant similarities even if contained within non-identical or moved lines. It never resorts to guessing or the first match that happens to be found, which may result in non-optimal matches elsewhere.\n\nNot only can Diff-Text spot whole paragraphs that have been moved up or down in a document, it can spot sentence re-ordering within a paragraph. To indicate this the background color of the text changes to light blue and yellow.\n\nIf the user specifies text movements should not be detected, its algorithm runs in (m log n) time, which is an improvement from the standard quadratic time often seen in software of this type. m and n refer to the sizes of the original and modified texts.\n\nConventional text comparison tools based on the longest common subsequence problem algorithm can potentially miss a lot of similarities between original and modified files, if blocks of text are moved around. Diff-Text is systematic and allows the user to specify the minimum number of contiguous words or characters to be considered a valid move.\n\n\n"}
{"id": "12745973", "url": "https://en.wikipedia.org/wiki?curid=12745973", "title": "Doubly linked face list", "text": "Doubly linked face list\n\nIn applied mathematics, a doubly linked face list (DLFL) is an efficient data structure for storing 2-manifold mesh data. The structure stores linked lists for a 3D mesh's faces, edges, vertices, and corners. The structure guarantees the preservation of the manifold property.\n"}
{"id": "3338987", "url": "https://en.wikipedia.org/wiki?curid=3338987", "title": "Euler's theorem in geometry", "text": "Euler's theorem in geometry\n\nIn geometry, Euler's theorem states that the distance \"d\" between the circumcentre and incentre of a triangle is given by\n\nor equivalently\n\nwhere \"R\" and \"r\" denote the circumradius and inradius respectively (the radii of the circumscribed circle and inscribed circle respectively). The theorem is named for Leonhard Euler, who published it in 1767. However, the same result was published earlier by William Chapple in 1746.\n\nFrom the theorem follows the Euler inequality:\n\nwhich holds with equality only in the equilateral case.\n\nLetting \"O\" be the circumcentre of triangle \"ABC\", and \"I\" be its incentre, the extension of \"AI\" intersects the circumcircle at \"L\". Then \"L\" is the midpoint of arc \"BC\". Join \"LO\" and extend it so that it intersects the circumcircle at \"M\". From \"I\" construct a perpendicular to AB, and let D be its foot, so \"ID\" = \"r\". It is not difficult to prove that triangle \"ADI\" is similar to triangle \"MBL\", so \"ID\" / \"BL\" = \"AI\" / \"ML\", i.e. \"ID\" × \"ML\" = \"AI\" × \"BL\". Therefore 2\"Rr\" = \"AI\" × \"BL\". Join \"BI\". Because\n\nwe have ∠ \"BIL\" = ∠ \"IBL\", so \"BL\" = \"IL\", and \"AI\" × \"IL\" = 2\"Rr\". Extend \"OI\" so that it intersects the circumcircle at \"P\" and \"Q\"; then \"PI\" × \"QI\" = \"AI\" × \"IL\" = 2\"Rr\", so (\"R\" + \"d\")(\"R\" − \"d\") = 2\"Rr\", i.e. \"d\" = \"R\"(\"R\" − 2\"r\").\n\nA stronger version is\n\nwhere \"a, b, c\" are the sidelengths of the triangle.\n\nIf formula_5 and formula_6 denote respectively the radius of the exscribed circle opposite to the vertex formula_7 and the distance between its centre and the centre of \nthe circumscribed circle, then formula_8.\n\nEuler's inequality, in the form stating that, for all triangles inscribed in a given circle, the maximum of the radius of the inscribed circle is reached for the equilateral triangle and only for it, is valid in absolute geometry. \n\n"}
{"id": "185427", "url": "https://en.wikipedia.org/wiki?curid=185427", "title": "Function (mathematics)", "text": "Function (mathematics)\n\nIn mathematics, a function was originally the idealization of how a varying quantity depends on another quantity. For example, the position of a planet is a \"function\" of time. Historically, the concept was elaborated with the infinitesimal calculus at the end of the 17th century, and, until the 19th century, the functions that were considered were differentiable (that is, they had a high degree of regularity). The concept of function was formalized at the end of the 19th century in terms of set theory, and this greatly enlarged the domains of application of the concept. \n\nA function is a process or a relation that associates each element of a set , the \"domain\" of the function, to a single element of another set (possibly the same set), the \"codomain\" of the function. If the function is called , this relation is denoted (read of ), the element is the \"argument\" or \"input\" of the function, and is the \"value of the function\", the \"output\", or the \"image\" of by . The symbol that is used for representing the input is the variable of the function (one often says that is a function of the variable ).\n\nA function is uniquely represented by its graph which is the set of all pairs . When the domain and the codomain are sets of numbers, each such pair may be considered as the Cartesian coordinates of a point in the plane. In general, these points form a curve, which is also called the graph of the function. This is a useful representation of the function, which is commonly used everywhere, for example in newspapers.\n\nFunctions are widely used in science, and in most fields of mathematics. Their role is so important that it has been said that they are \"the central objects of investigation\" in most fields of mathematics.\n\nIntuitively, a function is a process that associates to each element of a set a unique element of a set .\n\nFormally, a function from a set to a set is defined by a set of ordered pairs such that , , and every element of is the first component of exactly one ordered pair in . In other words, for every in , there is exactly one element such that the ordered pair belongs to the set of pairs defining the function . The set is called the graph of the function. Formally speaking, it may be identified with the function, but this hides the usual interpretation of a function as a process. Therefore, in common usage, the function is generally distinguished from its graph. Functions are also called \"maps\" or \"mappings\". However, some authors reserve the word \"mapping\" to the case where the codomain \"Y\" belongs explicitly to the definition of the function. In this sense, the graph of the mapping recovers the function as the set of pairs.\n\nIn the definition of function, and are respectively called the \"domain\" and the \"codomain\" of the function . If belongs to the set defining , then is the \"image\" of under , or the \"value\" of applied to the \"argument\" . Especially in the context of numbers, one says also that is the value of for the \"value of its variable\", or, still shorter, is the \"value of\" \"of\" , denoted as .\n\nThe domain and codomain are not always explicitly given when a function is defined, and, without some (possibly difficult) computation, one knows only that the domain is contained in a larger set. Typically, this occurs in mathematical analysis, where \"a function often refers to a function that may have a proper subset of as domain. For example, a \"function from the reals to the reals\" may refer to a real-valued function of a real variable, and this phrase does not mean that the domain of the function is the whole set of the real numbers, but only that the domain is a set of real numbers that contains a non-empty open interval. For example, if is a function that has the real numbers as domain and codomain, then a function mapping the value to the value formula_1 is a function from the reals to the reals, whose domain is the set of the reals , such that . In many cases, the exact domains are difficult to determine, but this is rarely a problem for working with such functions.\n\nThe range of a function is the set of the images of all elements in the domain. However, \"range\" is sometimes used as a synonym of codomain, generally in old textbooks.\n\nAny subset of the Cartesian product of a domain formula_2 and a codomain formula_3 is said to define a binary relation formula_4 between these two sets. It is immediate that an arbitrary relation may contain pairs that violate the necessary conditions for a function, given above. \n\nA univalent relation is a relation such that\nUnivalent relations may be identified to functions whose domain is a subset of .\n\nA left-total relation is a relation such that \nFormal functions may be strictly identified to relations that are both univalent and left total. Violating the left-totality is similar to giving a convenient encompassing set instead of the true domain, as explained above.\n\nVarious properties of functions and function composition may be reformulated in the language of relations. For example, a function is injective if the converse relation formula_7 is univalent, where the converse relation is defined as formula_8\n\nThere are various standard ways for denoting functions. The most commonly used notation is functional notation, which defines the function using an equation that gives the names of the function and the argument explicitly. This gives rise to a subtle point, often glossed over in elementary treatments of functions: \"functions\" are distinct from their \"values\". Thus, a function should be distinguished from its value at the value in its domain. To some extent, even working mathematicians will conflate the two in informal settings for convenience, and to avoid the use of pedantic language. However, strictly speaking, it is an abuse of notation to write \"let formula_9 be the function \", since and should both be understood as the \"value\" of \"f\" at \"x\", rather than the function itself. Instead, it is correct, though pedantic, to write \"let formula_9 be the function defined by the equation valid for all real values of \". \n\nThis distinction in language and notation becomes important in cases where functions themselves serve as inputs for other functions. (A function taking another function as an input is termed a \"functional\".) Other approaches to denoting functions, detailed below, avoid this problem but are less commonly used.\n\nFirst used by Leonhard Euler in 1734, it is often useful to use a symbol for denoting a function. This symbol consists generally of a single letter in italic font, most often the lower-case letters . Some widely used functions are represented by a symbol consisting of several letters (usually two or three, generally an abbreviation of their name). By convention, the symbol for standard functions is set in roman type, such as \"\" for the sine function, in contrast to functions defined on an \"ad hoc\" basis.\n\nThe notation (read: \" equals of \")\nmeans that the pair belongs to the set of pairs defining the function . If is the domain of , the set of pairs defining the function is thus, using set-builder notation,\n\nOften, a definition of the function is given by what \"f\" does to the explicit argument \"x.\" For example, a function \"f\" can be defined by the equation\n\nfor all real numbers \"x.\" In this example, \"f\" can be thought of as the composite of several simpler functions: squaring, adding 1, and taking the sine. However, only the sine function has a common explicit symbol (sin), while the combination of squaring and then adding 1 is described by the polynomial expression formula_14. In order to explicitly reference functions such as squaring or adding 1 without introducing new function names (e.g., by defining function \"g\" and \"h\" by formula_15 and formula_16), one of the methods below (arrow notation or dot notation) could be used. \n\nSometimes the parentheses of functional notation are omitted when the symbol denoting the function consists of several characters and no ambiguity may arise. For example, formula_17 can be written instead of formula_18\n\nFor explicitly expressing domain and the codomain of a function , the arrow notation is often used (read: or ):\n\nor\n\nThis is often used in relation with the arrow notation for elements (read: \" maps to \"), often stacked immediately below the arrow notation giving the function symbol, domain, and codomain: \n\nFor example, if a multiplication is defined on a set , then the square function formula_22 on is unambiguously defined by (read: \"the function formula_22 from to that maps to \")\n\nthe latter line being more commonly written \n\nOften, the expression giving the function symbol, domain and codomain is omitted. Thus, the arrow notation is useful for avoiding introducing a symbol for a function that is defined, as it is often the case, by a formula expressing the value of the function in terms of its argument. As a common application of the arrow notation, suppose formula_26 is a two-argument function, and we want to refer to a partially applied function formula_27 produced by fixing the second argument to the value without introducing a new function name. The map in question could be denoted formula_28 using the arrow notation for elements. Note that the expression formula_28 (read: \"the map taking to formula_30\") represents this new function with just one argument, whereas the expression formula_31 refers to the value of the function at the \n\nIndex notation is often used instead of functional notation. That is, instead of writing , one writes formula_32 \n\nThis is typically the case for functions whose domain is the set of the natural numbers. Such a function is called a sequence, and, in this case the element formula_33 is called the th element of sequence.\n\nThe index notation is also often used for distinguishing some variables called parameters from the \"true variables\". In fact, parameters are specific variables that are considered as being fixed during the study of a problem. For example, the map formula_34 (see above) would be denoted formula_35 using index notation, if we define the collection of maps formula_35 by the formula formula_37 for all formula_38.\n\nIn the notation \nformula_39\nthe symbol does not represent any value, it is simply a placeholder meaning that, if is replaced by any value on the left of the arrow, it should be replaced by the same value on the right of the arrow. Therefore, may be replaced by any symbol, often an interpunct \"\". This may be useful for distinguishing the function from its value at . \n\nFor example, formula_40 may stand for the function formula_41, and formula_42 may stand for a function defined by an integral with variable upper bound: formula_43.\n\nThere are other, specialized notations for functions in sub-disciplines of mathematics. For example, in linear algebra and functional analysis, linear forms and the vectors they act upon are denoted using a dual pair to show the underlying duality. This is similar to the use of bra–ket notation in quantum mechanics. In logic and the theory of computation, the function notation of lambda calculus is used to explicitly express the basic notions of function abstraction and application. In category theory and homological algebra, networks of functions are described in terms of how they and their compositions commute with each other using commutative diagrams that extend and generalize the arrow notation for functions described above.\n\nAccording to the definition of a function, a specific function is, in general, defined by associating to every element of its domain one element of its codomain. When the domain and the codomain are sets of numbers, this association may take the form of a computation taking as input any element of the domain and producing an output in the codomain. This computation may be described by a formula. (This is the starting point of algebra, where many similar numerical computations can be replaced by a single formula that describes these computations by means of variables that represent computation inputs as unspecified numbers). This type of specification of a function frequently uses previously defined auxiliary functions. \n\nFor example, the function from the reals to the reals, defined by the formula \nformula_44\nemploys, as auxiliary functions, the square function (mapping all the reals to the non-negative reals), the square root function (mapping the non-negative reals to the non-negative reals), and the addition of real numbers. The whole set of real numbers may be taken as the domain of , even though the domain of the square root function is restricted to the non-negative real numbers; the image of consists of the reals that are not less than one.\n\nA computation that defines a function may often be described by an algorithm, and any kind of algorithm may be used. Sometimes, the definition of a function may involve elements or properties that can be defined, but not computed. For example, if one considers the set formula_45 of the programs in a given programming language that take an integer as input. The \"terminating function\" is the function that returns 1 if a program of formula_45 runs forever when executed on a given integer input, and returns 0 otherwise. It is a basic theorem of computability theory that there does not exist an algorithm for computing this function. More generally, computability theory is the study of computable functions, that is, the functions that can be computed by an algorithm.\n\nThe above ways of defining functions define them \"pointwise\", that is, each value is defined independently of the other values. This is not necessarily the case.\n\nWhen the domain of a function is the set of nonnegative integers or, more generally, when the domain is a well ordered set, a function may be defined by induction or recursion, meaning (roughly) that the calculation of the value of the function for some given input requires values of the function for \"lesser\" inputs. For example, the Fibonacci sequence is a function from the natural numbers into themselves that is defined by two starting values and a formula, recurring to the two immediately preceding arguments (see above for the use of indices for the argument of a function): \n\nIn calculus, the usual functions considered have extensive regularities. That is, the value of the function at a point is related to the values of the function at neighboring points. This allows defining them by functional equations (for example, the gamma function is the unique meromorphic function such that formula_48, and formula_49 for any complex that is not a non-positive integer), by differential equations (for example, the natural logarithm is the solution of the differential equation formula_50 such that ), by integral equations or by analytic continuation.\n\nAs functions may be complicated objects, it is often useful to draw the graph of a function for getting a global view of its properties. Some functions may also represented histograms\n\nGiven a function formula_51 its \"graph\" is, formally, the set \n\nIn the frequent case where and are subsets of the real numbers (or may be identified to such subsets), an element formula_53 may be identified with the point of coordinates in the Cartesian plane. Marking these points provides a drawing, generally a curve, that is also called the \"graph of the function\". For example the graph of the square function\n\nis a parabola that consists of all points of coordinates formula_55 for formula_56\n\nIt is possible to draw effectively the graph of a function only if the function is sufficiently regular, that is, either if the function is differentiable (or piecewise differentiable) or if its domain may be identified with the integers or a subset of the integers.\n\nIf either the domain or the codomain of the function is a subset of formula_57 the graph is a subset of a Cartesian space of higher dimension, and various technics have been developed for drawing it, including the use of colors for representing one of the dimensions.\n\nHistograms are often used for representing functions whose domain is finite, or is the natural numbers or the integers. In this case, an element of the domain is represented by an interval of the -axis, and a point of the graph is represented by a rectangle with basis the interval corresponding to and height .\n\nIn statistic, histogram are often used for representing very irregular functions. For example, for representing the function that associates his weight to each member of some population, one draws the histogram of the function that associates to each weight interval the number of people, whose weights belong to this interval. \n\nThere are many variants of this method, see Histogram for details.\n\nThis section describes general properties of functions, that are independent of specific properties of the domain and the codomain.\n\nSome functions are uniquely defined by their domain and codomain, and are sometimes called \"canonical\": \n\n\nTwo functions \"f\" and \"g\" are equal if their domain and codomain sets agree and their output values agree on the whole domain. Formally, \"f\"=\"g\" if \"f\"(\"x\")=\"g\"(\"x\") for all \"x\"∈\"X\", where \"f\":\"X\"→\"Y\" and \"g\":\"X\"→\"Y\".\n\nGiven two functions formula_61 and formula_62 such that the domain of is the codomain of , their \"composition\" is the function formula_63 defined by\n\nThat is, the value of formula_65 is obtained by first applying to to obtain and then applying to the result to obtain . In the notation the function that is applied first is always written on the right.\n\nThe composition formula_66 is an operation on functions that is defined only if the codomain of the first function is the domain of the second one. Even when both formula_65 and formula_68 satisfy these conditions, the composition is not necessarily commutative, that is, the functions formula_65 and formula_70 need not be equal, but may deliver different values for the same argument. For example, let and , then formula_71 and formula_72 agree just for formula_73\n\nThe function composition is associative in the sense that, if one of formula_74 and formula_75 is defined, then the other is also defined, and they are equal. Thus, one writes \n\nThe identity functions formula_60 and formula_78 are respectively a right identity and a left identity for functions from to . That is, if is a function with domain , and codomain , one has \nformula_79\n\nLet formula_80 The \"image\" by of an element of the domain is . If is any subset of , then the \"image\" of by , denoted is the subset of the codomain consisting of all images of elements of , that is,\n\nThe \"image\" of is the image of the whole domain, that is . It is also called the range of , although the term may also refer to the codomain.\n\nOn the other hand, the \"inverse image\", or \"preimage\" by of a subset of the codomain is the subset of the domain consisting of all elements of whose images belong to . It is denoted by formula_82 That is \nFor example, the preimage of {4, 9} under the square function is the set {−3,−2,2,3}. \n\nBy definition of a function, the image of an element of the domain is always a single element of the codomain. However, the preimage of a single element , denoted formula_84 may be empty or contain any number of elements. For example, if is the function from the integers to themselves that map every integer to 0, then . \n\nIf formula_61 is a function, and are subsets of , and and are subsets of , then one has the following properties:\n\nThe preimage by of an element of the codomain is sometimes called, in some contexts, the fiber of under . \n\nIf a function has an inverse (see below), this inverse is denoted formula_92 In this case formula_93 may denote either the image by formula_94 or the preimage by of . This is not a problem, as these sets are equal. The notation formula_95 and formula_93 may be ambiguous in the case of sets that contain some subsets as elements, such as formula_97 In this case, some care may be needed, for example, by using square brackets formula_98 for images and preimages of subsets, and ordinary parentheses for images and preimages of elements.\n\nLet formula_61 be a function.\n\nThe function is \"injective\" (or \"one-to-one\", or is an \"injection\") if for any two different elements and of . Equivalently, is injective if, for any formula_100 the preimage formula_101 contains at most one element. An empty function is always injective. If is not the empty set, and if, as usual, the axiom of choice is assumed, then is injective if and only if there exists a function formula_102 such that formula_103 that is, if has a left inverse. The axiom of choice is needed, because, if is injective, one defines by formula_104 if formula_105 and by formula_106, if formula_107 where formula_108 is an \"arbitrarily chosen\" element of .\n\nThe function is \"surjective\" (or \"onto\", or is a \"surjection\") if the range equals the codomain, that is, if . In other words, the preimage formula_101 of every formula_110 is nonempty. If, as usual, the axiom of choice is assumed, then is surjective if and only if there exists a function formula_102 such that formula_112 that is, if has a right inverse. The axiom of choice is needed, because, if is injective, one defines by formula_113 where formula_114 is an \"arbitrarily chosen\" element of formula_115\n\nThe function is \"bijective\" (or is \"bijection\" or a \"one-to-one correspondence\") if it is both injective and surjective. That is is bijective if, for any formula_100 the preimage formula_101 contains exactly one element. The function is bijective if and only if it admits an inverse function, that is a function formula_102 such that formula_103 and formula_120 (Contrarily to the case of injections and surjections, this does not require the axiom of choice.)\n\nEvery function formula_61 may be factorized as the composition of a surjection followed by an injection, where is the canonical surjection of onto , and is the canonical injection of into . This is the \"canonical factorization\" of .\n\n\"One-to-one\" and \"onto\" are terms that were more common in the older English language literature; \"injective\", \"surjective\", and \"bijective\" were originally coined as French words in the second quarter of the 20th century by the Bourbaki group and imported into English. As a word of caution, \"a one-to-one function\" is one that is injective, while a \"one-to-one correspondence\" refers to a bijective function. Also, the statement \" maps \"onto\" \" differs from \" maps \"into\" \" in that the former implies that is surjective), while the latter makes no assertion about the nature of the mapping. In a complicated reasoning, the one letter difference can easily be missed. Due to the confusing nature of this older terminology, these terms have declined in popularity relative to the Bourbakian terms, which have also the advantage to be more symmetrical.\n\nIf formula_61 is a function, and is a subset of , then the \"restriction\" of to , denoted , is the function from to that is defined by \n\nThis often used for define partial inverse functions: if there is a subset of a function such that is injective, then the canonical surjection of on its image is a bijection, which has an inverse function from to . This is in this way that inverse trigonometric functions are defined. The cosine function, for example, is injective, when restricted to the interval ; the image of this restriction is the interval ; this defines thus an inverse function from to , which is called arccosine and denoted .\n\nFunction restriction may also be used for \"gluing\" functions together: let formula_124 be the decomposition of as a union of subsets. Suppose that a function formula_125 is defined on each formula_126 such that, for each pair of indices, the restrictions of formula_127 and formula_128 to formula_129 are equal. Then, this defines a unique function formula_61 such that formula_131 for every . This is generally in this way that functions on manifolds are defined.\n\nAn \"extension\" of a function is a function such that is a restriction of . A typical use of this concept is the process of analytic continuation, that allows extending functions whose domain is a small part of the complex plane to functions whose domain is almost the whole complex plane.\n\nHere is another classical example of a function extension that is encountered when studying homographies of the real line. An \"homography\" is a function formula_132 such that . Its domain is the set of all real numbers different from formula_133 and its image is the set of all real numbers different from formula_134 If one extends the real line to the projectively extended real line by adding to the real numbers, one may extend for being a bijection of the extended real line to itself, by setting formula_135 and formula_136\n\nA multivariate function, or function of several variables is a function that depends on several arguments. Such functions are commonly encountered. For example, the position of a car on a road is a function of the time and its speed.\n\nMore formally, a function of variables is a function whose domain is a set of -tuples.\nFor example, multiplication of integers is a function of two variables, or bivariate function, whose domain is the set of all pairs (2-tuples) of integers, and whose codomain is the set of integers. The same is true for every binary operation. More generally, every mathematical operation is defined as a multivariate function.\n\nThe Cartesian product formula_137 of sets formula_138 is the set of all -tuples formula_139 such that formula_140 for every with formula_141. Therefore, a function of variables is a function\nwhere the domain has the form\nWhen using function notation, one usually omits the parentheses surrounding tuples, writing formula_144 instead of formula_145\n\nIn the case where all the formula_146 are equal to the set formula_147 of real numbers, one has a function of several real variables. If the formula_146 are equal to the set formula_149 of complex numbers, one has a function of several complex variables.\n\nIt is common to also consider functions whose codomain is a product of sets. For example, Euclidean division maps every pair of integers with to a pair of integers called the \"quotient\" and the \"remainder\":\nThe codomain may also be a vector space. In this case, one talks of a vector-valued function. If the domain is contained in a Euclidean space, or more generally a manifold, a vector-valued function is often called a vector field.\n\nThe idea of function, starting in the 17th century, was fundamental to the new infinitesimal calculus (see History of the function concept). At that time, only real-valued functions of a real variable were considered, and all functions were assumed to be smooth. But the definition was soon extended to functions of several variables and to function of a complex variable. In the second half of 19th century, the mathematically rigorous definition of a function was introduced, and functions with arbitrary domains and codomains were defined. \n\nFunctions are now used throughout all areas of mathematics. In introductory calculus, when the word \"function\" is used without qualification, it means a real-valued function of a single real variable. The more general definition of a function is usually introduced to second or third year college students with STEM majors, and in their senior year they are introduced to calculus in a larger, more rigorous setting in courses such as real analysis and complex analysis.\n\nA \"real function\" is a real-valued function of a real variable, that is, a function whose codomain is the field of real numbers and whose domain is a set of real numbers that contains an interval. In this section, these functions are simply called \"functions\".\n\nThe functions that are most commonly considered in mathematics and its applications have some regularity, that is they are continuous, differentiable, and even analytic. This regularity insures that these functions can be visualized by their graphs. In this section, all functions are differentiable in some interval.\n\nFunctions enjoy pointwise operations, that is, if and are functions, their sum, difference and product are functions defined by \nThe domains of the resulting functions are the intersection of the domains of and . The quotient of two functions is defined similarly by \nbut the domain of the resulting function is obtained by removing the zeros of from the intersection of the domains of and .\n\nThe polynomial functions are defined by polynomials, and their domain is the whole set of real numbers. They include constant functions, linear functions and quadratic functions. Rational functions are quotients of two polynomial functions, and their domain is the real numbers with a finite number of them removed to avoid division by zero. The simplest rational function is the function formula_153 whose graph is an hyperbola, and whose domain is the whole real line except for 0.\n\nThe derivative of a real differentiable function is a real function. An antiderivative of a continuous real function is a real function that is differentiable in any open interval in which the original function is continuous. For example, the function formula_154 is continuous, and even differentiable, on the positive real numbers. Thus one antiderivative, which takes the value zero for , is a differentiable function called the natural logarithm.\n\nA real function is monotonic in an interval if the sign of formula_155 does not depend of the choice of and in the interval. If the function is differentiable in the interval, it is monotonic if the sign of the derivative is constant in the interval. If a real function is monotonic in an interval , it has an inverse function, which is a real function with domain and image . This is how inverse trigonometric functions are defined in terms of trigonometric functions, where the trigonometric functions are monotonic. Another example: the natural logarithm is monotonic on the positive real numbers, and its image is the whole real line; therefore it has an inverse function that is a bijection between the real numbers and the positive real numbers. This inverse is the exponential function.\n\nMany other real functions are defined either by the implicit function theorem (the inverse function is a particular instance) or as solutions of differential equations. For example the sine and the cosine functions are the solutions of the linear differential equation \nsuch that \n\nWhen working with complex numbers different types of functions are used: \n\nThe study of complex functions is a vast subject in mathematics with many applications, and that can claim to be an ancestor to many other areas of mathematics, like homotopy theory, and manifolds.\n\nIn mathematical analysis, and more specifically in functional analysis, a function space is a set of scalar-valued or vector-valued functions, which share a specific property and form a topological vector space. For example, the real smooth functions with a compact support (that is, they are zero outside some compact set) form a function space that is at the basis of the theory of distributions.\n\nFunction spaces play a fundamental role in advanced mathematical analysis, by allowing the use of their algebraic and topological properties for studying properties of functions. For example, all theorems of existence and uniqueness of solutions of ordinary or partial differential equations result of the study of function spaces.\n\nIt is rather frequent that a function with domain may be naturally extended to a function whose domain is a set that is built from .\n\nFor example, for any set , its power set is the set of all subsets of . Any function formula_51 may be extended to a function on power sets by \nwhere is the image by of the subset of .\n\nAccording to the definition, a function maps each element from its domain to some element of its codomain . It is often convenient to extend this meaning to apply to arbitrary subsets of the domain, which are, as immediately can be checked, mapped to subsets of the codomain, thus considering a function formula_164 mapping its domain, the powerset () of 's domain , to its codomain, a subset of the powerset () of 's codomain .\nUnder slight abuse of notation this function on subsets is often denoted also by .\n\nAnother example is the following. If the function\nformula_166\nis a ring homomorphism, it may be extended to a function on polynomial rings\nwhich is also a ring homomorphism.\n\nSeveral methods for specifying functions of real or complex variables start from a local definition of the funcion at a point or on a neighbourhood of a point, and then extend by continuity the function to a much larger domain. Frequently, for a starting point formula_168 there are several possible starting values for the function. \n\nFor example, in defining the square root as the inverse function of the square function, for any positive real number formula_168 there are two choices for the value of the square root, one of which is positive and denoted formula_170 and another which is negative and denoted formula_171 These choices define two continuous functions, both having the nonnegative real numbers as a domain, and having either the nonnegative or the nonpositive real numbers as images. When looking at the graphs of these functions, one can see that, together, they form a single smooth curve. It is therefore often useful to consider these two square root functions as a single function that has two values for positive , one value for 0 and no value for negative .\n\nIn the preceding example, one choice, the positive square root, is more natural than the other. This is not the case in general. For example, let consider the implicit function that maps to a root of formula_172 (see the figure on the right). For one may choose either formula_173 for By the implicit function theorem, each choice defines a function; for the first one, the (maximal) domain is the interval and the image is ; for the second one, the domain is and the image is ; for the last one, the domain is and the image is . As the three graphs together form a smooth curve, and there is no reason for preferring one choice, these three functions are often considered as a single \"multi-valued function\" of that has three values for , and only one value for and .\n\nUsefulness of the concept of multi-valued functions is clearer when considering complex functions, typically analytic functions. The domain to which a complex function may be extended by analytic continuation generally consists of almost the whole complex plane. However, when extending the domain through two different paths, one often gets different values. For example, when extending the domain of the square root function, along a path of complex numbers with positive imaginary parts, one gets for the square root of –1; while, when extending through complex numbers with negative imaginary parts, one gets . There are generally two ways of solving the problem. One may define a function that is not continuous along some curve, called a branch cut. Such a function is called the principal value of the function. The other way is to consider that one has a \"multi-valued function\", which is analytic everywhere except for isolated singularities, but whose value may \"jump\" if one follows a closed loop around a singularity. This jump is called the monodromy.\n\nThe definition of a function that is given in this article requires the concept of set, since the domain and the codomain of a function must be a set. This is not a problem in usual mathematics, as it is generally not difficult to consider only functions whose domain and codomain are sets, which are well defined, even if the domain is not explicitly defined. However, it is sometimes useful to consider more general functions. \n\nFor example the singleton set may be considered as a function formula_174 Its domain would include all sets, and therefore would not be a set. In usual mathematics, one avoids this kind of problem by specifying a domain, which means that one has many singleton functions. However, when establishing foundations of mathematics, one may have to use functions whose domain, codomain or both are not specified, and some authors, often logicians, give precise definition for these weakly specified functions.\n\nThese generalized functions may be critical in the development of a formalization of foundations of mathematics. For example, the Von Neumann–Bernays–Gödel set theory, is an extension of the set theory in which the collection of all sets is a class. This theory includes the replacement axiom, which may be interpreted as \"if is a set, and is a function, then is a set\".\n\n\n\n"}
{"id": "22782409", "url": "https://en.wikipedia.org/wiki?curid=22782409", "title": "Gary Chartrand", "text": "Gary Chartrand\n\nGary Theodore Chartrand (born 1936) is an American-born mathematician who specializes in graph theory. He is known for his textbooks on introductory graph theory and for the concept of a\nhighly irregular graph.\n\nGary Chartrand was born in 1936. He was raised in Sault Ste. Marie, Michigan and attended J. W. Sexton High School located in Lansing, Michigan. As an undergraduate student, he initially majored in chemical engineering, but switched to mathematics in his junior year, in which he also became a member of the honorary mathematics society Pi Mu Epsilon.\n\nHe earned his B. S. from Michigan State University, where he majored in mathematics and minored in physical sciences and foreign languages. Michigan State University also awarded him a Master of Science and a PhD for his work in graph theory in 1964. Chartrand became the first doctoral student of Edward Nordhaus, and the first doctoral student at Michigan State University to research graph theory. His dissertation was \"Graphs and Their Associated Line-Graphs\". Chartrand worked with Frank Harary at the University of Michigan, where he spent a year as a Research Associate, and the two have published numerous papers together (along with other authors).\n\nThe topic of highly irregular graphs was introduced by Chartrand, Paul Erdős and Ortrud Oellermann.\n\nOther contributions that Chartrand has made involve dominating sets, distance in graphs, and graph coloring. During his career at Western Michigan University, he advised 22 doctoral students in their research on aspects of graph theory. Chartrand is currently a professor emeritus of mathematics at Western Michigan University.\n\n\n"}
{"id": "21630555", "url": "https://en.wikipedia.org/wiki?curid=21630555", "title": "Gerard of Brussels", "text": "Gerard of Brussels\n\nGerard of Brussels (, ) was an early thirteenth-century geometer and philosopher known primarily for his Latin book \"Liber de motu\" (\"On Motion\"), which was a pioneering study in kinematics, probably written between 1187 and 1260. It has been described as \"the first Latin treatise that was to take the fundamental approach to kinematics that was to characterize modern kinematics.\" He brought the works of Euclid and Archimedes back into popularity and was a direct influence on the Oxford Calculators (four kinematicists of Merton College) in the next century. Gerard is cited by Thomas Bradwardine in his \"Tractatus de proportionibus velocitatum\" (1328). His chief contribution was in moving away from Greek mathematics and closer to the notion of \"a ratio of two unlike quantities such as distance and time\", which is how modern physics defines velocity. \n\n\n"}
{"id": "38041703", "url": "https://en.wikipedia.org/wiki?curid=38041703", "title": "Gowers norm", "text": "Gowers norm\n\nIn mathematics, in the field of additive combinatorics, a Gowers norm or uniformity norm is a class of norm on functions on a finite group or group-like object which are used in the study of arithmetic progressions in the group. It is named after Timothy Gowers, who introduced it in his work on Szemerédi's theorem.\n\nLet \"f\" be a complex-valued function on a finite Abelian group \"G\" and let \"J\" denote complex conjugation. The Gowers \"d\"-norm is\n\nGowers norms are also defined for complex valued functions \"f\" on a segment \"[N]={0,1,2...,N-1}\", where \"N\" is a positive integer. In this context, the uniformity norm is given as formula_2, where formula_3 is a large integer, formula_4 denotes the indicator function of \"[N]\", and formula_5 is equal to formula_6 for formula_7 and formula_8 for all other formula_9. This definition does not depend on formula_3, as long as formula_11.\n\nAn \"inverse conjecture\" for these norms is a statement asserting that if a bounded function \"f\" has a large Gowers \"d\"-norm then \"f\" correlates with a polynomial phase of degree \"d-1\" or other object with polynomial behaviour (e.g. a \"(d-1)\"-step nilsequence). The precise statement depends on the Gowers norm under consideration.\n\nThe Inverse Conjecture for vector spaces over a finite field formula_12 asserts that for any formula_13 there exists a constant formula_14 such that for any finite dimensional vector space \"V\" over formula_12 and any complex valued function formula_16 on formula_17, bounded by \"1\", such that formula_18, there exists a polynomial sequence formula_19 such that \n\nwhere formula_21. This conjecture was proved to be true by Bergelson, Tao, and Ziegler.\n\nThe Inverse Conjecture for Gowers formula_22 norm asserts that for any formula_13, a finite collection of \"(d-1)\"-step \"nilmanifolds\" formula_24 and constants formula_25 can be found, so that the following is true. If formula_26 is a positive integer and formula_27 is bounded in absolute value by \"1\" and formula_28, then there exists a nilmanifold formula_29 and a nilsequence formula_30 where formula_31 and formula_32 bounded by \"1\" in absolute value and with Lipschitz constant bounded by formula_33 such that:\n\nThis conjecture was proved to be true by Green, Tao, and Ziegler. It should be stressed that the appearance of nilsequences in the above statement is necessary. The statement is no longer true if we only consider polynomial phases. \n\n"}
{"id": "25221990", "url": "https://en.wikipedia.org/wiki?curid=25221990", "title": "Ground axiom", "text": "Ground axiom\n\nIn set theory, the ground axiom was introduced by and . It states that the universe is not a nontrivial set forcing extension of an inner model.\n\n"}
{"id": "33751809", "url": "https://en.wikipedia.org/wiki?curid=33751809", "title": "Hermite's cotangent identity", "text": "Hermite's cotangent identity\n\nIn mathematics, Hermite's cotangent identity is a trigonometric identity discovered by Charles Hermite. Suppose \"a\", ..., \"a\" are complex numbers, no two of which differ by an integer multiple of . Let\n\n(in particular, \"A\", being an empty product, is 1). Then\n\nThe simplest non-trivial example is the case \"n\" = 2:\n"}
{"id": "56652586", "url": "https://en.wikipedia.org/wiki?curid=56652586", "title": "Highly powerful number", "text": "Highly powerful number\n\nIn elementary number theory, a highly powerful number is a positive integer that satisfies a property introduced by the Indo-Canadian mathematician Mathukumalli V. Subbarao. The set of highly powerful numbers is a proper subset of the set of powerful numbers.\n\nDefine prodex(1) = 1. Let formula_1 be a positive integer, such that formula_2, where formula_3 are formula_4 distinct primes in increasing order and formula_5 is a positive integer for formula_6. Define formula_7. The positive integer formula_1 is defined to be a highly powerful number if and only if, for every positive integer formula_9 implies that formula_10\n\nThe first 25 highly powerful numbers are: 1, 4, 8, 16, 32, 64, 128, 144, 216, 288, 432, 864, 1296, 1728, 2592, 3456, 5184, 7776, 10368, 15552, 20736, 31104, 41472, 62208, 86400. \n"}
{"id": "29549852", "url": "https://en.wikipedia.org/wiki?curid=29549852", "title": "Hilbert–Bernays provability conditions", "text": "Hilbert–Bernays provability conditions\n\nIn mathematical logic, the Hilbert–Bernays provability conditions, named after David Hilbert and Paul Bernays, are a set of requirements for formalized provability predicates in formal theories of arithmetic (Smith 2007:224).\n\nThese conditions are used in many proofs of Kurt Gödel's second incompleteness theorem. They are also closely related to axioms of provability logic.\n\nLet \"T\" be a formal theory of arithmetic with a formalized provability predicate Prov(\"n\"), which is expressed as a formula of \"T\" with one free number variable. For each formula φ in the theory, let #(φ) be the Gödel number of φ. The Hilbert–Bernays provability conditions are:\n\n\n"}
{"id": "18731310", "url": "https://en.wikipedia.org/wiki?curid=18731310", "title": "Horrocks bundle", "text": "Horrocks bundle\n\nIn algebraic geometry, Horrocks bundles are certain indecomposable rank 3 vector bundles (locally free sheaves) on 5-dimensional projective space, found by .\n\n"}
{"id": "22204279", "url": "https://en.wikipedia.org/wiki?curid=22204279", "title": "Indefinite product", "text": "Indefinite product\n\nIn mathematics, the indefinite product operator is the inverse operator of formula_1. It is a discrete version of the geometric integral of geometric calculus, one of the non-Newtonian calculi. Some authors use term discrete multiplicative integration\n\nThus\n\nMore explicitly, if formula_3, then\n\nIf \"F\"(\"x\") is a solution of this functional equation for a given \"f\"(\"x\"), then so is \"CF\"(\"x\") for any constant \"C\". Therefore, each indefinite product actually represents a family of functions, differing by a multiplicative constant.\n\nIf formula_5 is a period of function formula_6 then\n\nIndefinite product can be expressed in terms of indefinite sum:\n\nSome authors use the phrase \"indefinite product\" in a slightly different but related way to describe a product in which the numerical value of the upper limit is not given. e.g.\n\nThis is a list of indefinite products formula_13. Not all functions have an indefinite product which can be expressed in elementary functions.\n\n\n\n"}
{"id": "8298904", "url": "https://en.wikipedia.org/wiki?curid=8298904", "title": "International Centre for Mathematical Sciences", "text": "International Centre for Mathematical Sciences\n\nThe International Centre for Mathematical Sciences (ICMS) is a mathematical research centre based in Edinburgh. According to its website, the Centre is \"designed to bring together mathematicians and practitioners in science, industry and commerce for research workshops and other meetings.\" \n\nThe Centre was jointly established in 1990 by the University of Edinburgh and Heriot-Watt University, under the supervision of Professor Elmer Rees, with initial support from Edinburgh District Council, the Scottish Development Agency and the International Centre for Theoretical Physics. In April 1994 the Centre moved to 14 India Street, Edinburgh, the birthplace of James Clerk Maxwell and home of the James Clerk Maxwell Foundation. In 2010 it relocated to 15 South College Street to accommodate larger events. The current scientific director (appointed in 2016) is Professor Paul Glendinning.\n\n\n"}
{"id": "1034217", "url": "https://en.wikipedia.org/wiki?curid=1034217", "title": "Ioan James", "text": "Ioan James\n\nIoan Mackenzie James FRS (born 23 May 1928) is a British mathematician working in the field of topology particularly in homotopy theory. \nJames was born in Croydon, Surrey, England, and was educated at St Paul's School, London and Queen's College, Oxford. In 1953 He earned a D. Phil. from the University of Oxford for his thesis entitled \"Some problems in algebraic topology\", written under the direction of J. H. C. Whitehead. \n\nIn 1957 he was appointed reader in pure mathematics, a post which he held until 1969. From 1959 until 1969 he was a senior research fellow at St John's College. He held the Savilian Chair of Geometry at the University of Oxford from 1970 to 1995. He is now a professor emeritus. \n\nHe was elected a Fellow of the Royal Society in 1968. In 1978 the London Mathematical Society awarded him the Senior Whitehead Prize, which was established in honour of his doctoral supervisor, Whitehead. In 1984 he became President of the London Mathematical Society.\n\n\n\n"}
{"id": "43053125", "url": "https://en.wikipedia.org/wiki?curid=43053125", "title": "Iterated forcing", "text": "Iterated forcing\n\nIn mathematics, iterated forcing is a method for constructing models of set theory by repeating Cohen's forcing method a transfinite number of times. Iterated forcing was introduced by in their construction of a model of set theory with no Suslin tree. They also showed that iterated forcing can construct models where Martin's axiom holds and the continuum is any given regular cardinal.\n\nIn iterated forcing, one has a transfinite sequence \"P\" of forcing notions indexed by some ordinals α, which give a family of Boolean-valued models \"V\". If α+1 is a successor ordinal then \"P\" is often constructed from \"P\" using a forcing notion in \"V\", while if α is a limit ordinal then \"P\" is often constructed as some sort of limit (such as the direct limit) of the \"P\" for β<α.\n\nA key consideration is that, typically, it is necessary that formula_1 is not collapsed. This is often accomplished by the use of a preservation theorem such as:\n\n+ Finite support iteration of c.c.c. forcings (see countable chain condition) are c.c.c. and thus preserve formula_1.\n\n+ Countable support iterations of proper forcings are proper (see Fundamental Theorem of Proper Forcing) and thus preserve formula_1.\n\n+ Revised countable support iterations of semi-proper forcings are semi-proper and thus preserve formula_1.\n\nSome non-semi-proper forcings, such as Namba forcing, can be iterated with appropriate cardinal collapses while preserving formula_1 using methods developed by Saharon Shelah\n\n"}
{"id": "36618866", "url": "https://en.wikipedia.org/wiki?curid=36618866", "title": "Katydid sequence", "text": "Katydid sequence\n\nThe Katydid sequence is a sequence of numbers first defined in Clifford A. Pickover's book \"Wonders of Numbers\" (2001).\n\nIt is the smallest sequence of integers that can be reached from 1 by a sequence of the two operations \"n\" ↦ 2\"n\" + 2 and 7\"n\" + 7 (in any order). For instance, applying the first operation to 1 produces the number 4, and applying the second operation to 4 produces the number 35, both of which are in the sequence.\n\nThe first 10 elements of the sequence are:\n\nPickover asked whether there exist numbers that can be reached by more than one sequence of operations.\nThe answer is yes. For instance, 1814526 can be reached by the two sequences\n1 – 4 – 10 – 22 – 46 – 329 – 660 – 4627 – 9256 – 18514 – 37030 – 259217 – 1814526 and\n1 – 14 – 30 – 62 – 441 – 884 – 1770 – 3542 – 7086 – 14174 – 28350 – 56702 – 113406 – 226814 – 453630 – 907262 – 1814526\n"}
{"id": "34589275", "url": "https://en.wikipedia.org/wiki?curid=34589275", "title": "Lill's method", "text": "Lill's method\n\nIn mathematics, Lill's method is a visual method of finding the real roots of polynomials of any degree. It was developed by Austrian engineer Eduard Lill in 1867. A later paper by Lill dealt with the problem of complex roots.\n\nLill's method involves expressing the coefficients of a polynomial as magnitudes of segments at right angles to each other, starting from the origin, creating a path to a terminus, then finding a non-right angle path from the start to the terminus reflecting or refracting on the lines of the first path.\n\nTo employ the method a diagram is drawn starting at the origin. A line segment is drawn rightwards by the magnitude of the first coefficient (the coefficient of the highest-power term) (so that with a negative coefficient the segment will end left of the origin). From the end of the first segment another segment is drawn upwards by the magnitude of the second coefficient, then left by the magnitude of the third, and down by the magnitude of the fourth, and so on. The sequence of directions (not turns) is always rightward, upward, leftward, downward, then repeating itself. Thus each turn is counterclockwise. The process continues for every coefficient of the polynomial including zeroes, with negative coefficients \"walking backwards\". The final point reached, at the end of the segment corresponding to the equation's constant term, is the terminus.\n\nA line is then launched from the origin at some angle , reflected off of each line segment at a right angle (not necessarily the \"natural\" angle of reflection), and refracted at a right angle through the line through each segment (including a line for the zero coefficients) when the angled path does not hit the line segment on that line. The vertical and horizontal lines are reflected off or refracted through in the following sequence: the line containing the segment corresponding to the coefficient of formula_1 then of formula_2 etc. Choosing so that the path lands on the terminus, the negative of the tangent of is a root of this polynomial. For every real zero of the polynomial there will be one unique initial angle and path that will land on the terminus. A quadratic with two real roots, for example, will have exactly two angles that satisfy the above conditions.\n\nThe construction in effect evaluates the polynomial according to Horner's method. For the polynomial formula_3 the values of formula_4, formula_5, formula_6 are successively generated. A solution line giving a root is similar to the Lill's construction for the polynomial with that root removed.\n\nIn 1936 Margharita P. Beloch showed how Lill's method could be adapted to solve cubic equations using paper folding. If simultaneous folds are allowed then any \"n\"th degree equation with a real root can be solved using \"n\"–2 simultaneous folds.\n\n\n"}
{"id": "53723727", "url": "https://en.wikipedia.org/wiki?curid=53723727", "title": "Linkurious", "text": "Linkurious\n\nLinkurious is a French software company that provides social network analysis primarily through graph visualization.\n\nLinkurious was founded in 2013 by Sébastien Heymann, David Rapin and Jean Villedieu following the development of Gephi, which was inspired by the prototype for Stanford’s Center for Spatial and Textual Analysis project \"Mapping the Republic of Letters\" and looked at connections across thousands of communities in Europe and North America during The Enlightenment.\n\nLinkurious Enterprise provides search and visualization capabilities for various graph databases such as Neo4j, TitanDB, DataStax and AllegroGraph.\n\nLinkurious' graph visualization tool is used for NASA's Lessons Learned database, identifying connections between seemingly unlikely subjects, such as a correlation between contaminated fluid and battery fire risk.\n\nThe International Consortium of Investigative Journalists used a commercial version of Linkurious and Neo4j in the investigation of the Panama papers, uncovering 4.8 million leaked files consisting of emails, 3 million database entries, 2.2 million PDFs, 1.2 million images, 320,000 text files, and 2242 files, evidence of money laundering, tax evasion or political corruption.\n\nICIJ also utilized the software during the Swiss Leaks investigation that revealed a massive tax evasion scheme in which 180.6 billion euros passed through HSBC accounts.\n\n- Arcade Analytics\n"}
{"id": "753756", "url": "https://en.wikipedia.org/wiki?curid=753756", "title": "Lissajous curve", "text": "Lissajous curve\n\nIn mathematics, a Lissajous curve , also known as Lissajous figure or Bowditch curve , is the graph of a system of parametric equations\n\nwhich describe complex harmonic motion. This family of curves was investigated by Nathaniel Bowditch in 1815, and later in more detail by Jules Antoine Lissajous in 1857.\n\nThe appearance of the figure is highly sensitive to the ratio . For a ratio of 1, the figure is an ellipse, with special cases including circles (, radians) and lines (). Another simple Lissajous figure is the parabola (, ). Other ratios produce more complicated curves, which are closed only if is rational. The visual form of these curves is often suggestive of a three-dimensional knot, and indeed many kinds of knots, including those known as Lissajous knots, project to the plane as Lissajous figures.\n\nVisually, the ratio determines the number of \"lobes\" of the figure. For example, a ratio of or produces a figure with three major lobes (see image). Similarly, a ratio of produces a figure with five horizontal lobes and four vertical lobes. Rational ratios produce closed (connected) or \"still\" figures, while irrational ratios produce figures that appear to rotate. The ratio determines the relative width-to-height ratio of the curve. For example, a ratio of produces a figure that is twice as wide as it is high. Finally, the value of determines the apparent \"rotation\" angle of the figure, viewed as if it were actually a three-dimensional curve. For example, produces and components that are exactly in phase, so the resulting figure appears as an apparent three-dimensional figure viewed from straight on (0°). In contrast, any non-zero produces a figure that appears to be rotated, either as a left–right or an up–down rotation (depending on the ratio ).\n\nLissajous figures where , ( is a natural number) and\n\nare Chebyshev polynomials of the first kind of degree . This property is exploited to produce a set of points, called Padua points, at which a function may be sampled in order to compute either a bivariate interpolation or quadrature of the function over the domain .\n\nThe relation of some Lissajous curves to Chebyshev polynomials is clearer to understand if the Lissajous curve which generates each of them is expressed using cosine functions rather than sine functions.\n\nThe animation shows the curve adaptation with continuously increasing fraction from 0 to 1 in steps of 0.01 ().\n\nBelow are examples of Lissajous figures with , an odd natural number , an even natural number , and .\n\nPrior to modern electronic equipment, Lissajous curves could be generated mechanically by means of a harmonograph.\n\nLissajous curves can also be generated using an oscilloscope (as illustrated). An octopus circuit can be used to demonstrate the waveform images on an oscilloscope. Two phase-shifted sinusoid inputs are applied to the oscilloscope in X-Y mode and the phase relationship between the signals is presented as a Lissajous figure.\n\nIn the professional audio world, this method is used for realtime analysis of the phase relationship between the left and right channels of a stereo audio signal. On larger, more sophisticated audio mixing consoles an oscilloscope may be built-in for this purpose.\n\nOn an oscilloscope, we suppose is CH1 and is CH2, is the amplitude of CH1 and is the amplitude of CH2, is the frequency of CH1 and is the frequency of CH2, so is the ratio of frequencies of the two channels, and is the phase shift of CH1.\n\nA purely mechanical application of a Lissajous curve with , is in the driving mechanism of the Mars Light type of oscillating beam lamps popular with railroads in the mid-1900s. The beam in some versions traces out a lopsided figure-8 pattern on its side.\n\nWhen the input to an LTI system is sinusoidal, the output is sinusoidal with the same frequency, but it may have a different amplitude and some phase shift. Using an oscilloscope that can plot one signal against another (as opposed to one signal against time) to plot the output of an LTI system against the input to the LTI system produces an ellipse that is a Lissajous figure for the special case of . The aspect ratio of the resulting ellipse is a function of the phase shift between the input and output, with an aspect ratio of 1 (perfect circle) corresponding to a phase shift of ±90° and an aspect ratio of ∞ (a line) corresponding to a phase shift of 0° or 180°.\n\nThe figure below summarizes how the Lissajous figure changes over different phase shifts. The phase shifts are all negative so that delay semantics can be used with a causal LTI system (note that −270° is equivalent to +90°). The arrows show the direction of rotation of the Lissajous figure.\n\nA Lissajous curve is used in experimental tests to determine if a device may be properly categorized as a memristor.\n\nLissajous figures were sometimes displayed on oscilloscopes meant to simulate high-tech equipment in science-fiction TV shows and movies in the 1960s and 1970s.\n\nThe title sequence by John Whitney for Alfred Hitchcock's 1958 film \"Vertigo\" is based on Lissajous figures.\n\nIn a sequence towards the end of an episode of Columbo entitled \"Make me a Perfect Murder\", the detective sits watching Lissajous curves displayed to music on monitors in a TV outside broadcast van.\n\nLissajous figures are sometimes used in graphic design as logos. Examples include:\n\n\n\n\n"}
{"id": "24364843", "url": "https://en.wikipedia.org/wiki?curid=24364843", "title": "List of perfect numbers", "text": "List of perfect numbers\n\nThe following is a list of the known perfect numbers, and the exponents \"p\" that can be used to generate them (using the expression 2× (2 − 1)) whenever 2 − 1 is a Mersenne prime. All even perfect numbers are of this form. It is not known whether there are any odd perfect numbers. there are 50 known perfect numbers in total. The ratio \"p\" / digits approaches log(10) / log(4) = 1.6609640474...\n\nThe displayed ranks are among those perfect numbers which are known . Some ranks may change later if smaller perfect numbers are discovered. It is known there is no odd perfect number below 10. GIMPS reported that by 8 April 2018 the search for Mersenne primes (and thereby even perfect numbers) became exhaustive up to the 47th above.\n\n"}
{"id": "313384", "url": "https://en.wikipedia.org/wiki?curid=313384", "title": "Long division", "text": "Long division\n\nIn arithmetic, long division is a standard division algorithm suitable for dividing multidigit numbers that is simple enough to perform by hand. It breaks down a division problem into a series of easier steps. As in all division problems, one number, called the dividend, is divided by another, called the divisor, producing a result called the quotient. It enables computations involving arbitrarily large numbers to be performed by following a series of simple steps. The abbreviated form of long division is called short division, which is almost always used instead of long division when the divisor has only one digit. Chunking (also known as the partial quotients method or the hangman method) is a less-efficient form of long division which may be easier to understand.\n\nWhile related algorithms have existed since the 12th century AD, the specific algorithm in modern use was introduced by Henry Briggs 1600 AD.\n\nInexpensive calculators and computers have become the most common way to solve division problems, eliminating a traditional mathematical exercise, and decreasing the educational opportunity to show how to do so by paper and pencil techniques. (Internally, those devices use one of a variety of division algorithms). In the United States, long division has been especially targeted for de-emphasis, or even elimination from the school curriculum, by reform mathematics, though traditionally introduced in the 4th or 5th grades.\n\nIn English-speaking countries, long division does not use the division slash or obelus signs but instead constructs a tableau. The divisor is separated from the dividend by a right parenthesis or vertical bar ; the dividend is separated from the quotient by a vinculum (i.e., overbar). The combination of these two symbols is sometimes known as a long division symbol or division bracket. It developed in the 18th century from an earlier single-line notation separating the dividend from the quotient by a left parenthesis.\n\nThe process is begun by dividing the left-most digit of the dividend by the divisor. The quotient (rounded down to an integer) becomes the first digit of the result, and the remainder is calculated (this step is notated as a subtraction). This remainder carries forward when the process is repeated on the following digit of the dividend (notated as 'bringing down' the next digit to the remainder). When all digits have been processed and no remainder is left, the process is complete.\n\nAn example is shown below, representing the division of 500 by 4 (with a result of 125).\nIn the above example, the first step is to find the shortest sequence of digits starting from the left end of the dividend, 500, that the divisor 4 goes into at least once; this shortest sequence in this example is simply the first digit, 5. The largest number that the divisor 4 can be multiplied by without exceeding 5 is 1, so the digit 1 is put above the 5 to start constructing the quotient. Next, the 1 is multiplied by the divisor 4, to obtain the largest whole number (4 in this case) that is a multiple of the divisor 4 without exceeding the 5; this product of 1 times 4 is 4, so 4 is placed underneath the 5. Next the 4 under the 5 is subtracted from the 5 to get the remainder, 1, which is placed under the 4 under the 5. This remainder 1 is necessarily smaller than the divisor 4. Next the first as-yet unused digit in the dividend, in this case the first digit 0 after the 5, is copied directly underneath itself and next to the remainder 1, to form the number 10. At this point the process is repeated enough times to reach a stopping point: The largest number by which the divisor 4 can be multiplied without exceeding 10 is 2, so 2 is written above the 0 that is next to the 5 – that is, directly above the last digit in the 10. Then the latest entry to the quotient, 2, is multiplied by the divisor 4 to get 8, which is the largest multiple of 4 that does not exceed 10; so 8 is written below 10, and the subtraction 10 minus 8 is performed to get the remainder 2, which is placed below the 8. This remainder 2 is necessarily smaller than the divisor 4. The next digit of the dividend (the last 0 in 500) is copied directly below itself and next to the remainder 2, to form 20. Then the largest number by which the divisor 4 can be multiplied without exceeding 20 is ascertained; this number is 5, so 5 is placed above the last dividend digit that was brought down (i.e., above the rightmost 0 in 500). Then this new quotient digit 5 is multiplied by the divisor 4 to get 20, which is written at the bottom below the existing 20. Then 20 is subtracted from 20, yielding 0, which is written below the 20. We know we are done now because two things are true: there are no more digits to bring down from the dividend, and the last subtraction result was 0.\n\nIf the last remainder when we ran out of dividend digits had been something other than 0, there would have been two possible courses of action. (1) We could just stop there and say that the dividend divided by the divisor is the quotient written at the top with the remainder written at the bottom; equivalently we could write the answer as the quotient followed by a fraction that is the remainder divided by the divisor. Or, (2) we could extend the dividend by writing it as, say, 500.000... and continue the process (using a decimal point in the quotient directly above the decimal point in the dividend), in order to get a decimal answer, as in the following example.\n\nIn this example, the decimal part of the result is calculated by continuing the process beyond the units digit, \"bringing down\" zeros as being the decimal part of the dividend.\n\nThis example also illustrates that, at the beginning of the process, a step that produces a zero can be omitted. Since the first digit 1 is less than the divisor 4, the first step is instead performed on the first two digits 12. Similarly, if the divisor were 13, one would perform the first step on 127 rather than 12 or 1.\n\n\nA divisor of any number of digits can be used. In this example, 1260257 is to be divided by 37. First the problem is set up as follows:\n\nDigits of the number 1260257 are taken until a number greater than or equal to 37 occurs. So 1 and 12 are less than 37, but 126 is greater. Next, the greatest multiple of 37 less than or equal to 126 is computed. So 3 × 37 = 111 < 126, but 4 × 37 > 126. The multiple 111 is written underneath the 126 and the 3 is written on the top where the solution will appear:\n\nNote carefully which place-value column these digits are written into. The 3 in the quotient goes in the same column (ten-thousands place) as the 6 in the dividend 1260257, which is the same column as the last digit of 111.\n\nThe 111 is then subtracted from the line above, ignoring all digits to the right:\n\nNow the digit from the next smaller place value of the dividend is copied down appended to the result 15:\n\nThe process repeats: the greatest multiple of 37 less than or equal to 150 is subtracted. This is 148 = 4 × 37, so a 4 is added to the solution line. Then the result of the subtraction is extended by another digit taken from the dividend:\n\nThe greatest multiple of 37 less than or equal to 22 is 0 × 37 = 0. Subtracting 0 from 22 gives 22, we often don't write the subtraction step. Instead, we simply take another digit from the dividend:\n\nThe process is repeated until 37 divides the last line exactly:\n\nFor non-decimal currencies (such as the British £sd system before 1971) and measures (such as avoirdupois) mixed mode division must be used. Consider dividing 50 miles 600 yards into 37 pieces:\n\nEach of the four columns is worked in turn. Starting with the miles: 50/37 = 1 remainder 13. No further division is\npossible, so perform a long multiplication by 1,760 to convert miles to yards, the result is 22,880 yards. Carry this to the top of the yards column and add it to the 600 yards in the dividend giving 23,480. Long division of 23,480 / 37 now proceeds as normal yielding 634 with remainder 22. The remainder is multiplied by 3 to get feet and carried up to the feet column. Long division of the feet gives 1 remainder 29 which is then multiplied by twelve to get 348 inches. Long division continues with the final remainder of 15 inches being shown on the result line.\n\nThe same method and layout can be used for, e.g., binary, octal and hexadecimal numeral systems. For example, a hexadecimal address range of 0xf412df divided into 0x12 parts is:\n\nCalculation within the binary number system is more immediate, because each digit in the course can only be 1 or 0:\n\nWhen the quotient is not an integer and the division process is extended beyond the decimal point, one of two things can happen. (1) The process can terminate, which means that a remainder of 0 is reached; or (2) a remainder could be reached that is identical to a previous remainder that occurred after the decimal points were written. In the latter case, continuing the process would be pointless, because from that point onward the same sequence of digits would appear in the quotient over and over. So a bar is drawn over the repeating sequence to indicate that it repeats forever.\n\nChina, Japan, Korea use the same notation as English-speaking nations including India. Elsewhere, the same general principles are used, but the figures are often arranged differently.\n\nIn Latin America (except Argentina, Bolivia, Mexico, Colombia, Paraguay, Venezuela, Uruguay and Brazil), the calculation is almost exactly the same, but is written down differently as shown below with the same two examples used above. Usually the quotient is written under a bar drawn under the divisor. A long vertical line is sometimes drawn to the right of the calculations.\n\nand\n\nIn Mexico, the US notation is used, except that only the result of the subtraction is annotated and the calculation is done mentally, as shown below:\n\nIn Bolivia, Brazil, Paraguay, Venezuela, Uruguay, Quebec, Colombia, and Peru, the European notation (see below) is used, except that the quotient is not separated by a vertical line, as shown below:\n\nSame procedure applies in Mexico and Argentina, only the result of the subtraction is annotated and the calculation is done mentally.\n\nIn Spain, Italy, France, Portugal, Lithuania, Romania, Turkey, Greece, Belgium, Belarus, Ukraine, and Russia, the divisor is to the right of the dividend, and separated by a vertical bar. The division also occurs in the column, but the quotient (result) is written below the divider, and separated by the horizontal line. The same method is used in Iran and Mongolia.\n\nIn Cyprus, as well as in France, a long vertical bar separates the dividend and subsequent subtractions from the quotient and divisor, as in the below of 6359 divided by 17, which is 374 with a remainder of 1.\n\nDecimal numbers are not divided directly, the dividend and divisor are multiplied by a power of ten so that the division involves two whole numbers. Therefore, if one were dividing 12,7 by 0,4 (commas being used instead of decimal points), the dividend and divisor would first be changed to 127 and 4, and then the division would proceed as above.\n\nIn Austria, Germany and Switzerland, the notational form of a normal equation is used. <dividend> : <divisor> = <quotient>, with the colon \":\" denoting a binary infix symbol for the division operator (analogous to \"/\" or \"÷\"). In these regions the decimal separator is written as a comma. (cf. first section of Latin American countries above, where it's done virtually the same way):\n\nThe same notation is adopted in Denmark, Norway, Bulgaria, Macedonia, Poland, Croatia, Slovenia, Hungary, Czech Republic, Slovakia, Vietnam and in Serbia.\n\nIn the Netherlands, the following notation is used:\n\nLong division of integers can easily be extended to include non-integer dividends, as long as they are rational. This is because every rational number has a recurring decimal expansion. The procedure can also be extended to include divisors which have a finite or terminating decimal expansion (i.e. decimal fractions). In this case the procedure involves multiplying the divisor and dividend by the appropriate power of ten so that the new divisor is an integer – taking advantage of the fact that \"a\" ÷ \"b\" = (\"ca\") ÷ (\"cb\") – and then proceeding as above.\n\nA generalised version of this method called polynomial long division is also used for dividing polynomials (sometimes using a shorthand version called synthetic division).\n\n\n"}
{"id": "26874", "url": "https://en.wikipedia.org/wiki?curid=26874", "title": "Metric prefix", "text": "Metric prefix\n\nA metric prefix is a unit prefix that precedes a basic unit of measure to indicate a multiple or fraction of the unit. While all metric prefixes in common use today are decadic, historically there have been a number of binary metric prefixes as well. Each prefix has a unique symbol that is prepended to the unit symbol. The prefix \"kilo-\", for example, may be added to \"gram\" to indicate \"multiplication\" by one thousand: one kilogram is equal to one thousand grams. The prefix \"milli-\", likewise, may be added to \"metre\" to indicate \"division\" by one thousand; one millimetre is equal to one thousandth of a metre.\n\nDecimal multiplicative prefixes have been a feature of all forms of the metric system, with six dating back to the system's introduction in the 1790s. Metric prefixes have even been prepended to non-metric units. The SI prefixes are standardized for use in the International System of Units (SI) by the International Bureau of Weights and Measures (BIPM) in resolutions dating from 1960 to 1991. Since 2009, they have formed part of the International System of Quantities.\n\nThe BIPM specifies twenty prefixes for the International System of Units (SI).\n\nEach prefix name has a symbol that is used in combination with the symbols for units of measure. For example, the symbol for \"kilo-\" is 'k', and is used to produce 'km', 'kg', and 'kW', which are the SI symbols for kilometre, kilogram, and kilowatt, respectively. Where Greek letters are unavailable, the symbol for micro 'µ' is commonly replaced by 'u'.\n\nPrefixes corresponding to an integer power of one thousand are generally preferred. Hence \"100 m\" is preferred over \"1 hm\" (hectometre) or \"10 dam\" (decametres). The prefixes hecto, deca, deci, and centi are commonly used for everyday purposes, and the centimetre (cm) is especially common. However, some modern building codes require that the millimetre be used in preference to the centimetre, because \"use of centimetres leads to extensive usage of decimal points and confusion\".\n\nPrefixes may not be used in combination. This also applies to mass, for which the SI base unit (kilogram) already contains a prefix. For example, milligram (mg) is used instead of microkilogram (µkg).\n\nIn the arithmetic of measurements having units, the units are treated as multiplicative factors to values. If they have prefixes, all but one of the prefixes must be expanded to their numeric multiplier, except when combining values with identical units. Hence,\n\nWhen units occur in exponentiation, for example, in square and cubic forms, the multiplication prefix must be considered part of the unit, and thus included in the exponentiation.\n\n\nThe use of prefixes can be traced back to the introduction of the metric system in the 1790s, long before the 1960 introduction of the SI. The prefixes, including those introduced after 1960, are used with any metric unit, whether officially included in the SI or not (e.g., millidynes and milligauss). Metric prefixes may also be used with non-metric units.\n\nThe choice of prefixes with a given unit is usually dictated by convenience of use. Unit prefixes for amounts that are much larger or smaller than those actually encountered are seldom used.\n\nIn use, the kilogram, gram, milligram, microgram, and smaller are fairly common. However, megagram (and gigagram, teragram, etc.) are rarely used; tonnes (and kilotonnes, megatonnes, etc. – although these units generally are not used as a measure of mass \"per se\", but rather TNT energy equivalent of a mass) or scientific notation are used instead. Megagram is occasionally used to disambiguate the metric tonne from the various non-metric tons. An exception is pollution emission rates, which are typically on the order of Tg/yr. Sometimes, only one element or compound is denoted for an emission, such as Tg C/yr or Tg N/yr.\n\nAlone among SI units, the base unit of mass, the kilogram, already includes a prefix. The prefixes consequently do not indicate corresponding multipliers of the base unit in the case of mass; for example, a megagram is  kg, whereas \"mega-\" indicates a multiplier of .\n\nThe litre (equal to a cubic decimetre), millilitre (equal to a cubic centimetre), microlitre, and smaller are common. In Europe, the centilitre is often used for packaged products (such as wine) and the decilitre less frequently. (The latter two items include prefixes corresponding to an exponent that is not divisible by three.)\n\nLarger volumes are usually denoted in kilolitres, megalitres or gigalitres, or else in cubic metres (1 cubic metre = 1 kilolitre) or cubic kilometres (1 cubic kilometre = 1 teralitre). For scientific purposes, the cubic metre is usually used.\n\nThe kilometre, metre, centimetre, millimetre, and smaller are common. (However, the decimetre is rarely used.) The micrometre is often referred to by the non-SI term \"micron\". In some fields, such as chemistry, the ångström (equal to 0.1 nm) historically competed with the nanometre. The femtometre, used mainly in particle physics, is sometimes called a fermi. For large scales, megametre, gigametre, and larger are rarely used. Instead, non-metric units are used, such as astronomical units, light years, and parsecs; the astronomical unit is mentioned in the SI standards as an accepted non-SI unit.\n\nThe second, millisecond, microsecond, and shorter are common. The kilosecond and megasecond also have some use, though for these and longer times one usually uses either scientific notation or minutes, hours, and so on.\n\nOfficial policies about the use of these prefixes vary slightly between the Bureau International des Poids et Mesures (BIPM) and the American National Institute of Standards and Technology (NIST); and some of the policies of both bodies are at variance with everyday practice. For instance, the NIST advises that \"to avoid confusion, prefix symbols (and prefix names) are not used with the time-related unit symbols (names) min (minute), h (hour), d (day); nor with the angle-related symbols (names) ° (degree), ′ (minute), and ″ (second)\".\n\nThe BIPM's position on the use of SI prefixes with units of time larger than the second is the same as that of the NIST, but their position with regard to angles differs: they state \"However astronomers use milliarcsecond, which they denote mas, and microarcsecond, µas, which they use as units for measuring very small angles.\" The SI unit of angle is the radian, but, as mentioned above, degrees, minutes and seconds see some scientific use.\n\nOfficial policy also varies from common practice for the degree Celsius (°C). NIST states: \"Prefix symbols may be used with the unit symbol °C and prefix names may be used with the unit name 'degree Celsius'. For example, 12 m°C (12 millidegrees Celsius) is acceptable.\" In practice, it is more common for prefixes to be used with the kelvin when it is desirable to denote extremely large or small absolute temperatures or temperature differences. Thus, temperatures of star interiors may be given in units of MK (megakelvins), and molecular cooling may be described in mK (millikelvins).\n\nIn use the joule and kilojoule are common, with larger multiples seen in limited contexts. In addition, the kilowatt hour, a composite unit formed from the kilowatt and hour, is often used for electrical energy; other multiples can be formed by modifying the prefix of watt (e.g. terawatt hour).\n\nThere exist a number of definitions for the non-SI unit, the calorie. There are gram calories and kilogram calories. One kilogram calorie, which equals one thousand gram calories, often appears capitalized and without a prefix (i.e. 'Cal') when referring to \"dietary calories\" in food. It is common to apply metric prefixes to the gram calorie, but not to the kilogram calorie: thus, 1 kcal = 1000 cal = 1 Cal.\n\nMetric prefixes are widely used outside the system of metric units. Common examples include the megabyte and the decibel. Metric prefixes rarely appear with imperial or US units except in some special cases (e.g., microinch, kilofoot, kilopound or 'kip'). They are also used with other specialized units used in particular fields (e.g., megaelectronvolt, gigaparsec, millibarn). They are also occasionally used with currency units (e.g., gigadollar), mainly by people who are familiar with the prefixes from scientific usage. In geology and paleontology, the year, with symbol a (from the Latin \"annus\"), is commonly used with metric prefixes: ka, Ma, and Ga.\n\nWhen an SI prefix is affixed to a root word, the prefix carries the stress, while the root drops its stress but retains a full vowel in the syllable that is stressed when the root word stands alone. For example, \"kilobyte\" is , with stress on the first syllable. However, words in common use outside the scientific community may follow idiosyncratic stress rules. In English speaking countries, \"kilometre\" is often pronounced , with reduced vowels on both syllables of \"metre\".\n\nThe prefix \"giga\" is usually pronounced in English as , with hard 〈g〉 as in \"get\", but sometimes , with soft 〈g〉 as in \"gin\".\n\nThe LaTeX typesetting system features an \"SIunitx\" package in which the units of measurement are spelled out, for example, codice_1 formats as \"3 THz\".\n\nSome of the prefixes formerly used in the metric system have fallen into disuse and were not adopted into the SI. The decimal prefix \"myria-\" (sometimes also written as \"myrio-\") (ten thousand) as well as the binary prefixes \"double-\" and \"demi-\", denoting a factor of 2 and (one half), respectively, were parts of the original metric system adopted by France in 1795. These were not retained when the SI prefixes were internationally adopted by the 11th CGPM conference in 1960.\n\nOther metric prefixes used historically include hebdo- (10) and micri- (10).\n\nDouble prefixes have been used in the past, such as \"micromillimetres\" or \"millimicrons\" (now nanometres), \"micromicrofarads\" (now picofarads), \"kilomegatons\" (now gigatons), \"hectokilometres\" (now 100 kilometres) and the derived adjective \"hectokilometric\" (typically used for qualifying the fuel consumption measures). These were disallowed with the introduction of the SI.\n\nOther obsolete double prefixes included \"decimilli-\" (10), which was contracted to \"dimi-\" and standardized in France up to 1961.\n\nIn 2010, UC Davis student Austin Sendek started a petition to designate \"hella\" as the SI prefix for one octillion (Short scale; Long scale: Quadrilliard; 10). The petition gathered over 60,000 supporters by circulating through Facebook and receiving a significant amount of media coverage. Although the Consultative Committee for Units considered the proposal, it was rejected. However, \"hella\" has been adopted by certain websites, such as Google Calculator and Wolfram Alpha.\n\nBrian C. Lacki follows Z and Y with the adopted prefixes X, W and V to mean , and respectively, thus continuing the inverse alphabetical order.\n\nIn written English, the symbol \"K\" is often used informally to indicate a multiple of thousand in many contexts. For example, one may talk of a \"40K salary\" (), or call the Year 2000 problem the \"Y2K problem\". In these cases, an uppercase K is often used with an implied unit (although it could then be confused with the symbol for the kelvin temperature unit if the context is unclear). This informal postfix is read or spoken as \"thousand\" or \"grand\", or just \"k\", but never \"kilo\" (despite that being the origin of the letter).\n\nThe financial and general news media mostly use m/M, b/B and t/T as abbreviations for million, billion (10) and trillion (10), respectively, for large quantities, typically currency and population.\n\nThe medical and automotive fields in the United States use the abbreviations \"cc\" or \"ccm\" for cubic centimetres. 1 cubic centimetre is equivalent to 1 millilitre.\n\nFor nearly a century, the electrical construction industry used the abbreviation \"MCM\" to designate a \"thousand circular mils\" in specifying thicknesses of large electrical cables. Since the mid-1990s, \"kcmil\" has been adopted as the \"official\" designation of a thousand circular mils, but the designation \"MCM\" still remains in wide use. A similar system is used in natural gas sales in the United States: m (or M) for thousands and mm (or MM) for millions of British thermal units or therms, and in the oil industry, where 'MMbbl' is the symbol for 'millions of barrels'. This usage of the capital letter M for 'thousand' is from Roman numerals, in which M means 1,000.\n\nIn some fields of information technology, it has been common to designate non-decimal multiples based on powers of 1024, rather than 1000, for some SI prefixes (kilo, mega, giga), contrary to the definitions in the International System of Units (SI). This practice was once sanctioned by some industry associations, including JEDEC. The International Electrotechnical Commission (IEC) standardized the system of binary prefixes (kibi, mebi, gibi, etc.) for this purpose.\n\n"}
{"id": "44342518", "url": "https://en.wikipedia.org/wiki?curid=44342518", "title": "Multidimensional network", "text": "Multidimensional network\n\nMultidimensional networks, a special type of \"multilayer network\", are networks with multiple kinds of relations. Increasingly sophisticated attempts to model real-world systems as multidimensional networks have yielded valuable insight in the fields of social network analysis, economics, urban and international transport, ecology, psychology, medicine, biology, commerce, climatology, physics, computational neuroscience, operations management, and finance.\n\nThe rapid exploration of complex networks in recent years has been dogged by a lack of standardized naming conventions, as various groups use overlapping and contradictory terminology to describe specific network configurations (e.g., multiplex, multilayer, multilevel, multidimensional, multirelational, interconnected). Formally, multidimensional networks are edge-labeled multigraphs. The term \"fully multidimensional\" has also been used to refer to a multipartite edge-labeled multigraph. Multidimensional networks have also recently been reframed as specific instances of multilayer networks. In this case, there are as many layers as there are dimensions, and the links between nodes within each layer are simply all the links for a given dimension.\n\nIn elementary network theory, a network is represented by a graph formula_1 in which formula_2 is the set of nodes and formula_3 the links between nodes, typically represented as a tuple of nodes formula_4. While this basic formalization is useful for analyzing many systems, real world networks often have added complexity in the form of multiple types of relations between system elements. An early formalization of this idea came through its application in the field of social network analysis (see, e.g., and papers on relational algebras in social networks) in which multiple forms of social connection between people were represented by multiple types of links.\n\nTo accommodate the presence of more than one type of link, a multidimensional network is represented by a triple formula_5, where formula_6 is a set of dimensions (or layers), each member of which is a different type of link, and formula_3 consists of triples formula_8 with formula_4 and formula_10.\n\nNote that as in all directed graphs, the links formula_8 and formula_12 are distinct.\n\nBy convention, the number of links between two nodes in a given dimension is either 0 or 1 in a multidimensional network. However, the total number of links between two nodes across all dimensions is less than or equal to formula_13.\n\nIn the case of a weighted network, this triplet is expanded to a quadruplet formula_14, where formula_15 is the weight on the link between formula_16 and formula_17 in the dimension formula_18.\n]Further, as is often useful in social network analysis, link weights may take on positive or negative values. Such signed networks can better reflect relations like amity and enmity in social networks. Alternatively, link signs may be figured as dimensions themselves, e.g. formula_5 where formula_20 and formula_21 This approach has particular value when considering unweighted networks.\nThis conception of dimensionality can be expanded should attributes in multiple dimensions need specification. In this instance, links are \"n\"-tuples formula_22. Such an expanded formulation, in which links may exist within multiple dimensions, is uncommon but has been used in the study of multidimensional time-varying networks.\n\nWhereas unidimensional networks have two-dimensional adjacency matrices of size formula_23, in a multidimensional network with formula_6 dimensions, the adjacency matrix becomes a multilayer adjacency tensor, a four-dimensional matrix of size formula_25. By using index notation, adjacency matrices can be indicated by formula_26, to encode connections between nodes formula_27 and formula_28, whereas multilayer adjacency tensors are indicated by formula_29, to encode connections between node formula_27 in layer formula_31 and node formula_28 in layer formula_33. As in unidimensional matrices, directed links, signed links, and weights are all easily accommodated by this framework.\n\nIn the case of multiplex networks, special types of multilayer networks where nodes can not be interconnected with other nodes in other layers, a three-dimensional matrix of size formula_34 with entries formula_35 is enough to represent the structure of the system by encoding connections between nodes formula_27 and formula_28 in layer formula_31.\n\nIn a multidimensional network, the neighbors of some node formula_17 are all nodes connected to formula_17 across dimensions.\n\nA path between two nodes in a multidimensional network can be represented by a vector r formula_41 in which the formula_27th entry in r is the number of links traversed in the formula_27th dimension of formula_44. As with overlapping degree, the sum of these elements can be taken as a rough measure of a path length between two nodes.\n\nThe existence of multiple layers (or dimensions) allows to introduce the new concept of \"network of layers\", peculiar of multilayer networks. In fact, layers might be interconnected in such a way that their structure can be described by a network, as shown in the figure.\n\nThe network of layers is usually weighted (and might be directed), although, in general, the weights depends on the application of interest. A simple approach is, for each pair of layers, to sum all of the weights in the connections between their nodes to obtain edge weights that can be encoded into a matrix formula_45. The rank-2 adjacency tensor, representing the underlying network of layers in the space formula_46 is given by\n\nformula_47\n\nwhere formula_48 is the canonical matrix with all components equal to zero except for the entry corresponding to row formula_31 and column formula_33, that is equal to one. Using the tensorial notation, it is possible to obtain the (weighted) network of layers from the multilayer adjacency tensor as formula_51.\n\nIn a non-interconnected multidimensional network, where interlayer links are absent, the degree of a node is represented by a vector of length formula_52. Here formula_13 is an alternative way to denote the number of layers formula_54 in multilayer networks. However, for some computations it may be more useful to simply sum the number of links adjacent to a node across all dimensions. This is the \"overlapping degree\": formula_55. As with unidimensional networks, distinction may similarly be drawn between incoming links and outgoing links.\nIf interlayer links are present, the above definition must be adapted to account for them, and the \"multilayer degree\" is given by\n\nformula_56\n\nwhere the tensors formula_57 and formula_58 have all components equal to 1. The heterogeneity in the number of connections of a node across the different layers can be taken into account through the participation coefficient.\n\nWhen extended to interconnected multilayer networks, i.e. those systems where nodes are connected across layers, the concept of centrality is better understood in terms of versatility. Nodes that are not central in each layer might be the most important for the multilayer systems in certain scenarios. For instance, this is the case where two layers encode different networks with only one node in common: it is very likely that such a node will have the highest centrality score because it is responsible for the information flow across layers.\n\nAs for unidimensional networks, eigenvector versatility can be defined as the solution of the eigenvalue problem given by formula_59, where Einstein summation convention is used for sake of simplicity. Here, formula_60 gives the multilayer generalization of Bonacich's eigenvector centrality per node per layer. The overall eigenvector versatility is simply obtained by summing up the scores across layers as formula_61.\n\nAs for its unidimensional counterpart, the Katz versatility is obtained as the solution formula_62 of the tensorial equation formula_63, where formula_64, formula_65 is a constant smaller than the largest eigenvalue and formula_66 is another constant generally equal to 1. The overall Katz versatility is simply obtained by summing up the scores across layers as formula_67.\n\nFor unidimensional networks, the HITS algorithm has been originally introduced by Jon Kleinberg to rate Web Pages. The basic assumption of the algorithm is that relevant pages, named authorities, are pointed by special Web pages, named hubs. This mechanism can be mathematically described by two coupled equations which reduce to two eigenvalue problems. When the network is undirected, Authority and Hub centrality are equivalent to eigenvector centrality.\nThese properties are preserved by the natural extension of the equations proposed by Kleinberg to the case of interconnected multilayer networks, given by\nformula_68 and formula_69, where formula_70 indicates the transpose operator, formula_71 and formula_72 indicate hub and authority centrality, respectively. By contracting the hub and authority tensors, one obtains the overall versatilities as formula_73 and formula_74, respectively.\n\nPageRank, better known as \"Google Search Algorithm\" is another measure of centrality in complex networks, originally introduced to rank Web pages. Its extension to the case of interconnected multilayer networks can be obtained as follows.\n\nFirst, it is worth remarking that PageRank can be seen as the steady-state solution of a special Markov process on the top of the network. Random walkers explore the network according to a special transition matrix and their dynamics is governed by a random walk master equation. It is easy to show that the solution of this equation is equivalent to the leading eigenvector of the transition matrix.\n\nRandom walks have been defined also in the case of interconnected multilayer networks and edge-colored multigraphs (also known as multiplex networks). For interconnected multilayer networks, the transition tensor governing the dynamics of the random walkers within and across layers is given by formula_75, where formula_76 is a constant, generally set to 0.85, formula_77 is the number of nodes and formula_54 is the number of layers or dimensions. Here, formula_79 might be named \"Google tensor\" and formula_80 is the rank-4 tensor with all components equal to 1.\n\nAs its unidimensional counterpart, PageRank versatility consists of two contributions: one encoding a classical random walk with rate formula_76 and one encoding teleportation across nodes and layers with rate formula_82.\n\nIf we indicate by formula_83 the eigentensor of the Google tensor formula_79, denoting the steady-state probability to find the walker in node formula_27 and layer formula_31, the multilayer PageRank is obtained by summing up over layers the eigentensor: formula_87\n\nLike many other network statistics, the meaning of a clustering coefficient becomes ambiguous in multidimensional networks, due to the fact that triples may be closed in different dimensions than they originated. Several attempts have been made to define local clustering coefficients, but these attempts have highlighted the fact that the concept must be fundamentally different in higher dimensions: some groups have based their work off of non-standard definitions, while others have experimented with different definitions of random walks and 3-cycles in multidimensional networks.\n\nWhile cross-dimensional structures have been studied previously, they fail to detect more subtle associations found in some networks. Taking a slightly different take on the definition of \"community\" in the case of multidimensional networks allows for reliable identification of communities without the requirement that nodes be in direct contact with each other.\nFor instance, two people who never communicate directly yet still browse many of the same websites would be viable candidates for this sort of algorithm.\n\nA generalization of the well-known modularity maximization method for community discovery has been originally proposed by Mucha et al. This multiresolution method assumes a three-dimensional tensor representation of the network connectivity within layers, as for edge-colored multigraphs, and a three-dimensional tensor representation of the network connectivity across layers. It depends on the resolution parameter formula_88 and the weight formula_89 of interlayer connections. In a more compact notation, making use of the tensorial notation, modularity can be written as formula_90, where formula_91, formula_29 is the multilayer adjacency tensor, formula_93 is the tensor encoding the null model and the value of components of formula_94 is defined to be 1 when a node formula_27 in layer formula_31 belongs to a particular community, labeled by index formula_65, and 0 when it does not.\n\nNon-negative matrix factorization has been proposed to extract the community-activity structure of temporal networks. The multilayer network is represented by a three-dimensional tensor formula_98, like an edge-colored multigraph, where the order of layers encode the arrow of time. Tensor factorization by means of Kruskal decomposition is thus applied to formula_98 to assign each node to a community across time.\n\nMethods based on statistical inference, generalizing existing approaches introduced for unidimensional networks, have been proposed. Stochastic block model is the most used generative model, appropriately generalized to the case of multilayer networks.\n\nAs for unidimensional networks, principled methods like minimum description length can be used for model selection in community detection methods based on information flow.\n\nGiven the higher complexity of multilayer networks with respect to unidimensional networks, an active field of research is devoted to simplify the structure of such systems by employing some kind of dimensionality reduction.\n\nA popular method is based on the calculation of the quantum Jensen-Shannon divergence between all pairs of layers, which is then exploited for its metric properties to build a distance matrix and hierarchically cluster the layers. Layers are successively aggregated according to the resulting hierarchical tree and the aggregation procedure is stopped when the objective function, based on the entropy of the network, gets a global maximum. This greedy approach is necessary because the underlying problem would require to verify all possible layer groups of any size, requiring a huge number of possibile combinations (which is given by the Bell number and scales super-exponentially with the number of units). Nevertheless, for multilayer systems with a small number of layers, it has been shown that the method performs optimally in the majority of cases.\n\nThe question of degree correlations in unidimensional networks is fairly straightforward: do networks of similar degree tend to connect to each other? In multidimensional networks, what this question means becomes less clear. When we refer to a node's degree, are we referring to its degree in one dimension, or collapsed over all? When we seek to probe connectivity between nodes, are we comparing the same nodes across dimensions, or different nodes within dimensions, or a combination? What are the consequences of variations in each of these statistics on other network properties? In one study, assortativity was found to decrease robustness in a duplex network.\n\nGiven two multidimensional paths, r and s, we say that r \"dominates\" s if and only if: formula_100 and formula_101 such that formula_102.\n\nAmong other network statistics, many centrality measures rely on the ability to assess shortest paths from node to node. Extending these analyses to a multidimensional network requires incorporating additional connections between nodes into the algorithms currently used (e.g., Dijkstra's). Current approaches include collapsing multi-link connections between nodes in a preprocessing step before performing variations on a breadth-first search of the network.\n\nOne way to assess the distance between two nodes in a multidimensional network is by comparing all the multidimensional paths between them and choosing the subset that we define as shortest via path dominance: let formula_103 be the set of all paths between formula_16 and formula_17. Then the distance between formula_16 and formula_17 is a set of paths formula_108 such that formula_109 such that formula_110 dominates formula_111. The length of the elements in the set of shortest paths between two nodes is therefore defined as the \"multidimensional distance\".\n\nIn a multidimensional network formula_5, the relevance of a given dimension (or set of dimensions) formula_113 for one node can be assessed by the ratio: formula_114.\n\nIn a multidimensional network in which different dimensions of connection have different real-world values, statistics characterizing the distribution of links to the various classes are of interest. Thus it is useful to consider two metrics that assess this: dimension connectivity and edge-exclusive dimension connectivity. The former is simply the ratio of the total number of links in a given dimension to the total number of links in every dimension: formula_115. The latter assesses, for a given dimension, the number of pairs of nodes connected only by a link in that dimension: formula_115.\n\nBurstiness is a well-known phenomenon in many real-world networks, e.g. email or other human communication networks. Additional dimensions of communication provide a more faithful representation of reality and may highlight these patterns or diminish them. Therefore it is of critical importance that our methods for detecting bursty behavior in networks accommodate multidimensional networks.\n\nDiffusion processes are widely used in physics to explore physical systems, as well as in other disciplines as social sciences, neuroscience, urban and international transportation or finance. Recently, simple and more complex diffusive processes have been generalized to multilayer networks. One result common to many studies is that diffusion in multiplex networks, a special type of multilayer system, exhibits two regimes: 1) the weight of inter-layer links, connecting layers each other, is not high enough and the multiplex system behaves like two (or more) uncoupled networks; 2) the weight of inter-layer links is high enough that layers are coupled each other, raising unexpected physical phenomena. It has been shown that there is an abrupt transition between these two regimes.\n\nIn fact, all network descriptors depending on some diffusive process, from centrality measures to community detection, are affected by the layer-layer coupling. For instance, in the case of community detection, low coupling (where information from each layer separately is more relevant than the overall structure) favors clusters within layers, whereas high coupling (where information from all layer simultaneously is more relevant than the each layer separately) favors cross-layer clusters.\n\nDiffusion reaction process on a multilayer system has been studied by Lazaridis et al. It is found that for the process formula_117 where A and B are initially in different layers there appear, due to the reaction, a kind of repulsion between A and B that delays them.\n\nAs for unidimensional networks, it is possible to define random walks on the top of multilayer systems. However, given the underlying multilayer structure, random walkers are not limited to move from one node to another within the same layer (\"jump\"), but are also allowed to move across layers (\"switch\").\n\nRandom walks can be used to explore a multilayer system with the ultimate goal to unravel its mesoscale organization, i.e. to partition it in communities, and have been recently used to better understand navigability of multilayer networks and their resilience to random failures, as well as for exploring efficiently this type of topologies.\n\nIn the case of interconnected multilayer systems, the probability to move from a node formula_27 in layer formula_31 to node formula_28 in layer formula_33 can be encoded into the rank-4 transition tensor formula_122 and the discrete-time walk can be described by the master equation\n\nformula_123\n\nwhere formula_124 indicates the probability of finding the walker in node formula_27 in layer formula_31 at time formula_70.\n\nThere are many different types of walks that can be encoded into the transition tensor formula_122, depending on how the walkers are allowed to jump and switch. For instance, the walker might either jump or switch in a single time step without distinguishing between inter- and intra-layer links (\"classical random walk\"), or it can choose either to stay in the current layer and jump, or to switch layer and then jump to another node in the same time step (\"physical random walk\"). More complicated rules, corresponding to specific problems to solve, can be found in the literature. In some cases, it is possible to find, analytically, the stationary solution of the master equation.\n\nThe problem of classical diffusion in complex networks is to understand how a quantity will flow through the system and how much time it will take to reach the stationary state. Classical diffusion in multiplex networks has been recently studied by introducing the concept of supra-adjacency matrix, later recognized as a special flattening of the multilayer adjacency tensor. In tensorial notation, the diffusion equation on the top of a general multilayer system can be written, concisely, as\n\nformula_129\n\nwhere formula_130 is the amount of diffusing quantity at time formula_70 in node formula_27 in layer formula_31. The rank-4 tensor governing the equation is the Laplacian tensor, generalizing the combinatorial Laplacian matrix of unidimensional networks. It is worth remarking that in non-tensorial notation, the equation takes a more complicated form.\n\nMany of the properties of this diffusion process are completely understood in terms of the second smallest eigenvalue of the Laplacian tensor. It is interesting that diffusion in a multiplex system can be faster than diffusion in each layer separately, or in their aggregation, provided that certain spectral properties are satisfied.\n\nRecently, how information (or diseases) spread through a multilayer system has been the subject of intense research.\n\nBuldyrev et al developed a framework to study percolation in multilayer networks with dependency links between the layers. New physical phenomena has been found, including abrupt transitions and cascading failures. When the networks are embedded in space they become extremely vulnerable even for a very small fraction of dependency links and for localized attacks on a zero fraction of nodes. When recovery of nodes is introduced a rich phase diagram is found that include multicritical points and metastable regimes.\n\n"}
{"id": "10265555", "url": "https://en.wikipedia.org/wiki?curid=10265555", "title": "Notation for differentiation", "text": "Notation for differentiation\n\nIn differential calculus, there is no single uniform notation for differentiation. Instead, several different notations for the derivative of a function or variable have been proposed by different mathematicians. The usefulness of each notation varies with the context, and it is sometimes advantageous to use more than one notation in a given context. The most common notations for differentiation (and its opposite operation, the antidifferentiation or indefinite integration) are listed below.\n\nThe original notation employed by Gottfried Leibniz is used throughout mathematics. It is particularly common when the equation is regarded as a functional relationship between dependent and independent variables and . Leibniz's notation makes this relationship explicit by writing the derivative as\n\nThe function whose value at is the derivative of at is therefore written\n\nHigher derivatives are written as\nThis is a suggestive notational device that comes from formal manipulations of symbols, as in,\nLogically speaking, these equalities are not theorems. Instead, they are simply definitions of notation.\n\nThe value of the derivative of \"y\" at a point may be expressed in two ways using Leibniz's notation:\n\nLeibniz's notation allows one to specify the variable for differentiation (in the denominator). This is especially helpful when considering partial derivatives. It also makes the chain rule easy to remember and recognize:\n\nLeibniz's notation for differentiation does require assigning a meaning to symbols such as \"dx\" or \"dy\" on their own, and some authors do not attempt to assign these symbols meaning. Leibniz treated these symbols as infinitesimals. Later authors have assigned them other meanings, such as infinitesimals in non-standard analysis or exterior derivatives.\n\nLeibniz introduced the integral symbol in \"Analyseos tetragonisticae pars secunda\" and \"Methodi tangentium inversae exempla\" (both from 1675). It is now the standard symbol for integration.\n\nOne of the most common modern notations for differentiation is due to Joseph Louis Lagrange. In Lagrange's notation, a prime mark denotes a derivative. If \"f\" is a function, then its derivative evaluated at \"x\" is written\nLagrange first used the notation in unpublished works, and it appeared in print in 1770.\n\nHigher derivatives are indicated using additional prime marks, as in formula_9 for the second derivative and formula_10 for the third derivative. The use of repeated prime marks eventually becomes unwieldy. Some authors continue by employing Roman numerals, as in\nto denote fourth, fifth, sixth, and higher order derivatives. Other authors use Arabic numerals in parentheses, as in\nThis notation also makes it possible to describe the \"n\"th derivative, where \"n\" is a variable. This is written\n\nUnicode characters related to Lagrange's notation include\n\nWhen there are two independent variables for a function \"f\"(\"x\",\"y\"), the following convention may be followed:\n\nWhen taking the antiderivative, Lagrange followed Leibniz's notation:\n\nHowever, because integration is the inverse of differentiation, Lagrange's notation for higher order derivatives extends to integrals as well. Repeated integrals of \"f\" may be written as\n\nLeonhard Euler's notation uses a differential operator suggested by Louis François Antoine Arbogast, denoted as (D operator) or (Newton–Leibniz operator) When applied to a function , it is defined by\nHigher derivatives are notated as powers of \"D\", as in\n\nEuler's notation leaves implicit the variable with respect to which differentiation is being done. However, this variable can also be notated explicitly. When \"f\" is a function of a variable \"x\", this is done by writing\nWhen \"f\" is a function of several variables, it's common to use a \"∂\" rather than . As above, the subscripts denote the derivatives that are being taken. For example, the second partial derivatives of a function are:\nSee .\n\nEuler's notation is useful for stating and solving linear differential equations, as it simplifies presentation of the differential equation, which can make seeing the essential elements of the problem easier.\n\nEuler's notation can be used for antidifferentiation in the same way that Lagrange's notation is. as follows\n\nNewton's notation for differentiation (also called the dot notation for differentiation) places a dot over the dependent variable. That is, if \"y\" is a function of \"t\", then the derivative of \"y\" with respect to \"t\" is\nHigher derivatives are represented using multiple dots, as in\nNewton extended this idea quite far:\n\nUnicode characters related to Newton's notation include:\n\nNewton's notation is generally used when the independent variable denotes time. If location is a function of \"t\", then formula_36 denotes velocity and formula_40 denotes acceleration. This notation is popular in physics and mathematical physics. It also appears in areas of mathematics connected with physics such as differential equations. It is only popular for first and second derivatives, but in applications these are usually the only derivatives that are necessary.\n\nWhen taking the derivative of a dependent variable \"y\" = \"f\"(\"x\"), an alternative notation exists:\n\nNewton developed the following partial differential operators using side-dots on a curved X ( ⵋ ). Definitions given by Whiteside are below:\n\nNewton developed many different notations for integration in his \"Quadratura curvarum\" (1704) and later works: he wrote a small vertical bar or prime above the dependent variable (), a prefixing rectangle (), or the inclosure of the term in a rectangle () to denote the \"fluent\" or time integral (absement).\n\nTo denote multiple integrals, Newton used two small vertical bars or primes (), or a combination of previous symbols  , to denote the second time integral (absity).\n\nHigher order time integrals were as follows:\n\nThis mathematical notation did not become widespread because of printing difficulties and the Leibniz–Newton calculus controversy.\n\nWhen more specific types of differentiation are necessary, such as in multivariate calculus or tensor analysis, other notations are common.\n\nFor a function \"f\"(\"x\"), we can express the derivative using subscripts of the independent variable:\n\nThis type of notation is especially useful for taking partial derivatives of a function of several variables.\n\nPartial derivatives are generally distinguished from ordinary derivatives by replacing the differential operator \"d\" with a \"∂\" symbol. For example, we can indicate the partial derivative of with respect to \"x\", but not to \"y\" or \"z\" in several ways:\n\nOther notations can be found in various subfields of mathematics, physics, and engineering, see for example the Maxwell relations of thermodynamics. The symbol formula_48 is the derivative of the temperature \"T\" with respect to the volume \"V\" while keeping constant the entropy (subscript) \"S\", while formula_49 is the derivative of the temperature with respect to the volume while keeping constant the pressure \"P\".\n\nHigher-order partial derivatives with respect to one variable are expressed as\n\nMixed partial derivatives can be expressed as\n\nIn this last case the variables are written in inverse order between the two notations, explained as follows:\n\nVector calculus concerns differentiation and integration of vector or scalar fields. Several notations specific to the case of three-dimensional Euclidean space are common.\n\nAssume that is a given Cartesian coordinate system, that A is a vector field with components formula_55, and that formula_56 is a scalar field.\n\nThe differential operator introduced by William Rowan Hamilton, written ∇ and called del or nabla, is symbolically defined in the form of a vector,\nwhere the terminology \"symbolically\" reflects that the operator ∇ will also be treated as an ordinary vector.\n\n\n\n\nMany symbolic operations of derivatives can be generalized in a straightforward manner by the gradient operator in Cartesian coordinates. For example, the single-variable product rule has a direct analogue in the multiplication of scalar fields by applying the gradient operator, as in\n\nFurther notations have been developed for more exotic types of spaces. For calculations in Minkowski space, the d'Alembert operator, also called the d'Alembertian, wave operator, or box operator is represented as formula_71, or as formula_72 when not in conflict with the symbol for the Laplacian.\n\n\n"}
{"id": "453041", "url": "https://en.wikipedia.org/wiki?curid=453041", "title": "Orthogonal functions", "text": "Orthogonal functions\n\nIn mathematics, orthogonal functions belong to a function space which is a vector space that has a bilinear form. When the function space has an interval as the domain, the bilinear form may be the integral of the product of functions over the interval:\n\nThe functions formula_2 and formula_3 are orthogonal when this integral is zero, i.e. formula_4 whenever formula_5. \nAs with a basis of vectors in a finite-dimensional space, orthogonal functions can form an infinite basis for a function space.\n\nSuppose formula_6 is a sequence of orthogonal functions of nonzero \"L\"-norms formula_7. It follows that the sequence formula_8 is of functions of \"L\"-norm one, forming an orthonormal sequence. To have a defined \"L\"-norm, the integral must be bounded, which restricts the functions to being square-integrable.\n\nSeveral sets of orthogonal functions have become standard bases for approximating functions. For example, the sine functions and are orthogonal on the interval formula_9 when formula_10. For then \nand the integral of the product of the two sine functions vanishes. Together with cosine functions, these orthogonal functions may be assembled into a trigonometric polynomial to approximate a given function on the interval with its Fourier series.\n\nIf one begins with the monomial sequence formula_12 on the interval formula_13 and applies the Gram–Schmidt process, then one obtains the Legendre polynomials. Another collection of orthogonal polynomials are the associated Legendre polynomials.\n\nThe study of orthogonal polynomials involves weight functions formula_14 that are inserted in the bilinear form:\nFor Laguerre polynomials on formula_16 the weight function is formula_17.\n\nBoth physicists and probability theorists use Hermite polynomials on formula_18, where the weight function is formula_19 or formula_20\n\nChebyshev polynomials are defined on formula_13 and use weights formula_22 or formula_23.\n\nZernike polynomials are defined on the unit disk and have orthogonality of both radial and angular parts.\n\nWalsh functions and Haar wavelets are examples of orthogonal functions with discrete ranges.\n\nLegendre and Chebyshev polynomials provide orthogonal families for the interval while occasionally orthogonal families are required on . In this case it is convenient to apply the Cayley transform first, to bring the argument into . This procedure results in families of rational orthogonal functions called Legendre rational functions and Chebyshev rational functions.\n\nSolutions of linear differential equations with boundary conditions can often be written as a weighted sum of orthogonal solution functions (a.k.a. eigenfunctions), leading to generalized Fourier series.\n\n\n"}
{"id": "6025550", "url": "https://en.wikipedia.org/wiki?curid=6025550", "title": "Persistence (computer science)", "text": "Persistence (computer science)\n\nIn computer science, persistence refers to the characteristic of state that outlives the process that created it. This is achieved in practice by storing the state as data in computer data storage. Programs have to transfer data to and from storage devices and have to provide mappings from the native programming-language data structures to the storage device data structures.\n\nPicture editing programs or word processors, for example, achieve state persistence by saving their documents to files.\n\nPersistence is said to be \"orthogonal\" or \"transparent\" when it is implemented as an intrinsic property of the execution environment of a program. An orthogonal persistence environment does not require any specific actions by programs running in it to retrieve or save their state.\n\nNon-orthogonal persistence requires data to be written and read to and from storage using specific instructions in a program, resulting in the use of \"persist\" as a transitive verb: \"On completion, the program persists the data\".\n\nThe advantage of orthogonal persistence environments is simpler and less error-prone programs.\n\nOrthogonal persistence is widely adopted in operating systems for hibernation and in platform virtualization systems such as VMware and VirtualBox for state saving.\n\nResearch prototype languages such as PS-algol, Napier88, Fibonacci and pJama, successfully demonstrated the concepts along with the advantages to programmers.\n\nUsing system images is the simplest persistence strategy. Notebook hibernation is an example of orthogonal persistence using a system image because it does not require any actions by the programs running on the machine. An example of non-orthogonal persistence using a system image is a simple text editing program executing specific instructions to save an entire document to a file.\n\nShortcomings: Requires enough RAM to hold the entire system state. State changes made to a system after its last image was saved are lost in the case of a system failure or shutdown. Saving an image for every single change would be too time-consuming for most systems, so images are not used as the single persistence technique for critical systems.\n\nUsing journals is the second simplest persistence technique. Journaling is the process of storing events in a log before each one is applied to a system. Such logs are called journals.\n\nOn startup, the journal is read and each event is reapplied to the system, avoiding data loss in the case of system failure or shutdown.\n\nThe entire \"Undo/Redo\" history of user commands in a picture editing program, for example, when written to a file, constitutes a journal capable of recovering the state of an edited picture at any point in time.\n\nJournals are used by journaling file systems, prevalent systems and database management systems where they are also called \"transaction logs\" or \"redo logs\".\n\nShortcomings: Journals are often combined with other persistence techniques so that the entire (potentially large) history of all system events does not have to be reapplied on system startup.\n\nThis technique is the writing to storage of only those portions of system state that have been modified (are dirty) since their last write. Sophisticated document editing applications, for example, will use dirty writes to save only those portions of a document that were actually changed since the last save.\n\nShortcomings: This technique requires state changes to be intercepted within a program. This is achieved in a non-transparent way by requiring specific storage-API calls or in a transparent way with automatic program transformation. This results in code that is slower than native code and more complicated to debug.\n\nAny software layer that makes it easier for a program to persist its state is generically called a persistence layer. Most persistence layers will not achieve persistence directly but will use an underlying database management system.\n\nSystem prevalence is a technique that combines system images and transaction journals, mentioned above, to overcome their limitations.\n\nShortcomings: A prevalent system must have enough RAM to hold the entire system state.\n\nDBMSs use a combination of the dirty writes and transaction journaling techniques mentioned above. They provide not only persistence but also other services such as queries, auditing and access control.\n\nPersistent operating systems are operating systems that remain persistent even after a crash or unexpected shutdown. Operating systems that employ this ability include\n\n"}
{"id": "9856577", "url": "https://en.wikipedia.org/wiki?curid=9856577", "title": "Point-finite collection", "text": "Point-finite collection\n\nIn mathematics, a collection formula_1 of subsets of a topological space formula_2 is said to be point finite or a point finite collection if every point of formula_2 lies in only finitely many members of formula_1.\nA topological space in which every open cover admits a point-finite open refinement is called metacompact. Every locally finite collection of subsets of a topological space is also point finite. A topological space in which every open cover admits a locally finite open refinement is called paracompact. Every paracompact space is metacompact.\n"}
{"id": "13760785", "url": "https://en.wikipedia.org/wiki?curid=13760785", "title": "Prehomogeneous vector space", "text": "Prehomogeneous vector space\n\nIn mathematics, a prehomogeneous vector space (PVS) is a finite-dimensional vector space \"V\" together with a subgroup \"G\" of the general linear group GL(\"V\") such that \"G\" has an open dense orbit in \"V\". Prehomogeneous vector spaces were introduced by Mikio Sato in 1970 and have many applications in geometry, number theory and analysis, as well as representation theory. The irreducible PVS were classified by Sato and Tatsuo Kimura in 1977, up to a transformation known as \"castling\". They are subdivided into two types, according to whether the semisimple part of \"G\" acts prehomogeneously or not. If it doesn't then there is a homogeneous polynomial on \"V\" which is invariant under the semisimple part of \"G\".\n\nIn the setting of Sato, \"G\" is an algebraic group and \"V\" is a rational representation of \"G\" which has a (nonempty) open orbit in the Zariski topology. However, PVS can also be studied from the point of view of Lie theory: for instance, in Knapp (2002), \"G\" is a complex Lie group and \"V\" is a holomorphic representation of \"G\" with an open dense orbit. The two approaches are essentially the same, and the theory has validity over the real numbers. We assume, for simplicity of notation, that the action of \"G\" on \"V\" is a faithful representation. We can then identify \"G\" with its image in GL(\"V\"), although in practice it is sometimes convenient to let \"G\" be a covering group.\n\nAlthough prehomogeneous vector spaces do not necessarily decompose into direct sums of irreducibles, it is natural to study the irreducible PVS (i.e., when \"V\" is an irreducible representation of \"G\"). In this case, a theorem of Élie Cartan shows that\n\nis a reductive group, with a centre that is at most one-dimensional. This, together with the obvious dimensional restriction\n\nis the key ingredient in the Sato–Kimura classification.\n\nThe classification of PVS is complicated by the following fact. Suppose \"m\" > \"n\" > 0 and \"V\" is an \"m\"-dimensional representation of \"G\" over a field F. Then:\nThe proof is to observe that both conditions are equivalent to there being an open dense orbit of the action of \"G\" on the Grassmannian of\n\"n\"-planes in \"V\", because this is isomorphic to the Grassmannian of (\"m\"-\"n\")-planes in \"V\".\n\nThis transformation of PVS is called castling. Given a PVS \"V\", a new PVS can be obtained by tensoring \"V\" with F and castling. By repeating this process, and regrouping tensor products, many new examples can be obtained, which are said to be \"castling-equivalent\". Thus PVS can be grouped into castling equivalence classes. Sato and Kimura show that in each such class, there is essentially one PVS of minimal dimension, which they call \"reduced\", and they classify the reduced irreducible PVS.\n\nThe classification of irreducible reduced PVS (\"G\",\"V\") splits into two cases: those for which \"G\" is semisimple, and those for which it is reductive with one-dimensional centre. If \"G\" is semisimple, it is (perhaps a covering of) a subgroup of SL(\"V\"), and hence \"G\"×GL(1) acts prehomogenously on \"V\", with one-dimensional centre. We exclude such trivial extensions of semisimple PVS from the PVS with one-dimensional center. In other words, in the case that \"G\" has one-dimensional center, we assume that the semisimple part does \"not\" act prehomogeneously; it follows that there is a \"relative invariant\", i.e., a function invariant under the semisimple part of \"G\", which is homogeneous of a certain degree \"d\".\n\nThis makes it possible to restrict attention to semisimple \"G\" ≤ SL(\"V\") and split the classification as follows:\n\nHowever, it turns out that the classification is much shorter, if one allows not just products with GL(1), but also with SL(\"n\") and GL(\"n\"). This is quite natural in terms of the castling transformation discussed previously. Thus we wish to classify irreducible reduced PVS in terms of semisimple \"G\" ≤ SL(\"V\") and \"n\" ≥ 1 such that either:\n\nIn the latter case, there is a homogeneous polynomial which separates the \"G\"×GL(\"n\") orbits into \"G\"×SL(n) orbits.\n\nThis has an interpretation in terms of the grassmannian Gr(\"V\") of \"n\"-planes in \"V\" (at least for \"n\" ≤ dim \"V\"). In both cases \"G\" acts on Gr(\"V\") with a dense open orbit \"U\". In the first case the complement Gr(\"V\")-\"U\" has codimension ≥ 2; in the second case it is a divisor of some degree \"d\", and the relative invariant is a homogeneous polynomial of degree \"nd\".\n\nIn the following, the classification list will be presented over the complex numbers.\n\n Strictly speaking, we must restrict to \"n\" ≤ (dim \"V\")/2 to obtain a reduced example.\n\nType 1\n\nType 2\n\nBoth of these examples are PVS only for \"n\"=1.\n\nThe remaining examples are all type 2. To avoid discussing the finite groups appearing, the lists present the Lie algebra of the isotropy group rather than the isotropy group itself.\n\nHere formula_8 denotes the space of 3-forms whose contraction with the given symplectic form is zero.\n\nSato and Kimura establish this classification by producing a list of possible irreducible prehomogeneous (\"G\",\"V\"), using the fact that \"G\" is reductive and the dimensional restriction. They then check whether each member of this list is prehomogeneous or not.\n\nHowever, there is a general explanation why most of the pairs (\"G\",\"V\") in the classification are prehomogeneous, in terms of isotropy representations of generalized flag varieties. Indeed, in 1974, Richardson observed that if \"H\" is a semisimple Lie group with a parabolic subgroup \"P\", then the action of \"P\" on the nilradical formula_9 of its Lie algebra has a dense open orbit. This shows in particular (and was noted independently by Vinberg in 1975) that the Levi factor \"G\" of \"P\" acts prehomogeneously on formula_10. Almost all of the examples in the classification can be obtained by applying this construction with \"P\" a maximal parabolic subgroup of a simple Lie group \"H\": these are classified by connected Dynkin diagrams with one distinguished node.\n\nOne reason that PVS are interesting is that they classify generic objects that arise in \"G\"-invariant situations. For example, if \"G\"=GL(7), then the above tables show that there are generic 3-forms under the action of \"G\", and the stabilizer of such a 3-form is isomorphic to the exceptional Lie group G.\n\nAnother example concerns the prehomogeneous vector spaces with a cubic relative invariant. By the Sato-Kimura classification, there are essentially four such examples, and they all come from complexified isotropy representations of hermitian symmetric spaces for a larger group \"H\" (i.e., \"G\" is the semisimple part of the stabilizer of a point, and \"V\" is the corresponding tangent representation).\n\nIn each case a generic point in \"V\" identifies it with the complexification of a Jordan algebra of 3 x 3 hermitian matrices (over the division algebras R, C, H and O respectively) and the cubic relative invariant is identified with a suitable determinant. The isotropy algebra of such a generic point, the Lie algebra of \"G\" and the Lie algebra of \"H\" give the complexifications of the first three rows of the Freudenthal magic square.\n\nOther Hermitian symmetric spaces yields prehomogeneous vector spaces whose generic points define Jordan algebras in a similar way.\n\nThe Jordan algebra \"J\"(\"m\"−1) in the last row is the spin factor (which is the vector space R ⊕ R, with a Jordan algebra structure defined using the inner product on R). It reduces to formula_11 for \"m\"= 3, 4, 6 and 10 respectively.\n\nThe relation between hermitian symmetric spaces and Jordan algebras can be explained using Jordan triple systems.\n\n"}
{"id": "4720923", "url": "https://en.wikipedia.org/wiki?curid=4720923", "title": "Ran Libeskind-Hadas", "text": "Ran Libeskind-Hadas\n\nRan Libeskind-Hadas is a professor of Computer Science at Harvey Mudd College. His research interests lie in the fields of algorithm design and analysis and complexity theory, but focus more specifically on routing algorithms for optical networks and collective communication in parallel computers and networks.\n\nLibeskind-Hadas graduated from Harvard University with a degree in Applied Mathematics in 1987. He went on to complete an M.S. and Ph.D. in Computer Science at the University of Illinois at Urbana-Champaign in 1993. In August of that year he was hired into the Department of Mathematics at Harvey Mudd College, a liberal arts school that focuses on science and engineering. He has been in the Department of Computer Science since May 1994, and served as acting chair of the department in 2006-2007.\n\nLibeskind-Hadas held the Joseph B. Platt Endowed Chair for effective teaching from 2005 to 2010. His research has been funded by the National Science Foundation. Most recently, he has been the director of an NSF Research Experiences for Undergraduates (REU) site at Harvey Mudd College, which has allowed Harvey Mudd to greatly increase the number of undergraduate research students it can support during the summers, in addition to increasing cross-university interactions. Additionally, Libeskind-Hadas has been the key faculty member in encouraging a group of students to form a student chapter for the Association for Computing Machinery (ACM), which has been successfully running and helping members of the Claremont Colleges and surrounding community since the beginning of 2005.\n\nLibeskind-Hadas has produced noted research in the \"module orientation problem\", a branch of computer science important in design of large scale integrated circuits.\n\nLibeskind-Hadas is a coauthor of R. Libeskind-Hadas, N. Hasan, J. Cong, P. McKinley, and C. L. Liu. \"Fault Covering Problems in Reconfigurable VLSI Systems. \" Kluwer Academic Publishers, 1992., and has published 16 peer reviewed papers and 20 peer-reviewed conference proceedings.\n\n"}
{"id": "56717276", "url": "https://en.wikipedia.org/wiki?curid=56717276", "title": "Sara Negri", "text": "Sara Negri\n\nSara Negri (born January 21, 1967) is a mathematical logician who studies proof theory.\nShe is Italian, but works in Finland, where she is a professor of theoretical philosophy in the University of Helsinki.\n\nNegri was born in Padua, and studied at the University of Padua. She earned a master's degree there in 1991 and a Ph.D. in 1996, both in mathematics. Her dissertation, \"Dalla Topologia Formale all'Analisi\", was supervised by Giovanni Sambin.\nShe came to Helsinki as a docent in 1998, and became a full professor there in 2015. She has also taken several visiting positions, including a Humboldt Fellowship in 2004–2005 at the Ludwig Maximilian University of Munich.\n\nNegri is the co-author, with Jan von Plato, of two books:\n"}
{"id": "58486357", "url": "https://en.wikipedia.org/wiki?curid=58486357", "title": "Soft configuration model", "text": "Soft configuration model\n\nIn applied mathematics, the soft configuration model (SCM) is a random graph model subject to the principle of maximum entropy under constraints on the expectation of the degree sequence of sampled graphs. Whereas the configuration model (CM) uniformly samples random graphs of a specific degree sequence, the SCM only retains the specified degree sequence on average over all network realizations; in this sense the SCM is has very relaxed constraints relative to those of the CM (\"soft\" rather than \"sharp\" constraints). The SCM for graphs of size formula_1 has a nonzero probability of sampling any graph of size formula_1, whereas the CM is restricted to only graphs having precisely the perscribed connectivity structure.\n\nThe SCM is a statistical ensemble of random graphs formula_3 having formula_1 vertices (formula_5) labeled formula_6, producing a probability distribution on formula_7 (the set of graphs of size formula_1). Imposed on the ensemble are formula_1 constraints, namely that the ensemble average of the degree formula_10 of vertex formula_11 is equal to a designated value formula_12, for all formula_13. The model is fully parameterized by it's size formula_1 and expected degree sequence formula_15. These constraints are both local (one constraint associated with each vertex) and soft (constraints on the ensemble average of certain observable quantities), and thus yields a canonical ensemble with an extensive number of constraints. The conditions formula_16 are imposed on the ensemble by the method of Lagrange multipliers (see Maximum-entropy random graph model).\n\nThe probability formula_17 of the SCM producing a graph formula_3 is determined by maximizing the Gibbs entropy formula_19 subject to constraints formula_20 and normalization formula_21. This amounts to optimizing the multi-constraint Lagrange function below:\n\nwhere formula_23 and formula_24 are the formula_25 multipliers to be fixed by the formula_25 constraints (normalization and the expected degree sequence). Setting to zero the derivative of the above with respect to formula_17 for an arbitrary formula_28 yields\n\nthe constant formula_30 being the partition function normalizing the distribution; the above exponential expression applies to all formula_31, and thus is the probability distribution. Hence we have an exponential family parameterized by formula_24, which are related to the expected degree sequence formula_15 by the following equivalent expressions:\n"}
{"id": "663772", "url": "https://en.wikipedia.org/wiki?curid=663772", "title": "Strict conditional", "text": "Strict conditional\n\nIn logic, a strict conditional is a conditional governed by a modal operator, that is, a logical connective of modal logic. It is logically equivalent to the material conditional of classical logic, combined with the necessity operator from modal logic. For any two propositions \"p\" and \"q\", the formula \"p\" → \"q\" says that \"p\" materially implies \"q\" while formula_1 says that \"p\" strictly implies \"q\". Strict conditionals are the result of Clarence Irving Lewis's attempt to find a conditional for logic that can adequately express indicative conditionals in natural language. They have also been used in studying Molinist theology.\n\nThe strict conditionals may avoid paradoxes of material implication. The following statement, for example, is not correctly formalized by material implication:\n\nThis condition should clearly be false: the degree of Bill Gates has nothing to do with whether Elvis is still alive. However, the direct encoding of this formula in classical logic using material implication leads to:\n\nThis formula is true because whenever the antecedent \"A\" is false, a formula \"A\" → \"B\" is true. Hence, this formula is not an adequate translation of the original sentence. An encoding using the strict conditional is:\n\nIn modal logic, this formula means (roughly) that, in every possible world in which Bill Gates graduated in Medicine, Elvis never died. Since one can easily imagine a world where Bill Gates is a Medicine graduate and Elvis is dead, this formula is false. Hence, this formula seems to be a correct translation of the original sentence.\n\nAlthough the strict conditional is much closer to being able to express natural language conditionals than the material conditional, it has its own problems with consequents that are necessarily true (such as 2 + 2 = 4) or antecedents that are necessarily false. The following sentence, for example, is not correctly formalized by a strict conditional:\n\nUsing strict conditionals, this sentence is expressed as:\n\nIn modal logic, this formula means that, in every possible world where Bill Gates graduated in medicine, it holds that 2 + 2 = 4. Since 2 + 2 is equal to 4 in all possible worlds, this formula is true, although it does not seem that the original sentence should be. A similar situation arises with 2 + 2 = 5, which is necessarily false:\n\nSome logicians view this situation as indicating that the strict conditional is still unsatisfactory. Others have noted that the strict conditional cannot adequately express counterfactual conditionals, and that it does not satisfy certain logical properties. In particular, the strict conditional is transitive, while the counterfactual conditional is not.\n\nSome logicians, such as Paul Grice, have used conversational implicature to argue that, despite apparent difficulties, the material conditional is just fine as a translation for the natural language 'if...then...'. Others still have turned to relevance logic to supply a connection between the antecedent and consequent of provable conditionals.\n\n\nFor an introduction to non-classical logic as an attempt to find a better translation of the conditional, see:\nFor an extended philosophical discussion of the issues mentioned in this article, see:\n"}
{"id": "1804931", "url": "https://en.wikipedia.org/wiki?curid=1804931", "title": "Studia Mathematica", "text": "Studia Mathematica\n\nStudia Mathematica is a Polish mathematics journal published by the Polish Academy of Sciences and accepts papers written in English, French, German, or Russian, primarily in functional analysis, abstract methods of mathematical analysis and probability theory.\n\n\"Studia Mathematica\" was founded in 1929 by Stefan Banach and Hugo Steinhaus in Lwów and its first editors were Banach, Steinhaus and Herman Auerbach.\n\nDue to the Second World War publication stopped after volume 9 (1940) and could not be resumed until volume 10 in 1948, published in Breslau since Lwów was no longer Polish.\n\nIn 2010 the Impact Factor of the journal was 0.549; in the statistics of the Web of Science it occupied place 157 of 295 in the category \"Mathematics\".\nIn the SCImago Journal Rank of mathematical journals in 2017, \"Studia Mathematica\" was ranked 395 out of 1382 journals with a rank indicator of 0.846.\n\nThe first issues after the war display the effects of the Nazi oppression on Polish mathematics in various footnotes and posthumous publications.\n\nIn volume 10 (1948) Hugo Steinhaus writes in a footnote on Herman Auerbach:\n\"Ce mathématicien distingué et homme de rare qualités d'esprit et de cœur a été assassiné par les Allemands à Lwów en 1942.\"\n\nMeier Eidelheit's posthumously published article \"Quelques remarques sur les fonctionelles linéaires\" in volume 10 was prefaced with the following lines:\n\"L’auteur de ce travail a été assassiné par les Allemands en mars de 1943. Le manuscrit qu’il fut parvenir à la Rédaction en 1941 a été retrouvé récemment entre les papiers laissés par S. Banach.\"\n(The author of this work was murdered in March 1943 by the Germans.\nThe manuscript, which reached the editors in 1941, was recently found among the writings left by S. Banach.)\n\nIn volume 11 (1950) Andrzej Alexiewicz refers as follows to his dissertation in a footnote:\n\"Presented with some insignificant alterations as Doctor Thesis, on March 10, 1944 to the , during the terror of the German occupation.\"\n\nG. Sirvint's posthumously published article \"Weak compactness in Banach spaces\" in volume 11 begins with the lines:\n\"The author was murdered by the Germans during the second world war. The present work was received by the editor in 1941 and has been prepared by A. Alexiewicz.\"\n\n"}
{"id": "18668810", "url": "https://en.wikipedia.org/wiki?curid=18668810", "title": "Supernatural number", "text": "Supernatural number\n\nIn mathematics, the supernatural numbers, sometimes called generalized natural numbers or Steinitz numbers, are a generalization of the natural numbers. They were used by Ernst Steinitz in 1910 as a part of his work on field theory.\n\nA supernatural number formula_1 is a formal product:\n\nwhere formula_3 runs over all prime numbers, and each formula_4 is zero, a natural number or infinity. Sometimes formula_5 is used instead of formula_4. If no formula_7 and there are only a finite number of non-zero formula_4 then we recover the positive integers. Slightly less intuitively, if all formula_4 are formula_10, we get zero. Supernatural numbers extend beyond natural numbers by allowing the possibility of infinitely many prime factors, and by allowing any given prime to divide formula_1 \"infinitely often,\" by taking that prime's corresponding exponent to be the symbol formula_10.\n\nThere is no natural way to add supernatural numbers, but they can be multiplied, with formula_13. Similarly, the notion of divisibility extends to the supernaturals with formula_14 if formula_15 for all formula_3. The notion of the least common multiple and greatest common divisor can also be generalized for supernatural numbers, by defining\n\nWith these definitions, the gcd or lcm of infinitely many natural numbers (or supernatural numbers) is a supernatural number.\nWe can also extend the usual formula_3-adic order functions to supernatural numbers by defining formula_20 for each formula_3\n\nSupernatural numbers are used to define orders and indices of profinite groups and subgroups, in which case many of the theorems from finite group theory carry over exactly. They are used to encode the algebraic extensions of a finite field. They are also used implicitly in many number-theoretical proofs, such as the density of the square-free integers and bounds for odd perfect numbers.\n\n\n\n"}
{"id": "24765100", "url": "https://en.wikipedia.org/wiki?curid=24765100", "title": "Tangent indicatrix", "text": "Tangent indicatrix\n\nIn differential geometry, the tangent indicatrix of a closed space curve is a curve on the unit sphere intimately related to the curvature of the original curve. Let formula_1 be a closed curve with nowhere-vanishing tangent vector formula_2. Then the tangent indicatrix formula_3 of formula_4 is the closed curve on the unit sphere given by formula_5.\n\nThe total curvature of formula_4 (the integral of curvature with respect to arc length along the curve) is equal to the arc length of formula_7.\n\n"}
{"id": "6748071", "url": "https://en.wikipedia.org/wiki?curid=6748071", "title": "Time-bin encoding", "text": "Time-bin encoding\n\nTime-bin encoding is a technique used in Quantum information science to encode a qubit of information on a photon. Quantum information science makes use of qubits as a basic resource similar to bits in classical computing. Qubits are any two-level quantum mechanical system; there are many different physical implementations of qubits, one of which is time-bin encoding.\n\nWhile the time-bin encoding technique is very robust against decoherence, it does not allow easy interaction between the different qubits. As such, it is much more useful in quantum communication (such as quantum teleportation and quantum key distribution) than in quantum computation.\n\nTime-bin encoding is done by having a single-photon go through a Mach-Zender interferometer (MZ), shown in black here. The photon coming from the left is guided through one of two paths (shown in blue and red); the guiding can be made by optical fiber or simply in free space using mirrors and polarising cubes. One of the two paths is longer than the other. The difference in path length must be longer than the coherence length of the photon to make sure the path taken can be unambiguously distinguished. The interferometer has to keep a stable phase, which means that the path length difference must vary by much less than the wavelength of light during the experiment. This usually requires active temperature stabilization.\n\nIf the photon takes the short path, it is said to be in the state formula_1; if it takes the long path, it is said to be in the state formula_2. If the photon has a non-zero probability to take either path, then it is in a coherent superposition of the two states:\n\nThese coherent superpositions of the two possible states are called qubits and are the basic ingredient of Quantum information science.\n\nIn general, it is easy to vary the phase gained by the photon between the two paths, for example by stretching the fiber, while it is much more difficult to vary the amplitudes which are therefore fixed, typically at 50%. The created qubit is then\n\nwhich covers only a subset of all possible qubits.\n\nMeasurement in the {formula_1,formula_2} basis is done by measuring the time of arrival of the photon. Measurement in other bases can be achieved by letting the photon go through a second MZ before measurement, though, similar to the state preparation, the possible measurement setups are restricted to only a small subset of possible qubit measurements.\n\nTime-bin qubits do not suffer from depolarization or polarization mode-dispersion, making them better suited to fiber optics applications than polarization encoding. Photon loss is easily detectable since the absence of photons does not correspond to an allowed state, making it better suited than a photon-number based encoding.\n\n"}
{"id": "13250641", "url": "https://en.wikipedia.org/wiki?curid=13250641", "title": "Tutte–Berge formula", "text": "Tutte–Berge formula\n\nIn the mathematical discipline of graph theory the Tutte–Berge formula is a characterization of the size of a maximum matching in a graph. It is a generalization of Tutte's theorem on perfect matchings, and is named after W. T. Tutte (who proved Tutte's theorem) and Claude Berge (who proved its generalization).\n\nThe theorem states that the size of a maximum matching of a graph formula_1 equals\n\nwhere formula_3 counts how many of the connected components of the graph formula_4 have an odd number of vertices.\n\nIntuitively, for any subset \"U\" of the vertices, the only way to completely cover an odd component of \"G\" − \"U\" by a matching is for one of the matched edges covering the component to be incident to \"U\". If, instead, some odd component had no matched edge connecting it to \"U\", then the part of the matching that covered the component would cover its vertices in pairs, but since the component has an odd number of vertices it would necessarily include at least one leftover and unmatched vertex. Therefore, if some choice of \"U\" has few vertices but its removal creates a large number of odd components, then there will be many unmatched vertices, implying that the matching itself will be small. This reasoning can be made precise by stating that the size of a maximum matching is at most equal to the value given by the Tutte–Berge formula.\n\nThe characterization of Tutte and Berge proves that this is the only obstacle to creating a large matching: the size of the optimal matching will be determined by the subset \"U\" with the biggest difference between its numbers of odd components outside \"U\" and vertices inside \"U\". That is, there always exists a subset \"U\" such that deleting \"U\" creates the correct number of odd components needed to make the formula true. One way to find such a set \"U\" is to choose any maximum matching \"M\", and to let \"X\" be the set of vertices that are either unmatched in \"M\", or that can be reached from an unmatched vertex by an alternating path that ends with a matched edge. Then, let \"U\" be the set of vertices that are matched by \"M\" to vertices in \"X\". No two vertices in \"X\" can be adjacent, for if they were then their alternating paths could be concatenated to give a path by which the matching could be increased, contradicting the maximality of \"M\". Every neighbor of a vertex \"x\" in \"X\" must belong to \"U\", for otherwise we could extend an alternating path to \"x\" by one more pair of edges, through the neighbor, causing the neighbor to become part of \"U\". Therefore, in \"G\" − \"U\", every vertex of \"X\" forms a single-vertex component, which is odd. There can be no other odd components, because all other vertices remain matched after deleting \"U\". So with this construction the size of \"U\" and the number of odd components created by deleting \"U\" are what they need to be to make the formula be true.\n\nTutte's theorem characterizes the graphs with perfect matchings as being the ones for which deleting any subset \"U\" of vertices creates at most |\"U\"| odd components. (A subset \"U\" that creates at least |\"U\"| odd components can always be found in the empty set.) In this case, by the Tutte–Berge formula, the size of the matching is |\"V\"|/2; that is, the maximum matching is a perfect matching. Thus, Tutte's theorem can be derived as a corollary of the Tutte–Berge formula, and the formula can be seen as a generalization of Tutte's theorem.\n\n\n"}
{"id": "38573159", "url": "https://en.wikipedia.org/wiki?curid=38573159", "title": "Type family", "text": "Type family\n\nIn computer science, a type family associates data types with other data types, using a type-level function defined by an open-ended collection of valid instances of input types and the corresponding output types.\n\nType families are a feature of some type systems that allow partial functions between types to be defined by pattern matching. This is in contrast to data type constructors, which define injective functions from all types of a particular kind to a new set of types, and type synonyms (a.k.a. typedef), which define functions from all types of a particular kind to another existing set of types using a single case.\n\nType families and type classes are closely related: normal type classes define partial functions from types to a collection of named \"values\" by pattern matching on the input types, while type families define partial functions from types to \"types\" by pattern matching on the input types. In fact, in many uses of type families there is a single type class which logically contains both values and types associated with each instance. A type family declared inside a type class is called an associated type.\n\nProgramming languages with support for type families or similar features include Haskell (with a common language extension), Standard ML (through its module system), Scala (under the name \"abstract types\"), and C++ (through use of typedefs in templates).\n\nThe codice_1 extension in the Glasgow Haskell Compiler supports both \"type synonym families\" and \"data families\". Type synonym families are the more flexible (but harder to type-check) form, permitting the types in the codomain of the type function to be any type whatsoever with the appropriate kind. Data families, on the other hand, restrict the codomain by requiring each instance to define a new type constructor for the function's result. This ensures that the function is injective, allowing clients' contexts to deconstruct the type family and obtain the original argument type.\n\nType families are useful in abstracting patterns where a common \"organization\" or \"structure\" of types is repeated, but with different specific types in each case. Typical use cases include describing abstract data types like generic collections, or design patterns like model–view–controller.\n\nOne of the original motivations for the introduction of associated types was to allow abstract data types to be parameterized by their content type such that the data structure implementing the abstract type varies in a \"self-optimizing\" way. Normal algebraic data type parameters can only describe data structures that behave uniformly with respect to all argument types. Associated types, however, can describe a family of data structures that have a uniform interface but vary in implementation according to one or more type parameters. For example, using Haskell's associated types notation, we can declare a type class of valid array element types, with an associated data family representing an array of that element type:\n\nInstances can then be defined for this class, which define both the data structure used and the operations on the data structure in a single location. For efficiency, we might use a packed bit vector representation for arrays of Boolean values, while using a normal array data structure for integer values. The data structure for arrays of ordered pairs is defined recursively as a pair of arrays of each of the element types.\n\nWith these definitions, when a client refers to an codice_2, an implementation is automatically selected using the defined instances.\n\nInverting the previous example, we can also use type families to define a class for collection types, where the type function maps each collection type to its corresponding element type:\n\nIn this example, the use of a type synonym family instead of a data family is essential, since multiple collection types may have the same element type.\n\nFunctional dependencies are another type system feature that have similar uses to associated types. While an associated type adds a named type function mapping the enclosing type class's parameters to another type, a functional dependency lists the result type as another parameter of the type class and adds a constraint between the type parameters (e.g. \"parameter \"a\" uniquely determines parameter \"b\"\", written codice_3). The most common uses of functional dependencies can be directly converted to associated types and vice versa.\n\nType families are regarded as being generally easier to type-check than functional dependencies. Another advantage of associated types over functional dependencies is that the latter requires clients using the type class to state all of the dependent types in their contexts, including ones they do not use; since associated types do not require this, adding another associated type to the class requires updating only the class's instances, while clients can remain unchanged. The main advantages of functional dependencies over type families are in their added flexibility in handling a few unusual cases.\n\n"}
{"id": "16861692", "url": "https://en.wikipedia.org/wiki?curid=16861692", "title": "Unfolding (functions)", "text": "Unfolding (functions)\n\nIn mathematics, an unfolding of a function is a certain family of functions.\n\nLet formula_1 be a smooth manifold and consider a smooth mapping formula_2 Let us assume that for given formula_3 and formula_4 we have formula_5. Let formula_6 be a smooth formula_7-dimensional manifold, and consider the family of mappings (parameterised by formula_8) given by formula_9 We say that formula_10 is a formula_7-parameter unfolding of formula_12 if formula_13 for all formula_14 In other words the functions formula_15 and formula_16 are the same: the function formula_12 is contained in, or is unfolded by, the family formula_18\n\nLet formula_19 be given by formula_20 An example of an unfolding of formula_12 would be formula_22 given by \nAs is the case with unfoldings, formula_24 and formula_25 are called variables and formula_26 formula_27 and formula_28 are called parameters – since they parameterise the unfolding.\n\nIn practice we require that the unfoldings have certain nice properties. In formula_29 notice that formula_12 is a smooth mapping from formula_1 to formula_29 and so belongs to the function space formula_33 As we vary the parameters of the unfolding we get different elements of the function space. Thus, the unfolding induces a function formula_34 The space formula_35 where formula_36 denotes the group of diffeomorphisms of formula_1 etc., acts on formula_33 The action is given by formula_39 If formula_40 lies in the orbit of formula_12 under this action then there is a diffeomorphic change of coordinates in formula_1 and formula_29 which takes formula_40 to formula_12 (and vice versa). One nice property that we may like to impose is that \nwhere \"formula_47\" denotes \"transverse to\". This property ensures that as we vary the unfolding parameters we can predict – by knowing how the orbit foliate formula_48 – how the resulting functions will vary.\n\nThere is an idea of a versal unfolding. Every versal unfolding has the property that \nformula_46, but the converse is false. Let formula_50 be local coordinates on formula_1, and let formula_52 denote the ring of smooth functions. We define the Jacobian ideal of formula_53 denoted by formula_54 as follows:\n\nThen a basis for a versal unfolding of formula_12 is given by quotient\n\nThis quotient is known as the local algebra of formula_58 The dimension of the local algebra is called the Milnor number of formula_12. The minimum number of unfolding parameters for a versal unfolding is equal to the Milnor number; that is not to say that every unfolding with that many parameters will be versal! Consider the function formula_20 A calculation shows that\n\nThis means that formula_62 give a basis for a versal unfolding, and that \n\nis a versal unfolding. A versal unfolding with the minimum possible number of unfolding parameters is called a miniversal unfolding.\n\nSometimes unfoldings are called deformations, versal unfoldings are called versal deformations, etc.\n\nAn important object associated to an unfolding is its bifurcation set. This set lives in the parameter space of the unfolding, and gives all parameter values for which the resulting function has degenerate singularities.\n\n"}
{"id": "528491", "url": "https://en.wikipedia.org/wiki?curid=528491", "title": "Von Neumann–Bernays–Gödel set theory", "text": "Von Neumann–Bernays–Gödel set theory\n\nIn the foundations of mathematics, von Neumann–Bernays–Gödel set theory (NBG) is an axiomatic set theory that is a conservative extension of Zermelo–Fraenkel set theory (ZFC). NBG introduces the notion of class, which is a collection of sets defined by a formula whose quantifiers range only over sets. NBG can define classes that are larger than sets, such as the class of all sets and the class of all ordinals. Morse–Kelley set theory (MK) allows classes to be defined by formulas whose quantifiers range over classes. NBG is finitely axiomatizable, while ZFC and MK are not.\n\nA key theorem of NBG is the class existence theorem, which states that for every formula whose quantifiers range only over sets, there is a class consisting of the sets satisfying the formula. This class is built by mirroring the step-by-step construction of the formula with classes. Since all set-theoretic formulas are constructed from two kinds of atomic formulas (membership and equality) and finitely many logical symbols, only finitely many axioms are needed to build the classes satisfying them. This is why NBG is finitely axiomatizable. Classes are also used for other constructions, for handling the set-theoretic paradoxes, and for stating the axiom of global choice, which is stronger than ZFC's axiom of choice.\n\nJohn von Neumann introduced classes into set theory in 1925. The primitive notions of his theory were function and argument. Using these notions, he defined class and set. Paul Bernays reformulated von Neumann's theory by taking class and set as primitive notions. Kurt Gödel simplified Bernays' theory for his relative consistency proof of the axiom of choice and the generalized continuum hypothesis.\n\nClasses have several uses in NBG:\n\nOnce classes are added to the language, it is simple to axiomatize a set theory with classes that is similar to ZFC. First, the axiom schema of class comprehension is added. This axiom schema states: For every formula formula_18 that quantifies only over sets, there exists a class formula_19 consisting of the satisfying the formula—that is, formula_20 Then the axiom schema of replacement is replaced by a single axiom that uses a class. Finally, ZFC's axiom of extensionality is modified to handle classes: If two classes have the same elements, then they are identical. The other axioms of ZFC are not modified.\n\nThis theory is not finitely axiomatized. ZFC's replacement schema has been replaced by a single axiom, but the axiom schema of class comprehension has been introduced. \n\nTo produce a theory with finitely many axioms, the axiom schema of class comprehension must be replaced with a class existence theorem that makes the same statement: For every formula that quantifies only over sets, there exists a class of satisfying it. The proof of this theorem, which is given below, requires only finitely many class existence axioms to translate the construction of a formula into a construction of the class satisfying the formula.\n\nNBG has two types of objects: classes and sets. Intuitively, every set is also a class. There are two ways to axiomatize this. Bernays used many-sorted logic with two sorts: classes and sets. Gödel avoided sorts by introducing primitive predicates: formula_21 for \"formula_19 is a class\" and formula_23 for \"formula_19 is a set\" (in German, \"set\" is \"Menge\"). He also introduced axioms stating that every set is a class and that if class formula_19 is a member of a class, then formula_19 is a set. Using predicates is the standard way to eliminate sorts. Elliott Mendelson modified Gödel's approach by having everything be a class and defining the set predicate formula_27 as formula_28 This modification eliminates Gödel's class predicate and his two axioms.\n\nBernays' two-sorted approach may appear more natural at first, but it creates a more complex theory. In Bernays' theory, every set has two representations: one as a set and the other as a class. Also, there are two membership relations: the first, denoted by \"∈\", is between two sets; the second, denoted by \"η\", is between a set and a class. This redundancy is required by many-sorted logic because variables of different sorts range over disjoint subdomains of the domain of discourse.\n\nThe differences between these two approaches does not affect what can be proved, but it does affect how statements are written. In Gödel's approach, formula_29 where formula_19 and formula_31 are classes is a valid statement. In Bernays' approach this statement has no meaning. However, if formula_19 is a set, there is an equivalent statement: Define \"set formula_33 represents class formula_19\" if they have the same sets as members—that is, formula_35 The statement formula_36 where set formula_33 represents class formula_19 is equivalent to Gödel's formula_39\n\nThe approach adopted in this article is that of Gödel with Mendelson's modification. This means that NBG is a axiomatic system in first-order predicate logic with equality, and its only primitive notions are class and the membership relation.\n\nA set is a class that belongs to at least one class: formula_19 is a set if and only if formula_41.\nA class that is not a set is called a proper class: formula_19 is a proper class if and only if formula_43.\nTherefore, every class is either a set or a proper class, and no class is both (if the theory is consistent).\n\nGödel introduced the convention that uppercase variables range over classes, while lowercase variables range over sets. Gödel also used names that begin with an uppercase letter to denote particular classes, including functions and relations defined on the class of all sets. Gödel's convention is used in this article. It allows us to write:\n\nThe following axioms and definitions are needed for the proof of the class existence theorem.\n\nAxiom of extensionality. If two classes have the same elements, then they are identical.\n\nThis axiom generalizes ZFC's axiom of extensionality to classes. \n\nAxiom of pairing. If formula_49 and formula_50 are sets, then there exists a set formula_51 whose only members are formula_49 and formula_50.\n\nAs in ZFC, the axiom of extensionality implies the uniqueness of the set formula_51, which allows us to introduce the notation formula_56. \n\nOrdered pairs are defined by: \nTuples are defined inductively using ordered pairs:\n\nThe class existence axioms will be used to prove the class existence theorem: For every formula in formula_60 free set variables that quantifies only over sets, there exists a class of that satisfy it. The following example starts with two classes that are functions and builds a composite function. This example illustrates the techniques used to prove the class existence theorem, and hence the types of class existence axioms that are needed.\n\nThe class existence axioms are divided into two groups: axioms handling language primitives and axioms handling tuples. There are four axioms in the first group and three axioms in the second group.\n\nAxioms for handling language primitives:\n\nMembership. There exists a class formula_61 containing all the ordered pairs whose first component is a member of the second component.\n\nIntersection (conjunction). For any two classes formula_19 and formula_64, there is a class formula_31 consisting precisely of the sets that belong to both formula_19 and formula_64.\n\nComplement (negation). For any class formula_19, there is a class formula_64 consisting precisely of the sets not belonging to formula_19.\n\nDomain (existential quantifier). For any class formula_19, there is a class formula_64 consisting precisely of the first components of the ordered pairs of formula_19. \n\nBy the axiom of extensionality, class formula_31 in the intersection axiom and class formula_64 in the complement and domain axioms are unique. They will be denoted by: formula_79 formula_80 and formula_81 respectively. On the other hand, extensionality is not applicable to formula_61 in the membership axiom since it specifies only those sets in formula_61 that are ordered pairs. \n\nThe first three axioms imply the existence of the empty class and the class of all sets: The membership axiom implies the existence of a class formula_84 The intersection and complement axioms imply the existence of formula_85, which is empty. By the axiom of extensionality, this class is unique; it is denoted by formula_86 The complement of formula_87 is the class formula_88of all sets, which is also unique by extensionality. The set predicate formula_27, which was defined as formula_90, is now redefined as formula_91 to avoid quantifying over classes.\n\nAxioms for handling tuples:\n\nProduct by formula_88. For any class formula_19, there is a class formula_64 consisting precisely of the ordered pairs whose first component belongs to formula_19. \n\nCircular permutation. For any class formula_19, there is a class formula_64 whose 3tuples are obtained by applying the circular permutation formula_99 to the 3tuples of formula_19.\n\nTransposition. For any class formula_19, there is a class formula_64 whose 3tuples are obtained by transposing the last two components of the 3tuples of formula_19.\n\nBy extensionality, the product by formula_88 axiom implies the existence of a unique class, which is denoted by formula_107 This axiom is used to define the class formula_108 of all : formula_109 and formula_110 If formula_19 is a class, extensionality implies that formula_112 is the unique class consisting of the of formula_113 For example, the membership axiom produces a class formula_61 that may contain elements that are not ordered pairs, while the intersection formula_115 contains only the ordered pairs of formula_61.\n\nThe circular permutation and transposition axioms do not imply the existence of unique classes because they specify only the 3tuples of class formula_117 By specifying the 3tuples, these axioms also specify the for formula_118 since: formula_119 The axioms for handling tuples and the domain axiom imply the following lemma, which is used in the proof of the class existence theorem.\n\nTuple lemma.\n\nProof:Class formula_124: Apply product by formula_88 to formula_19 to produce formula_127<br>\nClass formula_128: Apply transposition to formula_124 to produce formula_130<br>\nClass formula_131: Apply circular permutation to formula_124 to produce formula_133<br>\nClass formula_134: Apply circular permutation to formula_128, then apply domain to produce formula_136\n\nOne more axiom is needed to prove the class existence theorem: the axiom of regularity. Since the existence of the empty class has been proved, the usual statement of this axiom is given.\n\nAxiom of regularity. Every nonempty set has at least one element with which it has no element in common.\n\nThis axiom implies that a set cannot belong to itself: Assume that formula_138 and let formula_139 Then formula_140 since formula_141 This contradicts the axiom of regularity because formula_49 is the only element in formula_143 Therefore, formula_144 The axiom of regularity also prohibits infinite descending membership sequences: formula_145\n\nGödel stated regularity for classes rather than for sets in his 1940 monograph, which was based on lectures given in 1938. In 1939, he proved that regularity for sets implies regularity for classes.\n\nClass existence theorem. Let formula_146 be a formula that quantifies only over sets and contains no free variables other than formula_147 (not necessarily all of these). Then for all formula_148, there exists a unique class formula_19 of such that: formula_150 The class formula_19 is denoted by formula_152\n\nThe theorem's proof will be done in two steps:\n\nTransformation rules. In rules 1 and 2, formula_157 and formula_158 denote set or class variables. These two rules eliminate all occurrences of class variables before an formula_11 and all occurrences of equality. Each time rule 1 or 2 is applied to a subformula, formula_160 is chosen so that formula_161 differs from the other variables in the current formula. The three rules are repeated until there are no subformulas to which they can be applied. This produces a formula that is built only with formula_154, formula_155, formula_156, formula_11, set variables, and class variables formula_166 where formula_166 does not appear before an formula_11.\n\n\nTransformation rules: bound variables. Consider the composite function formula of example 1 with its free set variables replaced by formula_177 and formula_178: formula_179 The inductive proof will remove formula_180, which produces the formula formula_181 However, since the class existence theorem is stated for subscripted variables, this formula does not have the form expected by the induction hypothesis. This problem is solved by replacing the variable formula_182 with formula_183 Bound variables within nested quantifiers are handled by increasing the subscript by one for each successive quantifier. This leads to rule 4, which must be applied after the other rules since rules 1 and 2 produce quantified variables.\n\nProof of the class existence theorem. The proof starts by applying the transformation rules to the given formula to produce a transformed formula. Since this formula is equivalent to the given formula, the proof is completed by proving the class existence theorem for transformed formulas.\n\nGödel pointed out that the class existence theorem \"is a metatheorem, that is, a theorem about the system [NBG], not in the system …\" It is a theorem about NBG because it is proved in the metatheory by induction on NBG formulas. Also, its proof—instead of invoking finitely many NBG axioms—inductively describes how to use NBG axioms to construct a class satisfying a given formula. For every formula, this description can be turned into a constructive existence proof that is in NBG. Therefore, this metatheorem can generate infinitely many NBG proofs.\n\nA recursive computer program succinctly captures the construction of a class from a given formula. The definition of this program does not depend on the proof of the class existence theorem. However, this proof is needed to prove that the class constructed by the program satisfies the given formula and is built using the axioms. This program is written in pseudocode that uses a Pascal-style case statement.\n\nformula_184<br>\nformula_185<br>\n\nformula_186<br>\nformula_189\n\nLet formula_153 be the formula of example 2. The function call formula_192 generates the class formula_193 which is compared below with formula_194 This shows that the construction of the class formula_19 mirrors the construction of its defining formula formula_194\n\nGödel extended the class existence theorem to formulas formula_153 containing relations over classes (such as formula_199 and the unary relation formula_200), special classes (such as formula_9), and operations (such as formula_202 and formula_203). To extend the class existence theorem, the formulas defining relations, special classes, and operations must quantify only over sets. Then formula_153 can be transformed into an equivalent formula satisfying the hypothesis of the class existence theorem.\n\nThe following definitions specify how formulas define relations, special classes, and operations:\n\nA is defined by:\n\nThe following transformation rules eliminate relations, special classes, and operations. Each time rule 2b, 3b, or 4 is applied to a subformula, formula_160 is chosen so that formula_161 differs from the other variables in the current formula. The rules are repeated until there are no subformulas to which they can be applied. formula_217 and formula_157 denote terms.\n\nClass existence theorem (extended version). Let formula_146 be a formula that quantifies only over sets, contains no free variables other than formula_220, and may contain relations, special classes, and operations defined by formulas that quantify only over sets. Then for all formula_221 there exists a unique class formula_19 of such that formula_150\n\nProof: Apply the transformation rules to formula_153 to produce an equivalent formula containing no relations, special classes, or operations. This formula satisfies the hypothesis of the class existence theorem. Therefore, for all formula_221 there is a unique class formula_19 of satisfying formula_227\n\nThe axioms of pairing and regularity, which were needed for the proof of the class existence theorem, have been given above. NBG contains four other set axioms. Three of these axioms deal with class operations being applied to sets.\n\nDefinition. formula_228 is a function if formula_229\n\nIn set theory, the definition of a function does not require specifying the domain or codomain of the function (see Function (set theory)). NBG's definition of function generalizes ZFC's definition from a set of ordered pairs to a class of ordered pairs. \n\nZFC's definitions of the set operations: image, union, and power set are also generalized to class operations. The image of class formula_19 under the function formula_228 is formula_232 This definition does not require that formula_233 The union of class formula_19 is formula_235 The power class of formula_19 is formula_237 The extended version of the class existence theorem implies the existence of these classes. The axioms of replacement, union, and power set imply that when these operations are applied to sets, they produce sets.\n\nAxiom of replacement. If formula_228 is a function and formula_33 is a set, then formula_240, the image of formula_33 under formula_228, is a set.\n\nNot having the requirement formula_244 in the definition of formula_245 produces a stronger axiom of replacement, which is used in the following proof.\nTheorem (NBG's axiom of separation). If formula_19 is a set and formula_64 is a subclass of formula_193 then formula_64 is a set. <br>\nProof: The class existence theorem constructs the restriction of the identity function to formula_64: formula_251 Since the image of formula_19 under formula_253 is formula_64, the axiom of replacement implies that formula_64 is a set. This proof depends on the definition of image not having the requirement formula_244 since formula_257 rather than formula_258\n\nAxiom of union. If formula_33 is a set, then there is a set containing formula_260\n\nAxiom of power set. If formula_33 is a set, then there is a set containing formula_263\n\nTheorem. If formula_33 is a set, then formula_266 and formula_267 are sets.<br>\nProof: The axiom of union states that formula_266 is a subclass of a set formula_269, so the axiom of separation implies formula_266 is a set. Likewise, the axiom of power set states that formula_267 is a subclass of a set formula_269, so the axiom of separation implies that formula_267 is a set.\n\nAxiom of infinity. There exists a nonempty set formula_33 such that for all formula_49 in formula_33, there exists a formula_50 in formula_33 such that formula_49 is a proper subset of formula_50.\n\nThe axioms of infinity and replacement prove the existence of the empty set. In the discussion of the class existence axioms, the existence of the empty class formula_87 was proved. We now prove that formula_87 is a set. Let function formula_284 and let formula_33 be the set given by the axiom of infinity. By replacement, the image of formula_33 under formula_228, which equals formula_87, is a set.\n\nNBG's axiom of infinity is implied by ZFC's axiom of infinity: formula_289 The first conjunct of ZFC's axiom, formula_290, implies the first conjunct of NBG's axiom. The second conjunct of ZFC's axiom, formula_291, implies the second conjunct of NBG's axiom since formula_292 To prove ZFC's axiom of infinity from NBG's axiom of infinity requires some of the other NBG axioms (see Weak axiom of infinity).\n\nThe class concept allows NBG to have a stronger axiom of choice than ZFC. A choice function is a function formula_5, defined on a set formula_4 of nonempty sets, such that formula_7 for all formula_8 ZFC's axiom of choice states that for every set of nonempty sets, there exists a choice function. A global choice function is a function formula_1 defined on the class of all nonempty sets such that formula_2 for every nonempty set formula_3 The axiom of global choice states that there exists a global choice function. This axiom implies ZFC's axiom of choice since for every set formula_4 of nonempty sets, formula_301 (the restriction of formula_1 to formula_4) is a choice function for formula_304 In 1964, William B. Easton proved that global choice is stronger than the axiom of choice by using forcing to construct a model that satisfies the axiom of choice and all the axioms of NBG except the axiom of global choice. The axiom of global choice is equivalent to every class having a well-ordering, while ZFC's axiom of choice is equivalent to every set having a well-ordering.\n\nAxiom of global choice. There exists a function that chooses an element from every nonempty set.\n\nVon Neumann published an introductory article on his axiom system in 1925. In 1928, he provided a detailed treatment of his system. Von Neumann based his axiom system on two domains of primitive objects: functions and arguments. These domains overlap—objects that are in both domains are called argument-functions. Functions correspond to classes in NBG, and argument-functions correspond to sets. Von Neumann's primitive operation is function application, denoted by [\"a\", \"x\"] rather than \"a\"(\"x\") where \"a\" is a function and \"x\" is an argument. This operation produces an argument. Von Neumann defined classes and sets using functions and argument-functions that take only two values, \"A\" and \"B\". He defined \"x\" ∈ \"a\" if [\"a\", \"x\"] ≠ \"A\".\n\nVon Neumann's work in set theory was influenced by Georg Cantor's articles, Ernst Zermelo's 1908 axioms for set theory, and the 1922 critiques of Zermelo's set theory that were given independently by Abraham Fraenkel and Thoralf Skolem. Both Fraenkel and Skolem pointed out that Zermelo's axioms cannot prove the existence of the set {\"Z\", \"Z\", \"Z\", ...} where \"Z\" is the set of natural numbers and \"Z\" is the power set of \"Z\". They then introduced the axiom of replacement, which would guarantee the existence of such sets. However, they were reluctant to adopt this axiom: Fraenkel stated \"that Replacement was too strong an axiom for 'general set theory'\", while \"Skolem only wrote that 'we could introduce' Replacement\".\n\nVon Neumann worked on the problems of Zermelo set theory and provided solutions for some of them:\n\nIn 1929, von Neumann published an article containing the axioms that would lead to NBG. This article was motivated by his concern about the consistency of the axiom of limitation of size. He stated that this axiom \"does a lot, actually too much.\" Besides implying the axioms of separation and replacement, and the well-ordering theorem, it also implies that any class whose cardinality is less than that of \"V\" is a set. Von Neumann thought that this last implication went beyond Cantorian set theory and concluded: \"We must therefore discuss whether its [the axiom's] consistency is not even more problematic than an axiomatization of set theory that does not go beyond the necessary Cantorian framework.\"\n\nVon Neumann started his consistency investigation by introducing his 1929 axiom system, which contains all the axioms of his 1925 axiom system except the axiom of limitation of size. He replaced this axiom with two of its consequences, the axiom of replacement and a choice axiom. Von Neumann's choice axiom states: \"Every relation \"R\" has a subclass that is a function with the same domain as \"R\".\"\n\nLet \"S\" be von Neumann's 1929 axiom system. Von Neumann introduced the axiom system \"S\" + Regularity (which consists of \"S\" and the axiom of regularity) to demonstrate that his 1925 system is consistent relative to \"S\". He proved:\nThese results imply: If \"S\" is consistent, then von Neumann's 1925 axiom system is consistent. Proof: If \"S\" is consistent, then \"S\" + Regularity is consistent (result 1). Using proof by contradiction, assume that the 1925 axiom system is inconsistent, or equivalently: the 1925 axiom system implies a contradiction. Since \"S\" + Regularity implies the axioms of the 1925 system (result 2), \"S\" + Regularity also implies a contradiction. However, this contradicts the consistency of \"S\" + Regularity. Therefore, if \"S\" is consistent, then von Neumann's 1925 axiom system is consistent.\n\nSince \"S\" is his 1929 axiom system, von Neumann's 1925 axiom system is consistent relative to his 1929 axiom system, which is closer to Cantorian set theory. The major differences between Cantorian set theory and the 1929 axiom system are classes and von Neumann's choice axiom. The axiom system \"S\" + Regularity was modified by Bernays and Gödel to produce the equivalent NBG axiom system.\n\nIn 1929, Paul Bernays started modifying von Neumann's new axiom system by taking classes and sets as primitives. He published his work in a series of articles appearing from 1937 to 1954. Bernays stated that:\n\nBernays handled sets and classes in a two-sorted logic and introduced two membership primitives: one for membership in sets and one for membership in classes. With these primitives, he rewrote and simplified von Neumann's 1929 axioms. Bernays also included the axiom of regularity in his axiom system.\n\nIn 1931, Bernays sent a letter containing his set theory to Kurt Gödel. Gödel simplified Bernays' theory by making every set a class, which allowed him to use just one sort and one membership primitive. He also weakened some of Bernays' axioms and replaced von Neumann's choice axiom with the equivalent axiom of global choice. Gödel used his axioms in his 1940 monograph on the relative consistency of global choice and the generalized continuum hypothesis.\n\nSeveral reasons have been given for Gödel choosing NBG for his monograph:\n\nGödel's achievement together with the details of his presentation led to the prominence that NBG would enjoy for the next two decades. In 1963, Paul Cohen proved his independence proofs for ZF with the help of some tools that Gödel had developed for his relative consistency proofs for NBG. Later, ZFC became more popular than NBG. This was caused by several factors, including the extra work required to handle forcing in NBG, Cohen's 1966 presentation of forcing, which used ZF, and the proof that NBG is a conservative extension of ZFC.\n\nNBG is not logically equivalent to ZFC because its language is more expressive: it can make statements about classes, which cannot be made in ZFC. However, NBG and ZFC imply the same statements about sets. Therefore, NBG is a conservative extension of ZFC. NBG implies theorems that ZFC does not imply, but since NBG is a conservative extension, these theorems must involve proper classes. For example, NBG implies that the class \"V\" of all sets can be well-ordered and that every proper class can be put into one-to-one correspondence with \"V\".\n\nOne consequence of conservative extension is that ZFC and NBG are equiconsistent. Proving this uses the principle of explosion: from a contradiction, everything is provable. Assume that either ZFC or NBG is inconsistent. Then the inconsistent theory implies the contradictory statements ∅ = ∅ and ∅ ≠ ∅, which are statements about sets. By the conservative extension property, the other theory also implies these statements. Therefore, it is also inconsistent. So although NBG is more expressive, it is equiconsistent with ZFC. This result together with von Neumann's 1929 relative consistency proof implies that his 1925 axiom system with the axiom of limitation of size is equiconsistent with ZFC. This completely resolves von Neumann's concern about the relative consistency of this powerful axiom since ZFC is within the Cantorian framework.\n\nEven though NBG is a conservative extension of ZFC, a theorem may have a shorter and more elegant proof in NBG than in ZFC (or vice versa). For a survey of known results of this nature, see . \n\nMorse–Kelley set theory has an axiom schema of class comprehension that includes formulas whose quantifiers range over classes. MK is a stronger theory than NBG because MK proves the consistency of NBG, while Gödel's second incompleteness theorem implies that NBG cannot prove the consistency of NBG. \n\nFor a discussion of some ontological and other philosophical issues posed by NBG, especially when contrasted with ZFC and MK, see Appendix C of .\n\nZFC, NBG, and MK have models describable in terms of the cumulative hierarchy \"V\" and the constructible hierarchy \"L\". Let \"V\" include an inaccessible cardinal κ and let Def(\"X\") denote the Δ definable subsets of \"X\" (see constructible universe). Then:\n\nThe ontology of NBG provides scaffolding for speaking about \"large objects\" without risking paradox. For instance, in some developments of category theory, a \"large category\" is defined as one whose objects and morphisms make up a proper class. On the other hand, a \"small category\" is one whose objects and morphisms are members of a set. Thus, we can speak of the \"category of all sets\" or \"category of all small categories\" without risking paradox since NBG supports large categories. \n\nHowever, NBG does not support a \"category of all categories\" since large categories would be members of it and NBG does not allow proper classes to be members of anything. An ontological extension that enables us to talk formally about such a \"category\" is the conglomerate, which is a collection of classes. Then the \"category of all categories\" is defined by its objects: the conglomerate of all categories; and its morphisms: the conglomerate of all morphisms from \"A\" to \"B\" where \"A\" and \"B\" are objects. On whether an ontology including classes as well as sets is adequate for category theory, see .\n\n"}
{"id": "33383638", "url": "https://en.wikipedia.org/wiki?curid=33383638", "title": "Walter Ledermann", "text": "Walter Ledermann\n\nWalter Ledermann FRSE (18 March 1911 Berlin, Germany – 22 May 2009 London, England) was a German and British mathematician who worked on matrix theory, group theory, homological algebra, number theory, statistics, and stochastic processes. He was elected to the Royal Society of Edinburgh in 1944.\n\nHe taught at the universities of Dundee, St Andrews, Manchester, and finally Sussex. At Sussex, Ledermann was appointed professor in 1965, where he continued to teach until he was 89. He wrote various mathematics textbooks.\n\n\n"}
