{"id": "394843", "url": "https://en.wikipedia.org/wiki?curid=394843", "title": "47 (number)", "text": "47 (number)\n\n47 (forty-seven) is the natural number following 46 and preceding 48.\n\nForty-seven is the fifteenth prime number, a safe prime, the thirteenth supersingular prime, and the sixth Lucas prime. Forty-seven is a highly cototient number. It is an Eisenstein prime with no imaginary part and real part of the form .\n\nIt is a Lucas number. It is also a Keith number because its digits appear as successive terms earlier in the series of Lucas numbers: 2, 1, 3, 4, 7, 11, 18, 29, 47… ;.\n\nForty-seven is a strictly non-palindromic number.\n\nIts representation in binary being 00101111, 47 is a prime Thabit number, and as such is related to the pair of amicable numbers {17296, 18416}.\n\nForty-seven is a Carol number.\n\n\n\nForty-seven has been the favorite number of Pomona College, California, United States, since 1964. A mathematical proof, written in 1964 by Professor Donald Bentley, supposedly demonstrates that all numbers are equal to 47. However, Bentley offered it as a \"joke proof\" to further a popular student research project that listed real and imaginative \"47 sightings\". Bentley used the invalid proof to introduce his students to the concept of mathematical proofs. The proof used limits to show that the sum of the two equal sides of an isosceles triangle is equal to the base side. Bentley chose forty-seven as the base side, but he could have used any number.\n\nJoe Menosky graduated from Pomona College in 1979 and went on to become one of the story writers of \"\". Menosky \"infected\" other Star Trek writers with an enthusiasm for the number 47. As a result, 47, its reverse 74, its multiples, or combinations of 47 occur in a large number of episodes of the program and its spin-offs \"\", \"\", and \"\", usually in the form of dialogue, on-screen labels, or computer screens. For example:\nJ. J. Abrams, who produced and directed \"Star Trek\", frequently uses the number 47 in his productions, including episodes of his TV series \"Fringe.\" In the Season 1 episode \"Bad Dreams\", aired shortly before the release of \"Star Trek\" in theaters, Nick Lane's bulletin board features a large centrally-located sheet of paper with only the number 47 in huge typeface. It recurs in the series: for example, 47 minutes being the maximum amount of time for a time chamber in the series to last, and there being exactly 47 shapeshifters. J.J. Abrams continues to incorporate 47 into movies and series he produces and directs. \nThe final sequence of Mission Impossible: Ghost Protocol takes place on Pier 47.\nThere are many 47s in \"Fringe\", \"Alias\", and \"Revolution\". In \"\", the thermal oscillator is located in Precinct 47. In the Season 1 episode \"Soul Train\" of the series \"Revolution\", the characters are involved with an old train engine where the engine number happens to be 47.\n\nDuring the 2012 election, Republican candidate Mitt Romney made a comment claiming that 47 percent of Americans do not pay any income tax. Since the comment potentially sabotaged his chances of winning the election against Barack Obama, the term \"47 percent\" has been used by critics to describe actions that could potentially damage a political candidate. For example, during the 2016 election, Hillary Clinton's speech labeling half Donald Trump's supporters as \"deplorable\" was compared by critics to Romney's 47 percent speech.\n\nThe Brooklyn-based hip hop collective Pro Era and its late co-founder Jamal Dewar, better known by his stage name Capital Steez, have made references to the number 47 in various songs by members of the group. The design of one of Pro Era's logos is the number 47 with its digits joined together. The origins of the group's connection with the number can be linked to the production of Capital Steez's 2012 debut mixtape \"AmeriKKKan Korruption.\" The rapper was heavily fixated with the number during that time; he felt that 47 was a perfect expression of balance in the world, representing the tension between the heart and the brain (the fourth and seventh chakra, respectively.)\n\n\n\n"}
{"id": "1519889", "url": "https://en.wikipedia.org/wiki?curid=1519889", "title": "Annuity function", "text": "Annuity function\n\nAnnuity Functions are mathematical functions frequently used by Actuarial science students in their introduction to the mathematics of finance and more specifically in their introduction to annuities.\n\n"}
{"id": "53341083", "url": "https://en.wikipedia.org/wiki?curid=53341083", "title": "Automated efficiency model", "text": "Automated efficiency model\n\nAn automated efficiency model (AEM) is a mathematical model that estimates a real estate property’s efficiency by using details specific to the property which are available publicly and/or housing characteristics which are aggregated over a given area such as a zip code. AEMs have some similarities to an automated valuation model (AVM) in terms of concept, advantages and disadvantages.\n\nAEMs calculate specific efficiencies such as location, water, energy or solar efficiency. The Council of Multiple Listing Services defines an AEM as, “any algorithm or scoring model that estimates the [efficiency] of a home without an on-site inspection. They are similar to Automated Valuation Models (AVMs), but are more reliant on public data such as square footage...and estimated energy usage.”\n\nMost AEMs calculate a property’s selected efficiency by analyzing available public information and may also apply proprietary data or formulas, and allow for a user such as a home owner to make additional inputs. Housing characteristics such as age of the home or square footage may be obtained by data providers such as those on this list of online real estate databases or a similar offerings. Estimates of energy usage may be available from published sources such as through the Residential Energy Consumption Survey by the Energy Information Administration.\n\nBy design, the AEM score output is provided as a preliminary comparison tool so the score of one property may be compared to other homes, against an average score for the area, etc. Primary users may vary from buyers and sellers to real estate agents and appraisers as they complete relevant comparisons. For example, REColorado, the multiple listing service covering the Denver metro area, presents a UtilityScore widget on homes for sale. Zillow publishes a Sun Number score on the home fact sheet so website visitors can compare the solar energy potential of prospective properties. Trulia has published a report using automated estimates from UtilityScore to present water, natural gas and electric rates into a single price per square foot by zip code.\n\nBeyond usage for consumer preliminary comparisons, usage of AEMs varies by industry. AEMs may also be used by solar installers, home improvement contractors, efficiency inspectors, and mortgage lenders.\n\nIn the photovoltaics industry, installers use Sun Number to reduce the soft costs associated with motivating consumers to invest in solar systems and in recording property specifications to create quotes. The U.S. Department of Energy has found that Sun Number eliminates 7–10 days from the quotation process when solar suitability is determined digitally and eliminates the need for an onsite inspection.\n\nAEMs have been used in the mortgage industry to support a niche loan product called a Location Efficient Mortgage (LEM). During underwriting, an AEM such as the H+T Affordability Index is used to calculate the location efficient value\n\nAccording to National Mortgage Professional Magazine AEMs may one day be incorporated into loan underwriting as well, “Since utilities are as big or bigger part of home expenses than even real estate taxes, we may see [estimated utility usage] begin to be factored into underwriting.”\n\nAEMs generate a score for a specific property based on both publicly available housing characteristics about the subject property as well as mathematical modeling. AEMs are technology-driven scores without an onsite inspection or human assessment. For more accurate information unique to a specific property an onsite inspection such as an energy audit is required.\n\nDetailed information on the data accessed to calculate an AEM, the modeling formulas and algorithms are generally not published. A summary of general information is listed in the table below:\n\nAs shown in the section above, AEMs tend to rely on public information rather than information which is private to the resident such as actual utility bills. Utility bills can vary based on the occupancy and personal property within a structure. The public information used in AEMs is relatively static as it is focused on details of the structure, location and/or mechanical systems and therefore tends to reflect the real property transferred during a real estate transaction.\n\nAccording to the Council of Multiple Listing Services advantages are, “AEMs provide consumers with a quick comparison of all properties across a specified market. Since most focus on the attached systems and structure, they are only meant to reflect the efficiency of the real property.”\n\nAccording to the Council of Multiple Listing Services advantages are, “AEMs are dependent on data used, the assumptions made, and the model methodology. Since models and methodologies differ and no on-site inspections are performed, accuracy may vary among scoring systems.”\n"}
{"id": "1007613", "url": "https://en.wikipedia.org/wiki?curid=1007613", "title": "Bell state", "text": "Bell state\n\nThe Bell states, a concept in quantum information science, are specific quantum states of two qubits that represent the simplest (and maximal) examples of quantum entanglement. They are named after John S. Bell because they are the subject of his famous Bell inequality. An EPR pair is a pair of qubits (or quantum bits) that are in a Bell state together. Because of the entanglement, measurement of one qubit will assign one of two possible values to the other qubit instantly, where the value assigned depends on which Bell state the two qubits are in. This behaviour is not subject to relativistic limitations such as the speed of light, but the no-communication theorem prevents this behaviour to be used to transmit information faster than light. The phenomena can be used to agree upon random numbers in less time than it would take to communicate the numbers at the speed of light over the distance between the peers – this was believed to be a result of an error in quantum physics and was named the EPR paradox. Originally, the paradox was resolved by giving up the assumption that the principle of locality is true, but other interpretations have also emerged.\n\nThe Bell states are four specific maximally entangled quantum states of two qubits.\n\nThe degree to which a state in a quantum system consisting of two \"particles\" is entangled is measured by the Von Neumann entropy of either of the two reduced density operators of the state. The Von Neumann entropy of a pure state is zero—also for the Bell states, which are specific pure states. But the 2x2 density matrix formula_1 corresponding to the Bell states can be formed as usual, and the von Neumann entropy of this density operator of the Bell states is positive and maximal, if the matrix does not degenerate to a projector.\n\nThe qubits are usually thought to be spatially separated. However far apart they are, they exhibit perfect correlation even though there is no way to tell which state each qubit is in — a situation that cannot be explained without quantum mechanics.\n\nIn order to explain this, consider first the Bell state formula_2:\n\nThis superposition means the following: The qubit held by Alice (subscript \"A\") can be 0 as well as 1. If Alice measured her qubit in the standard basis, the outcome would be perfectly random, either possibility 0 or 1 having probability 1/2. But if Bob (subscript \"B\") then measured his qubit, the outcome would be the same as the one Alice got. So, if Bob measured, he would also get a random outcome on first sight, but if Alice and Bob communicated, they would find out that, although their outcomes seemed random, they are perfectly correlated.\n\nThe perfect correlation at a distance is special: maybe the two particles \"agreed\" in advance, when the pair was created (before the qubits were separated), which outcome they would show in case of a measurement.\n\nHence, following Einstein, Podolsky, and Rosen in 1935 in their famous \"EPR paper\", there is something missing in the description of the qubit pair given above—namely this \"agreement\", called more formally a hidden variable.\n\nBut quantum mechanics allows qubits to be in quantum superposition—i.e. in 0 and 1 simultaneously—that is, a linear combination of the two states — for example, the states formula_4 or formula_5. If Alice and Bob chose to measure in this basis, i.e. check whether their qubit were formula_6 or formula_7, they would find the same correlations as above. That is because the Bell state can be formally rewritten as follows:\n\nNote that this is still the \"same\" state.\n\nIn his famous paper of 1964, John S. Bell showed by simple probability theory arguments that these correlations (the one for the 0,1 basis and the one for the +,- basis) cannot \"both\" be made perfect by the use of any \"pre-agreement\" stored in some hidden variables—but that quantum mechanics predicts perfect correlations. In a more formal and refined formulation known as the Bell-CHSH inequality, it is shown that a certain correlation measure cannot exceed the value 2 if one assumes that physics respects the constraints of local \"hidden variable\" theory (a sort of common-sense formulation of how information is conveyed), but certain systems permitted in quantum mechanics can attain values as high as formula_9.\n\nFour specific two-qubit states with the maximal value of formula_9 are designated as \"Bell states\". They are known as the four \"maximally entangled two-qubit Bell states\", and they form a maximally entangled basis, known as the Bell basis, of the four-dimensional Hilbert space for two qubits: \n\nThe Bell measurement is an important concept in quantum information science: It is a joint quantum-mechanical measurement of two qubits that determines which of the four Bell states the two qubits are in.\n\nIf the qubits were not in a Bell state before, they get projected into a Bell state (according to the projection rule of quantum measurements), and as Bell states are entangled, a Bell measurement is an entangling operation.\n\nBell state measurement is the crucial step in quantum teleportation. The result of a Bell state measurement is used by one's co-conspirator to reconstruct the original state of a teleported particle from half of an entangled pair (the \"quantum channel\") that was previously shared between the two ends.\n\nExperiments that utilize so-called \"linear evolution, local measurement\" techniques cannot realize a complete Bell state measurement. Linear evolution means that the detection apparatus acts on each particle independently from the state or evolution of the other, and local measurement means that each particle is localized at a particular detector registering a \"click\" to indicate that a particle has been detected. Such devices can be constructed, for example, from mirrors, beam splitters, and wave plates, and are attractive from an experimental perspective because they are easy to use and have a high measurement cross-section.\n\nFor entanglement in a single qubit variable, only three distinct classes out of four Bell states are distinguishable using such linear optical techniques. This means two Bell states cannot be distinguished from each other, limiting the efficiency of quantum communication protocols such as teleportation. If a Bell state is measured from this ambiguous class, the teleportation event fails.\n\nEntangling particles in multiple qubit variables, such as (for photonic systems) polarization and a two-element subset of orbital angular momentum states, allows the experimenter to trace over one variable and achieve a complete Bell state measurement in the other. Leveraging so-called hyper-entangled systems thus has an advantage for teleportation. It also has advantages for other protocols such as superdense coding, in which hyper-entanglement increases the channel capacity.\n\nIn general, for hyper-entanglement in formula_15 variables, one can distinguish between at most formula_16 classes out of formula_17 Bell states using linear optical techniques.\n\nIndependent measurements made on two qubits that are entangled in Bell states positively correlate perfectly, if each qubit is measured in the relevant basis. For the formula_2 state, this means selecting the same basis for both qubits. If an experimenter chose to measure both qubits in a formula_19 Bell state using the same basis, the qubits would appear positively correlated when measuring in the <math>\\\n\n\n"}
{"id": "40958342", "url": "https://en.wikipedia.org/wiki?curid=40958342", "title": "Brauner space", "text": "Brauner space\n\nIn functional analysis and related areas of mathematics Brauner space is a complete compactly generated locally convex space formula_1 having a sequence of compact sets formula_2 such that every other compact set formula_3 is contained in some formula_2.\n\nBrauner spaces are named after Kalman George Brauner, who began their study. All Brauner spaces are stereotype and are in the stereotype duality relations with Fréchet spaces:\n\n\n\n"}
{"id": "54109699", "url": "https://en.wikipedia.org/wiki?curid=54109699", "title": "Cellular space", "text": "Cellular space\n\nA cellular space is a T2-space that has the structure of a CW complex.\n"}
{"id": "3702870", "url": "https://en.wikipedia.org/wiki?curid=3702870", "title": "Commutativity of conjunction", "text": "Commutativity of conjunction\n\nIn propositional logic, the commutativity of conjunction is a valid argument form and truth-functional tautology. It is considered to be a law of classical logic. It is the principle that the conjuncts of a logical conjunction may switch places with each other, while preserving the truth-value of the resulting proposition.\n\n\"Commutativity of conjunction\" can be expressed in sequent notation as:\n\nand\n\nwhere formula_3 is a metalogical symbol meaning that formula_4 is a syntactic consequence of formula_5, in the one case, and formula_5 is a syntactic consequence of formula_4 in the other, in some logical system;\n\nor in rule form:\n\nand\n\nwhere the rule is that wherever an instance of \"formula_5\" appears on a line of a proof, it can be replaced with \"formula_4\" and wherever an instance of \"formula_4\" appears on a line of a proof, it can be replaced with \"formula_5\";\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nand\n\nwhere formula_16 and formula_17 are propositions expressed in some formal system.\n\nFor any propositions H, H, ... H, and permutation σ(n) of the numbers 1 through n, it is the case that:\n\nis equivalent to\n\nFor example, if H is\n\nH is\n\nand H is\n\nthen\n\n\"It is raining and Socrates is mortal and 2+2=4\"\n\nis equivalent to\n\n\"Socrates is mortal and 2+2=4 and it is raining\"\n\nand the other orderings of the predicates.\n"}
{"id": "8203600", "url": "https://en.wikipedia.org/wiki?curid=8203600", "title": "Cotlar–Stein lemma", "text": "Cotlar–Stein lemma\n\nIn mathematics, in the field of functional analysis, the Cotlar–Stein almost orthogonality lemma is named after mathematicians Mischa Cotlar\nand Elias Stein. It may be used to obtain information on the operator norm on an operator, acting from one Hilbert space into another\nwhen the operator can be decomposed into \"almost orthogonal\" pieces.\nThe original version of this lemma\nwas proved by Mischa Cotlar in 1955 and allowed him to conclude that the Hilbert transform\nis a continuous linear operator in formula_1\nwithout using the Fourier transform.\nA more general version was proved by Elias Stein.\n\nLet formula_2 be two Hilbert spaces.\nConsider a family of operators\nformula_3, formula_4,\nwith each formula_3\na bounded linear operator from formula_6 to formula_7.\n\nDenote\n\nThe family of operators\nformula_9, formula_10\nis \"almost orthogonal\" if\n\nThe Cotlar–Stein lemma states that if formula_3\nare almost orthogonal,\nthen the series\nformula_13\nconverges in the strong operator topology,\nand that\n\nIf \"R\", ..., \"R\" is a finite collection of bounded operators, then\n\nSo under the hypotheses of the lemma,\n\nIt follows that \n\nand that\n\nHence the partial sums \n\nform a Cauchy sequence. \n\nThe sum is therefore absolutely convergent with limit satisfying the stated inequality.\n\nTo prove the inequality above set\n\nwith |\"a\"| ≤ 1 chosen so that\n\nThen\n\nHence\n\nTaking 2\"m\"th roots and letting \"m\" tend to ∞,\n\nwhich immediately implies the inequality.\n\nThere is a generalization of the Cotlar–Stein lemma with sums replaced by integrals.Let \"X\" be a locally compact space and μ a Borel measure on \"X\". Let \"T\"(\"x\") be a map from \"X\" into bounded operators from \"E\" to \"F\" which is uniformly bounded and continuous in the strong operator topology. If \n\nare finite, then the function \"T\"(\"x\")\"v\" is integrable for each \"v\" in \"E\" with\n\nThe result can be proved by replacing sums by integrals in the previous proof or by using Riemann sums to approximate the integrals.\n\nHere is an example of an \"orthogonal\" family of operators. Consider the inifite-dimensional matrices\n\nand also\n\nThen\nformula_29 for each formula_30,\nhence the series formula_31\ndoes not converge in the uniform operator topology.\n\nYet, since\nformula_32\nand \nformula_33\nfor formula_34,\nthe Cotlar–Stein almost orthogonality lemma tells us that \n\nconverges in the strong operator topology and is bounded by 1.\n\n"}
{"id": "976365", "url": "https://en.wikipedia.org/wiki?curid=976365", "title": "Divided differences", "text": "Divided differences\n\nIn mathematics, divided differences is an algorithm, historically used for computing tables of logarithms and trigonometric functions. Charles Babbage's difference engine, an early mechanical calculator, was designed to use this algorithm in its operation.\n\nDivided differences is a recursive division process. The method can be used to calculate the coefficients in the interpolation polynomial in the Newton form.\n\nGiven \"k+1\" data points\n\nThe forward divided differences are defined as:\n\nThe backward divided differences are defined as:\n\nIf the data points are given as a function \"ƒ\",\n\none sometimes writes\n\nSeveral notations for the divided difference of the function \"ƒ\" on the nodes \"x\", ..., \"x\" are used:\n\netc.\n\nDivided differences for formula_12 and the first few values of formula_13:\n\nTo make the recursive process more clear, the divided differences can be put in a tabular form:\n\n\n\n\n\nThe divided difference scheme can be put into an upper triangular matrix.\nLet formula_24.\n\nThen it holds\n\nformula_44\n\nWith the help of a polynomial function formula_45 with formula_46 this can be written as\n\nAlternatively, we can allow counting backwards from the start of the sequence by defining formula_48 whenever formula_49 or formula_50. This definition allows formula_51 to be interpreted as formula_52, formula_53 to be interpreted as formula_54, formula_55 to be interpreted as formula_56, etc. The expanded form of the divided difference thus becomes\n\nformula_57\n\nYet another characterization utilizes limits:\n\nformula_58\n\nYou can represent partial fractions using the expanded form of divided differences. (This does not simplify computation, but is interesting in itself.) If formula_59 and formula_45 are polynomial functions, where formula_61 and formula_45 is given in terms of linear factors by formula_63, then it follows from partial fraction decomposition that\nIf limits of the divided differences are accepted, then this connection does also hold, if some of the formula_65 coincide.\n\nIf formula_66 is a polynomial function with arbitrary degree\nand it is decomposed by formula_67 using polynomial division of formula_66 by formula_45,\nthen\n\nThe divided differences can be expressed as\n\nwhere formula_72 is a B-spline of degree formula_73 for the data points formula_74 and formula_75 is the formula_76-th derivative of the function formula_66.\n\nThis is called the Peano form of the divided differences and formula_72 is called the Peano kernel for the divided differences, both named after Giuseppe Peano.\n\nIf nodes are cumulated, then the numerical computation of the divided differences is inaccurate, because you divide almost two zeros, each of which with a high relative error due to differences of similar values. However we know, that difference quotients approximate the derivative and vice versa:\n\nThis approximation can be turned into an identity whenever Taylor's theorem applies.\n\nYou can eliminate the odd powers of formula_83 by expanding the Taylor series at the center between formula_84 and formula_85:\n\nThe Taylor series or any other representation with function series can in principle be used to approximate divided differences. Taylor series are infinite sums of power functions. The mapping from a function formula_66 to a divided difference formula_92 is a linear functional. We can as well apply this functional to the function summands.\n\nExpress power notation with an ordinary function: formula_93\n\nRegular Taylor series is a weighted sum of power functions: formula_94\n\nTaylor series for divided differences: formula_95\n\nWe know that the first formula_76 terms vanish, because we have a higher difference order than polynomial order, and in the following term the divided difference is one:\nIt follows that the Taylor series for the divided difference essentially starts with formula_98 which is also a simple approximation of the divided difference, according to the mean value theorem for divided differences.\n\nIf we would have to compute the divided differences for the power functions in the usual way, we would encounter the same numerical problems that we had when computing the divided difference of formula_66. The nice thing is, that there is a simpler way.\nIt holds\nConsequently, we can compute the divided differences of formula_101 by a division of formal power series. See how this reduces to the successive computation of powers when we compute formula_102 for several formula_76.\n\nIf you need to compute a whole divided difference scheme with respect to a Taylor series, see the section about divided differences of power series.\n\nDivided differences of polynomials are particularly interesting, because they can benefit from the Leibniz rule.\nThe matrix formula_104 with\n\ncontains the divided difference scheme for the identity function with respect to the nodes formula_74,\nthus formula_107 contains the divided differences for the power function with exponent formula_76.\nConsequently, you can obtain the divided differences for a polynomial function formula_109\nwith respect to the polynomial formula_59\nby applying formula_59 (more precisely: its corresponding matrix polynomial function formula_112) to the matrix formula_104.\nThis is known as \"Opitz' formula\".\nNow consider increasing the degree of formula_59 to infinity,\ni.e. turn the Taylor polynomial to a Taylor series.\nLet formula_66 be a function which corresponds to a power series.\nYou can compute a divided difference scheme by computing the according matrix series applied to formula_104.\nIf the nodes formula_74 are all equal,\nthen formula_104 is a Jordan block and\ncomputation boils down to generalizing a scalar function to a matrix function using Jordan decomposition.\n\nWhen the data points are equidistantly distributed we get the special case called forward differences. They are easier to calculate than the more general divided differences.\n\nNote that the \"divided portion\" from forward divided difference must still be computed, to recover the forward divided difference from the forward difference.\n\nGiven \"n\" data points\n\nwith\n\nthe divided differences can be calculated via forward differences defined as\n\nThe relationship between divided differences and forward differences is\n\n\n\n"}
{"id": "32335603", "url": "https://en.wikipedia.org/wiki?curid=32335603", "title": "Donaldson–Thomas theory", "text": "Donaldson–Thomas theory\n\nIn mathematics, specifically algebraic geometry, Donaldson–Thomas theory is the theory of Donaldson–Thomas invariants. Given a compact moduli space of sheaves on a Calabi–Yau threefold, its Donaldson–Thomas invariant is the virtual number of its points, i.e., the integral of the cohomology class 1 against the virtual fundamental class. The Donaldson–Thomas invariant is a holomorphic analogue of the Casson invariant. The invariants were introduced by . Donaldson–Thomas invariants have close connections to Gromov–Witten invariants of algebraic three-folds and the theory of stable pairs due to Rahul Pandharipande and Thomas. \n\nDonaldson–Thomas theory is physically motivated by certain BPS states that occur in string and gauge theory.\n\nThe basic idea of Gromov–Witten invariants is to probe the geometry of a space by studying pseudoholomorphic maps from Riemann surfaces to a smooth target. The moduli stack of all such maps admits a virtual fundamental class, and intersection theory on this stack yields numerical invariants that can often contain enumerative information. In similar spirit, the approach of Donaldson–Thomas theory is to study curves in an algebraic three-fold by their equations. More accurately, by studying ideal sheaves on a space. This moduli space also admits a virtual fundamental class and yields certain numerical invariants that are enumerative.\n\nWhereas in Gromov–Witten theory, maps are allowed to be multiple covers and collapsed components of the domain curve, Donaldson–Thomas theory allows for nilpotent information contained in the sheaves, however, these are integer valued invariants. There are deep conjectures due to Davesh Maulik, Andrei Okounkov, Nikita Nekrasov and Pandharipande, proved in increasing generality, that Gromov–Witten and Donaldson–Thomas theories of algebraic three-folds are actually equivalent. More concretely, their generating functions are equal after an appropriate change of variables. For Calabi–Yau threefolds, the Donaldson–Thomas invariants can be formulated as weighted Euler characteristic on the moduli space. There have also been recent connections between these invariants, the motivic Hall algebra, and the ring of functions on the quantum torus.\n\n\n\n\n"}
{"id": "7918341", "url": "https://en.wikipedia.org/wiki?curid=7918341", "title": "Essentially unique", "text": "Essentially unique\n\nIn mathematics, the term essentially unique is used to indicate that while some object is not the only one that satisfies certain properties, all such objects are \"the same\" in some sense appropriate to the circumstances. This notion of \"sameness\" is often formalized using an equivalence relation.\n\nA related notion is a universal property, where an object is not only essentially unique, but unique \"up to a unique isomorphism\" (meaning that it has trivial automorphism group). In general given two isomorphic examples of an essentially unique object, there is no \"natural\" (unique) isomorphism between them.\n\nMost basically, there is an essentially unique set of any given cardinality, whether one labels the elements formula_1 or formula_2.\nIn this case the non-uniqueness of the isomorphism (does one match 1 to \"a\" or to \"c\"?) is reflected in the symmetric group.\n\nOn the other hand, there is an essentially unique \"ordered\" set of any given finite cardinality: if one writes formula_3 and formula_4, then the only order-preserving isomorphism maps 1 to \"a,\" 2 to \"b,\" and 3 to \"c.\"\n\nThe fundamental theorem of arithmetic establishes that the factorization of any positive integer into prime numbers is essentially unique, i.e., unique up to the ordering of the prime factors.\n\nSuppose that we seek to classify all possible groups. We would find that there is an essentially unique group containing exactly 3 elements, the cyclic group of order three. No matter how we choose to write those three elements and denote the group operation, all such groups are isomorphic, hence, \"the same\".\n\nOn the other hand, there is not an essentially unique group with exactly 4 elements, as there are two non-isomorphic examples: the cyclic group of order 4 and the Klein four group.\n\nSuppose that we seek a translation-invariant, strictly positive, locally finite measure on the real line. The solution to this problem is essentially unique: any such measure must be a constant multiple of Lebesgue measure. Specifying that the measure of the unit interval should be 1 then determines the solution uniquely.\n\nSuppose that we seek to classify all two-dimensional, compact, simply connected manifolds. We would find an essentially unique solution to this problem: the 2-sphere. In this case, the solution is unique up to homeomorphism.\n\nIn the area of topology known as knot theory, there is an analogue of the fundamental theorem of arithmetic: the decomposition of a knot into a sum of prime knots is essentially unique.\n\nA maximal compact subgroup of a semisimple Lie group may not be unique, but is unique up to conjugation.\n\nGiven the task of using 24-bit words to store 12 bits of information in such a way that 7-bit errors can be detected and 3-bit errors can be corrected, the solution is essentially unique: the extended binary Golay code.\n\n"}
{"id": "2828651", "url": "https://en.wikipedia.org/wiki?curid=2828651", "title": "Exact cover", "text": "Exact cover\n\nIn mathematics, given a collection formula_1 of subsets of a set \"X\", an exact cover is a subcollection formula_2 of formula_1 such that each element in \"X\" is contained in \"exactly one\" subset in formula_2.\nOne says that each element in \"X\" is covered by exactly one subset in formula_2.\nAn exact cover is a kind of cover.\n\nIn computer science, the exact cover problem is a decision problem to determine if an exact cover exists.\nThe exact cover problem is NP-complete\nand is one of Karp's 21 NP-complete problems.\nThe exact cover problem is a kind of constraint satisfaction problem.\n\nAn exact cover problem can be represented by an incidence matrix or a bipartite graph.\n\nKnuth's Algorithm X is an algorithm that finds all solutions to an exact cover problem. DLX is the name given to Algorithm X when it is implemented efficiently using Donald Knuth's Dancing Links technique on a computer.\n\nThe standard exact cover problem can be generalized slightly to involve not only \"exactly one\" constraints but also \"at-most-one\" constraints.\nFinding Pentomino tilings and solving Sudoku are noteworthy examples of exact cover problems.\nThe N queens problem is a slightly generalized exact cover problem.\n\nGiven a collection formula_1 of subsets of a set \"X\", an exact cover of \"X\" is a subcollection formula_2 of formula_1 that satisfies two conditions:\n\nIn short, an exact cover is \"exact\" in the sense that each element in \"X\" is contained in \"exactly one\" subset in formula_2.\n\nEquivalently, an exact cover of \"X\" is a subcollection formula_2 of formula_1 that partitions \"X\".\n\nFor an exact cover of \"X\" to exist, it is necessary that:\n\nIf the empty set ∅ is contained in formula_1, then it makes no difference whether or not it is in any exact cover.\nThus it is typical to assume that:\n\nLet formula_1 = {\"N\", \"O\", \"P\", \"E\"} be a collection of subsets of a set \"X\" = {1, 2, 3, 4} such that:\n\nThe subcollection {\"O\", \"E\"} is an exact cover of \"X\", since the subsets \"O\" = {1, 3} and \"E\" = {2, 4} are disjoint and their union is \"X\" = {1, 2, 3, 4}.\n\nThe subcollection {\"N\", \"O\", \"E\"} is also an exact cover of \"X\".\nIncluding the empty set \"N\" = { } makes no difference, as it is disjoint with all subsets and does not change the union.\n\nThe subcollection {\"E\", \"P\"} is not an exact cover of \"X\".\nThe intersection of the subsets \"E\" and \"P\", {2}, is not empty:\nThe subsets \"E\" and \"P\" are not disjoint.\nMoreover, the union of the subsets \"E\" and \"P\", {2, 3, 4}, is not \"X\" = {1, 2, 3, 4}:\nNeither \"E\" nor \"P\" covers the element 1.\n\nOn the other hand, there is no exact cover—indeed, not even a cover—of \"Y\" = {1, 2, 3, 4, 5} because formula_24 = {1, 2, 3, 4} is a proper subset of \"Y\":\nNone of the subsets in formula_1 contains the element 5.\n\nLet formula_1 = {\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"} be a collection of subsets\nof a set \"X\" = {1, 2, 3, 4, 5, 6, 7} such that:\n\nThen the subcollection formula_2 = {\"B\", \"D\", \"F\"} is an exact cover, since each element in \"X\" is contained in exactly one of the subsets:\n\nMoreover, {\"B\", \"D\", \"F\"} is the only exact cover, as the following argument demonstrates:\nBecause \"A\" and \"B\" are the only subsets containing 1, an exact cover must contain \"A\" or \"B\", but not both.\nIf an exact cover contains \"A\", then it doesn't contain \"B\", \"C\", \"E\", or \"F\", as each of these subsets has an element in common with \"A\".\nThen \"D\" is the only remaining subset, but the collection {\"A\", \"D\"} doesn't cover the element 2.\nIn conclusion, there is no exact cover containing \"A\".\nOn the other hand, if an exact cover contains \"B\", then it doesn't contain \"A\" or \"C\", as each of these subsets has an element in common with \"B\".\nBecause \"D\" is the only remaining subset containing 5, \"D\" must be part of the exact cover.\nIf an exact cover contains \"D\", then it doesn't contain \"E\", as \"E\" has an element in common with \"D\".\nThen \"F\" is the only remaining subset, and the collection {\"B\", \"D\", \"F\"} is indeed an exact cover.\nSee the example in the article on Knuth's Algorithm X for a matrix-based version of this argument.\n\nAn exact cover problem is defined by the binary relation \"contains\" between subsets in formula_1 and elements in \"X\".\nThere are different equivalent ways to represent this relation.\n\nThe standard way to represent the relation \"contains\" is to list the elements in each subset.\n\nFor example, the detailed example above uses this standard representation:\n\nAgain, the subcollection formula_2 = {\"B\", \"D\", \"F\"} is an exact cover, since each element is contained in exactly one selected subset, as the highlighting makes clear.\n\nThe relation \"contains\" between subsets and elements can be converted, listing the subsets in which each element is contained. \n\nFor example, the relation \"contains\" in the detailed example above can be represented by listing the subsets each element is contained in:\n\nAgain, the subcollection formula_2 = {\"B\", \"D\", \"F\"} is an exact cover, since each element is contained in exactly one selected subset, as the highlighting makes clear.\n\nWhen solving an exact cover problem, it is often useful to switch between the standard and inverse representations.\nis an element of\n\nThe relation \"contains\" can be represented by an incidence matrix. \n\nThe matrix includes one row for each subset in formula_1 and one column for each element in \"X\".\nThe entry in a particular row and column is 1 if the corresponding subset contains the corresponding element, and is 0 otherwise.\nAs each row represents the elements contained in the corresponding subset and each column represents the subsets containing the corresponding element, an incidence matrix effectively provides both the standard and inverse representations.\n\nIn the matrix representation, an exact cover is a selection of rows such that each column contains a 1 in exactly one selected row.\n\nFor example, the relation \"contains\" in the detailed example above can be represented by a 6×7 incidence matrix:\n\nAgain, the subcollection formula_2 = {\"B\", \"D\", \"F\"} is an exact cover, since each element is contained in exactly one selected subset, i.e., each column contains a 1 in exactly one selected row, as the highlighting makes clear.\n\nSee the example in the article on Knuth's Algorithm X for a matrix-based solution to the detailed example above.\n\nIn turn, the incidence matrix can be seen also as describing a hypergraph. The hypergraph includes one node for each element in \"X\" and one edge for each subset in formula_1; each node is included in exactly one of the edges forming the cover.\n\nThe relation \"contains\" can be represented by a bipartite graph.\n\nThe vertices of the graph are divided into two disjoint sets, one representing the subsets in formula_1 and another representing the elements in \"X\".\nIf a subset contains an element, an edge connects the corresponding vertices in the graph.\n\nIn the graph representation, an exact cover is a selection of vertices corresponding to subsets such that each vertex corresponding to an element is connected to exactly one selected vertex.\n\nFor example, the relation \"contains\" in the detailed example above can be represented by a bipartite graph with 6+7 = 13 vertices:\n\nAgain, the subcollection formula_2 = {\"B\", \"D\", \"F\"} is an exact cover, since each element is contained in exactly one selected subset, i.e., the vertex corresponding to each element in \"X\" is connected to exactly one selected vertex, as the highlighting makes clear.\n\nAlthough the canonical exact cover problem involves a collection formula_1 of subsets of a set \"X\", the logic does not depend on the presence of subsets containing elements.\nAn \"abstract exact cover problem\" arises whenever there is a heterogeneous relation between two sets \"P\" and \"Q\" and the goal is to select a subset \"P*\" of \"P\" such that each element in \"Q\" is related to \"exactly one\" element in \"P*\".\nIn general, the elements of \"P\" represent choices and the elements of \"Q\" represent \"exactly one\" constraints on those choices.\n\nMore formally, given a binary relation \"R\" formula_37 \"P\" × \"Q\" between sets \"P\" and \"Q\", one can call a subset \"P*\" of \"P\" an \"abstract exact cover\" of \"Q\" if each element in \"Q\" is \"R\"-related to exactly one element in \"P*\". Here \"R\" is the converse of \"R\".\n\nIn general, \"R\" restricted to \"Q\" × \"P*\" is a function from \"Q\" to \"P*\", which maps each element in \"Q\" to the unique element in \"P*\" that is \"R\"-related to that element in \"Q\".\nThis function is onto, unless \"P*\" contains the \"empty set,\" i.e., an element which isn't \"R\"-related to any element in \"Q\".\n\nIn the canonical exact cover problem, \"P\" is a collection formula_1 of subsets of \"X\", \"Q\" is the set \"X\", \"R\" is the binary relation \"contains\" between subsets and elements, and \"R\" restricted to \"Q\" × \"P*\" is the function \"is contained in\" from elements to selected subsets.\n\nIn mathematics, given a collection formula_1 of subsets of a set \"X\", an exact hitting set \"X*\" is a subset of \"X\" such that each subset in formula_1 contains \"exactly one\" element in \"X*\". One says that each subset in formula_1 is hit by exactly one element in \"X*\".\n\nIn computer science, the exact hitting set problem is a decision problem to find an exact hitting set or else determine none exists.\n\nThe exact hitting set problem is an abstract exact cover problem.\nIn the notation above, \"P\" is the set \"X\", \"Q\" is a collection formula_1 of subsets of \"X\", \"R\" is the binary relation \"is contained in\" between elements and subsets, and \"R\" restricted to \"Q\" × \"P*\" is the function \"contains\" from subsets to selected elements.\n\nWhereas an exact cover problem involves selecting subsets and the relation \"contains\" from subsets to elements, an exact hitting set problem involves selecting elements and the relation \"is contained in\" from elements to subsets.\nIn a sense, an exact hitting set problem is the inverse of the exact cover problem involving the same set and collection of subsets.\n\nAs in the detailed exact cover example above, let formula_1 = {\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"} be a collection of subsets of a set \"X\" = {1, 2, 3, 4, 5, 6, 7} such that:\n\nThen \"X*\" = {1, 2, 5} is an exact hitting set, since each subset in formula_1 contains exactly one element in \"X*\", as the highlighting makes clear.\n\nMoreover, {1, 2, 5} is the only exact hitting set, as the following argument demonstrates:\nBecause 2 and 7 are the only elements that hit \"F\", an exact hitting set must contain 2 or 7, but not both.\nIf an exact hitting set contains 7, then it doesn't contain 1, 2, 3, 4, 5, or 6, as each of these elements are contained in some subset also containing 7.\nThen there are no more remaining elements, but {7} is not an exactly hitting set, as it doesn't hit \"B\" or \"D\".\nIn conclusion, there is no exact hitting set containing 7.\nOn the other hand, if an exact hitting set contains 2, then it doesn't contain 3, 6, or 7, as each of these elements are contained in some subset also containing 2.\nBecause 5 is the only remaining element that hits \"D\", the exact hitting set must contain 5.\nIf an exact hitting set contains 5, then it doesn't contain 4, as both hit \"C\".\nBecause 1 is the only remaining element that hits \"A\", the exact hitting set must contain 1.\nThen there are no more remaining elements, and {1, 2, 5} is indeed an exact hitting set.\n\nAlthough this example involves the same collection of subsets as the detailed exact cover example above, it is essentially a different problem. In a sense, the exact hitting set problem is the inverse (or transpose or converse) of the corresponding exact cover problem above, as the matrix representation makes clear:\n\nBut there is another exact hitting set problem that is essentially the same as the detailed exact cover example above, in which numbered elements become subsets and lettered subsets become elements, effectively inverting the relation between subsets and element.\n\nFor example, as the subset \"B\" contains the elements 1 and 4 in the exact cover problem, the subsets \"I\" and \"IV\" contain the element \"b\" in the dual exact hitting set problem.\n\nIn particular, let formula_1 = {\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"} be a collection of subsets of a set \"X\" = {\"a\", \"b\", \"c\", \"d\", \"e\", \"f\"} such that:\n\nThen \"X*\" = {\"b\", \"d\", \"f\"} is an exact hitting set, since each subset in formula_1 contains (is hit by) exactly one element in \"X*\", as the highlighting makes clear.\n\nThe exact hitting set \"X*\" = {\"b\", \"d\", \"f\"} here is essentially the same as the exact cover formula_2 = {\"B\", \"D\", \"F\"} above, as the matrix representation makes clear:\n\nAlgorithm X is the name Donald Knuth gave for \"the most obvious trial-and-error approach\" for finding all solutions to the exact cover problem. Technically, Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm.\n\nWhen Algorithm X is implemented efficiently using Donald Knuth's Dancing Links technique on a computer, Knuth calls it DLX. DLX uses the matrix representation of the problem, implemented as a series of doubly linked lists of the 1s of the matrix: each 1 element has a link to the next 1 above, below, to the left, and to the right of itself. (Technically, because the lists are circular, this forms a torus). Because exact cover problems tend to be sparse, this representation is usually much more efficient in both size and processing time required. DLX then uses the Dancing Links technique to quickly select permutations of rows as possible solutions and to efficiently backtrack (undo) mistaken guesses.\n\nIn a standard exact cover problem, each constraint must be satisfied exactly once.\nIt is a simple generalization to relax this requirement slightly and allow for the possibility that some \"primary\" constraints must be satisfied by \"exactly one\" selection but other \"secondary\" constraints can be satisfied by \"at most one\" selection.\n\nAs Knuth explains, a generalized exact cover problem can be converted to an equivalent exact cover problem by simply appending one row for each secondary column, containing a single 1 in that column. If in a particular candidate solution a particular secondary column is satisfied, then the added row isn't needed.\nBut if the secondary column isn't satisfied, as is allowed in the generalized problem but not the standard problem, then the added row can be selected to ensure the column is satisfied.\n\nBut Knuth goes on to explain that it is better working with the generalized problem directly, because the generalized algorithm is simpler and faster:\nA simple change to his Algorithm X allows secondary columns to be handled directly.\n\nThe N queens problem is an example of a generalized exact cover problem, as the constraints corresponding to the diagonals of the chessboard have a maximum rather than an exact queen count.\n\nDue to its NP-completeness, any problem in NP can be reduced to exact cover problems, which then can be solved with techniques such as Dancing Links.\nHowever, for some well known problems, the reduction is particularly direct.\nFor instance, the problem of tiling a board with pentominoes, and solving Sudoku can both be viewed as exact cover problems.\n\nThe problem of tiling a 60-square board with the 12 different free pentominoes is an example of an exact cover problem, as Donald Knuth explains in his paper \"Dancing links.\"\n\nFor example, consider the problem of tiling with pentominoes an 8×8 chessboard with the 4 central squares removed:\n\nThe problem involves two kinds of constraints:\n\nThus there are 12+60 = 72 constraints in all.\n\nAs both kinds of constraints are \"exactly one\" constraints, the problem is an exact cover problem.\n\nThe problem involves many choices, one for each way to place a pentomino on the board.\nIt is convenient to consider each choice as satisfying a set of 6 constraints: 1 constraint for the pentomino being placed and 5 constraints for the five squares where it is placed.\n\nIn the case of an 8×8 chessboard with the 4 central squares removed, there are 1568 such choices, for example:\n\nOne of many solutions of this exact cover problem is the following set of 12 choices:\n\nThis set of choices corresponds to the following solution to the pentomino tiling problem:\n\nA pentomino tiling problem is more naturally viewed as an exact cover problem than an exact hitting set problem, because it is more natural to view each choice as a set of constraints than each constraint as a set of choices.\n\nEach choice relates to just 6 constraints, which are easy to enumerate. On the other hand, each constraint relates to many choices, which are harder to enumerate.\n\nWhether viewed as an exact cover problem or an exact hitting set problem, the matrix representation is the same, having 1568 rows corresponding to choices and 72 columns corresponding to constraints.\nEach row contains a single 1 in the column identifying the pentomino and five 1s in the columns identifying the squares covered by the pentomino.\n\nUsing the matrix, a computer can find all solutions relatively quickly, for example, using Dancing Links.\n\n\"Main articles: Sudoku, Sudoku solving algorithms\"\n\nThe problem in Sudoku is to assign numbers (or digits, values, symbols) to cells (or squares) in a grid so as to satisfy certain constraints.\n\nIn the standard 9×9 Sudoku variant, there are four kinds of constraints:\n\nWhile the first constraint might seem trivial, it is nevertheless needed to ensure there is only one number per cell. Intuitively, placing a number into a cell prohibits placing that number in any other cell sharing the same column, row, or box and also prohibits \"placing any other number\" into the now occupied cell. \n\nSolving Sudoku is an exact cover problem.\n\nMore precisely, solving Sudoku is an exact hitting set problem, which is equivalent to an exact cover problem, when viewed as a problem to select possibilities such that each constraint set contains (i.e., is hit by) exactly one selected possibility.\nIn the notation above for the (generalized) exact cover problem, \"X\" is the set of possibilities, \"Y\" is a set of constraint sets, and \"R\" is the binary relation \"is contained in.\"\n\nEach possible assignment of a particular number to a particular cell is a possibility (or candidate).\nWhen Sudoku is played with pencil and paper, possibilities are often called pencil marks.\n\nIn the standard 9×9 Sudoku variant, in which each of 9×9 cells is assigned one of 9 numbers, there are 9×9×9=729 possibilities.\nUsing obvious notation for rows, columns and numbers, the possibilities can be labeled\n\nThe fact that each kind of constraint involves exactly one of something is what makes Sudoku an exact hitting set problem.\nThe constraints can be represented by constraint sets.\nThe problem is to select possibilities such that each constraint set contains (i.e., is hit by) exactly one selected possibility.\n\nIn the standard 9×9 Sudoku variant, there are four kinds of constraints sets corresponding to the four kinds of constraints:\n\nSince there are 9 rows, 9 columns, 9 boxes and 9 numbers, there are 9×9=81 row-column constraint sets, 9×9=81 row-number constraint sets, 9×9=81 column-number constraint sets, and 9×9=81 box-number constraint sets: 81+81+81+81=324 constraint sets in all.\n\nIn brief, the standard 9×9 Sudoku variant is an exact hitting set problem with 729 possibilities and 324 constraint sets.\nThus the problem can be represented by a 729×324 matrix.\n\nAlthough it is difficult to present the entire 729×324 matrix, the general nature of the matrix can be seen from several snapshots:\nThe complete 729×324 matrix is available from Bob Hanson.\n\nNote that the set of possibilities R\"x\"C\"y\"#\"z\" can be arranged as a 9×9×9 cube in a 3-dimensional space with coordinates \"x\", \"y\", and \"z\".\nThen each row R\"x\", column C\"y\", or number #\"z\" is a 9×9×1 \"slice\" of possibilities; each box B\"w\" is a 9x3×3 \"tube\" of possibilities; each row-column constraint set R\"x\"C\"y\", row-number constraint set R\"x\"#\"z\", or column-number constraint set C\"y\"#\"z\" is a 9x1×1 \"strip\" of possibilities; each box-number constraint set B\"w\"#\"z\" is a 3x3×1 \"square\" of possibilities; and each possibility R\"x\"C\"y\"#\"z\" is a 1x1×1 \"cubie\" consisting of a single possibility.\nMoreover, each constraint set or possibility is the intersection of the component sets.\nFor example, R1C2#3 = R1 ∩ C2 ∩ #3, where ∩ denotes set intersection.\n\nAlthough other Sudoku variations have different numbers of rows, columns, numbers and/or different kinds of constraints, they all involve possibilities and constraint sets, and thus can be seen as exact hitting set problems.\n\n \n\nThe \"N\" queens problem is an example of a generalized exact cover problem. The problem involves four kinds of constraints:\n\nNote that the 2\"N\" rank and file constraints form the primary constraints, while the 4\"N\" − 2 diagonal and reverse diagonals form the secondary constraints. Further, because each of first and last diagonal and reverse diagonals involves only one square on the chessboard, these can be omitted and thus one can reduce the number of secondary constraints to 4\"N\" − 6. The matrix for the \"N\" queens problem then has \"N\" rows and 6\"N\" − 6 columns, each row for a possible queen placement on each square on the chessboard, and each column for each constraint.\n\n\n\n"}
{"id": "23467710", "url": "https://en.wikipedia.org/wiki?curid=23467710", "title": "Exponential map (discrete dynamical systems)", "text": "Exponential map (discrete dynamical systems)\n\nIn the theory of dynamical systems, the exponential map can be used as the evolution function of the discrete nonlinear dynamical system.\n\nThe family of exponential functions is called the exponential family.\n\nThere are many forms of these maps, many of which are equivalent under a coordinate transformation. For example two of the most common ones are:\n\n\nThe second one can be mapped to the first using the fact that formula_3, so formula_4 is the same under the transformation formula_5. The only difference is that, due to multi-valued properties of exponentiation, there may be a few select cases that can only be found in one version. Similar arguments can be made for many other formulas.\n"}
{"id": "51698", "url": "https://en.wikipedia.org/wiki?curid=51698", "title": "Extended real number line", "text": "Extended real number line\n\nIn mathematics, the affinely extended real number system is obtained from the real number system by adding two elements: and (read as positive infinity and negative infinity respectively). These new elements are not real numbers. It is useful in describing various limiting behaviors in calculus and mathematical analysis, especially in the theory of measure and integration. The affinely extended real number system is denoted formula_1 or or .\n\nWhen the meaning is clear from context, the symbol is often written simply as .\n\nWe often wish to describe the behavior of a function formula_2, as either the argument formula_3 or the function value formula_2 gets \"very big\" in some sense. For example, consider the function\n\nThe graph of this function has a horizontal asymptote at y = 0. Geometrically, as we move farther and farther to the right along the formula_3-axis, the value of formula_7 approaches 0. This limiting behavior is similar to the limit of a function at a real number, except that there is no real number to which formula_3 approaches.\n\nBy adjoining the elements formula_9 and formula_10 to formula_11, we allow a formulation of a \"limit at infinity\" with topological properties similar to those for formula_11.\n\nTo make things completely formal, the Cauchy sequences definition of formula_11 allows us to define formula_9 as the set of all sequences of rationals which, for any formula_15, from some point on exceed formula_16. We can define formula_10 similarly.\n\nIn measure theory, it is often useful to allow sets that have infinite measure and integrals whose value may be infinite.\n\nSuch measures arise naturally out of calculus. For example, in assigning a measure to formula_11 that agrees with the usual length of intervals, this measure must be larger than any finite real number. Also, when considering improper integrals, such as\n\nthe value \"infinity\" arises. Finally, it is often useful to consider the limit of a sequence of functions, such as \n\nWithout allowing functions to take on infinite values, such essential results as the monotone convergence theorem and the dominated convergence theorem would not make sense.\n\nThe affinely extended real number system turns into a totally ordered set by defining formula_21 for all formula_22. This order has the desirable property that every subset has a supremum and an infimum: it is a complete lattice.\n\nThis induces the order topology on formula_1. In this topology, a set formula_24 is a neighborhood of formula_9 if and only if it contains a set formula_26 for some real number formula_22, and analogously for the neighborhoods of formula_10. formula_29 is a compact Hausdorff space homeomorphic to the unit interval formula_30. Thus the topology is metrizable, corresponding (for a given homeomorphism) to the ordinary metric on this interval. There is no metric that is an extension of the ordinary metric on formula_11.\n\nWith this topology the specially defined limits for formula_3 tending to formula_9 and formula_10, and the specially defined concepts of limits equal to formula_9 and formula_10, reduce to the general topological definitions of limits.\n\nThe arithmetic operations of formula_11 can be partially extended to formula_38 as follows:\n\nFor exponentiation, see Exponentiation#Limits of powers.\nHere, \"formula_40\" means both \"formula_41\" and \"formula_42\", while \"formula_43\" means both \"formula_44\" and \n\nThe expressions formula_45 and formula_46 (called indeterminate forms) are usually left undefined. These rules are modeled on the laws for infinite limits. However, in the context of probability or measure theory, formula_47 is often defined as formula_48.\n\nThe expression formula_49 is not defined either as formula_9 or formula_10, because although it is true that whenever formula_52 for a continuous function formula_2 it must be the case that formula_54 is eventually contained in every neighborhood of the set formula_55, it is \"not\" true that formula_54 must tend to one of these points. An example is formula_57 which is of the form formula_54 but does not tend to either formula_9 or formula_10 when formula_61. For instance, formula_62 but formula_63 does not exist because formula_64 but formula_65. (The modulus formula_66, nevertheless, does approach formula_9.)\n\nWith these definitions formula_38 is not even a semigroup, let alone a group, a ring or a field, like formula_11 is one. However, it still has several convenient properties:\n\nIn general, all laws of arithmetic are valid in formula_38 as long as all occurring expressions are defined.\n\nSeveral functions can be continuously extended to formula_38 by taking limits. For instance, one defines formula_91 etc.\n\nSome singularities may additionally be removed. For example, the function formula_92 can be continuously extended to formula_38 (under \"some\" definitions of continuity) by setting the value to formula_9 for formula_95, and formula_48 for formula_97 and formula_98. The function formula_99 can \"not\" be continuously extended because the function approaches formula_10 as formula_3 approaches 0 from below, and formula_9 as formula_3 approaches formula_48 from above.\n\nCompare the projectively extended real line, which does not distinguish between formula_9 and formula_10. As a result, on one hand a function may have limit formula_107 on the projectively extended real line, while in the affinely extended real number system only the absolute value of the function has a limit, e.g. in the case of the function formula_99 at formula_95. On the other hand\ncorrespond on the projectively extended real line to only a limit from the right and one from the left, respectively, with the full limit only existing when the two are equal. Thus formula_112 and formula_113 cannot be made continuous at formula_114 on the projectively extended real line.\n\n\n"}
{"id": "1046024", "url": "https://en.wikipedia.org/wiki?curid=1046024", "title": "Formal equivalence checking", "text": "Formal equivalence checking\n\nFormal equivalence checking process is a part of electronic design automation (EDA), commonly used during the development of digital integrated circuits, to formally prove that two representations of a circuit design exhibit exactly the same behavior.\n\nIn general, there is a wide range of possible definitions of functional equivalence covering comparisons between different levels of abstraction and varying granularity of timing details. \n\nThe register transfer level (RTL) behavior of a digital chip is usually described with a hardware description language, such as Verilog or VHDL. This description is the golden reference model that describes in detail which operations will be executed during which clock cycle and by which pieces of hardware. Once the logic designers, by simulations and other verification methods, have verified register transfer description, the design is usually converted into a netlist by a logic synthesis tool. Equivalence is not to be confused with functional correctness, which must be determined by functional verification.\n\nThe initial netlist will usually undergo a number of transformations such as optimization, addition of Design For Test (DFT) structures, etc., before it is used as the basis for the placement of the logic elements into a physical layout. Contemporary physical design software will occasionally also make significant modifications (such as replacing logic elements with equivalent similar elements that have a higher or lower drive strength and/or area) to the netlist. Throughout every step of a very complex, multi-step procedure, the original functionality and the behavior described by the original code must be maintained. When the final tape-out is made of a digital chip, many different EDA programs and possibly some manual edits will have altered the netlist.\n\nIn theory, a logic synthesis tool guarantees that the first netlist is logically equivalent to the RTL source code. All the programs later in the process that make changes to the netlist also, in theory, ensure that these changes are logically equivalent to a previous version.\n\nIn practice, programs have bugs and it would be a major risk to assume that all steps from RTL through the final tape-out netlist have been performed without error. Also, in real life, it is common for designers to make manual changes to a netlist, commonly known as Engineering Change Orders, or ECOs, thereby introducing a major additional error factor. Therefore, instead of blindly assuming that no mistakes were made, a verification step is needed to check the logical equivalence of the final version of the netlist to the original description of the design (golden reference model).\n\nHistorically, one way to check the equivalence was to re-simulate, using the final netlist, the test cases that were developed for verifying the correctness of the RTL. This process is called gate level logic simulation. However, the problem with this is that the quality of the check is only as good as the quality of the test cases. Also, gate-level simulations are notoriously slow to execute, which is a major problem as the size of digital designs continues to grow exponentially.\n\nAn alternative way to solve this is to formally prove that the RTL code and the netlist synthesized from it have exactly the same behavior in all (relevant) cases. This process is called formal equivalence checking and is a problem that is studied under the broader area of formal verification.\n\nA formal equivalence check can be performed between any two representations of a design: RTL <> netlist, netlist <> netlist or RTL <> RTL, though the latter is rare compared to the first two. Typically, a formal equivalence checking tool will also indicate with great precision at which point there exists a difference between two representations.\n\nThere are two basic technologies used for boolean reasoning in equivalence checking programs:\n\nMajor products in the Logic Equivalence Checking (\"LEC\") area of \"EDA\" are:\n\n\n\n\n\n"}
{"id": "38755390", "url": "https://en.wikipedia.org/wiki?curid=38755390", "title": "Future Orientation Index", "text": "Future Orientation Index\n\nThe Future Orientation Index was introduced by Tobias Preis, Helen Susannah Moat, H. Eugene Stanley and Steven Bishop using Google Trends to demonstrate that Google users from countries with a higher per capita GDP are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviour and real-world economic indicators. The authors of the study examined Google query logs made by Google users in 45 different countries in 2010 and calculated the ratio of the volume of searches for the coming year (‘2011’) to the volume of searches for the previous year (‘2009’).\n"}
{"id": "15841869", "url": "https://en.wikipedia.org/wiki?curid=15841869", "title": "Generalized logistic distribution", "text": "Generalized logistic distribution\n\nThe term generalized logistic distribution is used as the name for several different families of probability distributions. For example, Johnson et al. list four forms, which are listed below. One family described here has also been called the skew-logistic distribution. For other families of distributions that have also been called generalized logistic distributions, see the shifted log-logistic distribution, which is a generalization of the log-logistic distribution.\n\nThe following definitions are for standardized versions of the families, which can be expanded to the full form as a location-scale family. Each is defined using either the cumulative distribution function (\"F\") or the probability density function (\"ƒ\"), and is defined on (-∞,∞).\n\nThe corresponding probability density function is:\nThis type has also been called the \"skew-logistic\" distribution.\n\nThe corresponding probability density function is:\n\nHere \"B\" is the beta function. The moment generating function for this type is\nThe corresponding cumulative distribution function is:\n\nAgain, \"B\" is the beta function. The moment generating function for this type is\nThis type is also called the \"exponential generalized beta of the second type\".\n\nThe corresponding cumulative distribution function is:\n\n"}
{"id": "4358338", "url": "https://en.wikipedia.org/wiki?curid=4358338", "title": "Halpern–Läuchli theorem", "text": "Halpern–Läuchli theorem\n\nIn mathematics, the Halpern–Läuchli theorem is a partition result about finite products of infinite trees. Its original purpose was to give a model for set theory in which the Boolean prime ideal theorem is true but the axiom of choice is false. It is often called the Halpern–Läuchli theorem, but the proper attribution for the theorem as it is formulated below is to Halpern–Läuchli–Laver–Pincus or HLLP (named after James D. Halpern, Hans Läuchli, Richard Laver, and David Pincus), following .\n\nLet d,r < ω, formula_1 be a sequence of finitely splitting trees of height ω. Let \n\nthen there exists a sequence of subtrees formula_3 strongly embedded in formula_1 such that\n\nAlternatively, let\n\nand\n\nThe HLLP theorem says that not only is the collection formula_8 partition regular for each \"d\" < \"ω\", but that the homogeneous subtree guaranteed by the theorem is strongly embedded in\n\n"}
{"id": "944852", "url": "https://en.wikipedia.org/wiki?curid=944852", "title": "Herbert Marsh", "text": "Herbert Marsh\n\nHerbert Marsh DD (10 December 1757 – 1 May 1839) was a bishop in the Church of England.\n\nThe son of Richard Marsh (1709–1779), Vicar of Faversham in Kent, Marsh was born there and educated at Faversham Grammar School, the King's School, Canterbury, and St John's College, Cambridge, where he graduated BA as second wrangler and was elected a fellow of St John's in 1779, the year of the death of his father. He won prizes in 1780 and 1781, proceeded to MA in 1782 and to Bachelor of Divinity in 1792.\n\nWhile retaining his fellowship at St John's, Marsh studied with J. D. Michaelis at Halle in Prussia and learned the higher criticism. When he returned to England, he translated Michaelis's \"Introduction to the New Testament\" and added to it his own hypothesis on the problem of the Synoptic Gospels. Arguing from textual analysis, he advanced a proto-gospel hypothesis, a variant and modification of the contemporary claim by Johann Gottfried Eichhorn. His \"Dissertation\" (1801) deduced that there had been an original Aramaean gospel-narrative which had been translated into Greek, and had been circulated in copies into which additional information was afterwards added or interpolated. St Mark (he claimed) had had access to two such copies containing variant additions (some of which had been interpolated into those texts), and drew upon both copies when compiling his own Gospel. These same two copies each then independently received further additions (from a \"Gnomology\" or Hebrew document of sayings and precepts of Christ), before one of them was employed by St Matthew, and the other by St Luke, when compiling their Gospels. He drew in the claim that St Matthew's Gospel had originally been written in Hebrew, and that when it was afterwards translated into Greek the translator was able to make use of passages for which he found existing Greek versions in St Mark and St Luke. His hypothesis, now itself superseded, in its time offered a challenge to the conventional or received explanations. It brought him under attack from the conservatives of his church, and into a published debate with John Randolph, then Bishop of Oxford and Regius Professor of Greek in the University of Oxford.\n\nHe was Junior Bursar of St John's for the year 1801–1802. In 1805 he began to preach against Calvinism, and in particular against the doctrines of justification by faith and the inadmissibility of grace, which brought him into conflict with the Evangelicals. In 1807 he resigned his fellowship at St John's on being elected as the Lady Margaret's Professor of Divinity at Cambridge and began presenting lectures there on Higher Criticism. He was the first person in the theological school there to give his lectures in English rather than the traditional Latin. In 1808 he was awarded the Oxford degree of Doctor of Divinity, before in 1816 he was appointed the bishop of Llandaff. In 1819 he was translated to Peterborough.\n\nAs a bishop, Marsh was controversial for preaching against the Evangelicals and for refusing to license clergy with Calvinist beliefs (for which he incurred the ire of Sydney Smith). He was a rigorous proponent of strict ecclesiastical conformity.\n\n\n"}
{"id": "9155837", "url": "https://en.wikipedia.org/wiki?curid=9155837", "title": "Hyperbolization theorem", "text": "Hyperbolization theorem\n\nIn geometry, Thurston's geometrization theorem or hyperbolization theorem implies that closed atoroidal Haken manifolds are hyperbolic, and in particular satisfy the Thurston conjecture.\n\nOne form of Thurston's geometrization theorem states:\nIf \"M\" is a compact irreducible atoroidal Haken manifold whose boundary has zero Euler characteristic, then the interior of \"M\" has a complete hyperbolic structure of finite volume.\n\nThe Mostow rigidity theorem implies that if a manifold of dimension at least 3 has a hyperbolic structure of finite volume, then it is essentially unique.\n\nThe conditions that the manifold \"M\" should be irreducible and atoroidal are necessary, as hyperbolic manifolds have these properties. However the condition that the manifold be Haken is unnecessarily strong. Thurston's hyperbolization conjecture states that a closed irreducible atoroidal 3-manifold with infinite fundamental group is hyperbolic, and this follows from Perelman's proof of the Thurston geometrization conjecture.\n\n showed that if a compact 3 manifold is prime, homotopically atoroidal, and has non-empty boundary, then it has a complete hyperbolic structure unless it is homeomorphic to a certain manifold (\"T\"×[0,1])/Z/2Z with boundary \"T\".\n\nA hyperbolic structure on the interior of a compact orientable 3-manifold has finite volume if and only if all boundary components are tori, except for the manifold \"T\"×[0,1] which has a hyperbolic structure but none of finite volume .\n\nThurston never published a complete proof of his theorem for reasons that he explained in , though parts of his argument are contained in . and gave summaries of Thurston's proof. gave a proof in the case of manifolds that fiber over the circle, and and gave proofs for the generic case of manifolds that do not fiber over the circle. Thurston's geometrization theorem also follows from Perelman's proof using Ricci flow of the more general Thurston geometrization conjecture.\n\nThurston's original argument for this case was summarized by . \n\nThurston's geometrization theorem in this special case states that if \"M\" is a 3-manifold that fibers over the circle and whose monodromy is a pseudo-Anosov diffeomorphism, then the interior of \"M\" has a complete hyperbolic metric of finite volume.\n\n and gave proofs of Thurston's theorem for the generic case of manifolds that do not fiber over the circle.\n\nThe idea of the proof is to cut a Haken manifold \"M\" along an incompressible surface, to obtain a new manifold \"N\". By induction one assumes that the interior of \"N\" has a hyperbolic structure, and the problem is to modify it so that it can be extended to the boundary of \"N\" and glued together. Thurston showed that this follows from the existence of a fixed point for a map of Teichmuller space called the skinning map. The core of the proof of the geometrization theorem is to prove that if \"N\" is not an interval bundle over an interval and \"M\" is an atoroidal then the skinning map has a fixed point. (If \"N\" is an interval bundle then the skinning map has no fixed point, which is why one needs a separate argument when \"M\" fibers over the circle.) gave a new proof of the existence of a fixed point of the skinning map.\n\n"}
{"id": "46934541", "url": "https://en.wikipedia.org/wiki?curid=46934541", "title": "Inserter category", "text": "Inserter category\n\nIn category theory, a branch of mathematics, the inserter category is a variation of the comma category where the two functors are required to have the same domain category.\n\nIf \"C\" and \"D\" are two categories and \"F\" and \"G\" are two functors from \"C\" to \"D\", the inserter category \"Ins\"(\"F\",\"G\") is the category whose objects are pairs (\"X\",\"f\") where \"X\" is an object of \"C\" and \"f\" is a morphism in \"D\" from \"F\"(\"X\") to \"G\"(\"X\") and whose morphisms from (\"X\",\"f\") to (\"Y\",\"g\") are morphisms \"h\" in \"C\" from \"X\" to \"Y\" such that formula_1.\n\nIf \"C\" and \"D\" are locally presentable, \"F\" and \"G\" are functors from \"C\" to \"D\", and either \"F\" is cocontinuous or \"G\" is continuous; then the inserter category \"Ins\"(\"F\",\"G\") is also locally presentable.\n"}
{"id": "21138303", "url": "https://en.wikipedia.org/wiki?curid=21138303", "title": "Inverse consequences", "text": "Inverse consequences\n\nThe term \"inverse consequences\" or the \"Law of Inverse Consequences\" refers to results that are the opposite of the expected results as initially intended or planned.\nOne consequence is in the \"reverse predicament\" of the other.\n\nThe term \"inverse consequences\" has been in use for over 175 years (since at least 1835).\nThe term was also used by Auguste Comte (1798–1857) in his book \"System of Positive Polity\" (published 1875), stating, \"Inevitable increase in Complication, in proportion with the decrease of Generality, gives rise to two inverse consequences.\"\nThe term \"inverse consequences\" has been applied in numerous situations, for example:\n\n\nThe concept of \"inverse consequences\" has a corollary in other phrases, as well:\n\n\n\n"}
{"id": "38295848", "url": "https://en.wikipedia.org/wiki?curid=38295848", "title": "Irene Fonseca", "text": "Irene Fonseca\n\nIrene Maria Quintanilha Coelho da Fonseca is a Portuguese-American applied mathematician, the Mellon College of Science Professor of Mathematics at Carnegie Mellon University, where she directs the Center for Nonlinear Analysis.\n\nFonseca was born in Portugal, and did her undergraduate studies at the University of Lisbon. She earned a Ph.D. from the University of Minnesota in 1985, under the supervision of David Kinderlehrer, who later followed his student to CMU. She joined the CMU faculty after postdoctoral studies in Paris, France.\n\nIn 2011, Fonseca was elected president of the Society for Industrial and Applied Mathematics.\n\nFonseca is the co-author of:\n\nFonseca is a knight of the Order of Saint James of the Sword.\nIn 2009, Fonseca was elected as a fellow of SIAM \"for contributions to nonlinear partial differential equations and the calculus of variations\". In 2012 she became a fellow of the American Mathematical Society.\n\nFonseca is married to stem cell researcher Gerald Schatten.\n"}
{"id": "48050254", "url": "https://en.wikipedia.org/wiki?curid=48050254", "title": "Isabella Bashmakova", "text": "Isabella Bashmakova\n\nIsabella Grigoryevna Bashmakova (, 1921–2005) was a Russian historian of mathematics.\n\nBashmakova was born on January 3, 1921, in Rostov-on-Don, to a family of Armenian descent. Her father, Grigory Georgiyevich Bashmakov, was a lawyer. Her family moved to Moscow in 1932. She began studies in the Faculty of Mechanics and Mathematics at Moscow State University in 1938, but was evacuated from Moscow during World War II, during which she served as a nurse in Samarkand. She completed a Ph.D. in 1948, under the supervision of Sofya Yanovskaya.\n\nShe continued at Moscow State as an assistant professor, and in 1949 was promoted to associate professor. In 1950 her husband, mathematician Andrei I. Lapin, was arrested for his opposition to Lysenkoism, but in part due to Bashmakova's efforts he was freed again in 1952. Bashmakova completed her D.Sc. in 1961 and became a full professor in 1968.\n\nShe retired and become a professor emeritus in 1999, and died on July 17, 2005 while vacationing in Zvenigorod.\n\nBashmakova's dissertation concerned the history of definitions of integers and rational numbers, from Euclid and Eudoxus to Zolotarev, Dedekind, and Kronecker.\n\nHer later research contributions include a comparison of the tools used by Diophantus to solve Diophantine equation, versus more modern methods; following a line of thought suggested by Jacobi, she suggested that Diophantus' methods were more sophisticated than previously thought, but that their sophistication had been hidden by the emphasis on specific cases in Diophantus's writings. She used complex numbers to reinterpret the geometric transformations studied by François Viète. She has also studied the history of algebraic curves, and translated the works of Fermat into Russian.\n\nIn 1986, the International Congress of Mathematicians initially published a list of speakers that included no women. After protests, the executive committee of the congress invited six women to speak at the congress. Bashmakova was one of those six; she was unable to travel to the congress, but her paper appears in its proceedings.\n\nThe International Academy of the History of Science elected her as a corresponding member in 1966, and a full member in 1971. She was awarded honorary diplomas in 1971, 1976, and 1980. In 2001, she was awarded the Alexander Koyré́ Medal of the International Academy of the History of Science. In 2011, a conference of the Russian Academy of Sciences was dedicated in her honor.\n"}
{"id": "2443873", "url": "https://en.wikipedia.org/wiki?curid=2443873", "title": "Jean-Raymond Abrial", "text": "Jean-Raymond Abrial\n\nJean-Raymond Abrial (born 1938) is a French computer scientist and inventor of the Z and B formal methods.\n\nJ.-R. Abrial is the father of the Z notation (typically used for formal specification of software), during his time at the Programming Research Group within the Oxford University Computing Laboratory (now Oxford University Department of Computer Science), and later the B-Method (normally used for software development), two leading formal methods for software engineering. He is the author of \"The B-Book: Assigning Programs to Meanings\". For much of his career he has been an independent consultant, as much at home working with industry as academia. Latterly, he became a Professor at ETH Zurich in Switzerland.\n\n"}
{"id": "47666123", "url": "https://en.wikipedia.org/wiki?curid=47666123", "title": "Journal of Integer Sequences", "text": "Journal of Integer Sequences\n\nThe Journal of Integer Sequences is a peer-reviewed open-access academic journal in mathematics, specializing in research papers about integer sequences.\n\nIt was founded in 1998 by Neil Sloane. Sloane had previously published two books on integer sequences, and in 1996 he founded the On-Line Encyclopedia of Integer Sequences (OEIS). Needing an outlet for research papers concerning the sequences he was collecting in OEIS, he founded the journal. Since 2002 the journal has been hosted by the David R. Cheriton School of Computer Science at the University of Waterloo, with Waterloo professor Jeffrey Shallit as its editor-in-chief. There are no page charges for authors, and all papers are free to all readers. The journal publishes approximately 50–75 papers annually.\n\nIn most years from 1999 to 2014, SCImago Journal Rank has ranked the \"Journal of Integer Sequences\" as a third-quartile journal in discrete mathematics and combinatorics. It is indexed by \"Mathematical Reviews\" and Zentralblatt MATH.\n"}
{"id": "22217360", "url": "https://en.wikipedia.org/wiki?curid=22217360", "title": "Kaidā glyphs", "text": "Kaidā glyphs\n\nKaidā glyphs () are a set of pictograms once used in the Yaeyama Islands of southwestern Japan. The word \"kaidā\" was taken from Yonaguni, and most studies on the pictographs focused on Yonaguni Island. However, there is evidence for their use in Yaeyama's other islands, most notably on Taketomi Island. They were used primarily for tax notices, thus were closely associated with the poll tax imposed on Yaeyama by Ryūkyū on Okinawa Island, which was in turn dominated by Satsuma Domain on Southern Kyushu.\n\nSudō (1944) hypothesized that the etymology of \"kaidā\" was , which meant \"government office\" in Satsuma Domain. This term was borrowed by Ryūkyū on Okinawa and also by the bureaucrats of Yaeyama (\"karja:\" in Modern Ishigaki). Standard Japanese /j/ regularly corresponds to /d/ in Yonaguni, and /r/ is often dropped when surrounded by vowels. This theory is in line with the primary impetus for Kaidā glyphs, taxation.\n\nImmediately after conquering Ryūkyū, Satsuma conducted a land survey in Okinawa in 1609 and in Yaeyama in 1611. By doing so, Satsuma decided the amount of tribute to be paid annually by Ryūkyū. Following that, Ryūkyū imposed a poll tax on Yaeyama in 1640. A fixed quota was allocated to each island and then was broken up into each community. Finally, quotas were set for the individual islanders, adjusted only by age and gender. Community leaders were notified of quotas in the government office on Ishigaki. They checked the calculation using \"warazan\" (\"barazan\" in Yaeyama), a straw-based method of calculation and recording numerals that was reminiscent of Incan Quipu. After that, the quota for each household was written on a wooden plate called . That was where Kaidā glyphs were used. Although \"sōrō\"-style Written Japanese had the status of administrative language, the remote islands had to rely on pictograms to notify illiterate peasants. According to a 19th-century document cited by the \"Yaeyama rekishi\" (1954), an official named Ōhama Seiki designed \"perfect ideographs\" for \"itafuda\" in the early 19th century although it suggests the existence of earlier, \"imperfect\" ideographs. Sudō (1944) recorded an oral history on Yonaguni: 9 generations ago, an ancestor of the Kedagusuku lineage named Mase taught Kaidā glyphs and \"warazan\" to the public. Sudō dated the event to the second half of the 17th century.\n\nAccording to Ikema (1959), Kaidā glyphs and \"warazan\" were evidently accurate enough to make corrections to official announcements. The poll tax was finally abolished in 1903. They were used until the introduction of the nationwide primary education system rapidly lowered the illiteracy rate during the Meiji period. They are currently used on Yonaguni and Taketomi for folk art, T-shirts, and other products, more for their artistic value than as a record-keeping system.\n\nKaidā glyphs consist of\nAs for numerals, similar systems called \"sūchūma\" can be found in Okinawa and Miyako and appear to have their roots in the Suzhou numerals.\n\nThe first non-Yaeyama author to comment on kaidā glyphs was Gisuke Sasamori, who left copies of many short kaidā texts in his \"Nantō Tanken\" (南島探検, \"Exploration of the Southern Islands\"), a record of his 1893 visit to Okinawa Prefecture which also mentions the hard labor imposed on the islanders by the regime. Yasusada Tashiro collected various numeral systems found in Okinawa and Miyako and donated them to the Tokyo National Museum in 1887. A paper on \"sūchūma\" by British Japanologist Basil Chamberlain (1898) appears to have been based on Tashiro's collection.\n\nIn 1915 the mathematics teacher Kiichi Yamuro (矢袋喜一) included many more examples of kaidā glyphs, \"barazan\" knotted counting ropes, and local number words (along with a reproduction of Sasamori's records) in his book on Old Ryukyuan Mathematics (琉球古来の数学). Although Yamuro did not visit Yonaguni by himself, his records suggest that kaidā glyphs were still in daily use in the 1880s. Anthropologist Tadao Kawamura, who made his anthropological study of the islands in the 1930s, noted \"they were in use until recently.\" He showed how kaidā glyphs were used in sending packages. Sudō (1944) showed how business transactions were recorded on leaves using kaidā glyphs. He also proposed an etymology for kaidā.\n"}
{"id": "1720938", "url": "https://en.wikipedia.org/wiki?curid=1720938", "title": "Killing horizon", "text": "Killing horizon\n\nA Killing horizon is a null hypersurface defined by the vanishing of the norm of a Killing vector field (both are named after Wilhelm Killing). \n\nIn Minkowski space-time, in pseudo-Cartesian coordinates formula_1 with signature formula_2 an example of Killing horizon is provided by the Lorentz boost (a Killing vector of the space-time)\n\nThe square of the norm of formula_4 is \n\nTherefore, formula_4 is null only on the hyperplanes of equations\n\nthat, taken together, are the Killing horizons generated by formula_4. \n\nAssociated to a Killing horizon is a geometrical quantity known as surface gravity, formula_9. If the surface gravity vanishes, then the Killing horizon is said to be degenerate.\n\nExact black hole metrics such as the Kerr–Newman metric contain Killing horizons which coincide with their ergospheres. For this spacetime, the Killing horizon is located at\n\nIn the usual coordinates, outside the Killing horizon, the Killing vector field formula_11 is timelike, whilst inside it is spacelike. The temperature of Hawking radiation is related to the surface gravity formula_12 by formula_13 with formula_14 the Boltzmann constant.\n\nDe Sitter space has a Killing horizon at formula_15 which emits thermal radiation at temperature formula_16.\n"}
{"id": "356213", "url": "https://en.wikipedia.org/wiki?curid=356213", "title": "List of logicians", "text": "List of logicians\n\nA logician is a person whose topic of scholarly study is logic. Some famous logicians are listed below in English alphabetical transliteration order (by surname).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "37236195", "url": "https://en.wikipedia.org/wiki?curid=37236195", "title": "List of things named after Richard Dedekind", "text": "List of things named after Richard Dedekind\n\nThis is a list of things named after Richard Dedekind. Richard Dedekind (1831–1916), a mathematician, is the eponym of all of the things (and topics) listed below.\n\n"}
{"id": "30860415", "url": "https://en.wikipedia.org/wiki?curid=30860415", "title": "Michele Mosca", "text": "Michele Mosca\n\nMichele Mosca is co-founder and deputy director of the Institute for Quantum Computing at the University of Waterloo, researcher and founding member of the Perimeter Institute for Theoretical Physics, and professor of mathematics in the department of Combinatorics & Optimization at the University of Waterloo. He has held a Tier 2 Canada Research Chair in Quantum Computation since January 2002, and has been a scholar for the Canadian Institute for Advanced Research since September 2003. Mosca's principal research interests concern the design of quantum algorithms, but he is also known for his early work on NMR quantum computation together with Jonathan A. Jones.\n\nMosca received a B.Math degree from the University of Waterloo in 1995. In 1996 he received a Commonwealth Scholarship to attend Wolfson College, Oxford University, where he received his M.Sc. degree in mathematics and foundations of computer science. On another scholarship (and while holding a fellowship), Mosca received his D.Phil degree on the topic of quantum computer algorithms, also at the University of Oxford.\n\n\n\n"}
{"id": "53455", "url": "https://en.wikipedia.org/wiki?curid=53455", "title": "Minkowski's theorem", "text": "Minkowski's theorem\n\nIn mathematics, Minkowski's theorem is the statement that any convex set in formula_1 which is symmetric with respect to the origin and which has volume greater than formula_2 contains a non-zero integer point. The theorem was proved by Hermann Minkowski in 1889 and became the foundation of the branch of number theory called the geometry of numbers. It can be extended from the integers to any lattice formula_3 and to any symmetric convex set with volume greater than formula_4, where formula_5 denotes the covolume of the lattice (the absolute value of the determinant of any of its bases).\n\nSuppose that is a lattice of determinant in the -dimensional real vector space and is a convex subset of that is symmetric with respect to the origin, meaning that if is in then is also in . Minkowski's theorem states that if the volume of is strictly greater than , then must contain at least one lattice point other than the origin. (Since the set is symmetric, it would then contain at least three lattice points: the origin 0 and a pair of points , where .)\n\nThe simplest example of a lattice is the integer lattice of all points with integer coefficients; its determinant is 1. For , the theorem claims that a convex figure in the Euclidean plane symmetric about the origin and with area greater than 4 encloses at least one lattice point in addition to the origin. The area bound is sharp: if is the interior of the square with vertices then is symmetric and convex, and has area 4, but the only lattice point it contains is the origin. This example, showing that the bound of the theorem is sharp, generalizes to hypercubes in every dimension .\n\nThe following argument proves Minkowski's theorem for the specific case of . It can be generalized to arbitrary lattices in arbitrary dimensions.\n\nConsider the map\nIntuitively, this map cuts the plane into 2 by 2 squares, then stacks the squares on top of each other. Clearly has area less than or equal to 4, because this set lies within a 2 by 2 square. Assume for a contradiction that could be injective, which means the pieces of cut out by the squares stack up in a non-overlapping way. Because is locally area-preserving, this non-overlapping property would make it area-preserving for all of , so the area of would be the same as that of , which is greater than 4. That is not the case, so the assumption must be false: is not injective, meaning that there exist at least two distinct points in that are mapped by to the same point: .\n\nBecause of the way was defined,\nthe only way that can equal is for \nto equal for some integers and , not both zero.\nThat is, the coordinates of the two points differ by two even integers. \nSince is symmetric about the origin, is also a point in . Since is convex, the line segment between and lies entirely in , and in particular the midpoint of that segment lies in . In other words,\nis a point in . But this point is an integer point, and is not the origin since and are not both zero.\nTherefore, contains a nonzero integer point.\n\nAn application of this theorem is the result that every class in the ideal class group of a number field contains an integral ideal of norm not exceeding a certain bound, depending on , called Minkowski's bound: the finiteness of the class number of an algebraic number field follows immediately.\n\nMinkowski's theorem is also useful to prove Lagrange's four-square theorem, which states that every natural number can be written as the sum of the squares of four natural numbers.\n\n\n\n"}
{"id": "1441125", "url": "https://en.wikipedia.org/wiki?curid=1441125", "title": "Mondex", "text": "Mondex\n\nMondex is a smart card electronic cash system, implemented as a stored-value card.\n\nMondex was conceived by Tim Jones and Graham Higgins of the National Westminster Bank in the United Kingdom. The system was initially developed between 1990 and 1993, with internal trials being carried out by approximately 6,000 London-based NatWest staff from 1992. The system was publicly unveiled in December 1993. Initial public trials of the payment system were carried out from July 1995, by the newly incorporated Mondex International in Swindon, Wiltshire. The public phase had required the development and manufacture of numerous merchant devices and smart cards, with BT, NatWest and the Midland Bank sponsoring and installing retail terminals at the car parks, payphones, buses and 700 of the merchants in the town, and issuing Mondex cards to the residents. The system was subsequently sold to MasterCard International in 2001. MasterCard by then already owned 49% of the company.\n\nMondex launched in a number of markets during the 1990s, expanding from the original trial in Swindon to Hong Kong, New York and Guelph, Canada. It was also trialled on several British university campuses from the late 1990s, including the University of Edinburgh, University of Exeter (between 1997 and 2001), University of York, University of Nottingham, Aston University and Sheffield Hallam University. Mondex's Canadian pilot ended after three years, in 1998, when partners backed out.\n\nThe Z notation was used to prove security properties about Mondex, allowing it to achieve ITSEC level E6, ITSEC's highest granted security-level classification. \n\n\n\n"}
{"id": "20674820", "url": "https://en.wikipedia.org/wiki?curid=20674820", "title": "Moufang polygon", "text": "Moufang polygon\n\nIn mathematics, Moufang polygons are a generalization by Jacques Tits of the Moufang planes studied by Ruth Moufang, and are irreducible buildings of rank two that admit the action of root groups.\nIn a book on the topic, Tits and Richard Weiss classify them all. An earlier theorem, proved independently by Tits and Weiss, showed that a Moufang polygon must be a generalized 3-gon, 4-gon, 6-gon, or 8-gon, so the purpose of the aforementioned book was to analyze these four cases.\n\n\nA Moufang 3-gon can be identified with the incidence graph of a Moufang projective plane. In this identification, the points and lines of the plane correspond to the vertices of the building.\nReal forms of Lie groups give rise to examples which are the three main types of Moufang 3-gons. There are four real division algebras: the real numbers, the complex numbers, the quaternions, and the octonions, of dimensions 1,2,4 and 8, respectively. The projective plane over such a division algebra then gives rise to a Moufang 3-gon.\n\nThese projective planes correspond to the building attached to SL(R), SL(C), a real form of A and to a real form of E, respectively.\n\nIn the first diagram the circled nodes represent 1-spaces and 2-spaces in a three-dimensional vector space. In the second diagram the circled nodes represent 1-space and 2-spaces in a 3-dimensional vector space over the quaternions, which in turn represent certain 2-spaces and 4-spaces in a 6-dimensional complex vector space, as expressed by the circled nodes in the A diagram. \nThe fourth case — a form of E — is exceptional, and its analogue for Moufang 4-gons is a major feature of Weiss’s book.\n\nGoing from the real numbers to an arbitrary field, Moufang 3-gons can be divided into three cases as above. The split case in the first diagram exists over any field. The second case extends to all associative, non-commutative division algebras; over the reals these are limited to the algebra of quaternions, which has degree 2 (and dimension 4), but some fields admit central division algebras of other degrees. \nThe third case involves ‘alternative’ division algebras (which satisfy a weakened form of the associative law), and a theorem of Richard Bruck and Erwin Kleinfeld shows that these are Cayley-Dickson algebras. This concludes the discussion of Moufang 3-gons.\n\nMoufang 4-gons are also called Moufang quadrangles.\nThe classification of Moufang 4-gons was the hardest of all, and when Tits and Weiss started to write it up, a hitherto unnoticed type came into being, arising from groups of type F4. They can be divided into three classes:\n\n\nThere is some overlap here, in the sense that some classical groups arising from pseudo-quadratic spaces can be obtained from quadrangular algebras (which Weiss calls special), but there are other, non-special ones. The most important of these arise from algebraic groups of types E6, E7, and E8. They are k-forms of algebraic groups belonging to the following diagrams:\nE6\nE7\nE8. \nThe E6 one exists over the real numbers, though the E7 and E8 ones do not. Weiss calls the quadrangular algebras in all these cases Weiss regular, but not special. \nThere is a further type that he calls defective arising from groups of type F4. These are the most exotic of all—they involve purely inseparable field extensions in characteristic 2—and Weiss only discovered them during the joint work with Tits on the classification of Moufang 4-gons by investigating a strange lacuna that should not have existed but did.\n\nThe classification of Moufang 4-gons by Tits and Weiss is related to their intriguing monograph in two ways. One is that the use of quadrangular algebras short-cuts some of the methods known before. The other is that the concept is an analogue to the octonion algebras, and quadratic Jordan division algebras of degree 3, that give rise to Moufang 3-gons and 6-gons.\n\nIn fact all the exceptional Moufang planes, quadrangles, and hexagons that do not arise from \"mixed groups\" (of characteristic 2 for quadrangles or characteristic 3 for hexagons) come from octonions, quadrangular algebras, or Jordan algebras.\n\nMoufang 6-gons are also called Moufang hexagons. A classification of Moufang 6-gons was stated by Tits, though the details remained unproven until the joint work with Weiss on Moufang Polygons.\n\nMoufang 8-gons are also called Moufang octagons. They were classified by Tits, where he showed that they all arise from Ree groups of type ²F₄.\n\nA potential use for quadrangular algebras is to analyze two open questions. One is the Kneser-Tits conjecture that concerns the full group of linear transformations of a building (e.g. GL) factored out by the subgroup generated by root groups (e.g. SL).\n\nThe conjecture is proved for all Moufang buildings except the 6-gons and 4-gons of type E8, in which case the group of linear transformations is conjectured to be equal to the subgroup generated by root groups. For the E8 hexagons this can be rephrased as a question on quadratic Jordan algebras, and for the E8 quadrangles it can now be rephrased in terms of quadrangular algebras.\n\nAnother open question about the E8 quadrangle concerns fields that are complete with respect to a discrete valuation: is there, in such cases, an affine building that yields the quadrangle as its structure at infinity?\n\n\n"}
{"id": "17458663", "url": "https://en.wikipedia.org/wiki?curid=17458663", "title": "Nullity (graph theory)", "text": "Nullity (graph theory)\n\nThe nullity of a graph in the mathematical subject of graph theory can mean either of two unrelated numbers. If the graph has \"n\" vertices and \"m\" edges, then:\n\n\nRank (graph theory)\n\n"}
{"id": "3447712", "url": "https://en.wikipedia.org/wiki?curid=3447712", "title": "Parallel computation thesis", "text": "Parallel computation thesis\n\nIn computational complexity theory, the parallel computation thesis is a hypothesis which states that the \"time\" used by a (reasonable) parallel machine is polynomially related to the \"space\" used by a sequential machine. The parallel computation thesis was set forth by Chandra and Stockmeyer in 1976.\n\nIn other words, for a computational model which allows computations to branch and run in parallel without bound, a formal language which is decidable under the model using no more than formula_1 steps for inputs of length \"n\" is decidable by a non-branching machine using no more than formula_2 units of storage for some constant \"k\". Similarly, if a machine in the unbranching model decides a language using no more than formula_3 storage, a machine in the parallel model can decide the language in no more than formula_4 steps for some constant \"k\".\n\nThe parallel computation thesis is not a rigorous formal statement, as it does not clearly define what constitutes an acceptable parallel model. A parallel machine must be sufficiently powerful to emulate the sequential machine in time polynomially related to the sequential space; compare Turing machine, non-deterministic Turing machine, and alternating Turing machine. N. Blum (1983) introduced a model for which the thesis does not hold.\nHowever, the model allows formula_5 parallel threads of computation after formula_6 steps. (See Big O notation.) Parberry (1986) suggested a more \"reasonable\" bound would be formula_7 or formula_8, in defense of the thesis.\nGoldschlager (1982) proposed a model which is sufficiently universal to emulate all \"reasonable\" parallel models, which adheres to the thesis.\nChandra and Stockmeyer originally formalized and proved results related to the thesis for deterministic and alternating Turing machines, which is where the thesis originated.\n"}
{"id": "41771027", "url": "https://en.wikipedia.org/wiki?curid=41771027", "title": "Paritosh Pandya", "text": "Paritosh Pandya\n\nParitosh K. Pandya is an Indian computer scientist based at the Tata Institute of Fundamental Research (TIFR) in Mumbai, India.\n\nParitosh Pandya studied for a BE degree in Electronics at the Maharaja Sayajirao University of Baroda (1980), MTech degree in Computer Science at IIT Kanpur (1982), and a PhD in Computer Science at Bombay University/TIFR (1988).\n\nFrom 1988, Paritosh Pandya has held academic posts at TIFR. He was a researcher at the Oxford University Computing Laboratory in England during 1989–91, on leave from TIFR, undertaking research with Jonathan Bowen, Jifeng He, and Tony Hoare, amongst others, as part of the ESPRIT ProCoS project on \"Provably Correct Systems\". He then returned to TIFR, where he has spent most of his career. Pandya leads the Theoretical Computer Science Group there.\n\nPandya's main research interest is in the area of formal methods, including real-time systems. He has been especially involved with research concerning Duration Calculus, including the DCVALID model-checking tool. His most cited paper, \"Finding Response Times in a Real-Time System\", with over 900 citations on Google Scholar, was joint work with Mathai Joseph, published in \"The Computer Journal\" in 1986.\n\nParitosh Pandya has been a member of the Editorial Board for the \"Formal Aspects of Computing\" journal published by Springer.\n\n"}
{"id": "35709754", "url": "https://en.wikipedia.org/wiki?curid=35709754", "title": "Parvaresh–Vardy code", "text": "Parvaresh–Vardy code\n\nParvaresh–Vardy codes are a family of error-correcting codes first described in 2005 by Farzad Parvaresh and Alexander Vardy. They can be used for efficient list-decoding.\n\n"}
{"id": "1712680", "url": "https://en.wikipedia.org/wiki?curid=1712680", "title": "Percent sign", "text": "Percent sign\n\nThe percent (per cent) sign (%) is the symbol used to indicate a percentage, a number or ratio as a fraction of 100. Related signs include the permille (per thousand) sign ‰ and the permyriad (per ten thousand) sign ‱ (also known as a basis point), which indicate that a number is divided by one thousand or ten thousand respectively. Higher proportions use parts-per notation.\n\nEnglish style guides prescribe writing the number and percent sign without any space between. However, the International System of Units and ISO 31-0 standard prescribe a space between the number and percent sign, in line with the general practice of using a non-breaking space between a numerical value and its corresponding unit of measurement.\n\nOther languages have other rules for spacing in front of the percent sign:\n\nIt is often recommended that the percent sign only be used in tables and other places with space restrictions. In running text, it should be spelled out as \"percent\" or \"per cent\" (often in newspapers). For example, not \"Sales increased by 24% over 2006\", but rather \"Sales increased by 24 percent over 2006\".\n\nPrior to 1425 there is no known evidence of a special symbol being used for percentage. The Italian term \"per cento\", \"for a hundred\", was used as well as several different abbreviations (e.g. \"per 100\", \"p 100\", \"p cento\", etc.). Examples of this can be seen in the 1339 arithmetic text (author unknown) depicted below. The letter p with its descender crossed by a horizontal or diagonal strike conventionally stood for per, por, par, or pur in Mediaeval and Renaissance palaeography.\n\nAt some point a scribe of some sort used the abbreviation \"pc\" with a tiny loop or circle (depicting the ending \"-o\" used in Italian numeration for \"primo, secondo,\" etc.) This appears in some additional pages of a 1425 text which were probably added around 1435. This is shown below (source, \"Rara Arithmetica\" p. 440).\n\nThe \"pc\" with a loop eventually evolved into a horizontal fraction sign by 1650 (see below for an example in a 1684 text) and thereafter lost the \"per\".\n\nIn 1925 D.E. Smith wrote, \"The solidus form () is modern.\"\n\nThe Unicode code points are:\n\nThere is also , which has the circles replaced by square dots set on edge, the shape of the digit 0 in Arabic numerals.\n\nThe ASCII code for the percent character is 37, or 0x25 in hexadecimal.\n\nNames for the percent sign include percent sign (in ITU-T), mod, grapes (in hacker jargon), and the humorous double-oh-seven (in INTERCAL).\n\nIn computing, the percent character is also used for the modulo operation in programming languages that derive their syntax from the C programming language, which in turn acquired this usage from the earlier B.\n\nIn the textual representation of URIs, a % immediately followed by a 2-digit hexadecimal number denotes an octet specifying (part of) a character that might otherwise not be allowed in URIs (see percent-encoding).\n\nIn SQL, the percent sign is a wildcard character in \"LIKE\" expressions, for example codice_2 will fetch all records whose names start with \"Lisa \".\n\nIn TeX (and therefore also in LaTeX) and PostScript, and in GNU Octave and MATLAB, a % denotes a line comment.\n\nIn BASIC, a trailing % after a variable name marks it as an integer.\n\nIn Perl % is the sigil for hashes.\n\nIn many programming languages' string formatting operations (performed by functions such as printf), the percent sign denotes parts of the template string that will be replaced with arguments. (See printf format string.) In Python and Ruby the percent sign is also used as the string formatting operator.\n\nIn the command processors COMMAND.COM (DOS) and CMD.EXE (OS/2 and Windows), %1, %2... stand for the first, second... parameters of a batch file. %0 stands for the specification of the batch file itself as typed on the command line. The % sign is also used similarly in the FOR command.\n%VAR1% represents the value of an environment variable named VAR1. Thus:\nsets a new value for PATH, that being the old value preceded by \"c:\\;\".\nBecause these uses give the percent sign special meaning, the sequence %% (two percent signs) is used to represent a literal percent sign, so that:\nwould set PATH to the literal value \"c:\\;%PATH%\".\n\nIn the C Shell, % is part of the default command prompt.\n\nIn linguistics, the percent sign is prepended to an example string to show that it is judged well-formed by some speakers and ill-formed by others. This may be due to differences in dialect or even individual idiolects. This is similar to the asterisk to mark ill-formed strings, the question mark to mark strings where well-formedness is unclear, and the number sign to mark strings that are syntactically well-formed but semantically nonsensical.\n\n\n"}
{"id": "5558285", "url": "https://en.wikipedia.org/wiki?curid=5558285", "title": "Polarization of an algebraic form", "text": "Polarization of an algebraic form\n\nIn mathematics, in particular in algebra, polarization is a technique for expressing a homogeneous polynomial in a simpler fashion by adjoining more variables. Specifically, given a homogeneous polynomial, polarization produces a multilinear form from which the original polynomial can be recovered by evaluating along a certain diagonal.\n\nAlthough the technique is deceptively simple, it has applications in many areas of abstract mathematics: in particular to algebraic geometry, invariant theory, and representation theory. Polarization and related techniques form the foundations for Weyl's invariant theory.\n\nThe fundamental ideas are as follows. Let \"f\"(u) be a polynomial in \"n\" variables u = (\"u\", \"u\", ..., \"u\"). Suppose that \"f\" is homogeneous of degree \"d\", which means that\n\nLet u, u, ..., u be a collection of indeterminates with u = (\"u\", \"u\", ..., \"u\"), so that there are \"dn\" variables altogether. The polar form of \"f\" is a polynomial\nwhich is linear separately in each u (i.e., \"F\" is multilinear), symmetric in the u, and such that\n\nThe polar form of \"f\" is given by the following construction\nIn other words, \"F\" is a constant multiple of the coefficient of λ λ...λ in the expansion of \"f\"(λu + ... + λu).\n\nThen the polarization of \"f\" is a function in x = (\"x\", \"y\") and x = (\"x\", \"y\") given by\n\n\nThe polarization of a homogeneous polynomial of degree \"d\" is valid over any commutative ring in which \"d\"! is a unit. In particular, it holds over any field of characteristic zero or whose characteristic is strictly greater than \"d\".\n\nFor simplicity, let \"k\" be a field of characteristic zero and let be the polynomial ring in \"n\" variables over \"k\". Then \"A\" is graded by degree, so that\nThe polarization of algebraic forms then induces an isomorphism of vector spaces in each degree\nwhere \"Sym\" is the \"d\"-th symmetric power of the \"n\"-dimensional space \"k\".\n\nThese isomorphisms can be expressed independently of a basis as follows. If \"V\" is a finite-dimensional vector space and \"A\" is the ring of \"k\"-valued polynomial functions on \"V\", graded by homogeneous degree, then polarization yields an isomorphism\n\nFurthermore, the polarization is compatible with the algebraic structure on \"A\", so that\nwhere \"Sym\"\"V\" is the full symmetric algebra over \"V\".\n\n\n"}
{"id": "23565253", "url": "https://en.wikipedia.org/wiki?curid=23565253", "title": "Quadratic algebra", "text": "Quadratic algebra\n\nIn mathematics, a quadratic algebra is a filtered algebra generated by degree one elements, with defining relations of degree 2. It was pointed out by Yuri Manin that such algebras play an important role in the theory of quantum groups. The most important class of graded quadratic algebras is Koszul algebras.\n\nA graded quadratic algebra \"A\" is determined by a vector space of generators \"V\" = \"A\" and a subspace of homogeneous quadratic relations \"S\" ⊂ \"V\" ⊗ \"V\" . Thus\n\nand inherits its grading from the tensor algebra \"T\"(\"V\"). \n\nIf the subspace of relations is instead allowed to also contain inhomogeneous degree 2 elements, i.e. \"S\" ⊂ \"k\" ⊕ \"V\" ⊕ (\"V\" ⊗ \"V\"), this construction results in a filtered quadratic algebra.\n\nA graded quadratic algebra \"A\" as above admits a quadratic dual: the quadratic algebra generated by \"V\" and with quadratic relations forming the orthogonal complement of \"S\" in \"V\" ⊗ \"V\".\n\n\n"}
{"id": "659094", "url": "https://en.wikipedia.org/wiki?curid=659094", "title": "Quantum information science", "text": "Quantum information science\n\nQuantum information science is an area of study based on the idea that information science depends on quantum effects in physics. It includes theoretical issues in computational models as well as more experimental topics in quantum physics including what can and cannot be done with quantum information. The term quantum information theory is sometimes used, but it fails to encompass experimental research in the area and can be confused with a subfield of quantum information science that studies the processing of quantum information.\n\nSubfields include:\n\n\n\n"}
{"id": "38707053", "url": "https://en.wikipedia.org/wiki?curid=38707053", "title": "Rhombitetraheptagonal tiling", "text": "Rhombitetraheptagonal tiling\n\nIn geometry, the rhombitetraheptagonal tiling is a uniform tiling of the hyperbolic plane. It has Schläfli symbol of rr{4,7}. It can be seen as constructed as a rectified tetraheptagonal tiling, r{7,4}, as well as an expanded order-4 heptagonal tiling or expanded order-7 square tiling.\n\nThe dual is called the \"deltoidal tetraheptagonal tiling\" with face configuration V.4.4.4.7.\n\n\n\n"}
{"id": "47107406", "url": "https://en.wikipedia.org/wiki?curid=47107406", "title": "Robert Brown Gardner", "text": "Robert Brown Gardner\n\nRobert Brown (Robby) Gardner (February 27, 1939 – May 5, 1998) was an American mathematician who worked on differential geometry, a field in which he obtained several novel results. He was the author and co-author of three influential books, produced more than fifty papers, eighteen masters students and thirteen Ph.D students. His 1991 book, \"Exterior Differential Systems\", coauthored with R. Bryant Robert Bryant, S. S. Chern Shiing-Shen Chern, H. Goldschmidt and P. Griffiths Phillips Griffiths, is the standard reference for the subject. Robert Bryant, Duke University's Professor of Mathematics and the president of the American Mathematical Society (2015-2017) was a student of his.\n\nHe is better known in the United States for his improvements and popularization of the methods of Élie Cartan (most notably, Cartan's equivalence method, an algorithmic procedure for determining if two geometric shapes are different). The works of Cartan were hard to grasp for most students, and Gardner worked to explain them in more accessible ways.\n\nHe was born on February 27, 1939. Gardner graduated from Princeton University in 1959, earned a master's degree from Columbia University in 1960, and completed his PhD in 1965 from the University of California, Berkeley, under the orientation of Shiing-Shen Chern. After this, he worked at many places, including becoming a member of the Institute for Advanced Study, and some years as assistant professor at Columbia University. He joined the faculty of the University of North Carolina at Chapel Hill in 1971 and became a full professor there in 1977. He died on May 5, 1998.\n\nIn his memory, the UNC Mathematics Department created the Robert Brown Gardner Memorial Fund, devoted to supporting graduate student activities.\n\n\n"}
{"id": "52167936", "url": "https://en.wikipedia.org/wiki?curid=52167936", "title": "Secant plane", "text": "Secant plane\n\nA secant plane is a plane containing a nontrivial section of a sphere or an ellipsoid, or such a plane that a sphere is projected onto. Secant planes are similar to tangent planes, which contact the sphere's surface at a point, while secant planes contact the surface along curves.\n\nThe two-dimensional representations of secant planes are secant lines, the lines that join two distinct points on a curve.\n\nSecant planes are used in map projections. The secant plane intersects a globe along a small circle with no distortion, forming a standard parallel which has true scale.\n\n\n"}
{"id": "377931", "url": "https://en.wikipedia.org/wiki?curid=377931", "title": "Seismic tomography", "text": "Seismic tomography\n\nSeismic tomography is a technique for imaging the subsurface of the Earth with seismic waves produced by earthquakes or explosions. P-, S-, and surface waves can be used for tomographic models of different resolutions based on seismic wavelength, wave source distance, and the seismograph array coverage. The data received at seismometers are used to solve an inverse problem, wherein the locations of reflection and refraction of the wave paths are determined. This solution can be used to create 3D images of velocity anomalies which may be interpreted as structural, thermal, or compositional variations. Geoscientists use these images to better understand core, mantle, and plate tectonic processes.\n\nTomography is solved as an inverse problem. Seismic travel time data are compared to an initial Earth model and the model is modified until the best possible fit between the model predictions and observed data is found. Seismic waves would travel in straight lines if Earth was of uniform composition, but the compositional layering, tectonic structure, and thermal variations reflect and refract seismic waves. The location and magnitude of these variations can be calculated by the inversion process, although solutions to tomographic inversions are non-unique.\n\nSeismic tomography is similar to medical x-ray computed tomography (CT scan) in that a computer processes receiver data to produce a 3D image, although CT scans use attenuation instead of traveltime difference. Seismic tomography has to deal with the analysis of curved ray paths which are reflected and refracted within the earth and potential uncertainty in the location of the earthquake hypocenter. CT scans use linear x-rays and a known source.\n\nSeismic tomography requires large datasets of seismograms and well-located earthquake or explosion sources. These became more widely available in the 1960s with the expansion of global seismic networks and in the 1970s when digital seismograph data archives were established. These developments occurred concurrently with advancements in computing power that were required to solve inverse problems and generate theoretical seismograms for model testing.\n\nIn 1977, P-wave delay times were used to create the first seismic array-scale 2D map of seismic velocity. In the same year, P-wave data were used to determine 150 spherical harmonic coefficients for velocity anomalies in the mantle. The first model using iterative techniques, required when there are a large numbers of unknowns, was done in 1984. This built upon the first radially anisotropic model of the Earth, which provided the required initial reference frame to compare tomographic models to for iteration. Initial models had resolution of ~3000 to 5000 km, as compared to the few hundred kilometer resolution of current models.\n\nSeismic tomographic models improve with advancements in computing and expansion of seismic networks. Recent models of global body waves used over 10 traveltimes to model 10 to 10 unknowns.\n\nSeismic tomography uses seismic records to create 2D and 3D images of subsurface anomalies by solving large inverse problems such that generate models consistent with observed data. Various methods are used to resolve anomalies in the crust and lithosphere, shallow mantle, whole mantle, and core based on the availability of data and types of seismic waves that penetrate the region at a suitable wavelength for feature resolution. The accuracy of the model is limited by availability and accuracy of seismic data, wave type utilized, and assumptions made in the model.\n\nP-wave data are used in most local models and global models in areas with sufficient earthquake and seismograph density. S- and surface wave data are used in global models when this coverage is not sufficient, such as in ocean basins and away from subduction zones. First-arrival times are the most widely used, but models utilizing reflected and refracted phases are used in more complex models, such as those imaging the core. Differential traveltimes between wave phases or types are also used.\n\nLocal tomographic models are often based on a temporary seismic array targeting specific areas, unless in a seismically active region with extensive permanent network coverage. These allow for the imaging of the crust and upper mantle. \n\nRegional to global scale tomographic models are generally based on long wavelengths. Various models have better agreement with each other than local models due to the large feature size they image, such as subducted slabs and superplumes. The trade off from whole mantle to whole earth coverage is the coarse resolution (hundreds of kilometers) and difficulty imaging small features (e.g. narrow plumes). Although often used to image different parts of the subsurface, P- and S-wave derived models broadly agree where there is image overlap. These models use data from both permanent seismic stations and supplementary temporary arrays. \n\nSeismic tomography can resolve anisotropy, anelasticity, density, and bulk sound density. Variations in these parameters may be a result of thermal or chemical differences, which are attributed to processes such as mantle plumes, subducting slabs, and mineral phase changes. Larger scale features that can be imaged with tomography include the high velocities beneath continental shields and low velocities under ocean spreading centers.\n\nThe mantle plume hypothesis proposes that areas of volcanism not readily explained by plate tectonics, called hotspots, are a result of thermal upwelling from as deep as the core-mantle boundary that become diapirs in the crust. This is an actively contested theory, although tomographic images suggest there are anomalies beneath some hotspots. The best imaged of these are large low-shear-velocity provinces, or superplumes, visible on S-wave models of the lower mantle and believed to reflect both thermal and compositional differences.\n\nThe Yellowstone hotspot is responsible for volcanism at the Yellowstone Caldera and a series of extinct calderas along the Snake River Plain. The Yellowstone Geodynamic Project sought to image the plume beneath the hotspot. They found a strong low-velocity body from ~30 to 250 km depth beneath Yellowstone and a weaker anomaly from 250 to 650 km depth which dipped 60° west-northwest. The authors attribute these features to the mantle plume beneath the hotspot being deflected eastward by flow in the upper mantle seen in S-wave models.\n\nThe Hawaii hotspot produced the Hawaiian–Emperor seamount chain. Tomographic images show it to be 500 to 600 km wide and up to 2,000 km deep.\n\nSubducting plates are colder than the mantle into which they are moving. This creates a fast anomaly that is visible in tomographic images. Both the Farallon plate that subducted beneath the west coast of North America and the northern portion of the Indian plate that has subducted beneath Asia have been imaged with tomography.\n\nGlobal seismic networks have expanded steadily since the 1960s, but are still concentrated on continents and in seismically active regions. Oceans, particularly in the southern hemisphere, are under-covered. Tomographic models in these areas will improve when more data becomes available. The uneven distribution of earthquakes naturally biases models to better resolution in seismically active regions.\n\nThe type of wave used in a model limits the resolution it can achieve. Longer wavelengths are able to penetrate deeper into the earth, but can only be used to resolve large features. Finer resolution can be achieved with surface waves, with the trade off that they cannot be used in models of the deep mantle. The disparity between wavelength and feature scale causes anomalies to appear of reduced magnitude and size in images. P- and S-wave models respond differently to the types of anomalies depending on the driving material property. First arrival time based models naturally prefer faster pathways, causing models based on these data to have lower resolution of slow (often hot) features. Shallow models must also consider the significant lateral velocity variations in continental crust.\n\nSeismic tomography provides only the current velocity anomalies. Any prior structures are unknown and the slow rates of movement in the subsurface (mm to cm per year) prohibit resolution of changes over modern timescales.\n\nTomographic solutions are non-unique. Although statistical methods can be used to analyze the validity of a model, unresolvable uncertainty remains. This contributes to difficulty comparing the validity of different model results.\n\nComputing power limits the amount of seismic data, number of unknowns, mesh size, and iterations in tomographic models. This is of particular importance in ocean basins, which due to limited network coverage and earthquake density require more complex processing of distant data. Shallow oceanic models also require smaller model mesh size due to the thinner crust.\n\nTomographic images are typically presented with a color ramp representing the strength of the anomalies. This has the consequence of making equal changes appear of differing magnitude based on visual perceptions of color, such as the change from orange to red being more subtle than blue to yellow. The degree of color saturation can also visually skew interpretations. These factors should be considered when analyzing images.\n\n\n"}
{"id": "2372007", "url": "https://en.wikipedia.org/wiki?curid=2372007", "title": "Takeuti's conjecture", "text": "Takeuti's conjecture\n\nIn mathematics, Takeuti's conjecture is the conjecture of Gaisi Takeuti that a sequent formalisation of second-order logic has cut-elimination (Takeuti 1953). It was settled positively:\n\nTakeuti's conjecture is equivalent to the consistency of second-order arithmetic in the sense that each of the statements can be derived from each other in the weak system PRA of arithmetic; consistency refers here to the truth of the Gödel sentence for second-order arithmetic. It is also equivalent to the strong normalization of the Girard/Reynold's System F.\n\n\n"}
{"id": "30977", "url": "https://en.wikipedia.org/wiki?curid=30977", "title": "Theorem", "text": "Theorem\n\nIn mathematics, a theorem is a statement that has been proven on the basis of previously established statements, such as other theorems, and generally accepted statements, such as axioms. A theorem is a logical consequence of the axioms. The proof of a mathematical theorem is a logical argument for the theorem statement given in accord with the rules of a deductive system. The proof of a theorem is often interpreted as justification of the truth of the theorem statement. In light of the requirement that theorems be proved, the concept of a theorem is fundamentally \"deductive\", in contrast to the notion of a scientific law, which is \"experimental\".\n\nMany mathematical theorems are conditional statements. In this case, the proof deduces the conclusion from conditions called hypotheses or premises. In light of the interpretation of proof as justification of truth, the conclusion is often viewed as a necessary consequence of the hypotheses, namely, that the conclusion is true in case the hypotheses are true, without any further assumptions. However, the conditional could be interpreted differently in certain deductive systems, depending on the meanings assigned to the derivation rules and the conditional symbol.\n\nAlthough they can be written in a completely symbolic form, for example, within the propositional calculus, theorems are often expressed in a natural language such as English. The same is true of proofs, which are often expressed as logically organized and clearly worded informal arguments, intended to convince readers of the truth of the statement of the theorem beyond any doubt, and from which a formal symbolic proof can in principle be constructed. Such arguments are typically easier to check than purely symbolic ones—indeed, many mathematicians would express a preference for a proof that not only demonstrates the validity of a theorem, but also explains in some way \"why\" it is obviously true. In some cases, a picture alone may be sufficient to prove a theorem. Because theorems lie at the core of mathematics, they are also central to its aesthetics. Theorems are often described as being \"trivial\", or \"difficult\", or \"deep\", or even \"beautiful\". These subjective judgments vary not only from person to person, but also with time: for example, as a proof is simplified or better understood, a theorem that was once difficult may become trivial. On the other hand, a deep theorem may be stated simply, but its proof may involve surprising and subtle connections between disparate areas of mathematics. Fermat's Last Theorem is a particularly well-known example of such a theorem.\n\nLogically, many theorems are of the form of an indicative conditional: \"if A, then B\". Such a theorem does not assert \"B\", only that \"B\" is a necessary consequence of \"A\". In this case \"A\" is called the hypothesis of the theorem (\"hypothesis\" here is something very different from a conjecture) and \"B\" the conclusion (formally, \"A\" and \"B\" are termed the \"antecedent\" and \"consequent\"). The theorem \"If \"n\" is an even natural number then \"n\"/2 is a natural number\" is a typical example in which the hypothesis is \"\"n\" is an even natural number\" and the conclusion is \"\"n\"/2 is also a natural number\".\n\nTo be proved, a theorem must be expressible as a precise, formal statement. Nevertheless, theorems are usually expressed in natural language rather than in a completely symbolic form, with the intention that the reader can produce a formal statement from the informal one.\n\nIt is common in mathematics to choose a number of hypotheses within a given language and declare that the theory consists of all statements provable from these hypotheses. These hypotheses form the foundational basis of the theory and are called axioms or postulates. The field of mathematics known as proof theory studies formal languages, axioms and the structure of proofs.\nSome theorems are \"trivial\", in the sense that they follow from definitions, axioms, and other theorems in obvious ways and do not contain any surprising insights. Some, on the other hand, may be called \"deep\", because their proofs may be long and difficult, involve areas of mathematics superficially distinct from the statement of the theorem itself, or show surprising connections between disparate areas of mathematics. A theorem might be simple to state and yet be deep. An excellent example is Fermat's Last Theorem, and there are many other examples of simple yet deep theorems in number theory and combinatorics, among other areas.\n\nOther theorems have a known proof that cannot easily be written down. The most prominent examples are the four color theorem and the Kepler conjecture. Both of these theorems are only known to be true by reducing them to a computational search that is then verified by a computer program. Initially, many mathematicians did not accept this form of proof, but it has become more widely accepted. The mathematician Doron Zeilberger has even gone so far as to claim that these are possibly the only nontrivial results that mathematicians have ever proved. Many mathematical theorems can be reduced to more straightforward computation, including polynomial identities, trigonometric identities and hypergeometric identities.\n\nTo establish a mathematical statement as a theorem, a proof is required, that is, a line of reasoning from axioms in the system (and other, already established theorems) to the given statement must be demonstrated. However, the proof is usually considered as separate from the theorem statement. Although more than one proof may be known for a single theorem, only one proof is required to establish the status of a statement as a theorem. The Pythagorean theorem and the law of quadratic reciprocity are contenders for the title of theorem with the greatest number of distinct proofs.\n\nTheorems in mathematics and theories in science are fundamentally different in their epistemology. A scientific theory cannot be proved; its key attribute is that it is falsifiable, that is, it makes predictions about the natural world that are testable by experiments. Any disagreement between prediction and experiment demonstrates the incorrectness of the scientific theory, or at least limits its accuracy or domain of validity. Mathematical theorems, on the other hand, are purely abstract formal statements: the proof of a theorem cannot involve experiments or other empirical evidence in the same way such evidence is used to support scientific theories.\nNonetheless, there is some degree of empiricism and data collection involved in the discovery of mathematical theorems. By establishing a pattern, sometimes with the use of a powerful computer, mathematicians may have an idea of what to prove, and in some cases even a plan for how to set about doing the proof. For example, the Collatz conjecture has been verified for start values up to about 2.88 × 10. The Riemann hypothesis has been verified for the first 10 trillion zeroes of the zeta function. Neither of these statements is considered proved.\n\nSuch evidence does not constitute proof. For example, the Mertens conjecture is a statement about natural numbers that is now known to be false, but no explicit counterexample (i.e., a natural number \"n\" for which the Mertens function \"M\"(\"n\") equals or exceeds the square root of \"n\") is known: all numbers less than 10 have the Mertens property, and the smallest number that does not have this property is only known to be less than the exponential of 1.59 × 10, which is approximately 10 to the power 4.3 × 10. Since the number of particles in the universe is generally considered less than 10 to the power 100 (a googol), there is no hope to find an explicit counterexample by exhaustive search.\n\nThe word \"theory\" also exists in mathematics, to denote a body of mathematical axioms, definitions and theorems, as in, for example, group theory. There are also \"theorems\" in science, particularly physics, and in engineering, but they often have statements and proofs in which physical assumptions and intuition play an important role; the physical axioms on which such \"theorems\" are based are themselves falsifiable.\n\nA number of different terms for mathematical statements exist; these terms indicate the role statements play in a particular subject. The distinction between different terms is sometimes rather arbitrary and the usage of some terms has evolved over time.\n\n\n\nThere are other terms, less commonly used, that are conventionally attached to proved statements, so that certain theorems are referred to by historical or customary names. For example:\n\n\nA few well-known theorems have even more idiosyncratic names. The division algorithm (see Euclidean division) is a theorem expressing the outcome of division in the natural numbers and more general rings. Bézout's identity is a theorem asserting that the greatest common divisor of two numbers may be written as a linear combination of these numbers. The Banach–Tarski paradox is a theorem in measure theory that is paradoxical in the sense that it contradicts common intuitions about volume in three-dimensional space.\n\nA theorem and its proof are typically laid out as follows:\n\nThe end of the proof may be signalled by the letters Q.E.D. (\"quod erat demonstrandum\") or by one of the tombstone marks \"□\" or \"∎\" meaning \"End of Proof\", introduced by Paul Halmos following their usage in magazine articles.\n\nThe exact style depends on the author or publication. Many publications provide instructions or macros for typesetting in the house style.\n\nIt is common for a theorem to be preceded by definitions describing the exact meaning of the terms used in the theorem. It is also common for a theorem to be preceded by a number of propositions or lemmas which are then used in the proof. However, lemmas are sometimes embedded in the proof of a theorem, either with nested proofs, or with their proofs presented after the proof of the theorem.\n\nCorollaries to a theorem are either presented between the theorem and the proof, or directly after the proof. Sometimes, corollaries have proofs of their own that explain why they follow from the theorem.\n\nIt has been estimated that over a quarter of a million theorems are proved every year.\n\nThe well-known aphorism, , is probably due to Alfréd Rényi, although it is often attributed to Rényi's colleague Paul Erdős (and Rényi may have been thinking of Erdős), who was famous for the many theorems he produced, the number of his collaborations, and his coffee drinking.\n\nThe classification of finite simple groups is regarded by some to be the longest proof of a theorem. It comprises tens of thousands of pages in 500 journal articles by some 100 authors. These papers are together believed to give a complete proof, and several ongoing projects hope to shorten and simplify this proof. Another theorem of this type is the four color theorem whose computer generated proof is too long for a human to read. It is certainly the longest known proof of a theorem whose statement can be easily understood by a layman.\n\nLogic, especially in the field of proof theory, considers theorems as statements (called formulas or well formed formulas) of a formal language. The statements of the language are strings of symbols and may be broadly divided into nonsense and well-formed formulas. A set of deduction rules, also called transformation rules or rules of inference, must be provided. These deduction rules tell exactly when a formula can be derived from a set of premises. The set of well-formed formulas may be broadly divided into theorems and non-theorems. However, according to Hofstadter, a formal system often simply defines all its well-formed formula as theorems.\n\nDifferent sets of derivation rules give rise to different interpretations of what it means for an expression to be a theorem. Some derivation rules and formal languages are intended to capture mathematical reasoning; the most common examples use first-order logic. Other deductive systems describe term rewriting, such as the reduction rules for λ calculus.\n\nThe definition of theorems as elements of a formal language allows for results in proof theory that study the structure of formal proofs and the structure of provable formulas. The most famous result is Gödel's incompleteness theorem; by representing theorems about basic number theory as expressions in a formal language, and then representing this language within number theory itself, Gödel constructed examples of statements that are neither provable nor disprovable from axiomatizations of number theory.\n\nA theorem may be expressed in a formal language (or \"formalized\"). A formal theorem is the purely formal analogue of a theorem. In general, a formal theorem is a type of well-formed formula that satisfies certain logical and syntactic conditions. The notation formula_1 is often used to indicate that formula_1 is a theorem.\n\nFormal theorems consist of formulas of a formal language and the transformation rules of a formal system. Specifically, a formal theorem is always the last formula of a derivation in some formal system each formula of which is a logical consequence of the formulas that came before it in the derivation. The initially accepted formulas in the derivation are called its axioms, and are the basis on which the theorem is derived. A set of theorems is called a theory.\n\nWhat makes formal theorems useful and of interest is that they can be interpreted as true propositions and their derivations may be interpreted as a proof of the truth of the resulting expression. A set of formal theorems may be referred to as a formal theory. A theorem whose interpretation is a true statement about a formal system is called a metatheorem.\n\nThe concept of a formal theorem is fundamentally syntactic, in contrast to the notion of a \"true proposition,\" which introduces semantics. Different deductive systems can yield other interpretations, depending on the presumptions of the derivation rules (i.e. belief, justification or other modalities). The soundness of a formal system depends on whether or not all of its theorems are also validities. A validity is a formula that is true under any possible interpretation, e.g. in classical propositional logic validities are tautologies. A formal system is considered semantically complete when all of its tautologies are also theorems.\n\nThe notion of a theorem is very closely connected to its formal proof (also called a \"derivation\"). To illustrate how derivations are done, we will work in a very simplified formal system. Let us call ours formula_3 Its alphabet consists only of two symbols { A, B } and its formation rule for formulas is:\n\nThe single axiom of formula_3 is:\n\nThe only rule of inference (transformation rule) for formula_3 is:\n\nTheorems in formula_3 are defined as those formulae that have a derivation ending with that formula. For example,\n\n\nis a derivation. Therefore, \"ABBBAB\" is a theorem of formula_8 The notion of truth (or falsity) cannot be applied to the formula \"ABBBAB\" until an interpretation is given to its symbols. Thus in this example, the formula does not yet represent a proposition, but is merely an empty abstraction.\n\nTwo metatheorems of formula_3 are:\n\n\n\n"}
{"id": "56061994", "url": "https://en.wikipedia.org/wiki?curid=56061994", "title": "Xuong tree", "text": "Xuong tree\n\nIn graph theory, a Xuong tree is a spanning tree formula_1 of a given graph formula_2 with the property that, in the remaining graph formula_3, the number of connected components with an odd number of edges is as small as possible.\nThey are named after Nguyen Huy Xuong, who used them to characterize the cellular embeddings of a given graph having the largest possible genus.\n\nAccording to Xuong's results, if formula_1 is a Xuong tree\nand the numbers of edges in the components of formula_3 are formula_6, then the maximum genus of an embedding of formula_2 is formula_8.\nAny one of these components, having formula_9 edges, can be partitioned into formula_10 edge-disjoint two-edge paths, with possibly one additional left-over edge.\nAn embedding of maximum genus may be obtained from a planar embedding of the Xuong tree by adding each two-edge path to the embedding in such a way that it increases the genus by one.\n\nA Xuong tree, and a maximum-genus embedding derived from it, may be found in any graph in polynomial time, by a transformation to a more general computational problem on matroids, the matroid parity problem for linear matroids.\n"}
