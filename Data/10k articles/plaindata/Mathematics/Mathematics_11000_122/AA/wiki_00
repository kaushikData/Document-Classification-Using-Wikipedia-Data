{"id": "28877379", "url": "https://en.wikipedia.org/wiki?curid=28877379", "title": "Abstract logic", "text": "Abstract logic\n\nIn mathematical logic, an abstract logic is a formal system consisting of a class of sentences and a satisfaction relation with specific properties related to occurrence, expansion, isomorphism, renaming and quantification.\n\nBased on Lindström's characterization, first-order logic is, up to equivalence, the only abstract logic which is countably compact and has Löwenheim number ω.\n\n"}
{"id": "58019333", "url": "https://en.wikipedia.org/wiki?curid=58019333", "title": "Aline Bonami", "text": "Aline Bonami\n\nAline Bonami (née Nivat) is a French mathematician known for her expertise in mathematical analysis. She is a professor emeritus at the University of Orléans, and was president of the Société mathématique de France for 2012–2013.\n\nBonami was a student at the École normale supérieure de jeunes filles from 1963 to 1967, when she became a researcher at the Centre national de la recherche scientifique (CNRS). In 1970, she completed a doctorate at the University of Paris-Sud, under the supervision of Yves Meyer; her dissertation was \"Etude des coefficients de Fourier des fonctions de formula_1\". She joined the University of Orléans in 1973 and retired as a professor emeritus in 2006.\n\nThe French Academy of Sciences gave Bonami their in 2001, for her results on \nBergman and Szegö projections, on Hankel operators with several complex variables, and on inequalities for hypercontractivity.\nThe University of Gothenburg gave her an honorary doctorate in 2002. A conference on harmonic analysis was held in her honor in Orléans in 2014.\n\nBonami is the sister of French computer scientist Maurice Nivat.\n"}
{"id": "44528136", "url": "https://en.wikipedia.org/wiki?curid=44528136", "title": "Approximate max-flow min-cut theorem", "text": "Approximate max-flow min-cut theorem\n\nApproximate max-flow min-cut theorems are mathematical propositions in network flow theory. They deal with the relationship between maximum flow rate (\"max-flow\") and minimum cut (\"min-cut\") in a multi-commodity flow problem. The theorems have enabled the development of approximation algorithms for use in graph partition and related problems.\n\nA \"commodity\" in a network flow problem is a pair of source and sink nodes. In a multi-commodity flow problem, there are commodities, each with its own source formula_1, sink formula_2, and demand formula_3. The objective is to simultaneously route formula_4 units of commodity from formula_1 to formula_2 for each , such that the total amount of all commodities passing through any edge is no greater than its capacity. (In the case of undirected edges, the sum of the flows in both directions cannot exceed the capacity of the edge).\nSpecially, a 1-commodity (or single commodity) flow problem is also known as a maximum flow problem. According to the Ford–Fulkerson algorithm, the max-flow and min-cut are always equal in a 1-commodity flow problem.\n\nIn a multicommodity flow problem, \"max-flow\" is the maximum value of , where is the common fraction of each commodity that is routed, such that formula_7 units of commodity can be simultaneously routed for each without violating any capacity constraints.\n\"min-cut\" is the minimum of all cuts of the ratio formula_8 of the capacity of the cut to the demand of the cut.\nMax-flow is always upper bounded by the min-cut for a multicommodity flow problem.\n\nIn a uniform multicommodity flow problem, there is a commodity for every pair of nodes and the demand for every commodity is the same. (Without loss of generality, the demand for every commodity is set to one.) The underlying network and capacities are arbitrary.\n\nIn a product multicommodity flow problem, there is a nonnegative weight for each node formula_9 in graph formula_10. The demand for the commodity between nodes and is the product of the weights of node and node . The uniform multicommodity flow problem is a special case of the product multicommodity flow problem for which the weight is set to 1 for all nodes formula_11.\n\nIn general, the dual of a multicommodity flow problem for a graph is the problem of apportioning a fixed amount of weight (where weights can be considered as distances) to the edges of such that to maximize the cumulative distance between the source and sink pairs.\n\nThe research on the relationship between the max-flow and min-cut of multicommodity flow problem has obtained great interest since Ford and Fulkerson's result for 1-commodity flow problems. Hu\nshowed that the max-flow and min-cut are always equal for two commodities. Okamura and Seymour illustrated a 4-commodity flow problem with max-flow equals to 3/4 and min-cut equals 1. Shahrokhi and Matula also proved that the max-flow and min-cut are equal provided the dual of the flow problem satisfies a certain cut condition in a uniform multicommodity flow problem. Many other researchers also showed concrete research results in similar problems\n\nFor a general network flow problem, the max-flow is within a factor of of the min-cut since each commodity can be optimized separately using formula_12 of the capacity of each edge. This is not a good result especially in case of large numbers of commodities.\n\nThere are two theorems first introduced by Tom Leighton and Satish Rao in 1988\nand then extended in 1999. Theorem 2 gives a tighter bound compared to Theorem 1.\n\nTheorem 1. \"For any , there is an -node uniform multicommodity flow problem with max-flow and min-cut formula_8 for which formula_14.\"\n\nTheorem 2. \"For any uniform multicommodity flow problem, formula_15, where is the max-flow and formula_8 is the min-cut of the uniform multicommodity flow problem.\"\n\nTo prove Theorem 2, both the max-flow and the min-cut should be discussed. \nFor the max-flow, the techniques from duality theory of linear programming have to be employed. According to the duality theory of linear programming, an optimal distance function results in a total weight that is equal to the max-flow of the uniform multicommodity flow problem. \nFor the min-cut, a 3-stage process has to be followed:\n\nStage 1: Consider the dual of uniform commodity flow problem and use the optimal solution to define a graph with distance labels on the edges.\n\nStage 2: Starting from a source or a sink, grow a region in the graph until find a cut of small enough capacity separating the root from its mate.\n\nStage 3: Remove the region and repeat the process of stage 2 until all nodes get processed.\n\nTheorem 3. \"For any product multicommodity flow problem with commodities, formula_17, where is the max-flow and formula_8 is the min-cut of the product multicommodity flow problem.\"\n\nThe proof methodology is similar to that for Theorem 2; the major difference is to take node weights into consideration.\n\nIn a directed multicommodity flow problem, each edge has a direction, and the flow is restricted to move in the specified direction. In a directed uniform multicommodity flow problem, the demand is set to 1 for every directed edge.\n\nTheorem 4. \"For any directed uniform multicommodity flow problem with nodes, formula_19, where is the max-flow and formula_8 is the min-cut of the uniform multicommodity flow problem.\"\n\nThe major difference in the proof methodology compared to Theorem 2 is that, now the edge directions need to be considered when defining distance labels in stage 1 and for growing the regions in stage 2, more details can be found in.\n\nSimilarly, for product multicommodity flow problem, we have the following extended theorem:\n\nTheorem 5. \"For any directed product multicommodity flow problem with commodities, formula_21, where is the max-flow and formula_8 is the directed min-cut of the product multicommodity flow problem.\"\n\nThe above theorems are very useful to design approximation algorithms for NP-hard problems, such as the graph partition problem and its variations. Here below we briefly introduce a few examples, and the in-depth elaborations can be found in Leighton and Rao (1999).\n\nA sparsest cut of a graph formula_10 is a partition for which the ratio of the number of edges connecting the two partitioned components over the product of the numbers of nodes of both components. This is a NP-hard problem, and it can be approximated to within formula_24 factor using Theorem 2. Also, a sparsest cut problem with weighted edges, weighted nodes or directed edges can be approximated within an formula_25 factor where is the number of nodes with nonzero weight according to Theorem 3, 4 and 5.\n\nIn some applications, we want to find a small cut in a graph formula_10 that partitions the graph into nearly equal-size pieces. We usually call a cut \"b-balanced\" or a -\"separator\" (for ) if formula_27 where formula_28 is the sum of the node weights in . This is also an NP-hard problem. An approximation algorithm has been designed for this problem, and the core idea is that has a -balanced cut of size , then we find a -balanced cut of size formula_29 for any where and . Then we repeat the process then finally obtain the result that total weight of the edges in the cut is at most formula_30.\n\nIt is helpful to find a layout of minimum size when designing a VLSI circuit. Such a problem can often be modeled as a graph embedding problem. The objective is to find an embedding for which the layout area is minimized. Finding the minimum layout area is also NP-hard. An approximation algorithm has been introduced and the result is formula_31 times optimal.\n\nGiven an -node graph and an embedding of formula_32 in , Chung et al.\ndefined the \"forwarding index\" of the embedding to be the maximum number of paths (each corresponding to an edge of formula_32) that pass through any node of . The objective is to find an embedding that minimizes the forwarding index. Using embedding approaches it is possible to bound the node and edge-forwarding indices to within an formula_24-factor for every graph .\n\nTragoudas\nuses the approximation algorithm for balanced separators to find a set of \nformula_35\nedges whose removal from a bounded-degree graph results in a planar graph, where is the minimum number of edges that need to be removed from before it becomes planar. It remains an open question if there is a polylog times optimal approximation algorithm for .\n"}
{"id": "24750413", "url": "https://en.wikipedia.org/wiki?curid=24750413", "title": "Argand system", "text": "Argand system\n\nIn mathematics, an \"n\"th-order Argand system (named after French mathematician Jean-Robert Argand) is a coordinate system constructed around the \"n\"th roots of unity. From the origin, \"n\" axes extend such that the angle between each axis and the axes immediately before and after it is 360/\"n\" degrees. For example, the number line is the 2nd-order Argand system because the two axes extending from the origin represent 1 and −1, the 2nd roots of unity. The complex plane (sometimes called the Argand plane, also named after Argand) is the 4th-order Argand system because the 4 axes extending from the origin represent 1, \"i\", −1, and −\"i\", the 4th roots of unity.\n\n"}
{"id": "48999111", "url": "https://en.wikipedia.org/wiki?curid=48999111", "title": "AxCrypt", "text": "AxCrypt\n\nAxCrypt is open-source encryption software offering both a free version and a premium (at-cost) version for Microsoft Windows, macOS, Android, and iOS. It was originally developed in Sweden by Svante Seleborg and is produced and maintained by him and the staff of AxCrypt AB. It can compress, delete, encrypt/decrypt, and edit files (though, thus far, the Android, iOS, and free macOS versions are read-only viewer programs). It is a file-based encryption program, as differs from a container-based encryption program, i.e., each single to-be-protected file is encrypted individually by the program, rather than the program putting all to-be-protected files into a single encrypted container like TrueCrypt or VeraCrypt does. Axcrypt comes in installer versions for all mentioned operating systems and also a standalone (portable) version for Windows.\n\nAxCrypt can encrypt a file or a folder by using either a key file or a password. AxCrypt can also secure files on the file sharing services such as DropBox or Google Drive. AxCrypt is using AES-128 and AES-256 encryption standard.\n\nUsers need to sign up for a user account in order to use AxCrypt. Users may log in by entering the password to encrypt files or folders. As long the user is logged in, he or she can open secured files and folders. The key file is actually a plain text document containing a code to open the file. An existing file can also be used to encrypt a file or a folder.\n\nAxCrypt allows secured files to be opened by other AxCrypt users with the same password that was used to encrypt. AxCrypt can also delete data safely: the tool scrambles the data and then deletes them. The purpose of this feature is to prevent a malicious user to retrieve data later.\n"}
{"id": "2699467", "url": "https://en.wikipedia.org/wiki?curid=2699467", "title": "Bidirected graph", "text": "Bidirected graph\n\nIn the mathematical domain of graph theory, a bidirected graph (introduced by ) is a graph in which each edge is given an independent orientation (or direction, or arrow) at each end. Thus, there are three kinds of bidirected edges: those where the arrows point outward, towards the vertices, at both ends; those where both arrows point inward, away from the vertices; and those in which one arrow points away from its vertex and towards the opposite end, while the other arrow points in the same direction as the first, away from the opposite end and towards its own vertex.\n\nEdges of these three types may be called, respectively, extraverted, introverted, and directed. The \"directed\" edges are the same as ordinary directed edges in a directed graph; thus, a directed graph is a special kind of bidirected graph.\n\nIt is sometimes desirable to have also edges with only one end (half-edges); these get only one arrow. An edge with no ends (a loose edge) has no arrows. The edges that are neither half nor loose edges may be called ordinary edges.\n\nA skew-symmetric graph is the double covering graph of a bidirected graph.\n\nA symmetric directed graph (that is, a directed graph in which the reverse of every edge is also an edge) is sometimes also called a \"bidirected graph\".\n\n"}
{"id": "7184", "url": "https://en.wikipedia.org/wiki?curid=7184", "title": "C*-algebra", "text": "C*-algebra\n\nC-algebras (pronounced \"C-star\") are an area of research in functional analysis, a branch of mathematics. A C*-algebra is a complex algebra \"A\" of continuous linear operators on a complex Hilbert space with two additional properties:\n\n\nC*-algebras were first considered primarily for their use in quantum mechanics to model algebras of physical observables. This line of research began with Werner Heisenberg's matrix mechanics and in a more mathematically developed form with Pascual Jordan around 1933. Subsequently, John von Neumann attempted to establish a general framework for these algebras which culminated in a series of papers on rings of operators. These papers considered a special class of C*-algebras which are now known as von Neumann algebras.\n\nAround 1943, the work of Israel Gelfand and Mark Naimark yielded an abstract characterisation of C*-algebras making no reference to operators on a Hilbert space.\n\nC*-algebras are now an important tool in the theory of unitary representations of locally compact groups, and are also used in algebraic formulations of quantum mechanics. Another active area of research is the program to obtain classification, or to determine the extent of which classification is possible, for separable simple nuclear C*-algebras.\n\nWe begin with the abstract characterization of C*-algebras given in the 1943 paper by Gelfand and Naimark.\n\nA C*-algebra, \"A\", is a Banach algebra over the field of complex numbers, together with a map * : \"A\" → \"A\". One writes \"x*\" for the image of an element \"x\" of \"A\". The map * has the following properties:\n\n\n\n\n\nRemark. The first three identities say that \"A\" is a *-algebra. The last identity is called the C* identity and is equivalent to:\n\nformula_6\n\nwhich is sometimes called the B*-identity. For history behind the names C*- and B*-algebras, see the section below.\n\nThe C*-identity is a very strong requirement. For instance, together with the spectral radius formula, it implies that the C*-norm is uniquely determined by the algebraic structure:\n\nA bounded linear map, π : \"A\" → \"B\", between C*-algebras \"A\" and \"B\" is called a *-homomorphism if\n\n\n\nIn the case of C*-algebras, any *-homomorphism π between C*-algebras is contractive, i.e. bounded with norm ≤ 1. Furthermore, an injective *-homomorphism between C*-algebras is isometric. These are consequences of the C*-identity.\n\nA bijective *-homomorphism π is called a C*-isomorphism, in which case \"A\" and \"B\" are said to be isomorphic.\n\nThe term B*-algebra was introduced by C. E. Rickart in 1946 to describe Banach *-algebras that satisfy the condition:\n\n\nThis condition automatically implies that the *-involution is isometric, that is, formula_11. Hence, formula_12, and therefore, a B*-algebra is also a C*-algebra. Conversely, the C*-condition implies the B*-condition. This is nontrivial, and can be proved without using the condition formula_13. For these reasons, the term B*-algebra is rarely used in current terminology, and has been replaced by the term 'C*-algebra'.\n\nThe term C*-algebra was introduced by I. E. Segal in 1947 to describe norm-closed subalgebras of \"B\"(\"H\"), namely, the space of bounded operators on some Hilbert space \"H\". 'C' stood for 'closed'. In his paper Segal defines a C*-algebra as a \"uniformly closed, self-adjoint algebra of bounded operators on a Hilbert space\".\n\nC*-algebras have a large number of properties that are technically convenient. Some of these properties can be established by using the continuous functional calculus or by reduction to commutative C*-algebras. In the latter case, we can use the fact that the structure of these is completely determined by the Gelfand isomorphism.\n\nSelf-adjoint elements are those of the form \"x\"=\"x\"*. The set of elements of a C*-algebra \"A\" of the form \"x*x\" forms a closed convex cone. This cone is identical to the elements of the form \"xx*\". Elements of this cone are called \"non-negative\" (or sometimes \"positive\", even though this terminology conflicts with its use for elements of R.)\n\nThe set of self-adjoint elements of a C*-algebra \"A\" naturally has the structure of a partially ordered vector space; the ordering is usually denoted ≥. In this ordering, a self-adjoint element \"x\" of \"A\" satisfies \"x\" ≥ 0 if and only if the spectrum of \"x\" is non-negative, if and only if \"x\" = \"s*s\" for some \"s\". Two self-adjoint elements \"x\" and \"y\" of \"A\" satisfy \"x\" ≥ \"y\" if \"x\"−\"y\" ≥ 0.\n\nThis partially ordered subspace allows the definition of a positive linear functional on a C*-algebra, which in turn is used to define the states of a C*-algebra, which in turn can be used to construct the spectrum of a C*-algebra using the GNS construction.\n\nAny C*-algebra \"A\" has an approximate identity. In fact, there is a directed family {\"e\"} of self-adjoint elements of \"A\" such that\n\nUsing approximate identities, one can show that the algebraic quotient of a C*-algebra by a closed proper two-sided ideal, with the natural norm, is a C*-algebra.\n\nSimilarly, a closed two-sided ideal of a C*-algebra is itself a C*-algebra.\n\nThe algebra M(\"n\", C) of \"n\" × \"n\" matrices over C becomes a C*-algebra if we consider matrices as operators on the Euclidean space, C, and use the operator norm ||.|| on matrices. The involution is given by the conjugate transpose. More generally, one can consider finite direct sums of matrix algebras. In fact, all C*-algebras that are finite dimensional as vector spaces are of this form, up to isomorphism. The self-adjoint requirement means finite-dimensional C*-algebras are semisimple, from which fact one can deduce the following theorem of Artin–Wedderburn type:\n\nTheorem. A finite-dimensional C*-algebra, \"A\", is canonically isomorphic to a finite direct sum\nwhere min \"A\" is the set of minimal nonzero self-adjoint central projections of \"A\".\n\nEach C*-algebra, \"Ae\", is isomorphic (in a noncanonical way) to the full matrix algebra M(dim(\"e\"), C). The finite family indexed on min \"A\" given by {dim(\"e\")} is called the \"dimension vector\" of \"A\". This vector uniquely determines the isomorphism class of a finite-dimensional C*-algebra. In the language of K-theory, this vector is the positive cone of the \"K\" group of \"A\".\n\nA †-algebra (or, more explicitly, a \"†-closed algebra\") is the name occasionally used in physics for a finite-dimensional C*-algebra. The dagger, †, is used in the name because physicists typically use the symbol to denote a Hermitian adjoint, and are often not worried about the subtleties associated with an infinite number of dimensions. (Mathematicians usually use the asterisk, *, to denote the Hermitian adjoint.) †-algebras feature prominently in quantum mechanics, and especially quantum information science.\n\nAn immediate generalization of finite dimensional C*-algebras are the approximately finite dimensional C*-algebras.\n\nThe prototypical example of a C*-algebra is the algebra \"B(H)\" of bounded (equivalently continuous) linear operators defined on a complex Hilbert space \"H\"; here \"x*\" denotes the adjoint operator of the operator \"x\" : \"H\" → \"H\". In fact, every C*-algebra, \"A\", is *-isomorphic to a norm-closed adjoint closed subalgebra of \"B\"(\"H\") for a suitable Hilbert space, \"H\"; this is the content of the Gelfand–Naimark theorem.\n\nLet \"H\" be a separable infinite-dimensional Hilbert space. The algebra \"K\"(\"H\") of compact operators on \"H\" is a norm closed subalgebra of \"B\"(\"H\"). It is also closed under involution; hence it is a C*-algebra.\n\nConcrete C*-algebras of compact operators admit a characterization similar to Wedderburn's theorem for finite dimensional C*-algebras:\n\nTheorem. If \"A\" is a C*-subalgebra of \"K\"(\"H\"), then there exists Hilbert spaces {\"H\"} such that\nwhere the (C*-)direct sum consists of elements (\"T\") of the Cartesian product Π \"K\"(\"H\") with ||\"T\"|| → 0.\n\nThough \"K\"(\"H\") does not have an identity element, a sequential approximate identity for \"K\"(\"H\") can be developed. To be specific, \"H\" is isomorphic to the space of square summable sequences \"l\"; we may assume that \"H\" = \"l\". For each natural number \"n\" let \"H\" be the subspace of sequences of \"l\" which vanish for indices \"k\" ≤ \"n\" and let \"e\" be the orthogonal projection onto \"H\". The sequence {\"e\"} is an approximate identity for \"K\"(\"H\").\n\n\"K\"(\"H\") is a two-sided closed ideal of \"B\"(\"H\"). For separable Hilbert spaces, it is the unique ideal. The quotient of \"B\"(\"H\") by \"K\"(\"H\") is the Calkin algebra.\n\nLet \"X\" be a locally compact Hausdorff space. The space formula_18 of complex-valued continuous functions on \"X\" that \"vanish at infinity\" (defined in the article on local compactness) form a commutative C*-algebra formula_18 under pointwise multiplication and addition. The involution is pointwise conjugation. formula_18 has a multiplicative unit element if and only if formula_21 is compact. As does any C*-algebra, formula_18 has an approximate identity. In the case of formula_18 this is immediate: consider the directed set of compact subsets of formula_21, and for each compact formula_25 let formula_26 be a function of compact support which is identically 1 on formula_25. Such functions exist by the Tietze extension theorem which applies to locally compact Hausdorff spaces. Any such sequence of functions formula_28 is an approximate identity.\n\nThe Gelfand representation states that every commutative C*-algebra is *-isomorphic to the algebra formula_18, where formula_21 is the space of characters equipped with the weak* topology. Furthermore, if formula_18 is isomorphic to formula_32 as C*-algebras, it follows that formula_21 and formula_34 are homeomorphic. This characterization is one of the motivations for the noncommutative topology and noncommutative geometry programs.\n\nGiven a Banach *-algebra \"A\" with an approximate identity, there is a unique (up to C*-isomorphism) C*-algebra E(\"A\") and *-morphism π from \"A\" into E(\"A\") which is universal, that is, every other continuous *-morphism factors uniquely through π. The algebra E(\"A\") is called the C*-enveloping algebra of the Banach *-algebra \"A\".\n\nOf particular importance is the C*-algebra of a locally compact group \"G\". This is defined as the enveloping C*-algebra of the group algebra of \"G\". The C*-algebra of \"G\" provides context for general harmonic analysis of \"G\" in the case \"G\" is non-abelian. In particular, the dual of a locally compact group is defined to be the primitive ideal space of the group C*-algebra. See spectrum of a C*-algebra.\n\nVon Neumann algebras, known as W* algebras before the 1960s, are a special kind of C*-algebra. They are required to be closed in the weak operator topology, which is weaker than the norm topology.\n\nThe Sherman–Takeda theorem implies that any C*-algebra has a universal enveloping W*-algebra, such that any homomorphism to a W*-algebra factors through it.\n\nA C*-algebra \"A\" is of type I if and only if for all non-degenerate representations π of \"A\" the von Neumann algebra π(\"A\")′′ (that is, the bicommutant of π(\"A\")) is a type I von Neumann algebra. In fact it is sufficient to consider only factor representations, i.e. representations π for which π(\"A\")′′ is a factor.\n\nA locally compact group is said to be of type I if and only if its group C*-algebra is type I.\n\nHowever, if a C*-algebra has non-type I representations, then by results of James Glimm it also has representations of type II and type III. Thus for C*-algebras and locally compact groups, it is only meaningful to speak of type I and non type I properties.\n\nIn quantum mechanics, one typically describes a physical system with a C*-algebra \"A\" with unit element; the self-adjoint elements of \"A\" (elements \"x\" with \"x*\" = \"x\") are thought of as the \"observables\", the measurable quantities, of the system. A \"state\" of the system is defined as a positive functional on \"A\" (a C-linear map φ : \"A\" → C with φ(\"u*u\") ≥ 0 for all \"u\" ∈ \"A\") such that φ(1) = 1. The expected value of the observable \"x\", if the system is in state φ, is then φ(\"x\").\n\nThis C*-algebra approach is used in the Haag-Kastler axiomatization of local quantum field theory, where every open set of Minkowski spacetime is associated with a C*-algebra.\n\n\n"}
{"id": "860507", "url": "https://en.wikipedia.org/wiki?curid=860507", "title": "Centered polygonal number", "text": "Centered polygonal number\n\nThe centered polygonal numbers are a class of series of figurate numbers, each formed by a central dot, surrounded by polygonal layers with a constant number of sides. Each side of a polygonal layer contains one dot more than a side in the previous layer, so starting from the second polygonal layer each layer of a centered \"k\"-gonal number contains \"k\" more points than the previous layer.\n\nEach sequence is a multiple of the triangular numbers plus 1. For example, the centered square numbers are four times the triangular numbers plus 1.\n\nThese series consist of the\nand so on.\n\nThe following diagrams show a few examples of centered polygonal numbers and their geometric construction. Compare these diagrams with the diagrams in Polygonal number.\n\nAs can be seen in the above diagrams, the \"n\"th centered \"k\"-gonal number can be obtained by placing \"k\" copies of the (\"n\"−1)th triangular number around a central point; therefore, the \"n\"th centered \"k\"-gonal number can be mathematically represented by\n\nJust as is the case with regular polygonal numbers, the first centered \"k\"-gonal number is 1. Thus, for any \"k\", 1 is both \"k\"-gonal and centered \"k\"-gonal. The next number to be both \"k\"-gonal and centered \"k\"-gonal can be found using the formula:\n\nwhich tells us that 10 is both triangular and centered triangular, 25 is both square and centered square, etc.\n\nWhereas a prime number \"p\" cannot be a polygonal number (except of course that each \"p\" is the second \"p\"-agonal number), many centered polygonal numbers are primes.\n\n"}
{"id": "20709270", "url": "https://en.wikipedia.org/wiki?curid=20709270", "title": "Child sex ratio", "text": "Child sex ratio\n\nIn India, the Child Sex Ratio is defined as the number of females per thousand males in the age group 0–6 years in a human population. Thus it is equal to 1000 x the reciprocal of the sex ratio (ratio of males to females in a population) in the same age group, i.e. under age seven. An imbalance in this age group will extend to older age groups in future years. Currently, the ratio of males to females is generally significantly greater than 1, i.e. there are more boys than girls.\n\nAccording to the decennial Indian census, the sex ratio in the 0-6 age group in India went from 104.0 males per 100 females in 1981 to 105.8 in 1991, to 107.8 in 2001, to 108.8 in 2011. The ratio is significantly higher in certain states such as Punjab and Haryana (118 and 120 respectively per 2011 census). The child sex ratio has been more prominent for males in India for quite a while, since the 1980s with thirty fewer females to males \n\nGenetically, the odds of having a boy or girl is a 50/50 chance, therefore in a perfect theoretical world the child sex ratio would be split down the middle. There is a fifty percent chance of having a boy or girl due to the genetics. A women would give one of her X chromosomes, whereas the male would either give the Y chromosome or the X chromosome resulting in a baby boy or baby girl. Oddly enough the child sex ratio is not a fifty percent male and fifty percent females, in South Asian countries there is a larger percentage of male children than female children. Many counties in South Asia have a uneven child sex ratio. A large city in India, Jhajjar had almost 15,000 more baby boys than baby girls, that is 128 boys per 100 girls. \n\nUnlike India, the number of males born surpassed the number of females born in England after World War I. For every 100 girls born, there were two more boys born in the UK. It was believed the men were producing baby boys to replace the men lost during battle. Others believed it was genetics, if the males had a family of many brothers it was likely for them to produce more sons than daughters. Many more boys were born in many countries after the world wars. There are about 105 males to 100 females in the United States and Britain in 2008. \n\nIn the Asian culture, families want baby boys, because it is traditional that the boys take care of the parents, while the daughters go marry and leave the family. These families want to ensure elderly security, therefor they want more boys in the family. Typically it cost more to have a daughter and they cannot contribute to the family nearly as much as the son can. These factors cause family to get an abortion because they want the variables that the boys have to offer, and unfortunately it causes the child male to child female sex ratio it be imbalanced.\n\nOver the last forty years, India's fertility rate has decreased by more than half. Fertility factor rates are cohort measures that are grouped together by year and are studied within a longer time frame. Over time, this decrease in fertility would have a factor to the child sex ratio because the overall fertility rate has gone down. Fewer women having babies will result in fewer babies and the child sex ratio to alter. \n\nThe impact of a skewed sex ratio with more male children than females is already being felt in some parts of India and China and is likely to continue to tighten the skewed ratio between genders.: \n\n\n\n"}
{"id": "709999", "url": "https://en.wikipedia.org/wiki?curid=709999", "title": "Clifford module", "text": "Clifford module\n\nIn mathematics, a Clifford module is a representation of a Clifford algebra. In general a Clifford algebra \"C\" is a central simple algebra over some field extension \"L\" of the field \"K\" over which the quadratic form \"Q\" defining \"C\" is defined.\n\nThe abstract theory of Clifford modules was founded by a paper of M. F. Atiyah, R. Bott and Arnold S. Shapiro. A fundamental result on Clifford modules is that the Morita equivalence class of a Clifford algebra (the equivalence class of the category of Clifford modules over it) depends only on the signature . This is an algebraic form of Bott periodicity.\n\nWe will need to study \"anticommuting\" matrices () because in Clifford algebras orthogonal vectors anticommute\n\nFor the real Clifford algebra formula_2, we need mutually anticommuting matrices, of which \"p\" have +1 as square and \"q\" have −1 as square.\n\nSuch a basis of gamma matrices is not unique. One can always obtain another set of gamma matrices satisfying the same Clifford algebra by means of a similarity transformation.\n\nwhere \"S\" is a non-singular matrix. The sets \"γ\" and \"γ\" belong to the same equivalence class.\n\nDeveloped by Ettore Majorana, this Clifford module enables the construction of a Dirac-like equation without complex numbers, and its elements are called Majorana spinors.\n\nThe four basis vectors are the three Pauli matrices and a fourth antihermitian matrix. The signature is (+++−). For the signatures (+−−−) and (−−−+) often used in physics, 4×4 complex matrices or 8×8 real matrices are needed.\n\n\n"}
{"id": "243709", "url": "https://en.wikipedia.org/wiki?curid=243709", "title": "Cubic function", "text": "Cubic function\n\nIn algebra, a cubic function is a function of the form\n\nin which is nonzero.\n\nSetting produces a cubic equation of the form\n\nThe solutions of this equation are called roots of the polynomial . If all of the coefficients , , , and of the cubic equation are real numbers, then it has at least one real root (this is true for all odd degree polynomials). All of the roots of the cubic equation can be found algebraically. (This is also true of quadratic (second degree) or quartic (fourth degree) equations, but not of higher-degree equations, by the Abel–Ruffini theorem.) The roots can also be found trigonometrically. Alternatively, numerical approximations of the roots can be found using root-finding algorithms such as Newton's method.\n\nThe coefficients do not need to be complex numbers. Much of what is covered below is valid for coefficients of any field with characteristic  or greater than . The solutions of the cubic equation do not necessarily belong to the same field as the coefficients. For example, some cubic equations with rational coefficients have roots that are non-rational (and even non-real) complex numbers.\n\nCubic equations were known to the ancient Babylonians, Greeks, Chinese, Indians, and Egyptians. Babylonian (20th to 16th centuries BC) cuneiform tablets have been found with tables for calculating cubes and cube roots. The Babylonians could have used the tables to solve cubic equations, but no evidence exists to confirm that they did. The problem of doubling the cube involves the simplest and oldest studied cubic equation, and one for which the ancient Egyptians did not believe a solution existed. In the 5th century BC, Hippocrates reduced this problem to that of finding two mean proportionals between one line and another of twice its length, but could not solve this with a compass and straightedge construction, a task which is now known to be impossible. Methods for solving cubic equations appear in \"The Nine Chapters on the Mathematical Art\", a Chinese mathematical text compiled around the 2nd century BC and commented on by Liu Hui in the 3rd century. In the 3rd century AD, the Greek mathematician Diophantus found integer or rational solutions for some bivariate cubic equations (Diophantine equations). Hippocrates, Menaechmus and Archimedes are believed to have come close to solving the problem of doubling the cube using intersecting conic sections, though historians such as Reviel Netz dispute whether the Greeks were thinking about cubic equations or just problems that can lead to cubic equations. Some others like T. L. Heath, who translated all Archimedes' works, disagree, putting forward evidence that Archimedes really solved cubic equations using intersections of two conics, but also discussed the conditions where the roots are 0, 1 or 2. \nIn the 7th century, the Tang dynasty astronomer mathematician Wang Xiaotong in his mathematical treatise titled Jigu Suanjing systematically established and solved numerically 25 cubic equations of the form , 23 of them with , and two of them with .\n\nIn the 11th century, the Persian poet-mathematician, Omar Khayyam (1048–1131), made significant progress in the theory of cubic equations. In an early paper, he discovered that a cubic equation can have more than one solution and stated that it cannot be solved using compass and straightedge constructions. He also found a geometric solution. In his later work, the \"Treatise on Demonstration of Problems of Algebra\", he wrote a complete classification of cubic equations with general geometric solutions found by means of intersecting conic sections.\n\nIn the 12th century, the Indian mathematician Bhaskara II attempted the solution of cubic equations without general success. However, he gave one example of a cubic equation: . In the 12th century, another Persian mathematician, Sharaf al-Dīn al-Tūsī (1135–1213), wrote the \"Al-Muʿādalāt\" (\"Treatise on Equations\"), which dealt with eight types of cubic equations with positive solutions and five types of cubic equations which may not have positive solutions. He used what would later be known as the \"Ruffini-Horner method\" to numerically approximate the root of a cubic equation. He also used the concepts of maxima and minima of curves in order to solve cubic equations which may not have positive solutions. He understood the importance of the discriminant of the cubic equation to find algebraic solutions to certain types of cubic equations.\n\nIn his book \"Flos\", Leonardo de Pisa, also known as Fibonacci (1170–1250), was able to closely approximate the positive solution to the cubic equation . Writing in Babylonian numerals he gave the result as 1,22,7,42,33,4,40 (equivalent to 1 + 22/60 + 7/60 + 42/60 + 33/60 + 4/60 + 40/60), which has a relative error of about 10.\n\nIn the early 16th century, the Italian mathematician Scipione del Ferro (1465–1526) found a method for solving a class of cubic equations, namely those of the form . In fact, all cubic equations can be reduced to this form if we allow and to be negative, but negative numbers were not known to him at that time. Del Ferro kept his achievement secret until just before his death, when he told his student Antonio Fiore about it.\n\nIn 1530, Niccolò Tartaglia (1500–1557) received two problems in cubic equations from Zuanne da Coi and announced that he could solve them. He was soon challenged by Fiore, which led to a famous contest between the two. Each contestant had to put up a certain amount of money and to propose a number of problems for his rival to solve. Whoever solved more problems within 30 days would get all the money. Tartaglia received questions in the form , for which he had worked out a general method. Fiore received questions in the form , which proved to be too difficult for him to solve, and Tartaglia won the contest.\n\nLater, Tartaglia was persuaded by Gerolamo Cardano (1501–1576) to reveal his secret for solving cubic equations. In 1539, Tartaglia did so only on the condition that Cardano would never reveal it and that if he did write a book about cubics, he would give Tartaglia time to publish. Some years later, Cardano learned about Ferro's prior work and published Ferro's method in his book \"Ars Magna\" in 1545, meaning Cardano gave Tartaglia six years to publish his results (with credit given to Tartaglia for an independent solution). Cardano's promise with Tartaglia stated that he not publish Tartaglia's work, and Cardano felt he was publishing del Ferro's, so as to get around the promise. Nevertheless, this led to a challenge to Cardano by Tartaglia, which Cardano denied. The challenge was eventually accepted by Cardano's student Lodovico Ferrari (1522–1565). Ferrari did better than Tartaglia in the competition, and Tartaglia lost both his prestige and income.\n\nCardano noticed that Tartaglia's method sometimes required him to extract the square root of a negative number. He even included a calculation with these complex numbers in \"Ars Magna\", but he did not really understand it. Rafael Bombelli studied this issue in detail and is therefore often considered as the discoverer of complex numbers.\n\nFrançois Viète (1540–1603) independently derived the trigonometric solution for the cubic with three real roots, and René Descartes (1596–1650) extended the work of Viète.\n\nThe critical points of a function are those values of where the slope of the function is zero. The critical points of a cubic function defined by , occur at values of such that the derivative of the cubic function is zero:\nThe solutions of that equation are the critical points of the cubic equation and are given, using the quadratic formula, by \nThe expression inside of the square root,\ndetermines what type of critical points the function has. If , then the cubic function has a local maximum and a local minimum. If , then the cubic's inflection point is the only critical point. If , then there are no critical points. In cases where , the cubic function is strictly monotonic. The adjacent diagram is an example of the case where . The other two cases do not have the local maximum or the local minimum but still have an inflection point.\n\nThe value of also plays an important role in determining the nature of the roots of the cubic equation and in the calculation of those roots; see below.\n\nThe inflection point of a function is where that function changes concavity. The inflection point of our cubic function occurs at:\n\na value that is also important in solving the cubic equation. The cubic function has point symmetry about its inflection point.\n\nAll of the above assumes that the coefficients are real as well as the variable .\n\nThis section is about how to solve the cubic equation using various methods. For details and proofs see . The general cubic equation has the form:\n\nwith .\n\nThe algebraic solution of the cubic equation can be derived in a number of different ways. (See for example Cardano's method and Vieta's method below.)\n\nThe numbers of real and complex roots are determined by the discriminant of the cubic equation,\n\nIt turns out that:\n\n\nThe general solution of the cubic equation involves first calculating:\n\nThe general formula for one of the roots, in terms of the coefficients, is as follows:\nNote that, while this equality is valid for all non-zero , it is not the most convenient form for multiple roots (), which is covered in the next section. (The case when only occurs when both and are equal to and is also covered in the next section.)\n\nThe other two roots of the cubic equation can be determined using the same equality, using the other two choices for the cube root in the equation for : denoting the first choice simply as , the others can be written as\n\nThe above equality can be expressed compactly including all roots as follows:\n\nwhere (which is a cube root of unity). In the case of three real roots, this solution expresses them in terms of non-real complex terms (since any choice of is non-real) whose imaginary components offset each other but cannot be eliminated from the formula.\n\nThis formula for the three roots applies even when the coefficients in the cubic are non-real, although the analysis of the sign of does not hold since is then not real in general.\n\nIf both and are equal to , then the equation has a single root (which is a triple root):\n\nIf and , then there are both a double root,\nand a simple root,\n\nDividing by and substituting for we get the equation\n\nwhere\n\nThe left hand side of equation () is a monic trinomial called a depressed cubic, because the quadratic term has coefficient 0.\n\nAny formula for the roots of a depressed cubic may be transformed into a formula for the roots of equation () by substituting the above values for and and using the relation .\n\nTherefore, only equation () is considered in the following.\n\nWhen a cubic equation has three real roots, the formulas expressing these roots in terms of radicals involve complex numbers. It has been proved that when none of the three real roots is rational—the \"casus irreducibilis\"— one cannot express the roots in terms of real radicals. Nevertheless, purely real expressions of the solutions may be obtained using hypergeometric functions, or more elementarily in terms of trigonometric functions, specifically in terms of the cosine and arccosine functions.\n\nThe formulas which follow, due to François Viète, are true in general (except when \"p\" = 0), and are purely real when the equation has three real roots, but involve complex cosines and arccosines when there is only one real root.\n\nStarting from equation (), , let us set . The idea is to choose to make equation () coincide with the identity\nIn fact, choosing formula_18 and dividing equation () by formula_19 we get \nCombining with the above identity, we get \nand thus the roots are \n\nThis formula involves only real terms if and the argument of the arccosine is between and . The last condition is equivalent to , which itself implies . Thus the above formula for the roots involves only real terms if and only if the three roots are real.\n\nDenoting by the above value of , and using the inequalities for a real number such that , the three roots may also be expressed as\nIf the three roots are real, we have . All these formulas may be straightforwardly transformed into formulas for the roots of the general cubic equation (), using the back substitution described above.\n\nWhen there is only one real root (and ), it may be similarly represented using hyperbolic functions, as \nIf and the inequalities on the right are not satisfied (the case of three real roots), the formulas remain valid but involve complex quantities.\n\nWhen , the above values of are sometimes called the Chebyshev cube root. More precisely, the values involving cosines and hyperbolic cosines define, when , the same analytic function denoted , which is the proper Chebyshev cube root. The value involving hyperbolic sines is similarly denoted , when .\n\nIf a cubic function with integer coefficients is reducible over the rationals, meaning that it can be factored into lower-degree polynomials with rational coefficients, then it has a rational root, which can be found using the rational root test: If the root is fully reduced, then is a factor of and is a factor of , so all possible combinations of values for and (both positive and negative for one of them) can be checked for whether they are roots of the cubic.\n\nThe rational root test may also be used for a cubic equation with rational coefficients: by multiplication by the lowest common denominator of the coefficients, one gets an equation with integer coefficients which has exactly the same roots.\n\nThe rational root test is particularly useful when there are three real roots because the algebraic solution unhelpfully expresses the real roots in terms of complex entities; if the test yields a rational root, it can be factored out and the remaining roots can be found by solving a quadratic. The rational root test is also helpful in the presence of one real and two complex roots because again, if it yields a rational root, it allows all of the roots to be written without the use of cube roots: If is any root of the cubic, then we may factor out using polynomial long division to obtain\nHence if we know one root, perhaps from the rational root test, we can find the other two by using the quadratic formula to find the roots of the quadratic , giving\nfor the other two roots.\n\nAs shown in this graph, to solve the third-degree equation where , Omar Khayyám constructed the parabola , the circle which has as a diameter the line segment on the positive -axis, and a vertical line through the point above the -axis where the circle and parabola intersect. The solution is given by the length of the horizontal line segment from the origin to the intersection of the vertical line and the -axis.\n\nA simple modern proof of the method is the following: multiplying the equation by and regrouping the terms gives \nThe left-hand side is the value of on the parabola. The equation of the circle being , the right hand side is the value of on the circle.\n\nA cubic equation with real coefficients can be solved geometrically using compass, straightedge, and an angle trisector if and only if it has three real roots.\n\nEvery cubic equation (), , with real coefficients and , has three solutions (some of which may equal each other if they are real, and two of which may be complex non-real numbers) and at least one real solution , this last assertion being a consequence of the intermediate value theorem. If is factored out of the cubic polynomial, what remains is a quadratic polynomial whose roots and are roots of the cubic; by the quadratic formula, these roots are either both real (giving a total of three real roots for the cubic) or are complex conjugates, in which case the cubic has one real and two non-real roots.\n\nIt was explained above how to use the sign of the discriminant in order to distinguish between these cases. In fact,\n\nbecause a straightforward computation shows that\n\nand, by Vieta's formulas, the right hand side of this equality is equal to\n\nThe equality () shows that if and only if the equation has a multiple root. This cannot possibly be the case when and are non-real complex numbers, because the fact that is real assures that is different from and from and, on the other hand, the fact that and are non-real and that each of them is the conjugate of the other one assures that .\n\nIf and are non-real, then\n\nSince this is the product of a non-zero real number by , its square is a real number less than  and therefore . Finally, if the numbers , , and are three distinct real numbers, then the product is a non-zero real number, and so .\n\nViète's trigonometric expression of the roots in the three-real-roots case lends itself to a geometric interpretation in terms of a circle. When the cubic is written in depressed form (), , as shown above, the solution can be expressed as\n\nHere formula_32 is an angle in the unit circle; taking of that angle corresponds to taking a cube root of a complex number; adding for finds the other cube roots; and multiplying the cosines of these resulting angles by formula_33 corrects for scale.\n\nFor the non-depressed case () (shown in the accompanying graph), the depressed case as indicated previously is obtained by defining such that so . Graphically this corresponds to simply shifting the graph horizontally when changing between the variables and , without changing the angle relationships. This shift moves the point of inflection and the centre of the circle onto the -axis. Consequently, the roots of the equation in sum to zero.\n\nIf a cubic is plotted in the Cartesian plane, the real root can be seen graphically as the horizontal intercept of the curve. But further, if the complex conjugate roots are written as then is the abscissa (the positive or negative horizontal distance from the origin) of the tangency point of a line that is tangent to the cubic curve and intersects the horizontal axis at the same place as does the cubic curve; and is the square root of the tangent of the angle between this line and the horizontal axis.\n\nWith one real and two complex roots, the three roots can be represented as points in the complex plane, as can the two roots of the cubic's derivative. There is an interesting geometrical relationship among all these roots.\n\nThe points in the complex plane representing the three roots serve as the vertices of an isosceles triangle. (The triangle is isosceles because one root is on the horizontal (real) axis and the other two roots, being complex conjugates, appear symmetrically above and below the real axis.) Marden's theorem says that the points representing the roots of the derivative of the cubic are the foci of the Steiner inellipse of the triangle—the unique ellipse that is tangent to the triangle at the midpoints of its sides. If the angle at the vertex on the real axis is less than then the major axis of the ellipse lies on the real axis, as do its foci and hence the roots of the derivative. If that angle is greater than , the major axis is vertical and its foci, the roots of the derivative, are complex conjugates. And if that angle is , the triangle is equilateral, the Steiner inellipse is simply the triangle's incircle, its foci coincide with each other at the incenter, which lies on the real axis, and hence the derivative has duplicate real roots.\n\nThe solutions can be found with the following method due to Scipione del Ferro and Tartaglia, published by Gerolamo Cardano in 1545 in his book \"Ars Magna\". \nThis method applies to the depressed cubic (), . We introduce two variables and linked by the condition and substitute this in the depressed cubic (), giving\n\nAt this point Cardano imposed a second condition for the variables and : . As the first parenthesis vanishes in previous equality, we get and . The combination of these two equations leads to a quadratic equation (since they are the sum and the product of and ). Thus and are the two roots of the quadratic equation . Cardano assumed that . He suggested that his readers consult another of his books, \"De Regula Aliza\", which was published only in 1570, for the case in which . Solving this quadratic equation and using the fact that and may be exchanged, we find\nTherefore, is equal to:\n\nIn his book \"L'Algebra\", published in 1572, Rafael Bombelli explained that what was done above still works, with a small difference, when , as long as one knows how to use complex numbers. The small difference is due to the fact that a non-zero complex number has cube roots and not just one. Therefore, although the equality implies that , this is \"not\" an equivalence. So, we do not simply take \"any\" cube root of\nand add it to \"any\" cube root of\n\n(unless one of them is ); besides, that would provide solutions to equation (). Instead, since we want to have , means the sum of \"a\" cube root of () with (or, if () is equal to , the sum of \"a\" cube root of () with ). This only fails if \"both\" numbers () and () are equal to , in which case and Cardano's formula simply means , which is compatible with the fact that, since , () simplifies to .\n\nActually, it is not necessary to compute the three cube roots of (). To see why, let . Then and are the non-real cube roots of . If is a cube root of (), and is a cube root of () such that , then the roots of () are , , and , since, in each case, we have the sum of a cube root of () with a cube root () and moreover the product of these two roots is, in each case, equal to .\n\nNote that and that . Thus Cardano's formula, written unambiguously to give the three roots, is\n\nwhere the cube roots expressed as radicals are defined to be any pair of cube roots whose product is . If and are real and , this is the same thing as requiring that the cube roots be complex conjugates, while if and are real and , the real cube roots can be chosen.\n\nCardano's formula, interpreted in this way, is equivalent to the general solution given earlier when the coefficient of the quadratic term is .\n\nWe will examine certain particular cases. Before that, it is convenient to note that, if is a cube root of (), if is a cube root of (), and if both numbers and are real, then it is automatically true that . This is so because\nLet us now see the particular cases.\n\n\nIn this last case (that is, when but and are not ), although the computations made above do suggest that is a simple root of () whereas is a double root (having been obtained in two different ways), they don't really prove it. However, this can be easily confirmed. Just note that\nsince .\n\nThe numbers provided by Cardano's formula are solutions of the equation (), but there might be other solutions besides these. However, this does not occur. Let and be numbers such that and . In order to see that , , and are the only roots of the polynomial , it is enough to notice that\n\nTherefore, if and only if , or .\n\nStarting from the depressed cubic (), , we make the substitution , known as Vieta's substitution. This results in the equation\n\nMultiplying by , it becomes a sextic equation in , which is in fact a quadratic equation in :\nThe quadratic formula allows equation () to be solved for . If , and are the three cube roots of one of the solutions in , then the roots of the original depressed cubic are , , and . Another way of expressing the roots is to take ; then the roots of the original depressed cubic are , , and, . This method only fails when both roots of the equation () are equal to , but this only happens when , in which case the only solution of equation () is .\n\nActually, the substitution originally used by Vieta (in a text published posthumously in 1615) was , but it leads to similar computations. More precisely, Vieta introduced a new variable and he imposed the condition .\n\nAs far as formulae are concerned, Vieta's approach leads to the same result as Cardano's method. However, it is theoretically simpler, for two reasons:\n\nIn his paper \"Réflexions sur la résolution algébrique des équations\" (\"Thoughts on the algebraic solving of equations\"), Joseph Louis Lagrange introduced a new method to solve equations of low degree.\n\nThis method works well for cubic and quartic equations, but Lagrange did not succeed in applying it to a quintic equation, because it requires solving a resolvent polynomial of degree at least six. This is explained by the Abel–Ruffini theorem, which proves that such polynomials cannot be solved by radicals. Nevertheless, the modern methods for solving solvable quintic equations are mainly based on Lagrange's method.\n\nIn the case of cubic equations, Lagrange's method gives the same solution as Cardano's. By drawing attention to a geometrical problem that involves two cubes of different size Cardano explains in his book \"Ars Magna\" how he arrived at the idea of considering the unknown of the cubic equation as a sum of two other quantities. Lagrange's method may also be applied directly to the general cubic equation (), , without using the reduction to the depressed cubic equation (), . Nevertheless, the computation is much easier with this reduced equation.\n\nSuppose that , and are the roots of equation () or (), and define (a complex cube root of , i.e. a primitive third root of unity) which satisfies the relation . We now set\nThis is the discrete Fourier transform of the roots: observe that while the coefficients of the polynomial are symmetric in the roots, in this formula an \"order\" has been chosen on the roots, so these are not symmetric in the roots. The roots may then be recovered from the three by inverting the above linear transformation via the inverse discrete Fourier transform, giving\n\nThe polynomial is equal, by Vieta's formulas, to in case of equation () and to in case of equation (), so we only need to seek values for the other two.\n\nThe polynomials and are not symmetric functions of the roots: is invariant, while the two non-trivial cyclic permutations of the roots send to and to , or to and to (depending on which permutation), while transposing and switches and ; other transpositions switch these sums and multiply them by a power of .\n\nThus , and are left invariant by the cyclic permutations of the roots, which multiply them by . Also and are left invariant by the transposition of and which exchanges and . As the permutation group of the roots is generated by these permutations, it follows that and are symmetric functions of the roots and may thus be written as polynomials in the elementary symmetric polynomials and thus as rational functions of the coefficients of the equation. Let and in these expressions, which will be explicitly computed below.\n\nWe have that and are the two roots of the quadratic equation . Thus the resolution of the equation may be finished exactly as described for Cardano's method, with and in place of and .\n\nSetting , and , the elementary symmetric polynomials, we have, using that :\nThe expression for is the same with and exchanged. Thus, using we get\nand a straightforward computation gives \n\nSimilarly we have \n\nWhen solving equation () we have , and . With equation (), we have , and and thus and .\n\nNote that with equation (), we have and , while in Cardano's method we have set and . Thus we have, up to the exchange of and , and . In other words, in this case, Cardano's method and Lagrange's method compute exactly the same things, up to a factor of three in the auxiliary variables, the main difference being that Lagrange's method explains why these auxiliary variables appear in the problem.\n\nIf we are dealing with a cubic equation whose coefficients belong to some field (whose characteristic is either or greater than ), then what was done above algebraically still works, with one exception: the results concerning the sign of the discriminant, since they make no sense for general fields, although the fact that the equation has a multiple root if and only if is still true (and for the same reason). In this more general case, we work with an extension of in which every non-zero element has two square roots and three cube roots. For instance, if , we can take , the field of algebraic numbers.\n\nIn particular, all that was done above algebraically still works if . Therefore, every cubic equation with complex coefficients has some complex root, which is a particular case of the fundamental theorem of algebra.\n\nIn this general context, the formulae for roots in the case in which show that these roots also belong to the field .\n\nIn a field whose characteristic is either or , this approach does not work because then the formulae for the roots became meaningless, since they involve division by and .\n\nThe Galois group of an irreducible separable polynomial of degree is a transitive subgroup of . In particular, the Galois group of an irreducible separable cubic is a transitive subgroup of and there are only two such subgroups: and . There is a simple way of determining the Galois group of a concrete irreducible cubic over a field : it is if the discriminant of the cubic is the square of an element of and otherwise. Indeed, if is not the square of an element of , then is an extension of degree of . On the other hand, if , , and are the roots of , then, since the equality () holds, that is, since , , and so, by the multiplicativity formula for degrees, the degree of over (that is, the order of the Galois group of ) must be a multiple of the degree of , which is . Therefore, it must be an even number, and so the Galois group can only be .\n\nOn the other hand, if \"is\" the square of an element of , then, again by the equality (), we have . Therefore, if  belongs to the Galois group of , then maps into itself. But then cannot act on the set } as the transposition that exchanges and and leaves fixed, because then would map into . So, in this case, the Galois group of is not and therefore it must be .\n\nIt is clear from this criterion that, if we are working over the field , the Galois group of most irreducible cubic polynomials is . An example of an irreducible cubic polynomial with rational coefficients whose Galois group is is , whose discriminant is . The polynomial is used in the standard proof of the impossibility of trisecting arbitrary angles using straightedge and compass only.\n\nThe tangent lines to a cubic at three collinear points intercept the cubic again at collinear points. This can be seen as follows. If the cubic is defined by and if is a real number, then the tangent to the graph of at the point is the line\n\nSo, the intersection point between this line and the graph of can be obtained solving the equation . This is a cubic equation, but it is clear that is a root, and in fact a double root, since the line is tangent to the graph. The remaining root is . So, the other intersection point between the tangent line and the graph of  is the point\n\nTherefore, if is a point of the graph of , the other intersection point between the tangent line at and the graph is the point , where is the map defined by\n\nSince is an affine map, if , , and are collinear, then so are the points , , and .\n\nThe graph of a cubic function has rotational or point symmetry about its inflection point. The inflection point of a general cubic polynomial,\noccurs at a point such that . Since , the inflection point is . Translating the function so that the inflection point is at the origin, one obtains the function defined by:\nAs all terms are odd powers of , proving that all cubic functions are rotationally symmetrical about their inflection points.\n\nCubic equations arise in various other contexts.\n\nMarden's theorem states that the foci of the Steiner inellipse of any triangle can be found by using the cubic function whose roots are the coordinates in the complex plane of the triangle's three vertices. The roots of the first derivative of this cubic are the complex coordinates of those foci.\n\nThe area of a regular heptagon can be expressed in terms of the roots of a cubic. Further, the ratios of the long diagonal to the side, the side to the short diagonal, and the negative of the short diagonal to the long diagonal all satisfy a particular cubic equation. In addition, the ratio of the inradius to the circumradius of a heptagonal triangle is one of the solutions of a cubic equation. The values of trigonometric functions of angles related to formula_55 satisfy cubic equations.\n\nGiven the cosine (or other trigonometric function) of an arbitrary angle, the cosine of one-third of that angle is one of the roots of a cubic.\n\nThe solution of the general quartic equation relies on the solution of its resolvent cubic.\n\nThe eigenvalues of a 3×3 matrix are the roots of a cubic polynomial which is the characteristic polynomial of the matrix.\n\nThe characteristic equation of a third-order linear difference equation or differential equation is a cubic equation.\n\nIn analytical chemistry, the Charlot equation, which can be used to find the pH of buffer solutions, can be solved using a cubic equation.\n\nIn chemical engineering and thermodynamics, cubic equations of state are used to model the PVT (pressure, volume, temperature) behavior of substances.\n\nKinematic equations involving changing rates of acceleration are cubic.\n\n\n\n"}
{"id": "41392084", "url": "https://en.wikipedia.org/wiki?curid=41392084", "title": "Cyclic subspace", "text": "Cyclic subspace\n\nIn mathematics, in linear algebra, a cyclic subspace is a certain special subspace of a finite-dimensional vector space associated with a vector in the vector space and a linear transformation of the vector space. The cyclic subspace associated with a vector \"v\" in a vector space \"V\" and a linear transformation \"T\" of \"V\" is called the \"T\"-cyclic subspace generated by \"v\". The concept of a cyclic subspace is a basic component in the formulation of the cyclic decomposition theorem in linear algebra.\n\nLet formula_1 be a linear transformation of a vector space formula_2 and let formula_3 be a vector in formula_2. The formula_5-cyclic subspace of formula_2 generated by formula_7 is the subspace formula_8 of formula_2 generated by the set of vectors formula_10. This subspace is denoted by formula_11. If formula_12, then formula_7 is called a cyclic vector for formula_5. \n\nThere is another equivalent definition of cyclic spaces. Let formula_1 be a linear transformation of a finite dimensional vector space over a field formula_16 and formula_7 be a vector in formula_2. The set of all vectors of the form formula_19, where formula_20 is a polynomial in the ring formula_21 of all polynomials in formula_22 over formula_16, is the formula_5-cyclic subspace generated by formula_7.\n\n\nLet formula_47 be a linear transformation of a formula_48-dimensional vector space formula_2 over a field formula_16 and formula_7 be a cyclic vector for formula_5. Then the vectors \n\nform an ordered basis for formula_2. Let the characteristic polynomial for formula_5 be \n\nThen \n\nTherefore, relative to the ordered basis formula_58, the operator formula_5 is represented by the matrix \n\nThis matrix is called the \"companion matrix\" of the polynomial formula_61.\n\n\n"}
{"id": "53345922", "url": "https://en.wikipedia.org/wiki?curid=53345922", "title": "DNADynamo", "text": "DNADynamo\n\nDNADynamo is a commercial DNA sequence analysis software package produced by Blue Tractor Software Ltd that runs on Microsoft Windows, Mac OS X and Linux \nIt is used by molecular biologists to analyze DNA and Protein sequences. A free demo is available from the software developers website.\n\nDNADynamo is a general purpose DNA and Protein sequence analysis package that can carry out most of the functions required by a standard research molecular biology laboratory\n\n\nDNADynamo has been developed since 2004 by BlueTractorSoftware Ltd, a software development company based in North Wales, UK \n\n\n"}
{"id": "8361", "url": "https://en.wikipedia.org/wiki?curid=8361", "title": "Definable real number", "text": "Definable real number\n\nInformally, a definable real number is a real number that can be uniquely specified by its description. The description may be expressed as a construction or as a formula of a formal language. For example, the positive square root of 2, formula_1, can be defined as the unique positive solution to the equation formula_2, and it can be constructed with a compass and straightedge.\n\nDifferent choices of a formal language or its interpretation can give rise to different notions of definability. Specific varieties of definable numbers include the constructible numbers of geometry, the algebraic numbers, and the computable numbers.\n\nOne way of specifying a real number uses geometric techniques. A real number \"r\" is a constructible number if there is a method to construct a line segment of length \"r\" using a compass and straightedge, beginning with a fixed line segment of length 1.\n\nEach positive integer, and each positive rational number, is constructible. The positive square root of 2 is constructible. However, the cube root of 2 is not constructible; this is related to the impossibility of doubling the cube.\n\nA real number \"r\" is called an algebraic number if there is a polynomial \"p\"(\"x\"), with only integer coefficients, so that \"r\" is a root of \"p\", that is, \"p\"(\"r\")=0. \nEach algebraic number can be defined individually using the order relation on the reals. For example, if a polynomial \"q\"(\"x\") has 5 roots, the third one can be defined as the unique \"r\" such that \"q\"(\"r\") = 0 and such that there are two distinct numbers less than \"r\" for which \"q\" is zero.\n\nAll rational numbers are algebraic, and all constructible numbers are algebraic. There are numbers such as the cube root of 2 which are algebraic but not constructible.\n\nThe algebraic numbers form a subfield of the real numbers. This means that 0 and 1 are algebraic numbers and, moreover, if \"a\" and \"b\" are algebraic numbers, then so are \"a\"+\"b\", \"a\"−\"b\", \"ab\" and, if \"b\" is nonzero, \"a\"/\"b\".\n\nThe algebraic numbers also have the property, which goes beyond being a subfield of the reals, that for each positive integer \"n\" and each algebraic number \"a\", all of the \"n\"th roots of \"a\" that are real numbers are also algebraic.\n\nThere are only countably many algebraic numbers, but there are uncountably many real numbers, so in the sense of cardinality most real numbers are not algebraic. This nonconstructive proof that not all real numbers are algebraic was first published by\nGeorg Cantor in his 1874 paper \"On a Property of the Collection of All Real Algebraic Numbers\".\n\nNon-algebraic numbers are called transcendental numbers. Specific examples of transcendental numbers include π and Euler's number \"e\".\n\nA real number is a computable number if there is an algorithm that, given a natural number \"n\", produces a decimal expansion for the number accurate to \"n\" decimal places. This notion was introduced by Alan Turing in 1936.\n\nThe computable numbers include the algebraic numbers along with many transcendental numbers including π and \"e\". Like the algebraic numbers, the computable numbers also form a subfield of the real numbers, and the positive computable numbers are closed under taking \"n\"th roots for each positive \"n\".\n\nNot all real numbers are computable. The entire set of computable numbers is countable, so most reals are not computable. Specific examples of noncomputable real numbers include the limits of Specker sequences, and algorithmically random real numbers such as Chaitin's Ω numbers.\n\nAnother notion of definability comes from the formal theories of arithmetic, such as Peano arithmetic. The language of arithmetic has symbols for 0, 1, the successor operation, addition, and multiplication, intended to be interpreted in the usual way over the natural numbers. Because no variables of this language range over the real numbers, a different sort of definability is needed to refer to real numbers. A real number \"a\" is \"definable in the language of arithmetic\" (or \"arithmetical\") if its Dedekind cut can be defined as a predicate in that language; that is, if there is a first-order formula \"φ\" in the language of arithmetic, with three free variables, such that\n\nA real number \"a\" is first-order definable in the language of set theory, without parameters, if there is a formula \"φ\" in the language of set theory, with one free variable, such that \"a\" is the unique real number such that \"φ\"(\"a\") holds (see ). This notion cannot be expressed as a formula in the language of set theory.\n\nAll analytical numbers, and in particular all computable numbers, are definable in the language of set theory. Thus the real numbers definable in the language of set theory include all familiar real numbers such as 0, 1, π, \"e\", et cetera, along with all algebraic numbers. Assuming that they form a set in the model, the real numbers definable in the language of set theory over a particular model of ZFC form a field. \nEach set model \"M\" of ZFC set theory that contains uncountably many real numbers must contain real numbers that are not definable within \"M\" (without parameters). This follows from the fact that there are only countably many formulas, and so only countably many elements of \"M\" can be definable over \"M\". Thus, if \"M\" has uncountably many real numbers, we can prove from \"outside\" \"M\" that not every real number of \"M\" is definable over \"M\". \nThis argument becomes more problematic if it is applied to class models of ZFC, such as the von Neumann universe . The argument that applies to set models cannot be directly generalized to class models in ZFC because the property \"the real number \"x\" is definable over the class model \"N\"\" cannot be expressed as a formula of ZFC. Similarly, the question whether the von Neumann universe contains real numbers that it cannot define cannot be expressed as a sentence in the language of ZFC. Moreover, there are countable models of ZFC in which all real numbers, all sets of real numbers, functions on the reals, etc. are definable .\n\n\n"}
{"id": "55335", "url": "https://en.wikipedia.org/wiki?curid=55335", "title": "Discounting", "text": "Discounting\n\nDiscounting is a financial mechanism in which a debtor obtains the right to delay payments to a creditor, for a defined period of time, in exchange for a charge or fee. Essentially, the party that owes money in the present purchases the right to delay the payment until some future date. The discount, or charge, is the difference between the original amount owed in the present and the amount that has to be paid in the future to settle the debt.\n\nThe discount is usually associated with a \"discount rate\", which is also called the \"discount yield\". The discount yield is the proportional share of the initial amount owed (initial liability) that must be paid to delay payment for 1 year.\n\nSince a person can earn a return on money invested over some period of time, most economic and financial models assume the discount yield is the same as the rate of return the person could receive by investing this money elsewhere (in assets of similar risk) over the given period of time covered by the delay in payment. The concept is associated with the opportunity cost of not having use of the money for the period of time covered by the delay in payment. The relationship between the discount yield and the rate of return on other financial assets is usually discussed in economic and financial theories involving the inter-relation between various market prices, and the achievement of Pareto optimality through the operations in the capitalistic price mechanism, as well as in the discussion of the efficient (financial) market hypothesis. The person delaying the payment of the current liability is essentially compensating the person to whom he/she owes money for the lost revenue that could be earned from an investment during the time period covered by the delay in payment. Accordingly, it is the relevant \"discount yield\" that determines the \"discount\", and not the other way around.\n\nAs indicated, the rate of return is usually calculated in accordance to an annual return on investment. Since an investor earns a return on the original principal amount of the investment as well as on any prior period investment income, investment earnings are \"compounded\" as time advances. Therefore, considering the fact that the \"discount\" must match the benefits obtained from a similar investment asset, the \"discount yield\" must be used within the same compounding mechanism to negotiate an increase in the size of the \"discount\" whenever the time period of the payment is delayed or extended. The \"discount rate\" is the rate at which the \"discount\" must grow as the delay in payment is extended. This fact is directly tied into the time value of money and its calculations.\n\nThe \"time value of money\" indicates there is a difference between the \"future value\" of a payment and the \"present value\" of the same payment. The rate of return on investment should be the dominant factor in evaluating the market's assessment of the difference between the future value and the present value of a payment; and it is the market's assessment that counts the most. Therefore, the \"discount yield\", which is predetermined by a related return on investment that is found in the financial markets, is what is used within the time-value-of-money calculations to determine the \"discount\" required to delay payment of a financial liability for a given period of time.\n\nIf we consider the value of the original payment presently due to be \"P\", and the debtor wants to delay the payment for \"t\" years, then an \"r\" market rate of return on a similar investment asset means the future value of \"P\" is formula_2, and the discount would be calculated as\n\nwhere \"r\" is also the discount yield.\n\nIf \"F\" is a payment that will be made \"t\" years in the future, then the \"present value\" of this payment, also called the \"discounted value\" of the payment, is\n\nTo calculate the present value of a single cash flow, it is divided by one plus the interest rate for each period of time that will pass. This is expressed mathematically as raising the divisor to the power of the number of units of time.\n\nConsider the task to find the present value \"PV\" of $100 that will be received in five years, or equivalently, to find which amount of money today will grow to $100 in five years when subject to a constant discount rate.\n\nAssuming a 12% per year interest rate, it follows that\n\nThe discount rate which is used in financial calculations is usually chosen to be equal to the cost of capital. The cost of capital, in a financial market equilibrium, will be the same as the market rate of return on the financial asset mixture the firm uses to finance capital investment. Some adjustment may be made to the discount rate to take account of risks associated with uncertain cash flows, with other developments.\n\nThe discount rates typically applied to different types of companies show significant differences:\n\n\nThe higher discount rate for start-ups reflects the various disadvantages they face, compared to established companies:\n\n\nOne method that looks into a correct discount rate is the capital asset pricing model. This model takes into account three variables that make up the discount rate:\n\n1. Risk free rate: The percentage of return generated by investing in risk free securities such as government bonds.\n\n2. Beta: The measurement of how a company's stock price reacts to a change in the market. A beta higher than 1 means that a change in share price is exaggerated compared to the rest of shares in the same market. A beta less than 1 means that the share is stable and not very responsive to changes in the market. Less than 0 means that a share is moving in the opposite direction from the rest of the shares in the same market.\n\n3. Equity market risk premium: The return on investment that investors require above the risk free rate.\n\nThe discount factor, \"DF(T)\", is the factor by which a future cash flow must be multiplied in order to obtain the present value. For a zero-rate (also called spot rate) \"r\", taken from a yield curve, and a time to cash flow \"T\" (in years), the discount factor is:\n\nIn the case where the only discount rate one has is not a zero-rate (neither taken from a zero-coupon bond nor converted from a swap rate to a zero-rate through bootstrapping) but an annually-compounded rate (for example if the benchmark is a US Treasury bond with annual coupons) and one only has its yield to maturity, one would use an annually-compounded discount factor:\n\nHowever, when operating in a bank, where the amount the bank can lend (and therefore get interest) is linked to the value of its assets (including accrued interest), traders usually use daily compounding to discount cash flows. Indeed, even if the interest of the bonds it holds (for example) is paid semi-annually, the value of its book of bond will increase daily, thanks to accrued interest being accounted for, and therefore the bank will be able to re-invest these daily accrued interest (by lending additional money or buying more financial products). In that case, the discount factor is then (if the usual money market day count convention for the currency is ACT/360, in case of currencies such as United States dollar, euro, Japanese yen), with \"r\" the zero-rate and \"T\" the time to cash flow in years:\n\nor, in case the market convention for the currency being discounted is ACT/365 (AUD, CAD, GBP):\n\nSometimes, for manual calculation, the continuously-compounded hypothesis is a close-enough approximation of the daily-compounding hypothesis, and makes calculation easier (even though it does not have any real application as no financial instrument is continuously compounded). In that case, the discount factor is:\n\nFor discounts in marketing, see discounts and allowances, sales promotion, and pricing. The article on discounted cash flow provides an example about discounting and risks in real estate investments.\n\n\nNotes\n\n"}
{"id": "1029137", "url": "https://en.wikipedia.org/wiki?curid=1029137", "title": "Eigenplane", "text": "Eigenplane\n\nIn mathematics, an eigenplane is a two-dimensional invariant subspace in a given vector space. By analogy with the term \"eigenvector\" for a vector which, when operated on by a linear operator is another vector which is a scalar multiple of itself, the term eigenplane can be used to describe a two-dimensional plane (a \"2-plane\"), such that the operation of a linear operator on a vector in the 2-plane always yields another vector in the same 2-plane.\n\nA particular case that has been studied is that in which the linear operator is an isometry \"M\" of the hypersphere (written \"S\") represented within four-dimensional Euclidean space:\n\nwhere s and t are four-dimensional column vectors and Λ is a two-dimensional eigenrotation within the eigenplane.\n\nIn the usual eigenvector problem, there is freedom to multiply an eigenvector by an arbitrary scalar; in this case there is freedom to multiply by an arbitrary non-zero rotation.\n\nThis case is potentially physically interesting in the case that the shape of the universe is a multiply connected 3-manifold, since finding the angles of the eigenrotations of a candidate isometry for topological lensing is a way to falsify such hypotheses.\n\n\n"}
{"id": "30265224", "url": "https://en.wikipedia.org/wiki?curid=30265224", "title": "Enzo Martinelli", "text": "Enzo Martinelli\n\nEnzo Martinelli (11 November 1911 – 27 August 1999) was an Italian mathematician, working in the theory of functions of several complex variables: he is best known for his work on the theory of integral representations for holomorphic functions of several variables, notably for discovering the Bochner–Martinelli formula in 1938, and for his work in the theory of multi-dimensional residues.\n\nHe was born in Pescia on 11 November 1911, where his father was the Director of the local agricultural school. His family later went to Rome, where his father ended his working career as the Director-general of the Italian Ministry of Public Education. Enzo Martilnelli lived in Rome almost all of his life: the only exception was a period of nearly eight years, from 1947 to 1954, when he was in Genova, working at the local university. In 1946 he married in Rome Luigia Panella, also her a mathematician, who later become an associate professor at the faculty of Engineering of the Sapienza University of Rome, and who was his loving companion for the rest of his life. They had a son, Roberto, and a daughter, Maria Renata, who later followed her parents footsteps becoming also her a mathematician: four grandchildren completed their family.\n\nIn 1933 he earned his laurea from the Sapienza University of Rome: the title of his thesis was \"\"Sulle funzioni poligene di una e di due variabili complesse\", and his thesis supervisor was Francesco Severi. From 1934 to 1946 he worked as an assistant professor first to the chair of mathematical analysis held by Francesco Severi and then to the chair of geometry held by Enrico Bompiani. In 1939 he became \"Libero Docente\" (free professor) of Mathematical analysis: he taught also courses on analytic geometry, algebraic geometry and topology as associate professor. In 1946 he won a competitive examination by a judging commission for the chair of \"\"Geometria analitica con elementi di Geometria Proiettiva e Geometria Descrittiva con Disegno\", awarded by the University of Genova: the second place and the third place went respectively to Giovanni Dantoni and Guido Zappa. Martinelli held that chair from 1946 to 1954, teaching also mathematical analysis, function theory, differential geometry and algebraic analysis as associate professor. In 1954 He went back in Rome to the chair of Geometry at the university, holding that chair up to his retirement, in 1982: he also taught courses on topology, higher mathematics, higher geometry upon charge. In the years 1968–1969, during a very difficult period for the Sapienza University of Rome, he served the university as the director of the Guido Castelnuovo Institute of Mathematics.\n\nHe attended to various conferences and meetings. In 1943 and in 1946 he was invited in Zurich by Rudolf Fueter, in order to present his researches: later and during all his career he lectured in almost all Italian and foreign universities.\n\nHe was also a member of the UMI Scientific Commission (from 1967 to 1972), of the editorial boards of the Rendiconti di Matematica e delle sue Applicazioni (from 1955 to 1992) and of the Annali di Matematica Pura ed Applicata (from 1965 to 1999).\n\nAccording to , Enzo's talent for mathematics was already evident when he was only a lyceum student. While still attending the university, he won the Cotronei Foundation prize, and after earning his laurea, the Beltrami Foundation prize, the Fubini and Torelli prizes, and the Prize for Mathematical Sciences of the Ministry of National Education: this last one was awarded him in 1943, and the judging commission consisted of Francesco Severi (as the president of the commission), Ugo Amaldi and Antonio Signorini (as the supervisor of the commission).\nIn 1948 he was elected Corresponding Member of the Accademia Ligure di Scienze e Lettere: in 1961 and in 1977 he was elected respectively Corresponding and Full Member of the Accademia dei Lincei, and from 1982 to 1985 he was \"Professore Linceo\". Finally, in 1980 he was elected Corresponding Member of the Accademia delle Scienze di Torino and then, in 1994, Full Member. Also, in 1986, the Sapienza University of Rome, to which Enzo Martinelli was particularly tied for all his life, awarded him the title of professor emeritus.\n\nHe is unanimously remembered as a real gentleman, gifted by a caring attention, politeness, generosity and the rare ability to listen to colleagues and students alike: and remember long conversations with him on various mathematical research topics, and his disposability to give help and advice to whoever asked for it. In particular recalls the time when he was his doctoral student at the University of Genova: they meet every Sunday in the afternoon at Martinelli's home, since Martinelli was not able to meet him during the week. During one of their meetings, lasting a little more than two hours, Martinelli taught him Élie Cartan's theory of exterior differential forms, and Rizza used successfully this tool in his first research works. Another episode illustrating this aspect of Martinelli's personality is recalled by Gaetano Fichera. When he was back in Rome in 1945, at the end of the Second World War, he exposed to Martinelli a theory identical to the theory of differential form: he developed it while being prisoner of the nazists in Teramo during wartime. Martinelli, very tactfully, told him that the idea was already being developed by Élie Cartan and Georges de Rham.\n\nAn excellent teacher himself, capable to arose curiosity and enthusiasm by his lessons, he admired and respected much his own: however, this was quite common for the Italian scientists of the same and the preceding generations, who were advised in the early days of their scientific career by some of the best Italian scientists ever. His doctoral advisor was Francesco Severi: other great Italian mathematicians where among his teachers. Guido Castelnuovo, Federigo Enriques, Enrico Bompiani, Tullio Levi-Civita Mauro Picone and Antonio Signorini were all working at the Sapienza University of Rome when Enzo Martinelli was a student there, following their lessons: describes the activity of the institute of mathematics during that period as extremely stimulating.\n\nAnother central aspect of his personality was a deep sense of justice and legality: Martinelli was very careful in performing his citizen and university professor duties, and he was also ready to fight for his own rights and for the needs of higher education. Concerned by the growing interference of bureaucracy in university education, already in the 1950s he was heard by complaining that: \"\"In Italia mancano le menti semplificatrici\". Martinelli was also free from every kind of authoritarianism to the point that when, during the protests of 1968 in Italy, many newspapers accused the Italian university scientific community of being so, all the assistant professors and students of Martinelli (and perhaps Martinelli himself) were perplexed. In the same period, while performing his duties as the director of the Guido Castelnuovo Institute of Mathematics at the Sapienza university of Rome, his rare intellectual honesty and rigorous rationality, according to Rizza, caused him troubles when dealing with many who \"\"believed in everything except the cold light of reason\".\n\nHe is the author of more than 50 research works, the first of which was published when Martinelli still was an undergraduate student: precisely, his research production consist of 47 papers and 30 between treatises, textbooks and various other publications. According to , his research personality can be described by two words: \"\"enthusiasm\" and \"dissatisfaction\": enthusiasm is meant as his steady interest in mathematics at all levels, while dissatisfaction is meant as the desire to going deeper into all mathematical problems investigated, without stopping at first success and expressing all the results in a simple, elegant and essential form.\n\nThe aspects of his personality described before and his deep professional commitment also made him a great teacher: at least fifteen textbooks on geometry, topology, complex analysis testify his didactic activity. Those books appear as models of clarity and mathematical rigour, and also offer insights on more complex theories and problems to the clever student: indeed, it was one of Martinelli's concerns to teach mathematics showing its lively development and its attractiveness in term of interesting difficult problems offered, in order that no gifted student would abandon the idea to do mathematical research.\n\n\n\n\n\n\n"}
{"id": "17872730", "url": "https://en.wikipedia.org/wiki?curid=17872730", "title": "Ergodic Ramsey theory", "text": "Ergodic Ramsey theory\n\nErgodic Ramsey theory is a branch of mathematics where problems motivated by additive combinatorics are proven using ergodic theory.\n\nErgodic Ramsey theory arose shortly after Endre Szemerédi's proof that a set of positive upper density contains arbitrarily long arithmetic progressions, when Hillel Furstenberg gave a new proof of this theorem using ergodic theory. It has since produced combinatorial results, some of which have yet to be obtained by other means, and has also given a deeper understanding of the structure of measure-preserving dynamical systems.\n\nSzemerédi's theorem is a result in arithmetic combinatorics, concerning arithmetic progressions in subsets of the integers. In 1936, Erdős and Turán conjectured that every set of integers \"A\" with positive natural density contains a \"k\" term arithmetic progression for every \"k\". This conjecture, which became Szemerédi's theorem, generalizes the statement of van der Waerden's theorem. Hillel Furstenberg proved the theorem using ergodic principles in 1977.\n\n\n"}
{"id": "23271188", "url": "https://en.wikipedia.org/wiki?curid=23271188", "title": "Format-preserving encryption", "text": "Format-preserving encryption\n\nIn cryptography, format-preserving encryption (FPE), refers to encrypting in such a way that the output (the ciphertext) is in the same format as the input (the plaintext). The meaning of \"format\" varies. Typically only finite domains are discussed, for example:\n\n\nFor such finite domains, and for the purposes of the discussion below, the cipher is equivalent to a permutation of \"N\" integers } where \"N\" is the size of the domain.\n\nOne motivation for using FPE comes from the problems associated with integrating encryption into existing applications, with well-defined data models. A typical example would be a credit card number, such as codice_1 (16 bytes long, digits only).\n\nAdding encryption to such applications might be challenging if data models are to be changed, as it usually involves changing field length limits or data types. For example, output from a typical block cipher would turn credit card number into a hexadecimal (e.g.codice_2, 34 bytes, hexadecimal digits) or Base64 value (e.g. codice_3, 24 bytes, alphanumeric and special characters), which will break any existing applications expecting the credit card number to be a 16-digit number.\n\nApart from simple formatting problems, using AES-128-CBC, this credit card number might get encrypted to the hexadecimal value codice_4. In addition to the problems caused by creating invalid characters and increasing the size of the data, data encrypted using the CBC mode of an encryption algorithm also changes its value when it is decrypted and encrypted again. This happens because the random seed value that is used to initialize the encryption algorithm and is included as part of the encrypted value is different for each encryption operation. Because of this, it is impossible to use data that has been encrypted with the CBC mode as a unique key to identify a row in a database.\n\nFPE attempts to simplify the transition process by preserving the formatting and length of the original data, allowing a drop-in replacement of plaintext values with their ciphertexts in legacy applications.\n\nAlthough a truly random permutation is the ideal FPE cipher, for large domains it is infeasible to pre-generate and remember a truly random permutation. So the problem of FPE is to generate a pseudorandom permutation from a secret key, in such a way that the computation time for a single value is small (ideally constant, but most importantly smaller than O(N)).\n\nAn n-bit block cipher technically \"is\" a FPE on the set }. If an FPE is needed on one of these standard sized sets (for example, n = 64 for DES and n = 128 for AES) a block cipher of the right size can be used.\n\nHowever, in typical usage, a block cipher is used in a mode of operation that allows it to encrypt arbitrarily long messages, and with an initialization vector as discussed above. In this mode, a block cipher is not an FPE.\n\nIn cryptographic literature (see most of the references below), the measure of a \"good\" FPE is whether an attacker can distinguish the FPE from a truly random permutation. Various types of attackers are postulated, depending on whether they have access to oracles or known ciphertext/plaintext pairs.\n\nIn most of the approaches listed here, a well-understood block cipher (such as AES) is used as a primitive to take the place of an ideal random function. This has the advantage that incorporation of a secret key into the algorithm is easy. Where AES is mentioned in the following discussion, any other good block cipher would work as well.\n\nImplementing FPE with security provably related to that of the underlying block cipher was first undertaken in a paper by cryptographers John Black and Phillip Rogaway, which described three ways to do this. They proved that each of these techniques is as secure as the block cipher that is used to construct it. This means that if the AES algorithm is used to create an FPE algorithm, then the resulting FPE algorithm is as secure as AES because an adversary capable of defeating the FPE algorithm can also defeat the AES algorithm. Therefore, if AES is secure, then the FPE algorithms constructed from it are also secure. In all of the following, \"E\" denotes the AES encryption operation that is used to construct an FPE algorithm and \"F\" denotes the FPE encryption operation.\n\nOne simple way to create an FPE algorithm on {0...,N-1} is to assign a pseudorandom weight to each integer, then sort by weight. The weights are defined by applying an existing block cipher to each integer. Black and Rogaway call this technique a \"prefix cipher\" and showed it was provably as good as the block cipher used.\n\nThus, to create a FPE on the domain {0,1,2,3}, given a key K apply AES(K) to each integer, giving, for example,\n\ncodice_5\ncodice_6\ncodice_7\ncodice_8\n\nSorting [0,1,2,3] by weight gives [3,1,2,0], so the cipher is\n\ncodice_9\ncodice_10\ncodice_11\ncodice_12.\n\nThis method is only useful for small values of N. For larger values, the size of the lookup table and the required number of encryptions to initialize the table gets too big to be practical.\n\nIf there is a set \"M\" of allowed values within the domain of a pseudorandom permutation P (for example P can be a block cipher like AES), an FPE algorithm can be created from the block cipher by repeatedly applying the block cipher until the result is one of the allowed values (within \"M\").\n\nThe recursion is guaranteed to terminate. (Because \"P\" is one-to-one and the domain is finite, repeated application of \"P\" forms a cycle, so starting with a point in \"M\" the cycle will eventually terminate in \"M\".)\n\nThis has the advantage that the elements of \"M\" do not have to be mapped to a consecutive sequence {0...,\"N\"-1} of integers. It has the disadvantage, when \"M\" is much smaller than P's domain, that too many iterations might be required for each operation. If P is a block cipher of a fixed size, such as AES, this is a severe restriction on the sizes of \"M\" for which this method is efficient.\n\nFor example, an application may want to encrypt 100-bit values with AES in a way that creates another 100-bit value. With this technique, AES-128-ECB encryption can be applied until it reaches a value which has all of its 28 highest bits set to 0, which will take an average of 2 iterations to happen.\n\nIt is also possible to make a FPE algorithm using a Feistel network. A Feistel network needs a source of pseudo-random values for the sub-keys for each round, and the output of the AES algorithm can be used as these pseudo-random values. When this is done, the resulting Feistel construction is good if enough rounds are used.\n\nOne way to implement an FPE algorithm using AES and a Feistel network is to use as many bits of AES output as are needed to equal the length of the left or right halves of the Feistel network. If a 24-bit value is needed as a sub-key, for example, it is possible to use the lowest 24 bits of the output of AES for this value.\n\nThis may not result in the output of the Feistel network preserving the format of the input, but it is possible to iterate the Feistel network in the same way that the cycle-walking technique does to ensure that format can be preserved. Because it is possible to adjust the size of the inputs to a Feistel network, it is possible to make it very likely that this iteration ends very quickly on average. In the case of credit card numbers, for example, there are 10 possible 16-digit credit card numbers, and because the 10 = 2, using a 54-bit wide Feistel network along with cycle walking will create an FPE algorithm that encrypts fairly quickly on average.\n\nA Thorp shuffle is like an idealized card-shuffle, or equivalently a maximally-unbalanced Feistel cipher where one side is a single bit. It is easier to prove security for unbalanced Feistel ciphers than for balanced ones.\n\nFor domain sizes that are a power of two, and an existing block cipher with a smaller block size, a new cipher may be created using VIL mode as described by Bellare, Rogaway.\n\nThe Hasty Pudding Cipher uses custom constructions (not depending on existing block ciphers as primitives) to encrypt arbitrary finite small domains.\n\nThe FFSEM mode of AES (specification) that has been accepted for consideration by NIST uses the Feistel network construction of Black and Rogaway described above, with AES for the round function, with one slight modification: a single key is used and is tweaked slightly for each round.\n\nAs of February 2010, FFSEM has been superseded by the FFX mode written by Mihir Bellare, Phillip Rogaway, and Terence Spies. (specification, ).\n\nIn JPEG 2000 standard, the marker codes (in the range 0xFF90 through 0xFFFF) should not appear in the plaintext and ciphertext. The simple modular-0xFF90 technique cannot be applied to solve the JPEG 2000 encryption problem. For example, the ciphertext words 0x23FF and 0x9832 are valid, but their combination 0x23FF9832 becomes invalid since it introduces the marker code 0xFF98. Similarly, the simple cycle-walking technique cannot be applied to solve the JPEG2000 encryption problem since two valid ciphertext blocks may give invalid ciphertext when they get combined. For example, if the first ciphertext block ends with bytes \"...30FF\" and the second ciphertext block starts with bytes \"9832...\", then the marker code \"0xFF98\" would appear in the ciphertext.\n\nTwo mechanisms for format-preserving encryption of JPEG 2000 were given in the paper \"Efficient and Secure Encryption Schemes for JPEG2000\" by Hongjun Wu and Di Ma. To perform format-preserving encryption of JPEG 2000, the technique is to exclude the byte \"0xFF\" in the encryption and decryption. Then a JPEG 2000 encryption mechanism performs modulo-n addition with stream cipher; another JPEG 2000 encryption mechanism performs the cycle-walking technique with block cipher.\n\nSeveral FPE constructs are based on adding the output of a standard cipher, modulo n, to the data to be encrypted, with various methods of unbiasing the result. The modulo-n addition shared by many of the constructs is the immediately obvious solution to the FPE problem (thus its use in a number of cases), with the main differences being the unbiasing mechanisms used.\n\nSection 8 of the FIPS 74, \"Federal Information Processing Standards Publication 1981 Guidelines for Implementing and Using the NBS Data Encryption Standard\", describes a way to use the DES encryption algorithm in a manner that preserves the format of the data via modulo-n addition followed by an unbiasing operation. This standard was withdrawn on May 19, 2005, so the technique should be considered obsolete in terms of being a formal standard.\n\nAnother early mechanism for format-preserving encryption was Peter Gutmann's \"Encrypting data with a restricted range of values\" which again performs modulo-n addition on any cipher with some adjustments to make the result uniform, with the resulting encryption being as strong as the underlying encryption algorithm on which it is based.\n\nThe paper \"Using Datatype-Preserving Encryption to Enhance Data Warehouse Security\" by Michael Brightwell and Harry Smith describes a way to use the DES encryption algorithm in a way that preserves the format of the plaintext. This technique doesn't appear to apply an unbiasing step as do the other modulo-n techniques referenced here.\n\nThe paper \"Format-Preserving Encryption\" by Mihir Bellare and Thomas Ristenpart describes using \"nearly balanced\" Feistel networks to create secure FPE algorithms.\n\nThe paper \"Format Controlling Encryption Using Datatype Preserving Encryption\" by Ulf Mattsson describes other ways to create FPE algorithms.\n\nAn example of FPE algorithm is FNR (\"Flexible Naor and Reingold\").\n\nNIST Special Publication 800-38G, \"Recommendation for Block Cipher Modes of Operation: Methods for Format-Preserving Encryption\" specifies two methods: FF1 and FF3. Details on the proposals submitted for each can be found at the NIST Block Cipher Modes Development site, including patent and test vector information. Sample values are available for both FF1 and FF3.\n\n\nAnother mode was included in the draft NIST guidance but was removed before final publication.\n\n"}
{"id": "3789770", "url": "https://en.wikipedia.org/wiki?curid=3789770", "title": "Francisco Javier González-Acuña", "text": "Francisco Javier González-Acuña\n\nFrancisco Javier González-Acuña (nickname \"Fico\") is a mathematician in the UNAM's institute of mathematics and CIMAT, specializing in low-dimensional topology.\n\nHe did his graduate studies at Princeton University, obtaining his Ph.D. in 1970. His thesis, written under the supervision of Ralph Fox, was titled \"On homology spheres\".\n\nAn early result of González-Acuña is that a group \"G\" is the homomorphic image of some knot group if and only if \"G\" is finitely generated and has weight at most one. This result (a \"remarkable theorem\", as Lee Neuwirth called it in his review),\nwas published in 1975 in the highly respected journal, Annals of Mathematics. In 1978, together with José María Montesinos, he answered a question posed by Fox, proving the existence of 2-knots whose groups have infinitely many ends.\n\nWith Hamish Short, González-Acuña proposed and worked on the cabling conjecture: the only knots in the 3-sphere which admit a reducible Dehn surgery, i.e. a surgery which results in a reducible 3-manifold, are the cable knots. This conjecture is one of the most relevant, unresolved questions in the theory of Dehn surgery on knots in the 3-sphere.\n\nGonzález-Acuña has made other significant contributions, which have been published in journals such as \"Transactions of the American Mathematical Society\", \"Topology\" and \"Mathematical Proceedings of the Cambridge Philosophical Society\".\n\n\n"}
{"id": "61891", "url": "https://en.wikipedia.org/wiki?curid=61891", "title": "Genus (mathematics)", "text": "Genus (mathematics)\n\nIn mathematics, genus (plural genera) has a few different, but closely related, meanings. The most common concept, the genus of an (orientable) surface, is the number of \"holes\" it has. This is made more precise below.\n\nThe genus of a connected, orientable surface is an integer representing the maximum number of cuttings along non-intersecting closed simple curves without rendering the resultant manifold disconnected. It is equal to the number of handles on it. Alternatively, it can be defined in terms of the Euler characteristic \"χ\", via the relationship \"χ\" = 2 − 2\"g\" for closed surfaces, where \"g\" is the genus. For surfaces with \"b\" boundary components, the equation reads \"χ\" = 2 − 2\"g\" − \"b\". In layman's terms, it's the number of \"holes\" an object has (\"holes\" interpreted in the sense of doughnut holes; a hollow sphere would be considered as having zero holes in this sense). A doughnut, or torus, has 1 such hole. A sphere has 0. The green surface pictured above has 2 holes of the relevant sort.\n\nFor instance:\n\nAn explicit construction of surfaces of genus \"g\" is given in the article on the fundamental polygon.\nIn simpler terms, the value of an orientable surface's genus is equal to the number of \"holes\" it has.\n\nThe non-orientable genus, demigenus, or Euler genus of a connected, non-orientable closed surface is a positive integer representing the number of cross-caps attached to a sphere. Alternatively, it can be defined for a closed surface in terms of the Euler characteristic χ, via the relationship χ = 2 − \"k\", where \"k\" is the non-orientable genus.\n\nFor instance:\n\nThe genus of a knot \"K\" is defined as the minimal genus of all Seifert surfaces for \"K\". A Seifert surface of a knot is however a manifold with boundary, the boundary being the knot, i.e.\nhomeomorphic to the unit circle. The genus of such a surface is defined to be the genus of the two-manifold, which is obtained by gluing the unit disk along the boundary.\n\nThe genus of a 3-dimensional handlebody is an integer representing the maximum number of cuttings along embedded disks without rendering the resultant manifold disconnected. It is equal to the number of handles on it.\n\nFor instance:\n\nThe genus of a graph is the minimal integer \"n\" such that the graph can be drawn without crossing itself on a sphere with \"n\" handles (i.e. an oriented surface of genus \"n\"). Thus, a planar graph has genus 0, because it can be drawn on a sphere without self-crossing.\n\nThe non-orientable genus of a graph is the minimal integer \"n\" such that the graph can be drawn without crossing itself on a sphere with \"n\" cross-caps (i.e. a non-orientable surface of (non-orientable) genus \"n\"). (This number is also called the demigenus.)\n\nThe Euler genus is the minimal integer \"n\" such that the graph can be drawn without crossing itself on a sphere with \"n\" cross-caps or on a sphere with \"n/2\" handles.\n\nIn topological graph theory there are several definitions of the genus of a group. Arthur T. White introduced the following concept. The genus of a group \"G\" is the minimum genus of a (connected, undirected) Cayley graph for \"G\".\n\nThe graph genus problem is NP-complete.\n\nThere are two related definitions of genus of any projective algebraic scheme \"X\": the arithmetic genus and the geometric genus. When \"X\" is an algebraic curve with field of definition the complex numbers, and if \"X\" has no singular points, then these definitions agree and coincide with the topological definition applied to the Riemann surface of \"X\" (its manifold of complex points). For example, the definition of elliptic curve from algebraic geometry is \"connected non-singular projective curve of genus 1 with a given rational point on it\".\n\nBy the Riemann-Roch theorem, an irreducible plane curve of degree \"d\" has geometric genus\n\nwhere \"s\" is the number of singularities when properly counted.\n\n"}
{"id": "20101246", "url": "https://en.wikipedia.org/wiki?curid=20101246", "title": "Gompertz distribution", "text": "Gompertz distribution\n\nIn probability and statistics, the Gompertz distribution is a continuous probability distribution, named after Benjamin Gompertz (1779 - 1865). The Gompertz distribution is often applied to describe the distribution of adult lifespans by demographers and actuaries. Related fields of science such as biology and gerontology also considered the Gompertz distribution for the analysis of survival. More recently, computer scientists have also started to model the failure rates of computer codes by the Gompertz distribution. In Marketing Science, it has been used as an individual-level simulation for customer lifetime value modeling. In network theory, particularly the Erdős–Rényi model, the walk length of a random self-avoiding walk (SAW) is distributed according to the Gompertz distribution.\n\nThe probability density function of the Gompertz distribution is:\n\nwhere formula_2 is the scale parameter and formula_3 is the shape parameter of the Gompertz distribution. In the actuarial and biological sciences and in demography, the Gompertz distribution is parametrized slightly differently (Gompertz–Makeham law of mortality).\n\nThe cumulative distribution function of the Gompertz distribution is:\n\nwhere formula_5 and formula_6\n\nThe moment generating function is:\nwhere\n\nThe Gompertz distribution is a flexible distribution that can be skewed to the right and to the left. Its hazard function is a convex function of formula_9. The model can be fitted into the innovation-imitation paradigm with \nformula_10 as the coefficient of innovation and formula_11 as the coefficient of imitation. When formula_12 becomes large, formula_13 approaches formula_14. The model can also belong to the propensity-to-adopt paradigm with \nformula_15 as the propensity to adopt and formula_16 as the overall appeal of the new offering. \nThe Gompertz density function can take on different shapes depending on the values of the shape parameter formula_17:\n\nIf formula_21 and formula_22 are the probability density functions of two Gompertz distributions, then their Kullback-Leibler divergence is given by\nwhere formula_24 denotes the exponential integral and formula_25 is the upper incomplete gamma function.\n\n\n\n"}
{"id": "30780490", "url": "https://en.wikipedia.org/wiki?curid=30780490", "title": "Groupoid algebra", "text": "Groupoid algebra\n\nIn mathematics, the concept of groupoid algebra generalizes the notion of group algebra.\n\nGiven a groupoid formula_1 and a field formula_2 (in the sense of a category with all arrows invertible), it is possible to define the groupoid algebra formula_3 as the algebra over formula_2 formed by the vector space having the elements of (the arrows of) formula_5 as generators and having the multiplication of these elements defined by formula_6, whenever this product is defined, and formula_7 otherwise. The product is then extended by linearity.\n\nSome examples of groupoid algebras are the following:\n\n\n\n"}
{"id": "43814660", "url": "https://en.wikipedia.org/wiki?curid=43814660", "title": "IFRS 4", "text": "IFRS 4\n\nIFRS 4 is an International Financial Reporting Standard (IFRS) issued by the International Accounting Standards Board (IASB) providing guidance for the accounting of insurance contracts. The standard was issued in March 2004, and was amended in 2005 to clarify that the standard covers most financial guarantee contracts. Paragraph 35 of IFRS also applies the standard to financial instruments with discretionary participation features.\n\nIFRS 4 was intended to provide limited improvements to accounting for insurance contracts until the IASB completed the second, more comprehensive phase of its insurance accounting project. The replacement standard, IFRS 17 was issued in May 2017 and will become effective on January 1, 2021, supplanting IFRS 4 at that time.\n\nGenerally, IFRS 4 permitted companies to continue previous accounting practices for insurance contracts, but did enhance the disclosure requirements. IFRS 4 defines an insurance contract as a \"contract under which one party (the insurer) accepts significant insurance risk from another party (the policyholder) by agreeing to compensate the policyholder if a specified uncertain future event (the insured event) adversely affects the policyholder.\" The standard provides definitions to distinguish \"insurance risk\" from \"financial risk.\" IFRS 4 exempts insurance companies from certain other IFRS standards, including IAS 8 on changes in accounting policies, until phase II is complete, but IFRS 4 does introduce its own requirements for changes in accounting policies.\n\nAmong the accounting requirements IFRS 4 introduced are a requirement to test that insurance liabilities are adequate and that reinsurance assets are not impaired. It also prohibits setting up a liability for insurance claims that have not been incurred. Although insurance contracts are subject to the requirements of IFRS 9 that embedded derivatives within other contracts be measured separately at fair value, IFRS 4 makes a limited exception for embedded derivatives that meet the definition of an insurance contract. Such embedded derivatives within insurance contracts do not need to be measured separately.\n\n6 of the 14 IASB board members dissented from issuing IFRS 4. Board members James J. Leisenring, Mary E. Barth, Robert P. Garnett, Gilbert Gélard and John T. Smith dissented because they disagreed with the temporary exemption from the accounting policy changes of IAS 8. Leisenring, Barth, Garnett and Smith further objected to the certain practices permitted by IFRS 4 related to the accounting for assets backing insurance companies, including \"shadow accounting.\" Leisenring, Barth and Smith also objected to the inclusion of financial instruments with discretionary participation features within IFRS 4 rather than within the accounting guidance for financial instruments (which at the time was IAS 39), and Smith raised other objections as well, including the exception to separately measure embedded derivatives that meet the definition of insurance. Board member Tatsumi Yamada dissented separately because he did not believe that IFRS 4 appropriately addressed mismatches between the accounting for insurance contracts and the assets backing the insurance contracts.\n\nLeisenring continued to criticize IFRS 4 after its issue, including a statement that “IFRS 4 is a gift of the IASB to the insurance community that keeps on giving.\"\n"}
{"id": "57632776", "url": "https://en.wikipedia.org/wiki?curid=57632776", "title": "JSON Web Encryption", "text": "JSON Web Encryption\n\nJSON Web Encryption (JWE) is an IETF standard providing a standardised syntax for the exchange of encrypted data, based on JSON and Base64. It is defined by RFC7516. Along with JSON Web Signature (JWS), it is one of the two possible formats of a JWT (JSON Web Token). JWE forms part of the JavaScript Object Signing and Encryption (JOSE) suite of protocols.\n\nIn March 2017, a serious flaw was discovered in many popular implementations of JWE, the invalid curve attack. \n\nOne implementation of an early (pre-finalised) version of JWE also suffered from Bleichenbacher’s attack.\n"}
{"id": "245430", "url": "https://en.wikipedia.org/wiki?curid=245430", "title": "Jean Leray", "text": "Jean Leray\n\nJean Leray (; 7 November 1906 – 10 November 1998) was a French mathematician, who worked on both partial differential equations and algebraic topology.\n\nHe was born in Chantenay-sur-Loire (today part of Nantes). He studied at École Normale Supérieure from 1926 to 1929. He received his Ph.D. in 1933. In 1934 Leray published an important paper that founded the study of weak solutions of the Navier–Stokes equations. In the same year, he and Juliusz Schauder discovered a topological invariant, now called the Leray–Schauder degree, which they applied to prove the existence of solutions for partial differential equations lacking uniqueness.\n\nFrom 1938 to 1939 he was professor at the University of Nancy. He did not join the Bourbaki group, although he was close with its founders.\n\nHis main work in topology was carried out while he was in a prisoner of war camp in Edelbach, Austria from 1940 to 1945. He concealed his expertise on differential equations, fearing that its connections with applied mathematics could lead him to be asked to do war work.\n\nLeray's work of this period proved seminal to the development of spectral sequences and sheaves. These were subsequently developed by many others, each separately becoming an important tool in homological algebra.\n\nHe returned to work on partial differential equations from about 1950.\n\nHe was professor at the University of Paris from 1945 to 1947, and then at the Collège de France until 1978.\n\nHe was awarded the Malaxa Prize (Romania, 1938), the Grand Prix in mathematical sciences (French Academy of Sciences, 1940), the Feltrinelli Prize (Accademia dei Lincei, 1971), the Wolf Prize in Mathematics (Israel, 1979), and the Lomonosov Gold Medal (Moscow, 1988).\n\n\n"}
{"id": "164695", "url": "https://en.wikipedia.org/wiki?curid=164695", "title": "Karl Pearson", "text": "Karl Pearson\n\nKarl Pearson HFRSE LLD (; originally named Carl; 27 March 1857 – 27 April 1936) was an English mathematician and biostatistician. He has been credited with establishing the discipline\nof mathematical statistics. He founded the world's first university statistics department at University College London in 1911, and contributed significantly to the field of biometrics and meteorology. Pearson was also a proponent of social Darwinism and eugenics. Pearson was a protégé and biographer of Sir Francis Galton.\n\nPearson was born in Islington, London to William Pearson QC of the Inner Temple, and his wife Fanny (née Smith), and had two siblings, Arthur and Amy. Pearson was educated privately at University College School, after which he went to King's College, Cambridge in 1876 to study mathematics, graduating in 1879 as Third Wrangler in the Mathematical Tripos. He then travelled to Germany to study physics at the University of Heidelberg under G H Quincke and metaphysics under Kuno Fischer. He next visited the University of Berlin, where he attended the lectures of the physiologist Emil du Bois-Reymond on Darwinism (Emil was a brother of Paul du Bois-Reymond, the mathematician). Pearson also studied Roman Law, taught by Bruns and Mommsen, medieval and 16th century German Literature, and Socialism. He became an accomplished historian and Germanist and spent much of the 1880s in Berlin, Heidelberg, Vienna, Saig bei Lenzkirch, and Brixlegg. He wrote on Passion plays, religion, Goethe, Werther, as well as sex-related themes, and was a founder of the Men and Women's Club.\nPearson was offered a Germanics post at Kings College, Cambridge. Comparing Cambridge students to those he knew from Germany, Karl found German students inathletic and weak. He wrote his mother, \"I used to think athletics and sport was overestimated at Cambridge, but now I think it cannot be too highly valued.\"\n\nOn returning to England in 1880, Pearson first went to Cambridge:\n\nIn his first book, \"The New Werther\", Pearson gives a clear indication of why he studied so many diverse subjects:\n\nPearson then returned to London to study law, emulating his father. Quoting Pearson's own account:\nHis next career move was to the Inner Temple, where he read law until 1881 (although he never practised). After this, he returned to mathematics, deputising for the mathematics professor at King's College, London in 1881 and for the professor at University College, London in 1883. In 1884, he was appointed to the Goldsmid Chair of Applied Mathematics and Mechanics at University College, London. Pearson became the editor of \"Common Sense of the Exact Sciences\" (1885) when William Kingdon Clifford died. 1891 saw him also appointed to the professorship of Geometry at Gresham College; here he met Walter Frank Raphael Weldon, a zoologist who had some interesting problems requiring quantitative solutions. The collaboration, in biometry and evolutionary theory, was a fruitful one and lasted until Weldon died in 1906. Weldon introduced Pearson to Charles Darwin's cousin Francis Galton, who was interested in aspects of evolution such as heredity and eugenics. Pearson became Galton's protégé, at times to the verge of hero worship.\n\nIn 1890 Pearson married Maria Sharpe. The couple had three children: Sigrid Loetitia Pearson, Helga Sharpe Pearson, and Egon Pearson, who became a statistician himself and succeeded his father as head of the Applied Statistics Department at University College. Maria died in 1928 and in 1929 Karl married Margaret Victoria Child, a co-worker at the Biometric Laboratory. He and his family lived at 7 Well Road in Hampstead, now marked with a blue plaque.\n\nAfter Galton's death in 1911, Pearson embarked on producing his definitive biography — a three-volume tome of narrative, letters, genealogies, commentaries, and photographs — published in 1914, 1924, and 1930, with much of Pearson's own money paying for their print runs. The biography, done \"to satisfy myself and without regard to traditional standards, to the needs of publishers or to the tastes of the reading public\", triumphed Galton's life, work and personal heredity. He predicted that Galton, rather than Charles Darwin, would be remembered as the most prodigious grandson of Erasmus Darwin.\n\nWhen Galton died, he left the residue of his estate to the University of London for a Chair in Eugenics. Pearson was the first holder of this chair — the \"Galton Chair of Eugenics\", later the \"Galton Chair of Genetics\"—in accordance with Galton's wishes. He formed the Department of Applied Statistics (with financial support from the Drapers' Company), into which he incorporated the Biometric and Galton laboratories. He remained with the department until his retirement in 1933, and continued to work until his death at Coldharbour, Surrey on 27 April 1936.\n\nPearson was a \"zealous\" atheist and a freethinker.\n\nHe married twice. First in 1890 to Maria Sharpe; Then following Maria's death in 1928, he married Margaret Victoria Child.\n\nWhen the 23-year-old Albert Einstein started the Olympia Academy study group in 1902, with his two younger friends, Maurice Solovine and Conrad Habicht, his first reading suggestion was Pearson's \"The Grammar of Science\". This book covered several themes that were later to become part of the theories of Einstein and other scientists. Pearson asserted that the laws of nature are relative to the perceptive ability of the observer. Irreversibility of natural processes, he claimed, is a purely relative conception. An observer who travels at the exact velocity of light would see an eternal now, or an absence of motion. He speculated that an observer who travelled faster than light would see time reversal, similar to a cinema film being run backwards. Pearson also discussed antimatter, the fourth dimension, and wrinkles in time.\n\nPearson's relativity was based on idealism, in the sense of ideas or pictures in a mind. \"There are many signs,\" he wrote, \"that a sound idealism is surely replacing, as a basis for natural philosophy, the crude materialism of the older physicists.\" (Preface to 2nd Ed., \"The Grammar of Science\") Further, he stated, \"...science is in reality a classification and analysis of the contents of the mind...\" \"In truth, the field of science is much more consciousness than an external world.\" (\"Ibid.\", Ch. II, § 6) \"Law in the scientific sense is thus essentially a product of the human mind and has no meaning apart from man.\" (\"Ibid.\", Ch. III, § 4)\n\nA eugenicist who applied his social Darwinism to entire nations, Pearson saw war against \"inferior races\" as a logical implication of the theory of evolution. \"My view – and I think it may be called the scientific view of a nation,\" he wrote, \"is that of an organized whole, kept up to a high pitch of internal efficiency by insuring that its numbers are substantially recruited from the better stocks, and kept up to a high pitch of external efficiency by contest, chiefly by way of war with inferior races.\" He reasoned that, if August Weismann's theory of germ plasm is correct, the nation is wasting money when it tries to improve people who come from poor stock.\n\nWeismann claimed that acquired characteristics could not be inherited. Therefore, training benefits only the trained generation. Their children will not exhibit the learned improvements and, in turn, will need to be improved. \"No degenerate and feeble stock will ever be converted into healthy and sound stock by the accumulated effects of education, good laws, and sanitary surroundings. Such means may render the individual members of a stock passable if not strong members of society, but the same process will have to be gone through again and again with their offspring, and this in ever-widening circles, if the stock, owing to the conditions in which society has placed it, is able to increase its numbers.\"\n\n\"History shows me one way, and one way only, in which a high state of civilization has been produced, namely, the struggle of race with race, and the survival of the physically and mentally fitter race. If you want to know whether the lower races of man can evolve a higher type, I fear the only course is to leave them to fight it out among themselves, and even then the struggle for existence between individual and individual, between tribe and tribe, may not be supported by that physical selection due to a particular climate on which probably so much of the Aryan's success depended.\"\n\nPearson was known in his lifetime as a prominent \"freethinker\" and socialist. He gave lectures on such issues as \"the woman's question\" (this was the era of the suffragist movement in the UK) and upon Karl Marx. His commitment to socialism and its ideals led him to refuse the offer of being created an OBE (Officer of the Order of the British Empire) in 1920 and also to refuse a knighthood in 1935.\n\nIn \"The Myth of the Jewish Race\" Raphael and Jennifer Patai cite Karl Pearson's 1925 opposition (in the first issue of the journal \"Annals of Eugenics\" which he founded) to Jewish immigration into Britain. Pearson alleged that these immigrants \"will develop into a parasitic race. [...] Taken \"on the average\", and regarding both sexes, this alien Jewish population is somewhat inferior physically and mentally to the native population\".\n\nKarl Pearson was important in the founding of the school of biometrics, which was a competing theory to describe evolution and population inheritance at the turn of the 20th century. His series of eighteen papers, \"Mathematical Contributions to the Theory of Evolution\" established him as the founder of the biometrical school for inheritance. In fact, Pearson devoted much time during 1893 to 1904 to developing statistical techniques for biometry. These techniques, which are widely used today for statistical analysis, include the chi-squared test, standard deviation, and correlation and regression coefficients. Pearson's Law of Ancestral Heredity stated that germ plasm consisted of heritable elements inherited from the parents as well as from more distant ancestors, the proportion of which varied for different traits. Karl Pearson was a follower of Galton, and although the two differed in some respects, Pearson used a substantial amount of Francis Galton's statistical concepts in his formulation of the biometrical school for inheritance, such as the law of regression. The biometric school, unlike the Mendelians, focused not on providing a mechanism for inheritance, but rather on providing a mathematical description for inheritance that was not causal in nature. While Galton proposed a discontinuous theory of evolution, in which species would have to change via large jumps rather than small changes that built up over time, Pearson pointed out flaws in Galton's argument and actually used Galton's ideas to further a continuous theory of evolution, whereas the Mendelian's favored a discontinuous theory of evolution. While Galton focused primarily on the application of statistical methods to the study of heredity, Pearson and his colleague Weldon expanded statistical reasoning to the fields of inheritance, variation, correlation, and natural and sexual selection.\n\nFor Pearson, the theory of evolution was not intended to identify a biological mechanism that explained patterns of inheritance, whereas the Mendelian's postulated the gene as the mechanism for inheritance. Pearson criticized Bateson and other biologists for their failure to adopt biometrical techniques in their study of evolution. Pearson criticized biologists who did not focus on the statistical validity of their theories, stating that \"before we can accept [any cause of a progressive change] as a factor we must have not only shown its plausibility but if possible have demonstrated its quantitative ability\" Biologists had succumb to \"almost metaphysical speculation as to the causes of heredity,\" which had replaced the process of experimental data collection that actually might allow scientists to narrow down potential theories.\n\nFor Pearson, laws of nature were useful for making accurate predictions and for concisely describing trends in observed data. Causation was the experience \"that a certain sequence has occurred and recurred in the past\". Thus, identifying a particular mechanism of genetics was not a worthy pursuit of biologists, who should instead focus on mathematical descriptions of empirical data. This, in part led to the fierce debate between the biometricians and the Mendelians, including Bateson. After Bateson rejected one of Pearson's manuscripts that described a new theory for the variability of an offspring, or homotyposis, Pearson and Weldon established Biometrika in 1902. Although the biometric approach to inheritance eventually lost to the Mendelian approach, the techniques Pearson and the biometricians at the time developed are vital to studies of biology and evolution today.\n\nPearson achieved widespread recognition across a range of disciplines and his membership of, and awards from, various professional bodies reflects this:\n\nHe was also elected an Honorary Fellow of King's College, Cambridge, the Royal Society of Edinburgh, University College London and the Royal Society of Medicine, and a Member of the Actuaries' Club. A sesquicentenary conference was held in London on 23 March 2007, to celebrate the 150th anniversary of his birth.\n\nPearson's work was all-embracing in the wide application and development of mathematical statistics, and encompassed the fields of biology, epidemiology, anthropometry, medicine, psychology and social history. In 1901, with Weldon and Galton, he founded the journal \"Biometrika\" whose object was the development of statistical theory. He edited this journal until his death. Among those who assisted Pearson in his research were a number of female mathematicians who included Beatrice Mabel Cave-Browne-Cave and Frances Cave-Browne-Cave. He also founded the journal \"Annals of Eugenics\" (now \"Annals of Human Genetics\") in 1925. He published the \"Drapers' Company Research Memoirs\" largely to provide a record of the output of the Department of Applied Statistics not published elsewhere.\n\nPearson's thinking underpins many of the 'classical' statistical methods which are in common use today. Examples of his contributions are:\n\n\n\nArticles\n\nMiscellany\n\n\nMost of the biographical information above is taken from the Karl Pearson page at the Department of Statistical Sciences at University College London, which has been placed in the public domain. The main source for that page was \"A list of the papers and correspondence of Karl Pearson (1857–1936)\" held in the Manuscripts Room, University College London Library, compiled by M. Merrington, B. Blundell, S. Burrough, J. Golden and J. Hogarth and published by the Publications Office, University College London, 1983.\n\nAdditional information from entry for Karl Pearson in the Sackler Digital Archive of the Royal Society\n\n\n"}
{"id": "7431140", "url": "https://en.wikipedia.org/wiki?curid=7431140", "title": "Kermeta", "text": "Kermeta\n\nKermeta is a modeling and programming language for metamodel engineering.\n\nThe Kermeta language was initiated by Franck Fleurey in 2005 within the Triskell team of IRISA (gathering researchers of the INRIA, CNRS, INSA and the University of Rennes 1).\n\nThe Kermeta language borrows concepts from languages such MOF, OCL and QVT, but also from BasicMTL, a model transformation language implemented in 2004 in the Triskell team by D. Vojtisek and F. Fondement. It is also inspired by the previous experience on MTL, the first transformation language created by Triskell, and by the Xion action language for UML.\n\nThe name Kermeta is an abbreviation for \"Kernel Metamodeling\" and reflects the fact that the language is conceived as a core for (meta-)modeling. The Breton language consonance of this name is an intentional reflection of the Triskell team's location in Brittany.\n\nKermeta, and its execution platform under Eclipse is currently available under its version 2.0.4 released in 2012. It is open-source, under the Eclipse Public License.\n\nKermeta is a modeling and aspect oriented programming language. Its underlying metamodel conforms to the EMOF standard. It is designed to write programs which are also models, to write transformations of models (programs that transform a model into another), to write constraints on these models, and to execute them 1). The goal of this model approach is to bring an additional level of abstraction on top of the \"object\" level and thus to see a given system like a set of concepts (and instances of concepts) that form an explicitly coherent whole, which one will call a model.\n\nKermeta thus brings:\n\n\nThe main characteristics of the Kermeta language are :\n\n\nThe curious reader will find further information on the Kermeta website.\n\n\n"}
{"id": "37966029", "url": "https://en.wikipedia.org/wiki?curid=37966029", "title": "Kerner's breakdown minimization principle", "text": "Kerner's breakdown minimization principle\n\nKerner’s breakdown minimization principle (BM principle) is a principle for the optimization of vehicular traffic networks introduced by Boris Kerner in 2011.\n\nThe BM principle states that the optimum of a traffic network with \"N\" network bottlenecks is reached, when dynamic traffic optimization and/or control are performed in the network in such a way that the probability for spontaneous occurrence of traffic breakdown in at least one of the network bottlenecks during a given observation time reaches the minimum possible value. The BM principle is equivalent to the maximization of the probability that traffic breakdown occurs at none of the network bottlenecks.\n\nThe empirical ground for Kerner’s BM principle is the set of fundamental empirical features of traffic breakdown at a highway bottleneck found in measured traffic data:\n\nA spontaneous traffic breakdown occurs, where there are free flows both upstream and downstream of the bottleneck before the breakdown has occurred (Fig. 1(a)). In contrast, an induced traffic breakdown is caused by a propagation of a congested pattern that has earlier emerged for example at another downstream bottleneck (Figure 1 (b)).\n\nThe set 1–4 of the fundamental empirical features of traffic breakdown at a highway bottleneck has first been explained in Kerner’s three-phase theory (Figure 3). In Kerner’s theory there are three phases: Free flow (F), synchronized flow (S), wide moving jam (J). Synchronized flow and wide moving jam are associated with congested traffic. The synchronized flow phase is defined as congested traffic whose downstream front is fixed at the bottleneck. Therefore, in accordance with empirical feature 1 traffic breakdown is a phase transition from free flow to synchronized flow (called \"F\" → \"S\" transition). The main feature of an \"F\" → \"S\" transition is as follows (Figure 3 (c, d)): There is a broad range of flow rates on a link of the traffic network between the minimum and maximum capacities of free flow. Within this range of flow rates traffic breakdown occurs with some probability, which depends on the flow rate (Figure 3 (c)).\n\nAssuming that at different bottlenecks traffic breakdown occurs independently, the probability for spontaneous occurrence of traffic breakdown in at least one of the network bottlenecks during a given observation time can be written as:\n\nIn accordance with the BM principle, the network optimum is reached at\n\nHere, formula_3 is the number of network links for which flow rates can be adjusted; formula_4 is the link inflow rate for a link with index formula_5; formula_6, where formula_7; formula_8 is the bottleneck index, formula_9, formula_10; formula_11 is the probability that during the observation time interval traffic breakdown occurs at bottleneck with index formula_8.\n\nResults of simulations of the BM principle for a simple network consisting of only two routes are shown in Figure 4(a). Although the probability of traffic breakdown is an increasing flow rate function for each of the bottlenecks (Figure 3(c)), the probability of traffic breakdown in the network formula_13 has a minimum as a function of the link inflow rates formula_14 and formula_15 (Figure 4(b, c)). \n\nBefore the BM principle is applied to a large traffic network, for each of the origin-destination (O–D) pairs of the network a set of alternative routes (paths) should be found. The alternative routes can be calculated based on the following assumptions: (i) there is free flow in the whole network and (ii) the maximum difference between travel times for alternative routes does not exceed a given value that can be chosen differently for different O–D pairs.\n\nNetwork optimization with measured features of traffic breakdown can consist of the stages: (i) A spatial limitation of congestion growth with the subsequent congestion dissolution at the bottleneck, if the dissolution of congestion due to traffic management in a neighborhood of the bottleneck is possible. (ii) The minimization of traffic breakdown probability with the BM principle in the remaining network, i.e., the network part that is not influenced by congestion.\n\nThe BM principle is an alternative to well-known principles for vehicular network optimization and control based on the minimization of travel costs (travel time, fuel consumption, etc.) or the maximization of traffic throughput (like the maximization of band width of green wave in a city). In particular, the most prominent classical principles for the minimization of travel costs in a traffic network are Wardrop’s user equilibrium (UE) and system optimum (SO) principles that are widely used in a theory of dynamic traffic assignment in the network. Wardrop's SO and UE principles have been explained in Secs. 7.1 and 7.2 of Wikipedia article traffic flow.\n\nHowever, when the flow rate on a link of a network is between the maximum and minimum capacities, there may be at least two different states of a bottleneck on the link denoted by circles F and S shown in Figure 3 (d). The state F is related to free flow and state S to synchronized flow. Therefore, hypothetically assuming that each of the link flow rates for each of the N network bottlenecks is between the associated minimum and maximum bottleneck capacities, we find that there may be formula_16 different bottleneck states in the network at the same distribution of the flow rates in the network. If we apply an optimization algorithm associated with the minimization of travel cost in the network random transitions between these bottleneck states F and S may occur at different network bottlenecks during network optimization and/or control.\n\nRather than some travel costs, in the BM principle the objective function that should be minimized is the probability of traffic breakdown in the network formula_13. Thus the objective function in the BM principle that should be minimized depends neither on travel time nor other travel cost. The BM principle demands the minimization of the probability of traffic breakdown, i.e., the probability of the occurrence of congestion in the network. Under great traffic demand, the application of the BM principle should result in relatively small travel cost associated with free flow in a network.\n"}
{"id": "20627460", "url": "https://en.wikipedia.org/wiki?curid=20627460", "title": "Kind (type theory)", "text": "Kind (type theory)\n\nWhile a `type` is a description of a specific set of values, a `kind` is a descripton of a specific quantity of `types` as in 1, 2, 3, etc. Some `type` constructors, such as a `Monad`, require only one `type` to be able to fully construct a new `type` while others may require two or three `types` to completely construct/describe their `type`. Still, another `type` constuctor may return a `type` constructor that requires even more `types`. Note, that the function `type` constructor, `->`, has a `kind` of 2 and is shown as, `(* -> *)`, where `*` represents a `kind`. The idea of kinds generalizes the notion of generics.\n\nIn the area of mathematical logic and computer science known as type theory, a kind is the type of a type constructor or, less commonly, the type of a higher-order type operator. A kind system is essentially a simply typed lambda calculus \"one level up\", endowed with a primitive type, denoted formula_1 and called \"type\", which is the kind of any data type which does not need any type parameters.\n\nA kind is sometimes confusingly described as the \"type of a (data) type\", but it is actually more of an arity specifier. Syntactically, it is natural to consider polymorphic types to be type constructors, thus non-polymorphic types to be nullary type constructors. But all nullary constructors, thus all monomorphic types, have the same, simplest kind; namely formula_1.\n\nSince higher-order type operators are uncommon in programming languages, in most programming practice, kinds are used to distinguish between data types and the types of constructors which are used to implement parametric polymorphism. Kinds appear, either explicitly or implicitly, in languages whose type systems account for parametric polymorphism in a programatically accessible way, such as Haskell and Scala.\n\n\nHaskell's kind system has just two rules:\n\n\nAn inhabited type (as proper types are called in Haskell) is a type which has values. For instance, ignoring type classes which complicate the picture, codice_1 is a value of type codice_2, while codice_3 is a value of type codice_4 (list of Ints). Therefore, codice_2 and codice_4 have kind formula_1, but so does any function type, for instance codice_7 or even codice_8.\n\nA type constructor takes one or more type arguments, and produces a data type when enough arguments are supplied, i.e. it supports partial application thanks to currying. This is how Haskell achieves parametric types. For instance, the type codice_9 (list) is a type constructor - it takes a single argument to specify the type of the elements of the list. Hence, codice_4 (list of Ints), codice_11 (list of Floats) and even codice_12 (list of lists of Ints) are valid applications of the codice_9 type constructor. Therefore, codice_9 is a type of kind formula_4. Because codice_2 has kind formula_1, applying it to codice_9 results in codice_4, of kind formula_1. The 2-tuple constructor codice_18 has kind formula_5, the 3-tuple constructor codice_19 has kind formula_17 and so on.\n\nStandard Haskell does not allow polymorphic kinds. This is in contrast to parametric polymorphism on types, which is supported in Haskell. For instance, in the following example:\n\nthe kind of codice_20 could be anything, including formula_1, but also formula_4 etc. Haskell by default will always infer kinds to be formula_1, unless the type explicitly indicates otherwise (see below). Therefore the type checker will reject the following use of codice_21:\nbecause the kind of codice_9, formula_4 does not match the expected kind for codice_20, which is always formula_1.\n\nHigher-order type operators are allowed however. For instance:\n\nhas kind formula_23, i.e. codice_24 is expected to be a unary data constructor, which gets applied to its argument, which must be a type, and returns another type.\n\nGHC has the extension codice_25, which, together with codice_26, allows polymorphic kinds. For example:\nIn GHC 8.0.1, types and kinds can merged using the experimental compiler option codice_27.\n\n\n"}
{"id": "8564970", "url": "https://en.wikipedia.org/wiki?curid=8564970", "title": "Landau–Kolmogorov inequality", "text": "Landau–Kolmogorov inequality\n\nIn mathematics, the Landau–Kolmogorov inequality, named after Edmund Landau and Andrey Kolmogorov, is the following family of interpolation inequalities between different derivatives of a function \"f\" defined on a subset \"T\" of the real numbers:\n\nFor \"k\" = 1, \"n\" = 2, \"T\"=R the inequality was first proved by Edmund Landau with the sharp constant \"C\"(2, 1, R) = 2. Following contributions by Jacques Hadamard and Georgiy Shilov, Andrey Kolmogorov found the sharp constants and arbitrary \"n\", \"k\":\n\nwhere \"a\" are the Favard constants.\n\nFollowing work by Matorin and others, the extremising functions were found by Isaac Jacob Schoenberg, explicit forms for the sharp constants are however still unknown.\n\nThere are many generalisations, which are of the form\n\nHere all three norms can be different from each other (from \"L\" to \"L\", with \"p\"=\"q\"=\"r\"=∞ in the classical case) and \"T\" may be the real axis, semiaxis or a closed segment.\n\nThe Kallman–Rota inequality generalizes the Landau–Kolmogorov inequalities from the derivative operator to more general contractions on Banach spaces.\n"}
{"id": "31875947", "url": "https://en.wikipedia.org/wiki?curid=31875947", "title": "Larry Guth", "text": "Larry Guth\n\nLawrence David Guth is a professor of mathematics at the Massachusetts Institute of Technology.\n\nGuth received his Ph.D. in 2005 from the Massachusetts Institute of Technology under the supervision of Tomasz Mrowka.\n\nHe has previously worked at the New York University's Courant Institute of Mathematical Sciences and at the University of Toronto.\n\nIn his research, Guth has strengthened Gromov's systolic inequality for essential manifolds and, along with Nets Katz, found a solution to the Erdős distinct distances problem. His wide-ranging interests include the Kakeya conjecture and the systolic inequality.\n\nGuth is the 2013 winner of the Salem Prize.\n\nHe won an Alfred P. Sloan Fellowship in 2010. He was an invited speaker at the International Congress of Mathematicians in India in 2010, where he spoke about systolic geometry.\nIn 2014 he received a Simons Investigator Award.\nIn 2015 he received the Clay Research Award.\n\nHe was included in the 2019 class of fellows of the American Mathematical Society \"for contributions to harmonic analysis, combinatorics and geometry, and for exposition of high level mathematics\".\n\nHe is the son of physicist Alan Guth known for the theory of Inflation in cosmology.\n\n"}
{"id": "4791359", "url": "https://en.wikipedia.org/wiki?curid=4791359", "title": "Legendre's equation", "text": "Legendre's equation\n\nIn mathematics, Legendre's equation is the Diophantine equation\n\nThe equation is named for Adrien Marie Legendre who proved in 1785 that it is solvable in integers \"x\", \"y\", \"z\", not all zero, if and only if\n−\"bc\", −\"ca\" and −\"ab\" are quadratic residues modulo \"a\", \"b\" and \"c\", respectively, where \"a\", \"b\", \"c\" are nonzero, square-free, pairwise relatively prime integers, not all positive or all negative .\n\n"}
{"id": "22609470", "url": "https://en.wikipedia.org/wiki?curid=22609470", "title": "List of mathematical abbreviations", "text": "List of mathematical abbreviations\n\nThis article is a listing of abbreviated names of mathematical functions, function-like operators and other mathematical terminology.\n\n\n"}
{"id": "950971", "url": "https://en.wikipedia.org/wiki?curid=950971", "title": "Mohr–Coulomb theory", "text": "Mohr–Coulomb theory\n\nMohr–Coulomb theory is a mathematical model (see yield surface) describing the response of brittle materials such as concrete, or rubble piles, to shear stress as well as normal stress. Most of the classical engineering materials somehow follow this rule in at least a portion of their shear failure envelope. Generally the theory applies to materials for which the compressive strength far exceeds the tensile strength.\n\nIn geotechnical engineering it is used to define shear strength of soils and rocks at different effective stresses.\n\nIn structural engineering it is used to determine failure load as well as the angle of fracture of a displacement fracture in concrete and similar materials. Coulomb's friction hypothesis is used to determine the combination of shear and normal stress that will cause a fracture of the material. Mohr's circle is used to determine which principal stresses that will produce this combination of shear and normal stress, and the angle of the plane in which this will occur. According to the principle of normality the stress introduced at failure will be perpendicular to the line describing the fracture condition.\n\nIt can be shown that a material failing according to Coulomb's friction hypothesis will show the displacement introduced at failure forming an angle to the line of fracture equal to the angle of friction. This makes the strength of the material determinable by comparing the external mechanical work introduced by the displacement and the external load with the internal mechanical work introduced by the strain and stress at the line of failure. By conservation of energy the sum of these must be zero and this will make it possible to calculate the failure load of the construction.\n\nA common improvement of this model is to combine Coulomb's friction hypothesis with Rankine's principal stress hypothesis to describe a separation fracture.\n\nThe Mohr–Coulomb theory is named in honour of Charles-Augustin de Coulomb and Christian Otto Mohr. Coulomb's contribution was a 1773 essay entitled \"\"Essai sur une application des règles des maximis et minimis à quelques problèmes de statique relatifs à l'architecture\"\nMohr developed a generalised form of the theory around the end of the 19th century.\nAs the generalised form affected the interpretation of the criterion, but not the substance of it, some texts continue to refer to the criterion as simply the 'Coulomb criterion'.\n\nThe Mohr–Coulomb failure criterion represents the linear envelope that is obtained from a plot of the shear strength of a material versus the applied normal stress. This relation is expressed as\nwhere formula_2 is the shear strength, formula_3 is the normal stress, formula_4 is the intercept of the failure envelope with the formula_2 axis, and formula_6 is the slope of the failure envelope. The quantity formula_4 is often called the cohesion and the angle formula_8 is called the angle of internal friction . Compression is assumed to be positive in the following discussion. If compression is assumed to be negative then formula_3 should be replaced with formula_10.\n\nIf formula_11, the Mohr–Coulomb criterion reduces to the Tresca criterion. On the other hand, if formula_12 the Mohr–Coulomb model is equivalent to the Rankine model. Higher values of formula_8 are not allowed.\n\nFrom Mohr's circle we have\nwhere\nand formula_16 is the maximum principal stress and formula_17 is the minimum principal stress.\n\nTherefore, the Mohr–Coulomb criterion may also be expressed as\n\nThis form of the Mohr–Coulomb criterion is applicable to failure on a plane that is parallel to the formula_19 direction.\n\nThe Mohr–Coulomb criterion in three dimensions is often expressed as\nThe Mohr–Coulomb failure surface is a cone with a hexagonal cross section in deviatoric stress space.\n\nThe expressions for formula_2 and formula_3 can be generalized to three dimensions by developing expressions for the normal stress and the resolved shear stress on a plane of arbitrary orientation with respect to the coordinate axes (basis vectors). If the unit normal to the plane of interest is\nwhere formula_24 are three orthonormal unit basis vectors, and if the principal stresses formula_25 are aligned with the basis vectors formula_26, then the expressions for formula_27 are\nThe Mohr–Coulomb failure criterion can then be evaluated using the usual expression\nfor the six planes of maximum shear stress.\n\nThe Mohr–Coulomb yield surface is often used to model the plastic flow of geomaterials (and other cohesive-frictional materials). Many such materials show dilatational behavior under triaxial states of stress which the Mohr–Coulomb model does not include. Also, since the yield surface has corners, it may be inconvenient to use the original Mohr–Coulomb model to determine the direction of plastic flow (in the flow theory of plasticity).\n\nA common approach is to use a non-associated plastic flow potential that is smooth. An example of such a potential is the function\n\nwhere formula_31 is a parameter, formula_32 is the value of formula_4 when the plastic strain is zero (also called the initial cohesion yield stress), formula_34 is the angle made by the yield surface in the Rendulic plane at high values of formula_35 (this angle is also called the dilation angle), and formula_36 is an appropriate function that is also smooth in the deviatoric stress plane.\n\n\n"}
{"id": "12739372", "url": "https://en.wikipedia.org/wiki?curid=12739372", "title": "N-category number", "text": "N-category number\n\nIn mathematics, the category number of a mathematician is a humorous construct invented by Dan Freed, \nintended to measure the capacity of that mathematician to stomach the use of higher categories. It is defined as the largest number \"n\" such that he or she can think about \"n\"-categories for a half hour without getting a splitting headache.\n\n"}
{"id": "28961424", "url": "https://en.wikipedia.org/wiki?curid=28961424", "title": "Network Science CTA", "text": "Network Science CTA\n\nThe Network Science Collaborative Technology Alliance (NS CTA)\nis a collaborative research alliance funded by the US Army Research Laboratory (ARL) and focused on fundamental\nresearch on the critical scientific and technical challenges that\nemerge from the close interdependence of several genres of\nnetworks such as social/cognitive, information, and communications networks.\nThe primary goal of the NS CTA is to deeply understand the underlying commonalities\namong these intertwined networks, and, by understanding,\nimprove our ability to analyze, predict, design, and influence\ncomplex systems interweaving many kinds of networks.\n\nThis emerging research domain, termed network science,\nalso has the potential to accelerate understanding of each\ngenre of network by cross-fertilization of insights, theories,\nalgorithms, and approaches and by expanding their study\ninto the larger context of the multi-genre (or \"composite\") network environments\nwithin which each must act.\n\nThe NS CTA is an alliance between ARL, other government\nresearchers, and a consortium of four research centers: an\nAcademic Research Center (ARC) focused on social/cognitive\nnetworks (the SCNARC), an ARC focused on information\nnetworks (the INARC), an ARC focused on communications\nnetworks (the CNARC), and an Interdisciplinary Research Center\n(the IRC) focused on interdisciplinary research and technology transition. Overall, these centers include roughly one hundred\nPhD-level researchers from about 30 universities and industrial\nresearch labs, engaged with as many graduate students and interns.\nThe Alliance unites research across organizations and research\ndisciplines to address the critical technical challenges faced by\nthe Army in a world where all missions are embedded in and\ndepend upon many genres of networks. The expected impact\nof its transdisciplinary research includes greatly enhanced\nhuman performance for network-embedded missions and\ngreatly enhanced speed and precision for complex military\noperations. Beyond this vital focus, its research is also\nexpected to accelerate the reach and depth of our understanding\nof the interwoven networks that so profoundly influence all our\nlives.\n\nThe Alliance conducts interdisciplinary research in network\nscience and transitions the results of this fundamental\nresearch to address the technical challenges of network-embedded\nArmy operations. The NS CTA research program exploits\nintellectual synergies across its disciplines by uniting\nfundamental and applied network science research in parallel.\nIt drives the synergistic combination of these technical areas\nfor network-centric and network-enabling capabilities in\nsupport of all missions required of today's military forces,\nincluding humanitarian support, peacekeeping, and combat\noperations in any kind of terrain, but especially in complex\nand urban settings. It also supports and stimulates dual-use\napplications of this research and resulting technology to benefit\ncommercial use.\n\nAs a critical element of this program, the Alliance has\ncreated a network science research facility in Cambridge, MA,\nas well as shared distributed experimental resources throughout\nthe Alliance. The NS CTA also serves the Army’s technical needs\nthrough an education component, which acts to increase the\npool of network science expertise in the Army and the nation\nwhile bringing greater awareness of Army technical challenges\ninto the academic and industrial network science research community.\nIn association with the NS CTA research program, there is\na separate technology transition component that provides a\ncontractual vehicle for other organizations to fund work focused\non transitioning scientific and technical advances into more\nspecific applications. \n\nResearch projects in the NS CTA are by design, highly collaborative and multi-disciplinary, whether based in one of the three academic research centers, the interdisciplinary research center, or one of the two \"cross-cutting research initiatives\" (CCRI).\n\nThe CNARC’s research is focused on characterizing complex communications networks, such as those used for network-centric warfare and operations, so that their behavior can be predicted\naccurately and networks can be configured for optimal information sharing and gathering. In particular, the CNARC will focus on characterizing and controlling the operational information content capacity (OICC) of a tactical network. OICC is a function of the quality and amount of information that is delivered to decision makers. This includes data delivery and security properties of the network. Thus, it is vastly different than other measures of network capacity that are traditionally modeled. In essence, the CNARC models treat the network as an information source. \n\nINARC is aimed at developing the information network technologies required to improve the capabilities of the U.S. Army and providing users with reliable and actionable intelligence across the full spectrum of Network-Centric Operations. INARC will systematically develop the foundation, methodologies, algorithms, and implementations needed for effective, scalable, hierarchical, and most importantly, dynamic and resilient information networks for military applications. The center focuses on Distributed and Real Time Data Integration and Information Fusion; Scalable, Human-Centric Information Network System; and Knowledge Discovery in Information Networks. \n\nThe modern military increasingly needs to rely on bottom up network processes, as compared to top down hierarchic processes. How does the pattern of interactions within a military unit affect\nperformance of tasks? What kinds of ties external to the Army are necessary to success? How can we use massive streams of data to detect adversarial networks? How can a social and cognitive network quickly extract the most meaningful information for the soldier and decision maker that is useful in all aspects of their operations from supporting humanitarian operations to force protection and full combat operations? These are but a sample of network-related questions with which the 21st century Army must wrestle. The long-term objective of the center is to advance the scientific understanding of how the social networks form, operate and evolve and how they affect the functioning of large, complex organizations such as the Army; how adversary networks hidden in large social networks can be detected, monitored or dissolved; and how human cognition directs and is impacted by the network-centric interactions. SCNARC will undertake research to gain a fundamental understanding of the underlying theory, as well as create scientific foundations for modeling, simulation, measurements, analysis, prediction, and influence of social/cognitive networks and their impact on the U.S. Army. \n\nThe IRC is focused on performing interdisciplinary research (both basic and applied) that spans the interests in the Alliance and leads to cross-cutting insights into network science and innovative technologies. The IRC transitions basic research from across the Consortium (its own and the ARCs’) into applied research, and, through the Technology Transition component, promotes the rapid transition of technology to meet the specific needs of a network-centric Army. The IRC leads the Education component as a cooperative program across all four centers. The IRC also operates the NS CTA facility, which supports and coordinates distributed collaborative research and experimentation.\n\nProgrammatically, the IRC is the leader of the Consortium, both intellectually (responsible for setting research directions for the four centers to ensure that research is focused on fundamental\nnetwork science issues that are relevant to network-centric operations and the Army mission) and administratively (responsible for financial management and for tracking and reporting on the\nConsortium’s work).\n\nIn addition to the core research, the NS CTA is investigating two research issues that fundamentally cut across the social/cognitive, information, and communications network genres.\n\nThe goal of the Trust CCRI is to enhance distributed decision making capabilities of the Army in the context of network-centric operations, in particular, for irregular warfare and\ncounterinsurgency by understanding the role trust plays in composite networks that consist of large systems with complex interactions between communication, information, and\nsocial/cognitive networks. Adding to the complexity, trust itself can be highly dynamic with uni- and bi-directional relationships forming, adapting, and dissolving at multiple timescales. Our work directly addresses these dynamics from the propagation and staging of data in the network to support establishment of trust and the revocation of network privileges or use in situations of\ndistrust.\n\nTrust is contextual, and the degree of trust placed in a relationship can directly relate to factors that exist in each network type. Counterinsurgency and irregular warfare place significant demands on trusted tactical decision-making, as any number of social and cultural factors can influence relationships with the local population and governmental officials. Our work in developing models of trust in composite networks will identify these factors and develop trust metrics than expose utility versus risk in specific courses of action.\n\nTrust is also a significant factor in how soldiers perceive and act upon information provided through tactical information systems. Our work in developing cognitive models of trust at the human-machine interface (HMI), models of corroboration for disambiguating data, and the use of argumentation as a mechanism to support automated trust reasoning will directly impact how we build trustworthy systems to convey information to the warfighter.\n\nThe goal of the EDIN CCRI is to develop appropriate mathematical representations and models of dynamic, composite networks composed of social, information, and communication networks; and to establish theories for analyzing their behavior and predicting the evolution of specific properties of such networks over time. The objective of a tactical network may be viewed as delivering the right information at the right time to the right user (persons, applications, and systems) to enable timely and accurate decision-making and therefore, mission success. The tactical network is composed of multiple interacting networks: communications networks, information networks, command-and-control and other social networks. Understanding the structure of the component networks and the dynamics therein, and of the dynamic interactions between these networks is crucial to the design of robust interdisciplinary (or composite1) networks which is one of the primary goals of the NS CTA program.\n\nThe overall approach in EDIN is to mathematically characterize the rich interactions between composite networks (which may comprise multiple networks with varying levels of coupling between them), and the dynamics that occur within each network or across networks. Examples of key technical approaches within EDIN include the development of formal models for reasoning about interacting networks; development of a theory of composite graphs for modeling interacting networks; modeling and analysis of group behaviors using techniques ranging beyond traditional graph theory; development of community discovery algorithms; characterization of temporal graph properties; development of mathematically tractable tactical mobility models; development of theories of co-evolution of interacting networks, etc.\n\nModeling the evolution and dynamics of a network entails understanding both the structural properties of dynamic networks and understanding the dynamics of processes (or behaviors) of interest embedded in the network. Typically, the dynamics of network structure impacts certain processes (e.g., how information propagates through the network); but at the same time the dynamics of processes (or behaviors) may result in alteration of network structures. Therefore, gaining a fundamental understanding of such relationships under several kinds of network dynamics is of paramount importance for obtaining significant insights into the behavior and evolution of complex military networks as well as adversarial networks. EDIN will also investigate a particular type of network dynamics, i.e., mobility (not only of communication devices but also of human beings, information etc.), which is particularly important in the military network context.\n\n\nThere are approximately 30 institutions (universities and industrial R&D labs) involved in the NS CTA.\n\nThe NS CTA facility is located in Cambridge, Massachusetts in the campus of BBN Technologies.\n"}
{"id": "47206618", "url": "https://en.wikipedia.org/wiki?curid=47206618", "title": "Nigerian Association of Mathematical Physics", "text": "Nigerian Association of Mathematical Physics\n\nThe Nigerian Association of Mathematical Physics is a professional academic association of Nigerian mathematical physicists. The association is governed by its Council, which is chaired by the association's President, according to a set of Statutes and Standing Orders.\n\n"}
{"id": "12727", "url": "https://en.wikipedia.org/wiki?curid=12727", "title": "Original proof of Gödel's completeness theorem", "text": "Original proof of Gödel's completeness theorem\n\nThe proof of Gödel's completeness theorem given by Kurt Gödel in his doctoral dissertation of 1929 (and a rewritten version of the dissertation, published as an article in 1930) is not easy to read today; it uses concepts and formalism that are no longer used and terminology that is often obscure. The version given below attempts to represent all the steps in the proof and all the important ideas faithfully, while restating the proof in the modern language of mathematical logic. This outline should not be considered a rigorous proof of the theorem.\n\nWe work with first-order predicate calculus. Our languages allow constant, function and relation symbols. Structures consist of (non-empty) domains and interpretations of the relevant symbols as constant members, functions or relations over that domain.\n\nWe assume classical logic (as opposed to intuitionistic logic for example).\n\nWe fix some axiomatization (i.e. a syntax-based, machine-manageable proof system) of the predicate calculus: logical axioms and rules of inference. Any of the several well-known equivalent axiomatizations will do. Gödel's original proof assumed the Hilbert-Ackermann proof system.\n\nWe assume without proof all the basic well-known results about our formalism that we need, such as the normal form theorem or the soundness theorem.\n\nWe axiomatize predicate calculus \"without equality\" (sometimes confusingly called \"without identity\"), i.e. there are no special axioms expressing the properties of (object) equality as a special relation symbol. After the basic form of the theorem has been proved, it will be easy to extend it to the case of predicate calculus \"with equality\".\n\nIn the following, we state two equivalent forms of the theorem, and show their equivalence.\n\nLater, we prove the theorem. This is done in the following steps:\n\n\nThis is the most basic form of the completeness theorem. We immediately restate it in a form more convenient for our purposes:\nWhen we say \"all structures\", it is important to specify that the structures involved are classical (Tarskian) interpretations I, where I= (U is a non-empty (possibly infinite) set of objects, whereas F is a set of functions from expressions of the interpreted symbolism into U). [By contrast, so-called \"free logics\" countenance possibly empty sets for U. For more regarding free logics, see the work of Karel Lambert.]\n\n\"φ is refutable\" means \"by definition\" \"¬φ is provable\".\n\nTo see the equivalence, note first that if Theorem 1 holds, and φ is not satisfiable in any structure, then ¬φ is valid in all structures and therefore provable, thus φ is refutable and Theorem 2 holds. If on the other hand Theorem 2 holds and φ is valid in all structures, then ¬φ is not satisfiable in any structure and therefore refutable; then ¬¬φ is provable and then so is φ, thus Theorem 1 holds.\n\nWe approach the proof of Theorem 2 by successively restricting the class of all formulas φ for which we need to prove \"φ is either refutable or satisfiable\". At the beginning we need to prove this for all possible formulas φ in our language. However, suppose that for every formula φ there is some formula ψ taken from a more restricted class of formulas C, such that \"ψ is either refutable or satisfiable\" → \"φ is either refutable or satisfiable\". Then, once this claim (expressed in the previous sentence) is proved, it will suffice to prove \"φ is either refutable or satisfiable\" only for φ's belonging to the class C. Note also that if φ is provably equivalent to ψ (\"i.e.\", (φ≡ψ) is provable), then it is indeed the case that \"ψ is either refutable or satisfiable\" → \"φ is either refutable or satisfiable\" (the soundness theorem is needed to show this).\n\nThere are standard techniques for rewriting an arbitrary formula into one that does not use function or constant symbols, at the cost of introducing additional quantifiers; we will therefore assume that all formulas are free of such symbols. Gödel's paper uses a version of first-order predicate calculus that has no function or constant symbols to begin with.\n\nNext we consider a generic formula φ (which no longer uses function or constant symbols) and apply the prenex form theorem to find a formula ψ in \"normal form\" such that φ≡ψ (ψ being in \"normal form\" means that all the quantifiers in ψ, if there are any, are found at the very beginning of ψ). It follows now that we need only prove Theorem 2 for formulas φ in normal form.\n\nNext, we eliminate all free variables from φ by quantifying them existentially: if, say, x...x are free in φ, we form formula_1. If ψ is satisfiable in a structure M, then certainly so is φ and if ψ is refutable, then formula_2 is provable, and then so is ¬φ, thus φ is refutable. We see that we can restrict φ to be a \"sentence\", that is, a formula with no free variables.\n\nFinally, we would like, for reasons of technical convenience, that the \"prefix\" of φ (that is, the string of quantifiers at the beginning of φ, which is in normal form) begin with a universal quantifier and end with an existential quantifier. To achieve this for a generic φ (subject to restrictions we have already proved), we take some one-place relation symbol F unused in φ, and two new variables y and z.. If φ = (P)Φ, where (P) stands for the prefix of φ and Φ for the \"matrix\" (the remaining, quantifier-free part of φ) we form formula_3. Since formula_4 is clearly provable, it is easy to see that formula_5 is provable.\n\nOur generic formula φ now is a sentence, in normal form, and its prefix starts with a universal quantifier and ends with an existential quantifier. Let us call the class of all such formulas R. We are faced with proving that every formula in R is either refutable or satisfiable. Given our formula φ, we group strings of quantifiers of one kind together in blocks:\n\nWe define the degree of formula_7 to be the number of universal quantifier blocks, separated by existential quantifier blocks as shown above, in the prefix of formula_7. The following lemma, which Gödel adapted from Skolem's proof of the Löwenheim–Skolem theorem, lets us sharply reduce the complexity of the generic formula formula_7 we need to prove the theorem for:\n\nLemma. Let k>=1. If every formula in R of degree k is either refutable or satisfiable, then so is every formula in R of degree k+1.\n\nProof. Let φ be a formula of degree k+1; then we can write it as\n\nwhere (P) is the remainder of the prefix of formula_7 (it is thus of degree k-1) and formula_16 is the quantifier-free matrix of formula_7. x, y, u and v denote here \"tuples\" of variables rather than single variables; \"e.g.\" formula_18 really stands for formula_19 where formula_20 are some distinct variables.\n\nLet now x' and y' be tuples of previously unused variables of the same length as x and y respectively, and let Q be a previously unused relation symbol that takes as many arguments as the sum of lengths of x and y; we consider the formula\n\nClearly, formula_22 is provable.\n\nNow since the string of quantifiers formula_23 does not contain variables from x or y, the following equivalence is easily provable with the help of whatever formalism we're using:\n\nAnd since these two formulas are equivalent, if we replace the first with the second inside Φ, we obtain the formula Φ' such that Φ≡Φ':\n\nNow Φ' has the form formula_26, where (S) and (S') are some quantifier strings, ρ and ρ' are quantifier-free, and, furthermore, no variable of (S) occurs in ρ' and no variable of (S') occurs in ρ. Under such conditions every formula of the form formula_27, where (T) is a string of quantifiers containing all quantifiers in (S) and (S') interleaved among themselves in any fashion, but maintaining the relative order inside (S) and (S'), will be equivalent to the original formula Φ'(this is yet another basic result in first-order predicate calculus that we rely on). To wit, we form Ψ as follows:\n\nand we have formula_29.\n\nNow formula_30 is a formula of degree k and therefore by assumption either refutable or satisfiable.\nIf formula_30 is satisfiable in a structure M, then, considering formula_32, we see that formula_7 is satisfiable as well.\nIf formula_30 is refutable, then so is formula_35, which is equivalent to it; thus formula_36 is provable.\nNow we can replace all occurrences of Q inside the provable formula formula_36 by some other formula dependent on the same variables, and we will still get a provable formula.\n\nIn this particular case, we replace Q(x',y') in formula_36 with the formula formula_40. Here (x,y|x',y') means that instead of ψ we are writing a different formula, in which x and y are replaced with x' and y'. Note that Q(x,y) is simply replaced by formula_41.\n\nformula_36 then becomes\n\nand this formula is provable; since the part under negation and after the formula_44 sign is obviously provable, and the part under negation and before the formula_44 sign is obviously φ, just with x and y replaced by x' and y', we see that formula_46 is provable, and φ is refutable. We have proved that φ is either satisfiable or refutable, and this concludes the proof of the Lemma.\n\nNotice that we could not have used formula_40 instead of Q(x',y') from the beginning, because formula_30 would not have been a well-formed formula in that case. This is why we cannot naively use the argument appearing at the comment that precedes the proof.\n\nAs shown by the Lemma above, we only need to prove our theorem for formulas φ in R of degree 1. φ cannot be of degree 0, since formulas in R have no free variables and don't use constant symbols. So the formula φ has the general form:\n\nNow we define an ordering of the k-tuples of natural numbers as follows: formula_50 should hold if either formula_51, or formula_52, and formula_53 precedes formula_54 in lexicographic order. [Here formula_55 denotes the sum of the terms of the tuple.] Denote the nth tuple in this order by formula_56.\n\nSet the formula formula_57 as formula_58. Then put formula_59 as \n\nLemma: For every \"n\", φformula_61.\n\nProof: By induction on n; we have formula_62, where the latter implication holds by variable substitution, since the ordering of the tuples is such that formula_63. But the last formula is equivalent to formula_64φ.\n\nFor the base case, formula_65 is obviously a corollary of φ as well. So the Lemma is proven.\n\nNow if formula_66 is refutable for some \"n\", it follows that φ is refutable. On the other hand, suppose that formula_66 is not refutable for any \"n\". Then for each \"n\" there is some way of assigning truth values to the distinct subpropositions formula_68 (ordered by their first appearance in formula_59; \"distinct\" here means either distinct predicates, or distinct bound variables) in formula_70, such that formula_66 will be true when each proposition is evaluated in this fashion. This follows from the completeness of the underlying propositional logic.\n\nWe will now show that there is such an assignment of truth values to formula_68, so that all formula_59 will be true: The formula_68 appear in the same order in every formula_66; we will inductively define a general assignment to them by a sort of \"majority vote\": Since there are infinitely many assignments (one for each formula_66) affecting formula_77, either infinitely many make formula_77 true, or infinitely many make it false and only finitely many make it true. In the former case, we choose formula_77 to be true in general; in the latter we take it to be false in general. Then from the infinitely many \"n\" for which formula_77 through formula_81 are assigned the same truth value as in the general assignment, we pick a general assignment to formula_68 in the same fashion.\n\nThis general assignment must lead to every one of the formula_83 and formula_84 being true, since if one of the formula_83 were false under the general assignment, formula_59 would also be false for every \"n > k\". But this contradicts the fact that for the finite collection of general formula_68 assignments appearing in formula_84, there are infinitely many \"n\" where the assignment making formula_59 true matches the general assignment.\n\nFrom this general assignment, which makes all of the formula_84 true, we construct an interpretation of the language's predicates that makes φ true. The universe of the model will be the natural numbers. Each i-ary predicate formula_30 should be true of the naturals formula_92 precisely when the proposition formula_93 is either true in the general assignment, or not assigned by it (because it never appears in any of the formula_84).\n\nIn this model, each of the formulas formula_95 is true by construction. But this implies that φ itself is true in the model, since the formula_96 range over all possible k-tuples of natural numbers. So φ is satisfiable, and we are done.\n\nWe may write each B as Φ(x...x,y...y) for some x-s, which we may call \"first arguments\" and y-s that we may call \"last arguments\".\n\nTake B for example. Its \"last arguments\" are z,z...z, and for every possible combination of k of these variables there is some j so that they appear as \"first arguments\" in B. Thus for large enough n, D has the property that the \"last arguments\" of B appear, in every possible combinations of k of them, as \"first arguments\" in other B-s within D. For every B there is a D with the corresponding property.\n\nTherefore in a model that satisfies all the D-s, there are objects corresponding to z, z... and each combination of k of these appear as \"first arguments\" in some B, meaning that for every k of these objects z...z there are z...z, which makes Φ(z...z,z...z) satisfied. By taking a submodel with only these z, z... objects, we have a model satisfying φ.\n\nGödel reduced a formula containing instances of the equality predicate to ones without it in an extended language. His method involves replacing a formula φ containing some instances of equality with the formula\n\nHere formula_105 denote the predicates appearing in φ (with formula_106 their respective arities), and φ' is the formula φ with all occurrences of equality replaced with the new predicate \"Eq\". If this new formula is refutable, the original φ was as well; the same is true of satisfiability, since we may take a quotient of satisfying model of the new formula by the equivalence relation representing \"Eq\". This quotient is well-defined with respect to the other predicates, and therefore will satisfy the original formula φ.\n\nGödel also considered the case where there are a countably infinite collection of formulas. Using the same reductions as above, he was able to consider only those cases where each formula is of degree 1 and contains no uses of equality. For a countable collection of formulas formula_107 of degree 1, we may define formula_108 as above; then define formula_109 to be the closure of formula_110. The remainder of the proof then went through as before.\n\nWhen there is an uncountably infinite collection of formulas, the Axiom of Choice (or at least some weak form of it) is needed. Using the full AC, one can well-order the formulas, and prove the uncountable case with the same argument as the countable one, except with transfinite induction. Other approaches can be used to prove that the completeness theorem in this case is equivalent to the Boolean prime ideal theorem, a weak form of AC.\n\n\n"}
{"id": "2801284", "url": "https://en.wikipedia.org/wiki?curid=2801284", "title": "Proof complexity", "text": "Proof complexity\n\nIn theoretical computer science, and specifically computational complexity theory, proof complexity is the field aiming to understand and analyse the computational resources that are required to prove or refute statements. Research in proof complexity is predominantly concerned with proving proof-length lower and upper bounds in various propositional proof systems. For example, among the major challenges of proof complexity is showing that the usual propositional-calculus does not admit polynomial-size proofs of all tautologies (the actual formalization of a propositional-calculus is immaterial here, since all natural formalizations have been proved to be polynomially-identical); here the size of the proof is simply the number of symbols in it, and a proof is said to be of polynomial size if it is polynomial in the size of the tautology it proves. \n\nSystematic study of proof complexity began with the work of Cook and Reckhow (1979) who provided the basic definition of a propositional proof system from the perspective of computational complexity. Specifically Cook and Reckhow observed that proving proof size lower bounds on stronger and stronger propositional proof systems can be viewed as a step towards separating NP from coNP (and thus P from NP), since the existence of a propositional proof system that admits polynomial size proofs for all tautologies implies that NP=coNP.\n\nContemporary proof complexity research draws ideas and methods from many areas in computational complexity, algorithms and mathematics. Since many important algorithms and algorithmic techniques can be cast as proof search algorithms for certain proof systems, proving lower bounds on these proof sizes implies run time lower bounds on the corresponding algorithms.\n\nMathematical logic can also serve as a framework to study propositional proof sizes. Specifically weak fragments of Peano Arithmetic, which come under the name Bounded Arithmetic theories, serve as a uniform version of propositional proofs, corresponding to different propositional proof systems.\n\nDifferent propositional proof systems for propositional logic, such as the sequent calculus, the cutting-plane method, resolution, etc., may provide different proofs for the same formula. Proof complexity measures the efficiency of the proof system usually in terms of the minimal size of proofs possible in the system for a given tautology (or dually, and unsatisfiable formula).\n\nTwo points make the study of proof complexity non-trivial:\n\n\nThe first point is taken into account by comparing the size of a proof of a formula with the size of the formula. This comparison is made using the usual assumptions of computational complexity: first, a polynomial proof size/formula size ratio means that the proof is of size similar to that of the formula; second, this ratio is studied in the asymptotic case as the size of the formula increases.\n\nThe second point is taken into account by considering, for each formula, the shortest possible proof the considered method can produce.\n\nThe question of polynomiality of proofs is whether a method can always produce a proof of size polynomial in the size of the formula. If such a method exists, then NP would be equal to coNP: this is why the question of polynomiality of proofs is considered important in computational complexity. For some methods, the existence of formulae whose shortest proofs are always superpolynomial has been proved. For other methods, it is an open question.\n\nA second question about proof complexity is whether a method is more efficient than another. Since the proof size depends on the formula, it is possible that one method can produce a short proof of a formula and only long proofs of another formula, while a second method can have exactly the opposite behavior. The assumptions of measuring the size of the proofs relative to the size of the formula and considering only the shortest proofs are also used in this context.\n\nWhen comparing two proof methods, two outcomes are possible:\n\n\nSeveral proofs of the second kind involve contradictory formulae expressing the negation of the pigeonhole principle, namely that formula_1 pigeons can fit formula_2 holes with no hole containing two or more pigeons.\n\nA proof method is automatizable if one of the shorter proofs of a formula can always be generated in time polynomial (or sub-exponential) in the size of the proof. Some methods, but not all, are automatizable. Automatizability results are not in contrast with the assumption that the polynomial hierarchy does not collapse, which would happen if generating a proof in time polynomial in the size \"of the formula\" were always possible.\n\nConsider a tautology of the form formula_3. The tautology is true for every choice of formula_4, and after fixing formula_4 the evaluation of formula_6 and formula_7 are independent because are defined on disjoint sets of variables. This means that it is possible to define an \"interpolant\" circuit formula_8, such that both formula_9 and formula_10 hold. The interpolant circuit decides either if formula_11 is false or if formula_12 is true, by only considering formula_4. The nature of the interpolant circuit can be arbitrary. Nevertheless, it is possible to use a proof of the initial tautology formula_3 as a hint on how to construct formula_15. Some proof systems (e.g. resolution) are said to have \"efficient interpolation\" because the interpolant formula_8 is efficiently computable from any proof of the tautology formula_17 in such proof system. The efficiency is measured with respect to the length of the proof: it is easier to compute interpolants for longer proofs, so this property seems to be anti-monotone in the strength of the proof system.\n\nInterpolation is a weak form of automatization: a way to deduce the existence of small circuits from the existence of small proofs. In particular the following three statements cannot be simultaneously true: (a) formula_17 has a short proof in a some proof system; (b) such proof system has efficient interpolation; (c) the interpolant circuit solves a computationally hard problem. It is clear that (a) and (b) imply that there is a small interpolant circuit, which is in contradiction with (c). Such relation allows to turn proof length upper bounds into lower bounds on computations, and dually to turn efficient interpolation algorithms into lower bounds on proof length.\n\nThe idea of comparing the size of proofs can be used for any automated reasoning procedure that generates a proof. Some research has been done about the size of proofs for propositional non-classical logics, in particular, intuitionistic, modal, and non-monotonic logics.\n\n\n\n"}
{"id": "32838069", "url": "https://en.wikipedia.org/wiki?curid=32838069", "title": "Q-Konhauser polynomials", "text": "Q-Konhauser polynomials\n\nIn mathematics, the \"q\"-Konhauser polynomials are a q-analog of the Konhauser polynomials, introduced by .\n"}
{"id": "3224219", "url": "https://en.wikipedia.org/wiki?curid=3224219", "title": "Second derivative", "text": "Second derivative\n\nIn calculus, the double derivative, or the double anti-integral, of a function is the derivative of the derivative of . Roughly speaking, the second derivative measures how the rate of change of a quantity is itself changing; for example, the second derivative of the position of a vehicle with respect to time is the instantaneous acceleration of the vehicle, or the rate at which the velocity of the vehicle is changing with respect to time. In Leibniz notation:\n\nwhere the last term is the second derivative expression.\n\nOn the graph of a function, the second derivative corresponds to the curvature or concavity of the graph. The graph of a function with a positive second derivative is upwardly concave, while the graph of a function with a negative second derivative curves in the opposite way.\n\nThe power rule for the first derivative, if applied twice, will produce the second derivative power rule as follows:\n\nformula_2\n\nThe second derivative of a function formula_3 is usually denoted formula_4. That is:\nWhen using Leibniz's notation for derivatives, the second derivative of a dependent variable \"y\" with respect to an independent variable \"x\" is written\nThis notation is derived from the following formula:\n\nGiven the function\nthe derivative of \"f\" is the function\nThe second derivative of \"f\" is the derivative of \"f\"′, namely\n\nThe second derivative of a function \"f\" measures the concavity of the graph of \"f\". A function whose second derivative is positive will be concave up (also referred to as convex), meaning that the tangent line will lie below the graph of the function. Similarly, a function whose second derivative is negative will be concave down (also simply called concave), and its tangent lines will lie above the graph of the function.\n\nIf the second derivative of a function changes sign, the graph of the function will switch from concave down to concave up, or vice versa. A point where this occurs is called an inflection point. Assuming the second derivative is continuous, it must take a value of zero at any inflection point, although not every point where the second derivative is zero is necessarily a point of inflection.\n\nThe relation between the second derivative and the graph can be used to test whether a stationary point for a function (i.e. a point where formula_11 ) is a local maximum or a local minimum. Specifically,\nThe reason the second derivative produces these results can be seen by way of a real-world analogy. Consider a vehicle that at first is moving forward at a great velocity, but with a negative acceleration. Clearly the position of the vehicle at the point where the velocity reaches zero will be the maximum distance from the starting position – after this time, the velocity will become negative and the vehicle will reverse. The same is true for the minimum, with a vehicle that at first has a very negative velocity but positive acceleration.\n\nIt is possible to write a single limit for the second derivative:\n\nThe limit is called the second symmetric derivative. Note that the second symmetric derivative may exist even when the (usual) second derivative does not.\n\nThe expression on the right can be written as a difference quotient of difference quotients:\nThis limit can be viewed as a continuous version of the second difference for sequences.\n\nPlease note that the existence of the above limit does not mean that the function formula_22 has a second derivative. The limit above just gives a possibility for calculating the second derivative but does not provide a definition. As a counterexample look on the sign function formula_23 which is defined through\n\nThe sign function is not continuous at zero and therefore the second derivative for formula_25 does not exist. But the above limit exists for formula_25:\n\nJust as the first derivative is related to linear approximations, the second derivative is related to the best quadratic approximation for a function \"f\". This is the quadratic function whose first and second derivatives are the same as those of \"f\" at a given point. The formula for the best quadratic approximation to a function \"f\" around the point \"x\" = \"a\" is\nThis quadratic approximation is the second-order Taylor polynomial for the function centered at \"x\" = \"a\".\n\nFor many combinations of boundary conditions explicit formulas for eigenvalues and eigenvectors of the second derivative can be obtained. For example, assuming formula_29 and homogeneous Dirichlet boundary conditions, i.e., formula_30, the eigenvalues are formula_31 and the corresponding eigenvectors (also called eigenfunctions) are formula_32. Here, formula_33\n\nFor other well-known cases, see the main article eigenvalues and eigenvectors of the second derivative.\n\nThe second derivative generalizes to higher dimensions through the notion of second partial derivatives. For a function \"f\":R → R, these include the three second-order partials\n\nand the mixed partials\n\nIf the function's image and domain both have a potential, then these fit together into a symmetric matrix known as the Hessian. The eigenvalues of this matrix can be used to implement a multivariable analogue of the second derivative test. (See also the second partial derivative test.)\n\nAnother common generalization of the second derivative is the Laplacian. This is the differential operator formula_36 defined by\nThe Laplacian of a function is equal to the divergence of the gradient and the trace of the Hessian matrix.\n\n\n\n\n"}
{"id": "5308894", "url": "https://en.wikipedia.org/wiki?curid=5308894", "title": "Space (mathematics)", "text": "Space (mathematics)\n\nIn mathematics, a space is a set (sometimes called a universe) with some added structure.\n\nWhile modern mathematics uses many types of spaces, such as Euclidean spaces, linear spaces, topological spaces, Hilbert spaces, or probability spaces, it does not define the notion of \"space\" itself.\n\nA space consists of selected mathematical objects that are treated as points, and selected relationships between these points. \nThe nature of the points can vary widely: for example, the points can be elements of a set, functions on another space, or subspaces of another space. It is the relationships that define the nature of the space. More precisely, isomorphic spaces are considered identical, where an isomorphism between two spaces is a one-to-one correspondence between their points that preserves the relationships. For example, the relationships between the points of a three-dimensional Euclidean space are uniquely determined by Euclid's axioms, and all three-dimensional Euclidean spaces are considered identical.\n\nTopological notions such as continuity have natural definitions in every Euclidean space. \nHowever, topology does not distinguish straight lines from curved lines, and the relation between Euclidean and topological spaces is thus \"forgetful\". Relations of this kind are sketched in Figure 1, and treated in more detail in the Section \"Types of spaces\". \n\nIt is not always clear whether a given mathematical object should be considered as a geometric \"space\", or an algebraic \"structure\". A general definition of \"structure\", proposed by Bourbaki, embraces all common types of spaces, provides a general definition of isomorphism, and justifies the transfer of properties between isomorphic structures.\n\nIn ancient Greek mathematics, \"space\" was a geometric abstraction of the three-dimensional reality observed in everyday life. About 300 BC, Euclid gave axioms for the properties of space. Euclid built all of mathematics on these geometric foundations, going so far as to define numbers by comparing the lengths of line segments to the length of a chosen reference segment.\n\nThe method of coordinates (analytic geometry) was adopted by René Descartes in 1637. At that time, geometric theorems were treated as absolute objective truths knowable through intuition and reason, similar to objects of natural science; and axioms were treated as obvious implications of definitions.\n\nTwo equivalence relations between geometric figures were used: congruence and similarity. Translations, rotations and reflections transform a figure into congruent figures; homotheties — into similar figures. For example, all circles are mutually similar, but ellipses are not similar to circles. A third equivalence relation, introduced by Gaspard Monge in 1795, occurs in projective geometry: not only ellipses, but also parabolas and hyperbolas, turn into circles under appropriate projective transformations; they all are projectively equivalent figures.\n\nThe relation between the two geometries, Euclidean and projective, shows that mathematical objects are not given to us \"with their structure\". Rather, each mathematical theory describes its objects by \"some\" of their properties, precisely those that are put as axioms at the foundations of the theory.\n\nDistances and angles cannot appear in theorems of projective geometry, since these notions are neither mentioned in the axioms of projective geometry nor defined from the notions mentioned there. The question \"what is the sum of the three angles of a triangle\" is meaningful in Euclidean geometry but meaningless in projective geometry.\n\nA different situation appeared in the 19th century: in some geometries the sum of the three angles of a triangle is well-defined but different from the classical value (180 degrees). Non-Euclidean hyperbolic geometry, introduced by Nikolai Lobachevsky in 1829 and János Bolyai in 1832 (and Carl Friedrich Gauss in 1816, unpublished) stated that the sum depends on the triangle and is always less than 180 degrees. Eugenio Beltrami in 1868 and Felix Klein in 1871 obtained Euclidean \"models\" of the non-Euclidean hyperbolic geometry, and thereby completely justified this theory as a logical possibility.\n\nThis discovery forced the abandonment of the pretensions to the absolute truth of Euclidean geometry. It showed that axioms are not \"obvious\", nor \"implications of definitions\". Rather, they are hypotheses. To what extent do they correspond to an experimental reality? This important physical problem no longer has anything to do with mathematics. Even if a \"geometry\" does not correspond to an experimental reality, its theorems remain no less \"mathematical truths\".\n\nA Euclidean model of a non-Euclidean geometry is a choice of some objects existing in Euclidean space and some relations between these objects that satisfy all axioms (and therefore, all theorems) of the non-Euclidean geometry. These Euclidean objects and relations \"play\" the non-Euclidean geometry like contemporary actors playing an ancient performance. Actors can imitate a situation that never occurred in reality. Relations between the actors on the stage imitate relations between the characters in the play. Likewise, the chosen relations between the chosen objects of the Euclidean model imitate the non-Euclidean relations. It shows that relations between objects are essential in mathematics, while the nature of the objects is not.\n\nThe word \"geometry\" (from Ancient Greek: geo- \"earth\", -metron \"measurement\") initially meant a practical way of processing lengths, regions and volumes in the space in which we live, but was then extended widely (as well as the notion of space in question here).\n\nAccording to Bourbaki, the period between 1795 (\"Géométrie descriptive\" of Monge) and 1872 (the \"Erlangen programme\" of Klein) can be called the golden age of geometry. The original space investigated by Euclid is now called three-dimensional Euclidean space. Its axiomatization, started by Euclid 23 centuries ago, was reformed with Hilbert's axioms, Tarski's axioms and Birkhoff's axioms. These axiom systems describe the space via primitive notions (such as \"point\", \"between\", \"congruent\") constrained by a number of axioms.\n\nAnalytic geometry made great progress and succeeded in replacing theorems of classical geometry with computations via invariants of transformation groups. Since that time, new theorems of classical geometry have been of more interest to amateurs than to professional mathematicians. However, the heritage of classical geometry was not lost. According to Bourbaki, \"passed over in its role as an autonomous and living science, classical geometry is thus transfigured into a universal language of contemporary mathematics\".\n\nSimultaneously, numbers began to displace geometry as the foundation of mathematics. For instance, in Richard Dedekind's 1872 essay \"Stetigkeit und irrationale Zahlen\" (\"Continuity and irrational numbers\"), he asserts that points on a line ought to have the properties of Dedekind cuts, and that therefore a line was the same thing as the set of real numbers. Dedekind is careful to note that this is an assumption that is incapable of being proven. In modern treatments, Dedekind's assertion is often taken to be the definition of a line, thereby reducing geometry to arithmetic. Three-dimensional Euclidean space is defined to be an affine space whose associated vector space of differences of its elements is equipped with an inner product. A definition \"from scratch\", as in Euclid, is now not often used, since it does not reveal the relation of this space to other spaces. Also, a three-dimensional projective space is now defined as the space of all one-dimensional subspaces (that is, straight lines through the origin) of a four-dimensional vector space. This shift in foundations requires a new set of axioms, and if these axioms are adopted, the classical axioms of geometry become theorems.\n\nA space now consists of selected mathematical objects (for instance, functions on another space, or subspaces of another space, or just elements of a set) treated as points, and selected relationships between these points. Therefore, spaces are just mathematical structures of convenience. One may expect that the structures called \"spaces\" are perceived more geometrically than other mathematical objects, but this is not always true.\n\nAccording to the famous inaugural lecture given by Bernhard Riemann in 1854, every mathematical object parametrized by \"n\" real numbers may be treated as a point of the \"n\"-dimensional space of all such objects. Contemporary mathematicians follow this idea routinely and find it extremely suggestive to use the terminology of classical geometry nearly everywhere.\n\nFunctions are important mathematical objects. Usually they form infinite-dimensional function spaces, as noted already by Riemann and elaborated in the 20th century by functional analysis.\n\nWhile each type of spaces has its own definition, the general idea of \"space\" evades formalization. Some structures are called spaces, other are not, without a formal criterion. Moreover, there is no consensus on the general idea of \"structure\".\nAccording to Pudlák, \"Mathematics [...] cannot be explained completely by a single concept such as the mathematical structure. Nevertheless, Bourbaki's structuralist approach is the best that we have.\"\nWe will return to Bourbaki's structuralist approach in the last section \"Spaces and structures\", while we now outline a possible classification of spaces (and structures) in the spirit of Bourbaki.\n\nWe classify spaces on three levels. Given that each mathematical theory describes its objects by some of their properties, the first question to ask is: which properties? This leads to the first (upper) classification level. On the second level, one takes into account answers to especially important questions (among the questions that make sense according to the first level). On the third level of classification, one takes into account answers to all possible questions.\n\nFor example, the \"upper-level classification\" distinguishes between Euclidean and projective spaces, since the distance between two points is defined in Euclidean spaces but undefined in projective spaces. \nAnother example. The question \"what is the sum of the three angles of a triangle\" makes sense in a Euclidean space but not in a projective space. In a non-Euclidean space the question makes sense but is answered differently, which is not an upper-level distinction.\n\nAlso, the distinction between a Euclidean plane and a Euclidean 3-dimensional space is not an upper-level distinction; the question \"what is the dimension\" makes sense in both cases.\n\nThe \"second-level classification\" distinguishes, for example, between Euclidean and non-Euclidean spaces; between finite-dimensional and infinite-dimensional spaces; between compact and non-compact spaces, etc.\nIn Bourbaki's terms, the second-level classification is the classification by \"species\". Unlike biological taxonomy, a space may belong to several species.\n\nThe \"third-level classification\" distinguishes, for example, between spaces of different dimension, but does not distinguish between a plane of a three-dimensional Euclidean space, treated as a two-dimensional Euclidean space, and the set of all pairs of real numbers, also treated as a two-dimensional Euclidean space. Likewise it does not distinguish between different Euclidean models of the same non-Euclidean space.\nMore formally, the third level classifies spaces up to isomorphism. An isomorphism between two spaces is defined as a one-to-one correspondence between the points of the first space and the points of the second space, that preserves all relations stipulated according to the first level. Mutually isomorphic spaces are thought of as copies of a single space. If one of them belongs to a given species then they all do.\n\nThe notion of isomorphism sheds light on the upper-level classification. Given a one-to-one correspondence between two spaces of the same upper-level class, one may ask whether it is an isomorphism or not. This question makes no sense for two spaces of different classes.\n\nAn isomorphism to itself is called an automorphism. Automorphisms of a Euclidean space are shifts, rotations, reflections and compositions of these. Euclidean space is homogeneous in the sense that every point can be transformed into every other point by some automorphism.\n\nEuclidean axioms leave no freedom; they determine uniquely all geometric properties of the space. More exactly: all three-dimensional Euclidean spaces are mutually isomorphic. In this sense we have \"the\" three-dimensional Euclidean space. In Bourbaki's terms, the corresponding theory is \"univalent\". In contrast, topological spaces are generally non-isomorphic; their theory is \"multivalent\". A similar idea occurs in mathematical logic: a theory is called categorical if all its models of the same cardinality are mutually isomorphic. According to Bourbaki, the study of multivalent theories is the most striking feature which distinguishes modern mathematics from classical mathematics.\n\nTopological notions (continuity, convergence, open sets, closed sets etc.) are defined naturally in every Euclidean space. In other words, every Euclidean space is also a topological space. Every isomorphism between two Euclidean spaces is also an isomorphism between the corresponding topological spaces (called \"homeomorphism\"), but the converse is wrong: a homeomorphism may distort distances. In Bourbaki's terms, \"topological space\" is an \"underlying\" structure of the \"Euclidean space\" structure. Similar ideas occur in category theory: the category of Euclidean spaces is a concrete category over the category of topological spaces; the forgetful (or \"stripping\") functor maps the former category to the latter category.\n\nA three-dimensional Euclidean space is a special case of a Euclidean space. In Bourbaki's terms, the species of three-dimensional Euclidean space is \"richer\" than the species of Euclidean space. Likewise, the species of compact topological space is richer than the species of topological space.\n\nSuch relations between species of spaces may be expressed diagrammatically as shown in Fig. 3. An arrow from A to B means that every is also a or may be treated as a or provides a etc. Treating A and B as classes of spaces one may interpret the arrow as a transition from A to B. (In Bourbaki's terms, \"procedure of deduction\" of a from a Not quite a function unless the classes A,B are sets; this nuance does not invalidate the following.) The two arrows on Fig. 3 are not invertible, but for different reasons. \n\nThe transition from \"Euclidean\" to \"topological\" is forgetful. Topology distinguishes continuous from discontinuous, but does not distinguish rectilinear from curvilinear. Intuition tells us that the Euclidean structure cannot be restored from the topology. A proof uses an automorphism of the topological space (that is, self-homeomorphism) that is not an automorphism of the Euclidean space (that is, not a composition of shifts, rotations and reflections). Such transformation turns the given Euclidean structure into a (isomorphic but) different Euclidean structure; both Euclidean structures correspond to a single topological structure.\n\nIn contrast, the transition from \"3-dim Euclidean\" to \"Euclidean\" is not forgetful; a Euclidean space need not be 3-dimensional, but if it happens to be 3-dimensional, it is full-fledged, no structure is lost. In other words, the latter transition is injective (one-to-one), while the former transition is not injective (many-to-one). We denote injective transitions by an arrow with a barbed tail, \"↣\" rather than \"→\".\n\nBoth transitions are not surjective, that is, not every B-space results from some A-space. First, a 3-dim Euclidean space is a special (not general) case of a Euclidean space. Second, a topology of a Euclidean space is a special case of topology (for instance, it must be non-compact, and connected, etc). We denote surjective transitions by a two-headed arrow, \"↠\" rather than \"→\". See for example Fig. 4; there, the arrow from \"real linear topological\" to \"real linear\" is two-headed, since every real linear space admits some (at least one) topology compatible with its linear structure.\n\nSuch topology is non-unique in general, but unique when the real linear space is finite-dimensional. For these spaces the transition is both injective and surjective, that is, bijective; see the arrow from \"finite-dim real linear topological\" to \"finite-dim real linear\" on Fig. 4. The inverse transition exists (and could be shown by a second, backward arrow). The two species of structures are thus equivalent. In practice, one makes no distinction between equivalent species of structures. Equivalent structures may be treated as a single structure, as shown by a large box on Fig. 4.\n\nThe transitions denoted by the arrows obey isomorphisms. That is, two isomorphic lead to two isomorphic .\n\nThe diagram on Fig. 4 is commutative. That is, all directed paths in the diagram with the same start and endpoints lead to the same result. Other diagrams below are also commutative, except for dashed arrows on Fig. 9. The arrow from \"topological\" to \"measurable\" is dashed for the reason explained there: \"In order to turn a topological space into a measurable space one endows it with a σ-algebra. The σ-algebra of Borel sets is the most popular, but not the only choice.\" A solid arrow denotes a prevalent, so-called \"canonical\" transition that suggests itself naturally and is widely used, often implicitly, by default. For example, speaking about a continuous function on a Euclidean space, one need not specify its topology explicitly. In fact, alternative topologies exist and are used sometimes, for example, the fine topology; but these are always specified explicitly, since they are much less notable that the prevalent topology. A dashed arrow indicates that several transitions are in use and no one is quite prevalent.\n\nTwo basic spaces are linear spaces (also called vector spaces) and topological spaces.\n\nLinear spaces are of algebraic nature; there are real linear spaces (over the field of real numbers),\ncomplex linear spaces (over the field of complex numbers), and more generally, linear spaces over any field. Every complex linear space is also a real linear space (the latter \"underlies\" the former), since each real number is also a complex number.\nMore generally, a vector space over a field also has the structure of a vector space over a subfield of that field.\nLinear operations, given in a linear space by definition, lead to such notions as straight lines (and planes, and other linear subspaces); parallel lines; ellipses (and ellipsoids). However, it is impossible to define \northogonal (perpendicular) lines, or to single out circles among ellipses, because in a linear space \nthere is no structure like a scalar product that could be used for measuring angles. The dimension of a linear space is defined as the maximal number of linearly independent vectors or, equivalently, as the minimal number of vectors that span the space; it may be finite or infinite. Two linear spaces over the same field are isomorphic if and only if they are of the same dimension. A complex linear space is also a real linear space.\n\nTopological spaces are of analytic nature. Open sets, given in a topological space by definition, lead to such notions as continuous functions, paths, maps; convergent sequences, limits; interior, boundary, exterior. However, uniform continuity, bounded sets, Cauchy sequences, differentiable functions (paths, maps) remain undefined. Isomorphisms between topological spaces are traditionally called homeomorphisms; these are one-to-one correspondences continuous in both directions. The open interval (0,1) is homeomorphic to the whole real line (-∞,∞) but not homeomorphic to the closed interval [0,1], nor to a circle. The surface of a cube is homeomorphic to a sphere (the surface of a ball) but not homeomorphic to a torus. Euclidean spaces of different dimensions are not homeomorphic, which seems evident, but is not easy to prove. The dimension of a topological space is difficult to define; inductive dimension (based on the observation that the dimension of the boundary of a geometric figure is usually one less than the dimension of the figure itself) and Lebesgue covering dimension can be used. In the case of a Euclidean space, both topological dimensions are equal to \"n\".\n\nEvery subset of a topological space is itself a topological space (in contrast, only \"linear\" subsets of a linear space are linear spaces). Arbitrary topological spaces, investigated by general topology (called also point-set topology) are too diverse for a complete classification up to homeomorphism. Compact topological spaces are an important class of topological spaces (\"species\" of this \"type\"). Every continuous function is bounded on such space. The closed interval [0,1] and the extended real line [-∞,∞] are compact; the open interval (0,1) and the line (-∞,∞) are not. Geometric topology investigates manifolds (another \"species\" of this \"type\"); these are topological spaces locally homeomorphic to Euclidean spaces (and satisfying a few extra conditions). Low-dimensional manifolds are completely classified up to homeomorphism.\n\nBoth the linear and topological structures underly the linear topological space (in other words, topological vector space) structure. A linear topological space is both a real or complex linear space and a topological space, such that the linear operations are continuous. So a linear space that is also topological is not in general a linear topological space. \n\nEvery finite-dimensional real or complex linear space is a linear topological space in the sense that it carries one and only one topology that makes it a linear topological space. The two structures, \"finite-dimensional real or complex linear space\" and \"finite-dimensional linear topological space\", are thus equivalent, that is, mutually underlying. Accordingly, every invertible linear transformation of a finite-dimensional linear topological space is a homeomorphism. The three notions of dimension (one algebraic and two topological) agree for finite-dimensional real linear spaces. In infinite-dimensional spaces, however, different topologies can conform to a given linear structure, and invertible linear transformations are generally not homeomorphisms.\n\nIt is convenient to introduce affine and projective spaces by means of linear spaces, as follows. A linear subspace of a linear space, being itself a linear space, is not homogeneous; it contains a special point, the origin. Shifting it by a vector external to it, one obtains a affine subspace. It is homogeneous. An affine space need not be included into a linear space, but is isomorphic to an affine subspace of a linear space. All affine spaces are mutually isomorphic. In the words of John Baez, \"an affine space is a vector space that's forgotten its origin\". In particular, every linear space is also an affine space.\n\nGiven an affine subspace \"A\" in a linear space \"L\", a straight line in \"A\" may be defined as the intersection of \"A\" with a linear subspace of \"L\" that intersects \"A\": in other words, with a plane through the origin that is not parallel to \"A\". More generally, a affine subspace of \"A\" is the intersection of \"A\" with a linear subspace of \"L\" that intersects \"A\".\n\nEvery point of the affine subspace \"A\" is the intersection of \"A\" with a linear subspace of \"L\". However, some subspaces of \"L\" are parallel to \"A\"; in some sense, they intersect \"A\" at infinity. The set of all linear subspaces of a linear space is, by definition, a projective space. And the affine subspace \"A\" is embedded into the projective space as a proper subset. However, the projective space itself is homogeneous. A straight line in the projective space corresponds to a linear subspace of the (n+1)-dimensional linear space. More generally, a projective subspace of the projective space corresponds to a linear subspace of the (n+1)-dimensional linear space, and is isomorphic to the projective space.\n\nDefined this way, affine and projective spaces are of algebraic nature; they can be real, complex, and more generally, over any field.\n\nEvery real or complex affine or projective space is also a topological space. An affine space is a non-compact manifold; a projective space is a compact manifold. In a real projective space a straight line is homeomorphic to a circle, therefore compact, in contrast to a straight line in a linear of affine space.\n\nDistances between points are defined in a metric space. Isomorphisms between metric spaces are called isometries. Every metric space is also a topological space. A topological space is called metrizable, if it underlies a metric space. All manifolds are metrizable.\n\nIn a metric space, we can define \nbounded sets and Cauchy sequences. A metric space is called complete if all Cauchy sequences converge. Every incomplete space is isometrically embedded, as a dense subset, into a complete space (the completion). Every compact metric space is complete; the real line is non-compact but complete; the open interval (0,1) is incomplete. \n\nEvery Euclidean space is also a complete metric space. Moreover, all geometric notions immanent to a Euclidean space can be characterized in terms of its metric. For example, the straight segment connecting two given points \"A\" and \"C\" consists of all points \"B\" such that the distance between \"A\" and \"C\" is equal to the sum of two distances, between \"A\" and \"B\" and between \"B\" and \"C\".\n\nThe Hausdorff dimension (related to the number of small balls that cover the given set) applies to metric spaces, and can be non-integer (especially for fractals). For a Euclidean space, the Hausdorff dimension is equal to \"n\".\n\nUniform spaces do not introduce distances, but still allow one to use uniform continuity, Cauchy sequences (or filters or nets), completeness and completion. Every uniform space is also a topological space. Every \"linear\" topological space (metrizable or not) is also a uniform space, and is complete in finite dimension but generally incomplete in infinite dimension. More generally, every commutative topological group is also a uniform space. A non-commutative topological group, however, carries two uniform structures, one left-invariant, the other right-invariant.\n\nVectors in a Euclidean space form a linear space, but each vector formula_1 has also a length, in other words, norm, formula_2. A real or complex linear space endowed with a norm is a normed space. Every normed space is both a linear topological space and a metric space. A Banach space is a complete normed space. Many spaces of sequences or functions are infinite-dimensional Banach spaces.\n\nThe set of all vectors of norm less than one is called the unit ball of a normed space. It is a convex, centrally symmetric set, generally not an ellipsoid; for example, it may be a polygon (in the plane) or. more generally, a polytope (in arbitrary finite dimension). The parallelogram law (called also parallelogram identity) \ngenerally fails in normed spaces, but holds for vectors in Euclidean spaces, which follows from the fact that the squared Euclidean norm of a vector is its inner product with itself, formula_4.\n\nAn inner product space is a real or complex linear space, endowed with a bilinear or respectively sesquilinear form, satisfying some conditions and called an inner product. Every inner product space is also a normed space. A normed space underlies an inner product space if and only if it satisfies the parallelogram law, or equivalently, if its unit ball is an ellipsoid. Angles between vectors are defined in inner product spaces. A Hilbert space is defined as a complete inner product space. (Some authors insist that it must be complex, others admit also real Hilbert spaces.) Many spaces of sequences or functions are infinite-dimensional Hilbert spaces. Hilbert spaces are very important for quantum theory.\n\nAll real inner product spaces are mutually isomorphic. One may say that the Euclidean space is the real inner product space that forgot its origin.\n\nSmooth manifolds are not called \"spaces\", but could be. Every smooth manifold is a topological manifold, and can be embedded into a finite-dimensional linear space. Smooth surfaces in a finite-dimensional linear space are smooth manifolds: for example, the surface of an ellipsoid is a smooth manifold, a polytope is not. Real or complex finite-dimensional linear, affine and projective spaces are also smooth manifolds.\n\nAt each one of its points, a smooth path in a smooth manifold has a tangent vector that belongs to the manifold's tangent space at this point. Tangent spaces to an smooth manifold are linear spaces. The differential of a smooth function on a smooth manifold provides a linear functional on the tangent space at each point. \n\nA Riemannian manifold, or Riemann space, is a smooth manifold whose tangent spaces are endowed with inner products satisfying some conditions. Euclidean spaces are also Riemann spaces. Smooth surfaces in Euclidean spaces are Riemann spaces. A hyperbolic space is also a Riemann space. A curve in a Riemann space has a length, and the length of the shortest curve between two points defines a distance, such that the Riemann space is a metric space. The angle between two curves intersecting at a point is the angle between their tangent lines.\n\nWaiving positivity of inner products on tangent spaces, one obtains pseudo-Riemann spaces, including the Lorentzian spaces that are very important for general relativity.\n\nWaiving distances and angles while retaining volumes (of geometric bodies) one reaches measure theory. Besides the volume, a measure generalizes the notions of area, length, mass (or charge) distribution, and also probability distribution, according to Andrey Kolmogorov's approach to probability theory.\n\nA \"geometric body\" of classical mathematics is much more regular than just a set of points. The boundary of the body is of zero volume. Thus, the volume of the body is the volume of its interior, and the interior can be exhausted by an infinite sequence of cubes. In contrast, the boundary of an arbitrary set of points can be of non-zero volume (an example: the set of all rational points inside a given cube). Measure theory succeeded in extending the notion of volume to a vast class of sets, the so-called measurable sets. Indeed, non-measurable sets almost never occur in applications.\n\nMeasurable sets, given in a measurable space by definition, lead to measurable functions and maps. In order to turn a topological space into a measurable space one endows it with a The of Borel sets is the most popular, but not the only choice. (Baire sets, universally measurable sets, etc, are also used sometimes.) \nThe topology is not uniquely determined by the Borel for example, the norm topology and the weak topology on a separable Hilbert space lead to the same Borel .\nNot every is the Borel of some topology.\nActually, a can be generated by a given collection of sets (or functions) irrespective of any topology. Every subset of a measurable space is itself a measurable space.\n\nStandard measurable spaces (also called standard Borel spaces) are especially useful due to some similarity to compact spaces (see EoM). Every bijective measurable mapping between standard measurable spaces is an isomorphism; that is, the inverse mapping is also measurable. And a mapping between such spaces is measurable if and only if its graph is measurable in the product space. Similarly, every bijective continuous mapping between compact metric spaces is a homeomorphism; that is, the inverse mapping is also continuous. And a mapping between such spaces is continuous if and only if its graph is closed in the product space.\n\nEvery Borel set in a Euclidean space (and more generally, in a complete separable metric space), endowed with the Borel is a standard measurable space. All uncountable standard measurable spaces are mutually isomorphic.\n\nA measure space is a measurable space endowed with a measure. A Euclidean space with the Lebesgue measure is a measure space. Integration theory defines integrability and integrals of measurable functions on a measure space.\n\nSets of measure 0, called null sets, are negligible. Accordingly, a \"mod 0 isomorphism\" is defined as isomorphism between subsets of full measure (that is, with negligible complement).\n\nA probability space is a measure space such that the measure of the whole space is equal to 1. The product of any family (finite or not) of probability spaces is a probability space. In contrast, for measure spaces in general, only the product of finitely many spaces is defined. Accordingly, there are many infinite-dimensional probability measures (especially, Gaussian measures), but no infinite-dimensional Lebesgue measures.\n\nStandard probability spaces are especially useful. On a standard probability space a conditional expectation may be treated as the integral over the conditional measure (regular conditional probabilities, see also disintegration of measure). Given two standard probability spaces, every homomorphism of their measure algebras is induced by some measure preserving map. Every probability measure on a standard measurable space leads to a standard probability space. The product of a sequence (finite or not) of standard probability spaces is a standard probability space. All non-atomic standard probability spaces are mutually isomorphic mod 0; one of them is the interval (0,1) with the Lebesgue measure.\n\nThese spaces are less geometric. In particular, the idea of dimension, applicable (in one form or another) to all other spaces, does not apply to measurable, measure and probability spaces.\n\nThe theoretical study of calculus, known as mathematical analysis, led in the early 20th century to the consideration of linear spaces of real-valued or complex-valued functions. The earliest examples of these were function spaces, each one adapted to its own class of problems. These examples shared many common features, and these features were soon abstracted into Hilbert spaces, Banach spaces, and more general topological vector spaces. These were a powerful toolkit for the solution of a wide range of mathematical problems.\n\nThe most detailed information was carried by a class of spaces called Banach algebras. These are Banach spaces together with a continuous multiplication operation. An important early example was the Banach algebra of essentially bounded measurable functions on a measure space \"X\". This set of functions is a Banach space under pointwise addition and scalar multiplication. With the operation of pointwise multiplication, it becomes a special type of Banach space, one now called a commutative von Neumann algebra. Pointwise multiplication determines a representation of this algebra on the Hilbert space of square integrable functions on \"X\". An early observation of John von Neumann was that this correspondence also worked in reverse: Given some mild technical hypotheses, a commutative von Neumann algebra together with a representation on a Hilbert space determines a measure space, and these two constructions (of a von Neumann algebra plus a representation and of a measure space) are mutually inverse.\n\nVon Neumann then proposed that non-commutative von Neumann algebras should have geometric meaning, just as commutative von Neumann algebras do. Together with Francis Murray, he produced a classification of von Neumann algebras. The direct integral construction shows how to break any von Neumann algebra into a collection of simpler algebras called \"factors\". Von Neumann and Murray classified factors into three types. Type I was nearly identical to the commutative case. Types II and III exhibited new phenomena. A type II von Neumann algebra determined a geometry with the peculiar feature that the dimension could be any non-negative real number, not just an integer. Type III algebras were those that were neither types I nor II, and after several decades of effort, these were proven to be closely related to type II factors.\n\nA slightly different approach to the geometry of function spaces developed at the same time as von Neumann and Murray's work on the classification of factors. This approach is the theory of Here, the motivating example is the formula_5, where \"X\" is a locally compact Hausdorff topological space. By definition, this is the algebra of continuous complex-valued functions on \"X\" that vanish at infinity (which loosely means that the farther you go from a chosen point, the closer the function gets to zero) with the operations of pointwise addition and multiplication. The Gelfand–Naimark theorem implied that there is a correspondence between commutative and geometric objects: Every commutative is of the form formula_5 for some locally compact Hausdorff space \"X\". Consequently it is possible to study locally compact Hausdorff spaces purely in terms of commutative Non-commutative geometry takes this as inspiration for the study of non-commutative If there were such a thing as a \"non-commutative space \"X\",\" then its formula_5 would be a non-commutative ; if in addition the Gelfand–Naimark theorem applied to these non-existent objects, then spaces (commutative or not) would be the same as so, for lack of a direct approach to the definition of a non-commutative space, a non-commutative space is \"defined\" to be a non-commutative Many standard geometric tools can be restated in terms of and this gives geometrically-inspired techniques for studying non-commutative .\n\nBoth of these examples are now cases of a field called non-commutative geometry. The specific examples of von Neumann algebras and are known as non-commutative measure theory and non-commutative topology, respectively. Non-commutative geometry is not merely a pursuit of generality for its own sake and is not just a curiosity. Non-commutative spaces arise naturally, even inevitably, from some constructions. For example, consider the non-periodic Penrose tilings of the plane by kites and darts. It is a theorem that, in such a tiling, every finite patch of kites and darts appears infinitely often. As a consequence, there is no way to distinguish two Penrose tilings by looking at a finite portion. This makes it impossible to assign the set of all tilings a topology in the traditional sense. Despite this, the Penrose tilings determine a non-commutative and consequently they can be studied by the techniques of non-commutative geometry. Another example, and one of great interest within differential geometry, comes from foliations of manifolds. These are ways of splitting the manifold up into smaller-dimensional submanifolds called \"leaves\", each of which is locally parallel to others nearby. The set of all leaves can be made into a topological space. However, the example of an irrational rotation shows that this topological space can be inacessible to the techniques of classical measure theory. However, there is a non-commutative von Neumann algebra associated to the leaf space of a foliation, and once again, this gives an otherwise unintelligible space a good geometric structure.\n\nAlgebraic geometry studies the geometric properties of polynomial equations. Polynomials are a type of function defined from the basic arithmetic operations of addition and multiplication. Because of this, they are closely tied to algebra. Algebraic geometry offers a way to apply geometric techniques to questions of pure algebra, and vice versa.\n\nPrior to the 1940s, algebraic geometry worked exclusively over the complex numbers, and the most fundamental variety was projective space. The geometry of projective space is closely related to the theory of perspective, and its algebra is described by homogeneous polynomials. All other varieties were defined as subsets of projective space. Projective varieties were subsets defined by a set of homogeneous polynomials. At each point of the projective variety, all the polynomials in the set were required to equal zero. The complement of the zero set of a linear polynomial is an affine space, and an affine variety was the intersection of a projective variety with an affine space.\n\nAndré Weil saw that geometric reasoning could sometimes be applied in number-theoretic situations where the spaces in question might be discrete or even finite. In pursuit of this idea, Weil rewrote the foundations of algebraic geometry, both freeing algebraic geometry from its reliance on complex numbers and introducing \"abstract algebraic varieties\" which were not embedded in projective space. These are now simply called \"varieties\".\n\nThe type of space that underlies most modern algebraic geometry is even more general than Weil's abstract algebraic varieties. It was introduced by Alexander Grothendieck and is called a scheme. One of the motivations for scheme theory is that polynomials are unusually structured among functions, and algebraic varieties are consequently rigid. This presents problems when attempting to study degenerate situations. For example, almost any pair of points on a circle determines a unique line called the secant line, and as the two points move around the circle, the secant line varies continuously. However, when the two points collide, the secant line degenerates to a tangent line. The tangent line is unique, but the geometry of this configuration—a single point on a circle—is not expressive enough to determine a unique line. Studying situations like this requires a theory capable of assigning extra data to degenerate situations.\n\nOne of the building blocks of a scheme is a topological space. Topological spaces have continuous functions, but continuous functions are too general to reflect the underlying algebraic structure of interest. The other ingredient in a scheme, therefore, is a sheaf on the topological space, called the \"structure sheaf\". On each open subset of the topological space, the sheaf specifies a collection of functions, called \"regular functions\". The topological space and the structure sheaf together are required to satisfy conditions that mean the functions come from algebraic operations.\n\nLike manifolds, schemes are defined as spaces that are locally modeled on a familiar space. In the case of manifolds, the familiar space is Euclidean space. For a scheme, the local models are called affine schemes. Affine schemes provide a direct link between algebraic geometry and commutative algebra. The fundamental objects of study in commutative algebra are commutative rings. If formula_8 is a commutative ring, then there is a corresponding affine scheme formula_9 which translates the algebraic structure of formula_8 into geometry. Conversely, every affine scheme determines a commutative ring, namely, the ring of global sections of its structure sheaf. These two operations are mutually inverse, so affine schemes provide a new language with which to study questions in commutative algebra. By definition, every point in a scheme has an open neighborhood which is an affine scheme.\n\nThere are many schemes that are not affine. In particular, projective spaces satisfy a condition called properness which is analogous to compactness. Affine schemes cannot be proper (except in trivial situations like when the scheme has only a single point), and hence no projective space is an affine scheme (except for zero-dimensional projective spaces). Projective schemes, meaning those that arise as closed subschemes of a projective space, are the single most important family of schemes.\n\nSeveral generalizations of schemes have been introduced. Michael Artin defined an algebraic space as the quotient of a scheme by the equivalence relations that define étale morphisms. Algebraic spaces retain many of the useful properties of schemes while simultaneously being more flexible. For instance, the Keel–Mori theorem can be used to show that many moduli spaces are algebraic spaces.\n\nMore general than an algebraic space is a Deligne–Mumford stack. DM stacks are similar to schemes, but they permit singularities that cannot be described solely in terms of polynomials. They play the same role for schemes that orbifolds do for manifolds. For example, the quotient of the affine plane by a finite group of rotations around the origin yields a Deligne–Mumford stack that is not a scheme or an algebraic space. Away from the origin, the quotient by the group action identifies finite sets of equally spaced points on a circle. But at the origin, the circle consists of only a single point, the origin itself, and the group action fixes this point. In the quotient DM stack, however, this point comes with the extra data of being a quotient. This kind of refined structure is useful in the theory of moduli spaces, and in fact, it was originally introduced to describe moduli of algebraic curves.\n\nA further generalization are the algebraic stacks, also called Artin stacks. DM stacks are limited to quotients by finite group actions. While this suffices for many problems in moduli theory, it is too restrictive for others, and Artin stacks permit more general quotients.\n\nIn Grothendieck's work on the Weil conjectures, he introduced a new type of topology now called a Grothendieck topology. A topological space (in the ordinary sense) axiomatizes the notion of \"nearness,\" making two points be nearby if and only if they lie in many of the same open sets. By contrast, a Grothendieck topology axiomatizes the notion of \"covering\". A covering of a space is a collection of subspaces that jointly contain all the information of the ambient space. Since sheaves are defined in terms of coverings, a Grothendieck topology can also be seen as an axiomatization of the theory of sheaves.\n\nGrothendieck's work on his topologies led him to the theory of topoi. In his memoir \"Récoltes et Semailles\", he called them his \"most vast conception\". A sheaf (either on a topological space or with respect to a Grothendieck topology) is used to express local data. The category of all sheaves carries all possible ways of expressing local data. Since topological spaces are constructed from points, which are themselves a kind of local data, the category of sheaves can therefore be used as a replacement for the original space. Grothendieck consequently defined a topos to be a category of sheaves and studied topoi as objects of interest in their own right. These are now called Grothendieck topoi.\n\nEvery topological space determines a topos, and vice versa. There are topological spaces where taking the associated topos loses information, but these are generally considered pathological. (A necessary and sufficient condition is that the topological space be a sober space.) Conversely, there are topoi whose associated topological spaces do not capture the original topos. But, far from being pathological, these topoi can be of great mathematical interest. For instance, Grothendieck's theory of étale cohomology (which eventually led to the proof of the Weil conjectures) can be phrased as cohomology in the étale topos of a scheme, and this topos does not come from a topological space.\n\nTopological spaces in fact lead to very special topoi called locales. The set of open subsets of a topological space determines a lattice. The axioms for a topological space cause these lattices to be complete Heyting algebras. The theory of locales takes this as its starting point. A locale is defined to be a complete Heyting algebra, and the elementary properties of topological spaces are re-expressed and reproved in these terms. The concept of a locale turns out to be more general than a topological space, in that every sober topological space determines a unique locale, but many interesting locales do not come from topological spaces. Because locales need not have points, the study of locales is somewhat jokingly called pointless topology.\n\nTopoi also display deep connections to mathematical logic. Every Grothendieck topos has a special sheaf called a subobject classifier. This subobject classifier functions like the set of all possible truth values. In the topos of sets, the subobject classifier is the set formula_11, corresponding to \"False\" and \"True\". But in other topoi, the subobject classifier can be much more complicated. Lawvere and Tierney recognized that axiomatizing the subobject classifier yielded a more general kind of topos, now known as an elementary topos, and that elementary topoi were models of intuitionistic logic. In addition to providing a powerful way to apply tools from logic to geometry, this made possible the use of geometric methods in logic.\n\nAccording to Kevin Carlson,\n\nNevertheless, a general definition of \"structure\" was proposed by Bourbaki; it embraces all types of spaces mentioned above, (nearly?) all types of mathematical structures used till now, and more. It provides a general definition of isomorphism, and justifies transfer of properties between isomorphic structures. However, it was never used actively in mathematical practice (not even in the mathematical treatises written by Bourbaki himself). Here are the last phrases from a review by Robert Reed of a book by Leo Corry:\n\nFor more information on mathematical structures see Wikipedia: mathematical structure, equivalent definitions of mathematical structures, and transport of structure.\n\nThe distinction between geometric \"spaces\" and algebraic \"structures\" is sometimes clear, sometimes elusive. Clearly, groups are algebraic, while Euclidean spaces are geometric. Modules over rings are as algebraic as groups. In particular, when the ring appears to be a field, the module appears to be a linear space; is it algebraic or geometric? In particular, when it is finite-dimensional, over real numbers, and endowed with inner product, it becomes Euclidean space; now geometric. The (algebraic?) field of real numbers is the same as the (geometric?) real line. Its algebraic closure, the (algebraic?) field of complex numbers, is the same as the (geometric?) complex plane. It is first of all \"a place we do analysis\" (rather than algebra or geometry).\n\nEvery space treated in Section \"Types of spaces\" above, except for \"Non-commutative geometry\", \"Schemes\" and \"Topoi\" subsections, is a set (the \"principal base set\" of the structure, according to Bourbaki) endowed with some additional structure; elements of the base set are usually called \"points\" of this space. In contrast, elements of (the base set of) an algebraic structure usually are not called \"points\".\n\nHowever, sometimes one uses more than one principal base set. For example, two-dimensional projective geometry may be formalized via two base sets, the set of points and the set of lines. Moreover, a striking feature of projective planes is the symmetry of the roles played by points and lines. A less geometric example: a graph may be formalized via two base sets, the set of vertices (called also nodes or points) and the set of edges (called also arcs or lines). Generally, finitely many principal base sets and finitely many auxiliary base sets are stipulated by Bourbaki.\n\nMany mathematical structures of geometric flavor treated in the \"Non-commutative geometry\", \"Schemes\" and \"Topoi\" subsections above do not stipulate a base set of points. For example, \"pointless topology\" (in other words, point-free topology, or locale theory) starts with a single base set whose elements imitate open sets in a topological space (but are not sets of points); see also mereotopology and point-free geometry.\n\n\n"}
{"id": "26467605", "url": "https://en.wikipedia.org/wiki?curid=26467605", "title": "Stefan Burr", "text": "Stefan Burr\n\nStefan Andrus Burr (born 1940) is a mathematician and computer scientist. He is a retired professor of Computer Science at The City College of New York.\n\nBurr received his Ph.D. in 1969 from Princeton University under the supervision of Bernard Dwork; his thesis research involved the Waring–Goldbach problem in number theory, which concerns the representations of integers as sums of powers of prime numbers.\n\nMany of his subsequent publications involve problems from the field of Ramsey theory. He has published 27 papers with Paul Erdős. The Erdős–Burr conjecture, published as a conjecture by Erdős and Burr in 1975, solved only in 2015, states that sparse graphs have linearly growing Ramsey numbers.\n\n"}
{"id": "47121936", "url": "https://en.wikipedia.org/wiki?curid=47121936", "title": "The Bridges Organization", "text": "The Bridges Organization\n\nThe Bridges Organization is an organization that was founded in Kansas, United States, in 1998 with the goal of promoting interdisciplinary work in mathematics and art. The Bridges Conference is an annual conference on connections between art and mathematics. The conference features papers, educational workshops, an art exhibition, a mathematical poetry reading, and a short movie festival.\n"}
{"id": "2655832", "url": "https://en.wikipedia.org/wiki?curid=2655832", "title": "Transposition (logic)", "text": "Transposition (logic)\n\nIn propositional logic, transposition is a valid rule of replacement that permits one to switch the antecedent with the consequent of a conditional statement in a logical proof if they are also both negated. It is the inference from the truth of \"\"A\" implies \"B\"\" the truth of \"Not-\"B\" implies not-\"A\"\", and conversely. It is very closely related to the rule of inference modus tollens. It is the rule that:\n\nformula_1\n\nWhere \"formula_2\" is a metalogical symbol representing \"can be replaced in a proof with.\"\n\nThe \"transposition\" rule may be expressed as a sequent:\n\nwhere formula_4 is a metalogical symbol meaning that formula_5 is a syntactic consequence of formula_6 in some logical system;\n\nor as a rule of inference:\nwhere the rule is that wherever an instance of \"formula_8\" appears on a line of a proof, it can be replaced with \"formula_9\";\n\nor as the statement of a truth-functional tautology or theorem of propositional logic. The principle was stated as a theorem of propositional logic by Russell and Whitehead in \"Principia Mathematica\" as:\n\nwhere formula_11 and formula_12 are propositions expressed in some formal system.\n\nIn the inferred proposition, the consequent is the contradictory of the antecedent in the original proposition, and the antecedent of the inferred proposition is the contradictory of the consequent of the original proposition. The symbol for material implication signifies the proposition as a hypothetical, or the \"if-then\" form, e.g. \"if P then Q\".\n\nThe biconditional statement of the rule of transposition (↔) refers to the relation between hypothetical (→) \"propositions\", with each proposition including an antecent and consequential term. As a matter of logical inference, to transpose or convert the terms of one proposition requires the conversion of the terms of the propositions on both sides of the biconditional relationship. Meaning, to transpose or convert (P → Q) to (Q → P) requires that the other proposition, (~Q → ~P), be transposed or converted to (~P → ~Q). Otherwise, to convert the terms of one proposition and not the other renders the rule invalid, violating the sufficient condition and necessary condition of the terms of the propositions, where the violation is that the changed proposition commits the fallacy of denying the antecedent or affirming the consequent by means of illicit conversion.\n\nThe truth of the rule of transposition is dependent upon the relations of sufficient condition and necessary condition in logic.\n\nIn the proposition \"If P then Q\", the occurrence of 'P' is sufficient reason for the occurrence of 'Q'. 'P', as an individual or a class, materially implicates 'Q', but the relation of 'Q' to 'P' is such that the converse proposition \"If Q then P\" does not necessarily have sufficient condition. The rule of inference for sufficient condition is \"modus ponens\", which is an argument for conditional implication:\n\nPremise (1): If P, then Q\n\nPremise (2): P\n\nConclusion: Therefore, Q\n\nSince the converse of premise (1) is not valid, all that can be stated of the relationship of 'P' and 'Q' is that in the absence of 'Q', 'P' does not occur, meaning that 'Q' is the necessary condition for 'P'. The rule of inference for necessary condition is \"modus tollens\":\n\nPremise (1): If P, then Q\n\nPremise (2): not Q\n\nConclusion: Therefore, not P\n\nAn example traditionally used by logicians contrasting sufficient and necessary conditions is the statement \"If there is fire, then oxygen is present\". An oxygenated environment is necessary for fire or combustion, but simply because there is an oxygenated environment does not necessarily mean that fire or combustion is occurring. While one can infer that fire stipulates the presence of oxygen, from the presence of oxygen the converse \"If there is oxygen present, then fire is present\" cannot be inferred. All that can be inferred from the original proposition is that \"If oxygen is not present, then there cannot be fire\".\n\nThe symbol for the biconditional (\"↔\") signifies the relationship between the propositions is both necessary and sufficient, and is verbalized as \"if and only if\", or, according to the example \"If P then Q 'if and only if' if not Q then not P\".\n\nNecessary and sufficient conditions can be explained by analogy in terms of the concepts and the rules of immediate inference of traditional logic. In the categorical proposition \"All S is P\", the subject term 'S' is said to be distributed, that is, all members of its class are exhausted in its expression. Conversely, the predicate term 'P' cannot be said to be distributed, or exhausted in its expression because it is indeterminate whether every instance of a member of 'P' as a class is also a member of 'S' as a class. All that can be validly inferred is that \"Some P are S\". Thus, the type 'A' proposition \"All P is S\" cannot be inferred by conversion from the original 'A' type proposition \"All S is P\". All that can be inferred is the type \"A\" proposition \"All non-P is non-S\" (Note that (P → Q) and (~Q → ~P) are both 'A' type propositions). Grammatically, one cannot infer \"all mortals are men\" from \"All men are mortal\". An 'A' type proposition can only be immediately inferred by conversion when both the subject and predicate are distributed, as in the inference \"All bachelors are unmarried men\" from \"All unmarried men are bachelors\".\n\nIn traditional logic the reasoning process of transposition as a rule of inference is applied to categorical propositions through contraposition and obversion, a series of immediate inferences where the rule of obversion is first applied to the original categorical proposition \"All S is P\"; yielding the obverse \"No S is non-P\". In the obversion of the original proposition to an 'E' type proposition, both terms become distributed. The obverse is then converted, resulting in \"No non-P is S\", maintaining distribution of both terms. The No non-P is S\" is again obverted, resulting in the [contrapositive] \"All non-P is non-S\". Since nothing is said in the definition of contraposition with regard to the predicate of the inferred proposition, it is permissible that it could be the original subject or its contradictory, and the predicate term of the resulting 'A' type proposition is again undistributed. This results in two contrapositives, one where the predicate term is distributed, and another where the predicate term is undistributed.\n\nNote that the method of transposition and contraposition should not be confused. Contraposition is a type of immediate inference in which from a given categorical proposition another categorical proposition is inferred which has as its subject the contradictory of the original predicate. Since nothing is said in the definition of contraposition with regard to the predicate of the inferred proposition, it is permissible that it could be the original subject or its contradictory. This is in contradistinction to the form of the propositions of transposition, which may be material implication, or a hypothetical statement. The difference is that in its application to categorical propositions the result of contraposition is two contrapositives, each being the obvert of the other, i.e. \"No non-P is S\" and \"All non-P is non-S\". The distinction between the two contrapositives is absorbed and eliminated in the principle of transposition, which presupposes the \"mediate inferences\" of contraposition and is also referred to as the \"law of contraposition\".\n\nSee Transposition (mathematics), Set theory\n\n\n\n"}
{"id": "276894", "url": "https://en.wikipedia.org/wiki?curid=276894", "title": "Uniqueness quantification", "text": "Uniqueness quantification\n\nIn mathematics and logic, the phrase \"there is one and only one\" is used to indicate that exactly one object with a certain property exists. In mathematical logic, this sort of quantification is known as uniqueness quantification or unique existential quantification.\n\nUniqueness quantification is often denoted with the symbols \"∃!\" or \"∃\". For example, the formal statement \nmay be read aloud as \"there is exactly one natural number \"n\" such that \"n\" − 2 = 4\".\n\nThe most common technique to proving unique existence is to first prove existence of entity with the desired condition; then, to assume there exist two entities (say, \"a\" and \"b\") that both satisfy the condition, and logically deduce their equality, i.e. \"a\" = \"b\".\n\nAs a simple high school example, to show \"x\" + 2 = 5 has exactly one solution, we first show by demonstration that at least one solution exists, namely 3; the proof of this part is simply the calculation\n\nWe now assume that there are two solutions, namely \"a\" and \"b\", satisfying \"x\" + 2 = 5. Thus\n\nBy transitivity of equality,\n\nBy cancellation,\n\nThis simple example shows how a proof of uniqueness is done, the end result being the equality of the two quantities that satisfy the condition.\n\nBoth existence and uniqueness must be proven, in order to conclude that there exists exactly one solution.\n\nAn alternative way to prove uniqueness is to prove there exists a value formula_6 satisfying the condition, and then proving that, for all formula_7, the condition for formula_7 implies formula_9.\n\nUniqueness quantification can be expressed in terms of the existential and universal quantifiers of predicate logic by defining the formula\n∃!\"x\" \"P(x)\" to mean literally,\nwhich is the same as\nAn equivalent definition that has the virtue of separating the notions of existence and uniqueness into two clauses, at the expense of brevity, is\nAnother equivalent definition with the advantage of brevity is\n\nOne generalization of uniqueness quantification is counting quantification. This includes both quantification of the form \"exactly \"k\" objects exist such that …\" as well as \"infinitely many objects exist such that …\" and \"only finitely many objects exist such that…\". The first of these forms is expressible using ordinary quantifiers, but the latter two cannot be expressed in ordinary first-order logic.\n\nUniqueness depends on a notion of equality. Loosening this to some coarser equivalence relation yields quantification of uniqueness up to that equivalence (under this framework, regular uniqueness is \"uniqueness up to equality\"). For example, many concepts in category theory are defined to be unique up to isomorphism.\n\n\n"}
{"id": "38333812", "url": "https://en.wikipedia.org/wiki?curid=38333812", "title": "Éléments de mathématique", "text": "Éléments de mathématique\n\nÉléments de mathématique is a treatise on mathematics by the collective Nicolas Bourbaki, composed of twelve books (each divided into one or more chapters). The first volumes were published by Éditions Hermann from 1939 initially in the form of booklets and then as bound volumes. Following a disagreement with the editor, the publication was resumed in the 1970s by the CCLS, and then in the 1980s by Éditions Masson. Since 2006, Springer Verlag has republished all the fascicles, and has published a new volume in 2016.\n\nThe strange singular \"mathématique\" in the title is deliberate, to convey the authors' belief that the material is a unity, contrary to what conventional form of the title might suggest. Conversely, the title \"Éléments d'histoire des mathématiques\" by the same authors employs the plural to indicate that before Bourbaki, mathematics was a set of scattered disciplines, and that the modern notion of structure has allowed their unification.\n\nThe first six volumes follow a logical sequence. The subsequent volumes are dependent on the first six, but not on each other.\n\nThe series is divided into books and each book into chapters. Below is the list of books in the series. \n1. Set theory\n\n\n2. Algebra\n\n\n3. Topology\n\n\n4. Functions of a Real Variable\n\n\n5. Topological vector spaces\n\n\n6. Integration\n\n\n7. Commutative algebra\n\n\n8. Differential manifolds (summary of results)\n\n9. Lie groups and algebras\n\n\n10. Spectral theory\n\n11. History of mathematics\n\n\n12. Algebraic Topology\n\nIn the first six books, every statement in the text assumes as known only those results which have already been discussed in the same chapter, or in the previous chapters ordered as follows:\n\n\nLater books assume knowledge of the first six books and their relationship to the other books in the series will be indicated at the outset.\n\nThe first volume, published in 1939, was the \"Fascicule de résultats\" of \"Théorie des ensembles\". The publication of subsequent volumes did not follow the order of the Treatise. Publication continues intermittently - the tenth chapter of \"Algèbre commutative\" was published in 1998, an expanded second edition of the eighth chapter of \"Algèbre\" in 2012, and the first four chapters of a new book \"Topologie algébrique\" in 2016. This latest book was initially planned as the eleventh chapter of \"Topologie générale\". The \"Éléments de mathématique\" remains unfinished to this day.\n\nEarly versions are available online. Most of the books published were out of print for years. The publisher Springer started their republication in 2006.\n"}
{"id": "26004695", "url": "https://en.wikipedia.org/wiki?curid=26004695", "title": "Π01 class", "text": "Π01 class\n\nIn computability theory, a Π class is a subset of 2 of a certain form. These classes are of interest as technical tools within recursion theory and effective descriptive set theory. They are also used in the application of recursion theory to other branches of mathematics (Cenzer 1999, p. 39).\n\nThe set 2 consists of all finite sequences of 0s and 1s, while the set 2 consists of all infinite sequences of 0s and 1s (that is, functions from } to the set }).\n\nA tree on 2 is a subset of 2 that is closed under taking initial segments. An element \"f\" of 2 is a path through a tree \"T\" on 2 if every finite initial segment of \"f\" is in \"T\".\n\nA (lightface) Π class is a subset \"C\" of 2 for which there is a computable tree \"T\" such that \"C\" consists of exactly the paths through \"T\". A boldface Π class is a subset \"D\" of 2 for which there is an oracle \"f\" in 2 and a subtree tree \"T\" of 2 from computable from \"f\" such that \"D\" is the set of paths through \"T\".\n\nThe boldface Π classes are exactly the same as the closed sets of 2 and thus the same as the boldface Π subsets of 2 in the Borel hierarchy.\n\nLightface Π classes in 2 (that is, Π classes whose tree is computable with no oracle) correspond to effectively closed sets. A subset \"B\" of 2 is effectively closed if there is a recursively enumerable sequence 〈σ : i ∈ ω〉 of elements of 2 such that each \"g\" ∈ 2 is in \"B\" if and only if σ is an initial segment of \"B\".\n\nFor each effectively axiomatized theory \"T\" of first-order logic, the set of all completions of \"T\" is a formula_1 class. Moreover, for each formula_1 subset \"S\" of formula_3 there is an effectively axiomatized theory \"T\" such that each element of \"S\" computes a completion of \"T\", and each completion of \"T\" computes an element of \"S\" (Jockusch and Soare 1972b).\n\n\n"}
