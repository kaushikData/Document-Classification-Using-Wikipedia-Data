{"id": "19721499", "url": "https://en.wikipedia.org/wiki?curid=19721499", "title": "Berwick Prize", "text": "Berwick Prize\n\nThe Berwick Prize and Senior Berwick Prize are two prizes of the London Mathematical Society awarded in alternating years in memory of William Edward Hodgson Berwick, a previous Vice-President of the LMS. Berwick left some money to be given to the society to establish two prizes. His widow Daisy May Berwick gave the society the money and the society established the prizes, with the first Senior Berwick Prize being presented in 1946 and the first Junior Berwick Prize the following year. The prizes are awarded \"in recognition of an outstanding piece of mathematical research ... published by the Society\" in the eight years before the year of the award.\n\nThe Berwick Prize was known as the Junior Berwick Prize up to 1999, and was given its current name for the 2001 award.\n\nSource:\n\nSource: \n\n"}
{"id": "12461348", "url": "https://en.wikipedia.org/wiki?curid=12461348", "title": "Blom's scheme", "text": "Blom's scheme\n\nBlom's scheme is a symmetric threshold key exchange protocol in cryptography. The scheme was proposed by the Swedish cryptographer Rolf Blom in a series of articles in the early 1980s.\n\nA trusted party gives each participant a secret key and a public identifier, which enables any two participants to independently create a shared key for communicating. However, if an attacker can compromise the keys of at least k users, they can break the scheme and reconstruct every shared key. Blom's scheme is a form of threshold secret sharing.\n\nBlom's scheme is currently used by the HDCP (Version 1.x only) copy protection scheme to generate shared keys for high-definition content sources and receivers, such as HD DVD players and high-definition televisions. \n\nThe key exchange protocol involves a trusted party (Trent) and a group of formula_1 users. Let Alice and Bob be two users of the group.\n\nTrent chooses a random and secret symmetric matrix formula_2 over the finite field formula_3, where p is a prime number. formula_4 is required when a new user is to be added to the key sharing group.\n\nFor example:\n\nformula_5\n\nNew users Alice and Bob want to join the key exchanging group. Trent chooses public identifiers for each of them; i.e., k-element vectors:\n\nformula_6.\n\nFor example:\n\nformula_7\n\nTrent then computes their private keys:\n\nformula_8\n\nUsing formula_9 as described above:\n\nformula_10\n\nEach will use their private key to compute shared keys with other participants of the group.\n\nNow Alice and Bob wish to communicate with one another. Alice has Bob's identifier formula_11 and her private key formula_12.\n\nShe computes the shared key formula_13, where formula_14 denotes matrix transpose. Bob does the same, using his private key and her identifier, giving the same result:\n\nformula_15\n\nThey will each generate their shared key as follows:\n\nformula_16\n\nIn order to ensure at least k keys must be compromised before every shared key can be computed by an attacker, identifiers must be k-linearly independent: all sets of k randomly selected user identifiers must be linearly independent. Otherwise, a group of malicious users can compute the key of any other member whose identifier is linearly dependent to theirs. To ensure this property, the identifiers shall be preferably chosen from a MDS-Code matrix (maximum distance separable error correction code matrix). The rows of the MDS-Matrix would be the identifiers of the users. A MDS-Code matrix can be chosen in practice using the code-matrix of the Reed–Solomon error correction code (this error correction code requires only easily understandable mathematics and can be computed extremely quickly).\n"}
{"id": "782099", "url": "https://en.wikipedia.org/wiki?curid=782099", "title": "Bott periodicity theorem", "text": "Bott periodicity theorem\n\nIn mathematics, the Bott periodicity theorem describes a periodicity in the homotopy groups of classical groups, discovered by , which proved to be of foundational significance for much further research, in particular in K-theory of stable complex vector bundles, as well as the stable homotopy groups of spheres. Bott periodicity can be formulated in numerous ways, with the periodicity in question always appearing as a period-2 phenomenon, with respect to dimension, for the theory associated to the unitary group. See for example topological K-theory.\n\nThere are corresponding period-8 phenomena for the matching theories, (real) KO-theory and (quaternionic) KSp-theory, associated to the real orthogonal group and the quaternionic symplectic group, respectively. The J-homomorphism is a homomorphism from the homotopy groups of orthogonal groups to stable homotopy groups of spheres, which causes the period 8 Bott periodicity to be visible in the stable homotopy groups of spheres.\n\nBott showed that if formula_1 is defined as the inductive limit of the orthogonal groups, then its homotopy groups are periodic:\nand the first 8 homotopy groups are as follows:\n\nThe context of Bott periodicity is that the homotopy groups of spheres, which would be expected to play the basic part in algebraic topology by analogy with homology theory, have proved elusive (and the theory is complicated). The subject of stable homotopy theory was conceived as a simplification, by introducing the suspension (smash product with a circle) operation, and seeing what (roughly speaking) remained of homotopy theory once one was allowed to suspend both sides of an equation, as many times as one wished. The stable theory was still hard to compute with, in practice.\n\nWhat Bott periodicity offered was an insight into some highly non-trivial spaces, with central status in topology because of the connection of their cohomology with characteristic classes, for which all the (\"unstable\") homotopy groups could be calculated. These spaces are the (infinite, or \"stable\") unitary, orthogonal and symplectic groups \"U\", \"O\" and Sp. In this context, \"stable\" refers to taking the union \"U\" (also known as the direct limit) of the sequence of inclusions\n\nand similarly for \"O\" and Sp. Bott's (now somewhat awkward) use of the word \"stable\" in the title of his seminal paper refers to these stable classical groups and not to stable homotopy groups.\n\nThe important connection of Bott periodicity with the stable homotopy groups of spheres formula_5 comes via the so-called stable \"J\"-homomorphism from the (unstable) homotopy groups of the (stable) classical groups to these stable homotopy groups formula_5. Originally described by George W. Whitehead, it became the subject of the famous Adams conjecture (1963) which was finally resolved in the affirmative by Daniel Quillen (1971).\n\nBott's original results may be succinctly summarized in:\n\nCorollary: The (unstable) homotopy groups of the (infinite) classical groups are periodic:\n\nNote: The second and third of these isomorphisms intertwine to give the 8-fold periodicity results:\n\nFor the theory associated to the infinite unitary group, \"U\", the space \"BU\" is the classifying space for stable complex vector bundles (a Grassmannian in infinite dimensions). One formulation of Bott periodicity describes the twofold loop space, Ω\"BU\" of \"BU\". Here, Ω is the loop space functor, right adjoint to suspension and left adjoint to the classifying space construction. Bott periodicity states that this double loop space is essentially \"BU\" again; more precisely,\n\nis essentially (that is, homotopy equivalent to) the union of a countable number of copies of \"BU\". An equivalent formulation is\n\nEither of these has the immediate effect of showing why (complex) topological \"K\"-theory is a 2-fold periodic theory.\n\nIn the corresponding theory for the infinite orthogonal group, \"O\", the space \"BO\" is the classifying space for stable real vector bundles. In this case, Bott periodicity states that, for the 8-fold loop space,\n\nor equivalently,\n\nwhich yields the consequence that \"KO\"-theory is an 8-fold periodic theory. Also, for the infinite symplectic group, Sp, the space BSp is the classifying space for stable quaternionic vector bundles, and Bott periodicity states that\n\nor equivalently\n\nThus both topological real \"K\"-theory (also known as \"KO\"-theory) and topological quaternionic \"K\"-theory (also known as KSp-theory) are 8-fold periodic theories.\n\nOne elegant formulation of Bott periodicity makes use of the observation that there are natural embeddings (as closed subgroups) between the classical groups. The loop spaces in Bott periodicity are then homotopy equivalent to the symmetric spaces of successive quotients, with additional discrete factors of Z.\n\nOver the complex numbers:\n\nOver the real numbers and quaternions:\n\nThese sequences corresponds to sequences in Clifford algebras – see classification of Clifford algebras; over the complex numbers:\n\nOver the real numbers and quaternions:\n\nwhere the division algebras indicate \"matrices over that algebra\".\n\nAs they are 2-periodic/8-periodic, they can be arranged in a circle, where they are called the Bott periodicity clock and Clifford algebra clock.\n\nThe Bott periodicity results then refine to a sequence of homotopy equivalences:\n\nFor complex \"K\"-theory:\n\nFor real and quaternionic \"KO\"- and KSp-theories:\n\nThe resulting spaces are homotopy equivalent to the classical reductive symmetric spaces, and are the successive quotients of the terms of the Bott periodicity clock. These equivalences immediately yield the Bott periodicity theorems.\n\nThe specific spaces are, (for groups, the principal homogeneous space is also listed):\nBott's original proof used Morse theory, which had used earlier to study the homology of Lie groups. Many different proofs have been given.\n\n"}
{"id": "53002210", "url": "https://en.wikipedia.org/wiki?curid=53002210", "title": "Bouncing ball", "text": "Bouncing ball\n\nThe physics of a bouncing ball concerns the physical behaviour of bouncing balls, particularly its motion before, during, and after impact against the surface of another body. Several aspects of a bouncing ball's behaviour serve as an introduction to mechanics in high school or undergraduate level physics courses. However, the exact modelling of the behaviour is complex and of interest in sports engineering.\n\nThe motion of a ball is generally described by projectile motion (which can be affected by gravity, drag, the Magnus effect, and buoyancy), while its impact is usually characterized through the coefficient of restitution (which can be affected by the nature of the ball, the nature of the impacting surface, the impact velocity, rotation, and local conditions such as temperature and pressure). To ensure fair play, many sports governing bodies set limits on the bounciness of their ball and forbid tampering with the ball's aerodynamic properties. The bounciness of balls has been a feature of sports as ancient as the Mesoamerican ballgame.\n\nThe motion of a bouncing ball obeys projectile motion. Many forces act on a real ball, namely the gravitational force (F), the drag force due to air resistance (F), the Magnus force due to the ball's spin (F), and the buoyant force (F). In general, one has to use Newton's second law taking all forces into account to analyze the ball's motion:\nwhere \"m\" is the ball's mass. Here, a, v, r represent the ball's acceleration, velocity, and position over time \"t\".\n\nThe gravitational force is directed downwards and is equal to\nwhere \"m\" is the mass of the ball, and \"g\" is the gravitational acceleration, which on Earth varies between and . Because the other forces are usually small, the motion is often idealized as being only under the influence of gravity. If only the force of gravity acts on the ball, the mechanical energy will be conserved during its flight. In this idealized case, the equations of motion are given by\nwhere a, v, and r denote the acceleration, velocity, and position of the ball, and v and r are the initial velocity and position of the ball, respectively.\n\nMore specifically, if the ball is bounced at an angle \"θ\" with the ground, the motion in the \"x\"- and \"y\"-axes (representing \"horizontal\" and \"vertical\" motion, respectively) is described by\nThe equations imply that the maximum height (\"H\") and range (\"R\") and time of flight (\"T\") of a ball bouncing on a flat surface are given by\n\nFurther refinements to the motion of the ball can be made by taking into account air resistance (and related effects such as drag and wind), the Magnus effect, and buoyancy. Because lighter balls accelerate more readily, their motion tends to be affected more by such forces.\n\nAir flow around the ball can be either laminar or turbulent depending on the Reynolds number (Re), defined as:\nwhere \"ρ\" is the density of air, \"μ\" the dynamic viscosity of air, \"D\" the diameter of the ball, and \"v\" the velocity of the ball through air. At a temperature of , and .\n\nIf the Reynolds number is very low (Re < 1), the drag force on the ball is described by Stokes' law:\nwhere \"r\" is the radius of the ball. This force acts in opposition to the ball's direction (in the direction of formula_7). For most sports balls, however, the Reynolds number will be between 10 and 10 and Stokes' law does not apply. At these higher values of the Reynolds number, the drag force on the ball is instead described by the drag equation:\nwhere \"C\" is the drag coefficient, and \"A\" the cross-sectional area of the ball.\n\nDrag will cause the ball to lose mechanical energy during its flight, and will reduce the range and the height of a ball, while crosswinds will deflect it from its original path. Both effects have to be taken into account by players in sports such as golf.\n\nThe spin of the ball will affect its trajectory through the Magnus effect. According to the Kutta–Joukowski theorem for a spinning sphere with an inviscid flow of air, the Magnus force is equal to\nwhere \"r\" is the radius of the ball, \"ω\" the angular velocity (or spin rate) of the ball, \"ρ\" the density of air, and \"v\" the velocity of the ball relative to air. This force is directed perpendicular to the motion and perpendicular to the axis of rotation (in the direction of formula_10). The force is directed upwards for backspin and downwards for topspin. In reality, flow is never inviscid, and the Magnus lift is better described by\nwhere \"ρ\" is the density of air, \"C\" the lift coefficient, \"A\" the cross-sectional area of the ball, and \"v\" the velocity of the ball relative to air. The lift coefficient is a complex factor which depends amongst other things on the ratio \"rω\"/\"v\", the Reynolds number, and surface roughness. In certain conditions, the lift coefficient can even be negative, changing the direction of the Magnus force (reverse Magnus effect).\n\nIn sports like tennis or volleyball, the player can use the Magnus effect to control the ball's trajectory (e.g. via topspin or backspin) during flight. In golf, the effect is responsible for slicing and hooking which are usually a detriment to the golfer, but also helps with increasing the range of a drive and other shots. In baseball, pitchers use the effect to create curveballs and other special pitches.\n\nBall tampering is often illegal, and is often at the centre of cricket controversies such as the one between England and Pakistan in August 2006. In baseball, the term 'spitball' refers to the illegal coating of the ball with spit or other substances to alter the aerodynamics of the ball.\n\nAny object immersed in a fluid such as water or air will experience an upwards buoyancy. According to Archimedes' principle, this buoyant force is equal to the weight of the fluid displaced by the object. In the case of a sphere, this force is equal to\n\nThe buoyant force is usually small compared to the drag and Magnus forces and can often be neglected. However, in the case of a basketball, the buoyant force can amount to about 1.5% of the ball's weight. Since buoyancy is directed upwards, it will act to increase the range and height of the ball.\n\nWhen a ball impacts a surface, the surface recoils and vibrates, as does the ball, creating both sound and heat, and the ball loses kinetic energy. Additionally, the impact can impart some rotation to the ball, transferring some of its translational kinetic energy into rotational kinetic energy. This energy loss is usually characterized (indirectly) through the coefficient of restitution (or COR, denoted \"e\"):\nwhere \"v\" and \"v\" are the final and initial velocities of the ball, and \"u\" and \"u\" are the final and initial velocities impacting surface, respectively. In the specific case where a ball impacts on an immovable surface, the COR simplifies to\n\nFor a ball dropped against a floor, the COR will therefore vary between 0 (no bounce, total loss of energy) and 1 (perfectly bouncy, no energy loss). A COR value below 0 or above 1 is theoretically possible, but would indicate that the ball went \"through\" the surface (), or that the surface was not \"relaxed\" when the ball impacted it (), like in the case of a ball landing on spring-loaded platform.\n\nTo analyze the vertical and horizontal components of the motion, the COR is sometimes split up into a \"normal\" COR (\"e\"), and \"tangential\" COR (\"e\"), defined as\nwhere \"r\" and \"ω\" denote the radius and angular velocity of the ball, while \"R\" and \"Ω\" denote the radius and angular velocity the impacting surface (such as a baseball bat). In particular \"rω\" is the tangential velocity of the ball's surface, while \"RΩ\" is the tangential velocity of the impacting surface. These are especially of interest when the ball impacts the surface at an oblique angle, or when rotation is involved.\n\nFor a straight drop on the ground with no rotation, with only the force of gravity acting on the ball, the COR can be related to several other quantities by:\nHere, \"K\" and \"U\" denote the kinetic and potential energy of the ball, \"H\" is the maximum height of the ball, and \"T\" is the time of flight of the ball. The 'i' and 'f' subscript refer to the initial (before impact) and final (after impact) states of the ball. Likewise, the energy loss at impact can be related to the COR by\n\nThe COR of a ball can be affected by several things, mainly\n\nExternal conditions such as temperature can change the properties of the impacting surface or of the ball, making them either more flexible or more rigid. This will, in turn, affect the COR. In general, the ball will deform more at higher impact velocities and will accordingly lose more of its energy, decreasing its COR.\n\nUpon impacting the ground, some translational kinetic energy can be converted to rotational kinetic energy and vice versa depending on the ball's impact angle and angular velocity. If the ball moves horizontally at impact, friction will have a 'translational' component in the direction opposite to the ball's motion. In the figure, the ball is moving to the \"right\", and thus it will have a translational component of friction pushing the ball to the \"left\". Additionally, if the ball is spinning at impact, friction will have a 'rotational' component in the direction opposite to the ball's rotation. On the figure, the ball is spinning clockwise, and the point impacting the ground is moving to the \"left\" with respect to the ball's center of mass. The rotational component of friction is therefore pushing the ball to the \"right\". Unlike the normal force and the force of gravity, these frictional forces will exert a torque on the ball, and change its angular velocity (\"ω\").\n\nThree situations can arise:\n\nIf the surface is inclined by some amount \"θ\", the entire diagram would be rotated by \"θ\", but the force of gravity would remain pointing downwards (forming an angle \"θ\" with the surface). Gravity would then have a component parallel to the surface, which would contribute to friction, and thus contribute to rotation.\n\nIn racquet sports such as table tennis or racquetball, skilled players will use spin (including sidespin) to suddenly alter the ball's direction when it impacts surface, such as the ground or their opponent's racquet.\n\nThe bounce of an oval-shaped ball (such as those used in gridiron football or rugby football) is in general much less predictable than the bounce of a spherical ball. Depending on the ball's alignment at impact, the normal force can act ahead or behind the centre of mass of the ball, and friction from the ground will depend on the alignment of the ball, as well as its rotation, spin, and impact velocity. Where the forces act with respect to the centre of mass of the ball changes as the ball rolls on the ground, and all forces can exert a torque on the ball, including the normal force and the force of gravity. This can cause the ball to bounce forward, bounce back, or sideways. Because it is possible to transfer some rotational kinetic energy into translational kinetic energy, it is even possible for the COR to be greater than 1, or for the forward velocity of the ball to increase upon impact.\n\nA popular demonstration involves the bounce of multiple stacked balls. If a tennis ball is stacked on top of a basketball, and the two of them are dropped at the same time, the tennis ball will bounce much higher than it would have if dropped on its own, even exceeding its original release height. The result is surprising as it apparently violates conservation of energy. However, upon closer inspection, the basketball does not bounce as high as it would have if the tennis ball had not been on top of it, and transferred some of its energy into the tennis ball, propelling it to a greater height.\n\nThe usual explanation involves considering two separate impacts: the basketball impacting with the floor, and then the basketball impacting with the tennis ball. Assuming perfectly elastic collisions, the basketball impacting the floor at 1 m/s would rebound at 1 m/s. The tennis ball going at 1 m/s would then have a relative impact velocity of 2 m/s, which means it would rebound at 2 m/s relative to the basketball, or 3 m/s relative to the floor, and \"triple\" its rebound velocity compared to impacting the floor on its own. This implies that the ball would bounce to \"9 times\" its original height.\nIn reality, due to inelastic collisions, the tennis ball will increase its velocity and rebound height by a smaller factor, but still will bounce faster and higher than it would have on its own.\n\nWhile the assumptions of separate impacts is not actually valid (the balls remain in close contact with each other during most of the impact), this model will nonetheless reproduce experimental results with good agreement, and is often used to understand more complex phenomena such as the core collapse of supernovae, or gravitational slingshot manoeuvres.\n\nSeveral sports governing bodies regulate the bounciness of a ball through various ways, some direct, some indirect.\n\nThe pressure of an American football was at the center of the deflategate controversy. Some sports do not regulate the bouncing properties of balls directly, but instead specify a construction method. In baseball, the introduction of a cork-based ball helped to end the dead-ball era and trigger the live-ball era.\n\n\n"}
{"id": "6333385", "url": "https://en.wikipedia.org/wiki?curid=6333385", "title": "C-theorem", "text": "C-theorem\n\nIn theoretical physics, specifically quantum field theory, \"C\"-theorem states that there exists a positive real function, formula_1, depending on the coupling constants of the quantum field theory considered, formula_2, and on the energy scale, formula_3, which has the following properties:\n\n\nThe theorem formalizes the notion that theories at high energies have more degrees of freedom than theories at low energies and that information is lost as we flow from the former to the latter.\n\nAlexander Zamolodchikov proved in 1986 that two-dimensional quantum field theory always has such a \"C\"-function. Moreover, at fixed points of the RG flow, which correspond to conformal field theories, Zamolodchikov's \"C\"-function is equal to the central charge of the corresponding conformal field theory, which lends the name \"C\" to the theorem.\n\nJohn Cardy in 1988 considered the possibility to generalise \"C\"-theorem to higher-dimensional quantum field theory. He conjectured that in four spacetime dimensions, the quantity behaving monotonically under renormalization group flows, and thus playing the role analogous to the central charge in two dimensions, is a certain anomaly coefficient which came to be denoted as . \nFor this reason, the analog of the \"C\"-theorem in four dimensions is called the \"A\"-theorem.\n\nIn perturbation theory, that is for renormalization flows which do not deviate much from free theories, the \"A\"-theorem in four dimensions was proved by Hugh Osborn using the local renormalization group equation. However, the problem of finding a proof valid beyond perturbation theory remained open for many years.\n\nIn 2011, Zohar Komargodski and Adam Schwimmer of the Weizmann Institute of Science proposed a nonperturbative proof for the \"A\"-theorem, which has gained acceptance. (Still, simultaneous monotonic and cyclic (limit cycle) or even chaotic RG flows are compatible with such flow functions when multivalued in the couplings, as evinced in specific systems.) RG flows of theories in 4 dimensions and the question of whether scale invariance implies conformal invariance, is a field of active research and not all questions are settled.\n\n"}
{"id": "1460126", "url": "https://en.wikipedia.org/wiki?curid=1460126", "title": "Chromatic polynomial", "text": "Chromatic polynomial\n\nThe chromatic polynomial is a graph polynomial studied in algebraic graph theory, a branch of mathematics. It counts the number of graph colorings as a function of the number of colors and was originally defined by George David Birkhoff to attack the four color problem. It was generalised to the Tutte polynomial by Hassler Whitney and W. T. Tutte, linking it to the Potts model of statistical physics.\n\nGeorge David Birkhoff introduced the chromatic polynomial in 1912, defining it only for planar graphs, in an attempt to prove the four color theorem. If formula_1 denotes the number of proper colorings of \"G\" with \"k\" colors then one could establish the four color theorem by showing formula_2 for all planar graphs \"G\". In this way he hoped to apply the powerful tools of analysis and algebra for studying the roots of polynomials to the combinatorial coloring problem.\n\nHassler Whitney generalised Birkhoff’s polynomial from the planar case to general graphs in 1932. In 1968, Read asked which polynomials are the chromatic polynomials of some graph, a question that remains open, and introduced the concept of chromatically equivalent graphs. Today, chromatic polynomials are one of the central objects of algebraic graph theory.\n\nFor a graph \"G\", formula_3 counts the number of its (proper) vertex \"k\"-colorings. \nOther commonly used notations include formula_4, formula_5, or formula_6.\nThere is a unique polynomial formula_7 which evaluated at any integer \"k ≥ 0\" coincides with formula_3; it is called the chromatic polynomial of \"G\".\n\nFor example, to color the path graph formula_9 on 3 vertices with \"k\" colors, one may choose any of the \"k\" colors for the first vertex, any of the formula_10 remaining colors for the second vertex, and lastly for the third vertex, any of the formula_10 colors that are different from the second vertex's choice.\nTherefore, formula_12 is the number of \"k\"-colorings of formula_9.\nFor a variable \"x\" (not necessarily integer), we thus have formula_14.\n\nThe fact that the number of \"k\"-colorings is a polynomial in \"k\" follows from a recurrence relation called the deletion–contraction recurrence or Fundamental Reduction Theorem. \nIt is based on edge contraction: for a pair of vertices formula_15 and formula_16 the graph formula_17 is obtained by merging the two vertices and removing any edges between them. \nIf formula_15 and formula_16 are adjacent in \"G\", let formula_20 denote the graph obtained by removing the edge formula_21.\nThen the numbers of \"k\"-colorings of these graphs satisfy:\nEquivalently, if formula_15 and formula_16 are not adjacent in \"G\" and formula_25 is the graph with the edge formula_21 added, then\nThis follows from the observation that every \"k\"-coloring of \"G\" either gives different colors to formula_15 and formula_16, or the same colors. In the first case this gives a (proper) \"k\"-coloring of formula_25, while in the second case it gives a coloring of formula_17.\nConversely, every \"k\"-coloring of \"G\" can be uniquely obtained from a \"k\"-coloring of formula_25 or formula_17 (if formula_15 and formula_16 are not adjacent in \"G\").\n\nThe chromatic polynomial can hence be recursively defined as \nSince the number of \"k\"-colorings of the edgeless graph is indeed formula_39, it follows by induction on the number of edges that for all \"G\", the polynomial formula_7 coincides with the number of \"k\"-colorings at every integer point \"x=k\".\nIn particular, the chromatic polynomial is the unique interpolating polynomial of degree at most \"n\" through the points\n\nTutte’s curiosity about which other graph invariants satisfied such recurrences led him to discover a bivariate generalization of the chromatic polynomial, the Tutte polynomial formula_42.\n\nFor fixed \"G\" on \"n\" vertices, the chromatic polynomial formula_43 is a monic polynomial of degree exactly \"n\", with integer coefficients. \n\nThe chromatic polynomial includes at least as much information about the colorability of \"G\" as does the chromatic number. Indeed, the chromatic number is the smallest positive integer that is not a zero of the chromatic polynomial,\n\nThe polynomial evaluated at formula_45, that is formula_46, yields formula_47 times the number of acyclic orientations of \"G\".\n\nThe derivative evaluated at 1, formula_48 equals the chromatic invariant, formula_49, up to sign.\nIf \"G\" has \"n\" vertices and \"c\" components formula_50, then\n\nThe last property is generalized by the fact that if \"G\" is a \"k\"-clique-sum of formula_59 and formula_60 (i.e., a graph obtained by gluing the two at a clique on \"k\" vertices), then\nA graph \"G\" with \"n\" vertices is a tree if and only if \n\nTwo graphs are said to be \"chromatically equivalent\" if they have the same chromatic polynomial. Isomorphic graphs have the same chromatic polynomial, but non-isomorphic graphs can be chromatically equivalent. For example, all trees on \"n\" vertices have the same chromatic polynomial.\nIn particular, formula_63 is the chromatic polynomial of both the claw graph and the path graph on 4 vertices.\n\nA graph is \"chromatically unique\" if it is determined by its chromatic polynomial, up to isomorphism. In other words, \"G\" is chromatically unique, then formula_64 would imply that \"G\" and \"H\" are isomorphic.\nAll cycle graphs are chromatically unique.\n\nA root (or \"zero\") of a chromatic polynomial, called a “chromatic root”, is a value \"x\" where formula_65. Chromatic roots have been very well studied, in fact, Birkhoff’s original motivation for defining the chromatic polynomial was to show that for planar graphs, formula_66 for \"x\" ≥ 4. This would have established the four color theorem.\n\nNo graph can be 0-colored, so 0 is always a chromatic root. Only edgeless graphs can be 1-colored, so 1 is a chromatic root of every graph with at least one edge. On the other hand, except for these two points, no graph can have a chromatic root at a real number smaller than or equal to 32/27. A result of Tutte connects the golden ratio formula_67 with the study of chromatic roots, showing that chromatic roots exist very close to formula_68:\nIf formula_69 is a planar triangulation of a sphere then\n\nWhile the real line thus has large parts that contain no chromatic roots for any graph, every point in the complex plane is arbitrarily close to a chromatic root in the sense that there exists an infinite family of graphs whose chromatic roots are dense in the complex plane.\n\nFor a graph \"G\" on \"n\" vertices, let formula_71 denote the number of colorings using exactly \"k\" colors \"up to renaming colors\" (so colorings that can be obtained from one another by permuting colors are counted as one; colorings obtained by automorphisms of \"G\" are still counted separately).\nIn other words, formula_71 counts the number of partitions of the vertex set into \"k\" (non-empty) independent sets.\nThen formula_73 counts the number of colorings using exactly \"k\" colors (with distinguishable colors).\nFor an integer \"x\", all \"x\"-colorings of \"G\" can be uniquely obtained by choosing an integer \"k ≤ x\", choosing \"k\" colors to be used out of \"x\" available, and a coloring using exactly those \"k\" (distinguishable) colors.\nTherefore:\nwhere formula_75 denotes the falling factorial.\nThus the numbers formula_71 are the coefficients of the polynomial formula_7 in the basis formula_78 of falling factorials.\n\nLet formula_79 be the \"k\"-th coefficient of formula_7 in the standard basis formula_81, that is:\nStirling numbers give a change of basis between the standard basis and the basis of falling factorials.\nThis implies:\n\nThe chromatic polynomial is categorified by a homology theory closely related to Khovanov homology.\n\nComputational problems associated with the chromatic polynomial include\n\n\nThe first problem is more general because if we knew the coefficients of formula_43 we could evaluate it at any point in polynomial time because the degree is \"n\". The difficulty of the second type of problem depends strongly on the value of \"x\" and has been intensively studied in computational complexity. When \"x\" is a natural number, this problem is normally viewed as computing the number of \"x\"-colorings of a given graph. For example, this includes the problem #3-coloring of counting the number of 3-colorings, a canonical problem in the study of complexity of counting, complete for the counting class #P.\n\nFor some basic graph classes, closed formulas for the chromatic polynomial are known. For instance this is true for trees and cliques, as listed in the table above.\n\nPolynomial time algorithms are known for computing the chromatic polynomial for wider classes of graphs, including chordal graphs and graphs of bounded clique-width. The latter class includes cographs and graphs of bounded tree-width, such as outerplanar graphs.\n\nThe deletion–contraction recurrence gives a way of computing the chromatic polynomial, called the \"deletion–contraction algorithm\". In the first form (with a minus), the recurrence terminates in a collection of empty graphs. In the second form (with a plus), it terminates in a collection of complete graphs. \nThis forms the basis of many algorithms for graph coloring. The ChromaticPolynomial function in the computer algebra system Mathematica uses the second recurrence if the graph is dense, and the first recurrence if the graph is sparse. The worst case running time of either formula satisfies the same recurrence relation as the Fibonacci numbers, so in the worst case, the algorithm runs in time within a polynomial factor of\n\non a graph with \"n\" vertices and \"m\" edges. The analysis can be improved to within a polynomial factor of the number formula_89 of spanning trees of the input graph. In practice, branch and bound strategies and graph isomorphism rejection are employed to avoid some recursive calls, the running time depends on the heuristic used to pick the vertex pair.\n\nThere is a natural geometric perspective on graph colorings by observing that, as an assignment of natural numbers to each vertex, a graph coloring is a vector in the integer lattice.\nSince two vertices formula_90 and formula_91 being given the same color is equivalent to the formula_90’th and formula_91’th coordinate in the coloring vector being equal, each edge can be associated with a hyperplane of the form formula_94. The collection of such hyperplanes for a given graph is called its graphic arrangement. The proper colorings of a graph are those lattice points which avoid forbidden hyperplanes.\nRestricting to a set of formula_95 colors, the lattice points are contained in the cube formula_96. In this context the chromatic polynomial counts the number of lattice points in the formula_97-cube that avoid the graphic arrangement.\n\nThe problem of computing the number of 3-colorings of a given graph is a canonical example of a #P-complete problem, so the problem of computing the coefficients of the chromatic polynomial is #P-hard. Similarly, evaluating formula_98 for given \"G\" is #P-complete. On the other hand, for formula_99 it is easy to compute formula_1, so the corresponding problems are polynomial-time computable. For integers formula_101 the problem is #P-hard, which is established similar to the case formula_102. In fact, it is known that formula_43 is #P-hard for all \"x\" (including negative integers and even all complex numbers) except for the three “easy points”. Thus, from the perspective of #P-hardness, the complexity of computing the chromatic polynomial is completely understood.\n\nIn the expansion\n\nthe coefficient formula_105 is always equal to 1, and several other properties of the coefficients are known. This raises the question if some of the coefficients are easy to compute. However the computational problem of computing \"a\" for a fixed \"r ≥ 1\" and a given graph \"G\" is #P-hard, even for bipartite planar graphs.\n\nNo approximation algorithms for computing formula_43 are known for any \"x\" except for the three easy points. At the integer points formula_107, the corresponding decision problem of deciding if a given graph can be \"k\"-colored is NP-hard. Such problems cannot be approximated to any multiplicative factor by a bounded-error probabilistic algorithm unless NP = RP, because any multiplicative approximation would distinguish the values 0 and 1, effectively solving the decision version in bounded-error probabilistic polynomial time. In particular, under the same assumption, this rules out the possibility of a fully polynomial time randomised approximation scheme (FPRAS). For other points, more complicated arguments are needed, and the question is the focus of active research. , it is known that there is no FPRAS for computing formula_43 for any \"x\" > 2, unless NP = RP holds.\n\n"}
{"id": "23937135", "url": "https://en.wikipedia.org/wiki?curid=23937135", "title": "Code officiel géographique", "text": "Code officiel géographique\n\nThe (English: Official geographic code) is a document listing the INSEE code which defines some French geographical codes.\n"}
{"id": "406922", "url": "https://en.wikipedia.org/wiki?curid=406922", "title": "Complete Fermi–Dirac integral", "text": "Complete Fermi–Dirac integral\n\nIn mathematics, the complete Fermi–Dirac integral, named after Enrico Fermi and Paul Dirac, for an index \"j \" is defined by\n\nThis equals \nwhere formula_3 is the polylogarithm.\n\nIts derivative is\nand this derivative relationship is used to define the Fermi-Dirac integral for nonpositive indices \"j\". Differing notation for formula_5 appears in the literature, for instance some authors omit the factor formula_6. The definition used here matches that in the NIST DLMF.\n\nThe closed form of the function exists for \"j\" = 0:\n\n\n\n"}
{"id": "46117", "url": "https://en.wikipedia.org/wiki?curid=46117", "title": "Eratosthenes", "text": "Eratosthenes\n\nEratosthenes of Cyrene (; , ;  – ) was a Greek mathematician, geographer, poet, astronomer, and music theorist. He was a man of learning, becoming the chief librarian at the Library of Alexandria. He invented the discipline of geography, including the terminology used today.\n\nHe is best known for being the first person to calculate the circumference of the Earth, which he did by comparing altitudes of the mid-day sun at two places a known North-South distance apart. His calculation was remarkably accurate. He was also the first to calculate the tilt of the Earth's axis, again with remarkable accuracy. Additionally, he may have accurately calculated the distance from the Earth to the Sun and invented the leap day. He created the first map of the world, incorporating parallels and meridians based on the available geographic knowledge of his era.\n\nEratosthenes was the founder of scientific chronology; he endeavored to revise the dates of the chief literary and political events from the conquest of Troy. Eratosthenes dated The Sack of Troy to 1183 BC. In number theory, he introduced the sieve of Eratosthenes, an efficient method of identifying prime numbers.\n\nHe was a figure of influence in many fields. According to an entry in the Suda (a 10th-century reference), his critics scorned him, calling him \"Beta\" (the second letter of the Greek alphabet) because he always came in second in all his endeavors. Nonetheless, his devotees nicknamed him \"Pentathlos\" after the Olympians who were well rounded competitors, for he had proven himself to be knowledgeable in every area of learning. Eratosthenes yearned to understand the complexities of the entire world.\n\nThe son of Aglaos, Eratosthenes was born in 276 BC in Cyrene. Now part of modern-day Libya, Cyrene had been founded by Greeks centuries earlier and became the capital of Pentapolis (North Africa), a country of five cities: Cyrene, Arsinoe, Berenice, Ptolemias, and Apollonia. Alexander the Great conquered Cyrene in 332 BC, and following his death in 323 BC, its rule was given to one of his generals, Ptolemy I Soter, the founder of the Ptolemaic Kingdom. Under Ptolemaic rule the economy prospered, based largely on the export of horses and silphium, a plant used for rich seasoning and medicine. Cyrene became a place of cultivation, where knowledge blossomed. Like any young Greek, Eratosthenes would have studied in the local gymnasium, where he would have learned physical skills and social discourse as well as reading, writing, arithmetic, poetry, and music.\nEratosthenes went to Athens to further his studies. There he was taught Stoicism by its founder, Zeno of Citium, in philosophical lectures on living a virtuous life. He then studied under Aristo of Chios, who led a more cynical school of philosophy. He also studied under the head of the Platonic Academy, who was Arcesilaus of Pitane. His interest in Plato led him to write his very first work at a scholarly level, \"Platonikos\", inquiring into the mathematical foundation of Plato's philosophies. Eratosthenes was a man of many perspectives and investigated the art of poetry under Callimachus. He was a talented and imaginative poet. He wrote poems: one in hexameters called \"Hermes\", illustrating the god's life history; and another, in elegiacs called \"Erigone\", describing the suicide of the Athenian maiden Erigone (daughter of Icarius). He wrote \"Chronographies\", a text that scientifically depicted dates of importance, beginning with the Trojan War. This work was highly esteemed for its accuracy. George Syncellus was later able to preserve from \"Chronographies\" a list of 38 kings of the Egyptian Thebes. Eratosthenes also wrote \"Olympic Victors\", a chronology of the winners of the Olympic Games. It is not known when he wrote his works, but they highlighted his abilities.\n\nThese works and his great poetic abilities led the pharaoh Ptolemy III Euergetes to seek to place him as a librarian at the Library of Alexandria in the year 245 BC. Eratosthenes, then thirty years old, accepted Ptolemy's invitation and traveled to Alexandria, where he lived for the rest of his life. Within about five years he became Chief Librarian, a position that the poet Apollonius Rhodius had previously held. As head of the library Eratosthenes tutored the children of Ptolemy, including Ptolemy IV Philopator who became the fourth Ptolemaic pharaoh. He expanded the library's holdings: in Alexandria all books had to be surrendered for duplication. It was said that these were copied so accurately that it was impossible to tell if the library had returned the original or the copy.\nHe sought to maintain the reputation of the Library of Alexandria against competition from the Library of Pergamum. Eratosthenes created a whole section devoted to the examination of Homer, and acquired original works of great tragic dramas of Aeschylus, Sophocles and Euripides.\n\nEratosthenes made several important contributions to mathematics and science, and was a friend of Archimedes. Around 255 BC, he invented the armillary sphere. In \"On the Circular Motions of the Celestial Bodies\", Cleomedes credited him with having calculated the Earth's circumference around 240 BC, using knowledge of the angle of elevation of the Sun at noon on the summer solstice in Alexandria and on Elephantine Island near Syene (modern Aswan, Egypt).\n\nEratosthenes believed there was good and bad in every nation and criticized Aristotle for arguing that humanity was divided into Greeks and barbarians, and that the Greeks should keep themselves racially pure. As he aged he contracted ophthalmia, becoming blind around 195 BC. Losing the ability to read and to observe nature plagued and depressed him, leading him to voluntarily starve himself to death. He died in 194 BC at 82 in Alexandria.\n\nEratosthenes calculated the Earth's circumference without leaving Egypt. He knew that at local noon on the summer solstice in Syene (modern Aswan, Egypt), the Sun was directly overhead. (Syene is at latitude 24°05′ North, near to the Tropic of Cancer, which was 23°42′ North in 100 BC) He knew this because the shadow of someone looking down a deep well at that time in Syene blocked the reflection of the Sun on the water. He then measured the Sun's angle of elevation at noon in Alexandria by using a vertical rod, known as a gnomon, and measuring the length of its shadow on the ground. Using the length of the rod, and the length of the shadow, as the legs of a triangle, he calculated the angle of the sun's rays. This turned out to be about 7°, or 1/50th the circumference of a circle. Taking the Earth as spherical, and knowing both the distance and direction of Syene, he concluded that the Earth's circumference was fifty times that distance.\n\nHis knowledge of the size of Egypt was founded on the work of many generations of surveying trips. Pharaonic bookkeepers gave a distance between Syene and Alexandria of 5,000 stadia (a figure that was checked yearly). Some historians say that the distance was corroborated by inquiring about the time that it took to travel from Syene to Alexandria by camel. Some claim Eratosthenes used the Olympic stade of 176.4 m, which would imply a circumference of 44,100 km, an error of 10%, but the 184.8 m Italian stade became (300 years later) the most commonly accepted value for the length of the stade, which implies a circumference of 46,100 km, an error of 15%. He made five important assumptions (none of which is perfectly accurate):\n\nEratosthenes later rounded the result to a final value of 700 stadia per degree, which implies a circumference of 252,000 stadia, likely for reasons of calculation simplicity as the larger number is evenly divisible by 60. In 2012, Anthony Abreu Mora repeated Eratosthenes's calculation with more accurate data; the result was 40,074 km, which is 66 km different (0.16%) from the currently accepted polar circumference of the Earth.\n\nSeventeen hundred years after Eratosthenes's death, while Christopher Columbus studied what Eratosthenes had written about the size of the Earth, he chose to believe, based on a map by Toscanelli, that the Earth's circumference was one-third smaller. Had Columbus set sail knowing that Eratosthenes's larger circumference value was more accurate, he would have known that the place that he made landfall was not Asia, but rather the New World.\n\nEratosthenes now continued from his knowledge about the Earth. Using his discoveries and knowledge of its size and shape, he began to sketch it. In the Library of Alexandria he had access to various travel books, which contained various items of information and representations of the world that needed to be pieced together in some organized format. In his three-volume work \"Geography\" (), he described and mapped his entire known world, even dividing the Earth into five climate zones: two freezing zones around the poles, two temperate zones, and a zone encompassing the equator and the tropics. He had invented geography. He created terminology that is still used today. He placed grids of overlapping lines over the surface of the Earth. He used parallels and meridians to link together every place in the world. It was now possible to estimate one's distance from remote locations with this network over the surface of the Earth. In the \"Geography\" the names of over 400 cities and their locations were shown: this had never been achieved before. Unfortunately, his \"Geography\" has been lost to history, but fragments of the work can be pieced together from other great historians like Pliny, Polybius, Strabo, and Marcianus.\n\n\nEratosthenes was described by the Suda Lexicon as a Πένταθλος (Pentathlos) which can be translated as \"All-Rounder\", for he was skilled in a variety of things: He was a true polymath. He was nicknamed Beta because he was great at many things and tried to get his hands on every bit of information but never achieved the highest rank in anything; Strabo accounts Eratosthenes as a mathematician among geographers and a geographer among mathematicians.\n\n\nEratosthenes proposed a simple algorithm for finding prime numbers. This algorithm is known in mathematics as the Sieve of Eratosthenes.\n\nIn mathematics, the sieve of Eratosthenes (Greek: κόσκινον Ἐρατοσθένους), one of a number of prime number sieves, is a simple, ancient algorithm for finding all prime numbers up to any given limit. It does so by iteratively marking as composite, \"i.e.\", not prime, the multiples of each prime, starting with the multiples of 2. The multiples of a given prime are generated starting from that prime, as a sequence of numbers with the same difference, equal to that prime, between consecutive numbers. This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.\n\nEratosthenes was one of the most pre-eminent scholarly figures of his time, and produced works covering a vast area of knowledge before and during his time at the Library. He wrote on many topics — geography, mathematics, philosophy, chronology, literary criticism, grammar, poetry, and even old comedies. Unfortunately, there are only fragments left of his works after the Destruction of the Library of Alexandria.\n\n\n\n\n"}
{"id": "4813235", "url": "https://en.wikipedia.org/wiki?curid=4813235", "title": "Ewald summation", "text": "Ewald summation\n\nEwald summation, named after Paul Peter Ewald, is a method for computing long-range interactions (e.g., electrostatic interactions) in periodic systems. It was first developed as the method for calculating electrostatic energies of ionic crystals, and is now commonly used for calculating long-range interactions in computational chemistry. Ewald summation is a special case of the Poisson summation formula, replacing the summation of interaction energies in real space with an equivalent summation in Fourier space. In this method, the long-range interaction is divided into two parts: a short-range contribution, and a long-range contribution which does not have a singularity. The short-range contribution is calculated in real space, whereas the long-range contribution is calculated using a Fourier transform. The advantage of this method is the rapid convergence of the energy compared with that of a direct summation. This means that the method has high accuracy and reasonable speed when computing long-range interactions, and it is thus the de facto standard method for calculating long-range interactions in periodic systems. The method requires charge neutrality of the molecular system in order to calculate accurately the total Coulombic interaction. A study of the truncation errors introduced in the energy and force calculations of disordered point-charge systems is provided by Kolafa and Perram.\n\nEwald summation rewrites the interaction potential as the sum of two terms,\n\nwhere formula_2 represents the short-range term whose sum quickly converges in real space and formula_3 represents the long-range term whose sum quickly converges in Fourier (reciprocal) space. The long-ranged part should be finite for all arguments (most notably \"r\" = 0) but may have any convenient mathematical form, most typically a Gaussian distribution. The method assumes that the short-range part can be summed easily; hence, the problem becomes the summation of the long-range term. Due to the use of the Fourier sum, the method implicitly assumes that the system under study is infinitely periodic (a sensible assumption for the interiors of crystals). One repeating unit of this hypothetical periodic system is called a \"unit cell\". One such cell is chosen as the \"central cell\" for reference and the remaining cells are called \"images\".\n\nThe long-range interaction energy is the sum of interaction energies between the charges of a central unit cell and all the charges of the lattice. Hence, it can be represented as a \"double\" integral over two charge density fields representing the fields of the unit cell and the crystal lattice\n\nwhere the unit-cell charge density field formula_5 is a sum over the positions formula_6 of the charges formula_7 in the central unit cell\n\nand the \"total\" charge density field formula_9 is the same sum over the unit-cell charges formula_10 and their periodic images\n\nHere, formula_12 is the Dirac delta function, formula_13, formula_14 and formula_15 are the lattice vectors and formula_16, formula_17 and formula_18 range over all integers. The total field formula_9 can be represented as a convolution of formula_5 with a \"lattice function\" formula_21\n\nSince this is a convolution, the Fourier transformation of formula_9 is a product\n\nwhere the Fourier transform of the lattice function is another sum over delta functions\n\nwhere the reciprocal space vectors are defined formula_26 (and cyclic permutations) where formula_27 is the volume of the central unit cell (if it is geometrically a parallelepiped, which is often but not necessarily the case). Note that both formula_21 and formula_29 are real, even functions.\n\nFor brevity, define an effective single-particle potential\n\nSince this is also a convolution, the Fourier transformation of the same equation is a product\n\nwhere the Fourier transform is defined\n\nThe energy can now be written as a \"single\" field integral\n\nUsing Parseval's theorem, the energy can also be summed in Fourier space\n\nwhere formula_35\nin the final summation.\n\nThis is the essential result. Once formula_36 is calculated, the summation/integration over formula_37 is straightforward and should converge quickly. The most common reason for lack of convergence is a poorly defined unit cell, which must be charge neutral to avoid infinite sums.\n\nEwald summation was developed as a method in theoretical physics, long before the advent of computers. However, the Ewald method has enjoyed widespread use since the 1970s in computer simulations of particle systems, especially those whose particles interact via an inverse square force law such as gravity or electrostatics. Recently, PME has also been used to calculate the formula_38 part of the Lennard-Jones potential in order to eliminate artifacts due to truncation. Applications include simulations of plasmas, galaxies and molecules.\n\nIn the particle mesh method, just as in standard Ewald summation, the generic interaction potential is separated into two terms\nformula_1. The basic idea of particle mesh Ewald summation is to replace the direct summation of interaction energies between point particles\n\nwith two summations, a direct sum formula_41 of the short-ranged potential in real space\n\n(that is the particle part of particle mesh Ewald) and a summation in Fourier space of the long-ranged\npart\n\nwhere formula_44 and formula_45 represent the Fourier transforms of the potential and the charge density (that's the Ewald part). Since both summations converge quickly in their respective spaces (real and Fourier), they may be truncated with little loss of accuracy and great improvement in required computational time. To evaluate the Fourier transform formula_45 of the charge density field efficiently, one uses the Fast Fourier transform, which requires that the density field be evaluated on a discrete lattice in space (that's the mesh part).\n\nDue to the periodicity assumption implicit in Ewald summation, applications of the PME method to physical systems require the imposition of periodic symmetry. Thus, the method is best suited to systems that can be simulated as infinite in spatial extent. In molecular dynamics simulations this is normally accomplished by deliberately constructing a charge-neutral unit cell that can be infinitely \"tiled\" to form images; however, to properly account for the effects of this approximation, these images are reincorporated back into the original simulation cell. The overall effect is called a periodic boundary condition. To visualize this most clearly, think of a unit cube; the upper face is effectively in contact with the lower face, the right with the left face, and the front with the back face. As a result, the unit cell size must be carefully chosen to be large enough to avoid improper motion correlations between two faces \"in contact\", but still small enough to be computationally feasible. The definition of the cutoff between short- and long-range interactions can also introduce artifacts.\n\nThe restriction of the density field to a mesh makes the PME method more efficient for systems with \"smooth\" variations in density, or continuous potential functions. Localized systems or those with large fluctuations in density may be treated more efficiently with the fast multipole method of Greengard and Rokhlin.\n\nThe electrostatic energy of a polar crystal (i.e., a crystal with a net dipole formula_47 in the unit cell) is conditionally convergent, i.e., depends on the order of the summation. For example, if the dipole-dipole interactions of a central unit cell with unit cells located on an ever-increasing cube, the energy converges to a different value than if the interaction energies had been summed spherically. Roughly speaking, this conditional convergence arises because (1) the number of interacting dipoles on a shell of radius formula_48 grows like formula_49; (2) the strength of a single dipole-dipole interaction falls like formula_50; and (3) the mathematical summation formula_51 diverges.\n\nThis somewhat surprising result can be reconciled with the finite energy of real crystals because such crystals are not infinite, i.e., have a particular boundary. More specifically, the boundary of a polar \ncrystal has an effective surface charge density on its surface formula_52 where formula_53 is the surface normal vector and formula_54 represents the net dipole moment per volume. The interaction energy formula_55 of the dipole in a central unit cell with that surface charge density can be written\n\nwhere formula_47 and formula_58 are the net dipole moment and volume of the unit cell, formula_59 is an infinitesimal area on the crystal surface and formula_60\nis the vector from the central unit cell to the infinitesimal area. This formula results from integrating the energy formula_61 where formula_62 represents the infinitesimal electric field generated by an infinitesimal surface charge formula_63 (Coulomb's law)\n\nThe negative sign derives from the definition of formula_60, which points towards the charge, not away from it.\n\nThe Ewald summation was developed by Paul Peter Ewald in 1921 (see References below) to determine the electrostatic energy (and, hence, the Madelung constant) of ionic crystals.\n\nGenerally different Ewald summation methods give different time complexities. Direct calculation gives formula_66, where formula_67 is the number of atoms in the system. The PME method gives formula_68.\n\n\n"}
{"id": "60022", "url": "https://en.wikipedia.org/wiki?curid=60022", "title": "Fractal compression", "text": "Fractal compression\n\nFractal compression is a lossy compression method for digital images, based on fractals. The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image. Fractal algorithms convert these parts into mathematical data called \"fractal codes\" which are used to recreate the encoded image.\n\nFractal image representation may be described mathematically as an iterated function system (IFS).\n\nWe begin with the representation of a binary image, where the image may be thought of as a subset of formula_1. An IFS is a set of contraction mappings \"ƒ\"...,\"ƒ\",\n\nAccording to these mapping functions, the IFS describes a two-dimensional set \"S\" as the fixed point of the Hutchinson operator\n\nThat is, \"H\" is an operator mapping sets to sets, and \"S\" is the unique set satisfying \"H\"(\"S\") = \"S\". The idea is to construct the IFS such that this set \"S\" is the input binary image. The set \"S\" can be recovered from the IFS by fixed point iteration: for any nonempty compact initial set \"A\", the iteration \"A\" = \"H\"(\"A\") converges to \"S\".\n\nThe set \"S\" is self-similar because \"H\"(\"S\") = \"S\" implies that \"S\" is a union of mapped copies of itself:\nSo we see the IFS is a fractal representation of \"S\".\n\nIFS representation can be extended to a grayscale image by considering the image's graph as a subset of formula_5. For a grayscale image \"u\"(\"x\",\"y\"), consider the set\n\"S\" = {(\"x\",\"y\",\"u\"(\"x\",\"y\"))}. Then similar to the binary case, \"S\" is described by an IFS using a set of contraction mappings \"ƒ\"...,\"ƒ\", but in formula_5,\n\nA challenging problem of ongoing research in fractal image representation is how to choose the \"ƒ\"...,\"ƒ\" such that its fixed point approximates the input image, and how to do this efficiently.\n\nA simple approach for doing so is the following partitioned iterated function system (PIFS):\n\n\nIn the second step, it is important to find a similar block so that the IFS accurately represents the input image, so a sufficient number of candidate blocks for \"D\" need to be considered. On the other hand, a large search considering many blocks is computationally costly. \nThis bottleneck of searching for similar blocks is why PIFS fractal encoding is much slower than for example DCT and wavelet based image representation.\n\nThe initial square partitioning and brute-force search algorithm presented by Jacquin provides a starting point for further research and extensions in many possible directions -- different ways of partitioning the image into range blocks of various sizes and shapes; fast techniques for quickly finding a close-enough matching domain block for each range block rather than brute-force searching, such as fast motion estimation algorithms; different ways of encoding the mapping from the domain block to the range block; etc.\n\nOther researchers attempt to find algorithms to automatically encode an arbitrary image as RIFS (recurrent iterated function systems) or global IFS, rather than PIFS; and algorithms for fractal video compression including motion compensation and three dimensional iterated function systems.\n\nFractal image compression has many similarities to vector quantization image compression.\n\nWith fractal compression, encoding is extremely computationally expensive because of the search used to find the self-similarities. Decoding, however, is quite fast. While this asymmetry has so far made it impractical for real time applications, when video is archived for distribution from disk storage or file downloads fractal compression becomes more competitive.\n\nAt common compression ratios, up to about 50:1, Fractal compression provides similar results to DCT-based algorithms such as JPEG. At high compression ratios fractal compression may offer superior quality. For satellite imagery, ratios of over 170:1 have been achieved with acceptable results. Fractal video compression ratios of 25:1–244:1 have been achieved in reasonable compression times (2.4 to 66 sec/frame).\n\nCompression efficiency increases with higher image complexity and color depth, compared to simple grayscale images.\n\nAn inherent feature of fractal compression is that images become resolution independent after being converted to fractal code. This is because the iterated function systems in the compressed file scale indefinitely. This indefinite scaling property of a fractal is known as \"fractal scaling\".\n\nThe resolution independence of a fractal-encoded image can be used to increase the display resolution of an image. This process is also known as \"fractal interpolation\". In fractal interpolation, an image is encoded into fractal codes via fractal compression, and subsequently decompressed at a higher resolution. The result is an up-sampled image in which iterated function systems have been used as the interpolant.\nFractal interpolation maintains geometric detail very well compared to traditional interpolation methods like bilinear interpolation and bicubic interpolation. Since the interpolation cannot reverse Shannon entropy however, it ends up sharpening the image by adding random instead of meaningful detail. One cannot, for example, enlarge an image of a crowd where each person's face is one or two pixels and hope to identify them.\n\nMichael Barnsley led development of fractal compression in 1987, and was granted several patents on the technology. The most widely known practical fractal compression algorithm was invented by Barnsley and Alan Sloan. Barnsley's graduate student Arnaud Jacquin implemented the first automatic algorithm in software in 1992. All methods are based on the fractal transform using iterated function systems. Michael Barnsley and Alan Sloan formed Iterated Systems Inc. in 1987 which was granted over 20 additional patents related to fractal compression.\n\nA major breakthrough for Iterated Systems Inc. was the automatic fractal transform process which eliminated the need for human intervention during compression as was the case in early experimentation with fractal compression technology. In 1992, Iterated Systems Inc. received a US$2.1 million government grant to develop a prototype digital image storage and decompression chip using fractal transform image compression technology.\n\nFractal image compression has been used in a number of commercial applications: onOne Software, developed under license from Iterated Systems Inc., Genuine Fractals 5 which is a Photoshop plugin capable of saving files in compressed FIF (Fractal Image Format). To date the most successful use of still fractal image compression is by Microsoft in its Encarta multimedia encyclopedia, also under license.\n\nIterated Systems Inc. supplied a shareware encoder (Fractal Imager), a stand-alone decoder, a Netscape plug-in decoder and a development package for use under Windows. As wavelet-based methods of image compression improved and were more easily licensed by commercial software vendors the adoption of the Fractal Image Format failed to evolve. The redistribution of the \"decompressor DLL\" provided by the ColorBox III SDK was governed by restrictive per-disk or year-by-year licensing regimes for proprietary software vendors and by a discretionary scheme that entailed the promotion of the Iterated Systems products for certain classes of other users.\n\nDuring the 1990s Iterated Systems Inc. and its partners expended considerable resources to bring fractal compression to video. While compression results were promising, computer hardware of that time lacked the processing power for fractal video compression to be practical beyond a few select usages. Up to 15 hours were required to compress a single minute of video.\n\nClearVideo also known as RealVideo (Fractal) and SoftVideo were early fractal video compression products. ClearFusion was Iterated's freely distributed streaming video plugin for web browsers. In 1994 SoftVideo was licensed to Spectrum Holobyte for use in its CD-ROM games including Falcon Gold and .\n\nIn 1996, Iterated Systems Inc. announced an alliance with the Mitsubishi Corporation to market ClearVideo to their Japanese customers. The original ClearVideo 1.2 decoder driver is still supported by Microsoft in Windows Media Player although the encoder is no longer supported.\n\nTwo firms, Total Multimedia Inc. and Dimension, both claim to own or have the exclusive licence to Iterated's video technology, but neither has yet released a working product. The technology basis appears to be Dimension's U.S. patents 8639053 and 8351509, which have been considerably analyzed. In summary, it is a simple quadtree block-copying system with neither the bandwidth efficiency nor PSNR quality of traditional DCT-based codecs. In January 2016, TMMI announced that it was abandoning fractal-based technology altogether.\n\nNumerous research papers have been published during the past few years discussing possible solutions to improve fractal algorithms and encoding hardware.\n\nA library called Fiasco was created by Ullrich Hafner and described in \"Linux Journal\".\n\nThe Netpbm library includes a Fiasco library.\n\nThere is a video library for fractal compression.\n\nThere is another example implementation from Femtosoft.\n\n\n"}
{"id": "6532631", "url": "https://en.wikipedia.org/wiki?curid=6532631", "title": "Frattini's argument", "text": "Frattini's argument\n\nIn group theory, a branch of mathematics, Frattini's argument is an important lemma in the structure theory of finite groups. It is named after Giovanni Frattini, who used it in a paper from 1885 when defining the Frattini subgroup of a group. The argument was taken by Frattini, as he himself admits, from a paper of Alfredo Capelli dated 1884.\n\nIf formula_1 is a finite group with normal subgroup formula_2, and if formula_3 is a Sylow \"p\"-subgroup of formula_2, then\n\nformula_5\n\nwhere formula_6 denotes the normalizer of formula_3 in formula_1 and formula_9 means the product of group subsets.\n\nThe group formula_3 is a Sylow formula_11-subgroup of formula_2, so every Sylow formula_11-subgroup of formula_2 is an formula_2-conjugate of formula_3, that is, it is of the form formula_17, for some formula_18 (see Sylow theorems). Let formula_19 be any element of formula_1. Since formula_2 is normal in formula_1, the subgroup formula_23 is contained in formula_2. This means that formula_23 is a Sylow formula_11-subgroup of formula_2. Then by the above, it must be formula_2-conjugate to formula_3: that is, for some formula_18\n\nand so\n\nThus,\n\nand therefore formula_34. But formula_35 was arbitrary, and so formula_36\n\n\n\n"}
{"id": "8356786", "url": "https://en.wikipedia.org/wiki?curid=8356786", "title": "Fred Diamond", "text": "Fred Diamond\n\nFred Irvin Diamond (born November 19, 1964) is a mathematician, known for his role in proving the modularity theorem for elliptic curves. His research interest is in modular forms and Galois representations.\n\nDiamond received his B.A. from the University of Michigan in 1984, and received his Ph.D. in mathematics from Princeton University in 1988 as a doctoral student of Andrew Wiles. He has held positions at Brandeis University and Rutgers University, and is currently a professor at King's College London.\n\nDiamond is the author of several research papers, and is also a coauthor along with Jerry Shurman of \"A First Course in Modular Forms\", in the Graduate Texts in Mathematics series published by Springer-Verlag.\n\n"}
{"id": "35987396", "url": "https://en.wikipedia.org/wiki?curid=35987396", "title": "Gelfand–Kirillov dimension", "text": "Gelfand–Kirillov dimension\n\nIn algebra, the Gelfand–Kirillov dimension (or GK dimension) of a right module \"M\" over a \"k\"-algebra \"A\" is:\n\nwhere the sup is taken over all finite-dimensional subspaces formula_2 and formula_3.\n\nAn algebra is said to have polynomial growth if its Gelfand–Kirillov dimension is finite.\n\n\nGiven a right module \"M\" over the Weyl algebra formula_5, the Gelfand–Kirillov dimension of \"M\" over the Weyl algebra coincides with the dimension of \"M\", which is by definition the degree of the Hilbert polynomial of \"M\". This enables to prove additivity in short exact sequences for the Gelfand–Kirillov dimension and finally to prove Bernstein's inequality, which states that the dimension of \"M\" must be at least \"n\". This leads to the definition of holonomic D-Modules as those with the minimal dimension \"n\", and these modules play a great role in the geometric Langlands program.\n\n"}
{"id": "43304622", "url": "https://en.wikipedia.org/wiki?curid=43304622", "title": "Graph realization problem", "text": "Graph realization problem\n\nThe graph realization problem is a decision problem in graph theory. Given a finite sequence formula_1 of natural numbers, the problem asks whether there is labeled simple graph such that formula_1 is the degree sequence of this graph.\n\nThe problem can be solved in polynomial time. One method of showing this uses the Havel–Hakimi algorithm constructing a special solution with the use of a recursive algorithm. Alternatively, following the characterization given by the Erdős–Gallai theorem, the problem can be solved by testing the validity of formula_3 inequalities.\n\nThe problem can also be stated in terms of symmetric matrices of zeros and ones. The connection can be seen if one realizes that each graph has an adjacency matrix where the column sums and row sums correspond to formula_4. The problem is then sometimes denoted by \"symmetric 0-1-matrices for given row sums\".\n\nSimilar problems describe the degree sequences of simple bipartite graphs or the degree sequences of simple directed graphs. The first problem is the so-called bipartite realization problem. The second is known as the digraph realization problem.\n\nThe problem of constructing a solution for the graph realization problem with the additional constraint that each such solution comes with the same probability was shown to have a polynomial-time approximation scheme for the degree sequences of regular graphs by Cooper et al. The general problem is still unsolved.\n"}
{"id": "40645", "url": "https://en.wikipedia.org/wiki?curid=40645", "title": "Haar measure", "text": "Haar measure\n\nIn mathematical analysis, the Haar measure assigns an \"invariant volume\" to subsets of locally compact topological groups, consequently defining an integral for functions on those groups.\n\nThis measure was introduced by Alfréd Haar in 1933, though its special case for Lie groups had been introduced by Adolf Hurwitz in 1897 under the name \"invariant integral\". Haar measures are used in many parts of analysis, number theory, group theory, representation theory, statistics, probability theory, and ergodic theory.\n\nLet formula_1 be a locally compact Hausdorff topological group. The formula_2-algebra generated by all open subsets of formula_3 is called the Borel algebra. An element of the Borel algebra is called a Borel set. If formula_4 is an element of formula_3 and formula_6 is a subset of formula_3, then we define the left and right translates of formula_6 as follows:\n\nLeft and right translates map Borel sets into Borel sets.\n\nA measure formula_11 on the Borel subsets of formula_3 is called \"left-translation-invariant\" if for all Borel subsets formula_13 and all formula_14 one has\nA measure formula_11 on the Borel subsets of formula_3 is called \"right-translation-invariant\" if for all Borel subsets formula_13 and all formula_14 one has\n\nThere is, up to a positive multiplicative constant, a unique countably additive, nontrivial measure formula_11 on the Borel subsets of formula_3 satisfying the following properties:\n\n\n\nSuch a measure on formula_3 is called a \"left Haar measure.\" It can be shown as a consequence of the above properties that formula_37 for every non-empty open subset formula_34. In particular, if formula_3 is compact then formula_40 is finite and positive, so we can uniquely specify a left Haar measure on formula_3 by adding the normalization condition formula_42.\n\nSome authors define a Haar measure on Baire sets rather than Borel sets. This makes the regularity conditions unnecessary as Baire measures are automatically regular. Halmos rather confusingly uses the term \"Borel set\" for elements of the formula_2-ring generated by compact sets, and defines Haar measure on these sets.\n\nThe left Haar measure satisfies the inner regularity condition for all formula_2-finite Borel sets, but may not be inner regular for \"all\" Borel sets. For example, the product of the unit circle (with its usual topology) and the real line with the discrete topology is a locally compact group with the product topology and Haar measure on this group is not inner regular for the closed subset formula_45. (Compact subsets of this vertical segment are finite sets and points have measure formula_46, so the measure of any compact subset of this vertical segment is formula_46. But, using outer regularity, one can show the segment has infinite measure.)\n\nThe existence and uniqueness (up to scaling) of a left Haar measure was first proven in full generality by André Weil. Weil's proof used the axiom of choice and Henri Cartan furnished a proof which avoided its use. Cartan's proof also establishes the existence and the uniqueness simultaneously. A simplified and complete account of Cartan's argument was given by Alfsen in 1963. The special case of invariant measure for second countable locally compact groups had been shown by Haar in 1933.\n\nThe following method of constructing Haar measure is essentially the method used by Haar and Weil.\n\nFor any subsets formula_48 with formula_6 nonempty define formula_50 to be the smallest number of left translates of formula_6 that cover formula_52 (so this is a non-negative integer or infinity). This is not additive on compact sets formula_29, though it does have the property that formula_54 for disjoint compact sets formula_55 provided that formula_56 is a sufficiently small open neighborhood of the identity (depending on formula_57 and formula_58). The idea of Haar measure is to take a sort of limit of formula_59 as formula_56 becomes smaller to make it additive on all pairs of disjoint compact sets, though it first has to be normalized so that the limit is not just infinity. So fix a compact set formula_61 with non-empty interior (which exists as the group is locally compact) and for a compact set formula_57 define\nwhere the limit is taken over a suitable directed set of open neighborhoods of the identity eventually contained in any given neighborhood; the existence of a directed set such that the limit exists follows using Tychonoff's theorem.\n\nThe function formula_64 is additive on disjoint compact subsets of formula_3, which implies that it is a regular content. From a regular content one can construct a measure by first extending formula_64 to open sets by inner regularity, then to all sets by outer regularity, and then restricting it to Borel sets. (Even for open sets formula_56, the corresponding measure formula_68 need not be given by the lim sup formula above. The problem is that the function given by the lim sup formula is not countably subadditive in general and in particular is infinite on any set without compact closure, so is not an outer measure.)\n\nCartan introduced another way of constructing Haar measure as a Radon measure (a positive linear functional on compactly supported continuous functions) which is similar to the construction above except that formula_61, formula_57, and formula_56 are positive continuous functions of compact support rather than subsets of formula_3. In this case we define formula_59 to be the infimum of numbers formula_74 such that formula_75 is less than the linear combination formula_76 of left translates of formula_56 for some formula_78.\nAs before we define\nThe fact that the limit exists takes some effort to prove, though the advantage of doing this is that the proof avoids the use of the axiom of choice and also gives uniqueness of Haar measure as a by-product. The functional formula_64 extends to a positive linear functional on compactly supported continuous functions and so gives a Haar measure. (Note that even though the limit is linear in formula_57, the individual terms formula_59 are not usually linear in formula_57.)\n\nVon Neumann gave a method of constructing Haar measure using mean values of functions, though it only works for compact groups. The idea is that given a function formula_84 on a compact group, one can find a convex combination formula_85 (where formula_86) of its left translates that differs from a constant function by at most some small number formula_87. Then one shows that as formula_87 tends to zero the values of these constant functions tend to a limit, which is called the mean value (or integral) of the function formula_84.\n\nFor groups that are locally compact but not compact this construction does not give Haar measure as the mean value of compactly supported functions is zero. However something like this does work for almost periodic functions on the group which do have a mean value, though this is not given with respect to Haar measure.\n\nOn an \"n\"-dimensional Lie group, Haar measure can be constructed easily as the measure induced by a left-invariant \"n\"-form. This was known before Haar's theorem.\n\nIt can also be proved that there exists a unique (up to multiplication by a positive constant) right-translation-invariant Borel measure formula_90 satisfying the above regularity conditions and being finite on compact sets, but it need not coincide with the left-translation-invariant measure formula_11. The left and right Haar measures are the same only for so-called \"unimodular groups\" (see below). It is quite simple, though, to find a relationship between formula_11 and formula_93.\n\nIndeed, for a Borel set formula_6, let us denote by formula_95 the set of inverses of elements of formula_6. If we define \nthen this is a right Haar measure. To show right invariance, apply the definition:\n\nBecause the right measure is unique, it follows that formula_99 is a multiple of formula_93 and so\nfor all Borel sets formula_6, where formula_103 is some positive constant.\n\nThe \"left\" translate of a right Haar measure is a right Haar measure. More precisely, if formula_93 is a right Haar measure, then\n\nis also right invariant. Thus, by uniqueness of the Haar measure, there exists a function formula_106 from the group to the positive reals, called the Haar modulus, modular function or modular character, such that for every Borel set formula_6\n\nSince right Haar measure is well-defined up to a positive scaling factor, this equation shows the modular function is independent of the choice of right Haar measure in the above equation.\n\nThe modular function is a continuous group homomorphism into the multiplicative group of positive real numbers. A group is called unimodular if the modular function is identically formula_109, or, equivalently, if the Haar measure is both left and right invariant. Examples of unimodular groups are abelian groups, compact groups, discrete groups (e.g., finite groups), semisimple Lie groups and connected nilpotent Lie groups. An example of a non-unimodular group is the group of affine transformations\n\non the real line. This example shows that a solvable Lie group need not be unimodular.\nIn this group a left Haar measure is given by formula_111, and a right Haar measure by formula_112.\n\nIf the locally compact group formula_3 acts transitively on a homogeneous space formula_114, one can ask if this space has an invariant measure, or more generally a semi-invariant measure with the property that formula_115 for some character formula_116 of formula_3. A necessary and sufficient condition for the existence of such a measure is that the restriction formula_118 is equal to formula_119, where formula_106 and formula_121 are the modular functions of formula_3 and formula_123 respectively. \nIn particular an invariant measure on formula_114 exists if and only if the modular function formula_106 of formula_3 restricted to formula_123 is the modular function formula_121 of formula_123.\n\nIf formula_3 is the group formula_131 and formula_123 is the subgroup of upper triangular matrices, then the modular function of formula_123 is nontrivial but the modular function of formula_3 is trivial. The quotient of these cannot be extended to any character of formula_3, so the quotient space formula_114 (which can be thought of as 1-dimensional real projective space) does not have even a semi-invariant measure.\n\nUsing the general theory of Lebesgue integration, one can then define an integral for all Borel measurable functions formula_84 on formula_3. This integral is called the Haar integral. If formula_11 is a left Haar measure, then\nfor any Haar integrable function formula_84 on formula_3. This is immediate for indicator functions, being essentially the definition of left invariance.\n\n\n\n\n\n\nIn the same issue of \"Annals of Mathematics\" and immediately after Haar's paper, the Haar theorem was used to solve Hilbert's fifth problem for compact groups by John von Neumann.\n\nUnless formula_3 is a discrete group, it is impossible to define a countably additive left-invariant regular measure on \"all\" subsets of formula_3, assuming the axiom of choice, according to the theory of non-measurable sets.\n\nThe Haar measures are used in harmonic analysis on locally compact groups, particularly in the theory of Pontryagin duality. To prove the existence of a Haar measure on a locally compact group formula_3 it suffices to exhibit a left-invariant Radon measure on formula_3.\n\nIn mathematical statistics, Haar measures are used for prior measures, which are prior probabilities for compact groups of transformations. These prior measures are used to construct admissible procedures, by appeal to the characterization of admissible procedures as Bayesian procedures (or limits of Bayesian procedures) by Wald. For example, a right Haar measure for a family of distributions with a location parameter results in the Pitman estimator, which is best equivariant. When left and right Haar measures differ, the right measure is usually preferred as a prior distribution. For the group of affine transformations on the parameter space of the normal distribution, the right Haar measure is the Jeffreys prior measure. Unfortunately, even right Haar measures sometimes result in useless priors, which cannot be recommended for practical use, like other methods of constructing prior measures that avoid subjective information.\n\nAnother use of Haar measure in statistics is in conditional inference, in which the sampling distribution of a statistic is conditioned on another statistic of the data. In invariant-theoretic conditional inference, the sampling distribution is conditioned on an invariant of the group of transformations (with respect to which the Haar measure is defined). The result of conditioning sometimes depends on the order in which invariants are used and on the choice of a maximal invariant, so that by itself a statistical principle of invariance fails to select any unique best conditional statistic (if any exist); at least another principle is needed.\n\nFor non-compact groups, statisticians have extended Haar-measure results using amenable groups.\n\nIn 1936 Weil proved a converse (of sorts) to Haar's theorem, by showing that if a group has a left invariant measure for which one can define a convolution product, then one can define a topology on the group, and the completion of the group is locally compact and the given measure is essentially the same as Haar measure on this completion.\n\n\n\n"}
{"id": "44148118", "url": "https://en.wikipedia.org/wiki?curid=44148118", "title": "Incidence (graph)", "text": "Incidence (graph)\n\nIn graph theory, an incidence is a pair formula_1 where formula_2 is a vertex and formula_3 is an edge incident to formula_2. \n\nTwo distinct incidences formula_1 and formula_6 are adjacent if and only if formula_7, formula_8 or formula_9 or formula_10. \n\nAn incidence coloring of a graph formula_11 is an assignment of a color to each incidence of G in such a way that adjacent incidences get distinct colors. It is equivalent to a strong edge coloring of the graph obtained by subdivising once each edge of formula_11.\n"}
{"id": "1126638", "url": "https://en.wikipedia.org/wiki?curid=1126638", "title": "Invariant (mathematics)", "text": "Invariant (mathematics)\n\nIn mathematics, an invariant is a property, held by a class of mathematical objects, which remains unchanged when transformations of a certain type are applied to the objects. The particular class of objects and type of transformations are usually indicated by the context in which the term is used. For example, the area of a triangle is an invariant with respect to isometries of the Euclidean plane. The phrases \"invariant under\" and \"invariant to\" a transformation are both used. More generally, an invariant with respect to an equivalence relation is a property that is constant on each equivalence class. \n\nInvariants are used in diverse areas of mathematics such as geometry, topology, algebra and discrete mathematics. Some important classes of transformations are defined by an invariant they leave unchanged, for example conformal maps are defined as transformations of the plane that preserve angles. The discovery of invariants is an important step in the process of classifying mathematical objects.\n\nA simple example of invariance is expressed in our ability to count. For a finite set of objects of any kind, there is a number to which we always arrive, regardless of the order in which we count the objects in the set. The quantity—a cardinal number—is associated with the set, and is invariant under the process of counting.\n\nAn identity is an equation that remains true for all values of its variables. There are also inequalities that remain true when the values of their variables change.\n\nThe distance between two points on a number line is not changed by adding the same quantity to both numbers. On the other hand, multiplication does not have this same property as addition, so distance is not invariant under multiplication.\n\nAngles and ratios of distances are invariant under scalings, rotations, translations and reflections. These transformations produce similar shapes, which is the basis of trigonometry. In contrast, angles and ratios are not invariant under non-uniform scaling (such as stretching). The sum of a triangle's interior angles (180°) is invariant under all the above operations. As another example, all circles are similar: they can be transformed into each other and the ratio of the circumference to the diameter is invariant (denoted by the Greek letter pi).\n\nSome more complicated examples:\n\nThe MU puzzle is a good example of a logical problem where determining an invariant is of use for an impossibility proof. \nThe puzzle asks one to start with the word MI and transform it into the word MU using in each step one of the following transformation rules:\n\n\nAn example derivation (superscripts indicating the applied rules) is\n\nIs it possible to convert MI into MU using these four transformation rules only?\n\nOne could spend many hours applying these transformation rules to strings. However, it might be quicker to find a property that is invariant to all rules (i.e. that isn't changed by any of them), and demonstrates that getting to MU is impossible. Logically looking at the puzzle, the only way to get rid of any I's is to have three consecutive I's in the string. This makes the following invariant interesting to consider: \n\nThis is an invariant to the problem if for each of the transformation rules the following holds: if the invariant held before applying the rule, it will also hold after applying it. If we look at the net effect of applying the rules on the number of I's and U's we can see this actually is the case for all rules:\n\nThe table above shows clearly that the invariant holds for each of the possible transformation rules, which basically means that whichever rule we pick, at whatever state, if the number of I's was not a multiple of three before applying the rule, it won't be afterwards either.\n\nGiven that there is a single I in the starting string MI, and one is not a multiple of three, it's impossible to go from MI to MU as zero is a multiple of three.\n\nA subset \"S\" of the domain \"U\" of a mapping \"T\": \"U\" → \"U\" is an invariant set under the mapping when formula_2 Note that the elements of \"S\" are not fixed, but rather the set \"S\" is fixed in the power set of \"U\". (Some authors use the terminology \"setwise invariant\" vs. \"pointwise invariant\" to distinguish between these cases.)\nFor example, a circle is an invariant subset of the plane under a rotation about the circle’s center. Further, a conical surface is invariant as a set under a homothety of space.\n\nAn invariant set of an operation \"T\" is also said to be stable under \"T\". For example, the normal subgroups that are so important in group theory are those subgroups that are stable under the inner automorphisms of the ambient group.\nOther examples occur in linear algebra. Suppose a linear transformation \"T\" has an eigenvector v. Then the line through 0 and v is an invariant set under \"T\". The eigenvectors span an invariant subspace which is stable under \"T\".\n\nWhen \"T\" is a screw displacement, the screw axis is an invariant line, though if the pitch is non-zero, \"T\" has no fixed points.\n\nThe notion of invariance is formalized in three different ways in mathematics: via group actions, presentations, and deformation.\n\nFirstly, if one has a group \"G\" acting on a mathematical object (or set of objects) \"X,\" then one may ask which points \"x\" are unchanged, \"invariant\" under the group action, or under an element \"g\" of the group.\n\nVery frequently one will have a group acting on a set \"X\" and ask which objects in an \"associated\" set \"F\"(\"X\") are invariant. For example, rotation in the plane about a point leaves the point about which it rotates invariant, while translation in the plane does not leave any points invariant, but does leave all lines parallel to the direction of translation invariant as lines. Formally, define the set of lines in the plane \"P\" as \"L\"(\"P\"); then a rigid motion of the plane takes lines to lines – the group of rigid motions acts on the set of lines – and one may ask which lines are unchanged by an action.\n\nMore importantly, one may define a \"function\" on a set, such as \"radius of a circle in the plane\" and then ask if this function is invariant under a group action, such as rigid motions.\n\nDual to the notion of invariants are \"coinvariants,\" also known as \"orbits,\" which formalizes the notion of congruence: objects which can be taken to each other by a group action. For example, under the group of rigid motions of the plane, the perimeter of a triangle is an invariant, while the set of triangles congruent to a given triangle is a coinvariant.\n\nThese are connected as follows: invariants are constant on coinvariants (for example, congruent triangles have the same perimeter), while two objects which agree in the value of one invariant may or may not be congruent (two triangles with the same perimeter need not be congruent). In classification problems, one seeks to find a complete set of invariants, such that if two objects have the same values for this set of invariants, they are congruent.\n\nFor example, triangles such that all three sides are equal are congruent under rigid motions, via SSS congruence, and thus the lengths of all three sides form a complete set of invariants for triangles. The three angle measures of a triangle are also invariant under rigid motions, but do not form a complete set as incongruent triangles can share the same angle measures. However, if one allows scaling in addition to rigid motions, then the AAA similarity criterion shows that this is a complete set of invariants.\n\nSecondly, a function may be defined in terms of some presentation or decomposition of a mathematical object; for instance, the Euler characteristic of a cell complex is defined as the alternating sum of the number of cells in each dimension. One may forget the cell complex structure and look only at the underlying topological space (the manifold) – as different cell complexes give the same underlying manifold, one may ask if the function is \"independent\" of choice of \"presentation,\" in which case it is an \"intrinsically\" defined invariant. This is the case for the Euler characteristic, and a general method for defining and computing invariants is to define them for a given presentation and then show that they are independent of the choice of presentation. Note that there is no notion of a group action in this sense.\n\nThe most common examples are:\n\nThirdly, if one is studying an object which varies in a family, as is common in algebraic geometry and differential geometry, one may ask if the property is unchanged under perturbation – if an object is constant on families or invariant under change of metric, for instance.\n\nIn computer science, one can encounter invariants that can be relied upon to be true during the execution of a program, or during some portion of it. It is a logical assertion that is always held to be true during a certain phase of execution. For example, a loop invariant is a condition that is true at the beginning and end of every execution of a loop.\n\nInvariants are especially useful when reasoning about whether a computer program is correct. The theory of optimizing compilers, the methodology of design by contract, and formal methods for determining program correctness, all rely heavily on invariants.\n\nProgrammers often use assertions in their code to make invariants explicit. Some object oriented programming languages have a special syntax for specifying class invariants.\n\nAbstract interpretation tools can compute simple invariants of given imperative computer programs. The kind of properties that can be found depend on the abstract domains used. Typical example properties are single integer variable ranges like codice_1, relations between several variables like codice_2, and modulus information like codice_3. Academic research prototypes also consider simple properties of pointer structures.\n\nMore sophisticated invariants generally have to be provided manually.\nIn particular, when verifying an imperative program using the Hoare calculus, a loop invariant has to be provided manually for each loop in the program, which is one of the reasons this is generally impractical for most programs.\n\nIn the context of the above MU puzzle example, there is currently no general automated tool that can detect that a derivation, \"MI →...→ MU\", is impossible only from rules 1-4. However, once the abstraction from the string to the number of its \"I\"s has been made by hand, leading e.g. to the following C program, an abstract interpretation tool will be able to detect that codice_4 can't be 0 and hence the \"while\"-loop will never terminate.\n\n\n\n"}
{"id": "16974", "url": "https://en.wikipedia.org/wiki?curid=16974", "title": "Knapsack problem", "text": "Knapsack problem\n\nThe knapsack problem or rucksack problem is a problem in combinatorial optimization: Given a set of items, each with a weight and a value, determine the number of each item to include in a collection so that the total weight is less than or equal to a given limit and the total value is as large as possible. It derives its name from the problem faced by someone who is constrained by a fixed-size knapsack and must fill it with the most valuable items.\n\nThe problem often arises in resource allocation where there are financial constraints and is studied in fields such as combinatorics, computer science, complexity theory, cryptography, applied mathematics, and daily fantasy sports.\n\nThe knapsack problem has been studied for more than a century, with early works dating as far back as 1897. The name \"knapsack problem\" dates back to the early works of mathematician Tobias Dantzig (1884–1956), and refers to the commonplace problem of packing the most valuable or useful items without overloading the luggage.\n\nA 1998 study of the Stony Brook University Algorithm Repository showed that, out of 75 algorithmic problems, the knapsack problem was the 19th most popular and the third most needed after suffix trees and the bin packing problem.\n\nKnapsack problems appear in real-world decision-making processes in a wide variety of fields, such as finding the least wasteful way to cut raw materials, selection of investments and portfolios, selection of assets for asset-backed securitization, and generating keys for the Merkle–Hellman and other knapsack cryptosystems.\n\nOne early application of knapsack algorithms was in the construction and scoring of tests in which the test-takers have a choice as to which questions they answer. For small examples, it is a fairly simple process to provide the test-takers with such a choice. For example, if an exam contains 12 questions each worth 10 points, the test-taker need only answer 10 questions to achieve a maximum possible score of 100 points. However, on tests with a heterogeneous distribution of point values, it is more difficult to provide choices. Feuerman and Weiss proposed a system in which students are given a heterogeneous test with a total of 125 possible points. The students are asked to answer all of the questions to the best of their abilities. Of the possible subsets of problems whose total point values add up to 100, a knapsack algorithm would determine which subset gives each student the highest possible score.\n\nThe most common problem being solved is the 0-1 knapsack problem, which restricts the number \"formula_1\" of copies of each kind of item to zero or one. Given a set of \"n\" items numbered from 1 up to \"n\", each with a weight \"formula_2\" and a value \"formula_3\", along with a maximum weight capacity \"W\",\nHere \"x\" represents the number of instances of item \"i\" to include in the knapsack. Informally, the problem is to maximize the sum of the values of the items in the knapsack so that the sum of the weights is less than or equal to the knapsack's capacity.\n\nThe bounded knapsack problem (BKP) removes the restriction that there is only one of each item, but restricts the number formula_1 of copies of each kind of item to a maximum non-negative integer value formula_8:\n\nThe unbounded knapsack problem (UKP) places no upper bound on the number of copies of each kind of item and can be formulated as above except for that the only restriction on formula_1 is that it is a non-negative integer.\n\nOne example of the unbounded knapsack problem is given using the figure shown at the beginning of this article and the text \"if any number of each box is available\" in the caption of that figure.\n\nThe knapsack problem is interesting from the perspective of computer science for many reasons:\n\nThere is a link between the \"decision\" and \"optimization\" problems in that if there exists a polynomial algorithm that solves the \"decision\" problem, then one can find the maximum value for the optimization problem in polynomial time by applying this algorithm iteratively while increasing the value of k . On the other hand, if an algorithm finds the optimal value of the optimization problem in polynomial time, then the decision problem can be solved in polynomial time by comparing the value of the solution output by this algorithm with the value of k . Thus, both versions of the problem are of similar difficulty.\n\nOne theme in research literature is to identify what the \"hard\" instances of the knapsack problem look like, or viewed another way, to identify what properties of instances in practice might make them more amenable than their worst-case NP-complete behaviour suggests. The goal in finding these \"hard\" instances is for their use in public key cryptography systems, such as the Merkle-Hellman knapsack cryptosystem.\n\nSeveral algorithms are available to solve knapsack problems, based on dynamic programming approach, branch and bound approach or hybridizations of both approaches.\n\nThe unbounded knapsack problem (UKP) places no restriction on the number of copies of each kind of item. Besides, here we assume that formula_16\n\nObserve that formula_20 has the following properties:\n\n1. formula_21 (the sum of zero items, i.e., the summation of the empty set).\n\n2. formula_22\n, formula_23, where formula_3 is the value of the formula_25-th kind of item.\n\nThe second property needs to be explained in detail. During the process of the running of this method, how do we get the weight formula_26? There are only formula_25 ways and the previous weights are formula_28 where there are total formula_25 kinds of different item(by saying different, we mean that the weight and the value are not completely the same). If we know each value of these formula_25 items and the related maximum value previously, we just compare them to each other and get the maximum value ultimately and we are done.\n\nHere the maximum of the empty set is taken to be zero. Tabulating the results from formula_31 up through formula_32 gives the solution. Since the calculation of each formula_20 involves examining at most formula_34 items, and there are at most formula_35 values of formula_20 to calculate, the running time of the dynamic programming solution is formula_37. Dividing formula_38 by their greatest common divisor is a way to improve the running time.\n\nThe formula_37 complexity does not contradict the fact that the knapsack problem is NP-complete, since formula_35, unlike formula_34, is not polynomial in the length of the input to the problem. The length of the formula_35 input to the problem is proportional to the number of bits in formula_35, formula_44, not to formula_35 itself.\n\nA similar dynamic programming solution for the 0/1 knapsack problem also runs in pseudo-polynomial time. Assume formula_46 are strictly positive integers. Define formula_47 to be the maximum value that can be attained with weight less than or equal to formula_26 using items up to formula_25 (first formula_25 items).\n\nWe can define formula_47 recursively as follows: (Definition A)\n\n\nThe solution can then be found by calculating formula_57. To do this efficiently, we can use a table to store previous computations.\n\nThe following is pseudo code for the dynamic program:\n\nThis solution will therefore run in formula_37 time and formula_37 space.\n\nHowever, if we take it a step or two further, we should know that the method will run in the time between formula_37 and formula_61. From Definition A, we can know that there is no need for computing all the weights when the number of items and the items themselves that we chose are fixed. That is to say, the program above computes more than expected because that the weight changes from 0 to W all the time. All we need to do is to compare m[i-1, j] and m[i-1, j-w[i]] + v[i] for m[i, j], and when m[i-1, j-w[i]] is out of range, we just give the value of m[i-1, j] to m[i, j]. From this perspective, we can program this method so that it runs recursively!\n\nFor example, there are 10 different items and the weight limit is 67. So,\nIf you use above method to compute for formula_63, you will get:\n\nBesides, we can break the recursion and convert it into a tree. Then we can cut some leaves and use parallel computing to expedite the running of this method!\n\nAnother algorithm for 0-1 knapsack, discovered in 1974 and sometimes called \"meet-in-the-middle\" due to parallels to a similarly named algorithm in cryptography, is exponential in the number of different items but may be preferable to the DP algorithm when formula_35 is large compared to \"n\". In particular, if the formula_2 are nonnegative but not integers, we could still use the dynamic programming algorithm by scaling and rounding (i.e. using fixed-point arithmetic), but if the problem requires formula_67 fractional digits of precision to arrive at the correct answer, formula_35 will need to be scaled by formula_69, and the DP algorithm will require formula_70 space and formula_71 time.\n\nThe algorithm takes formula_72 space, and efficient implementations of step 3 (for instance, sorting the subsets of B by weight, discarding subsets of B which weigh more than other subsets of B of greater or equal value, and using binary search to find the best match) result in a runtime of formula_73. As with the meet in the middle attack in cryptography, this improves on the formula_74 runtime of a naive brute force approach (examining all subsets of formula_75), at the cost of using exponential rather than constant space (see also baby-step giant-step).\n\nAs for most NP-complete problems, it may be enough to find workable solutions even if they are not optimal. Preferably, however, the approximation comes with a guarantee on the difference between the value of the solution found and the value of the optimal solution.\n\nAs with many useful but computationally complex algorithms, there has been substantial research on creating and analyzing algorithms that approximate a solution. The knapsack problem, though NP-Hard, is one of a collection of algorithms that can still be approximated to any specified degree. This means that the problem has a polynomial time approximation scheme. To be exact, the knapsack problem has a fully polynomial time approximation scheme (FPTAS).\n\nGeorge Dantzig proposed a greedy approximation algorithm to solve the unbounded knapsack problem. His version sorts the items in decreasing order of value per unit of weight, formula_76. It then proceeds to insert them into the sack, starting with as many copies as possible of the first kind of item until there is no longer space in the sack for more. Provided that there is an unlimited supply of each kind of item, if formula_77 is the maximum value of items that fit into the sack, then the greedy algorithm is guaranteed to achieve at least a value of formula_78. However, for the bounded problem, where the supply of each kind of item is limited, the algorithm may be far from optimal.\n\nThe fully polynomial time approximation scheme (FPTAS) for the knapsack problem takes advantage of the fact that the reason the problem has no known polynomial time solutions is because the profits associated with the items are not restricted. If one rounds off some of the least significant digits of the profit values then they will be bounded by a polynomial and 1/ε where ε is a bound on the correctness of the solution. This restriction then means that an algorithm can find a solution in polynomial time that is correct within a factor of (1-ε) of the optimal solution.\n\nTheorem: The set formula_85 computed by the algorithm above satisfies formula_86, where formula_87 is an optimal solution.\n\nSolving the unbounded knapsack problem can be made easier by throwing away items which will never be needed. For a given item formula_25, suppose we could find a set of items formula_89 such that their total weight is less than the weight of formula_25, and their total value is greater than the value of formula_25. Then formula_25 cannot appear in the optimal solution, because we could always improve any potential solution containing formula_25 by replacing formula_25 with the set formula_89. Therefore, we can disregard the formula_25-th item altogether. In such cases, formula_89 is said to dominate formula_25. (Note that this does not apply to bounded knapsack problems, since we may have already used up the items in formula_89.)\n\nFinding dominance relations allows us to significantly reduce the size of the search space. There are several different types of dominance relations, which all satisfy an inequality of the form:\n\nformula_100, and formula_101 for some formula_102\n\nwhere\nformula_103 and formula_104. The vector formula_105 denotes the number of copies of each member of formula_89.\n\n\nThere are many variations of the knapsack problem that have arisen from the vast number of applications of the basic problem. The main variations occur by changing the number of some problem parameter such as the number of items, number of objectives, or even the number of knapsacks.\n\nThis variation changes the goal of the individual filling the knapsack. Instead of one objective, such as maximizing the monetary profit, the objective could have several dimensions. For example, there could be environmental or social concerns as well as economic goals. Problems frequently addressed include portfolio and transportation logistics optimizations.\n\nAs an example, suppose you ran a cruise ship. You have to decide how many famous comedians to hire. This boat can handle no more than one ton of passengers and the entertainers must weigh less than 1000 lbs. Each comedian has a weight, brings in business based on their popularity and asks for a specific salary. In this example you have multiple objectives. You want, of course, to maximize the popularity of your entertainers while minimizing their salaries. Also, you want to have as many entertainers as possible.\n\nIn this variation, the weight of knapsack item formula_25 is given by a D-dimensional vector formula_154 and the knapsack has a D-dimensional capacity vector formula_155. The target is to maximize the sum of the values of the items in the knapsack so that the sum of weights in each dimension formula_67 does not exceed formula_157.\n\nMulti-dimensional knapsack is computationally harder than knapsack; even for formula_158, the problem does not have EPTAS unless Pformula_159NP. However, the algorithm in is shown to solve sparse instances efficiently. An instance of multi-dimensional knapsack is sparse if there is a set formula_160 for formula_161 such that for every knapsack item formula_25, formula_163 such that formula_164 and formula_165. Such instances occur, for example, when scheduling packets in a wireless network with relay nodes. The algorithm from also solves sparse instances of the multiple choice variant, multiple-choice multi-dimensional knapsack.\n\nThe IHS (Increasing Height Shelf) algorithm is optimal for 2D knapsack (packing squares into a two-dimensional unit size square): when there are at most five square in an optimal packing.\n\nThis variation is similar to the Bin Packing Problem. It differs from the Bin Packing Problem in that a subset of items can be selected, whereas, in the Bin Packing Problem, all items have to be packed to certain bins. The concept is that there are multiple knapsacks. This may seem like a trivial change, but it is not equivalent to adding to the capacity of the initial knapsack. This variation is used in many loading and scheduling problems in Operations Research and has a Polynomial-time approximation scheme.\n\nAs described by Wu et al.:\n\nThe quadratic knapsack problem (QKP) maximizes a quadratic objective function subject to a binary and linear capacity constraint.\n\nThe quadratic knapsack problem was discussed under that title by Gallo, Hammer, and Simeone in 1980. However, Gallo and Simeone attribute the first treatment of the problem to Witzgall in 1975.\n\nThe subset sum problem is a special case of the decision and 0-1 problems where each kind of item, the weight equals the value: formula_166. In the field of cryptography, the term \"knapsack problem\" is often used to refer specifically to the subset sum problem and is commonly known as one of Karp's 21 NP-complete problems.\n\nThe generalization of subset sum problem is called multiple subset-sum problem, in which multiple bins exist with the same capacity. It has been shown that the generalization does not have an FPTAS.\n\n\n\n\n"}
{"id": "44092279", "url": "https://en.wikipedia.org/wiki?curid=44092279", "title": "Kyoto Prize in Basic Sciences", "text": "Kyoto Prize in Basic Sciences\n\nThe Kyoto Prize in Basic Sciences is awarded once a year by the Inamori Foundation. The Prize is one of three Kyoto Prize categories; the others are the Kyoto Prize in Advanced Technology and the Kyoto Prize in Arts and Philosophy. The first Kyoto Prize in Basic Sciences was awarded to Claude Elwood Shannon, the “Establishment of Mathematical Foundation of Information Theory”. The Prize is widely regarded as the most prestigious award available in fields which are traditionally not honored with a Nobel Prize.\n\nThe Kyoto Prize in Basic Sciences is awarded on a rotating basis to researchers in the following four fields: \n\n"}
{"id": "38234611", "url": "https://en.wikipedia.org/wiki?curid=38234611", "title": "Laguerre plane", "text": "Laguerre plane\n\nIn mathematics, a Laguerre plane is one of the Benz planes: Möbius plane, Laguerre plane and Minkowski plane, named after the French mathematician Edmond Nicolas Laguerre.\n\nEssentially the classical Laguerre plane is an incidence structure which describes the incidence behaviour of the curves formula_1 , i.e. parabolas and lines, in the real affine plane. In order to simplify the structure, to any curve formula_1 the point formula_3 is added. A further advantage of this completion is: The plane geometry of the completed parabolas/lines is isomorphic to the geometry of the plane sections of a cylinder (s. below).\n\nOriginally the classical Laguerre plane was defined as the geometry of the oriented lines and circles in the real euclidean plane (see ). Here we prefer the parabola model of the classical Laguerre plane.\n\nWe define:\n\nformula_4 the set of points,\nformula_5 the set of cycles.\n\nThe incidence structure formula_6 is called classical Laguerre plane.\n\nThe point set is formula_7 plus a copy of formula_8 (see figure). Any parabola/line formula_9 gets the additional point formula_3.\n\nPoints with the same x-coordinate cannot be connected by curves formula_9. Hence we define:\n\nTwo points formula_12 are parallel (formula_13)\nif formula_14 or there is no cycle containing formula_15 and formula_16.\n\nFor the description of the classical real Laguerre plane above two points formula_17 are parallel if and only if formula_18. formula_19 is an equivalence relation, similar to the parallelity of lines.\n\nThe incidence structure formula_6 has the following properties:\n\nLemma:\n\nSimilar to the sphere model of the classical Moebius plane there is a cylinder model for the classical Laguerre plane:\n\nformula_6 is isomorphic to the geometry of plane sections of a circular cylinder in formula_39 .\n\nThe following mapping formula_40 is a projection with center formula_41 that maps the x-z-plane onto the cylinder with the equation formula_42, axis formula_43 and radius formula_44\n\nThe Lemma above gives rise to the following definition:\n\nLet formula_55 be an incidence structure with point set formula_56 and set of cycles formula_57. \nTwo points formula_12 are parallel (formula_13) if formula_14 or there is no cycle containing formula_15 and formula_16.\nformula_63 is called Laguerre plane if the following axioms hold:\n\nFour points formula_81 are concyclic if there is a cycle formula_22 with formula_83.\n\nFrom the definition of relation formula_19 and axiom B2 we get\n\nLemma:\nRelation formula_19 is an equivalence relation.\n\nFollowing the cylinder model of the classical Laguerre-plane we introduce the denotation:\n\na) For formula_86 we set formula_87.\nb) An equivalence class formula_88 is called generator.\n\nFor the classical Laguerre plane a generator is a line parallel to the y-axis (plane model) or a line on the cylinder (space model).\n\nThe connection to linear geometry is given by the following definition:\n\nFor a Laguerre plane formula_55 we define the local structure\n\nand call it the residue at point P.\n\nIn the plane model of the classical Laguerre plane formula_91 is the real affine plane formula_7.\nIn general we get\n\nTheorem: Any residue of a Laguerre plane is an affine plane.\n\nAnd the equivalent definition of a Laguerre plane:\n\nTheorem:\nAn incidence structure together with an equivalence relation formula_19 on formula_94 is a\nLaguerre plane if and only if for any point formula_24 the residue formula_96 is an affine plane.\n\nThe following incidence structure is a minimal model of a Laguerre plane:\nHence formula_100 and formula_101\n\nFor finite Laguerre planes, i.e. formula_102, we get:\n\nLemma:\nFor any cycles formula_103 and any generator formula_88 of a finite Laguerre plane\nformula_105 we have:\n\nFor a finite Laguerre plane formula_105 and a cycle formula_108 the integer formula_109 is called order of formula_63.\n\nFrom combinatorics we get\n\nLemma:\nLet formula_105 be a Laguerre—plane of order formula_112. Then\n\nUnlike Moebius planes the formal generalization of the classical model of a Laguerre plane, i.e. replacing formula_117 by an arbitrary field formula_118, leads in any case to an example of a Laguerre plane.\n\nTheorem:\nFor a field formula_119 and\n\nSimilar to a Möbius plane the Laguerre version of the Theorem of Miquel holds:\nTheorem of MIQUEL:\nFor the Laguerre plane formula_126 the following is true:\n\nThe importance of the Theorem of Miquel shows the following theorem which is due to v. d. Waerden, Smid and Chen:\n\nTheorem: Only a Laguerre plane formula_126 satisfies the theorem of Miquel.\n\nBecause of the last Theorem formula_126 is called a miquelian Laguerre plane.\n\nRemark: The minimal model of a Laguerre plane is miquelian.\n\nRemark: A suitable stereographic projection shows: formula_126 is isomorphic to the geometry of the plane sections on a quadric cylinder over field formula_119.\n\nThere are a lot of Laguerre planes which are not miquelian (s. weblink below). The class which is most similar to miquelian Laguerre planes are the ovoidal Laguerre planes. An ovoidal Laguerre plane is the geometry of the plane sections of a cylinder which is constructed by using an oval instead of a non degenerate conic. An oval is a quadratic set and bears the same geometric properties as a non degenerate conic in a projective plane: 1) a line intersects an oval in zero, one, or two points and 2) at any point there is a unique tangent. A simple oval in the real plane can be constructed by glueing together two suitable halves of different ellipses, such that the result is not a conic. Even in the finite case there exist ovals (see quadratic set).\n\n"}
{"id": "690686", "url": "https://en.wikipedia.org/wiki?curid=690686", "title": "List edge-coloring", "text": "List edge-coloring\n\nIn mathematics, list edge-coloring is a type of graph coloring that combines list coloring and edge coloring.\nAn instance of a list edge-coloring problem consists of a graph together with a list of allowed colors for each edge. A list edge-coloring is a choice of a color for each edge, from its list of allowed colors; a coloring is proper if no two adjacent edges receive the same color.\n\nA graph \"G\" is \"k\"-edge-choosable if every instance of list edge-coloring that has \"G\" as its underlying graph and that provides at least \"k\" allowed colors for each edge of \"G\" has a proper coloring.\nThe edge choosability, or \"list edge colorability\", \"list edge chromatic number\", or \"list chromatic index\", ch′(\"G\") of graph \"G\" is the least number \"k\" such that \"G\" is \"k\"-edge-choosable. It is conjectured that it always equals the chromatic index.\n\nSome properties of ch′(\"G\"):\nHere χ′(\"G\") is the chromatic index of \"G\"; and K, the complete bipartite graph with equal partite sets.\n\nThe most famous open problem about list edge-coloring is probably the \"list coloring conjecture\".\n\nThis conjecture has a fuzzy origin; overview its history. The Dinitz conjecture, proven by , is the special case of the list coloring conjecture for the complete bipartite graphs K.\n\n"}
{"id": "20928503", "url": "https://en.wikipedia.org/wiki?curid=20928503", "title": "List of things named after Alfred Tarski", "text": "List of things named after Alfred Tarski\n\nIn the history of mathematics, Alfred Tarski (1901–1983) is one of the most important logicians. His name is now associated with a number of theorems and concepts in that field.\n\n\n\n"}
{"id": "1678115", "url": "https://en.wikipedia.org/wiki?curid=1678115", "title": "Mantel test", "text": "Mantel test\n\nThe Mantel test, named after Nathan Mantel, is a statistical test of the correlation between two matrices. The matrices must be of the same dimension; in most applications, they are matrices of interrelations between the same vectors of objects. The test was first published by Nathan Mantel, a biostatistician at the National Institutes of Health, in 1967. Accounts of it can be found in advanced statistics books (e.g., Sokal & Rohlf 1995).\n\nThe test is commonly used in ecology, where the data are usually estimates of the \"distance\" between objects such as species of organisms. For example, one matrix might contain estimates of the genetic distances (i.e., the amount of difference between two different genomes) between all possible pairs of species in the study, obtained by the methods of molecular systematics; while the other might contain estimates of the geographical distance between the ranges of each species to every other species.\n\nIf there are \"n\" objects, and the matrix is symmetrical (so the distance from object \"a\" to object \"b\" is the same as the distance from \"b\" to \"a\") such a matrix contains \n\ndistances. Because distances are not independent of each other – since changing the \"position\" of one object would change formula_2 of these distances (the distance from that object to each of the others) – we can not assess the relationship between the two matrices by simply evaluating the correlation coefficient between the two sets of distances and testing its statistical significance. The Mantel test deals with this problem.\n\nThe procedure adopted is a kind of randomization or permutation test. The correlation between the two sets of formula_3 distances is calculated, and this is both the measure of correlation reported and the test statistic on which the test is based. In principle, any correlation coefficient could be used, but normally the Pearson product-moment correlation coefficient is used. \n\nIn contrast to the ordinary use of the correlation coefficient, to assess significance of any apparent departure from a zero correlation, the rows and columns of one of the matrices are subjected to random permutations many times, with the correlation being recalculated after each permutation. The significance of the observed correlation is the proportion of such permutations that lead to a higher correlation coefficient. \n\nThe reasoning is that if the null hypothesis of there being no relation between the two matrices is true, then permuting the rows and columns of the matrix should be equally likely to produce a larger or a smaller coefficient. In addition to overcoming the problems arising from the statistical dependence of elements within each of the two matrices, use of the permutation test means that no reliance is being placed on assumptions about the statistical distributions of elements in the matrices. \n\nMany statistical packages include routines for carrying out the Mantel test.\n\nThe various papers introducing the Mantel test and its extension the partial Mantel test lack a clear statistical framework specifying fully the null and alternative hypotheses. This may convey the wrong idea that these tests are universal. For example, the Mantel and partial Mantel tests can be flawed in the presence of spatial auto-correlation and return erroneously low p-values\nSee e.g. Guillot and Rousset, 2013 )\n\n\n"}
{"id": "236801", "url": "https://en.wikipedia.org/wiki?curid=236801", "title": "Markov chain Monte Carlo", "text": "Markov chain Monte Carlo\n\nformula_1\n\nIn statistics, Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a Markov chain that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by observing the chain after a number of steps. The more steps there are, the more closely the distribution of the sample matches the actual desired distribution.\n\nRandom-walk Monte Carlo methods make up a large subclass of Markov chain Monte Carlo methods.\n\nMarkov chain Monte Carlo methods are primarily used for calculating numerical approximations of multi-dimensional integrals, for example in Bayesian statistics, computational physics, computational biology and computational linguistics.\n\nIn Bayesian statistics, the recent development of Markov chain Monte Carlo methods has been a key step in making it possible to compute large hierarchical models that require integrations over hundreds or even thousands of unknown parameters.\n\nIn rare event sampling, they are also used for generating samples that gradually populate the rare failure region.\n\nWhen a Markov chain Monte Carlo method is used for approximating a multi-dimensional integral, an ensemble of \"walkers\" move around randomly. At each point where a walker steps, the integrand value at that point is counted towards the integral. The walker then may make a number of tentative steps around the area, looking for a place with a reasonably high contribution to the integral to move into next.\n\nRandom walk Monte Carlo methods are a kind of random simulation or Monte Carlo method. However, whereas the random samples of the integrand used in a conventional Monte Carlo integration are statistically independent, those used in Markov chain Monte Carlo methods are \"correlated\". A Markov chain is constructed in such a way as to have the integrand as its equilibrium distribution.\n\nExamples of random walk Monte Carlo methods include the following:\n\n\nUnlike most of the current Markov chain Monte Carlo methods that ignore the previous trials, using a new algorithm the Markov chain Monte Carlo algorithm is able to use the previous steps and generate the next candidate. This training-based algorithm is able to speed-up the Markov chain Monte Carlo algorithm by an order of magnitude.\n\nInteracting Markov chain Monte Carlo methodologies are a class of mean field particle methods for obtaining random samples from a sequence of probability distributions with an increasing level of sampling complexity. These probabilistic models include path space state models with increasing time horizon, posterior distributions w.r.t. sequence of partial observations, increasing constraint level sets for conditional distributions, decreasing temperature schedules associated with some Boltzmann-Gibbs distributions, and many others. In principle, any Markov chain Monte Carlo sampler can be turned into an interacting Markov chain Monte Carlo sampler. These interacting Markov chain Monte Carlo samplers can be interpreted as a way to run in parallel a sequence of Markov chain Monte Carlo samplers. For instance, interacting simulated annealing algorithms are based on independent Metropolis-Hastings moves interacting sequentially with a selection-resampling type mechanism. In contrast to traditional Markov chain Monte Carlo methods, the precision parameter of this class of interacting Markov chain Monte Carlo samplers is \"only\" related to the number of interacting Markov chain Monte Carlo samplers. These advanced particle methodologies belong to the class of Feynman-Kac particle models, also called Sequential Monte Carlo or particle filter methods in Bayesian inference and signal processing communities. Interacting Markov chain Monte Carlo methods can also be interpreted as a mutation-selection genetic particle algorithm with Markov chain Monte Carlo mutations.\n\nMarkov Chain quasi-Monte Carlo (MCQMC)\nThe advantage of low-discrepancy sequences in lieu of random numbers for simple independent Monte Carlo sampling is well known. This procedure, known as Quasi-Monte Carlo method (QMC), yields an integration error that decays at a superior rate to that obtained by IID sampling, by the Koksma-Hlawka inequality. Empirically it allows the reduction of both estimation error and convergence time by an order of magnitude . \nThe Array-RQMC method combines randomized quasi-Monte Carlo and Markov chain simulation by simulating formula_2 chains simultaneously in a way that the empirical distribution of the formula_2 states at any given step is a better approximation of the true distribution of the chain than with ordinary MCMC. In empirical experiments, the variance of the average of a function of the state sometimes converges at rate formula_4 or even faster, instead of the formula_5 Monte Carlo rate. \n\nMore sophisticated methods use various ways of reducing the correlation between successive samples. These algorithms may be harder to implement, but they usually exhibit faster convergence (i.e. fewer steps for an accurate result).\n\nExamples of non-random walk Markov chain Monte Carlo methods include the following:\n\n\nUsually it is not hard to construct a Markov chain with the desired properties. The more difficult problem is to determine how many steps are needed to converge to the stationary distribution within an acceptable error. A good chain will have rapid mixing: the stationary distribution is reached quickly starting from an arbitrary position. A standard empirical method to assess convergence is to run several independent simulated Markov chains and check that the ratio of inter-chain to intra-chain variances for all the parameters sampled is close to 1.\n\nTypically, Markov chain Monte Carlo sampling can only approximate the target distribution, as there is always some residual effect of the starting position. More sophisticated Markov chain Monte Carlo-based algorithms such as coupling from the past can produce exact samples, at the cost of additional computation and an unbounded (though finite in expectation) running time.\n\nMany random walk Monte Carlo methods move around the equilibrium distribution in relatively small steps, with no tendency for the steps to proceed in the same direction. These methods are easy to implement and analyze, but unfortunately it can take a long time for the walker to explore all of the space. The walker will often double back and cover ground already covered.\n\nSeveral software programs provide MCMC sampling capabilities, for example:\n\n\n\n"}
{"id": "4915939", "url": "https://en.wikipedia.org/wiki?curid=4915939", "title": "Midy's theorem", "text": "Midy's theorem\n\nIn mathematics, Midy's theorem, named after French mathematician E. Midy, is a statement about the decimal expansion of fractions \"a\"/\"p\" where \"p\" is a prime and \"a\"/\"p\" has a repeating decimal expansion with an even period . If the period of the decimal representation of \"a\"/\"p\" is 2\"n\", so that\n\nthen the digits in the second half of the repeating decimal period are the 9s complement of the corresponding digits in its first half. In other words,\n\nFor example,\n\nIf \"k\" is any divisor of the period of the decimal expansion of \"a\"/\"p\" (where \"p\" is again a prime), then Midy's theorem can be generalised as follows. The extended Midy's theorem states that if the repeating portion of the decimal expansion of \"a\"/\"p\" is divided into \"k\"-digit numbers, then their sum is a multiple of 10 − 1.\n\nFor example, \nhas a period of 18. Dividing the repeating portion into 6-digit numbers and summing them gives \nSimilarly, dividing the repeating portion into 3-digit numbers and summing them gives \n\nMidy's theorem and its extension do not depend on special properties of the decimal expansion, but work equally well in any base \"b\", provided we replace 10 − 1 with \"b\" − 1 and carry out addition in base \"b\".\n\nFor example, in octal\n\nIn duodecimal (using inverted two and three for ten and eleven, respectively)\n\nShort proofs of Midy's theorem can be given using results from group theory. However, it is also possible to prove Midy's theorem using elementary algebra and modular arithmetic:\n\nLet \"p\" be a prime and \"a\"/\"p\" be a fraction between 0 and 1. Suppose the expansion of \"a\"/\"p\" in base \"b\" has a period of \"ℓ\", so\n\nwhere \"N\" is the integer whose expansion in base \"b\" is the string \"a\"\"a\"...\"a\".\n\nNote that \"b\" − 1 is a multiple of \"p\" because (\"b\" − 1)\"a\"/\"p\" is an integer. Also \"b\"−1 is \"not\" a multiple of \"p\" for any value of \"n\" less than \"ℓ\", because otherwise the repeating period of \"a\"/\"p\" in base \"b\" would be less than \"ℓ\".\n\nNow suppose that \"ℓ\" = \"hk\". Then \"b\" − 1 is a multiple of \"b\" − 1. (To see this, substitute \"x\" for \"b\"; then \"b\" = \"x\" and \"x\" − 1 is a factor of \"x\" − 1. ) Say \"b\" − 1 = \"m\"(\"b\" − 1), so\n\nBut \"b\" − 1 is a multiple of \"p\"; \"b\" − 1 is \"not\" a multiple of \"p\" (because \"k\" is less than \"ℓ\" ); and \"p\" is a prime; so \"m\" must be a multiple of \"p\" and\n\nis an integer. In other words,\n\nNow split the string \"a\"\"a\"...\"a\" into \"h\" equal parts of length \"k\", and let these represent the integers \"N\"...\"N\" in base \"b\", so that\n\nTo prove Midy's extended theorem in base \"b\" we must show that the sum of the \"h\" integers \"N\" is a multiple of \"b\" − 1.\n\nSince \"b\" is congruent to 1 modulo \"b\" − 1, any power of \"b\" will also be congruent to 1 modulo \"b\" − 1. So\n\nwhich proves Midy's extended theorem in base \"b\".\n\nTo prove the original Midy's theorem, take the special case where \"h\" = 2. Note that \"N\" and \"N\" are both represented by strings of \"k\" digits in base \"b\" so both satisfy\n\n\"N\" and \"N\" cannot both equal 0 (otherwise \"a\"/\"p\" = 0) and cannot both equal \"b\" − 1 (otherwise \"a\"/\"p\" = 1), so\n\nand since \"N\" + \"N\" is a multiple of \"b\" − 1, it follows that\n\nFrom the above,\n\nThus formula_23\n\nAnd thus for formula_24\n\nFor formula_26 and is an integer\n\nand so on.\n\n"}
{"id": "19456533", "url": "https://en.wikipedia.org/wiki?curid=19456533", "title": "Minimum distance estimation", "text": "Minimum distance estimation\n\nMinimum distance estimation (MDE) is a statistical method for fitting a mathematical model to data, usually the empirical distribution.\n\nLet formula_1 be an independent and identically distributed (iid) random sample from a population with distribution formula_2 and formula_3.\n\nLet formula_4 be the empirical distribution function based on the sample.\n\nLet formula_5 be an estimator for formula_6. Then formula_7 is an estimator for formula_8.\n\nLet formula_9 be a functional returning some measure of \"distance\" between the two arguments. The functional formula_10 is also called the criterion function.\n\nIf there exists a formula_11 such that formula_12, then formula_5 is called the minimum distance estimate of formula_6.\n\nMost theoretical studies of minimum distance estimation, and most applications, make use of \"distance\" measures which underlie already-established goodness of fit tests: the test statistic used in one of these tests is used as the distance measure to be minimised. Below are some examples of statistical tests that have been used for minimum distance estimation.\n\nThe chi-square test uses as its criterion the sum, over predefined groups, of the squared difference between the increases of the empirical distribution and the estimated distribution, weighted by the increase in the estimate for that group.\n\nThe Cramér–von Mises criterion uses the integral of the squared difference between the empirical and the estimated distribution functions .\n\nThe Kolmogorov–Smirnov test uses the supremum of the absolute difference between the empirical and the estimated distribution functions .\n\nThe Anderson–Darling test is similar to the Cramér–von Mises criterion except that the integral is of a weighted version of the squared difference, where the weighting relates the variance of the empirical distribution function .\n\nThe theory of minimum distance estimation is related to that for the asymptotic distribution of the corresponding statistical goodness of fit tests. Often the cases of the Cramér–von Mises criterion, the Kolmogorov–Smirnov test and the Anderson–Darling test are treated simultaneously by treating them as special cases of a more general formulation of a distance measure. Examples of the theoretical results that are available are: consistency of the parameter estimates; the asymptotic covariance matrices of the parameter estimates.\n\n\n"}
{"id": "58377750", "url": "https://en.wikipedia.org/wiki?curid=58377750", "title": "NitrosBase", "text": "NitrosBase\n\nNitrosBase is a Russian\nhigh-performance multi-model database system. The database system supports relational, graph and document database models.\n\nThe developer initially implemented the database as a triplestore, being a Semantic Web pioneer in Russia. Remodelling into a multi-model database was supported by the Skolkovo Innovation Center in 2017.\nThe database is used in information systems that support the health-care reform in modern Russia.\n\nIn NitrosBase, all data are stored in the format of the internal graph model, while data in other models are their views (representations; similar to SQL views). Regardless of the model in which format data were imported, it is possible to query them using the same query language thereby uniformely addressing data imported in different models.\n\nMoreover, it is possible to query data in any model using query language that is native for that model. NitrosBase supports the following languages:\n\n\nThe internal graph model is close to RDF* which is used in and Amazon Neptune. That allows it to treat the internal data graph both as RDF graph and as Property Graph, performing queries both in SPARQL and Gremlin-style languages.\n\nInstead of indexes based on B+-trees traditionally used in graph databases, NitrosBase uses a sparse link index of its own devising. Another source of performance gain is storage optimization on the physical level in order to reduce the number of random access operations.\n\nLike memSQL, NitrosBase translates a query into C++ code.\n\nNitrosbase-derived product \"MS SQL Server Accelerator\" was awarded first prize at the \"Silicon Valley Open Doors\" (www.svod.org) conference in 2009 and named \"startup of the day\" of the Microsoft BizSpark program on .\n\n"}
{"id": "47501483", "url": "https://en.wikipedia.org/wiki?curid=47501483", "title": "Ohsawa–Takegoshi theorem", "text": "Ohsawa–Takegoshi theorem\n\nIn several complex variables, the Ohsawa–Takegoshi theorem is a fundamental result concerning the holomorphic extension of a formula_1-holomorphic function defined on a bounded Stein manifold (such as a pseudoconvex compact set in formula_2 of dimension less than formula_3) to a domain of higher dimension, with a bound on the growth. It was discovered by Takeo Ohsawa and Kensho Takegoshi in 1987, using what have been described as \"ad hoc\" methods involving twisted Laplace–Beltrami operators, but simpler proofs have since been discovered. Many generalizations and similar results exist, and are known as theorems of Ohsawa-Takegoshi type.\n"}
{"id": "38771161", "url": "https://en.wikipedia.org/wiki?curid=38771161", "title": "Onsager–Machlup function", "text": "Onsager–Machlup function\n\nThe Onsager–Machlup function is a function that summarizes the dynamics of a continuous stochastic process. It is used to define a probability density for a stochastic process, and it is similar to the Lagrangian of a dynamical system. It is named after Lars Onsager and S. Machlup who were the first to consider such probability densities.\n\nThe dynamics of a continuous stochastic process from time to in one dimension, satisfying a stochastic differential equation\n\nwhere is a Wiener process, can in approximation be described by the probability density function of its value at a finite number of points in time :\n\nwhere\n\nand , and . A similar approximation is possible for processes in higher dimensions. The approximation is more accurate for smaller time step sizes , but in the limit the probability density function becomes ill defined, one reason being that the product of terms\n\ndiverges to infinity. In order to nevertheless define a density for the continuous stochastic process , ratios of probabilities of lying within a small distance from smooth curves and are considered:\n\nas , where is the Onsager–Machlup function.\n\nConsider a -dimensional Riemannian manifold and a diffusion process on with infinitesimal generator , where is the Laplace–Beltrami operator and is a vector field. For any two smooth curves ,\n\nwhere is the Riemannian distance, formula_7 denote the first derivatives of , and is called the Onsager–Machlup function.\n\nThe Onsager–Machlup function is given by\n\nwhere is the Riemannian norm in the tangent space at , is the divergence of at , and is the scalar curvature at .\n\nThe following examples give explicit expressions for the Onsager–Machlup function of a continuous stochastic processes.\n\nThe Onsager–Machlup function of a Wiener process on the real line is given by\n\nThe Onsager–Machlup function in the one-dimensional case with constant diffusion coefficient is given by\n\nIn the -dimensional case, with equal to the unit matrix, it is given by\n\nwhere is the Euclidean norm and\n\nGeneralizations have been obtained by weakening the differentiability condition on the curve . Rather than taking the maximum distance between the stochastic process and the curve over a time interval, other conditions have been considered such as distances based on completely convex norms and Hölder, Besov and Sobolev type norms.\n\nThe Onsager–Machlup function can be used for purposes of reweighting and sampling trajectories,\nas well as for determining the most probable trajectory of a diffusion process.\n\n\n\n"}
{"id": "2311118", "url": "https://en.wikipedia.org/wiki?curid=2311118", "title": "Parallel terraced scan", "text": "Parallel terraced scan\n\nThe parallel terraced scan is a multi-agent based search technique that is basic to cognitive architectures, such as Copycat, Letter-string, the Examiner, Tabletop, and others. It was developed by John Rehling and Douglas Hofstadter at the Center for Research on Concepts and Cognition at Indiana University, Bloomington. \n\nThe parallel terraced scan builds on the concepts of the workspace, coderack, conceptual memory, and temperature. According to Hofstadter the parallel and random nature of the processing captures aspects of human cognition.\n\n\n"}
{"id": "471332", "url": "https://en.wikipedia.org/wiki?curid=471332", "title": "Partition of an interval", "text": "Partition of an interval\n\nIn mathematics, a partition of an interval on the real line is a finite sequence of real numbers such that\n\nIn other terms, a partition of a compact interval is a strictly increasing sequence of numbers (belonging to the interval itself) starting from the initial point of and arriving at the final point of .\n\nEvery interval of the form is referred to as a subinterval of the partition \"x\".\n\nAnother partition of the given interval, , is defined as a refinement of the partition, , when it contains all the points of and possibly some other points as well; the partition is said to be “finer” than . Given two partitions, and , one can always form their common refinement, denoted , which consists of all the points of and , re-numbered in order.\n\nThe norm (or mesh) of the partition\n\nis the length of the longest of these subintervals\n\nPartitions are used in the theory of the Riemann integral, the Riemann–Stieltjes integral and the regulated integral. Specifically, as finer partitions of a given interval are considered, their mesh approaches zero and the Riemann sum based on a given partition approaches the Riemann integral.\n\nA tagged partition is a partition of a given interval together with a finite sequence of numbers subject to the conditions that for each ,\n\nIn other words, a tagged partition is a partition together with a distinguished point of every subinterval: its mesh is defined in the same way as for an ordinary partition. It is possible to define a partial order on the set of all tagged partitions by saying that one tagged partition is bigger than another if the bigger one is a refinement of the smaller one.\n\nSuppose that together with is a tagged partition of , and that together with is another tagged partition of . We say that and together is a refinement of a tagged partition together with if for each integer with , there is an integer such that and such that for some with . Said more simply, a refinement of a tagged partition takes the starting partition and adds more tags, but does not take any away.\n\n"}
{"id": "1575708", "url": "https://en.wikipedia.org/wiki?curid=1575708", "title": "Paul Seymour (mathematician)", "text": "Paul Seymour (mathematician)\n\nPaul Seymour (born July 26, 1950) is currently a professor at Princeton University; half in the department of mathematics and half in the program in applied and computational math. His research interest is in discrete mathematics, especially graph theory. He (with others) was responsible for important progress on regular matroids and totally unimodular matrices, the four colour theorem, linkless embeddings, graph minors and structure, the perfect graph conjecture, the Hadwiger conjecture, and claw-free graphs. Many of his recent papers are available from his website.\n\nHe won a Sloan Fellowship in 1983, and the Ostrowski Prize in 2004; and (sometimes with others) won the Fulkerson Prize in 1979, 1994, 2006 and 2009, and the Pólya Prize in 1983 and 2004. He received an honorary doctorate from the University of Waterloo in 2008 and one from the Technical University of Denmark in 2013.\n\nSeymour was born in Plymouth, Devon, England. He was a day student at Plymouth College, and then studied at Exeter College, Oxford, gaining a BA degree in 1971, and D.Phil in 1975.\n\nFrom 1974–1976 he was a college research fellow at University College of Swansea, and then returned to Oxford for 1976–1980 as a Junior Research Fellow at Merton College, Oxford, with the year 1978–79 at University of Waterloo. He became an associate and then a full professor at Ohio State University, Columbus, Ohio, between 1980 and 1983, where he began research with Neil Robertson,\na fruitful collaboration that continued for many years. From 1983 until 1996, he was at Bellcore (Bell Communications Research), Morristown, New Jersey (now Telcordia Technologies). He was also an adjunct professor at Rutgers University from 1984–1987 and at the University of Waterloo from 1988–1993. He became professor at Princeton University in 1996. He is Editor-in-Chief (jointly with Carsten Thomassen) for the \"Journal of Graph Theory\".\n\nHe married Shelley MacDonald of Ottawa in 1979, and they have two children, Amy and Emily. The couple separated amicably in 2007. His brother Leonard W. Seymour is Professor of gene therapy at Oxford University.\n\nCombinatorics in Oxford in the 1970s was dominated by matroid theory, due\nto the influence of Dominic Welsh and Aubrey William Ingleton.\nMuch of Seymour's early work, up to about 1980, was on matroid theory, and\nincluded three important matroid results: his D.Phil. thesis\non matroids with the max-flow min-cut property (for which he won his first\nFulkerson prize); a characterization by excluded minors of the matroids\nrepresentable over the three-element field; and a theorem that all \nregular matroids consist of graphic and cographic matroids pieced together in a\nsimple way (which won his first Pólya prize). There were several other\nsignificant papers from this period: a paper with Welsh on the critical\nprobabilities for bond percolation on the square lattice; a paper in which\nthe cycle double cover conjecture was introduced; a paper on\nedge-multicolouring of cubic graphs, which foreshadows\nthe matching lattice theorem of László Lovász; a paper proving that all\nbridgeless graphs admit nowhere-zero 6-flows, a step towards Tutte's nowhere-zero 5-flow conjecture;\nand a paper solving the\ntwo-paths problem, which was the engine behind much of Seymour's\nfuture work.\n\nIn 1980 he moved to Ohio State University, and began work with Neil\nRobertson. This led eventually to Seymour's most important accomplishment,\nthe so-called \"Graph Minors Project\", a series of 23 papers (joint with Robertson), \npublished over the next thirty years, with several\nsignificant results: the graph minors structure theorem,\nthat for any fixed graph, all graphs that do not contain it as a minor\ncan be built from graphs that are essentially of bounded genus by piecing them\ntogether at small cutsets in a tree structure; a proof of a conjecture\nof Wagner that in any infinite set of graphs, one of them is a minor\nof another (and consequently that any property of graphs that can be characterized by excluded minors can be characterized by a finite list of excluded minors);\na proof of a similar conjecture of Nash-Williams that in any infinite set\nof graphs, one of them can be immersed in another; and\npolynomial-time algorithms to test if a graph contains a fixed graph as a\nminor, and to solve the k vertex-disjoint paths problem for all fixed k.\n\nIn about 1990 Robin Thomas began to work with Robertson and Seymour. Their\ncollaboration resulted in several important joint papers over the next ten years:\na proof of a conjecture of Sachs, characterizing\nby excluded minors the graphs that admit linkless embeddings in 3-space;\na proof that every graph that is not five-colourable has a six-vertex complete\ngraph as a minor (the four-colour theorem is assumed to obtain this result, which is a\ncase of Hadwiger's conjecture);\nwith Dan Sanders, a new, simplified, computer based proof of the four-colour theorem;\na description of the bipartite graphs that admit Pfaffian orientations;\nand the reduction to the ``almost-planar\" case of a conjecture of Tutte that every\nbridgeless cubic graph that is not three-edge-colourable contains the Petersen graph\nas a minor. (The remaining ``almost-planar\" case has now been solved, completing the proof of Tutte's conjecture, in papers\nby subsets of Sanders, Seymour, Thomas and Katherine Edwards. This does not assume the four-colour theorem, and re-proves it in an extended form).\n\nIn 2000 the trio were supported by the American Institute of Mathematics to\nwork on the strong perfect graph conjecture, a famous\nopen question that had been raised by Claude Berge in the early 1960s.\nSeymour's student Maria Chudnovsky joined them in 2001, and in 2002\nthe four jointly proved the conjecture.\nSeymour continued to work with Chudnovsky, and obtained several more\nresults about induced subgraphs, in particular (with\nCornuéjols, Liu, Vuskovic) a polynomial-time algorithm to test whether\na graph is perfect, and\na general description of all claw-free graphs. Most recently, in a series of papers with Alex Scott and partly with Chudnovsky,\nthey proved two conjectures of Andras Gyarfas, that every graph with bounded clique number and sufficiently large\nchromatic number has an induced cycle of odd length at least five, and has an induced cycle of length at least\nany specified number.\n\n\n"}
{"id": "28498835", "url": "https://en.wikipedia.org/wiki?curid=28498835", "title": "Pedal equation", "text": "Pedal equation\n\nFor a plane curve \"C\" and a given fixed point \"O\", the pedal equation of the curve is a relation between \"r\" and \"p\" where \"r\" is the distance from \"O\" to a point on \"C\" and \"p\" is the perpendicular distance from \"O\" to the tangent line to \"C\" at the point. The point \"O\" is called the \"pedal point\" and the values \"r\" and \"p\" are sometimes called the \"pedal coordinates\" of a point relative to the curve and the pedal point. It is also useful to measure the distance of \"O\" to the normal formula_1 (the \"contrapedal coordinate\") even though it is not an independent quantity and it relates to formula_2 as formula_3.\n\nSome curves have particularly simple pedal equations and knowing the pedal equation of a curve may simplify the calculation of certain of its properties such as curvature. This coordinates are also well suited for solving certain type of force problems in classical mechanics and celestial mechanics.\n\nFor \"C\" given in rectangular coordinates by \"f\"(\"x\", \"y\") = 0, and with \"O\" taken to be the origin, the pedal coordinates of the point (\"x\", \"y\") are given by:\n\nThe pedal equation can be found by eliminating \"x\" and \"y\" from these equations and the equation of the curve.\n\nThe expression for \"p\" may be simplified if the equation of the curve is written in homogeneous coordinates by introducing a variable \"z\", so that the equation of the curve is \"g\"(\"x\", \"y\", \"z\") = 0. The value of \"p\" is then given by\nwhere the result is evaluated at \"z\"=1\n\nFor \"C\" given in polar coordinates by \"r\" = \"f\"(θ), then\n\nwhere formula_8 is the polar tangential angle given by\n\nThe pedal equation can be found by eliminating θ from these equations.\n\nAlternatively, from the above we can find that\n\nwhere formula_11 is the \"contrapedal\" coordinate, i.e. distance to the normal. This implies that if a curve satisfies an autonomous differential equation in polar coordinates of the form:\n\nits pedal equation becomes\n\nAs an example take the logarithmic spiral with the spiral angle α:\nDifferentiating with respect to formula_15 we obtain\nhence \nand thus in pedal coordinates we get\nor using the fact that formula_19 we obtain\n\nThis approach can be generalized to include autonomous differential equations of any order as follows: A curve \"C\" which a solution of an \"n\"-th order autonomous differential equation (formula_21) in polar coordinates\n\nis the pedal curve of a curve given in pedal coordinates by\n\nwhere the differentiation is done with respect to formula_24.\n\nSolutions to some force problems of classical mechanics can be surprisingly easily obtained in pedal coordinates.\n\nConsider a dynamical system:\n\ndescribing an evolution of a test particle (with position formula_26 and velocity formula_27) in the plane in the presence of central formula_28 and Lorentz like formula_29 potential. The quantities:\n\nare conserved in this system.\n\nThen the curve traced by formula_26 is given in pedal coordinates by\n\nwith the pedal point at the origin. This fact was discovered by P. Blaschke in 2017.\n\nAs an example consider the so-called Kepler problem, i.e. central force problem, where the force varies inversely as a square of the distance:\n\n\n"}
{"id": "13556959", "url": "https://en.wikipedia.org/wiki?curid=13556959", "title": "Perpendicular distance", "text": "Perpendicular distance\n\nIn geometry, the perpendicular distance between two objects is the distance from one to the other, measured along a line that is perpendicular to one or both.\nIn particular, see:\n"}
{"id": "5017608", "url": "https://en.wikipedia.org/wiki?curid=5017608", "title": "Probabilistic logic", "text": "Probabilistic logic\n\nThe aim of a probabilistic logic (also probability logic and probabilistic reasoning) is to combine the capacity of probability theory to handle uncertainty with the capacity of deductive logic to exploit structure of formal argument. The result is a richer and more expressive formalism with a broad range of possible application areas. Probabilistic logics attempt to find a natural extension of traditional logic truth tables: the results they define are derived through probabilistic expressions instead. A difficulty with probabilistic logics is that they tend to multiply the computational complexities of their probabilistic and logical components. Other difficulties include the possibility of counter-intuitive results, such as those of Dempster-Shafer theory in evidence-based subjective logic. The need to deal with a broad variety of contexts and issues has led to many different proposals.\n\nThere are numerous proposals for probabilistic logics. Very roughly, they can be categorized into two different classes: those logics that attempt to make a probabilistic extension to logical entailment, such as Markov logic networks, and those that attempt to address the problems of uncertainty and lack of evidence (evidentiary logics).\n\nThat probability and uncertainty are not quite the same thing may be understood by noting that, despite the mathematization of probability in the Enlightenment, mathematical probability theory remains, to this very day, entirely unused in criminal courtrooms, when evaluating the \"probability\" of the guilt of a suspected criminal.\n\nMore precisely, in evidentiary logic, there is a need to distinguish the truth of a statement from the confidence in its truth: thus, being uncertain of a suspect's guilt is not the same as assigning a numerical probability to the commission of the crime. A single suspect may be guilty or not guilty, just as a coin may be flipped heads or tails. Given a large collection of suspects, a certain percentage may be guilty, just as the probability of flipping \"heads\" is one-half. However, it is incorrect to take this law of averages with regard to a single criminal (or single coin-flip): the criminal is no more \"a little bit guilty\" than a single coin flip is \"a little bit heads and a little bit tails\": we are merely uncertain as to which it is. Conflating probability and uncertainty may be acceptable when making scientific measurements of physical quantities, but it is an error, in the context of \"common sense\" reasoning and logic. Just as in courtroom reasoning, the goal of employing uncertain inference is to gather evidence to strengthen the confidence of a proposition, as opposed to performing some sort of probabilistic entailment.\n\nHistorically, attempts to quantify probabilistic reasoning date back to antiquity. There was a particularly strong interest starting in the 12th century, with the work of the Scholastics, with the invention of the half-proof (so that two half-proofs are sufficient to prove guilt), the elucidation of moral certainty (sufficient certainty to act upon, but short of absolute certainty), the development of Catholic probabilism (the idea that it is always safe to follow the established rules of doctrine or the opinion of experts, even when they are less probable), the case-based reasoning of casuistry, and the scandal of Laxism (whereby probabilism was used to give support to almost any statement at all, it being possible to find an expert opinion in support of almost any proposition.).\n\nBelow is a list of proposals for probabilistic and evidentiary extensions to classical and predicate logic.\n\n\n\n\n"}
{"id": "11153041", "url": "https://en.wikipedia.org/wiki?curid=11153041", "title": "Saint-Venant's principle", "text": "Saint-Venant's principle\n\nSaint-Venant's principle, named after Adhémar Jean Claude Barré de Saint-Venant, a French elasticity theorist, may be expressed as follows:\n\nThe original statement was published in French by Saint-Venant in 1855. Although this informal statement of the principle is well known among structural and mechanical engineers, more recent mathematical literature gives a rigorous interpretation in the context of partial differential equations. An early such interpretation was made by von Mises in 1945.\n\nThe Saint-Venant's principle allows elasticians to replace complicated stress distributions or weak boundary conditions with ones that are easier to solve, as long as that boundary is geometrically short. Quite analogous to the electrostatics, where the electric field due to the \"i\"-th moment of the load (with 0th being the net charge, 1st the dipole, 2nd the quadrupole) decays as formula_1 over space, Saint-Venant's principle states that high order momentum of mechanical load (moment with order higher than torque) decays so fast that they never need to be considered for regions far from the short boundary. Therefore, the Saint-Venant's principle can be regarded as a statement on the asymptotic behavior of the Green's function by a point-load.\n\n"}
{"id": "161879", "url": "https://en.wikipedia.org/wiki?curid=161879", "title": "Scalar field", "text": "Scalar field\n\nIn mathematics and physics, a scalar field associates a scalar value to every point in a space – possibly physical space. The scalar may either be a (dimensionless) mathematical number or a physical quantity. In a physical context, scalar fields are required to be independent of the choice of reference frame, meaning that any two observers using the same units will agree on the value of the scalar field at the same absolute point in space (or spacetime) regardless of their respective points of origin. Examples used in physics include the temperature distribution throughout space, the pressure distribution in a fluid, and spin-zero quantum fields, such as the Higgs field. These fields are the subject of scalar field theory.\n\nMathematically, a scalar field on a region \"U\" is a real or complex-valued function or distribution on \"U\". The region \"U\"may be a set in some Euclidean space, Minkowski space, or more generally a subset of a manifold, and it is typical in mathematics to impose further conditions on the field, such that it be continuous or often continuously differentiable to some order. A scalar field is a tensor field of order zero, and the term \"scalar field\" may be used to distinguish a function of this kind with a more general tensor field, density, or differential form.\n\nPhysically, a scalar field is additionally distinguished by having units of measurement associated with it. In this context, a scalar field should also be independent of the coordinate system used to describe the physical system—that is, any two observers using the same units must agree on the numerical value of a scalar field at any given point of physical space. Scalar fields are contrasted with other physical quantities such as vector fields, which associate a vector to every point of a region, as well as tensor fields and spinor fields. More subtly, scalar fields are often contrasted with pseudoscalar fields.\n\nIn physics, scalar fields often describe the potential energy associated with a particular force. The force is a vector field, which can be obtained as the gradient of the potential energy scalar field. Examples include:\n\n\n\n"}
{"id": "7951270", "url": "https://en.wikipedia.org/wiki?curid=7951270", "title": "Sign (mathematics)", "text": "Sign (mathematics)\n\nIn mathematics, the concept of sign originates from the property of every non-zero real number of being positive or negative. Zero itself is signless, although in some contexts it makes sense to consider a signed zero, and in some contexts it makes sense to call 0 its own sign. Along with its application to real numbers, \"change of sign\" is used throughout mathematics and physics to denote the additive inverse (negation, or multiplication by −1), even for quantities which are not real numbers (so, which are not prescribed to be either positive, negative, or zero). Also, the word \"sign\" can indicate aspects of mathematical objects that resemble positivity and negativity, such as the sign of a permutation (see below).\n\nEvery number has multiple attributes (such as value, sign and magnitude). A real number is said to be positive if its value (\"not\" its magnitude) is greater than zero, and negative if it is less than zero. The attribute of being positive or negative is called the sign of the number. Zero itself is not considered to have a sign (though this is context dependent, see below). Also, signs are not defined for complex numbers, although the argument generalizes it in some sense.\n\nIn common numeral notation (which is used in arithmetic and elsewhere), the sign of a number is often denoted by placing a plus sign or a minus sign before the number. For example, +3 denotes \"positive three\", and −3 denotes \"negative three\". When no plus or minus sign is given, the default interpretation is that a number is positive. Because of this notation, as well as the definition of negative numbers through subtraction, the minus sign is perceived to have a strong association with negative numbers (of the negative sign). Likewise, \"+\" associates with positivity.\n\nIn algebra, a minus sign is usually thought of as representing the operation of additive inverse (sometimes called \"negation\"), with the additive inverse of a positive number being negative and the additive inverse of a negative number being positive. In this context, it makes sense to write −(−3) = +3.\n\nAny non-zero number can be changed to a positive one using the absolute value function. For example, the absolute value of −3 and the absolute value of 3 are both equal to 3. In symbols, this would be written |−3| = 3 and |3| = 3.\n\nThe number zero is neither positive nor negative, and therefore has no sign. In arithmetic, +0 and −0 both denote the same number 0, which is the additive inverse of itself.\n\nNote that this definition is culturally determined. In France and Belgium, 0 is said to be both positive and negative. The positive numbers without zero and the negative numbers without zero are said to be \"strictly positive\" and \"strictly negative\" respectively.\n\nIn some contexts, such as signed number representations in computing, it makes sense to consider signed versions of zero, with positive zero and negative zero being different numbers (see signed zero).\n\nOne also sees +0 and −0 in calculus and mathematical analysis when evaluating one-sided limits. This notation refers to the behaviour of a function as the input variable approaches 0 from positive or negative values respectively; these behaviours are not necessarily the same.\n\nBecause zero is neither positive nor negative (in most countries), the following phrases are sometimes used to refer to the sign of an unknown number:\nThus a non-negative number is either positive or zero, while a non-positive number is either negative or zero. For example, the absolute value of a real number is always non-negative, but is not necessarily positive.\n\nThe same terminology is sometimes used for functions that take real or integer values. For example, a function would be called positive if all of its values are positive, or non-negative if all of its values are non-negative.\n\nIn many contexts the choice of sign convention (which range of values is considered positive and which negative) is natural, whereas in others the choice is arbitrary subject only to consistency, the latter necessitating an explicit sign convention.\n\nThe sign function or signum function is sometimes used to extract the sign of a number. This function is usually defined as follows:\nThus sgn(\"x\") is 1 when \"x\" is positive, and sgn(\"x\") is −1 when \"x\" is negative. For nonzero values of \"x\", this function can also be defined by the formula\nwhere |\"x\"| is the absolute value of \"x\".\n\nIn many contexts, it is common to associate a sign with the measure of an angle, particularly an oriented angle or an angle of rotation. In such a situation, the sign indicates whether the angle is in the clockwise or counterclockwise direction. Though different conventions can be used, it is common in mathematics to have counterclockwise angles count as positive, and clockwise angles count as negative.\n\nIt is also possible to associate a sign to an angle of rotation in three dimensions, assuming the axis of rotation has been oriented. Specifically, a right-handed rotation around an oriented axis typically counts as positive, while a left-handed rotation counts as negative.\n\nWhen a quantity \"x\" changes over time, the change in the value of \"x\" is typically defined by the equation\n\nUsing this convention, an increase in \"x\" counts as positive change, while a decrease of \"x\" counts as negative change. In calculus, this same convention is used in the definition of the derivative. As a result, any increasing function has positive derivative, while a decreasing function has negative derivative.\n\nIn analytic geometry and physics, it is common to label certain directions as positive or negative. For a basic example, the number line is usually drawn with positive numbers to the right, and negative numbers to the left:\n\nAs a result, when discussing linear motion, displacement or velocity to the right is usually thought of as being positive, while similar motion to the left is thought of as being negative.\n\nOn the Cartesian plane, the rightward and upward directions are usually thought of as positive, with rightward being the positive \"x\"-direction, and upward being the positive \"y\"-direction. If a displacement or velocity vector is separated into its vector components, then the horizontal part will be positive for motion to the right and negative for motion to the left, while the vertical part will be positive for motion upward and negative for motion downward.\n\nIn computing, an integer value may be either signed or unsigned, depending on whether the computer is keeping track of a sign for the number. By restricting an integer variable to non-negative values only, one more bit can be used for storing the value of a number. Because of the way integer arithmetic is done within computers, the sign of a signed integer variable is usually not stored as a single independent bit, but is instead stored using two's complement or some other signed number representation.\n\nIn contrast, real numbers are stored and manipulated as Floating point values. The floating point values are represented using three separate values, mantissa, exponent, and sign. Given this separate sign bit, it is possible to represent both positive and negative zero. Most programming languages normally treat positive zero and negative zero as equivalent values, albeit, they provide means by which the distinction can be detected.\n\nIn addition to the sign of a real number, the word sign is also used in various related ways throughout mathematics and the sciences:\n\n"}
{"id": "39781", "url": "https://en.wikipedia.org/wiki?curid=39781", "title": "Simplex", "text": "Simplex\n\nIn geometry, a simplex (plural: simplexes or simplices) is a generalization of the notion of a triangle or tetrahedron to arbitrary dimensions.\nSpecifically, a \"k\"-simplex is a \"k\"-dimensional polytope which is the convex hull of its \"k\" + 1 vertices. \nMore formally, suppose the \"k\" + 1 points formula_1 are affinely independent, which means formula_2 are linearly independent.\nThen, the simplex determined by them is the set of points\n\nFor example, a 2-simplex is a triangle, a 3-simplex is a tetrahedron, and a 4-simplex is a 5-cell. A single point may be considered a 0-simplex, and a line segment may be considered a 1-simplex. A simplex may be defined as the smallest convex set containing the given vertices.\n\nA regular simplex is a simplex that is also a regular polytope. A regular \"n\"-simplex may be constructed from a regular (\"n\" − 1)-simplex by connecting a new vertex to all original vertices by the common edge length.\n\nThe standard simplex or probability simplex is the simplex formed from the \"k\" + 1 standard unit vectors, or \n\nIn topology and combinatorics, it is common to “glue together” simplices to form a simplicial complex. The associated combinatorial structure is called an abstract simplicial complex, in which context the word “simplex” simply means any finite set of vertices.\n\nThe concept of a simplex was known to William Kingdon Clifford, who wrote about these shapes in 1886 but called them \"prime confines\". \nHenri Poincaré, writing about algebraic topology in 1900, called them \"generalized tetrahedra\".\nIn 1902 Pieter Hendrik Schoute described the concept first with the Latin superlative \"simplicissimum\" (\"simplest\") and then with the same Latin adjective in the normal form \"simplex\" (\"simple\").\n\n\nThe convex hull of any nonempty subset of the \"n\" + 1 points that define an \"n\"-simplex is called a face of the simplex. Faces are simplices themselves. In particular, the convex hull of a subset of size \"m\" + 1 (of the \"n\" + 1 defining points) is an \"m\"-simplex, called an \"m\"-face of the \"n\"-simplex. The 0-faces (i.e., the defining points themselves as sets of size 1) are called the vertices (singular: vertex), the 1-faces are called the edges, the (\"n\" − 1)-faces are called the facets, and the sole \"n\"-face is the whole \"n\"-simplex itself. In general, the number of \"m\"-faces is equal to the binomial coefficient formula_5. Consequently, the number of \"m\"-faces of an \"n\"-simplex may be found in column (\"m\" + 1) of row (\"n\" + 1) of Pascal's triangle. A simplex \"A\" is a coface of a simplex \"B\" if \"B\" is a face of \"A\". \"Face\" and \"facet\" can have different meanings when describing types of simplices in a simplicial complex; see simplical complex for more detail.\n\nThe regular simplex family is the first of three regular polytope families, labeled by Coxeter as \"α\", the other two being the cross-polytope family, labeled as \"β\", and the hypercubes, labeled as \"γ\". A fourth family, the infinite tessellation of hypercubes, he labeled as \"δ\".\n\nThe number of 1-faces (edges) of the \"n\"-simplex is the \"n\"-th triangle number, the number of 2-faces of the \"n\"-simplex is the (\"n\" − 1)th tetrahedron number, the number of 3-faces of the \"n\"-simplex is the (\"n\" − 2)th 5-cell number, and so on.\n\nAn (\"n\" + 1)-simplex can be constructed as a join (∨ operator) of an \"n\"-simplex and a point, ( ). An (\"m\" + \"n\" + 1)-simplex can be constructed as a join of an \"m\"-simplex and an \"n\"-simplex. The two simplices are oriented to be completely normal from each other, with translation in a direction orthogonal to both of them. A 1-simplex is the join of two points: ( ) ∨ ( ) = 2 · ( ). A general 2-simplex (scalene triangle) is the join of three points: ( ) ∨ ( ) ∨ ( ). An isosceles triangle is the join of a 1-simplex and a point: { } ∨ ( ). An equilateral triangle is 3·( ) or {3}. A general 3-simplex is the join of 4 points: ( ) ∨ ( ) ∨ ( ) ∨ ( ). A 3-simplex with mirror symmetry can be expressed as the join of an edge and two points: { } ∨ ( ) ∨ ( ). A 3-simplex with triangular symmetry can be expressed as the join of an equilateral triangle and 1 point: 3.( )∨( ) or {3}∨( ). A regular tetrahedron is 4 · ( ) or {3,3} and so on.\n\nIn some conventions, the empty set is defined to be a (−1)-simplex. The definition of the simplex above still makes sense if \"n\" = −1. This convention is more common in applications to algebraic topology (such as simplicial homology) than to the study of polytopes.\nThese Petrie polygons (skew orthogonal projections) show all the vertices of the regular simplex on a circle, and all vertex pairs connected by edges.\n\nThe standard \"n\"-simplex (or unit \"n\"-simplex) is the subset of R given by\n\nThe simplex Δ lies in the affine hyperplane obtained by removing the restriction \"t\" ≥ 0 in the above definition.\n\nThe \"n\" + 1 vertices of the standard \"n\"-simplex are the points \"e\" ∈ R, where\nThere is a canonical map from the standard \"n\"-simplex to an arbitrary \"n\"-simplex with vertices (\"v\", …, \"v\") given by\nThe coefficients \"t\" are called the barycentric coordinates of a point in the \"n\"-simplex. Such a general simplex is often called an affine \"n\"-simplex, to emphasize that the canonical map is an affine transformation. It is also sometimes called an oriented affine \"n\"-simplex to emphasize that the canonical map may be orientation preserving or reversing.\n\nMore generally, there is a canonical map from the standard formula_9-simplex (with \"n\" vertices) onto any polytope with \"n\" vertices, given by the same equation (modifying indexing):\nThese are known as generalized barycentric coordinates, and express every polytope as the \"image\" of a simplex: formula_11\n\nA commonly used function from R to the interior of the standard formula_9-simplex is the softmax function, or normalized exponential function; this generalizes the standard logistic function.\n\n\nAn alternative coordinate system is given by taking the indefinite sum:\nThis yields the alternative presentation by \"order,\" namely as nondecreasing \"n\"-tuples between 0 and 1:\nGeometrically, this is an \"n\"-dimensional subset of formula_15 (maximal dimension, codimension 0) rather than of formula_16 (codimension 1). The facets, which on the standard simplex correspond to one coordinate vanishing, formula_17 here correspond to successive coordinates being equal, formula_18 while the interior corresponds to the inequalities becoming \"strict\" (increasing sequences).\n\nA key distinction between these presentations is the behavior under permuting coordinates – the standard simplex is stabilized by permuting coordinates, while permuting elements of the \"ordered simplex\" do not leave it invariant, as permuting an ordered sequence generally makes it unordered. Indeed, the ordered simplex is a (closed) fundamental domain for the action of the symmetric group on the \"n\"-cube, meaning that the orbit of the ordered simplex under the \"n\"! elements of the symmetric group divides the \"n\"-cube into formula_19 mostly disjoint simplices (disjoint except for boundaries), showing that this simplex has volume formula_20 Alternatively, the volume can be computed by an iterated integral, whose successive integrands are formula_21\n\nA further property of this presentation is that it uses the order but not addition, and thus can be defined in any dimension over any ordered set, and for example can be used to define an infinite-dimensional simplex without issues of convergence of sums.\n\nEspecially in numerical applications of probability theory a projection onto the standard simplex is of interest. Given formula_22 with possibly negative entries, the closest point formula_23 on the simplex has coordinates \nwhere formula_25 is chosen such that formula_26\n\nformula_25 can be easily calculated from sorting formula_28.\nThe sorting approach takes formula_29 complexity, which can be improved to formula_30 complexity via median-finding algorithms. Projecting onto the simplex is computationally similar to projecting onto the formula_31 ball.\n\nFinally, a simple variant is to replace \"summing to 1\" with \"summing to at most 1\"; this raises the dimension by 1, so to simplify notation, the indexing changes:\nThis yields an \"n\"-simplex as a corner of the \"n\"-cube, and is a standard orthogonal simplex. This is the simplex used in the simplex method, which is based at the origin, and locally models a vertex on a polytope with \"n\" facets.\n\nThe coordinates of the vertices of a regular \"n\"-dimensional simplex can be obtained from these two properties,\n\nThese can be used as follows. Let vectors (\"v\", \"v\", ..., \"v\") represent the vertices of an \"n\"-simplex center the origin, all unit vectors so a distance 1 from the origin, satisfying the first property. The second property means the dot product between any pair of the vectors is formula_34. This can be used to calculate positions for them.\n\nFor example in three dimensions the vectors (\"v\", \"v\", \"v\", \"v\") are the vertices of a 3-simplex or tetrahedron. Write these as\n\nChoose the first vector \"v\" to have all but the first component zero, so by the first property it must be (1, 0, 0) and the vectors become\n\nBy the second property the dot product of \"v\" with all other vectors is -, so each of their \"x\" components must equal this, and the vectors become\n\nNext choose \"v\" to have all but the first two elements zero. The second element is the only unknown. It can be calculated from the first property using the Pythagorean theorem (choose any of the two square roots), and so the second vector can be completed:\n\nThe second property can be used to calculate the remaining \"y\" components, by taking the dot product of \"v\" with each and solving to give\n\nFrom which the \"z\" components can be calculated, using the Pythagorean theorem again to satisfy the first property, the two possible square roots giving the two results\n\nThis process can be carried out in any dimension, using \"n\" + 1 vectors, applying the first and second properties alternately to determine all the values.\n\nThe volume of an \"n\"-simplex in \"n\"-dimensional space with vertices (\"v\", ..., \"v\") is\n\nwhere each column of the \"n\" × \"n\" determinant is the difference between the vectors representing two vertices. Another common way of computing the volume of the simplex is via the Cayley–Menger determinant. It can also compute the volume of a simplex embedded in a higher-dimensional space, e.g., a triangle in formula_42.\n\nWithout the 1/\"n\"! it is the formula for the volume of an \"n\"-parallelotope. \nThis can be understood as follows: Assume that \"P\" is an \"n\"-parallelotope constructed on a basis formula_43 of formula_44.\nGiven a permutation formula_45 of formula_46, call a list of vertices formula_47 a \"n\"-path if \n(so there are \"n\"! \"n\"-paths and formula_49 does not depend on the permutation). The following assertions hold:\n\nIf \"P\" is the unit \"n\"-hypercube, then the union of the \"n\"-simplexes formed by the convex hull of each \"n\"-path is \"P\", and these simplexes are congruent and pairwise non-overlapping. In particular, the volume of such a simplex is \n\nIf \"P\" is a general parallelotope, the same assertions hold except that it is no more true, in dimension > 2, that the simplexes need to be pairwise congruent; yet their volumes remain equal, because the \"n\"-parallelotop is the image of the unit \"n\"-hypercube by the linear isomorphism that sends the canonical basis of formula_44 to formula_52. As previously, this implies that the volume of a simplex coming from a \"n\"-path is:\n\nConversely, given an \"n\"-simplex formula_54 of formula_44, it can be supposed that the vectors formula_56 form a basis of formula_44. Considering the parallelotope constructed from formula_58 and formula_52, one sees that the previous formula is valid for every simplex.\n\nFinally, the formula at the beginning of this section is obtained by observing that \n\nFrom this formula, it follows immediately that the volume under a standard \"n\"-simplex (i.e. between the origin and the simplex in R) is\n\nThe volume of a regular \"n\"-simplex with unit side length is\n\nas can be seen by multiplying the previous formula by \"x\", to get the volume under the \"n\"-simplex as a function of its vertex distance \"x\" from the origin, differentiating with respect to \"x\", at formula_63   (where the \"n\"-simplex side length is 1), and normalizing by the length formula_64 of the increment, formula_65, along the normal vector.\n\nThe dihedral angle of a regular \"n\"-dimensional simplex is cos(1/\"n\"), while its central angle is cos(-1/\"n\").\n\nAn \"orthogonal corner\" means here that there is a vertex at which all adjacent edges are pairwise orthogonal. It immediately follows that all adjacent faces are pairwise orthogonal. Such simplices are generalizations of right triangles and for them there exists an \"n\"-dimensional version of the Pythagorean theorem:\n\nThe sum of the squared (\"n\" − 1)-dimensional volumes of the facets adjacent to the orthogonal corner equals the squared (\"n\" − 1)-dimensional volume of the facet opposite of the orthogonal corner.\n\nwhere formula_67 are facets being pairwise orthogonal to each other but not orthogonal to formula_68, which is the facet opposite the orthogonal corner.\n\nFor a 2-simplex the theorem is the Pythagorean theorem for triangles with a right angle and for a 3-simplex it is de Gua's theorem for a tetrahedron \nwith an orthogonal corner.\n\nThe Hasse diagram of the face lattice of an \"n\"-simplex is isomorphic to the graph of the (\"n\" + 1)-hypercube's edges, with the hypercube's vertices mapping to each of the \"n\"-simplex's elements, including the entire simplex and the null polytope as the extreme points of the lattice (mapped to two opposite vertices on the hypercube). This fact may be used to efficiently enumerate the simplex's face lattice, since more general face lattice enumeration algorithms are more computationally expensive.\n\nThe \"n\"-simplex is also the vertex figure of the (\"n\" + 1)-hypercube. It is also the facet of the (\"n\" + 1)-orthoplex.\n\nTopologically, an \"n\"-simplex is equivalent to an \"n\"-ball. Every \"n\"-simplex is an \"n\"-dimensional manifold with corners.\n\nIn probability theory, the points of the standard \"n\"-simplex in (\"n\" + 1)-space are the space of possible parameters (probabilities) of the categorical distribution on \"n\" + 1 possible outcomes.\n\nSince all simplices are self-dual, they can form a series of compounds;\n\n\nIn algebraic topology, simplices are used as building blocks to construct an interesting class of topological spaces called simplicial complexes. These spaces are built from simplices glued together in a combinatorial fashion. Simplicial complexes are used to define a certain kind of homology called simplicial homology.\n\nA finite set of \"k\"-simplexes embedded in an open subset of R is called an affine \"k\"-chain. The simplexes in a chain need not be unique; they may occur with multiplicity. Rather than using standard set notation to denote an affine chain, it is instead the standard practice to use plus signs to separate each member in the set. If some of the simplexes have the opposite orientation, these are prefixed by a minus sign. If some of the simplexes occur in the set more than once, these are prefixed with an integer count. Thus, an affine chain takes the symbolic form of a sum with integer coefficients.\n\nNote that each facet of an \"n\"-simplex is an affine (\"n\" − 1)-simplex, and thus the boundary of an \"n\"-simplex is an affine \"n\" − 1-chain. Thus, if we denote one positively oriented affine simplex as\n\nwith the formula_70 denoting the vertices, then the boundary formula_71 of \"σ\" is the chain\n\nIt follows from this expression, and the linearity of the boundary operator, that the boundary of the boundary of a simplex is zero:\n\nLikewise, the boundary of the boundary of a chain is zero: formula_74.\n\nMore generally, a simplex (and a chain) can be embedded into a manifold by means of smooth, differentiable map formula_75. In this case, both the summation convention for denoting the set, and the boundary operation commute with the embedding. That is,\n\nwhere the formula_77 are the integers denoting orientation and multiplicity. For the boundary operator formula_78, one has:\n\nwhere ρ is a chain. The boundary operation commutes with the mapping because, in the end, the chain is defined as a set and little more, and the set operation always commutes with the map operation (by definition of a map).\n\nA continuous map formula_80 to a topological space \"X\" is frequently referred to as a singular \"n\"-simplex. (A map is generally called \"singular\" if it fails to have some desirable property such as continuity and, in this case, the term is meant to reflect to the fact that the continuous map need not be an embedding.)\n\nSince classical algebraic geometry allows to talk about polynomial equations, but not inequalities, the \"algebraic standard n-simplex\" is commonly defined as the subset of affine (\"n\" + 1)-dimensional space, where all coordinates sum up to 1 (thus leaving out the inequality part). The algebraic description of this set is\n\nwhich equals the scheme-theoretic description formula_82 with\n\nthe ring of regular functions on the algebraic \"n\"-simplex (for any ring formula_84).\n\nBy using the same definitions as for the classical \"n\"-simplex, the \"n\"-simplices for different dimensions \"n\" assemble into one simplicial object, while the rings formula_85 assemble into one cosimplicial object formula_86 (in the category of schemes resp. rings, since the face and degeneracy maps are all polynomial).\n\nThe algebraic \"n\"-simplices are used in higher K-theory and in the definition of higher Chow groups.\n\nSimplices are used in plotting quantities that sum to 1, such as proportions of subpopulations, as in a ternary plot.\n\nIn industrial statistics, simplices arise in problem formulation and in algorithmic solution. In the design of bread, the producer must combine yeast, flour, water, sugar, etc. In such mixtures, only the relative proportions of ingredients matters: For an optimal bread mixture, if the flour is doubled then the yeast should be doubled. Such mixture problem are often formulated with normalized constraints, so that the nonnegative components sum to one, in which case the feasible region forms a simplex. The quality of the bread mixtures can be estimated using response surface methodology, and then a local maximum can be computed using a nonlinear programming method, such as sequential quadratic programming.\n\nIn operations research, linear programming problems can be solved by the simplex algorithm of George Dantzig.\n\nIn geometric design and computer graphics, many methods first perform simplicial triangulations of the domain and then fit interpolating polynomials to each simplex.\n\nIn chemistry, the hydrides of most elements in the p-block can resemble a simplex if one is to connect each atom. Neon does not react with hydrogen and as such is a point, fluorine bonds with one hydrogen atom and forms a line segment, oxygen bonds with two hydrogen in a bent fashion resembling a triangle, nitrogen reacts to form a tetrahedron, and carbon will form a structure resembling a Schlegel diagram of the 5-cell. This trend continues for the heavier analogues of each element, as well as if the hydrogen is replaced by halogen atoms.\n\n"}
{"id": "2330936", "url": "https://en.wikipedia.org/wiki?curid=2330936", "title": "Smn theorem", "text": "Smn theorem\n\nIn computability theory the \"s\" theorem, (also called the translation lemma, parameter theorem, and the parameterization theorem) is a basic result about programming languages (and, more generally, Gödel numberings of the computable functions) (Soare 1987, Rogers 1967). It was first proved by Stephen Cole Kleene (1943). The name \"s\" comes from the occurrence of a \"s\" with subscript \"n\" and superscript \"m\" in the original formulation of the theorem (see below).\n\nIn practical terms, the theorem says that for a given programming language and positive integers \"m\" and \"n\", there exists a particular algorithm that accepts as input the source code of a program with \"m\" + \"n\" free variables, together with \"m\" values. This algorithm generates source code that effectively substitutes the values for the first \"m\" free variables, leaving the rest of the variables free.\n\nThe basic form of the theorem applies to functions of two arguments (Nies 2009, p. 6). Given a Gödel numbering formula_1 of recursive functions, there is a primitive recursive function \"s\" of two arguments with the following property: for every Gödel number \"p\" of a partial computable function \"f\" with two arguments, the expressions formula_2 and formula_3 are defined for the same combinations of natural numbers \"x\" and \"y\", and their values are equal for any such combination. In other words, the following extensional equality of functions holds for every \"x\":\n\nMore generally, for any \"m\", \"n\" > 0, there exists a primitive recursive function formula_5 of \"m\" + 1 arguments that behaves as follows: for every Gödel number \"p\" of a partial computable function with \"m\" + \"n\" arguments, and all values of \"x\",…,\"x\":\n\nThe function \"s\" described above can be taken to be formula_7.\n\nFor all arities formula_8 and formula_9,\n\nFor every Turing Machine formula_10 of arity formula_11, and for all possible values of inputs formula_12,\n\nthere exists a Turing Machine formula_13 of arity formula_9, such that\n\nFurthermore, there is a Turing Machine formula_16 that allows formula_17 to be calculated from formula_18 and formula_19; it is denoted formula_20.\n\nFurthermore, the result generalizes to any Turing Complete computing model.\n\nInformally, formula_16 finds the Turing Machine formula_13 which is the result of hardcoding the values of formula_19 into formula_10.\n\nThe following Lisp code implements s for Lisp.\n\nFor example, evaluates to .\n\n\n"}
{"id": "27791", "url": "https://en.wikipedia.org/wiki?curid=27791", "title": "Sophie Germain", "text": "Sophie Germain\n\nMarie-Sophie Germain (; 1 April 1776 – 27 June 1831) was a French mathematician, physicist, and philosopher. Despite initial opposition from her parents and difficulties presented by society, she gained education from books in her father's library including ones by Leonhard Euler and from correspondence with famous mathematicians such as Lagrange, Legendre, and Gauss. One of the pioneers of elasticity theory, she won the grand prize from the Paris Academy of Sciences for her essay on the subject. Her work on Fermat's Last Theorem provided a foundation for mathematicians exploring the subject for hundreds of years after. Because of prejudice against her sex, she was unable to make a career out of mathematics, but she worked independently throughout her life. Before her death Gauss had recommended that she be awarded an honorary degree, but that never occurred. On June 27, 1831, she died from breast cancer . At the centenary of her life, a street and a girls’ school were named after her. The Academy of Sciences established the Sophie Germain Prize in her honor.\n\nMarie-Sophie Germain was born on April 1, 1776, in Paris, France, in a house on Rue Saint-Denis. According to most sources, her father, Ambroise-François, was a wealthy silk merchant, though some believe he was a goldsmith. In 1789, he was elected as a representative of the bourgeoisie to the États-Généraux, which he saw change into the Constitutional Assembly. It is therefore assumed that Sophie witnessed many discussions between her father and his friends on politics and philosophy. Gray proposes that after his political career, Ambroise-François became the director of a bank; at least, the family remained well-off enough to support Germain throughout her adult life.\n\nMarie-Sophie had one younger sister, named Angélique-Ambroise, and one older sister, named Marie-Madeline. Her mother was also named Marie-Madeline, and this plethora of \"Maries\" may have been the reason she went by Sophie. Germain's nephew Armand-Jacques Lherbette, Marie-Madeline's son, published some of Germain's work after she died (see Work in Philosophy).\n\nWhen Germain was 13, the Bastille fell, and the revolutionary atmosphere of the city forced her to stay inside. For entertainment she turned to her father's library. Here she found J. E. Montucla's \"L'Histoire des Mathématiques\", and his story of the death of Archimedes intrigued her.\n\nSophie Germain thought that if the geometry method, which at that time referred to all of pure mathematics, could hold such fascination for Archimedes, it was a subject worthy of study. So she pored over every book on mathematics in her father's library, even teaching herself Latin and Greek so she could read works like those of Sir Isaac Newton and Leonhard Euler. She also enjoyed \"Traité d'Arithmétique\" by Étienne Bézout and \"Le Calcul Différentiel\" by Jacques Antoine-Joseph Cousin. Later, Cousin visited her in her house, encouraging her in her studies.\n\nGermain's parents did not at all approve of her sudden fascination with mathematics, which was then thought inappropriate for a woman. When night came, they would deny her warm clothes and a fire for her bedroom to try to keep her from studying, but after they left she would take out candles, wrap herself in quilts and do mathematics. As Lynn Osen describes, when her parents found Sophie \"asleep at her desk in the morning, the ink frozen in the ink horn and her slate covered with calculations,\" they realized that their daughter was serious and relented. After some time, her mother even secretly supported her.\n\nIn 1794, when Germain was 18, the École Polytechnique opened. As a woman, Germain was barred from attending, but the new system of education made the \"lecture notes available to all who asked.\" The new method also required the students to \"submit written observations.\" Germain obtained the lecture notes and began sending her work to Joseph Louis Lagrange, a faculty member. She used the name of a former student Monsieur Antoine-August Le Blanc, \"fearing,\" as she later explained to Gauss, \"the ridicule attached to a female scientist.\" When Lagrange saw the intelligence of M. LeBlanc, he requested a meeting, and thus Sophie was forced to disclose her true identity. Fortunately, Lagrange did not mind that Germain was a woman, and he became her mentor. He visited her in her home, giving her moral support.\n\nGermain first became interested in number theory in 1798 when Adrien-Marie Legendre published \"Essai sur la théorie des nombres\". After studying the work, she opened correspondence with him on number theory, and later, elasticity. Legendre showed some of Germain's work in the \"Supplément\" to his second edition of the \"Théorie des Nombres\", where he calls it \"très ingénieuse\" [\"very ingenious\"] (See Her work on Fermat's Last Theorem).\n\nGermain's interest in number theory was renewed when she read Carl Friedrich Gauss' monumental work \"Disquisitiones Arithmeticae\". After three years of working through the exercises and trying her own proofs for some of the theorems, she wrote, again under the pseudonym of M. LeBlanc, to the author himself, who was one year younger than she. The first letter, dated 21 November 1804, discussed Gauss' \"Disquisitiones\" and presented some of Germain's work on Fermat's Last Theorem. In the letter, Germain claimed to have proved the theorem for \"n\" = \"p\" – 1, where \"p\" is a prime number of the form \"p\" = 8\"k\" + 7. However, her proof contained a weak assumption, and Gauss' reply did not comment on Germain's proof.\n\nAround 1807 (sources differ), during the Napoleonic wars, the French were occupying the German town of Braunschweig, where Gauss lived. Germain, concerned that he might suffer the fate of Archimedes, wrote to General Pernety, a family friend, requesting that he ensure Gauss' safety. General Pernety sent a chief of a battalion to meet with Gauss personally to see that he was safe. As it turned out, Gauss was fine, but he was confused by the mention of Sophie's name.\n\nThree months after the incident, Germain disclosed her true identity to Gauss. He replied,\nHow can I describe my astonishment and admiration on seeing my esteemed correspondent M leBlanc metamorphosed into this celebrated person. . . when a woman, because of her sex, our customs and prejudices, encounters infinitely more obstacles than men in familiarising herself with [number theory's] knotty problems, yet overcomes these fetters and penetrates that which is most hidden, she doubtless has the most noble courage, extraordinary talent, and superior genius.\nGauss' letters to Olbers show that his praise for Germain was sincere. In the same 1807 letter, Sophie claimed that if formula_1 is of the form formula_2 then formula_3 is also of that form. Gauss replied with a counterexample: formula_4 can be written as formula_2 but formula_6 cannot.\n\nAlthough Gauss thought well of Germain, his replies to her letters were often delayed, and he generally did not review her work. Eventually his interests turned away from number theory, and in 1809 the letters ceased. Despite the friendship of Germain and Gauss, they never met.\n\nWhen Germain's correspondence with Gauss ceased, she took interest in a contest sponsored by the Paris Academy of Sciences concerning Ernst Chladni's experiments with vibrating metal plates. The object of the competition, as stated by the Academy, was \"to give the mathematical theory of the vibration of an elastic surface and to compare the theory to experimental evidence.\" Lagrange's comment that a solution to the problem would require the invention of a new branch of analysis deterred all but two contestants, Denis Poisson and Germain. Then Poisson was elected to the Academy, thus becoming a judge instead of a contestant, and leaving Germain as the only entrant to the competition.\n\nIn 1809 Germain began work. Legendre assisted by giving her equations, references, and current research. She submitted her paper early in the fall of 1811, and did not win the prize. The judging commission felt that \"the true equations of the movement were not established,\" even though \"the experiments presented ingenious results.\" Lagrange was able to use Germain's work to derive an equation that was \"correct under special assumptions.\"\n\nThe contest was extended by two years, and Germain decided to try again for the prize. At first Legendre continued to offer support, but then he refused all help. Germain's anonymous 1813 submission was still littered with mathematical errors, especially involving double integrals, and it received only an honorable mention because \"the fundamental base of the theory [of elastic surfaces] was not established.\" The contest was extended once more, and Germain began work on her third attempt. This time she consulted with Poisson. In 1814 he published his own work on elasticity, and did not acknowledge Germain's help (although he had worked with her on the subject and, as a judge on the Academy commission, had had access to her work).\n\nGermain submitted her third paper, \"Recherches sur la théorie des surfaces élastiques\" under her own name, and on 8 January 1816 she became the first woman to win a prize from the Paris Academy of Sciences. She did not appear at the ceremony to receive her award. Although Germain had at last been awarded the \"prix extraordinaire\", the Academy was still not fully satisfied. Sophie had derived the correct differential equation, but her method did not predict experimental results with great accuracy, as she had relied on an incorrect equation from Euler, which led to incorrect boundary conditions. Here is Germain's final equation:\n\nwhere \"N\" is a constant.\n\nAfter winning the Academy contest, she was still not able to attend its sessions because of the Academy's tradition of excluding women other than the wives of members. Seven years later this situation was transformed when she made friends with Joseph Fourier, a secretary of the Academy, who obtained tickets to the sessions for her.\n\nGermain published her prize-winning essay at her own expense in 1821, mostly because she wanted to present her work in opposition to that of Poisson. In the essay she pointed out some of the errors in her method.\n\nIn 1826 she submitted a revised version of her 1821 essay to the Academy. According to Andrea Del Centina, the revision included attempts to clarify her work by \"introducing certain simplifying hypotheses.\" This put the Academy in an awkward position, as they felt the paper to be \"inadequate and trivial,\" but they did not want to \"treat her as a professional colleague, as they would any man, by simply rejecting the work.\" So Augustin-Louis Cauchy, who had been appointed to review her work, recommended she publish it, and she followed his advice.\n\nOne further work of Germain's on elasticity was published posthumously in 1831: her \"Mémoire sur la courbure des surfaces.\" She used the mean curvature in her research (see Honors in Number Theory).\n\nGermain's best work was in number theory, and her most significant contribution to number theory dealt with Fermat's Last Theorem. In 1815, after the elasticity contest, the Academy offered a prize for a proof of Fermat's Last Theorem. It reawakened Germain's interest in number theory, and she wrote to Gauss again after ten years of no correspondence.\n\nIn the letter, Germain said that number theory was her preferred field, and that it was in her mind all the time she was studying elasticity. She outlined a strategy for a general proof of Fermat's Last Theorem, including a proof for a special case. Germain's letter to Gauss contained her substantial progress toward a proof. She asked Gauss if her approach to the theorem was worth pursuing. Gauss never answered.\n\nFermat's Last Theorem can be divided into two cases. Case 1 involves all \"p\" that do not divide any of \"x\", \"y\", or \"z\". Case 2 includes all \"p\" that divide at least one of \"x\", \"y\", or \"z\". Germain proposed the following, commonly called \"Sophie Germain's theorem\":\nLet \"p\" be an odd prime. If there exists an auxiliary prime \"P\" = 2\"Np\" + 1 (N any positive integer not divisible by 3) such that:\nThen the first case of Fermat's Last Theorem holds true for \"p\".\nGermain used this result to prove the first case of Fermat's Last Theorem for all odd primes \"p\"<100, but according to Andrea Del Centina, \"she had actually shown that it holds for every exponent \"p\"<197.\" L. E. Dickson later used Germain's theorem to prove Fermat's Last Theorem for odd primes less than 1700.\n\nIn an unpublished manuscript entitled \"Remarque sur l'impossibilité de satisfaire en nombres entiers a l'équation x + y = z\", Germain showed that any counterexamples to Fermat's theorem for \"p\">5 must be numbers \"whose size frightens the imagination,\" around 40 digits long. Sophie did not publish this work. Her brilliant theorem is known only because of the footnote in Legendre's treatise on number theory, where he used it to prove Fermat's Last Theorem for \"p\" = 5 (see Correspondence with Legendre). Germain also proved or nearly proved several results that were attributed to Lagrange or were rediscovered years later. Del Centina states that \"after almost two hundred years her ideas were still central\", but ultimately her method did not work.\n\nIn addition to mathematics, Germain studied philosophy and psychology. She wanted to classify facts and generalize them into laws that could form a system of psychology and sociology, which were then just coming into existence. Her philosophy was highly praised by Auguste Comte.\n\nTwo of her philosophical works, \"Pensées diverses\" and \"Considérations générales sur l'état des sciences et des lettres, aux différentes époques de leur culture\", were published, both posthumously. This was due in part to the efforts of Lherbette, her nephew, who collected her philosophical writings and published them. \"Pensées\" is a history of science and mathematics with Sophie's commentary. In \"Considérations\", the work admired by Comte, Sophie argues that there are no differences between the sciences and the humanities.\n\nIn 1829 Germain learned she had breast cancer. Despite the pain, she continued to work. In 1831 \"Crelle's Journal\" published her paper on the curvature of elastic surfaces and \"a note about finding and in formula_8.\" Mary Gray records, \"She also published in \"Annales de chimie et de physique\" an examination of principles which led to the discovery of the laws of equilibrium and movement of elastic solids.\" On 27 June 1831, she died in the house at 13 rue de Savoie.\n\nDespite Germain's intellectual achievements, her death certificate lists her as a \"rentière – annuitant\" (property holder), not a \"mathématicienne.\" But her work was not unappreciated by everyone. When the matter of honorary degrees came up at the University of Göttingen in 1837—six years after Germain's death—Gauss lamented, \"she [Germain] proved to the world that even a woman can accomplish something worthwhile in the most rigorous and abstract of the sciences and for that reason would well have deserved an honorary degree.\"\n\nGermain's resting place in the Père Lachaise Cemetery in Paris is marked by a gravestone. At the centennial celebration of her life, a street and a girls' school were named after her, and a plaque was placed at the house where she died. The school houses a bust commissioned by the Paris City Council.\n\nE. Dubouis defined a \"sophien\" of a prime to be a prime where , for such that yield such that has no solutions when and are prime to .\n\nA Sophie Germain prime is a prime such that is also prime.\n\nThe \"Germain curvature\" (also called mean curvature) is formula_9, when and are the maximum and minimum values of the normal curvature.\n\n\"Sophie Germain's Identity\" states that for any }, then,\n\nVesna Petrovich found that the educated world's response to the publication in 1821 of Germain's prize-winning essay \"ranged from polite to indifferent\". Yet, some critics had high praise for it. Of her essay in 1821, Cauchy said, \"[it] was a work for which the name of its author and the importance of the subject both deserved the attention of mathematicians.\" Germain was also included in H. J. Mozans' book \"Woman in Science\", although Marilyn Bailey Ogilvie claims that the biography \"is inaccurate and the notes and bibliography are unreliable\". Nevertheless, it quotes the mathematician Claude-Louis Navier as saying, \"it is a work which few men are able to read and which only one woman was able to write.\"\n\nGermain's contemporaries also had good things to say relating to her work in mathematics. Osen relates that \"Baron de Prony called her the Hypatia of the nineteenth century,\" and \"J.J Biot wrote, in the \"Journal de Savants\", that she had probably penetrated the science of mathematics more deeply than any other of her sex.\" Gauss certainly thought highly of her, and he recognized that European culture presented special difficulties to a woman in mathematics (see Correspondence with Gauss).\n\nThe modern view generally acknowledges that although Germain had great talent as a mathematician, her haphazard education had left her without the strong base she needed to truly excel. As explained by Gray, \"Germain's work in elasticity suffered generally from an absence of rigor, which might be attributed to her lack of formal training in the rudiments of analysis.\" Petrovich adds, \"This proved to be a major handicap when she could no longer be regarded as a young prodigy to be admired but was judged by her peer mathematicians.\"\n\nNot withstanding the problems with Germain's theory of vibrations, Gray states that \"Germain's work was fundamental in the development of a general theory of elasticity.\" Mozans writes, however, that when the Eiffel tower was built and the architects inscribed the names of 72 great French scientists, Germain's name was not among them: despite the salience of her work to the tower's construction. Mozans asked, \"Was she excluded from this list... because she was a woman? It would seem so.\"\n\nConcerning her early work in number theory, J. H. Sampson states, \"She was clever with formal algebraic manipulations; but there is little evidence that she really understood the \"Disquisitiones\", and her work of that period that has come down to us seems to touch only on rather superficial matters.\" Gray adds that \"The inclination of sympathetic mathematicians to praise her work rather than to provide substantive criticism from which she might learn was crippling to her mathematical development.\" Yet Marilyn Bailey Ogilvie recognizes that \"Sophie Germain's creativity manifested itself in pure and applied mathematics...[she] provided imaginative and provocative solutions to several important problems,\" and, as Petrovich proposes, it may have been her very lack of training that gave her unique insights and approaches. Louis Bucciarelli and Nancy Dworsky, Germain's biographers, summarize as follows: \"All the evidence argues that Sophie Germain had a mathematical brilliance that never reached fruition due to a lack of rigorous training available only to men.\"\n\nGermain was referenced and quoted in David Auburn's 2001 play \"Proof.\" The protagonist is a young struggling female mathematician, Catherine, who found great inspiration in the work of Germain. Germain was also mentioned in John Madden's film adaptation of the same play in a conversation between Catherine (Gwyneth Paltrow) and Hal (Jake Gyllenhaal).\n\nIn the fictional work \"The Last Theorem\" by Arthur C. Clarke and Frederik Pohl, Sophie Germain was credited with inspiring the central character, Ranjit Subramanian, to solve Fermat's Last Theorem.\n\nThe Sophie Germain Prize (Prix Sophie Germain), awarded annually by the Foundation Sophie Germain, is conferred by the Academy of Sciences in Paris. Its purpose is to honour a French mathematician for research in the foundations of mathematics. This award, in the amount of €8,000, was established in 2003, under the auspices of the Institut de France.\n\n\n\n"}
{"id": "28734956", "url": "https://en.wikipedia.org/wiki?curid=28734956", "title": "Spherical shell", "text": "Spherical shell\n\nIn geometry, a spherical shell is a generalization of an annulus to three dimensions. It is the region between two concentric spheres of differing radii.\n\nThe volume of a spherical shell is the difference between the enclosed volume of the outer sphere and the enclosed volume of the inner sphere:\nwhere is the radius of the inner sphere and \"R\" is the radius of the outer sphere.\n\nAn approximation for the volume of a thin spherical shell is the area of the outer sphere multiplied by the thickness \"t\" of the shell:\nwhen \"t\" is very small compared to \"r\" (formula_4).\n\nA Dyson sphere encloses a fictitious spherical shell around a star, as first described by author Olaf Stapledon.\n\n"}
{"id": "1140043", "url": "https://en.wikipedia.org/wiki?curid=1140043", "title": "Squeeze mapping", "text": "Squeeze mapping\n\nIn linear algebra, a squeeze mapping is a type of linear map that preserves Euclidean area of regions in the Cartesian plane, but is \"not\" a rotation or shear mapping.\n\nFor a fixed positive real number , the mapping\n\nis the \"squeeze mapping\" with parameter . Since\n\nis a hyperbola, if and , then and the points of the image of the squeeze mapping are on the same hyperbola as is. For this reason it is natural to think of the squeeze mapping as a hyperbolic rotation, as did Émile Borel in 1914, by analogy with \"circular rotations\", which preserve circles.\n\nThe squeeze mapping sets the stage for development of the concept of logarithms. The problem of finding the area bounded by a hyperbola (such as is one of quadrature. The solution, found by Grégoire de Saint-Vincent and Alphonse Antonio de Sarasa in 1647, required the natural logarithm function, a new concept. Some insight into logarithms comes through hyperbolic sectors that are permuted by squeeze mappings while preserving their area. The area of a hyperbolic sector is taken as a measure of a hyperbolic angle associated with the sector. The hyperbolic angle concept is quite independent of the ordinary circular angle, but shares a property of invariance with it: whereas circular angle is invariant under rotation, hyperbolic angle is invariant under squeeze mapping. Both circular and hyperbolic angle generate invariant measures but with respect to different transformation groups. The hyperbolic functions, which take hyperbolic angle as argument, perform the role that circular functions play with the circular angle argument.\n\nIf and are positive real numbers, the composition of their squeeze mappings is the squeeze mapping of their product. Therefore, the collection of squeeze mappings forms a one-parameter group isomorphic to the multiplicative group of positive real numbers. An additive view of this group arises from consideration of hyperbolic sectors and their hyperbolic angles.\n\nFrom the point of view of the classical groups, the group of squeeze mappings is , the identity component of the indefinite orthogonal group of 2 × 2 real matrices preserving the quadratic form . This is equivalent to preserving the form via the change of basis\n\nand corresponds geometrically to preserving hyperbolae. The perspective of the group of squeeze mappings as hyperbolic rotation is analogous to interpreting the group (the connected component of the definite orthogonal group) preserving quadratic form as being \"circular rotations\".\n\nNote that the \" notation corresponds to the fact that the reflections\n\nare not allowed, though they preserve the form (in terms of and these are and ; the additional \" in the hyperbolic case (as compared with the circular case) is necessary to specify the identity component because the group has connected components, while the group has components: has components, while only has 1. The fact that the squeeze transforms preserve area and orientation corresponds to the inclusion of subgroups – in this case – of the subgroup of hyperbolic rotations in the special linear group of transforms preserving area and orientation (a volume form). In the language of Möbius transformations, the squeeze transformations are the hyperbolic elements in the classification of elements.\n\nIn studying linear algebra there are the purely abstract applications such as illustration of the singular-value decomposition or in the important role of the squeeze mapping in the structure of 2 × 2 real matrices. These applications are somewhat bland compared to two physical and a philosophical application.\n\nIn fluid dynamics one of the fundamental motions of an incompressible flow involves bifurcation of a flow running up against an immovable wall.\nRepresenting the wall by the axis \"y\" = 0 and taking the parameter \"r\" = exp(\"t\") where \"t\" is time, then the squeeze mapping with parameter \"r\" applied to an initial fluid state produces a flow with bifurcation left and right of the axis \"x\" = 0. The same model gives fluid convergence when time is run backward. Indeed, the area of any hyperbolic sector is invariant under squeezing.\n\nFor another approach to a flow with hyperbolic streamlines, see .\n\nIn 1989 Ottino described the \"linear isochoric two-dimensional flow\" as\nwhere K lies in the interval [−1, 1]. The streamlines follow the curves\nso negative \"K\" corresponds to an ellipse and positive \"K\" to a hyperbola, with the rectangular case of the squeeze mapping corresponding to \"K\" = 1.\n\nStocker and Hosoi described their approach to corner flow as follows:\nStocker and Hosoi then recall Moffatt's consideration of \"flow in a corner between rigid boundaries, induced by an arbitrary disturbance at a large distance.\" According to Stocker and Hosoi,\n\nSelect (0,0) for a \"here and now\" in a spacetime. Light radiant left and right through this central event tracks two lines in the spacetime, lines that can be used to give coordinates to events away from (0,0). Trajectories of lesser velocity track closer to the original timeline (0,\"t\"). Any such velocity can be viewed as a zero velocity under a squeeze mapping called a Lorentz boost. This insight follows from a study of split-complex number multiplications and the diagonal basis which corresponds to the pair of light lines.\nFormally, a squeeze preserves the hyperbolic metric expressed in the form \"xy\"; in a different coordinate system. This application in the theory of relativity was noted in 1912 by Wilson and Lewis, by Werner Greub, and by Louis Kauffman. Furthermore, Wolfgang Rindler, in his popular textbook on relativity, used the squeeze mapping form of Lorentz transformations in his demonstration of their characteristic property.\n\nThe term \"squeeze transformation\" was used in this context in an article connecting the Lorentz group with Jones calculus in optics.\n\nThe area-preserving property of squeeze mapping has an application in setting the foundation of the transcendental functions natural logarithm and its inverse the exponential function:\n\nDefinition: Sector(\"a,b\") is the hyperbolic sector obtained with central rays to (\"a\", 1/\"a\") and (\"b\", 1/\"b\").\n\nLemma: If \"bc\" = \"ad\", then there is a squeeze mapping that moves the sector(\"a,b\") to sector(\"c,d\").\n\nProof: Take parameter \"r\" = \"c\"/\"a\" so that (\"u,v\") = (\"rx\", \"y\"/\"r\") takes (\"a\", 1/\"a\") to (\"c\", 1/\"c\") and (\"b\", 1/\"b\") to (\"d\", 1/\"d\").\n\nTheorem (Gregoire de Saint-Vincent 1647) If \"bc\" = \"ad\", then the quadrature of the hyperbola \"xy\" = 1 against the asymptote has equal areas between \"a\" and \"b\" compared to between \"c\" and \"d\".\n\nProof: An argument adding and subtracting triangles of area ½, one triangle being {(0,0), (0,1), (1,1)}, shows the hyperbolic sector area is equal to the area along the asymptote. The theorem then follows from the lemma.\n\nTheorem (Alphonse Antonio de Sarasa 1649) As area measured against the asymptote increases in arithmetic progression, the projections upon the asymptote increase in geometric sequence. Thus the areas form \"logarithms\" of the asymptote index.\n\nFor instance, for a standard position angle which runs from (1, 1) to (\"x\", 1/\"x\"), one may ask \"When is the hyperbolic angle equal to one?\" The answer is the transcendental number x = e.\n\nA squeeze with \"r\" = e moves the unit angle to one between (\"e\", 1/\"e\") and (\"ee\", 1/\"ee\") which subtends a sector also of area one. The geometric progression\ncorresponds to the asymptotic index achieved with each sum of areas\nwhich is a proto-typical arithmetic progression \"A\" + \"nd\" where \"A\" = 0 and \"d\" = 1 .\n\n\n"}
{"id": "30502017", "url": "https://en.wikipedia.org/wiki?curid=30502017", "title": "State-merging", "text": "State-merging\n\nIn quantum information theory, quantum state merging is the transfer of a quantum state when the receiver already has part of the state. The process optimally transfers partial information using entanglement and classical communication. It allows for sending information using an amount of entanglement given by the conditional quantum entropy, formula_1 with formula_2 the Von Neumann entropy, formula_3. It thus provides an operational meaning to this quantity.\n\nUnlike its classical counterpart, the quantum conditional entropy can be negative. In this case, the sender can transfer the state to the receiver using no entanglement, and as an added bonus, this amount of entanglement can be gained, rather than used. Thus quantum information can be negative.\n\nThe amount of classical information needed is the mutual information formula_4. The case where the classical communication is replaced by quantum communication was considered in. This is known as the Fully Quantum Slepian-Wolf Theorem, since everything is sent down the quantum channel. A single-shot version of state merging was found by Berta, and a multiparty single shot version was found in. The quantum discord has been interpreted using state merging.\n"}
{"id": "57091271", "url": "https://en.wikipedia.org/wiki?curid=57091271", "title": "Three-gap theorem", "text": "Three-gap theorem\n\nIn mathematics, the three-gap theorem, three-distance theorem, or Steinhaus conjecture states that if one places \"n\" points on a circle, at angles of \"θ\", 2\"θ\", 3\"θ\" ... from the starting point, then there will be at most three distinct distances between pairs of points in adjacent positions around the circle. When there are three distances, the largest of the three always equals the sum of the other two. Unless \"θ\" is a rational multiple of , there will also be at least two distinct distances.\n\nThis result was conjectured by Hugo Steinhaus, and proved in the 1950s by Vera T. Sós, , and Stanisław Świerczkowski. Its applications include the study of plant growth and musical tuning systems, and the theory of Sturmian words.\n\nIn phyllotaxis (the theory of plant growth), it has been observed that each successive leaf on the stems of many plants is turned from the previous leaf by the golden angle, approximately 137.5°. It has been suggested that this angle maximizes the sun-collecting power of the plant's leaves. If one looks end-on at a plant stem that has grown in this way, there will be at most three distinct angles between two leaves that are consecutive in the cyclic order given by this end-on view. In the figure, the largest of these three angles occurs three times, between the leaves numbered 3 and 6, between leaves 4 and 7, and between leaves 5 and 8. The second-largest angle occurs five times, between leaves 6 and 1, 9 and 4, 7 and 2, 10 and 5, and 8 and 3. And the smallest angle occurs only twice, between leaves 1 and 9 and between leaves 2 and 10. (This phenomenon has nothing to do with the golden ratio; the same property, of having only three distinct gaps between consecutive points on a circle, happens for any other rotation angle, and not just for the golden angle.)\nIn music theory, this theorem implies that if a tuning system is generated by some number of consecutive multiples of a given interval, reduced to a cyclic sequence by considering two tones to be equivalent when they differ by whole numbers of octaves, then there are at most three different intervals between consecutive tones of the scale. For instance, the Pythagorean tuning is constructed in this way from multiples of a perfect fifth. It has only two distinct intervals representing its semitones, but if it were extended by one more step then the sequence of intervals between its tones would include a third shorter interval, the Pythagorean comma.\n\nIn the theory of Sturmian words, the theorem implies that the words of a given length \"n\" that appear within a given Sturmian word have at most three distinct frequencies. If there are three frequencies, then one of them must equal the sum of the other two.\n\nThe three-gap theorem was conjectured by Hugo Steinhaus, and its first proofs were published in the late 1950s by Vera T. Sós, , and Stanisław Świerczkowski. Several later proofs have also been published.\n\nThe following simple proof is due to Frank Liang. Define a gap (an arc of the circle between adjacent points of the given set) to be \"rigid\" if rotating that gap by an angle of \"θ\" does not produce another gap of the same length. Each rotation by \"θ\" increases the position of the gap endpoints in the placement ordering of the points, and such an increase cannot be repeated indefinitely, so every gap has the same length as a rigid gap. But the only ways for a gap to be rigid are for one of its two endpoints to be the last point in the placement sequence (so that the corresponding point is missing from the rotated gap) or for another point to land in its rotated copy. An endpoint can only be missing if the gap is one of the two gaps on either side of the last point in the placement ordering. And a point can only land within the rotated copy if it is the first point in the placement ordering. So there can be at most three rigid gaps, and at most three lengths of gaps. Additionally, when there are three, the rotated copy of a rigid gap that has the first point in it is partitioned by that point into two smaller gaps, so in this case the longest gap length is the sum of the other two.\n\nA closely related but earlier theorem, also called the three-gap theorem, is that if \"A\" is any arc of the circle, then the integer sequence of multiples of \"θ\" that land in \"A\" has at most three gaps between sequence values. Again, if there are three gaps then one is the sum of the other two.\n"}
{"id": "2994276", "url": "https://en.wikipedia.org/wiki?curid=2994276", "title": "Thue's lemma", "text": "Thue's lemma\n\nIn modular arithmetic, Thue's lemma roughly states that every modular integer may be represented by a \"modular fraction\" such that the numerator and the denominator have absolute values not greater than the square root of the modulus.\n\nMore precisely for every pair of integers with , given two positive integers and such that , there are two integers and such that \nand \nUsually, one takes and equal to the smallest integer greater than the square root of , but the general form is sometimes useful, and make the unicity theorem (below) easier to state.\n\nThe first known proof is attributed to , who used a pigeonhole argument. It can be used to prove Fermat's theorem on sums of two squares by taking \"m\" to be a prime \"p\" that is 1 mod 4 and taking \"a\" to satisfy \"a\" + 1 = 0 mod \"p\". (Such an \"a\" is guaranteed for \"p\" by Wilson's Theorem.)\n\nIn general, the solution whose existence is asserted by Thue's lemma is not unique. For example, when there are usually several solutions , providing that and are not too small. Therefore, one may only hope unicity for the rational number , to which is congruent modulo if \"y\" and \"m\" are coprime. Nevertheless, this rational number needs not to be unique; for example, if , and , one has the two solutions\n\nHowever, for and small enough, if a solution exists, it is unique. More precisely, with above notation, if \nand \nwith \nand \nthen\n\nThis result is the basis for rational reconstruction, which allows using modular arithmetic for computing rational numbers for which one knows bounds for numerators and denominators.\n\nThe proof is rather easy: by multiplying each congruence by the other and subtracting, one gets\nThe hypotheses imply that each term has an absolute value lower than , and thus that the absolute value of their difference is lower than . This implies that formula_10, and thus the result.\n\nThe original proof of Thue's lemma is not efficient, in the sense that it does not provide any fast method for computing the solution. \nThe extended Euclidean algorithm, allows us to provide a proof that leads to an efficient algorithm that has the same computational complexity of the Euclidean algorithm.\n\nMore precisely, given the two integers and appearing in Thue's lemma, the extended Euclidean algorithm computes three sequences of integers , and such that\nwhere the are non-negative and strictly decreasing. The desired solution is, up to the sign, the first pair such that .\n\n"}
{"id": "22253525", "url": "https://en.wikipedia.org/wiki?curid=22253525", "title": "Verena Huber-Dyson", "text": "Verena Huber-Dyson\n\nVerena Esther Huber-Dyson (May 6, 1923 – March 12, 2016) was a Swiss-American mathematician, known for work in group theory and formal logic. She has been described as a \"brilliant mathematician\", and did research on the interface between algebra and logic, focusing on undecidability in group theory. At the time of her death she was emeritus faculty in the philosophy department of the University of Calgary, Alberta.\n\nHuber-Dyson was born Verena Esther Huber in Naples, Italy, on May 6, 1923. Her parents, Karl (Charles) Huber (1893-1946) and Berthy Ryffel (1899-1945), were Swiss nationals who raised Verena and her sister Adelheid (\"Heidi\", 1925-1987) in Athens, Greece, where the girls attended the German-speaking \"Deutsche Schule\", or German School of Athens, until forced to return to Switzerland in 1940 by the war.\n\nCharles Huber, who had managed the Middle Eastern operations of Bühler AG, a Swiss food-process engineering firm, began working for the International Committee of the Red Cross (ICRC), monitoring the treatment of prisoners of war in internment camps. As the ICRC delegate to India and Ceylon, he was responsible for Italian prisoners held in British camps, but also visited German and Allied camps in Europe, and in 1945-46 served as an ICRC delegate to the United States, which he described to Verena as a place she \"definitely ought to experience at length and in depth but just as definitely ought not to settle in.\"\n\nShe studied mathematics, with minors in physics and philosophy, at the University of Zurich, where she obtained her Ph.D in mathematics in 1947 with a thesis in finite group theory. under the supervision of Andreas Speiser.\n\nVerena married Hans-Georg Haefeli, a fellow mathematician, in 1942, and was divorced in 1948. Her first daughter, Katarina Haefeli (now Halm), was born in 1945. \n\nShe subsequently married Freeman Dyson in Ann Arbor, Michigan, on August 11, 1950. They had two children together, Esther Dyson (born July 14, 1951, in Zurich) and George Dyson (born 1953, Ithaca, New York) and divorced in 1958.\n\nHuber-Dyson accepted a postdoctoral fellow appointment at the Institute for Advanced Study in Princeton in 1948, where she worked on group theory and formal logic. She also began teaching at Goucher College near Baltimore during this time.\n\nShe moved to California with her daughter Katarina, began teaching at San Jose State University in 1959, and then joined Alfred Tarski's Group in Logic and the Methodology of Science at the University of California, Berkeley. \n\nHuber-Dyson taught at San Jose State University, the University of Zürich, University of Monash, as well as at UC Berkeley, Adelphi University, UCLA, and the University of Illinois at Chicago, in mathematics and in philosophy departments. She accepted a position in the philosophy department of the University of Calgary in 1973, becoming emerita in 1988.\n\n\n\n\n\nAfter retiring from Calgary, Verena Huber-Dyson moved to South Pender Island in British Columbia, where she lived for 14 years. She died on March 12, 2016 in Bellingham, Washington, at the age of 92.\n\n\n\n"}
