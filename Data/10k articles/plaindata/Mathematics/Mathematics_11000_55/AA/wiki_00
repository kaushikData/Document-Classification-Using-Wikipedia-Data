{"id": "1315827", "url": "https://en.wikipedia.org/wiki?curid=1315827", "title": "152 (number)", "text": "152 (number)\n\n152 (one hundred [and] fifty-two) is the natural number following 151 and preceding 153.\n\n152 is the sum of four consecutive primes (31 + 37 + 41 + 43). It is a nontotient since there is no integer with 152 coprimes below it.\n\n152 is a refactorable number since it is divisible by the total number of divisors it has, and in base 10 it is divisible by the sum of its digits, making it a Harshad number.\n\nRecently, the smallest repunit probable prime in base 152 was found, it has 589570 digits.\n\n\n\n\n152 is also:\n\n\n"}
{"id": "391876", "url": "https://en.wikipedia.org/wiki?curid=391876", "title": "83 (number)", "text": "83 (number)\n\n83 (eighty-three) is the natural number following 82 and preceding 84.\n\n83 is:\n\n\n\n\n\n\n\nEighty-three is also:\n"}
{"id": "18654902", "url": "https://en.wikipedia.org/wiki?curid=18654902", "title": "Architectural plan", "text": "Architectural plan\n\nIn the field of architecture an architectural plan is a design and planning for a building, and can contain architectural drawings, specifications of the design, calculations, time planning of the building process, and other documentation. \n\nThe term \"Architectural plan\" can have multiple related meanings:\nThis article will focus on the general meaning of architectural plan as a plan and documentation for a building project.\n\nA building is a man-made structure with a roof and walls standing more or less permanently in one place. Buildings come in a variety of shapes, sizes and functions, and have been adapted throughout history for a wide number of factors, from building materials available, to weather conditions, to land prices, ground conditions, specific uses and aesthetic reasons. To better understand the term building compare the list of nonbuilding structures. The gallery below gives an overview of different types of building.\nThe practice of designing, constructing, and operating buildings is most usually a collective effort of different groups of professionals and trades. Depending on the size, complexity, and purpose of a particular building project.\n\nA design process may include a series of steps followed by designers. Depending on the product or service, some of these stages may be irrelevant, ignored in real-world situations in order to save time, reduce cost, or because they may be redundant in the situation. Typical stages of the design process include:\n\nArchitectural drawings are used by architects and others for a number of purposes: to develop a design idea into a coherent proposal, to communicate ideas and concepts, to convince clients of the merits of a design, to enable a building contractor to construct it, as a record of the completed work, and to make a record of a building that already exists.\n\nArchitectural drawings are made according to a set of conventions, which include particular views (floor plan, section etc.), sheet sizes, units of measurement and scales, annotation and cross referencing. Conventionally, drawings were made in ink on paper or a similar material, and any copies required had to be laboriously made by hand. The twentieth century saw a shift to drawing on tracing paper, so that mechanical copies could be run off efficiently.\n\nArchitectural design values make up an important part of what influences architects and designers when they make their design decisions. However, architects and designers are not always influenced by the same values and intentions. Value and intentions differ between different architectural movements. It also differs between different schools of architecture and schools of design as well as among individual architects and designers.\n\nOne of the major tools in architectural design is the floor plan. This diagram shows the relationships between rooms, spaces and other physical features at one level of a structure. Dimensions are usually drawn between the walls to specify room sizes and wall lengths. Floor plans will also include details of fixtures like sinks, water heaters, furnaces, etc. Floor plans will include notes to specify finishes, construction methods, or symbols for electrical items.\n\nSimilar to a map in a floor plan the orientation of the view is downward from above, but unlike a conventional map, a plan is understood to be drawn at a particular vertical position (commonly at about 4 feet above the floor). Objects below this level are seen, objects at this level are shown 'cut' in plan-section, and objects above this vertical position within the structure are omitted or shown dashed. Plan view or \"planform\" is defined as a vertical orthographic projection of an object on a horizontal plane, like a map.\n\nA plan is typically any procedure used to achieve an objective. It is a set of intended actions, through which one expects to achieve a goal. Plans can be formal or informal:\n\nA lack of planning in any discipline may lead to a misallocation of resources, misunderstandings, or irrelevant sections added to Wikipedia articles such as this one.\n\nBuilding construction is the process of preparing for and forming buildings and building systems. Construction starts with planning, design, and financing and continues until the structure is ready for occupancy. Far from being a single activity, large scale construction is a feat of human multitasking. Normally, the job is managed by a project manager, and supervised by a construction manager, design engineer, construction engineer or project architect. For the successful execution of a project, effective planning is essential.\n\nGarden design is the art and process of designing and creating plans for layout and planting of gardens and landscapes. Garden design may be done by the garden owner themselves, or by professionals of varying levels of experience and expertise. Most professional garden designers are trained in principles of design and in horticulture, and have an expert knowledge and experience of using plants. Some professional garden designers are also landscape architects, a more formal level of training that usually requires an advanced degree and often a state license. Many amateur gardeners also attain a high level of experience from extensive hours working in their own gardens, through casual study or Master Gardener Programs offered by the American Horticultural Society.\n\nLandscape planning is a branch of landscape architecture. Urban park systems and greenways of the type planned by Frederick Law Olmsted are key examples of urban landscape planning. Landscape designers tend to work for clients who wish to commission construction work. Landscape planners can look beyond the 'closely drawn technical limits' and 'narrowly drawn territorial boundaries' which constrain design projects.\n\nLandscape planners tend to work on projects which:\nIn rural areas, the damage caused by unplanned mineral extraction was one of the early reasons for a public demand for landscape planning.\n\nA site plan is an architectural plan, and a detailed engineering drawing of proposed improvements to a given lot. A site plan usually shows a building footprint, travelways, parking, drainage facilities, sanitary sewer lines, water lines, trails, lighting, and landscaping.\n\nSuch a plan of a site is a graphic representation of the arrangement of buildings, parking, drives, landscaping and any other structure that is part of a development project.\n\nA site plan is a set of construction drawings that a builder or contractor uses to make improvements to a property. Counties can use the site plan to verify that development codes are being met and as a historical resource. Site plans are often prepared by a design consultant who must be either a licensed engineer, architect, landscape architect or land survey. The architect Map is part of a plan in Chandler, AZ.\n\nTransportation planning is the field involved with the siting of transportation facilities (generally streets, highways, sidewalks, bike lanes and public transport lines).\n\nTransportation planning historically has followed the rational planning model of defining goals and objectives, identifying problems, generating alternatives, evaluating alternatives, and developing the plan. Other models for planning include rational actor, satisficing, incremental planning, organizational process, and political bargaining. However, planners are increasingly expected to adopt a multi-disciplinary approach, especially due to the rising importance of environmentalism. For example, the use of behavioral psychology to persuade drivers to abandon their automobiles and use public transport instead. The role of the transport planner is shifting from technical analysis to promoting sustainability through integrated transport policies.\n\nUrban, city, and town planning is the integration of the disciplines of land use planning and transport planning, to explore a very wide range of aspects of the built and social environments of urbanized municipalities and communities. Regional planning deals with a still larger environment, at a less detailed level.\n\nAnother key role of urban planning is urban renewal, and re-generation of inner cities by adapting urban planning methods to existing cities suffering from long-term infrastructural decay. The picture below is an architecture map of a part of south Chandler, AZ.\n"}
{"id": "3987703", "url": "https://en.wikipedia.org/wiki?curid=3987703", "title": "Book embedding", "text": "Book embedding\n\nIn graph theory, a book embedding is a generalization of planar embedding of a graph to embeddings into a \"book\", a collection of half-planes all having the same line as their boundary. Usually, the vertices of the graph are required to lie on this boundary line, called the \"spine\", and the edges are required to stay within a single half-plane. The book thickness of a graph is the smallest possible number of half-planes for any book embedding of the graph. Book thickness is also called pagenumber, stacknumber or fixed outerthickness. Book embeddings have also been used to define several other graph invariants including the pagewidth and book crossing number.\n\nEvery graph with vertices has book thickness at most formula_1, and this formula gives the exact book thickness for complete graphs. The graphs with book thickness one are the outerplanar graphs. The graphs with book thickness at most two are the subhamiltonian graphs, which are always planar; more generally, every planar graph has book thickness at most four. All minor-closed graph families, and in particular the graphs with bounded treewidth or bounded genus, also have bounded book thickness. It is NP-hard to determine the exact book thickness of a given graph, with or without knowing a fixed vertex ordering along the spine of the book.\n\nOne of the original motivations for studying book embeddings involved applications in VLSI design, in which the vertices of a book embedding represent components of a circuit and the wires represent connections between them.\nBook embedding also has applications in graph drawing, where two of the standard visualization styles for graphs, arc diagrams and circular layouts, can be constructed using book embeddings.\n\nIn transportation planning, the different sources and destinations of foot and vehicle traffic that meet and interact at a traffic light can be modeled mathematically as the vertices of a graph, with edges connecting different source-destination pairs. A book embedding of this graph can be used to design a schedule that lets all the traffic move across the intersection with as few signal phases as possible.\nIn bioinformatics problems involving the folding structure of RNA, single-page book embeddings represent classical forms of nucleic acid secondary structure, and two-page book embeddings represent pseudoknots.\nOther applications of book embeddings include abstract algebra and knot theory.\n\nThere are several open problems concerning book thickness. It is unknown whether the book thickness of an arbitrary graph can be bounded by a function of the book thickness of its subdivisions.\nTesting the existence of a three-page book embedding of a graph, given a fixed ordering of the vertices along the spine of the embedding, has unknown computational complexity: it is neither known to be solvable in polynomial time nor known to be NP-hard. And, although every planar graph has book thickness at most four, it is unknown whether there exists a planar graph whose book thickness is exactly four.\n\nThe notion of a book, as a topological space, was defined by C. A. Persinger and Gail Atneosen in the 1960s. As part of this work, Atneosen already considered embeddings of graphs in books. The embeddings he studied used the same definition as embeddings of graphs into any other topological space: vertices are represented by distinct points, edges are represented by curves, and the only way that two edges can intersect is for them to meet at a common endpoint.\n\nIn the early 1970s, Paul C. Kainen and L. Taylor Ollmann developed a more restricted type of embedding that came to be used in most subsequent research. In their formulation, the graph's vertices must be placed along the spine of the book, and each edge must lie in a single page.\nImportant milestones in the later development of book embeddings include the proof by Mihalis Yannakakis in the late 1980s that planar graphs have book thickness at most four, and the discovery in the late 1990s of close connections between book embeddings and bioinformatics.\n\nA book is a particular kind of topological space, also called a fan of half-planes. It consists of a single line , called the spine or back of the book, together with a collection of one or more half-planes, called the pages or leaves of the book, each having the spine as its boundary. Books with a finite number of pages can be embedded into three-dimensional space, for instance by choosing to be the -axis of a Cartesian coordinate system and choosing the pages to be the half-planes whose dihedral angle with respect to the -plane is an integer multiple of .\n\nA book drawing of a finite graph onto a book is a drawing of on such that every vertex of is drawn as a point on the spine of , and every edge of is drawn as a curve that lies within a single page of . The -page book crossing number of is the minimum number of crossings in a -page book drawing.\n\nA book embedding of onto is a book drawing that forms a graph embedding of into . That is, it is a book drawing of on that does not have any edge crossings.\nEvery finite graph has a book embedding onto a book with a large enough number of pages. For instance, it is always possible to embed each edge of the graph on its own separate page.\nThe book thickness, pagenumber, or stack number of is the minimum number of pages required for a book embedding of .\nAnother parameter that measures the quality of a book embedding, beyond its number of pages, is its pagewidth. This is the maximum number of edges that can be crossed by a ray perpendicular to the spine within a single page. Equivalently (for book embeddings in which each edge is drawn as a monotonic curve), it is the maximum size of a subset of edges within a single page such that the intervals defined on the spine by pairs of endpoints of the edges all intersect each other.\n\nIt is crucial for these definitions that edges are only allowed to stay within a single page of the book. As Atneosen already observed, if edges may instead pass from one page to another across the spine of the book, then every graph may be embedded into a three-page book. For such a three-page topological book embedding in which spine crossings are allowed, every graph can be embedded with at most a logarithmic number of spine crossings per edge, and some graphs need this many spine crossings.\n\nAs shown in the first figure, the book thickness of the complete graph is three: as a non-planar graph its book thickness is greater than two, but a book embedding with three pages exists. More generally, the book thickness of every complete graph with vertices is exactly formula_1. This result also gives an upper bound on the maximum possible book thickness of any -vertex graph.\n\nThe two-page crossing number of the complete graph is\nmatching a still-unproven conjecture of Anthony Hill on what the unrestricted crossing number of this graph should be. That is, if Hill's conjecture is correct, then the drawing of this graph that minimizes the number of crossings is a two-page drawing.\n\nThe book thickness of the complete bipartite graph is at most . To construct a drawing with this book thickness, for each vertex on the smaller side of the bipartition, one can place the edges incident with that vertex on their own page. This bound is not always tight; for instance, has book thickness three, not four. However, when the two sides of the graph are very unbalanced, with , the book thickness of is exactly .\n\nFor the Turán graph (a complete multipartite graph formed from independent sets of vertices per independent set, with an edge between every two vertices from different independent sets) the book thickness is sandwiched between\nand when is odd the upper bound can be improved to\n\nThe book thickness of binary de Bruijn graphs, shuffle-exchange graphs, and cube-connected cycles (when these graphs are large enough to be nonplanar) is exactly three.\n\nThe book thickness of a given graph is at most one if and only if is an outerplanar graph. An outerplanar graph is a graph that has a planar embedding in which all vertices belong to the outer face of the embedding. For such a graph, placing the vertices in the same order along the spine as they appear in the outer face provides a one-page book embedding of the given graph. (An articulation point of the graph will necessarily appear more than once in the cyclic ordering of vertices around the outer face, but only one of those copies should be included in the book embedding.) Conversely, a one-page book embedding is automatically an outerplanar embedding. For, if a graph is embedded on a single page, and another half-plane is attached to the spine to extend its page to a complete plane, then the outer face of the embedding includes the entire added half-plane, and all vertices lie on this outer face.\n\nEvery two-page book embedding is a special case of a planar embedding, because the union of two pages of a book is a space topologically equivalent to the whole plane. Therefore, every graph with book thickness two is automatically a planar graph. More precisely, the book thickness of a graph is at most two if and only if is a subgraph of a planar graph that has a Hamiltonian cycle. If a graph is given a two-page embedding, it can be augmented to a planar Hamiltonian graph by adding (into any page) extra edges between any two consecutive vertices along the spine that are not already adjacent, and between the first and last spine vertices. The Goldner–Harary graph provides an example of a planar graph that does not have book thickness two: it is a maximal planar graph, so it is not possible to add any edges to it while preserving planarity, and it does not have a Hamiltonian cycle. Because of this characterization by Hamiltonian cycles, graphs that have two-page book embeddings are also known as subhamiltonian graphs.\nAll planar graphs whose maximum degree is at most four have book thickness at most two. Planar 3-trees have book thickness at most three. More generally, all planar graphs have book thickness at most four. It has been claimed by Mihalis Yannakakis in 1986 that there exist some planar graphs that have book thickness exactly four. However, a detailed proof of this claim, announced in a subsequent journal paper, has never been published. For this reason, list the problem of determining the maximum book thickness of planar graphs as still unsolved.\n\nSubdividing every edge of a graph into two-edge paths, by adding new vertices within each edge, may sometimes increase its book thickness. For instance, the diamond graph has book thickness one (it is outerplanar) but its subdivision has book thickness two (it is planar and subhamiltonian but not outerplanar). However, this subdivision process can also sometimes significantly reduce the book thickness of the subdivided graph. For instance, the book thickness of the complete graph is proportional to its number of vertices, but subdividing each of its edges into a two-edge path produces a subdivision whose book thickness is much smaller, only formula_6. Despite the existence of examples such as this one, conjectured that a subdivision's book thickness cannot be too much smaller than that of the original graph. Specifically, they conjectured that there exists a function such that, for every graph and for the graph formed by replacing every edge in by a two-edge path, if the book thickness of is then the book thickness of is at most .\nAs of 2013, the Blankenship–Oporowski conjecture remains unproven.\n\nBook thickness is related to thickness, the number of planar graphs needed to cover the edges of the given graph. A graph has thickness if it can be drawn in the plane, and its edges colored with colors, in such a way that edges of the same color as each other do not cross. Analogously, a graph has book thickness if it can be drawn in a half plane, with its vertices on the boundary of the half plane, with its edges colored with colors with no crossing between two edges of the same color. In this formulation of book thickness, the colors of the edges correspond to the pages of the book embedding. However, thickness and book thickness can be very different from each other: there exist graphs (subdivisions of complete graphs) that have unbounded book thickness, despite having thickness two.\n\nGraphs of treewidth have book thickness at most and this bound is tight for \nGraphs with edges have book thickness formula_7, and graphs of genus have book thickness formula_8. More generally, every minor-closed graph family has bounded book thickness. On the other hand, the 1-planar graphs, which are not closed under minors, have also bounded book thickness, but some 1-planar graphs including have book thickness at least four.\n\nEvery shallow minor of a graph of bounded book thickness is a sparse graph, whose ratio of edges to vertices is bounded by a constant that depends only on the depth of the minor and on the book thickness. That is, in the terminology of , the graphs of bounded book thickness have bounded expansion. However, even the graphs of bounded degree, a much stronger requirement than having bounded expansion, can have unbounded book thickness.\n\nBecause graphs of book thickness two are planar graphs, they obey the planar separator theorem: they have separators, subsets of vertices whose removal splits the graph into pieces with at most vertices each, with only formula_6 vertices in the separator. Here, refers to the number of vertices in the graph. However, there exist graphs of book thickness three that do not have separators of sublinear size.\n\nThe edges within a single page of a book embedding behave in some ways like a stack data structure. This can be formalized by considering an arbitrary sequence of push and pop operations on a stack, and forming a graph in which the stack operations correspond to the vertices of the graph, placed in sequence order along the spine of a book embedding. Then, if one draws an edge from each pop operation that pops an object from the stack, to the previous push operation that pushed , the resulting graph will automatically have a one-page embedding. For this reason, the page number of a graph has also been called its stack number. In the same way, one may consider an arbitrary sequence of enqueue and dequeue operations of a queue data structure, and form a graph that has these operations as its vertices, placed in order on the spine of a single page, with an edge between each enqueue operation and the corresponding dequeue. Then, in this graph, each two edges will either cross or cover disjoint intervals on the spine. By analogy, researchers have defined a queue embedding of a graph to be an embedding in a topological book such that each vertex lies on the spine, each edge lies in a single page, and each two edges in the same page either cross or cover disjoint intervals on the spine. The minimum number of pages needed for a queue embedding of a graph is called its queue number.\n\nFinding the book thickness of a graph is NP-hard.\nThis follows from the fact that finding Hamiltonian cycles in maximal planar graphs is NP-complete. In a maximal planar graph, the book thickness is two if and only if a Hamiltonian cycle exists. Therefore, it is also NP-complete to test whether the book thickness of a given maximal planar graph is two.\n\nIf an ordering of the vertices of a graph along the spine of an embedding is fixed, then a two-page embedding (if it exists) can be found in linear time, as an instance of planarity testing for a graph formed by augmenting the given graph with a cycle connecting the vertices in their spine ordering. claimed that finding three-page embeddings with a fixed spine ordering can also be performed in polynomial time although his writeup of this result omits many details. However, for graphs that require four or more pages, the problem of finding an embedding with the minimum possible number of pages remains NP-hard, via an equivalence to the NP-hard problem of coloring circle graphs, the intersection graphs of chords of a circle. Given a graph with a fixed spine ordering for its vertices, drawing these vertices in the same order around a circle and drawing the edges of as line segments produces a collection of chords representing . One can then form a circle graph that has the chords of this diagram as vertices and crossing pairs of chords as edges. A coloring of the circle graph represents a partition of the edges of into subsets that can be drawn without crossing on a single page. Therefore, an optimal coloring is equivalent to an optimal book embedding. Since circle graph coloring with four or more colors is NP-hard, and since any circle graph can be formed in this way from some book embedding problem, it follows that optimal book embedding is also NP-hard. For a fixed vertex ordering on the spine of a two-page book drawing, it is also NP-hard to minimize the number of crossings when this number is nonzero.\n\nIf the spine ordering is unknown but a partition of the edges into two pages is given, then it is possible to find a 2-page embedding (if it exists) in linear time by an algorithm based on SPQR trees. \nHowever, it is NP-complete to find a 2-page embedding when neither the spine ordering nor the edge partition is known.\nFinding the book crossing number of a graph is also NP-hard, because of the NP-completeness of the special case of testing whether the 2-page crossing number is zero.\n\nAs a consequence of bounded expansion, the subgraph isomorphism problem, of finding whether a pattern graph of bounded size exists as a subgraph of a larger graph, can be solved in linear time when the larger graph has bounded book thickness. The same is true for detecting whether the pattern graph is an induced subgraph of the larger graph, or whether it has a graph homomorphism to the larger graph. For the same reason, the problem of testing whether a graph of bounded book thickness obeys a given formula of first order logic is fixed-parameter tractable.\n\nOne of the main motivations for studying book embedding cited by involves an application in VLSI design, to the organization of fault-tolerant multiprocessors. In the DIOGENES system developed by these authors, the CPUs of a multiprocessor system are arranged into a logical sequence corresponding to the spine of a book (although this sequence may not necessarily be placed along a line in the physical layout of this system). Communication links connecting these processors are grouped into \"bundles\" which correspond to the pages of a book and act like stacks: connecting one of the processors to the start of a new communications link pushes all the previous links upward in the bundle, and connecting another processor to the end of a communication link connects it to the one at the bottom of the bundle and pops all the other ones down. Because of this stack behavior, a single bundle can handle a set of communications links that form the edges of a single page in a book embedding. By organizing the links in this way, a wide variety of different network topologies can be implemented, regardless of which processors have become faulty, as long as enough non-faulty processors remain to implement the network. The network topologies that can be implemented by this system are exactly the ones that have book thickness at most equal to the number of bundles that have been made available.\nBook embedding may also be used to model the placement of wires connecting VLSI components into the layers of a circuit.\n\nAnother application cited by concerns sorting permutations using stacks.\nAn influential result of showed that a system that processes a data stream by pushing incoming elements onto a \nstack and then, at appropriately chosen times, popping them from the stack onto an output stream can sort the data if and only if its initial order is described by a permutation that avoids the permutation pattern 231. Since then, there has been much work on similar problems of sorting data streams by more general systems of stacks and queues. In the system considered by , each element from an input data stream must be pushed onto one of several stacks. Then, once all of the data has been pushed in this way, the items are popped from these stacks (in an appropriate order) onto an output stream. As Chung et al. observe, a given permutation can be sorted by this system if and only if a certain graph, derived from the permutation, has a book embedding with the vertices in a certain fixed order along the spine and with a number of pages that is at most equal to the number of stacks.\n\nAs described, a book embedding may be used to describe the phases of a traffic signal at a controlled intersection.\nAt an intersection, the incoming and outgoing lanes of traffic (including the ends of pedestrian crosswalks and bicycle lanes as well as lanes for motor vehicles) may be represented as the vertices of a graph, placed on the spine of a book embedding in their clockwise order around the junction. The paths through the intersection taken by traffic to get from an incoming lane to an outgoing lane may be represented as the edges of an undirected graph. For instance, this graph might have an edge from an incoming to an outgoing lane of traffic that both belong to the same segment of road, representing a U-turn from that segment back to that segment, only if U-turns are allowed at the junction. For a given subset of these edges, the subset represents a collection of paths that can all be traversed without interference from each other if and only if the subset does not include any pair of edges that would cross if the two edges were placed in a single page of a book embedding. Thus, a book embedding of this graph describes a partition of the paths into non-interfering subsets, and the book thickness of this graph (with its fixed embedding on the spine) gives the minimum number of distinct phases needed for a signalling schedule that includes all possible traffic paths through the junction.\n\nBook embedding has also been frequently applied in the visualization of network data. Two of the standard layouts in graph drawing, arc diagrams and circular layouts, can be viewed as book embeddings, and book embedding has also been applied in the construction of clustered layouts, simultaneous embeddings, and three-dimensional graph drawings.\n\nAn arc diagram or linear embedding places vertices of a graph along a line, and draws the edges of the graph as semicircles either above or below this line, sometimes also allowing edges to be drawn on segments of the line. This drawing style corresponds to a book embedding with either one page (if all semicircles are above the line) or two pages (if both sides of the line are used), and was originally introduced as a way of studying the crossing numbers of graphs. Planar graphs that do not have two-page book embeddings may also be drawn in a similar way, by allowing their edges to be represented by multiple semicircles above and below the line. Such a drawing is not a book embedding by the usual definition, but has been called a topological book embedding. For every planar graph, it is always possible to find such an embedding in which each edge crosses the spine at most once.\nIn another drawing style, the circular layout, the vertices of a graph are placed on a circle and the edges are drawn either inside or outside the circle. Again, a placement of the edges within the circle (for instance as straight line segments) corresponds to a one-page book drawing, while a placement both inside and outside the circle corresponds to a two-page book drawing.\n\nFor one-page drawings of either style, it is important to keep the number of crossings small as a way of reducing the visual clutter of the drawing. Minimizing the number of crossings is NP-complete, but may be approximated with an approximation ratio of where is the number of vertices. Minimizing the one-page or two-page crossing number is fixed-parameter tractable when parameterized by the cyclomatic number of the given graph, or by a combination of the crossing number and the treewidth of the graph. Heuristic methods for reducing the crossing complexity have also been devised, based e.g. on a careful vertex insertion order and on local optimization.\n\nTwo-page book embeddings with a fixed partition of the edges into pages can be interpreted as a form of clustered planarity, in which the given graph must be drawn in such a way that parts of the graph (the two subsets of edges) are placed in the drawing in a way that reflects their clustering. Two-page book embedding has also been used to find simultaneous embeddings of graphs, in which two graphs are given on the same vertex set and one must find a placement for the vertices in which both graphs are drawn planarly with straight edges.\n\nBook embeddings with more than two pages have also been used to construct three-dimensional drawings of graphs. In particular, used a construction for book embeddings that keep the degree of each vertex within each page low, as part of a method for embedding graphs into a three-dimensional grid of low volume.\n\nIn the study of how RNA molecules fold to form their structure, the standard form of nucleic acid secondary structure can be described diagrammatically as a chain of bases (the RNA sequence itself), drawn along a line, together with a collection of arcs above the line describing the basepairs of the structure. That is, although these structures actually have a complicated three-dimensional shape, their connectivity (when a secondary structure exists) can be described by a more abstract structure, a one-page book embedding. However, not all RNA folds behave in this simple way. have proposed a so-called \"bi-secondary structure\" for certain RNA pseudoknots that takes the form of a two-page book embedding: the RNA sequence is again drawn along a line, but the basepairs are drawn as arcs both above and below this line. In order to form a bi-secondary structure, a graph must have maximum degree at most three: each base can only participate in one arc of the diagram, in addition to the two links to its neighbors in the base sequence. Advantages of this formulation include the facts that it excludes structures that are actually knotted in space, and that it matches most known RNA pseudoknots.\n\nBecause the spine ordering is known in advance for this application, testing for the existence of a bi-secondary structure for a given basepairing is straightforward. The problem of assigning edges to the two pages in a compatible way can be formulated as either an instance of 2-satisfiability, or as a problem of testing the bipartiteness of the circle graph whose vertices are the basepairs and whose edges describe crossings between basepairs. Alternatively and more efficiently, as show, a bi-secondary structure exists if and only if the \"diagram graph\" of the input (a graph formed by connecting the bases into a cycle in their sequence order and adding the given basepairs as edges) is a planar graph. This characterization allows bi-secondary structures to be recognized in linear time as an instance of planarity testing.\n\nAnd if an RNA structure is tertiary rather than bi-secondary (that is, if it requires more than two pages in its diagram), then determining the page number is again NP-hard.\n\n used book embedding to study the computational complexity theory of the reachability problem in directed graphs. As they have observed, reachability for two-page directed graphs may be solved in unambiguous logarithmic space (the analogue, for logarithmic space complexity, of the class UP of unambiguous polynomial-time problems). However, reachability for three-page directed graphs requires the full power of nondeterministic logarithmic space. Thus, book embeddings seem intimately connected with the distinction between these two complexity classes.\n\nThe existence of expander graphs with constant page number is the key step in proving that there is no subquadratic-time simulation of two-tape non-deterministic Turing machines by one-tape non-deterministic Turing machines.\n\n study applications of book thickness in abstract algebra, using graphs defined from the zero divisors of a finite local ring by making a vertex for each zero divisor and an edge for each pair of values whose product is zero.\n\nIn a multi-paper sequence, Dynnikov has studied the topological book embeddings of knots and links, showing that these embeddings can be described by a combinatorial sequence of symbols and that the topological equivalence of two links can be demonstrated by a sequence of local changes to the embeddings.\n"}
{"id": "6855275", "url": "https://en.wikipedia.org/wiki?curid=6855275", "title": "Courant bracket", "text": "Courant bracket\n\nIn a field of mathematics known as differential geometry, the Courant bracket is a generalization of the Lie bracket from an operation on the tangent bundle to an operation on the direct sum of the tangent bundle and the vector bundle of \"p\"-forms.\n\nThe case \"p\" = 1 was introduced by Theodore James Courant in his 1990 doctoral dissertation as a structure that bridges Poisson geometry and presymplectic geometry, based on work with his advisor Alan Weinstein. The twisted version of the Courant bracket was introduced in 2001 by Pavol Severa, and studied in collaboration with Weinstein.\n\nToday a complex version of the \"p\"=1 Courant bracket plays a central role in the field of generalized complex geometry, introduced by Nigel Hitchin in 2002. Closure under the Courant bracket is the integrability condition of a generalized almost complex structure.\n\nLet \"X\" and \"Y\" be vector fields on an N-dimensional real manifold \"M\" and let \"ξ\" and \"η\" be \"p\"-forms. Then \"X+ξ\" and \"Y+η\" are sections of the direct sum of the tangent bundle and the bundle of \"p\"-forms. The Courant bracket of \"X+ξ\" and \"Y+η\" is defined to be\n\nwhere formula_2 is the Lie derivative along the vector field \"X\", \"d\" is the exterior derivative and \"i\" is the interior product.\n\nThe Courant bracket is antisymmetric but it does not satisfy the Jacobi identity for \"p\" greater than zero.\n\nHowever, at least in the case \"p=1\", the Jacobiator, which measures a bracket's failure to satisfy the Jacobi identity, is an exact form. It is the exterior derivative of a form which plays the role of the Nijenhuis tensor in generalized complex geometry.\n\nThe Courant bracket is the antisymmetrization of the Dorfman bracket, which does satisfy a kind of Jacobi identity.\n\nLike the Lie bracket, the Courant bracket is invariant under diffeomorphisms of the manifold \"M\". It also enjoys an additional symmetry under the vector bundle automorphism\n\nwhere \"α\" is a closed \"p+1\"-form. In the \"p=1\" case, which is the relevant case for the geometry of flux compactifications in string theory, this transformation is known in the physics literature as a shift in the B field.\n\nThe cotangent bundle, formula_4 of M is the bundle of differential one-forms. In the case \"p\"=1 the Courant bracket maps two sections of formula_5, the direct sum of the tangent and cotangent bundles, to another section of formula_5. The fibers of formula_5 admit inner products with signature (N,N) given by\n\nA linear subspace of formula_5 in which all pairs of vectors have zero inner product is said to be an isotropic subspace. The fibers of formula_5 are \"2N\"-dimensional and the maximal dimension of an isotropic subspace is \"N\". An \"N\"-dimensional isotropic subspace is called a maximal isotropic subspace.\n\nA Dirac structure is a maximally isotropic subbundle of formula_5 whose sections are closed under the Courant bracket. Dirac structures include as special cases symplectic structures, Poisson structures and foliated geometries.\n\nA generalized complex structure is defined identically, but one tensors formula_5 by the complex numbers and uses the complex dimension in the above definitions and one imposes that the direct sum of the subbundle and its complex conjugate be the entire original bundle (Tformula_13\nT)formula_14C. Special cases of generalized complex structures include complex structure and a version of Kähler structure which includes the B-field.\n\nIn 1987 Irene Dorfman introduced the Dorfman bracket [,], which like the Courant bracket provides an integrability condition for Dirac structures. It is defined by\n\nThe Dorfman bracket is not antisymmetric, but it is often easier to calculate with than the Courant bracket because it satisfies a Leibniz rule which resembles the Jacobi identity\n\nThe Courant bracket does not satisfy the [[Jacobi identity]] and so it does not define a [[Lie algebroid]], in addition it fails to satisfy the Lie algebroid condition on the [[anchor map]]. Instead it defines a more general structure introduced by [[Zhang-Ju Liu]], [[Alan Weinstein]] and [[Ping Xu]] known as a [[Courant algebroid]].\n\nThe Courant bracket may be twisted by a \"(p+2)\"-form \"H\", by adding the interior product of the vector fields \"X\" and \"Y\" of \"H\". It remains antisymmetric and invariant under the addition of the interior product with a \"(p+1)\"-form \"B\". When \"B\" is not closed then this invariance is still preserved if one adds \"dB\" to the final \"H\".\n\nIf \"H\" is closed then the Jacobiator is exact and so the twisted Courant bracket still defines a Courant algebroid. In [[string theory]], \"H\" is interpreted as the [[Kalb-Ramond field|Neveu-Schwarz 3-form]].\n\nWhen \"p\"=0 the Courant bracket reduces to the Lie bracket on a [[principal bundle|principal]] [[circle bundle]] over \"M\" with [[Riemann curvature tensor|curvature]] given by the 2-form twist \"H\". The bundle of 0-forms is the trivial bundle, and a section of the direct sum of the tangent bundle and the trivial bundle defines a circle [[invariant vector field]] on this circle bundle.\n\nConcretely, a section of the sum of the tangent and trivial bundles is given by a vector field \"X\" and a function \"f\" and the Courant bracket is\nwhich is just the Lie bracket of the vector fields\nwhere \"θ\" is a coordinate on the circle fiber. Note in particular that the Courant bracket satisfies the Jacobi identity in the case \"p=0\".\n\nThe curvature of a circle bundle always represents an integral [[cohomology]] class, the [[Chern class]] of the circle bundle. Thus the above geometric interpretation of the twisted \"p=0\" Courant bracket only exists when \"H\" represents an integral class. Similarly at higher values of \"p\" the twisted Courant brackets can be geometrically realized as untwisted Courant brackets twisted by [[gerbe]]s when \"H\" is an integral cohomology class.\n\n\n[[Category:Differential geometry]]\n[[Category:Binary operations]]"}
{"id": "34783036", "url": "https://en.wikipedia.org/wiki?curid=34783036", "title": "Decoupling (probability)", "text": "Decoupling (probability)\n\nIn probability and statistics, decoupling is a reduction of a sample statistic to an average of the statistic evaluated on several independent sequences of the random variable. This sum, conditioned on all but one of the independent sequences becomes a sum of independent random variables. Decoupling is used in the study of U statistics, where decoupling should not be confused with Hoeffding's decomposition, however. (Such \"decoupling\" is unrelated to the use of \"couplings\" in the study of stochastic processes.)\n"}
{"id": "690647", "url": "https://en.wikipedia.org/wiki?curid=690647", "title": "Edge coloring", "text": "Edge coloring\n\nIn graph theory, an edge coloring of a graph is an assignment of \"colors\" to the edges of the graph so that no two incident edges have the same color. For example, the figure to the right shows an edge coloring of a graph by the colors red, blue, and green. Edge colorings are one of several different types of graph coloring. The edge-coloring problem asks whether it is possible to color the edges of a given graph using at most different colors, for a given value of , or with the fewest possible colors. The minimum required number of colors for the edges of a given graph is called the chromatic index of the graph. For example, the edges of the graph in the illustration can be colored by three colors but cannot be colored by two colors, so the graph shown has chromatic index three.\n\nBy Vizing's theorem, the number of colors needed to edge color a simple graph is either its maximum degree or . For some graphs, such as bipartite graphs and high-degree planar graphs, the number of colors is always , and for multigraphs, the number of colors may be as large as . There are polynomial time algorithms that construct optimal colorings of bipartite graphs, and colorings of non-bipartite simple graphs that use at most colors; however, the general problem of finding an optimal edge coloring is NP-hard and the fastest known algorithms for it take exponential time. Many variations of the edge coloring problem, in which an assignments of colors to edges must satisfy other conditions than non-adjacency, have been studied. Edge colorings have applications in scheduling problems and in frequency assignment for fiber optic networks.\n\nA cycle graph may have its edges colored with two colors if the length of the cycle is even: simply alternate the two colors around the cycle. However, if the length is odd, three colors are needed.\nA complete graph with vertices is edge-colorable with colors when is an even number; this is a special case of Baranyai's theorem. provides the following geometric construction of a coloring in this case: place points at the vertices and center of a regular -sided polygon. For each color class, include one edge from the center to one of the polygon vertices, and all of the perpendicular edges connecting pairs of polygon vertices. However, when is odd, colors are needed: each color can only be used for edges, a fraction of the total.\n\nSeveral authors have studied edge colorings of the odd graphs, -regular graphs in which the vertices represent teams of players selected from a pool of players, and in which the edges represent possible pairings of these teams (with one player left as \"odd man out\" to referee the game). The case that gives the well-known Petersen graph. As explains the problem (for ), the players wish to find a schedule for these pairings such that each team plays each of its six games on different days of the week, with Sundays off for all teams; that is, formalizing the problem mathematically, they wish to find a 6-edge-coloring of the 6-regular odd graph . When is 3, 4, or 8, an edge coloring of requires colors, but when it is 5, 6, or 7, only colors are needed.\n\nAs with its vertex counterpart, an edge coloring of a graph, when mentioned without any qualification, is always assumed to be a proper coloring of the edges, meaning no two adjacent edges are assigned the same color. Here, two distinct edges are considered to be adjacent when they share a common vertex. An edge coloring of a graph may also be thought of as equivalent to a vertex coloring of the line graph , the graph that has a vertex for every edge of and an edge for every pair of adjacent edges in .\n\nA proper edge coloring with different colors is called a (proper) -edge-coloring. A graph that can be assigned a (proper) -edge-coloring is said to be -edge-colorable. The smallest number of colors needed in a (proper) edge coloring of a graph is the chromatic index, or edge chromatic number, . The chromatic index is also sometimes written using the notation ; in this notation, the subscript one indicates that edges are one-dimensional objects. A graph is -edge-chromatic if its chromatic index is exactly . The chromatic index should not be confused with the chromatic number or , the minimum number of colors needed in a proper vertex coloring of .\n\nUnless stated otherwise all graphs are assumed to be simple, in contrast to multigraphs in which two or more edges may connecting the same pair of endpoints and in which there may be self-loops. For many problems in edge coloring, simple graphs behave differently from multigraphs, and additional care is needed to extend theorems about edge colorings of simple graphs to the multigraph case.\n\nA matching in a graph is a set of edges, no two of which are adjacent; a perfect matching is a matching that includes edges touching all of the vertices of the graph, and a maximum matching is a matching that includes as many edges as possible. In an edge coloring, the set of edges with any one color must all be non-adjacent to each other, so they form a matching. That is, a proper edge coloring is the same thing as a partition of the graph into disjoint matchings.\n\nIf the size of a maximum matching in a given graph is small, then many matchings will be needed in order to cover all of the edges of the graph. Expressed more formally, this reasoning implies that if a graph has edges in total, and if at most edges may belong to a maximum matching, then every edge coloring of the graph must use at least different colors. For instance, the 16-vertex planar graph shown in the illustration has edges. In this graph, there can be no perfect matching; for, if the center vertex is matched, the remaining unmatched vertices may be grouped into three different connected components with four, five, and five vertices, and the components with an odd number of vertices cannot be perfectly matched. However, the graph has maximum matchings with seven edges, so . Therefore, the number of colors needed to edge-color the graph is at least 24/7, and since the number of colors must be an integer it is at least four.\n\nFor a regular graph of degree that does not have a perfect matching, this lower bound can be used to show that at least colors are needed. In particular, this is true for a regular graph with an odd number of vertices (such as the odd complete graphs); for such graphs, by the handshaking lemma, must itself be even. However, the inequality does not fully explain the chromatic index of every regular graph, because there are regular graphs that do have perfect matchings but that are not \"k\"-edge-colorable. For instance, the Petersen graph is regular, with and with edges in its perfect matchings, but it does not have a 3-edge-coloring.\n\nThe edge chromatic number of a graph is very closely related to the maximum degree , the largest number of edges incident to any single vertex of . Clearly, , for if different edges all meet at the same vertex , then all of these edges need to be assigned different colors from each other, and that can only be possible if there are at least colors available to be assigned. Vizing's theorem (named for Vadim G. Vizing who published it in 1964) states that this bound is almost tight: for any graph, the edge chromatic number is either or .\nWhen , \"G\" is said to be of class 1; otherwise, it is said to be of class 2.\n\nEvery bipartite graph is of class 1, and almost all random graphs are of class 1. However, it is NP-complete to determine whether an arbitrary graph is of class 1.\n\nA 1-factorization of a \"k\"-regular graph, a partition of the edges of the graph into perfect matchings, is the same thing as a \"k\"-edge-coloring of the graph. That is, a regular graph has a 1-factorization if and only if it is of class 1. As a special case of this, a 3-edge-coloring of a cubic (3-regular) graph is sometimes called a Tait coloring.\n\nNot every regular graph has a 1-factorization; for instance, the Petersen graph does not. More generally the snarks are defined as the graphs that, like the Petersen graph, are bridgeless, 3-regular, and of class 2.\n\nAccording to the theorem of , every bipartite regular graph has a 1-factorization. The theorem was stated earlier in terms of projective configurations and was proven by Ernst Steinitz.\n\nFor multigraphs, in which multiple parallel edges may connect the same two vertices, results that are similar to but weaker than Vizing's theorem are known relating the edge chromatic number , the maximum degree , and the multiplicity , the maximum number of edges in any bundle of parallel edges. As a simple example showing that Vizing's theorem does not generalize to multigraphs, consider a Shannon multigraph, a multigraph with three vertices and three bundles of parallel edges connecting each of the three pairs of vertices. In this example, (each vertex is incident to only two out of the three bundles of parallel edges) but the edge chromatic number is (there are edges in total, and every two edges are adjacent, so all edges must be assigned different colors to each other). In a result that inspired Vizing, showed that this is the worst case: for any multigraph . Additionally, for any multigraph , , an inequality that reduces to Vizing's theorem in the case of simple graphs (for which ).\n\nBecause the problem of testing whether a graph is class 1 is NP-complete, there is no known polynomial time algorithm for edge-coloring every graph with an optimal number of colors. Nevertheless, a number of algorithms have been developed that relax one or more of these criteria: they only work on a subset of graphs, or they do not always use an optimal number of colors, or they do not always run in polynomial time.\n\nIn the case of bipartite graphs or multigraphs with maximum degree , the optimal number of colors is exactly . showed that an optimal edge coloring of these graphs can be found in the near-linear time bound , where is the number of edges in the graph; simpler, but somewhat slower, algorithms are described by and . The algorithm of begins by making the input graph regular, without increasing its degree or significantly increasing its size, by merging pairs of vertices that belong to the same side of the bipartition and then adding a small number of additional vertices and edges. Then, if the degree is odd, Alon finds a single perfect matching in near-linear time, assigns it a color, and removes it from the graph, causing the degree to become even. Finally, Alon applies an observation of , that selecting alternating subsets of edges in an Euler tour of the graph partitions it into two regular subgraphs, to split the edge coloring problem into two smaller subproblems, and his algorithm solves the two subproblems recursively. The total time for his algorithm is .\n\nFor planar graphs with maximum degree , the optimal number of colors is again exactly . With the stronger assumption that , it is possible to find an optimal edge coloring in linear time .\n\nFor d-regular graphs which are pseudo-random in the sense that their adjacency matrix has second largest eigenvalue (in absolute value) at most , d is the optimal number of colors .\n\n and describe polynomial time algorithms for coloring any graph with colors, meeting the bound given by Vizing's theorem; see Misra & Gries edge coloring algorithm.\n\nFor multigraphs, present the following algorithm, which they attribute to Eli Upfal. Make the input multigraph Eulerian by adding a new vertex connected by an edge to every odd-degree vertex, find an Euler tour, and choose an orientation for the tour. Form a bipartite graph in which there are two copies of each vertex of , one on each side of the bipartition, with an edge from a vertex on the left side of the bipartition to a vertex on the right side of the bipartition whenever the oriented tour has an edge from to in . Apply a bipartite graph edge coloring algorithm to . Each color class in corresponds to a set of edges in that form a subgraph with maximum degree two; that is, a disjoint union of paths and cycles, so for each color class in it is possible to form three color classes in . The time for the algorithm is bounded by the time to edge color a bipartite graph, using the algorithm of . The number of colors this algorithm uses is at most formula_1, close to but not quite the same as Shannon's bound of formula_2. It may also be made into a parallel algorithm in a straightforward way. In the same paper, Karloff and Shmoys also present a linear time algorithm for coloring multigraphs of maximum degree three with four colors (matching both Shannon's and Vizing's bounds) that operates on similar principles: their algorithm adds a new vertex to make the graph Eulerian, finds an Euler tour, and then chooses alternating sets of edges on the tour to split the graph into two subgraphs of maximum degree two. The paths and even cycles of each subgraph may be colored with two colors per subgraph. After this step, each remaining odd cycle contains at least one edge that may be colored with one of the two colors belonging to the opposite subgraph. Removing this edge from the odd cycle leaves a path, which may be colored using the two colors for its subgraph.\n\nA greedy coloring algorithm that considers the edges of a graph or multigraph one by one, assigning each edge the first available color, may sometimes use as many as colors, which may be nearly twice as many number of colors as is necessary. However, it has the advantage that it may be used in the online algorithm setting in which the input graph is not known in advance; in this setting, its competitive ratio is two, and this is optimal: no other online algorithm can achieve a better performance. However, if edges arrive in a random order, and the input graph has a degree that is at least logarithmic, then smaller competitive ratios can be achieved.\n\nSeveral authors have made conjectures that imply that the fractional chromatic index of any multigraph (a number that can be computed in polynomial time using linear programming) is within one of the chromatic index. If these conjectures are true, it would be possible to compute a number that is never more than one off from the chromatic index in the multigraph case, matching what is known via Vizing's theorem for simple graphs. Although unproven in general, these conjectures are known to hold when the chromatic index is at least formula_3, as can happen for multigraphs with sufficiently large multiplicity.\n\nIt is straightforward to test whether a graph may be edge colored with one or two colors, so the first nontrivial case of edge coloring is testing whether a graph has a 3-edge-coloring.\nAs showed, it is possible to test whether a graph has a 3-edge-coloring in time , while using only polynomial space. Although this time bound is exponential, it is significantly faster than a brute force search over all possible assignments of colors to edges. Every biconnected 3-regular graph with vertices has 3-edge-colorings; all of which can be listed in time (somewhat slower than the time to find a single coloring); as Greg Kuperberg observed, the graph of a prism over an -sided polygon has colorings (lower instead of upper bound), showing that this bound is tight.\n\nBy applying exact algorithms for vertex coloring to the line graph of the input graph, it is possible to optimally edge-color any graph with edges, regardless of the number of colors needed, in time and exponential space, or in time and only polynomial space .\n\nBecause edge coloring is NP-complete even for three colors, it is unlikely to be fixed parameter tractable when parametrized by the number of colors. However, it is tractable for other parameters. In particular, showed that for graphs of treewidth , an optimal edge coloring can be computed in time , a bound that depends superexponentially on but only linearly on the number of vertices in the graph.\n\nA graph is uniquely -edge-colorable if there is only one way of partitioning the edges into color classes, ignoring the possible permutations of the colors. For , the only uniquely -edge-colorable graphs are paths, cycles, and stars, but for other graphs may also be uniquely -edge-colorable. Every uniquely 3-edge-colorable graph has exactly three Hamiltonian cycles (formed by deleting one of the three color classes) but there exist 3-regular graphs that have three Hamiltonian cycles and are not uniquely 3-colorable, such as the generalized Petersen graphs for . The only known nonplanar uniquely 3-colorable graph is the generalized Petersen graph , and it has been conjectured that no others exist.\n\nThe De Bruijn–Erdős theorem may be used to transfer many edge coloring properties of finite graphs to infinite graphs. For instance, Shannon's and Vizing's theorems relating the degree of a graph to its chromatic index both generalize straightforwardly to infinite graphs.\n\nAs well as being related to the maximum degree and maximum matching number of a graph, the chromatic index is closely related to the linear arboricity of a graph , the minimum number of linear forests (disjoint unions of paths) into which the graph's edges may be partitioned. A matching is a special kind of linear forest, and in the other direction, any linear forest can be 2-edge-colored, so for every it follows that . Akiyama's conjecture (named for Jin Akiyama) states that formula_4, from which it would follow more strongly that . For graphs of maximum degree three, is always exactly two, so in this case the bound matches the bound given by Vizing's theorem.\n\nThe Thue number of a graph is the number of colors required in an edge coloring meeting the stronger requirement that, in every even-length path, the first and second halves of the path form different sequences of colors.\n\nThe arboricity of a graph is the minimum number of colors required so that the edges of each color have no cycles (rather than, in the standard edge coloring problem, having no adjacent pairs of edges). That is, it is the minimum number of forests into which the edges of the graph may be partitioned into. Unlike the chromatic index, the arboricity of a graph may be computed in polynomial time.\n\nList edge-coloring is a problem in which one is given a graph in which each edge is associated with a list of colors, and must find a proper edge coloring in which the color of each edge is drawn from that edge's list. The list chromatic index of a graph is the smallest number with the property that, no matter how one chooses lists of colors for the edges, as long as each edge has at least colors in its list, then a coloring is guaranteed to be possible. Thus, the list chromatic index is always at least as large as the chromatic index. The Dinitz conjecture on the completion of partial Latin squares may be rephrased as the statement that the list edge chromatic number of the complete bipartite graph equals its edge chromatic number, . resolved the conjecture by proving, more generally, that in every bipartite graph the chromatic index and list chromatic index are equal. The equality between the chromatic index and the list chromatic index has been conjectured to hold, even more generally, for arbitrary multigraphs with no self-loops; this conjecture remains open.\n\nMany other commonly studied variations of vertex coloring have also been extended to edge colorings. For instance, complete edge coloring is the edge-coloring variant of complete coloring, a proper edge coloring in which each pair of colors must be represented by at least one pair of adjacent edges and in which the goal is to maximize the total number of colors. Strong edge coloring is the edge-coloring variant of strong coloring, an edge coloring in which every two edges with adjacent endpoints must have different colors. Strong edge coloring has applications in channel allocation schemes for wireless networks.\n\nAcyclic edge coloring is the edge-coloring variant of acyclic coloring, an edge coloring for which every two color classes form an acyclic subgraph (that is, a forest). The acyclic chromatic index of a graph formula_5, denoted by formula_6, is the smallest number of colors needed to have a proper acyclic edge coloring of formula_5. It has been conjectured that formula_8, where formula_9 is the maximum degree of formula_5. Currently the best known bound is formula_11. The problem becomes easier when formula_5 has large girth. More specifically, there is a constant formula_13 such that if the girth of formula_5 is at least formula_15, then formula_16. A similar result is that for all formula_17 there exists an formula_18 such that if formula_5 has girth at least formula_18, then formula_21.\n\nTotal coloring is a form of coloring that combines vertex and edge coloring, by requiring both the vertices and edges to be colored. Any incident pair of a vertex and an edge, or an edge and an edge, must have distinct colors, as must any two adjacent vertices. It has been conjectured (combining Vizing's theorem and Brooks' theorem) that any graph has a total coloring in which the number of colors is at most the maximum degree plus two, but this remains unproven.\n\nIf a 3-regular graph on a surface is 3-edge-colored, its dual graph forms a triangulation of the surface which is also edge colored (although not, in general, properly edge colored) in such a way that every triangle has one edge of each color. Other colorings and orientations of triangulations, with other local constraints on how the colors are arranged at the vertices or faces of the triangulation, may be used to encode several types of geometric object. For instance, rectangular subdivisions (partitions of a rectangular subdivision into smaller rectangles, with three rectangles meeting at every vertex) may be described combinatorially by a \"regular labeling\", a two-coloring of the edges of a triangulation dual to the subdivision, with the constraint that the edges incident to each vertex form four contiguous subsequences, within each of which the colors are the same. This labeling is dual to a coloring of the rectangular subdivision itself in which the vertical edges have one color and the horizontal edges have the other color. Similar local constraints on the order in which colored edges may appear around a vertex may also be used to encode straight-line grid embeddings of planar graphs and three-dimensional polyhedra with axis-parallel sides. For each of these three types of regular labelings, the set of regular labelings of a fixed graph forms a distributive lattice that may be used to quickly list all geometric structures based on the same graph (such as all axis-parallel polyhedra having the same skeleton) or to find structures satisfying additional constraints.\n\nA deterministic finite automaton may be interpreted as a directed graph in which each vertex has the same out-degree , and in which the edges are -colored in such a way that every two edges with the same source vertex have distinct colors. The road coloring problem is the problem of edge-coloring a directed graph with uniform out-degrees, in such a way that the resulting automaton has a synchronizing word. solved the road coloring problem by proving that such a coloring can be found whenever the given graph is strongly connected and aperiodic.\n\nRamsey's theorem concerns the problem of -coloring the edges of a large complete graph in order to avoid creating monochromatic complete subgraphs of some given size . According to the theorem, there exists a number such that, whenever , such a coloring is not possible. For instance, , that is, if the edges of the graph are 2-colored, there will always be a monochromatic triangle.\n\nA path in an edge-colored graph is said to be a rainbow path if no color repeats on it. A graph is said to be rainbow colored if there is a rainbow path between any two pairs of vertices.\nAn edge-colouring of a graph G with colours 1. . . t is an interval t coloring if all colours are used, and the colours of edges incident to each vertex of G are distinct and form an interval of integers.\n\nEdge colorings of complete graphs may be used to schedule a round-robin tournament into as few rounds as possible so that each pair of competitors plays each other in one of the rounds; in this application, the vertices of the graph correspond to the competitors in the tournament, the edges correspond to games, and the edge colors correspond to the rounds in which the games are played. Similar coloring techniques may also be used to schedule other sports pairings that are not all-play-all; for instance, in the National Football League, the pairs of teams that will play each other in a given year are determined, based on the teams' records from the previous year, and then an edge coloring algorithm is applied to the graph formed by the set of pairings in order to assign games to the weekends on which they are played. For this application, Vizing's theorem implies that no matter what set of pairings is chosen (as long as no teams play each other twice in the same season), it is always possible to find a schedule that uses at most one more weekend than there are games per team.\n\nOpen shop scheduling is a problem of scheduling production processes, in which there are a set of objects to be manufactured, each object has a set of tasks to be performed on it (in any order), and each task must be performed on a specific machine, preventing any other task that requires the same machine from being performed at the same time. If all tasks have the same length, then this problem may be formalized as one of edge coloring a bipartite multigraph, in which the vertices on one side of the bipartition represent the objects to be manufactured, the vertices on the other side of the bipartition represent the manufacturing machines, the edges represent tasks that must be performed, and the colors represent time steps in which each task may be performed. Since bipartite edge coloring may be performed in polynomial time, the same is true for this restricted case of open shop scheduling.\n\nIn fiber-optic communication, the path coloring problem is the problem of assigning colors (frequencies of light) to pairs of nodes that wish to communicate with each other, and paths through a fiber-optic communications network for each pair, subject to the restriction that no two paths that share a segment of fiber use the same frequency as each other. Paths that pass through the same communication switch but not through any segment of fiber are allowed to use the same frequency. When the communications network is arranged as a star network, with a single central switch connected by separate fibers to each of the nodes, the path coloring problem may be modeled exactly as a problem of edge coloring a graph or multigraph, in which the communicating nodes form the graph vertices, pairs of nodes that wish to communicate form the graph edges, and the frequencies that may be used for each pair form the colors of the edge coloring problem. For communications networks with a more general tree topology, local path coloring solutions for the star networks defined by each switch in the network may be patched together to form a single global solution.\n\n list 23 open problems concerning edge coloring. They include:\n\n"}
{"id": "24497705", "url": "https://en.wikipedia.org/wiki?curid=24497705", "title": "Elementary theory", "text": "Elementary theory\n\nIn mathematical logic, an elementary theory is one that involves axioms using only finitary first-order logic, without reference to set theory or using any axioms which have consistency strength equal to set theory.\n\nSaying that a theory is elementary is a weaker condition than saying it is algebraic.\n\n\n"}
{"id": "21464095", "url": "https://en.wikipedia.org/wiki?curid=21464095", "title": "Fourier shell correlation", "text": "Fourier shell correlation\n\nIn structural biology, the three-dimensional Fourier shell correlation (FSC) measures the normalised cross-correlation coefficient between two 3-dimensional volumes over corresponding shells in Fourier space (i.e., as a function of spatial frequency). The FSC is the three-dimensional extension of the two-dimensional Fourier ring correlation (FRC); also known as: spatial frequency correlation function.\n\nwhere formula_2 is the complex structure Factor for volume 1, formula_3 is the complex conjugate of the structure Factor for volume 2, and formula_4 is the individual voxel element at radius formula_5. In this form, the FSC takes two three-dimensional data sets and converts them into a one-dimensional array.\n\nThe FSC originated in cryo-electron microscopy and gradually proliferated to other fields. To measure the FSC, two independently determined 3D volumes are required. In cryo-electron microscopy, the two volumes are the result of two three-dimensional reconstructions, each based on half of the available data set. Typically, the even particle images form one half and the odd particles the other half of the data set. Most publications quote the FSC 0.5 resolution cutoff, which \"ad hoc\" criterion refers to when the correlation coefficient of the Fourier shells is equal to 0.5. However, determining the resolution threshold remains a controversial issue: fixed-value thresholds were argued to be based on incorrect statistical assumptions. Many other criteria using the FSC curve exist, including 3-σ criterion, 5-σ criterion, and the 0.143 cutoff. The half-bit criterion indicates at which resolution we have collected enough information to reliably interpret the 3-dimensional volume, and the (modified) 3-sigma criterion indicates where the FSC systematically emerges above the expected random correlations of the background noise.\n\n\n"}
{"id": "48760367", "url": "https://en.wikipedia.org/wiki?curid=48760367", "title": "George Logemann", "text": "George Logemann\n\nGeorge Wahl Logemann (31 January 1938, Milwaukee, – 5 June 2012, Hartford) was an American mathematician and computer scientist. He became well known for the Davis–Putnam–Logemann–Loveland algorithm to solve Boolean satisfiability problems. He also contributed to the field of computer music.\n"}
{"id": "7338342", "url": "https://en.wikipedia.org/wiki?curid=7338342", "title": "Homology manifold", "text": "Homology manifold\n\nIn mathematics, a homology manifold (or generalized manifold)\nis a locally compact topological space \"X\" that looks locally like a topological manifold from the point of view of homology theory.\n\nA homology \"G\"-manifold (without boundary) of dimension \"n\" over an abelian group \"G\" of coefficients is a locally compact topological space X with finite \"G\"-cohomological dimension such that for any \"x\"∈\"X\", the homology groups\nare trivial unless \"p\"=\"n\", in which case they are isomorphic to \"G\". Here \"H\" is some homology theory, usually singular homology. Homology manifolds are the same as homology Z-manifolds.\n\nMore generally, one can define homology manifolds with boundary, by allowing the local homology groups to vanish\nat some points, which are of course called the boundary of the homology manifold. The boundary of an \"n\"-dimensional first-countable homology manifold is an \"n\"−1 dimensional homology manifold (without boundary).\n\n\n\n"}
{"id": "19592412", "url": "https://en.wikipedia.org/wiki?curid=19592412", "title": "IEEE 754-2008 revision", "text": "IEEE 754-2008 revision\n\nIEEE 754-2008 (previously known as \"IEEE 754r\") was published in August 2008 and is a significant revision to, and replaces, the IEEE 754-1985 floating point standard. The revision extended the previous standard where it was necessary, added decimal arithmetic and formats, tightened up certain areas of the original standard which were left undefined, and merged in IEEE 854 (the radix-independent floating-point standard).\n\nIn a few cases, where stricter definitions of binary floating-point arithmetic might be performance-incompatible with some existing implementation, they were made optional.\n\nThe standard had been under revision since 2000, with a target completion date of December 2006. The revision of an IEEE standard broadly follows three phases:\n\nOn 11 June 2008, it was approved unanimously by the IEEE Revision Committee (RevCom), and it was formally approved by the IEEE-SA Standards Board on 12 June 2008. It was published on 29 August 2008.\n\nParticipation in drafting the standard was open to people with a solid knowledge of floating-point arithmetic. More than 90 people attended at least one of the monthly meetings, which were held in Silicon Valley, and many more participated through the mailing list.\n\nProgress at times was slow, leading the chairman to declare at the September 15, 2005 meeting that \"no progress is being made, I am suspending these meetings until further notice on those grounds\". \nIn December 2005, the committee reorganized under new rules with a target completion date of December 2006.\n\nNew policies and procedures were adopted in February 2006. In September 2006 a working draft was approved to be sent to the parent sponsoring committee (the IEEE Microprocessor Standards Committee, or MSC) for editing and to be sent to sponsor ballot.\n\nThe last version of the draft, version 1.2.5, submitted to the MSC was from October 4, 2006. The MSC accepted the draft on 9 October 2006. Note that the draft has been changed significantly in detail during the balloting process.\n\nThe first sponsor ballot took place from 2006-11-29 through 2006-12-28. Of the 84 members of the voting body, 85.7% responded—78.6% voted approval. There were negative votes (and over 400 comments) so there was a recirculation ballot in March 2007; this received an 84% approval. There were sufficient comments (over 130) from that ballot that a third draft was prepared for second, 15-day, recirculation ballot which started in mid-April 2007. For a technical reason, the ballot process was restarted with the 4th ballot in October 2007; there were also substantial changes in the draft resulting from 650 voters' comments and from requests from the sponsor (the IEEE MSC); this ballot just failed to reach the required 75% approval. The 5th ballot had a 98.0% response rate with 91.0% approval, with comments leading to relatively small changes. The 6th, 7th, and 8th ballots sustained approval ratings of over 90% with progressively fewer comments on each draft; the 8th (which had no in-scope comments: 9 were repeats of previous comments and one referred to material not in the draft) was submitted to the IEEE Standards Revision Committee ('RevCom') for approval as an IEEE standard.\n\nThe IEEE Standards Revision Committee (RevCom) considered and unanimously approved the IEEE 754r draft at its June 2008 meeting, and it was approved by the IEEE-SA Standards Board on 12 June 2008. Final editing is complete and the document has now been forwarded to the IEEE Standards Publications Department for publication.\n\nThe new IEEE 754 (formally IEEE Std 754-2008, the IEEE Standard for Floating-Point Arithmetic) was published by the IEEE Computer Society on 29 August 2008, and is available from the IEEE Xplore website.\n\nThis standard replaces IEEE 754-1985. IEEE 854, the Radix-Independent floating-point standard was withdrawn in December 2008.\n\nThe most obvious enhancements to the standard are the addition of a 16-bit and a 128-bit binary type and three decimal types, some new operations, and many recommended functions. However, there have been significant clarifications in terminology throughout. This summary highlights the main differences in each major clause of the standard.\n\nThe scope (determined by the sponsor of the standard) has been widened to include decimal formats and arithmetic, and adds extendable formats.\n\nMany of the definitions have been rewritten for clarification and consistency. A few terms have been renamed for clarity (for example, \"denormalized\" has been renamed to \"subnormal\").\n\nThe description of formats has been made more regular, with a distinction between \"arithmetic formats\" (in which arithmetic may be carried out) and \"interchange formats\" (which have a standard encoding). Conformance to the standard is now defined in these terms.\n\nThe specification levels of a floating-point format have been enumerated, to clarify the distinction between:\n\nThe sets of representable entities are then explained in detail, showing that they can be treated with the significand being considered either as a fraction or an integer. The particular sets known as \"basic formats\" are defined, and the encodings used for interchange of binary and decimal formats are explained.\n\nThe binary interchange formats have the \"half precision\" (16-bit storage format) and \"quad precision\" (128-bit format) added, together with generalized formulae for some wider formats; the basic formats have 32-bit, 64-bit, and 128-bit encodings.\n\nThree new decimal formats are described, matching the lengths of the 32–128-bit binary formats. These give decimal interchange formats with 7, 16, and 34-digit significands, which may be normalized or unnormalized. For maximum range and precision, the formats merge part of the exponent and significand into a \"combination field\", and compress the remainder of the significand using either a decimal integer encoding (which uses \"Densely Packed Decimal\", or DPD, a compressed form of BCD) encoding or conventional binary integer encoding. The basic formats are the two larger sizes, which have 64-bit and 128-bit encodings. Generalized formulae for some other interchange formats are also specified.\n\nExtended and extendable formats allow for arithmetic at other precisions and ranges.\n\nThis clause has been changed to encourage the use of static attributes for controlling floating-point operations, and (in addition to required rounding attributes) allow for alternate exception handling, widening of intermediate results, value-changing optimizations, and reproducibility.\n\nThe \"round-to-nearest, ties away from zero\" rounding attribute has been added (required for decimal operations only).\n\nThis section has numerous clarifications (notably in the area of comparisons), and several previously recommended operations (such as copy, negate, abs, and class) are now required.\n\nNew operations include fused multiply–add (FMA), explicit conversions, classification predicates (isNan(\"x\"), \"etc.\"), various min and max functions, a total ordering predicate, and two decimal-specific operations (samequantum and quantize).\n\nThe min and max operations are defined but leave some leeway for the case where the inputs are equal in value but differ in representation. In particular:\n\nIn order to support operations such as windowing in which a NaN input should be quietly replaced with one of the end points, min and max are defined to select a number, x, in preference to a quiet NaN:\n\nIn the current draft, these functions are called \"minNum\" and \"maxNum\" to indicate their preference for a number over a quiet NaN.\n\nDecimal arithmetic, compatible with that used in Java, C#, PL/I, COBOL, Python, REXX, \"etc.\", is also defined in this section. In general, decimal arithmetic follows the same rules as binary arithmetic (results are correctly rounded, and so on), with additional rules that define the exponent of a result (more than one is possible in many cases).\n\nUnlike in 854, 754r requires correctly rounded base conversion between decimal and binary floating point within a range which depends on the format.\n\nThis clause has been revised and clarified, but with no major additions.\n\nThis clause has been revised and considerably clarified, but with no major additions.\n\nThis clause has been extended from the previous Clause 8 ('Traps') to allow optional exception handling in various forms, including traps and other models such as try/catch. Traps and other exception mechanisms remain optional, as they were in IEEE 754-1985.\n\nThis clause is new; it recommends fifty operations, including log, power, and trigonometric functions, that language standards should define. These are all optional (none are required in order to conform to the standard). The operations include some on dynamic modes for attributes, and also a set of reduction operations (sum, scaled product, \"etc.\").\n\nThis clause is new; it recommends how language standards should specify the semantics of sequences of operations, and points out the subtleties of literal meanings and optimizations that change the value of a result.\n\nThis clause is new; it recommends that language standards should provide a means to write reproducible programs (\"i.e.\", programs that will produce the same result in all implementations of a language), and describes what needs to be done to achieve reproducible results.\n\nThis annex is new; it lists some useful references.\n\nThis annex is new; it provides guidance to debugger developers for features that are desired for supporting the debugging of floating point code.\n\nThis is a new index, which lists all the operations described in the standard (required or optional).\n\nDue to changes in CPU design and development, the 2008 IEEE floating point standard could be viewed as historical or outdated as the 1985 standard it replaced. There were many outside discussions and items not covered in the standardization process, the items below are the ones that became public knowledge\n\n\n"}
{"id": "44305000", "url": "https://en.wikipedia.org/wiki?curid=44305000", "title": "Institute of Physics Rayleigh Medal and Prize", "text": "Institute of Physics Rayleigh Medal and Prize\n\nThe Rayleigh Medal and Prize is an award which has been made biennially in odd-numbered years since 2008 by the Institute of Physics; \"for distinguished research in theoretical, mathematical or computational physics\". It is named after John Strutt, 3rd Baron Rayleigh and comprises a medal and a prize of £1000. \n\nIt should not be confused with the medal of the same name awarded by the Institute of Acoustics.\n\nsource: Institute of Physics\n\n"}
{"id": "374220", "url": "https://en.wikipedia.org/wiki?curid=374220", "title": "Inverse trigonometric functions", "text": "Inverse trigonometric functions\n\nIn mathematics, the inverse trigonometric functions (occasionally also called arcus functions, antitrigonometric functions or cyclometric functions) are the inverse functions of the trigonometric functions (with suitably restricted domains). Specifically, they are the inverses of the sine, cosine, tangent, cotangent, secant, and cosecant functions, and are used to obtain an angle from any of the angle's trigonometric ratios. Inverse trigonometric functions are widely used in engineering, navigation, physics, and geometry.\n\nThere are several notations used for the inverse trigonometric functions.\n\nThe most common convention is to name inverse trigonometric functions using an arc- prefix, e.g., , , , etc. This convention is used throughout the article.\nWhen measuring in radians, an angle of θ radians will correspond to an arc whose length is rθ, where r is the radius of the circle. Thus, in the unit circle, \"the arc whose cosine is x\" is the same as \"the angle whose cosine is x\", because the length of the arc of the circle in radii is the same as the measurement of the angle in radians. Similarly, in computer programming languages the inverse trigonometric functions are usually called asin, acos, atan.\n\nThe notations , , , etc., as introduced by John Herschel in 1813, are often used as well in British sources, and this convention complies with the notation of an inverse function. This might appear to conflict logically with the common semantics for expressions like , which refer to numeric power rather than function composition, and therefore may result in confusion between multiplicative inverse and compositional inverse. The confusion is somewhat ameliorated by the fact that each of the reciprocal trigonometric functions has its own name—for example, = . Nevertheless, certain authors advise against using it for its ambiguity. Another convention used by a few authors is to use a majuscule (capital/upper-case) first letter along with a −1 superscript, e.g., , , , etc., which avoids confusing them with the multiplicative inverse, which should be represented by , , etc. Since 2009, the ISO 80000-2 standard has removed the ambiguity by specifying solely the \"arc\" prefix for the inverse functions.\n\nSince none of the six trigonometric functions are one-to-one, they are restricted in order to have inverse functions. Therefore the ranges of the inverse functions are proper subsets of the domains of the original functions.\n\nFor example, using \"function\" in the sense of multivalued functions, just as the square root function could be defined from , the function is defined so that sin(\"y\") = \"x\". For a given real number \"x\", with −1 ≤ \"x\" ≤ 1, there are multiple (in fact, countably infinitely many) numbers \"y\" such that ; for example, , but also , , etc. When only one value is desired, the function may be restricted to its principal branch. With this restriction, for each \"x\" in the domain the expression will evaluate only to a single value, called its principal value. These properties apply to all the inverse trigonometric functions.\n\nThe principal inverses are listed in the following table.\n\nIf \"x\" is allowed to be a complex number, then the range of \"y\" applies only to its real part.\n\nTrigonometric functions of inverse trigonometric functions are tabulated below. A quick way to derive them is by considering the geometry of a right-angled triangle, with one side of length 1, and another side of length \"x\" (any real number between 0 and 1), then applying the Pythagorean theorem and definitions of the trigonometric ratios. Purely algebraic derivations are longer.\n\nComplementary angles:\n\nNegative arguments:\n\nReciprocal arguments:\n\nIf you only have a fragment of a sine table:\n\nWhenever the square root of a complex number is used here, we choose the root with the positive real part (or positive imaginary part if the square was negative real).\n\nFrom the half-angle formula, formula_5, we get:\n\nThis is derived from the tangent addition formula\nby letting\n\nThe derivatives for complex values of \"z\" are as follows:\nOnly for real values of \"x\":\n\nInverse trigonometric functions are useful when trying to determine the remaining two angles of a right triangle when the lengths of the sides of the triangle are known. Recalling the right-triangle definitions of sine and cosine, it follows that\n\nOften, the hypotenuse is unknown and would need to be calculated before using arcsine or arccosine using the Pythagorean Theorem: formula_20 where formula_21 is the length of the hypotenuse. Arctangent comes in handy in this situation, as the length of the hypotenuse is not needed.\n\nFor example, suppose a roof drops 8 feet as it runs out 20 feet. The roof makes an angle \"θ\" with the horizontal, where \"θ\" may be computed as follows:\n\nThe two-argument atan2 function computes the arctangent of \"y\" / \"x\" given \"y\" and \"x\", but with a range of (−, ]. In other words, atan2(\"y\", \"x\") is the angle between the positive \"x\"-axis of a plane and the point (\"x\", \"y\") on it, with positive sign for counter-clockwise angles (upper half-plane, \"y\" > 0), and negative sign for clockwise angles (lower half-plane, \"y\" < 0). It was first introduced in many computer programming languages, but it is now also common in other fields of science and engineering.\nIn terms of the standard arctan function, that is with range of (−, ), it can be expressed as follows:\n\nIt also equals the principal value of the argument of the complex number \"x\" + i\"y\".\n\nThis function may also be defined using the tangent half-angle formulae as follows:\nprovided that either \"x\" > 0 or \"y\" ≠ 0. However this fails if given x ≤ 0 and y = 0 so the expression is unsuitable for computational use.\n\nThe above argument order (\"y\", \"x\") seems to be the most common, and in particular is used in ISO standards such as the C programming language, but a few authors may use the opposite convention (\"x\", \"y\") so some caution is warranted. These variations are detailed at atan2.\n\nIn many applications the solution formula_26 of the equation formula_27 is to come as close as possible to a given value formula_28. The adequate solution is produced by the parameter modified arctangent function \nThe function formula_30 rounds to the nearest integer.\n\nFor angles near 0 and , arccosine is ill-conditioned and will thus calculate the angle with reduced accuracy in a computer implementation (due to the limited number of digits). Similarly, arcsine is inaccurate for angles near −/2 and /2.\n\n\n"}
{"id": "37185041", "url": "https://en.wikipedia.org/wiki?curid=37185041", "title": "Jacobsthal sum", "text": "Jacobsthal sum\n\nIn mathematics, Jacobsthal sums are finite sums of Legendre symbols related to Gauss sums. They were introduced by .\n\nThe Jacobsthal sum is given by\nwhere \"p\" is prime and () is the Legendre symbol.\n\n"}
{"id": "11609808", "url": "https://en.wikipedia.org/wiki?curid=11609808", "title": "John Robinson (sculptor)", "text": "John Robinson (sculptor)\n\nJohn Robinson (4 May 1935 – 6 April 2007) was a British sculptor and co-founder of the Bradshaw Foundation. Accounts of his work may be seen at the Robinson estate website, the website of the Centre for the Popularisation of Mathematics and the June and July 2007, issues of Hyperseeing. Among other distinctions, he was the Official Sculptor for the British Olympic Committee in 1988, and a University of Wales Honorary Fellow, 1992.\n\nRobinson first made a name for himself with representational pieces. His figurative bronzes ranged in scope and scale from life-size commissioned sculptures of children to athletic sculptures, and included a commissioned bust of Queen Elizabeth and another of the Queen Mother. His representational sports figure \"Acrobats\" (1980, 5 metres) was first mounted at the Hyatt Regency hotel in Maui, Hawaii. There are another 7 examples around the world, one of which is located outside the Australian Institute of Sport in Canberra, Australia. \nAnother of his athletic sculptures, \"Hammer Thrower\", may be seen outside the Bowring Building in Tower Hill, London, at the United States Sports Academy, Daphne, Alabama, and in Melbourne, Australia. Robinson was Official Sculptor for the British Olympic Committee in 1988. His \"Gymnast\" is at the Olympic Museum in Lausanne, donated by the Australian Olympic Committee.\n\nIn 1975, after listening to a Mozart violin concerto, an abstract form came into his mind, which he then translated into a sculpture. This \"Adagio\" was the first of his non-figurative sculptures. Robinson then embarked on a series of abstract sculptures with the aim of symbolizing human values and our concepts of the dynamic processes which shape our lives. In this ‘Universe Series’ of symbolic sculptures and tapestries, which comprises over 100 works, he combined scientific and mathematical principles with artistic aesthetic. One example is \"Joy of Living\" (1993), a curving band of stainless steel. The symbolic \"Elation\" (1983) conveys the punching of the fist in triumph.\n\nA number of Robinson's sculptures show scientific concepts, such as matter/antimatter in \n\"Janus\", evolution in \"Evolution\" the explosions of stars in \"StarBurst\". He produced sculptures related to fiber bundles and to Borromean rings. He has also made a set of tapestries, woven in Aubusson, France.\n\nOne of Robinson’s best-known symbolic sculptures is \"Bonds of Friendship\" (1979), which he dedicated as symbolizing the notion that “trust is the basis of peace”. A 1.5m x 1m edition of \"Bonds\", in polished bronze, was unveiled in 1979 in Sydney Cove by the Governor General of Australia, to commemorate the landing in 1774 of the First Fleet. An analogous sculpture, but patinated to represent the Old Country, was unveiled in Portsmouth by Queen Elizabeth. Robinson said that ‘Bonds’ “symbolises the friendship that exists between the two countries, as well as between my patrons and myself, which has enriched my life beyond measure and made the Symbolic Sculptures possible.”\n\nSelections of Robinson’s work have been exhibited at the universities of Leeds, Bangor, Swansea, Liverpool, Wadham College Oxford, Churchill College Cambridge, London, Barcelona, Zaragoza, and at several sites in the U.S. His symbolic sculptures have been donated to a number of Universities, including Bangor (4), Cambridge (5), Durham (3), Oxford (2), Macquarie (1). Many of his sculptures can be viewed on web sites, including the Robinson estate website. A discussion of Robinson’s work within the context of mathematical principles can be seen at the website of the Centre for the Popularisation of Mathematics hosted by the University of Wales, Bangor.\n\nRobinson was born in London on 4 May 1935. His father was Australian and mother English. He was evacuated to Australia from 1940 to 1943, where attended Melbourne Grammar School. On returning to England, he attended Rugby School, where he received prizes for geometry and sculpture. He left school at the age of 16 and joined the Merchant Navy, but left the Navy upon arrival in Australia. There he engaged in a wide range of activities that enabled him to explore that continent. His adventures ranged from jackerooing and cattle droving to serving on a mounted patrol: he joined the last Mounted Police Patrol and trekked on horseback through 1100 kilometres of the King Leopold Ranges of the Kimberley region of Western Australia. In the 1950s, he married, bought a virgin scrub block in the Ninety Mile Desert of South Australia and for ten years, he and Margie developed a sheep farm. Their three sons were born on that farm.\nIn the late 1960s, he bought some modelling clay and started modelling friends and children, working in his shearing shed at weekends. His sculpting became so consuming that, in 1969 at the age of 35, having developed the farm totally and alone, he sold it for enough money to support himself and his family for two years to try his hand as a sculptor. Robinson returned to England with his wife and 3 sons to begin his sculpting career in earnest. Robinson first worked from a barn studio in Devon in the early 1970s, and later moved his family and studio to Somerset.\n\nIn 1983-1985 Robinson opened the Freeland Gallery in Albemarle Street, London, showing both figurative and representative sculptures. This led to contact with several art collectors and with the topologist Ronnie Brown. He and John collaborated in designing an exhibition of 13 full size sculptures at the Pop Maths Roadshow at Leeds University in 1989. The Catalogue they prepared for this exhibition, \"Symbolic Sculptures and Tapestries\", was widely distributed and led to further exhibitions.\n\nIn 1992 John was made an Honorary Fellow of the University of Wales, in recognition of the value of his sculpture and his collaboration with the Department of Mathematics. In 1996 he worked with Ronnie Brown and Cara Quinton on a web account of his sculptures, and this convinced him of the value of the internet. In typical fashion, he developed his own ideas, skills and a team of experts. Later he worked with Nick Mee of Virtual Image to create animations of some of his sculptures, and these were used in a CDRom produced by John, another developed by Ronnie Brown in WMY2000 under an EC grant, and in the two web sites referred to earlier.\n\nFor an indication of John's style of symbolism and craftmanship, see \"Mortality\" 'From nothing to nothing, cut from an egg, representing the cycle of life', and also \"Immortality\", \"Passing on the torch of life\".\n\nAfter a lifetime of interest in art, archaeology and anthropology, Robinson co-founded the Bradshaw Foundation. This foundation was formed in 1992 following an expedition to the Kimberley region of north-western Australia to examine a distributed set of rock art called the Bradshaws. Robinson’s travels to study rock art took him from the Sahara Desert to Easter Island. He was one of the few members of the Chauvet Cave scientific exploration team, led by Jean Clottes. In 1998, Robinson visited the Chauvet cave with Dr. Clottes, Dr. David Lewis-Williams, and several other team members, to visit the images estimated to be older than 35,000 years, the oldest art yet discovered. Robinson also travelled from Petra in Jordan to the Olgas in Central Australia, visited art academies throughout China, and explored the temples of Egypt, Mexico, Malta, Samarkand and Bokara in Uzbekistan.\n\nThe Bradshaw Foundation's first publication 'Bradshaws - Rock Paintings of North-Western Australia' was edited by Robinson in 1993. He foresaw the importance of the emerging World Wide Web, and in 1997 the Bradshaw Foundation website was established to highlight rock art research from around the world. In 2004 the website further expanded as Robinson introduced the genetic research of Professor Stephen Oppenheimer of Oxford University, in order to give rock art an anthropological context. This resulted in the 'Journey of Mankind - Genetic Map'. It was at this point the Bradshaw Foundation received a web award from \"Scientific American\" for work in the field of Anthropology & Paleontology.\n\nIn March 2007 Robinson was diagnosed with advanced lung cancer. He died on April 6 the same year at his home in Somerset, England after a short illness, fully in control, and, as he said, with no regrets. Two of his sculptures illustrate his attitude: \"Mortality\" (1982), \"From nothing to nothing, cut from an egg\", where the egg is the symbol of the cycle of life; and \"Immortality\" (1982), \"Passing on the torch of life\".\n\n\n"}
{"id": "4379392", "url": "https://en.wikipedia.org/wiki?curid=4379392", "title": "Julian Coolidge", "text": "Julian Coolidge\n\nJulian Lowell Coolidge (September 28, 1873 – March 5, 1954) was an American mathematician and a professor and chairman of the Harvard University Mathematics Department.\n\nBorn in Brookline, Massachusetts, he graduated from Harvard University and Oxford University.\n\nBetween 1897 and 1899 Julian Coolidge taught at the Groton School where one of his students was Franklin D. Roosevelt. He left the private school to accept a teaching position at Harvard and in 1902 was given an assistant professorship, but took two years off to further his education with studies in Turin, Italy before receiving his doctorate from the University of Bonn. Julian Coolidge then returned to teach at Harvard where he remained for his entire academic career, interrupted only by a year at the Sorbonne in Paris as an exchange professor.\n\nDuring World War I, he served with the U.S. Army's Overseas Expeditionary Force in France, rising to the rank of major. At the end of the war, the government of France awarded him the Legion of Honor.\n\nCoolidge returned to teach at Harvard where he was awarded a full professorship. In 1927 he was appointed chairman of the Mathematics Department at Harvard, a position he held until his retirement in 1940. A Fellow of the American Academy of Arts and Sciences, Coolidge served as president of the Mathematical Association of America and vice-president of the American Mathematical Society. He authored several books on mathematics and on the history of mathematics.\n\nJulian Coolidge died in 1954 in Cambridge, Massachusetts, aged 80.\n\n\n"}
{"id": "2658571", "url": "https://en.wikipedia.org/wiki?curid=2658571", "title": "Kakutani fixed-point theorem", "text": "Kakutani fixed-point theorem\n\nIn mathematical analysis, the Kakutani fixed-point theorem is a fixed-point theorem for set-valued functions. It provides sufficient conditions for a set-valued function defined on a convex, compact subset of a Euclidean space to have a fixed point, i.e. a point which is mapped to a set containing it. The Kakutani fixed point theorem is a generalization of Brouwer fixed point theorem. The Brouwer fixed point theorem is a fundamental result in topology which proves the existence of fixed points for continuous functions defined on compact, convex subsets of Euclidean spaces. Kakutani's theorem extends this to set-valued functions.\n\nThe theorem was developed by Shizuo Kakutani in 1941, and was used by John Nash in his description of Nash equilibria. It has subsequently found widespread application in game theory and economics.\n\nKakutani's theorem states:\n\n\nLet \"f\"(\"x\") be a set-valued function defined on the closed interval [0, 1] that maps a point \"x\" to the closed interval [1 − \"x\"/2, 1 − \"x\"/4]. Then \"f\"(\"x\") satisfies all the assumptions of the theorem and must have fixed points.\n\nIn the diagram, any point on the 45° line (dotted line in red) which intersects the graph of the function (shaded in grey) is a fixed point, so in fact there is an infinity of fixed points in this particular case. For example, \"x\" = 0.72 (dashed line in blue) is a fixed point since 0.72 ∈ [1 − 0.72/2, 1 − 0.72/4].\nThe requirement that \"φ\"(\"x\") be convex for all \"x\" is essential for the theorem to hold.\n\nConsider the following function defined on [0,1]:\nThe function has no fixed point. Though it satisfies all other requirements of Kakutani's theorem, its value fails to be convex at \"x\" = 0.5.\nSome sources, including Kakutani's original paper, use the concept of upper hemicontinuity while stating the theorem:\n\nThis statement of Kakutani's theorem is completely equivalent to the statement given at the beginning of this article.\n\nWe can show this by using the Closed graph theorem for set-valued functions, which says that a for a compact Hausdorff range space \"Y\", a set-valued function \"φ\": \"X\"→2 has a closed graph if and only if it is upper hemicontinuous and \"φ\"(\"x\") is a closed set for all \"x\". Since all Euclidean spaces are Hausdorff (being metric spaces) and \"φ\" is required to be closed-valued in the alternative statement of the Kakutani theorem, the Closed Graph Theorem implies that the two statements are equivalent.\n\nThe Kakutani fixed point theorem can be used to prove the minimax theorem in the theory of zero-sum games. This application was specifically discussed by Kakutani's original paper.\n\nMathematician John Nash used the Kakutani fixed point theorem to prove a major result in game theory.\nStated informally, the theorem implies the existence of a Nash equilibrium in every finite game with mixed strategies for any number of players. This work later earned him a Nobel Prize in Economics.\n\nIn this case, \"S\" is the set of tuples of mixed strategies chosen by each player in a game. The function φ(\"x\") gives a new tuple where each player's strategy is her best response to other players' strategies in \"x\". Since there may be a number of responses which are equally good, φ is set-valued rather than single-valued. Then the Nash equilibrium of the game is defined as a fixed point of φ, i.e. a tuple of strategies where each player's strategy is a best response to the strategies of the other players. Kakutani's theorem ensures that this fixed point exists.\n\nIn general equilibrium theory in economics, Kakutani's theorem has been used to prove the existence of set of prices which simultaneously equate supply with demand in all markets of an economy. The existence of such prices had been an open question in economics going back to at least Walras. The first proof of this result was constructed by Lionel McKenzie.\n\nIn this case, \"S\" is the set of tuples of commodity prices. φ(\"x\") is chosen as a function whose result is different from its arguments as long as the price-tuple \"x\" does not equate supply and demand everywhere. The challenge here is to construct φ so that it has this property while at the same time satisfying the conditions in Kakutani's theorem. If this can be done then φ has a fixed point according to the theorem. Given the way it was constructed, this fixed point must correspond to a price-tuple which equates supply with demand everywhere.\n\nKakutani's fixed-point theorem is used in proving the existence of cake allocations that are both envy-free and Pareto efficient. This result is known as Weller's theorem.\n\nThe proof of Kakutani's theorem is simplest for set-valued functions defined over closed intervals of the real line. However, the proof of this case is instructive since its general strategy can be carried over to the higher-dimensional case as well.\n\nLet φ: <nowiki>[0,1]</nowiki>→2 be a set-valued function on the closed interval <nowiki>[0,1]</nowiki> which satisfies the conditions of Kakutani's fixed-point theorem.\n\nLet (\"a\", \"b\", \"p\", \"q\") for \"i\" = 0, 1, … be a sequence with the following properties:\n\nThus, the closed intervals <nowiki>[</nowiki>\"a\", \"b\"<nowiki>]</nowiki> form a sequence of subintervals of <nowiki>[0,1]</nowiki>. Condition (2) tells us that these subintervals continue to become smaller while condition (3)–(6) tell us that the function φ shifts the left end of each subinterval to its right and shifts the right end of each subinterval to its left.\n\nSuch a sequence can be constructed as follows. Let \"a\" = 0 and \"b\" = 1. Let \"p\" be any point in φ(0) and \"q\" be any point in φ(1). Then, conditions (1)–(4) are immediately fulfilled. Moreover, since \"p\" ∈ φ(0) ⊂ <nowiki>[0,1]</nowiki>, it must be the case that \"p\" ≥ 0 and hence condition (5) is fulfilled. Similarly condition (6) is fulfilled by \"q\".\n\nNow suppose we have chosen \"a\", \"b\", \"p\" and \"q\" satisfying (1)–(6). Let,\nThen \"m\" ∈ <nowiki>[0,1]</nowiki> because <nowiki>[0,1]</nowiki> is convex.\n\nIf there is a \"r\" ∈ φ(\"m\") such that \"r\" ≥ \"m\", then we take,\nOtherwise, since φ(\"m\") is non-empty, there must be a \"s\" ∈ φ(\"m\") such that \"s\" ≤ \"m\". In this case let,\nIt can be verified that \"a\", \"b\", \"p\" and \"q\" satisfy conditions (1)–(6).\n\nThe cartesian product <nowiki>[0,1]</nowiki>×<nowiki>[0,1]</nowiki>×<nowiki>[0,1]</nowiki>×<nowiki>[0,1]</nowiki> is a compact set by Tychonoff's theorem. Since the sequence (\"a\", \"p\", \"b\", \"q\") lies in this compact set, it must have a convergent subsequence by the Bolzano-Weierstrass theorem. Let's fix attention on such a subsequence and let its limit be (\"a\"*, \"p\"*,\"b\"*,\"q\"*). Since the graph of φ is closed it must be the case that \"p\"* ∈ φ(\"a\"*) and \"q\"* ∈ φ(\"b\"*). Moreover, by condition (5), \"p\"* ≥ \"a\"* and by condition (6), \"q\"* ≤ \"b\"*.\n\nBut since (\"b\" − \"a\") ≤ 2 by condition (2),\nSo, \"b\"* equals \"a\"*. Let \"x\" = \"b\"* = \"a\"*.\n\nThen we have the situation that\n\nIf \"p\"* = \"q\"* then \"p\"* = \"x\" = \"q\"*. Since \"p\"* ∈ φ(\"x\"), \"x\" is a fixed point of φ.\n\nOtherwise, we can write the following. Recall that we can parameterize a line between two points a and b by (1-t)a + tb. Using our finding above that q<xx=\\left(\\frac{x-q^*}{p^*-q^*}\\right)p^*+\\left(1-\\frac{x-q^*}{p^*-q^*}\\right)q^*</math>\nit once again follows that \"x\" must belong to φ(\"x\") since \"p\"* and \"q\"* do and hence \"x\" is a fixed point of φ.\n\nIn dimensions greater one, \"n\"-simplices are the simplest objects on which Kakutani's theorem can be proved. Informally, a \"n\"-simplex is the higher-dimensional version of a triangle. Proving Kakutani's theorem for set-valued function defined on a simplex is not essentially different from proving it for intervals. The additional complexity in the higher-dimensional case exists in the first step of chopping up the domain into finer subpieces:\n\nOnce these changes have been made to the first step, the second and third steps of finding a limiting point and proving that it is a fixed point are almost unchanged from the one-dimensional case.\n\nKakutani's theorem for n-simplices can be used to prove the theorem for an arbitrary compact, convex \"S\". Once again we employ the same technique of creating increasingly finer subdivisions. But instead of triangles with straight edges as in the case of n-simplices, we now use triangles with curved edges. In formal terms, we find a simplex which covers \"S\" and then move the problem from \"S\" to the simplex by using a deformation retract. Then we can apply the already established result for n-simplices.\n\nKakutani's fixed-point theorem was extended to infinite-dimensional locally convex topological vector spaces by Irving Glicksberg\nand Ky Fan.\nTo state the theorem in this case, we need a few more definitions:\n\nThen the Kakutani–Glicksberg–Fan theorem can be stated as:\n\nThe corresponding result for single-valued functions is the Tychonoff fixed-point theorem.\n\nThere is another version that the statement of the theorem becomes the same as that in the Euclidean case:\n\nIn his game theory textbook, Ken Binmore recalls that Kakutani once asked him at a conference why so many economists had attended his talk. When Binmore told him that it was probably because of the Kakutani fixed point theorem, Kakutani was puzzled and replied, \"What is the Kakutani fixed point theorem?\"\n\n"}
{"id": "32095640", "url": "https://en.wikipedia.org/wiki?curid=32095640", "title": "LL grammar", "text": "LL grammar\n\nIn formal language theory, an LL grammar is a formal grammar that can be parsed by an LL parser, which parses the input from Left to right, and constructs a Leftmost derivation of the sentence (hence LL, compared with LR parser that constructs a rightmost derivation). A language that has an LL grammar is known as an LL language. These form subsets of deterministic context-free grammars (DCFGs) and deterministic context-free languages (DCFLs), respectively. One says that a given grammar or language \"is an LL grammar/language\" or simply \"is LL\" to indicate that it is in this class.\n\nLL parsers are table-based parsers, similar to LR parsers. LL grammars can alternatively be characterized as precisely those that can be parsed by a predictive parser – a recursive descent parser without backtracking – and these can be readily written by hand. This article is about the formal properties of LL grammars; for parsing, see LL parser or recursive descent parser.\n\nThere is a separate LL(\"k\") parser for each natural number \"k\" (0, 1, 2, ...). A LL parser is called a LL(\"k\") parser if it uses \"k\" tokens of lookahead when parsing a sentence. A LL(\"k\") parser recognizes the languages generated by some \"ε\"-free LL(\"k\") grammar. As allowing more tokens of lookahead makes the parser strictly more powerful, the languages that can be recognized with a LL(\"k\") parser are a strict subset of the languages that can be recognized by a LL(\"k+n\"), \"n > 0\" parser. This creates a strictly increasing sequence of sets: LL(0) ⊊ LL(1) ⊊ LL(2) ⊊ …. Since these are all DCFLs, a corollary is that for any fixed \"k\", there are DCFLs that cannot be recognized by a LL(\"k\") parser.\n\nAn LL parser is called an LL(*) parser if it is not restricted to a finite \"k\" tokens of lookahead, but can make parsing decisions by recognizing whether the following tokens belong to a regular language (for example by use of a Deterministic Finite Automaton), and accordingly there are the set of LL(*) grammars and the set of LL(*) languages.\n\nAlthough \"ε\"-free LL(\"k\") grammars are considered for LL(\"k\") parsers, allowing \"ε\"-rules increases the expressive power of the grammar: For every \"ε\"-free LL(\"k+1\") grammar, there exists a LL(\"k\") grammar with \"ε\"-rules that generates the same language.\n\nEvery LL(\"k\") grammar is also a LR(\"k\") grammar. It is also decidable if a given LR(\"k\") grammar is also a LL(\"m\") grammar for some \"m\". A \"ε\"-free LL(1) grammar is also a SLR(1) grammar. A LL(1) grammar with symbols that have both empty and non-empty derivations is also a LALR(1) grammar. A LL(1) grammar with symbols that have only the empty derivation may or may not be LALR(1).\n\nLL grammars cannot have rules containing left recursion. Removing left recursion from a context-free grammar is always possible. However, the resulting grammar may be bigger, require more lookahead tokens than preferred to be an LL grammar, or not be an LL grammar at all. LL(\"k\") grammars that are ε-free can be transformed into Greibach normal form (which by definition do not have rules with left recursion) without increasing the lookahead tokens.\n\nThe class of languages having an ε-free LL(1) grammar equals the class of simple deterministic languages,\nthe languages generated by \"simple\" context-free grammars,\nwhich are the context-free grammars in Greibach normal form (i.e. for which each rule has the form formula_1) such that different right hand sides for the same nonterminal formula_2 always start with different terminals formula_3.\n\nThis language class includes the regular sets with end-markers. Equivalence is decidable for it, while inclusion is not.\n\nLL grammars, particularly LL(1) grammars, are of great practical interest, as they are easy to parse, either by LL parsers or by recursive descent parsers, and many computer languages are designed to be LL(1) for this reason. Languages based on grammars with a high value of \"k\" have traditionally been considered to be difficult to parse, although this is less true now given the availability and widespread use of parser generators supporting LL(\"k\") grammars for arbitrary \"k\".\n\n"}
{"id": "104485", "url": "https://en.wikipedia.org/wiki?curid=104485", "title": "Laurent Lafforgue", "text": "Laurent Lafforgue\n\nLaurent Lafforgue (; born 6 November 1966) is a French mathematician. He has made outstanding contributions to Langlands' program in the fields of number theory and analysis, and in particular proved the Langlands conjectures for the automorphism group of a function field. The crucial contribution by Lafforgue to solve this question is the construction of compactifications of certain moduli stacks of shtukas. The monumental proof is the result of more than six years of concentrated efforts.\n\nIn 2002 at the 24th International Congress of Mathematicians in Beijing, China, he received the Fields Medal together with Vladimir Voevodsky.\n\nLaurent Lafforgue has two brothers, Thomas and Vincent, both mathematicians. Thomas is now a teacher in a \"classe préparatoire aux grandes écoles\" at Lycée Louis le Grand in Paris and Vincent a CNRS directeur de recherches at the Institut Fourier in Grenoble.\n\nHe won 2 silver medals at International Mathematical Olympiad (IMO) in 1984 and 1985. He entered the École Normale Supérieure in 1986. In 1994 he received his Ph.D. under the direction of Gérard Laumon in the Arithmetic and Algebraic Geometry team at the Université de Paris-Sud. Currently he is a research director of CNRS, detached as permanent professor of mathematics at the Institut des Hautes Études Scientifiques (I.H.E.S.) in Bures-sur-Yvette, France.\n\nLaurent is a devout catholic and never married. \n\nHe received the Clay Research Award in 2000, and the of the French Academy of Sciences in 2001. His younger brother Vincent Lafforgue is also a notable mathematician. On 22 May 2011 Lafforgue was awarded an honorary Doctor of Science from the University of Notre Dame.\n\nLafforgue is a critic of what he calls the \"pedagogically correct\" in France's educational system. In 2005, he was forced to resign from the Haut conseil de l'éducation after he expressed these views in a private letter that he sent to Bruno Racine, president of the HCE, that later was made public.\n\n\n\n"}
{"id": "12635200", "url": "https://en.wikipedia.org/wiki?curid=12635200", "title": "Lazy linear hybrid automaton", "text": "Lazy linear hybrid automaton\n\nLazy linear hybrid automata model the discrete time behavior of control systems containing finite-precision sensors and actuators interacting with their environment under bounded inertial delays. The model permits only linear flow constraints but the invariants and guards can be any computable function.\n\nThis computational model was proposed by Manindra Agrawal and P. S. Thiagarajan. This model is more realistic and also computationally amenable than the currently popular modeling paradigm of linear hybrid automaton.\n\n"}
{"id": "19222837", "url": "https://en.wikipedia.org/wiki?curid=19222837", "title": "Like terms", "text": "Like terms\n\nIn algebra, like terms are terms that have the same variables and powers. The coefficients do not need to match.\n\nUnlike terms are two or more terms that are not like terms, i.e. they do not have the same variables or powers. The order of the variables does not matter unless there is a power. For example, 8\"xyz\" and −5\"xyz\" are like terms because they have the same variables and power while 3\"abc\" and 3\"ghi\" are unlike terms because they have different variables. Since the coefficient doesn't affect likeness, all constant terms are like terms.\n\nIn this discussion, a \"term\" will refer to a string of numbers being multiplied or divided (remember that division is simply multiplication by a reciprocal) together. Terms are within the same expression and are combined by either addition or subtraction. For example, take the expression:\n\nformula_1\n\nThere are two terms in this expression. Notice that the two terms have a common factor, that is, both terms have an formula_2. This means that we can factor out that common factor variable, resulting in\n\nformula_3\n\nIf the expression in parentheses may be calculated, that is, if the variables in the expression in the parentheses are known numbers, then it is simpler to write the calculation formula_4. and juxtapose that new number with the remaining unknown number. Terms combined in an expression with a common, unknown factor (or multiple unknown factors) are called like terms.\n\nTo provide an example for above, let formula_5 and formula_6 have arbitrary values, so that their sum may be calculated. For ease of calculation, let formula_7 and formula_8. The original expression becomes\n\nformula_9\n\nwhich may be factored into\n\nformula_10\n\nor, equally,\n\nformula_11.\n\nThis demonstrates that\n\nformula_12\n\nThe known values assigned to the unlike part of two or more terms are called coefficients. As this example shows, when like terms exist in an expression, they may be combined by adding or subtracting (whatever the expression indicates) the coefficients, and maintaining the common factor of both terms. Such combination is called combining like terms, and it is an important tool used for solving equations.\n\nTake the expression, which is to be simplified:\n\nformula_13\n\nThe first step to grouping like terms in this expression is to get rid of the parentheses. Do this by distributing (multiplying) each number in front of a set of parentheses to each term in that set of parentheses:\n\nformula_14\n\nThe like terms in this expression are the terms that can be grouped together by having exactly the same set of unknown factors. Here, the sets of unknown factors are formula_15 formula_16 and formula_17. By the rule in the first example, all terms with the same set of unknown factors, that is, all like terms, may be combined by adding or subtracting their coefficients, while maintaining the unknown factors. Thus, the expression becomes\n\nformula_18\n\nThe expression is considered simplified when all like terms have been combined, and all terms present are unlike. In this case, all terms now have different unknown factors, and are thus unlike, and so the expression is completely simplified.\n"}
{"id": "30966390", "url": "https://en.wikipedia.org/wiki?curid=30966390", "title": "List of things named after Euclid", "text": "List of things named after Euclid\n\nThis is a list of topics named after the Greek mathematician Euclid.\n\n\n\n\n"}
{"id": "9767227", "url": "https://en.wikipedia.org/wiki?curid=9767227", "title": "Low basis theorem", "text": "Low basis theorem\n\nThe low basis theorem is one of several basis theorems in computability theory, each of which shows that, given an infinite subtree of the binary tree formula_1, it is possible to find an infinite path through the tree with particular computability properties. The low basis theorem, in particular, shows that there must be a path which is low, that is, the Turing jump of the path is Turing equivalent to the halting problem formula_2.\n\nThe low basis theorem states that every nonempty formula_3 class in formula_4 (see arithmetical hierarchy) contains a set of low degree (Soare 1987:109). This is equivalent, by definition, to the statement that each infinite computable subtree of the binary tree formula_1 has an infinite path of low degree. \n\nThe proof uses the method of forcing with formula_3 classes (Cooper 2004:330). Hájek and Kučera (1989) showed that the low basis is provable in the formal system of arithmetic known as formula_7.\n\nOne application of the low basis theorem is to construct completions of effective theories so that the completions have low Turing degree. For example, the low basis theorem implies the existence of PA degrees strictly below formula_2. \n\n"}
{"id": "3008091", "url": "https://en.wikipedia.org/wiki?curid=3008091", "title": "Mathematics Subject Classification", "text": "Mathematics Subject Classification\n\nThe Mathematics Subject Classification (MSC) is an alphanumerical classification scheme collaboratively produced by staff of, and based on the coverage of, the two major mathematical reviewing databases, Mathematical Reviews and Zentralblatt MATH. The MSC is used by many mathematics journals, which ask authors of research papers and expository articles to list subject codes from the Mathematics Subject Classification in their papers. The current version is MSC2010.\n\nThe MSC is a hierarchical scheme, with three levels of structure. A classification can be two, three or five digits long, depending on how many levels of the classification scheme are used.\n\nThe first level is represented by a two digit number, the second by a letter, and the third by another two digit number. For example:\n\n\nAt the top level, 64 mathematical disciplines are labeled with a unique 2 digit number. As well as the typical areas of mathematical research, there are top level categories for \"History and Biography\", \"Mathematics Education\", and for the overlap with different sciences. Physics (i.e. mathematical physics) is particularly well represented in the classification scheme with a number of different categories including:\n\nAll valid MSC classification codes must have at least the first level identifier.\n\nThe second level codes are a single letter from the Latin alphabet. These represent specific areas covered by the first level discipline. The second level codes vary from discipline to discipline.\n\nFor example, for differential geometry, the top-level code is 53, and the second-level codes are:\n\nIn addition, the special second level code \"-\" is used for specific kinds of materials. These codes are of the form:\n\n\nThe second and third level of these codes are always the same - only the first level changes. For example, it is not valid to use 53- as a classification. Either 53 on its own or, better yet, a more specific code should be used.\n\nThird level codes are the most specific, usually corresponding to a specific kind of mathematical object or a well-known problem or research area.\n\nThe third-level code 99 exists in every category and means \"none of the above, but in this section\"\n\nThe AMS recommends that papers submitted to its journals for publication have one primary classification and one or more optional secondary classifications. A typical MSC subject class line on a research paper looks like\n\nMSC Primary 03C90; Secondary 03-02;\n\nAccording to the American Mathematical Society (AMS) help page about MSC, the MSC has been revised a number of times since 1940. Based on a scheme to organize AMS's \"Mathematical Offprint Service\" (MOS scheme), the \"AMS Classification\" was established for the classification of reviews in \"Mathematical Reviews\" in the 1960s. It saw various ad-hoc changes. Despite its shortcomings, Zentralblatt für Mathematik started to use it as well in the 1970s. In the late 1980s, a jointly revised scheme with more formal rules was agreed upon by Mathematical Reviews and Zentralblatt für Mathematik under the new name Mathematics Subject Classification. It saw various revisions as \"MSC1990\", \"MSC2000\" and \"MSC2010\". In July 2016, Mathematical Reviews and zbMATH started collecting input from the mathematical community on the next revision of MSC, which is due to be released in 2020. The original classification of older items has not been changed. This can sometimes make it difficult to search for older works dealing with particular topics. Changes at the first level involved the subjects with (present) codes 03, 08, 12-20, 28, 37, 51, 58, 74, 90, 91, 92.\n\nFor physics papers the Physics and Astronomy Classification Scheme (PACS) is often used. Due to the large overlap between mathematics and physics research it is quite common to see both PACS and MSC codes on research papers, particularly for multidisciplinary journals and repositories such as the arXiv.\n\nThe ACM Computing Classification System (CCS) is a similar hierarchical classification scheme for computer science. There is some overlap between the AMS and ACM classification schemes, in subjects related to both mathematics and computer science, however the two schemes differ in the details of their organization of those topics.\n\nThe classification scheme used on the arXiv is chosen to reflect the papers submitted. As arXiv is multidisciplinary its classification scheme does not fit entirely with the MSC, ACM or PACS classification schemes. It is common to see codes from one or more of these schemes on individual papers.\n\nThe top level subjects under the MSC are, grouped here by common area names that are not part of the MSC:\n\n\n\n\n\n\n\n"}
{"id": "58526997", "url": "https://en.wikipedia.org/wiki?curid=58526997", "title": "Monroe H. Martin Prize", "text": "Monroe H. Martin Prize\n\nThe Monroe H. Martin Prize recognizes an outstanding paper in applied mathematics, including numerical analysis, by a young researcher not more than 35 years old and a resident of North America. First awarded in 1975, it is given every 5 years by the Institute for Physical Science and Technology, University of Maryland, College Park. The prize commemorates the achievements of Monroe H. Martin, former director of the Institute for Fluid Dynamics and Applied Mathematics and chair of the Mathematics Department at the University of Maryland. The prize carries a monetary award plus travel expenses; recipient presents his or her work at the Monroe H. Martin lecture at the University of Maryland.\n\nThe recipients of the Monroe H. Martin Prize are: \n\n"}
{"id": "28367322", "url": "https://en.wikipedia.org/wiki?curid=28367322", "title": "N-flake", "text": "N-flake\n\nAn \"n\"-flake, polyflake, or Sierpinski \"n\"-gon, is a fractal constructed starting from an \"n\"-gon. This \"n\"-gon is replaced by a flake of smaller \"n\"-gons, such that the scaled polygons are placed at the vertices, and sometimes in the center. This process is repeated recursively to result in the fractal. Typically, there is also the restriction that the \"n\"-gons must touch yet not overlap.\n\nThe most common variety of \"n\"-flake is two-dimensional (in terms of its topological dimension) and is formed of polygons. The four most common special cases are formed with triangles, squares, pentagons, and hexagons, but it can be extended to any polygon. Its boundary is the von Koch curve of varying types – depending on the \"n\"-gon – and infinitely many Koch curves are contained within. The fractals occupy zero area yet have an infinite perimeter.\n\nThe formula of the scale factor \"r\" for any \"n\"-flake is:\nwhere cosine is evaluated in radians and \"n\" is the number of sides of the \"n\"-gon. The Hausdorff dimension of a \"n\"-flake is formula_2, where \"m\" is the number of polygons in each individual flake and \"r\" is the scale factor.\n\nThe Sierpinski triangle is an \"n\"-flake formed by successive flakes of three triangles. Each flake is formed by placing triangles scaled by 1/2 in each corner of the triangle they replace. Its Hausdorff dimension is equal to formula_3 ≈ 1.585. The formula_3 is obtained because each iteration has 3 triangles that are scaled by 1/2.\nIf a sierpinski 4-gon were constructed from the given definition, the scale factor would be 1/2 and the fractal would simply be a square. A more interesting alternative, the Vicsek fractal, rarely called a quadraflake, is formed by successive flakes of five squares scaled by 1/3. Each flake is formed either by placing a scaled square in each corner and one in the center or one on each side of the square and one in the center. Its Hausdorff dimension is equal to formula_5 ≈ 1.4650. The formula_5 is obtained because each iteration has 5 squares that are scaled by 1/3. The boundary of the Vicsek Fractal is a Type 1 quadratic Koch curve.\n\nA pentaflake, or sierpinski pentagon, is formed by successive flakes of six regular pentagons.\nEach flake is formed by placing a pentagon in each corner and one in the center. Its Hausdorff dimension is equal to formula_7 ≈ 1.8617, where formula_8 (golden ratio). The formula_7 is obtained because each iteration has 6 pentagons that are scaled by formula_10. The boundary of a pentaflake is the Koch curve of 72 degrees.\n\nThere is also a variation of the pentaflake that has no central pentagon. Its Hausdorff dimension equals formula_11 ≈ 1.6723. This variation still contains infinitely many Koch curves, but they are somewhat more visible.\n\nA hexaflake, is formed by successive flakes of seven regular hexagons. Each flake is formed by placing a scaled hexagon in each corner and one in the center. Its Hausdorff dimension is equal to formula_12 ≈ 1.7712. The formula_12 is obtained because each iteration has 7 hexagons that are scaled by 1/3. The boundary of a hexaflake is the standard Koch curve of 60 degrees and infinitely many Koch snowflakes are contained within. Also, the projection of the cantor cube onto the plane orthogonal to its main diagonal is a hexaflake.\n\nLike the pentaflake, there is also a variation of the hexaflake, called the Sierpinski hexagon, that has no central hexagon. Its Hausdorff dimension equals formula_14 ≈ 1.6309. This variation still contains infinitely many Koch curves of 60 degrees.\n\"n\"-flakes of higher polygons also exist, though they are less common and don't usually have a central polygon. Some examples are shown below; the 7-flake through 12-flake. While it may not be obvious, these higher polyflakes still contain infinitely many Koch curves, but the angle of the Koch curves decreases as \"n\" increases. Their Hausdorff dimensions are slightly more difficult to calculate than lower \"n\"-flakes because their scale factor is less obvious. However, the Hausdorff dimension is always less than two but no less than one. An interesting \"n\"-flake is the ∞-flake, because as the value of \"n\" increases, an \"n\"-flake's Hausdorff dimension approaches 1,\n\"n\"-flakes can generalized to higher dimensions, in particular to a topological dimension of three. Instead of polygons, regular polyhedra are iteratively replaced. However, while there are an infinite number of regular polygons, there are only five regular, convex polyhedra. Because of this, three-dimensional n-flakes are also called platonic solid fractals. In three dimensions, the fractals' volume is zero.\n\nA Sierpinski tetrahedron is formed by successive flakes of four regular tetrahedrons. Each flake is formed by placing a tetrahedron scaled by 1/2 in each corner. Its Hausdorff dimension is equal to formula_15, which is exactly equal to 2. On every face there is a Sierpinski triangle and infinitely many are contained within.\nA hexahedron, or cube, flake defined in the same way as the Sierpinski tetrahedron is simply a cube and is not interesting as a fractal. However, there are two pleasing alternatives. One is the Menger Sponge, where every cube is replaced by a three dimensional ring of cubes. Its Hausdorff dimension is formula_16 ≈ 2.7268.\n\nAnother hexahedron flake can be produced in a manner similar to the Vicsek fractal extended to three dimensions. Every cube is divided into 27 smaller cubes and the center cross is retained, which is the opposite of the Menger sponge where the cross is removed. However, it is not the Menger Sponge complement. Its Hausdorff dimension is formula_12 ≈ 1.7712, because a cross of 7 cubes, each scaled by 1/3, replaces each cube.\nAn octahedron flake, or sierpinski octahedron, is formed by successive flakes of six regular octahedrons. Each flake is formed by placing an octahedron scaled by 1/2 in each corner. Its Hausdorff dimension is equal to formula_18 ≈ 2.5849. On every face there is a Sierpinski triangle and infinitely many are contained within.\nA dodecahedron flake, or sierpinski dodecahedron, is formed by successive flakes of twenty regular dodecahedrons. Each flake is formed by placing a dodecahedron scaled by formula_19 in each corner. Its Hausdorff dimension is equal to formula_20 ≈ 2.3296.\nAn icosahedron flake, or sierpinski icosahedron, is formed by successive flakes of twelve regular icosahedrons. Each flake is formed by placing an icosahedron scaled by formula_10 in each corner. Its Hausdorff dimension is equal to formula_22 ≈ 2.5819.\n\n"}
{"id": "58635600", "url": "https://en.wikipedia.org/wiki?curid=58635600", "title": "National Technological Initiative", "text": "National Technological Initiative\n\nThe National Technological Initiative (NTI) () is a 2014 program created by Vladimir Putin with the purpose of creating Russian global technical leadership in Russia. It is a $2.1 trillion “road map” for development of the cybernetics market until 2035.\n\n\n\n\n"}
{"id": "25005", "url": "https://en.wikipedia.org/wiki?curid=25005", "title": "Peano axioms", "text": "Peano axioms\n\nIn mathematical logic, the Peano axioms, also known as the Dedekind–Peano axioms or the Peano postulates, are axioms for the natural numbers presented by the 19th century Italian mathematician Giuseppe Peano. These axioms have been used nearly unchanged in a number of metamathematical investigations, including research into fundamental questions of whether number theory is consistent and complete.\n\nThe need to formalize arithmetic was not well appreciated until the work of Hermann Grassmann, who showed in the 1860s that many facts in arithmetic could be derived from more basic facts about the successor operation and induction. In 1881, Charles Sanders Peirce provided an axiomatization of natural-number arithmetic. In 1888, Richard Dedekind proposed another axiomatization of natural-number arithmetic, and in 1889, Peano published a simplified version of them as a collection of axioms in his book, \"The principles of arithmetic presented by a new method\" ().\n\nThe Peano axioms contain three types of statements. The first axiom asserts the existence of at least one member of the set of natural numbers. The next four are general statements about equality; in modern treatments these are often not taken as part of the Peano axioms, but rather as axioms of the \"underlying logic\". The next three axioms are first-order statements about natural numbers expressing the fundamental properties of the successor operation. The ninth, final axiom is a second order statement of the principle of mathematical induction over the natural numbers. A weaker first-order system called Peano arithmetic is obtained by explicitly adding the addition and multiplication operation symbols and replacing the second-order induction axiom with a first-order axiom schema.\n\nWhen Peano formulated his axioms, the language of mathematical logic was in its infancy. The system of logical notation he created to present the axioms did not prove to be popular, although it was the genesis of the modern notation for set membership (∈, which comes from Peano's ε) and implication (⊃, which comes from Peano's reversed 'C'.) Peano maintained a clear distinction between mathematical and logical symbols, which was not yet common in mathematics; such a separation had first been introduced in the \"Begriffsschrift\" by Gottlob Frege, published in 1879. Peano was unaware of Frege's work and independently recreated his logical apparatus based on the work of Boole and Schröder.\n\nThe Peano axioms define the arithmetical properties of \"natural numbers\", usually represented as a set N or formula_1 The non-logical symbols for the axioms consist of a constant symbol 0 and a unary function symbol \"S\".\n\nThe first axiom states that the constant 0 is a natural number:\n\nThe next four axioms describe the equality relation. Since they are logically valid in first-order logic with equality, they are not considered to be part of \"the Peano axioms\" in modern treatments.\n\nThe remaining axioms define the arithmetical properties of the natural numbers. The naturals are assumed to be closed under a single-valued \"successor\" function \"S\".\n\nPeano's original formulation of the axioms used 1 instead of 0 as the \"first\" natural number. This choice is arbitrary, as axiom 1 does not endow the constant 0 with any additional properties. However, because 0 is the additive identity in arithmetic, most modern formulations of the Peano axioms start from 0. Axioms 1, 6, 7, 8 define a unary representation of the intuitive notion of natural numbers: the number 1 can be defined as \"S\"(0), 2 as \"S\"(\"S\"(0)), etc. However, considering the notion of natural numbers as being defined by these axioms, axioms 1, 6, 7, 8 do not imply that the successor function generates all the natural numbers different from 0. Put differently, they do not guarantee that every natural number other than zero must succeed some other natural number.\n\nThe intuitive notion that each natural number can be obtained by applying \"successor\" sufficiently often to zero requires an additional axiom, which is sometimes called the \"axiom of induction\".\n\nThe induction axiom is sometimes stated in the following form:\nIn Peano's original formulation, the induction axiom is a second-order axiom. It is now common to replace this second-order principle with a weaker first-order induction scheme. There are important differences between the second-order and first-order formulations, as discussed in the section below.\n\nThe Peano axioms can be augmented with the operations of addition and multiplication and the usual total (linear) ordering on N. The respective functions and relations are constructed in set theory or second-order logic, and can be shown to be unique using the Peano axioms.\n\nAddition is a function that maps two natural numbers (two elements of N) to another one. It is defined recursively as:\n\nFor example:\n\nThe structure is a commutative monoid with identity element 0. is also a cancellative magma, and thus embeddable in a group. The smallest group embedding N is the integers.\n\nSimilarly, multiplication is a function mapping two natural numbers to another one. Given addition, it is defined recursively as:\n\nIt is easy to see that \"S\"(0) (or \"1\", in the familiar language of decimal representation) is the multiplicative right identity:\n\nTo show that \"S\"(0) is also the multiplicative left identity requires the induction axiom due to the way multiplication is defined:\n\n\nTherefore by the induction axiom \"S\"(0) is the multiplicative left identity of all natural numbers. Moreover, it can be shown that multiplication distributes over addition:\n\nThus, is a commutative semiring.\n\nThe usual total order relation ≤ on natural numbers can be defined as follows, assuming 0 is a natural number:\n\nThis relation is stable under addition and multiplication: for formula_5, if , then:\n\n\nThus, the structure is an ordered semiring; because there is no natural number between 0 and 1, it is a discrete ordered semiring.\n\nThe axiom of induction is sometimes stated in the following form that uses a stronger hypothesis, making use of the order relation \"≤\":\n\nThis form of the induction axiom, called \"strong induction\", is a consequence of the standard formulation, but is often better suited for reasoning about the ≤ order. For example, to show that the naturals are well-ordered—every nonempty subset of N has a least element—one can reason as follows. Let a nonempty be given and assume \"X\" has no least element.\n\n\nThus, by the strong induction principle, for every , . Thus, , which contradicts \"X\" being a nonempty subset of N. Thus \"X\" has a least element.\n\nAll of the Peano axioms except the ninth axiom (the induction axiom) are statements in first-order logic. The arithmetical operations of addition and multiplication and the order relation can also be defined using first-order axioms. The axiom of induction is in second-order, since it quantifies over predicates (equivalently, sets of natural numbers rather than natural numbers), but it can be transformed into a first-order \"axiom schema\" of induction. Such a schema includes one axiom per predicate definable in the first-order language of Peano arithmetic, making it weaker than the second-order axiom.\n\nFirst-order axiomatizations of Peano arithmetic have an important limitation, however. In second-order logic, it is possible to define the addition and multiplication operations from the successor operation, but this cannot be done in the more restrictive setting of first-order logic. Therefore, the addition and multiplication operations are directly included in the signature of Peano arithmetic, and axioms are included that relate the three operations to each other.\n\nThe following list of axioms (along with the usual axioms of equality), which contains six of the seven axioms of Robinson arithmetic, is sufficient for this purpose:\n\n\nIn addition to this list of numerical axioms, Peano arithmetic contains the induction schema, which consists of a countably infinite set of axioms. For each formula in the language of Peano arithmetic, the first-order induction axiom for \"φ\" is the sentence\n\nwhere formula_13 is an abbreviation for \"y\"...,\"y\". The first-order induction schema includes every instance of the first-order induction axiom, that is, it includes the induction axiom for every formula \"φ\".\n\nThere are many different, but equivalent, axiomatizations of Peano arithmetic. While some axiomatizations, such as the one just described, use a signature that only has symbols for 0 and the successor, addition, and multiplications operations, other axiomatizations use the language of ordered semirings, including an additional order relation symbol. One such axiomatization begins with the following axioms that describe a discrete ordered semiring.\n\nThe theory defined by these axioms is known as PA; the theory PA is obtained by adding the first-order induction schema. An important property of PA is that any structure formula_29 satisfying this theory has an initial segment (ordered by formula_30) isomorphic to formula_31. Elements in that segment are called standard elements, while other elements are called nonstandard elements.\n\nA model of the Peano axioms is a triple , where N is a (necessarily infinite) set, and satisfies the axioms above. Dedekind proved in his 1888 book, \"The Nature and Meaning of Numbers\" (, i.e., “What are the numbers and what are they good for?”) that any two models of the Peano axioms (including the second-order induction axiom) are isomorphic. In particular, given two models and of the Peano axioms, there is a unique homomorphism satisfying\n\nand it is a bijection. This means that the second-order Peano axioms are categorical. This is not the case with any first-order reformulation of the Peano axioms, however.\n\nThe Peano axioms can be derived from set theoretic constructions of the natural numbers and axioms of set theory such as ZF. The standard construction of the naturals, due to John von Neumann, starts from a definition of 0 as the empty set, ∅, and an operator \"s\" on sets defined as:\n\nThe set of natural numbers N is defined as the intersection of all sets closed under \"s\" that contain the empty set. Each natural number is equal (as a set) to the set of natural numbers less than it:\n\nand so on. The set N together with 0 and the successor function satisfies the Peano axioms.\n\nPeano arithmetic is equiconsistent with several weak systems of set theory. One such system is ZFC with the axiom of infinity replaced by its negation. Another such system consists of general set theory (extensionality, existence of the empty set, and the axiom of adjunction), augmented by an axiom schema stating that a property that holds for the empty set and holds of an adjunction whenever it holds of the adjunct must hold for all sets.\n\nThe Peano axioms can also be understood using category theory. Let \"C\" be a category with terminal object 1, and define the category of pointed unary systems, US(\"C\") as follows:\n\n\nThen \"C\" is said to satisfy the Dedekind–Peano axioms if US(\"C\") has an initial object; this initial object is known as a natural number object in \"C\". If is this initial object, and is any other object, then the unique map is such that\n\nThis is precisely the recursive definition of 0 and \"S\".\n\nAlthough the usual natural numbers satisfy the axioms of PA, there are other models as well (called \"non-standard models\"); the compactness theorem implies that the existence of nonstandard elements cannot be excluded in first-order logic. The upward Löwenheim–Skolem theorem shows that there are nonstandard models of PA of all infinite cardinalities. This is not the case for the original (second-order) Peano axioms, which have only one model, up to isomorphism. This illustrates one way the first-order system PA is weaker than the second-order Peano axioms.\n\nWhen interpreted as a proof within a first-order set theory, such as ZFC, Dedekind's categoricity proof for PA shows that each model of set theory has a unique model of the Peano axioms, up to isomorphism, that embeds as an initial segment of all other models of PA contained within that model of set theory. In the standard model of set theory, this smallest model of PA is the standard model of PA; however, in a nonstandard model of set theory, it may be a nonstandard model of PA. This situation cannot be avoided with any first-order formalization of set theory.\n\nIt is natural to ask whether a countable nonstandard model can be explicitly constructed. The answer is affirmative as Skolem in 1933 provided an explicit construction of such a nonstandard model. On the other hand, Tennenbaum's theorem, proved in 1959, shows that there is no countable nonstandard model of PA in which either the addition or multiplication operation is computable. This result shows it is difficult to be completely explicit in describing the addition and multiplication operations of a countable nonstandard model of PA. However, there is only one possible order type of a countable nonstandard model. Letting \"ω\" be the order type of the natural numbers, \"ζ\" be the order type of the integers, and \"η\" be the order type of the rationals, the order type of any countable nonstandard model of PA is , which can be visualized as a copy of the natural numbers followed by a dense linear ordering of copies of the integers.\n\nA cut in a nonstandard model \"M\" is a nonempty subset \"I\" of \"M\" so that \"I\" is downward closed ( \"x\"<\"y\" and \"y\"∈\"I\" implies \"x\"∈\"I\") and \"I\" is closed under successor. A proper cut is a cut that is a proper subset of \"M\". Each nonstandard model has many proper cuts, including one that corresponds to the standard natural numbers. However, the induction scheme in Peano arithmetic prevents any proper cut from being definable. The overspill lemma, first proved by Abraham Robinson, formalizes this fact. \n\nWhen the Peano axioms were first proposed, Bertrand Russell and others agreed that these axioms implicitly defined what we mean by a \"natural number\". Henri Poincaré was more cautious, saying they only defined natural numbers if they were \"consistent\"; if there is a proof that starts from just these axioms and derives a contradiction such as 0 = 1, then the axioms are inconsistent, and don't define anything. In 1900, David Hilbert posed the problem of proving their consistency using only finitistic methods as the second of his twenty-three problems. In 1931, Kurt Gödel proved his second incompleteness theorem, which shows that such a consistency proof cannot be formalized within Peano arithmetic itself.\n\nAlthough it is widely claimed that Gödel's theorem rules out the possibility of a finitistic consistency proof for Peano arithmetic, this depends on exactly what one means by a finitistic proof. Gödel himself pointed out the possibility of giving a finitistic consistency proof of Peano arithmetic or stronger systems by using finitistic methods that are not formalizable in Peano arithmetic, and in 1958, Gödel published a method for proving the consistency of arithmetic using type theory. In 1936, Gerhard Gentzen gave a proof of the consistency of Peano's axioms, using transfinite induction up to an ordinal called ε. Gentzen explained: \"The aim of the present paper is to prove the consistency of elementary number theory or, rather, to reduce the question of consistency to certain fundamental principles\". Gentzen's proof is arguably finitistic, since the transfinite ordinal ε can be encoded in terms of finite objects (for example, as a Turing machine describing a suitable order on the integers, or more abstractly as consisting of the finite trees, suitably linearly ordered). Whether or not Gentzen's proof meets the requirements Hilbert envisioned is unclear: there is no generally accepted definition of exactly what is meant by a finitistic proof, and Hilbert himself never gave a precise definition.\n\nThe vast majority of contemporary mathematicians believe that Peano's axioms are consistent, relying either on intuition or the acceptance of a consistency proof such as Gentzen's proof. A small number of philosophers and mathematicians, some of whom also advocate ultrafinitism, reject Peano's axioms because accepting the axioms amounts to accepting the infinite collection of natural numbers. In particular, addition (including the successor function) and multiplication are assumed to be total. Curiously, there are self-verifying theories that are similar to PA but have subtraction and division instead of addition and multiplication, which are axiomatized in such a way to avoid proving sentences that correspond to the totality of addition and multiplication, but which are still able to prove all true formula_40 theorems of PA, and yet can be extended to a consistent theory that proves its own consistency (stated as the non-existence of a Hilbert-style proof of \"0=1\").\n\n\n\n"}
{"id": "58740753", "url": "https://en.wikipedia.org/wiki?curid=58740753", "title": "Penny J. Davies", "text": "Penny J. Davies\n\nPenelope Jane Davies is a Scottish mathematician who in 2009 became the second female president of the Edinburgh Mathematical Society, after Elizabeth McHarg. Her research involves numerical simulation of the partial differential equations describing wave scattering. She is a senior lecturer in mathematics and statistics at the University of Strathclyde.\n\nDavies earned her Ph.D. in 1987 from Heriot-Watt University; her dissertation was \"Stability problems in nonlinear elasticity\".\nShe was on the staff of the University of Dundee before moving to the University of Strathclyde.\n\nDavies was made an Officer of the Order of the British Empire \"for services to mathematics\" in the 2014 Birthday Honours.\n"}
{"id": "1598022", "url": "https://en.wikipedia.org/wiki?curid=1598022", "title": "Pickover stalk", "text": "Pickover stalk\n\nPickover stalks are certain kinds of details to be found empirically in the Mandelbrot set, in the study of fractal geometry. They are so named after the researcher Clifford Pickover, whose \"epsilon cross\" method was instrumental in their discovery. An \"epsilon cross\" is a cross-shaped orbit trap.\n\nAccording to Vepstas (1997) \"Pickover hit on the novel concept of looking to see how closely the orbits of interior points come to the x and y axes. In these pictures, the closer that the point approaches, the higher up the color scale, with red denoting the closest approach. The logarithm of the distance is taken to accentuate the details\". \n\nBiomorphs are biological-looking Pickover Stalks. At the end of the 1980s, Pickover developed biological feedback organisms similar to Julia sets and the fractal Mandelbrot set. According to Pickover (1999) in summary, he \"described an algorithm which could be used for the creation of diverse and complicated forms resembling invertebrate organisms. The shapes are complicated and difficult to predict before actually experimenting with the mappings. He hoped these techniques would encourage others to explore further and discover new forms, by accident, that are on the edge of science and art\".\n\nPickover developed an algorithm (which uses neither random perturbations nor natural laws) to create very complicated forms resembling invertebrate organisms. The iteration, or recursion, of mathematical transformations is used to generate biological morphologies. He called them \"biomorphs.\" At the same time he coined \"biomorph\" for these patterns, the famous evolutionary biologist Richard Dawkins used the word to refer to his own set of biological shapes that were arrived at by a very different procedure. More rigorously, Pickover's \"biomorphs\" encompass the class of organismic morphologies created by small changes to traditional convergence tests in the field of \"Julia set\" theory.\n\nPickover's biomorphs show a self-similarity at different scales, a common feature of dynamical systems with feedback. Real systems, such as shorelines and mountain ranges, also show self-similarity over some scales. A 2-dimensional parametric 0L system can “look” like Pickover's biomorphs.\n\nThe below example, written in pseudocode, renders a Mandelbrot set colored using a Pickover Stalk with a transformation vector and a color dividend.\n\nThe transformation vector is used to offset the (x, y) position when sampling the point's distance to the horizontal and vertical axis.\n\nThe color dividend is a float used to determine how thick the stalk is when it is rendered. \n\nFor each pixel (x, y) on the target, do:\n\n"}
{"id": "367225", "url": "https://en.wikipedia.org/wiki?curid=367225", "title": "Plus-minus sign", "text": "Plus-minus sign\n\nThe plus-minus sign (±) is a mathematical symbol with multiple meanings.\nThe sign is normally pronounced \"plus or minus\" or \"plus-minus\".\n\nA version of the sign, including also the French word \"ou\" (\"or\") was used in its mathematical meaning by Albert Girard in 1626, and the sign in its modern form was used as early as William Oughtred's \"Clavis Mathematicae\" (1631).\n\nIn mathematical formulas, the ± symbol may be used to indicate a symbol that may be replaced by either the + or − symbols, allowing the formula to represent two values or two equations.\n\nFor example, given the equation \"x\" = 9, one may give the solution as \"x\" = ±3. This indicates that the equation has two solutions, each of which may be obtained by replacing this equation by one of the two equations \"x\" = +3 or \"x\" = −3. Only one of these two replaced equations is true for any valid solution. A common use of this notation is found in the quadratic formula\ndescribing the two solutions to the quadratic equation \"ax\" + \"bx\" + \"c\" = 0.\n\nSimilarly, the trigonometric identity\ncan be interpreted as a shorthand for two equations: one with \"+\" on both sides of the equation, and one with \"−\" on both sides. The two copies of the ± sign in this identity must both be replaced in the same way: it is not valid to replace one of them with \"+\" and the other of them with \"−\". In contrast to the quadratic formula example, both of the equations described by this identity are simultaneously valid.\n\nA third related usage is found in this presentation of the formula for the Taylor series of the sine function:\nHere, the plus-or-minus sign indicates that the signs of the terms alternate, where (starting the count at 0) the terms with an even index \"n\" are added while those with an odd index are subtracted. A more rigorous presentation of the same formula would multiply each term by a factor of (−1), which gives +1 when \"n\" is even and −1 when \"n\" is odd.\n\nThe use of for an approximation is most commonly encountered in presenting the numerical value of a quantity together with its tolerance or its statistical margin of error.\nFor example, \"\" may be anywhere in the range from 5.5 to 5.9 inclusive. In scientific usage it sometimes refers to a probability of being within the stated interval, usually corresponding to either 1 or 2 standard deviations (a probability of 68.3% or 95.4% in a normal distribution).\n\nA percentage may also be used to indicate the error margin. For example, refers to a voltage within 10% of either side of 230 V (from 207 V to 253 V inclusive). Separate values for the upper and lower bounds may also be used. For example, to indicate that a value is most likely 5.7 but may be as high as 5.9 or as low as 5.6, one may write .\n\nThe symbols ± and ∓ are used in chess notation to denote an advantage for white and black respectively. However, the more common chess notation would be only + and –. If a difference is made, the symbols + and − denote a larger advantage than ± and ∓.\n\nThe minus-plus sign (∓) is generally used in conjunction with the \"±\" sign, in such expressions as \"x ± y ∓ z\", which can be interpreted as meaning \"\"x\" + \"y\" − \"z\" and/or \"x\" − \"y\" + \"z\", but \"not\" \"x\" + \"y\" + \"z\" or \"x\" − \"y\" − \"z\"\". The upper \"−\" in \"∓\" is considered to be associated to the \"+\" of \"±\" (and similarly for the two lower symbols) even though there is no visual indication of the dependency. (However, the \"±\" sign is generally preferred over the \"∓\" sign, so if they both appear in an equation it is safe to assume that they are linked. On the other hand, if there are two instances of the \"±\" sign in an expression, it is impossible to tell from notation alone whether the intended interpretation is as two or four distinct expressions.) The original expression can be rewritten as \"\"x\" ± (\"y\" − \"z\")\" to avoid confusion, but cases such as the trigonometric identity\n\nare most neatly written using the \"∓\" sign. The trigonometric equation above thus represents the two equations:\nbut \"not\"\n\nbecause the signs are exclusively alternating.\n\nAnother example is\nwhich represents two equations.\n\n\nThe plus-minus sign resembles the Chinese characters and , whereas the minus-plus sign resembles .\n\n"}
{"id": "35054352", "url": "https://en.wikipedia.org/wiki?curid=35054352", "title": "Polygraph (mathematics)", "text": "Polygraph (mathematics)\n\nIn mathematics, and particularly in category theory, a polygraph is a generalisation of a directed graph. It is also known as a computad. They were introduced as \"polygraphs\" by Albert Burroni and as \"computads\" by Ross Street.\n\nIn the same way that a directed multigraph can freely generate a category, an \"n\"-computad is the \"most general\" structure which can generate a free n-category.\n"}
{"id": "308152", "url": "https://en.wikipedia.org/wiki?curid=308152", "title": "Quartic function", "text": "Quartic function\n\nIn algebra, a quartic function is a function of the form\nwhere \"a\" is nonzero,\nwhich is defined by a polynomial of degree four, called a quartic polynomial.\n\nSometimes the term biquadratic is used instead of \"quartic\", but, usually, biquadratic function refers to a quadratic function of a square (or, equivalently, to the function defined by a quartic polynomial without terms of odd degree), having the form\n\nA quartic equation, or equation of the fourth degree, is an equation that equates a quartic polynomial to zero, of the form\nwhere .\n\nThe derivative of a quartic function is a cubic function.\n\nSince a quartic function is defined by a polynomial of even degree, it has the same infinite limit when the argument goes to positive or negative infinity. If \"a\" is positive, then the function increases to positive infinity at both ends; and thus the function has a global minimum. Likewise, if \"a\" is negative, it decreases to negative infinity and has a global maximum. In both cases it may or may not have another local maximum and another local minimum.\n\nThe degree four (\"quartic\" case) is the highest degree such that every polynomial equation can be solved by radicals.\n\nLodovico Ferrari is credited with the discovery of the solution to the quartic in 1540, but since this solution, like all algebraic solutions of the quartic, requires the solution of a cubic to be found, it could not be published immediately. The solution of the quartic was published together with that of the cubic by Ferrari's mentor Gerolamo Cardano in the book \"Ars Magna\".\n\nThe Soviet historian I. Y. Depman claimed that even earlier, in 1486, Spanish mathematician Valmes was burned at the stake for claiming to have solved the quartic equation. Inquisitor General Tomás de Torquemada allegedly told Valmes that it was the will of God that such a solution be inaccessible to human understanding. However Beckmann, who popularized this story of Depman in the West, said that it was unreliable and hinted that it may have been invented as Soviet antireligious propaganda. Beckmann's version of this story has been widely copied in several books and internet sites, usually without his reservations and sometimes with fanciful embellishments. Several attempts to find corroborating evidence for this story, or even for the existence of Valmes, have failed.\n\nThe proof that four is the highest degree of a general polynomial for which such solutions can be found was first given in the Abel–Ruffini theorem in 1824, proving that all attempts at solving the higher order polynomials would be futile. The notes left by Évariste Galois prior to dying in a duel in 1832 later led to an elegant complete theory of the roots of polynomials, of which this theorem was one result.\n\nEach coordinate of the intersection points of two conic sections is a solution of a quartic equation. The same is true for the intersection of a line and a torus. It follows that quartic equations often arise in computational geometry and all related fields such as computer graphics, computer-aided design, computer-aided manufacturing and optics. Here are examples of other geometric problems whose solution involves solving a quartic equation.\n\nIn computer-aided manufacturing, the torus is a shape that is commonly associated with the endmill cutter. To calculate its location relative to a triangulated surface, the position of a horizontal torus on the -axis must be found where it is tangent to a fixed line, and this requires the solution of a general quartic equation to be calculated.\n\nA quartic equation arises also in the process of solving the crossed ladders problem, in which the lengths of two crossed ladders, each based against one wall and leaning against another, are given along with the height at which they cross, and the distance between the walls is to be found.\n\nIn optics, Alhazen's problem is \"Given a light source and a spherical mirror, find the point on the mirror where the light will be reflected to the eye of an observer.\" This leads to a quartic equation.\n\nFinding the distance of closest approach of two ellipses involves solving a quartic equation.\n\nThe eigenvalues of a 4×4 matrix are the roots of a quartic polynomial which is the characteristic polynomial of the matrix.\n\nThe characteristic equation of a fourth-order linear difference equation or differential equation is a quartic equation. An example arises in the Timoshenko-Rayleigh theory of beam bending.\n\nIntersections between spheres, cylinders, or other quadrics can be found using quartic equations.\n\nLetting and be the distinct inflection points of a quartic, and letting be the intersection of the inflection secant line and the quartic, nearer to than to , then divides into the golden section:\n\nMoreover, the area of the region between the secant line and the quartic below the secant line equals the area of the region between the secant line and the quartic above the secant line. One of those regions is disjointed into sub-regions of equal area.\n\nGiven the general quartic equation\nwith real coefficients and the nature of its roots is mainly determined by the sign of its discriminant \nThis may be refined by considering the signs of four other polynomials:\nsuch that is the second degree coefficient of the associated depressed quartic (see below);\nsuch that is the first degree coefficient of the associated depressed quartic; \nwhich is 0 if the quartic has a triple root; and\nwhich is 0 if the quartic has two double roots.\n\nThe possible cases for the nature of the roots are as follows:\n\n\nThere are some cases that do not seem to be covered, but they cannot occur. For example, , = 0 and ≤ 0 is not one of the cases. However, if and = 0 then > 0 so this combination is not possible.\n\nThe four roots , , , and for the general quartic equation\nwith ≠ 0 are given in the following formula, which is deduced from the one in the section on Ferrari's method by back changing the variables (see ) and using the formulas for the quadratic and cubic equations.\n\nwhere and are the coefficients of the second and of the first degree respectively in the associated depressed quartic\nand where\n\nwith\nand\n\n\n\nConsider the general quartic\nIt is reducible if , where and are non-constant polynomials with rational coefficients (or more generally with coefficients in the same field as the coefficients of ). Such a factorization will take one of two forms:\n\nor\nIn either case, the roots of are the roots of the factors, which may be computed using the formulas for the roots of a quadratic function or cubic function.\n\nDetecting the existence of such factorizations can be done using the resolvent cubic of. It turns out that:\n\nIn fact, several methods of solving quartic equations (Ferrari's method, Descartes' method, and, to a lesser extent, Euler's method) are based upon finding such factorizations.\n\nIf then the biquadratic function\n\ndefines a biquadratic equation, which is easy to solve.\n\nLet .\nThen becomes a quadratic in : . Let and be the roots of . Then the roots of our quartic are\n\nThe polynomial\nis almost palindromic, as (it is palindromic if ). The change of variables in produces the quadratic equation . Since , the quartic equation may be solved by applying the quadratic formula twice.\n\nFor solving purposes, it is generally better to convert the quartic into a depressed quartic by the following simple change of variable. All formulas are simpler and some methods work only in this case. The roots of the original quartic are easily recovered from that of the depressed quartic by the reverse change of variable.\n\nLet\nbe the general quartic equation we want to solve.\n\nDividing by , provides the equivalent equation , with , , , and .\nSubstituting for gives, after regrouping the terms, the equation ,\nwhere\nHowever, this induces a division by zero if . This implies , and thus that the depressed equation is bi-quadratic, and may be solved by an easier method (see above). This was not a problem at the time of Ferrari, when one solved only explicitly given equations with numeric coefficients. For a general formula that is always true, one thus needs to choose a root of the cubic equation such that . This is always possible except for the depressed equation .\n\nNow, if is a root of the cubic equation such that , equation (') becomes\n\nThis equation is of the form , which can be rearranged as or . Therefore, equation (') may be rewritten as\n\nThis equation is easily solved by applying to each factor the quadratic formula. Solving them we may write the four roots as\nwhere and denote either or . As the two occurrences of must denote the same sign, this leaves four possibilities, one for each root.\n\nTherefore, the solutions of the original quartic equation are\nA comparison with the general formula above shows that .\n\nDescartes introduced in 1637 the method of finding the roots of a quartic polynomial by factoring it into two quadratic ones. Let\n\nBy equating coefficients, this results in the following system of equations:\n\nThis can be simplified by starting again with the depressed quartic , which can be obtained by substituting for . Since the coefficient of is , we get , and:\n\nOne can now eliminate both and by doing the following:\n\nIf we set , then solving this equation becomes finding the roots of the resolvent cubic\n\nwhich is done elsewhere. This resolvent cubic is equivalent to the resolvent cubic given above (equation (1a)), as can be seen by substituting U = 2m.\n\nIf is a square root of a non-zero root of this resolvent (such a non-zero root exists except for the quartic , which is trivially factored),\n\nThe symmetries in this solution are as follows. There are three roots of the cubic, corresponding to the three ways that a quartic can be factored into two quadratics, and choosing positive or negative values of for the square root of merely exchanges the two quadratics with one another.\n\nThe above solution shows that a quartic polynomial with rational coefficients and a zero coefficient on the cubic term is factorable into quadratics with rational coefficients if and only if either the resolvent cubic (') has a non-zero root which is the square of a rational, or is the square of rational and ; this can readily be checked using the rational root test.\n\nA variant of the previous method is due to Euler. Unlike the previous methods, both of which use \"some\" root of the resolvent cubic, Euler's method uses all of them. Let us consider a depressed quartic . Observe that, if\nthen\nTherefore, . In other words, is one of the roots of the resolvent cubic (') and this suggests that the roots of that cubic are equal to , , and . This is indeed true and it follows from Vieta's formulas. It also follows from Vieta's formulas, together with the fact that we are working with a depressed quartic, that . (Of course, this also follows from the fact that .) Therefore, if , , and are the roots of the resolvent cubic, then the numbers , , , and are such that\nIt is a consequence of the first two equations that is a square root of and that is the other square root of . For the same reason,\nTherefore, the numbers , , , and are such that\nthe sign of the square roots will be dealt with below. The only solution of this system is:\nSince, in general, there are two choices for each square root, it might look as if this provides choices for the set }, but, in fact, it provides no more than  such choices, because the consequence of replacing one of the square roots by the symmetric one is that the set } becomes the set }.\n\nIn order to determine the right sign of the square roots, one simply chooses some square root for each of the numbers , , and and uses them to compute the numbers , , , and from the previous equalities. Then, one computes the number . Note that, since , , and are the roots of ('), it is a consequence of Vieta's formulas that their product is equal to and therefore that . But a straightforward computation shows that\nIf this number is , then the choice of the square roots was a good one (again, by Vieta's formulas); otherwise, the roots of the polynomial will be , , , and , which are the numbers obtained if one of the square roots is replaced by the symmetric one (or, what amounts to the same thing, if each of the three square roots is replaced by the symmetric one).\n\nThis argument suggests another way of choosing the square roots:\nOf course, this will make no sense if or is equal to , but is a root of (') only when , that is, only when we are dealing with a biquadratic equation, in which case there is a much simpler approach.\n\nThe symmetric group on four elements has the Klein four-group as a normal subgroup. This suggests using a whose roots may be variously described as a discrete Fourier transform or a Hadamard matrix transform of the roots; see Lagrange resolvents for the general method. Denote by , for from  to , the four roots of . If we set\n\nthen since the transformation is an involution we may express the roots in terms of the four in exactly the same way. Since we know the value , we only need the values for , and . These are the roots of the polynomial\n\nSubstituting the by their values in term of the , this polynomial may be expanded in a polynomial in whose coefficients are symmetric polynomials in the . By the fundamental theorem of symmetric polynomials, these coefficients may be expressed as polynomials in the coefficients of the monic quartic. If, for simplification, we suppose that the quartic is depressed, that is , this results in the polynomial\n\nThis polynomial is of degree six, but only of degree three in , and so the corresponding equation is solvable by the method described in the article about cubic function. By substituting the roots in the expression of the in terms of the , we obtain expression for the roots. In fact we obtain, apparently, several expressions, depending on the numbering of the roots of the cubic polynomial and of the signs given to their square roots. All these different expressions may be deduced from one of them by simply changing the numbering of the .\n\nThese expressions are unnecessarily complicated, involving the cubic roots of unity, which can be avoided as follows. If is any non-zero root of ('), and if we set\n\nthen\n\nWe therefore can solve the quartic by solving for and then solving for the roots of the two factors using the quadratic formula.\n\nNote that this gives exactly the same formula for the roots as the one provided by Descartes' method.\n\nThere is an alternative solution using algebraic geometry In brief, one interprets the roots as the intersection of two quadratic curves, then finds the three reducible quadratic curves (pairs of lines) that pass through these points (this corresponds to the resolvent cubic, the pairs of lines being the Lagrange resolvents), and then use these linear equations to solve the quadratic.\n\nThe four roots of the depressed quartic may also be expressed as the coordinates of the intersections of the two quadratic equations and i.e., using the substitution that two quadratics intersect in four points is an instance of Bézout's theorem. Explicitly, the four points are for the four roots of the quartic.\n\nThese four points are not collinear because they lie on the irreducible quadratic and thus there is a 1-parameter family of quadratics (a pencil of curves) passing through these points. Writing the projectivization of the two quadratics as quadratic forms in three variables:\nthe pencil is given by the forms for any point in the projective line — in other words, where and are not both zero, and multiplying a quadratic form by a constant does not change its quadratic curve of zeros.\n\nThis pencil contains three reducible quadratics, each corresponding to a pair of lines, each passing through two of the four points, which can be done formula_71 =  different ways. Denote these , , and . Given any two of these, their intersection has exactly the four points.\n\nThe reducible quadratics, in turn, may be determined by expressing the quadratic form as a  matrix: reducible quadratics correspond to this matrix being singular, which is equivalent to its determinant being zero, and the determinant is a homogeneous degree three polynomial in and and corresponds to the resolvent cubic.\n\n\n\n"}
{"id": "48720870", "url": "https://en.wikipedia.org/wiki?curid=48720870", "title": "Quasi-exact solvability", "text": "Quasi-exact solvability\n\nA linear differential operator \"L\" is called quasi-exactly-solvable (QES) if it has a finite-dimensional invariant subspace of functions formula_1 such that formula_2 where \"n\" is a dimension of formula_1. There are two important cases:\n\nThe most studied cases are one-dimensional formula_7-Lie-algebraic quasi-exactly-solvable (Schrödinger) operators. The best known example is the sextic QES anharmonic oscillator with the Hamiltonian\n\nformula_8\n\nwhere (\"n+1\") eigenstates of positive (negative) parity can be found algebraically. Their eigenfunctions are of the form\n\nformula_9\n\nwhere formula_10 is a polynomial of degree \"n\" and (energies) eigenvalues are roots of an algebraic equation of degree (\"n+1\"). In general, twelve families of one-dimensional QES problems are known, two of them characterized by elliptic potentials.\n\n"}
{"id": "3440936", "url": "https://en.wikipedia.org/wiki?curid=3440936", "title": "Residue-class-wise affine group", "text": "Residue-class-wise affine group\n\nIn mathematics, specifically in group theory, residue-class-wise affine\ngroups are certain permutation groups acting on\nformula_1 (the integers), whose elements are bijective\nresidue-class-wise affine mappings.\n\nA mapping formula_2 is called residue-class-wise affine\nif there is a nonzero integer formula_3 such that the restrictions of formula_4\nto the residue classes\n(mod formula_3) are all affine. This means that for any\nresidue class formula_6 there are coefficients\nformula_7\nsuch that the restriction of the mapping formula_4\nto the set formula_9 is given by\n\nResidue-class-wise affine groups are countable, and they are accessible\nto computational investigations.\nMany of them act multiply transitively on formula_1 or on subsets thereof.\n\nA particularly basic type of residue-class-wise affine permutations are the\nclass transpositions: given disjoint residue classes formula_12\nand formula_13, the corresponding class transposition is the permutation\nof formula_1 which interchanges formula_15 and\nformula_16 for every formula_17 and which\nfixes everything else. Here it is assumed that\nformula_18 and that formula_19.\n\nThe set of all class transpositions of formula_1 generates\na countable simple group which has the following properties:\n\n\nIt is straightforward to generalize the notion of a residue-class-wise affine group\nto groups acting on suitable rings other than formula_1,\nthough only little work in this direction has been done so far.\n\nSee also the Collatz conjecture, which is an assertion about a surjective,\nbut not injective residue-class-wise affine mapping.\n\n"}
{"id": "19403762", "url": "https://en.wikipedia.org/wiki?curid=19403762", "title": "Ronald M. Foster", "text": "Ronald M. Foster\n\nRonald Martin Foster (3 October 1896 – 2 February 1998), was a Bell Labs mathematician whose work was of significance regarding electronic filters for use on telephone lines. He published an important paper, \"A Reactance Theorem\", (see Foster's reactance theorem) which quickly inspired Wilhelm Cauer to begin his program of network synthesis filters which put the design of filters on a firm mathematical footing. He is also known for the Foster census of cubic symmetric graphs and the 90-vertex cubic symmetric Foster graph.\n\nFoster was a Harvard College graduate S.B. (Mathematics), summa cum laude, Class of 1917. He also received two honorary Sc.D.s.\n\n\n"}
{"id": "980971", "url": "https://en.wikipedia.org/wiki?curid=980971", "title": "Schismogenesis", "text": "Schismogenesis\n\nSchismogenesis literally means \"creation of division\". The term derives from the Greek words σχίσμα \"skhisma\" \"cleft\" (borrowed into English as schism, \"division into opposing factions\"), and γένεσις \"genesis\" \"generation, creation\" (deriving in turn from \"gignesthai\" \"be born or produced, creation, a coming into being\").\n\nThe concept of schismogenesis was developed by the anthropologist Gregory Bateson in the 1930s, to account for certain forms of social behavior between groups among the Iatmul people of the Sepik River. Bateson first published the concept in 1935, but elaborated on schismogenesis in his classic 1936 ethnography \"Naven: A Survey of the Problems suggested by a Composite Picture of the Culture of a New Guinea Tribe drawn from Three Points of View\", reissued with a new Epilogue in 1958. The word \"naven\" refers to an honorific ceremony among the Iatmul (still practiced) whereby certain categories of kin celebrate first-time cultural achievements. In a schematic summary, Bateson focused on how groups of women and groups of men (especially the honorees mothers' brothers) seemingly inverted their everyday, gendered-norms for dress, behavior, and emotional expression. For the most part, these groups of people belonged to different patrilineages who not only did not regularly renew their marriage alliances, but also interacted through the mode he called schismogenesis. Men and women, too, interacted in this mode. And thus the naven ritual served to correct schismogenesis, enabling the society to endure.\n\nIn his 1936 book \"Naven\", Bateson defined schismogenesis as \"a process of differentiation in the norms of individual behaviour resulting from cumulative interaction between individuals\" (p. 175). He continued:\n\n\"It is at once apparent that many systems of relationship, either between individuals or groups of individuals, contain a tendency towards progressive change. If, for example, one of the patterns of cultural behaviour, considered appropriate in individual A, is culturally labelled as an assertive pattern, while B is expected to reply to this with what is culturally regarded as submission, it is likely that this submission will encourage a further assertion, and that this assertion will demand still further submission. We have thus a potentially progressive state of affairs, and unless other factors are present to restrain the excesses of assertive and submissive behaviour, A must necessarily become more and more assertive, while B will become more and more submissive; and this progressive change will occur whether A and B are separate individuals or members of complementary groups.\"\n\n\"Progressive changes of this sort we may describe as complementary schismogenesis. But there is another pattern of relationships between individuals or groups of individuals which equally contains the germs of progressive change. If, for example, we find boasting as the cultural pattern of behaviour in one group, and that the other group replies to this with boasting, a competitive situation may develop in which boasting leads to more boasting, and so on. This type of progressive change we may call symmetrical schismogenesis\" (pp. 176-77). \n\nSomewhat analogous to Émile Durkheim's concepts of mechanical and organic solidarity (see functionalism), Bateson understood the symmetrical form of schismogenic behavior among Iatmul men to be one of a competitive relationship between categorical equals (e.g., rivalry). Thus one man, or a group of men, boast, and another man/group must offer an equal or better boast, prompting the first group to respond accordingly, and so forth. Complementary schismogenesis among the Iatmul was seen by Bateson between mainly men and women, or between categorical unequals (e.g., dominance and submission). Men would act dominant, leading women to act submissive, to which men responded with more dominance, and so forth. In both types of schismogenesis, the everyday emotional norms or ethos of Iatmul men and women prevented a halt to schismogenesis. The crux of the matter for Bateson was that, left unchecked, either form of schismogensis would cause Iatmul society simply to break apart. Thus some social or cultural mechanism was needed by society to maintain social integration. That mechanism among the Iatmul was the naven rite. Bateson's specific contribution was to suggest that certain concrete ritual behaviors either inhibited or stimulated the schismogenic relationship in its various forms.\n\nBateson's treatment of conflict escalation has been used to explain how conflicts arise over natural resources, including human-predator conflicts in Norway and also for conflicts among stakeholder groups in shared fisheries, In the latter case, Harrison and Loring compare conflict schismogenesis to the Tragedy of the Commons, arguing that it is a similar kind of escalation of behavior also caused by the failure of social institutions to ensure equity in fisheries management outcomes.\n\nSteven Feld (1994, p. 265-271), apparently in response to R. Murray Schafer's \"schizophonia\" and borrowing the term from Bateson, employs \"schismogenesis\" to name the recombination and recontextualization of sounds split from their sources.\n\nBateson, in \"Steps to an Ecology of Mind\" describes the two forms of schismogenesis and proposes that both forms are self-destructive to the parties involved. He goes on to suggest that researchers look into methods that one or both parties may employ to stop a schismogenesis before it reaches its destructive stage.\n\nThe first type of schismogenesis is best characterized by a class struggle, but is defined more broadly to include a range of other possible social phenomena. Given two groups of people, the interaction between them is such that a behavior \"X\" from one side elicits a behavior \"Y\" from the other side, The two behaviors complement one another, exemplified in the dominant-submissive behaviors of a class struggle. Furthermore, the behaviors may exaggerate one another, leading to a severe rift and possible conflict. Conflict can be reduced by narrowing information asymmetries between the two groups.\n\nThe second type of schismogenesis is best shown by an arms race. The behaviors of the parties involved elicit similar or symmetrical behaviors from the other parties. In the case of the United States and the Soviet Union, each party continually sought to amass more nuclear weapons than the other party, a clearly fruitless but seemingly necessary endeavor on both sides.\n\nA form of symmetrical schismogenesis exists in common sporting events, such as baseball, where the rules are the same for both teams.\n\nIn the field of communication, complementary schismogenesis is a force that can take effect in a conversation where people have different conversational styles, \"creating a split in a mutually aggravating way\". The effect causes two well-meaning individuals having a conversation to ramp up different styles, resulting in a disagreement that does not stem from actual difference of opinion. For example, if one person's conversational style favoured louder voices, while the other favoured softer speech, the first person might increase the loudness in their voice while the other spoke softer and softer, each trying to lead the conversation towards their style's conception of normal talking.\nSystems of holding back are also a form of schismogenesis. They are defined as \"mutually aggregating spirals which lead people to hold back contributions they could make because others hold back contributions they could make.\"\n\nIn Systems intelligence literature, it is held that human interaction has a tendency to fall into such systems unless conscious effort is made to counter this tendency. For example, although most managers would want to give support to their team and most team members would like to receive such support many times support does not result. This is because both parties might feel that the other party is not giving enough and thus they will themselves hold back what they in the best case could give. It has been suggested that systems of holding back are \"the single most important key to life-decreasing, reciprocity-trivializing and vitality-downgrading mechanisms in human life.\" \n"}
{"id": "1913196", "url": "https://en.wikipedia.org/wiki?curid=1913196", "title": "Shadow price", "text": "Shadow price\n\nA shadow price is commonly referred to as a monetary value assigned to currently unknowable or difficult-to-calculate costs. It is based on the willingness to pay principle - in the absence of market prices, the most accurate measure of the value of a good or service is what people are willing to give up in order to get it. Shadow pricing is often calculated on certain assumptions and premises. As a result, it is subjective and somewhat imprecise and inaccurate. The origin of these costs is typically due to an externalization of costs or an unwillingness to recalculate a system to account for marginal production. For example, consider a firm that already has a factory full of equipment and staff. They might estimate the shadow price for a few more units of production as simply the cost of the overtime. In this manner, some goods and services have near zero shadow prices, for example information goods. Less formally, a shadow price can be thought of as the cost of decisions made at the margin without consideration for the total cost.\n\nWhile shadow pricing may be imprecise and inaccurate, it is still frequently employed as a useful technique and is widely used in cost-benefit analyses. For instance, before taking on a project, businesses and governments may want to weigh the costs and benefits of the project to decide whether the project is worthwhile. While tangible costs and benefits such as the cost of labor are easy to quantify, intangible costs and benefits such as the number of hours saved is much more difficult to quantify. In this case, business owners and policymakers turn to shadow pricing to determine what these intangibles are. There are usually numerous tools that help those to determine what the monetary values of these intangibles are. Some of the most common are: contingent valuation, revealed preferences, and hedonic pricing.\n\nIn constrained optimization in economics, the shadow price is the change, per infinitesimal unit of the constraint, in the optimal value of the objective function of an optimization problem obtained by relaxing the constraint. If the objective function is utility, it is the marginal utility of relaxing the constraint. If the objective function is cost, it is the marginal cost of strengthening the constraint. In a business application, a shadow price is the maximum price that management is willing to pay for an extra unit of a given limited resource. For example, if a production line is already operating at its maximum 40-hour limit, the shadow price would be the maximum price the manager would be willing to pay for operating it for an additional hour, based on the benefits he would get from this change.\n\nIn advance of adequate regulation or market pricing for some commodity items conservative organizations will place on their balance sheets a value they believe to be an accurate reflection of the value of those items to their operations. This is common for companies with a large carbon footprint or water footprint. As an example Microsoft has placed a $27/ton price on its carbon emissions which is then billed to the P&L of its individual business units and used to fund the company's renewable energy and efficiency programs.\n\nMore formally, the shadow price is the value of the Lagrange multiplier at the optimal solution, which means that it is the infinitesimal change in the objective function arising from an infinitesimal change in the constraint. This follows from the fact that at the optimal solution the gradient of the objective function is a linear combination of the constraint function gradients with the weights equal to the Lagrange multipliers. Each constraint in an optimization problem has a shadow price or dual variable.\n\nShadow pricing is frequently used to figure out the monetary values of intangibles and hard to quantify factors during cost-benefit analyses. In the context of public economics, shadow pricing is very useful for governments and policymakers to evaluate whether a public project should be pursued. This is because public goods are very rarely exchanged in the market, making it difficult to determine its price. To help determine the monetary value of these goods, these three tools are often used. Take the example of a government determining whether it wants to undertake a freeway project that would save commuters 500,000 hours a year, save 5 lives a year, and reduce air pollution due to decreased congestion but with a present value cost of $250 million.\n\nContingent valuation estimates the value a person places on a good by asking him or her directly. It is essentially surveys for individuals on how much they would be willing to pay for some intangible benefits or to avoid some intangible harms. Typically, these surveys contain detailed descriptions of hypothetical public goods or services, ask respondents how much they would pay for it, and collect relevant demographic data of these respondents. Some common types of these survey questions include: open-ended, referendum-type, payment-card type, and double-bounded referendum-type.\n\nThe advantage of contingent valuation is that it is sometimes the only feasible method for valuing a public good. This is especially the case when there is no obvious market price that one can use to determine the value. On the contrary, there are also many disadvantages of this method. For instance, how the survey is structured and how the questions are framed can lead to widely varying results and can induce bias into the results. Other times, the respondents may simply have no idea how much they value the public good in question.\n\nIn the freeway project example, policymakers can design a survey that asks respondents on how much they would pay to save a certain period of time or to spend less time in traffic. However, it may be difficult or uncomfortable to ask respondents how much they value a life.\n\nRevealed preferences are based on observations on real world behaviors to determine how much individuals place on non-monetary outcomes. In other words, observing individuals' purchasing behaviors is the best way to determine their preferences. It assumes that individuals have made their purchasing decisions over other alternatives - making their final purchases the preferred one. It also allows room for the preferred choice to vary depending on the prices and the budgetary constraints. As such, by varying prices and budgetary constraints, a schedule can be created of an individual's/individuals' preferred choices under certain prices and constraints.\n\nThe advantage of revealed preferences is that it reduces biases that contingent valuation may bring. As it is based on real-world behaviors, it is much harder for individuals to manipulate or guess-work their answers. On the contrary, this tool also has its limits. For example, it is difficult to control for other factors that may make one prefer a choice over another. It also fails to fully incorporate indifference between two equally preferred choices.\n\nIn the freeway project example, where contingent valuation may fall short in determining how much individuals value lives, revealed preferences may be better suited. For instance, policymakers can look at how much more individuals need to be paid to take on riskier jobs that increase the probability of fatality. However, the drawbacks with revealed preferences also arise - in this case, if the riskier jobs increase the probability of not only death but also injury, the higher wages may incorporate the two factors, misrepresenting the result.\n\nHedonic pricing is a model that uses regression analysis to isolate the value of a specific intangible cost or benefit. It is based on the premise that that price is determined by both internal characteristics and external factors. It also assumes that individuals value the characteristics of a good rather than the good itself, which implies that price will reflect a set of internal and external characteristics. It is most often used to calculate variances in housing prices that reflect the value of local environmental factors. The model is based on widely-available and relatively accurate market data, making this method uncontroversial and inexpensive to use.\n\nAs such, one of hedonic pricing's main advantages is that it can be used to estimate values on actual choices. This method is also very versatile and can be adapted to incorporate multiple other interactions with other factors. However, one of its major downfalls is that it is rather limited - it can mostly only measure things that are related to housing prices. It also assumes that individuals have the freedom and power to select the preferred combination given their income but in actuality, this may not be the case as the market may be influenced by changes in taxes and interest rates.\n\nIn the freeway project example, hedonic pricing may be useful to value the benefits of reduced air pollution. It can run a regression of home values on clean air with a variety of control variables that can include home size, age of home, number of bedrooms and bathrooms, crime statistics, school qualities, etc. Hedonic pricing may also be considered in quantifying the monetary value of time saved. It can run a regression of home values on proximity to work with a similar set of control variables.\n\nSuppose a consumer with utility function formula_1 faces prices formula_2 and is endowed with income formula_3 Then the consumer's problem is:\nformula_4. Forming the Lagrangian auxiliary function formula_5, taking first order conditions and solving for its saddle point we obtain formula_6 which satisfy:\nThis gives us a clear interpretation of the Lagrange multiplier in the context of consumer maximization. If the consumer is given an extra dollar (the budget constraint is relaxed) at the optimal consumption level where the marginal utility per dollar for each good is equal to formula_8 as above, then the change in maximal utility per dollar of additional income will be equal to formula_8 since at the optimum the consumer gets the same amount of marginal utility per dollar from spending his additional income on either good.\n\nHolding prices fixed, if we define the indirect utility function as\nthen we have the identity\nwhere formula_12 are the demand functions, i.e. formula_13\n\nNow define the optimal expenditure function\n\nAssume differentiability and that formula_15 is the solution at formula_16, then we have from the multivariate chain rule:\nNow we may conclude that\nThis again gives the obvious interpretation, one extra dollar of optimal expenditure will lead to formula_8 units of optimal utility.\n\nIn optimal control theory, the concept of shadow price is reformulated as costate equations, and one solves the problem by minimization of the associated Hamiltonian via Pontryagin's minimum principle.\n\n"}
{"id": "292222", "url": "https://en.wikipedia.org/wiki?curid=292222", "title": "Stochastic", "text": "Stochastic\n\nThe word stochastic is an adjective in English that describes something that was randomly determined. The word first appeared in English to describe a mathematical object called a stochastic process, but now in mathematics the terms \"stochastic process\" and \"random process\" are considered interchangeable. The word, with its current definition meaning random, came from German, but it originally came .\n\nThe term \"stochastic\" is used in many different fields, particularly where stochastic or random processes are used to represent systems or phenomena that seem to change in a random way. Examples of such fields include the physical sciences such as biology, chemistry, ecology, neuroscience, and physics as well as technology and engineering fields such as image processing, signal processing, information theory, computer science, cryptography and telecommunications. It is also used in finance, due to seemingly random changes in financial markets.\n\nThe word \"stochastic\" in English was originally used as an adjective with the definition \"pertaining to conjecturing\", and stemming from a Greek word meaning \"to aim at a mark, guess\", and the Oxford English Dictionary gives the year 1662 as its earliest occurrence. In his work on probability \"Ars Conjectandi\", originally published in Latin in 1713, Jakob Bernoulli used the phrase \"Ars Conjectandi sive Stochastice\", which has been translated to \"the art of conjecturing or stochastics\". This phrase was used, with reference to Bernoulli, by Ladislaus Bortkiewicz who in 1917 wrote in German the word \"stochastik\" with a sense meaning random. The term \"stochastic process\" first appeared in English in a 1934 paper by Joseph Doob. For the term and a specific mathematical definition, Doob cited another 1934 paper, where the term \"stochastischer Prozeß\" was used in German by Aleksandr Khinchin, though the German term had been used earlier in 1931 by Andrey Kolmogorov.\n\nIn the early 1930s, Aleksandr Khinchin gave the first mathematical definition of a stochastic process as a set of random variables indexed by the real line. Further fundamental work on probability theory and stochastic processes was done by Khinchin as well as other mathematicians such as Andrey Kolmogorov, Joseph Doob, William Feller, Maurice Fréchet, Paul Lévy, Wolfgang Doeblin, and Harald Cramér. Decades later Cramér referred to the 1930s as the \"heroic period of mathematical probability theory\".\n\nIn mathematics, specifically probability theory, the theory of stochastic processes is considered to be an important contribution to mathematics and it continues to be an active topic of research for both theoretical reasons and applications.\n\nThe word \"stochastic\" is used to describe other terms and objects in mathematics. Examples include a stochastic matrix, which describes a stochastic process known as a Markov process, and stochastic calculus, which involves differential equations and integrals based on stochastic processes such as the Wiener process, also called the Brownian motion process.\n\nIn artificial intelligence, stochastic programs work by using probabilistic methods to solve problems, as in simulated annealing, stochastic neural networks, stochastic optimization, genetic algorithms, and genetic programming. A problem itself may be stochastic as well, as in planning under uncertainty.\n\nOne of the simplest continuous-time stochastic processes is Brownian motion. This was first observed by botanist Robert Brown while looking through a microscope at pollen grains in water.\n\nThe name \"Monte Carlo\" for the stochastic Monte Carlo method was popularized by physics researchers Stanisław Ulam, Enrico Fermi, John von Neumann, and Nicholas Metropolis, among others. The name is a reference to the Monte Carlo Casino in Monaco where Ulam's uncle would borrow money to gamble. The use of randomness and the repetitive nature of the process are analogous to the activities conducted at a casino.\nMethods of simulation and statistical sampling generally did the opposite: using simulation to test a previously understood deterministic problem. Though examples of an \"inverted\" approach do exist historically, they were not considered a general method until the popularity of the Monte Carlo method spread.\n\nPerhaps the most famous early use was by Enrico Fermi in 1930, when he used a random method to calculate the properties of the newly discovered neutron. Monte Carlo methods were central to the simulations required for the Manhattan Project, though were severely limited by the computational tools at the time. Therefore, it was only after electronic computers were first built (from 1945 on) that Monte Carlo methods began to be studied in depth. In the 1950s they were used at Los Alamos for early work relating to the development of the hydrogen bomb, and became popularized in the fields of physics, physical chemistry, and operations research. The RAND Corporation and the U.S. Air Force were two of the major organizations responsible for funding and disseminating information on Monte Carlo methods during this time, and they began to find a wide application in many different fields.\n\nUses of Monte Carlo methods require large amounts of random numbers, and it was their use that spurred the development of pseudorandom number generators, which were far quicker to use than the tables of random numbers which had been previously used for statistical sampling.\n\nStochastic resonance: In biological systems, introducing stochastic \"noise\" has been found to help improve the signal strength of the internal feedback loops for balance and other vestibular communication. It has been found to help diabetic and stroke patients with balance control. Many biochemical events also lend themselves to stochastic analysis. Gene expression, for example, has a stochastic component through the molecular collisions—as during binding and unbinding of RNA polymerase to a gene promoter—via the solution's Brownian motion.\n\nStochastic effect, or \"chance effect\" is one classification of radiation effects that refers to the random, statistical nature of the damage. In contrast to the deterministic effect, severity is independent of dose. Only the \"probability\" of an effect increases with dose.\n\nThe formation of river meanders has been analyzed as a stochastic process.\n\nSimonton (2003, \"Psych Bulletin\") argues that creativity in science (of scientists) is a constrained stochastic behaviour such that new theories in all sciences are, at least in part, the product of a stochastic process.\n\nStochastic ray tracing is the application of Monte Carlo simulation to the computer graphics ray tracing algorithm. \"Distributed ray tracing samples the integrand at many randomly chosen points and averages the results to obtain a better approximation. It is essentially an application of the Monte Carlo method to 3D computer graphics, and for this reason is also called \"Stochastic ray tracing\".\"\n\nStochastic forensics analyzes computer crime by viewing computers as stochastic processes.\n\nIn music, mathematical processes based on probability can generate stochastic elements.\n\nStochastic processes may be used in music to compose a fixed piece or may be produced in performance. Stochastic music was pioneered by Iannis Xenakis, who coined the term \"stochastic music\". Specific examples of mathematics, statistics, and physics applied to music composition are the use of the statistical mechanics of gases in \"Pithoprakta\", statistical distribution of points on a plane in \"Diamorphoses\", minimal constraints in \"Achorripsis\", the normal distribution in \"ST/10\" and \"Atrées\", Markov chains in \"Analogiques\", game theory in \"Duel\" and \"Stratégie\", group theory in \"Nomos Alpha\" (for Siegfried Palm), set theory in \"Herma\" and \"Eonta\", and Brownian motion in \"N'Shima\". Xenakis frequently used computers to produce his scores, such as the \"ST\" series including \"Morsima-Amorsima\" and \"Atrées\", and founded CEMAMu. Earlier, John Cage and others had composed \"aleatoric\" or indeterminate music, which is created by chance processes but does not have the strict mathematical basis (Cage's \"Music of Changes\", for example, uses a system of charts based on the \"I-Ching\"). Lejaren Hiller and Leonard Issacson used generative grammars and Markov chains in their 1957 \"Illiac Suite\". See: Generative music.\n\nWhen color reproductions are made, the image is separated into its component colors by taking multiple photographs filtered for each color. One resultant film or plate represents each of the cyan, magenta, yellow, and black data. Color printing is a binary system, where ink is either present or not present, so all color separations to be printed must be translated into dots at some stage of the work-flow. Traditional line screens which are amplitude modulated had problems with moiré but were used until stochastic screening became available. A stochastic (or frequency modulated) dot pattern creates a sharper image.\n\nNon-deterministic approaches in language studies are largely inspired by the work of Ferdinand de Saussure, for example, in functionalist linguistic theory, which argues that competence is based on performance. This distinction in functional theories of grammar should be carefully distinguished from the \"langue\" and \"parole\" distinction. To the extent that linguistic knowledge is constituted by experience with language, grammar is argued to be probabilistic and variable rather than fixed and absolute. This conception of grammar as probabilistic and variable follows from the idea that one's competence changes in accordance with one's experience with language. Though this conception has been contested, it has also provided the foundation for modern statistical natural language processing and for theories of language learning and change.\n\nStochastic social science theory is similar to systems theory in that events are interactions of systems, although with a marked emphasis on unconscious processes. The event creates its own conditions of possibility, rendering it unpredictable if simply for the number of variables involved. Stochastic social science theory can be seen as an elaboration of a kind of 'third axis' in which to situate human behavior alongside the traditional 'nature vs. nurture' opposition. See Julia Kristeva on her usage of the 'semiotic', Luce Irigaray on reverse Heideggerian epistemology, and Pierre Bourdieu on polythetic space for examples of stochastic social science theory.\n\nManufacturing processes are assumed to be stochastic processes. This assumption is largely valid for either continuous or batch manufacturing processes. Testing and monitoring of the process is recorded using a process control chart which plots a given process control parameter over time. Typically a dozen or many more parameters will be tracked simultaneously. Statistical models are used to define limit lines which define when corrective actions must be taken to bring the process back to its intended operational window.\n\nThis same approach is used in the service industry where parameters are replaced by processes related to service level agreements.\n\nThe financial markets use stochastic models to represent the seemingly random behaviour of assets such as stocks, commodities, relative currency prices (i.e., the price of one currency compared to that of another, such as the price of US Dollar compared to that of the Euro), and interest rates. These models are then used by quantitative analysts to value options on stock prices, bond prices, and on interest rates, see Markov models. Moreover, it is at the heart of the insurance industry.\n\nThe marketing and the changing movement of audience tastes and preferences, as well as the solicitation of and the scientific appeal of certain film and television debuts (i.e., their opening weekends, word-of-mouth, top-of-mind knowledge among surveyed groups, star name recognition and other elements of social media outreach and advertising), are determined in part by stochastic modeling. A recent attempt at repeat business analysis was done by Japanese scholars and is part of the Cinematic Contagion Systems patented by Geneva Media Holdings, and such modeling has been used in data collection from the time of the original Nielsen ratings to modern studio and television test audiences.\n\n\n"}
{"id": "3224213", "url": "https://en.wikipedia.org/wiki?curid=3224213", "title": "Symmetrically continuous function", "text": "Symmetrically continuous function\n\nIn mathematics, a function formula_1 is symmetrically continuous at a point \"x\n\nThe usual definition of continuity implies symmetric continuity, but the converse is not true. For example, the function formula_3 is symmetrically continuous at formula_4, but not continuous.\n\nAlso, symmetric differentiability implies symmetric continuity, but the converse is not true just like usual continuity does not imply differentiability.\n"}
{"id": "27420015", "url": "https://en.wikipedia.org/wiki?curid=27420015", "title": "System of polynomial equations", "text": "System of polynomial equations\n\nA system of polynomial equations (sometimes simply a polynomial system) is a set of simultaneous equations where the are polynomials in several variables, say , over some field . \n\nA \"solution\" of a polynomial system is a set of values for the s which belong to some algebraically closed field extension of , and make all equations true. When is the field of rational numbers, is generally assumed to be the field of complex numbers, because each solution belongs to a field extension of , which is isomorphic to a subfield of the complex numbers.\n\nThis article is about the methods for solving, that is, finding all solutions or describing them. As these methods are designed for being implemented in a computer, emphasis is given on fields in which computation (including equality testing) is easy and efficient, that is the field of rational numbers and finite fields.\n\nSearching for solutions that belong to a specific set is a problem which is generally much more difficult, and is outside the scope of this article, except for the case of the solutions in a given finite field. For the case of solutions whose all components are integers or rational numbers, see Diophantine equation.\n\nA trigonometric equation is an equation where is a trigonometric polynomial. Such an equation may be converted into a polynomial system by expanding the sines and cosines in it, replacing and by two new variables and and adding the new equation .\n\nFor example, the equation\n\nis equivalent to the polynomial system\n\nWhen solving a system over a finite field with elements, one is primarily interested in the solutions in . As the elements of are exactly the solutions of the equation , it suffices, for restricting the solutions to , to add the equation for each variable .\n\nThe elements of a number field are usually represented as polynomials in a generator of the field which satisfies some univariate polynomial equation. To work with a polynomial system whose coefficients belong to a number field, it suffices to consider this generator as a new variable and to add the equation of the generator to the equations of the system. Thus solving a polynomial system over a number field is reduced to solving another system over the rational numbers.\n\nFor example, if a system contains formula_3, a system over the rational numbers is obtained by adding the equation and replacing formula_3 by in the other equations.\n\nIn the case of a finite field, the same transformation allows always to suppose that the field has a prime order.\n\nA system is overdetermined if the number of equations is higher than the number of variables. A system is inconsistent if it has no solutions. By Hilbert's Nullstellensatz this means that 1 is a linear combination (with polynomials as coefficients) of the first members of the equations. Most but not all overdetermined systems, when constructed with random coefficients, are inconsistent. For example, the system  \"x\" − 1 = 0, \"x\" − 1 = 0 is overdetermined (having two equations but only one unknown), but it is not inconsistent since it has the solution \"x\" =1.\n\nA system is underdetermined if the number of equations is lower than the number of the variables. An underdetermined system is either inconsistent or has infinitely many solutions in an algebraically closed extension \"K\" of \"k\".\n\nA system is zero-dimensional if it has a finite number of solutions in an algebraically closed extension \"K\" of \"k\". This terminology comes from the fact that the algebraic variety of the solutions has dimension zero. A system with infinitely many solutions is said to be \"positive-dimensional\".\n\nA zero-dimensional system with as many equations as variables is said to be \"well-behaved\".\nBézout's theorem asserts that a well-behaved system whose equations have degrees \"d\", ..., \"d\" has at most \"d\"...\"d\" solutions. This bound is sharp. If all the degrees are equal to \"d\", this bound becomes \"d\" and is exponential in the number of variables.\n\nThis exponential behavior makes solving polynomial systems difficult and explains why there are few solvers that are able to automatically solve systems with Bézout's bound higher than, say, 25 (three equations of degree 3 or five equations of degree 2 are beyond this bound).\n\nThe first thing to do in solving a polynomial system is to decide if it is inconsistent, zero-dimensional or positive dimensional. This may be done by the computation of a Gröbner basis of the left-hand sides of the equations. The system is \"inconsistent\" if this Gröbner basis is reduced to 1. The system is \"zero-dimensional\" if, for every variable there is a leading monomial of some element of the Gröbner basis which is a pure power of this variable. For this test, the best monomial order is usually the graded reverse lexicographic one (grevlex).\n\nIf the system is \"positive-dimensional\", it has infinitely many solutions. It is thus not possible to enumerate them. It follows that, in this case, solving may only mean \"finding a description of the solutions from which the relevant properties of the solutions are easy to extract\". There is no commonly accepted such description. In fact there are many different \"relevant properties\", which involve almost every subfield of algebraic geometry.\n\nA natural example of such a question positive-dimensional systems is the following: \"decide if a polynomial system over the rational numbers has a finite number of real solutions and compute them\". A generalization of this question is \"find at least one solution in each connected component of the set of real solutions of a polynomial system\". The classical algorithm for solving these question is cylindrical algebraic decomposition, which has a doubly exponential computational complexity and therefore cannot be used in practice, except for very small examples.\n\nFor zero-dimensional systems, solving consists of computing all the solutions. There are two different ways of outputting the solutions. The most common, possible for real or complex solutions, consists of outputting numeric approximations of the solutions. Such a solution is called \"numeric\". A solution is \"certified\" if it is provided with a bound on the error of the approximations which separates the different solutions.\n\nThe other way to represent the solutions is said to be \"algebraic\". It uses the fact that, for a zero-dimensional system, the solutions belong to the algebraic closure of the field \"k\" of the coefficients of the system. There are several ways to represent the solution in an algebraic closure, which are discussed below. All of them allow one to compute a numerical approximation of the solutions by solving one or several univariate equations. For this computation, the representation involving the solving of only one univariate polynomial for each solution is preferable: computing the roots of a polynomial which has approximate coefficients is a highly unstable problem.\n\nThe usual way of representing the solutions is through zero-dimensional regular chains. Such a chain consists of a sequence of polynomials , , ..., such that, for every such that \n\nTo such a regular chain is associated a \"triangular system of equations\"\n\nThe solutions of this system are obtained by solving the first univariate equation, substituting the solutions in the other equations, then solving the second equation which is now univariate, and so on. The definition of regular chains implies that the univariate equation obtained from has degree and thus that the system has solutions, provided that there is no multiple root in this resolution process (fundamental theorem of algebra).\n\nEvery zero-dimensional system of polynomial equations is equivalent (i.e. has the same solutions) to a finite number of regular chains. Several regular chains may be needed, as it is the case for the following system which has three solutions.\n\nThere are several algorithms for computing a triangular decomposition of an arbitrary polynomial system (not necessarily zero-dimensional) into regular chains (or regular semi-algebraic systems).\n\nThere is also an algorithm which is specific to the zero-dimensional case and is competitive, in this case, with the direct algorithms. It consists in computing first the Gröbner basis for the graded reverse lexicographic order (grevlex), then deducing the lexicographical Gröbner basis by FGLM algorithm and finally applying the Lextriangular algorithm.\n\nThis representation of the solutions are fully convenient for coefficients in a finite field. However, for rational coefficients, two aspects have to be taken care of:\n\nThe first issue has been solved by Dahan and Schost: Among the sets of regular chains that represent a given set of solutions, there is a set for which the coefficients are explicitly bounded in terms of the size of the input system, with a nearly optimal bound. This set, called \"equiprojectable decomposition\", depends only on the choice of the coordinates. This allows the use of modular methods for computing efficiently the equiprojectable decomposition.\n\nThe second issue is generally solved by outputting regular chains of a special form, sometimes called \"shape lemma\", for which all but the first one are equal to . For getting such regular chains, one may have to add a further variable, called \"separating variable\", which is given the index . The \"rational univariate representation\", described below, allows computing such a special regular chain, satisfying Dahan–Schost bound, by starting from either a regular chain or a Gröbner basis.\n\nThe \"rational univariate representation\" or RUR is a representation of the solutions of a zero-dimensional polynomial system over the rational numbers which has been introduced by F. Rouillier.\n\nA RUR of a zero-dimensional system consists in a linear combination of the variables, called \"separating variable\", and a system of equations\nwhere is a univariate polynomial in of degree and are univariate polynomials in of degree less than .\n\nGiven a zero-dimensional polynomial system over the rational numbers, the RUR has the following properties.\n\n\nFor example, for the system in the previous section, every linear combination of the variable, except the multiples of , and , is a separating variable. If one chooses as a separating variable, then the RUR is\n\nThe RUR is uniquely defined for a given separating variable, independently of any algorithm, and it preserves the multiplicities of the roots. This is a notable difference with triangular decompositions (even the equiprojectable decomposition), which, in general, do not preserve multiplicities. The RUR shares with equiprojectable decomposition the property of producing an output with coefficients of relatively small size.\n\nFor zero-dimensional systems, the RUR allows retrieval of the numeric values of the solutions by solving a single univariate polynomial and substituting them in rational functions. This allows production of certified approximations of the solutions to any given precision.\n\nMoreover, the univariate polynomial of the RUR may be factorized, and this gives a RUR for every irreducible factor. This provides the \"prime decomposition\" of the given ideal (that is the primary decomposition of the radical of the ideal). In practice, this provides an output with much smaller coefficients, especially in the case of systems with high multiplicities.\n\nContrarily to triangular decompositions and equiprojectable decompositions, the RUR is not defined in positive dimension.\n\nThe general numerical algorithms which are designed for any system of nonlinear equations work also for polynomial systems. However the specific methods will generally be preferred, as the general methods generally do not allow one to find \"all\" solutions. In particular, when a general method does not find any solution, this is usually not an indication that there is no solution.\n\nNevertheless, two methods deserve to be mentioned here.\n\n\nThis is a semi-numeric method which supposes that the number of equations is equal to the number of variables. This method is relatively old but it has been dramatically improved in the last decades.\n\nThis method divides into three steps. First an upper bound on the number of solutions is computed. This bound has to be as sharp as possible. Therefore, it is computed by, at least, four different methods and the best value, say formula_9, is kept.\n\nIn the second step, a system formula_10 of polynomial equations is generated which has exactly formula_9 solutions that are easy to compute. This new system has the same number formula_12 of variables and the same number formula_12 of equations and the same general structure as the system to solve, formula_14.\n\nThen a homotopy between the two systems is considered. It consists, for example, of the straight line between the two systems, but other paths may be considered, in particular to avoid some singularities, in the system\n\nThe homotopy continuation consists in deforming the parameter formula_16 from 0 to 1 and \"following\" the formula_9 solutions during this deformation. This gives the desired solutions for formula_18. \"Following\" means that, if formula_19, the solutions for formula_20 are deduced from the solutions for formula_21 by Newton's method. The difficulty here is to well choose the value of formula_22 Too large, Newton's convergence may be slow and may even jump from a solution path to another one. Too small, and the number of steps slows down the method.\n\nTo deduce the numeric values of the solutions from a RUR seems easy: it suffices to compute the roots of the univariate polynomial and to substitute them in the other equations. This is not so easy because the evaluation of a polynomial at the roots of another polynomial is highly unstable.\n\nThe roots of the univariate polynomial have thus to be computed at a high precision which may not be defined once for all. There are two algorithms which fulfill this requirement.\n\n\nThere are at least four software packages which can solve zero-dimensional systems automatically (by automatically, one means that no human intervention is needed between input and output, and thus that no knowledge of the method by the user is needed). There are also several other software packages which may be useful for solving zero-dimensional systems. Some of them are listed after the automatic solvers.\n\nThe Maple function \"RootFinding[Isolate]\" takes as input any polynomial system over the rational numbers (if some coefficients are floating point numbers, they are converted to rational numbers) and outputs the real solutions represented either (optionally) as intervals of rational numbers or as floating point approximations of arbitrary precision. If the system is not zero dimensional, this is signaled as an error.\n\nInternally, this solver, designed by F. Rouillier computes first a Gröbner basis and then a Rational Univariate Representation from which the required approximation of the solutions are deduced. It works routinely for systems having up to a few hundred complex solutions.\n\nThe rational univariate representation may be computed with Maple function \"Groebner[RationalUnivariateRepresentation]\".\n\nTo extract all the complex solutions from a rational univariate representation, one may use MPSolve, which computes the complex roots of univariate polynomials to any precision. It is recommended to run MPSolve several times, doubling the precision each time, until solutions remain stable, as the substitution of the roots in the equations of the input variables can be highly unstable.\n\nThe second solver is PHCpack, written under the direction of J. Verschelde. PHCpack implements the homotopy continuation method. This solver computes the isolated complex solutions of polynomial systems having as many equations as variables.\n\nThe third solver is Bertini, written by D. J. Bates, J. D. Hauenstein, A. J. Sommese, and C. W. Wampler. Bertini uses numerical homotopy continuation with adaptive precision. In addition to computing zero-dimensional solution sets, both PHCpack and Bertini are capable of working with positive dimensional solution sets.\n\nThe fourth solver is the Maple command \"RegularChains[RealTriangularize]\". For any zero-dimensional input system with rational number coefficients it returns those solutions whose coordinates are real algebraic numbers. Each of these real numbers is encoded by an isolation interval and a defining polynomial.\n\nThe command \"RegularChains[RealTriangularize]\" is part of the Maple library RegularChains, written by Marc Moreno-Maza, his students and post-doctoral fellows (listed in chronological order of graduation) Francois Lemaire, Yuzhen Xie, Xin Li, Xiao Rong, Liyun Li, Wei Pan and Changbo Chen. Other contributors are Eric Schost, Bican Xia and Wenyuan Wu. This library provides a large set of functionalities for solving zero-dimensional and positive dimensional systems. In both cases, for input systems with rational number coefficients, routines for isolating the real solutions are available. For arbitrary input system of polynomial equations and inequations (with rational number coefficients or with coefficients in a prime field) one can use the command \"RegularChains[Triangularize]\" for computing the solutions whose coordinates are in the algebraic closure of the coefficient field. The underlying algorithms are based on the notion of a regular chain.\n\nWhile the command RegularChains[RealTriangularize] is currently limited to zero-dimensional systems, a future release will be able to process any system of polynomial equations, inequations and inequalities. The corresponding new algorithm is based on the concept of a regular semi-algebraic system.\n\n\n"}
{"id": "32905373", "url": "https://en.wikipedia.org/wiki?curid=32905373", "title": "Szegő limit theorems", "text": "Szegő limit theorems\n\nIn mathematical analysis, the Szegő limit theorems describe the asymptotic behaviour of the determinants of large Toeplitz matrices. They were first proved by Gábor Szegő.\n\nLet \"φ\" : T→C be a complex function (\"symbol\") on the unit circle. Consider the \"n\"×\"n\" Toeplitz matrices \"T\"(\"φ\"), defined by\n\nwhere\n\nare the Fourier coefficients of \"φ\".\n\nThe first Szegő theorem states that, if \"φ\" > 0 and \"φ\" ∈ \"L\"(T), then\n\nThe right-hand side of () is the geometric mean of \"φ\" (well-defined by the arithmetic-geometric mean inequality).\n\nDenote the right-hand side of () by \"G\". The second (or strong) Szegő theorem asserts that if, in addition, the derivative of \"φ\" is Hölder continuous of order \"α\" > 0, then\n"}
{"id": "4952620", "url": "https://en.wikipedia.org/wiki?curid=4952620", "title": "Tarski's axiomatization of the reals", "text": "Tarski's axiomatization of the reals\n\nIn 1936, Alfred Tarski set out an axiomatization of the real numbers and their arithmetic, consisting of only the 8 axioms shown below and a mere four primitive notions: the set of reals denoted R, a binary total order over R, denoted by infix <, a binary operation of addition over R, denoted by infix +, and the constant 1.\n\nThe literature occasionally mentions this axiomatization but never goes into detail, notwithstanding its economy and elegant metamathematical properties. This axiomatization appears little known, possibly because of its second-order nature. Tarski's axiomatization can be seen as a version of the more usual definition of real numbers as the unique Dedekind-complete ordered field; it is however made much more concise by using unorthodox variants of standard algebraic axioms and other subtle tricks (see e.g. axioms 4 and 5, which combine together the usual four axioms of abelian groups).\n\nThe term \"Tarski's axiomatization of real numbers\" also refers to the theory of real closed fields, which Tarski showed completely axiomatizes the first-order theory of the structure 〈R, +, ·, <〉.\n\n\"Axioms of order\" (primitives: R, <):\n\n\n\n\nTo clarify the above statement somewhat, let \"X\" ⊆ R and \"Y\" ⊆ R. We now define two common English verbs in a particular way that suits our purpose:\n\nAxiom 3 can then be stated as:\n\n\"Axioms of addition\" (primitives: R, <, +):\n\n\n\n\n\"Axioms for one\" (primitives: R, <, +, 1):\n\n\n\nThese axioms imply that R is a linearly ordered abelian group under addition with distinguished element 1. R is also Dedekind-complete and divisible.\n\nTarski stated, without proof, that these axioms gave a total ordering. The missing component was supplied in 2008 by Stefanie Ucsnay.\n\nThis axiomatization does not give rise to a first-order theory, because the formal statement of axiom 3 includes two universal quantifiers over all possible subsets of R. Tarski proved these 8 axioms and 4 primitive notions independent.\n\nTarski sketched the (nontrivial) proof of how these axioms and primitives imply the existence of a binary operation called multiplication and having the expected properties, so that R is a complete ordered field under addition and multiplication. This proof builds crucially on the integers with addition being an abelian group and has its origins in Eudoxus' definition of magnitude.\n"}
{"id": "3884961", "url": "https://en.wikipedia.org/wiki?curid=3884961", "title": "The Value of Science", "text": "The Value of Science\n\nThe Value of Science () is a book by the French mathematician, physicist, and philosopher Henri Poincaré. It was published in 1905. The book deals with questions in the philosophy of science and adds detail to the topics addressed by Poincaré's previous book, \"Science and Hypothesis\" (1902).\n\nThe first part of the book deals exclusively with the mathematical sciences, and particularly, the relationship between intuition and logic in mathematics. It first examines which parts of science correspond to each of these two categories of scientific thought, and outlines a few principles:\n\nThis \"historic\" intuition is therefore mathematical intuition. For Poincaré, it is a result of the principle of least effort, that is, of a link to scientific convention based on experimentation. Convention, thus given a context, permits one to consider different theories of the same problem, and subsequently make a choice based on the degree of simplicity and usefulness of explanations advanced by each of these theories (see also Occam's razor). The example chosen by Poincaré is that of three-dimensional space. He shows how the representation of this space is only one possibility, chosen for its usefulness among many models that the mind could create. His demonstration rests on the theory of \"The Mathematical Continuum\" (1893), one of Poincaré's earlier publications.\n\nFinally, Poincaré advances the idea of a fundamental relationship between the sciences of \"geometry\" and \"analysis\". According to him, intuition has two major roles: to permit one to choose which route to follow in search of scientific truth, and to allow one to comprehend logical developments: Moreover, this relation seems to him inseparable from scientific advancement, which he presents as an enlargement of the framework of science – new theories incorporating previous ones, even while breaking old patterns of thought.\n\nIn the second part of his book, Poincaré studies the links between physics and mathematics. His approach, at once historical and technical, illustrates the preceding general ideas.\n\nEven though he was rarely an experimenter, Poincaré recognizes and defends the importance of experimentation, which must remain a pillar of the scientific method. According to him, it is not necessary that mathematics incorporate physics into itself, but must develop as an asset unto itself. This asset would be above all a tool: in the words of Poincaré, mathematics is \"the only language in which [physicists] could speak\" to understand each other and to make themselves heard. This language of numbers seems elsewhere to reveal a unity hidden in the natural world, when there may well be only one part of mathematics that applies to theoretical physics. The primary objective of mathematical physics is not invention or discovery, but reformulation. It is an activity of synthesis, which permits one to assure the coherence of theories current at a given time. Poincaré recognized that it is impossible to systematize all of physics of a specific time period into one axiomatic theory. His ideas of a three dimensional space are given significance in this context.\n\nPoincaré states that mathematics (analysis) and physics are in the same spirit, that the two disciplines share a common aesthetic goal and that both can liberate humanity from its simple state. In a more pragmatic way, the interdependence of physics and mathematics is similar to his proposed relationship between intuition and analysis. The language of mathematics not only permits one to express scientific advancements, but also to take a step back to comprehend the broader world of nature. Mathematics demonstrates the extent of the specific and limited discoveries made by physicists. On the other hand, physics has a key role for the mathematician - a creative role since it presents atypical problems ingrained in reality. In addition, physics offers solutions and reasoning - thus the development of infinitesimal calculus by Isaac Newton within the framework of Newtonian mechanics.\n\nMathematical physics finds its scientific origins in the study of celestial mechanics. Initially, it was a consolidation of several fields of physics that dominated the 18th century and which had allowed advancements in both the theoretical and experimental fields. However, in conjunction with the development of thermodynamics (at the time disputed), physicists began developing an energy-based physics. In both its mathematics and its fundamental ideas, this new physics seemed to contradict the Newtonian concept of particle interactions. Poincaré terms this the \"first crisis of mathematical physics\".\n\nThroughout the 19th century, important discoveries were being made in laboratories and elsewhere. Many of these discoveries gave substance to important theories. Other discoveries could not be explained satisfactorily - either they had only been occasionally observed, or they were inconsistent with the new and emerging theories.\n\nAt the beginning of the 20th century, the unifying principles were thrown into question. Poincaré explains some of the most important principles and their difficulties:\n\nAt the beginning of the twentieth century, the majority of scientists spoke of Poincaré's \"diagnosis\" concerning the crisis of the \"physical principles\". In fact, it was difficult to do otherwise: they had discovered experimental facts which the principles could not account for, and which they evidently could not ignore. Poincaré himself remained relatively optimistic regarding the evolution of physics with respect to these severe experimental difficulties. He had little confidence in the nature of principles: they were constructed by physicists because they accommodate and take into account a large number of laws. Their objective value consists in forming a scientific convention, in other words in providing a firm foundation to the basis on which truth and falsehood (in the scientific meaning of the words) are separated.\n\nBut if these principles are conventions, they are not therefore totally dissociated from experimental fact. On the contrary, if the principles can no longer sustain laws adequately, in accordance with experimental observation, they lose their utility and are rejected, without even having been contradicted. The failure of the laws entails the failure of the principles, because they must account for the results of experiment. To abolish these principles, products of the scientific thought of several centuries, without finding a new explanation that encompasses them (in the same manner that the \"Physics of principles\" encompasses the \"Physics of central forces\"), is to claim that all of past physics has no intellectual value. Consequently, Poincaré had great confidence that the principles were salvageable. He said that it was the responsibility of mathematical physics to reconstitute those principles, or to find a replacement for them (the greater goal being to return the field to unity), given that it had played the main role in questioning them only after consolidating them to begin with. Moreover, it was the value of mathematical physics (in terms of the scientific method) which itself saw criticism, due to the implosion of certain theories. Two physics thus existed at the same time: the physics of Galileo and Newton, and the physics of Maxwell; but neither one was able to explain all the experimental observations that technical advances had produced.\n\nThe array of problems encountered concentrated on the electrodynamics of moving bodies. Poincaré swiftly proposed the idea that it is the ether modifying itself, and not the bodies acquiring mass, which came to contradict the older theories (based on a perfectly immovable ether). Overall, Poincaré shed light on the Zeeman effect, caused by discontinuous emissions of electrons. The problem of discontinuous matter forced the formulation of a minimally-destabilizing model of the atom. In 1913, Niels Bohr presented his atomic model which was based on the concept of electron orbits, and which explained spectroscopy as well as the stability of the atom. But, in 1905, the problem with all attempts to define the behavior of the microscopic world was that no one then knew if they needed to consider a similar model to the one known for the macroscopic objects (the model of classical mechanics), or if they should try to develop an entirely new model to give account of new facts. The latter idea, which was followed with the quantum theory, also implied definitively abandoning the unity already found in prior theories of mechanics.\n\nPoincaré argued that the advancement of the physical sciences would have to consider a new kind of determinism, giving a new place to chance. And in effect, the history of twentieth century physics is marked by a paradigm where probability reigns. In The Value of Science, Poincaré writes and repeats his enthusiasm for two lines of research : statistical laws (taking the place of differential laws), and relativistic mechanics (taking the place of Newtonian mechanics). Nevertheless, he did not take into account the ideas of Planck. This latter had in 1900 published the spectral laws governing blackbody radiation, which were the foundation of quantum mechanics. In 1905, the same year as the publication of The Value of Science, Albert Einstein published a decisive article on the photoelectric effect, which he based on the work of Planck. Despite the doubts of Poincaré, which were no doubt related to his vision of physics as an approximation of reality (in contrast to the exactness of mathematics), the probabilistic rules of quantum mechanics were clearly the response to the second crisis of mathematical physics, at the end of the nineteenth century. (One can point out that in 1902, Poincaré envisaged a relativistic physics which closely matched, in its theoretical development, the one developed and propounded by Einstein several years later.)\n\n\"What is the purpose of science?\" is the question repeatedly asked in Poincaré's book. To this teleological problem, Poincaré responds by taking the opposite position from that of Édouard Le Roy, philosopher and mathematician, who argued in a 1905 article (\"Sur la logique de l'invention\", \"On the logic of invention\") that science is intrinsically anti-intellectual (in the sense of Henri Bergson) and nominalistic. In contrast to Le Roy, Poincaré follows the thought of Pierre Duhem. He explains that the notion that science is anti-intellectual is self-contradictory, and that the accusation of nominalism can be strongly criticized, because it rests on confusions of thoughts and definitions. He defends the idea of \"conventional principles\", and the idea that scientific activity is not merely a set of conventions arranged arbitrarily around the raw observations of experiment. He wishes rather to demonstrate that objectivity in science comes precisely from the fact that the scientist does no more than translate raw facts into a particular language: \"(...) tout ce que crée le savant dans un fait, c'est le langage dans lequel il l'énonce\". The only contribution of science would be the development of a more and more mathematized language, a coherent language because it offers predictions which are useful – but not certain, as they remain forever subject to comparisons with real observations, and are always fallible.\n\n\n"}
{"id": "56550279", "url": "https://en.wikipedia.org/wiki?curid=56550279", "title": "Valérie Berthé", "text": "Valérie Berthé\n\nValérie Berthé (born 16 December 1968) is a French mathematician who works as a director of research for the Centre national de la recherche scientifique (CNRS) at the Institut de Recherche en Informatique Fondamentale (IRIF), a joint project between CNRS and Paris Diderot University. Her research involves symbolic dynamics, combinatorics on words, discrete geometry, numeral systems, tessellations, and fractals.\n\nBerthé completed her baccalauréat at age 16, and studied at the École Normale Supérieure from 1988 to 1993. She earned a licentiate and master's degree in pure mathematics from Pierre and Marie Curie University in 1989, a Diplôme d'études approfondies from University of Paris-Sud in 1991, completed her agrégation in 1992, and was recruited by CNRS in 1993. Continuing her graduate studies, she defended a doctoral thesis in 1994 at the University of Bordeaux 1. Her dissertation, \"Fonctions de Carlitz et automates: Entropies conditionnelles\" was supervised by Jean-Paul Allouche. She completed a habilitation in 1999, again under the supervision of Allouche, at the University of the Mediterranean Aix-Marseille II; her habilitation thesis was \"Étude arithmétique et dynamique de suites algorithmiques\".\n\nBerthé is a vice-president of the Société mathématique de France (SMF), and director of publications for the SMF.\nShe has played an active role in L'association femmes et mathématiques.\nBerthé has also been associated with the M. Lothaire pseudonymous mathematical collaboration on combinatorics on words\nand the Pythias Fogg pseudonymous collaboration on substitution systems.\n\nIn 2013, Berthé was elevated to the Legion of Honour.\n"}
{"id": "3603745", "url": "https://en.wikipedia.org/wiki?curid=3603745", "title": "Vertex configuration", "text": "Vertex configuration\n\nIn geometry, a vertex configuration is a shorthand notation for representing the vertex figure of a polyhedron or tiling as the sequence of faces around a vertex. For uniform polyhedra there is only one vertex type and therefore the vertex configuration fully defines the polyhedron. (Chiral polyhedra exist in mirror-image pairs with the same vertex configuration.)\n\nA vertex configuration is given as a sequence of numbers representing the number of sides of the faces going around the vertex. The notation \"a.b.c\" describes a vertex that has 3 faces around it, faces with \"a\", \"b\", and \"c\" sides.\n\nFor example, \"3.5.3.5\" indicates a vertex belonging to 4 faces, alternating triangles and pentagons. This vertex configuration defines the vertex-transitive icosidodecahedron. The notation is cyclic and therefore is equivalent with different starting points, so 3.5.3.5 is the same as 5.3.5.3. The order is important, so 3.3.5.5 is different from 3.5.3.5. (The first has two triangles followed by two pentagons.) Repeated elements can be collected as exponents so this example is also represented as (3.5).\n\nIt has variously been called a vertex description, vertex type, vertex symbol, vertex arrangement, vertex pattern, face-vector. It is also called a Cundy and Rollett symbol for its usage for the Archimedean solids in their 1952 book \"Mathematical Models\".\n\nA \"vertex configuration\" can also be represented as a polygonal vertex figure showing the faces around the vertex. This \"vertex figure\" has a 3-dimensional structure since the faces are not in the same plane for polyhedra, but for vertex-uniform polyhedra all the neighboring vertices are in the same plane and so this plane projection can be used to visually represent the vertex configuration.\n\nDifferent notations are used, sometimes with a comma (,) and sometimes a period (.) separator. The period operator is useful because it looks like a product and an exponent notation can be used. For example, 3.5.3.5 is sometimes written as (3.5).\n\nThe notation can also be considered an expansive form of the simple Schläfli symbol for regular polyhedra. The Schläfli notation {\"p\",\"q\"} means \"q\" \"p\"-gons around each vertex. So {\"p\",\"q\"} can be written as \"p.p.p...\" (\"q\" times) or \"p\". For example, an icosahedron is {3,5} = 3.3.3.3.3 or 3.\n\nThis notation applies to polygonal tilings as well as polyhedra. A planar vertex configuration denotes a uniform tiling just like a nonplanar vertex configuration denotes a uniform polyhedron.\n\nThe notation is ambiguous for chiral forms. For example, the snub cube has clockwise and counterclockwise forms which are identical across mirror images. Both have a 3.3.3.3.4 vertex configuration.\n\nThe notation also applies for nonconvex regular faces, the star polygons. For example, a pentagram has the symbol {5/2}, meaning it has 5 sides going around the centre twice.\n\nFor example, there are 4 regular star polyhedra with regular polygon or star polygon vertex figures. The small stellated dodecahedron has the Schläfli symbol of {5/2,5} which expands to an explicit vertex configuration 5/2.5/2.5/2.5/2.5/2 or combined as (5/2). The great stellated dodecahedron, {5/2,3} has a triangular vertex figure and configuration (5/2.5/2.5/2) or (5/2). The great dodecahedron, {5,5/2} has a pentagrammic vertex figure, with \"vertex configuration\" is (5.5.5.5.5)/2 or (5)/2. A great icosahedron, {3,5/2} also has a pentagrammic vertex figure, with vertex configuration (3.3.3.3.3)/2 or (3)/2.\nFaces on a vertex figure are considered to progress in one direction. Some uniform polyhedra have vertex figures with inversions where the faces progress retrograde. A vertex figure represents this in the star polygon notation of sides \"p/q\" such that \"p\"<2\"q\", where \"p\" is the number of sides and \"q\" the number of turns around a circle. For example, \"3/2\" means a triangle that has vertices that go around twice, which is the same as backwards once. Similarly \"5/3\" is a backwards pentagram 5/2.\n\nSemiregular polyhedra have vertex configurations with positive angle defect.\n\nNOTE: The vertex figure can represent a regular or semiregular tiling on the plane if its defect is zero. It can represent a tiling of the hyperbolic plane if its defect is negative.\n\nFor uniform polyhedra, the angle defect can be used to compute the number of vertices. Descartes' theorem states that all the angle defects in a topological sphere must sum to 4\"π\" radians or\n720 degrees.\n\nSince uniform polyhedra have all identical vertices, this relation allows us to compute the number of vertices, which is 4\"π\"/\"defect\" or\n720/\"defect\".\n\nExample: A truncated cube 3.8.8 has an angle defect of 30 degrees. Therefore, it has\n\nIn particular it follows that {\"a\",\"b\"} has vertices.\n\nEvery enumerated vertex configuration potentially uniquely defines a semiregular polyhedron. However, not all configurations are possible.\n\nTopological requirements limit existence. Specifically \"p.q.r\" implies that a \"p\"-gon is surrounded by alternating \"q\"-gons and \"r\"-gons, so either \"p\" is even or \"q\" equals \"r\". Similarly \"q\" is even or \"p\" equals \"r\", and \"r\" is even or \"p\" equals \"q\". Therefore, potentially possible triples are 3.3.3, 3.4.4, 3.6.6, 3.8.8, 3.10.10, 3.12.12, 4.4.\"n\" (for any \"n\">2), 4.6.6, 4.6.8, 4.6.10, 4.6.12, 4.8.8, 5.5.5, 5.6.6, 6.6.6. In fact, all these configurations with three faces meeting at each vertex turn out to exist.\n\nThe number in parentheses is the number of vertices, determined by the angle defect.\n\n\n\n\n\nThe uniform dual or Catalan solids, including the bipyramids and trapezohedra, are \"vertically-regular\" (face-transitive) and so they can be identified by a similar notation which is sometimes called face configuration. Cundy and Rollett prefixed these dual symbols by a \"V\". In contrast, \"Tilings and Patterns\" uses square brackets around the symbol for isohedral tilings.\n\nThis notation represents a sequential count of the number of faces that exist at each vertex around a face. For example, V3.4.3.4 or V(3.4) represents the rhombic dodecahedron which is face-transitive: every face is a rhombus, and alternating vertices of the rhombus contain 3 or 4 faces each.\n\nSpherical\n\n\nRegular\n\n\nSemi-regular\n\n\nHyperbolic\n\n\n\n\n\n\n\n"}
{"id": "45083486", "url": "https://en.wikipedia.org/wiki?curid=45083486", "title": "William Crawley-Boevey", "text": "William Crawley-Boevey\n\nWilliam Walstan Crawley-Boevey (born 1960) is an English mathematician who is Professor of Pure Mathematics at the University of Leeds. His research concerns representation theory and the theory of quivers.\n\nCrawley-Boevey is the second son of Sir Thomas Crawley-Boevey, 8th Baronet. He studied at the City of London School and read mathematics at St John's College, Cambridge. He received his PhD in 1986 from the University of Cambridge under the supervision of Stephen Donkin. Before his appointment in Leeds, he held post-doctoral positions at the University of Liverpool and the University of Oxford.\n\nHe was the 1991 winner of the Berwick Prize of the London Mathematical Society. In 2006, Crawley-Boevey presented an invited talk at the International Congress of Mathematicians. In 2012, he became one of the inaugural fellows of the American Mathematical Society.\n\n\n"}
{"id": "4388066", "url": "https://en.wikipedia.org/wiki?curid=4388066", "title": "Wu Wenjun", "text": "Wu Wenjun\n\nWu Wenjun (; 12 May 1919 – 7 May 2017), also commonly known as Wu Wen-tsün, was a Chinese mathematician and academician at the Chinese Academy of Sciences (CAS), best known for the Wu's method of characteristic set.\n\nWu's ancestral hometown was Jiashan, Zhejiang. He was born in Shanghai and graduated from National Chiao Tung University in 1940. In 1945, Wu taught several months at Hangchow University (later merged into Zhejiang University) in Hangzhou.\n\nIn 1947, he went to France for further study at the University of Strasbourg. In 1949, he received his PhD, for his thesis \"Sur les classes caractéristiques des structures fibrées sphériques\", written under the direction of Charles Ehresmann. Afterwards, he did some work in Paris with René Thom and discovered the Wu class and Wu formula in algebraic topology. In 1951 he was appointed to a post at Peking University. However, Wu may have been among a wave of recalls of Chinese academics working in the West following Chiang Kai-shek's ouster from the mainland in 1949, according to eyewitness testimony by Marcel Berger, as he disappeared from France one day, without saying a word to anyone.\n\nIn 1957, he was elected as an academician of the Chinese Academy of Sciences. In 1986 he was an Invited Speaker of the ICM in Berkeley. In 1990, he was elected as an academician of The World Academy of Sciences (TWAS).\n\nAlong with Yuan Longping, he was awarded the State Preeminent Science and Technology Award by President Jiang Zemin in 2000, when this highest scientific and technological prize in China began to be awarded. He also received the TWAS Prize in 1990 and the Shaw Prize in 2006. He was the President of the Chinese society of mathematics. He died on May 7, 2017, 5 days before his 98th birthday.\n\nThe research of Wu includes the following fields: algebraic topology, algebraic geometry, game theory, history of mathematics, automated theorem proving. His most important contributions are to algebraic topology. The Wu class and the Wu formula are named after him. In the field of automated theorem proving, he is known for Wu's method.\n\nHe was also active in the field of the history of Chinese mathematics. He was the chief editor\nof the ten-volume Grand Series of Chinese Mathematics, covering the time from antiquity to late part of the Qin dynasty.\n\n\n\n"}
{"id": "10952814", "url": "https://en.wikipedia.org/wiki?curid=10952814", "title": "Zionts–Wallenius method", "text": "Zionts–Wallenius method\n\nThe Zionts–Wallenius method is an interactive method used to find a best solution to a multi-criteria optimization problem.\n\nSpecifically it can help a user solve a linear programming problem having more than one (linear) objective. A user is asked to respond to comparisons between feasible solutions or to choose directions of change desired in each iteration. Providing certain mathematical assumptions hold, the method finds an optimal solution.\n\n"}
