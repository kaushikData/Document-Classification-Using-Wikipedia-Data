{"id": "562521", "url": "https://en.wikipedia.org/wiki?curid=562521", "title": "175 (number)", "text": "175 (number)\n\n175 (one hundred [and] seventy-five) is the natural number following 174 and preceding 176.\n\n175 is an odd number, a composite number, and a deficient number. It is a decagonal number, a 19-gonal number, and a centered 29-gonal number.\n\n175 is an Ulam number, and a Zuckerman number. It is the magic constant of the \"n\"×\"n\" normal magic square and n-Queens Problem for n = 7.\n\nIn base 10, raising the digits of 175 to powers of successive integers equals itself: 135, 518, 598, and 1306 also have this property.\n\n\n\n\n\n\nThe Bible says that Abraham lived to be 175 years old.\n\n\n\n175 is also:\n\n\n"}
{"id": "15967139", "url": "https://en.wikipedia.org/wiki?curid=15967139", "title": "Affine geometry of curves", "text": "Affine geometry of curves\n\nIn the mathematical field of differential geometry, the affine geometry of curves is the study of curves in an affine space, and specifically the properties of such curves which are invariant under the special affine group formula_1\n\nIn the classical Euclidean geometry of curves, the fundamental tool is the Frenet–Serret frame. In affine geometry, the Frenet–Serret frame is no longer well-defined, but it is possible to define another canonical moving frame along a curve which plays a similar decisive role. The theory was developed in the early 20th century, largely from the efforts of Wilhelm Blaschke and Jean Favard.\n\nLet x(\"t\") be a curve in formula_2. Assume, as one does in the Euclidean case, that the first \"n\" derivatives of x(\"t\") are linearly independent so that, in particular, x(\"t\") does not lie in any lower-dimensional affine subspace of formula_3. Then the curve parameter \"t\" can be normalized by setting determinant\n\nSuch a curve is said to be parametrized by its \"affine arclength\". For such a parameterization,\n\ndetermines a mapping into the special affine group, known as a special affine frame for the curve. That is, at each point of the quantities formula_6 define a special affine frame for the affine space formula_2, consisting of a point x of the space and a special linear basis formula_8 attached to the point at x. The pullback of the Maurer–Cartan form along this map gives a complete set of affine structural invariants of the curve. In the plane, this gives a single scalar invariant, the affine curvature of the curve.\n\nThe normalization of the curve parameter \"s\" was selected above so that \nIf \"n\"≡0 (mod 4) or \"n\"≡3 (mod 4) then the sign of this determinant is a discrete invariant of the curve. A curve is called dextrorse (right winding, frequently \"weinwendig\" in German) if it is +1, and sinistrorse (left winding, frequently \"hopfenwendig\" in German) if it is −1.\n\nIn three-dimensions, a right-handed helix is dextrorse, and a left-handed helix is sinistrorse.\n\nSuppose that the curve x in formula_2 is parameterized by affine arclength. Then the affine curvatures, \"k\", …, \"k\" of x are defined by\n\nThat such an expression is possible follows by computing the derivative of the determinant\n\nso that x is a linear combination of x′, …, x.\n\nConsider the matrix\n\nwhose columns are the first \"n\" derivatives of x (still parameterized by special affine arclength). Then,\n\nIn concrete terms, the matrix \"C\" is the pullback of the Maurer–Cartan form of the special linear group along the frame given by the first \"n\" derivatives of x.\n\n\n"}
{"id": "961805", "url": "https://en.wikipedia.org/wiki?curid=961805", "title": "Algebra of sets", "text": "Algebra of sets\n\nThe algebra of sets defines the properties and laws of sets, the set-theoretic operations of union, intersection, and complementation and the relations of set equality and set inclusion. It also provides systematic procedures for evaluating expressions, and performing calculations, involving these operations and relations.\n\nAny set of sets closed under the set-theoretic operations forms a Boolean algebra with the join operator being \"union\", the meet operator being \"intersection\", the complement operator being \"set complement\", the bottom being formula_1 and the top being the universe set under consideration.\n\nThe algebra of sets is the set-theoretic analogue of the algebra of numbers. Just as arithmetic addition and multiplication are associative and commutative, so are set union and intersection; just as the arithmetic relation \"less than or equal\" is reflexive, antisymmetric and transitive, so is the set relation of \"subset\".\n\nIt is the algebra of the set-theoretic operations of union, intersection and complementation, and the relations of equality and inclusion. For a basic introduction to sets see the article on sets, for a fuller account see naive set theory, and for a full rigorous axiomatic treatment see axiomatic set theory.\n\nThe binary operations of set union (formula_2) and intersection (formula_3) satisfy many identities. Several of these identities or \"laws\" have well established names.\n\nThe union and intersection of sets may be seen as analogous to the addition and multiplication of numbers. Like addition and multiplication, the operations of union and intersection are commutative and associative, and intersection \"distributes\" over union. However, unlike addition and multiplication, union also distributes over intersection.\n\nTwo additional pairs of laws involve the special sets called the empty set Ø and the universe set formula_10; together with the complement operator (formula_11 denotes the complement of formula_12. This can also be written as formula_13, read as A prime). The empty set has no members, and the universe set has all possible members (in a particular context).\n\nThe identity laws (together with the commutative laws) say that, just like 0 and 1 for addition and multiplication, Ø and U are the identity elements for union and intersection, respectively.\n\nUnlike addition and multiplication, union and intersection do not have inverse elements. However the complement laws give the fundamental properties of the somewhat inverse-like unary operation of set complementation.\n\nThe preceding five pairs of laws—the commutative, associative, distributive, identity and complement laws—encompass all of set algebra, in the sense that every valid proposition in the algebra of sets can be derived from them.\n\nNote that if the complement laws are weakened to the rule formula_18, then this is exactly the algebra of propositional linear logic.\n\nEach of the identities stated above is one of a pair of identities such that each can be transformed into the other by interchanging ∪ and ∩, and also Ø and U.\n\nThese are examples of an extremely important and powerful property of set algebra, namely, the principle of duality for sets, which asserts that for any true statement about sets, the dual statement obtained by interchanging unions and intersections, interchanging U and Ø and reversing inclusions is also true. A statement is said to be self-dual if it is equal to its own dual.\n\nThe following proposition states six more important laws of set algebra, involving unions and intersections.\n\nPROPOSITION 3: For any subsets \"A\" and \"B\" of a universe set U, the following identities hold:\n\nAs noted above, each of the laws stated in proposition 3 can be derived from the five fundamental pairs of laws stated above. As an illustration, a proof is given below for the idempotent law for union.\n\n\"Proof:\"\n\nThe following proof illustrates that the dual of the above proof is the proof of the dual of the idempotent law for union, namely the idempotent law for intersection.\n\n\"Proof:\"\nIntersection can be expressed in terms of set difference :\n\nformula_25\n\nThe following proposition states five more important laws of set algebra, involving complements.\n\nPROPOSITION 4: Let \"A\" and \"B\" be subsets of a universe U, then:\n\nNotice that the double complement law is self-dual.\n\nThe next proposition, which is also self-dual, says that the complement of a set is the only set that satisfies the complement laws. In other words, complementation is characterized by the complement laws.\n\nPROPOSITION 5: Let \"A\" and \"B\" be subsets of a universe U, then:\n\nThe following proposition says that inclusion, that is the binary relation of one set being a subset of another, is a partial order.\n\nPROPOSITION 6: If \"A\", \"B\" and \"C\" are sets then the following hold:\n\nThe following proposition says that for any set \"S\", the power set of \"S\", ordered by inclusion, is a bounded lattice, and hence together with the distributive and complement laws above, show that it is a Boolean algebra.\n\nPROPOSITION 7: If \"A\", \"B\" and \"C\" are subsets of a set \"S\" then the following hold:\n\nThe following proposition says that the statement formula_35 is equivalent to various other statements involving unions, intersections and complements.\n\nPROPOSITION 8: For any two sets \"A\" and \"B\", the following are equivalent:\n\nThe above proposition shows that the relation of set inclusion can be characterized by either of the operations of set union or set intersection, which means that the notion of set inclusion is axiomatically superfluous.\n\nThe following proposition lists several identities concerning relative complements and set-theoretic differences.\n\nPROPOSITION 9: For any universe U and subsets \"A\", \"B\", and \"C\" of U, the following identities hold:\n\n\n\n"}
{"id": "339555", "url": "https://en.wikipedia.org/wiki?curid=339555", "title": "Almost prime", "text": "Almost prime\n\nIn number theory, a natural number is called almost prime if there exists an absolute constant \"K\" such that the number has at most \"K\" prime factors. An almost prime \"n\" is denoted by \"P\" if and only if the number of prime factors of \"n\", counted according to multiplicity, is at most \"r\". A natural number is called \"k\"-almost prime if it has exactly \"k\" prime factors, counted with multiplicity. More formally, a number \"n\" is \"k\"-almost prime if and only if Ω(\"n\") = \"k\", where Ω(\"n\") is the total number of primes in the prime factorization of \"n\":\n\nA natural number is thus prime if and only if it is 1-almost prime, and semiprime if and only if it is 2-almost prime. The set of \"k\"-almost primes is usually denoted by \"P\". The smallest \"k\"-almost prime is 2. The first few \"k\"-almost primes are:\n\nThe number π(\"n\") of positive integers less than or equal to \"n\" with at most \"k\" prime divisors (not necessarily distinct) is asymptotic to:\n\na result of Landau. See also the Hardy–Ramanujan theorem.\n"}
{"id": "16364229", "url": "https://en.wikipedia.org/wiki?curid=16364229", "title": "Arakelov theory", "text": "Arakelov theory\n\nIn mathematics, Arakelov theory (or Arakelov geometry) is an approach to Diophantine geometry, named for Suren Arakelov. It is used to study Diophantine equations in higher dimensions.\n\nArakelov geometry studies a scheme \"X\" over the ring of integers Z, by putting Hermitian metrics on holomorphic vector bundles over \"X\"(C), the complex points of \"X\". This extra Hermitian structure is applied as a substitute for the failure of the scheme Spec(Z) to be a complete variety.\n\n defined an intersection theory on the arithmetic surfaces attached to smooth projective curves over number fields, with the aim of proving certain results, known in the case of function fields,\nin the case of number fields. extended Arakelov's work by establishing results such as a Riemann-Roch theorem, a Noether formula, a Hodge index theorem and the nonnegativity of the self-intersection of the dualizing sheaf in this context.\n\nArakelov theory was used by Paul Vojta (1991) to give a new proof of the Mordell conjecture, and by in his proof of Serge Lang's generalization of the Mordell conjecture.\n\nArakelov's theory was generalized by Henri Gillet and Christophe Soulé to higher dimensions. That is, Gillet and Soulé defined an intersection pairing on an arithmetic variety. One of the main results of Gillet and Soulé is the arithmetic Riemann–Roch theorem of , an extension of the Grothendieck–Riemann–Roch theorem to arithmetic varieties. \nFor this one defines arithmetic Chow groups CH(\"X\") of an arithmetic variety \"X\", and defines Chern classes for Hermitian vector bundles over \"X\" taking values in the arithmetic Chow groups. \nThe arithmetic Riemann–Roch theorem then describes how the Chern class behaves under pushforward of vector bundles under a proper map of arithmetic varieties. A complete proof of this theorem was only published recently by Gillet, Rössler and Soulé.\n\nArakelov's intersection theory for arithmetic surfaces was developed further by . The theory of Bost is based on the use of Green functions which, up to logarithmic singularities, belong to the Sobolev space formula_1. In this context Bost obtains an arithmetic Hodge index theorem and uses this to obtain Lefschetz theorems for arithmetic surfaces.\n\nAn arithmetic cycle of codimension \"p\" is a pair (\"Z\", \"g\") where \"Z\" ∈ \"Z\"(\"X\") is a \"p\"-cycle on \"X\" and \"g\" is a Green current for \"Z\", a higher-dimensional generalization of a Green function. The arithmetic Chow group formula_2 of codimension \"p\" is the quotient of this group by the subgroup generated by certain \"trivial\" cycles.\n\nThe usual Grothendieck–Riemann–Roch theorem describes how the Chern character ch behaves under pushforward of sheaves, and states that ch(\"f\"(\"E\"))= \"f\"(ch(E)Td), where \"f\" is a proper morphism from \"X\" to \"Y\" and \"E\" is a vector bundle over \"f\". The arithmetic Riemann–Roch theorem is similar except that the Todd class gets multiplied by a certain power series. \nThe arithmetic Riemann–Roch theorem states\nwhere\n\n\n\n"}
{"id": "4890", "url": "https://en.wikipedia.org/wiki?curid=4890", "title": "Bayesian probability", "text": "Bayesian probability\n\nBayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief.\n\nThe Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses, i.e., the propositions whose truth or falsity is uncertain. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.\n\nBayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies some prior probability, which is then updated to a posterior probability in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.\n\nThe term \"Bayesian\" derives from the 18th century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference. Mathematician Pierre-Simon Laplace pioneered and popularised what is now called Bayesian probability.\n\nBayesian methods are characterized by concepts and procedures as follows:\n\nBroadly speaking, there are two interpretations on Bayesian probability. For \"objectivists\", interpreting probability as extension of logic, \"probability\" quantifies the reasonable expectation everyone (even a \"robot\") sharing the same knowledge should share in accordance with the rules of Bayesian statistics, which can be justified by Cox's theorem. For \"subjectivists\", \"probability\" corresponds to a personal belief. Rationality and coherence allow for substantial variation within the constraints they pose; the constraints are justified by the Dutch book argument or by the decision theory and de Finetti's theorem. The objective and subjective variants of Bayesian probability differ mainly in their interpretation and construction of the prior probability.\n\nThe term \"Bayesian\" refers to Thomas Bayes (1702–1761), who proved a special case of what is now called Bayes' theorem in a paper titled \"An Essay towards solving a Problem in the Doctrine of Chances\". In that special case, the prior and posterior distributions were Beta distributions and the data came from Bernoulli trials. It was Pierre-Simon Laplace (1749–1827) who introduced a general version of the theorem and used it to approach problems in celestial mechanics, medical statistics, reliability, and jurisprudence. Early Bayesian inference, which used uniform priors following Laplace's principle of insufficient reason, was called \"inverse probability\" (because it infers backwards from observations to parameters, or from effects to causes). After the 1920s, \"inverse probability\" was largely supplanted by a collection of methods that came to be called frequentist statistics.\n\nIn the 20th century, the ideas of Laplace developed in two directions, giving rise to \"objective\" and \"subjective\" currents in Bayesian practice.\nHarold Jeffreys' \"Theory of Probability\" (first published in 1939) played an important role in the revival of the Bayesian view of probability, followed by works by Abraham Wald (1950) and Leonard J. Savage (1954). The adjective \"Bayesian\" itself dates to the 1950s; the derived \"Bayesianism\", \"neo-Bayesianism\" is of 1960s coinage.\nIn the objectivist stream, the statistical analysis depends on only the model assumed and the data analysed. No subjective decisions need to be involved. In contrast, \"subjectivist\" statisticians deny the possibility of fully objective analysis for the general case.\n\nIn the 1980s, there was a dramatic growth in research and applications of Bayesian methods, mostly attributed to the discovery of Markov chain Monte Carlo methods and the consequent removal of many of the computational problems, and to an increasing interest in nonstandard, complex applications. While frequentist statistics remains strong (as seen by the fact that most undergraduate teaching is still based on it ), Bayesian methods are widely accepted and used, e.g., in the field of machine learning.\n\nThe use of Bayesian probabilities as the basis of Bayesian inference has been supported by several arguments, such as Cox axioms, the Dutch book argument, arguments based on decision theory and de Finetti's theorem.\n\nRichard T. Cox showed that Bayesian updating follows from several axioms, including two functional equations and a hypothesis of differentiability. The assumption of differentiability or even continuity is controversial; Halpern found a counterexample based on his observation that the Boolean algebra of statements may be finite. Other axiomatizations have been suggested by various authors with the purpose of making the theory more rigorous.\n\nThe Dutch book argument was proposed by de Finetti; it is based on betting. A Dutch book is made when a clever gambler places a set of bets that guarantee a profit, no matter what the outcome of the bets. If a bookmaker follows the rules of the Bayesian calculus in the construction of his odds, a Dutch book cannot be made.\n\nHowever, Ian Hacking noted that traditional Dutch book arguments did not specify Bayesian updating: they left open the possibility that non-Bayesian updating rules could avoid Dutch books. For example, Hacking writes \"And neither the Dutch book argument, nor any other in the personalist arsenal of proofs of the probability axioms, entails the dynamic assumption. Not one entails Bayesianism. So the personalist requires the dynamic assumption to be Bayesian. It is true that in consistency a personalist could abandon the Bayesian model of learning from experience. Salt could lose its savour.\"\n\nIn fact, there are non-Bayesian updating rules that also avoid Dutch books (as discussed in the literature on \"probability kinematics\" following the publication of Richard C. Jeffreys' rule, which is itself regarded as Bayesian). The additional hypotheses sufficient to (uniquely) specify Bayesian updating are substantial and not universally seen as satisfactory.\n\nA decision-theoretic justification of the use of Bayesian inference (and hence of Bayesian probabilities) was given by Abraham Wald, who proved that every admissible statistical procedure is either a Bayesian procedure or a limit of Bayesian procedures. Conversely, every Bayesian procedure is admissible.\n\nFollowing the work on expected utility theory of Ramsey and von Neumann, decision-theorists have accounted for rational behavior using a probability distribution for the agent. Johann Pfanzagl completed the \"Theory of Games and Economic Behavior\" by providing an axiomatization of subjective probability and utility, a task left uncompleted by von Neumann and Oskar Morgenstern: their original theory supposed that all the agents had the same probability distribution, as a convenience. Pfanzagl's axiomatization was endorsed by Oskar Morgenstern: \"Von Neumann and I have anticipated...[the question whether probabilities] might, perhaps more typically, be subjective and have stated specifically that in the latter case axioms could be found from which could derive the desired numerical utility together with a number for the probabilities (cf. p. 19 of The Theory of Games and Economic Behavior). We did not carry this out; it was demonstrated by Pfanzagl ... with all the necessary rigor\".\n\nRamsey and Savage noted that the individual agent's probability distribution could be objectively studied in experiments. The role of judgment and disagreement in science has been recognized since Aristotle and even more clearly with Francis Bacon. The objectivity of science lies not in the psychology of individual scientists, but in the process of science and especially in statistical methods, as noted by C. S. Peirce. Recall that the objective methods for falsifying propositions about personal probabilities have been used for a half century, as noted previously. Procedures for testing hypotheses about probabilities (using finite samples) are due to Ramsey (1931) and de Finetti (1931, 1937, 1964, 1970). Both Bruno de Finetti and Frank P. Ramsey acknowledge their debts to pragmatic philosophy, particularly (for Ramsey) to Charles S. Peirce.\n\nThe \"Ramsey test\" for evaluating probability distributions is implementable in theory, and has kept experimental psychologists occupied for a half century.\nThis work demonstrates that Bayesian-probability propositions can be falsified, and so meet an empirical criterion of Charles S. Peirce, whose work inspired Ramsey. (This falsifiability-criterion was popularized by Karl Popper.)\n\nModern work on the experimental evaluation of personal probabilities uses the randomization, blinding, and Boolean-decision procedures of the Peirce-Jastrow experiment. Since individuals act according to different probability judgments, these agents' probabilities are \"personal\" (but amenable to objective study).\n\nPersonal probabilities are problematic for science and for some applications where decision-makers lack the knowledge or time to specify an informed probability-distribution (on which they are prepared to act). To meet the needs of science and of human limitations, Bayesian statisticians have developed \"objective\" methods for specifying prior probabilities.\n\nIndeed, some Bayesians have argued the prior state of knowledge defines \"the\" (unique) prior probability-distribution for \"regular\" statistical problems; cf. well-posed problems. Finding the right method for constructing such \"objective\" priors (for appropriate classes of regular problems) has been the quest of statistical theorists from Laplace to John Maynard Keynes, Harold Jeffreys, and Edwin Thompson Jaynes. These theorists and their successors have suggested several methods for constructing \"objective\" priors (Unfortunately, it is not clear how to assess the relative \"objectivity\" of the priors proposed under these methods):\n\nEach of these methods contributes useful priors for \"regular\" one-parameter problems, and each prior can handle some challenging statistical models (with \"irregularity\" or several parameters). Each of these methods has been useful in Bayesian practice. Indeed, methods for constructing \"objective\" (alternatively, \"default\" or \"ignorance\") priors have been developed by avowed subjective (or \"personal\") Bayesians like James Berger (Duke University) and José-Miguel Bernardo (Universitat de València), simply because such priors are needed for Bayesian practice, particularly in science. The quest for \"the universal method for constructing priors\" continues to attract statistical theorists.\n\nThus, the Bayesian statistician needs either to use informed priors (using relevant expertise or previous data) or to choose among the competing methods for constructing \"objective\" priors.\n\n\n"}
{"id": "9712648", "url": "https://en.wikipedia.org/wiki?curid=9712648", "title": "Bing metrization theorem", "text": "Bing metrization theorem\n\nIn topology, the Bing metrization theorem, named after R. H. Bing, characterizes when a topological space is metrizable.\nThe theorem states that a topological space formula_1 is metrizable if and only if it is regular and T and has a σ-discrete basis. A family of sets is called σ-discrete when it is a union of countably many discrete collections, where a family formula_2 of subsets of a space formula_1 is called discrete, when every point of formula_1 has a neighborhood that intersects at most one member of formula_2. \n\nThe theorem was proven by Bing in 1951 and was an independent discovery with the Nagata-Smirnov metrization theorem that was proved independently by both Nagata (1950) and Smirnov (1951). Both theorems are often merged in the Bing-Nagata-Smirnov metrization theorem. It is a common tool to prove other metrization theorems, e.g. the Moore metrization theorem: a collectionwise normal, Moore space is metrizable, is a direct consequence.\n\nUnlike the Urysohn's metrization theorem which provides a sufficient condition for metrization, this theorem provides both a necessary and sufficient condition for a topological space to be metrizable.\n\n"}
{"id": "1162065", "url": "https://en.wikipedia.org/wiki?curid=1162065", "title": "Boolean data type", "text": "Boolean data type\n\nIn computer science, the Boolean data type is a data type that has one of two possible values (usually denoted \"true\" and \"false\"), intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid 19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean \"condition\" evaluates to true or false. It is a special case of a more general \"logical data type (see probabilistic logic)—\"logic need not always be Boolean.\n\nIn programming languages with a built-in Boolean data type, such as Pascal and Java, the comparison operators such as codice_1 and codice_2 are usually defined to return a Boolean value. Conditional and iterative commands may be defined to test Boolean-valued expressions.\n\nLanguages with no explicit Boolean data type, like C90 and Lisp, may still represent truth values by some other data type. Common Lisp uses an empty list for false, and any other value for true. The C programming language uses an integer type, where relational expressions like codice_3 and logical expressions connected by codice_4 and codice_5 are defined to have value 1 if true and 0 if false, whereas the test parts of codice_6, codice_7, codice_8, etc., treat any non-zero value as true. Indeed, a Boolean variable may be regarded (and implemented) as a numerical variable with one binary digit (bit), which can store only two values. The implementation of Booleans in computers are most likely represented as a full word, rather than a bit; this is usually due to the ways computers transfer blocks of information.\n\nMost programming languages, even those with no explicit Boolean type, have support for Boolean algebraic operations such as conjunction (codice_9, codice_10, codice_11), disjunction (codice_12, codice_13, codice_14), equivalence (codice_15, codice_16, codice_17), exclusive or/non-equivalence (codice_18, codice_19, codice_20, codice_21), and negation (codice_22, codice_23, codice_24).\n\nIn some languages, like Ruby, Smalltalk, and Alice the \"true\" and \"false\" values belong to separate classes, i.e., codice_25 and codice_26, respectively, so there is no one Boolean \"type\".\n\nIn SQL, which uses a three-valued logic for explicit comparisons because of its special treatment of Nulls, the Boolean data type (introduced in ) is also defined to include more than two truth values, so that SQL \"Booleans\" can store all logical values resulting from the evaluation of predicates in SQL. A column of Boolean type can also be restricted to just codice_27 and codice_28 though.\n\nOne of the earliest programming languages to provide an explicit \"boolean\" data type was ALGOL 60 (1960) with values \"true\" and \"false\" and logical operators denoted by symbols 'formula_1' (and), 'formula_2' (or), 'formula_3' (implies), 'formula_4' (equivalence), and 'formula_5' (not). Due to input device and character set limits on many computers of the time, however, most compilers used alternative representations for many of the operators, such as codice_9 or codice_30.\n\nThis approach with \"boolean\" as a built-in (either primitive or otherwise predefined) data type was adopted by many later programming languages, such as Simula 67 (1967), ALGOL 68 (1970), Pascal (1970), Ada (1980), Java (1995), and C# (2000), among others.\n\nThe first version of FORTRAN (1957) and its successor FORTRAN II (1958) had no logical values or operations; even the conditional codice_31 statement took an arithmetic expression and branched to one of three locations according to its sign; see arithmetic IF. FORTRAN IV (1962), however, followed the ALGOL 60 example by providing a Boolean data type (codice_32), truth literals (codice_33 and codice_34), Boolean-valued numeric comparison operators (codice_35, codice_36, etc.), and logical operators (codice_37, codice_38, codice_39). In codice_40 statements, a specific control character ('codice_41') was provided for the parsing or formatting of logical values.\n\nThe language Lisp (1958) never had a built-in Boolean data type. Instead, conditional constructs like codice_42 assume that the logical value \"false\" is represented by the empty list codice_43, which is defined to be the same as the special atom codice_44 or codice_45; whereas any other s-expression is interpreted as \"true\". For convenience, most modern dialects of Lisp predefine the atom codice_46 to have value codice_46, so that codice_46 can be used as a mnemonic notation for \"true\".\n\nThis approach (\"any value can be used as a Boolean value\") was retained in most Lisp dialects (Common Lisp, Scheme, Emacs Lisp), and similar models were adopted by many scripting languages, even ones having a distinct Boolean type or Boolean values; although which values are interpreted as \"false\" and which are \"true\" vary from language to language. In Scheme, for example, the \"false\" value is an atom distinct from the empty list, so the latter is interpreted as \"true\".\n\nThe language Pascal (1970) introduced the concept of programmer-defined enumerated types. A built-in codice_49 data type was then provided as a predefined enumerated type with values codice_28 and codice_27. By definition, all comparisons, logical operations, and conditional statements applied to and/or yielded codice_49 values. Otherwise, the codice_49 type had all the facilities which were available for enumerated types in general, such as ordering and use as indices. In contrast, converting between codice_49s and integers (or any other types) still required explicit tests or function calls, as in ALGOL 60. This approach (\"Boolean is an enumerated type\") was adopted by most later languages which had enumerated types, such as Modula, Ada, and Haskell.\n\nInitial implementations of the language C (1972) provided no Boolean type, and to this day Boolean values are commonly represented by integers (codice_55s) in C programs. The comparison operators (codice_1, codice_17, etc.) are defined to return a signed integer (codice_55) result, either 0 (for false) or 1 (for true). Logical operators (codice_4, codice_5, codice_24, etc.) and condition-testing statements (codice_6, codice_7) assume that zero is false and all other values are true.\n\nAfter enumerated types (codice_64s) were added to the American National Standards Institute version of C, ANSI C (1989), many C programmers got used to defining their own Boolean types as such, for readability reasons. However, enumerated types are equivalent to integers according to the language standards; so the effective identity between Booleans and integers is still valid for C programs.\n\nStandard C (since C99) provides a boolean type, called codice_65. By including the header codice_66, one can use the more intuitive name codice_67 and the constants codice_68 and codice_69. The language guarantees that any two true values will compare equal (which was impossible to achieve before the introduction of the type). Boolean values still behave as integers, can be stored in integer variables, and used anywhere integers would be valid, including in indexing, arithmetic, parsing, and formatting. This approach (\"Boolean values are just integers\") has been retained in all later versions of C.\n\nC++ has a separate Boolean data type codice_67, but with automatic conversions from scalar and pointer values that are very similar to those of C. This approach was adopted also by many later languages, especially by some scripting languages such as AWK.\n\nObjective-C also has a separate Boolean data type codice_71, with possible values being codice_72 or codice_73, equivalents of true and false respectively. Also, in Objective-C compilers that support C99, C's codice_65 type can be used, since Objective-C is a superset of C.\n\nPerl has no boolean data type. Instead, any value can behave as boolean in boolean context (condition of codice_6 or codice_7 statement, argument of codice_4 or codice_5, etc.). The number codice_79, the strings codice_80 and codice_81, the empty list codice_43, and the special value codice_83 evaluate to false. All else evaluates to true.\n\nLua has a boolean data type, but non-boolean values can also behave as booleans. The non-value codice_44 evaluates to false, whereas every other data type always evaluates to true, regardless of value.\n\nTcl has no separate Boolean type. Like in C, the integers 0 (false) and 1 (true - in fact any nonzero integer) are used.\n\nExamples of coding:\n\ncodice_85\n\nThe above will show \"V is 1 or true\" since the expression evaluates to '1'\n\ncodice_86\n\nThe above will render an error as variable 'v' cannot be evaluated as '0' or '1'\n\nPython, from version 2.3 forward, has a codice_67 type which is a subclass of codice_55, the standard integer type. It has two possible values: codice_25 and codice_26, which are \"special versions\" of 1 and 0 respectively and behave as such in arithmetic contexts. Also, a numeric value of zero (integer or fractional), the null value (codice_91), the empty string, and empty containers (i.e. lists, sets, etc.) are considered Boolean false; all other values are considered Boolean true by default. Classes can define how their instances are treated in a Boolean context through the special method codice_92 (Python 2) or codice_93 (Python 3). For containers, codice_94 (the special method for determining the length of containers) is used if the explicit Boolean conversion method is not defined.\n\nIn Ruby, in contrast, only codice_44 (Ruby's null value) and a special codice_69 object are \"false\", all else (including the integer 0 and empty arrays) is \"true\".\n\nIn JavaScript, the empty string (codice_81), codice_98, codice_99, codice_100, +0, −0 and codice_69\nare sometimes called \"falsy\", and their complement, \"truthy\", to distinguish between strictly type-checked and coerced Booleans. Languages such as PHP also use this approach.\n\nThe standard introduced a BOOLEAN data type as an optional feature (T031). When restricted by a codice_102 constraint, a SQL BOOLEAN behaves like Booleans in other languages. However, in SQL the BOOLEAN type is nullable by default like all other SQL data types, meaning it can have the special null value also. Although the SQL standard defines three literals for the BOOLEAN type – TRUE, FALSE, and UNKNOWN – it also says that the NULL BOOLEAN and UNKNOWN \"may be used interchangeably to mean exactly the same thing\". This has caused some controversy because the identification subjects UNKNOWN to the equality comparison rules for NULL. More precisely UNKNOWN = UNKNOWN is not TRUE but UNKNOWN/NULL. As of 2012 few major SQL systems implement the T031 feature. Firebird and PostgreSQL are notable exceptions, although PostgreSQL implements no UNKNOWN literal; NULL can be used instead.\n\n"}
{"id": "1526742", "url": "https://en.wikipedia.org/wiki?curid=1526742", "title": "Boris Vladimirovich Gnedenko", "text": "Boris Vladimirovich Gnedenko\n\nBoris Vladimirovich Gnedenko (; January 1, 1912 – December 27, 1995) was a Soviet mathematician and a student of Andrey Nikolaevich Kolmogorov. He was born in Simbirsk (now Ulyanovsk), Russia, and died in Moscow. He is perhaps best known for his work with Kolmogorov, and his contributions to the study of probability theory, particularly extreme value theory, with such results as the Fisher–Tippett–Gnedenko theorem. Gnedenko was appointed as Head of the Physics, Mathematics and Chemistry Section of the Ukrainian Academy of Sciences in 1949, and also became Director of the Kiev Institute of Mathematics in the same year.\n\nGnedenko was a leading member of the Russian school of probability theory and statistics. He also worked on applications of statistics to reliability and quality control in manufacturing. He wrote a history of mathematics in Russia (published 1946) and with O. B. Sheynin the section on the history of probability theory in the history of mathematics by Andrei Kolmogorov and Adolph P. Yushkevich (published 1992). In 1958 he was a plenary speaker at the International Congress of Mathematicians in Edinburgh with a talk entitled \"Limit theorems of probability theory\".\n\n\n"}
{"id": "9320596", "url": "https://en.wikipedia.org/wiki?curid=9320596", "title": "Carleman's inequality", "text": "Carleman's inequality\n\nCarleman's inequality is an inequality in mathematics, named after Torsten Carleman, who proved it in 1923 and used it to prove the Denjoy–Carleman theorem on quasi-analytic classes.\n\nLet \"a\", \"a\", \"a\", ... be a sequence of non-negative real numbers, then\n\nThe constant \"e\" in the inequality is optimal, that is, the inequality does not always hold if \"e\" is replaced by a smaller number. The inequality is strict (it holds with \"<\" instead of \"≤\") if some element in the sequence is non-zero.\n\nCarleman's inequality has an integral version, which states that\n\nfor any \"f\" ≥ 0.\n\nA generalisation, due to Lennart Carleson, states the following:\n\nfor any convex function \"g\" with \"g\"(0) = 0, and for any -1 < \"p\" < ∞,\n\nCarleman's inequality follows from the case \"p\" = 0.\n\nAn elementary proof is sketched below. From the inequality of arithmetic and geometric means applied to the numbers formula_4\n\nwhere MG stands for geometric mean, and MA — for arithmetic mean. The Stirling-type inequality formula_6 applied to formula_7 implies\n\nTherefore,\n\nwhence\n\nproving the inequality. Moreover, the inequality of arithmetic and geometric means of formula_12 non-negative numbers is known to be an equality if and only if all the numbers coincide, that is, in the present case, if and only if formula_13 for formula_14. As a consequence, Carleman's inequality is never an equality for a convergent series, unless all formula_15 vanish, just because the harmonic series is divergent.\n\nOne can also prove Carleman's inequality by starting with Hardy's inequality\n\nfor the non-negative numbers \"a\",\"a\"... and \"p\" > 1, replacing each \"a\" with \"a\", and letting \"p\" → ∞.\n\n"}
{"id": "391832", "url": "https://en.wikipedia.org/wiki?curid=391832", "title": "Cobordism", "text": "Cobordism\n\nIn mathematics, cobordism is a fundamental equivalence relation on the class of compact manifolds of the same dimension, set up using the concept of the boundary (French \"bord\", giving \"cobordism\") of a manifold. Two manifolds of the same dimension are \"cobordant\" if their disjoint union is the \"boundary\" of a compact manifold one dimension higher.\n\nThe boundary of an (\"n\" + 1)-dimensional manifold \"W\" is an \"n\"-dimensional manifold ∂\"W\" that is closed, i.e., with empty boundary. In general, a closed manifold need not be a boundary: cobordism theory is the study of the difference between all closed manifolds and those that are boundaries. The theory was originally developed by René Thom for smooth manifolds (i.e., differentiable), but there are now also versions for piecewise-linear and topological manifolds.\n\nA \"cobordism\" between manifolds \"M\" and \"N\" is a compact manifold \"W\" whose boundary is the disjoint union of \"M\" and \"N\", formula_1.\n\nCobordisms are studied both for the equivalence relation that they generate, and as objects in their own right. Cobordism is a much coarser equivalence relation than diffeomorphism or homeomorphism of manifolds, and is significantly easier to study and compute. It is not possible to classify manifolds up to diffeomorphism or homeomorphism in dimensions ≥ 4 – because the word problem for groups cannot be solved – but it is possible to classify manifolds up to cobordism. Cobordisms are central objects of study in geometric topology and algebraic topology. In geometric topology, cobordisms are intimately connected with Morse theory, and \"h\"-cobordisms are fundamental in the study of high-dimensional manifolds, namely surgery theory. In algebraic topology, cobordism theories are fundamental extraordinary cohomology theories, and categories of cobordisms are the domains of topological quantum field theories.\n\nRoughly speaking, an \"n\"-dimensional manifold \"M\" is a topological space locally (i.e., near each point) homeomorphic to an open subset of Euclidean space formula_2 A manifold with boundary is similar, except that a point of \"M\" is allowed to have a neighborhood that is homeomorphic to an open subset of the half-space\n\nThose points without a neighborhood homeomorphic to an open subset of Euclidean space are the boundary points of formula_4; the boundary of formula_4 is denoted by formula_6. Finally, a closed manifold is, by definition, a compact manifold without boundary (formula_7.)\n\nAn formula_8-dimensional \"cobordism\" is a quintuple formula_9 consisting of an formula_8-dimensional compact differentiable manifold with boundary, formula_11; closed formula_12-manifolds formula_4, formula_14; and embeddings formula_15, formula_16 with disjoint images such that\n\nThe terminology is usually abbreviated to formula_18. \"M\" and \"N\" are called \"cobordant\" if such a cobordism exists. All manifolds cobordant to a fixed given manifold \"M\" form the \"cobordism class\" of \"M\".\n\nEvery closed manifold \"M\" is the boundary of the non-compact manifold \"M\" × [0, 1); for this reason we require \"W\" to be compact in the definition of cobordism. Note however that \"W\" is \"not\" required to be connected; as a consequence, if \"M\" = ∂\"W\" and \"N\" = ∂\"W\", then \"M\" and \"N\" are cobordant.\n\nThe simplest example of a cobordism is the unit interval \"I\" = [0, 1]. It is a 1-dimensional cobordism between the 0-dimensional manifolds {0}, {1}. More generally, for any closed manifold \"M\", (\"M\" × \"I\"; {0}, {1}) is a cobordism from \"M\" × {0} to \"M\" × {1}.\n\nIf \"M\" consists of a circle, and \"N\" of two circles, \"M\" and \"N\" together make up the boundary of a pair of pants \"W\" (see the figure at right). Thus the pair of pants is a cobordism between \"M\" and \"N\". A simpler cobordism between \"M\" and \"N\" is given by the disjoint union of three disks.\n\nThe pair of pants is an example of a more general cobordism: for any two \"n\"-dimensional manifolds \"M\", \"M\"′, the disjoint union formula_19 is cobordant to the connected sum formula_20 The previous example is a particular case, since the connected sum formula_21 is isomorphic to formula_22 The connected sum formula_23 is obtained from the disjoint union formula_19 by surgery on an embedding of formula_25 in formula_19, and the cobordism is the trace of the surgery.\n\nAn \"n\"-manifold \"M\" is called \"null-cobordant\" if there is a cobordism between \"M\" and the empty manifold; in other words, if \"M\" is the entire boundary of some (\"n\" + 1)-manifold. For example, the circle is null-cobordant since it bounds a disk. More generally, a \"n\"-sphere is null-cobordant since it bounds a (\"n\" + 1)-disk. Also, every orientable surface is null-cobordant, because it is the boundary of a handlebody. On the other hand, the 2\"n\"-dimensional real projective space formula_27 is a (compact) closed manifold that is not the boundary of a manifold, as is explained below.\n\nThe general \"bordism problem\" is to calculate the cobordism classes of manifolds subject to various conditions.\n\nNull-cobordisms with additional structure are called fillings. \"Bordism\" and \"cobordism\" are used by some authors interchangeably; others distinguish them. When one wishes to distinguish the study of cobordism classes from the study of cobordisms as objects in their own right, one calls the equivalence question \"bordism of manifolds\", and the study of cobordisms as objects \"cobordisms of manifolds\".\n\nThe term \"bordism\" comes from French , meaning boundary. Hence bordism is the study of boundaries. \"Cobordism\" means \"jointly bound\", so \"M\" and \"N\" are cobordant if they jointly bound a manifold, i.e., if their disjoint union is a boundary. Further, cobordism groups form an extraordinary \"cohomology theory\", hence the co-.\n\nThe above is the most basic form of the definition. It is also referred to as unoriented bordism. In many situations, the manifolds in question are oriented, or carry some other additional structure referred to as G-structure. This gives rise to \"oriented cobordism\" and \"cobordism with G-structure\", respectively. Under favourable technical conditions these form a graded ring called the cobordism ring formula_28, with grading by dimension, addition by disjoint union and multiplication by cartesian product. The cobordism groups formula_28 are the coefficient groups of a generalised homology theory.\n\nWhen there is additional structure, the notion of cobordism must be formulated more precisely: a \"G\"-structure on \"W\" restricts to a \"G\"-structure on \"M\" and \"N\". The basic examples are \"G\" = O for unoriented cobordism, \"G\" = SO for oriented cobordism, and \"G\" = U for complex cobordism using \"stably\" complex manifolds. Many more are detailed by Robert E. Stong.\n\nIn a similar vein, a standard tool in surgery theory is surgery on normal maps: such a process changes a normal map to another normal map within the same bordism class.\n\nInstead of considering additional structure, it is also possible to take into account various notions of manifold, especially piecewise linear (PL) and topological manifolds. This gives rise to bordism groups formula_30, which are harder to compute than the differentiable variants.\n\nRecall that in general, if \"X\", \"Y\" are manifolds with boundary, then the boundary of the product manifold is ∂(\"X\" × \"Y\") = (∂\"X\" × \"Y\") ∪ (\"X\" × ∂\"Y\").\n\nNow, given a manifold \"M\" of dimension \"n\" = \"p\" + \"q\" and an embedding formula_31 define the \"n\"-manifold\n\nobtained by surgery, via cutting out the interior of formula_33 and gluing in formula_34 along their boundary\n\nThe trace of the surgery\n\ndefines an elementary cobordism (\"W\"; \"M\", \"N\"). Note that \"M\" is obtained from \"N\" by surgery on formula_37 This is called reversing the surgery.\n\nEvery cobordism is a union of elementary cobordisms, by the work of Marston Morse, René Thom and John Milnor.\n\nAs per the above definition, a surgery on the circle consists of cutting out a copy of formula_38 and glueing in formula_39 The pictures in Fig. 1 show that the result of doing this is either (i) formula_40 again, or (ii) two copies of formula_40\n\nFor surgery on the 2-Sphere, there are more possibilities, since we can start by cutting out either formula_42 or formula_43\n\n\n\nSuppose that \"f\" is a Morse function on an (\"n\" + 1)-dimensional manifold, and suppose that \"c\" is a critical value with exactly one critical point in its preimage. If the index of this critical point is \"p\" + 1, then the level-set \"N\" := \"f\"(\"c\" + ε) is obtained from \"M\" := \"f\"(\"c\" − ε) by a \"p\"-surgery. The inverse image \"W\" := \"f\"([\"c\" − ε, \"c\" + ε]) defines a cobordism (\"W\"; \"M\", \"N\") that can be identified with the trace of this surgery.\n\nGiven a cobordism (\"W\"; \"M\", \"N\") there exists a smooth function \"f\" : \"W\" → [0, 1] such that \"f\"(0) = \"M\", \"f\"(1) = \"N\". By general position, one can assume \"f\" is Morse and such that all critical points occur in the interior of \"W\". In this setting \"f\" is called a Morse function on a cobordism. The cobordism (\"W\"; \"M\", \"N\") is a union of the traces of a sequence of surgeries on \"M\", one for each critical point of \"f\". The manifold \"W\" is obtained from \"M\" × [0, 1] by attaching one handle for each critical point of \"f\".\nThe Morse/Smale theorem states that for a Morse function on a cobordism, the flowlines of \"f\"′ give rise to a handle presentation of the triple (\"W\"; \"M\", \"N\"). Conversely, given a handle decomposition of a cobordism, it comes from a suitable Morse function. In a suitably normalized setting this process gives a correspondence between handle decompositions and Morse functions on a cobordism.\n\nCobordism had its roots in the (failed) attempt by Henri Poincaré in 1895 to define homology purely in terms of manifolds . Poincaré simultaneously defined both homology and cobordism, which are not the same, in general. See Cobordism as an extraordinary cohomology theory for the relationship between bordism and homology.\n\nBordism was explicitly introduced by Lev Pontryagin in geometric work on manifolds. It came to prominence when René Thom showed that cobordism groups could be computed by means of homotopy theory, via the Thom complex construction. Cobordism theory became part of the apparatus of extraordinary cohomology theory, alongside K-theory. It performed an important role, historically speaking, in developments in topology in the 1950s and early 1960s, in particular in the Hirzebruch–Riemann–Roch theorem, and in the first proofs of the Atiyah–Singer index theorem.\n\nIn the 1980s the category with compact manifolds as objects and cobordisms between these as morphisms played a basic role in the Atiyah–Segal axioms for topological quantum field theory, which is an important part of quantum topology.\n\nCobordisms are objects of study in their own right, apart from cobordism classes. Cobordisms form a category whose objects are closed manifolds and whose morphisms are cobordisms. Roughly speaking, composition is given by gluing together cobordisms end-to-end: the composition of (\"W\"; \"M\", \"N\") and (\"W\"′; \"N\", \"P\") is defined by gluing the right end of the first to the left end of the second, yielding (\"W\"′ ∪ \"W\"; \"M\", \"P\"). A cobordism is a kind of cospan: \"M\" → \"W\" ← \"N\". The category is a dagger compact category.\n\nA topological quantum field theory is a monoidal functor from a category of cobordisms to a category of vector spaces. That is, it is a functor whose value on a disjoint union of manifolds is equivalent to the tensor product of its values on each of the constituent manifolds.\n\nIn low dimensions, the bordism question is relatively trivial, but the category of cobordism is not. For instance, the disk bounding the circle corresponds to a null-ary operation, while the cylinder corresponds to a 1-ary operation and the pair of pants to a binary operation.\n\nThe set of cobordism classes of closed unoriented \"n\"-dimensional manifolds is usually denoted by formula_50 (rather than the more systematic formula_51); it is an abelian group with the disjoint union as operation. More specifically, if [\"M\"] and [\"N\"] denote the cobordism classes of the manifolds \"M\" and \"N\" respectively, we define formula_52; this is a well-defined operation which turns formula_50 into an abelian group. The identity element of this group is the class formula_54 consisting of all closed \"n\"-manifolds which are boundaries. Further we have formula_55 for every \"M\" since formula_56. Therefore, formula_50 is a vector space over formula_58, the field with two elements. The cartesian product of manifolds defines a multiplication formula_59 so\n\nis a graded algebra, with the grading given by the dimension.\n\nThe cobordism class formula_61 of a closed unoriented \"n\"-dimensional manifold \"M\" is determined by the Stiefel–Whitney characteristic numbers of \"M\", which depend on the stable isomorphism class of the tangent bundle. Thus if \"M\" has a stably trivial tangent bundle then formula_62. In 1954 René Thom proved\n\nthe polynomial algebra with one generator formula_64 in each dimension formula_65. Thus two unoriented closed \"n\"-dimensional manifolds \"M\",\"N\" are cobordant, formula_66 if and only if for each collection formula_67 of \"k\"-tuples of integers formula_68 such that formula_69 the Stiefel-Whitney numbers are equal\n\nwith formula_71 the \"i\"th Stiefel-Whitney class and formula_72 the formula_58-coefficient fundamental class.\n\nFor even \"i\" it is possible to choose formula_74, the cobordism class of the \"i\"-dimensional real projective space.\n\nThe low-dimensional unoriented cobordism groups are\n\nThis shows, for example, that every 3-dimensional closed manifold is the boundary of a 4-manifold (with boundary).\n\nThe Euler characteristic formula_76 of an unoriented manifold \"M\" is an unoriented cobordism invariant. This is implied by the equation\n\nfor any compact manifold with boundary formula_11.\n\nTherefore, formula_79 is a well-defined group homomorphism. For example, for any formula_80\n\nIn particular such a product of real projective spaces is not null-cobordant. The mod 2 Euler characteristic map formula_82 is onto for all formula_83 and a group isomorphism for formula_84\n\nMoreover, because of formula_85, these group homomorphism assemble into a homomorphism of graded algebras:\n\nCobordism can also be defined for manifolds that have additional structure, notably an orientation. This is made formal in a general way using the notion of \"X\"-structure (or G-structure). Very briefly, the normal bundle ν of an immersion of \"M\" into a sufficiently high-dimensional Euclidean space formula_87 gives rise to a map from \"M\" to the Grassmannian, which in turn is a subspace of the classifying space of the orthogonal group: ν: \"M\" → Gr(\"n\", \"n\" + \"k\") → \"BO\"(\"k\"). Given a collection of spaces and maps \"X\" → \"X\" with maps \"X\" → \"BO\"(\"k\") (compatible with the inclusions \"BO\"(\"k\") → \"BO\"(\"k\"+1), an \"X\"-structure is a lift of ν to a map formula_88. Considering only manifolds and cobordisms with \"X\"-structure gives rise to a more general notion of cobordism. In particular, \"X\" may be given by \"BG\"(\"k\"), where \"G\"(\"k\") → \"O\"(\"k\") is some group homomorphism. This is referred to as a G-structure. Examples include \"G\" = \"O\", the orthogonal group, giving back the unoriented cobordism, but also the subgroup SO(\"k\"), giving rise to oriented cobordism, the spin group, the unitary group \"U\"(\"k\"), and the trivial group, giving rise to framed cobordism.\n\nThe resulting cobordism groups are then defined analogously to the unoriented case. They are denoted by formula_28.\n\nOriented cobordism is the one of manifolds with an SO-structure. Equivalently, all manifolds need to be oriented and cobordisms (\"W\", \"M\", \"N\") (also referred to as \"oriented cobordisms\" for clarity) are such that the boundary (with the induced orientations) is formula_90, where −\"N\" denotes \"N\" with the reversed orientation. For example, boundary of the cylinder \"M\" × \"I\" is formula_91: both ends have opposite orientations. It is also the correct definition in the sense of extraordinary cohomology theory.\n\nUnlike in the unoriented cobordism group, where every element is two-torsion, 2\"M\" is not in general an oriented boundary, that is, 2[\"M\"] ≠ 0 belongs to formula_92\n\nThe oriented cobordism groups are given modulo torsion by\n\nthe polynomial algebra generated by the oriented cobordism classes\n\nof the complex projective spaces (Thom, 1952). The oriented cobordism group formula_95 is determined by the Stiefel–Whitney and Pontrjagin characteristic numbers (Wall, 1960). Two oriented manifolds are oriented cobordant if and only if their Stiefel–Whitney and Pontrjagin numbers are the same.\n\nThe low-dimensional oriented cobordism groups are :\n\nThe signature of an oriented 4\"i\"-dimensional manifold \"M\" is defined as the signature of the intersection form on formula_97 and is denoted by formula_98 It is an oriented cobordism invariant, which is expressed in terms of the Pontrjagin numbers by the Hirzebruch signature theorem.\n\nFor example, for any \"i\", ..., \"i\" ≥ 1\n\nThe signature map formula_100 is onto for all \"i\" ≥ 1, and an isomorphism for \"i\" = 1.\n\nEvery vector bundle theory (real, complex etc.) has an extraordinary cohomology theory called K-theory. Similarly, every cobordism theory Ω has an extraordinary cohomology theory, with homology (\"bordism\") groups formula_101 and cohomology (\"cobordism\") groups formula_102 for any space \"X\". The generalized homology groups formula_103 are covariant in \"X\", and the generalized cohomology groups formula_104 are contravariant in \"X\". The cobordism groups defined above are, from this point of view, the homology groups of a point: formula_105. Then formula_101 is the group of \"bordism\" classes of pairs (\"M\", \"f\") with \"M\" a closed \"n\"-dimensional manifold \"M\" (with G-structure) and \"f\" : \"M\" → \"X\" a map. Such pairs (\"M\", \"f\"), (\"N\", \"g\") are \"bordant\" if there exists a G-cobordism (\"W\"; \"M\", \"N\") with a map \"h\" : \"W\" → \"X\", which restricts to \"f\" on \"M\", and to \"g\" on \"N\".\n\nAn \"n\"-dimensional manifold \"M\" has a fundamental homology class [\"M\"] ∈ \"H\"(\"M\") (with coefficients in formula_107 in general, and in formula_108 in the oriented case), defining a natural transformation\n\nwhich is far from being an isomorphism in general.\n\nThe bordism and cobordism theories of a space satisfy the Eilenberg–Steenrod axioms apart from the dimension axiom. This does not mean that the groups formula_102 can be effectively computed once one knows the cobordism theory of a point and the homology of the space \"X\", though the Atiyah–Hirzebruch spectral sequence gives a starting point for calculations. The computation is only easy if the particular cobordism theory reduces to a product of ordinary homology theories, in which case the bordism groups are the ordinary homology groups\n\nThis is true for unoriented cobordism. Other cobordism theories do not reduce to ordinary homology in this way, notably framed cobordism, oriented cobordism and complex cobordism. The last-named theory in particular is much used by algebraic topologists as a computational tool (e.g., for the homotopy groups of spheres).\n\nCobordism theories are represented by Thom spectra \"MG\": given a group \"G\", the Thom spectrum is composed from the Thom spaces \"MG\" of the standard vector bundles over the classifying spaces \"BG\". Note that even for similar groups, Thom spectra can be very different: \"MSO\" and \"MO\" are very different, reflecting the difference between oriented and unoriented cobordism.\n\nFrom the point of view of spectra, unoriented cobordism is a product of Eilenberg–MacLane spectra – \"MO\" = \"H\"(π(\"MO\")) – while oriented cobordism is a product of Eilenberg–MacLane spectra rationally, and at 2, but not at odd primes: the oriented cobordism spectrum \"MSO\" is rather more complicated than \"MO\".\n\n\n\n"}
{"id": "147299", "url": "https://en.wikipedia.org/wiki?curid=147299", "title": "Constant factor rule in integration", "text": "Constant factor rule in integration\n\nThe constant factor rule in integration is a dual of the constant factor rule in differentiation, and is a consequence of the linearity of integration. It states that a constant factor within an integrand can be separated from the integrand and instead multiplied by the integral. For example, where k is a constant:\n\nformula_1\n\nStart by noticing that, from the definition of integration as the inverse process of differentiation:\n\nNow multiply both sides by a constant \"k\". Since \"k\" is a constant it is not dependent on \"x\":\n\nTake the constant factor rule in differentiation:\n\nIntegrate with respect to \"x\":\n\nNow from (1) and (2) we have:\n\nTherefore:\n\nNow make a new differentiable function:\n\nSubstitute in (3):\n\nNow we can re-substitute \"y\" for something different from what it was originally:\n\nSo:\n\nThis is the constant factor rule in integration.\n\nA special case of this, with \"k\"=-1, yields:\n"}
{"id": "1514142", "url": "https://en.wikipedia.org/wiki?curid=1514142", "title": "Convergence (evolutionary computing)", "text": "Convergence (evolutionary computing)\n\nConvergence is a phenomenon in evolutionary computation. It causes evolution to halt because precisely every individual in the population is identical. Full convergence might be seen in genetic algorithms (a type of evolutionary computation) using only crossover (a way of combining individuals to make new offspring). Premature convergence is when a population has converged to a single solution, but that solution is not as high of quality as expected, i.e. the population has gotten 'stuck'. However, convergence is not necessarily a negative thing, because populations often stabilise after a time, in the sense that the best programs all have a common ancestor and their behaviour is very similar (or identical) both to each other and to that of high fitness programs from the previous generations. Often the term \"convergence\" is loosely used. Convergence can be avoided with a variety of diversity-generating techniques.\n\nFoundations of Genetic Programming\n"}
{"id": "25347757", "url": "https://en.wikipedia.org/wiki?curid=25347757", "title": "Dexter and sinister", "text": "Dexter and sinister\n\nDexter and sinister are terms used in heraldry to refer to specific locations in an escutcheon bearing a coat of arms, and to the other elements of an achievement. \"Dexter\" (Latin for \"right\") means to the right from the viewpoint of the bearer of the shield, \"i.e.\" the bearer's proper right, to the left from that of the viewer. \"Sinister\" (Latin for \"left\") means to the left from the viewpoint of the bearer, the bearer's proper left, to the right from that of the viewer. \n\nThe dexter side is considered the side of greater honour, for example when impaling two arms. Thus, by tradition, a husband's arms occupy the dexter half of his shield, his wife's paternal arms the sinister half. The shield of a bishop shows the arms of his see in the dexter half, his personal arms in the sinister half. King Richard II adopted arms showing the attributed arms of Edward the Confessor in the dexter half, the royal arms of England in the sinister. More generally, by ancient tradition, the guest of greatest honour at a banquet sits at the right hand of the host. The Bible is replete with passages referring to being at the \"right hand\" of God.\n\nSinister is used to mark that an ordinary or other charge is turned to the heraldic left of the shield. A \"bend sinister\" is a bend which runs from the bearer's top left to bottom right, as opposed to top right to bottom left. As the shield would have been carried with the design facing outwards from the bearer, the bend sinister would slant in the same direction as a sash worn diagonally on the left shoulder.\n\nThis division is key to dimidiation, a method of joining two coats of arms by placing the dexter half of one coat of arms alongside the sinister half of the other. In the case of marriage, the dexter half of the husband's arms would be placed alongside the sinister half of the wife's. The practice fell out of use as early as the 14th century and was replaced by impalement, as in some cases, it could render the arms that are cut in half unrecognizable and in some cases, it would result in a shield that looked like one coat of arms rather than a combination of two.\n\nThe Great Seal of the United States features an eagle clutching an olive branch in its dexter talon and arrows in its sinister talon, indicating the nation's intended inclination to peace. In 1945, one of the changes ordered for the similarly arranged Flag of the President of the United States by President Harry S. Truman was having the eagle face towards its right (dexter, the direction of honor) and thus towards the olive branch.\n\nThe sides of a shield were originally named for the purpose of military training of knights and soldiers long before heraldry came into use early in the 13th century so the only viewpoint that was relevant was the bearer's. The front of the purely-functional shield was originally undecorated.\n\nIt is likely that the use of the shield as a defensive and offensive weapon was almost as developed as that of the sword itself and so the various positions or strokes of the shield needed to be described to students of arms. Such usage may indeed have descended directly from Roman training techniques that were spread throughout Roman Europe and then continued during the age of chivalry, when heraldry came into use.\n"}
{"id": "20697507", "url": "https://en.wikipedia.org/wiki?curid=20697507", "title": "Dunford–Schwartz theorem", "text": "Dunford–Schwartz theorem\n\nIn mathematics, particularly functional analysis, the Dunford–Schwartz theorem, named after Nelson Dunford and Jacob T. Schwartz states that the averages of powers of certain norm-bounded operators on \"L\" converge in a suitable sense.\n\nformula_1\n\nformula_3\n\nThe statement is no longer true when the boundedness condition is relaxed to even formula_4.\n"}
{"id": "311001", "url": "https://en.wikipedia.org/wiki?curid=311001", "title": "Green's function", "text": "Green's function\n\nIn mathematics, a Green's function is the impulse response of an inhomogeneous linear differential equation defined on a domain, with specified initial conditions or boundary conditions.\n\nThrough the superposition principle for linear operator problems, the convolution of a Green's function with an arbitrary function on that domain is the solution to the inhomogeneous differential equation for .\nIn other words, given a linear ordinary differential equation (ODE), \"L\"(solution) = source, one can first solve , for each , and realizing that, since the source is a sum of delta functions, the solution is a sum of Green's functions as well, by linearity of .\n\nGreen's functions are named after the British mathematician George Green, who first developed the concept in the 1830s. In the modern study of linear partial differential equations, Green's functions are studied largely from the point of view of fundamental solutions instead.\n\nUnder many-body theory, the term is also used in physics, specifically in quantum field theory, aerodynamics, aeroacoustics, electrodynamics, seismology and statistical field theory, to refer to various types of correlation functions, even those that do not fit the mathematical definition. In quantum field theory, Green's functions take the roles of propagators.\n\nA Green's function, , of a linear differential operator acting on distributions over a subset of the Euclidean space formula_1, at a point , is any solution of\n\nwhere is the Dirac delta function. This property of a Green's function can be exploited to solve differential equations of the form\nIf the kernel of is non-trivial, then the Green's function is not unique. However, in practice, some combination of symmetry, boundary conditions and/or other externally imposed criteria will give a unique Green's function. \nGreen's functions may be categorized, by the type of boundary conditions satisfied, by a Green's function number.\nAlso, Green's functions in general are distributions, not necessarily functions of a real variable.\n\nGreen's functions are also useful tools in solving wave equations and diffusion equations. In quantum mechanics, the Green's function of the Hamiltonian is a key concept with important links to the concept of density of states.\n\nAs a side note, the Green's function as used in physics is usually defined with the opposite sign, instead, that is,\nThis definition does not significantly change any of the properties of the Green's function.\n\nIf the operator is translation invariant, that is, when has constant coefficients with respect to , then the Green's function can be taken to be a convolution operator, that is,\n\nIn this case, the Green's function is the same as the impulse response of linear time-invariant system theory.\n\nLoosely speaking, if such a function can be found for the operator , then, if we multiply the equation (1) for the Green's function by , and then integrate with respect to , we obtain,\n\nThe right-hand side is now given by the equation (2) to be equal to , thus\n\nBecause the operator formula_6 is linear and acts on the variable alone (not on the variable of integration ), one may take the operator outside of the integration on the right-hand side, yielding\nwhich suggests\nThus, one may obtain the function through knowledge of the Green's function in equation (1) and the source term on the right-hand side in equation (2). This process relies upon the linearity of the operator .\n\nIn other words, the solution of equation (2), , can be determined by the integration given in equation (3). Although is known, this integration cannot be performed unless is also known. The problem now lies in finding the Green's function that satisfies equation (1). For this reason, the Green's function is also sometimes called the fundamental solution associated to the operator .\n\nNot every operator admits a Green's function. A Green's function can also be thought of as a right inverse of . Aside from the difficulties of finding a Green's function for a particular operator, the integral in equation (3) may be quite difficult to evaluate. However the method gives a theoretically exact result.\n\nThis can be thought of as an expansion of according to a Dirac delta function basis (projecting over ); and a superposition of the solution on each projection. Such an integral equation is known as a Fredholm integral equation, the study of which constitutes Fredholm theory.\n\nThe primary use of Green's functions in mathematics is to solve non-homogeneous boundary value problems. In modern theoretical physics, Green's functions are also usually used as propagators in Feynman diagrams; the term \"Green's function\" is often further used for any correlation function.\n\nLet be the Sturm–Liouville operator, a linear differential operator of the form\nand let be the boundary conditions operator\n\nLet be a continuous function in [0, ]. Further suppose that the problem\nis regular, i.e., only the trivial solution exists for the homogeneous problem.\n\nThere is one and only one solution that satisfies\nand it is given by\nwhere is a Green's function satisfying the following conditions:\n\nSometimes the Green's function can be split into a sum of two functions. One with the variable positive (+) and the other with the variable negative (-). These are the advanced and special Green's functions, and when the equation under study depends on time, one of the parts is causal and the other anti-causal. In these problems usually the causal part is the important one.\n\nWhile it doesn't uniquely fix the form the Green's function will take, performing a dimensional analysis to find the units a Green's function must have is an important sanity check on any Green's function found through other means. A quick examination of the defining equation,\nshows that the units formula_23 depend not only on the units of formula_24 but also on the number and units of the space of which the position vectors formula_14 and formula_15 are elements. This leads to the relationship:\nwhere formula_23 is defined as, \"the physical units of formula_23\", and formula_30 is the volume element of the space (or spacetime).\n\nFor example, if formula_31 and time is the only variable then: \nIf formula_35, the d'Alembert operator, and space has 3 dimensions then:\n\nIf a differential operator admits a set of eigenvectors (i.e., a set of functions and scalars such that = ) that is complete, then it is possible to construct a Green's function from these eigenvectors and eigenvalues.\n\n\"Complete\" means that the set of functions { } satisfies the following completeness relation,\n\nThen the following holds,\nwhere formula_40 represents complex conjugation.\n\nApplying the operator to each side of this equation results in the completeness relation, which was assumed.\n\nThe general study of the Green's function written in the above form, and its relationship to the function spaces formed by the eigenvectors, is known as Fredholm theory.\n\nThere are several other methods for finding Green's functions, including the method of images, separation of variables, and Laplace transforms (Cole 2011).\n\nIf the differential operator formula_24 can be factored as formula_42 then the Green's function of formula_24 can be constructed from the Green's functions for formula_44 and formula_45:\nThe above identity follows immediately from taking formula_47 to be the representation of the right operator inverse of formula_24, analogous to how for the invertable linear operator formula_49, defined by formula_50, is represented by its matrix elements formula_51.\n\nA further identity follows for differential operators that are scalar polynomials of the derivative, formula_52. The fundamental theorem of algebra, combined with the fact that formula_53 commutes with itself, guarantees that the polynomial can be factored, putting formula_24 in the form:\nwhere formula_56 are the zeros of formula_57. Taking the Fourier transform of formula_58 with respect to both formula_14 and formula_15 gives:\nThe fraction can then be split into a sum using a Partial fraction decomposition before Fourier transforming back to formula_14 and formula_15 space. This process yields identities that relate integrals of Green's functions and sums of the same. For example, if formula_64 then one form for its Green's function is:\nWhile the example presented is tractable analytically, it illustrates a process that works when the integral is not trivial (for example, when formula_66 is the operator in the polynomial).\n\nThe following table gives an overview of Green's functions of frequently appearing differential operators, where formula_67, formula_68, formula_69 is the Heaviside step function, formula_70 is a Bessel function, formula_71 is a modified Bessel function of the first kind, and formula_72 is a . Where time () appears in the first column, the advanced (causal) Green's function is listed.\n\nGreen's functions for linear differential operators involving the Laplacian may be readily put to use using the second of Green's identities.\n\nTo derive Green's theorem, begin with the divergence theorem (otherwise known as Gauss's theorem),\n\nLet formula_74 and substitute into Gauss' law.\n\nCompute formula_75 and apply the product rule for the ∇ operator,\n\nPlugging this into the divergence theorem produces Green's theorem,\n\nSuppose that the linear differential operator is the Laplacian, ∇², and that there is a Green's function for the Laplacian. The defining property of the Green's function still holds,\n\nLet formula_79 in Green's second identity, see Green's identities. Then,\n\nUsing this expression, it is possible to solve Laplace's equation ∇²\"φ(x)\" = 0 or Poisson's equation ∇²\"φ(x)\" =−\"ρ(x)\", subject to either Neumann or Dirichlet boundary conditions. In other words, we can solve for \"φ(x)\" everywhere inside a volume where either (1) the value of \"φ(x)\" is specified on the bounding surface of the volume (Dirichlet boundary conditions), or (2) the normal derivative of \"φ(x)\" is specified on the bounding surface (Neumann boundary conditions).\n\nSuppose the problem is to solve for \"φ(x)\" inside the region. Then the integral\nreduces to simply \"φ(x)\" due to the defining property of the Dirac delta function and we have\n\nThis form expresses the well-known property of harmonic functions, that \"if the value or normal derivative is known on a bounding surface, then the value of the function inside the volume is known everywhere\".\n\nIn electrostatics, \"φ(x)\" is interpreted as the electric potential, \"ρ(x)\" as electric charge density, and the normal derivative formula_83 as the normal component of the electric field.\n\nIf the problem is to solve a Dirichlet boundary value problem, the Green's function should be chosen such that \"G(x,x')\" vanishes when either \"x\" or \"x\"′ is on the bounding surface. Thus only one of the two terms in the surface integral remains. If the problem is to solve a Neumann boundary value problem, the Green's function is chosen such that its normal derivative vanishes on the bounding surface, as it would seem to be the most logical choice. (See Jackson J.D. classical electrodynamics, page 39). However, application of Gauss's theorem to the differential equation defining the Green's function yields\nmeaning the normal derivative of \"G(x,x')\" cannot vanish on the surface, because it must integrate to 1 on the surface. (Again, see Jackson J.D. classical electrodynamics, page 39 for this and the following argument).\n\nThe simplest form the normal derivative can take is that of a constant, namely 1/\"S\", where \"S\" is the surface area of the surface. \nThe surface term in the solution becomes\nwhere formula_86 is the average value of the potential on the surface. This number is not known in general, but is often unimportant, as the goal is often to obtain the electric field given by the gradient of the potential, rather than the potential itself.\n\nWith no boundary conditions, the Green's function for the Laplacian (Green's function for the three-variable Laplace equation) is\n\nSupposing that the bounding surface goes out to infinity and plugging in this expression for the Green's function finally yields the standard expression for electric potential in terms of electric charge density as\n\nExample. Find the Green function for the following problem, whose Green's function number is X11:\n\nFirst step: The Green's function for the linear operator at hand is defined as the solution to\n\nIf formula_90, then the delta function gives zero, and the general solution is\n\nFor formula_92 implies\n\nFor formula_96, the boundary condition at formula_97 implies\n\nThe equation of formula_99 is skipped for similar reasons.\n\nTo summarize the results thus far:\n\nEnsuring continuity in the Green's function at formula_102 implies\n\nOne can ensure proper discontinuity in the first derivative by integrating the defining differential equation from formula_104 to formula_105 and taking the limit as formula_106 goes to zero:\n\nThe two (dis)continuity equations can be solved for formula_108 and formula_101 to obtain\n\nSo the Green's function for this problem is:\n\n\n\n\n"}
{"id": "498727", "url": "https://en.wikipedia.org/wiki?curid=498727", "title": "Growth rate (group theory)", "text": "Growth rate (group theory)\n\nIn mathematics, the growth rate of a group with respect to a symmetric generating set describes the size of balls in the group. Every element in the group can be written as a product of generators, and the growth rate counts the number of elements that can be written as a product of length \"n\".\n\nSuppose \"G\" is a finitely generated group; and \"T\" is a finite \"symmetric\" set of generators\n(symmetric means that if formula_1 then formula_2).\nAny element formula_3 can be expressed as a word in the \"T\"-alphabet\n\nLet us consider the subset of all elements of \"G\" which can be presented by such a word of length ≤ \"n\"\n\nThis set is just the closed ball of radius \"n\" in the word metric \"d\" on \"G\" with respect to the generating set \"T\":\n\nMore geometrically, formula_7 is the set of vertices in the Cayley graph with respect to \"T\" which are within distance \"n\" of the identity.\n\nGiven two nondecreasing positive functions \"a\" and \"b\" one can say that they are equivalent (formula_8) if there is a constant \"C\" such that\n\nfor example formula_10 if formula_11.\n\nThen the growth rate of the group \"G\" can be defined as the corresponding equivalence class of the function\nwhere formula_13 denotes the number of elements in the set formula_7. Although the function formula_15 depends on the set of generators \"T\" its rate of growth does not (see below) and therefore the rate of growth gives an invariant of a group.\n\nThe word metric \"d\" and therefore sets formula_7 depend on the generating set \"T\". However, any two such metrics are \"bilipschitz\" \"equivalent\" in the following sense: for finite symmetric generating sets \"E\", \"F\", there is a positive constant \"C\" such that\nAs an immediate corollary of this inequality we get that the growth rate does not depend on the choice of generating set.\n\nIf\n\nfor some formula_19 we say that \"G\" has a polynomial growth rate.\nThe infimum formula_20 of such \"ks is called the order of polynomial growth\"'.\nAccording to Gromov's theorem, a group of polynomial growth is a virtually nilpotent group, i.e. it has a nilpotent subgroup of finite index. In particular, the order of polynomial growth formula_20 has to be a natural number and in fact formula_22.\n\nIf formula_23 for some formula_24 we say that \"G\" has an exponential growth rate.\nEvery finitely generated \"G\" has at most exponential growth, i.e. for some formula_25 we have formula_26.\n\nIf formula_15 grows more slowly than any exponential function, \"G\" has a subexponential growth rate. Any such group is amenable.\n\n\n\n"}
{"id": "19349240", "url": "https://en.wikipedia.org/wiki?curid=19349240", "title": "Guarded logic", "text": "Guarded logic\n\nGuarded logic is a choice set of dynamic logic involved in choices, where outcomes are limited.\n\nA simple example of guarded logic is as follows: if X is true, then Y, else Z can be expressed in dynamic logic as (X?;Y)∪(~X?;Z). This shows a guarded logical choice: if X holds, then X?;Y is equal to Y, and ~X?;Z is blocked, and a ∪block is also equal to Y. Hence, when X is true, the primary performer of the action can only take the Y branch, and when false the Z branch.\n\nA real-world example is the idea of paradox: something cannot be both true and false. A guarded logical choice is one where any change in true affects all decisions made down the line.\n\nBefore the use of guarded logic there were two major terms used to interpret modal logic. Mathematical logic and database theory (Artificial Intelligence) were first-order predicate logic. Both terms found sub-classes of first-class logic and efficiently used in solvable languages which can be used for research. But neither could explain powerful fixed-point extensions to modal style logics.\n\nLater Moshe Y. Vardi made a conjecture that a tree model would work for many modal style logics. The guarded fragment of first-order logic was first introduced by Hajnal Andréka, István Németi and Johan van Benthem in their article Modal languages and bounded fragments of predicate logic. They successfully transferred key properties of description, modal, and temporal logic to predicate logic. It was found that the robust decidability of guarded logic could be generalized with a tree model property. The tree model can also be a strong indication that guarded logic extends modal framework which retains the basics of modal logics.\n\nModal logics are generally characterized by invariances under bisimulation. It also so happens that invariance under bisimulation is the root of tree model property which helps towards defining automata theory.\n\nWithin Guarded Logic there exists numerous guarded objects. The first being guarded fragment which are first-order logic of modal logic. Guarded fragments generalize modal quantification through finding relative patterns of quantification. The syntax used to denote guarded fragment is GF. Another object is guarded fixed point logic denoted μGF naturally extends guarded fragment from fixed points of least to greatest. Guarded bisimulations are objects which when analyzing guarded logic. All relations in a slightly modified standard relational algebra with guarded bisimulation and first-order definable are known as \"guarded relational algebra\". This is denoted using GRA.\n\nAlong with first-order guarded logic objects, there are objects of second-order guarded logic. It is known as Guarded Second-Order Logic and denoted GSO. Similar to second-order logic, guarded second-order logic quantifies whose range over guarded relations restrict it semantically. This is different from second-order logic which the range is restricted over arbitrary relations.\n\nLet B be a relational structure with universe \"B\" and vocabulary τ.\n\n\"i)\" A set X ⊆ B is \"guarded\" in B if there exists a ground atom α(b_1, ..., b_k) in B such that X = {b_1, ..., b_k}.\n\n\"ii)\" A τ-structure A, in particular a substructure A ⊆ B, is \"guarded\" if its universe is a guarded set in \"A\" (in \"B\").\n\n\"iii)\" A tuple (b_1, ..., b_n) ∈ B^n is \"guarded\" in B if {b_1, ..., b_n} ⊆ X for some guarded set X ⊆ B.\n\n\"iv)\" A tuple (b_1, ..., b_k) ∈ B^k is a guarded list in B if its components are pairwise distinct and {b_1, ..., b_k} is a guarded set. The empty list is taken to be a guarded list.\n\n\"v)\" A relation X ⊆ B^n is \"guarded\" if it only consists of guarded tuples.\n\nA \"guarded bisimulation\" between two τ-structures A and B is a non-empty set \"I\" of finite partial isomorphic \"f: X → Y\" from A to B such that the back and forth conditions are satisfied.\n\nBack: For every \"f: X → Y in \"I\" and for every guarded set \"Y` ⊆ B\", there exists a partial isomorphic \"g: X` → Y`\" in \"I\" such that \"f^-1\" and \"g^-1\" agree on \"Y ∩ Y`\".\n\nForth For every \"f: X → Y\" in \"I\" and for every guarded set \"X` ⊆ A\", there exists a partial isomorphic \"g: X` → Y`\" in \"I\" such that \"f\" and \"g\" agree on \"X ∩ X`\".\n"}
{"id": "37801448", "url": "https://en.wikipedia.org/wiki?curid=37801448", "title": "Ilan Sadeh", "text": "Ilan Sadeh\n\nIlan Sadeh (born June 1, 1953) is an Israeli IT theoretician, entrepreneur, and human rights activist. He holds the position of Associate Professor of Computer Sciences and Mathematics at the University for Information Science and Technology \"St. Paul The Apostole\" in Ohrid, Republic of Macedonia.\n\nSadeh was the first to claim publicly in the Israeli media that Israel has no right to be called the \"heir\" to Holocaust victims and no right to represent Holocaust survivors. According to him, Zionist leaders have little cause for pride in their actions during the Second World War – Zionist financiers withheld funds, while the JDC refused to help save Europe's Jewry, instead prioritizing the needs of the Yishuv in Palestine.\n\nThe situation in Israel brought Sadeh to the conclusion that the political system must be replaced. He entered politics and led a movement in behalf of Holocaust survivors. He published a few articles in Israeli newspapers and had a public impact. Sadeh was elected a representative of that community and ran in the preliminary election of the Labour Party for the Knesset, or Israeli Parliament (1996), but was not elected. Following his activities, Sadeh was recently threatened and accused of being a traitor. Sadeh has taken libel action over the charges in Israeli Court (2011).\n\n\nIlan Sadeh has had pioneering results in a few research and developments fields: \"Smart camera,\" a long time before September 11 events, and \"Homeland Security\" projects, New Video compression, military applications for surveillance, seismic data processing and others.\n\nSadeh has established three start up companies: Meitav, Israel (1982), Visnet (1996) and Vipeg (2000). He has been intensively involved in establishing and R&D of new start-up companies, establishing the infrastructure, dealing with intellectual property issues, managing all activities, raising funding, coordinating consortium in EU FP5 FP6 programs.\n\nHowever, being unable to compete with the \"Fat Cat\" companies, bureaucracy, civil industry and the military establishment in Israel, as well as with the European Companies that were promoting only MPEG4, Sadeh could not raise government support nor get the support of the Israeli Army, temporarily left Israel in 2006 and moved to Macedonia in 2011.\n\n\n\"IEEE Computer Society Data Compression Committee on Computer Communications\" 3, pp. 148–158 (1993). Universal algorithms for data compression.\n\"Journal of Applied Mathematics and Computer Science 5\" (1), pp. 139–169 (1995).\n\nHe presented performance analysis based on LDT (Large Deviations Theory) and presented the trade-off between compression rate, distortion level and probability of error.\n\"Journal of Applied Mathematics and Computer Science 5\" (4), pp. 717–742 (1995).\n\nConvergence Theorems of Universal algorithms for data compression.\n\"Journal of Applied Mathematics and Computer Science 6\" (1), pp. 101–114 (1996).\n\nThe exact bound relations between rates, distortion levels in multiple description system. The results are expansions of Shannon's bounds for multiterminal network.\n\"Journal of Applied Mathematics and Computer Science\" December 1995.\n\nPresented sub-optimal universal coding schemes for voice coding.\n\"Probability in the Engineering and Informational Sciences\"\n\nEditor: Sheldon Ross, Cambridge University Press, 12 1998 pp. 189–210.\nPresented the first application of Large Deviation Theory approach to the asymptotic expansions of Shannon's bounds.\n\n\"Probability in the Engineering and Informational Sciences\"\n\nEditor: Sheldon Ross, Cambridge University Press,\n\nHe was the first to generalize Shannon McMillan Breiman Theorem (Lossy AEP).\n\nHe found important Limit Theorems. These theorems were \"re-invented\" by a member of the \"Israeli Clique\".\n\n\"Image Processing and Communications\", March 1996.\n\nPresented sub-optimal universal coding schemes for video coding.\n\"Computers and Mathematics with Applications\", February 1996.\n\nPresented a novel method for Image Coding based on Polynomial approximation of images. Theoretical and practical results were presented.\n\n\"Image Processing and Communications\", March 1996.\n\nMore theoretical and practical results about Image Coding based on Polynomial approximation of images.\n\nPh.D. Dissertation, School of Mathematical Sciences, Tel Aviv University, June 1993.\n\nHe found theoretical bounds on parallel computation of multivariate polynomial.\n\"Computers and Mathematics with Applications\", September 1996, pages 57–72\n\nHe found important Limit Theorems for Approximate String Matching for data compression and practical sub optimal results.\n\n\nPresentation of Large Deviation Theory approach to the asymptotic expansions of Shannon's data compression bounds.\n\"Journal of Applied Mathematics and Computer Science\" 1996 pp. 123–136\n\nHe presented new limit theorems for multiterminal systems and presented a new approach to the degraded diversity system problem.\n\n\"Computers and Mathematics with Applications\", February 1996\n\nNew theoretical and practical results about Image Coding based on Polynomial approximation of images.\nProceedings of the conference on Information Sciences and Systems 1992 Princeton University\nConference paper – New theoretical and practical results about Image Coding based on Polynomial approximation of images.\nConference paper – he showed by using the extended Kac's Lemma, that the compression rate, asymptotically achieved by the \"Sadeh Algorithm\", converges in probability to Shannon's bound. The algorithm has been patented in the USA and Israel.\nProceedings of the IEEE Information Theory Conference 1995 Vancouver Canada, 196.\nPresentation in Conference of First Large Deviation Theory approach to the asymptotic expansions of Shannon's data compression bounds.\nConference paper – I have shown that the compression rate, asymptotically achieved by the \"Sadeh Algorithm\", converges in probability to Shannon's bound. \nProceedings of the Conference on Control and Information at Hong Kong Chinese University Press. 1995 pp. 305–310\nPresentation in Conference of Large Deviation Theory approach to the asymptotic expansions of Shannon's theoretical bounds.\nUS patent 5836003\nHe had shown that the compression rate, asymptotically achieved by the \"Sadeh Algorithm\", converges in probability to Shannon's bound and showed suboptimal applications.\nHe have shown that the compression rate, asymptotically achieved by the \"Sadeh Algorithm\", converges in probability to Shannon's bound and showed suboptimal applications.\nVideo and Voice coding algorithms.\nA method and means for Tank Navigation. The method is operational even in severe electromagnetic environments, based on Sadeh's experience as Armored Forces Officer in Israel Army.\n"}
{"id": "52000501", "url": "https://en.wikipedia.org/wiki?curid=52000501", "title": "Inexact differential equation", "text": "Inexact differential equation\n\nAn inexact differential equation is a differential equation of the form\n\nThe solution to such equations came with the invention of the integrating factor by Leonhard Euler in 1739.\n\nIn order to solve the equation, we need to transform it into an exact differential equation. In order to do that, we need to find an integrating factor formula_2 to multiply the equation by. We'll start with the equation itself.formula_3, so we get formula_4. We will require formula_2 to satisfy formula_6. We get formula_7. After simplifying we get formula_8. Since this is a partial differential equation, it is mostly extremely hard to solve, however in most cases we will get either formula_9 or formula_10, in which case we only need to find formula_2 with a first-order linear differential equation or a separable differential equation, and as such either formula_12 or formula_13.\n\n"}
{"id": "45715334", "url": "https://en.wikipedia.org/wiki?curid=45715334", "title": "Josiah Willard Gibbs Lectureship", "text": "Josiah Willard Gibbs Lectureship\n\nThe Josiah Willard Gibbs Lectureship of the American Mathematical Society is an annually awarded mathematical prize, named in honor of Josiah Willard Gibbs. The prize is intended not only for mathematicians, but also for physicists, chemists, biologists, physicians, and other scientists who have made important applications of mathematics. The purpose of the prize is to recognize outstanding achievement in applied mathematics and \"to enable the public and the academic community to become aware of the contribution that mathematics is making to present-day thinking and to modern civilization.\"\n\nThe prize winner gives a lecture, which is subsequently published in the Bulletin of the American Mathematical Society.\n\n"}
{"id": "22074819", "url": "https://en.wikipedia.org/wiki?curid=22074819", "title": "Journal of Applied Mathematics and Mechanics", "text": "Journal of Applied Mathematics and Mechanics\n\nThe Journal of Applied Mathematics and Mechanics, also known as Zeitschrift für Angewandte Mathematik und Mechanik or ZAMM is a peer-reviewed academic journal dedicated to applied mathematics, the journal of the Gesellschaft für Angewandte Mathematik und Mechanik. It is published monthly by Wiley-VCH.\n\nThe journal's first issue appeared in 1921, published by the Verein Deutscher Ingenieure and edited by Richard von Mises.\n"}
{"id": "10161645", "url": "https://en.wikipedia.org/wiki?curid=10161645", "title": "Kazhdan–Lusztig polynomial", "text": "Kazhdan–Lusztig polynomial\n\nIn the mathematical field of representation theory, a Kazhdan–Lusztig polynomial formula_1 is a member of a family of integral polynomials introduced by . They are indexed by pairs of elements \"y\", \"w\" of a Coxeter group \"W\", which can in particular be the Weyl group of a Lie group.\n\nIn the spring of 1978 Kazhdan and Lusztig were studying Springer representations of the Weyl group of an algebraic group on formula_2-adic cohomology groups related to unipotent conjugacy classes. They found a new construction of these representations over the complex numbers . The representation had two natural bases, and the transition matrix between these two bases is essentially given by the Kazhdan–Lusztig polynomials. The actual Kazhdan–Lusztig construction of their polynomials is more elementary. Kazhdan and Lusztig used this to construct a canonical basis in the Hecke algebra of the Coxeter group and its representations. \n\nIn their first paper Kazhdan and Lusztig mentioned that their polynomials were related to the failure of local Poincaré duality for Schubert varieties. In they reinterpreted this in terms of the intersection cohomology of Mark Goresky and Robert MacPherson, and gave another definition of such a basis in terms of the dimensions of certain intersection cohomology groups.\n\nThe two bases for the Springer representation reminded Kazhdan and Lusztig of the two bases for the Grothendieck group of certain infinite dimensional representations of semisimple Lie algebras, given by Verma modules and simple modules. This analogy, and the work of Jens Carsten Jantzen and Anthony Joseph relating primitive ideals of enveloping algebras to representations of Weyl groups, led to the Kazhdan–Lusztig conjectures. \nFix a Coxeter group \"W\" with generating set \"S\", and write formula_3 for the length of an element \"w\" (the smallest length of an expression for \"w\" as a product of elements of \"S\"). The Hecke algebra of \"W\" has a basis of elements formula_4 for formula_5 over the ring formula_6, with multiplication defined by\n\nThe quadratic second relation implies that each generator is invertible in the Hecke algebra, with inverse . These inverses satisfy the relation (obtained by multiplying the quadratic relation for by \"q\"), and also the braid relations. From this it follows that the Hecke algebra has an automorphism \"D\" that sends \"q\" to \"q\" and each to \"T\". More generally one has formula_8; also \"D\" can be seen to be an involution.\n\nThe Kazhdan–Lusztig polynomials \"P\"(\"q\") are indexed by a pair of elements \"y\", \"w\" of \"W\", and uniquely determined by the following properties. \n\nTo establish existence of the Kazhdan–Lusztig polynomials, Kazhdan and Lusztig gave a simple recursive procedure for computing the polynomials \"P\"(\"q\") in terms of more elementary polynomials denoted \"R\"(\"q\"). defined by\n\nThey can be computed using the recursion relations\n\nThe Kazhdan–Lusztig polynomials can then be computed recursively using the relation\n\nusing the fact that the two terms on the left are polynomials in \"q\" and \"q\" without constant terms. These formulas are tiresome to use by hand for rank greater than about 3, but are well adapted for computers, and the only limit on computing Kazhdan–Lusztig polynomials with them is that for large rank the number of such polynomials exceeds the storage capacity of computers.\n\n\nThe Kazhdan–Lusztig polynomials arise as transition coefficients between their canonical basis and the natural basis of the Hecke algebra. The \"Inventiones\" paper also put forth two equivalent conjectures, known now as Kazhdan–Lusztig conjectures, which related the values of their polynomials at 1 with representations of complex semisimple Lie groups and Lie algebras, addressing a long-standing problem in representation theory. \n\nLet \"W\" be a finite Weyl group. For each w ∈ \"W\" denote by be the Verma module of highest weight where ρ is the half-sum of positive roots (or Weyl vector), and let be its irreducible quotient, the simple highest weight module of highest weight . Both and are locally-finite weight modules over the complex semisimple Lie algebra \"g\" with the Weyl group \"W\", and therefore admit an algebraic character. Let us write ch(\"X\") for the character of a \"g\"-module \"X\". The Kazhdan-Lusztig conjectures state:\n\nwhere is the element of maximal length of the Weyl group.\n\nThese conjectures were proved independently by and by . The methods introduced in the course of the proof have guided development of representation theory throughout the 1980s and 1990s, under the name \"geometric representation theory\".\n\n1. The two conjectures are known to be equivalent. Moreover, Borho–Jantzen's translation principle implies that can be replaced by for any dominant integral weight . Thus, the Kazhdan-Lusztig conjectures describe the Jordan–Hölder multiplicities of Verma modules in any regular integral block of Bernstein–Gelfand–Gelfand category O. \n\n2. A similar interpretation of \"all\" coefficients of Kazhdan–Lusztig polynomials follows from the \"Jantzen conjecture\", which roughly says that individual coefficients of are multiplicities of in certain subquotient of the Verma module determined by a canonical filtration, the Jantzen filtration. The Jantzen conjecture in regular integral case was proved in a later paper of .\n\n3. David Vogan showed as a consequence of the conjectures that\n\nand that vanishes if is odd, so the dimensions of all such Ext groups in category \"O\" are determined in terms of coefficients of Kazhdan–Lusztig polynomials. This result demonstrates that all coefficients of the Kazhdan–Lusztig polynomials of a finite Weyl group are non-negative integers. However, positivity for the case of a finite Weyl group \"W\" was already known from the interpretation of coefficients of the Kazhdan–Lusztig polynomials as the dimensions of intersection cohomology groups, irrespective of the conjectures. Conversely, the relation between Kazhdan–Lusztig polynomials and the Ext groups theoretically can be used to prove the conjectures, although this approach to proving them turned out to be more difficult to carry out.\n\n4. Some special cases of the Kazhdan–Lusztig conjectures are easy to verify. For example, \"M\" is the antidominant Verma module, which is known to be simple. This means that \"M\" = \"L\", establishing the second conjecture for \"w\" = 1, since the sum reduces to a single term. On the other hand, the first conjecture for \"w\" = \"w\" follows from the Weyl character formula and the formula for the character of a Verma module, together with the fact that all Kazhdan–Lusztig polynomials formula_18 are equal to 1.\n\n5. Kashiwara (1990) proved a generalization of the Kazhdan–Lusztig conjectures to symmetrizable Kac–Moody algebras.\n\nBy the Bruhat decomposition the space \"G\"/\"B\" of the algebraic group \"G\" with Weyl group \"W\" is a disjoint union of affine spaces \"X\" parameterized by elements \"w\" of \"W\". The closures of these spaces are called Schubert varieties, and Kazhdan and Lusztig, following a suggestion of Deligne, showed how to express Kazhdan–Lusztig polynomials in terms of intersection cohomology groups of Schubert varieties.\n\nMore precisely, the Kazhdan–Lusztig polynomial \"P\"(\"q\") is equal to \nwhere each term on the right means: take the complex IC of sheaves whose hyperhomology is the intersection homology of the Schubert variety of \"w\" (the closure of the cell ), take its cohomology of degree , and then take the dimension of the stalk of this sheaf at any point of the cell whose closure is the Schubert variety of \"y\". The odd-dimensional cohomology groups do not appear in the sum because they are all zero.\n\nThis gave the first proof that all coefficients of Kazhdan–Lusztig polynomials for finite Weyl groups are non-negative integers.\n\nLusztig–Vogan polynomials (also called Kazhdan–Lusztig polynomials or Kazhdan–Lusztig–Vogan polynomials) were introduced in . They are analogous to Kazhdan–Lusztig polynomials, but are tailored to representations of \"real\" semisimple Lie groups, and play major role in the conjectural description of their unitary duals. Their definition is more complicated, reflecting relative complexity of representations of real groups compared to complex groups.\n\nThe distinction, in the cases directly connection to representation theory, is explained on the level of double cosets; or in other terms of actions on analogues of complex flag manifolds \"G\"/\"B\" where \"G\" is a complex Lie group and \"B\" a Borel subgroup. The original (K-L) case is then about the details of decomposing \n\na classical theme of the Bruhat decomposition, and before that of Schubert cells in a Grassmannian. The L-V case takes a real form of \"G\", a maximal compact subgroup in that semisimple group , and makes the complexification \"K\" of . Then the relevant object of study is\n\nIn March 2007, it was announced that the L-V polynomials had been calculated for the split form of \"E\".\n\nThe second paper of Kazhdan and Lusztig established a geometric setting for definition of Kazhdan–Lusztig polynomials, namely, the geometry of singularities of Schubert varieties in the flag variety. Much of the later work of Lusztig explored analogues of Kazhdan–Lusztig polynomials in the context of other natural singular algebraic varieties arising in representation theory, in particular, closures of nilpotent orbits and quiver varieties. It turned out that the representation theory of quantum groups, modular Lie algebras and affine Hecke algebras are all tightly controlled by appropriate analogues of Kazhdan–Lusztig polynomials. They admit an elementary description, but the deeper properties of these polynomials necessary for representation theory follow from sophisticated techniques of modern algebraic geometry and homological algebra, such as the use of intersection cohomology, perverse sheaves and Beilinson–Bernstein–Deligne decomposition. \n\nThe coefficients of the Kazhdan–Lusztig polynomials are conjectured to be the dimensions of some homomorphism spaces in Soergel's bimodule category. This is the only known positive interpretation of these coefficients for arbitrary Coxeter groups.\n\nCombinatorial properties of Kazhdan–Lusztig polynomials and their generalizations are a topic of active current research. Given their significance in representation theory and algebraic geometry, attempts have been undertaken to develop the theory of Kazhdan–Lusztig polynomials in purely combinatorial fashion, relying to some extent on geometry, but without reference to intersection cohomology and other advanced techniques. This has led to exciting developments in algebraic combinatorics, such as \"pattern-avoidance phenomenon\". Some references are given in the textbook of . A research monograph on the subject is .\n\n, there is no known combinatorial interpretation of all the coefficients of the Kazhdan–Lusztig polynomials (as the cardinalities of some natural sets) even for the symmetric groups, though explicit formulas exist in many special cases.\n\n\n"}
{"id": "37919722", "url": "https://en.wikipedia.org/wiki?curid=37919722", "title": "List of periodic functions", "text": "List of periodic functions\n\nThis is a list of some well-known periodic functions.\n\n\n\n\n\n\n"}
{"id": "58125816", "url": "https://en.wikipedia.org/wiki?curid=58125816", "title": "Louis Kollros", "text": "Louis Kollros\n\nLouis Kollros (7 May 1878, La Chaux-de-Fonds – 19 June 1959, Zurich) was a Swiss mathematician. From 1909 to 1948 he was a professor ordinarius of geometry at ETH Zurich.\n\nKollros, the son of a baker, was from 1896 as a student of mathematics and physics at the Zurich Polytechnikum, where he was a fellow student of Albert Einstein and Marcel Grossmann. After graduating in 1900, Kollros taught mathematics from 1900 to 1909 in secondary school in his hometown of La Chaux-de-Fonds. In 1903–1904 to 1909 he studied in Göttingen with Hermann Minkowski and David Hilbert. From 1904 to 1909 Kollross was a \"privat-docent\" (lecturer) at the University of Neuchâtel. He received his doctorate in 1905 from the University of Zurich with thesis advisor Hermann Minkowski and thesis \"Un algorithme pour l'approximation simultaneé de deux grandeurs\". At ETH Zurich, where Marcel Grossmann taught until 1927 in the same field, Kollross held from 1909 to 1948 the francophone chair of \"géométrie descriptive et de géométrie euclidienne\".\n\nIn 1940–1941 he was president of the Swiss Mathematical Society and from 1958 an honorary member of the Society. He was president of the Steiner-Schläfli committee (tasked with the publication of their works). In this role, he was co-editor of Schläfli's collected works (3 vols.,1950–1956).\n\nHe wrote biographies of Evariste Galois (1949, 24 p.) and Jakob Steiner (1947, 24 p.), which appeared in the supplements to the \"Elemente der Mathematik\" (Birkhäuser Verlag).\n\nHis doctoral students include Ferdinand Gonseth.\n\n\n"}
{"id": "3520779", "url": "https://en.wikipedia.org/wiki?curid=3520779", "title": "Melvin Hochster", "text": "Melvin Hochster\n\nMelvin Hochster (born August 2, 1943) is an eminent American mathematician, regarded as one of the leading commutative algebraists active today. He is currently the Jack E. McLaughlin Distinguished University Professor of Mathematics at the University of Michigan.\n\nHochster attended Stuyvesant High School, where he was captain of the Math Team, and received a B.A. from Harvard University. While at Harvard, he was a Putnam Fellow in 1960. He earned his Ph.D. in 1967 from Princeton University, where he wrote a dissertation under Goro Shimura characterizing the prime spectra of commutative rings. He held positions at the University of Minnesota and Purdue University before joining the faculty at Michigan in 1977. Hochster shared the 1980 Cole Prize with Michael Aschbacher, received a Guggenheim Fellowship in 1981, and has been a member of both the National Academy of Sciences and the American Academy of Arts and Sciences since 1992. In 2008, on the occasion of his 65th birthday, he was honored with a conference in Ann Arbor and with a special volume of the Michigan Mathematical Journal.\n\nHochster's work is primarily in commutative algebra, especially the study of modules over local rings. He has established classic theorems concerning Cohen-Macaulay rings, invariant theory and homological algebra. For example, the \"Hochster-Roberts\" theorem states that the invariant ring of a linearly reductive group acting on a regular ring is Cohen-Macaulay. His best-known work is on the homological conjectures, many of which he established for local rings containing a field, thanks to his proof of the existence of big Cohen-Macaulay modules and his technique of reduction to prime characteristic. His most recent work on tight closure, introduced in 1986 with Craig Huneke, has found unexpected applications throughout commutative algebra and algebraic geometry.\n\nHochster has been recognized for his efforts in mentoring and popularizing mathematics through lectures and articles. He has had more than 40 doctoral students, and the Association for Women in Mathematics has pointed out his outstanding role in mentoring women students pursuing a career in mathematics. He is currently (since 2009) serving as the chair of the department of Mathematics at the University of Michigan.\n\nHochster's hobbies are cryptic crosswords and contract bridge.\n\n\n\n"}
{"id": "1196909", "url": "https://en.wikipedia.org/wiki?curid=1196909", "title": "Mohr–Mascheroni theorem", "text": "Mohr–Mascheroni theorem\n\nIn mathematics, the Mohr–Mascheroni theorem states that any geometric construction that can be performed by a compass and straightedge can be performed by a compass alone.\n\nIt must be understood that by \"any geometric construction\", we are referring figures that contain no straight lines, as it is clearly impossible to draw a straight line without a straightedge. It is understood that a line is determined provided that two distinct points on that line are given or constructed, even though no visual representation of the line will be present. The theorem can be stated more precisely as,\n\nThough the use of a straightedge can make a construction significantly easier, the theorem shows that any set of points that fully defines a constructed figure can be determined with compass alone, and the only reason to use a straightedge is for the aesthetics of seeing straight lines, which for the purposes of construction is functionally unnecessary.\n\nThe result was originally published by Georg Mohr in 1672, but his proof languished in obscurity until 1928. The theorem was independently discovered by Lorenzo Mascheroni in 1797 and it was known as \"Mascheroni's Theorem\" until Mohr's work was rediscovered.\n\nMotivated by Mascheroni's result, in 1822 Jean Victor Poncelet conjectured a variation on the same theme. He proposed that any construction possible by straightedge and compass could be done with straightedge alone. The one stipulation though is that a single circle with its center identified must be provided. The Poncelet-Steiner theorem was proved by Jakob Steiner eleven years later.\n\nTo prove the theorem, each of the basic constructions of compass and straightedge need to be proven to be possible by using a compass alone, as these are the foundations of, or elementary steps for, all other constructions. These are:\n\n\nIt is understood that a straight line cannot be drawn without a straightedge. A line is considered to be given by any two points, as any two points define a line uniquely, and a unique line can be defined by any two points on it.\n\n\nThis can be done with compass alone quite naturally. There is nothing to prove.\n\n\nThis construction can be done directly with a compass provided the centers and radii of the two circles are known. Due to the compass-only construction of the center of a circle (given below), it can always be assumed that any circle is described by its center and radius. Indeed, some authors include this in their descriptions of the basic constructions.\n\n\nThus, to prove the theorem, there only compass-only constructions for #3 and #4 need to be given.\n\nSeveral proofs of the result are known. Mascheroni's proof of 1797 was generally based on the idea of using reflection in a line as the major tool. Mohr's solution was different. In 1890, August Adler published a proof using the inversion transformation.\n\nAn algebraic approach uses the isomorphism between the Euclidean plane and the real coordinate space formula_1. This approach can be used to provide a stronger version of the theorem. It also shows the dependence of the theorem on Archimedes' axiom (which cannot be formulated in a first-order language).\n\nThe following notation will be used throughout this article. A circle whose center is located at point and that passes through point will be denoted by . A circle with center and radius specified by a number, , or a line segment will be denoted by or , respectively.\n\nIn general constructions there are often several variations that will produce the same result. The choices made in such a variant can be made without loss of generality. However, when a construction is being used to prove that something can be done, it is not necessary to describe all these various choices and, for the sake of clarity of exposition, only one variant will be given below. However, many constructions come in different forms depending on whether or not they use circle inversion and these alternatives will be given if possible.\nTo prove the above constructions #3 and #4, which are included below, a few necessary intermediary constructions are also explained below since they are used and referenced frequently. These are also compass-only constructions. All constructions below rely on #1,#2,#5, and any other construction that is listed prior to it.\n\nThe ability to translate, or copy, a circle to a new center is vital in these proofs and fundamental to establish the veracity of the theorem. The creation of a new circle with the same radius as the first, but centered at different point, is the key feature distinguishing the collapsing compass from the modern, rigid compass. The equivalence of a collapsing compass and a rigid compass was proved by Euclid (Book I Proposition 2 of \"The Elements\") using straightedge and collapsing compass when he, essentially, constructs a copy of a circle with a different center. This equivalence can also be established with compass alone. \n\n\n\nThis construction can be repeated as often as necessary to find a point so that the length of line segment = ⋅ length of line segment for any positive integer .\n\n\nPoint is such that the radius of is to as is to the radius; or .\n\nIn the event that the above construction fails (that is, the red circle and the black circle do not intersect in two points), find a point on the line so that the length of line segment is a positive integral multiple, say , of the length of and is greater than (this is possible by Archimede's axiom). Find the inverse of in circle as above (the red and black circles must now intersect in two points). The point is now obtained by extending so that = .\n\n\nThe compass-only construction of the intersection points of a line and a circle breaks into two cases depending upon whether the center of the circle is or is not collinear with the line. \n\nAssume that center of the circle does not lie on the line.\n\n\nAn alternate construction, using circle inversion can also be given.\n\n\nThus it has been shown that all of the basic construction one can perform with a straightedge and compass can be done with a compass alone, provided that it is understood that a line cannot be literally drawn but merely defined by two points.\n\n\n\n\n"}
{"id": "35098864", "url": "https://en.wikipedia.org/wiki?curid=35098864", "title": "Narasimhan–Seshadri theorem", "text": "Narasimhan–Seshadri theorem\n\nIn mathematics, the Narasimhan–Seshadri theorem, proved by , says that a holomorphic vector bundle over a Riemann surface is stable if and only if it comes from an irreducible projective unitary representation of the fundamental group.\n\nThe main case to understand is that of topologically trivial bundles, i.e. those of degree zero (and the other cases are a minor \ntechnical extension of this case). This case of the Narasimhan–Seshadri theorem says that a degree zero holomorphic vector bundle over a Riemann surface is stable if and only if it comes from an irreducible unitary representation of the fundamental group of the Riemann surface.\n\n\n"}
{"id": "1457846", "url": "https://en.wikipedia.org/wiki?curid=1457846", "title": "Nesbitt's inequality", "text": "Nesbitt's inequality\n\nIn mathematics, Nesbitt's inequality is a special case of the Shapiro inequality. It states that for positive real numbers \"a\", \"b\" and \"c\" we have:\n\nBy the AM-HM inequality on formula_2,\nClearing denominators yields\nfrom which we obtain\nby expanding the product and collecting like denominators. This then simplifies directly to the final result.\n\nSuppose formula_6, we have that \ndefine\nThe scalar product of the two sequences is maximum because of the rearrangement inequality if they are arranged the same way, call formula_10 and formula_11 the vector formula_12 shifted by one and by two, we have:\n\nAddition yields Nesbitt's inequality.\n\nThe following identity is true for all formula_15\n\nThis clearly proves that the left side is no less than formula_17 for positive a,b and c.\n\nNote: every rational inequality can be solved by transforming it to the appropriate identity, see Hilbert's seventeenth problem.\n\nInvoking the Cauchy–Schwarz inequality on the vectors formula_18 yields\nwhich can be transformed into the final result as we did in .\n\nWe first employ a Ravi substitution: let formula_20. We then apply the AM-GM inequality to the set of six values formula_21 to obtain\nDividing by formula_23 yields\nSubstituting out the formula_25 in favor of formula_26 yields\nwhich then simplifies to the final result.\n\nTitu's lemma, a direct consequence of the Cauchy–Schwarz inequality, states that for any sequence of formula_29 real numbers formula_30 and any sequence of formula_29 positive numbers formula_32, formula_33. We use its three-term instance with formula_34-sequence formula_26 and formula_36-sequence formula_37:\nBy multiplying out all the products on the lesser side and collecting like terms, we obtain\nwhich simplifies to\nBy the rearrangement inequality, we have formula_41, so the fraction on the lesser side must be at least formula_42. Thus,\nAs the left side of the inequality is homogeneous, we may assume formula_44. Now define formula_45, formula_46, and formula_47. The desired inequality turns into formula_48, or, equivalently, formula_49. This is clearly true by Titu's Lemma.\n\n"}
{"id": "2966898", "url": "https://en.wikipedia.org/wiki?curid=2966898", "title": "Outer billiard", "text": "Outer billiard\n\nOuter billiards is a dynamical system based on a convex shape in the plane. Classically, this system is defined for the Euclidean plane but one can also consider the system in the hyperbolic plane or in other spaces that suitably generalize the plane. Outer billiards differs from a usual dynamical billiard in that it deals with a discrete sequence of moves \"outside\" the shape rather than inside of it.\n\nLet P be a convex shape in the plane.\nGiven a point x0 outside P, there is typically a unique\npoint x1 (also outside P) so that the line segment connecting x0 to x1 is tangent to P at its midpoint and\na person walking from x0 to x1 would see P on the right. (See Figure.) The map\nF: x0 -> x1 is called the \"outer billiards map\".\nThe inverse (or backwards) outer billiards map is also defined, as the map x1 -> x0. \nOne gets the inverse map simply by replacing the word \"right\" by the word \"left\" in the definition given above.\nThe figure shows the situation in the Euclidean plane, but the definition in the\nhyperbolic plane is essentially the same.\n\nAn outer billiards orbit is the set of all iterations \nof the point, namely ... x0 <--> x1 <--> x2 <--> x3 ... That is, start at x0 and\niteratively apply both the outer billiards map and the backwards outer billiards map.\nWhen P is a strictly convex shape, such as an ellipse,\nevery point in the exterior of P has a well defined orbit. When P\nis a polygon, some points might not have well-defined orbits, on account of the\npotential ambiguity of choosing the midpoint of the relevant tangent line. Nevertheless, in\nthe polygonal case, almost every point has a well-defined orbit.\n\nDefining an outer billiards system in a higher-dimensional space is beyond the scope of this article. Unlike the case of ordinary billiards, the definition is not straightforward. One natural setting for the map is a complex vector space. In this case, there is a natural choice of line tangent to a convex body at each point. One obtains these tangents by starting with the normals and using the complex structure to rotate 90 degrees. These distinguished tangent lines can be used\nto define the outer billiards map roughly as above. See S. Tabachnikov's book (cited in the references) for details.\n\nMost people attribute the introduction of outer billiards to Bernhard Neumann in the late 1950s, \nthough it seems\nthat a few people cite an earlier construction in 1945, due to M. Day. Jürgen Moser popularized the system in the 1970s as a toy model for\ncelestial mechanics. This system has been studied classically in the Euclidean plane, and more recently in\nthe hyperbolic plane. One can also consider higher-dimensional spaces, though no serious study has yet been made.\nBernhard Neumann informally posed the question as to whether or not one can\nhave unbounded orbits in an outer billiards system, and Moser put it in writing in 1973.\nSometimes this basic question has been called \"the Moser-Neumann question\".\nThis question, originally posed for shapes in the Euclidean plane and\nsolved only recently, has been a\nguiding problem in the field.\n\nIn the 70's, Jürgen Moser sketched a proof, based on K.A.M. theory, that outer \nbilliards relative to a\n6-times-differentiable shape of positive curvature has all orbits bounded.\nIn 1982, Raphael Douady gave the full proof of this result.\nA big advance in the polygonal case came over a period of several years when\nthree teams of authors, Vivaldi-Shaidenko (1987), Kolodziej (1989), and Gutkin-Simanyi (1991), each \nusing different methods,\nshowed that outer\nbilliards relative to a \"quasirational\" polygon has all orbits bounded. The notion of quasirational is technical\n(see references) but it includes the class of regular polygons and \"convex rational polygons\", \nnamely those convex polygons whose vertices have rational coordinates. In the case of rational polygons, all the orbits are\nperiodic. In 1995, Tabachnikov showed that outer billiards for the regular pentagon has some aperiodic orbits,\nthus clarifying the distinction between the dynamics in the rational and regular cases.\nIn 1996, Boyland showed that outer billiards relative to some shapes can have orbits which accumulate on\nthe shape.\nIn 2005, D. Genin showed that all orbits are bounded when the shape is a trapezoid, thus\nshowing that quasirationality is not a \"necessary\" condition for the system to have all orbits bounded.\n\nIn 2007, R. E. Schwartz showed that outer billiards has some unbounded orbits when defined\nrelative to the Penrose Kite, thus answering the original Moser-Neumann question in the affirmative.\nThe Penrose kite is the convex quadrilateral from the kites-and-darts Penrose tilings.\nSubsequently, Schwartz showed that outer billiards has unbounded orbits when defined relative\nto any irrational kite. An\n\"irrational kite\" is a quadrilateral with the following property: \nOne of the diagonals of the quadrilateral divides the region into two triangles of equal area \nand the other diagonal divides the region into two triangles whose areas are not rational multiples\nof each other.\nIn 2008, Dolgopyat-Fayad showed that outer billiards defined relative to the semidisk has\nunbounded orbits. The \"semidisk\" is the region one gets by cutting a disk in half.\nThe proof of Dolgopyat-Fayad is robust, and also works for regions obtained by\ncutting a disk nearly in half, when the word \"nearly\" is suitably interpreted.\n\nIn 2003, Dogru and Tabachnikov showed that all orbits are unbounded\nfor a certain class of convex polygons in the hyperbolic plane.\nThe authors call such polygons \"large\". \n(See the reference for the definition.) Dogru and Otten then extended this work in 2011 by specifying the conditions under which a regular polygonal table in the hyperbolic plane have all orbits unbounded, that is, are large.\n\nIn ordinary polygonal billiards, the existence of periodic\norbits is a major unsolved problem. For instance, it is unknown if every\ntriangular shaped table has a periodic billiard path. More progress has\nbeen made for outer billiards, though the situation is far from well understood.\nAs mentioned above, all the orbits are periodic when the system is defined\nrelative to a convex rational polygon in the Euclidean plane. Moreover, it is a\nrecent theorem of C. Culter (written up by S. Tabachnikov) that outer\nbilliards relative to any convex polygon has periodic orbits—in fact a\nperiodic orbit outside of any given bounded region.\n\nOuter billiards is a subject still in its beginning phase. Most problems are still unsolved. \nHere are some open problems in the area.\n\n"}
{"id": "191101", "url": "https://en.wikipedia.org/wiki?curid=191101", "title": "Phase space", "text": "Phase space\n\nIn dynamical system theory, a phase space is a space in which all possible states of a system are represented, with each possible state corresponding to one unique point in the phase space. For mechanical systems, the phase space usually consists of all possible values of position and momentum variables. The concept of phase space was developed in the late 19th century by Ludwig Boltzmann, Henri Poincaré, and Willard Gibbs.\n\nIn a phase space, every degree of freedom or parameter of the system is represented as an axis of a multidimensional space; a one-dimensional system is called a phase line, while a two-dimensional system is called a phase plane. For every possible state of the system, or allowed combination of values of the system's parameters, a point is included in the multidimensional space. The system's evolving state over time traces a path (a phase space trajectory for the system) through the high-dimensional space. The phase space trajectory represents the set of states compatible with starting from one particular initial condition, located in the full phase space that represents the set of states compatible with starting from \"any\" initial condition. As a whole, the phase diagram represents all that the system can be, and its shape can easily elucidate qualities of the system that might not be obvious otherwise. A phase space may contain a great number of dimensions. For instance, a gas containing many molecules may require a separate dimension for each particle's \"x\", \"y\" and \"z\" positions and momenta (6 dimensions for an idealized monatomic gas), and for more complex molecular systems additional dimensions are required to describe vibrational modes of the molecular bonds, as well as spin around 3 axes. Phase spaces are easier to use when analyzing behavior of mechanical systems restricted to motion around and along various axes of rotation or translation - e.g. in robotics, like analyzing the range of motion of a robotic arm or determining the optimal path to achieve a particular position/momentum result.\n\nIn classical mechanics, any choice of generalized coordinates \"q\" for the position (i.e. coordinates on configuration space) defines conjugate generalized momenta \"p\" which together define co-ordinates on phase space. More abstractly, in classical mechanics phase space is the cotangent bundle of configuration space, and in this interpretation the procedure above expresses that a choice of local coordinates on configuration space induces a choice of natural local Darboux coordinates for the standard symplectic structure on a cotangent space.\n\nThe motion of an ensemble of systems in this space is studied by classical statistical mechanics. The local density of points in such systems obeys Liouville's Theorem, and so can be taken as constant. Within the context of a model system in classical mechanics, the phase space coordinates of the system at any given time are composed of all of the system's dynamic variables. Because of this, it is possible to calculate the state of the system at any given time in the future or the past, through integration of Hamilton's or Lagrange's equations of motion.\n\nFor simple systems, there may be as few as one or two degrees of freedom. One degree of freedom occurs when one has an autonomous ordinary differential equation in a single variable, formula_1 with the resulting one-dimensional system being called a phase line, and the qualitative behaviour of the system being immediately visible from the phase line. The simplest non-trivial examples are the exponential growth model/decay (one unstable/stable equilibrium) and the logistic growth model (two equilibria, one stable, one unstable).\n\nThe phase space of a two-dimensional system is called a phase plane, which occurs in classical mechanics for a single particle moving in one dimension, and where the two variables are position and velocity. In this case, a sketch of the phase portrait may give qualitative information about the dynamics of the system, such as the limit cycle of the Van der Pol oscillator shown in the diagram.\n\nHere, the horizontal axis gives the position and vertical axis the velocity. As the system evolves, its state follows one of the lines (trajectories) on the phase diagram.\nClassic examples of phase diagrams from chaos theory are :\n\nA plot of position and momentum variables as a function of time is sometimes called a phase plot or a phase diagram. Phase diagram, however, is more usually reserved in the physical sciences for a diagram showing the various regions of stability of the thermodynamic phases of a chemical system, which consists of pressure, temperature, and composition.\n\nIn quantum mechanics, the coordinates \"p\" and \"q\" of phase space normally become hermitian operators in a Hilbert space.\n\nBut they may alternatively retain their classical interpretation, provided functions of them compose in novel algebraic ways (through Groenewold's 1946 star product), consistent with the uncertainty principle of quantum mechanics. \nEvery quantum mechanical observable corresponds to a unique function or distribution on phase space, and vice versa, as specified by Hermann Weyl (1927) and supplemented by John von Neumann (1931); Eugene Wigner (1932); and, in a grand synthesis, by H J Groenewold (1946). \nWith J E Moyal (1949), these completed the foundations of the phase space formulation of quantum mechanics, a complete and logically autonomous reformulation of quantum mechanics. (Its modern abstractions include deformation quantization and geometric quantization.)\nExpectation values in phase-space quantization are obtained isomorphically to tracing operator observables with the density matrix in Hilbert space: they are obtained by phase-space integrals of observables, with the Wigner quasi-probability distribution effectively serving as a measure.\n\nThus, by expressing quantum mechanics in phase space (the same ambit as for classical mechanics), the Weyl map facilitates recognition of quantum mechanics as a deformation (generalization) of classical mechanics, with deformation parameter \"ħ/S\", where \"S\" is the action of the relevant process. (Other familiar deformations in physics involve the deformation of classical Newtonian into relativistic mechanics, with deformation parameter \"v\"/\"c\"; or the deformation of Newtonian gravity into General Relativity, with deformation parameter Schwarzschild radius/characteristic-dimension.)\n\nClassical expressions, observables, and operations (such as Poisson brackets) are modified by ħ-dependent quantum corrections, as the conventional commutative multiplication applying in classical mechanics is generalized to the noncommutative star-multiplication characterizing quantum mechanics and underlying its uncertainty principle.\n\nIn thermodynamics and statistical mechanics contexts, the term phase space has two meanings: It is used in the same sense as in classical mechanics. If a thermodynamic system consists of \"N\" particles, then a point in the 6\"N\"-dimensional phase space describes the dynamic state of every particle in that system, as each particle is associated with three position variables and three momentum variables. In this sense, as long as the particles are distinguishable, a point in phase space is said to be a microstate of the system. (For indistinguishable particles a microstate will consist of a set of \"N\"<nowiki>!</nowiki> points, corresponding to all possible exchanges of the \"N\" particles.) \"N\" is typically on the order of Avogadro's number, thus describing the system at a microscopic level is often impractical. This leads to the use of phase space in a different sense.\n\nThe phase space can also refer to the space that is parametrized by the \"macroscopic\" states of the system, such as pressure, temperature, etc. For instance, one may view the pressure-volume diagram or entropy-temperature diagrams as describing part of this phase space. A point in this phase space is correspondingly called a macrostate. There may easily be more than one microstate with the same macrostate. For example, for a fixed temperature, the system could have many dynamic configurations at the microscopic level. When used in this sense, a phase is a region of phase space where the system in question is in, for example, the liquid phase, or solid phase, etc.\n\nSince there are many more microstates than macrostates, the phase space in the first sense is usually a manifold of much larger dimensions than in the second sense. Clearly, many more parameters are required to register every detail of the system down to the molecular or atomic scale than to simply specify, say, the temperature or the pressure of the system.\n\nPhase space is extensively used in nonimaging optics, the branch of optics devoted to illumination. It is also an important concept in Hamiltonian optics.\n\nIn classical statistical mechanics (continuous energies) the concept of phase space provides a classical analog to the partition function (sum over states) known as the phase integral. Instead of summing the Boltzmann factor over discretely spaced energy states (defined by\nappropriate integer quantum numbers for each degree of freedom) one may integrate over continuous phase space. Such integration essentially consists of two parts: integration of the momentum component of all degrees of freedom (momentum space) and integration of the position component of all degrees of freedom (configuration space). Once the phase integral is known, it may be related to the classical partition function by multiplication of a normalization constant representing the number of quantum energy states per unit phase space. This normalization constant is simply the inverse of Planck's constant raised to a power equal to the number of degrees of freedom for the system.\n\n\n"}
{"id": "213996", "url": "https://en.wikipedia.org/wiki?curid=213996", "title": "Polygonal number", "text": "Polygonal number\n\nIn mathematics, a polygonal number is a number represented as dots or pebbles arranged in the shape of a regular polygon. The dots are thought of as alphas (units). These are one type of 2-dimensional figurate numbers.\n\nThe number 10 for example, can be arranged as a triangle (see triangular number):\n\nBut 10 cannot be arranged as a square. The number 9, on the other hand, can be (see square number):\n\nSome numbers, like 36, can be arranged both as a square and as a triangle (see square triangular number):\n\nBy convention, 1 is the first polygonal number for any number of sides. The rule for enlarging the polygon to the next size is to extend two adjacent arms by one point and to then add the required extra sides between those points. In the following diagrams, each extra layer is shown as in red.\n\nPolygons with higher numbers of sides, such as pentagons and hexagons, can also be constructed according to this rule, although the dots will no longer form a perfectly regular lattice like above.\n\nIf is the number of sides in a polygon, the formula for the th -gonal number is\n\nor\n\nThe th -gonal number is also related to the triangular numbers as follows:\n\nThus:\n\nFor a given -gonal number , one can find by\n\nApplying the formula above:\n\nto the case of 6 sides gives:\n\nbut since:\n\nit follows that:\n\nThis shows that the th hexagonal number is also the th triangular number . We can find every hexagonal number by simply taking the odd-numbered triangular numbers:\n\nThe first 6 values in the column \"sum of reciprocals\", for triangular to octagonal numbers, come from a published solution to the general problem, which also gives a general formula for any number of sides, in terms of the digamma function.\n\nThe On-Line Encyclopedia of Integer Sequences eschews terms using Greek prefixes (e.g., \"octagonal\") in favor of terms using numerals (i.e., \"8-gonal\").\n\nA property of this table can be expressed by the following identity (see ):\n\nwith\n\nSome numbers, such as 36 which is both square and triangular, fall into two polygonal sets. The problem of determining, given two such sets, all numbers that belong to both can be solved by reducing the problem to Pell's equation. The simplest example of this is the sequence of square triangular numbers.\n\nThe following table summarizes the set of -gonal -gonal numbers for small values of and .\n\nIn some cases, such as and , there are no numbers in both sets other than 1.\n\nThe problem of finding numbers that belong to three polygonal sets is more difficult. A computer search for pentagonal square triangular numbers has yielded only the trivial value of 1, though a proof that there are no other such numbers has yet to be found.\n\nThe number 1225 is hecatonicositetragonal (), hexacontagonal (), icosienneagonal (), hexagonal, square, and triangular.\n\nThe only polygonal set that is contained entirely in another polygonal set is the set of hexagonal numbers, which is contained in the set of triangular numbers.\n\n\n\n"}
{"id": "39291", "url": "https://en.wikipedia.org/wiki?curid=39291", "title": "Postcondition", "text": "Postcondition\n\nIn computer programming, a postcondition is a condition or predicate that must always be true just after the execution of some section of code or after an operation in a formal specification. Postconditions are sometimes tested using assertions within the code itself. Often, postconditions are simply included in the documentation of the affected section of code.\n\nFor example: The result of a factorial is always an integer and greater than or equal to 1. So a program that calculates the factorial of an input number would have postconditions that the result after the calculation be an integer and that it be greater than or equal to 1. Another example: a program that calculates the square root of an input number might have the postconditions that the result be a number and that its square be equal to the input.\n\nIn some software design approaches, postconditions, along with preconditions and class invariants, are components of the software construction method design by contract.\n\nThe postcondition for any routine is a declaration of the properties which are guaranteed upon completion of the routine's execution. As it relates to the routine's contract, the postcondition offers assurance to potential callers that in cases in which the routine is called in a state in which its precondition holds, the properties declared by the postcondition are assured.\n\nThe following example written in Eiffel sets the value of a class attribute codice_1 based on a caller-provided argument codice_2. The postcondition follows the keyword codice_3. In this example, the postcondition guarantees, in cases in which the precondition holds (i.e., when codice_2 represents a valid hour of the day), that after the execution of codice_5, the class attribute codice_1 will have the same value as codice_2. The tag \"codice_8\" describes this postcondition clause and serves to identify it in case of a runtime postcondition violation.\n\nIn the presence of inheritance, the routines inherited by descendant classes (subclasses) do so with their contracts, that is their preconditions and postconditions, in force. This means that any implementations or redefinitions of inherited routines also have to be written to comply with their inherited contracts. Postconditions can be modified in redefined routines, but they may only be strengthened. That is, the redefined routine may increase the benefits it provides to the client, but may not decrease those benefits.\n\n"}
{"id": "66193", "url": "https://en.wikipedia.org/wiki?curid=66193", "title": "Prefix code", "text": "Prefix code\n\nA prefix code is a type of code system (typically a variable-length code) distinguished by its possession of the \"prefix property\", which requires that there is no whole code word in the system that is a prefix (initial segment) of any other code word in the system. For example, a code with code words {9, 55} has the prefix property; a code consisting of {9, 5, 59, 55} does not, because \"5\" is a prefix of \"59\" and also of \"55\". A prefix code is a uniquely decodable code: given a complete and accurate sequence, a receiver can identify each word without requiring a special marker between words. However, there are uniquely decodable codes that are not prefix codes; for instance, the reverse of a prefix code is still uniquely decodable (it is a suffix code), but it is not necessarily a prefix code.\n\nPrefix codes are also known as prefix-free codes, prefix condition codes and instantaneous codes. Although Huffman coding is just one of many algorithms for deriving prefix codes, prefix codes are also widely referred to as \"Huffman codes\", even when the code was not produced by a Huffman algorithm. The term comma-free code is sometimes also applied as a synonym for prefix-free codes but in most mathematical books and articles (e.g.) a comma-free code is used to mean a self-synchronizing code, a subclass of prefix codes.\n\nUsing prefix codes, a message can be transmitted as a sequence of concatenated code words, without any out-of-band markers or (alternatively) special markers between words to frame the words in the message. The recipient can decode the message unambiguously, by repeatedly finding and removing sequences that form valid code words. This is not generally possible with codes that lack the prefix property, for example {0, 1, 10, 11}: a receiver reading a \"1\" at the start of a code word would not know whether that was the complete code word \"1\", or merely the prefix of the code word \"10\" or \"11\"; so the string \"10\" could be interpreted either as a single codeword or as the concatenation of the words \"1\" then \"0\".\n\nThe variable-length Huffman codes, country calling codes, the country and publisher parts of ISBNs, the Secondary Synchronization Codes used in the UMTS W-CDMA 3G Wireless Standard, and the instruction sets (machine language) of most computer microarchitectures are prefix codes.\n\nPrefix codes are not error-correcting codes. In practice, a message might first be compressed with a prefix code, and then encoded again with channel coding (including error correction) before transmission.\n\nFor any uniquely decodable code there is a prefix code that has the same code word lengths. Kraft's inequality characterizes the sets of code word lengths that are possible in a uniquely decodable code.\n\nIf every word in the code has the same length, the code is called a fixed-length code, or a block code (though the term block code is also used for fixed-size error-correcting codes in channel coding). For example, ISO 8859-15 letters are always 8 bits long. UTF-32/UCS-4 letters are always 32 bits long. ATM cells are always 424 bits (53 bytes) long. A fixed-length code of fixed length \"k\" bits can encode up to formula_1 source symbols.\n\nA fixed-length code is necessarily a prefix code. It is possible to turn any code into a fixed-length code by padding fixed symbols to the shorter prefixes in order to meet the length of the longest prefixes. Alternately, such padding codes may be employed to introduce redundancy that allows autocorrection and/or synchronisation. However, fixed length encodings are inefficient in situations where some words are much more likely to be transmitted than others.\n\nTruncated binary encoding is a straightforward generalization of fixed-length codes to deal with cases where the number of symbols \"n\" is not a power of two. Source symbols are assigned codewords of length \"k\" and \"k\"+1, where \"k\" is chosen so that \"2 < n ≤ 2\".\n\nHuffman coding is a more sophisticated technique for constructing variable-length prefix codes. The Huffman coding algorithm takes as input the frequencies that the code words should have, and constructs a prefix code that minimizes the weighted average of the code word lengths. (This is closely related to minimizing the entropy.) This is a form of lossless data compression based on entropy encoding.\n\nSome codes mark the end of a code word with a special \"comma\" symbol, different from normal data. This is somewhat analogous to the spaces between words in a sentence; they mark where one word ends and another begins. If every code word ends in a comma, and the comma does not appear elsewhere in a code word, the code is automatically prefix-free. However, modern communication systems send everything as sequences of \"1\" and \"0\" – adding a third symbol would be expensive, and using it only at the ends of words would be inefficient. Morse code is an everyday example of a variable-length code with a comma. The long pauses between letters, and the even longer pauses between words, help people recognize where one letter (or word) ends, and the next begins. Similarly, Fibonacci coding uses a \"11\" to mark the end of every code word.\n\nSelf-synchronizing codes are prefix codes that allow frame synchronization.\n\nA suffix code is a set of words none of which is a suffix of any other; equivalently, a set of words which are the reverse of a prefix code. As with a prefix code, the representation of a string as a concatenation of such words is unique. A bifix code is a set of words which is both a prefix and a suffix code.\nAn optimal prefix code is a prefix code with minimal average length. That is, assume an alphabet of symbols with probabilities formula_2 for a prefix code . If is another prefix code and formula_3 are the lengths of the codewords of , then formula_4.\n\nExamples of prefix codes include:\n\nCommonly used techniques for constructing prefix codes include Huffman codes and the earlier Shannon-Fano codes, and universal codes such as:\n\n\n\n "}
{"id": "408225", "url": "https://en.wikipedia.org/wiki?curid=408225", "title": "Principal value", "text": "Principal value\n\nIn complex analysis, the principal values of a multivalued function are the values along one chosen branch of that function, so that it is single-valued. The simplest case arises in taking the square root of a positive real number. For example, 4 has two square roots: 2 and –2; of these the positive root, 2, is considered the principal root and is denoted as formula_1\n\nConsider the complex logarithm function log \"z\". It is defined as the complex number \"w\" such that\nNow, for example, say we wish to find log i. This means we want to solve\nfor \"w\". Clearly iπ/2 is a solution. But is it the only solution?\n\nOf course, there are other solutions, which is evidenced by considering the position of i in the complex plane and in particular its argument arg \"i\". We can rotate counterclockwise π/2 radians from 1 to reach i initially, but if we rotate further another 2π we reach i again. So, we can conclude that i(π/2 + 2π) is \"also\" a solution for log i. It becomes clear that we can add any multiple of 2πi to our initial solution to obtain all values for log i.\n\nBut this has a consequence that may be surprising in comparison of real valued functions: log i does not have one definite value! For log \"z\", we have\nfor an integer \"k\", where Arg \"z\" is the (principal) argument of \"z\" defined to lie in the interval formula_5. Each value of \"k\" determines what is known as a \"branch\" (or \"sheet\"), a single-valued component of the multiple-valued log function.\n\nThe branch corresponding to \"k\"=0 is known as the \"principal branch\", and along this branch, the values the function takes are known as the \"principal values\".\n\nIn general, if \"f\"(\"z\") is multiple-valued, the principal branch of \"f\" is denoted\nsuch that for \"z\" in the domain of \"f\", pv \"f\"(\"z\") is single-valued.\n\nComplex valued elementary functions can be multiple valued over some domains. The principal value of some of these functions can be obtained by decomposing the function into simpler ones whereby the principal value of the simple functions are straightforward to obtain.\n\nWe have examined the logarithm function above, i.e., \nNow, arg \"z\" is intrinsically multivalued. One often defines the argument of some complex number to be between -π (exclusive) and π (inclusive), so we take this to be the principal value of the argument, and we write the argument function on this branch Arg \"z\" (with the leading capital A). Using Arg \"z\" instead of arg \"z\", we obtain the principal value of the logarithm, and we write\n\nFor a complex number formula_9 the principal value of the square root is: \n\nwith argument formula_11\n\nThe principal value of complex number argument measured in radians can be defined as:\n\nTo compute these values one can use functions :\n\n"}
{"id": "45486934", "url": "https://en.wikipedia.org/wiki?curid=45486934", "title": "Productive matrix", "text": "Productive matrix\n\nIn linear algebra, a square nonnegative matrix formula_1 of order formula_2 is said to be productive, or to be a Leontief matrix, if there exists a formula_3 nonnegative column matrix formula_4 such as formula_5 is a positive matrix.\n\nThe concept of productive matrix was developed by the economist Wassily Leontief (Nobel Prize in Economics in 1973) in order to model and analyze the relations between the different sectors of an economy. The interdependency linkages between the latter can be examined by the input-output model with empirical data.\n\nThe matrix formula_6 is productive if and only if formula_7 and formula_8 such as formula_9.\n\nHere formula_10 denotes the set of \"r\"×\"c\" matrices of real numbers, whereas formula_11 and formula_12 indicates a positive and a nonnegative matrix, respectively.\n\nThe following properties are proven e.g. in the textbook (Michel 1984).\n\nTheorem\nA nonnegative matrix formula_6 is productive if and only if formula_14 is invertible with a nonnegative inverse, where formula_15 denotes the formula_16 identity matrix.\n\nProof\n\n\"If\" :\n\"Only if\" :\n\nProposition\nThe transpose of a productive matrix is productive.\n\nProof\n\nWith a matrix approach of the input-output model, the consumption matrix is productive if it is economically viable and if the latter and the demand vector are nonnegative.\n"}
{"id": "55193623", "url": "https://en.wikipedia.org/wiki?curid=55193623", "title": "Quantum Computing Since Democritus", "text": "Quantum Computing Since Democritus\n\nQuantum Computing Since Democritus is a 2013 book written by Scott Aaronson. It is loosely based on a course Aaronson taught at the University of Waterloo, the lecture notes for which are available online.\n\nAaronson has stated that he intends the book to be at the same level as Leonard Susskind's \"The Theoretical Minimum\" or Roger Penrose's \"The Road to Reality\"; Physics Today compared it to George Gamow's \"One Two Three... Infinity\". The book covers everything from computer science to mathematics to quantum mechanics and quantum computing, starting, as the title indicates, with Democritus.\n\nScott Aaronson is a professor of theoretical computer science at the University of Texas at Austin. He was previously faculty at MIT.\n\nMichael Nielsen called the book \"a beautiful synthesis of what we know\", while Seth Lloyd praised it as \"lucid\", describing Aaronson as a \"tornado of intellectual activity\".\n\nThe Journal of the American Mathematical Society considered it to have \"much insight, wisdom, and fun\", but conceded that it \"is not for everyone', while Luboš Motl praised Aaronson's writing as \"very witty, narcisistically (sic) witty\", but considered the book to be scientifically dubious.\n"}
{"id": "37890436", "url": "https://en.wikipedia.org/wiki?curid=37890436", "title": "Reza Sadeghi (mathematician)", "text": "Reza Sadeghi (mathematician)\n\nReza Sadeghi (;(April 20, 1977 – March 17, 1998) was an Iranian mathematician. He is an alumnus of National Organization for Development of Exceptional Talents (NODET) Mashad, Iran (Hasheminejad highschool).\n\nReza Sadeghi won the silver medal in the International Mathematical Olympiad in Hong Kong in 1994. He won the gold medal in the International Mathematical Olympiad in Canada in 1995.\n"}
{"id": "960972", "url": "https://en.wikipedia.org/wiki?curid=960972", "title": "Schoenflies notation", "text": "Schoenflies notation\n\nThe Schoenflies (or Schönflies) notation, named after the German mathematician Arthur Moritz Schoenflies, is one of two conventions commonly used to describe point groups. This notation is used in spectroscopy. The other convention is the Hermann–Mauguin notation, also known as the international notation. A point group in the Schoenflies convention is completely adequate to describe the symmetry of a molecule; this is sufficient for spectroscopy. The Hermann–Mauguin notation is able to describe the space group of a crystal lattice, while the Schoenflies notation isn't. Thus the Hermann–Mauguin notation is used in crystallography.\n\nSymmetry elements are denoted by i for centers of inversion, C for proper rotation axes, σ for mirror planes, and S for improper rotation axes (rotation-reflection axes). C and S are usually followed by a subscript number (abstractly denoted n) denoting the order of rotation possible.\n\nBy convention, the axis of proper rotation of greatest order is defined as the principal axis. All other symmetry elements are described in relation to it. A vertical mirror plane (containing the principal axis) is denoted σ; a horizontal mirror plane (perpendicular to the principal axis) is denoted σ.\n\nIn three dimensions, there are an infinite number of point groups, but all of them can be classified by several families. \n\n\nAll groups that do not contain several higher-order axes (order 3 or more) can be arranged in a table, as shown below; symbols marked in red should not be used.\nIn crystallography, due to the crystallographic restriction theorem, \"n\" is restricted to the values of 1, 2, 3, 4, or 6. The noncrystallographic groups are shown with grayed backgrounds. \"D\" and \"D\" are also forbidden because they contain improper rotations with \"n\" = 8 and 12 respectively. The 27 point groups in the table plus \"T\", \"T\", \"T\", \"O\" and \"O\" constitute 32 crystallographic point groups. \n\nGroups with \"n = ∞\" are called limit groups or Curie groups. There are two more limit groups, not listed in the table: \"K\" (for \"Kugel\", German for ball, sphere), the group of all rotations in 3-dimensional space; and \"K\", the group of all rotations and reflections. In mathematics and theoretical physics they are known respectively as the \"special orthogonal group\" and the \"orthogonal group\" in three-dimensional space, with the symbols SO(3) and O(3).\n\nThe space groups with given point group are numbered by 1, 2, 3, ... (in the same order as their international number) and this number is added as a superscript to the Schönflies symbol for the corresponding point group. For example, groups numbers 3 to 5 whose point group is \"C\" have Schönflies symbols \"C\", \"C\", \"C\". \n\nWhile in case of point groups, Schönflies symbol defines the symmetry elements of group unambiguously, the additional superscript for space group doesn't have any information about translational symmetry of space group (lattice centering, translational components of axes and planes), hence one needs to refer to special tables, containing information about correspondence between Schönflies and Hermann–Mauguin notation. Such table is given in List of space groups page.\n\n\n\n"}
{"id": "19737192", "url": "https://en.wikipedia.org/wiki?curid=19737192", "title": "Semidiameter", "text": "Semidiameter\n\nIn geometry, the semidiameter or semi-diameter of a set of points may be one half of its diameter; or, sometimes, one half of its extent along a particular direction.\n\nThe semi-diameter of a sphere, circle, or interval is the same thing as its radius — namely, any line segment from the center to its boundary. \n\nThe semi-diameters of a non-circular ellipse are the halves of its extents along the two axes of symmetry. They are the parameters \"a\", \"b\" of the implicit equation\nLikewise, the semi-diameters of an ellipsoid are the parameters \"a\", \"b\", and \"c\" of its implicit equation\nThe semi-diameters of a superellipse, superellipsoid, or superquadric can be identified in the same way.\n\n"}
{"id": "870399", "url": "https://en.wikipedia.org/wiki?curid=870399", "title": "Set cover problem", "text": "Set cover problem\n\nThe set cover problem is a classical question in combinatorics, computer science and complexity theory. It is one of Karp's 21 NP-complete problems shown to be NP-complete in 1972.\n\nIt is a problem \"whose study has led to the development of fundamental techniques for the entire field\" of approximation algorithms.\n\nGiven a set of elements formula_1 (called the universe) and a collection formula_2 of formula_3 sets whose union equals the universe, the set cover problem is to identify the smallest sub-collection of formula_2 whose union equals the universe. For example, consider the universe formula_5 and the collection of sets formula_6. Clearly the union of formula_2 is formula_8. However, we can cover all of the elements with the following, smaller number of sets: formula_9.\n\nMore formally, given a universe formula_10 and a family formula_11 of subsets of formula_10,\na \"cover\" is a subfamily formula_13 of sets whose union is formula_10. In the set covering decision problem, the input is a pair formula_15 and an integer formula_16; the question is whether\nthere is a set covering of size formula_16 or less. In the set covering optimization problem, the input is a pair formula_15, and the task is to find a set covering that uses the fewest sets.\n\nThe decision version of set covering is NP-complete, and the optimization/search version of set cover is NP-hard.\n\nIf each set is assigned a cost, it becomes a \"weighted\" set cover problem.\n\nThe minimum set cover problem can be formulated as the following integer linear program (ILP).\n\nThis ILP belongs to the more general class of ILPs for covering problems.\nThe integrality gap of this ILP is at most formula_19, so its relaxation gives a factor-formula_19 approximation algorithm for the minimum set cover problem (where formula_21 is the size of the universe).\n\nIn weighted set cover, the sets are assigned weights. Denote the weight of set formula_22 by formula_23. Then the integer linear program describing weighted set cover is identical to the one given above, except that the objective function to minimize is formula_24.\n\nSet covering is equivalent to the hitting set problem. That is seen by observing that an instance of set covering can\nbe viewed as an arbitrary bipartite graph, with sets represented by vertices on the left, the universe represented by vertices on the\nright, and edges representing the inclusion of elements in sets. The task is then to find a minimum cardinality subset of left-vertices which covers all of the right-vertices. In the Hitting set problem, the objective is to cover the left-vertices using a minimum subset of the right vertices. Converting from one problem to the other is therefore achieved by interchanging the two sets of vertices.\n\nThere is a greedy algorithm for polynomial time approximation of set covering that chooses sets according to one rule: at each stage, choose the set that contains the largest number of uncovered elements. It can be shown that this algorithm achieves an approximation ratio of formula_25, where formula_26 is the size of the set to be covered. In other words, it finds a covering that may be formula_27 times as large as the minimum one, where formula_27 is the formula_29-th harmonic number:\n\nThis greedy algorithm actually achieves an approximation ratio of formula_31 where formula_32 is the maximum cardinality set of formula_2. For formula_34dense instances, however, there exists a formula_35-approximation algorithm for every formula_36.\n\nThere is a standard example on which the greedy algorithm achieves an approximation ratio of formula_37.\nThe universe consists of formula_38 elements. The set system consists of formula_16 pairwise disjoint sets \nformula_40 with sizes formula_41 respectively, as well as two additional disjoint sets formula_42,\neach of which contains half of the elements from each formula_43. On this input, the greedy algorithm takes the sets\nformula_44, in that order, while the optimal solution consists only of formula_45 and formula_46.\nAn example of such an input for formula_47 is pictured on the right.\n\nInapproximability results show that the greedy algorithm is essentially the best-possible polynomial time approximation algorithm for set cover up to lower order terms\n(see Inapproximability results below), under plausible complexity assumptions. A tighter analysis for the greedy algorithm shows that the approximation ratio is exactly formula_48.\n\nIf each element occurs in at most sets, then a solution can be found in polynomial time that approximates the optimum to within a factor of using LP relaxation.\n\nIf the constraint formula_49 is replaced by formula_50 for all in formula_10 in the integer linear program shown above, then it becomes a (non-integer) linear program . The algorithm can be described as follows:\n\nWhen formula_52 refers to the size of the universe, showed that set covering cannot be approximated in polynomial time to within a factor of formula_53, unless NP has quasi-polynomial time algorithms. Feige (1998) improved this lower bound to formula_54 under the same assumptions, which essentially matches the approximation ratio achieved by the greedy algorithm. established a lower bound\nof formula_55, where formula_56 is a certain constant, under the weaker assumption that Pformula_57NP.\nA similar result with a higher value of formula_56 was recently proved by . showed optimal inapproximability by proving that it cannot be approximated to formula_59 unless Pformula_60NP.\n\nRelaxing the integer linear program for weighted set cover stated above, one may use randomized rounding to get an formula_61-factor approximation. The corresponding analysis for nonweighted set cover is outlined in Randomized rounding#Randomized-rounding algorithm for set cover and can be adapted to the weighted case.\n\n\n\n"}
{"id": "25590565", "url": "https://en.wikipedia.org/wiki?curid=25590565", "title": "Sharp-SAT", "text": "Sharp-SAT\n\nIn computational complexity theory, #SAT, or Sharp-SAT, a function problem related to the Boolean satisfiability problem, is the problem of counting the number of satisfying assignments of a given Boolean formula. It is well-known example for the class of counting problems, which are of special interest in computational complexity theory.\n\nIt is a #P-complete problem, as any NP machine can be encoded into a Boolean formula by a process similar to that in Cook's theorem, such that the number of satisfying assignments of the Boolean formula is equal to the number of accepting paths of the NP machine. Any formula in SAT can be rewritten as a formula in 3-CNF form preserving the number of satisfying assignments, and so #SAT and #3SAT are equivalent and #3SAT is #P-complete as well.\n\nModel-counting is intractable in many special cases for which satisfiability is tractable. This includes the following.\n\nModel-counting is tractable (solvable in polynomial time) for (ordered) BDDs and for d-DNNFs.\n\nPrototype gpusat\n"}
{"id": "29819325", "url": "https://en.wikipedia.org/wiki?curid=29819325", "title": "Standard Borel space", "text": "Standard Borel space\n\nIn mathematics, a standard Borel space is the Borel space associated to a Polish space. Discounting Borel spaces of discrete Polish spaces, there is, up to isomorphism, only one standard Borel space.\n\nA measurable space is said to be \"standard Borel\" if there exists a metric on which makes it a complete separable metric space in such a way that is then the Borel σ-algebra.\nStandard Borel spaces have several useful properties that do not hold for general measurable spaces.\n\n\nTheorem. Let \"X\" be a Polish space, that is, a topological space such that there is a metric \"d\" on \"X\" which defines the topology of \"X\" and which makes \"X\" a complete separable metric space. Then \"X\" as a Borel space is Borel isomorphic to one of\n\nIt follows that a standard Borel space is characterized up to isomorphism by its cardinality, and that any uncountable standard Borel space has the cardinality of the continuum.\n\nBorel isomorphisms on standard Borel spaces are analogous to homeomorphisms on topological spaces: both are bijective and closed under composition, and a homeomorphism and its inverse are both continuous, instead of both being only Borel measurable.\n"}
{"id": "16081202", "url": "https://en.wikipedia.org/wiki?curid=16081202", "title": "String graph", "text": "String graph\n\nIn graph theory, a string graph is an intersection graph of curves in the plane; each curve is called a \"string\". Given a graph \"G\", \"G\" is a string graph if and only if there exists a set of curves, or strings, drawn in the plane such that no three strings intersect at a single point and such that the graph having a vertex for each curve and an edge for each intersecting pair of curves is isomorphic to \"G\".\n\n described a concept similar to string graphs as they applied to genetic structures. In that context, he also posed the specific case of intersecting intervals on a line, namely the now classical family of interval graphs. Later, specified the same idea to electrical networks and printed circuits. The mathematical study of string graphs began with the paper and \nthrough a collaboration between Sinden and Ronald Graham, where the characterization of string graphs eventually came to be posed as an open question at the 5th Hungarian Colloquium on Combinatorics in 1976. However, the recognition of string graphs was eventually proven to be NP-complete, implying that no simple characterization is likely to exist.\n\nEvery planar graph is a string graph: one may form a string graph representation of an arbitrary plane-embedded graph by drawing a string for each vertex that loops around the vertex and around the midpoint of each adjacent edge, as shown in the figure. For any edge \"uv\" of the graph, the strings for \"u\" and \"v\" cross each other twice near the midpoint of \"uv\", and there are no other crossings, so the pairs of strings that cross represent exactly the adjacent pairs of vertices of the original planar graph. Alternatively, by the circle packing theorem, any planar graph may be represented as a collection of circles, any two of which cross if and only if the corresponding vertices are adjacent; these circles (with a starting and ending point chosen to turn them into open curves) provide a string graph representation of the given planar graph. proved that every planar graph has a string representation in which each pair of strings has at most one crossing point, unlike the representations described above.\nScheinerman's conjecture, now proven, is the even stronger statement that every planar graph may be represented by the intersection graph of straight line segments, a very special case of strings.\nIf every edge of a given graph \"G\" is subdivided, the resulting graph is a string graph if and only if \"G\" is planar. In particular, the subdivision of the complete graph \"K\" shown in the illustration is not a string graph, because \"K\" is not planar.\n\nEvery circle graph, as an intersection graph of line segments (the chords of a circle), is also a string graph. Every chordal graph may be represented as a string graph: chordal graphs are intersection graphs of subtrees of trees, and one may form a string representation of a chordal graph by forming a planar embedding of the corresponding tree and replacing each subtree by a string that traces around the subtree's edges.\n\nThe complement graph of every comparability graph is also a string graph.\n\n showed computing the chromatic number of string graphs to be NP-hard. found that string graphs form an induced minor closed class, but not a minor closed class of graphs.\n\nEvery \"m\"-edge string graph can be partitioned into two subsets, each a constant fraction the size of the whole graph, by the removal of \"O\"(\"m\"log\"m\") edges. It follows that the biclique-free string graphs, string graphs containing no \"K\" subgraph for some constant \"t\", have \"O\"(\"n\") edges and more strongly have polynomial expansion.\n\n"}
{"id": "33084398", "url": "https://en.wikipedia.org/wiki?curid=33084398", "title": "The American Black Chamber", "text": "The American Black Chamber\n\nThe American Black Chamber is a 1931 book by Herbert O. Yardley. The book describes the inner workings of the interwar American governmental cryptography organization called the Black Chamber. The cryptography historian David Kahn called the book \"the most famous book on cryptology ever published.\" By describing the inner workings of the organization, the book created large interest public awareness of the United States's cryptographic abilities. In particular, the Japanese government became aware of the extent of experience the American government had with cryptography and increased the strength of their own knowledge in cryptography in response. Reviewers suggested the book may have cost the United States significantly in the Pacific theater against Japan in World War II.\n"}
{"id": "2639406", "url": "https://en.wikipedia.org/wiki?curid=2639406", "title": "Through and through", "text": "Through and through\n\nThrough and through describes a situation where an object, real or imaginary, passes completely through another object, also real or imaginary. The phrase has several common uses:\n\nThrough and through is used in forensics to describe a bullet that has passed through a body, leaving both entry and exit wounds Example..\n\nAn image may be through and through in the following cases:\n\nThrough and through images are more durable; they do not easily wear off.\n\nIn the case that the image can be viewed from the other side, we see the mirror image, just like in the case of a transparent image, such as a drawing on a transparent sheet.\n\nA sheet with a through and through image is achiral. We can distinguish two cases:\n\n"}
{"id": "14081394", "url": "https://en.wikipedia.org/wiki?curid=14081394", "title": "Triangular coordinates", "text": "Triangular coordinates\n\nThe term triangular coordinates may refer to any of at least three related systems of coordinates in the Euclidean plane:\n\n"}
{"id": "1979061", "url": "https://en.wikipedia.org/wiki?curid=1979061", "title": "Two-fluid model", "text": "Two-fluid model\n\nTwo-fluid model is a macroscopic traffic flow model to represent traffic in a town/city or metropolitan area, put forward in the 1970s by Ilya Prigogine and Robert Herman.\n\nThere is also a two-fluid model which helps explain the behavior of superfluid helium. This model states that there will be two components in liquid helium below its lambda point (the temperature where superfluid forms). These components are a normal fluid and a superfluid component. Each liquid has a different density and together their sum makes the total density, which remains constant. The ratio of superfluid density to the total density increases as the temperature approaches absolute zero.\n\n"}
{"id": "31993476", "url": "https://en.wikipedia.org/wiki?curid=31993476", "title": "Vinti Prize", "text": "Vinti Prize\n\nThe Cologero Vinti Prize is awarded by the Italian Mathematical Union to an Italian mathematician not exceeding the age of 40, in recognition of his/her contributions to the field of Mathematical Analysis. The prize is entitled to the memory of the Italian mathematician Calogero Vinti and is awarded on the occasion of the Italian Mathematical Union conference every four years. \n\nFurther prizes of the Italian Mathematical Union are the Caccioppoli Prize, the Bartolozzi Prize and the Stampacchia Medal.\n\nSource: Unione Matematica Italiana\n"}
