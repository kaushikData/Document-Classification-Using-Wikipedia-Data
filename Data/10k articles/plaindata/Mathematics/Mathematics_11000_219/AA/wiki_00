{"id": "152518", "url": "https://en.wikipedia.org/wiki?curid=152518", "title": "Abel–Ruffini theorem", "text": "Abel–Ruffini theorem\n\nIn algebra, the Abel–Ruffini theorem (also known as Abel's impossibility theorem) states that there is no algebraic solution—that is, solution in radicals—to the general polynomial equations of degree five or higher with arbitrary coefficients. The theorem is named after Paolo Ruffini, who made an incomplete proof in 1799, and Niels Henrik Abel, who provided a proof in 1824.\n\nThe theorem does \"not\" assert that some higher-degree polynomial equations have \"no\" solution. In fact, the opposite is true: \"every\" non-constant polynomial equation in one unknown, with real or complex coefficients, has at least one complex number as a solution (and thus, by polynomial division, as many complex roots as its degree, counting repeated roots); this is the fundamental theorem of algebra. These solutions can be computed to any desired degree of accuracy using numerical methods such as the Newton–Raphson method or the Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees. It also does \"not\" assert that \"no\" higher-degree polynomial equations can be solved in radicals: the equation can be solved in radicals for every positive integer , for example. The theorem only shows that there is no \"general solution in radicals\" that applies to \"all\" equations of a given degree greater than .\n\nThe solution of any second-degree polynomial equation can be expressed in terms of its coefficients, using only addition, subtraction, multiplication, division, and square roots, in the familiar quadratic formula: the roots of the equation (with ) are\n\nAnalogous formulas for third-degree equations and fourth-degree equations (using square roots and cube roots) have been known since the 16th century. What the Abel–Ruffini theorem says is that there is no similar formula for general equations of fifth degree or higher. In principle, it could be that the equations of the fifth degree could be split in several types and, for each one of these types, there could be some algebraic solution valid within that type. Or, as Ian Stewart wrote, \"for all that Abel's methods could prove, every particular quintic equation might be soluble, with a special formula for each equation.\" However, this is not so, but this impossibility is a strictly stronger result than the Abel–Ruffini theorem and is derived with Galois theory.\n\nThe following proof is based on Galois theory and it is valid for any field of characteristic . Historically, Ruffini and Abel's proofs precede Galois theory. For a modern presentation of Abel's proof see the article of Rosen or the books of Tignol or Pesic.\n\nOne of the fundamental theorems of Galois theory states that a polynomial is solvable by radicals over if and only if its splitting field over has a solvable Galois group, so the proof of the Abel–Ruffini theorem comes down to computing the Galois group of the general polynomial of the fifth degree, and showing that it is not solvable.\n\nConsider five indeterminates , , , , and , let , and let\n\nExpanding out yields the elementary symmetric functions of the :\n\nThe coefficient of in is thus . Let be the field obtained by adjoining the symmetric functions to the rationals. Then . Because the 's are indeterminates, every permutation in the symmetric group on letters induces a distinct automorphism on that leaves fixed and permutes the elements . Since an arbitrary rearrangement of the roots of the product form still produces the same polynomial, e.g.\n\nis the same polynomial as\n\nthe automorphisms also leave fixed, so they are elements of the Galois group . Therefore, we have shown that ; however there could possibly be automorphisms there that are not in . But, since the Galois group of the splitting field of a quintic polynomial has at most elements, and since is a splitting field of , it follows that is isomorphic to . Generalizing this argument shows that the Galois group of every general polynomial of degree is isomorphic to .\n\nThe only composition series of is } (where is the alternating group on five letters, also known as the icosahedral group). However, the quotient group } (isomorphic to itself) is not abelian, and so is not solvable, so it must be that the general polynomial of the fifth degree has no solution in radicals. Since the first nontrivial normal subgroup of the symmetric group on  letters is always the alternating group on  letters, and since the alternating groups on  letters for are always simple and non-abelian, and hence not solvable, it also says that the general polynomials of all degrees higher than the fifth also have no solution in radicals. Q.E.D.\n\nThe above construction of the Galois group for a fifth degree polynomial only applies to the \"general polynomial\"; specific polynomials of the fifth degree may have different Galois groups with quite different properties, e.g. has a splitting field generated by a primitive 5th root of unity, and hence its Galois group is abelian and the equation itself solvable by radicals; moreover, the argument does not provide any rational-valued quintic that has or as its Galois group. However, since the result is on the general polynomial, it does say that a general “quintic formula” for the roots of a quintic using only a finite combination of the arithmetic operations and radicals in terms of the coefficients is impossible.\n\nThe proof is not valid if applied to polynomials whose degree is less than . Indeed:\n\nThe proof remains valid if, instead of working with five indeterminates, one works with five concrete algebraically independent complex numbers, because, by the same argument, .\n\nAround 1770, Joseph Louis Lagrange began the groundwork that unified the many different tricks that had been used up to that point to solve equations, relating them to the theory of groups of permutations, in the form of Lagrange resolvents. This innovative work by Lagrange was a precursor to Galois theory, and its failure to develop solutions for equations of fifth and higher degrees hinted that such solutions might be impossible, but it did not provide conclusive proof. The first person who conjectured that the problem of solving quintics by radicals might be impossible to solve was Carl Friedrich Gauss, who wrote in 1798 in section 359 of his book \"Disquisitiones Arithmeticae\" (which would be published only in 1801) that “there is little doubt that this problem does not so much defy modern methods of analysis as that it proposes the impossible”. The next year, in his thesis, he wrote “After the labors of many geometers left little hope of ever arriving at the resolution of the general equation algebraically, it appears more and more likely that this resolution is impossible and contradictory.” And he added “Perhaps it will not be so difficult to prove, with all rigor, the impossibility for the fifth degree. I shall set forth my investigations of this at greater length in another place.” Actually, Gauss published nothing else on this subject.\nThe theorem was first nearly proved by Paolo Ruffini in 1799. He sent his proof to several mathematicians to get it acknowledged, amongst them Lagrange (who did not reply) and Augustin-Louis Cauchy, who sent him a letter saying: “Your memoir on the general solution of equations is a work which I have always believed should be kept in mind by mathematicians and which, in my opinion, proves conclusively the algebraic unsolvability of general equations of higher than fourth degree.” However, in general, Ruffini's proof was not considered convincing. Abel wrote: “The first and, if I am not mistaken, the only one who, before me, has sought to prove the impossibility of the algebraic solution of general equations is the mathematician Ruffini. But his memoir is so complicated that it is very difficult to determine the validity of his argument. It seems to me that his argument is not completely satisfying.”\n\nThe proof also, as it was discovered later, was incomplete. Ruffini assumed that all radicals that he was dealing with could be expressed from the roots of the polynomial using field operations alone; in modern terms, he assumed that the radicals belonged to the splitting field of the polynomial. To see why this is really an extra assumption, consider, for instance, the polynomial . According to Cardano's formula, one of its roots (all of them, actually) can be expressed as the sum of a cube root of with a cube root of . On the other hand, since , , , and , the roots , , and of are all real and therefore the field is a subfield of . But then the numbers cannot belong to . While Cauchy either did not notice Ruffini's assumption or felt that it was a minor one, most historians believe that the proof was not complete until Abel proved the theorem on natural irrationalities, which asserts that the assumption holds in the case of general polynomials. The Abel–Ruffini theorem is thus generally credited to Abel, who published a proof in just six pages in 1824. However, this short number of pages was obtained at the cost of writing in a very terse style. This was due to the fact that he had the proof printed at his own expenses and he needed to save paper and money. A more elaborated version of the proof would be published in 1826.\n\nProving that the general quintic (and higher) equations were unsolvable by radicals did not completely settle the matter, because the Abel–Ruffini theorem does not provide necessary and sufficient conditions for saying precisely which quintic (and higher) equations are unsolvable by radicals. Abel was working on a complete characterization when he died in 1829.\n\nAccording to Nathan Jacobson, “The proofs of Ruffini and of Abel […] were soon superseded by the crowning achievement of this line of research: Galois' discoveries in the theory of equations.” In 1830, Galois (at the age of 18) submitted to the Paris Academy of Sciences a memoir on his theory of solvability by radicals, which was ultimately rejected in 1831 as being too sketchy and for giving a condition in terms of the roots of the equation instead of its coefficients. Galois was aware of the contributions of Ruffini and Abel, since he wrote “It is a common truth, today, that the general equation of degree greater than cannot be solved by radicals… this truth has become common (by hearsay) despite the fact that geometers have ignored the proofs of Abel and Ruffini…” Galois then died in 1832 and his paper \"Mémoire sur les conditions de resolubilité des équations par radicaux\" remained unpublished until 1846, when it was published by Joseph Liouville accompanied by some of his own explanations. Prior to this publication, Liouville announced Galois' result to the Academy in a speech he gave on 4 July 1843. A simplification of Abel's proof was published by Pierre Wantzel in 1845. When he published it, he was already aware of the contributions by Galois and he mentions that, whereas Abel's proof is valid only for general polynomials, Galois' approach can be used to provide a concrete polynomial of degree  whose roots cannot be expressed in radicals from its coefficients.\n\nIn 1963, Vladimir Arnold discovered a topological proof of the Abel–Ruffini theorem, which served as a starting point for topological Galois theory.\n"}
{"id": "40564571", "url": "https://en.wikipedia.org/wiki?curid=40564571", "title": "Amplituhedron", "text": "Amplituhedron\n\nAn amplituhedron is a geometric structure introduced in 2013 by Nima Arkani-Hamed and Jaroslav Trnka. It enables simplified calculation of particle interactions in some quantum field theories. In planar \"N\" = 4 supersymmetric Yang–Mills theory, also equivalent to the perturbative topological B model string theory in twistor space, an amplituhedron is defined as a mathematical space known as the positive Grassmannian.\n\nAmplituhedron theory challenges the notion that space-time locality and unitarity are necessary components of a model of particle interactions. Instead, they are treated as properties that emerge from an underlying phenomenon.\n\nThe connection between the amplituhedron and scattering amplitudes is at present a conjecture that has passed many non-trivial checks, including an understanding of how locality and unitarity arise as consequences of positivity.\n\nResearch has been led by Nima Arkani-Hamed. Edward Witten described the work as “very unexpected\" and said that \"it is difficult to guess what will happen or what the lessons will turn out to be.\"\n\nWhen subatomic particles interact, different outcomes are possible. The evolution of the various possibilities is called a \"tree\" and the probability of a given outcome is called its scattering amplitude. According to the principle of unitarity, the sum of the probabilities for every possible outcome is 1.\n\nThe on-shell scattering process \"tree\" may be described by a positive Grassmannian, a structure in algebraic geometry analogous to a convex polytope, that generalizes the idea of a simplex in projective space. A polytope is the \"n\"-dimensional analogue of a 3-dimensional polyhedron, the values being calculated in this case are scattering amplitudes, and so the object is called an \"amplituhedron\".\n\nUsing twistor theory, BCFW recursion relations involved in the scattering process may be represented as a small number of twistor diagrams. These diagrams effectively provide the recipe for constructing the positive Grassmannian, i.e. the amplituhedron, which may be captured in a single equation. The scattering amplitude can thus be thought of as the volume of a certain polytope, the positive Grassmannian, in momentum twistor space.\n\nWhen the volume of the amplituhedron is calculated in the planar limit of \"N\" = 4 \"D\" = 4 supersymmetric Yang–Mills theory, it describes the scattering amplitudes of subatomic particles. The amplituhedron thus provides a more intuitive geometric model for calculations whose underlying principles were until then highly abstract.\n\nThe twistor-based representation provides a recipe for constructing specific cells in the Grassmannian which assemble to form a positive Grassmannian, i.e. the representation describes a specific cell decomposition of the positive Grassmannian.\n\nThe recursion relations can be resolved in many different ways, each giving rise to a different representation, with the final amplitude expressed as a sum of on-shell processes in different ways as well. Therefore, any given on-shell representation of scattering amplitudes is not unique, but all such representations of a given interaction yield the same amplituhedron.\n\nThe twistor approach is relatively abstract. While amplituhedron theory provides an underlying geometric model, the geometrical space is not physical spacetime and is also best understood as abstract.\n\nThe twistor approach simplifies calculations of particle interactions. In a conventional perturbative approach to quantum field theory, such interactions may require the calculation of thousands of Feynman diagrams, most describing off-shell \"virtual\" particles which have no directly observable existence. In contrast, twistor theory provides an approach in which scattering amplitudes can be computed in a way that yields much simpler expressions. Amplituhedron theory calculates scattering amplitudes without referring to such virtual particles. This undermines the case for even a transient, unobservable existence for such virtual particles.\n\nThe geometric nature of the theory suggests in turn that the nature of the universe, in both classical relativistic spacetime and quantum mechanics, may be described with geometry.\n\nCalculations can be done without assuming the quantum mechanical properties of locality and unitarity. In amplituhedron theory, locality and unitarity arise as a direct consequence of positivity. They are encoded in the positive geometry of the amplituhedron, via the singularity structure of the integrand for scattering amplitudes. Arkani-Hamed suggests this is why amplituhedron theory simplifies scattering-amplitude calculations: In the Feynman-diagrams approach, locality is manifest, whereas in the amplituhedron approach, it is implicit.\n\nSince the planar limit of the \"N\" = 4 supersymmetric Yang–Mills theory is a toy theory that does not describe the real world, the relevance of this technique for more realistic quantum field theories is currently unknown, but it provides promising directions for research into theories about the real world.\n\n\n\n"}
{"id": "864438", "url": "https://en.wikipedia.org/wiki?curid=864438", "title": "Arrangement of hyperplanes", "text": "Arrangement of hyperplanes\n\nIn geometry and combinatorics, an arrangement of hyperplanes is an arrangement of a finite set \"A\" of hyperplanes in a linear, affine, or projective space \"S\". \nQuestions about a hyperplane arrangement \"A\" generally concern geometrical, topological, or other properties of the complement, \"M\"(\"A\"), which is the set that remains when the hyperplanes are removed from the whole space. One may ask how these properties are related to the arrangement and its intersection semilattice.\nThe intersection semilattice of \"A\", written \"L\"(\"A\"), is the set of all subspaces that are obtained by intersecting some of the hyperplanes; among these subspaces are \"S\" itself, all the individual hyperplanes, all intersections of pairs of hyperplanes, etc. (excluding, in the affine case, the empty set). These subspaces are called the flats of \"A\". The intersection semilattice \"L\"(\"A\") is partially ordered by \"reverse inclusion\". \n\nIf the whole space \"S\" is 2-dimensional, the hyperplanes are lines; such an arrangement is often called an arrangement of lines. Historically, real arrangements of lines were the first arrangements investigated. If \"S\" is 3-dimensional one has an arrangement of planes.\n\nThe intersection semilattice \"L\"(\"A\") is a meet semilattice and more specifically is a geometric semilattice. If the arrangement is linear or projective, or if the intersection of all hyperplanes is nonempty, the intersection lattice is a geometric lattice.\n\nWhen \"L\"(\"A\") is a lattice, the matroid of \"A\", written \"M\"(\"A\"), has \"A\" for its ground set and has rank function \"r\"(\"S\") := codim(\"I\"), where \"S\" is any subset of \"A\" and \"I\" is the intersection of the hyperplanes in \"S\". In general, when \"L\"(\"A\") is a semilattice, there is an analogous matroid-like structure that might be called a semimatroid, which is a generalization of a matroid (and has the same relationship to the intersection semilattice as does the matroid to the lattice in the lattice case), but is not a matroid if \"L\"(\"A\") is not a lattice.\n\nFor a subset \"B\" of \"A\", let us define \"f\"(\"B\") := the intersection of the hyperplanes in \"B\"; this is \"S\" if \"B\" is empty. \nThe characteristic polynomial of \"A\", written \"p\"(\"y\"), can be defined by \n\nsummed over all subsets \"B\" of \"A\" except, in the affine case, subsets whose intersection is empty. (The dimension of the empty set is defined to be −1.) This polynomial helps to solve some basic questions; see below.\nAnother polynomial associated with \"A\" is the Whitney-number polynomial \"w\"(\"x\", \"y\"), defined by\n\nsummed over \"B\" ⊆ \"C\" ⊆ \"A\" such that \"f\"(\"B\") is nonempty.\n\nBeing a geometric lattice or semilattice, \"L\"(\"A\") has a characteristic polynomial, \"p\"(\"y\"), which has an extensive theory (see matroid). Thus it is good to know that \"p\"(\"y\") = \"y\" \"p\"(\"y\"), where \"i\" is the smallest dimension of any flat, except that in the projective case it equals \"y\"\"p\"(\"y\"). \nThe Whitney-number polynomial of \"A\" is similarly related to that of \"L\"(\"A\"). \n\nThe intersection semilattice determines another combinatorial invariant of the arrangement, the Orlik–Solomon algebra. To define it, fix a commutative subring \"K\" of the base field, and form the exterior algebra \"E\" of the vector space\ngenerated by the hyperplanes.\nA chain complex structure is defined on \"E\" with the usual boundary operator formula_4.\nThe Orlik–Solomon algebra is then the quotient of \"E\" by the ideal generated by elements of the form formula_5 (where formula_6 have an empty intersection) and by boundaries of elements of the same form for which formula_7 has codimension less than \"p\".\n\nIn real affine space, the complement is disconnected: it is made up of separate pieces called cells or regions or chambers, each of which is either a bounded region that is a convex polytope, or an unbounded region that is a convex polyhedral region which goes off to infinity. \nEach flat of \"A\" is also divided into pieces by the hyperplanes that do not contain the flat; these pieces are called the faces of \"A\". \nThe regions are faces because the whole space is a flat. \nThe faces of codimension 1 may be called the facets of \"A\". \nThe face semilattice of an arrangement is the set of all faces, ordered by \"inclusion\". Adding an extra top element to the face semilattice gives the face lattice.\n\nIn two dimensions (i.e., in the real affine plane) each region is a convex polygon (if it is bounded) or a convex polygonal region which goes off to infinity. \n\nTypical problems about an arrangement in \"n\"-dimensional real space is to say how many regions there are, or how many faces of dimension 4, or how many bounded regions. These questions can be answered just from the intersection semilattice. For instance, two basic theorems are that the number of regions of an affine arrangement equals (−1)\"p\"(−1) and the number of bounded regions equals (−1)p(1). Similarly, the number of \"k\"-dimensional faces or bounded faces can be read off as the coefficient of \"x\" in (−1) w (−\"x\", −1) or (−1)\"w\"(−\"x\", 1).\n\nAnother question about an arrangement in real space is to decide how many regions are simplices (the \"n\"-dimensional generalization of triangles and tetrahedra). This cannot be answered based solely on the intersection semilattice. The McMullen problem asks for the smallest arrangement of a given dimension in general position in real projective space for which there does not exist a cell touched by all hyperplanes.\n\nA real linear arrangement has, besides its face semilattice, a poset of regions, a different one for each region. This poset is formed by choosing an arbitrary base region, \"B\", and associating with each region \"R\" the set \"S\"(\"R\") consisting of the hyperplanes that separate \"R\" from \"B\". The regions are partially ordered so that \"R\" ≥ \"R\" if \"S\"(\"R\", \"R\") contains \"S\"(\"R\", \"R\"). In the special case when the hyperplanes arise from a root system, the resulting poset is the corresponding Weyl group with the weak Bruhat order. In general, the poset of regions is ranked by the number of separating hyperplanes and its Möbius function has been computed .\n\nVadim Schechtman and Alexander Varchenko introduced a matrix indexed by the regions. The matrix element for the region formula_8 and formula_9 is given by the product of indeterminate variables formula_10 for every hyperplane H that separates these two regions. If these variables are specialized to be all value q, then this is called the q-matrix (over the Euclidean domain formula_11) for the arrangement and much information is contained in its Smith normal form.\n\nIn complex affine space (which is hard to visualize because even the complex affine plane has four real dimensions), the complement is connected (all one piece) with holes where the hyperplanes were removed.\n\nA typical problem about an arrangement in complex space is to describe the holes.\n\nThe basic theorem about complex arrangements is that the cohomology of the complement \"M\"(\"A\") is completely determined by the intersection semilattice. To be precise, the cohomology ring of \"M\"(\"A\") (with integer coefficients) is isomorphic to the Orlik–Solomon algebra on Z.\n\nThe isomorphism can be described rather explicitly, and gives a presentation of the cohomology in terms of generators and relations, where generators are represented (in the de Rham cohomology) as logarithmic differential forms\n\nwith formula_13 any linear form defining the generic hyperplane of the arrangement.\n\nSometimes it is convenient to allow the degenerate hyperplane, which is the whole space \"S\", to belong to an arrangement. If \"A\" contains the degenerate hyperplane, then it has no regions because the complement is empty. However, it still has flats, an intersection semilattice, and faces. The preceding discussion assumes the degenerate hyperplane is not in the arrangement.\n\nSometimes one wants to allow repeated hyperplanes in the arrangement. We did not consider this possibility in the preceding discussion, but it makes no material difference.\n\n\n"}
{"id": "53280578", "url": "https://en.wikipedia.org/wiki?curid=53280578", "title": "Box-counting content", "text": "Box-counting content\n\nIn mathematics, the box-counting content is an analog of Minkowski content.\n\nLet formula_1 be a bounded subset of formula_2-dimensional Euclidean space formula_3 such that the box-counting dimension formula_4 exists.\nThe upper and lower box-counting contents of formula_1 are defined by\n\nwhere formula_7 is the maximum number of disjoint closed balls with centers\nformula_8 and radii formula_9.\n\nIf formula_10, then the common value, denoted formula_11, is called the \"box-counting content\" of formula_1.\n\nIf formula_13, then formula_1 is said to be \"box-counting measurable\". \n\nLet formula_15 denote the unit interval.\nNote that the box-counting dimension formula_16 and the Minkowski dimension formula_17 coincide with a common value of 1; i.e.\n\nNow observe that formula_19, where formula_20 denotes the integer part of formula_21. Hence formula_22 is \"box-counting measurable\" with formula_23.\n\nBy contrast, formula_22 is Minkowski measurable with formula_25.\n\n"}
{"id": "7795793", "url": "https://en.wikipedia.org/wiki?curid=7795793", "title": "British Society for the History of Mathematics", "text": "British Society for the History of Mathematics\n\nThe British Society for the History of Mathematics (BSHM) was founded in 1971 to promote research into the history of mathematics at all levels and to further the use of the history of mathematics in education.\n\nThe BSHM is concerned with all periods and cultures, and with all aspects of mathematics. It participates in the Joint Mathematical Council of the United Kingdom.\n\nThe BSHM Bulletin, or the \"Journal of the British Society for the History of Mathematics\", is published on behalf of BSHM by Taylor & Francis. Articles cover local mathematical history, the use of history of mathematics in education, and individual interests.\n\nThe Neumann prize is awarded biennially by the BSHM for \"a book in English (including books in translation) dealing with the history of mathematics and aimed at a broad audience.\" The prize was named in honour of Peter M. Neumann, who is a longstanding supporter of and contributor to the society. It carries an award of £600.The previous winners are:\n\n\n"}
{"id": "48673817", "url": "https://en.wikipedia.org/wiki?curid=48673817", "title": "Chamberlain's approach to unobserved effects models", "text": "Chamberlain's approach to unobserved effects models\n\nIn a linear panel data setting, it can be desirable to estimate the magnitude of the Fixed Effects, as they provide measures of the unobserved components. For instance, in wage equation regressions, Fixed Effects capture ability measures that are constant over time, such as motivation. Chamberlain's approach to unobserved effects models is a way of estimating the linear unobserved effects, under Fixed Effect (rather than Random Effects) assumptions, in the following unobserved effects model\n\n\"y = xb + c + u \" (1)\n\nwhere \"c\" is the unobserved effect and \"x\" contains only time-varying explanatory variables. Rather than differencing out the unobserved effect \"c\", Chamberlain proposed to replace it with the linear projection of it onto the explanatory variables in all time periods. Specifically, this leads to the following equation\n\n\"c = d + x λ + x λ + ... + x λ + e \" (2)\n\nwhere the conditional distribution of \"c\" given \"x\" is unspecified, as is standard in Fixed Effects models. Combining equations (1) and (2) then gives rise to the following model.\n\n\"y = d + x λ + ... + x (b+λ ) + ... + x λ + e + u\" (3)\n\nAn important advantage of this approach is the computational requirement. Chamberlain uses minimum distance estimation, but a GMM approach would be another valid way of estimating this model. The latter approach also gives rise to a larger number of instruments than moment conditions, which leads to useful overidentifying restrictions that can be used to test the strict exogeneity restrictions imposed by many static Fixed Effects models.\n\nSimilar approaches have been proposed to model the unobserved effect. For instance, Mundlak follows a very similar approach, but rather projects the unobserved effect \"c\" onto the average of all \"x\" across all T time periods, more specifically \n\n\"c = d + xλ + e\" (4)\n\nIt can be shown that the Chamberlain method is a generalization of Mundlak's model. The Chamberlain method has been popular in empirical work, ranging from studies trying to estimate the causal returns to union members to studies investigating growth convergence.\n"}
{"id": "1761690", "url": "https://en.wikipedia.org/wiki?curid=1761690", "title": "Complex convexity", "text": "Complex convexity\n\nA set formula_1 in formula_2 is called formula_3-convex if its intersection with any complex line is contractible.\n\nIn complex geometry and analysis, the notion of convexity and its generalizations play an important role in understanding function behavior. Examples of classes of functions with a rich structure are, in addition to the convex functions, the subharmonic functions and the plurisubharmonic functions. Geometrically, these classes of functions correspond to convex domains and pseudoconvex domains, but there are also other types of domains, for instance lineally convex domains which can be generalized using convex analysis. A great deal is already known about these domains, but there remain some fascinating, unsolved problems. This theme is mainly theoretical, but there are computational aspects of the domains studied, and these computational aspects are certainly worthy of further study.\n"}
{"id": "6811795", "url": "https://en.wikipedia.org/wiki?curid=6811795", "title": "Convergence of measures", "text": "Convergence of measures\n\nIn mathematics, more specifically measure theory, there are various notions of the convergence of measures. For an intuitive general sense of what is meant by \"convergence in measure\", consider a sequence of measures μ on a space, sharing a common collection of measurable sets. Such a sequence might represent an attempt to construct 'better and better' approximations to a desired measure μ that is difficult to obtain directly. The meaning of 'better and better' is subject to all the usual caveats for taking limits; for any error tolerance ε > 0 we require there be \"N\" sufficiently large for \"n\" ≥ \"N\" to ensure the 'difference' between μ and μ is smaller than ε. Various notions of convergence specify precisely what the word 'difference' should mean in that description; these notions are not equivalent to one another, and vary in strength.\n\nThree of the most common notions of convergence are described below.\n\nThis section attempts to provide a rough intuitive description of three notions of convergence, using terminology developed in calculus courses; this section is necessarily imprecise as well as inexact, and the reader should refer to the formal clarifications in subsequent sections. In particular, the descriptions here do not address the possibility that the measure of some sets could be infinite, or that the underlying space could exhibit pathological behavior, and additional technical assumptions are needed for some of the statements. The statements in this section are however all correct if formula_1 is a sequence of probability measures on a Polish space.\n\nThe various notions of convergence formalize the assertion that the 'average value' of each 'sufficiently nice' function should converge:\n\nTo formalize this requires a careful specification of the set of functions under consideration and how uniform the convergence should be.\n\nThe notion of \"weak convergence\" requires this convergence to take place for every continuous bounded function formula_3. \nThis notion treats convergence for different functions \"f\" independently of one another, \"i.e.\" different functions \"f\" may require different values of \"N\" ≤ \"n\" to be approximated equally well (thus, convergence is non-uniform in formula_3).\n\nThe notion of \"strong convergence\" formalizes the assertion that the measure of each measurable set should converge:\n\nAgain, no uniformity over the set formula_6 is required.\nIntuitively, considering integrals of 'nice' functions, this notion provides more uniformity than weak convergence. As a matter of fact, when considering sequences of measures with uniformly bounded\nvariation on a Polish space, strong convergence implies the convergence formula_2 for any bounded measurable function formula_3.\nAs before, this convergence is non-uniform in formula_3\n\nThe notion of \"total variation convergence\" formalizes the assertion that the measure of all measurable sets should converge \"uniformly\", i.e. for every formula_10 there exists \"N\" \nsuch that formula_11 for every \"n > N\" and for every measurable set formula_6. As before, this implies convergence of integrals against bounded measurable functions, but this time\nconvergence is uniform over all functions bounded by any fixed constant.\n\nThis is the strongest notion of convergence shown on this page and is defined as follows. Let formula_13 be a measurable space. The total variation distance between two (positive) measures μ and ν is then given by\n\nHere the supremum is taken over \"f\" ranging over the set of all measurable functions from \"X\" to [−1, 1]. This is in contrast, for example, to the Wasserstein metric, where the definition is of the same form, but the supremum is taken over \"f\" ranging over the set of measurable functions from \"X\" to [−1, 1] which have Lipschitz constant at most 1; and also in contrast to the Radon metric, where the supremum is taken over \"f\" ranging over the set of continuous functions from \"X\" to [−1, 1]. In the case where \"X\" is a Polish space, the total variation metric coincides with the Radon metric.\n\nIf μ and ν are both probability measures, then the total variation distance is also given by\n\nThe equivalence between these two definitions can be seen as a particular case of the Monge-Kantorovich duality. From the two definitions above, it is clear that the total variation distance between probability measures is always between 0 and 2.\n\nTo illustrate the meaning of the total variation distance, consider the following thought experiment. Assume that we are given two probability measures μ and ν, as well as a random variable \"X\". We know that \"X\" has law either μ or ν but we do not know which one of the two. Assume that these two measures have prior probabilities 0.5 each of being the true law of \"X\". Assume now that we are given \"one\" single sample distributed according to the law of \"X\" and that we are then asked to guess which one of the two distributions describes that law. The quantity\n\nthen provides a sharp upper bound on the prior probability that our guess will be correct.\n\nGiven the above definition of total variation distance, a sequence μ of measures defined on the same measure space is said to converge to a measure \"μ\" in total variation distance if for every \"ε\" > 0, there exists an \"N\" such that for all \"n\" > \"N\", one has that\n\nFor formula_13 a measurable space, a sequence μ is said to converge strongly to a limit \"μ\" if\n\nfor every set formula_20.\n\nFor example, as a consequence of the Riemann–Lebesgue lemma, the sequence μ of measures on the interval [−1, 1] given by μ(\"dx\") = (1+ sin(\"nx\"))\"dx\" converges strongly to Lebesgue measure, but it does not converge in total variation.\n\nIn mathematics and statistics, weak convergence is one of many types of convergence relating to the convergence of measures. It depends on a topology on the underlying space and thus is not a purely measure theoretic notion.\n\nThere are several equivalent definitions of weak convergence of a sequence of measures, some of which are (apparently) more general than others. The equivalence of these conditions is sometimes known as the Portmanteau theorem.\n\nDefinition. Let formula_21 be a metric space with its Borel formula_22-algebra formula_23. A bounded sequence of positive probability measures formula_24 on formula_25 is said to converge weakly to the finite positive measure formula_26 (denoted formula_27) if any of the following equivalent conditions is true (here formula_28 denotes expectation or the formula_29 norm with respect to formula_30, while formula_31 denotes expectation or the formula_29 norm with respect to formula_26):\n\n\nIn the case formula_51 with its usual topology, if formula_52 and formula_53 denote the cumulative distribution functions of the measures formula_30 and formula_26, respectively, then formula_30 converges weakly to formula_26 if and only if formula_58 for all points formula_59 at which formula_53 is continuous.\n\nFor example, the sequence where formula_30 is the Dirac measure located at formula_62 converges weakly to the Dirac measure located at 0 (if we view these as measures on formula_63 with the usual topology), but it does not converge strongly. This is intuitively clear: we only know that formula_62 is \"close\" to formula_65 because of the topology of formula_63.\n\nThis definition of weak convergence can be extended for formula_21 any metrizable topological space. It also defines a weak topology on formula_68, the set of all probability measures defined on formula_69. The weak topology is generated by the following basis of open sets:\n\nwhere\n\nIf formula_21 is also separable, then formula_68 is metrizable and separable, for example by the Lévy–Prokhorov metric, if formula_21 is also compact or Polish, so is formula_68.\n\nIf formula_21 is separable, it naturally embeds into formula_68 as the (closed) set of dirac measures, and its convex hull is dense.\n\nThere are many \"arrow notations\" for this kind of convergence: the most frequently used are formula_78, formula_79 and formula_80.\n\nLet formula_81 be a probability space and X be a metric space. If is a sequence of random variables then \"X\" is said to converge weakly (or in distribution or in law) to \"X\" as if the sequence of pushforward measures (\"X\")(P) converges weakly to \"X\"(P) in the sense of weak convergence of measures on X, as defined above.\n\n\n"}
{"id": "7984037", "url": "https://en.wikipedia.org/wiki?curid=7984037", "title": "Correlation (projective geometry)", "text": "Correlation (projective geometry)\n\nIn projective geometry, a correlation is a transformation of a \"d\"-dimensional projective space that maps subspaces of dimension \"k\" to subspaces of dimension , reversing inclusion and preserving incidence. Correlations are also called reciprocities or reciprocal transformations.\n\nIn the real projective plane, points and lines are dual to each other. As expressed by Coxeter,\nGiven a line \"m\" and \"P\" a point not on \"m\", an elementary correlation is obtained as follows: for every \"Q\" on \"m\" form the line \"PQ\". The inverse correlation starts with the pencil on \"P\": for any line \"q\" in this pencil take the point . The composition of two correlations that share the same pencil is a perspectivity.\n\nIn a 3-dimensional projective space a correlation maps a point to a plane. As stated in one textbook:\n\nThree-dimensional correlations also transform lines into lines, so they may be considered to be collineations of the two spaces.\n\nIn general \"n\"-dimensional projective space, a correlation takes a point to a hyperplane. This context was described by Paul Yale:\nHe proves a theorem stating that a correlation \"φ\" interchanges joins and intersections, and for any projective subspace \"W\" of P(\"V\"), the dimension of the image of \"W\" under \"φ\" is , where \"n\" is the dimension of the vector space \"V\" used to produce the projective space P(\"V\").\n\nCorrelations can exist only if the space is self-dual. For dimensions 3 and higher, self-duality is easy to test: A coordinatizing skewfield exists and self-duality fails if and only if the skewfield is not isomorphic to its opposite.\n\nIf a correlation \"φ\" is an involution (that is, two applications of the correlation equals the identity: for all points \"P\") then it is called a polarity. Polarities of projective spaces lead to polar spaces, which are defined by taking the collection of all subspace which are contained in their image under the polarity.\n\nThere is a natural correlation induced between a projective space P(\"V\") and its dual P(\"V\") by the natural pairing between the underlying vector spaces \"V\" and its dual \"V\", where every subspace \"W\" of \"V\" is mapped to its orthogonal complement \"W\" in \"V\", defined as \n\nComposing this natural correlation with an isomorphism of projective spaces induced by a semilinear map produces a correlation of P(\"V\") to itself. In this way, every nondegenerate semilinear map induces a correlation of a projective space to itself.\n\n"}
{"id": "23296041", "url": "https://en.wikipedia.org/wiki?curid=23296041", "title": "Cyrus Colton MacDuffee", "text": "Cyrus Colton MacDuffee\n\nCyrus Colton MacDuffee (June 29, 1895 – August 21, 1961) from Oneida, New York was a professor of mathematics at University of Wisconsin.\nHe wrote a number of influential research papers in abstract algebra. MacDuffee served on the Council of the American Mathematical Society (A.M.S.), was editor of the \"Transactions of the A.M.S.\", and served as president of the Mathematical Association of America (M.A.A).\n\nMacDuffee obtained his B.S. degree in 1917 from Colgate University and a Ph.D. in 1922 from the University of Chicago ; his thesis was on Nonassociative algebras under the direction of Leonard E. Dickson. In 1935, MacDuffee joined the University of Wisconsin, where he remained until his death in 1961. He served as chair of the department(1951–56). Later, Wisconsin endowed a university chair under his name. Prior to joining the University of Wisconsin, he served at Princeton and Ohio State. He guided 30 Ph.D. students, among them D. R. Fulkerson, H. J. Ryser, and Bonnie Stewart.\n\n\n\n"}
{"id": "386468", "url": "https://en.wikipedia.org/wiki?curid=386468", "title": "Discrete geometry", "text": "Discrete geometry\n\nDiscrete geometry and combinatorial geometry are branches of geometry that study combinatorial properties and constructive methods of discrete geometric objects. Most questions in discrete geometry involve finite or discrete sets of basic geometric objects, such as points, lines, planes, circles, spheres, polygons, and so forth. The subject focuses on the combinatorial properties of these objects, such as how they intersect one another, or how they may be arranged to cover a larger object.\n\nDiscrete geometry has a large overlap with convex geometry and computational geometry, and is closely related to subjects such as finite geometry, combinatorial optimization, digital geometry, discrete differential geometry, geometric graph theory, toric geometry, and combinatorial topology.\n\nAlthough polyhedra and tessellations had been studied for many years by people such as Kepler and Cauchy, modern discrete geometry has its origins in the late 19th century. Early topics studied were: the density of circle packings by Thue, projective configurations by Reye and Steinitz, the geometry of numbers by Minkowski, and map colourings by Tait, Heawood, and Hadwiger.\n\nLászló Fejes Tóth, H.S.M. Coxeter and Paul Erdős, laid the foundations of \"discrete geometry\".\n\nA polytope is a geometric object with flat sides, which exists in any general number of dimensions. A polygon is a polytope in two dimensions, a polyhedron in three dimensions, and so on in higher dimensions (such as a 4-polytope in four dimensions). Some theories further generalize the idea to include such objects as unbounded polytopes (apeirotopes and tessellations), and abstract polytopes.\n\nThe following are some of the aspects of polytopes studied in discrete geometry:\n\nPackings, coverings, and tilings are all ways of arranging uniform objects (typically circles, spheres, or tiles) in a regular way on a surface or manifold.\n\nA sphere packing is an arrangement of non-overlapping spheres within a containing space. The spheres considered are usually all of identical size, and the space is usually three-dimensional Euclidean space. However, sphere packing problems can be generalised to consider unequal spheres, \"n\"-dimensional Euclidean space (where the problem becomes circle packing in two dimensions, or hypersphere packing in higher dimensions) or to non-Euclidean spaces such as hyperbolic space.\n\nA tessellation of a flat surface is the tiling of a plane using one or more geometric shapes, called tiles, with no overlaps and no gaps. In mathematics, tessellations can be generalized to higher dimensions.\n\nSpecific topics in this area include:\n\nStructural rigidity is a combinatorial theory for predicting the flexibility of ensembles formed by rigid bodies connected by flexible linkages or hinges.\n\nTopics in this area include:\n\nIncidence structures generalize planes (such as affine, projective, and Möbius planes) as can be seen from their axiomatic definitions. Incidence structures also generalize the higher-dimensional analogs and the finite structures are sometimes called finite geometries.\n\nFormally, an incidence structure is a triple\n\nwhere \"P\" is a set of \"points\", \"L\" is a set of \"lines\" and formula_2 is the incidence relation. The elements of formula_3 are called flags. If\n\nwe say that point \"p\" \"lies on\" line formula_5.\n\nTopics in this area include:\n\nAn oriented matroid is a mathematical structure that abstracts the properties of directed graphs and of arrangements of vectors in a vector space over an ordered field (particularly for partially ordered vector spaces). In comparison, an ordinary (i.e., non-oriented) matroid abstracts the dependence properties that are common both to graphs, which are not necessarily \"directed\", and to arrangements of vectors over fields, which are not necessarily \"ordered\".\n\nA geometric graph is a graph in which the vertices or edges are associated with geometric objects. Examples include Euclidean graphs, the 1-skeleton of a polyhedron or polytope, intersection graphs, and visibility graphs.\n\nTopics in this area include:\n\nA simplicial complex is a topological space of a certain kind, constructed by \"gluing together\" points, line segments, triangles, and their \"n\"-dimensional counterparts (see illustration). Simplicial complexes should not be confused with the more abstract notion of a simplicial set appearing in modern simplicial homotopy theory. The purely combinatorial counterpart to a simplicial complex is an abstract simplicial complex.\n\nThe discipline of combinatorial topology used combinatorial concepts in topology and in the early 20th century this turned into the field of algebraic topology.\n\nIn 1978, the situation was reversed – methods from algebraic topology were used to solve a problem in combinatorics – when László Lovász proved the Kneser conjecture, thus beginning the new study of topological combinatorics. Lovász's proof used the Borsuk-Ulam theorem and this theorem retains a prominent role in this new field. This theorem has many equivalent versions and analogs and has been used in the study of fair division problems.\n\nTopics in this area include:\n\nA discrete group is a group \"G\" equipped with the discrete topology. With this topology, \"G\" becomes a topological group. A discrete subgroup of a topological group \"G\" is a subgroup \"H\" whose relative topology is the discrete one. For example, the integers, Z, form a discrete subgroup of the reals, R (with the standard metric topology), but the rational numbers, Q, do not.\n\nA lattice in a locally compact topological group is a discrete subgroup with the property that the quotient space has finite invariant measure. In the special case of subgroups of R, this amounts to the usual geometric notion of a lattice, and both the algebraic structure of lattices and the geometry of the totality of all lattices are relatively well understood. Deep results of Borel, Harish-Chandra, Mostow, Tamagawa, M. S. Raghunathan, Margulis, Zimmer obtained from the 1950s through the 1970s provided examples and generalized much of the theory to the setting of nilpotent Lie groups and semisimple algebraic groups over a local field. In the 1990s, Bass and Lubotzky initiated the study of \"tree lattices\", which remains an active research area.\n\nTopics in this area include:\n\nDigital geometry deals with discrete sets (usually discrete point sets) considered to be digitized models or images of objects of the 2D or 3D Euclidean space.\n\nSimply put, digitizing is replacing an object by a discrete set of its points. The images we see on the TV screen, the raster display of a computer, or in newspapers are in fact digital images.\n\nIts main application areas are computer graphics and image analysis.\n\nDiscrete differential geometry is the study of discrete counterparts of notions in differential geometry. Instead of smooth curves and surfaces, there are polygons, meshes, and simplicial complexes. It is used in the study of computer graphics and topological combinatorics.\n\nTopics in this area include:\n\n\n\n"}
{"id": "36673510", "url": "https://en.wikipedia.org/wiki?curid=36673510", "title": "Douady–Earle extension", "text": "Douady–Earle extension\n\nIn mathematics, the Douady–Earle extension, named after Adrien Douady and Clifford Earle, is a way of extending homeomorphisms of the unit circle in the complex plane to homeomorphisms of the closed unit disk, such that the extension is a diffeomorphism of the open disk. The extension is analytic on the open disk. The extension has an important equivariance property: if the homeomorphism is composed on either side with a Möbius transformation preserving the unit circle the extension is also obtained by composition with the same Möbius transformation. If the homeomorphism is quasisymmetric, the diffeomorphism is quasiconformal. An extension for quasisymmetric homeomorphisms had previously been given by Ahlfors and Arne Beurling; a different equivariant construction had been given in 1985 by Pekka Tukia. Equivariant extensions have important applications in Teichmüller theory, for example they lead to a quick proof of the contractibility of the Teichmüller space of a Fuchsian group.\n\nBy the Radó–Kneser–Choquet theorem, the Poisson integral\n\nof a homeomorphism \"f\" of the circle defines a harmonic diffeomorphism of the unit disk extending \"f\". If \"f\" is quasisymmetric, the extension is not necessarily quasiconformal, i.e. the complex dilatation\n\ndoes not necessarily satisfy\n\nhave an upper bound strictly less than one.\n\nOn the other hand if \"f\" is quasi-Möbius and fixes 1, \"i\" and −\"i\", then \"f\" satisfies a Hölder continuity condition:\n\nfor another positive constant \"C\" independent of \"f\". The same is true for the \"f\"'s. But then the Arzelà–Ascoli theorem implies these homeomorphisms form a compact subset in C(T). The non-linear functional Λ is continuous on this subset and therefore attains its upper bound at some \"f\". On the other hand Λ(\"f\") < 1, so the upper bound is strictly less than 1.\n\nThe uniform Hölder estimate for \"f\" is established in as follows. Take \"z\", \"w\" in T.\n\n\n\nComment. In fact every quasi-Möbius homeomorphism \"f\" is also quasisymmetric. This follows using the Douady–Earle extension, since every quasiconformal homeomorphism of the unit disk induces a quasisymmetric homeomorphism of the unit circle. It can also be proved directly, following \n\n"}
{"id": "3536499", "url": "https://en.wikipedia.org/wiki?curid=3536499", "title": "E4M", "text": "E4M\n\nEncryption for the Masses (E4M) is a free disk encryption software for Windows NT and Windows 9x families of operating systems. E4M is no longer maintained. Its author, Paul Le Roux, joined Shaun Hollingworth (the author of the Scramdisk) to produce the commercial encryption product DriveCrypt for the security company SecurStar.\n\nThe popular source-available freeware program TrueCrypt was based on E4M's source code. However, TrueCrypt uses a different container format than E4M, which makes it impossible to use one of these programs to access an encrypted volume created by the other.\n\nShortly after TrueCrypt version 1.0 was released in February 2004, the TrueCrypt Team reported receiving emails from Wilfried Hafner, manager of SecurStar, claiming that Paul Le Roux had stolen the source code of E4M from SecurStar as an employee. According to the TrueCrypt Team, the emails stated that Le Roux illegally distributed E4M, and authored an illegal license permitting anyone to base derivative work on E4M and distribute it freely, which Hefner alleges Le Roux did not have any right to do, claiming that all versions of E4M always belonged only to SecurStar. For a time, this led the TrueCrypt Team to stop developing and\ndistributing TrueCrypt.\n\n"}
{"id": "4347776", "url": "https://en.wikipedia.org/wiki?curid=4347776", "title": "Epispiral", "text": "Epispiral\n\nThe epispiral is a plane curve with polar equation \nThere are \"n\" sections if \"n\" is odd and 2\"n\" if \"n\" is even.\n\nIt is the polar or circle inversion of the rose curve.\n\n\n"}
{"id": "310767", "url": "https://en.wikipedia.org/wiki?curid=310767", "title": "Equaliser (mathematics)", "text": "Equaliser (mathematics)\n\nIn mathematics, an equalizer is a set of arguments where two or more functions have equal values.\nAn equalizer is the solution set of an equation.\nIn certain contexts, a difference kernel is the equalizer of exactly two functions.\n\nLet \"X\" and \"Y\" be sets.\nLet \"f\" and \"g\" be functions, both from \"X\" to \"Y\".\nThen the \"equalizer\" of \"f\" and \"g\" is the set of elements \"x\" of \"X\" such that \"f\"(\"x\") equals \"g\"(\"x\") in \"Y\".\nSymbolically:\nThe equalizer may be denoted Eq(\"f\",\"g\") or a variation on that theme (such as with lowercase letters \"eq\").\nIn informal contexts, the notation {\"f\" = \"g\"} is common.\n\nThe definition above used two functions \"f\" and \"g\", but there is no need to restrict to only two functions, or even to only finitely many functions.\nIn general, if F is a set of functions from \"X\" to \"Y\", then the \"equalizer\" of the members of F is the set of elements \"x\" of \"X\" such that, given any two members \"f\" and \"g\" of F, \"f\"(\"x\") equals \"g\"(\"x\") in \"Y\".\nSymbolically:\nThis equalizer may be written as Eq(\"f\",\"g\",\"h\"...) if formula_3 is the set {\"f\",\"g\",\"h\"...}.\nIn the latter case, one may also find {\"f\" = \"g\" = \"h\" = ···} in informal contexts.\n\nAs a degenerate case of the general definition, let F be a singleton {\"f\"}.\nSince \"f\"(\"x\") always equals itself, the equalizer must be the entire domain \"X\".\nAs an even more degenerate case, let F be the empty set {}.\nThen the equalizer is again the entire domain \"X\", since the universal quantification in the definition is vacuously true.\n\nA binary equalizer (that is, an equalizer of just two functions) is also called a \"difference kernel\".\nThis may also be denoted DiffKer(\"f\",\"g\"), Ker(\"f\",\"g\"), or Ker(\"f\" − \"g\").\nThe last notation shows where this terminology comes from, and why it is most common in the context of abstract algebra:\nThe difference kernel of \"f\" and \"g\" is simply the kernel of the difference \"f\" − \"g\".\nFurthermore, the kernel of a single function \"f\" can be reconstructed as the difference kernel Eq(\"f\",0), where 0 is the constant function with value zero.\n\nOf course, all of this presumes an algebraic context where the kernel of a function is its preimage under zero; that is not true in all situations.\nHowever, the terminology \"difference kernel\" has no other meaning.\n\nEqualizers can be defined by a universal property, which allows the notion to be generalized from the category of sets to arbitrary categories.\n\nIn the general context, \"X\" and \"Y\" are objects, while \"f\" and \"g\" are morphisms from \"X\" to \"Y\".\nThese objects and morphisms form a diagram in the category in question, and the equalizer is simply the limit of that diagram.\n\nIn more explicit terms, the equalizer consists of an object \"E\" and a morphism \"eq\" : \"E\" → \"X\" satisfying formula_4,\nand such that, given any object \"O\" and morphism \"m\" : \"O\" → \"X\", if formula_5, then there exists a unique morphism \"u\" : \"O\" → \"E\" such that formula_6.\n\nA morphism formula_7 is said to equalize formula_8 and formula_9 if formula_5.\nIn any universal algebraic category, including the categories where difference kernels are used, as well as the category of sets itself, the object \"E\" can always be taken to be the ordinary notion of equalizer, and the morphism \"eq\" can in that case be taken to be the inclusion function of \"E\" as a subset of \"X\".\n\nThe generalization of this to more than two morphisms is straightforward; simply use a larger diagram with more morphisms in it.\nThe degenerate case of only one morphism is also straightforward; then \"eq\" can be any isomorphism from an object \"E\" to \"X\".\n\nThe correct diagram for the degenerate case with \"no\" morphisms is slightly subtle: one might initially draw the diagram as consisting of the objects \"X\" and \"Y\" and no morphisms. This is incorrect, however, since the limit of such a diagram is the product of \"X\" and \"Y\", rather than the equalizer. (And indeed products and equalizers are different concepts: the set-theoretic definition of product doesn't agree with the set-theoretic definition of the equalizer mentioned above, hence they are actually different.) Instead, the appropriate insight is that every equalizer diagram is fundamentally concerned with \"X\", including \"Y\" only because \"Y\" is the codomain of morphisms which appear in the diagram. With this view, we see that if there are no morphisms involved, \"Y\" does not make an appearance and the equalizer diagram consists of \"X\" alone. The limit of this diagram is then any isomorphism between \"E\" and \"X\".\n\nIt can be proved that any equalizer in any category is a monomorphism.\nIf the converse holds in a given category, then that category is said to be \"regular\" (in the sense of monomorphisms).\nMore generally, a regular monomorphism in any category is any morphism \"m\" that is an equalizer of some set of morphisms.\nSome authorities require (more strictly) that \"m\" be a \"binary\" equalizer, that is an equalizer of exactly two morphisms.\nHowever, if the category in question is complete, then both definitions agree.\n\nThe notion of difference kernel also makes sense in a category-theoretic context.\nThe terminology \"difference kernel\" is common throughout category theory for any binary equalizer.\nIn the case of a preadditive category (a category enriched over the category of Abelian groups), the term \"difference kernel\" may be interpreted literally, since subtraction of morphisms makes sense.\nThat is, Eq(\"f\",\"g\") = Ker(\"f\" - \"g\"), where Ker denotes the category-theoretic kernel.\n\nAny category with fibre products (pullbacks) and products has equalizers.\n\n\n"}
{"id": "12868362", "url": "https://en.wikipedia.org/wiki?curid=12868362", "title": "Filling radius", "text": "Filling radius\n\nIn Riemannian geometry, the filling radius of a Riemannian manifold \"X\" is a metric invariant of \"X\". It was originally introduced in 1983 by Mikhail Gromov, who used it to prove his systolic inequality for essential manifolds, vastly generalizing Loewner's torus inequality and Pu's inequality for the real projective plane, and creating systolic geometry in its modern form.\n\nThe filling radius of a simple loop \"C\" in the plane is defined as the largest radius, \"R\" > 0, of a circle that fits inside \"C\":\n\nThere is a kind of a dual point of view that allows one to generalize this notion in an extremely fruitful way, as shown by Gromov. Namely, we consider the formula_2-neighborhoods of the loop \"C\", denoted\n\nAs formula_4 increases, the formula_2-neighborhood formula_6 swallows up more and more of the interior of the loop. The \"last\" point to be swallowed up is precisely the center of a largest inscribed circle. Therefore, we can reformulate the above definition by defining \nformula_7 to be the infimum of formula_8 such that the loop \"C\" contracts to a point in formula_6.\n\nGiven a compact manifold \"X\" imbedded in, say, Euclidean space \"E\", we could define the filling radius \"relative\" to the imbedding, by minimizing the size of the neighborhood formula_10 in which \"X\" could be homotoped to something smaller dimensional, e.g., to a lower-dimensional polyhedron. Technically it is more convenient to work with a homological definition.\n\nDenote by \"A\" the coefficient ring formula_11 or formula_12, depending on whether or not \"X\" is orientable. Then the fundamental class, denoted \"[X]\", of a compact \"n\"-dimensional manifold \"X\", is a generator of the homology group formula_13, and we set \n\nwhere formula_15 is the inclusion homomorphism.\n\nTo define an \"absolute\" filling radius in a situation where \"X\" is equipped with a Riemannian metric \"g\", Gromov proceeds as follows. One exploits an imbedding due to Kazimierz Kuratowski (the first name is sometimes spelled with a \"C\"). One imbeds \"X\" in the Banach space formula_16 of bounded Borel functions on \"X\", equipped with the sup norm formula_17. Namely, we map a point formula_18 to the function formula_19 defined by the formula formula_20\nfor all formula_21, where \"d\" is the distance function defined by the metric. By the triangle inequality we have formula_22 and therefore the imbedding is strongly isometric, in the precise sense that internal distance and ambient distance coincide. Such a strongly isometric imbedding is impossible if the ambient space is a Hilbert space, even when \"X\" is the Riemannian circle (the distance between opposite points must be\n, not 2!). We then set formula_23 in the formula above, and define\n\n\n\n"}
{"id": "34287440", "url": "https://en.wikipedia.org/wiki?curid=34287440", "title": "François Thureau-Dangin", "text": "François Thureau-Dangin\n\nFrançois Thureau-Dangin (3 January 1872 in Paris – 24 January 1944 in Paris) was a French archaeologist, assyriologist and epigrapher. He played a major role in the deciphering of Sumerian and Akkadian languages.\n\nHe studied under Julius Oppert in Paris, and from 1895, was associated with duties performed at the Louvre, where in 1908, he was appointed assistant curator of the Oriental Antiquities department. On behalf of the museum, he conducted excavations at Arslan Tash (1927) and at Til Barsip (1929–1931).\n\nHe was a leading expert on Babylonian cuneiform texts, and worked on a theory concerning the origins of cuneiform writing, publishing the treatise \"Recherches sur l'origine de l'écriture cunéiforme\" (1898) as a result.\n\nAlong with Georges Dossin, he founded the \"Rencontre Assyriologique Internationale\", an association of orientalists, which hosts international events. He was a member of the \"Académie des inscriptions et belles lettres\" and a corresponding fellow of the British Academy.\n\n\n"}
{"id": "6871218", "url": "https://en.wikipedia.org/wiki?curid=6871218", "title": "Generalized complex structure", "text": "Generalized complex structure\n\nIn the field of mathematics known as differential geometry, a generalized complex structure is a property of a differential manifold that includes as special cases a complex structure and a symplectic structure. Generalized complex structures were introduced by Nigel Hitchin in 2002 and further developed by his students Marco Gualtieri and Gil Cavalcanti.\n\nThese structures first arose in Hitchin's program of characterizing geometrical structures via functionals of differential forms, a connection which formed the basis of Robbert Dijkgraaf, Sergei Gukov, Andrew Neitzke and Cumrun Vafa's 2004 proposal that topological string theories are special cases of a topological M-theory. Today generalized complex structures also play a leading role in physical string theory, as supersymmetric flux compactifications, which relate 10-dimensional physics to 4-dimensional worlds like ours, require (possibly twisted) generalized complex structures.\n\nConsider an \"N\"-manifold \"M\". The tangent bundle of \"M\", which will be denoted T, is the vector bundle over \"M\" whose fibers consist of all tangent vectors to \"M\". A section of T is a vector field on \"M\". The cotangent bundle of \"M\", denoted T, is the vector bundle over \"M\" whose sections are one-forms on \"M\".\n\nIn complex geometry one considers structures on the tangent bundles of manifolds. In symplectic geometry one is instead interested in exterior powers of the cotangent bundle. Generalized geometry unites these two fields by treating sections of the generalized tangent bundle, which is the direct sum T formula_1\nT of the tangent and cotangent bundles, which are formal sums of a vector field and a one-form.\n\nThe fibers are endowed with a natural inner product with signature (\"N\", \"N\"). If \"X\" and \"Y\" are vector fields and \"ξ\" and \"η\" are one-forms then the inner product of \"X+ξ\" and \"Y+η\" is defined as\n\nA generalized almost complex structure is just an almost complex structure of the generalized tangent bundle which preserves the natural inner product:\n\nsuch that\n\nLike in the case of an ordinary almost complex structure, a generalized almost complex structure is uniquely determined by its formula_5-eigenbundle, i.e. a subbundle formula_6 of the complexified generalized tangent bundle formula_7\ngiven by\n\nSuch subbundle \"L\" satisfies the following properties:\n\n(i) the intersection with its complex conjugate is the zero section: formula_9;\n\n(ii) \"L\" is maximal isotropic, i.e. its complex rank equals \"N\" and formula_10 for all formula_11\n\nVice versa, any subbundle \"L\" satisfying (i), (ii) is the formula_5-eigenbundle of a unique generalized almost complex structure, so that the properties (i), (ii) can be considered as an alternative definition of generalized almost complex structure.\n\nIn ordinary complex geometry, an almost complex structure is integrable to a complex structure if and only if the Lie bracket of two sections of the holomorphic subbundle is another section of the holomorphic subbundle.\n\nIn generalized complex geometry one is not interested in vector fields, but rather in the formal sums of vector fields and one-forms. A kind of Lie bracket for such formal sums was introduced in 1990 and is called the Courant bracket which is defined by\n\nwhere formula_14 is the Lie derivative along the vector field \"X\", \"d\" is the exterior derivative and \"i\" is the interior product.\n\nA generalized complex structure is a generalized almost complex structure such that the space of smooth sections of \"L\" is closed under the Courant bracket.\n\nThere is a one-to-one correspondence between maximal isotropic subbundle of T formula_1 T and pairs (E,\"ε\") where E is a subbundle of T and \"ε\" is a 2-form. This correspondence extends straightforwardly to the complex case.\n\nGiven a pair (E,\"ε\") one can construct a maximally isotropic subbundle \"L\"(E,\"ε\") of T formula_1 T as follows. The elements of the subbundle are the formal sums \"X\" + ξ where the vector field \"X\" is a section of E and the one-form \"ξ\" restricted to the dual space E is equal to the one-form ε(\"X\").\n\nTo see that \"L\"(E, \"ε\") is isotropic, notice that if \"Y\" is a section of E and \"ξ\" restricted to E is \"ε(X)\" then ξ(\"Y\") = ε(\"X\", \"Y\"), as the part of ξ orthogonal to E annihilates \"Y\". Thesefore if \"X\" + ξ and \"Y\" + η are sections of T formula_1 T then\n\nand so \"L\"(E, \"ε\") is isotropic. Furthermore, \"L\"(E, \"ε\") is maximal because there are dim(E) (complex) dimensions of choices for E, and \"ε\" is unrestricted on the complement of E, which is of (complex) dimension \"n\" − dim(E). Thus the total (complex) dimension in \"n\". Gualtieri has proven that all maximal isotropic subbundles are of the form \"L\"(E,ε) for some E and ε.\n\nThe type of a maximal isotropic subbundle \"L\"(E,ε) is the real dimension of the subbundle that annihilates E. Equivalently it is \"2N\" minus the real dimension of the projection of \"L\"(E,ε) onto the tangent bundle T. In other words, the type of a maximal isotropic subbundle is the codimension of its projection onto the tangent bundle. In the complex case one uses the complex dimension and the type is sometimes referred to as the complex type. While the type of a subbundle can in principle be any integer between 0 and \"2N\", generalized almost complex structures cannot have a type greater than \"N\" because the sum of the subbundle and its complex conjugate must be all of (Tformula_1\nT)formula_20C.\n\nThe type of a maximal isotropic subbundle is invariant under diffeomorphisms and also under shifts of the B-field, which are isometries of Tformula_1T of the form\nwhere \"B\" is an arbitrary closed 2-form called the B-field in the string theory literature.\n\nThe type of a generalized almost complex structure is in general not constant, it can jump by any even integer. However it is upper semi-continuous, which means that each point has an open neighborhood in which the type does not increase. In practice this means that subsets of greater type than the ambient type occur on submanifolds with positive codimension.\n\nThe real index \"r\" of a maximal isotropic subspace \"L\" is the complex dimension of the intersection of \"L\" with its complex conjugate. A maximal isotropic subspace of (Tformula_1 T) formula_20 C is a generalized almost complex structure if and only if \"r\" = 0.\n\nAs in the case of ordinary complex geometry, there is a correspondence between generalized almost complex structures and complex line bundles. The complex line bundle corresponding to a particular generalized almost complex structure is often referred to as the canonical bundle, as it generalizes the canonical bundle in the ordinary case. It is sometimes also called the pure spinor bundle, as its sections are pure spinors.\n\nThe canonical bundle is a one complex dimensional subbundle of the bundle ΛTformula_20C of complex differential forms on \"M\". Recall that the gamma matrices define an isomorphism between differential forms and spinors. In particular even and odd forms map to the two chiralities of Weyl spinors. Vectors have an action on differential forms given by the interior product. One-forms have an action on forms given by the wedge product. Thus sections of the bundle (T formula_1 T) formula_20 C act on differential forms. This action is a representation of the action of the Clifford algebra on spinors.\n\nA spinor is said to be a pure spinor if it is annihilated by half of a set of a set of generators of the Clifford algebra. Spinors are sections of our bundle ΛT, and generators of the Clifford algebra are the fibers of our other bundle (T formula_1 T) formula_20 C.\nTherefore, a given pure spinor is annihilated by a half-dimensional subbundle E of (T formula_1 T) formula_20 C.\nSuch subbundles are always isotropic, so to define an almost complex structure one must only impose that the sum of E and its complex conjugate is all of (T formula_1 T) formula_20 C. This is true whenever the wedge product of the pure spinor and its complex conjugate contains a top-dimensional component. Such pure spinors determine generalized almost complex structures.\n\nGiven a generalized almost complex structure, one can also determine a pure spinor up to multiplication by an arbitrary complex function. These choices of pure spinors are defined to be the sections of the canonical bundle.\n\nIf a pure spinor that determines a particular complex structure is closed, or more generally if its exterior derivative is equal to the action of a gamma matrix on itself, then the almost complex structure is integrable and so such pure spinors correspond to generalized complex structures.\n\nIf one further imposes that the canonical bundle is holomorphically trivial, meaning that it is global sections which are closed forms, then it defines a generalized Calabi-Yau structure and \"M\" is said to be a generalized Calabi-Yau manifold.\n\nLocally all pure spinors can be written in the same form, depending on an integer \"k\", the B-field 2-form \"B\", a nondegenerate symplectic form ω and a \"k\"-form Ω. In a local neighborhood of any point a pure spinor Φ which generates the canonical bundle may always be put in the form\nwhere Ω is decomposable as the wedge product of one-forms.\n\nDefine the subbundle E of the complexified tangent bundle Tformula_20C to be the projection of the holomorphic subbundle L of (Tformula_1T)\nformula_20C to Tformula_20C. In the definition of a generalized almost complex structure we have imposed that the intersection of L and its conjugate contains only the origin, otherwise they would be unable to span the entirety of (Tformula_1T)\nformula_20C. However the intersection of their projections need not be trivial. In general this intersection is of the form\nfor some subbundle Δ. A point which has an open neighborhood in which the dimension of the fibers of Δ is constant is said to be a regular point.\n\nEvery regular point in a generalized complex manifold has an open neighborhood which, after a diffeomorphism and shift of the B-field, has the same generalized complex structure as the Cartesian product of the complex vector space C and the standard symplectic space R with the standard symplectic form, which is the direct sum of the two by two off-diagonal matrices with entries 1 and -1.\n\nNear non-regular points, the above classification theorem does not apply. However, about any point, a generalized complex manifold is, up to diffeomorphism and B-field, a product of a symplectic manifold with a generalized complex manifold which is of complex type at the point, much like Weinstein's theorem for the local structure of Poisson manifolds. The remaining question of the local structure is: what does a generalized complex structure look like near a point of complex type? In fact, it will be induced by a holomorphic Poisson structure.\n\nThe space of complex differential forms ΛTformula_20C has a complex conjugation operation given by complex conjugation in C. This allows one to define holomorphic and antiholomorphic one-forms and (\"m, n\")-forms, which are homogeneous polynomials in these one-forms with \"m\" holomorphic factors and \"n\" antiholomorphic factors. In particular, all (\"n,0\")-forms are related locally by multiplication by a complex function and so they form a complex line bundle.\n\n(\"n,0\")-forms are pure spinors, as they are annihilated by antiholomorphic tangent vectors and by holomorphic one-forms. Thus this line bundle can be used as a canonical bundle to define a generalized complex structure. Restricting the annihilator from (Tformula_1\nT)formula_20C to the complexified tangent bundle one gets the subspace of antiholomorphic vector fields. Therefore, this generalized complex structure on (Tformula_1\nT)formula_20C defines an ordinary complex structure on the tangent bundle.\n\nAs only half of a basis of vector fields are holomorphic, these complex structures are of type \"N\". In fact complex manifolds, and the manifolds obtained by multiplying the pure spinor bundle defining a complex manifold by a complex, formula_47-closed (2,0)-form, are the only type \"N\" generalized complex manifolds.\n\nThe pure spinor bundle generated by\n\nfor a nondegenerate two-form \"ω\" defines a symplectic structure on the tangent space. Thus symplectic manifolds are also generalized complex manifolds.\n\nThe above pure spinor is globally defined, and so the canonical bundle is trivial. This means that symplectic manifolds are not only generalized complex manifolds but in fact are generalized Calabi-Yau manifolds.\n\nThe pure spinor formula_49 is related to a pure spinor which is just a number by an imaginary shift of the B-field, which is a shift of the Kähler form. Therefore, these generalized complex structures are of the same type as those corresponding to a scalar pure spinor. A scalar is annihilated by the entire tangent space, and so these structures are of type \"0\".\n\nUp to a shift of the B-field, which corresponds to multiplying the pure spinor by the exponential of a closed, real 2-form, symplectic manifolds are the only type 0 generalized complex manifolds. Manifolds which are symplectic up to a shift of the B-field are sometimes called B-symplectic.\n\nSome of the almost structures in generalized complex geometry may be rephrased in the language of G-structures. The word \"almost\" is removed if the structure is integrable.\n\nThe bundle (Tformula_1T) formula_20 C with the above inner product is an O(2\"n\", 2\"n\") structure. A generalized almost complex structure is a reduction of this structure to a U(\"n\", \"n\") structure. Therefore, the space of generalized complex structures is the coset\n\nA generalized almost Kähler structure is a pair of commuting generalized complex structures such that minus the product of the corresponding tensors is a positive definite metric on (Tformula_1\nT)formula_20C.\nGeneralized Kähler structures are reductions of the structure group to U(\"n\")formula_55U(\"n\"). Generalized Kähler manifolds, and their twisted counterparts, are equivalent to the bihermitian manifolds discovered by Sylvester James Gates, Chris Hull and Martin Roček in the context of 2-dimensional supersymmetric quantum field theories in 1984.\n\nFinally, a generalized almost Calabi-Yau metric structure is a further reduction of the structure group to SU(\"n\")formula_55SU(\"n\").\n\nNotice that a generalized Calabi metric structure, which was introduced by Marco Gualtieri, is a stronger condition than a generalized Calabi–Yau structure, which was introduced by Nigel Hitchin. In particular a generalized Calabi–Yau metric structure implies the existence of two commuting generalized almost complex structures.\n\n"}
{"id": "2035678", "url": "https://en.wikipedia.org/wiki?curid=2035678", "title": "Gibbs' inequality", "text": "Gibbs' inequality\n\nIn information theory, Gibbs' inequality is a statement about the mathematical entropy of a discrete probability distribution. Several other bounds on the entropy of probability distributions are derived from Gibbs' inequality, including Fano's inequality.\nIt was first presented by J. Willard Gibbs in the 19th century.\n\nSuppose that\n\nis a probability distribution. Then for any other probability distribution\n\nthe following inequality between positive quantities (since the p and q are positive numbers less than one) holds\n\nwith equality if and only if\n\nfor all \"i\". Put in words, the information entropy of a distribution P is less than or equal to its cross entropy with any other distribution Q.\n\nThe difference between the two quantities is the Kullback–Leibler divergence or relative entropy, so the inequality can also be written:\n\nNote that the use of base-2 logarithms is optional, and \nallows one to refer to the quantity on each side of the inequality as an \n\"average surprisal\" measured in bits.\n\nFor simplicity we prove the statement using the natural logarithm (ln), since\n\nthe particular logarithm we choose only scales the relationship.\n\nLet formula_7 denote the set of all formula_8 for which \"p\" is non-zero. Then, since formula_9 for all \"x > 0\", with equality if and only if \"x=1\", we have:\n\nThe last inequality is a consequence of the \"p\" and \"q\" being part of a probability distribution. Therefore, the sum of all values is unity. Specifically, the sum of all non-zero values is also unity, however, some non-zero \"q\" may be excluded since the choice of indices is conditioned upon the \"p\". Therefore the sum of the \"q\" may be less than unity.\n\nWe now have:\n\nSince the \"p\" and \"q\" are probabilities, their logarithms are negative. The negation of the first sum is thus positive, and the un-negated second sum is negative. We may therefore add the negation of the second sum to both sides (a positive number) without changing the inequality to get:\n\nSince the logarithm of zero is negative infinity, restoring the indices for values of \"p\" that are zero requires some care. We notice that:\n\nThe quotient of zeros is an indeterminate form. Typically it is defined to be the convergent of asymptotic values in its neigborhood, or, if such a convergent does not exist, a convention convenient to the case at hand is adopted. In this context, the usual convention is to take the indeterminate form to be identically zero. This gives us:\n\nThe right hand side does not grow by our convention, and the left hand side does not grow either because zero times anything is zero, or by convention when \"q\" is also zero.\n\nFor equality to hold, we require:\n\nThis can happen if and only if\n\nfor \"i\" = 1, ..., \"n\".\n\nThe result can alternatively be proved using Jensen's inequality or log sum inequality. Below we give a proof based on Jensen's inequality:\n\nBecause log is a concave function, we have that:\n\nWhere the first inequality is due to Jensen's inequality, and the last equality is because formula_25 is a probability distribution.\n\nFurther, because formula_26 is not linear, therefore by the equality condition of Jensen's inequality, we get equality when\n\nSuppose that this ratio is formula_28, then we have that\n\nWhere we use the fact that formula_30 are probability distributions. Therefore the equality happens when formula_31.\n\nThe entropy of formula_32 is bounded by:\n\nThe proof is trivial – simply set formula_34 for all \"i\".\n\n"}
{"id": "50741202", "url": "https://en.wikipedia.org/wiki?curid=50741202", "title": "Golden Star (technical analysis)", "text": "Golden Star (technical analysis)\n\nGolden Star is a technical signal used in technical analysis of stock prices charts that first was introduced in 2007 by J. Stromberg at the website getagraph.com. The signal derives from the well known Golden Cross that uses the 200 days moving average and the 50 days moving averages to define what has been argued by many to be the best technical signal of them all. Some, however, disagree, and as late as in April 2016 Kate Rooney publish the article \"Don't trust the Dow's 'golden cross': Belski\", at CNN (Kate Rooney, 2016) where Belski states.\n\nThe Golden Star signal differs from the Golden Cross by adding more conditions. The moving averages has to be selected based on the time frame selected. In addition, it has to cross the price line at a given pattern as shown below.\n\n"}
{"id": "7830441", "url": "https://en.wikipedia.org/wiki?curid=7830441", "title": "Grandi's series", "text": "Grandi's series\n\nIn mathematics, the infinite series formula_1, also written\nis sometimes called Grandi's series, after Italian mathematician, philosopher, and priest Guido Grandi, who gave a memorable treatment of the series in 1703. It is a divergent series, meaning that it lacks a sum in the usual sense. On the other hand, its Cesàro sum is 1/2.\n\nOne obvious method to attack the series\nis to treat it like a telescoping series and perform the subtractions in place:\n\nOn the other hand, a similar bracketing procedure leads to the apparently contradictory result\n\nThus, by applying parentheses to Grandi's series in different ways, one can obtain either 0 or 1 as a \"value\". (Variations of this idea, called the Eilenberg–Mazur swindle, are sometimes used in knot theory and algebra.)\n\nTreating Grandi's series as a divergent geometric series and using the same algebraic methods that evaluate convergent geometric series to obtain a third value:\nresulting in \"S\" = 1/2.\nThe same conclusion results from calculating −\"S\", subtracting the result from \"S\", and solving 2\"S\" = 1.\n\nThe above manipulations do not consider what the sum of a series actually means and how said algebraic methods can be applied to divergent geometric series. Still, to the extent that it is important to be able to bracket series at will, and that it is more important to be able to perform arithmetic with them, one can arrive at two conclusions:\nIn fact, both of these statements can be made precise and formally proven, but only using well-defined mathematical concepts that arose in the 19th century. After the late 17th-century introduction of calculus in Europe, but before the advent of modern rigor, the tension between these answers fueled what has been characterized as an \"endless\" and \"violent\" dispute between mathematicians.\n\nThe formula for the sum to infinity of a Geometric series is formula_3, although the derivation for the sum is only valid when formula_4 (as with the modulus of r less than one then the terms of the series will decay to zero as n increases and the series will converge).\nIn the case of formula_5 with formula_6 and formula_7, the above formula would equate to formula_8 but since the series does not converge it is misleading to speak of its \"sum\".\n\nIn modern mathematics, the sum of an infinite series is defined to be the limit of the sequence of its partial sums, if it exists. The sequence of partial sums of Grandi's series is which clearly does not approach any number (although it does have two accumulation points at 0 and 1). Therefore, Grandi's series is divergent.\n\nIt can be shown that it is not valid to perform many seemingly innocuous operations on a series, such as reordering individual terms, unless the series is absolutely convergent. Otherwise these operations can alter the result of summation. Further, the terms of Grandi's series can be rearranged to have its accumulation points at any interval of two or more consecutive integer numbers, not only 0 or 1. For instance, the series\n(in which, after five initial +1 terms, the terms alternate in pairs of +1 and −1 terms) is a permutation of Grandi's series in which each value in the rearranged series corresponds to a value that is at most four positions away from it in the original series; its accumulation points are 3, 4, and 5.\n\n\n\n"}
{"id": "8518192", "url": "https://en.wikipedia.org/wiki?curid=8518192", "title": "Incomplete Cholesky factorization", "text": "Incomplete Cholesky factorization\n\nIn numerical analysis, an incomplete Cholesky factorization of a symmetric positive definite matrix is a sparse approximation of the Cholesky factorization. An incomplete Cholesky factorization is often used as a preconditioner for algorithms like the conjugate gradient method.\n\nThe Cholesky factorization of a positive definite matrix \"A\" is \"A\" = \"LL\"* where \"L\" is a lower triangular matrix. An incomplete Cholesky factorization is given by a sparse lower triangular matrix \"K\" that is in some sense close to \"L\". The corresponding preconditioner is \"KK\"*. \n\nOne popular way to find such a matrix \"K\" is to use the algorithm for finding the exact Cholesky decomposition, except that any entry is set to zero if the corresponding entry in \"A\" is also zero. This gives an incomplete Cholesky factorization which is as sparse as the matrix \"A\".\n\nFor formula_1 from formula_2 to formula_3:\n\nImplementation of the incomplete Cholesky factorization in the Octave scripting language. The factorization is stored as a lower triangular matrix, with the elements in the upper triangle set to zero.\n\n"}
{"id": "1141208", "url": "https://en.wikipedia.org/wiki?curid=1141208", "title": "Independence (mathematical logic)", "text": "Independence (mathematical logic)\n\nIn mathematical logic, independence refers to the unprovability of a sentence from other sentences.\nA sentence σ is independent of a given first-order theory \"T\" if \"T\" neither proves nor refutes σ; that is, it is impossible to prove σ from \"T\", and it is also impossible to prove from \"T\" that σ is false. Sometimes, σ is said (synonymously) to be undecidable from \"T\"; this is not the same meaning of \"decidability\" as in a decision problem.\n\nA theory \"T\" is independent if each axiom in \"T\" is not provable from the remaining axioms in \"T\". A theory for which there is an independent set of axioms is independently axiomatizable.\n\nSome authors say that σ is independent of \"T\" when \"T\" simply cannot prove σ, and do not necessarily assert by this that \"T\" cannot refute σ. These authors will sometimes say \"σ is independent of and consistent with \"T\"\" to indicate that \"T\" can neither prove nor refute σ.\n\nMany interesting statements in set theory are independent of Zermelo–Fraenkel set theory (ZF). The following statements in set theory are known to be independent of ZF, under the assumption that ZF is consistent:\n\nThe following statements (none of which have been proved false) cannot be proved in ZFC (the Zermelo-Fraenkel set theory plus the axiom of choice) to be independent of ZFC, under the added hypothesis that ZFC is consistent. \n\n\nThe following statements are inconsistent with the axiom of choice, and therefore with ZFC. However they are probably independent of ZF, in a corresponding sense to the above: They cannot be proved in ZF, and few working set theorists expect to find a refutation in ZF. However ZF cannot prove that they are independent of ZF, even with the added hypothesis that ZF is consistent.\n\n\nSince 2000, logical independence has become understood as having crucial significance in the foundations of physics.\n\n\n"}
{"id": "53292762", "url": "https://en.wikipedia.org/wiki?curid=53292762", "title": "James Keener", "text": "James Keener\n\nJames \"Jim\" Paul Keener is an American mathematician, currently Distinguished Professor at University of Utah. He is recognized as a pioneer in the field of mathematical physiology and cardiology.\n\nJim Keener received his PhD from the California Institute of Technology in 1972. Initially intending to work on bifurcation theory, he came across a paper by Otto Rossler that implied that heartbeat can be modeled using chaos theory. Looking to investigate this claim, he picked up the \"Textbook of Medical Physiology\" by Arthur Guyton to build some foundational knowledge in cardiology and discovered examples of dynamical systems that had previously been untouched by the applied mathematics community. He was invited to join the faculty at the University of Utah in 1978 by Frank Hoppensteadt to start a new group in mathematical biology. He served as editor-in-chief of the SIAM Journal on Applied Mathematics and was named a SIAM Fellow in 2012.\n\n"}
{"id": "22788078", "url": "https://en.wikipedia.org/wiki?curid=22788078", "title": "János Komlós (mathematician)", "text": "János Komlós (mathematician)\n\nJános Komlós (Budapest, 23 May 1942) is a Hungarian-American mathematician, working in probability theory and discrete mathematics. He has been a professor of mathematics at Rutgers University since 1988. He graduated from the Eötvös Loránd University, then became a fellow at the Mathematical Institute of the Hungarian Academy of Sciences. Between 1984–1988 he worked at the University of California, San Diego.\n\n\nformula_1\nformula_2\n\nKomlós received his Ph.D. in 1967 from Eötvös Loránd University under the supervision of Alfréd Rényi. In 1975 he received the Alfréd Rényi Prize, a prize established for researchers of the Alfréd Rényi Institute of Mathematics. In 1998 he was elected as an external member to the Hungarian Academy of Sciences.\n\n"}
{"id": "42826100", "url": "https://en.wikipedia.org/wiki?curid=42826100", "title": "Kleinian integer", "text": "Kleinian integer\n\nIn mathematical cryptography, a Kleinian integer is a complex number of the form formula_1, with \"m\" and \"n\" rational integers. They are named after Felix Klein.\n\nThe Kleinian integers form a ring called the Kleinian ring, which is the ring of integers in the imaginary quadratic field formula_2. This ring is a unique factorization domain.\n\n\n"}
{"id": "19575563", "url": "https://en.wikipedia.org/wiki?curid=19575563", "title": "Linear inequality", "text": "Linear inequality\n\nIn mathematics a linear inequality is an inequality which involves a linear function. A linear inequality contains one of the symbols of inequality:. It shows the data which is not equal in graph form.\n\nA linear inequality looks exactly like a linear equation, with the inequality sign replacing the equality sign.\n\nTwo-dimensional linear inequalities are expressions in two variables of the form:\nwhere the inequalities may either be strict or not. The solution set of such an inequality can be graphically represented by a half-plane (all the points on one \"side\" of a fixed line) in the Euclidean plane. The line that determines the half-planes (\"ax\" + \"by\" = \"c\") is not included in the solution set when the inequality is strict. A simple procedure to determine which half-plane is in the solution set is to calculate the value of \"ax\" + \"by\" at a point (\"x\", \"y\") which is not on the line and observe whether or not the inequality is satisfied.\n\nFor example, to draw the solution set of \"x\" + 3\"y\" < 9, one first draws the line with equation \"x\" + 3\"y\" = 9 as a dotted line, to indicate that the line is not included in the solution set since the inequality is strict. Then, pick a convenient point not on the line, such as (0,0). Since 0 + 3(0) = 0 < 9, this point is in the solution set, so the half-plane containing this point (the half-plane \"below\" the line) is the solution set of this linear inequality.\n\nIn R linear inequalities are the expressions that may be written in the form\n\nwhere \"f\" is a linear form (also called a \"linear functional\"), formula_4 and \"b\" a constant real number.\nMore concretely, this may be written out as\nor\n\nHere formula_7 are called the unknowns, and formula_8 are called the coefficients.\n\nAlternatively, these may be written as\n\nwhere \"g\" is an affine function.\n\nThat is\nor\n\nNote that any inequality containing a \"greater than\" or a \"greater than or equal\" sign can be rewritten with a \"less than\" or \"less than or equal\" sign, so there is no need to define linear inequalities using those signs.\n\nA system of linear inequalities is a set of linear inequalities in the same variables:\n\nHere formula_14 are the unknowns, formula_15 are the coefficients of the system, and formula_16 are the constant terms.\n\nThis can be concisely written as the matrix inequality\n\nwhere \"A\" is an \"m\"×\"n\" matrix, \"x\" is an \"n\"×1 column vector of variables, and \"b\" is an \"m\"×1 column vector of constants.\n\nIn the above systems both strict and non-strict inequalities may be used.\n\n\nThe set of solutions of a real linear inequality constitutes a half-space of the 'n'-dimensional real space, one of the two defined by the corresponding linear equation.\n\nThe set of solutions of a system of linear inequalities corresponds to the intersection of the half-spaces defined by individual inequalities. It is a convex set, since the half-spaces are convex sets, and the intersection of a set of convex sets is also convex. In the non-degenerate cases this convex set is a convex polyhedron (possibly unbounded, e.g., a half-space, a slab between two parallel half-spaces or a polyhedral cone). It may also be empty or a convex polyhedron of lower dimension confined to an affine subspace of the \"n\"-dimensional space R.\n\nA linear programming problem seeks to optimize (find a maximum or minimum value) a function (called the \"objective function\") subject to a number of constraints on the variables which, in general, are linear inequalities. The list of constraints is a system of linear inequalities.\n\nThe above definition requires well-defined operations of addition, multiplication and comparison; therefore, the notion of a linear inequality may be extended to ordered rings, and in particular to ordered fields.\n\n\n"}
{"id": "7820612", "url": "https://en.wikipedia.org/wiki?curid=7820612", "title": "Lipschitz domain", "text": "Lipschitz domain\n\nIn mathematics, a Lipschitz domain (or domain with Lipschitz boundary) is a domain in Euclidean space whose boundary is \"sufficiently regular\" in the sense that it can be thought of as locally being the graph of a Lipschitz continuous function. The term is named after the German mathematician Rudolf Lipschitz.\n\nSuch domains are also called strongly Lipschitz domains to contrast them with weakly Lipschitz domains, which are a more general class of domains. A weakly Lipschitz domain is a domain whose boundary is locally flattable by a Lipschitzeomorphism.\n\nLet formula_1.\nLet formula_2 be an open subset of formula_3\nand let formula_4 denote the boundary of formula_2. \nThen formula_2 is called a Lipschitz domain \nif for every point formula_7\nthere exists a hyperplane formula_8 of dimension formula_9 through formula_10,\na Lipschitz-continuous function formula_11 over that hyperplane,\nand the values formula_12 and formula_13 such that \nwhere \n\nMore generally, formula_2 is said to be weakly Lipschitz if for every point formula_21, there exists a radius formula_12 and a map formula_23 such that\nwhere formula_29 denotes the unit ball formula_30 in formula_31 and\n\nMany of the Sobolev embedding theorems require that the domain of study be a Lipschitz domain. Consequently, many partial differential equations and variational problems are defined on Lipschitz domains.\n"}
{"id": "50248508", "url": "https://en.wikipedia.org/wiki?curid=50248508", "title": "List of operator splitting topics", "text": "List of operator splitting topics\n\nThis is a list of operator splitting topics.\n\n"}
{"id": "6707908", "url": "https://en.wikipedia.org/wiki?curid=6707908", "title": "Loss reserving", "text": "Loss reserving\n\nLoss reserving refers to the calculation of the required reserves for a tranche of general insurance business. It includes outstanding claims reserves.\n\nTypically, the claims reserves represent the money which should be held by the insurer so as to be able to meet all future claims arising from policies currently in force and policies written in the past.\n\nMethods of calculating reserves in general insurance are different from those used in life insurance, pensions and health insurance since general insurance contracts are typically of a much shorter duration. Most general insurance contracts are written for a period of one year, and typically there is only one payment of premium at the start of the contract in exchange for coverage over the year. Reserves are calculated differently from contracts of a longer duration with multiple premium payments since there are no future premiums to consider in this case. The reserves are calculated by forecasting future losses from past losses.\n\nThe most popular methods of claims reserving include the chain-ladder method and the Bornhuetter-Ferguson method.\n\nThe chain-ladder method, also known as the development method, assumes that past experience is an indicator of future experience. Loss development patterns in the past are used to estimate how claim amounts will increase (or decrease) in the future.\n\nThe Bornhuetter-Ferguson method uses both past loss development as well as an independently derived prior estimate of ultimate expected losses.\n\nOutstanding claims reserves in general insurance are a type of technical reserve or accounting provision in the financial statements of an insurer. They seek to quantify the have loss liabilities for insurance claims which have been reported and not yet settled\n(RBNS) or which have been incurred but not yet reported (IBNR) reserves. This is a technical reserve of an insurance company, and is established to provide for the future liability for claims which have occurred but which have not yet been settled.\n\nAn insurance policy provides, in return for the payment of a premium, acceptance of the liability to make payments to the insured person on the occurrence of one or more specified events (insurance claims) over a specific time period. The occurrence of the specified events and the amount of the payment are both usually modeled as random variables. In general, there is a delay in the insurer's settlement of the claim, typical reasons are (i) reporting delay (time gap between\nclaims occurrence and claims reporting at the insurance company); (ii) settlement delay because\nit usually takes time to evaluate the whole size of the claim. The time difference between\nclaims occurrence and claims closing (final settlement) can take days (e.g. in property insurance)\nbut it can also take years (typically in liability insurance).\n\nClaims reserving now means, that the insurance company puts sufficient provisions from the premium payments aside, so that it is able to settle all the claims that\nare caused by these insurance contracts. This is different from social insurance where one typically has a pay-as-you-go system which means that premium payments are not matched to the contracts that cause the claims\n\nVarious statistical methods have been established for the calculation of outstanding claims reserves in general insurance. These include:\n\n\nMost of these methods started off as \"deterministic\" algorithms. Later actuaries started to develop and analyze underlying stochastic models that justify these algorithms. Probably, the most popular stochastic model is the distribution-free chain ladder method which was developed by T. Mack. These stochastic methods allow to analyze and quantify the prediction uncertainty in the outstanding loss liabilities. Classical analysis studies the total prediction uncertainty, whereas recent research (under the influence of Solvency 2) also studies the one-year uncertainty, called claims development result (CDR).\n\n\n"}
{"id": "9524572", "url": "https://en.wikipedia.org/wiki?curid=9524572", "title": "M8 (cipher)", "text": "M8 (cipher)\n\nIn cryptography, M8 is a block cipher designed by Hitachi in 1999. It is a modification of Hitachi's earlier M6 algorithm, designed for greater security and high performance in both hardware and 32-bit software implementations.\n\nLike M6, M8 is a 10-round Feistel cipher with a block size of 64 bits. The round function can include bit rotations, XORs, and modular addition, but the structure of each round function used is determined by the key. Making these variations key-dependent is intended to make cryptanalysis more difficult (see FROG for a similar design philosophy).\n"}
{"id": "27699417", "url": "https://en.wikipedia.org/wiki?curid=27699417", "title": "Morton Brown", "text": "Morton Brown\n\nMorton Brown (born August 12, 1931, in New York City, New York) is an American mathematician, who specializes in geometric topology.\n\nIn 1958 Brown earned his Ph.D. from the University of Wisconsin-Madison under R. H. Bing. From 1960 to 1962 he was at the Institute for Advanced Study. Afterwards he became a professor at the University of Michigan at Ann Arbor.\n\nWith Barry Mazur in 1965 he won the Oswald Veblen prize for their independent and nearly simultaneous proofs of the generalized Schoenflies hypothesis in geometric topology. Brown's short proof was elementary and fully general. Mazur's proof was also elementary, but it used a special assumption which was removed via later work of Morse.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "271164", "url": "https://en.wikipedia.org/wiki?curid=271164", "title": "Nomogram", "text": "Nomogram\n\nA nomogram (from Greek νόμος \"nomos\", \"law\" and γραμμή \"grammē\", \"line\"), also called a nomograph, alignment chart or abaque, is a graphical calculating device, a two-dimensional diagram designed to allow the approximate graphical computation of a mathematical function. The field of nomography was invented in 1884 by the French engineer Philbert Maurice d’Ocagne (1862-1938) and used extensively for many years to provide engineers with fast graphical calculations of complicated formulas to a practical precision. Nomograms use a parallel coordinate system invented by d'Ocagne rather than standard Cartesian coordinates.\n\nA nomogram consists of a set of n scales, one for each variable in an equation. Knowing the values of n-1 variables, the value of the unknown variable can be found, or by fixing the values of some variables, the relationship between the unfixed ones can be studied. The result is obtained by laying a straightedge across the known values on the scales and reading the unknown value from where it crosses the scale for that variable. The virtual or drawn line created by the straightedge is called an \"index line\" or \"isopleth\".\n\nNomograms flourished in many different contexts for roughly 75 years because they allowed quick and accurate computations before the age of pocket calculators. Results from a nomogram are obtained very quickly and reliably by simply drawing one or more lines. The user does not have to know how to solve algebraic equations, look up data in tables, use a slide rule, or substitute numbers into equations to obtain results. The user does not even need to know the underlying equation the nomogram represents. In addition, nomograms naturally incorporate implicit or explicit domain knowledge into their design. For example, to create larger nomograms for greater accuracy the nomographer usually includes only scale ranges that are reasonable and of interest to the problem. Many nomograms include other useful markings such as reference labels and colored regions. All of these provide useful guideposts to the user. \nLike a slide rule, a nomogram is a graphical analog computation device, and like the slide rule, its accuracy is limited by the precision with which physical markings can be drawn, reproduced, viewed, and aligned. While the slide rule is intended to be a general-purpose device, a nomogram is designed to perform a specific calculation, with tables of values effectively built into the construction of the scales. Nomograms are typically used in applications where the level of accuracy they offer is sufficient and useful. Alternatively, a nomogram can be used to check an answer obtained from another, more exact but possibly error-prone calculation. \n\nOther types of graphical calculators such as intercept charts, trilinear diagrams and hexagonal charts are sometimes called nomograms. Other such examples include the Smith chart, a graphical calculator used in electronics and systems analysis, thermodynamic diagrams and tephigrams, used to plot the vertical structure of the atmosphere and perform calculations on its stability and humidity content. These do not meet the strict definition of a nomogram as a graphical calculator whose solution is found by the use of one or more linear isopleths.\n\nA nomogram for a three-variable equation typically has three scales, although there exist nomograms in which two or even all three scales are common. Here two scales represent known values and the third is the scale where the result is read off. The simplest such equation is \"u\" + \"u\" + \"u\" = 0 for the three variables \"u\", \"u\" and \"u\". An example of this type of nomogram is shown on the right, annotated with terms used to describe the parts of a nomogram.\n\nMore complicated equations can sometimes be expressed as the sum of functions of the three variables. For example, the nomogram at the top of this article could be constructed as a parallel-scale nomogram because it can be expressed as such a sum after taking logarithms of both sides of the equation.\n\nThe scale for the unknown variable can lie between the other two scales or outside of them. The known values of the calculation are marked on the scales for those variables, and a line is drawn between these marks. The result is read off the unknown scale at the point where the line intersects that scale. The scales include 'tick marks' to indicate exact number locations, and they may also include labeled reference values. These scales may be linear, logarithmic, or have some more complex relationship.\n\nThe sample isopleth shown in red on the nomogram at the top of this article calculates the value of \"T\" when \"S\" = 7.30 and \"R\" = 1.17. The isopleth crosses the scale for \"T\" at just under 4.65; a larger figure printed in high resolution on paper would yield \"T\" = 4.64 to three-digit precision. Note that any variable can be calculated from values of the other two, a feature of nomograms that is particularly useful for equations in which a variable cannot be algebraically isolated from the other variables.\n\nStraight scales are useful for relatively simple calculations, but for more complex calculations the use of simple or elaborate curved scales may be required. Nomograms for more than three variables can be constructed by incorporating a grid of scales for two of the variables, or by concatenating individual nomograms of fewer numbers of variables into a compound nomogram.\n\nNomograms have been used in an extensive array of applications. A sample includes\n\n\nThe nomogram below performs the computation\n\nThis nomogram is interesting because it performs a useful nonlinear calculation using only straight-line, equally graduated scales. While the diagonal line has a scale formula_2 times larger than the axes scales, the numbers on it exactly match those directly below or to its left, and thus it can be easily created by drawing a straight line diagonally on a sheet of graph paper.\n\n\"A\" and \"B\" are entered on the horizontal and vertical scales, and the result is read from the diagonal scale. Being proportional to the harmonic mean of \"A\" and \"B\", this formula has several applications. For example, it is the parallel-resistance formula in electronics, and the thin-lens equation in optics.\n\nIn the example, the red line demonstrates that parallel resistors of 56 and 42 ohms have a combined resistance of 24 ohms. It also demonstrates that an object at a distance of 56 cm from a lens whose focal length is 24 cm forms a real image at a distance of 42 cm.\n\nThe nomogram below can be used to perform an approximate computation of some values needed when performing a familiar statistical test, Pearson's chi-squared test. This nomogram demonstrates the use of curved scales with unevenly spaced graduations.\n\nThe relevant expression is\n\nThe scale along the top is shared among five different ranges of observed values: A, B, C, D and E. The observed value is found in one of these ranges, and the tick mark used on that scale is found immediately above it. Then the curved scale used for the expected value is selected based on the range. For example, an observed value of 9 would use the tick mark above the 9 in range A, and curved scale A would be used for the expected value. An observed value of 81 would use the tick mark above 81 in range E, and curved scale E would be used for the expected value. This allows five different nomograms to be incorporated into a single diagram.\n\nIn this manner, the blue line demonstrates the computation of\n\nand the red line demonstrates the computation of\n\nIn performing the test, Yates's correction for continuity is often applied, and simply involves subtracting 0.5 from the observed values. A nomogram for performing the test with Yates's correction could be constructed simply by shifting each \"observed\" scale half a unit to the left, so that the 1.0, 2.0, 3.0, ... graduations are placed where the values 0.5, 1.5, 2.5, ... appear on the present chart.\n\nAlthough nomograms represent mathematical relationships, not all are mathematically derived. The following one was developed graphically to achieve appropriate end results that could readily be defined by the product of their relationships in subjective units rather than numerically. The use of non-parallel axes enabled the non-linear relationships to be incorporated into the model.\n\nThe numbers in square boxes denote the axes requiring input after appropriate assessment.\n\nThe pair of nomograms at the top of the image determine the probability of occurrence and the availability, which are then incorporated into the bottom multistage nomogram.\n\nLines 8 and 10 are ‘tie lines’ or ‘pivot lines’ and are used for the transition between the stages of the compound nomogram.\n\nThe final pair of parallel logarithmic scales (12) are not nomograms as such, but reading-off scales to translate the risk score (11, remote to extremely high) into a sampling frequency to address safety aspects and other ‘consumer protection’ aspects respectively. This stage requires political ‘buy in’ balancing cost against risk. The example uses a three-year minimum frequency for each, though with the high risk end of the scales different for the two aspects, giving different frequencies for the two, but both subject to an overall minimum sampling of every food for all aspects at least once every three years.\n\nThis risk assessment nomogram was developed by the UK Public Analyst Service with funding from the UK Food Standards Agency for use as a tool to guide the appropriate frequency of sampling and analysis of food for official food control purposes, intended to be used to assess all potential problems with all foods, although not yet adopted.\nThis nomograph can be used to estimate the sample size requirements for statistical analyses. It uses four parameters: \"α\" (fixed), effect size (\"ρ\" or \"δ\"), statistical power, and number of cases \"N\" (two scales for \"α\" = .05 (liberal) or .01 (conservative)).\n\nThe hypothesized effect size in the population can either be expressed as a correlation coefficient (\"ρ\") or a normalized difference in means (\"δ\") for a T-test. The normalized difference is equal to the absolute value of the difference between two population means (\"μ\"₁ − \"μ\"₂), divided by the pooled standard deviation (\"s\").\n\nThe statistical power desired is estimated by 1 − \"β\", where \"β\" is equal to the probability of making a Type II error. A \"Type II\" error is failing to reject the statistical null hypothesis (i.e., \"ρ\" or \"δ\" is zero), when in fact the null hypothesis is false in the population and should be rejected. Cohen (1977) recommends using power equal to 0.80 or 80%, for a \"β\" = 0.20 .\n\nThe sample size or number of cases required is reported for two standard levels of statistical significance (\"α\" = 0.01 or 0.05). The value of \"α\" is the probability of making a Type I error. A \"Type I\" error is rejecting the statistical null hypothesis (i.e., claiming that either \"ρ\" or \"δ\" is zero), when in fact it \"is\" true (the value \"is\" zero) in the population and should \"not\" be rejected. The most commonly used values of α are 0.05 or 0.01 .\n\nTo find the sample size requirements for a given statistical analysis, estimate the effect size expected in the population (\"ρ\" or \"δ\") on the left hand axis, select the desired level of power on the right hand axis, and draw a line between the two values.\n\nWhere the line intersects with either the \"α\" = 0.05 or \"α\" = 0.01 middle axis will indicate the sample size required to achieve statistical significance of \"α\" less than 0.05 or 0.01, respectively (for the previously given parameters).\n\nFor instance, if one estimates the population correlation (\"ρ\") to be 0.30, and desires statistical power equal to 0.80, then to obtain a significance level of \"α\" less than 0.05, the sample size requirement would be \"N\" = 70 cases rounded up (more precisely approximately 68 cases using interpolation).\n\nUsing a ruler, one can readily read the missing term of the Law of sines \nor the roots of the Quadratic- and Cubic- equation.\n\n\n"}
{"id": "46724044", "url": "https://en.wikipedia.org/wiki?curid=46724044", "title": "Outstar", "text": "Outstar\n\nOutstar is an output from the neurodes of the hidden layer of the neural network architecture which works as an input for output layer. Neurode of hidden layer provides input to neurode of the output layer.\n"}
{"id": "22822116", "url": "https://en.wikipedia.org/wiki?curid=22822116", "title": "PA degree", "text": "PA degree\n\nIn the mathematical field of computability theory, a PA degree is a Turing degree that computes a complete extension of Peano arithmetic (Jockusch 1987). These degrees are closely related to fixed-point-free (DNR) functions, and have been thoroughly investigated in recursion theory.\n\nIn recursion theory, formula_1 denotes the computable function with index (program) \"e\" in some standard numbering of computable functions, and formula_2 denotes the \"e\"th computable function using a set \"B\" of natural numbers as an oracle.\n\nA set \"A\" of natural numbers is Turing reducible to a set \"B\" if there is a computable function that, given an oracle for set \"B\", computes the characteristic function χ of the set \"A\". That is, there is an \"e\" such that formula_3. This relationship is denoted \"A\" ≤ \"B\"; the relation ≤ is a preorder.\n\nTwo sets of natural numbers are Turing equivalent if each is Turing reducible to the other. The notation \"A\" ≡ \"B\" indicates \"A\" and \"B\" are Turing equivalent. The relation ≡ is an equivalence relation known as Turing equivalence. A Turing degree is a collection of sets of natural numbers, such that any two sets in the collection are Turing equivalent. Equivalently, a Turing degree is an equivalence class of the relation ≡.\n\nThe Turing degrees are partially ordered by Turing reducibility. The notation a ≤ b indicates there is a set in degree b that computes a set in degree a. Equivalently, a ≤ b holds if and only if every set in b computes every set in a.\n\nA function \"f\" from the natural numbers to the natural numbers is said to be diagonally nonrecursive (DNR) if, for all \"n\", formula_4 (here inequality holds by definition if formula_5 is undefined). If the range of \"f\" is the set {0,1} then \"f\" is a DNR function. It is known that there are DNR functions that do not compute any DNR function.\n\nA completion of Peano arithmetic is a set of formulas in the language of Peano arithmetic, such that the set is consistent in first-order logic and such that, for each formula, either that formula or its negation is included in the set. Once a Gödel numbering of the formulas in the language of PA has been fixed, it is possible to identify completions of PA with sets of natural numbers, and thus to speak about the computability of these completions.\n\nA Turing degree is defined to be a PA degree if there is a set of natural numbers in the degree that computes a completion of Peano Arithmetic. (This is equivalent to the proposition that every set in the degree computes a completion of PA.) Because there are no computable completions of PA, the degree 0 consisting of the computable sets of natural numbers is not a PA degree.\n\nBecause PA is an effective first-order theory, the completions of PA can be characterized as the infinite paths through a particular computable subtree of 2. Thus the PA degrees are exactly the degrees that compute an infinite path through this tree.\n\nThe PA degrees are upward closed in the Turing degrees: if a is a PA degree and a ≤ b then b is a PA degree.\n\nThe Turing degree 0‘, which is the degree of the halting problem, is a PA degree. There are also PA degrees that are not above 0‘. For example, the low basis theorem implies that there is a low PA degree. On the other hand, Antonín Kučera has proved that there is a degree less than 0‘ that computes a DNR function but is not a PA degree (Jockusch 1989:197).\n\nCarl Jockusch and Robert Soare (1972) proved that the PA degrees are exactly the degrees of DNR functions.\n\nBy definition, a degree is PA if and only if it computes a path through the tree of completions of Peano arithmetic. A stronger property holds: a degree a is a PA degree if and only if a computes a path through \"every\" infinite computable subtree of 2 (Simpson 1977).\n\n\n"}
{"id": "166105", "url": "https://en.wikipedia.org/wiki?curid=166105", "title": "Paper size", "text": "Paper size\n\nMany paper size standards conventions have existed at different times and in different countries. Today, the A and B series of ISO 216, which includes the commonly used A4 size, are the international standard used by almost every country. However, in many countries in the Americas as well as in the Philippines, the North American series of paper sizes such as 'Letter' and 'Legal' is more prevalent. \n\nPaper sizes affect writing paper, stationery, cards, and some printed documents. The international standard for envelopes is the C series of ISO 269\nP10 size is the smallest of them all, containing less than 10 micrometers of paper\n\nThe international paper size standard is ISO 216. It is based on the German DIN 476 standard for paper sizes. ISO paper sizes are all based on a single aspect ratio of the square root of 2, or approximately 1:1.4142. There are different series, as well as several extensions.\n\nThe following international paper sizes are included in Cascading Style Sheets (CSS): \"A3\", \"A4\", \"A5\", \"B4\", \"B5\".\n\nThe base A0 size of paper is defined as having an area of 1 m and a dimension ratio of 1 to , making the A0 paper size exactly formula_1 m × formula_2 m. Rounded to the nearest millimetre, that is .\n\nSuccessive paper sizes in the series A1, A2, A3, and so forth, are defined by halving the preceding paper size across the larger dimension. This also effectively halves the area of each sheet. The most frequently used paper size is A4 measuring .\n\nThe significant advantage of this system is its scaling: if a sheet with an aspect ratio of is divided into two equal halves parallel to its shortest sides, then the halves will again have an aspect ratio of . Folded brochures of any size can be made by using sheets of the next larger size, e.g. A4 sheets are folded to make A5 brochures. The system allows scaling without compromising the aspect ratio from one size to another—as provided by office photocopiers, e.g. enlarging A4 to A3 or reducing A3 to A4. Similarly, two sheets of A4 can be scaled down and fit exactly on 1 sheet without any cutoff or margins.\n\nThe behavior of the aspect ratio is easily proven: on a sheet of paper, let \"a\" be the long side and \"b\" be the short side; thus,  = . When the sheet of paper is folded in half widthwise, let \"c\" be the length of the new short side: \"c\" = . If we take the ratio of the newly folded paper we have:\n\nformula_3\n\nTherefore, the aspect ratio is preserved for the new dimensions of the folded paper.\n\nWeights are easy to calculate as well: a standard A4 sheet made from 80 g/m paper weighs 5 g (as it is of an A0 page, measuring 1 m), allowing one to easily compute the weight—and associated postage rate—by counting the number of sheets used.\n\nThe advantages of basing a paper size upon an aspect ratio of were first noted in 1786 by the German scientist and philosopher Georg Christoph Lichtenberg. The formats that became A2, A3, B3, B4 and B5 were developed in France on proposition of the mathematician Lazare Carnot and published for judiciary purpose in 1798 during the French Revolution. Early in the 20th century, Dr Walter Porstmann turned Lichtenberg's idea into a proper system of different paper sizes. Porstmann's system was introduced as a DIN standard (DIN 476) in Germany in 1922, replacing a vast variety of other paper formats. Even today, the paper sizes are called \"DIN A4\" () in everyday use in Germany and Austria.\n\nThe DIN 476 standard spread quickly to other countries. Before the outbreak of World War II, it had been adopted by the following countries:\n\n\n\nDuring World War II, the standard was adopted by Uruguay (1942), Argentina (1943) and Brazil (1943), and afterwards spread to other countries:\n\n\n\nBy 1975, so many countries were using the German system that it was established as an ISO standard, as well as the official United Nations document format. By 1977, A4 was the standard letter format in 88 of 148 countries. Today the standard has been adopted by all countries in the world except the United States and Canada. In Mexico, Costa Rica, Colombia, Venezuela, Chile, and the Philippines, the US letter format is still in common use, despite their official adoption of the ISO standard.\n\nIn addition to the A series, there is a less common B series. The area of B series sheets is the geometric mean of successive A series sheets. So, B1 is between A0 and A1 in size, with an area of 0.707 m ( m). As a result, B0 is 1 metre wide, and other sizes in the B series are a half, a quarter or further fractions of a metre wide. While less common in office use, it is used for a variety of special situations. Many posters use B-series paper or a close approximation, such as 50 cm × 70 cm; B5 is a relatively common choice for books. The B series is also used for envelopes and passports. The B-series is widely used in the printing industry to describe both paper sizes and printing press sizes, including digital presses. B3 paper is used to print two US letter or A4 pages side by side using imposition; four pages would be printed on B2, eight on B1, etc.\n\nThe C series is usually used for envelopes and is defined in ISO 269. The area of C series sheets is the geometric mean of the areas of the A and B series sheets of the same number; for instance, the area of a C4 sheet is the geometric mean of the areas of an A4 sheet and a B4 sheet. This means that C4 is slightly larger than A4, and slightly smaller than B4. The practical usage of this is that a letter written on A4 paper fits inside a C4 envelope, and both A4 and C4 paper fits inside a B4 envelope.\n\nSome envelope formats with mixed sides from adjacent sizes (and thus an approximate aspect ratio of 2:1) are also defined in national adaptations of the ISO standard, e.g. DIN C6/C5 is 114 mm × 229 mm where the common side to C5 and C6 is 162 mm.\n\nThe α variables are the distinct first terms in the three geometric progressions of the same \"common-ratio\" equal to the square root of two. Each of the three geometric progressions (corresponding to the three series A, B, C) is formed by all possible paper dimensions (length and width) of the series arranged in a decreasing order. This interesting arrangement of dimensions is also very useful - not only it forms a geometric progression with easy to remember formulae, it also has that each consecutive pair of values (like a sliding window of size 2) will automatically correspond to the dimensions of a standard paper format in the series.\n\nThe tolerances specified in the standard are\n\n\nThe German standard DIN 476 was published on 18 August 1922 and is the original specification of the A, B and C sizes. In 1991, it was split into DIN 476-1 for the A and B formats on the one hand and 476-2 for the C series on the other hand. The former has been withdrawn in 2002 in favor of adopting the international standard as DIN EN ISO 216, but part 2 has been retained and was last updated in 2008. \n\nThe first and the second editions of DIN 476 from 1922 and 1925 also included a D series. \n\nThe smallest formats specified originally were A13, B13, C8 and D8.\n\nDIN 476 provides for formats larger than A0, denoted by a prefix factor. In particular, it lists the formats 2A0 and 4A0, which are twice and four times the size of A0 respectively.\nHowever, ISO 216:2007 notes 2A0 and 4A0 in the table of \"Main series of trimmed sizes\" (ISO A series) as well: \"The rarely used sizes [2A0 and 4A0] which follow also belong to this series.\"\n\nDIN 476 also used to specify slightly tighter tolerances than ISO 216:\n\n\nThe Swedish standard SIS 014711 generalized the ISO system of A, B, and C formats by adding D, E, F, and G formats to it. Its D format sits between a B format and the next larger A format (just like C sits between A and the next larger B). The remaining formats fit in between all these formats, such that the sequence of formats A4, E4, C4, G4, B4, F4, D4, \"H4\", A3 is a geometric progression, in which the dimensions grow by a factor from one size to the next. However, this SIS standard does not define any size between a D format and the next larger A format (called H in the previous example). \n\nOf these additional formats, G5 (169 × 239 mm) and E5 (155 × 220 mm) are popular in Sweden and the Netherlands for printing dissertations, but the other formats have not turned out to be particularly useful in practice and they have not been adopted internationally.\n\nThe Swedish and German D series basically contain the same sizes, but are offset by one, i.e. DIN D4 equals SIS D5 and so on.\n\nThe JIS defines two main series of paper sizes. The JIS A-series is identical to the ISO A-series, but with slightly different tolerances. The area of B-series paper is 1.5 times that of the corresponding A-paper (instead of the factor = 1.414... for the ISO B-series), so the length ratio is approximately 1.22 times the length of the corresponding A-series paper. The aspect ratio of the paper is the same as for A-series paper. Both A- and B-series paper is widely available in Japan, Taiwan and China, and most photocopiers are loaded with at least A4 and either one of A3, B4 and B5 paper.\n\nThere are also a number of traditional paper sizes, which are now used mostly by printers. The most common of these old series are the Shiroku-ban and the Kiku paper sizes.\n\nFollowing Japanese paper sizes are included in Cascading Style Sheets (CSS): \"JIS-B4\", \"JIS-B5\".\n\nThe Chinese standard GB/T 148-1997, which replaced GB 148-1989, documents the standard ISO series, A and B, but adds a custom D series. This Chinese format originates from the Republic of China (1912–49). The D series is not identical to the Swedish D series. It does not strictly follow the same principles as ISO paper sizes: The aspect ratio is only very roughly . The short side of a size is always 4 mm longer than the long side of the next smaller size. The long side of a size is always exactly – i.e. without further rounding – twice as long as the short side of the next smaller size.\n\nThe general adaptation of ISO 216 in the Soviet Union was GOST 9327-60. \nIn its 1960 version, it lists formats down to A13, B12 and C8 and also specifies ½, ¼ and ⅛ prefixes for halving the shorter side (repeatedly), e.g. ½A4 = 105 mm × 297 mm.\nA standard for technical drawings from 1960, GOST 3450-60, introduces alternative numeric format designations to deal with very high or very wide sheets.\nThese 2-digit codes are based upon A4 = \"11\": The first digit is the factor the longer side (297 mm) is multiplied by and the second digit is the one for the shorter side (210 mm), so \"24\" is 2×297 mm × 4×210 mm = 594 mm × 840 mm.\n\nGOST 3450-60 was replaced 8 years later by ESKD GOST 2301-68, but the numeric designations remained in popular use much longer.\nThe new designations were not purely numeric, but consisted of the ISO label followed by an 'x', or possibly the multiplication sign '×', and the factor, e.g. DIN 2A0 = GOST A0×2, but DIN 4A0 ≠ GOST A0×4, also listed are: A0×3, A1×3, A1×4, A2×3–A2×5, A3×3–A3×7, A4×3–A4×9. The formats …×1 and …×2 usually would be aliases for existing formats.\n\nThe United States, Canada, and the Philippines primarily use a different system of paper sizes compared to the rest of the world. The current standard sizes are unique to those countries, although due to the size of the North American market and proliferation of both software and printing hardware from the region, other parts of the world have become increasingly familiar with these sizes (though not necessarily the paper itself). Some traditional North American inch-based sizes differ from the Imperial British sizes described below.\n\n\"Letter, Legal\" and \"Ledger/Tabloid\" are by far the most commonly used of these for everyday activities, and the only ones included in Cascading Style Sheets (CSS).\n\nThe origins of the exact dimensions of Letter size paper () are lost in tradition and not well documented. The American Forest and Paper Association argues that the dimension originates from the days of manual paper making, and that the 11-inch length of the page is about a quarter of \"the average maximum stretch of an experienced vatman's arms.\" However, this does not explain the width or aspect ratio.\n\nOutside of North America, Letter size may also be known as \"American Quarto\". If one accepts some trimming, the size is indeed one quarter of the old Imperial paper size known as Demy, .\n\nUS paper sizes are currently standard in the United States and are the most commonly used formats at least in the Philippines, most of Mesoamerica and Chile. The latter use US Letter, but their Legal size is one inch shorter than its US equivalent.\n\nMexico and Colombia, for instance, have adopted the ISO standard, but US Letter format is still the system in use throughout the country. It is virtually impossible to encounter ISO standard papers in day-to-day uses, with \"Carta\" (Letter), \"Oficio\" (Government-Legal) and \"Doble carta\" (Ledger/Tabloid) being nearly universal.\n\nIn Canada, US paper sizes are a de facto standard. The government, however, also uses ISO paper sizes.\n\nThere is an additional paper size, , to which the name \"Government-Letter\" was given by the IEEE Printer Working Group (PWG). It was prescribed by Herbert Hoover when he was Secretary of Commerce to be used for US government forms, apparently to enable discounts from the purchase of paper for schools, but more likely due to the standard use of trimming books (after binding) and paper from the standard letter size paper to produce consistency and allow \"bleed\" printing. In later years, as photocopy machines proliferated, citizens wanted to make photocopies of the forms, but the machines did not generally have this size paper in their bins. Ronald Reagan therefore had the US government switch to regular Letter size, which is both half an inch longer and wider. The former government size is still commonly used in spiral-bound notebooks, for children's writing and the like, a result of trimming from the current Letter dimensions.\n\nBy extension of the American standards, the halved Letter size, , meets the needs of many applications. It is variably known as \"Statement\", \"Stationery\", \"Memo\", \"Half Letter\", \"Half A\" (from ANSI sizes) or simply \"Half Size\". Like the similar-sized ISO A5, it is used for everything from personal letter writing to official aeronautical maps. Organizers, notepads, and diaries also often use this size of paper; thus 3-ring binders are also available in this size. Booklets of this size are created using word processing tools with landscape printing in two columns on letter paper which are then cut or folded into the final size.\n\nIn 1996, the American National Standards Institute adopted ANSI/ASME Y14.1 which defined a regular series of paper sizes based upon the \"de facto\" standard Letter size which it assigned \"ANSI A\", intended for technical drawings, hence sometimes labeled \"Engineering\". This series is somewhat similar to the ISO standard in that cutting a sheet in half would produce two sheets of the next smaller size and therefore also includes Ledger/Tabloid as \"ANSI B\". Unlike the ISO standard, however, the arbitrary base sides forces this series to have two alternating aspect ratios. For example, ANSI A is less elongated than A4, while ANSI B is more elongated than A3.\n\nThe Canadian standard CAN2-9.60-M76 and its successor CAN/CGSB-9.60-94 \"Paper Sizes for Correspondence\" specified paper sizes P1 through P6, which are the ANSI paper sizes rounded to the nearest 5 mm. All custom Canadian paper size standards were withdrawn in 2012 and the respective ISO standards took their places.\nWith care, documents can be prepared so that the text and images fit on either ANSI or their equivalent ISO sheets at 1:1 reproduction scale.\n\nOther, informal, larger sizes continuing the alphabetic series illustrated above exist, but they are not part of the series \"per se\", because they do not exhibit the same aspect ratios. For example, Engineering F size is with ca. 1.4286:1; it is commonly required for NAVFAC drawings, but is generally less commonly used. Engineering G size is high, but it is a roll format with a variable width up to in increments of . Engineering H through N sizes are also roll formats.\n\nSuch huge sheets were at one time used for full-scale layouts of aircraft parts, automotive parts, wiring harnesses and the like, but are slowly being phased out, due to widespread use of computer-aided design (CAD) and computer-aided manufacturing (CAM). Some visual arts fields also continue to use these paper formats for large-scale printouts, such as for displaying digitally painted character renderings at life-size as references for makeup artists and costume designers, or to provide an immersive landscape reference.\n\nIn addition to the system as listed above, there is a corresponding series of paper sizes used for architectural purposes defined in the same standard, ANSI/ASME Y14.1, which is usually abbreviated \"Arch\". This series also shares the property that bisecting each size produces two of the size below, with alternating aspect ratios. It may be preferred by North American architects because the aspect ratios (4:3 and 3:2) are ratios of small integers, unlike their ANSI (or ISO) counterparts. Furthermore, the aspect ratio 4:3 matches the traditional aspect ratio for computer displays.\n\nThe size Arch E1 has a different aspect ratio because it derives from adding 6 inches to each side of Arch D or subtracting the same amount from Arch E. An intermediate size between Arch C and D with a long side of does not exist.\n\nThe sizes listed above are for paper sold loose in reams. There are many sizes of tablets of paper, that is, sheets of paper bound at one edge, usually by a strip of plastic or hardened PVA adhesive. Often there is a pad of cardboard (also known as chipboard or greyboard) at the bottom of the stack. Such a tablet serves as a portable writing surface, and the sheets often have lines printed on them, usually in non-repro blue, to make writing in a line easier. An older means of binding is to have the sheets stapled to the cardboard along the top of the tablet; there is a line of perforated holes across every page just below the top edge from which any page may be torn off. Lastly, a pad of sheets each weakly stuck with adhesive to the sheet below, trademarked as \"Post-It\" or \"Stick-Em\" and available in various sizes, serve as a sort of tablet.\n\n\"Letter pads\" are , while the term \"legal pad\" is often used by laymen to refer to pads of various sizes including those of . There are \"steno pads\" (used by stenographers) of .\n\nIn countries where the ISO sizes are standard, most notebooks and tablets are sized to ISO specifications (for example, most newsagents in Australia stock A4 and A3 tablets).\n\nThe international business card has the same size as the smallest rectangle containing a credit card. However, \"credit card size\", as defined in ISO/IEC 7810, also specifies rounded corners and thickness.\n\nThis implies that all postcards have a width:height aspect ratio\nin the range 1.18 to 1.71.\nThe only ISO 216 size in the post card range is A6.\n\nMost industry standards express the direction of the grain last when giving dimensions (that is, 17 × 11 inches is short grain paper and 11 × 17 inches is long grain paper), although alternatively the grain alignment can be explicitly indicated with an underline (11 × 17 is short grain) or the letter \"M\" for \"machine\" (11M × 17 is short grain). Grain is important because paper will crack if folded across the grain: for example, if a sheet 17 × 11 inches is to be folded to divide the sheet into two 8.5 × 11 halves, then the grain will be along the 11-inch side. Paper intended to be fed into a machine that will bend the paper around rollers, such as a printing press, photocopier or typewriter, should be fed grain side first so that the axis of the rollers is along the grain.\n\nTraditionally, a number of different sizes were defined for large sheets of paper, and paper sizes were defined by the sheet name and the number of times it had been folded. Thus a full sheet of \"royal\" paper was 25 × 20 inches, and \"royal octavo\" was this size folded three times, so as to make eight sheets, and was thus 10 × inches.\n\nImperial sizes were used in the United Kingdom and its territories.\n\nThese sizes are no longer commonly used since the UK switched to ISO sizes.\nMany of these sizes were only used for making books (see bookbinding), and would never have been offered for ordinary stationery purposes.\n\nFoolscap folio is often referred to simply as \"folio\" or \"foolscap\". Similarly, \"quarto\" is more correctly \"copy draught quarto\" and \"Kings\" is an alias for \"Foolscap quarto\".\n\nThe demitab or demi-tab (from the French \"demi\" or half tabloid) is , equal to one quarter of a sheet of tabloid size paper. In actual circulation, the size is common for a demitab. Tabloid newspapers, which are \"generally half the size of a broadsheet\", also vary in size. To add to the lack of uniformity, broadsheets also vary in size.\n\nBefore the adoption of the ISO standard system in 1967, France had its own paper size system. Some of these formats are still used today, and they are standardized by the AFNOR. Their names come from the watermarks that the papers were branded with when they were handcrafted, which is still the case for certain art papers. They also generally exist in double versions where the smallest measure is multiplied by two, or in quadruple versions where both measures have been doubled.\n\nA transitional size called PA4 (), sometimes dubbed L4, was proposed for inclusion into the ISO 216 standard in 1975. It has the height of Canadian P4 paper (215 mm × 280 mm, about  in × 11 in) and the width of international A4 paper (), i.e. it uses the smaller value among the two for each side. The table below, shows how this format can be generalized into an entire format series.\n\nThe PA formats did not end up in ISO 216, because the committee decided that the set of standardized paper formats should be kept to the minimum necessary. However, PA4 remains of practical use today. In landscape orientation, it has the same 4:3 aspect ratio as the displays of traditional TV sets, some computer displays (e.g. iPad) and data projectors. PA4, with appropriate margins, is therefore a good choice as the format of presentation slides.\n\nAs a compromise between the two most popular paper sizes globally, PA4 is used today by many international magazines, because it can be printed easily on equipment designed for either A4 or US Letter. That means it is not as much a paper size than a page format.\n\nThe size 210 mm × 280 mm was documented in the Canadian standard CAN2-200.2-M79 \"Common Image Area for Paper Sizes P4 and A4\".\n\nA non-standard F4 paper size is common in Southeast Asia. It is a transitional size with the shorter side from ISO A4 (210 mm) and the longer side from British Foolscap (13 in, 330 mm) and is sometimes known as \"(metric) foolscap\" or \"folio\" as well.\n\nIn Indonesia and Philippines, F4 paper is 215 × 330 mm (8.5 × 13 in). In Indonesia it is sometimes called Folio, while in Philippines it is sometimes also called Long Bond.\n\nA sheet of F4 can be cut from a sheet of SRA4 with very little wastage. The size is also smaller than its Swedish equivalent SIS F4 at 239 mm × 338 mm.\n\nAlthough the movement is towards the international standard metric paper sizes, on the way there from the traditional ones there has been at least one new size just a little larger than that used internationally.\n\nBritish architects and industrial designers once used a size called \"Antiquarian\", , as listed above, but given in the \"New Metric Handbook\" (Tutt & Adler 1981) as for board size. This is a little larger than ISO A0, 841 mm × 1189 mm. So for a short time, a size called A0a of was used in Britain, which is actually just a slightly shorter version of ISO B0 at 1414 mm.\n\nThe most common paper sizes used for commercial and industrial printing in Colombia are based upon a size referred to as \"pliego\" that is ISO B1 (707 mm × 1000 mm) cut to full decimetres. Smaller sizes are derived by halving as usual and just get a vulgar fraction prefix: \" pliego\" and \" pliego\".\n\nDL is a common envelope size from DIN 678-1 often with 90 mm × 45 mm address window in the lower left, which also fits A4 well.\nSometimes, it is erroneously labeled \"DLE\" instead, the E apparently standing for \"envelope\".\n\nISO 5457 specifies drawing paper sizes with a trimmed size equal to the A series sizes from A4 upward. The untrimmed sizes are 3 to 4 cm larger and rounded to the nearest centimeter. A0 through A3 are used in landscape orientation, while A4 is used in portrait orientation. Designations for preprinted drawing paper include the base sizes and a suffix, either \"T\" for trimmed or \"U\" for untrimmed sheets.\n\nNewspapers have a separate set of sizes.\n\n\nIn a recent trend many newspapers have been undergoing what is known as \"web cut down\", in which the publication is redesigned to print using a narrower (and less expensive) roll of paper. In extreme examples, some broadsheet papers are nearly as narrow as traditional tabloids.\n\n\n"}
{"id": "25935140", "url": "https://en.wikipedia.org/wiki?curid=25935140", "title": "Percolation critical exponents", "text": "Percolation critical exponents\n\nIn the context of percolation theory, a percolation transition is characterized by a set of \"universal\" critical exponents, which describe the fractal properties of the percolating \nmedium at large scales and sufficiently close to the transition. The exponents are universal in \nthe sense that they only depend on the type of percolation model and on the space dimension. They are expected to not depend on microscopic details such as the lattice structure, or whether site or bond percolation is considered. This article deals with the critical exponents of random percolation.\n\nPercolating systems have a parameter formula_1 which controls the occupancy of sites or bonds in the system. At a critical value formula_2, the mean cluster size goes to infinity and the percolation transition takes place. As one approaches formula_2, various quantities either diverge or go to a constant value by a power law in formula_4, and the exponent of that power law is the critical exponent. While the exponent of that power law is generally the same on both sides of the threshold, the coefficient or \"amplitude\" is generally different, leading to a universal amplitude ratio.\n\nThermodynamic or configurational systems near a critical point or a continuous phase transition become fractal, and the behavior of many quantities in such circumstances is described by universal critical exponents. Percolation theory is a particularly simple and fundamental model in statistical mechanics which has a critical point, and a great deal of work has been done in finding its critical exponents, both theoretically (limited to two dimensions) and numerically.\n\nCritical exponents exist for a variety of observables, but most of them are linked to each other by exponent (or scaling) relations. Only a few of them are independent, and the choice of the fundamental exponents depends on the focus of the study at hand. One choice is the set formula_5 motivated by the cluster size distribution, another choice is formula_6 motivated by the structure of the infinite cluster. So-called correction exponents extend these sets, they refer to higher orders of the asymptotic expansion around the critical point.\n\nPercolation clusters become self-similar precisely at the threshold density formula_2 for sufficiently large length scales, entailing the following asymptotic power laws:\n\nThe fractal dimension formula_8 relates how the mass of the incipient infinite cluster depends on the radius or another length measure, formula_9 at formula_10 and for large probe sizes, formula_11. Other notation: magnetic exponent formula_12 and co-dimension formula_13.\n\nThe Fisher exponent formula_14 characterizes the cluster-size distribution formula_15, which is often determined in computer simulations. The latter counts the number of clusters with a given size (volume) formula_16, normalized by the total volume (number of lattice sites). The distribution obeys a power law at the threshold, formula_17 asymptotically as formula_18.\n\nThe probability for two sites separated by a distance formula_19 to belong to the same cluster decays as formula_20 or formula_21 for large distances, which introduces the anomalous dimension formula_22. Also, formula_23 and formula_24.\n\nThe exponent formula_25 is connected with the leading correction to scaling, which appears, e.g., in the asymptotic expansion of the cluster-size distribution,\nformula_26 for formula_18. Also, formula_28.\n\nFor quantities like the mean cluster size formula_29, the corrections are controlled by the exponent formula_30.\n\nThe minimum or chemical distance or shortest-path exponent formula_31 describes how the average minimum distance formula_32 relates to the Euclidean distance formula_33, namely formula_34 . The elastic backbone has the same fractal dimension as the shortest path. A related quantity is the spreading dimension formula_35, which describes the scaling of the mass M of a critical cluster within a chemical distance formula_36 as formula_37, and is related to the fractal dimension formula_38 of the cluster by formula_39. The chemical distance can also be thought of as a time in an epidemic growth process, and one also defines formula_40 where formula_41, and formula_42 is the dynamical exponent. One also writes formula_43.\n\nAlso related to the minimum dimension is the simultaneous growth of two nearby clusters. The probability that the two clusters coalesce exactly in time formula_44 scales as formula_45 with formula_46.\n\nThe dimension of the backbone, which is defined as the subset of cluster sites\ncarrying the current when a voltage difference is applied between two sites far apart, is formula_47 (or formula_48).\n\nThe fractal dimension of the random walk on an infinite incipient percolation cluster is given by formula_49.\n\nThe spectral dimension formula_50 such that the average number of distinct sites visited in an formula_51-step random walk scales as formula_52.\n\nThe approach to the percolation threshold is governed by power laws again, which hold asymptotically close to formula_2:\n\nThe exponent formula_54 describes the divergence of the correlation length formula_55 as the percolation transition is approached, formula_56. The infinite cluster becomes homogeneous at length scales beyond the correlation length; further, it is a measure for the linear extent of the largest finite cluster. Other notation: Thermal exponent formula_57 and dimension formula_58.\n\nOff criticality, only finite clusters exist up to a largest cluster size formula_59, and the cluster-size distribution is smoothly cut off by a rapidly decaying function, formula_60. The exponent formula_61 characterizes the divergence of the cutoff parameter, formula_62. From the fractal relation we have formula_63, yielding formula_64.\n\nThe density of clusters (number of clusters per site) formula_65 is continuous at the threshold but its third derivative goes to infinity as determined by the exponent formula_66: formula_67, where formula_68 represents the coefficient above and below the transition point.\n\nThe strength or weight of the percolating cluster, formula_69 or formula_70, is the probability that a site belongs to an infinite cluster. formula_69 is zero below the transition and is non-analytic. Just above the transition, formula_72, defining the exponent formula_73. formula_74 plays the role of an order parameter.\n\nThe divergence of the mean cluster size formula_75 introduces the exponent formula_76.\n\nThe conductivity exponent formula_77 describes how the electrical conductivity formula_78 goes to zero in a conductor-insulator mixture, formula_79. Also, formula_80\n\nThe probability a point at a surface belongs to the percolating or infinite cluster is formula_81\n\nThe surface fractal dimension is given by formula_82 \n\nFor higher-order terms in the formula_100 expansions, see \n\nNote that it has been claimed that the numerical values of exponents of percolation depends only on the dimension of lattice. However, percolation on WPSL is an exception in the sense that albeit it is two dimensional yet it does not belong to the same universality where all the planar lattices belong.\n\nDirected percolation (DP) refers to percolation in which the fluid can flow only in one direction along bonds—such as only in the downward direction on a square lattice rotated by 45 degrees. This system is referred to as \"1 + 1 dimensional DP\" where the two dimensions are thought of as space and time.\n\nformula_101 and formula_102 are the transverse (perpendicular) and longitudinal (parallel) correlation length exponents, respectively. Also formula_103.\n\nformula_104 is the exponent corresponding to the behavior of the survival probability as a function of time: formula_105.\n\nThe d(space)+1(time) dimensional exponents are given below.\n\n! \n! \n! \n"}
{"id": "29008739", "url": "https://en.wikipedia.org/wiki?curid=29008739", "title": "Periodic graph (graph theory)", "text": "Periodic graph (graph theory)\n\nIn graph theory, a branch of mathematics, a periodic graph with respect to an operator \"F\" on graphs is one for which there exists an integer \"n\" > 0 such that \"F\"(\"G\") is isomorphic to \"G\". For example, every graph is periodic with respect to the complementation operator, whereas only complete graphs are periodic with respect to the operator that assigns to each graph the complete graph on the same vertices. Periodicity is one of many properties of graph operators, the central topic in graph dynamics.\n"}
{"id": "51465170", "url": "https://en.wikipedia.org/wiki?curid=51465170", "title": "Poisson boundary", "text": "Poisson boundary\n\nIn mathematics, the Poisson boundary is a measure space associated to a random walk. It is an object designed to encode the asymptotic behaviour of the random walk, i.e. how trajectories diverge when the number of steps goes to infinity. Despite being called a boundary it is in general a purely measure-theoretical object and not a boundary in the topological sense. However, in the case where the random walk is on a topological space the Poisson boundary can be related to the Martin boundary which is an analytic construction yielding a genuine topological boundary. Both boundaries are related to harmonic functions on the space via generalisations of the Poisson formula.\n\nThe Poisson formula states that given a positive harmonic function formula_1 on the unit disc formula_2 (that is, formula_3 where formula_4 is the Laplace–Beltrami operator associated to the Poincaré metric on formula_5) there exists a unique measure formula_6 on the boundary formula_7 such that the equality \nholds for all formula_10. One way to interpret this is that the functions formula_11 for formula_12 are up to scaling all the extreme points in the cone of nonnegative harmonic functions. This analytical interpretation of the set formula_13 leads to the more general notion of \"minimal Martin boundary\" (which in this case is the full \"Martin boundary\").\n\nThis fact can also be interpreted in a probabilistic manner. If formula_14 is the Markov process associated to formula_4 (i.e. the Brownian motion on the disc with the Poincaré Riemannian metric), then the process formula_16 is a continuous-time martingale, and as such converges almost everywhere to a function on the Wiener space of possible (infinite) trajectories for formula_14. Thus the Poisson formula identifies this measured space with the Martin boundary constructed above, and ultimately to formula_13 endowed with the class of Lebesgue measure (note that this identification can be made directly since a path in Wiener space converges almost surely to a point on formula_13). This interpretation of formula_13 as the space of trajectories for a Markov process is a special case of the construction of the Poisson boundary.\n\nFinally, the constructions above can be discretised, i.e. restricted to the random walks on the orbits of a Fuchsian group acting on formula_5. This gives an identification of the extremal positive harmonic functions on the group, and to the space of trajectories of the random walk on the group (both with respect to a given probability measure), with the topological/measured space formula_5.\n\nLet formula_23 be a discrete group and formula_6 a probability measure on formula_23, which will be used to define a random walk formula_26 on formula_23 (a discrete-time Markov process whose transition probabilities are formula_28); the measure formula_6 is called the \"step distribution\" for the random walk. Let formula_30 be another measure on formula_23, which will be the initial state for the random walk. The space formula_32 of trajectories for formula_26 is endowed with the measure formula_34 (where formula_35 denotes convolution of measures). There is also an equivalence relation formula_36 on formula_32, which identifies formula_38 to formula_39 if there exists formula_40 such that formula_41 for all formula_42 (the two trajectories have the same \"tail\"). The \"Poisson boundary\" of formula_43 is then the measured space formula_44 obtained as the quotient of formula_45 by the equivalence relation formula_36.\n\nIf formula_47 is the initial distribution of a random walk with step distribution formula_6 then the measure formula_49 on formula_44 obtained as the pushforward of formula_51. It is a stationary measure for formula_43, meaning that \nIt is possible to give an implicit definition of the Poisson boundary as the maximal formula_23-set with a formula_43-stationary measure formula_56, satisfying the additional condition that formula_57 almost surely weakly converges to a Dirac mass.\n\nLet formula_1 be a formula_6-harmonic function on formula_23, meaning that formula_61. Then the random variable formula_62 is a discrete-time martingale and so it converges almost surely. Denote by formula_63 the function on formula_44 obtained by taking the limit of the values of formula_1 along a trajectory (this is defined almost everywhere on formula_32 and shift-invariant). Let formula_67 and let formula_68 be the measure obtained by the constriction above with formula_69 (the Dirac mass at formula_70). If formula_1 is either positive or bounded then formula_63 is as well and we have the \"Poisson formula\":\nThis establishes a bijection between formula_6-harmonic bounded functions and essentially bounded measurable functions on formula_44. In particular the Poisson boundary of formula_43 is trivial, that is reduced to a point, if and only if the only bounded formula_6-harmonic functions on formula_23 are constant.\n\nThe general setting is that of a \"Markov operator\" on a measured space, a notion which generalises the Markov operator formula_79 associated to a random walk. Much of the theory can be developed in this abstract and very general setting.\n\nLet formula_80 be a random walk on a discrete group. Let formula_81 be the probability to get from formula_70 to formula_83 in formula_84 steps, i.e. formula_85. The Green kernel is by definition:\nIf the walk is transient then this series is convergent for all formula_87. Fix a point formula_88 and define the Martin kernel by: \nformula_89. \nThe embedding formula_90 has a relatively compact image for the topology of pointwise convergence, and the Martin compactification is the closure of this image. A point formula_91 is usually represented by the notation formula_92.\n\nThe Martin kernels are positive harmonic functions and every positive harmonic function can be expressed as an integral of functions on the boundary, that is for every positive harmonic function there is a measure formula_93 on formula_44 such that a Poisson-like formula holds:\nThe measures formula_93 are supported on the \"minimal Martin boundary, whose elements can also be characterised by being minimal. A positive harmonic function formula_97 is said to be \"minimal\" if for any harmonic function formula_98 with formula_99 there exists formula_100 such that formula_101.\n\nThere is actually a whole family of Martin compactifications. Define the Green generating series as\nDenote by formula_103 the radius of convergence of this power series\nand define for formula_104 the formula_105-Martin kernel by\nformula_106.\nThe closure of the embedding formula_107 is called the formula_105-Martin compactification.\n\nFor a Riemannian manifold the Martin boundary is constructed, when it exists, in the same way as above, using the Green function of the Laplace–Beltrami operator formula_4. In this case there is again a whole family of Martin compactifications associated to the operators formula_110 for formula_111 where formula_112 is the bottom of the spectrum. Examples where this construction can be used to define a compactification are bounded domains in the plane and symmetric spaces of non-compact type.\n\nThe measure formula_113 corresponding to the constant function is called the \"harmonic measure\" on the Martin boundary. With this measure the Martin boundary is isomorphic to the Poisson boundary.\n\nThe Poisson and Martin boundaries are trivial for symmetric random walks in nilpotent groups. On the other hand, when the random walk is non-centered, the study of the full Martin boundary, including the minimal functions, is far less conclusive.\n\nFor random walks on a semisimple Lie group (with step distribution absolutely continuous with respect to the Haar measure) the Poisson boundary is equal to the Furstenberg boundary. The Poisson boundary of the Brownian motion on the associated symmetric space is also the Furstenberg boundary. The full Martin boundary is also well-studied in these cases and can always be described in a geometric manner. For example, for groups of rank one (for example the isometry groups of hyperbolic spaces) the full Martin boundary is the same as the minimal Martin boundary (the situation in higher-rank groups is more complicated).\n\nThe Poisson boundary of a Zariski-dense subgroup of a semisimple Lie group, for example a lattice, is also equal to the Furstenberg boundary of the group.\n\nFor random walks on an hyperbolic group, under rather weak assumptions on the step distribution which always hold for a simple walk (a more general condition is that the first moment be finite) the Poisson boundary is always equal to the Gromov boundary. For example, the Poisson boundary of a free group is the space of ends of its Cayley tree. The identification of the full Martin boundary is more involved; in case the random walk has finite range (the step distribution is supported on a finite set) the Martin boundary coincides with the minimal Martin boundary and both coincide with the Gromov boundary.\n\n"}
{"id": "3011353", "url": "https://en.wikipedia.org/wiki?curid=3011353", "title": "Preferential entailment", "text": "Preferential entailment\n\nPreferential entailment is a non-monotonic logic based on selecting only models that are considered the most plausible. The plausibility of models is expressed by an ordering among models called a preference relation, hence the name preference entailment.\n\nFormally, given a propositional formula formula_1 and an ordering over propositional models formula_2, preferential entailment selects only the models of formula_1 that are minimal according to formula_2. This selection leads to a non-monotonic inference relation: formula_5 holds if and only if all minimal models of formula_1 according to formula_2 are also models of formula_8.\n\nCircumscription can be seen as the particular case of preferential entailment when the ordering is based on containment of the sets of variables assigned to true (in the propositional case) or containment of the extensions of predicates (in the first-order logic case).\n\n"}
{"id": "11737468", "url": "https://en.wikipedia.org/wiki?curid=11737468", "title": "Probabilistic bisimulation", "text": "Probabilistic bisimulation\n\nIn theoretical computer science, probabilistic bisimulation is an extension of the concept of bisimulation for fully probabilistic transition systems first described by K.G. Larsen and A. Skou.\n\nA discrete probabilistic transition system is a triple\n\nwhere formula_2 gives the probability of starting in the state \"s\", performing the action \"a\" and ending up in the state \"t\". The set of states is assumed to be countable. There is no attempt to assign probabilities to actions. It is assumed that the actions are chosen nondeterministically by an adversary or by the environment. This type of system is fully probabilistic, there is no other indeterminacy.\n\nThe definition of a probabilistic bisimulation on a system \"S\" is an equivalence relation \"R\" on the state space St, such that for every pair \"s\",\"t\" in St with sRt and for every action a in Act and for every equivalence class \"C\" of \"R\"\nformula_3 Two states are said to be probabilistically bisimilar if there is some such \"R\" relating them.\n\nWhen applied to Markov chains, probabilistic bisimulation is the same concept as lumpability.\nProbabilistic bisimulation extends naturally to weighted bisimulation.\n"}
{"id": "332264", "url": "https://en.wikipedia.org/wiki?curid=332264", "title": "Recursive set", "text": "Recursive set\n\nIn computability theory, a set of natural numbers is called recursive, computable or decidable if there is an algorithm which takes a number as input, terminates after a finite amount of time (possibly depending on the given number) and correctly decides whether the number belongs to the set.\n\nA more general class of sets consists of the recursively enumerable sets, also called semidecidable sets. For these sets, it is only required that there is an algorithm that correctly decides when a number \"is\" in the set; the algorithm may give no answer (but not the wrong answer) for numbers not in the set.\n\nA set which is not computable is called noncomputable or undecidable.\n\nA subset of the natural numbers is called recursive if there exists a total computable function such that \n\n\nIf \"A\" is a recursive set then the complement of \"A\" is a recursive set. If \"A\" and \"B\" are recursive sets then \"A\" ∩ \"B\", \"A\" ∪ \"B\" and the image of \"A\" × \"B\" under the Cantor pairing function are recursive sets.\n\nA set \"A\" is a recursive set if and only if \"A\" and the complement of \"A\" are both recursively enumerable sets. The preimage of a recursive set under a total computable function is a recursive set. The image of a computable set under a total computable bijection is computable.\n\nA set is recursive if and only if it is at level of the arithmetical hierarchy.\n\nA set is recursive if and only if it is either the range of a nondecreasing total computable function or the empty set. The image of a computable set under a nondecreasing total computable function is computable.\n\n"}
{"id": "29430", "url": "https://en.wikipedia.org/wiki?curid=29430", "title": "Simple module", "text": "Simple module\n\nIn mathematics, specifically in ring theory, the simple modules over a ring \"R\" are the (left or right) modules over \"R\" that have no non-zero proper submodules. Equivalently, a module \"M\" is simple if and only if every cyclic submodule generated by a non-zero element of \"M\" equals \"M\". Simple modules form building blocks for the modules of finite length, and they are analogous to the simple groups in group theory.\n\nIn this article, all modules will be assumed to be right unital modules over a ring \"R\".\n\nZ-modules are the same as abelian groups, so a simple Z-module is an abelian group which has no non-zero proper subgroups. These are the cyclic groups of prime order.\n\nIf \"I\" is a right ideal of \"R\", then \"I\" is simple as a right module if and only if \"I\" is a minimal non-zero right ideal: If \"M\" is a non-zero proper submodule of \"I\", then it is also a right ideal, so \"I\" is not minimal. Conversely, if \"I\" is not minimal, then there is a non-zero right ideal \"J\" properly contained in \"I\". \"J\" is a right submodule of \"I\", so \"I\" is not simple.\n\nIf \"I\" is a right ideal of \"R\", then \"R\"/\"I\" is simple if and only if \"I\" is a maximal right ideal: If \"M\" is a non-zero proper submodule of \"R\"/\"I\", then the preimage of \"M\" under the quotient map is a right ideal which is not equal to \"R\" and which properly contains \"I\". Therefore, \"I\" is not maximal. Conversely, if \"I\" is not maximal, then there is a right ideal \"J\" properly containing \"I\". The quotient map has a non-zero kernel which is not equal to , and therefore is not simple.\n\nEvery simple \"R\"-module is isomorphic to a quotient \"R\"/\"m\" where \"m\" is a maximal right ideal of \"R\". By the above paragraph, any quotient \"R\"/\"m\" is a simple module. Conversely, suppose that \"M\" is a simple \"R\"-module. Then, for any non-zero element \"x\" of \"M\", the cyclic submodule \"xR\" must equal \"M\". Fix such an \"x\". The statement that \"xR\" = \"M\" is equivalent to the surjectivity of the homomorphism that sends \"r\" to \"xr\". The kernel of this homomorphism is a right ideal \"I\" of \"R\", and a standard theorem states that \"M\" is isomorphic to \"R\"/\"I\". By the above paragraph, we find that \"I\" is a maximal right ideal. Therefore, \"M\" is isomorphic to a quotient of \"R\" by a maximal right ideal.\n\nIf \"k\" is a field and \"G\" is a group, then a group representation of \"G\" is a left module over the group ring \"k[G]\" (for details, see the main page on this relationship). The simple \"k[G]\" modules are also known as irreducible representations. A major aim of representation theory is to understand the irreducible representations of groups.\n\nThe simple modules are precisely the modules of length 1; this is a reformulation of the definition.\n\nEvery simple module is indecomposable, but the converse is in general not true.\n\nEvery simple module is cyclic, that is it is generated by one element.\n\nNot every module has a simple submodule; consider for instance the Z-module Z in light of the first example above.\n\nLet \"M\" and \"N\" be (left or right) modules over the same ring, and let \"f\" : \"M\" → \"N\" be a module homomorphism. If \"M\" is simple, then \"f\" is either the zero homomorphism or injective because the kernel of \"f\" is a submodule of \"M\". If \"N\" is simple, then \"f\" is either the zero homomorphism or surjective because the image of \"f\" is a submodule of \"N\". If \"M\" = \"N\", then \"f\" is an endomorphism of \"M\", and if \"M\" is simple, then the prior two statements imply that \"f\" is either the zero homomorphism or an isomorphism. Consequently, the endomorphism ring of any simple module is a division ring. This result is known as Schur's lemma.\n\nThe converse of Schur's lemma is not true in general. For example, the Z-module Q is not simple, but its endomorphism ring is isomorphic to the field Q.\n\nIf \"M\" is a module which has a non-zero proper submodule \"N\", then there is a short exact sequence\nA common approach to proving a fact about \"M\" is to show that the fact is true for the center term of a short exact sequence when it is true for the left and right terms, then to prove the fact for \"N\" and \"M\"/\"N\". If \"N\" has a non-zero proper submodule, then this process can be repeated. This produces a chain of submodules\nIn order to prove the fact this way, one needs conditions on this sequence and on the modules \"M\"/\"M\". One particularly useful condition is that the length of the sequence is finite and each quotient module \"M\"/\"M\" is simple. In this case the sequence is called a composition series for \"M\". In order to prove a statement inductively using composition series, the statement is first proved for simple modules, which form the base case of the induction, and then the statement is proved to remain true under an extension of a module by a simple module. For example, the Fitting lemma shows that the endomorphism ring of a finite length indecomposable module is a local ring, so that the strong Krull-Schmidt theorem holds and the category of finite length modules is a Krull-Schmidt category.\n\nThe Jordan–Hölder theorem and the Schreier refinement theorem describe the relationships amongst all composition series of a single module. The Grothendieck group ignores the order in a composition series and views every finite length module as a formal sum of simple modules. Over semisimple rings, this is no loss as every module is a semisimple module and so a direct sum of simple modules. Ordinary character theory provides better arithmetic control, and uses simple C\"G\" modules to understand the structure of finite groups \"G\". Modular representation theory uses Brauer characters to view modules as formal sums of simple modules, but is also interested in how those simple modules are joined together within composition series. This is formalized by studying the Ext functor and describing the module category in various ways including quivers (whose nodes are the simple modules and whose edges are composition series of non-semisimple modules of length 2) and Auslander–Reiten theory where the associated graph has a vertex for every indecomposable module.\n\nAn important advance in the theory of simple modules was the Jacobson density theorem. The Jacobson density theorem states:\nIn particular, any primitive ring may be viewed as (that is, isomorphic to) a ring of \"D\"-linear operators on some \"D\"-space.\n\nA consequence of the Jacobson density theorem is Wedderburn's theorem; namely that any right artinian simple ring is isomorphic to a full matrix ring of \"n\" by \"n\" matrices over a division ring for some \"n\". This can also be established as a corollary of the Artin–Wedderburn theorem.\n\n"}
{"id": "39661691", "url": "https://en.wikipedia.org/wiki?curid=39661691", "title": "Spherinder", "text": "Spherinder\n\nIn four-dimensional geometry, the spherinder, or spherical cylinder or spherical prism, is a geometric object, defined as the Cartesian product of a 3-ball (or solid 2-sphere), radius \"r\" and a line segment of length 2\"r\":\n\nLike the duocylinder, it is also analogous to a cylinder in 3-space, which is the Cartesian product of a disk with a line segment.\n\nIt can be seen in 3-dimensional space by stereographic projection as two concentric spheres, in a similar way that a tesseract (cubic prism) can be projected as two concentric cubes.\n\nIn 3-space, a cylinder can be considered intermediate between a cube and a sphere. In 4-space there are three intermediate forms between the tesseract and the hypersphere. Altogether, they are the:\n\nThese constructions correspond to the five partitions of 4, the number of dimensions.\n\nIf the two ends of a spherinder are connected together, or equivalently if a sphere is dragged around a circle perpendicular to its 3-space, it traces out a spheritorus.\n\nThe spherinder is related to the uniform prismatic polychora, which are cartesian product of a regular or semiregular polyhedron and a line segment. There are eighteen convex uniform prisms based on the Platonic and Archimedean solids (tetrahedral prism, truncated tetrahedral prism, cubic prism, cuboctahedral prism, octahedral prism, rhombicuboctahedral prism, truncated cubic prism, truncated octahedral prism, truncated cuboctahedral prism, snub cubic prism, dodecahedral prism, icosidodecahedral prism, icosahedral prism, truncated dodecahedral prism, rhombicosidodecahedral prism, truncated icosahedral prism, truncated icosidodecahedral prism, snub dodecahedral prism), plus an infinite family based on antiprisms, and another infinite family of uniform duoprisms, which are products of two regular polygons.\n\n\n"}
{"id": "9590289", "url": "https://en.wikipedia.org/wiki?curid=9590289", "title": "Systems Tool Kit", "text": "Systems Tool Kit\n\nSystems Tool Kit (formerly Satellite Tool Kit), often referred to by its initials STK, is a physics-based software package from Analytical Graphics, Inc. that allows engineers and scientists to perform complex analyses of ground, sea, air, and space assets, and share results in one integrated solution. At the core of STK is a geometry engine for determining the time-dynamic position and attitude of objects (\"assets\"), and the spatial relationships among the objects under consideration including their relationships or accesses given a number of complex, simultaneous constraining conditions. STK has been developed since 1989 as a commercial off the shelf software tool. Originally created to solve problems involving Earth-orbiting satellites, it is now used in the aerospace and defense communities and for many other applications.\n\nAGI states that STK has more than 50,000 installations at more than 800 global organizations such as NASA, ESA, CNES, DLR, Boeing, JAXA, ISRO, Lockheed Martin, Northrop Grumman, Airbus, DOD, and Civil Air Patrol.\n\nAGI posts Case Studies here of its customers in the areas of:\n\n\nSTK can be downloaded for free and has paid upgrade modules for more complex capability. As of release 10, the free version of STK includes 3D visualization, previously a paid upgrade module. The latest version available is STK 11.1.\n\nIn 1989, the three founders of Analytical Graphics, Inc. - Paul Graziani, Scott Reynolds and Jim Poland, left GE Aerospace to create Satellite Tool Kit (STK) as an alternative to bespoke, project-specific aerospace software.\n\nThe original version of STK ran only on Sun Microsystems computers, but as PCs became more powerful, the code was converted to run on Windows.\n\nSTK was first adopted by the aerospace community for orbit analysis and access calculations (when a satellite can see a ground-station or image target), but as the software was expanded, more modules were added that included the ability to perform calculations for communications systems, radar, interplanetary missions and orbit collision avoidance.\n\nThe addition of 3D viewing capabilities led to the adoption of the tool by military users for real-time visualization of air, land and sea forces as well as the space component. STK has also been used by various news organizations to graphically depict current events to a wider audience, including the deorbit of Russia's Mir Space Station, the Space Shuttle Columbia disaster, the Iridium/Cosmos collision, the asteroid 2012 DA14 close approach and various North Korea missile tests.\n\nAs of version 10, the software underwent a name change from Satellite Tool Kit to Systems Tool Kit to reflect its applicability in land, sea, air, and space systems.\n\nIn 2016 AGI released STK 11, offering a 64-bit Desktop version in addition to a 32-bit version, and Windows 10 support. This release featured Volumetric coverage with custom dynamic isosurfaces in any coordinate system, streaming Terrain Server for visualization, position, access and coverage, phased array antennas in the Comm and Radar modules, and custom models, materials and time dynamic thermal profiles with EOIR.\n\nThe STK interface is a standard GUI display with customizable toolbars and dockable maps and 3D viewports. All analysis can be done through mouse and keyboard interaction.\n\nIn addition, there is a scripting interface named Connect that enables STK to act within a client/server environment (via TCP/IP) and is language independent. Users on Windows have the option of using STK programatically via OLE automation.\n\nEach analysis or design space within STK is called a \"scenario\". Within each scenario any number of satellites, aircraft, targets, ships, communications systems or other objects can be created. Each scenario defines the default temporal limits to the child objects, as well as the base unit selection and properties. All of these properties can be overridden for each child object individually, as necessary. Only one scenario may exist at any one time, although data can be exported and reused in subsequent analyses.\n\nFor each object within a scenario, various reports and graphics (both static and dynamic) may be created. Relative parameters, between one object and another can also be reported and the effect of real-world restrictions (\"constraints\") enabled so that more accurate reporting is obtained. Through the use of the \"constellation\" and \"chains\" objects, multiple child objects may be grouped together and the multipath interactions between them investigated.\n\nAGI also offers software development kits for embedding STK capabilities into third-party applications or creating new applications based on AGI technology.\n\nSTK is a modular product, in much the same way as MATLAB and Simulink, and allows users to add modules to the baseline package to enhance specific functions.\n\nSTK can be embedded within another application (as an ActiveX component) or controlled from an external application (through TCP/IP or Component Object Model (COM)). Both integration techniques can make use of the \"connect\" scripting language to accomplish this task. There is also an object model for more \"programmer oriented\" integration methodologies. STK can be driven from a script that is run from the STK internal web browser in the free version of the tool. To control STK from an external source, or embed STK in another application requires the STK/Integration module.\n\nSince connect is a messaging format, it has the advantage of being completely language independent. This allows applications and client tools to be created in the programming language of the user's or developer's choice. In practice, as long as it is possible to create a socket connection, send information through that socket and then receive information that way then STK can be controlled with connect using that language.\n\nApplications have been developed in C, C++, C#, Perl, Visual Basic, VBScript, Java, JavaScript and MATLAB. Examples can also be found in the STK help files or downloaded from the AGI website.\n\n\n"}
{"id": "12210742", "url": "https://en.wikipedia.org/wiki?curid=12210742", "title": "Uniform k 21 polytope", "text": "Uniform k 21 polytope\n\nIn geometry, a uniform \"k\" polytope is a polytope in \"k\" + 4 dimensions constructed from the \"E\" Coxeter group, and having only regular polytope facets. The family was named by their Coxeter symbol \"k\" by its bifurcating Coxeter–Dynkin diagram, with a single ring on the end of the \"k\"-node sequence.\n\nThorold Gosset discovered this family as a part of his 1900 enumeration of the regular and semiregular polytopes, and so they are sometimes called Gosset's semiregular figures. Gosset named them by their dimension from 5 to 9, for example the \"5-ic semiregular figure\".\n\nThe sequence as identified by Gosset ends as an infinite tessellation (space-filling honeycomb) in 8-space, called the E8 lattice. (A final form was not discovered by Gosset and is called the E9 lattice: 6. It is a tessellation of hyperbolic 9-space constructed of (∞ 9-simplex and ∞ 9-orthoplex facets with all vertices at infinity.)\n\nThe family starts uniquely as 6-polytopes. The \"triangular prism\" and \"rectified 5-cell\" are included at the beginning for completeness. The \"demipenteract\" also exists in the demihypercube family.\n\nThey are also sometimes named by their symmetry group, like E6 polytope, although there are many uniform polytopes within the \"E\" symmetry.\n\nThe complete family of Gosset semiregular polytopes are:\n\nEach polytope is constructed from (\"n\" − 1)-simplex and (\"n\" − 1)-orthoplex facets.\n\nThe orthoplex faces are constructed from the Coxeter group \"D\" and have a Schläfli symbol of {3} rather than the regular {3,4}. This construction is an implication of two \"facet types\". Half the facets around each orthoplex ridge are attached to another orthoplex, and the others are attached to a simplex. In contrast, every simplex ridge is attached to an orthoplex.\n\nEach has a vertex figure as the previous form. For example, the \"rectified 5-cell\" has a vertex figure as a \"triangular prism\".\n\n\n\n"}
{"id": "287826", "url": "https://en.wikipedia.org/wiki?curid=287826", "title": "Upper half-plane", "text": "Upper half-plane\n\nIn mathematics, the upper half-plane H is the set of points (\"x,y\") in the Cartesian plane with \"y\" > 0.\n\nMathematicians sometimes identify the Cartesian plane with the complex plane, and then the upper half-plane corresponds to the set of complex numbers with positive imaginary part:\n\nThe term arises from a common visualization of the complex number \"x\" + \"iy\" as the point (\"x\",\"y\") in the plane endowed with Cartesian coordinates. When the Y-axis is oriented vertically, the \"upper half-plane\" corresponds to the region above the X-axis and thus complex numbers for which \"y\" > 0.\n\nIt is the domain of many functions of interest in complex analysis, especially modular forms. The lower half-plane, defined by \"y\" < 0, is equally good, but less used by convention. The open unit disk D (the set of all complex numbers of absolute value less than one) is equivalent by a conformal mapping to H (see \"Poincaré metric\"), meaning that it is usually possible to pass between H and D.\n\nIt also plays an important role in hyperbolic geometry, where the Poincaré half-plane model provides a way of examining hyperbolic motions. The Poincaré metric provides a hyperbolic metric on the space.\n\nThe uniformization theorem for surfaces states that the upper half-plane is the universal covering space of surfaces with constant negative Gaussian curvature.\n\nThe closed upper half-plane is the union of the upper half-plane and the real axis. It is the closure of the upper half-plane.\n\nThe affine transformations of the upper half-plane include (1) shifts (\"x,y\") → (\"x\" + \"c, y\"), \"c\" ∈ ℝ, and (2) dilations (\"x,y\") → (λ \"x\", λ \"y\"), λ > 0.\n\nProposition: Let \"A\" and \"B\" be semicircles in the upper half-plane with centers on the boundary. Then there is an affine mapping that takes \"A\" to \"B\".\n\nDefinition: formula_2\n\n\"Z\" can be recognized as the circle of radius 1/2 centered at (1/2, 0), and as the polar plot of formula_3\n\nProposition: (0,0), ρ(θ) in \"Z\", and (1, tan θ) are collinear points.\n\nIn fact, \"Z\" is the reflection of the line (1,\"y\"), \"y\" > 0, in the unit circle. Indeed, the diagonal from (0,0) to (1, tan θ) has squared length formula_4 so that formula_5 is the reciprocal of that length.\n\nThe distance between any two points \"p\" and \"q\" in the upper half-plane can be consistently defined as follows: The perpendicular bisector of the segment from \"p\" to \"q\" either intersects the boundary or is parallel to it. In the latter case \"p\" and \"q\" lie on a ray perpendicular to the boundary and logarithmic measure can be used to define a distance that is invariant under dilation. In the former case \"p\" and \"q\" lie on a circle centered at the intersection of their perpendicular bisector and the boundary. By the above proposition this circle can be moved by affine motion to \"Z\". Distances on \"Z\" can be defined using the correspondence with points on (1,\"y\"), \"y\" > 0, and logarithmic measure on this ray. In consequence, the upper half-plane becomes a metric space. The generic name of this metric space is the hyperbolic plane. In terms of the models of hyperbolic geometry, this model is frequently designated the Poincaré half-plane model.\n\nOne natural generalization in differential geometry is hyperbolic \"n\"-space H, the maximally symmetric, simply connected, \"n\"-dimensional Riemannian manifold with constant sectional curvature −1. In this terminology, the upper half-plane is H since it has real dimension 2.\n\nIn number theory, the theory of Hilbert modular forms is concerned with the study of certain functions on the direct product H of \"n\" copies of the upper half-plane. Yet another space interesting to number theorists is the Siegel upper half-space H, which is the domain of Siegel modular forms.\n\n"}
{"id": "31076367", "url": "https://en.wikipedia.org/wiki?curid=31076367", "title": "William Lax", "text": "William Lax\n\nWilliam Lax (1761 – 29 October 1836) was an English astronomer and mathematician who served as Lowndean Professor of Astronomy and Geometry at the University of Cambridge for 41 years.\n\nLax was born in Ravensworth in the North Riding of Yorkshire. He attended Trinity College, Cambridge and graduated Bachelor of Arts as the Senior Wrangler and first Smith's Prizeman of his year. He was elected a fellow of Trinity College, ordained as a minister, and received his Master of Arts. Lax was granted the livings of vicar of Marsworth, Buckinghamshire and of St Ippolyts near Hitchin, Hertfordshire, where he erected an observatory.\n\nLax was best known for his \"Remarks on a Supposed Error in the Elements of Euclid\" (1807) and his work regarding the \"Nautical Almanac\", which was an important reference for navigation in the period. An obituary claimed that \"To whatever Professor Lax applied, he made himself completely master of it\". His daughter married Andrew Amos and through that line Lax is the grandfather of Sheldon Amos and the great grandfather of Maurice Amos, a notable legal dynasty.\n\nLax was born in the village of Ravensworth, near Richmond in the North Riding of Yorkshire, England, the son of William (1731 – 19 August 1812) also born in Ravensworth, and Hannah Lax (1738 – 10 June 1811). He was christened on 27 October 1761 in Burneston. He was educated at the Kirby Ravensworth Free Grammar School, where he learned Latin (in which he became fluent) and Greek as well as English language, arithmetic and mathematics. Although the school was subsidised by a charitable trust, \"Free\" in the context of the school's name meant free from all authority save for the Crown.\n\nLax was admitted as a sizar to Trinity College, Cambridge University on 22 November 1780 at the age of 19. Trinity was at the time the richest college at Cambridge. Sizars were students who were not of the gentlemanly class, who were charged lower fees and obtained free food and/or lodging and other assistance during their period of study in exchange for performing work at their colleges. By the eighteenth century, sizars were fully integrated members of the community, who were as likely to be employed by Fellow commoners as companions rather than servants. They were expected to wait at table (as were pensioners and scholars), but by the eighteenth century they had their own gyps (servants) and bedmakers.\n\nLax matriculated in the Michaelmas term of 1781 and became a private tutor to John Pond, later Astronomer Royal. Lax was elected a scholar (i.e. one on a scholarship) of Trinity in 1784; John Cranke and Henry Therond were his tutors, a role which would have seen them not only teaching Lax, but also acting in the role of \"in loco parentis\". Lax was conferred a Bachelor of Arts (B.A.) in 1785 and graduated as the Senior Wrangler and was awarded the first Smith's Prize of his year. Until 1790, all examinations at Trinity were written in Latin.\n\nIn 1785 Lax was appointed curate of Tideswell in the Peak District of Derbyshire with an annual stipend of £35. In 1786, as was essentially the due of Senior Wranglers, he was elected a fellow of Trinity College. According to Peter Linehan, fellows at this time, \"were becoming richer, living and behaving more like gentlemen\". He was ordained as a minister in 1787 at Peterborough and received his Master of Arts (M.A.) in 1788. He was a moderator from 1789 to 1791 which entailed him presiding over oral examinations which were then necessary for the B.A. to be awarded. As a moderator Lax was responsible for the introduction of \"very high flown compliments, and at the same time extending the disputations to double the usual length, which was around one hour and ten minutes\" which \"sent a ripple through tradition\" according to Greg Dening. Dening argues that this was, \"Lax's way of getting into the act and making Acts flourish\". In 1791 he was appointed as a taxor by the University. Lax was an assistant tutor from 1797 until 1801, but resigned when he married Margaret Cradock, as College fellows were not permitted to marry.\n\nIn 1795 Lax was appointed Lowndean Professor of Astronomy and Geometry in succession to John Smith. The position was a sinecure with an annual salary initially of around £300, later rising to around £500 per annum by 1821. Cambridge had two astronomical chairs, and the Lowndean was seen as the more theoretical and less experimental of the two. During Lax's tenure a mathematical chair was seen as \"a prize or a means of securing leisure, and at best, merely as offering a position where a man could pursue his own researches undisturbed by other duties\". In 1816 Lax was described as holding the professorship with \"great reputation\". The sole duty of Lax's professorship was that he was required to examine students annually for the Smith's Prize, including John Herschel, Adam Sedgwick, George Biddell Airy and William Cavendish, 7th Duke of Devonshire.\n\nDuring Lax's time at Cambridge: \"the mathematicians were in the saddle, and it would be difficult to dispute the judgement that they controlled Cambridge studies almost as completely as the logicians had done in the Middle Ages.\" In early nineteenth century Cambridge \"the discipline of mathematics was at the very heart\". Newtonian mathematics teaching as exemplified by \"Cambridge traditionalists\" such as Lax and his generation began to wane as the Georgian era drew to a close. In 1817 George Peacock successfully introduced the new French mathematics (such as Pierre-Simon Laplace and Joseph Louis Lagrange) into the Senate House Examinations. Peacock reported to Herschel, \"The introduction of d's into the papers excited much remark. Wood, Vince, Lax & Milner were very angry & threatened to protest against [the infiltration of] French mathematics.\" For the traditionalists the struggle was more than one of intellectual difference as for them, \"Newton's rational mechanics, fluxions, and experimental philosophy were an excellent antidote against materialism and atheism.\" However, from 1816 to 1824 Lax continued to sit on the Peacock-led board that established Cambridge Observatory.\n\nLax did encounter some criticism during his tenure. Whilst at the university he \"never, as far as is known, delivered a single lecture\", despite his chair's bequest that the holder deliver forty lectures each year, although his predecessor had not given any lectures either. The excuse was made that there was already an astronomical chair at Cambridge established before the Lowndean that already gave lectures. By the 1820s it was no longer acceptable to consider chairs as sinecures, and Lax received criticism from a living descendent of the original benefactor, Thomas Lowndes, for being remiss in his duties. His successor to the Lowndean chair George Peacock promised \"to do his duty in a less lax manner than his predecessor\", and although he struggled to get anyone to attend his lectures on pure mathematics, his lectures on practical astronomy were well attended. However, Peacock's translation as Dean of Ely three years later meant that he was largely absent from his chair, which he was severely criticised for retaining.\n\nLax was admitted a Fellow of the Royal Society on 5 April 1796. He was nominated by the Astronomer Royal Nevil Maskelyne, Anthony Shepherd, Richard Farmer and William Wales. However, due to an enmity of the President Joseph Banks, friends of Charles Hutton and Maskelyne, such as Lax, Samuel Vince and Thomas Mudge, frequently saw their submissions for publications overlooked. If any of them submitted papers to the Society: they had the honour of having them carefully lodged in the archives of the Society, where the world in general, or even the members of the Society, would derive no more benefit from them than if they were deposited at the centre of the earth.\n\nLax delivered two papers to the Royal Society which were published in \"Philosophical Transactions\". In 1799 he delivered \"A Method of finding the Latitude of a place, by Means of two Altitudes of the Sun and the Time elapsed betwixt the Observations\", described as containing \"several valuable remarks\", but criticised as \"a subject of no great importance\" by the \"Philosophical Magazine\". In his 1809 work \"On A Method of Examining the Divisions of Astronomical Instruments\" Lax wrote that no instrument was to be trusted without \"previous examination\". This argument had an influence on scientists such as Henry Cavendish and was described as an \"ingenious...examination\" in the \"Edinburgh Encyclopedia\". However the method described by Lax \"though very ingenious, requires great labour and time, and is inferior in accuracy and efficiency to that which was adopted by Mr. Troughton for tabulating the errors of the primary divisions of circular instruments.\" It was also criticised for \"greatly resembl[ing]\" a method first explicated by the Duke of Chaulnes. In 1807 Lax delivered \"Remarks on a Supposed Error in the Elements of Euclid\" to the Royal Society, however it was not published in \"Philosophical Transactions\", but was eventually published independently. In it Lax defended the Greek mathematician against a charge levied at him by Georges-Louis Le Sage in 1756. Lax's defence was applauded by the \"British Critic\" as 'perfectly sound'. Lax also espoused the worth of Euclid's \"Elements\" in the work, which he considered to reflect \"the highest honour upon the human intellect\".\n\nLax was elected to the Board of Longitude after he was nominated to the Lowndean chair in 1795, and remained on the board until it was dissolved in 1828. The Board was a governmental body charged with administering a scheme of prizes intended to encourage innovators to solve the problem of finding longitude at sea, which was vital for accurate navigation. Lax published a set of tables for use with the \"Nautical Almanac\" for finding latitude and longitude; these were published by the Board of Longitude in 1821, and whilst they were not considered to be of much practical use for seamen, they were described by \"The Nautical Magazine\" as a \"very meritorious attempt to solve the problems of nautical astronomy by one uniform system.\" In 1821, the Board awarded Lax £1050 for his tables, which were intended to replace Nevil Maskelyne's \"Requisite Tables\". However, the extraordinarily accurate chronometers of John Harrison were generally available from the 1820s onwards, rendering the lunar distance method, which Lax had used to create the tables, immaterial. Meanwhile, Edward Sabine criticised errors in Lax's work.\n\nAs a scientific member of the Board, Lax was one of eighteen men who were, according to Edmund Dews, \"ultimately responsible for the form and contents of the \"Nautical Almanac\". It would have been difficult in these years to select another group equally eminent in their field.\" Lax was notable for his strong attendance record at the quarterly meetings, not missing a single meeting between 1822 – 25, a record equalled by only three other members, although non-attendance of meetings would have resulted in his not being paid his annual salary of £100. In 1828 Lax appended \"An easy method of correcting the lunar distance, on account of the spheroidal figure of the earth\" to the \"Nautical Almanac\". After the Board was dissolved in 1828 Lax unsuccessfully attempted to convince George Biddell Airy to aid in a campaign for its restoration. In 1834 a new edition of his nautical tables was published posthumously. Eva Germaine Rimington Taylor later concluded that all of Lax's works were \"of value to the art of navigation\".\n\nOn 28 February 1801 Lax was granted the livings as vicar of Marsworth, Buckinghamshire and squarson of Great Wymondley with St Ippolyts near Hitchin, Hertfordshire \"after some years of teaching work\". He lived at St Ippolyts where he erected a private observatory which he had transported from Cambridge and had originally belonged to Isaac Newton. Charles Hutton's 1815 list of England's 20 most notable private observatories (excluding the King's private observatory) included Professor Lax's. Lax spent the last thirty years of his life occupied with \"studies and pursuits connected with the advancement of astronomy.\"\n\nWhen he arrived at St Ippolyts Lax had trees planted in the vicarage grounds in the form of his initials \"W L\". That same year he created a park opposite to the vicarage similar to The Backs of Cambridge, built a replica of Trinity College Bridge, dammed the stream and opened springs to form a lake which was used for ice skating in the winter. In September 1801 he married Margaret Cradock (11 June 1776 – 20 January 1854) at the church in Gilling West in the North Riding of Yorkshire. Margaret was the eldest daughter of Sheldon Cradock of Hartforth who was the lord of the manor of Lax's home village of Ravensworth. Lax was a proposer of Robert Woodhouse, Henry Coddington, Herbert Marsh and John Bell for Royal Society fellowship, the latter being one of Lax's closest friends, and he was a keen supporter of George Biddell Airy throughout Airy's career. Lax was a chief supporter of the Whig John Romilly, 1st Baron Romilly's parliamentary bid. In 1824 Lax purchased a coat of arms for himself.\n\nHis brother Thomas Lax (1770 – 1 Apr 1851) lived in Ravensworth. He was a gentleman farmer who became a record holding breeder of shorthorn cattle and at one point was credited with the best shorthorn herd in the country. He was \"unquestionably a great breeder\". He also acted as Chief Constable of the wapentake of Gilling West. The Kirkby Ravensworth parish church has a memorial dedicated to Thomas Lax, as well as a memorial dedicated to the mother of the two brothers.\n\nOn 1 December 1834, Lax reported that he had been \"of late in a very weak state of health\". He died \"suddenly\" on 28 October 1836 at his home in St Ippolyts. His obituary in \"The Gentleman's Magazine\" reported that \"his constitution was broken in early life [which] made his last years a period of weakness and suffering, so that his physical strength was unequal to the workings of his active mind. To whatever Professor Lax applied, he made himself completely master of it...[a] most excellent and amiable man.\" He left behind a widow and two daughters, the eldest Margaret and the younger Marian or Marianne (died 21 June 1873). In 1826 Margaret was married to Andrew Amos at St Ippolyts Church, and via that line Lax is the grandfather of Sheldon Amos and the great grandfather of Maurice Amos.\n\n"}
{"id": "17708915", "url": "https://en.wikipedia.org/wiki?curid=17708915", "title": "Zonal spherical function", "text": "Zonal spherical function\n\nIn mathematics, a zonal spherical function or often just spherical function is a function on a locally compact group \"G\" with compact subgroup \"K\" (often a maximal compact subgroup) that arises as the matrix coefficient of a \"K\"-invariant vector in an irreducible representation of \"G\". The key examples are the matrix coefficients of the \"spherical principal series\", the irreducible representations appearing in the decomposition of the unitary representation of \"G\" on \"L\"(\"G\"/\"K\"). In this case the commutant of \"G\" is generated by the algebra of biinvariant functions on \"G\" with respect to \"K\" acting by right convolution. It is commutative if in addition \"G\"/\"K\" is a symmetric space, for example when \"G\" is a connected semisimple Lie group with finite centre and \"K\" is a maximal compact subgroup. The matrix coefficients of the spherical principal series describe precisely the spectrum of the corresponding\nC* algebra generated by the biinvariant functions of compact support, often called a Hecke algebra. The spectrum of the commutative Banach *-algebra of biinvariant \"L\" functions is larger; when \"G\" is a semisimple Lie group with maximal compact subgroup \"K\", additional characters come from matrix coefficients of the complementary series, obtained by analytic continuation of the spherical principal series.\n\nZonal spherical functions have been explicitly determined for real semisimple groups by Harish-Chandra. For special linear groups, they were independently discovered by Israel Gelfand and Mark Naimark. For complex groups, the theory simplifies significantly, because \"G\" is the complexification of \"K\", and the formulas are related to analytic continuations of the Weyl character formula on \"K\". The abstract functional analytic theory of zonal spherical functions was first developed by Roger Godement. Apart from their group theoretic interpretation, the zonal spherical functions for a semisimple Lie group \"G\" also provide a set of simultaneous eigenfunctions for the natural action of the centre of the universal enveloping algebra of \"G\" on \"L\"(\"G\"/\"K\"), as differential operators on the symmetric space \"G\"/\"K\". For semisimple p-adic Lie groups, the theory of zonal spherical functions and Hecke algebras was first developed by Satake and Ian G. Macdonald. The analogues of the Plancherel theorem and Fourier inversion formula in this setting generalise the eigenfunction expansions of Mehler, Weyl and Fock for singular ordinary differential equations: they were obtained in full generality in the 1960s in terms of Harish-Chandra's c-function.\n\nThe name \"zonal spherical function\" comes from the case when \"G\" is SO(3,R) acting on a 2-sphere and \"K\" is the subgroup fixing a point: in this case the zonal spherical functions can be regarded as certain functions on the sphere invariant under rotation about a fixed axis.\n\nLet \"G\" be a locally compact unimodular topological group and \"K\" a compact subgroup and let \"H\" = \"L\"(\"G\"/\"K\"). Thus \"H\" admits a unitary representation π of \"G\" by left translation. This is a subrepresentation of the regular representation, since if \"H\"= \"L\"(\"G\") with left and right regular representations λ and ρ of \"G\" and \"P\" is the orthogonal projection\n\nfrom \"H\" to \"H\" then \"H\" can naturally be identified with \"PH\" with the action of \"G\" given by the restriction of λ.\n\nOn the other hand, by von Neumann's commutation theorem\n\nwhere \"S\"' denotes the commutant of a set of operators \"S\", so that\n\nThus the commutant of π is generated as a von Neumann algebra by operators\n\nwhere \"f\" is a continuous function of compact support on \"G\".\n\nHowever \"P\"ρ(\"f\") \"P\" is just the restriction of ρ(\"F\") to \"H\", where\n\nis the \"K\"-biinvariant continuous function of compact support obtained by averaging \"f\" by \"K\" on both sides.\n\nThus the commutant of π is generated by the restriction of the operators ρ(\"F\") with \"F\" in\n\"C\"(\"K\"\\\"G\"/\"K\"), the \"K\"-biinvariant continuous functions of compact support on \"G\".\n\nThese functions form a * algebra under convolution with involution\n\noften called the Hecke algebra for the pair (\"G\", \"K\").\n\nLet \"A\"(\"K\"\\\"G\"/\"K\") denote the C* algebra generated by the operators ρ(\"F\") on \"H\".\n\nThe pair (\"G\", \"K\")\nis said to be a Gelfand pair if one, and hence all, of the following algebras are commutative:\n\n\nSince \"A\"(\"K\"\\\"G\"/\"K\") is a commutative C* algebra, by the Gelfand–Naimark theorem it has the form \"C\"(\"X\"),\nwhere \"X\" is the locally compact space of norm continuous * homomorphisms of \"A\"(\"K\"\\\"G\"/\"K\") into C.\n\nA concrete realization of the * homomorphisms in \"X\" as \"K\"-biinvariant uniformly bounded functions on \"G\" is obtained as follows.\n\nBecause of the estimate\n\nthe representation π of \"C\"(\"K\"\\\"G\"/\"K\") in \"A\"(\"K\"\\\"G\"/\"K\") extends by continuity\nto L(\"K\"\\\"G\"/\"K\"), the * algebra of \"K\"-biinvariant integrable functions. The image forms\na dense * subalgebra of \"A\"(\"K\"\\\"G\"/\"K\"). The restriction of a * homomorphism χ continuous for the operator norm is\nalso continuous for the norm ||·||. Since the Banach space dual of L is L,\nit follows that\n\nfor some unique uniformly bounded \"K\"-biinvariant function \"h\" on \"G\". These functions \"h\" are exactly the zonal spherical functions for the pair (\"G\", \"K\").\n\nA zonal spherical function \"h\" has the following properties:\n\n\nThese are easy consequences of the fact that the bounded linear functional χ defined by \"h\" is a homomorphism. Properties 2, 3 and 4 or properties 3, 4 and 5 characterize zonal spherical functions. A more general class of zonal spherical functions can be obtained by dropping positive definiteness from the conditions, but for these functions there is no longer any connection\nwith unitary representations. For semisimple Lie groups, there is a further characterization as eigenfunctions of\ninvariant differential operators on \"G\"/\"K\" (see below).\n\nIn fact, as a special case of the Gelfand–Naimark–Segal construction, there is one-one correspondence between\nirreducible representations σ of \"G\" having a unit vector \"v\" fixed by \"K\" and zonal spherical functions\n\"h\" given by\n\nSuch irreducible representations are often described as having class one. They are precisely the irreducible representations required to decompose the induced representation π on \"H\". Each representation σ extends uniquely by continuity\nto \"A\"(\"K\"\\\"G\"/\"K\"), so that each zonal spherical function satisfies\n\nfor \"f\" in \"A\"(\"K\"\\\"G\"/\"K\"). Moreover, since the commutant π(\"G\")' is commutative,\nthere is a unique probability measure μ on the space of * homomorphisms \"X\" such that\n\nμ is called the Plancherel measure. Since π(\"G\")' is the centre of the von Neumann algebra generated by \"G\", it also gives the measure associated with the direct integral decomposition of \"H\" in terms of the irreducible representations σ.\n\nIf \"G\" is a connected Lie group, then, thanks to the work of Cartan, Malcev, Iwasawa and Chevalley, \"G\" has a maximal compact subgroup, unique up to conjugation. In this case \"K\" is connected and the quotient \"G\"/\"K\" is diffeomorphic to a Euclidean space. When \"G\" is in addition semisimple, this can be seen directly using the Cartan decomposition associated to the symmetric space \"G\"/\"K\", a generalisation of the polar decomposition of invertible matrices. Indeed, if τ is the associated period two automorphism of \"G\" with fixed point subgroup \"K\", then\n\nwhere\n\nUnder the exponential map, \"P\" is diffeomorphic to the -1 eigenspace of τ in the Lie algebra of \"G\".\nSince τ preserves \"K\", it induces an automorphism of the Hecke algebra \"C\"(\"K\"\\\"G\"/\"K\"). On the\nother hand, if \"F\" lies in \"C\"(\"K\"\\\"G\"/\"K\"), then\n\nso that τ induces an anti-automorphism, because inversion does. Hence, when \"G\" is semisimple,\n\n\nMore generally the same argument gives the following criterion of Gelfand for (\"G\",\"K\") to be a Gelfand pair:\n\n\nThe two most important examples covered by this are when:\n\n\nThe three cases cover the three types of symmetric spaces \"G\"/\"K\":\n\n\nLet \"G\" be a compact semisimple connected and simply connected Lie group and τ a period two automorphism of a \"G\" with fixed point subgroup \"K\" = \"G\". In this case \"K\" is a connected compact Lie group. In addition let \"T\" be a maximal torus of \"G\" invariant under τ, such that \"T\" formula_19 \"P\" is a maximal torus in \"P\", and set\n\n\"S\" is the direct product of a torus and an elementary abelian 2-group.\n\nIn 1929 Élie Cartan found a rule to determine the decomposition of L(\"G\"/\"K\") into the direct sum of finite-dimensional irreducible representations of \"G\", which was proved rigorously only in 1970 by Sigurdur Helgason. Because the commutant of \"G\" on L(\"G\"/\"K\") is commutative, each irreducible representation appears with multiplicity one. By Frobenius reciprocity for compact groups, the irreducible representations \"V\" that occur are precisely those admitting a non-zero vector fixed by \"K\".\n\nFrom the representation theory of compact semisimple groups, irreducible representations of \"G\" are classified by their highest weight. This is specified by a homomorphism of the maximal torus \"T\" into T.\n\nThe Cartan–Helgason theorem states that\n\nThe corresponding irreducible representations are called \"spherical representations\".\n\nThe theorem can be proved using the Iwasawa decomposition:\n\nwhere formula_22, formula_23, formula_24 are the complexifications of the Lie algebras of \"G\", \"K\", \"A\" = \"T\" formula_19 \"P\" and\n\nsummed over all eigenspaces for \"T\" in formula_22 corresponding to positive roots α not fixed by τ.\n\nLet \"V\" be a spherical representation with highest weight vector \"v\" and \"K\"-fixed vector \"v\". Since \"v\" is an eigenvector of the solvable Lie algebra formula_28, the Poincaré–Birkhoff–Witt theorem\nimplies that the \"K\"-module generated by \"v\" is the whole of \"V\". If \"Q\" is the orthogonal projection onto the fixed points of \"K\" in \"V\" obtained by averaging over \"G\" with respect to Haar measure, it follows that\n\nfor some non-zero constant \"c\". Because \"v\" is fixed by \"S\" and \"v\" is an eigenvector for \"S\", the subgroup \"S\" must actually fix \"v\", an equivalent form of the triviality condition on \"S\".\n\nConversely if \"v\" is fixed by \"S\", then it can be shown that the matrix coefficient\n\nis non-negative on \"K\". Since \"f\"(1) > 0, it follows that (\"Qv\", \"v\") > 0 and hence that \"Qv\" is a non-zero vector fixed by \"K\".\n\nIf \"G\" is a non-compact semisimple Lie group, its maximal compact subgroup \"K\" acts by conjugation on the component \"P\" in the Cartan decomposition. If \"A\" is a maximal Abelian subgroup of \"G\" contained in \"P\", then \"A\" is diffeomorphic to its Lie algebra under the exponential map and, as a further generalisation of the polar decomposition of matrices, every element of \"P\" is conjugate under \"K\" to an element of \"A\", so that\n\nThere is also an associated Iwasawa decomposition\n\nwhere \"N\" is a closed nilpotent subgroup, diffeomorphic to its Lie algebra under the exponential map and normalised by \"A\". Thus\n\"S\"=\"AN\" is a closed solvable subgroup of \"G\", the semidirect product of \"N\" by \"A\", and \"G\" = \"KS\".\n\nIf α in Hom(\"A\",T) is a character of \"A\", then α extends to a character of \"S\", by defining it to be trivial on \"N\". There is a corresponding unitary induced representation σ of \"G\" on L(\"G\"/\"S\") = L(\"K\"), a so-called (spherical) principal series representation.\n\nThis representation can be described explicitly as follows. Unlike \"G\" and \"K\", the solvable Lie group \"S\" is not unimodular. Let \"dx\" denote left invariant Haar measure on \"S\" and Δ the modular function of \"S\". Then\n\nThe principal series representation σ is realised on L(\"K\") as\n\nwhere\n\nis the Iwasawa decomposition of \"g\" with \"U\"(\"g\") in \"K\" and \"X\"(\"g\") in \"S\" and\n\nfor \"k\" in \"K\" and \"x\" in \"S\".\n\nThe representation σ is irreducible, so that if \"v\" denotes the constant function 1 on \"K\", fixed by \"K\",\n\ndefines a zonal spherical function of \"G\".\n\nComputing the inner product above leads to Harish-Chandra's formula for the zonal spherical function\n\nas an integral over \"K\".\n\nHarish-Chandra proved that these zonal spherical functions exhaust the characters of the C* algebra generated by the \"C\"(\"K\" \\ \"G\" / \"K\") acting by right convolution on \"L\"(\"G\" / \"K\"). He also showed that two different characters α and β give the same zonal spherical function if and only if α = β·\"s\", where \"s\" is in the Weyl group of \"A\"\n\nthe quotient of the normaliser of \"A\" in \"K\" by its centraliser, a finite reflection group.\n\nIt can also be verified directly that this formula defines a zonal spherical function, without using representation theory. The proof for general semisimple Lie groups that every zonal spherical formula arises in this way requires the detailed study of \"G\"-invariant differential operators on \"G\"/\"K\" and their simultaneous eigenfunctions (see below). In the case of complex semisimple groups, Harish-Chandra and Felix Berezin realised independently that the formula simplified considerably and could be proved more directly.\n\nThe remaining positive-definite zonal spherical functions are given\nby Harish-Chandra's formula with α in Hom(\"A\",C*) instead of Hom(\"A\",T). Only certain α are permitted and the corresponding irreducible\nrepresentations arise as analytic continuations of the spherical principal series. This so-called \"complementary series\" was first studied by for \"G\" = SL(2,R) and by and for \"G\" = SL(2,C).\nSubsequently in the 1960s, the construction of a complementary series by analytic continuation of the spherical principal series was systematically developed for general semisimple Lie groups by Ray Kunze, Elias Stein and Bertram Kostant. Since these irreducible representations are not tempered, they are not usually required for harmonic analysis on \"G\" (or \"G\" / \"K\").\n\nHarish-Chandra proved that zonal spherical functions can be characterised as those normalised positive definite \"K\"-invariant functions on \"G\"/\"K\" that are eigenfunctions of \"D\"(\"G\"/\"K\"), the algebra of invariant differential operators on \"G\". This algebra acts on \"G\"/\"K\" and commutes with the natural action of \"G\" by left translation. It can be identified with the subalgebra of the universal enveloping algebra of \"G\" fixed under the adjoint action of \"K\". As for the commutant of \"G\" on L(\"G\"/\"K\") and the corresponding Hecke algebra, this algebra of operators is commutative; indeed it is a subalgebra of the algebra of mesurable operators affiliated with the commutant π(\"G\")', an Abelian von Neumann algebra. As Harish-Chandra proved, it is isomorphic to the algebra of \"W\"(\"A\")-invariant polynomials on the Lie algebra of \"A\", which itself is a polynomial ring by the Chevalley–Shephard–Todd theorem on polynomial invariants of finite reflection groups. The simplest invariant differential operator on \"G\"/\"K\" is the Laplacian operator; up to a sign this operator is just the image under π of the Casimir operator in the centre of the universal enveloping algebra of \"G\".\n\nThus a normalised positive definite \"K\"-biinvariant function \"f\" on \"G\" is a zonal spherical function if and only if for each \"D\" in \"D\"(\"G\"/\"K\") there is a constant λ such that\n\ni.e. \"f\" is a simultaneous eigenfunction of the operators π(\"D\").\n\nIf ψ is a zonal spherical function, then, regarded as a function on \"G\"/\"K\", it is an eigenfunction of the Laplacian\nthere, an elliptic differential operator with real analytic coefficients. By analytic elliptic regularity,\nψ is a real analytic function on \"G\"/\"K\", and hence \"G\".\n\nHarish-Chandra used these facts about the structure of the invariant operators to prove that his formula gave all zonal spherical functions for real semisimple Lie groups. Indeed, the commutativity of the commutant implies that the simultaneous eigenspaces of the algebra of invariant differential operators all have dimension one; and the polynomial structure of this algebra forces the simultaneous eigenvalues to be precisely those already associated with Harish-Chandra's formula.\n\nThe group \"G\" = SL(2,C) is the complexification of the compact Lie group \"K\" = SU(2) and the double cover of the Lorentz group. The infinite-dimensional representations of the Lorentz group were first studied by Dirac in 1945, who considered the discrete series representations, which he termed \"expansors\". A systematic study was taken up shortly afterwards by Harish-Chandra, Gelfand–Naimark and\nBargmann. The irreducible representations of class one, corresponding to the zonal spherical functions, can be determined easily using the radial\ncomponent of the Laplacian operator.\n\nIndeed, any unimodular complex 2×2 matrix \"g\" admits a unique polar decomposition \"g\" = \"pv\" with \"v\" unitary and \"p\" positive. In turn\n\"p\" = \"uau\"*, with \"u\" unitary and \"a\" a diagonal matrix with positive entries. Thus \"g\" = \"uaw\" with \"w\" = \"u\"* \"v\", so that any \"K\"-biinvariant function on \"G\" corresponds to a function of the diagonal matrix\n\ninvariant under the Weyl group. Identifying \"G\"/\"K\" with hyperbolic 3-space, the zonal hyperbolic functions ψ correspond to radial functions that are eigenfunctions of the Laplacian. But in terms of the radial coordinate \"r\", the Laplacian is given by\n\nSetting \"f\"(\"r\") = sinh (\"r\")·ψ(\"r\"), it follows that \"f\" is an odd function of \"r\" and an eigenfunction of formula_40.\n\nHence\n\nwhere formula_41 is real.\n\nThere is a similar elementary treatment for the generalized Lorentz groups SO(\"N\",1) in and (recall that SO(3,1) = SL(2,C) / ±I).\n\nIf \"G\" is a complex semisimple Lie group, it is the complexification of its maximal compact subgroup \"K\". If formula_42 and formula_23 are their Lie algebras, then\n\nLet \"T\" be a maximal torus in \"K\" with Lie algebra formula_45. Then\n\nLet\n\nbe the Weyl group of \"T\" in \"K\". Recall characters in Hom(\"T\",T) are called weights and can be identified with elements of the weight lattice Λ in\nHom(formula_45, R) = formula_49. There is a natural ordering on weights and every finite-dimensional irreducible representation (π, \"V\") of \"K\" has a unique highest weight λ. The weights of the adjoint representation of \"K\" on formula_50 are called roots and ρ is used to denote half the sum of the positive roots α, Weyl's character formula asserts that for \"z\" = exp \"X\" in \"T\"\n\nwhere, for μ in formula_49, \"A\" denotes the antisymmetrisation\n\nand ε denotes the \"sign character\" of the finite reflection group \"W\".\n\nWeyl's denominator formula expresses the denominator \"A\" as a product:\n\nwhere the product is over the positive roots.\n\nWeyl's dimension formula asserts that\n\nwhere the inner product on formula_49 is that associated with the Killing form on formula_23.\n\nNow\n\n\nThe Berezin–Harish–Chandra formula asserts that for \"X\" in formula_60\n\nIn other words:\n\n\nOne of the simplest proofs of this formula involves the \"radial component\" on \"A\" of the Laplacian on \"G\", a proof formally parallel to Helgason's reworking of Freudenthal's classical proof of the Weyl character formula, using the radial component on \"T\" of the Laplacian on \"K\".\n\nIn the latter case the class functions on \"K\" can be identified with \"W\"-invariant functions on \"T\". The\nradial component of Δ on \"T\" is just the expression for the restriction of Δ to \"W\"-invariant functions on \"T\", where\nit is given by the formula\n\nwhere\n\nfor \"X\" in formula_45. If χ is a character with highest weight λ, it follows that φ = \"h\"·χ satisfies\n\nThus for every weight μ with non-zero Fourier coefficient in φ,\n\nThe classical argument of Freudenthal shows that μ + ρ must have the form \"s\"(λ + ρ) for some \"s\" in \"W\", so the character formula\nfollows from the antisymmetry of φ.\n\nSimilarly \"K\"-biinvariant functions on \"G\" can be identified with \"W\"(\"A\")-invariant functions on \"A\". The\nradial component of Δ on \"A\" is just the expression for the restriction of Δ to \"W\"(\"A\")-invariant functions on \"A\".\nIt is given by the formula\n\nwhere\n\nfor \"X\" in formula_60.\n\nThe Berezin–Harish–Chandra formula for a zonal spherical function φ can be established by introducing the antisymmetric function\n\nwhich is an eigenfunction of the Laplacian Δ. Since \"K\" is generated by copies of subgroups that are homomorphic images of SU(2) corresponding to simple roots, its complexification \"G\" is generated by the corresponding homomorphic images of SL(2,C). The formula for zonal spherical functions of SL(2,C) implies that \"f\" is a periodic function on formula_60 with respect to some sublattice. Antisymmetry under the Weyl group and the argument of Freudenthal again imply that ψ must have the stated form up to a multiplicative constant, which can be determined using the Weyl dimension formula.\n\nThe theory of zonal spherical functions for SL(2,R) originated in the work of Mehler in 1881 on hyperbolic geometry. He discovered the analogue of the Plancherel theorem, which was rediscovered by Fock in 1943. The corresponding eigenfunction expansion is termed the Mehler–Fock transform. It was already put on a firm footing in 1910 by Hermann Weyl's important work on the spectral theory of ordinary differential equations. The radial part of the Laplacian in this case leads to a hypergeometric differential equation, the theory of which was treated in detail by Weyl. Weyl's approach was subsequently generalised by Harish-Chandra to study zonal spherical functions and the corresponding Plancherel theorem for more general semimisimple Lie groups. Following the work of Dirac on the discrete series representations of SL(2,R), the general theory of unitary irreducible representations of SL(2,R) was developed independently by Bargmann, Harish-Chandra and Gelfand–Naimark. The irreducible representations of class one, or equivalently the theory of zonal spherical functions, form an important special case of this theory.\n\nThe group \"G\" = SL(2,R) is a double cover of the 3-dimensional Lorentz group SO(2,1), the symmetry group of the hyperbolic plane with its Poincaré metric. It acts by Möbius transformations. The upper half-plane can be identified with the unit disc by the Cayley transform. Under this identification \"G\" becomes identified with the group SU(1,1), also acting by Möbius transformations. Because the action is transitive, both spaces can be identified with \"G\"/\"K\", where \"K\" = SO(2). The metric is invariant under \"G\" and the associated Laplacian is \"G\"-invariant, coinciding with the image of the Casimir operator. In the upper half-plane model the Laplacian is given by the formula\n\nIf \"s\" is a complex number and \"z\" = \"x + i y\" with \"y\" > 0, the function\n\nis an eigenfunction of Δ:\n\nSince Δ commutes with \"G\", any left translate of \"f\" is also an eigenfunction with the same eigenvalue. In particular, averaging over \"K\", the function\n\nis a \"K\"-invariant eigenfunction of Δ on \"G\"/\"K\". When\n\nwith τ real, these functions give all the zonal spherical functions on \"G\". As with Harish-Chandra's more general formula for semisimple Lie groups, φ is a zonal spherical function because it is the matrix coefficient corresponding to a vector fixed by \"K\" in the principal series. Various arguments are available to prove that there are no others. One of the simplest classical Lie algebraic arguments is to note that, since Δ is an elliptic operator with analytic coefficients, by analytic elliptic regularity any eigenfunction is necessarily real analytic. Hence, if the zonal spherical function corresponds is the matrix coefficient for a vector \"v\" and representation σ, the vector \"v\" is an analytic vector for \"G\" and\n\nfor \"X\" in formula_60. The infinitesimal form of the irreducible unitary representations with a vector fixed by \"K\" were worked out classically by Bargmann. They correspond precisely to the principal series of SL(2,R). It follows that the zonal spherical function corresponds to a principal series representation.\n\nAnother classical argument proceeds by showing that on radial functions the Laplacian has the form\n\nso that, as a function of \"r\", the zonal spherical function φ(\"r\") must satisfy the ordinary differential equation\n\nfor some constant α. The change of variables \"t\" = sinh \"r\" transforms this equation into the hypergeometric differential equation. The general solution in terms of Legendre functions of complex index is given by\n\nwhere α = ρ(ρ+1). Further restrictions on ρ are imposed by boundedness and positive-definiteness of the zonal spherical function on \"G\".\n\nThere is yet another approach, due to Mogens Flensted-Jensen, which derives the properties of the zonal spherical functions on SL(2,R), including the Plancherel formula, from the corresponding results for SL(2,C), which are simple consequences of the Plancherel formula and Fourier inversion formula for R. This \"method of descent\" works more generally, allowing results for a real semisimple Lie group to be derived by descent from the corresponding results for its complexification.\n\n\n\n"}
