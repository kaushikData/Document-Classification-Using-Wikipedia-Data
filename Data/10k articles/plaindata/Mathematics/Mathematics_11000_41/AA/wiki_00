{"id": "394783", "url": "https://en.wikipedia.org/wiki?curid=394783", "title": "39 (number)", "text": "39 (number)\n\n39 (thirty-nine) is the natural number following 38 and preceding 40.\n\n\n\n\n\n"}
{"id": "15843438", "url": "https://en.wikipedia.org/wiki?curid=15843438", "title": "Amy Cohen-Corwin", "text": "Amy Cohen-Corwin\n\nAmy Cohen-Corwin (formerly known as Amy C. Murray) is a professor emerita of mathematics at Rutgers University and former Dean of University College at Rutgers University. Dr. Cohen-Corwin is especially interested in the Korteweg–de Vries equation, cubic Schrödinger equation on the line, and improving undergraduate education, especially for future teachers. Dr. Cohen-Corwin has held numerous organizational positions, including Co-organizer for the AIM (American Institute of Mathematics) and NSF (National Science Foundation)-sponsored workshop \"Finding and Keeping Graduate Students in the Mathematical Sciences.\"\n\n\n\nRutgers Office for the Promotion of Women in Science, Technology, and Engineering\n"}
{"id": "14346064", "url": "https://en.wikipedia.org/wiki?curid=14346064", "title": "Autoregressive conditional duration", "text": "Autoregressive conditional duration\n\nIn financial econometrics, an autoregressive conditional duration (ACD, Engle and Russell (1998)) model considers irregularly spaced and autocorrelated intertrade durations. ACD is analogous to GARCH. Indeed, in a continuous double auction (a common trading mechanism in many financial markets) waiting times between two consecutive trades vary at random.\n\nSpecifically, let formula_1 denote the duration \n(the waiting time between consecutive trades) and\nassume that formula_2, where\nformula_3 are independent and identically distributed random variables, positive and with formula_4 and where\nthe series formula_5 is given by\n\nformula_6\n\nand where formula_7, formula_8,\nformula_9, formula_10.\n\n"}
{"id": "346611", "url": "https://en.wikipedia.org/wiki?curid=346611", "title": "Axiom of countability", "text": "Axiom of countability\n\nIn mathematics, an axiom of countability is a property of certain mathematical objects (usually in a category) that asserts the existence of a countable set with certain properties. Without such an axiom, such a set might not provably exist.\n\nImportant countability axioms for topological spaces include:\n\nThese axioms are related to each other in the following ways:\nOther examples of mathematical objects obeying axioms of countability include sigma-finite measure spaces, and lattices of countable type.\n"}
{"id": "17181013", "url": "https://en.wikipedia.org/wiki?curid=17181013", "title": "Beltrami identity", "text": "Beltrami identity\n\nThe Beltrami identity, named after Eugenio Beltrami, is a simplified and less general version of the Euler–Lagrange equation in the calculus of variations.\n\nThe Euler–Lagrange equation serves to extremize action functionals of the form \nwhere are constants and .\n\nFor the special case of , the Euler–Lagrange equation reduces to the Beltrami identity,\nwhere is a constant.\n\nThe following derivation of the Beltrami identity starts with the Euler–Lagrange equation, \nMultiplying both sides by ,\n\nAccording to the chain rule,\nwhere .\n\nRearranging this yields\nThus, substituting this expression for into the second equation of this derivation, \nBy the product rule, the last term is re-expressed as\nand rearranging,\n\nFor the case of , this reduces to \nso that taking the antiderivative results in the Beltrami identity,\nwhere is a constant.\n\nAn example of an application of the Beltrami identity is the Brachistochrone problem, which involves finding the curve that minimizes the integral\nThe integrand \ndoes not depend explicitly on the variable of integration , so the Beltrami identity applies,\nSubstituting for and simplifying,\nwhich can be solved with the result put in the form of parametric equations \nwith being half the above constant, 1/(2\"C\" ²), and being a variable. These are the parametric equations for a cycloid.\n"}
{"id": "46396972", "url": "https://en.wikipedia.org/wiki?curid=46396972", "title": "Big q-Legendre polynomials", "text": "Big q-Legendre polynomials\n\nIn mathematics, the big q-Legendre polynomials are an orthogonal family of polynomials defined in terms of Heine's basic hypergeometric series as\n\nThey obey the orthogonality relation\n\nand have the limiting behavior\n\nwhere formula_4 is the formula_5th Legendre polynomial.\n"}
{"id": "3025671", "url": "https://en.wikipedia.org/wiki?curid=3025671", "title": "Blagovest Sendov", "text": "Blagovest Sendov\n\nBlagovest Hristov Sendov () (born 8 February 1932) is a Bulgarian diplomat, mathematician and politician.\n\nHe was born in Asenovgrad, Bulgaria.\n\nSendov was the rector of Sofia University, located in Sofia, Bulgaria; and the Deputy Chairman of Bulgarian Academy of Sciences, also located in Sofia. He has more than 200 publications in fields related to mathematics and computer science.\n\nSendov took part as an independent in the 1992 Bulgarian presidential election with Ognyan Saparev as his running mate, finishing in 4th place with 2.24% of the votes.\n\nFrom 1995 to 1997, he was the Chairperson of the National Assembly of Bulgaria; and from 1997 to 2002, he was the its Deputy Chairperson. His candidacy for that position was supported by the Bulgarian Socialist Party (BSP), the successor to the Bulgarian Communist Party (BCP). Although never a member of the BCP, Sendov had close ties to former Bulgarian communist dictator Todor Zhivkov.\n\nThe rightist Union of the Democratic Forces removed him temporarily from that duty in 2000 when Sendov cosigned, together with four members of the BSP, a letter to the Israeli president asking that portraits of the Bulgarian Royal Family (from the 1940s) be removed from a memorial in Israel. This memorial commemorates that all Bulgarian Jews were saved from deportation to concentration camps during World War II.\n\nSendov was Bulgarian ambassador to Japan from 2004 to 2009.\n\nSendov's name is attached to one of the major unsolved problems in the study of polynomial zeros, Sendov's conjecture (sometimes incorrectly known as Ilieff's conjecture).\n\nIn 2000 he was elected as a member of Serbian Academy of Sciences and Arts, an academic institution located in Belgrade, Serbia.\n\n\n"}
{"id": "40634", "url": "https://en.wikipedia.org/wiki?curid=40634", "title": "Convex hull", "text": "Convex hull\n\nIn mathematics, the convex hull or convex envelope or convex closure of a set \"X\" of points in the Euclidean plane or in a Euclidean space (or, more generally, in an affine space over the reals) is the smallest convex set that contains \"X\". For instance, when \"X\" is a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around \"X\".\n\nFormally, the convex hull may be defined either as the intersection of all convex sets containing \"X\", or as the set of all convex combinations of points in \"X\". With the latter definition, convex hulls may be extended from Euclidean spaces to arbitrary real vector spaces; they may also be generalized further, to oriented matroids.\n\nThe algorithmic problem of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces is one of the fundamental problems of computational geometry.\n\nA set of points is defined to be convex if it contains the line segments connecting each pair of its points. The convex hull of a given set \"X\" may be defined as\n\nIt is not obvious that the first definition makes sense: why should there exist a unique minimal convex set containing \"X\", for every \"X\"? However, the second definition, the intersection of all convex sets containing \"X\", is well-defined, and it is a subset of every other convex set \"Y\" that contains \"X\", because \"X\" is included among the sets being intersected. Thus, it is exactly the unique minimal convex set containing \"X\". Each convex set containing \"X\" must (by the assumption that it is convex) contain all convex combinations of points in \"X\", so the set of all convex combinations is contained in the intersection of all convex sets containing \"X\". Conversely, the set of all convex combinations is itself a convex set containing \"X\", so it also contains the intersection of all convex sets containing \"X\", and therefore the sets given by these two definitions must be equal.\nIn fact, according to Carathéodory's theorem, if \"X\" is a subset of an \"N\"-dimensional vector space, convex combinations of at most \"N\" + 1 points are sufficient in the definition above. Therefore, the convex hull of a set \"X\" of three or more points in the plane is the union of all the triangles determined by triples of points from \"X\", and more generally in \"N\"-dimensional space the convex hull is the union of the simplices determined by at most \"N\" + 1 vertices from X.\n\nIf the convex hull of \"X\" is a closed set (as happens, for instance, if \"X\" is a finite set or more generally a compact set), then it is the intersection of all closed half-spaces containing \"X\". The hyperplane separation theorem proves that in this case, each point not in the convex hull can be separated from the convex hull by a half-space. However, there exist convex sets, and convex hulls of sets, that cannot be represented in this way. Open halfspaces are such examples.\n\nMore abstractly, the convex-hull operator Conv() has the characteristic properties of a closure operator:\n\nThe convex hull of a finite point set formula_1 is the set of all convex combinations of its points. In a convex combination, each point formula_2 in formula_1 is assigned a weight or coefficient formula_4 in such a way that the coefficients are all non-negative and sum to one, and these weights are used to compute a weighted average of the points. For each choice of coefficients, the resulting convex combination is a point in the convex hull, and the whole convex hull can be formed by choosing coefficients in all possible ways. Expressing this as a single formula, the convex hull is the set:\n\nThe convex hull of a finite point set formula_6 forms a convex polygon when \"n\" = 2, or more generally a convex polytope in formula_7. Each point formula_2 in formula_1 that is not in the convex hull of the other points (that is, such that formula_10) is called a vertex of formula_11. In fact, every convex polytope in formula_7 is the convex hull of its vertices.\n\nIf the points of formula_1 are all on a line, the convex hull is the line segment joining the outermost two points. When the set formula_1 is a nonempty finite subset of the plane (that is, two-dimensional), we may imagine stretching a rubber band so that it surrounds the entire set formula_1 and then releasing it, allowing it to contract; when it becomes taut, it encloses the convex hull of formula_1.\n\nIn two dimensions, the convex hull is sometimes partitioned into two polygonal chains, the upper hull and the lower hull, stretching between the leftmost and rightmost points of the hull. More generally, for points in any dimension in general position, each facet of the convex hull is either oriented upwards (separating the hull from points directly above it) or downwards; the union of the upward-facing facets forms a topological disk, the upper hull, and similarly the union of the downward-facing facets forms the lower hull.\n\nIn computational geometry, a number of algorithms are known for computing the convex hull for a finite set of points and for other geometric objects.\n\nComputing the convex hull means constructing an unambiguous, efficient representation of the required convex shape. The complexity of the corresponding algorithms is usually estimated in terms of n, the number of input points, and h, the number of points on the convex hull.\n\nFor points in two and three dimensions, output-sensitive algorithms are known that compute the convex hull in time O(\"n\" log \"h\"). For dimensions \"d\" higher than 3, the time for computing the convex hull is formula_17, matching the worst-case output complexity of the problem.\n\nThe operation of taking convex hulls behaves well with respect to the Minkowski addition of sets.\n\n\n\nThese results show that \"Minkowski addition\" differs from the \"union \"operation of set theory; indeed, the union of two convex sets need \"not\" be convex: The inclusion \n\nis generally strict. The convex-hull operation is needed for the set of convex sets to form a lattice, in which the \"join\" operation is the convex hull of the union of two convex sets:\n\nThe Delaunay triangulation of a point set and its dual, the Voronoi diagram, are mathematically related to convex hulls: the Delaunay triangulation of a point set in formula_7 can be viewed as the projection of a convex hull in formula_31\n\nTopologically, the convex hull of an open set is always itself open, and the convex hull of a compact set is always itself compact; however, there exist closed sets for which the convex hull is not closed. For instance, the closed set\n\nhas the open upper half-plane as its convex hull.\n\nThe problem of finding convex hulls finds its practical applications in pattern recognition, image processing, statistics, geographic information system, game theory, construction of phase diagrams, and static code analysis by abstract interpretation. It also serves as a tool, a building block for a number of other computational-geometric algorithms such as the rotating calipers method for computing the width and diameter of a point set.\n\nThe convex hull is commonly known as the minimum convex polygon (MCP) in ethology, where it is a classic, though perhaps simplistic, approach in estimating an animal's home range based on points where the animal has been observed. Outliers can make the MCP excessively large, which has motivated relaxed approaches that contain only a subset of the observations (e.g., find an MCP that contains at least 95% of the points).\n\n\n\n"}
{"id": "321895", "url": "https://en.wikipedia.org/wiki?curid=321895", "title": "Cullen number", "text": "Cullen number\n\nIn mathematics, a Cullen number is a natural number of the form formula_1 (written formula_2). Cullen numbers were first studied by James Cullen in 1905. Cullen numbers are special cases of Proth numbers.\n\nIn 1976 Christopher Hooley showed that the natural density of positive integers formula_3 for which \"C\" is a prime is of the order \"o(x)\" for formula_4. In that sense, almost all Cullen numbers are composite. Hooley's proof was reworked by Hiromi Suyama to show that it works for any sequence of numbers \"n\" · 2 + \"b\" where \"a\" and \"b\" are integers, and in particular also for Woodall numbers. The only known Cullen primes are those for \"n\" equal:\nStill, it is conjectured that there are infinitely many Cullen primes.\n\n, the largest known Cullen prime is 6679881 × 2 + 1. It is a megaprime with 2,010,852 digits and was discovered by Magnus Bergman, a PrimeGrid participant from Japan.\n\nA Cullen number \"C\" is divisible by \"p\" = 2\"n\" − 1 if \"p\" is a prime number of the form 8\"k\" - 3; furthermore, it follows from Fermat's little theorem that if \"p\" is an odd prime, then p divides \"C\" for each \"m\"(\"k\") = (2 − \"k\") \n (\"p\" − 1) − \"k\" (for \"k\" > 0). It has also been shown that the prime number \"p\" divides \"C\" when the Jacobi symbol (2 | \"p\") is −1, and that \"p\" divides \"C\" when the Jacobi symbol (2 | \"p\") is +1.\n\nIt is unknown whether there exists a prime number \"p\" such that \"C\" is also prime.\n\nSometimes, a generalized Cullen number base \"b\" is defined to be a number of the form \"n\" × \"b\" + 1, where \"n\" + 2 > \"b\"; if a prime can be written in this form, it is then called a generalized Cullen prime. Woodall numbers are sometimes called Cullen numbers of the second kind.\n\nAccording to Fermat's little theorem, if there is a prime \"p\" such that \"n\" is divisible by \"p\" - 1 and \"n\" + 1 is divisible by \"p\" (especially, when \"n\" = \"p\" - 1) and \"p\" does not divide \"b\", then \"b\" must be congruent to 1 mod \"p\" (since \"b\" is a power of \"b\" and \"b\" is congruent to 1 mod \"p\"). Thus, \"n\" × \"b\" + 1 is divisible by \"p\", so it is not prime. For example, if some \"n\" congruent to 2 mod 6 (i.e. 2, 8, 14, 20, 26, 32, ...), \"n\" × \"b\" + 1 is prime, then \"b\" must be divisible by 3 (except \"b\" = 1).\n\nLeast \"n\" such that \"n\" × \"b\" + 1 is prime are (with question marks if this term is currently unknown)\n\n, the largest known generalized Cullen prime is 1806676 × 41 + 1. It has 2,913,785 digits and was discovered by a PrimeGrid participant.\n\n\n"}
{"id": "7768222", "url": "https://en.wikipedia.org/wiki?curid=7768222", "title": "Distance-transitive graph", "text": "Distance-transitive graph\n\nIn the mathematical field of graph theory, a distance-transitive graph is a graph such that, given any two vertices \"v\" and \"w\" at any distance \"i\", and any other two vertices \"x\" and \"y\" at the same distance, there is an automorphism of the graph that carries \"v\" to \"x\" and \"w\" to \"y\". Distance-transitive graphs were first defined in 1971 by Norman L. Biggs and D. H. Smith.\n\nA distance-transitive graph is interesting partly because it has a large automorphism group. Some interesting finite groups are the automorphism groups of distance-transitive graphs, especially of those whose diameter is 2.\n\nSome first examples of families of distance-transitive graphs include:\nThe simplest asymptotic family of examples of distance-transitive graphs is the hypercube graphs.\n\nAfter introducing them in 1971, Biggs and Smith showed that there are only 12 finite trivalent distance-transitive graphs. These are:\nEvery distance-transitive graph is distance-regular, but the converse is not necessarily true.\n\nIn 1969, before publication of the Biggs–Smith definition, a Russian group led by Georgy Adelson-Velsky showed that there exist graphs that are distance-regular but not distance-transitive. The only graph of this type with degree three is the 126-vertex Tutte 12-cage. The smallest distance-regular graph that is not distance-transitive is the Shrikhande graph. Complete lists of distance-transitive graphs are known for some degrees larger than three, but the classification of distance-transitive graphs with arbitrarily large vertex degree remains open.\n\n\n"}
{"id": "26703335", "url": "https://en.wikipedia.org/wiki?curid=26703335", "title": "Eulerian poset", "text": "Eulerian poset\n\nIn combinatorial mathematics, an Eulerian poset is a graded poset in which every nontrivial interval has the same number of elements of even rank as of odd rank. An Eulerian poset which is a lattice is an Eulerian lattice. These objects are named after Leonhard Euler. Eulerian lattices generalize face lattices of convex polytopes and much recent research has been devoted to extending known results from polyhedral combinatorics, such as various restrictions on \"f\"-vectors of convex simplicial polytopes, to this more general setting.\n\n\n\n\n\n"}
{"id": "45357429", "url": "https://en.wikipedia.org/wiki?curid=45357429", "title": "Exposed point", "text": "Exposed point\n\nIn mathematics, an exposed point of a convex set formula_1 is a point formula_2 at which some continuous linear functional attains its strict maximum over formula_1. Such a functional is then said to \"expose\" formula_4. Note that there can be many exposing functionals for formula_4. The set of exposed points of formula_1 is usually denoted formula_7.\n\nA stronger notion is that of \"strongly exposed point\" of formula_1 which is an exposed point formula_9 such that some exposing functional formula_10 of formula_4 attains its strong maximum over formula_1 at formula_4, i.e. for each sequence formula_14 we have the following implication: formula_15. The set of all strongly exposed points of formula_1 is usually denoted formula_17.\n\nThere are two weaker notions, that of extreme point and that of support point of formula_1.\n"}
{"id": "1113108", "url": "https://en.wikipedia.org/wiki?curid=1113108", "title": "Factorial number system", "text": "Factorial number system\n\nIn combinatorics, the factorial number system, also called factoradic, is a mixed radix numeral system adapted to numbering permutations. It is also called factorial base, although factorials do not function as base, but as place value of digits. By converting a number less than \"n\"! to factorial representation, one obtains a sequence of \"n\" digits that can be converted to a permutation of \"n\" in a straightforward way, either using them as Lehmer code or as inversion table representation; in the\nformer case the resulting map from integers to permutations of \"n\" lists them in lexicographical order. General mixed radix systems were studied by Georg Cantor.\nThe term \"factorial number system\" is used by Knuth,\nwhile the French equivalent \"numération factorielle\" was first used in 1888. The term \"factoradic\", which is a portmanteau of factorial and mixed radix, appears to be of more recent date.\n\nThe factorial number system is a mixed radix numeral system: the \"i\"-th digit from the right has base \"i\", which means that the digit must be strictly less than \"i\", and that (taking into account the bases of the less significant digits) its value to be multiplied by ! (its place value).\n\nFrom this it follows that the rightmost digit is always 0, the second can be 0 or 1, the third 0, 1 or 2, and so on . The factorial number system is sometimes defined with the 0! place omitted because it is always zero .\n\nIn this article, a factorial number representation will be flagged by a subscript \"!\", so for instance 341010 stands for 341010, whose value is\n\nGeneral properties of mixed radix number systems also apply to the factorial number system. For instance, one can convert a number into factorial representation producing digits from right to left, by repeatedly dividing the number by the radix (1, 2, 3, ...), taking the remainder as digits, and continuing with the integer quotient, until this quotient becomes 0.\n\nFor example, 463 can be transformed into a factorial representation by these successive divisions:\n\nThe process terminates when the quotient reaches zero. Reading the remainders backward gives 341010.\n\nIn principle, this system may be extended to represent fractional numbers, though rather than the natural extension of place values (−1)!, (−2)!, etc., which are undefined, the symmetric choice of radix values n = 0, 1, 2, 3, 4, etc. after the point may be used instead. Again, the 0 and 1 places may be omitted as these are always zero. The corresponding place values are therefore 1/1, 1/1, 1/2, 1/6, 1/24, ..., 1/n!, etc.\n\nThe following sortable table shows the 24 permutations of four elements with different inversion related vectors. The left and right inversion counts formula_1 and formula_2 (the latter often called Lehmer code) are particularly eligible to be interpreted as factorial numbers. formula_1 gives the permutation's position in reverse colexicographic order (the default order of this table), and the latter the position in lexicographic order (both counted from 0).\n\nSorting by a column that has the omissible 0 on the right makes the factorial numbers in that column correspond to the index numbers in the immovable column on the left. The small columns are reflections of the columns next to them, and can be used to bring those in colexicographic order. The rightmost column shows the digit sums of the factorial numbers ( in the tables default order).\n\nFor another example, the greatest number that could be represented with six digits would be 543210 which equals 719 in decimal:\n\nClearly the next factorial number representation after 543210 is 1000000 which designates 6! = 720, the place value for the radix-7 digit. So the former number, and its summed out expression above, is equal to:\n\nThe factorial number system provides a unique representation for each natural number, with the given restriction on the \"digits\" used. No number can be represented in more than one way because the sum of consecutive factorials multiplied by their index is always the next factorial minus one:\n\nThis can be easily proved with mathematical induction, or simply by noticing that formula_5 : subsequent terms cancel each other, leaving the first and last term (see Telescoping series)\n\nHowever, when using Arabic numerals to write the digits (and not including the subscripts as in the above examples), their simple concatenation becomes ambiguous for numbers having a \"digit\" greater than 9. The smallest such example is the number 10 × 10! = 36288000, which may be written A0000000000, but not 100000000000 which denotes 11! = 39916800. Thus using letters A–Z to denote digits 10, 11, 12, ..., 35 as in other base-N make the largest representable number 36 × 36! − 1. For arbitrarily greater numbers one has to choose a base for representing individual digits, say decimal, and provide a separating mark between them (for instance by subscripting each digit by its base, also given in decimal, like 2010, this number also can be written as 2:0:1:0). In fact the factorial number system itself is not truly a numeral system in the sense of providing a representation for all natural numbers using only a finite alphabet of symbols.\n\nThere is a natural mapping between the integers (or equivalently the numbers with \"n\" digits in factorial representation) and permutations of \"n\" elements in lexicographical order, when the integers are expressed in factoradic form. This mapping has been termed the Lehmer code (or inversion table). For example, with , such a mapping is\n\nThe leftmost factoradic digit 0, 1, or 2 is chosen as the first permutation digit from the ordered list (0,1,2) and is removed from the list. Think of this new list as zero indexed and each successive digit dictates which of the remaining elements is to be chosen. If the second factoradic digit is \"0\" then the first element of the list is selected for the second permutation digit and is then removed from the list. Similarly if the second factoradic digit is \"1\", the second is selected and then removed. The final factoradic digit is always \"0\", and since the list now contains only one element it is selected as the last permutation digit.\n\nThe process may become clearer with a longer example. For example, here is how the digits in the factoradic 4041000 (equal to 2982) pick out the digits in (4,0,6,2,1,3,5), the 2982nd permutation of the numbers 0 through 6.\n\nA natural index for the group direct product of two permutation groups is the concatenation of two factoradic numbers, with two subscript \"!\"s.\n\nUnlike single radix systems whose place values are \"base\" for both positive and negative integral n, the factorial number base cannot be extended to negative place values as these would be (−1)!, (−2)! and so on, and these values are undefined. (see factorial)\n\nOne possible extension is therefore to use 1/0!, 1/1!, 1/2!, 1/3!, ..., 1/n! etc. instead, possibly omitting the 1/0! and 1/1! places which are always zero.\n\nWith this method, all rational numbers have a terminating expansion, whose length in 'digits' is less than or equal to the denominator of the rational number represented. This may be proven by considering that there exists a factorial for any integer and therefore the denominator divides into its own factorial even if it does not divide into any smaller factorial.\n\nBy necessity, therefore, the factoradic expansion of the reciprocal of a prime has a length of exactly that prime (less one if the 1/1! place is omitted). Other terms are given as the sequence A046021 on the OEIS. It can also be proven that the last 'digit' or term of the representation of a rational with prime denominator is equal to the difference between the numerator and the prime denominator.\n\nThere is also a non-terminating equivalent for every rational number akin to the fact that in decimal 0.24999... = 0.25 = 1/4 and 0.999... = 1, etc., which can be created by reducing the final term by 1 and then filling in the remaining infinite number of terms with the highest value possible for the radix of that position.\n\nIn the following selection of examples, spaces are used to separate the place values, otherwise represented in decimal. The rational numbers on the left are also in decimal:\n\n\nThere are also a small number of constants that have patterned representations with this method:\n\n\n\n\n"}
{"id": "47226218", "url": "https://en.wikipedia.org/wiki?curid=47226218", "title": "Farthest-first traversal", "text": "Farthest-first traversal\n\nIn computational geometry, the farthest-first traversal of a bounded metric space is a sequence of points in the space, where the first point is selected arbitrarily and each successive point is as far as possible from the set of previously-selected points. The same concept can also be applied to a finite set of geometric points, by restricting the selected points to belong to the set or equivalently by considering the finite metric space generated by these points. For a finite metric space or finite set of geometric points, the resulting sequence forms a permutation of the points, known as the greedy permutation.\n\nFarthest-point traversals have many applications, including the approximation of the traveling salesman problem and the metric \"k\"-center problem. They may be constructed in polynomial time, or (for low-dimensional Euclidean spaces) approximated in near-linear time.\n\nFix a number \"k\", and consider the subset formed by the first \"k\" points of the farthest-first traversal of any metric space. Let \"r\" be the distance between the final point of the prefix and the set of previously-selected points. Then this subset has the following two properties:\nIn other words, each prefix of the farthest-first traversal forms a Delone set.\n\nThe first use of the farthest-first traversal was by in connection with heuristics for the travelling salesman problem. In the farthest-insertion heuristic, discussed by Rosenkrantz et al., a tour is built up incrementally, by adding one point at a time in the ordering given by a farthest-first traversal. To add each point to the traveling salesman tour of the previous points, this heuristic considers all possible ways of breaking one edge of the tour and replacing it by two edges through the new point, and chooses the cheapest of these replacements. Although Rosenkrantz et al. prove only a logarithmic approximation ratio for this method, they show that in practice it often works better than other insertion methods with better provable approximation ratios.\n\nLater, the same sequence of points was popularized by , who used it as part of a greedy approximation algorithm for the problem of finding \"k\" clusters that minimize the maximum diameter of a cluster. The same algorithm applies also, with the same approximation quality, to the metric \"k\"-center problem. This problem is one of several formulations of cluster analysis and facility location, in which the goal is to partition a given set of points into \"k\" different clusters, each with a chosen center point, such that the maximum distance from any point to the center of its cluster is minimized. For instance, this problem can be used to model the placement of fire stations within a city, in order to ensure that every address within the city can be reached quickly by a fire truck. Gonzalez described a clustering heuristic that selects as centers the first \"k\" points of a farthest-first traversal, and then assigns each of the input points to its nearest center. If \"r\" is the distance from the set of \"k\" selected centers to the next point at position \"k\" + 1 in the traversal, then this\nclustering achieves a distance of \"r\". However, the subset of \"k\" centers together with the next point are all at distance at least \"r\" from each other, and any \"k\"-clustering would put two of these points into one cluster, so there is no clustering with distance better than \"r\"/2. Thus, Gonzalez's heuristic gives an approximation ratio of 2 for this problem. This heuristic, and the name \"farthest-first traversal\", are often incorrectly attributed to a different paper from the same time, . However, Hochbaum and Shmoys used graph-theoretic techniques, not the farthest-first traversal, to obtain a different approximation algorithm for the metric \"k\"-center with the same approximation ratio as Gonzalez's heuristic. For both the min-max diameter clustering problem and the metric \"k\"-center problem, these approximations are optimal: the existence of a polynomial-time heuristic with any constant approximation ratio less than 2 would imply that P = NP.\n\nAs well as for clustering, the farthest-first traversal can also be used in another type of facility location problem, the max-min facility dispersion problem, in which the goal is to choose the locations of \"k\" different facilities so that they are as far apart from each other as possible. More precisely, the goal in this problem is to choose \"k\" points from a given metric space or a given set of candidate points, in such a way as to maximize the minimum pairwise distance between the selected points. Again, this can be approximated by choosing the first \"k\" points of a farthest-first traversal. If \"r\" denotes the distance of the \"k\"th point from all previous points, then every point of the metric space or the candidate set is within distance \"r\" of the first \"k\" − 1 points. By the pigeonhole principle, some two points of the optimal solution (whatever it is) must both be within distance \"r\" of the same point among these first \"k\" − 1 chosen points, and (by the triangle inequality) within distance 2\"r\" of each other. Therefore, the heuristic solution given by the farthest-first traversal is within a factor of two of optimal.\n\nOther applications of the farthest-first traversal include color quantization (clustering the colors in an image to a smaller set of representative colors),\nprogressive scanning of images (choosing an order to display the pixels of an image so that prefixes of the ordering produce good lower-resolution versions of the whole image rather than filling in the image from top to bottom),\npoint selection in the probabilistic roadmap method for motion planning,\nsimplification of point clouds,\ngenerating masks for halftone images,\nhierarchical clustering,\nfinding the similarities between polygon meshes of similar surfaces,\nunderwater robot task planning,\nfault detection in sensor networks,\nmodeling phylogenetic diversity,\nmatching vehicles in a heterogenous fleet to customer delivery requests,\nuniform distribution of geodetic observatories on the Earth's surface\nor of other types of sensor network,\ngeneration of virtual point lights in the instant radiosity computer graphics rendering method,\nand geometric range searching data structures.\n\nThe farthest-first traversal of a finite point set may be computed by a greedy algorithm that maintains the distance of each point from the previously selected points, performing the following steps:\nFor a set of \"n\" points, this algorithm takes \"O\"(\"n\") steps and \"O\"(\"n\") distance computations. A faster approximation algorithm, given by , applies to any subset of points in a metric space with bounded doubling dimension, a class of spaces that include the Euclidean spaces of bounded dimension. Their algorithm finds a sequence of points in which each successive point has distance within a 1 − \"ε\" factor of the farthest distance from the previously-selected point, where \"ε\" can be chosen to be any positive number. It takes time \"O\"(\"n\" log \"n\").\n\nFor selecting points from a continuous space such as the Euclidean plane, rather than a finite set of candidate points, these methods will not work directly, because there would be an infinite number of distances to maintain. Instead, each new point should be selected as the center of the largest empty circle defined by the previously-selected point set. This center will always lie on a vertex of the Voronoi diagram of the already selected points, or at a point where an edge of the Voronoi diagram crosses the domain boundary. In this formulation the method for constructing farthest-first traversals has also been called incremental Voronoi insertion. It is similar to Ruppert's algorithm for finite element mesh generation, but differs in the choice of which Voronoi vertex to insert at each step.\n\n"}
{"id": "19884227", "url": "https://en.wikipedia.org/wiki?curid=19884227", "title": "Final value theorem", "text": "Final value theorem\n\nIn mathematical analysis, the final value theorem (FVT) is one of several similar theorems used to relate frequency domain expressions to the time domain behavior as time approaches infinity. A final value theorem allows the time domain behavior to be directly calculated by taking a limit of a frequency domain expression, as opposed to converting to a time domain expression and taking its limit.\n\nMathematically, if formula_1 is bounded on formula_2 and\nhas a finite limit, then\nwhere formula_5 is the (unilateral) Laplace transform of formula_6.\n\nLikewise, in discrete time\nwhere formula_8 is the (unilateral) Z-transform of formula_9.\n\nThe first proof below has the virtue of being totally elementary and self-contained. If one is willing to use not-quite-so-elementary convergence theorems one can give a one-line proof using the Dominated Convergence Theorem.\n\nSuppose for convenience that formula_10 on formula_2, and let formula_12. Let formula_13,\nand choose formula_14 so that formula_15 for all\nformula_16. Since formula_17, for every\nformula_18 we have\n\nhence\n\nNow for every formula_18 we have \nOn the other hand, since formula_23 is fixed it is clear that formula_24, and\nso formula_25 if formula_18 is small enough.\n\nLet formula_12. A change of variable in the integral defining formula_5 shows that\n\nSince formula_1 is bounded, there exists formula_31 such that formula_32 is dominated by the integrable function formula_33\n\nThus, by the Dominated Convergence Theorem, \n\nFor example, for a system described by transfer function\nand so the impulse response converges to\nThat is, the system returns to zero after being disturbed by a short impulse. However, the Laplace transform of the unit step response is\nand so the step response converges to\nand so a zero-state system will follow an exponential rise to a final value of 3.\n\nFor a system described by the transfer function\n\nthe final value theorem \"appears\" to predict the final value of the impulse response to be 0 and the final value of the step response to be 1. However, neither time-domain limit exists, and so the final value theorem predictions are not valid. In fact, both the impulse response and step response oscillate, and (in this special case) the final value theorem describes the average values around which the responses oscillate.\n\nThere are two checks performed in Control theory which confirm valid results for the Final Value Theorem:\n\nRule 1 was not satisfied in this example, in that the roots of the denominator are formula_42 and formula_43.\n\n\n"}
{"id": "2851410", "url": "https://en.wikipedia.org/wiki?curid=2851410", "title": "Five-dimensional space", "text": "Five-dimensional space\n\nA five-dimensional space is a space with five dimensions. If interpreted physically, that is one more than the usual three spatial dimensions and the fourth dimension of time used in relativitistic physics. It is an abstraction which occurs frequently in mathematics, where it is a legitimate construct. In physics and mathematics, a sequence of \"N\" numbers can be understood to represent a location in an \"N\"-dimensional space. Whether or not the universe is five-dimensional is a topic of debate.\n\nMuch of the early work on five dimensional space was in an attempt to develop a theory that unifies the four fundamental interactions in nature: strong and weak nuclear forces, gravity and electromagnetism. German mathematician Theodor Kaluza and Swedish physicist Oskar Klein independently developed the Kaluza–Klein theory in 1921, which used the fifth dimension to unify gravity with electromagnetic force. Although their approaches were later found to be at least partially inaccurate, the concept provided a basis for further research over the past century.\n\nTo explain why this dimension would not be directly observable, Klein suggested that the fifth dimension would be rolled up into a tiny, compact loop on the order of 10 centimeters. Under his reasoning, he envisioned light as a disturbance caused by rippling in the higher dimension just beyond human perception, similar to how fish in a pond can only see shadows of ripples across the surface of the water caused by raindrops. While not detectable, it would indirectly imply a connection between seemingly unrelated forces. Kaluza-Klein theory experienced a revival in the 1970s due to the emergence of superstring theory and supergravity: the concept that reality is composed of vibrating strands of energy, a postulate only mathematically viable in ten dimensions or more. Superstring theory then evolved into a more generalized approach known as M-theory. M-theory suggested a potentially observable extra dimension in addition to the ten essential dimensions which would allow for the existence of superstrings. The other 10 dimensions are compacted, or \"rolled up\", to a size below the subatomic level. Kaluza–Klein theory today is seen as essentially a gauge theory, with the gauge being the circle group.\n\nThe fifth dimension is difficult to directly observe, though the Large Hadron Collider provides an opportunity to record indirect evidence of its existence. Physicists theorize that collisions of subatomic particles in turn produce new particles as a result of the collision, including a graviton that escapes from the fourth dimension, or brane, leaking off into a five-dimensional bulk. M-theory would explain the weakness of gravity relative to the other fundamental forces of nature, as can be seen, for example, when using a magnet to lift a pin off a table — the magnet is able to overcome the gravitational pull of the entire earth with ease.\n\nMathematical approaches were developed in the early 20th century that viewed the fifth dimension as a theoretical construct. These theories make reference to Hilbert space, a concept that postulates an infinite number of mathematical dimensions to allow for a limitless number of quantum states. Einstein, Bergmann and Bargmann later tried to extend the four-dimensional spacetime of general relativity into an extra physical dimension to incorporate electromagnetism, though they were unsuccessful. In their 1938 paper, Einstein and Bergmann were among the first to introduce the modern viewpoint that a four-dimensional theory, which coincides with Einstein-Maxwell theory at long distances, is derived from a five-dimensional theory with complete symmetry in all five dimensions. They suggested that electromagnetism resulted from a gravitational field that is “polarized” in the fifth dimension.\n\nThe main novelty of Einstein and Bergmann was to seriously consider the fifth dimension as a physical entity, rather than an excuse to combine the metric tensor and electromagnetic potential. But they then reneged, modifying the theory to break its five-dimensional symmetry. Their reasoning, as suggested by Edward Witten, was that the more symmetric version of the theory predicted the existence of a new long range field, one that was both massless and scalar, which would have required a fundamental modification to Einstein's theory of general relativity. Minkowski space and Maxwell's equations in vacuum can be embedded in a five-dimensional Riemann curvature tensor.\nIn 1993, the physicist Gerard 't Hooft put forward the holographic principle, which explains that the \"information about an extra dimension is visible as a curvature in a spacetime with one fewer dimension\". For example, holograms are three-dimensional pictures placed on a two-dimensional surface, which gives the image a curvature when the observer moves. Similarly, in general relativity, the fourth dimension is manifested in observable three dimensions as the curvature path of a moving infinitesimal (test) particle. 'T Hooft has speculated that the fifth dimension is really the \"spacetime fabric\".\n\nAccording to Klein’s definition, \"a geometry is the study of the invariant properties of a spacetime, under transformations within itself.\" Therefore, the geometry of the 5th dimension studies the invariant properties of such space-time, as we move within it, expressed in formal equations.\n\nIn five or more dimensions, only three regular polytopes exist. In five dimensions, they are:\n\n\nAn important uniform 5-polytope is the 5-demicube, h{4,3,3,3} has half the vertices of the 5-cube (16), bounded by alternating 5-cell and 16-cell hypercells. The expanded or stericated 5-simplex is the vertex figure of the A lattice, . It and has a doubled symmetry from its symmetric Coxeter diagram. The kissing number of the lattice, 30, is represented in its vertices. The rectified 5-orthoplex is the vertex figure of the D lattice, . Its 40 vertices represent the kissing number of the lattice and the highest for dimension 5.\n\nA hypersphere in 5-space (also called a 4-sphere due to its surface being 4-dimensional) consists of the set of all points in 5-space at a fixed distance \"r\" from a central point P. The hypervolume enclosed by this hypersurface is:\n\n\n\n"}
{"id": "5260042", "url": "https://en.wikipedia.org/wiki?curid=5260042", "title": "Flux qubit", "text": "Flux qubit\n\nIn quantum computing, and more specifically in superconducting quantum computing, flux qubits (also known as persistent current qubits) are micrometer sized loops of superconducting metal interrupted by a number of Josephson junctions, functioning as quantum bits. The junction parameters are engineered during fabrication so that a persistent current will flow continuously when an external magnetic flux is applied. As only an integer number of flux quanta are allowed to penetrate the superconducting ring, clockwise or counter-clockwise currents are developed in the loop to compensate (screen or enhance) a non-integer external flux bias. When the applied flux through the loop area is close to a half integer number of flux quanta, the two lowest energy eigenstates of the loop will be a quantum superposition of the clockwise and counter-clockwise currents. The two lowest energy eigenstates differ only by the relative quantum phase between the composing current-direction states. Higher energy eigenstates correspond to much larger persistent currents, that induce an additional flux quantum to the qubit loop, thus are well separated energetically from the lowest two eigenstates. This separation, known as the \"qubit non linearity\" criteria, allows operations with the two lowest eigenstates only, effectively creating a two level system. Usually, the two lowest eigenstates will serve as the computational basis for the logical qubit.\n\nComputational operations are performed by pulsing the qubit with microwave frequency radiation which has an energy comparable to that of the gap between the energy of the two basis states. Properly selected pulse duration can put the qubit into a quantum superposition of the two basis states while subsequent pulses can manipulate the probability weighting that the qubit will be measured in either of the two basis states, thus performing a computational operation.\n\nFlux qubits are fabricated using techniques similar to those used for microelectronics. The devices are usually made on silicon or sapphire wafers using electron beam lithography and metallic thin film evaporation processes. To create Josephson junctions, a technique known as shadow evaporation is normally used; this involves evaporating the source metal alternately at two angles through the lithography defined mask in the electron beam resist. This results in two overlapping layers of the superconducting metal, in between which a thin layer of insulator (normally aluminum oxide) is deposited.\n\nThe flux qubit is distinguished from other types of superconducting qubit such as the charge qubit or phase qubit by the coupling energy and charging energy of its junctions. In the charge qubit regime the charging energy of the junctions dominates the coupling energy, while in a flux qubit the situation is reversed and the coupling energy dominates. Typically in a flux qubit the coupling energy is 10-100 times greater than the charging energy. It is this ratio that allows the Cooper pairs to flow continuously around the loop, rather than tunnel discretely across the junctions as in a charge qubit.\n\nCoupling between two or more qubits is essential to implement many-qubit gates. The two basic coupling mechanisms are the direct inductive coupling and coupling via a microwave resonator. In the direct coupling, the circulating currents of the qubits inductively affect one another - clockwise current in one qubit induces counter-clockwise current in the other. In the Pauli Matrices formalism, a term appears in the Hamiltonian, essential for the controlled NOT gate implementation. The direct coupling might be further enhanced by kinetic inductance, if the qubit loops are made to share an edge, so that the currents will flow through the same superconducting line. Inserting a Josephson junction on that joint line will add a Josephson inductance term, and increase the coupling even more. To implement a switchable coupling in the direct coupling mechanism, as required to implement a gate of finite duration, an intermediate coupling loop may be used. The control magnetic flux applied to the coupler loop switches the coupling on and off, as implemented, for example, in the D-Wave Systems machines. The second method of coupling utilizes an intermediate microwave cavity resonator, commonly implemented in a coplanar waveguide geometry. By tuning the energy separation of the qubits to match the one of the resonator, the phases of the loop currents are synchronized, and a coupling is implemented. Tuning the qubits in and out of resonance (for example, by modifying their bias magnetic flux) controls the duration of the gate operation.\n\nLike all quantum bits, flux qubits require a suitably sensitive probe coupled to it in order to measure its state after a computation has been carried out. Such quantum probes should introduce as little back-action as possible onto the qubit during measurement. Ideally they should be decoupled during computation and then turned \"on\" for a short time during read-out. Read-out probes for flux qubits work by interacting with one of the qubit's macroscopic variables, such as the circulating current, the flux within the loop or the macroscopic phase of the superconductor. This interaction then changes some variable of the read-out probe which can be measured using conventional low-noise electronics. The read-out probe is typically the technology aspect that separates the research of different University groups working on flux qubits.\n\nProf. Mooij's group at Delft in the Netherlands, along with collaborators, has pioneered flux qubit technology, and were the first to conceive, propose and implement flux qubits as they are known today. The Delft read-out scheme is based on a SQUID loop that is inductively coupled to the qubit, the qubit's state influences the critical current of the SQUID. The critical current can then be read-out using ramped measurement currents through the SQUID. Recently the group has used the plasma frequency of the SQUID as the read-out variable.\n\nDr. Il'ichev's group at IPHT Jena in Germany are using impedance measurement techniques based on the flux qubit influencing the resonant properties of a high quality tank circuit, which, like the Delft group is also inductively coupled to the qubit. In this scheme the qubit's magnetic susceptibility, which is defined by its state, changes the phase angle between the current and voltage when a small A.C. signal is passed into the tank circuit.\n\nRecently Prof. Petrashov's group at Royal Holloway are using an Andreev interferometer probe to read out flux qubits. This read-out uses the phase influence of a superconductor on the conductance properties of a normal metal. A length of normal metal is connected at either end to either side of the qubit using superconducting leads, the phase across the qubit, which is defined by its state, is translated into the normal metal, the resistance of which is then read-out using low noise resistance measurements.\n"}
{"id": "285907", "url": "https://en.wikipedia.org/wiki?curid=285907", "title": "Fractal dimension", "text": "Fractal dimension\n\nIn mathematics, more specifically in fractal geometry, a fractal dimension is a ratio providing a statistical index of complexity comparing how detail in a pattern (strictly speaking, a fractal pattern) changes with the scale at which it is measured. It has also been characterized as a measure of the space-filling capacity of a pattern that tells how a fractal scales differently from the space it is embedded in; a fractal dimension does not have to be an integer.\n\nThe essential idea of \"fractured\" dimensions has a long history in mathematics, but the term itself was brought to the fore by Benoit Mandelbrot based on his 1967 paper on self-similarity in which he discussed \"fractional dimensions\". In that paper, Mandelbrot cited previous work by Lewis Fry Richardson describing the counter-intuitive notion that a coastline's measured length changes with the length of the measuring stick used (see Fig. 1). In terms of that notion, the fractal dimension of a coastline quantifies how the number of scaled measuring sticks required to measure the coastline changes with the scale applied to the stick. There are several formal mathematical definitions of fractal dimension that build on this basic concept of change in detail with change in scale.\n\nUltimately, the term \"fractal dimension\" became the phrase that Mandelbrot himself became most comfortable with respect to encapsulating the meaning of the word \"fractal\", a term he created. After several iterations over years, Mandelbrot settled on this use of the language: \"...to use \"fractal\" without a pedantic definition, to use \"fractal dimension\" as a generic term applicable to \"all\" the variants.\"\n\nOne non-trivial example is the fractal dimension of a Koch snowflake. It has a topological dimension of 1, but it is by no means a rectifiable curve: the length of the curve between any two points on the Koch snowflake is infinite. No small piece of it is line-like, but rather it is composed of an infinite number of segments joined at different angles. The fractal dimension of a curve can be explained intuitively thinking of a fractal line as an object too detailed to be one-dimensional, but too simple to be two-dimensional. Therefore its dimension might best be described not by its usual topological dimension of 1 but by its fractal dimension, which is often a number between one and two; in the case of the Koch snowflake, it is about 1.262.\n\nA fractal dimension is an index for characterizing fractal patterns or sets by quantifying their complexity as a ratio of the change in detail to the change in scale. Several types of fractal dimension can be measured theoretically and empirically (see Fig. 2). Fractal dimensions are used to characterize a broad spectrum of objects ranging from the abstract to practical phenomena, including turbulence, river networks, urban growth, human physiology,<ref name=\"doi10.1364/boe.1.000268\"></ref><ref name=\"doi10.1007/s11682-008-9057-9\"></ref> medicine, and market trends. The essential idea of \"fractional\" or \"fractal\" dimensions has a long history in mathematics that can be traced back to the 1600s, but the terms \"fractal\" and \"fractal dimension\" were coined by mathematician Benoit Mandelbrot in 1975.\n\n\"Fractal dimensions\" were first applied as an index characterizing complicated geometric forms for which the details seemed more important than the gross picture. For sets describing ordinary geometric shapes, the theoretical fractal dimension equals the set's familiar Euclidean or topological dimension. Thus, it is 0 for sets describing points (0-dimensional sets); 1 for sets describing lines (1-dimensional sets having length only); 2 for sets describing surfaces (2-dimensional sets having length and width); and 3 for sets describing volumes (3-dimensional sets having length, width, and height). But this changes for fractal sets. If the theoretical fractal dimension of a set exceeds its topological dimension, the set is considered to have fractal geometry.\n\nUnlike topological dimensions, the fractal index can take non-integer values, indicating that a set fills its space qualitatively and quantitatively differently from how an ordinary geometrical set does. For instance, a curve with a fractal dimension very near to 1, say 1.10, behaves quite like an ordinary line, but a curve with fractal dimension 1.9 winds convolutedly through space very nearly like a surface. Similarly, a surface with fractal dimension of 2.1 fills space very much like an ordinary surface, but one with a fractal dimension of 2.9 folds and flows to fill space rather nearly like a volume. This general relationship can be seen in the two images of fractal curves in Fig.2 and Fig. 3 – the 32-segment contour in Fig. 2, convoluted and space filling, has a fractal dimension of 1.67, compared to the perceptibly less complex Koch curve in Fig. 3, which has a fractal dimension of 1.26.\n\nThe relationship of an increasing fractal dimension with space-filling might be taken to mean fractal dimensions measure density, but that is not so; the two are not strictly correlated. Instead, a fractal dimension measures complexity, a concept related to certain key features of fractals: self-similarity and detail or irregularity. These features are evident in the two examples of fractal curves. Both are curves with topological dimension of 1, so one might hope to be able to measure their length or slope, as with ordinary lines. But we cannot do either of these things, because fractal curves have complexity in the form of self-similarity and detail that ordinary lines lack. The \"self-similarity\" lies in the infinite scaling, and the \"detail\" in the defining elements of each set. The length between any two points on these curves is undefined because the curves are theoretical constructs that never stop repeating themselves. Every smaller piece is composed of an infinite number of scaled segments that look exactly like the first iteration. These are not rectifiable curves, meaning they cannot be measured by being broken down into many segments approximating their respective lengths. They cannot be characterized by finding their lengths or slopes. However, their fractal dimensions can be determined, which shows that both fill space more than ordinary lines but less than surfaces, and allows them to be compared in this regard.\n\nNote that the two fractal curves described above show a type of self-similarity that is exact with a repeating unit of detail that is readily visualized. This sort of structure can be extended to other spaces (e.g., a fractal that extends the Koch curve into 3-d space has a theoretical D=2.5849). However, such neatly countable complexity is only one example of the self-similarity and detail that are present in fractals. The example of the coast line of Britain, for instance, exhibits self-similarity of an approximate pattern with approximate scaling. Overall, fractals show several types and degrees of self-similarity and detail that may not be easily visualized. These include, as examples, strange attractors for which the detail has been described as in essence, smooth portions piling up, the Julia set, which can be seen to be complex swirls upon swirls, and heart rates, which are patterns of rough spikes repeated and scaled in time. Fractal complexity may not always be resolvable into easily grasped units of detail and scale without complex analytic methods but it is still quantifiable through fractal dimensions.\n\nThe terms \"fractal dimension\" and \"fractal\" were coined by Mandelbrot in 1975, about a decade after he published his paper on self-similarity in the coastline of Britain. Various historical authorities credit him with also synthesizing centuries of complicated theoretical mathematics and engineering work and applying them in a new way to study complex geometries that defied description in usual linear terms. The earliest roots of what Mandelbrot synthesized as the fractal dimension have been traced clearly back to writings about undifferentiable, infinitely self-similar functions, which are important in the mathematical definition of fractals, around the time that calculus was discovered in the mid-1600s. There was a lull in the published work on such functions for a time after that, then a renewal starting in the late 1800s with the publishing of mathematical functions and sets that are today called canonical fractals (such as the eponymous works of von Koch, Sierpiński, and Julia), but at the time of their formulation were often considered antithetical mathematical \"monsters\". These works were accompanied by perhaps the most pivotal point in the development of the concept of a fractal dimension through the work of Hausdorff in the early 1900s who defined a \"fractional\" dimension that has come to be named after him and is frequently invoked in defining modern fractals.\n\n\"See Fractal history for more information\"\n\nThe concept of a fractal dimension rests in unconventional views of scaling and dimension. As Fig. 4 illustrates, traditional notions of geometry dictate that shapes scale predictably according to intuitive and familiar ideas about the space they are contained within, such that, for instance, measuring a line using first one measuring stick then another 1/3 its size, will give for the second stick a total length 3 times as many sticks long as with the first. This holds in 2 dimensions, as well. If one measures the area of a square then measures again with a box of side length 1/3 the size of the original, one will find 9 times as many squares as with the first measure. Such familiar scaling relationships can be defined mathematically by the general scaling rule in Equation 1, where the variable formula_1 stands for the number of sticks, formula_2 for the scaling factor, and formula_3 for the fractal dimension:\n\nThe symbol formula_4 above denotes proportionality. This scaling rule typifies conventional rules about geometry and dimension – for lines, it quantifies that, because formula_1=3 when formula_2=1/3 as in the example above, formula_3=1, and for squares, because formula_1=9 when formula_2=1/3, formula_3=2.\n\nThe same rule applies to fractal geometry but less intuitively. To elaborate, a fractal line measured at first to be one length, when remeasured using a new stick scaled by 1/3 of the old may not be the expected 3 but instead 4 times as many scaled sticks long. In this case, formula_1=4 when formula_2=1/3, and the value of formula_3 can be found by rearranging Equation 1:\n\nThat is, for a fractal described by formula_1=4 when formula_2=1/3, formula_3=1.2619, a non-integer dimension that suggests the fractal has a dimension not equal to the space it resides in. The scaling used in this example is the same scaling of the Koch curve and snowflake. Of note, the images shown are not true fractals because the scaling described by the value of formula_3 cannot continue infinitely for the simple reason that the images only exist to the point of their smallest component, a pixel. The theoretical pattern that the digital images represent, however, has no discrete pixel-like pieces, but rather is composed of an infinite number of infinitely scaled segments joined at different angles and does indeed have a fractal dimension of 1.2619.\n\nAs is the case with dimensions determined for lines, squares, and cubes, fractal dimensions are general descriptors that do not uniquely define patterns. The value of \"D\" for the Koch fractal discussed above, for instance, quantifies the pattern's inherent scaling, but does not uniquely describe nor provide enough information to reconstruct it. Many fractal structures or patterns could be constructed that have the same scaling relationship but are dramatically different from the Koch curve, as is illustrated in Figure 6.\n\n\"For examples of how fractal patterns can be constructed, see Fractal, Sierpinski triangle, Mandelbrot set, Diffusion limited aggregation, L-System.\"\n\nThe concept of fractal dimension described in this article is a basic view of a complicated construct. The examples discussed here were chosen for clarity, and the scaling unit and ratios were known ahead of time. In practice, however, fractal dimensions can be determined using techniques that approximate scaling and detail from limits estimated from regression lines over log vs log plots of size vs scale. Several formal mathematical definitions of different types of fractal dimension are listed below. Although for some classic fractals all these dimensions coincide, in general they are not equivalent:\nformula_24\n\nMany real-world phenomena exhibit limited or statistical fractal properties and fractal dimensions that have been estimated from sampled data using computer based fractal analysis techniques. \nPractically, measurements of fractal dimension are affected by various methodological issues, and are sensitive to numerical or experimental noise and limitations in the amount of data. Nonetheless, the field is rapidly growing as estimated fractal dimensions for statistically self-similar phenomena may have many practical applications in various fields including\ndiagnostic imaging,\nphysiology,\nneuroscience,\nmedicine,\nphysics,\nimage analysis,\necology,\nacoustics,\nRiemann zeta zeros,\nand electrochemical processes.\n\nAn alternative to a direct measurement, is considering a mathematical model that resembles formation of a real-world fractal object. In this case, a validation can also be done by comparing other than fractal properties implied by the model, with measured data. In colloidal physics, systems composed of particles with various fractal dimensions arise. To describe these systems, it is convenient to speak about a distribution of fractal dimensions, and eventually, a time evolution of the latter: a process that is driven by a complex interplay between aggregation and coalescence.\n\n\n\n"}
{"id": "473514", "url": "https://en.wikipedia.org/wiki?curid=473514", "title": "Generalized coordinates", "text": "Generalized coordinates\n\nIn analytical mechanics, specifically the study of the rigid body dynamics of multibody systems, the term generalized coordinates refers to the parameters that describe the configuration of the system relative to some reference configuration. These parameters must uniquely define the configuration of the system relative to the reference configuration. This is done assuming that this can be done with a single chart. The generalized velocities are the time derivatives of the generalized coordinates of the system.\n\nAn example of a generalized coordinate is the angle that locates a point moving on a circle. The adjective \"generalized\" distinguishes these parameters from the traditional use of the term coordinate to refer to Cartesian coordinates: for example, describing the location of the point on the circle using x and y coordinates.\n\nAlthough there may be many choices for generalized coordinates for a physical system, parameters which are convenient are usually selected for the specification of the configuration of the system and which make the solution of its equations of motion easier. If these parameters are independent of one another, the number of independent generalized coordinates is defined by the number of degrees of freedom of the system.\n\nGeneralized coordinates are usually selected to provide the minimum number of independent coordinates that define the configuration of a system, which simplifies the formulation of Lagrange's equations of motion. However, it can also occur that a useful set of generalized coordinates may be \"dependent\", which means that they are related by one or more constraint equations.\n\nFor a system of \"N\" particles in 3D real coordinate space, the position vector of each particle can be written as a 3-tuple in Cartesian coordinates:\n\nAny of the position vectors can be denoted r where \"k\" = 1, 2, ..., \"N\" labels the particles. A \"holonomic constraint\" is a \"constraint equation\" of the form for particle \"k\"\n\nwhich connects all the 3 spatial coordinates of that particle together, so they are not independent. The constraint may change with time, so time \"t\" will appear explicitly in the constraint equations. At any instant of time, any one coordinate will be determined from the other coordinates, e.g. if \"x\" and \"z\" are given, then so is \"y\". One constraint equation counts as \"one\" constraint. If there are \"C\" constraints, each has an equation, so there will be \"C\" constraint equations. There is not necessarily one constraint equation for each particle, and if there are no constraints on the system then there are no constraint equations.\n\nSo far, the configuration of the system is defined by 3\"N\" quantities, but \"C\" coordinates can be eliminated, one coordinate from each constraint equation. The number of independent coordinates is \"n\" = 3\"N\" − \"C\". (In \"D\" dimensions, the original configuration would need \"ND\" coordinates, and the reduction by constraints means \"n\" = \"ND\" − \"C\"). It is ideal to use the minimum number of coordinates needed to define the configuration of the entire system, while taking advantage of the constraints on the system. These quantities are known as generalized coordinates in this context, denoted \"q\"(\"t\"). It is convenient to collect them into an \"n\"-tuple\n\nwhich is a point in the \"configuration space\" of the system. They are all independent of one other, and each is a function of time. Geometrically they can be lengths along straight lines, or arc lengths along curves, or angles; not necessarily Cartesian coordinates or other standard orthogonal coordinates. There is one for each degree of freedom, so the number of generalized coordinates equals the number of degrees of freedom, \"n\". A degree of freedom corresponds to one quantity that changes the configuration of the system, for example the angle of a pendulum, or the arc length traversed by a bead along a wire.\n\nIf it is possible to find from the constraints as many independent variables as there are degrees of freedom, these can be used as generalized coordinates The position vector r of particle \"k\" is a function of all the \"n\" generalized coordinates (and, through them, of time),\n\nand the generalized coordinates can be thought of as parameters associated with the constraint.\n\nThe corresponding time derivatives of q are the generalized velocities,\n\n(each dot over a quantity indicates one time derivative). The velocity vector v is the total derivative of r with respect to time\n\nand so generally depends on the generalized velocities and coordinates. Since we are free to specify the initial values of the generalized coordinates and velocities separately, the generalized coordinates \"q\" and velocities \"dq\"/\"dt\" can be treated as \"independent variables\".\n\nA mechanical system can involve constraints on both the generalized coordinates and their derivatives. Constraints of this type are known as non-holonomic. First-order non-holonomic constraints have the form\n\nAn example of such a constraint is a rolling wheel or knife-edge that constrains the direction of the velocity vector. Non-holonomic constraints can also involve next-order derivatives such as generalized accelerations.\n\nThe total kinetic energy of the system is the energy of the system's motion, defined as\n\nin which · is the dot product. The kinetic energy is a function only of the velocities v, not the coordinates r themselves. By contrast an important observation is\n\nwhich illustrates the kinetic energy is in general a function of the generalized velocities, coordinates, and time if the constraints also vary with time, so \"T\" = \"T\"(q, \"d\"q/\"dt\", \"t\").\n\nIn the case the constraints on the particles are time-independent, then all partial derivatives with respect to time are zero, and the kinetic energy is a homogeneous function of degree 2 in the generalized velocities.\n\nStill for the time-independent case, this expression is equivalent to taking the line element squared of the trajectory for particle \"k\",\n\nand dividing by the square differential in time, \"dt\", to obtain the velocity squared of particle \"k\". Thus for time-independent constraints it is sufficient to know the line element to quickly obtain the kinetic energy of particles and hence the Lagrangian.\n\nIt is instructive to see the various cases of polar coordinates in 2d and 3d, owing to their frequent appearance. In 2d polar coordinates (\"r\", \"θ\"),\n\nin 3d cylindrical coordinates (\"r\", \"θ\", \"z\"),\n\nin 3d spherical coordinates (\"r\", \"θ\", \"φ\"),\n\nThe \"generalized momentum\" \"canonically conjugate to\" the coordinate \"q\" is defined by\n\nIf the Lagrangian \"L\" does \"not\" depend on some coordinate \"q\", then it follows from the Euler–Lagrange equations that the corresponding generalized momentum will be a conserved quantity, because the time derivative is zero implying the momentum is a constant of the motion;\n\nFor a bead sliding on a frictionless wire subject only to gravity in 2d space, the constraint on the bead can be stated in the form \"f\"(r) = 0, where the position of the bead can be written r = (\"x\"(\"s\"), \"y\"(\"s\")), in which \"s\" is a parameter, the arc length \"s\" along the curve from some point on the wire. This is a suitable choice of generalized coordinate for the system. Only \"one\" coordinate is needed instead of two, because the position of the bead can be parameterized by one number, \"s\", and the constraint equation connects the two coordinates \"x\" and \"y\"; either one is determined from the other. The constraint force is the reaction force the wire exerts on the bead to keep it on the wire, and the non-constraint applied force is gravity acting on the bead.\n\nSuppose the wire changes its shape with time, by flexing. Then the constraint equation and position of the particle are respectively\n\nwhich now both depend on time \"t\" due to the changing coordinates as the wire changes its shape. Notice time appears implicitly via the coordinates \"and\" explicitly in the constraint equations.\n\nThe relationship between the use of generalized coordinates and Cartesian coordinates to characterize the movement of a mechanical system can be illustrated by considering the constrained dynamics of a simple pendulum.\n\nA simple pendulum consists of a mass M hanging from a pivot point so that it is constrained to move on a circle of radius L. The position of the mass is defined by the coordinate vector r=(x, y) measured in the plane of the circle such that y is in the vertical direction. The coordinates x and y are related by the equation of the circle\nthat constrains the movement of M. This equation also provides a constraint on the velocity components,\n\nNow introduce the parameter θ, that defines the angular position of M from the vertical direction. It can be used to define the coordinates x and y, such that\nThe use of θ to define the configuration of this system avoids the constraint provided by the equation of the circle.\n\nNotice that the force of gravity acting on the mass m is formulated in the usual Cartesian coordinates,\nwhere g is the acceleration of gravity.\n\nThe virtual work of gravity on the mass m as it follows the trajectory r is given by\nThe variation δr can be computed in terms of the coordinates x and y, or in terms of the parameter θ,\nThus, the virtual work is given by\n\nNotice that the coefficient of δy is the y-component of the applied force. In the same way, the coefficient of δθ is known as the generalized force along generalized coordinate θ, given by\n\nTo complete the analysis consider the kinetic energy T of the mass, using the velocity,\nso,\n\nD'Alembert's form of the principle of virtual work for the pendulum in terms of the coordinates x and y are given by,\nThis yields the three equations\nin the three unknowns, x, y and λ.\n\nUsing the parameter θ, those equations take the form\nwhich becomes,\nor\nThis formulation yields one equation because there is a single parameter and no constraint equation.\n\nThis shows that the parameter θ is a generalized coordinate that can be used in the same way as the Cartesian coordinates x and y to analyze the pendulum.\n\nThe benefits of generalized coordinates become apparent with the analysis of a double pendulum. \nFor the two masses m, i=1, 2, let r=(x, y), i=1, 2 define their two trajectories. These vectors satisfy the two constraint equations,\nThe formulation of Lagrange's equations for this system yields six equations in the four Cartesian coordinates x, y i=1, 2 and the two Lagrange multipliers λ, i=1, 2 that arise from the two constraint equations.\n\nNow introduce the generalized coordinates θ i=1,2 that define the angular position of each mass of the double pendulum from the vertical direction. In this case, we have\n\nThe force of gravity acting on the masses is given by,\nwhere g is the acceleration of gravity. Therefore, the virtual work of gravity on the two masses as they follow the trajectories r, i=1,2 is given by\n\nThe variations δr i=1, 2 can be computed to be\n\nThus, the virtual work is given by\nand the generalized forces are\n\nCompute the kinetic energy of this system to be\n\nEuler–Lagrange equation yield two equations in the unknown generalized coordinates θ i=1, 2, given by\nand\n\nThe use of the generalized coordinates θ i=1, 2 provides an alternative to the Cartesian formulation of the dynamics of the double pendulum.\n\nFor a 3d example, a spherical pendulum with constant length \"l\" free to swing in any angular direction subject to gravity, the constraint on the pendulum bob can be stated in the form\n\nwhere the position of the pendulum bob can be written\n\nin which (\"θ\", \"φ\") are the spherical polar angles because the bob moves in the surface of a sphere. The position r is measured along the suspension point to the bob, here treated as a point particle. A logical choice of generalized coordinates to describe the motion are the angles (\"θ\", \"φ\"). Only two coordinates are needed instead of three, because the position of the bob can be parameterized by two numbers, and the constraint equation connects the three coordinates \"x\", \"y\", \"z\" so any one of them is determined from the other two.\n\nThe \"principle of virtual work\" states that if a system is in static equilibrium, the virtual work of the applied forces is zero for all virtual movements of the system from this state, that is, δW=0 for any variation δr. When formulated in terms of generalized coordinates, this is equivalent to the requirement that the generalized forces for any virtual displacement are zero, that is \"F\"=0.\n\nLet the forces on the system be F, \"j=1, ..., m\" be applied to points with Cartesian coordinates r, j=1..., m, then the virtual work generated by a virtual displacement from the equilibrium position is given by\nwhere δr, \"j=1, ..., m\" denote the virtual displacements of each point in the body.\n\nNow assume that each δr depends on the generalized coordinates \"q\", \"i=1, ..., n\", then\nand\n\nThe \"n\" terms\nare the generalized forces acting on the system. Kane shows that these generalized forces can also be formulated in terms of the ratio of time derivatives,\nwhere v is the velocity of the point of application of the force F.\n\nIn order for the virtual work to be zero for an arbitrary virtual displacement, each of the generalized forces must be zero, that is\n\n\n"}
{"id": "4566542", "url": "https://en.wikipedia.org/wiki?curid=4566542", "title": "Hamming space", "text": "Hamming space\n\nIn statistics and coding theory, a Hamming space is usually the set of all formula_1 binary strings of length \"N\". It is used in the theory of coding signals and transmission.\n\nMore generally, a Hamming space can be defined over any alphabet (set) \"Q\" as the set of words of a fixed length \"N\" with letters from \"Q\". If \"Q\" is a finite field, then a Hamming space over \"Q\" is an \"N\"-dimensional vector space over \"Q\". In the typical, binary case, the field is thus GF(2) (also denoted by Z).\n\nIn coding theory, if \"Q\" has \"q\" elements, then any subset \"C\" (usually assumed of cardinality at least two) of the \"N\"-dimensional Hamming space over \"Q\" is called a q-ary code of length N; the elements of \"C\" are called codewords. In the case where \"C\" is a linear subspace of its Hamming space, it is called a linear code. A typical example of linear code is the Hamming code. Codes defined via a Hamming space necessarily have the same length for every codeword, so they are called block codes when it is necessary to distinguish them from variable-length codes that are defined by unique factorization on a monoid.\n\nThe Hamming distance endows a Hamming space with a metric, which is essential in defining basic notions of coding theory such as error detecting and error correcting codes.\n\nHamming spaces over non-field alphabets have also been considered, especially over finite rings (most notably over Z) giving rise to modules instead of vector spaces and ring-linear codes (identified with submodules) instead of linear codes. The typical metric used in this case the Lee distance. There exist a Gray isometry between formula_2 (i.e. GF(2)) with the Hamming distance and formula_3 (also denoted as GR(4,m)) with the Lee distance.\n"}
{"id": "1695214", "url": "https://en.wikipedia.org/wiki?curid=1695214", "title": "Hilbert's ninth problem", "text": "Hilbert's ninth problem\n\nHilbert's ninth problem, from the list of 23 Hilbert's problems (1900), asked to find the most general reciprocity law for the norm residues of \"k\"-th order in a general algebraic number field, where \"k\" is a power of a prime. \n\nThe problem was partially solved by Emil Artin (1924; 1927; 1930) by establishing the Artin reciprocity law which deals with abelian extensions of algebraic number fields. Together with the work of Teiji Takagi and Helmut Hasse (who established the more general Hasse reciprocity law), this led to the development of the class field theory, realizing Hilbert's program in an abstract fashion. Certain explicit formulas for norm residues were later found by Igor Shafarevich (1948; 1949; 1950).\nThe non-abelian generalization, also connected with Hilbert's twelfth problem, is one of the long-standing challenges in number theory and is far from being complete.\n\n\n"}
{"id": "54978454", "url": "https://en.wikipedia.org/wiki?curid=54978454", "title": "Ignaz Schütz", "text": "Ignaz Schütz\n\nIgnaz Robert Schütz (1867, Březová (Moravia) – 1927, Brno) was a Czech–German mathematician and a physicist.\n\nHe studied at the University of Munich where in 1894 he obtained a Ph.D in physics. Schütz was assistant to Ludwig Boltzmann in Munich from 1891 to 1894, the year of Boltzmann's departure from Munich. In 1897, Ignaz R. Schütz, then a member of the Institute for Theoretical Physics at Göttingen, showed how time translational symmetry induces conservation of energy.\n"}
{"id": "16434659", "url": "https://en.wikipedia.org/wiki?curid=16434659", "title": "IsaPlanner", "text": "IsaPlanner\n\nIsaPlanner is a proof planner for the interactive proof assistant, Isabelle. Originally developed by Lucas Dixon as part of his PhD thesis at the University of Edinburgh, it is now maintained by members of the Mathematical Reasoning Group, in the School of Informatics at Edinburgh.\nIsaPlanner is the latest of a series of proof planners written at Edinburgh. Earlier planners include Clam and LambdaClam.\n\nIsaPlanner allows the user to encode reasoning techniques, using a combinator language, for conjecturing and proving theorems. IsaPlanner works by manipulating reasoning states, records of open goals, the current proof plan and other important information, and combinators are functions mapping reasoning states to lazy lists of successor reasoning states.\n\nIsaPlanner's library supplies combinators for branching and iteration, amongst other tasks, and powerful reasoning techniques can be created by combining simpler reasoning techniques with these combinators.\n\nSeveral reasoning techniques come ready implemented within IsaPlanner, notably, IsaPlanner features an implementation of dynamic rippling, a rippling heuristic capable of working in higher order settings, a best-first rippling heuristic and a reasoning technique for proofs by induction.\n\nAdditional features include an interactive tracing tool, for manually stepping through proof attempts and a module for viewing and manipulating hierarchical proofs.\n\nFeatures currently being implemented, or planned for the future, are an expanded set of proof critics, suitable for use in higher order domains, dynamic relational rippling, a rippling heuristic suitable for rippling over relational expressions as opposed to functional expressions, again suitable for use in higher order domains, and integration of IsaPlanner with Proof General.\n\n"}
{"id": "87793", "url": "https://en.wikipedia.org/wiki?curid=87793", "title": "Joseph-Louis Lagrange", "text": "Joseph-Louis Lagrange\n\nJoseph-Louis Lagrange ( or ; ; born Giuseppe Luigi Lagrangia or Giuseppe Ludovico De la Grange Tournier, 25 January 1736 – 10 April 1813; also reported as Giuseppe Luigi Lagrange or Lagrangia) was an Italian Enlightenment Era mathematician and astronomer. He made significant contributions to the fields of analysis, number theory, and both classical and celestial mechanics.\n\nIn 1766, on the recommendation of Euler and d'Alembert, Lagrange succeeded Euler as the director of mathematics at the Prussian Academy of Sciences in Berlin, Prussia, where he stayed for over twenty years, producing volumes of work and winning several prizes of the French Academy of Sciences. Lagrange's treatise on analytical mechanics (\"Mécanique analytique\", 4. ed., 2 vols. Paris: Gauthier-Villars et fils, 1788–89), written in Berlin and first published in 1788, offered the most comprehensive treatment of classical mechanics since Newton and formed a basis for the development of mathematical physics in the nineteenth century.\n\nIn 1787, at age 51, he moved from Berlin to Paris and became a member of the French Academy of Sciences. He remained in France until the end of his life. He was significantly involved in the decimalisation in Revolutionary France, became the first professor of analysis at the École Polytechnique upon its opening in 1794, was a founding member of the Bureau des Longitudes, and became Senator in 1799.\n\nLagrange was one of the creators of the calculus of variations, deriving the Euler–Lagrange equations for extrema of functionals. He also extended the method to take into account possible constraints, arriving at the method of Lagrange multipliers.\nLagrange invented the method of solving differential equations known as variation of parameters, applied differential calculus to the theory of probabilities and attained notable work on the solution of equations. He proved that every natural number is a sum of four squares. His treatise \"Theorie des fonctions analytiques\" laid some of the foundations of group theory, anticipating Galois. In calculus, Lagrange developed a novel approach to interpolation and Taylor series. He studied the three-body problem for the Earth, Sun and Moon (1764) and the movement of Jupiter's satellites (1766), and in 1772 found the special-case solutions to this problem that yield what are now known as Lagrangian points. But above all, he is best known for his work on mechanics, where he transformed Newtonian mechanics into a branch of analysis, Lagrangian mechanics as it is now called, and presented the so-called mechanical \"principles\" as simple results of the variational calculus.\n\nBorn as \"Giuseppe Lodovico Lagrangia\", Lagrange was of Italian and French descent. His paternal great-grandfather was a French army officer who had moved to Turin, the \"de facto\" capital of the kingdom of Piedmont-Sardinia at Lagrange's time, and married an Italian; so did his grandfather and his father. His mother was from the countryside of Turin. He was raised as a Roman Catholic (but later on became an agnostic).\n\nHis father, who had charge of the king's military chest and was Treasurer of the Office of Public Works and Fortifications in Turin, should have maintained a good social position and wealth, but before his son grew up he had lost most of his property in speculations. A career as a lawyer was planned out for Lagrange by his father, and certainly Lagrange seems to have accepted this willingly. He studied at the University of Turin and his favourite subject was classical Latin. At first he had no great enthusiasm for mathematics, finding Greek geometry rather dull.\n\nIt was not until he was seventeen that he showed any taste for mathematics – his interest in the subject being first excited by a paper by Edmond Halley which he came across by accident. Alone and unaided he threw himself into mathematical studies; at the end of a year's incessant toil he was already an accomplished mathematician. Charles Emmanuel III appointed Lagrange to serve as the \"Sostituto del Maestro di Matematica\" (mathematics assistant professor) at the Royal Military Academy of the Theory and Practice of Artillery in 1755, where he taught courses in calculus and mechanics to support the Piedmontese army's early adoption of the ballistics theories of Benjamin Robins and Leonhard Euler. In that capacity, Lagrange was the first to teach calculus in an engineering school. According to , the academy's military commander and famous artillery theorist, Lagrange unfortunately proved to be a problematic professor with his oblivious teaching style, abstract reasoning, and impatience with artillery and fortification-engineering applications. In this Academy one of his students was François Daviet de Foncenex.\n\nLagrange is one of the founders of the calculus of variations. Starting in 1754, he worked on the problem of tautochrone, discovering a method of maximising and minimising functionals in a way similar to finding extrema of functions. Lagrange wrote several letters to Leonhard Euler between 1754 and 1756 describing his results. He outlined his \"δ-algorithm\", leading to the Euler–Lagrange equations of variational calculus and considerably simplifying Euler's earlier analysis. Lagrange also applied his ideas to problems of classical mechanics, generalising the results of Euler and Maupertuis.\n\nEuler was very impressed with Lagrange's results. It has been stated that \"with characteristic courtesy he withheld a paper he had previously written, which covered some of the same ground, in order that the young Italian might have time to complete his work, and claim the undisputed invention of the new calculus\"; however, this chivalric view has been disputed. Lagrange published his method in two memoirs of the Turin Society in 1762 and 1773.\n\nIn 1758, with the aid of his pupils (mainly Daviet de Foncenex), Lagrange established a society, which was subsequently incorporated as the Turin Academy of Sciences, and most of his early writings are to be found in the five volumes of its transactions, usually known as the \"Miscellanea Taurinensia\". Many of these are elaborate papers. The first volume contains a paper on the theory of the propagation of sound; in this he indicates a mistake made by Newton, obtains the general differential equation for the motion, and integrates it for motion in a straight line. This volume also contains the complete solution of the problem of a string vibrating transversely; in this paper he points out a lack of generality in the solutions previously given by Brook Taylor, D'Alembert, and Euler, and arrives at the conclusion that the form of the curve at any time \"t\" is given by the equation formula_1. The article concludes with a masterly discussion of echoes, beats, and compound sounds. Other articles in this volume are on recurring series, probabilities, and the calculus of variations.\n\nThe second volume contains a long paper embodying the results of several papers in the first volume on the theory and notation of the calculus of variations; and he illustrates its use by deducing the principle of least action, and by solutions of various problems in dynamics.\n\nThe third volume includes the solution of several dynamical problems by means of the calculus of variations; some papers on the integral calculus; a solution of Fermat's problem mentioned above: given an integer \"n\" which is not a perfect square, to find a number \"x\" such that \"x\"\"n\" + 1 is a perfect square; and the general differential equations of motion for three bodies moving under their mutual attractions.\n\nThe next work he produced was in 1764 on the libration of the Moon, and an explanation as to why the same face was always turned to the earth, a problem which he treated by the aid of virtual work. His solution is especially interesting as containing the germ of the idea of generalised equations of motion, equations which he first formally proved in 1780.\n\nAlready in 1756, Euler and Maupertuis, seeing his mathematical talent, tried to persuade him to come to Berlin, but Lagrange had no such intention and shyly refused the offer. In 1765, d'Alembert interceded on Lagrange's behalf with Frederick of Prussia and by letter, asked him to leave Turin for a considerably more prestigious position in Berlin. Lagrange again turned down the offer, responding that\n\nIn 1766, Euler left Berlin for Saint Petersburg, and Frederick himself wrote to Lagrange expressing the wish of \"the greatest king in Europe\" to have \"the greatest mathematician in Europe\" resident at his court. Lagrange was finally persuaded and he spent the next twenty years in Prussia, where he produced not only the long series of papers published in the Berlin and Turin transactions, but also his monumental work, the \"Mécanique analytique\". In 1767, he married his cousin Vittoria Conti.\n\nLagrange was a favourite of the king, who used frequently to discourse to him on the advantages of perfect regularity of life. The lesson went home, and thenceforth Lagrange studied his mind and body as though they were machines, and found by experiment the exact amount of work which he was able to do without breaking down. Every night he set himself a definite task for the next day, and on completing any branch of a subject he wrote a short analysis to see what points in the demonstrations or in the subject-matter were capable of improvement. He always thought out the subject of his papers before he began to compose them, and usually wrote them straight off without a single erasure or correction.\n\nNonetheless, during his years in Berlin, Lagrange's health was rather poor on many occasions, and that of his wife Vittoria was even worse. She died in 1783 after years of illness and Lagrange was very depressed. In 1786, Frederick II died, and the climate of Berlin became rather trying for Lagrange.\n\nIn 1786, following Frederick's death, Lagrange received similar invitations from states including Spain and Naples, and he accepted the offer of Louis XVI to move to Paris. In France he was received with every mark of distinction and special apartments in the Louvre were prepared for his reception, and he became a member of the French Academy of Sciences, which later became part of the Institut de France (1795). At the beginning of his residence in Paris he was seized with an attack of melancholy, and even the printed copy of his \"Mécanique\" on which he had worked for a quarter of a century lay for more than two years unopened on his desk. Curiosity as to the results of the French revolution first stirred him out of his lethargy, a curiosity which soon turned to alarm as the revolution developed.\n\nIt was about the same time, 1792, that the unaccountable sadness of his life and his timidity moved the compassion of 24-year-old Renée-Françoise-Adélaïde Le Monnier, daughter of his friend, the astronomer Pierre Charles Le Monnier. She insisted on marrying him, and proved a devoted wife to whom he became warmly attached.\n\nIn September 1793, the Reign of Terror began. Under intervention of Antoine Lavoisier, who himself was by then already thrown out of the Academy along with many other scholars, Lagrange was specifically exempted by name in the decree of October 1793 that ordered all foreigners to leave France. On 4 May 1794, Lavoisier and 27 other tax farmers were arrested and sentenced to death and guillotined on the afternoon after the trial. Lagrange said on the death of Lavoisier:\n\nThough Lagrange had been preparing to escape from France while there was yet time, he was never in any danger; different revolutionary governments (and at a later time, Napoleon) loaded him with honours and distinctions. This luckiness or safety may to some extent be due to his life attitude he expressed many years before: \"I believe that, in general, one of the first principles of every wise man is to conform strictly to the laws of the country in which he is living, even when they are unreasonable\". A striking testimony to the respect in which he was held was shown in 1796 when the French commissary in Italy was ordered to attend in full state on Lagrange's father, and tender the congratulations of the republic on the achievements of his son, who \"had done honor to all mankind by his genius, and whom it was the special glory of Piedmont to have produced.\" It may be added that Napoleon, when he attained power, warmly encouraged scientific studies in France, and was a liberal benefactor of them. Appointed senator in 1799, he was the first signer of the Sénatus-consulte which in 1802 annexed his fatherland Piedmont to France. He acquired French citizenship in consequence. The French claimed he was a French mathematician, but the Italians continued to claim him as Italian.\"\n\nLagrange was considerably involved in the process of making new standard units of measurement in the 1790s. He was offered the presidency of the Commission for the reform of weights and measures (\"la Commission des Poids et Mesures\") when he was preparing to escape. And after Lavoisier's death in 1794, it was largely owing to Lagrange's influence that the final choice of the unit system of metre and kilogram was settled and the decimal subdivision was finally accepted by the commission of 1799. Lagrange was also one of the founding members of the Bureau des Longitudes in 1795.\n\nIn 1795, Lagrange was appointed to a mathematical chair at the newly established École Normale, which enjoyed only a brief existence of four months. His lectures there were quite elementary, and contain nothing of any special importance, but they were published because the professors had to \"pledge themselves to the representatives of the people and to each other neither to read nor to repeat from memory,\" and the discourses were ordered to be taken down in shorthand to enable the deputies to see how the professors acquitted themselves.\n\nIn 1794, Lagrange was appointed professor of the École Polytechnique; and his lectures there, described by mathematicians who had the good fortune to be able to attend them, were almost perfect both in form and matter. Beginning with the merest elements, he led his hearers on until, almost unknown to themselves, they were themselves extending the bounds of the subject: above all he impressed on his pupils the advantage of always using general methods expressed in a symmetrical notation.\n\nBut Lagrange does not seem to have been a successful teacher. Fourier, who attended his lectures in 1795, wrote:\n\nIn 1810, Lagrange commenced a thorough revision of the \"Mécanique analytique\", but he was able to complete only about two-thirds of it before his death at Paris in 1813, in 128 rue du Faubourg Saint-Honoré. Napoleon honoured him with the Grand Croix of the Ordre Impérial de la Réunion just two days before he died. He was buried that same year in the Panthéon in Paris. The inscription on his tomb reads in translation:JOSEPH LOUIS LAGRANGE. Senator. Count of the Empire. Grand Officer of the Legion of Honour. Grand Cross of the Imperial Order of the Reunion. Member of the Institute and the Bureau of Longitude. Born in Turin on 25 January 1736. Died in Paris on 10 April 1813.\n\nLagrange was extremely active scientifically during twenty years he spent in Berlin. Not only did he produce his \"Mécanique analytique\", but he contributed between one and two hundred papers to the Academy of Turin, the Berlin Academy, and the French Academy. Some of these are really treatises, and all without exception are of a high order of excellence. Except for a short time when he was ill he produced on average about one paper a month. Of these, note the following as amongst the most important.\n\nFirst, his contributions to the fourth and fifth volumes, 1766–1773, of the \"Miscellanea Taurinensia\"; of which the most important was the one in 1771, in which he discussed how numerous astronomical observations should be combined so as to give the most probable result. And later, his contributions to the first two volumes, 1784–1785, of the transactions of the Turin Academy; to the first of which he contributed a paper on the pressure exerted by fluids in motion, and to the second an article on integration by infinite series, and the kind of problems for which it is suitable.\n\nMost of the papers sent to Paris were on astronomical questions, and among these including his paper on the Jovian system in 1766, his essay on the problem of three bodies in 1772, his work on the secular equation of the Moon in 1773, and his treatise on cometary perturbations in 1778. These were all written on subjects proposed by the Académie française, and in each case the prize was awarded to him.\n\nBetween 1772 and 1788, Lagrange re-formulated Classical/Newtonian mechanics to simplify formulas and ease calculations. These mechanics are called Lagrangian mechanics.\n\nThe greater number of his papers during this time were, however, contributed to the Prussian Academy of Sciences. Several of them deal with questions in algebra.\n\n\nSeveral of his early papers also deal with questions of number theory.\n\n\nThere are also numerous articles on various points of analytical geometry. In two of them, written rather later, in 1792 and 1793, he reduced the equations of the quadrics (or conicoids) to their canonical forms.\n\nDuring the years from 1772 to 1785, he contributed a long series of papers which created the science of partial differential equations. A large part of these results were collected in the second edition of Euler's integral calculus which was published in 1794.\n\nLastly, there are numerous papers on problems in astronomy. Of these the most important are the following:\n\n\nOver and above these various papers he composed his great treatise, the \"Mécanique analytique\". In this he lays down the law of virtual work, and from that one fundamental principle, by the aid of the calculus of variations, deduces the whole of mechanics, both of solids and fluids.\n\nThe object of the book is to show that the subject is implicitly included in a single principle, and to give general formulae from which any particular result can be obtained. The method of generalised co-ordinates by which he obtained this result is perhaps the most brilliant result of his analysis. Instead of following the motion of each individual part of a material system, as D'Alembert and Euler had done, he showed that, if we determine its configuration by a sufficient number of variables whose number is the same as that of the degrees of freedom possessed by the system, then the kinetic and potential energies of the system can be expressed in terms of those variables, and the differential equations of motion thence deduced by simple differentiation. For example, in dynamics of a rigid system he replaces the consideration of the particular problem by the general equation, which is now usually written in the form\n\nwhere \"T\" represents the kinetic energy and \"V\" represents the potential energy of the system.\nHe then presented what we now know as the method of Lagrange multipliers—though this is not the first time that method was published—as a means to solve this equation.\nAmongst other minor theorems here given it may suffice to mention the proposition that the kinetic energy imparted by the given impulses to a material system under given constraints is a maximum, and the principle of least action. All the analysis is so elegant that Sir William Rowan Hamilton said the work could be described only as a scientific poem. Lagrange remarked that mechanics was really a branch of pure mathematics analogous to a geometry of four dimensions, namely, the time and the three coordinates of the point in space; and it is said that he prided himself that from the beginning to the end of the work there was not a single diagram. At first no printer could be found who would publish the book; but Legendre at last persuaded a Paris firm to undertake it, and it was issued under the supervision of Laplace, Cousin, Legendre (editor) and Condorcet in 1788.\n\nLagrange's lectures on the differential calculus at École Polytechnique form the basis of his treatise \"Théorie des fonctions analytiques\", which was published in 1797. This work is the extension of an idea contained in a paper he had sent to the Berlin papers in 1772, and its object is to substitute for the differential calculus a group of theorems based on the development of algebraic functions in series, relying in particular on the principle of the generality of algebra.\n\nA somewhat similar method had been previously used by John Landen in the \"Residual Analysis\", published in London in 1758. Lagrange believed that he could thus get rid of those difficulties, connected with the use of infinitely large and infinitely small quantities, to which philosophers objected in the usual treatment of the differential calculus. The book is divided into three parts: of these, the first treats of the general theory of functions, and gives an algebraic proof of Taylor's theorem, the validity of which is, however, open to question; the second deals with applications to geometry; and the third with applications to mechanics.\n\nAnother treatise on the same lines was his \"Leçons sur le calcul des fonctions\", issued in 1804, with the second edition in 1806. It is in this book that Lagrange formulated his celebrated method of Lagrange multipliers, in the context of problems of variational calculus with integral constraints. These works devoted to differential calculus and calculus of variations may be considered as the starting point for the researches of Cauchy, Jacobi, and Weierstrass.\n\nAt a later period Lagrange fully embraced the use of infinitesimals in preference to founding the differential calculus on the study of algebraic forms; and in the preface to the second edition of the \"Mécanique Analytique\", which was issued in 1811, he justifies the employment of infinitesimals, and concludes by saying that:\n\nHis \"Résolution des équations numériques\", published in 1798, was also the fruit of his lectures at École Polytechnique. There he gives the method of approximating to the real roots of an equation by means of continued fractions, and enunciates several other theorems. In a note at the end he shows how Fermat's little theorem, that is\n\nwhere \"p\" is a prime and \"a\" is prime to \"p\", may be applied to give the complete algebraic solution of any binomial equation. He also here explains how the equation whose roots are the squares of the differences of the roots of the original equation may be used so as to give considerable information as to the position and nature of those roots.\n\nThe theory of the planetary motions had formed the subject of some of the most remarkable of Lagrange's Berlin papers. In 1806 the subject was reopened by Poisson, who, in a paper read before the French Academy, showed that Lagrange's formulae led to certain limits for the stability of the orbits. Lagrange, who was present, now discussed the whole subject afresh, and in a letter communicated to the Academy in 1808 explained how, by the variation of arbitrary constants, the periodical and secular inequalities of any system of mutually interacting bodies could be determined.\n\nEuler proposed Lagrange for election to the Berlin Academy and he was elected on 2 September 1756. He was elected a Fellow of the Royal Society of Edinburgh in 1790, a Fellow of the Royal Society and a foreign member of the Royal Swedish Academy of Sciences in 1806. In 1808, Napoleon made Lagrange a Grand Officer of the Legion of Honour and a Count of the Empire. He was awarded the Grand Croix of the Ordre Impérial de la Réunion in 1813, a week before his death in Paris.\n\nLagrange was awarded the 1764 prize of the French Academy of Sciences for his memoir on the libration of the Moon. In 1766 the Academy proposed a problem of the motion of the satellites of Jupiter, and the prize again was awarded to Lagrange. He also shared or won the prizes of 1772, 1774, and 1778.\n\nLagrange is one of the 72 prominent French scientists who were commemorated on plaques at the first stage of the Eiffel Tower when it first opened. \"Rue Lagrange\" in the 5th Arrondissement in Paris is named after him. In Turin, the street where the house of his birth still stands is named \"via Lagrange\". The lunar crater Lagrange also bears his name.\n\n\nThe initial version of this article was taken from the public domain resource \"A Short Account of the History of Mathematics\" (4th edition, 1908) by W. W. Rouse Ball.\n\n\n"}
{"id": "4827691", "url": "https://en.wikipedia.org/wiki?curid=4827691", "title": "Leibniz operator", "text": "Leibniz operator\n\nIn abstract algebraic logic, a branch of mathematical logic, the Leibniz operator is a tool used to classify deductive systems, which have a precise technical definition, and capture a large number of logics. The Leibniz operator was introduced by Wim Blok and Don Pigozzi, two of the founders of the field, as a means to abstract the well-known Lindenbaum–Tarski process, that leads to the \nassociation of Boolean algebras to classical propositional calculus, and make it applicable\nto as wide a variety of sentential logics as possible. It is an \noperator that assigns to a given theory of a given \nsentential logic, perceived as a free algebra\nwith a consequence operation on its universe, the\nlargest congruence on the algebra that is \ncompatible with the theory.\n\nIn this article, we introduce the Leibniz \noperator in the special case of classical\npropositional calculus, then we abstract it to the general notion applied to an arbitrary sentential logic and, finally, we summarize\nsome of the most important consequences of\nits use in the theory of abstract algebraic \nlogic. \n\nLet \n\ndenote the classical propositional calculus. According to the classical \nLindenbaum–Tarski process, given a theory\nformula_2 of formula_3,\nif formula_4\ndenotes the binary relation on the set of formulas \nof formula_3, defined by\n\nwhere formula_8 denotes the usual\nclassical propositional equivalence connective, then\nformula_4 turns out to be a congruence\non the formula algebra. Furthermore, the quotient \nformula_10 is a Boolean algebra\nand every Boolean algebra may be formed in this way.\n\nThus, the variety of Boolean algebras, which is,\nin algebraic logic terminology, the \nequivalent algebraic semantics (algebraic counterpart)\nof classical propositional calculus, is the class of\nall algebras formed by taking appropriate quotients\nof free algebras by those special kinds of\ncongruences.\n\nThe condition \n\nthat defines \nformula_6 is equivalent to the \ncondition \n\nPassing now to an arbitrary sentential logic \n\ngiven a theory formula_2,\nthe Leibniz congruence associated with formula_2 is\ndenoted by formula_18 and is defined, for all\nformula_19, by\n\nif and only if, for every formula \nformula_21 containing a variable formula_22\nand possibly other variables in the list formula_23,\nand all formulas formula_24 forming a list of the same \nlength as that of formula_23, we have that\n\nif and only if formula_27.\n\nIt turns out that this binary relation is a congruence relation\non the formula algebra and, in fact, may alternatively be characterized\nas the largest congruence on the formula algebra that is compatible\nwith the theory formula_2, in the sense that\nif formula_20 and formula_13, then we must have also formula_14. It is this congruence that\nplays the same role as the congruence used in the\ntraditional Lindenbaum–Tarski process described above in the \ncontext of an arbitrary sentential logic. \n\nIt is not, however, the case that for arbitrary sentential logics the quotients of the free algebras by\nthese Leibniz congruences over different theories yield all algebras\nin the class that forms the natural algebraic counterpart of the\nsentential logic. This phenomenon occurs only in the case\nof \"nice\" logics and one of the main goals of abstract algebraic logic\nis to make this vague notion of a logic being \"nice\", in this\nsense, mathematically precise. \n\nThe Leibniz operator \n\nis the operator that maps a theory formula_2 of a given logic to the \nLeibniz congruence \n\nassociated with the theory. Thus, formally, \n\nis a mapping from the collection \n\nformula_3 to the collection \n\nof all congruences on the formula algebra formula_39\nof the sentential logic.\n\nThe Leibniz operator and the study of various of its \nproperties that may or may not be satisfied for particular\nsentential logics have given rise to what is now known as\nthe abstract algebraic hierarchy or Leibniz hierarchy of\nsentential logics. Logics are classified in various levels\nof this hierarchy depending on how strong a tie exists \nbetween the logic and its algebraic counterpart.\n\nThe properties of the Leibniz operator that help classify\nthe logics are monotonicity, injectivity, continuity\nand commutativity with inverse substitutions. For instance,\nprotoalgebraic logics, forming the widest class in the hierarchy,\ni.e., the one that lies in the bottom of the hierarchy\nand contains all other classes, are characterized by\nthe monotonicity of the Leibniz operator on their theories.\nOther notable classes are formed by the equivalential logics,\nthe weakly algebraizable logics and the algebraizable logics, among others.\n\nThere is a generalization of the \nLeibniz operator, in the context of categorical\nabstract algebraic logic, that makes it possible\nto apply a wide variety of techniques that were\npreviously applicable only in the sentential logic\nframework to logics formalized as formula_40-institutions.\nThe formula_40-institution framework is significantly wider\nin scope than the framework of sentential logics\nbecause it allows incorporating multiple signatures\nand quantifiers in the language and it provides a mechanism for\nhandling logics that are not syntactically based.\n\n"}
{"id": "17902", "url": "https://en.wikipedia.org/wiki?curid=17902", "title": "Leonhard Euler", "text": "Leonhard Euler\n\nLeonhard Euler ( ; ; 15 April 170718 September 1783) was a Swiss mathematician, physicist, astronomer, logician and engineer, who made important and influential discoveries in many branches of mathematics, such as infinitesimal calculus and graph theory, while also making pioneering contributions to several branches such as topology and analytic number theory. He also introduced much of the modern mathematical terminology and notation, particularly for mathematical analysis, such as the notion of a mathematical function. He is also known for his work in mechanics, fluid dynamics, optics, astronomy, and music theory.\n\nEuler was one of the most eminent mathematicians of the 18th century and is held to be one of the greatest in history. He is also widely considered to be the most prolific mathematician of all time. His collected works fill 60 to 80 quarto volumes, more than anybody in the field. He spent most of his adult life in Saint Petersburg, Russia, and in Berlin, then the capital of Prussia.\n\nA statement attributed to Pierre-Simon Laplace expresses Euler's influence on mathematics: \"Read Euler, read Euler, he is the master of us all.\"\n\nLeonhard Euler was born on 15 April 1707, in Basel, Switzerland to Paul III Euler, a pastor of the Reformed Church, and Marguerite Brucker, a pastor's daughter. He had two younger sisters: Anna Maria and Maria Magdalena, and a younger brother Johann Heinrich. Soon after the birth of Leonhard, the Eulers moved from Basel to the town of Riehen, where Euler spent most of his childhood. Paul Euler was a friend of the Bernoulli family; Johann Bernoulli was then regarded as Europe's foremost mathematician, and would eventually be the most important influence on young Leonhard.\n\nEuler's formal education started in Basel, where he was sent to live with his maternal grandmother. In 1720, aged thirteen, he enrolled at the University of Basel, and in 1723, he received a Master of Philosophy with a dissertation that compared the philosophies of Descartes and Newton. During that time, he was receiving Saturday afternoon lessons from Johann Bernoulli, who quickly discovered his new pupil's incredible talent for mathematics. At that time Euler's main studies included theology, Greek, and Hebrew at his father's urging in order to become a pastor, but Bernoulli convinced his father that Leonhard was destined to become a great mathematician.\n\nIn 1726, Euler completed a dissertation on the propagation of sound with the title \"De Sono\". At that time, he was unsuccessfully attempting to obtain a position at the University of Basel. In 1727, he first entered the \"Paris Academy Prize Problem\" competition; the problem that year was to find the best way to place the masts on a ship. Pierre Bouguer, who became known as \"the father of naval architecture\", won and Euler took second place. Euler later won this annual prize twelve times.\n\nAround this time Johann Bernoulli's two sons, Daniel and Nicolaus, were working at the Imperial Russian Academy of Sciences in Saint Petersburg. On 31 July 1726, Nicolaus died of appendicitis after spending less than a year in Russia, and when Daniel assumed his brother's position in the mathematics/physics division, he recommended that the post in physiology that he had vacated be filled by his friend Euler. In November 1726 Euler eagerly accepted the offer, but delayed making the trip to Saint Petersburg while he unsuccessfully applied for a physics professorship at the University of Basel.\nEuler arrived in Saint Petersburg on 17 May 1727. He was promoted from his junior post in the medical department of the academy to a position in the mathematics department. He lodged with Daniel Bernoulli with whom he often worked in close collaboration. Euler mastered Russian and settled into life in Saint Petersburg. He also took on an additional job as a medic in the Russian Navy.\n\nThe Academy at Saint Petersburg, established by Peter the Great, was intended to improve education in Russia and to close the scientific gap with Western Europe. As a result, it was made especially attractive to foreign scholars like Euler. The academy possessed ample financial resources and a comprehensive library drawn from the private libraries of Peter himself and of the nobility. Very few students were enrolled in the academy in order to lessen the faculty's teaching burden, and the academy emphasized research and offered to its faculty both the time and the freedom to pursue scientific questions.\n\nThe Academy's benefactress, Catherine I, who had continued the progressive policies of her late husband, died on the day of Euler's arrival. The Russian nobility then gained power upon the ascension of the twelve-year-old Peter II. The nobility was suspicious of the academy's foreign scientists, and thus cut funding and caused other difficulties for Euler and his colleagues.\n\nConditions improved slightly after the death of Peter II, and Euler swiftly rose through the ranks in the academy and was made a professor of physics in 1731. Two years later, Daniel Bernoulli, who was fed up with the censorship and hostility he faced at Saint Petersburg, left for Basel. Euler succeeded him as the head of the mathematics department.\n\nOn 7 January 1734, he married Katharina Gsell (1707–1773), a daughter of Georg Gsell, a painter from the Academy Gymnasium. The young couple bought a house by the Neva River. Of their thirteen children, only five survived childhood.\n\nConcerned about the continuing turmoil in Russia, Euler left St. Petersburg on 19 June 1741 to take up a post at the \"Berlin Academy\", which he had been offered by Frederick the Great of Prussia. He lived for 25 years in Berlin, where he wrote over 380 articles. In Berlin, he published the two works for which he would become most renowned: the \"Introductio in analysin infinitorum\", a text on functions published in 1748, and the \"Institutiones calculi differentialis\", published in 1755 on differential calculus. In 1755, he was elected a foreign member of the Royal Swedish Academy of Sciences.\n\nIn addition, Euler was asked to tutor Friederike Charlotte of Brandenburg-Schwedt, the Princess of Anhalt-Dessau and Frederick's niece. Euler wrote over 200 letters to her in the early 1760s, which were later compiled into a best-selling volume entitled \"Letters of Euler on different Subjects in Natural Philosophy Addressed to a German Princess\". This work contained Euler's exposition on various subjects pertaining to physics and mathematics, as well as offering valuable insights into Euler's personality and religious beliefs. This book became more widely read than any of his mathematical works and was published across Europe and in the United States. The popularity of the \"Letters\" testifies to Euler's ability to communicate scientific matters effectively to a lay audience, a rare ability for a dedicated research scientist.\n\nDespite Euler's immense contribution to the Academy's prestige, he eventually incurred the ire of Frederick and ended up having to leave Berlin. The Prussian king had a large circle of intellectuals in his court, and he found the mathematician unsophisticated and ill-informed on matters beyond numbers and figures. Euler was a simple, devoutly religious man who never questioned the existing social order or conventional beliefs, in many ways the polar opposite of Voltaire, who enjoyed a high place of prestige at Frederick's court. Euler was not a skilled debater and often made it a point to argue subjects that he knew little about, making him the frequent target of Voltaire's wit. Frederick also expressed disappointment with Euler's practical engineering abilities:\n\nEuler's eyesight worsened throughout his mathematical career. In 1738, three years after nearly expiring from fever, he became almost blind in his right eye, but Euler rather blamed the painstaking work on cartography he performed for the St. Petersburg Academy for his condition. Euler's vision in that eye worsened throughout his stay in Germany, to the extent that Frederick referred to him as \"Cyclops\". Euler remarked on his loss of vision, \"Now I will have fewer distractions.\" He later developed a cataract in his left eye, which was discovered in 1766. Just a few weeks after its discovery, he was rendered almost totally blind. However, his condition appeared to have little effect on his productivity, as he compensated for it with his mental calculation skills and exceptional memory. For example, Euler could repeat the \"Aeneid\" of Virgil from beginning to end without hesitation, and for every page in the edition he could indicate which line was the first and which the last. With the aid of his scribes, Euler's productivity on many areas of study actually increased. He produced, on average, one mathematical paper every week in the year 1775. The Eulers bore a double name, Euler-Schölpi, the latter of which derives from \"schelb\" and \"schief\", signifying squint-eyed, cross-eyed, or crooked. This suggests that the Eulers may have had a susceptibility to eye problems.\n\nIn 1760, with the Seven Years' War raging, Euler's farm in Charlottenburg was ransacked by advancing Russian troops. Upon learning of this event, General Ivan Petrovich Saltykov paid compensation for the damage caused to Euler's estate, later Empress Elizabeth of Russia added a further payment of 4000 roubles—an exorbitant amount at the time. The political situation in Russia stabilized after Catherine the Great's accession to the throne, so in 1766 Euler accepted an invitation to return to the St. Petersburg Academy. His conditions were quite exorbitant—a 3000 ruble annual salary, a pension for his wife, and the promise of high-ranking appointments for his sons. All of these requests were granted. He spent the rest of his life in Russia. However, his second stay in the country was marred by tragedy. A fire in St. Petersburg in 1771 cost him his home, and almost his life. In 1773, he lost his wife Katharina after 40 years of marriage.\n\nThree years after his wife's death, Euler married her half-sister, Salome Abigail Gsell (1723–1794). This marriage lasted until his death. In 1782 he was elected a Foreign Honorary Member of the American Academy of Arts and Sciences.\n\nIn St. Petersburg on 18 September 1783, after a lunch with his family, Euler was discussing the newly discovered planet Uranus and its orbit with a fellow academician Anders Johan Lexell, when he collapsed from a brain hemorrhage. He died a few hours later. wrote a short obituary for the Russian Academy of Sciences and Russian mathematician Nicolas Fuss, one of Euler's disciples, wrote a more detailed eulogy, which he delivered at a memorial meeting. In his eulogy for the French Academy, French mathematician and philosopher Marquis de Condorcet, wrote:\n\nEuler was buried next to Katharina at the Smolensk Lutheran Cemetery on Goloday Island. In 1785, the Russian Academy of Sciences put a marble bust of Leonhard Euler on a pedestal next to the Director's seat and, in 1837, placed a headstone on Euler's grave. To commemorate the 250th anniversary of Euler's birth, the headstone was moved in 1956, together with his remains, to the 18th-century necropolis at the Alexander Nevsky Monastery.\n\nEuler worked in almost all areas of mathematics, such as geometry, infinitesimal calculus, trigonometry, algebra, and number theory, as well as continuum physics, lunar theory and other areas of physics. He is a seminal figure in the history of mathematics; if printed, his works, many of which are of fundamental interest, would occupy between 60 and 80 quarto volumes. Euler's name is associated with a large number of topics.\n\nEuler is the only mathematician to have \"two\" numbers named after him: the important Euler's number in calculus, \"e\", approximately equal to 2.71828, and the Euler–Mascheroni constant γ (gamma) sometimes referred to as just \"Euler's constant\", approximately equal to 0.57721. It is not known whether γ is rational or irrational.\nEuler introduced and popularized several notational conventions through his numerous and widely circulated textbooks. Most notably, he introduced the concept of a function and was the first to write \"f\"(\"x\") to denote the function \"f\" applied to the argument \"x\". He also introduced the modern notation for the trigonometric functions, the letter for the base of the natural logarithm (now also known as Euler's number), the Greek letter Σ for summations and the letter to denote the imaginary unit. The use of the Greek letter \"π\" to denote the ratio of a circle's circumference to its diameter was also popularized by Euler, although it originated with Welsh mathematician William Jones.\n\nThe development of infinitesimal calculus was at the forefront of 18th-century mathematical research, and the Bernoullis—family friends of Euler—were responsible for much of the early progress in the field. Thanks to their influence, studying calculus became the major focus of Euler's work. While some of Euler's proofs are not acceptable by modern standards of mathematical rigour (in particular his reliance on the principle of the generality of algebra), his ideas led to many great advances.\nEuler is well known in analysis for his frequent use and development of power series, the expression of functions as sums of infinitely many terms, such as\n\nNotably, Euler directly proved the power series expansions for and the inverse tangent function. (Indirect proof via the inverse power series technique was given by Newton and Leibniz between 1670 and 1680.) His daring use of power series enabled him to solve the famous Basel problem in 1735 (he provided a more elaborate argument in 1741):\n\nEuler introduced the use of the exponential function and logarithms in analytic proofs. He discovered ways to express various logarithmic functions using power series, and he successfully defined logarithms for negative and complex numbers, thus greatly expanding the scope of mathematical applications of logarithms. He also defined the exponential function for complex numbers, and discovered its relation to the trigonometric functions. For any real number (taken to be radians), Euler's formula states that the complex exponential function satisfies\n\nA special case of the above formula is known as Euler's identity,\ncalled \"the most remarkable formula in mathematics\" by Richard P. Feynman, for its single uses of the notions of addition, multiplication, exponentiation, and equality, and the single uses of the important constants 0, 1, , and . In 1988, readers of the \"Mathematical Intelligencer\" voted it \"the Most Beautiful Mathematical Formula Ever\". In total, Euler was responsible for three of the top five formulae in that poll.\n\nDe Moivre's formula is a direct consequence of Euler's formula.\n\nIn addition, Euler elaborated the theory of higher transcendental functions by introducing the gamma function and introduced a new method for solving quartic equations. He also found a way to calculate integrals with complex limits, foreshadowing the development of modern complex analysis. He also invented the calculus of variations including its best-known result, the Euler–Lagrange equation.\n\nEuler also pioneered the use of analytic methods to solve number theory problems. In doing so, he united two disparate branches of mathematics and introduced a new field of study, analytic number theory. In breaking ground for this new field, Euler created the theory of hypergeometric series, q-series, hyperbolic trigonometric functions and the analytic theory of continued fractions. For example, he proved the infinitude of primes using the divergence of the harmonic series, and he used analytic methods to gain some understanding of the way prime numbers are distributed. Euler's work in this area led to the development of the prime number theorem.\n\nEuler's interest in number theory can be traced to the influence of Christian Goldbach, his friend in the St. Petersburg Academy. A lot of Euler's early work on number theory was based on the works of Pierre de Fermat. Euler developed some of Fermat's ideas and disproved some of his conjectures.\n\nEuler linked the nature of prime distribution with ideas in analysis. He proved that the sum of the reciprocals of the primes diverges. In doing so, he discovered the connection between the Riemann zeta function and the prime numbers; this is known as the Euler product formula for the Riemann zeta function.\n\nEuler proved Newton's identities, Fermat's little theorem, Fermat's theorem on sums of two squares, and he made distinct contributions to Lagrange's four-square theorem. He also invented the totient function φ(\"n\"), the number of positive integers less than or equal to the integer \"n\" that are coprime to \"n\". Using properties of this function, he generalized Fermat's little theorem to what is now known as Euler's theorem. He contributed significantly to the theory of perfect numbers, which had fascinated mathematicians since Euclid. He proved that the relationship shown between even perfect numbers and Mersenne primes earlier proved by Euclid was one-to-one, a result otherwise known as the Euclid–Euler theorem. Euler also conjectured the law of quadratic reciprocity. The concept is regarded as a fundamental theorem of number theory, and his ideas paved the way for the work of Carl Friedrich Gauss.\nBy 1772 Euler had proved that 2 − 1 = 2,147,483,647 is a Mersenne prime. It may have remained the largest known prime until 1867.\n\nIn 1735, Euler presented a solution to the problem known as the Seven Bridges of Königsberg. The city of Königsberg, Prussia was set on the Pregel River, and included two large islands that were connected to each other and the mainland by seven bridges. The problem is to decide whether it is possible to follow a path that crosses each bridge exactly once and returns to the starting point. It is not possible: there is no Eulerian circuit. This solution is considered to be the first theorem of graph theory, specifically of planar graph theory.\n\nEuler also discovered the formula formula_5 relating the number of vertices, edges and faces of a convex polyhedron, and hence of a planar graph. The constant in this formula is now known as the Euler characteristic for the graph (or other mathematical object), and is related to the genus of the object. The study and generalization of this formula, specifically by Cauchy and L'Huilier, is at the origin of topology.\n\nSome of Euler's greatest successes were in solving real-world problems analytically, and in describing numerous applications of the Bernoulli numbers, Fourier series, Euler numbers, the constants and , continued fractions and integrals. He integrated Leibniz's differential calculus with Newton's Method of Fluxions, and developed tools that made it easier to apply calculus to physical problems. He made great strides in improving the numerical approximation of integrals, inventing what are now known as the Euler approximations. The most notable of these approximations are Euler's method and the Euler–Maclaurin formula. He also facilitated the use of differential equations, in particular introducing the Euler–Mascheroni constant:\n\nOne of Euler's more unusual interests was the application of mathematical ideas in music. In 1739 he wrote the \"Tentamen novae theoriae musicae,\" hoping to eventually incorporate musical theory as part of mathematics. This part of his work, however, did not receive wide attention and was once described as too mathematical for musicians and too musical for mathematicians.\n\nEuler helped develop the Euler–Bernoulli beam equation, which became a cornerstone of engineering. Aside from successfully applying his analytic tools to problems in classical mechanics, Euler also applied these techniques to celestial problems. His work in astronomy was recognized by a number of Paris Academy Prizes over the course of his career. His accomplishments include determining with great accuracy the orbits of comets and other celestial bodies, understanding the nature of comets, and calculating the parallax of the sun. His calculations also contributed to the development of accurate longitude tables.\n\nIn addition, Euler made important contributions in optics. He disagreed with Newton's corpuscular theory of light in the \"Opticks\", which was then the prevailing theory. His 1740s papers on optics helped ensure that the wave theory of light proposed by Christiaan Huygens would become the dominant mode of thought, at least until the development of the quantum theory of light.\n\nIn 1757 he published an important set of equations for inviscid flow, that are now known as the Euler equations. In differential form, the equations are:\n\nwhere\n\nEuler is also well known in structural engineering for his formula giving the critical buckling load of an ideal strut, which depends only on its length and flexural stiffness:\n\nwhere\n\nEuler is also credited with using closed curves to illustrate syllogistic reasoning (1768). These diagrams have become known as Euler diagrams.\nAn Euler diagram is a diagrammatic means of representing sets and their relationships. Euler diagrams consist of simple closed curves (usually circles) in the plane that depict sets. Each Euler curve divides the plane into two regions or \"zones\": the interior, which symbolically represents the elements of the set, and the exterior, which represents all elements that are not members of the set. The sizes or shapes of the curves are not important; the significance of the diagram is in how they overlap. The spatial relationships between the regions bounded by each curve (overlap, containment or neither) corresponds to set-theoretic relationships (intersection, subset and disjointness). Curves whose interior zones do not intersect represent disjoint sets. Two curves whose interior zones intersect represent sets that have common elements; the zone inside both curves represents the set of elements common to both sets (the intersection of the sets). A curve that is contained completely within the interior zone of another represents a subset of it. Euler diagrams (and their generalization in Venn diagrams) were incorporated as part of instruction in set theory as part of the new math movement in the 1960s. Since then, they have also been adopted by other curriculum fields such as reading.\n\nEven when dealing with music, Euler's approach is mainly mathematical. His writings on music are not particularly numerous (a few hundred pages, in his total production of about thirty thousand pages), but they reflect an early preoccupation and one that did not leave him throughout his life.\n\nA first point of Euler's musical theory is the definition of \"genres\", i.e. of possible divisions of the octave using the prime numbers 3 and 5. Euler describes 18 such genres, with the general definition 2A, where A is the \"exponent\" of the genre (i.e. the sum of the exponents of 3 and 5) and 2 (where \"m is an indefinite number, small or large, so long as the sounds are perceptible\"), expresses that the relation holds independently of the number of octaves concerned. The first genre, with A = 1, is the octave itself (or its duplicates); the second genre, 2.3, is the octave divided by the fifth (fifth + fourth, C–G–C); the third genre is 2.5, major third + minor sixth (C–E–C); the fourth is 2.3, two fourths and a tone (C–F–B–C); the fifth is 2.3.5 (C–E–G–B–C); etc. Genres 12 (2.3.5), 13 (2.3.5) and 14 (2.3.5) are corrected versions of the diatonic, chromatic and enharmonic, respectively, of the Ancients. Genre 18 (2.3.5) is the \"diatonico-chromatic\", \"used generally in all compositions\", and which turns out to be identical with the system described by Johann Mattheson. Euler later envisaged the possibility of describing genres including the prime number 7.\n\nEuler devised a specific graph, the \"Speculum musicum\", to illustrate the diatonico-chromatic genre, and discussed paths in this graph for specific intervals, recalling his interest in the Seven Bridges of Königsberg (see above). The device drew renewed interest as the Tonnetz in neo-Riemannian theory (see also Lattice (music)).\n\nEuler further used the principle of the \"exponent\" to propose a derivation of the \"gradus suavitatis\" (degree of suavity, of agreeableness) of intervals and chords from their prime factors – one must keep in mind that he considered just intonation, i.e. 1 and the prime numbers 3 and 5 only. Formulas have been proposed extending this system to any number of prime numbers, e.g. in the form\n\nwhere \"p\" are prime numbers and \"k\" their exponents.\n\nEuler and his friend Daniel Bernoulli were opponents of Leibniz's monadism and the philosophy of Christian Wolff. Euler insisted that knowledge is founded in part on the basis of precise quantitative laws, something that monadism and Wolffian science were unable to provide. Euler's religious leanings might also have had a bearing on his dislike of the doctrine; he went so far as to label Wolff's ideas as \"heathen and atheistic\".\n\nMuch of what is known of Euler's religious beliefs can be deduced from his \"Letters to a German Princess\" and an earlier work, \"Rettung der Göttlichen Offenbahrung Gegen die Einwürfe der Freygeister\" (\"Defense of the Divine Revelation against the Objections of the Freethinkers\"). These works show that Euler was a devout Christian who believed the Bible to be inspired; the \"Rettung\" was primarily an argument for the divine inspiration of scripture.\n\nThere is a famous legend inspired by Euler's arguments with secular philosophers over religion, which is set during Euler's second stint at the St. Petersburg Academy. The French philosopher Denis Diderot was visiting Russia on Catherine the Great's invitation. However, the Empress was alarmed that the philosopher's arguments for atheism were influencing members of her court, and so Euler was asked to confront the Frenchman. Diderot was informed that a learned mathematician had produced a proof of the existence of God: he agreed to view the proof as it was presented in court. Euler appeared, advanced toward Diderot, and in a tone of perfect conviction announced this non-sequitur: \"Sir, =\"x\", hence God exists—reply!\"\nDiderot, to whom (says the story) all mathematics was gibberish, stood dumbstruck as peals of laughter erupted from the court. Embarrassed, he asked to leave Russia, a request that was graciously granted by the Empress. However amusing the anecdote may be, it is apocryphal, given that Diderot himself did research in mathematics.\nThe legend was apparently first told by\nDieudonné Thiébault with significant embellishment by Augustus De Morgan.\n\nEuler was featured on the sixth series of the Swiss 10-franc banknote and on numerous Swiss, German, and Russian postage stamps. The asteroid 2002 Euler was named in his honor. He is also commemorated by the Lutheran Church on their Calendar of Saints on 24 May—he was a devout Christian (and believer in biblical inerrancy) who wrote apologetics and argued forcefully against the prominent atheists of his time.\n\nEuler has an extensive bibliography. His best-known books include:\n\nA definitive collection of Euler's works, entitled \"Opera Omnia\", has been published since 1911 by the Euler Commission of the Swiss Academy of Sciences. A complete chronological list of Euler's works is available at the following page: \"The Eneström Index\" (PDF).\n\n"}
{"id": "1749665", "url": "https://en.wikipedia.org/wiki?curid=1749665", "title": "List of finite simple groups", "text": "List of finite simple groups\n\nIn mathematics, the classification of finite simple groups states that every finite simple group is cyclic, or alternating, or in one of 16 families of groups of Lie type, or one of 26 sporadic groups.\n\nThe list below gives all finite simple groups, together with their order, the size of the Schur multiplier, the size of the outer automorphism group, usually some small representations, and lists of all duplicates.\n\nThe following table is a complete list of the 18 families of finite simple groups and the 26 sporadic simple groups, along with their orders. Any non-simple members of each family are listed, as well as any members duplicated within a family or between families. (In removing duplicates it is useful to note that no two finite simple groups have the same order, except that the group A = \"A\"(2) and \"A\"(4) both have order 20160, and that the group \"B\"(\"q\") has the same order as \"C\"(\"q\") for \"q\" odd, \"n\" > 2. The smallest of the latter pairs of groups are \"B\"(3) and \"C\"(3) which both have order 4585351680.)\n\nThere is an unfortunate conflict between the notations for the alternating groups A and the groups of Lie type \"A\"(\"q\"). Some authors use various different fonts for A to distinguish them. In particular,\nin this article we make the distinction by setting the alternating groups A in Roman font and the Lie-type groups \"A\"(\"q\") in italic.\n\nIn what follows, \"n\" is a positive integer, and \"q\" is a positive power of a prime number \"p\", with the restrictions noted. The notation (\"a\",\"b\") represents the greatest common divisor of the integers \"a\" and \"b\".\n\nSimplicity: Simple for \"p\" a prime number.\n\nOrder: \"p\"\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Cyclic of order \"p\" − 1.\n\nOther names: Z/\"p\"Z\n\nRemarks: These are the only simple groups that are not perfect.\n\nSimplicity: Solvable for \"n\" < 5, otherwise simple.\n\nOrder: \"n\"!/2 when \"n\" > 1.\n\nSchur multiplier: 2 for \"n\" = 5 or \"n\" > 7, 6 for \"n\" = 6 or 7; see \"Covering groups of the alternating and symmetric groups\"\n\nOuter automorphism group: In general 2. Exceptions: for \"n\" = 1, \"n\" = 2, it is trivial, and for \"n\" = 6, it has order 4 (elementary abelian).\n\nOther names: Alt.\n\nIsomorphisms: A and A are trivial. A is cyclic of order 3. A is isomorphic to \"A\"(3) (solvable). A is isomorphic to \"A\"(4) and to \"A\"(5). A is isomorphic to \"A\"(9) and to the derived group \"B\"(2)′. A is isomorphic to \"A\"(2).\n\nRemarks: An index 2 subgroup of the symmetric group of permutations of \"n\" points when \"n\" > 1.\n\nNotation: \"n\" is a positive integer, \"q\" > 1 is a power of a prime number \"p\", and is the order of some underlying finite field. The order of the outer automorphism group is written as \"d\"⋅\"f\"⋅\"g\", where \"d\" is the order of the group of \"diagonal automorphisms\", \"f\" is the order of the (cyclic) group of \"field automorphisms\" (generated by a Frobenius automorphism), and \"g\" is the order of the group of \"graph automorphisms\" (coming from automorphisms of the Dynkin diagram). The notation (\"a\",\"b\") represents the greatest common divisor of the integers \"a\" and \"b\".\n\nSimplicity: Simple for \"n\" ≥ 1. The group\n\"B\"(2) is solvable.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 2.\n\nSchur multiplier: Trivial for \"n\" ≠ 1, elementary abelian of order 4\nfor \"B\"(8).\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1.\n\nOther names: Suz(2), Sz(2).\n\nIsomorphisms: \"B\"(2) is the Frobenius group of order 20.\n\nRemarks: Suzuki group are Zassenhaus groups acting on sets of size (2) + 1, and have 4-dimensional representations over the field with 2 elements. They are the only non-cyclic simple groups whose order is not divisible by 3. They are not related to the sporadic Suzuki group.\n\nSimplicity: Simple for \"n\" ≥ 1. The derived group \"F\"(2)′ is simple of index 2\nin \"F\"(2), and is called the Tits group,\nnamed for the Belgian mathematician Jacques Tits.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 2.\n\nThe Tits group has order 17971200 = 2 ⋅ 3 ⋅ 5 ⋅ 13.\n\nSchur multiplier: Trivial for \"n\" ≥ 1 and for the Tits group.\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1. Order 2 for the Tits group.\n\nRemarks: Unlike the other simple groups of Lie type, the Tits group does not have a BN pair, though its automorphism group does so most authors count it as a sort of honorary group of Lie type.\n\nSimplicity: Simple for \"n\" ≥ 1. The group \"G\"(3) is not simple, but its derived group \"G\"(3)′ is a simple subgroup of index 3.\n\nOrder:\n\"q\"\n(\"q\" − 1),\nwhere\n\"q\" = 3\n\nSchur multiplier: Trivial for \"n\" ≥ 1 and for \"G\"(3)′.\n\nOuter automorphism group:\nwhere \"f\" = 2\"n\" + 1.\n\nOther names: Ree(3), R(3), E(3) .\n\nIsomorphisms: The derived group \"G\"(3)′ is isomorphic to \"A\"(8).\n\nRemarks: \"G\"(3) has a doubly transitive permutation representation on 3 + 1 points and acts on a 7-dimensional vector space over the field with 3 elements.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 = 44352000\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Order 2.\n\nRemarks: It acts as a rank 3 permutation group on the Higman Sims graph with 100 points, and is contained in Co and in Co.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 = 898128000\n\nSchur multiplier: Order 3.\n\nOuter automorphism group: Order 2.\n\nRemarks: Acts as a rank 3 permutation group on the McLaughlin graph with 275 points, and is contained in Co and in Co.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 17 = 4030387200\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Order 2.\n\nOther names: Held–Higman–McKay group, HHM, \"F\", HTH\n\nRemarks: Centralizes an element of order 7 in the monster group.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 13 ⋅ 29 = 145926144000\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Trivial.\n\nRemarks: The double cover acts on a 28-dimensional lattice over the Gaussian integers.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 13 = 448345497600\n\nSchur multiplier: Order 6.\n\nOuter automorphism group: Order 2.\n\nOther names: Sz\n\nRemarks: The 6 fold cover acts on a 12-dimensional lattice over the Eisenstein integers. It is not related to the Suzuki groups of Lie type.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 19 ⋅ 31 = 460815505920\n\nSchur multiplier: Order 3.\n\nOuter automorphism group: Order 2.\n\nOther names: O'Nan–Sims group, O'NS, O–S\n\nRemarks:\nThe triple cover has two 45-dimensional representations over the field with 7 elements, exchanged by an outer automorphism.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 19 = 273030912000000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Order 2.\n\nOther names: \"F\", \"D\"\n\nRemarks: Centralizes an element of order 5 in the monster group.\n\nOrder:\n2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 11 ⋅ 31 ⋅ 37 ⋅ 67 = 51765179004000000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: Lyons–Sims group, LyS\n\nRemarks: Has a 111-dimensional representation over the field with 5 elements.\n\nOrder: 2 ⋅ 3 ⋅ 5 ⋅ 7 ⋅ 13 ⋅ 19 ⋅ 31 = 90745943887872000\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\", \"E\"\n\nRemarks: Centralizes an element of order 3 in the monster, and is contained in \"E\"(3), so has a 248-dimensional representation over the field with 3 elements.\n\nOrder:\n\nSchur multiplier: Order 2.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\"\n\nRemarks: The double cover is contained in the monster group. It has a representation of dimension 4371 over the complex numbers (with no nontrivial invariant product), and a representation of dimension 4370 over the field with 2 elements preserving a commutative but non-associative product.\n\nOrder:\n\nSchur multiplier: Trivial.\n\nOuter automorphism group: Trivial.\n\nOther names: \"F\", M, Monster group, Friendly giant, Fischer's monster.\n\nRemarks: Contains all but 6 of the other sporadic groups as subquotients. Related to monstrous moonshine. The monster is the automorphism group of the 196,883-dimensional Griess algebra and the infinite-dimensional monster vertex operator algebra, and acts naturally on the monster Lie algebra.\n\n lists the 56 non-cyclic simple groups of order less than a million.\n\n\n\n"}
{"id": "51373403", "url": "https://en.wikipedia.org/wiki?curid=51373403", "title": "Louis Necker", "text": "Louis Necker\n\nLouis Necker, called de Germany (31 August 1730 in Geneva – 31 July 1804 in Cologny) was a Genevan mathematician, physicist, professor and a banker in Paris. He was the elder brother of Jacques Necker, minister of Finance in France when the French Revolution broke out. \n\nLouis Necker studied mathematics and physics at the Academy of Geneva. He finished his studies in philosophy with a thesis on electricity (1747), then graduated in law (1751). For a while he became the governor of probably Charles Christian, Prince of Nassau-Weilburg and Simon August, Count of Lippe-Detmold during their stay in Geneva and traveled with them to the University of Turin. He managed a boarding school for young English held by his father Charles Frederick, lawyer and professor of law at the Geneva Academy. He was appointed as the hofmeister of a Baron van Van Wassenaer and a Bentinck. \n\nIn 1752 he purchased 's physics laboratory and in 1757 acceded the chair of mathematics and the honorary chair of Experimental Physics of the Academy of Geneva. As a correspondent of the Académie royale des sciences he had written an article for the Encyclopedia on Friction in mechanics. \nIn 1759 he lost his wife Isabelle André, whom he had married in 1752 and came from Marseille. In 1761 he was forced to resign from his professorship after a scandal (Vernes-Necker case). \nIn 1762 with the help of his brother he was appointed in a trading house in Marseille and added to his last name de Germany, after the family estate near Rolle. He was dropped from the Académie des sciences's list of Corresponding Members in 1767.\n\nIn 1770 he moved to Paris. In 1772 he became a banker at . In 1773 he remarried. Between 1774 and 1778 he must have been very busy collecting interest for his rich and noble clients in Utrecht, the Netherlands. An astonishing number of notarial deeds are on his name. In 1776 he became resident for the Republic of Geneva, succeeding his brother. When Emmanuel Haller was appointed in the Girardot bank in 1777, Louis became a silent partner. At some time (1777?) he became a friend of Benjamin Franklin. Jacques Necker was dismissed on 19 May 1781 as controller of the royal treasury. It seems the brothers were still cooperating as Jacques and Louis received annually 8 million livres as a pension. \n\nAs a result of changes during the liberal phase of the French Revolution, he thought it prudent to return to his homeland in 1791. The disgrace of his younger brother Jacques, who resigned in 1790, contributed to his decision. The Neckers were far from welcome in Geneva. Many of the French émigrés considered them Jacobins, and many of the Swiss Jacobins thought them conservative.\n\nHis son Jacques (1757-1825), who had joined the French army, married Albertine Necker de Saussure in 1785. The French Revolution ended his military career. In 1790, he began teaching as a demonstrator in botany at the Academy of Geneva as Professor of Botany.\n\n\n"}
{"id": "5756885", "url": "https://en.wikipedia.org/wiki?curid=5756885", "title": "Luzin N property", "text": "Luzin N property\n\nIn mathematics, a function \"f\" on the interval [\"a\", \"b\"] has the Luzin N property, named after Nikolai Luzin (also called Luzin property or N property) if for all formula_1 such that formula_2, there holds: formula_3, where formula_4 stands for the Lebesgue measure.\n\nNote that the image of such a set \"N\" is not necessarily measurable, but since the Lebesgue measure is complete, it follows that if the Lebesgue outer measure of that set is zero, then it is measurable and its Lebesgue measure is zero as well.\n\nEvery absolutely continuous function has the Luzin N property. The Cantor function on the other hand does not: the Lebesgue measure of the Cantor set is zero, however its image is the complete [0,1] interval.\n\nAlso, if a function \"f\" on the interval [\"a\",\"b\"] is continuous, is of bounded variation and has the Luzin N property, then it is absolutely continuous.\n\n"}
{"id": "50947627", "url": "https://en.wikipedia.org/wiki?curid=50947627", "title": "Map graph", "text": "Map graph\n\nIn graph theory, a branch of mathematics, a map graph is an undirected graph formed as the intersection graph of finitely many simply connected and internally disjoint regions of the Euclidean plane. The map graphs include the planar graphs, but are more general. Any number of regions can meet at a common corner (as in the Four Corners of the United States, where four states meet), and when they do the map graph will contain a clique connecting the corresponding vertices, unlike planar graphs in which the largest cliques have only four vertices. Another example of a map graph is the king's graph, a map graph of the squares of the chessboard connecting pairs of squares between which the chess king can move.\n\nMap graphs can be represented combinatorially as the \"half-squares of planar bipartite graphs\". That is, let be a planar bipartite graph, with bipartition . The square of is another graph on the same vertex set, in which two vertices are adjacent in the square when they are at most two steps apart in . The half-square or bipartite half is the induced subgraph of one side of the bipartition (say ) in the square graph: its vertex set is and it has an edge between each two vertices in that are two steps apart in . The half-square is a map graph. It can be represented geometrically by finding a planar embedding of , and expanding each vertex of and its adjacent edges into a star-shaped region, so that these regions touch at the vertices of . Conversely, every map graph can be represented as a half-square in this way.\n\nMap graphs can be recognized in polynomial time. However, the high exponent of the known polynomial time algorithm for this problem makes it impractical. The maximum independent set problem has a polynomial-time approximation scheme for map graphs, and the chromatic number can be approximated to within a factor of two in polynomial time. The theory of bidimensionality leads to many other approximation algorithms and fixed-parameter tractable algorithms for optimization problems on map graphs.\n\nA -map graph is a map graph derived from a set of regions in which at most regions meet at any point. Equivalently, it is the half-square of a planar bipartite graph in which the vertex set (the side of the bipartition not used to induce the half-square) has maximum degree . A 3-map graph is a planar graph, and every planar graph can be represented as a 3-map graph. Every 4-map graph is a 1-planar graph, a graph that can be drawn with at most one crossing per edge, and every optimal 1-planar graph (a graph formed from a planar quadrangulation by adding two crossing diagonals to every quadrilateral face) is a 4-map graph. However, some other 1-planar graphs are not map graphs, because (unlike map graphs) they include crossing edges that are not part of a four-vertex complete subgraph.\n"}
{"id": "19823", "url": "https://en.wikipedia.org/wiki?curid=19823", "title": "Maya numerals", "text": "Maya numerals\n\nThe Mayan numeral system was the system to represent numbers and calendar dates in the Maya civilization. It was a vigesimal (base-20) positional numeral system. The numerals are made up of three symbols; zero (shell shape, with the plastron uppermost), one (a dot) and five (a bar). For example, thirteen is written as three dots in a horizontal row above two horizontal bars; sometimes it is also written as three vertical dots to the left of two vertical bars. With these three symbols each of the twenty vigesimal digits could be written.\nNumbers after 19 were written vertically in powers of twenty. The Mayan used powers of twenty, just as our Hindu–Arabic numeral system uses powers of tens. For example, thirty-three would be written as one dot, above three dots atop two bars. The first dot represents \"one twenty\" or \"1×20\", which is added to three dots and two bars, or thirteen. Therefore, (1×20) + 13 = 33. Upon reaching 20 or 400, another row is started (20 or 8000, then 20 or 160,000, and so on). The number 429 would be written as one dot above one dot above four dots and a bar, or (1×20) + (1×20) + 9 = 429. \n\nOther than the bar and dot notation, Maya numerals were sometimes illustrated by face type glyphs or pictures. The face glyph for a number represents the deity associated with the number. These face number glyphs were rarely used, and are mostly seen on some of the most elaborate monumental carving.\n\nAdding and subtracting numbers below 20 using Maya numerals is very simple.\nAddition is performed by combining the numeric symbols at each level:<br>\nIf five or more dots result from the combination, five dots are removed and replaced by a bar. If four or more bars result, four bars are removed and a dot is added to the next higher row.\n\nSimilarly with subtraction, remove the elements of the subtrahend symbol from the minuend symbol:<br>\nIf there are not enough dots in a minuend position, a bar is replaced by five dots. If there are not enough bars, a dot is removed from the next higher minuend symbol in the column and four bars are added to the minuend symbol which is being worked on.\n\nThe \"Long Count\" portion of the Maya calendar uses a variation on the strictly vigesimal numbering. In the second position, only the digits up to 17 are used, and the place value of the third position is not 20×20 = 400, as would otherwise be expected, but 18×20 = 360, so that one dot over two zeros signifies 360. Presumably, this is because 360 is roughly the number of days in a year. (The Maya had however a quite accurate estimation of 365.2422 days for the solar year at least since the early Classic era.) Subsequent positions use all twenty digits and the place values continue as 18×20×20 = 7,200 and 18×20×20×20 = 144,000, etc.\n\nEvery known example of large numbers in the Maya system uses this 'modified vigesimal' system, with the third position representing multiples of 18×20. It is reasonable to assume, but not proven by any evidence, that the normal system in use was a pure base-20 system.\n\nSeveral Mesoamerican cultures used similar numerals and base-twenty systems and the Mesoamerican Long Count calendar requiring the use of zero as a place-holder. The earliest long count date (on Stela 2 at Chiapa de Corzo, Chiapas) is from 36 BC.\n\nSince the eight earliest Long Count dates appear outside the Maya homeland, it is assumed that the use of zero and the Long Count calendar predated the Maya, and was possibly the invention of the Olmec. Indeed, many of the earliest Long Count dates were found within the Olmec heartland. However, the Olmec civilization had come to an end by the 4th century BC, several centuries before the earliest known Long Count dates—which suggests that zero was \"not\" an Olmec discovery.\n\nMayan numerals were added to the Unicode Standard in June, 2018 with the release of version 11.0.\n\nThe Unicode block for Mayan Numerals is U+1D2E0–U+1D2FF:\n\n\n"}
{"id": "363890", "url": "https://en.wikipedia.org/wiki?curid=363890", "title": "One-way function", "text": "One-way function\n\nIn computer science, a one-way function is a function that is easy to compute on every input, but hard to invert given the image of a random input. Here, \"easy\" and \"hard\" are to be understood in the sense of computational complexity theory, specifically the theory of polynomial time problems. Not being one-to-one is not considered sufficient of a function for it to be called one-way (see Theoretical definition, below).\n\nThe existence of such one-way functions is still an open conjecture. In fact, their existence would prove that the complexity classes P and NP are not equal, thus resolving the foremost unsolved question of theoretical computer science. The converse is not known to be true, i.e. the existence of a proof that P and NP are not equal would not directly imply the existence of one-way functions.\n\nIn applied contexts, the terms \"easy\" and \"hard\" are usually interpreted relative to some specific computing entity; typically \"cheap enough for the legitimate users\" and \"prohibitively expensive for any malicious agents\". One-way functions, in this sense, are fundamental tools for cryptography, personal identification, authentication, and other data security applications. While the existence of one-way functions in this sense is also an open conjecture, there are several candidates that have withstood decades of intense scrutiny. Some of them are essential ingredients of most telecommunications, e-commerce, and e-banking systems around the world.\n\nA function \"f\" : {0,1} → {0,1} is one-way if \"f\" can be computed by a polynomial time algorithm, but any polynomial time randomized algorithm formula_1 that attempts to compute a pseudo-inverse for \"f\" succeeds with negligible probability. That is, for all randomized algorithms formula_1 , all positive integers \"c\" and all sufficiently large \"n\" = length(\"x\") ,\n\nwhere the probability is over the choice of \"x\" from the discrete uniform distribution on {0,1}, and the randomness of formula_1.\n\nNote that, by this definition, the function must be \"hard to invert\" in the average-case, rather than worst-case sense. This is different from much of complexity theory (e.g., NP-hardness), where the term \"hard\" is meant in the worst-case. That is why even if some candidates for one-way functions (described below) are known to be NP-complete, it does not imply their one-wayness. The latter property is only based on the lack of known algorithm to solve the problem.\n\nIt is not sufficient to make a function \"lossy\" (not one-to-one) to have a one-way function. In particular, the function that outputs the string of \"n\" zeros on any input of length \"n\" is \"not\" a one-way function because it is easy to come up with an input that will result in the same output. More precisely: For such a function that simply outputs a string of zeroes, an algorithm \"F\" that just outputs any string of length \"n\" on input \"f\"(\"x\") will \"find\" a proper preimage of the output, even if it is not the input which was originally used to find the output string.\n\nA one-way permutation is a one-way function that is also a permutation—that is, a one-way function that is bijective. One-way permutations are an important cryptographic primitive, and it is not known if their existence is implied by the existence of one-way functions.\n\nA trapdoor one-way function or trapdoor permutation is a special kind of one-way function. Such a function is hard to invert unless some secret information, called the \"trapdoor\", is known.\n\nA collision-free hash function \"f\" is a one-way function that is also \"collision-resistant\"; that is, no randomized polynomial time algorithm can find a collision—distinct values \"x\", \"y\" such that \"f\"(\"x\") = \"f\"(\"y\")—with non-negligible probability.\n\nIf \"f\" is a one-way function, then the inversion of \"f\" would be a problem whose output is hard to compute (by definition) but easy to check (just by computing \"f\" on it). Thus, the existence of a one-way function implies that FP≠FNP, which in turn implies that P≠NP. However, it is not known whether P≠NP implies the existence of one-way functions.\n\nThe existence of a one-way function implies the existence of many other useful concepts, including:\n\n\nThe existence of one-way functions also implies that there is no natural proof for P≠NP.\n\nThe following are several candidates for one-way functions (as of April 2009). Clearly, it is not known whether\nthese functions are indeed one-way; but extensive research has so far failed to produce an efficient inverting algorithm for any of them.\n\nThe function \"f\" takes as inputs two prime numbers \"p\" and \"q\" in binary notation and returns their product. This function can be \"easily\" computed in \"O\"(\"b\") time, where \"b\" is the total number of bits of the inputs. Inverting this function requires finding the factors of a given integer \"N\". The best factoring algorithms known run in formula_5time, where b is the number of bits needed to represent \"N\".\n\nThis function can be generalized by allowing \"p\" and \"q\" to range over a suitable set of semiprimes. Note that \"f\" is not one-way for randomly selected integers \"p,q\">1, since the product will have 2 as a factor with probability 3/4 (because the probability that an arbitrary \"p\" is odd is 1/2, and likewise for \"q\", so if they're chosen independently, the probability that both are odd is therefore 1/4; hence the probability that p or q is even is 1 - 1/4 = 3/4).\n\nThe Rabin function, or squaring modulo formula_6, where and are primes is believed to be a collection of one-way functions. We write\nto denote squaring modulo : a specific member of the Rabin collection. It can be shown that extracting square roots, i.e. inverting the Rabin function, is computationally equivalent to factoring (in the sense of polynomial-time reduction). Hence it can be proven that the Rabin collection is one-way if and only if factoring is hard. This also holds for the special case in which and are of the same bit length. The Rabin cryptosystem is based on the assumption that this Rabin function is one-way.\n\nModular exponentiation can be done in polynomial time. Inverting this function requires computing the discrete logarithm. Currently there are several popular groups for which no known algorithm to calculate the underlying discrete logarithm in polynomial time is known. These groups are all finite abelian groups and the general discrete logarithm problem can be described as thus.\n\nLet \"G\" be a finite abelian group of cardinality \"n\". Denote its group operation by multiplication. Consider a primitive element α ∈ \"G\" and another element β ∈ \"G\". The discrete logarithm problem is to find the positive integer \"k\", where 1 ≤ \"k\" ≤ n, such that:\n\nThe integer \"k\" that solves the equation is termed the discrete logarithm of β to the base α. One writes \"k\" = log β.\n\nPopular choices for the group \"G\" in discrete logarithm cryptography are the cyclic groups (Z) (e.g. ElGamal encryption, Diffie–Hellman key exchange, and the Digital Signature Algorithm) and cyclic subgroups of elliptic curves over finite fields (\"see\" elliptic curve cryptography).\n\nAn elliptic curve is a set of pairs of elements of a field satisfying \"y\" = \"x\" + \"ax\" + \"b\". The elements of the curve form a group under an operation called \"point addition\" (which is not the same as the addition operation of the field). Multiplication \"kP\" of a point \"P\" by an integer \"k\" (\"i.e.\", a group action of the additive group of the integers) is defined as repeated addition of the point to itself. If \"k\" and \"P\" are known, it is easy to compute \"R\" = \"kP\", but if only \"R\" and \"P\" are known, it is assumed to be hard to compute \"k\".\n\nThere are a number of cryptographic hash functions that are fast to compute, such as SHA 256. Some of the simpler versions have fallen to sophisticated analysis, but the strongest versions continue to offer fast, practical solutions for one-way computation. Most of the theoretical support for the functions are more techniques for thwarting some of the previously successful attacks.\n\nOther candidates for one-way functions have been based on the hardness of the decoding of random linear codes, the subset sum problem (Naccache-Stern knapsack cryptosystem), or other problems.\n\nThere is an explicit function \"f\" that has been proved to be one-way, if and only if one-way functions exist. In other words, if any function is one-way, then so is \"f\". Since this function was the first combinatorial complete one-way function to be demonstrated, it is known as the \"universal one-way function\". The problem of finding a one way function is thus reduced to proving that one such function exists.\n\n\n"}
{"id": "358196", "url": "https://en.wikipedia.org/wiki?curid=358196", "title": "Optimal solutions for Rubik's Cube", "text": "Optimal solutions for Rubik's Cube\n\nOptimal solutions for Rubik's Cube refer to solutions that are the shortest. There are two common ways to measure the length of a solution. The first is to count the number of quarter turns. The second is to count the number of outer-layer twists, called \"face turns\". A move to turn an outer layer two quarter (90°) turns in the same direction would be counted as two moves in the quarter turn metric (QTM), but as one turn in the face metric (FTM, or HTM \"Half Turn Metric\", or OBTM \"Outer Block Turn Metric\").\n\nThe maximum number of face turns needed to solve any instance of the Rubik's Cube is 20, and the maximum number of quarter turns is 26. These numbers are also the diameters of the corresponding Cayley graphs of the Rubik's Cube group. In STM (slice turn metric) it's unknown.\n\nThere are many algorithms to solve scrambled Rubik's Cubes. An algorithm that solves a cube in the minimum number of moves is known as God's algorithm.\n\nTo denote a sequence of moves on the 3×3×3 Rubik's Cube, this article uses \"Singmaster notation\", which was developed by David Singmaster. \n\nThe letters L, R, F, B, U, and D indicate a clockwise quarter turn of the left, right, front, back, up, and down face respectively. Half turns are indicated by appending a 2. A counterclockwise quarter turn is indicated by appending a prime symbol ( ′ ).\n\nThe letters M, S and E are used to denote the turning of a middle layer. M represents turning the layer between the R and L faces 1 quarter turn top to bottom. S represents turning the layer between the F and B faces 1 quarter turn clockwise, as seen from the front. E represents turning the layer between the U and D faces 1 quarter turn clockwise left to right. As with regular turns, a 2 signifies a double turn and a prime (') indicates a quarter turn anticlockwise.\n\nThe letters X, Y and Z are used to signify cube rotations. X signifies the rotation of the cube forwards 90 degrees. Y signifies the rotation of the cube to the left 90 degrees. Z signifies the rotation of the cube clockwise 90 degrees. These cube rotations are used in algorithms to the algorithms smoother and faster. As with regular turns, a 2 signifies a half rotation and a prime (') indicates a quarter rotation in the opposite direction. Note that these letters are usually lowercase instead.\n\nIt can be proven by counting arguments that there exist positions needing at least 18 moves to solve. To show this, first count the number of cube positions that exist in total, then count the number of positions achievable using at most 17 moves starting from a solved cube. It turns out that the latter number is smaller. \n\nThis argument was not improved upon for many years. Also, it is not a constructive proof: it does not exhibit a concrete position that needs this many moves. It was conjectured that the so-called superflip would be a position that is very difficult. A Rubik's Cube is in the superflip pattern when each corner piece is in the correct position, but each edge piece is incorrectly oriented. In 1992, a solution for the superflip with 20 face turns was found by Dik T. Winter, of which the minimality was shown in 1995 by Michael Reid, providing a new lower bound for the diameter of the cube group. Also in 1995, a solution for superflip in 24 quarter turns was found by Michael Reid, with its minimality proven by Jerry Bryan. In 1998, a new position requiring more than 24 quarter turns to solve was found. The position, which was called a 'superflip composed with four spot' needs 26 quarter turns.\n\nThe first upper bounds were based on the 'human' algorithms. By combining the worst-case scenarios for each part of these algorithms, the typical upper bound was found to be around 100.\n\nPerhaps the first concrete value for an upper bound was the 277 moves mentioned by David Singmaster in early 1979. He simply counted the maximum number of moves required by his cube-solving algorithm. Later, Singmaster reported that Elwyn Berlekamp, John Conway, and Richard Guy had come up with a different algorithm that took at most 160 moves. Soon after, Conway’s Cambridge Cubists reported that the cube could be restored in at most 94 moves.\n\nThe breakthrough, known as \"descent through nested sub-groups\" was found by Morwen Thistlethwaite; details of Thistlethwaite's algorithm were published in \"Scientific American\" in 1981 by Douglas Hofstadter. The approaches to the cube that led to algorithms with very few moves are based on group theory and on extensive computer searches. Thistlethwaite's idea was to divide the problem into subproblems. Where algorithms up to that point divided the problem by looking at the parts of the cube that should remain fixed, he divided it by restricting the type of moves that could be executed. In particular he divided the cube group into the following chain of subgroups:\n\n\nNext he prepared tables for each of the right coset spaces formula_6. For each element he found a sequence of moves that took it to the next smaller group. After these preparations he worked as follows. A random cube is in the general cube group formula_7. Next he found this element in the right coset space formula_8. He applied the corresponding process to the cube. This took it to a cube in formula_9. Next he looked up a process that takes the cube to formula_10, next to formula_11 and finally to formula_12.\nAlthough the whole cube group formula_7 is very large (~4.3×10), the right coset spaces formula_14 and formula_11 are much smaller.\nThe coset space formula_16 is the largest and contains only 1082565 elements. The number of moves required by this algorithm is the sum of the largest process in each step.\n\nInitially, Thistlethwaite showed that any configuration could be solved in at most 85 moves. In January 1980 he improved his strategy to yield a maximum of 80 moves. Later that same year, he reduced the number to 63, and then again to 52. By exhaustively searching the coset spaces it was later found that the best possible number of moves for each stage was 7, 10, 13, and 15 giving a total of 45 moves at most.\n\nThistlethwaite's algorithm was improved by Herbert Kociemba in 1992. He reduced the number of intermediate groups to only two:\n\nAs with Thistlethwaite's Algorithm, he would search through the right coset space formula_8 to take the cube to group formula_9. Next he searched the optimal solution for group formula_9. The searches in formula_8 and formula_9 were both done with a method equivalent to IDA*. The search in formula_8 needs at most 12 moves and the search in formula_9 at most 18 moves, as Michael Reid showed in 1995. By also generating suboptimal solutions that take the cube to group formula_9 and looking for short solutions in formula_9, much shorter overall solutions are usually obtained. Using this algorithm solutions are typically found of fewer than 21 moves, though there is no proof that it will always do so.\n\nIn 1995 Michael Reid proved that using these two groups every position can be solved in at most 29 face turns, or in 42 quarter turns. This result was improved by Silviu Radu in 2005 to 40.\n\nAt first glance, this algorithm appears to be impractically inefficient - If formula_7contains 18 possible moves (each move, its prime, and its 180 degree rotation), that leaves formula_30(Over 1 quadrillion) cube states to be searched. Even with a heuristic-based computer algorithm like IDA*, which may narrow it down considerably, searching through that many states is likely not practical. To solve this problem, Kociemba devised a lookup table that provides an exact heuristic for formula_7. When the exact number of moves needed to reach formula_9is available, the search becomes virtually instantaneous - one need only generate 18 cube state for each of the 12 moves and choose the one with the lowest heuristic each time. This allows the second heuristic, that for formula_9, to be less precise and still allow for a solution to be computed in reasonable time on a modern computer.\n\nUsing these group solutions combined with computer searches will generally quickly give very short solutions. But these solutions do not always come with a guarantee of their minimality. To search specifically for minimal solutions a new approach was needed.\n\nIn 1997 Richard Korf announced an algorithm with which he had optimally solved random instances of the cube. Of the ten random cubes he did, none required more than 18 face turns. The method he used is called IDA* and is described in his paper \"Finding Optimal Solutions to Rubik's Cube Using Pattern Databases\". Korf describes this method as follows\n\nIt works roughly as follows. First he identified a number of subproblems that are small enough to be solved optimally. He used:\n\n\nClearly the number of moves required to solve any of these subproblems is a lower bound for the number of moves needed to solve the entire cube.\n\nGiven a random cube C, it is solved as iterative deepening. First all cubes are generated that are the result of applying 1 move to them. That is C * F, C * U, … Next, from this list, all cubes are generated that are the result of applying two moves. Then three moves and so on. If at any point a cube is found that needs too many moves based on the upper bounds to still be optimal it can be eliminated from the list.\n\nAlthough this algorithm will always find optimal solutions, there is no worst case analysis. It is not known how many moves this algorithm might need. An implementation of this algorithm can be found here.\n\nIn 2006, Silviu Radu further improved his methods to prove that every position can be solved in at most 27 face turns or 35 quarter turns. Daniel Kunkle and Gene Cooperman in 2007 used a supercomputer to show that all unsolved cubes can be solved in no more than 26 moves (in face-turn metric). Instead of attempting to solve each of the billions of variations explicitly, the computer was programmed to bring the cube to one of 15,752 states, each of which could be solved within a few extra moves. All were proved solvable in 29 moves, with most solvable in 26. Those that could not initially be solved in 26 moves were then solved explicitly, and shown that they too could be solved in 26 moves.\n\nTomas Rokicki reported in a 2008 computational proof that all unsolved cubes could be solved in 25 moves or fewer. This was later reduced to 23 moves. In August 2008 Rokicki announced that he had a proof for 22 moves.\n\nFinally, in 2010, Tomas Rokicki, Herbert Kociemba, Morley Davidson, and John Dethridge gave the final computer-assisted proof that all cube positions could be solved with a maximum of 20 face turns.\nIn 2009, Tomas Rokicki proved that 29 moves in the quarter-turn metric is enough to solve any scrambled cube. And in 2014, Tomas Rokicki and Morley Davidson proved that the maximum number of quarter-turns needed to solve the cube is 26.\n\nThe face-turn and quarter-turn metrics differ in the nature of their antipodes.\nAn antipode is a scrambled cube that is maximally far from solved, one that requires the maximum number of moves to solve. In the half-turn metric with a maximum number of 20, there are hundreds of millions of such positions. In the quarter-turn metric, only a single position (and its two rotations) is known that requires the maximum of 26 moves. Despite significant effort, no additional quarter-turn distance-26 positions have been found. Even at distance 25, only two positions (and their rotations) are known to exist. At distance 24, perhaps 150,000 positions exist.\n\n"}
{"id": "10217078", "url": "https://en.wikipedia.org/wiki?curid=10217078", "title": "Oskar Bolza", "text": "Oskar Bolza\n\nOskar Bolza (12 May 1857 – 5 July 1942) was a German mathematician, and student of Felix Klein. He was born in Bad Bergzabern, Palatinate, then a district of Bavaria, known for his research in the calculus of variations, particularly influenced by Karl Weierstrass' 1879 lectures on the subject.\n\nBolza entered the University of Berlin in 1875. His first interest was linguistics, then he studied physics with Kirchhoff and Helmholtz, but experimental work did not attract him, so he decided on mathematics in 1878. The years 1878–1881 were spent studying under Elwin Christoffel and Theodor Reye at Strasbourg, Hermann Schwarz at Göttingen, and particularly Karl Weierstrass in Berlin.\n\nIn the spring of 1888 he landed in Hoboken, NJ, searching for a job in the United States: he succeeded in finding a position in 1889 at Johns Hopkins University and then at the then newly founded Clark University. In 1892 Bolza joined the University of Chicago and worked there up to 1910 when, after becoming unhappy in the United States as a consequence of the death of his friend Heinrich Maschke in 1908, he and his wife returned to Freiburg in Germany. The events of World War I greatly affected Bolza and, after 1914, he stopped his research in mathematics. He became interested in religious psychology, languages (particularly Sanskrit), and Indian religions. He published the book \"Glaubenlose Religion\" (religion without belief) in 1930 under the pseudonym F. H. Marneck. However, later in his life he returned to do research in mathematics, lecturing at University of Freiburg from 1929 up to his retirement in 1933.\n\nHe completed his doctoral studies, after eight years of study and many changes of direction, in 1886, from the Georg-August-Universität Göttingen. He wrote his thesis, titled \"Über die Reduction hyperelliptischer Integrale erster Ordnung und erster Gattung auf elliptische, insbesondere über die Reduction durch eine Transformation vierten Grades\" (translated as \"On reduction of hyperelliptic integrals of first order and first kind to elliptic integrals, especially on reduction by transformation of fourth degree\") under the supervision of Felix Klein.\n\nIn 1889 Bolza worked at Johns Hopkins University, where Simon Newcomb gave him a temporary short-term appointment \"reader in mathematics\", then he obtained a position as an associate professor at Clark University. While at Clark, Bolza published the important paper \"On the theory of substitution groups and its application to algebraic equations\" in the American Journal of Mathematics. In 1892 Bolza joined the University of Chicago and worked there up to 1910, when he decided to return to Freiburg in Germany: he was appointed there as honorary professor, while the University Chicago awarded him the title of \"non-resident professor of mathematics\" which he retained for the rest of his life.\n\nBolza published \"The elliptic s-functions considered as a special case of the hyperelliptic s-functions\" in 1900 which related to work he had been studying for his doctorate under Klein. However, he worked on the calculus of variations from 1901. Papers which appeared in the Transactions of the American Mathematical Society over the next few years were: \"New proof of a theorem of Osgood's in the calculus of variations\" (1901); \"Proof of the sufficiency of Jacobi's condition for a permanent sign of the second variation in the so-called isoperimetric problems\" (1902); \"Weierstrass' theorem and Kneser's theorem on transversals for the most general case of an extremum of a simple definite integral\" (1906); and \"Existence proof for a field of extremals tangent to a given curve\" (1907). His text \"Lectures on the Calculus of Variations\" published by the University of Chicago Press in 1904, became a classic in its field and was republished several times: the augmented German edition of the same work was considered by his former student Gilbert Ames Bliss \"a classic, indispensable to every scholar in the field, and much wider in its scope than his earlier book\".\n\nImmediately after his return to Germany Bolza continued teaching and research, in particular on function theory, integral equations and the calculus of variations. Two papers of 1913 and 1914 are particularly important. The first \"Problem mit gemischten Bedingungen und variablen Endpunkten\" formulated a new type of variational problem now called \"the Bolza problem of Bolza\"\" after him and the second studied variations for an integral problem involving inequalities. This latter work was to become important in control theory. Bolza returned to Chicago for part of 1913 giving lecturers during the summer on function theory and integral equations.\n\nBolza joined the University of Chicago in 1892. Between 1892 and 1910 the mathematics department was outstandingly successful with thirty-nine students graduating with doctorates (nine of them students of Bolza). These included Leonard Dickson, who was the first to be awarded a Ph.D. in mathematics by the University of Chicago, Gilbert Bliss, Oswald Veblen, Robert Lee Moore, George D. Birkhoff, and T. H. Hildebrandt.\n\n\n\n\n\n"}
{"id": "485457", "url": "https://en.wikipedia.org/wiki?curid=485457", "title": "Periodogram", "text": "Periodogram\n\nIn signal processing, a periodogram is an estimate of the spectral density of a signal.  The term was coined by Arthur Schuster in 1898. Today, the periodogram is a component of more sophisticated methods (see spectral estimation). It is the most common tool for examining the amplitude vs frequency characteristics of FIR filters and window functions. FFT spectrum analyzers are also implemented as a time-sequence of periodograms.\n\nThere are at least two different definitions in use today. One of them involves time-averaging, and one does not. Time-averaging is also the purview of other articles (Bartlett's method and Welch's method). This article is not about time-averaging. The definition of interest here is that the power spectral density of a continuous function, formula_1  is the Fourier transform of its auto-correlation function (see Cross-correlation theorem):\n\nFor sufficiently small values of parameter T, an arbitrarily-accurate approximation for X(f) can be observed in the region  formula_3  of the function:\n\nwhich is precisely determined by the samples x(nT) that span the non-zero duration of x(t)  (see Discrete-time Fourier transform).\n\nAnd for sufficiently large values of parameter N,  formula_5 can be evaluated at an arbitrarily close frequency by a summation of the form:\n\nwhere \"k\" is an integer. The periodicity of  formula_7  allows this to be written very simply in terms of a Discrete Fourier transform:\n\nwhere \"x\" is a periodic summation:  formula_9\n\nWhen evaluated for all integers, k, between 0 and N-1, the array:\n\nis a \"periodogram\".\n\nThe parameter N is known to Matlab and Octave users as NFFT.  When a periodogram is used to examine the detailed characteristics of an FIR filter or window function, N is chosen to be several multiples of the non-zero duration of the x[n] sequence, which is called \"zero-padding\" (see Sampling the DTFT). And when it is used to implement a filter bank, N is several sub-multiples of the non-zero duration of the x[n] sequence (see Sampling the DTFT).\n\nOne of the periodogram's deficiencies is that the variance at a given frequency does not decrease as the number of samples used in the computation increases. It does not provide the averaging needed to analyze noiselike signals or even sinusoids at low signal-to-noise ratios. Window functions and filter impulse responses are noiseless, but many other signals require more sophisticated methods of spectral estimation. Two of the alternatives use periodograms as part of the process:\n\nPeriodogram-based techniques introduce small biases that are unacceptable in some applications. Other techniques that do not rely on periodograms are presented in the spectral density estimation article.\n\n\n"}
{"id": "53675503", "url": "https://en.wikipedia.org/wiki?curid=53675503", "title": "Photomath", "text": "Photomath\n\nPhotomath is a mobile application described as a \"camera calculator\", which utilizes a phone's camera to recognise mathematical equations and to display the step-by-step solution onscreen. It is available for free on Google Android and iOS. The application was released in 2014 by Microblink, a company based in Zagreb, Croatia, with a registered office in London, United Kingdom. The company specializes in text recognition software.\n\nAs of 2016, apart from printed texts, it also recognizes handwriting and provides steps for the mathematical equation.\n\nSince 2017, Photomath has been operating as a separate company.\n\nIt was included on the list of top 20 best teaching and learning apps placing third. It received both praise and criticism from teachers.\n\nIt was downloaded between 10 and 50 million times on Google Play.\n\n"}
{"id": "34974588", "url": "https://en.wikipedia.org/wiki?curid=34974588", "title": "Planar Fourier capture array", "text": "Planar Fourier capture array\n\nA planar Fourier capture array (PFCA) is a tiny camera that requires no mirror, lens, focal length, or moving parts. It is composed of angle-sensitive pixels, which can be manufactured in unmodified CMOS processes.\n\nAngle-sensitive pixels have a sensitivity to light that is sinusoidal in incident angle along the optically-sensitive axis, which can be interpreted as measuring one component of the 2D Fourier transform of the far-away scene. By making them all unique, each sensor of the PFCA relates a distinct component of the 2D Fourier transform of the far-away scene, and together they relate full Fourier information. Original images are reconstructed computationally after acquisition, or if raw Fourier coefficients are more useful for the application at hand, they are used directly.\n\nPFCAs do not perform an exact Fourier transform since outputs are real-valued and are not perfect sinusoidal transforms of the image. The transform is closer to a Hartley transform, but even this correspondence is not exact. Still, the mathematics underlying completeness of the Fourier transform are useful in designing and understanding PFCAs.\n\nBecause PFCAs do not require focusing optics or moving parts, they can be made smaller than the smallest focusing camera. Counting only the active portions of the PFCA (and not the structural substrate giving it physical robustness), PFCAs are a factor of 10 smaller than the smallest focusing camera by volume.\n\n"}
{"id": "2120001", "url": "https://en.wikipedia.org/wiki?curid=2120001", "title": "Retract", "text": "Retract\n\nIn topology, a branch of mathematics, a retraction is a continuous mapping from a topological space into a subspace which preserves the position of all points in that subspace. A deformation retraction is a mapping which captures the idea of \"continuously shrinking\" a space into a subspace.\n\nAn absolute neighborhood retract (ANR) is a particularly well-behaved type of topological space. For example, every topological manifold is an ANR. Every ANR has the homotopy type of a very simple topological space, a CW complex.\n\nLet \"X\" be a topological space and \"A\" a subspace of \"X\". Then a continuous map\n\nis a retraction if the restriction of \"r\" to \"A\" is the identity map on \"A\"; that is, formula_2 for all \"a\" in \"A\". Equivalently, denoting by\n\nthe inclusion, a retraction is a continuous map \"r\" such that\nthat is, the composition of \"r\" with the inclusion is the identity of \"A\". Note that, by definition, a retraction maps \"X\" onto \"A\". A subspace \"A\" is called a retract of \"X\" if such a retraction exists. For instance, any non-empty space retracts to a point in the obvious way (the constant map yields a retraction). If \"X\" is Hausdorff, then \"A\" must be a closed subset of \"X\".\n\nIf formula_5 is a retraction, then the composition ι∘\"r\" is an idempotent continuous map from \"X\" to \"X\". Conversely, given any idempotent continuous map formula_6 we obtain a retraction onto the image of \"s\" by restricting the codomain.\n\nA continuous map\nis a \"deformation retraction\" of a space \"X\" onto a subspace \"A\" if, for every \"x\" in \"X\" and \"a\" in \"A\",\nIn other words, a deformation retraction is a homotopy between a retraction and the identity map on \"X\". The subspace \"A\" is called a deformation retract of \"X\". A deformation retraction is a special case of a homotopy equivalence.\n\nA retract need not be a deformation retract. For instance, having a single point as a deformation retract of a space \"X\" would imply that \"X\" is path connected (and in fact that \"X\" is contractible).\n\n\"Note:\" An equivalent definition of deformation retraction is the following. A continuous map formula_5 is a deformation retraction if it is a retraction and its composition with the inclusion is homotopic to the identity map on \"X\". In this formulation, a deformation retraction carries with it a homotopy between the identity map on \"X\" and itself.\n\nIf, in the definition of a deformation retraction, we add the requirement that\nfor all \"t\" in [0, 1] and \"a\" in \"A\", then \"F\" is called a strong deformation retraction. In other words, a strong deformation retraction leaves points in \"A\" fixed throughout the homotopy. (Some authors, such as Hatcher, take this as the definition of deformation retraction.)\n\nAs an example, the \"n\"-sphere \"formula_11\" is a strong deformation retract of formula_12 as strong deformation retraction one can choose the map \n\nA map \"f\": \"A\" → \"X\" of topological spaces is a (Hurewicz) cofibration if it has the homotopy extension property for maps to any space. This is one of the central concepts of homotopy theory. A cofibration \"f\" is always injective, in fact a homeomorphism to its image. If \"X\" is Hausdorff (or a compactly generated weak Hausdorff space), then the image of a cofibration \"f\" is closed in \"X\".\n\nAmong all closed inclusions, cofibrations can be characterized as follows. The inclusion of a closed subspace \"A\" in a space \"X\" is a cofibration if and only if \"A\" is a neighborhood deformation retract of \"X\", meaning that there is a continuous map formula_14 (where formula_15) with formula_16 and a homotopy formula_17such that formula_18formula_19and formula_20.\n\nFor example, the inclusion of a subcomplex in a CW complex is a cofibration.\n\n\nThe boundary of the \"n\"-dimensional ball, that is, the (\"n\"−1)-sphere, is not a retract of the ball. (See .)\n\nA closed subset formula_25 of a topological space formula_26 is called a neighborhood retract of formula_26 if formula_25 is a retract of some open subset of formula_26 that contains formula_25.\n\nLet formula_31 be a class of topological spaces, closed under homeomorphisms and passage to closed subsets. Following Borsuk (starting in 1931), a space \"formula_25\" is called an absolute retract for the class formula_31, written formula_34 if \"formula_25\" is in formula_31 and whenever \"formula_25\" is a closed subset of a space formula_26in formula_31, formula_25 is a retract of formula_26. A space formula_25 is an absolute neighborhood retract for the class formula_31, written formula_44 if formula_25 is in formula_31 and whenever formula_25 is a closed subset of a space formula_26 in formula_31, formula_25 is a neighborhood retract of formula_26.\n\nVarious classes formula_31 such as normal spaces have been considered in this definition, but the class formula_53 of metrizable spaces has been found to give the most satisfactory theory. For that reason, the notations AR and ANR by themselves are used in this article to mean formula_54 and formula_55.\n\nA metrizable space is an AR if and only if it is contractible and an ANR. By Dugundji, every locally convex metrizable topological vector space formula_56 is an AR; more generally, every nonempty convex subset of such a vector space formula_56 is an AR. For example, any normed vector space (complete or not) is an AR. More concretely, Euclidean space formula_58 the unit cube formula_59and the Hilbert cube formula_60 are ARs.\n\nANRs form a remarkable class of \"well-behaved\" topological spaces. Among their properties are:\n\n"}
{"id": "32744201", "url": "https://en.wikipedia.org/wiki?curid=32744201", "title": "Rogers polynomials", "text": "Rogers polynomials\n\nIn mathematics, the Rogers polynomials, also called Rogers–Askey–Ismail polynomials and continuous q-ultraspherical polynomials, are a family of orthogonal polynomials introduced by in the course of his work on the Rogers–Ramanujan identities. They are \"q\"-analogs of ultraspherical polynomials, and are the Macdonald polynomials for the special case of the \"A\" affine root system .\n\nThe Rogers polynomials can be defined in terms of the descending Pochhammer symbol and the basic hypergeometric series by\nwhere \"x\" = cos(\"θ\").\n\n"}
{"id": "47759481", "url": "https://en.wikipedia.org/wiki?curid=47759481", "title": "Slonimski's Theorem", "text": "Slonimski's Theorem\n\nSlonimski's Theorem is an observation by Hayyim Selig Slonimski that the sequence of carry digits in a multiplication table is the Farey sequence.\n\nThis observation allowed Slonimski to create very compact multiplication tables for use in hand calculations. He received several awards for different devices for presenting these tables. The most common format were Joffe Bars similar to Napier's Rods. Joffe Bars were popular in Eastern Europe in the late 19th and early 20th century.\n\n"}
{"id": "3820897", "url": "https://en.wikipedia.org/wiki?curid=3820897", "title": "Spectrum of a theory", "text": "Spectrum of a theory\n\nIn model theory, a branch of mathematical logic, the spectrum of a theory\nis given by the number of isomorphism classes of models in various cardinalities. More precisely, \nfor any complete theory \"T\" in a language we write I(\"T\", α) for the number of models of \"T\" (up to isomorphism) of cardinality α. The spectrum problem is to describe the possible behaviors of I(\"T\", α) as a function of α. It has been almost completely solved for the case of a countable theory \"T\".\n\nIn this section \"T\" is a countable complete theory and \"κ\" is a cardinal.\n\nThe Löwenheim–Skolem theorem shows that if \"I\"(\"T\",\"κ\") is nonzero for one infinite cardinal then it is nonzero for all of them.\n\nMorley's categoricity theorem was the first main step in solving the spectrum problem: it states that if \"I\"(\"T\",\"κ\") is 1 for some uncountable \"κ\" then it is 1 for all uncountable \"κ\". \n\nRobert Vaught showed that \"I\"(\"T\",ℵ) cannot be 2. It is easy to find examples where it is any given non-negative integer other than 2. Morley proved that if \"I\"(\"T\",ℵ) is infinite then it must be ℵ or ℵ or 2. It is not known if it can be ℵ if the continuum hypothesis is false: this is called the Vaught conjecture and is the main remaining open problem (in 2005) in the theory of the spectrum.\n\nMorley's problem is a conjecture (first proposed by Michael D. Morley) in mathematical logic that \"I\"(\"T\",\"κ\") is nondecreasing in \"κ\" for uncountable \"κ\". This was proved by Saharon Shelah. For this, he proved a very deep dichotomy theorem.\n\nSaharon Shelah gave an almost complete solution to the spectrum problem. For a given complete theory \"T\", either \"I\"(\"T\",\"κ\") = 2 for all uncountable cardinals \"κ\", or formula_1 for all ordinals ξ (See aleph number and beth number for an explanation of the notation), which is usually much smaller than the bound in the first case. Roughly speaking this means that either there are the maximum possible number of models in all uncountable cardinalities, or there are only \"few\" models in all uncountable cardinalities. Shelah also gave a description of the possible spectra in the case when there are few models.\n\nBy extending Shelah's work, Bradd Hart, Ehud Hrushovski and Michael C. Laskowski gave the following complete solution to the spectrum problem for countable theories in uncountable cardinalities. \nIf \"T\" is a countable complete theory, then the number I(\"T\", ℵ) of isomorphism classes of models is given for ordinals α>0 by the minimum of 2 and one of the following maps:\n\nMoreover, all possibilities above occur as the spectrum of some countable complete theory.\n\nThe number \"d\" in the list above is the depth of the theory.\nIf \"T\" is a theory we define a new theory 2 to be the theory with an equivalence relation such that there are infinitely many equivalence classes each of which is a model of \"T\". We also define theories formula_14 by formula_15, formula_16. Then \nformula_17 . This can be used to construct examples of theories with spectra in the list above for non-minimal values of \"d\" from examples for the minimal value of \"d\".\n\n\n"}
{"id": "3955943", "url": "https://en.wikipedia.org/wiki?curid=3955943", "title": "Static cast", "text": "Static cast\n\nIn C++ type conversion, the static_cast operator performs an explicit type conversion.\n\nThe \"type\" parameter must be a data type to which \"object\" can be converted via a known method, whether it be a builtin or a cast. The type can be a reference or an enumerator.\nAll types of conversions that are well-defined and allowed by the compiler are performed using static_cast.\n\nThe static_cast operator can be used for operations such as:\n\nAlthough static_cast conversions are checked at compile time to prevent obvious incompatibilities, no run-time type check is performed that would prevent a cast between incompatible data types, such as pointers. Also, the result of a static_cast from a pointer of a virtual base class to a pointer of a derived class is undefined.\n\n"}
{"id": "42223508", "url": "https://en.wikipedia.org/wiki?curid=42223508", "title": "Superpermutation", "text": "Superpermutation\n\nIn combinatorial mathematics, a superpermutation on \"n\" symbols is a string that contains each permutation of \"n\" symbols as a substring.\n\nIt has been shown that for 1 ≤ \"n\" ≤ 5, the smallest superpermutation on \"n\" symbols has length 1! + 2! + … + \"n\"! . The first five superpermutations have respective lengths 1, 3, 9, 33, and 153, forming the strings 1, 121, 123121321, 123412314231243121342132413214321, and the string:\n\nIn September 2011, an anonymous poster of the internet imageboard 4chan proved that the smallest superpermutation on \"n\" symbols (\"n\" ≥ 2) has at least length \"n\"! + (\"n\"−1)! + (\"n\"−2)! + \"n\" − 3. The proof for this lower bound came to the general public interest in October 2018, after mathematician and computer scientist Robin Houston tweeted about it. On 25 October 2018, Robin Houston, Jay Pantone, and Vince Vatter posted a refined version of this proof in OEIS.\n\nOn 20 October 2018, by adapting a construction by Aaron Williams for constructing Hamiltonian paths through the Cayley graph of the symmetric group, Greg Egan devised an algorithm to produce superpermutations of length \"n\"! + (\"n\"−1)! + (\"n\"−2)! + (\"n\"−3)! + \"n\" − 3. Up to 2018, these are the smallest superpermutations known for \"n\" ≥ 7.\n\n\n\n"}
{"id": "3219921", "url": "https://en.wikipedia.org/wiki?curid=3219921", "title": "Symmetric derivative", "text": "Symmetric derivative\n\nIn mathematics, the symmetric derivative is an operation generalizing the ordinary derivative. It is defined as:\n\nThe expression under the limit is sometimes called the symmetric difference quotient. A function is said symmetrically differentiable at a point \"x\" if its symmetric derivative exists at that point.\n\nIf a function is differentiable (in the usual sense) at a point, then it is also symmetrically differentiable, but the converse is not true. A well-known counterexample is the absolute value function f(x) = |x|, which is not differentiable at x = 0, but is symmetrically differentiable here with symmetric derivative 0. For differentiable functions, the symmetric difference quotient does provide a better numerical approximation of the derivative than the usual difference quotient.\n\nThe symmetric derivative at a given point equals the arithmetic mean of the left and right derivatives at that point, if the latter two both exist.\n\nNeither Rolle's theorem nor the mean value theorem hold for the symmetric derivative; some similar but weaker statements have been proved.\n\nFor the modulus function, formula_2, we have, at formula_3, \n\nwhere since formula_5 we have formula_6 = formula_7. So, we observe that the symmetric derivative of the modulus function exists at formula_3, and is equal to zero, even though its ordinary derivative does not exist at that point (due to a \"sharp\" turn in the curve at formula_3).\n\nNote in this example both the left and right derivatives at 0 exist, but they are unequal (one is -1 and the other is 1); their average is 0, as expected.\n\nFor the function formula_10, we have, at formula_3, \n\nwhere again, formula_5. Again, for this function the symmetric derivative exists at formula_3, while its ordinary derivative does not exist at formula_3, due to discontinuity in the curve there. Furthermore, neither the left nor the right derivative is finite at 0; i.e. this is an essential discontinuity.\n\nThe Dirichlet function, defined as\n\nformula_16\n\nhas symmetric derivatives formula_17 but not formula_18; i.e. the symmetric derivative exists for rational numbers but not for irrational numbers.\n\nThe symmetric derivative does not obey the usual mean value theorem (of Lagrange). As counterexample, the symmetric derivative of \"f\"(\"x\") = |\"x\"| has the image {-1, 0, 1}, but secants for \"f\" can have a wider range of slopes; for instance, on the interval [-1, 2], the mean value theorem would mandate that there exist a point where the (symmetric) derivative takes the value formula_19.\n\nA theorem somewhat analogous to Rolle's theorem but for the symmetric derivative was established in 1967 by C.E. Aull, who named it Quasi-Rolle theorem. If \"f\" is continuous on the closed interval [\"a\", \"b\"] and symmetrically differentiable on the open interval (\"a\", \"b\") and \"f\"(\"b\") = \"f\"(\"a\") = 0, then there exist two points \"x\", \"y\" in (\"a\", \"b\") such that \"f\"(\"x\") ≥ 0 and \"f\"(\"y\") ≤ 0. A lemma also established by Aull as a stepping stone to this theorem states that if \"f\" is continuous on the closed interval [\"a\", \"b\"] and symmetrically differentiable on the open interval (\"a\", \"b\") and additionally \"f\"(\"b\") > \"f\"(\"a\") then there exist a point \"z\" in (\"a\", \"b\") where the symmetric derivative is non-negative, or with the notation used above, \"f\"(\"z\") ≥ 0. Analogously, if \"f\"(\"b\") < \"f\"(\"a\"), then there exists a point \"z\" in (\"a\", \"b\") where \"f\"(\"z\") ≤ 0.\n\nThe quasi-mean value theorem for a symmetrically differentiable function states that if \"f\" is continuous on the closed interval [\"a\", \"b\"] and symmetrically differentiable on the open interval (\"a\", \"b\"), then there exist \"x\", \"y\" in (\"a\", \"b\") such that\n\nAs an application, the quasi-mean value theorem for \"f\"(\"x\") = |\"x\"| on an interval containing 0 predicts that the slope of any secant of \"f\" is between -1 and 1.\n\nIf the symmetric derivative of \"f\" has the Darboux property, then the (form of the) regular mean value theorem (of Lagrange) holds, i.e. there exists \"z\" in (\"a\", \"b\"):\n\nAs a consequence, if a function is continuous and its symmetric derivative is also continuous (thus has the Darboux property), then the function is differentiable in the usual sense.\n\nThe notion generalizes to higher-order symmetric derivatives and also to \"n\"-dimensional Euclidean spaces.\n\nIt is defined as\n\nIf the (usual) second derivative exists, then the second symmetric derivative equals it. The second symmetric derivative may exist however even when the (ordinary) second derivative does not. As example, consider the sign function formula_23 which is defined through\n\nThe sign function is not continuous at zero and therefore the second derivative for formula_3 does not exist. But the second symmetric derivative exists for formula_3:\n\n\n\n"}
{"id": "1481185", "url": "https://en.wikipedia.org/wiki?curid=1481185", "title": "Tornado code", "text": "Tornado code\n\nIn computer science, Tornado codes are a class of erasure codes that support error correction. Tornado codes require a constant C more redundant blocks than the more data-efficient Reed–Solomon erasure codes, but are much faster to generate and can fix erasures faster. Software-based implementations of tornado codes are about 100 times faster on small lengths and about 10,000 times faster on larger lengths than Reed–Solomon erasure codes. Since the introduction of Tornado codes, many other similar erasure codes have emerged, most notably Online codes, LT codes and Raptor codes.\n\nTornado codes use a layered approach. All layers except the last use an LDPC error correction code, which is fast but has a chance of failure. The final layer uses a Reed–Solomon correction code, which is slower but is optimal in terms of failure recovery. Tornado codes dictates how many levels, how many recovery blocks in each level, and the distribution used to generate blocks for the non-final layers.\n\nThe input data is divided into blocks. Blocks are sequences of bits that are all the same size. Recovery data uses the same block size as the input data. The erasure of a block (input or recovery) is detected by some other means. (For example, a block from disk does not pass a CRC check or a network packet with a given sequence number never arrived.)\n\nThe number of recovery blocks is given by the user. Then the number of levels is determined along with the number of blocks in each level. The number in each level is determined by a factor B which is less than one. If there are N input blocks, the first recovery level has B*N blocks, the second has B*B*N, the third has B*B*B*N, and so on.\n\nAll levels of recovery except the final one use an LDPC, which works by xor (exclusive-or). Xor operates on binary values, 1s and 0s. A xor B is 1 if A and B have different values and 0 if A and B have the same values. If you are given result of (A xor B) and A, you can determine the value for B. (A xor B xor A = B) Similarly, if you are given result of (A xor B) and B, you can determine the value for A. This extends to multiple values, so given result of (A xor B xor C xor D) and any 3 of the values, the missing value can be recovered. \n\nSo the recovery blocks in level one are just the xor of some set of input blocks. Similarly, the recovery blocks in level two are each the xor of some set of blocks in level one. The blocks used in the xor are chosen randomly, without repetition. However, the \"number\" of blocks xor'ed to make a recovery block is chosen from a very specific distribution for each level.\n\nSince xor is a fast operation and the recovery blocks are an xor of only a subset of the blocks in the input (or at a lower recovery level), the recovery blocks can be generated quickly.\n\nThe final level is a Reed–Solomon code. Reed–Solomon codes are optimal in terms of recovering from failures, but slow to generate and recover. Since each level has fewer blocks than the one before, the Reed–Solomon code has a small number of recovery blocks to generate and to use in recovery. So, even though Reed–Solomon is slow, it only has a small amount of data to handle.\n\nDuring recovery, the Reed–Solomon code is recovered first. This is guaranteed to work if the number of missing blocks in the next-to-final level is less than the present blocks in the final level. \n\nGoing lower, the LDPC (xor) recovery level can be used to recover the level beneath it \"with high probability\" if all the recovery blocks are present and the level beneath is missing at most C' fewer blocks than the recovery level. The algorithm for recovery is to find some recovery block that has only one of its generating set missing from the lower level. Then the xor of the recovery block with all of the blocks that are present is equal to the missing block.\n\nTornado codes are patented inside the United States of America. Patents US6163870 A (filed Nov 6, 1997) and US 6081909 A (filed Nov 6, 1997) describe Tornado codes, and have expired as of November 6, 2017. Patents US6307487 B1 (filed Feb 5, 1999) and US6320520 B1 (filed Sep 17, 1999) also mention Tornado codes. Patents are currently valid until 20 years after filing, and since '870 A and '909 A have both expired, a case can be made that tornado codes are no longer patent encumbered.\n\nMichael Luby created the Tornado codes.\n\nA readable description from CMU (PostScript) and another from Luby at the International Computer Science Institute (PostScript) . \n\n\n"}
{"id": "25512250", "url": "https://en.wikipedia.org/wiki?curid=25512250", "title": "Truth table", "text": "Truth table\n\nA truth table is a mathematical table used in logic—specifically in connection with Boolean algebra, boolean functions, and propositional calculus—which sets out the functional values of logical expressions on each of their functional arguments, that is, for each combination of values taken by their logical variables (Enderton, 2001). In particular, truth tables can be used to show whether a propositional expression is true for all legitimate input values, that is, logically valid.\n\nA truth table has one column for each input variable (for example, P and Q), and one final column showing all of the possible results of the logical operation that the table represents (for example, P XOR Q). Each row of the truth table contains one possible configuration of the input variables (for instance, P=true Q=false), and the result of the operation for those values. See the examples below for further clarification. Ludwig Wittgenstein is often credited with inventing the truth table in his \"Tractatus Logico-Philosophicus\", though it appeared at least a year earlier in a paper on propositional logic by Emil Leon Post.\n\nThere are 4 unary operations:\n\nThe output value is always true, regardless of the input value of p\n\nThe output value is never true: that is, always false, regardless of the input value of p\nLogical identity is an operation on one logical value p, for which the output value remains p.\n\nThe truth table for the logical identity operator is as follows:\n\nLogical negation is an operation on one logical value, typically the value of a proposition, that produces a value of \"true\" if its operand is false and a value of \"false\" if its operand is true.\n\nThe truth table for NOT p (also written as ¬p, Np, Fpq, or ~p) is as follows:\n\nThere are 16 possible truth functions of two binary variables:\n\nHere is an extended truth table giving definitions of all possible truth functions of two Boolean variables P and Q:\n\nwhere\n\nThe four combinations of input values for p, q, are read by row from the table above.\nThe output function for each p, q combination, can be read, by row, from the table.\n\nKey:\n\nThe following table is oriented by column, rather than by row. There are four columns rather than four rows, to display the four combinations of p, q, as input. \n\np: T T F F<br>\nq: T F T F\n\nThere are 16 rows in this key, one row for each binary function of the two binary variables, p, q. For example, in row 2 of this Key, the value of Converse nonimplication ('formula_1') is solely T, for the column denoted by the unique combination p=F, q=T; while in row 2, the value of that 'formula_1' operation is F for the three remaining columns of p, q. The output row for formula_1 is thus\n\n2: F F T F\n\nand the 16-row key is\n\nLogical operators can also be visualized using Venn diagrams.\n\nLogical conjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both of its operands are true.\n\nThe truth table for p AND q (also written as p ∧ q, Kpq, p & q, or p formula_4 q) is as follows:\n\nIn ordinary language terms, if both \"p\" and \"q\" are true, then the conjunction \"p\" ∧ \"q\" is true. For all other assignments of logical values to \"p\" and to \"q\" the conjunction \"p\" ∧ \"q\" is false.\n\nIt can also be said that if \"p\", then \"p\" ∧ \"q\" is \"q\", otherwise \"p\" ∧ \"q\" is \"p\".\n\nLogical disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if at least one of its operands is true.\n\nThe truth table for p OR q (also written as p ∨ q, Apq, p || q, or p + q) is as follows:\n\nStated in English, if \"p\", then \"p\" ∨ \"q\" is \"p\", otherwise \"p\" ∨ \"q\" is \"q\".\n\nLogical implication and the material conditional are both associated with an operation on two logical values, typically the values of two propositions, which produces a value of \"false\" if the first operand is true and the second operand is false, and a value of \"true\" otherwise.\n\nThe truth table associated with the logical implication p implies q (symbolized as p ⇒ q, or more rarely Cpq) is as follows:\n\nThe truth table associated with the material conditional if p then q (symbolized as p → q) is as follows:\n\nIt may also be useful to note that p ⇒ q and p → q are equivalent to ¬p ∨ q.\n\nLogical equality (also known as biconditional) is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both operands are false or both operands are true.\n\nThe truth table for p XNOR q (also written as p ↔ q, Epq, p = q, or p ≡ q) is as follows:\n\nSo p EQ q is true if p and q have the same truth value (both true or both false), and false if they have different truth values.\n\nExclusive disjunction is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if one but not both of its operands is true.\n\nThe truth table for p XOR q (also written as p ⊕ q, Jpq, p ≠ q, or p ↮ q) is as follows:\n\nFor two propositions, XOR can also be written as (p ∧ ¬q) ∨ (¬p ∧ q).\n\nThe logical NAND is an operation on two logical values, typically the values of two propositions, that produces a value of \"false\" if both of its operands are true. In other words, it produces a value of \"true\" if at least one of its operands is false.\n\nThe truth table for p NAND q (also written as p ↑ q, Dpq, or p | q) is as follows:\n\nIt is frequently useful to express a logical operation as a compound operation, that is, as an operation that is built up or composed from other operations. Many such compositions are possible, depending on the operations that are taken as basic or \"primitive\" and the operations that are taken as composite or \"derivative\".\n\nIn the case of logical NAND, it is clearly expressible as a compound of NOT and AND.\n\nThe negation of a conjunction: ¬(\"p\" ∧ \"q\"), and the disjunction of negations: (¬\"p\") ∨ (¬\"q\") can be tabulated as follows:\n\nThe logical NOR is an operation on two logical values, typically the values of two propositions, that produces a value of \"true\" if both of its operands are false. In other words, it produces a value of \"false\" if at least one of its operands is true. ↓ is also known as the Peirce arrow after its inventor, Charles Sanders Peirce, and is a Sole sufficient operator.\n\nThe truth table for p NOR q (also written as p ↓ q, or Xpq) is as follows:\n\nThe negation of a disjunction ¬(\"p\" ∨ \"q\"), and the conjunction of negations (¬\"p\") ∧ (¬\"q\") can be tabulated as follows:\n\nInspection of the tabular derivations for NAND and NOR, under each assignment of logical values to the functional arguments \"p\" and \"q\", produces the identical patterns of functional values for ¬(\"p\" ∧ \"q\") as for (¬\"p\") ∨ (¬\"q\"), and for ¬(\"p\" ∨ \"q\") as for (¬\"p\") ∧ (¬\"q\"). Thus the first and second expressions in each pair are logically equivalent, and may be substituted for each other in all contexts that pertain solely to their logical values.\n\nThis equivalence is one of De Morgan's laws.\n\nTruth tables can be used to prove many other logical equivalences. For example, consider the following truth table:\n\nThis demonstrates the fact that formula_5 is logically equivalent to formula_6.\n\nHere is a truth table that gives definitions of the 6 most commonly used out of the 16 possible truth functions of two Boolean variables P and Q:\n\nwhere\n\nFor binary operators, a condensed form of truth table is also used, where the row headings and the column headings specify the operands and the table cells specify the result. For example, Boolean logic uses this condensed truth table notation:\n\nThis notation is useful especially if the operations are commutative, although one can additionally specify that the rows are the first operand and the columns are the second operand. This condensed notation is particularly useful in discussing multi-valued extensions of logic, as it significantly cuts down on combinatoric explosion of the number of rows otherwise needed. It also provides for quickly recognizable characteristic \"shape\" of the distribution of the values in the table which can assist the reader in grasping the rules more quickly.\n\nTruth tables are also used to specify the function of hardware look-up tables (LUTs) in digital logic circuitry. For an n-input LUT, the truth table will have 2^\"n\" values (or rows in the above tabular format), completely specifying a boolean function for the LUT. By representing each boolean value as a bit in a binary number, truth table values can be efficiently encoded as integer values in electronic design automation (EDA) software. For example, a 32-bit integer can encode the truth table for a LUT with up to 5 inputs.\n\nWhen using an integer representation of a truth table, the output value of the LUT can be obtained by calculating a bit index \"k\" based on the input values of the LUT, in which case the LUT's output value is the \"k\"th bit of the integer. For example, to evaluate the output value of a LUT given an array of \"n\" boolean input values, the bit index of the truth table's output value can be computed as follows: if the \"i\"th input is true, let formula_14, else let formula_15. Then the \"k\"th bit of the binary representation of the truth table is the LUT's output value, where formula_16.\n\nTruth tables are a simple and straightforward way to encode boolean functions, however given the exponential growth in size as the number of inputs increase, they are not suitable for functions with a large number of inputs. Other representations which are more memory efficient are text equations and binary decision diagrams.\n\nIn digital electronics and computer science (fields of applied logic engineering and mathematics), truth tables can be used to reduce basic boolean operations to simple correlations of inputs to outputs, without the use of logic gates or code. For example, a binary addition can be represented with the truth table:\n\nThis truth table is read left to right:\n\nNote that this table does not describe the logic operations necessary to implement this operation, rather it simply specifies the function of inputs to output values.\n\nWith respect to the result, this example may be arithmetically viewed as modulo 2 binary addition, and as logically equivalent to the exclusive-or (exclusive disjunction) binary logic operation.\n\nIn this case it can be used for only very simple inputs and outputs, such as 1s and 0s. However, if the number of types of values one can have on the inputs increases, the size of the truth table will increase.\n\nFor instance, in an addition operation, one needs two operands, A and B. Each can have one of two values, zero or one. The number of combinations of these two values is 2×2, or four. So the result is four possible outputs of C and R. If one were to use base 3, the size would increase to 3×3, or nine possible outputs.\n\nThe first \"addition\" example above is called a half-adder. A full-adder is when the carry from the previous operation is provided as input to the next adder. Thus, a truth table of eight rows would be needed to describe a full adder's logic:\n\nIrving Anellis has done the research to show that C.S. Peirce appears to be the earliest logician (in 1893) to devise a truth table matrix. From the summary of his paper:\n\n\n"}
{"id": "41107445", "url": "https://en.wikipedia.org/wiki?curid=41107445", "title": "Vladimir Popov (mathematician)", "text": "Vladimir Popov (mathematician)\n\nVladimir Leonidovich Popov (; born 3 September 1946) is a Russian mathematician working in the invariant theory and the theory of transformation groups. He is a member of the Steklov Institute of Mathematics and a professor of the National Research University – Higher School of Economics. In 1986, he was an invited speaker at the International Congress of Mathematicians (Berkeley, USA), and in 2008–2010 he was a core member of the panel for Section 2, \"Algebra\" of the Program Committee for the 2010 International Congress of Mathematicians (Hyderabad, India).\n\nIn 2012, he was elected a member of the inaugural class of Fellows of the American Mathematical Society which recognizes mathematicians who have made significant contributions to the field. \n\nIn 2016, he was elected a corresponding member of the Russian Academy of Sciences. \n\n\n"}
{"id": "3126130", "url": "https://en.wikipedia.org/wiki?curid=3126130", "title": "Wagner's theorem", "text": "Wagner's theorem\n\nIn graph theory, Wagner's theorem is a mathematical forbidden graph characterization of planar graphs, named after Klaus Wagner, stating that a finite graph is planar if and only if its minors include neither \"K\" (the complete graph on five vertices) nor \"K\" (the utility graph, a complete bipartite graph on six vertices). This was one of the earliest results in the theory of graph minors and can be seen as a forerunner of the Robertson–Seymour theorem.\n\nA planar embedding of a given graph is a drawing of the graph in the Euclidean plane, with points for its vertices and curves for its edges, in such a way that the only intersections between pairs of edges are at a common endpoint of the two edges. A minor of a given graph is another graph formed by deleting vertices, deleting edges, and contracting edges. When an edge is contracted, its two endpoints are merged to form a single vertex. In some versions of graph minor theory the graph resulting from a contraction is simplified by removing self-loops and multiple adjacencies, while in other version multigraphs are allowed, but this variation makes no difference to Wagner's theorem.\nWagner's theorem states that every graph has either a planar embedding, or a minor of one of two types, the complete graph \"K\" or the complete bipartite graph \"K\". (It is also possible for a single graph to have both types of minor.)\n\nIf a given graph is planar, so are all its minors: vertex and edge deletion obviously preserve planarity, and edge contraction can also be done in a planarity-preserving way, by leaving one of the two endpoints of the contracted edge in place and routing all of the edges that were incident to the other endpoint along the path of the contracted edge.\nA \"minor-minimal\" non-planar graph is a graph that is not planar, but in which all proper minors (minors formed by at least one deletion or contraction) are planar. Another way of stating Wagner's theorem is that there are only two minor-minimal non-planar graphs, \"K\" and \"K\".\n\nAnother result also sometimes known as Wagner's theorem states that a four-connected graph is planar if and only if it has no \"K\" minor. That is, by assuming a higher level of connectivity, the graph \"K\" can be made unnecessary in the characterization, leaving only a single forbidden minor, \"K\". Correspondingly, the Kelmans–Seymour conjecture states that a 5-connected graph is planar if and only if it does not have \"K\" as a topological minor.\n\nWagner published both theorems in 1937, subsequent to the 1930 publication of Kuratowski's theorem, according to which a graph is planar if and only if it does not contain as a subgraph a subdivision of one of the same two forbidden graphs \"K\" and \"K\". In a sense, Kuratowski's theorem is stronger than Wagner's theorem: a subdivision can be converted into a minor of the same type by contracting all but one edge in each path formed by the subdivision process, but converting a minor into a subdivision of the same type is not always possible. However, in the case of the two graphs \"K\" and \"K\", it is straightforward to prove that a graph that has at least one of these two graphs as a minor also has at least one of them as a subdivision, so the two theorems are equivalent.\n\nOne consequence of the stronger version of Wagner's theorem for four-connected graphs is to characterize the graphs that do not have a \"K\" minor. The theorem can be rephrased as stating that every such graph is either planar or it can be decomposed into simpler pieces. Using this idea, the \"K\"-minor-free graphs may be characterized as the graphs that can be formed as combinations of planar graphs and the eight-vertex Wagner graph, glued together by clique-sum operations. For instance, \"K\" can be formed in this way as a clique-sum of three planar graphs, each of which is a copy of the tetrahedral graph \"K\".\n\nWagner's theorem is an important precursor to the theory of graph minors, which culminated in the proofs of two deep and far-reaching results: the graph structure theorem (a generalization of Wagner's clique-sum decomposition of \"K\"-minor-free graphs) and the Robertson–Seymour theorem (a generalization of the forbidden minor characterization of planar graphs, stating that every graph family closed under the operation of taking minors has a characterization by a finite number of forbidden minors). Analogues of Wagner's theorem can also be extended to the theory of matroids: in particular, the same two graphs \"K\" and \"K\" (along with three other forbidden configurations) appear in a characterization of the graphic matroids by forbidden matroid minors.\n"}
{"id": "24064939", "url": "https://en.wikipedia.org/wiki?curid=24064939", "title": "Watkins snark", "text": "Watkins snark\n\nIn the mathematical field of graph theory, the Watkins snark is a snark with 50 vertices and 75 edges. It was discovered by John J. Watkins in 1989.\n\nAs a snark, the Watkins graph is a connected, bridgeless cubic graph with chromatic index equal to 4. The Watkins snark is also non-planar and non-hamiltonian. It has book thickness 3 and queue number 2.\n\nAnother well known snark on 50 vertices is the Szekeres snark, the fifth known snark, discovered by George Szekeres in 1973.\n\n1,2], [1,4], [1,15], [2,3], [2,8], [3,6], [3,37], [4,6], [4,7], [5,10], [5,11], [5,22], [6,9], [7,8], [7,12], [8,9], [9,14], [10,13], [10,17], [11,16], [11,18], [12,14], [12,33], [13,15], [13,16], [14,20], [15,21], [16,19], [17,18], [17,19], [18,30], [19,21], [20,24], [20,26], [21,50], [22,23], [22,27], [23,24], [23,25], [24,29], [25,26], [25,28], [26,31], [27,28], [27,48], [28,29], [29,31], [30,32], [30,36], [31,36], [32,34], [32,35], [33,34], [33,40], [34,41], [35,38], [35,40], [36,38], [37,39], [37,42], [38,41], [39,44], [39,46], [40,46], [41,46], [42,43], [42,45], [43,44], [43,49], [44,47], [45,47], [45,48], [47,50], [48,49], [49,50\n"}
{"id": "34359", "url": "https://en.wikipedia.org/wiki?curid=34359", "title": "Yoneda lemma", "text": "Yoneda lemma\n\nIn mathematics, specifically in category theory, the Yoneda lemma is an abstract result on functors of the type \"morphisms into a fixed object\". It is a vast generalisation of Cayley's theorem from group theory (viewing a group as a particular kind of category with just one object and only isomorphisms). It allows the embedding of any category into a category of functors (contravariant set-valued functors) defined on that category. It also clarifies how the embedded category, of representable functors and their natural transformations, relates to the other objects in the larger functor category. It is an important tool that underlies several modern developments in algebraic geometry and representation theory. It is named after Nobuo Yoneda.\n\nThe Yoneda lemma suggests that instead of studying the (locally small) category formula_1, one should study the category of all functors of formula_1 into formula_3 (the category of sets with functions as morphisms). formula_3 is a category we think we understand well, and a functor of formula_1 into formula_3 can be seen as a \"representation\" of formula_1 in terms of known structures. The original category formula_1 is contained in this functor category, but new objects appear in the functor category, which were absent and \"hidden\" in formula_1. Treating these new objects just like the old ones often unifies and simplifies the theory.\n\nThis approach is akin to (and in fact generalizes) the common method of studying a ring by investigating the modules over that ring. The ring takes the place of the category formula_1, and the category of modules over the ring is a category of functors defined on formula_1.\n\nYoneda's lemma concerns functors from a fixed category formula_1 to the category of sets, formula_3. If formula_1 is a locally small category (i.e. the hom-sets are actual sets and not proper classes), then each object formula_15 of formula_1 gives rise to a natural functor to formula_3 called a hom-functor. This functor is denoted:\nThe (covariant) hom-functor formula_19 sends formula_20 to the set of morphisms formula_21 and sends a morphism formula_22 to the morphism formula_23 (composition with formula_24 on the left) that sends a morphism formula_25 in formula_21 to the morphism formula_27 in formula_28. That is, \n\nLet formula_31 be an arbitrary functor from formula_1 to formula_3. Then Yoneda's lemma says that:\n\nFor each object formula_15 of formula_1, the natural transformations from formula_19 to formula_31 are in one-to-one correspondence with the elements of formula_38. That is,\n\nMoreover this isomorphism is natural in formula_40 and formula_41 when both sides are regarded as functors from formula_42 to formula_3. (Here the notation formula_44 denotes the category of functors from formula_1 to formula_3.)\n\nGiven a natural transformation formula_47 from formula_19 to formula_31, the corresponding element of formula_38 is formula_51; and given an element formula_52 of formula_38, the corresponding natural transformation is given by formula_54.\n\nThere is a contravariant version of Yoneda's lemma, which concerns contravariant functors from formula_1 to formula_3. This version involves the contravariant hom-functor\nwhich sends formula_58 to the hom-set formula_59. Given an arbitrary contravariant functor formula_60 from formula_1 to formula_3, Yoneda's lemma asserts that\n\nThe use of formula_19 for the covariant hom-functor and formula_65 for the contravariant hom-functor is not completely standard. Many texts and articles either use the opposite convention or completely unrelated symbols for these two functors. However, most modern algebraic geometry texts starting with Alexander Grothendieck's foundational EGA use the convention in this article.\n\nThe mnemonic \"falling into something\" can be helpful in remembering that formula_66 is the contravariant hom-functor. When the letter formula_15 is falling (i.e. a subscript), formula_66 assigns to an object formula_58 the morphisms from formula_58 into formula_15.\n\nThe proof of Yoneda's lemma is indicated by the following commutative diagram:\n\nThis diagram shows that the natural transformation formula_72 is completely determined by formula_73 since for each morphism formula_74 one has\nMoreover, any element formula_76 defines a natural transformation in this way. The proof in the contravariant case is completely analogous.\n\nAn important special case of Yoneda's lemma is when the functor formula_41 from formula_1 to formula_3 is another hom-functor formula_80. In this case, the covariant version of Yoneda's lemma states that\n\nThat is, natural transformations between hom-functors are in one-to-one correspondence with morphisms (in the reverse direction) between the associated objects. Given a morphism formula_82 the associated natural transformation is denoted formula_83.\n\nMapping each object formula_15 in formula_1 to its associated hom-functor formula_86 and each morphism formula_82 to the corresponding natural transformation formula_88 determines a contravariant functor formula_89 from formula_1 to formula_44, the functor category of all (covariant) functors from formula_1 to formula_3. One can interpret formula_89 as a covariant functor:\nThe meaning of Yoneda's lemma in this setting is that the functor formula_96 is fully faithful, and therefore gives an embedding of formula_97 in the category of functors to formula_3. The collection of all functors formula_99 is a subcategory of formula_100. Therefore, Yoneda embedding implies that the category formula_97 is isomorphic to the category formula_102.\n\nThe contravariant version of Yoneda's lemma states that\nTherefore, formula_104 gives rise to a covariant functor from formula_1 to the category of contravariant functors to formula_3:\nYoneda's lemma then states that any locally small category formula_1 can be embedded in the category of contravariant functors from formula_1 to formula_3 via formula_111. This is called the \"Yoneda embedding\".\n\nThe Yoneda embedding essentially states that for every (locally small) category, objects in that category can be represented by presheaves, in a full and faithful manner. That is,\nfor a presheaf \"P\". Many common categories are, in fact, pre-sheaves, and on closer inspection, prove to be sheaves, and, as such examples are commonly topological in nature, they can be seen to be topoi in general. The Yoneda lemma provides a point of leverage by which the topological structure of a category can be studied and understood.\n\nA \"preadditive category\" is a category where the morphism sets form abelian groups and the composition of morphisms is bilinear; examples are categories of abelian groups or modules. In a preadditive category, there is both a \"multiplication\" and an \"addition\" of morphisms, which is why preadditive categories are viewed as generalizations of rings. Rings are preadditive categories with one object.\n\nThe Yoneda lemma remains true for preadditive categories if we choose as our extension the category of \"additive\" contravariant functors from the original category into the category of abelian groups; these are functors which are compatible with the addition of morphisms and should be thought of as forming a \"module category\" over the original category. The Yoneda lemma then yields the natural procedure to enlarge a preadditive category so that the enlarged version remains preadditive — in fact, the enlarged version is an abelian category, a much more powerful condition. In the case of a ring formula_113, the extended category is the category of all right modules over formula_113, and the statement of the Yoneda lemma reduces to the well-known isomorphism\n\nAs stated above, the Yoneda lemma may be considered as a vast generalization of Cayley's theorem from group theory. To see this, let formula_1 be a category with a single object formula_119 such that every morphism is an isomorphism (i.e. a groupoid with one object). Then formula_120 forms a group under the operation of composition, and any group can be realized as a category in this way. \n\nIn this context, a covariant functor formula_121 consists of a set formula_20 and a group homomorphism formula_123, where formula_124 is the group of permutations of formula_20; in other words, formula_20 is a G-set. A natural transformation between such functors is the same thing as an equivariant map between formula_127-sets: a set function formula_128 with the property that formula_129 for all formula_25 in formula_127 and formula_132 in formula_20. (On the left side of this equation, the formula_134 denotes the action of formula_127 on formula_20, and on the right side the action on formula_137.) \n\nNow the covariant hom-functor formula_138 corresponds to the action of formula_127 on itself by left-multiplication (the contravariant version corresponds to right-multiplication). The Yoneda lemma with formula_140 states that\nthat is, the equivariant maps from this formula_127-set to itself are in bijection with formula_127. But it is easy to see that (1) these maps form a group under composition, which is a subgroup of formula_144, and (2) the function which gives the bijection is a group homomorphism. (Going in the reverse direction, it associates to every formula_25 in formula_127 the equivariant map of right-multiplication by formula_25.) Thus formula_127 is isomorphic to a subgroup of formula_144, which is the statement of Cayley's theorem.\n\nYoshiki Kinoshita stated in 1996 that the term \"Yoneda lemma\" was coined by Saunders Mac Lane following an interview he had with Yoneda.\n\n\n\n"}
