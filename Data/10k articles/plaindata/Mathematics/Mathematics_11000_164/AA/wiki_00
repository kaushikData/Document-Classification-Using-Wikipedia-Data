{"id": "20484367", "url": "https://en.wikipedia.org/wiki?curid=20484367", "title": "Bayesian econometrics", "text": "Bayesian econometrics\n\nBayesian econometrics is a branch of econometrics which applies Bayesian principles to economic modelling. Bayesianism is based on a degree-of-belief interpretation of probability, as opposed to a relative-frequency interpretation.\n\nThe Bayesian principle relies on Bayes' theorem which states that the probability of B conditional on A is the ratio of joint probability of A and B divided by probability of B. Bayesian econometricians assume that coefficients in the model have prior distributions.\n\nThis approach was first propagated by Arnold Zellner.\n\nSubjective probabilities have to satisfy the standard axioms of probability theory if one wishes to avoid losing a bet regardless of the outcome. Before the data is observed, the parameter formula_1 is regarded as an unknown quantity and thus random variable, which is assigned a prior distribution formula_2 with formula_3. Bayesian analysis concentrates on the inference of the posterior distribution formula_4, i.e. the distribution of the random variable formula_1 conditional on the observation of the discrete data formula_6. The posterior density function formula_4 can be computed based on Bayes' Theorem:\nwhere formula_9, yielding a normalized probability function. For continuous data formula_6, this corresponds to:\nwhere formula_12 and which is the centerpiece of Bayesian statistics and econometrics. It has the following components:\nThe posterior function is given by formula_22, i.e., the posterior function is proportional to the product of the likelihood function and the prior distribution, and can be understood as a method of updating information, with the difference between formula_2 and formula_4 being the information gain concerning formula_1 after observing new data. The choice of the prior distribution is used to impose restrictions on formula_1, e.g. formula_27, with the beta distribution as a common choice due to (i) being defined between 0 and 1, (ii) being able to produce a variety of shapes, and (iii) yielding a posterior distribution of the standard form if combined with the likelihood function formula_28. Based on the properties of the beta distribution, an ever-larger sample size implies that the mean of the posterior distribution approximates the maximum likelihood estimator formula_29\nThe assumed form of the likelihood function is part of the prior information and has to be justified. Different distributional assumptions can be compared using posterior odds ratios if a priori grounds fail to provide a clear choice. Commonly assumed forms include the beta distribution, the gamma distribution, and the uniform distribution, among others. If the model contains multiple parameters, the parameter can be redefined as a vector. Applying probability theory to that vector of parameters yields the marginal and conditional distributions of individual parameters or parameter groups. If data generation is sequential, Bayesian principles imply that the posterior distribution for the parameter based on new evidence will be proportional to the product of the likelihood for the new data, given previous data and the parameter, and the posterior distribution for the parameter, given the old data, which provides an intuitive way of allowing new information to influence beliefs about a parameter through Bayesian updating. If the sample size is large, (i) the prior distribution plays a relatively small role in determining the posterior distribution, (ii) the posterior distribution converges to a degenerate distribution at the true value of the parameter, and (iii) the posterior distribution is approximately normally distributed with mean formula_30.\n\nThe ideas underlying Bayesian statistics were developed by Rev. Thomas Bayes during the 18th century and later expanded by Pierre-Simon Laplace. As early as 1950, the potential of the Bayesian inference in econometrics was recognized by Jacob Marschak. The Bayesian approach was first applied to econometrics in the early 1960s by W. D. Fisher, Jacques Drèze, Clifford Hildreth, Thomas J. Rothenberg, George Tiao, and Arnold Zellner. The central motivation behind these early endeavors in Bayesian econometrics was the combination of the parameter estimators with available uncertain information on the model parameters that was not included in a given model formulation. From the mid-1960s to the mid-1970s, the reformulation of econometric techniques along Bayesian principles under the traditional structural approach dominated the research agenda, with Zellner's \"An Introduction to Bayesian Inference in Econometrics\" in 1971 as one of its highlights, and thus closely followed the work of frequentist econometrics. Therein, the main technical issues were the difficulty of specifying prior densities without losing either economic interpretation or mathematical tractability and the difficulty of integral calculation in the context of density functions. The result of the Bayesian reformulation program was to highlight the fragility of structural models to uncertain specification. This fragility came to motivate the work of Edward Leamer, who emphatically criticized modelers' tendency to indulge in \"post-data model construction\" and consequently developed a method of economic modelling based on the selection of regression models according to the types of prior density specification in order to identify the prior structures underlying modelers' working rules in model selection explicitly. Bayesian econometrics also became attractive to Christopher Sims' attempt to move from structural modeling to VAR modeling due to its explicit probability specification of parameter restrictions. Driven by the rapid growth of computing capacities from the mid-1980s on, the application of Markov chain Monte Carlo simulation to statistical and econometric models, first performed in the early 1990s, enabled Bayesian analysis to drastically increase its influence in economics and econometrics.\n\nSince the beginning of the 21st century, research in Bayesian econometrics has concentrated on:\n\n"}
{"id": "31356616", "url": "https://en.wikipedia.org/wiki?curid=31356616", "title": "Beltrami equation", "text": "Beltrami equation\n\nIn mathematics, the Beltrami equation, named after Eugenio Beltrami, is the partial differential equation\n\nfor \"w\" a complex distribution of the complex variable \"z\" in some open set \"U\", with derivatives that are locally \"L\", and where \"μ\" is a given complex function in \"L\"(\"U\") of norm less than 1, called the Beltrami coefficient. Classically this differential equation was used by Gauss to prove the existence locally of isothermal coordinates on a surface with analytic Riemannian metric. Various techniques have been developed for solving the equation. The most powerful, developed in the 1950s, provides global solutions of the equation on C and relies on the L theory of the Beurling transform, a singular integral operator defined on L(C) for all 1 < \"p\" < ∞. The same method applies equally well on the unit disk and upper half plane and plays a fundamental role in Teichmüller theory and the theory of quasiconformal mappings. Various uniformization theorems can be proved using the equation, including the measurable Riemann mapping theorem and the simultaneous uniformization theorem. The existence of conformal weldings can also be derived using the Beltrami equation. One of the simplest applications is to the Riemann mapping theorem for simply connected bounded open domains in the complex plane. When the domain has smooth boundary, elliptic regularity for the equation can be used to show that the uniformizing map from the unit disk to the domain extends to a C function from the closed disk to the closure of the domain.\n\nConsider a 2-dimensional Riemannian manifold, say with an (\"x\", \"y\") coordinate system on it. The curves of constant \"x\" on that surface typically don't intersect the curves of constant \"y\" orthogonally. A new coordinate system (\"u\", \"v\") is called isothermal when the curves of constant \"u\" do intersect the curves of constant \"v\" orthogonally and, in addition, the parameter spacing is the same — that is, for small enough \"h\", the little region with formula_2 and formula_3 is nearly square, not just nearly rectangular. The Beltrami equation is the equation that has to be solved in order to construct isothermal coordinate systems.\n\nTo see how this works, let \"S\" be an open set in C and let\n\nbe a smooth metric \"g\" on \"S\". The first fundamental form of \"g\"\n\nis a positive real matrix (\"E\" > 0, \"G\" > 0, \"EG\" − \"F\" > 0) that varies smoothly with \"x\" and \"y\".\n\nThe Beltrami coefficient of the metric \"g\" is defined to be\n\nThis coefficient has modulus strictly less than one since the identity\n\nimplies that \n\nLet \"f\"(\"x\",\"y\") =(\"u\"(\"x\",\"y\"),\"v\"(\"x\",\"y\")) be a smooth diffeomorphism of \"S\" onto another open set \"T\" in C. The map \"f\" preserves orientation just when its Jacobian is positive:\n\nAnd using \"f\" to pull back to \"S\" the standard Euclidean metric \"ds\" = \"du\" + \"dv\" on \"T\" induces a metric on \"S\" given by\n\na metric whose first fundamental form is\n\nWhen \"f\" both preserves orientation and induces a metric that differs from the original metric \"g\" only by a positive, smoothly varying scale factor \"r\"(\"x\", \"y\"), the new coordinates \"u\" and \"v\" defined on \"S\" by \"f\" are called isothermal coordinates. \n\nTo determine when this happens, we reinterpret \"f\" as a complex-valued function of a complex variable \"f\"(\"x\"+i\"y\") = \"u\"(\"x\"+i\"y\") + i\"v\"(\"x\"+i\"y\") so that we can apply the Wirtinger derivatives:\n\nSince\n\nthe metric induced by \"f\" is given by\n\nThe Beltrami quotient of this induced metric is defined to be formula_16. \n\nThe Beltrami quotient formula_16 of formula_18 equals the Beltrami coefficient formula_19 of the original metric \"g\" just when\n\nThe real and imaginary parts of this identity linearly relate formula_22 formula_23 formula_24 and formula_25 and solving for formula_26 and formula_27 gives\n\nIt follows that the metric induced by \"f\" is then \"r\"(\"x\", \"y\") \"g\"(\"x\",\"y\"), where formula_29 which is positive, while the Jacobian of \"f\" is then formula_30 which is also positive. So, when formula_31 the new coordinate system given by \"f\" is isothermal.\n\nConversely, consider a diffeomorphiam \"f\" that does give us isothermal coordinates. We then have\n\nwhere the scale factor \"r\"(\"x\", \"y\") has dropped out and the expression inside the square root is the perfect square formula_33 Since \"f\" must preserve orientation to give isothermal coordinates, the Jacobian formula_34 is the positive square root; so we have\n\nThe right-hand factors in the numerator and denominator are equal and, since the Jacobian is positive, their common value can't be zero; so formula_36 \n\nThus, the local coordinate system given by a diffeomorphism \"f\" is isothermal just when \"f\" solves the Beltrami equation for formula_37\n\nGauss proved the existence of isothermal coordinates locally in the analytic case by reducing the Beltrami to an ordinary differential equation in the complex domain. Here is a cookbook presentation of Gauss's technique.\n\nAn isothermal coordinate system, say in a neighborhood of the origin (\"x\", \"y\") = (0, 0), is given by the real and imaginary parts of a complex-valued function \"f\"(\"x\", \"y\") that satisfies \n\nLet formula_18 be such a function, and let formula_40 be a complex-valued function of a complex variable that is holomorphic and whose derivative is nowhere zero. Since any holomorphic function formula_40 has formula_42 identically zero, we have\n\nThus, the coordinate system given by the real and imaginary parts of formula_44 is also isothermal. Indeed, if we fix formula_18 to give one isothermal coordinate system, then all of the possible isothermal coordinate systems are given by formula_44 for the various holomorphic formula_40 with nonzero derivative.\n\nWhen \"E\", \"F\", and \"G\" are real analytic, Gauss constructed a particular isothermal coordinate system formula_48 the one that he chose being the one with formula_49 for all \"y\". So the \"v\" (imaginary) axis of his isothermal coordinate system coincides with the \"y\" axis of the original coordinates and is parameterized in the same way. All other isothermal coordinate systems are then of the form formula_50 for a holomorphic formula_40 with nonzero derivative. (If you would rather have the isothermal coordinate system formula_52 with formula_53 for all \"x\", so that your \"u\" (real) axis coincides with the original \"x\" axis, you can swap \"x\" with \"y\", apply the following, and then swap \"u\" with \"v\".)\n\nGauss lets \"q\"(\"t\") be some complex-valued function of a real variable \"t\" that satisfies the following ordinary differential equation:\n\nwhere \"E\", \"F\", and \"G\" are here evaluated at \"x\" = \"t\" and \"y\" = \"q\"(\"t\"). If we specify the value of \"q\"(\"s\") for some start value \"s\", this differential equation determines the values of \"q\"(\"t\") for \"t\" either less than or greater than \"s\". Gauss then defines his isothermal coordinate system \"h\" by setting \"h\"(\"x\", \"y\") to be formula_55 along the solution path of that differential equation that passes through the point (\"x\", \"y\"), and thus has \"q\"(\"x\") = \"y\".\n\nThis rule sets \"h\"(0, \"y\") to be formula_56, since the starting condition is then \"q\"(0)=\"y\". More generally, suppose that we move by an infinitesimal vector (\"dx\", \"dy\") away from some point (\"x\", \"y\"), where \"dx\" and \"dy\" satisfy\n\nSince formula_58, the vector (\"dx\", \"dy\") is then tangent to the solution curve of the differential equation that passes through the point (\"x\", \"y\"). Because we are assuming the metric to be analytic, it follows that \n\nfor some smooth, complex-valued function formula_60 We thus have\n\nfrom which it follows that \n\nGauss's function \"h\" thus gives the desired isothermal coordinates.\n\nIn the simplest cases the Beltrami equation can be solved only Hilbert space techniques and the Fourier transform. The method of proof is the prototype for the general solution using L spaces, although Adrien Douady has indicated a method for handling the general case using only Hilbert spaces: the method relies on the classical theory of quasiconformal mappings to establish Hölder estimates that are automatic in the L theory for \"p\" > 2.\nLet \"T\" be the Beurling transform on L(C) defined on the Fourier transform of an L function \"f\" as a multiplication operator:\n\nIt is a unitary operator and if \"h\" is a tempered distribution on C with partial derivatives in\nL then\n\nwhere the subscripts denote complex partial derivatives.\n\nThe fundamental solution of the operator\n\nis given by the distribution\n\na locally integrable function on C. Thus on Schwartz functions \"f\"\n\nThe same holds for distributions of compact support on C. In particular if \"f\" is an L function with compact support, then its Cauchy transform, defined as\n\nis locally square integrable. The above equation can be written\n\nMoreover, still regarding \"f\" and \"Cf\" as distributions,\n\nIndeed, the operator \"D\" is given on Fourier transforms as multiplication by \"iz\"/2 and \"C\" as multiplication by its inverse.\n\nNow in the Beltrami equation\n\nwith \"μ\" a smooth function of compact support, set\n\nand assume that the first derivatives of \"g\" are L. Let \"h\" = \"g\" = \"f\" – 1. Then\n\nIf \"A\" and \"B\" are the operators defined by\n\nthen their operator norms are strictly less that 1 and\n\nHence\n\nwhere the right hand sides can be expanded as Neumann series. It follows that\n\nhas the same support as \"μ\" and \"g\". Hence \"f\" is given by\n\nElliptic regularity can now be used to deduce that \"f\" is smooth.\n\nIn fact, off the support of \"μ\",\n\nso by Weyl's lemma \"f\" is even holomorphic for |\"z\"| > \"R\". Since \"f\" = \"CT*h\" + \"z\", it follows that\n\"f\" tends to 0 uniformly as |\"z\"| tends to ∞.\n\nThe elliptic regularity argument to prove smoothness, however, is the same everywhere and uses the theory of L Sobolev spaces on the torus. Let ψ be a smooth function of compact support on C, identically equal to 1 on a neighbourhood of the support of \"μ\" and set \"F\" = \"ψ\" \"f\". The support of \"F\" lies in a large square |\"x\"|, |\"y\"| ≤ \"R\", so, identifying opposite sides of the square, \"F\" and \"μ\" can be regarded as a distribution and smooth function on a torus T. By construction \"F\" is in \"L\"(T). As a distribution on T it satisfies\n\nwhere \"G\" is smooth. On the canonical basis \"e\" of L(T) with \"m\" in Z + \"i\" Z, define\n\nThus \"U\" is a unitary and on trigonometric polynomials or smooth functions \"P\"\n\nSimilarly it extends to a unitary on each Sobolev space H(T) with the same property. It is the counterpart on the torus of the Beurling transform. The standard theory of Fredholm operators shows that the operators corresponding to \"I\" – \"μ\" \"U\" and \"I\" – \"U\" \"μ\" are invertible on each Sobolev space. On the other hand,\n\nSince \"UG\" is smooth, so too is (\"I\" – \"μU\")\"F\" and hence also \"F\".\n\nThus the original function \"f\" is smooth. Regarded as a map of C = R into itself, the Jacobian is given by\n\nThis Jacobian is nowhere vanishing by a classical argument of . In fact formally writing\n\"f\" = \"e\", it follows that\n\nThis equation for \"k\" can be solved by the same methods as above giving a solution tending to 0 at ∞.\nBy uniqueness \"h\" + 1 = \"e\" so that\n\nis nowhere vanishing. Since \"f\" induces a smooth map of the Riemann sphere C ∪ ∞ into itself which is locally a diffeomorphism, \"f\" must be a diffeomorphism. In fact \"f\" must be onto by connectedness of the sphere, since its image is an open and closed subset; but then, as a covering map, \"f\" must cover each point of the sphere the same number of times. Since only ∞ is sent to ∞, it follows that \"f\" is one-to-one.\n\nThe solution \"f\" is a quasiconformal conformal diffeomorphism. These form a group and their Beltrami coefficients can be computed according to the following rule:\n\nMoreover, if \"f\"(0) = 0 and\n\nthen\n\nThis formula reflects the fact that on a Riemann surface, a Beltrami coefficient is not a function.\nUnder a holomorphic change of coordinate \"w\" = \"w\"(\"z\"), the coefficient is transformed to\n\nDefining a smooth Beltrami coefficient on the sphere in this way, if \"μ\" is such a coefficient then, taking a smooth bump function ψ equal to 0 near 0, equal 1 for |\"z\"| > 1 and satisfying 0 ≤ \"ψ\" ≤ 1, \"μ\" can be written as a sum of two Beltrami coefficients:\n\nLet \"g\" be the quasiconformal diffeomorphism of the sphere fixing 0 and ∞ with coefficient \n\"μ\". Let λ be the Beltrami coefficient of compact support on C defined by\n\nIf \"f\" is the quasiconformal diffeomorphism of the sphere fixing 0 and ∞ with coefficient λ, then\nthe transformation formulas above show that \"f\" ∘ \"g\" is a quasiconformal diffeomorphism of the sphere fixing 0 and ∞ with coefficient \"μ\".\n\nThe solutions of Beltrami's equation restrict to diffeomorphisms of the upper halfplane or unit disk if the coefficient \"μ\" has extra symmetry properties; since the two regions are related by a Möbius transformation (the Cayley transform), the two cases are essentially the same.\n\nFor the upper halfplane Im \"z\" > 0, if \"μ\" satisfies\n\nthen by uniqueness the solution \"f\" of the Beltrami equation satisfies\n\nso leaves the real axis and hence the upper halfplane invariant.\n\nSimilarly for the unit disc |\"z\"| < 1, if \"μ\" satisfies\n\nthen by uniqueness the solution \"f\" of the Beltrami equation satisfies\n\nso leaves the unit circle and hence the unit disk invariant.\n\nConversely Beltrami coefficients defined on the closures of the upper halfplane or unit disk which satisfy these conditions on the boundary can be \"reflected\" using the formulas above. If the extended functions are smooth the preceding theory can be applied. Otherwise the extensions will be continuous but with a jump in the derivatives at the boundary. In that case the more general theory for measurable coefficients \"μ\" is required and is most directly handled within the L theory.\n\nLet \"U\" be an open simply connected domain in the complex plane with smooth boundary containing 0 in its interior and let \"F\" be a diffeomorphism of the unit disk \"D\" onto \"U\" extending smoothly to the boundary and the identity on a neighbourhood of 0. Suppose that in addition the induced metric on the closure of the unit disk can be reflected in the unit circle to define a smooth metric on C. The corresponding Beltrami coefficient is then a smooth function on C vanishing near 0 and ∞ and satisfying\n\nThe quasiconformal diffeomorphism \"h\" of C satisfying\n\npreserves the unit circle together with its interior and exterior. From the composition formulas for Beltrami coefficients\n\nso that \"f\" = \"F\"∘ \"h\" is a smooth diffeomorphism between the closures of \"D\" and \"U\" which is holomorphic on the interior. Thus, if a suitable diffeomorphism \"F\" can be constructed, the mapping \"f\" proves the smooth Riemann mapping theorem for the domain \"U\".\n\nTo produce a diffeomorphism \"F\" with the properties above, it can be assumed after an affine transformation that the boundary of \"U\" has length 2π and that 0 lies in \"U\". The smooth version of the Schoenflies theorem produces a smooth diffeomorphism \"G\" from the closure of \"D\" onto the closure of \"u\" equal to the identity on a neighbourhood of 0 and with an explicit form on a tubular neighbourhood of the unit circle. In fact taking polar coordinates (\"r\",\"θ\") in \"R\" and letting (\"x\"(\"θ\"),\"y\"(\"θ\")) (\"θ\" in [0,2]) be a parametrization of ∂\"U\" by arclength, \"G\" has the form\n\nTaking \"t\" = 1 − \"r\" as parameter, the induced metric near the unit circle is given by\n\nwhere\n\nis the curvature of the plane curve (\"x\"(\"θ\"),\"y\"(\"θ\")).\n\nLet\n\nAfter a change of variable in the \"t\" coordinate and a conformal change in the metric, the metric takes the form\n\nwhere ψ is an analytic real-valued function of \"t\":\n\nA formal diffeomorphism sending (\"θ\",\"t\") to (\"f\"(\"θ\",\"t\"),\"t\") can be defined as a formal power series in \"t\":\n\nwhere the coefficients \"f\" are smooth functions on the circle. These coefficients can be defined by recurrence so that the transformed metric only has even powers of \"t\" in the coefficients. This condition is imposed by demanding that no odd powers of \"t\" appear in the formal power series expansion:\n\nBy Borel's lemma, there is a diffeomorphism defined in a neighbourhood of the unit circle, \"t\" = 0, for which the formal expression \"f\"(\"θ\",\"t\") is the Taylor series expansion in the \"t\" variable. It follows that, after composing with this diffeomorphism, the extension of the metric obtained by reflecting in the line \"t\" = 0 is smooth.\n\nDouady and others have indicated ways to extend the \"L\" theory to prove the existence and uniqueness of solutions when the Beltrami coefficient \"μ\" is bounded and measurable with \"L\" norm \"k\" strictly less than one. Their approach involved the theory of quasiconformal mappings to establish directly the solutions of Beltrami's equation when \"μ\" is smooth with fixed compact support are uniformly Hölder continuous. In the L approach Hölder continuity follows automatically from operator theory.\n\nThe \"L\" theory when \"μ\" is smooth of compact support proceeds as in the L case. By the Calderón–Zygmund theory the Beurling transform and its inverse are known to be continuous for the L norm. The Riesz–Thorin convexity theorem implies that the norms \"C\" are continuous functions of \"p\". In particular \"C\" tends to 1 when \"p\" tends to 2.\n\nIn the Beltrami equation\n\nwith \"μ\" a smooth function of compact support, set\n\nand assume that the first derivatives of \"g\" are L. Let \"h\" = \"g\" = \"f\" – 1. Then\n\nIf \"A\" and \"B\" are the operators defined by \"AF\" = \"TμF\" and \"BF\" = \"μTF\", then their operator norms are strictly less that 1 and (\"I\" − \"A\")\"h\" = \"T\"μ. Hence\n\nwhere the right hand sides can be expanded as Neumann series. It follows that\n\nhas the same support as \"μ\" and \"g\". Hence, up to the addition of a constant, \"f\" is given by\n\nConvergence of functions with fixed compact support in the L norm for \"p\" > 2 implies convergence in\nL, so these formulas are compatible with the L theory if \"p\" > 2.\n\nThe Cauchy transform \"C\" is not continuous on L except as a map into functions of vanishing mean oscillation.\n\nNote that the constant is added so that \"Pf\"(0) = 0. Since \"Pf\" only differs from \"Cf\" by a constant, it follows exactly as in the \"L\" theory that\n\nMoreover, \"P\" can be used instead of \"C\" to produce a solution:\n\nOn the other hand, the integrand defining \"Pf\" is in L if \"q\" = 1 − \"p\". The Hölder inequality implies that \"Pf\" is Hölder continuous with an explicit estimate:\n\nwhere\n\nFor any \"p\" > 2 sufficiently close to 2, \"C\"\"k\" <1. Hence the Neumann series for (\"I\" − \"A\") and (\"I\" − \"B\") converge. The Hölder estimates for \"P\" yield the following uniform estimates for the normalized solution of the Beltrami equation:\n\nIf \"μ\" is supported in |\"z\"| ≤ \"R\", then\n\nSetting \"w\" = \"z\" and \"w\" = 0, it follows that for |\"z\"| ≤ \"R\"\n\nwhere the constant \"C\" > 0 depends only on the L norm of \"μ\". So the Beltrami coefficient of \"f\" is smooth and supported in\n\"z\"| ≤ \"CR\". It has the same L norm as that of \"f\". So the inverse diffeomorphisms also satisfy uniform Hölder estimates.\n\nThe theory of the Beltrami equation can be extended to measurable Beltrami coefficients \"μ\". For simplicity only a special class of \"μ\" will be considered—adequate for most applications—namely those functions which are smooth an open set Ω (the regular set) with complement Λ a closed set of measure zero (the singular set). Thus Λ is a closed set that is contained in open sets of arbitrarily small area. For measurable Beltrami coefficients \"μ\" with compact support in |\"z\"| < \"R\", the solution of the Beltrami equation can be obtained as a limit of solutions for smooth Beltrami coefficients.\n\nIn fact in this case the singular set Λ is compact. Take smooth functions φ of compact support with 0 ≤ φ ≤ 1, equal to 1 on a neighborhood of Λ and 0 off a slightly larger neighbourhood, shrinking to Λ as \"n\" increases. Set\n\nThe \"μ\" are smooth with compact support in |\"z\"| < \"R\" and\n\nThe \"μ\" tend to \"μ\" in any \"L\" norm with \"p\" < ∞.\n\nThe corresponding normalised solutions \"f\" of the Beltrami equations and their inverses \"g\" satisfy uniform Hölder estimates. They are therefore equicontinuous on any compact subset of C; they are even holomorphic for |\"z\"| > \"R\". So by the Arzelà–Ascoli theorem, passing to a subsequence if necessary, it can be assumed that both \"f\" and \"g\" converge uniformly on compacta to \"f\" and \"g\". The limits will satisfy the same Hölder estimates and be holomorphic for |\"z\"| > \"R\". The relations \"f\"∘\"g\" = id = \"g\"∘\"f\" imply that in the limit \"f\"∘\"g\" = id = \"g\"∘\"f\", so that \"f\" and \"g\" are homeomorphisms.\n\n\n\n\n\n\n\nThis establishes the existence of homeomorphic solutions of Beltrami's equation in the case of Beltrami coefficients of compact support. It also shows that the inverse homeomorphisms and composed homeomorphisms satisfy Beltrami equations and that all computations can be performed by restricting to regular sets.\n\nIf the support is not compact the same trick used in the smooth case can be used to construct a solution in terms of two homeomorphisms associated to compactly supported Beltrami coefficients. Note that, because of the assumptions on the Beltrami coefficient, a Möbius transformation of the extended complex plane can be applied to make the singular set of the Beltrami coefficient compact. In that case one of the homeomorphisms can be chosen to be a diffeomorphism.\n\nThere are several proofs of the uniqueness of solutions of the Beltrami equation with a given Beltrami coefficient. Since applying a Möbius transformation of the complex plane to any solution gives another solution, solutions can be normalised so that they fix 0, 1 and ∞. The method of solution of the Beltrami equation using the Beurling transform also provides a proof of uniqueness for coefficients of compact support \"μ\" and for which the distributional derivatives are in 1 + L and L. The relations\n\nfor smooth functions ψ of compact support are also valid in the distributional sense for L functions \"h\" since they can be written as L of ψ's. If \"f\" is a solution of the Beltrami equation with \"f\"(0) = 0 and \"f\" - 1 in L then\n\nsatisfies\n\nSo \"F\" is weakly holomorphic. Applying Weyl's lemma it is possible to conclude that there exists a holomorphic function \"G\" that is equal to \"F\" almost everywhere. Abusing notation redefine \"F:=G\". The conditions \"F\" '(z) − 1 lies in L and \"F\"(0) = 0 force \"F\"(\"z\") = \"z\". Hence\n\nand so differentiating\n\nIf \"g\" is another solution then\n\nSince \"T\"μ has operator norm on L less than 1, this forces\n\nBut then from the Beltrami equation\n\nHence \"f\" − \"g\" is both holomorphic and antiholomorphic, so a constant. Since \"f\"(0) = 0 = \"g\"(0), it follows that \"f\" = \"g\". Note that since \"f\" is holomorphic off the support of \"μ\" and \"f\"(∞) = ∞, the conditions that the derivatives are locally in L force\n\nFor a general \"f\" satisfying Beltrami's equation and with distributional derivatives locally in L, it can be assumed after applying a Möbius transformation that 0 is not in the singular set of the Beltrami coefficient \"μ\". If \"g\" is a smooth diffeomorphism \"g\" with Beltrami coefficient λ supported near 0, the Beltrami coefficient \"ν\" for \"f\" ∘ \"g\" can be calculated directly using the change of variables formula for distributional derivatives:\n\n\"λ\" can be chosen so that ν vanishes near zero. Applying the map \"z\" results in a solution of Beltrami's equation with a Beltrami coefficient of compact support. The directional derivatives are still locally in L. The coefficient ν depends only on \"μ\", \"λ\" and \"g\", so any two solutions of the original equation will produce solutions near 0 with distributional derivatives locally in \"L\" and the same Beltrami coefficient. They are therefore equal. Hence the solutions of the original equation are equal.\n\nThe method used to prove the smooth Riemann mapping theorem can be generalized to multiply connected planar regions with smooth boundary. The Beltrami coefficient in these cases is smooth on an open set, the complement of which has measure zero. The theory of the Beltrami equation with measurable coefficients is therefore required.\n\nDoubly connected domains. If Ω is a doubly connected planar region, then there is a diffeomorphism \"F\" of an annulus \"r\" ≤ |z| ≤ 1 onto the closure of Ω, such that after a conformal change the induced metric on the annulus can be continued smoothly by reflection in both boundaries. The annulus is a fundamental domain for the group generated by the two reflections, which reverse orientation. The images of the fundamental domain under the group fill out C with 0 removed and the Beltrami coefficient is smooth there. The canonical solution \"h\" of the Beltrami equation on C, by the L theory is a homeomorphism. It is smooth on away from 0 by elliptic regularity. By uniqueness it preserves the unit circle, together with its interior and exterior. Uniqueness of the solution also implies that reflection there is a conjugate Möbius transformation \"g\" such that \"h\" ∘ \"R\" = \"g\" ∘ \"h\" where \"R\" denotes reflection in |\"z\"| = \"r\". Composing with a Möbius transformation that fixes the unit circle it can be assumed that \"g\" is a reflection in a circle |\"z\"| = \"s\" with \"s\" < 1. It follows that \"F\" ∘ \"h\" is a smooth diffeomorphism of the annulus \"s\" ≤ |\"z\"| ≤ 1 onto the closure of Ω, holomorphic in the interior.\n\nMultiply connected domains. For regions with a higher degree of connectivity \"k\" + 1, the result is essentially Bers' generalization of the retrosection theorem. There is a smooth diffeomorphism \"F\" of the region Ω, given by the unit disk with \"k\" open disks removed, onto the closure of Ω. It can be assumed that 0 lies in the interior of the domain. Again after a modification of the diffeomorphism and conformal change near the boundary, the metric can be assumed to be compatible with reflection. Let \"G\" be the group generated by reflections in the boundary circles of Ω. The interior of Ω iz a fundamental domain for \"G\". Moreover, the index two normal subgroup \"G\" consisting of orientation-preserving mappings is a classical Schottky group. Its fundamental domain consists of the original fundamental domain with its reflection in the unit circle added. If the reflection is \"R\", it is a free group with generators \"R\"∘\"R\" where \"R\" are the reflections in the interior circles in the original domain. The images of the original domain by the \"G\", or equivalently the reflected domain by the Schottky group, fill out the regular set for the Schottky group. It acts properly discontinuously there. The complement is the limit set of \"G\". It has measure zero. The induced metric on Ω extends by reflection to the regular set. The corresponding Beltrami coefficient is invariant for the reflection group generated by the reflections \"R\" for \"i\" ≥ 0. Since the limit set has measure zero, the Beltrami coefficient extends uniquely to a bounded measurable function on C. smooth on the regular set. The normalised solution of the Beltrami equation \"h\" is a smooth diffeomorphism of the closure of Ω onto itself preserving the unit circle, its exterior and interior. Necessarily \"h\" ∘ \"R\" = \"S\" ∘ \"h\". where \"S\" is the reflection in another circle in the unit disk. Looking at fixed points, the circles arising this way for different \"i\" must be disjoint. It follows that \"F\" ∘ \"h\" defines a smooth diffeomorphism of the unit disc with the interior of these circles removed onto the closure of Ω, which is holomorphic in the interior.\n\n showed that two compact Riemannian 2-manifolds \"M\", \"M\" of genus \"g\" > 1 can be simultaneously uniformized.\n\nAs topological spaces \"M\" and \"M\" are homeomorphic to a fixed quotient of the upper half plane H by a discrete cocompact subgroup Γ of PSL(2,R). Γ can be identified with the fundamental group of the manifolds and H is a universal covering space. The homeomorphisms can be chosen to be piecewise linear on corresponding triangulations. A result of implies that the homeomorphisms can be adjusted near the edges and the vertices of the triangulation to produce diffeomorphisms. The metric on \"M\" induces a metric on H which is Γ-invariant. Let \"μ\" be the corresponding Beltrami coefficient on H. It can be extended to C by reflection\n\nIt satisfies the invariance property\n\nfor \"g\" in Γ. The solution \"f\" of the corresponding Beltrami equation defines a homeomorphism of C, preserving the real axis and the upper and lower half planes. Conjugation of the group elements by \"f\" gives a new cocompact subgroup Γ of PSL(2,R). Composing the original diffeomorphism with the inverse of \"f\" then yield zero as the Beltrami coefficient. Thus the metric induced on H is invariant under Γ and conformal to the Poincaré metric on H. It must therefore be given by multiplying by a positive smooth function that is Γ-invariant. Any such function corresponds to a smooth function on \"M\". Dividing the metric on \"M\" by this function results in a conformally equivalent metric on \"M\" which agrees with the Poincaré metric on H / Γ. In this way \"M\" becomes a compact Riemann surface, i.e. is uniformized and inherits a natural complex structure.\n\nWith this conformal change in metric \"M\" can be identified with H / Γ. The diffeomorphism between onto \"M\" induces another metric on H which is invariant under Γ. It defines a Beltrami coefficient λ\nomn H which this time is extended to C by defining λ to be 0 off H. The solution \"h\" of the Beltrami equation is a homeomorphism of C which is holomorphic on the lower half plane and smooth on the upper half plane. The image of the real axis is a Jordan curve dividing C into two components. Conjugation of Γ by \"h\" gives a quasi-Fuchsian subgroup Γ of PSL(2,C). It leaves invariant the Jordan curve and acts properly discontinuously on each of the two components. The quotients of the two components by Γ are naturally identified with \"M\" and \"M\". This identification is compatible with the natural complex structures on both \"M\" and \"M\".\n\nAn orientation-preserving homeomorphism \"f\" of the circle is said to be quasisymmetric if there are positive constants \"a\" and \"b\" such that\n\nIf\n\nConversely if this condition is satisfied for all such triples of points, then \"f\" is quasisymmetric.\n\nAn apparently weaker condition on a homeomorphism \"f\" of the circle is that it be \"quasi-Möbius\", that is there are constants \"c\", \"d\" > 0 such that\n\nwhere\n\ndenotes the cross-ratio. In fact if \"f\" is quasisymmetric then it is also quasi-Möbius, with \"c\" = \"a\" and \"d\" = \"b\": this follows by multiplying the first inequality above for (\"z\",\"z\",\"z\") and (\"z\",\"z\",\"z\").\n\nConversely if \"f\" is a quasi-Möbius homeomorphism then it is also quasisymmetric. Indeed, it is immediate that if \"f\" is quasi-Möbius so is its inverse. It then follows that \"f\" (and hence \"f\") is Hölder continuous. To see this let \"S\" be the set of cube roots of unity, so that if \"a\" ≠ \"b\" in \"S\", then |\"a\" − \"b\"| = 2 sin /3 = . To prove a Hölder estimate, it can be assumed that \"x\" – \"y\" is uniformly small. Then both \"x\" and \"y\" are greater than a fixed distance away from \"a\", \"b\" in \"S\" with \"a\" ≠ \"b\", so the estimate follows by applying the quasi-Möbius inequality to \"x\", \"a\", \"y\", \"b\". To check that \"f\" is quasisymmetric, it suffices to find a uniform upper bound for |\"f\"(\"x\") − \"f\"(\"y\")| / |\"f\"(\"x\") − \"f\"(\"z\")| in the case of a triple with |\"x\" − \"z\"| = |\"x\" − \"y\"|, uniformly small. In this case there is a point \"w\" at a distance greater than 1 from \"x\", \"y\" and \"z\". Applying the quasi-Möbius inequality to \"x\", \"w\", \"y\" and \"z\" yields the required upper bound.\n\nA homeomorphism \"f\" of the unit circle can be extended to a homeomorphism \"F\" of the closed unit disk which is diffeomorphism on its interior. , generalizing earlier results of Ahlfors and Beurling, produced such an extension with the additional properties that it commutes with the action of SU(1,1) by Möbius transformations and is quasiconformal if \"f\" is quasisymmetric. (A less elementary method was also found independently by : Tukia's approach has the advantage of also applying in higher dimensions.) When \"f\" is a diffeomorphism of the circle, the Alexander extension provides another way of extending \"f\":\n\nwhere ψ is a smooth function with values in [0,1], equal to 0 near 0 and 1 near 1, and\n\nwith \"g\"(\"θ\" + 2) = \"g\"(\"θ\") + 2. give a survey of various methods of extension, including variants of the Ahlfors-Beurling extension which are smooth or analytic in the open unit disk.\n\nIn the case of a diffeomorphism, the Alexander extension \"F\" can be continued to any larger disk |\"z\"| < \"R\" with \"R\" > 1. Accordingly, in the unit disc\n\nThis is also true for the other extensions when \"f\" is only quasisymmetric.\n\nNow extend \"μ\" to a Beltrami coefficient on the whole of C by setting it equal to 0 for |\"z\"| ≥ 1. Let \"G\" be the corresponding solution of the Beltrami equation. Let \"F\"(\"z\") = \"G\" ∘ \"F\"(\"z\") for |\"z\"| ≤ 1 and\n\"F\"(\"z\") = \"G\" (\"z\") for |\"z\"| ≥ 1. Thus \"F\" and \"F\" are univalent holomorphic maps of |\"z\"| < 1 and |\"z\"| > 1 onto the inside and outside of a Jordan curve. They extend continuously to homeomorphisms \"f\" of the unit circle onto the Jordan curve on the boundary. By construction they satisfy the\nconformal welding condition:\n\n\n"}
{"id": "18706230", "url": "https://en.wikipedia.org/wiki?curid=18706230", "title": "Bolyai Prize", "text": "Bolyai Prize\n\nThe International János Bolyai Prize of Mathematics is an international prize for mathematicians founded by the Hungarian Academy of Sciences. The prize is awarded in every five years to mathematicians having published their monograph describing their own highly important new results in the past 10 years.\n\n"}
{"id": "19475686", "url": "https://en.wikipedia.org/wiki?curid=19475686", "title": "Cheetah Math", "text": "Cheetah Math\n\nCheetah Math: Learning about Division from Baby Cheetahs is a math book published by Henry Holt and Co. in 2007. \"Cheetah Math\" was designed to help students understand division. Ann Whitehead Nagda wrote the book with the cooperation of the San Diego Zoo. The book follows the lives of two baby cheetahs, Majani and Kubali, and relates their story to the principles of division. Sally Woolsey called the book \"well done\" and it is a popular item in many elementary school libraries. \"Kirkus Reviews\" called the book \"A great addition to both the math and wild-animal conservation bookshelves\". The \"School Library Journal\" also gave a favorable review, saying \"Cheetah Math\" \"is a wonderful cross-curricular book and an appealing way to introduce math\".\n\n"}
{"id": "6042", "url": "https://en.wikipedia.org/wiki?curid=6042", "title": "Compact space", "text": "Compact space\n\nIn mathematics, and more specifically in general topology, compactness is a property that generalizes the notion of a subset of Euclidean space being closed (that is, containing all its limit points) and bounded (that is, having all its points lie within some fixed distance of each other). Examples include a closed interval, a rectangle, or a finite set of points. This notion is defined for more general topological spaces than Euclidean space in various ways.\n\nOne such generalization is that a topological space is \"sequentially\" compact if every infinite sequence of points sampled from the space has an infinite subsequence that converges to some point of the space. The Bolzano–Weierstrass theorem states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the \"closed\" unit interval some of those points will get arbitrarily close to some real number in that space. For instance, some of the numbers accumulate to 0 (others accumulate to 1). The same set of points would not accumulate to any point of the \"open\" unit interval ; so the open unit interval is not compact. Euclidean space itself is not compact since it is not bounded. In particular, the sequence of points has no subsequence that converges to any real number.\n\nApart from closed and bounded subsets of Euclidean space, typical examples of compact spaces include spaces consisting not of geometrical points but of functions. The term \"compact\" was introduced into mathematics by Maurice Fréchet in 1904 as a distillation of this concept. Compactness in this more general situation plays an extremely important role in mathematical analysis, because many classical and important theorems of 19th-century analysis, such as the extreme value theorem, are easily generalized to this situation. A typical application is furnished by the Arzelà–Ascoli theorem or the Peano existence theorem, in which one is able to conclude the existence of a function with some required properties as a limiting case of some more elementary construction.\n\nVarious equivalent notions of compactness, including sequential compactness and limit point compactness, can be developed in general metric spaces. In general topological spaces, however, different notions of compactness are not necessarily equivalent. The most useful notion, which is the standard definition of the unqualified term \"compactness\", is phrased in terms of the existence of finite families of open sets that \"cover\" the space in the sense that each point of the space lies in some set contained in the family. This more subtle notion, introduced by Pavel Alexandrov and Pavel Urysohn in 1929, exhibits compact spaces as generalizations of finite sets. In spaces that are compact in this sense, it is often possible to patch together information that holds locally—that is, in a neighborhood of each point—into corresponding statements that hold throughout the space, and many theorems are of this character.\n\nThe term compact set is sometimes a synonym for compact space, but usually refers to a compact subspace of a topological space.\n\nIn the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness. On the one hand, Bernard Bolzano (1817) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a limit point. Bolzano's proof relied on the method of bisection: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected. The process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts until it closes down on the desired limit point. The full significance of Bolzano's theorem, and its method of proof, would not emerge until almost 50 years later when it was rediscovered by Karl Weierstrass.\n\nIn the 1880s, it became clear that results similar to the Bolzano–Weierstrass theorem could be formulated for spaces of functions rather than just numbers or geometrical points. The idea of regarding functions as themselves points of a generalized space dates back to the investigations of Giulio Ascoli and Cesare Arzelà. The culmination of their investigations, the Arzelà–Ascoli theorem, was a generalization of the Bolzano–Weierstrass theorem to families of continuous functions, the precise conclusion of which was that it was possible to extract a uniformly convergent sequence of functions from a suitable family of functions. The uniform limit of this sequence then played precisely the same role as Bolzano's \"limit point\". Towards the beginning of the twentieth century, results similar to that of Arzelà and Ascoli began to accumulate in the area of integral equations, as investigated by David Hilbert and Erhard Schmidt. For a certain class of Green functions coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzelà–Ascoli theorem held in the sense of mean convergence—or convergence in what would later be dubbed a Hilbert space. This ultimately led to the notion of a compact operator as an offshoot of the general notion of a compact space. It was Maurice Fréchet who, in 1906, had distilled the essence of the Bolzano–Weierstrass property and coined the term \"compactness\" to refer to this general phenomenon (he used the term already in his 1904 paper which led to the famous 1906 thesis).\n\nHowever, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the continuum, which was seen as fundamental for the rigorous formulation of analysis. In 1870, Eduard Heine showed that a continuous function defined on a closed and bounded interval was in fact uniformly continuous. In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. The significance of this lemma was recognized by Émile Borel (1895), and it was generalized to arbitrary collections of intervals by Pierre Cousin (1895) and Henri Lebesgue (1904). The Heine–Borel theorem, as the result is now known, is another special property possessed by closed and bounded sets of real numbers.\n\nThis property was significant because it allowed for the passage from local information about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function). This sentiment was expressed by , who also exploited it in the development of the integral now bearing his name. Ultimately the Russian school of point-set topology, under the direction of Pavel Alexandrov and Pavel Urysohn, formulated Heine–Borel compactness in a way that could be applied to the modern notion of a topological space. showed that the earlier version of compactness due to Fréchet, now called (relative) sequential compactness, under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers. It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.\nAny finite space is trivially compact. A non-trivial example of a compact space is the (closed) unit interval of real numbers. If one chooses an infinite number of distinct points in the unit interval, then there must be some accumulation point in that interval. For instance, the odd-numbered terms of the sequence get arbitrarily close to 0, while the even-numbered ones get arbitrarily close to 1. The given example sequence shows the importance of including the boundary points of the interval, since the limit points must be in the space itself — an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be bounded, since in the interval one could choose the sequence of points , of which no sub-sequence ultimately gets arbitrarily close to any given real number.\n\nIn two dimensions, closed disks are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary. However, an open disk is not compact, because a sequence of points can tend to the boundary without getting arbitrarily close to any point in the interior. Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can tend to the missing point, thereby not getting arbitrarily close to any point \"within\" the space. Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.\n\nVarious definitions of compactness may apply, depending on the level of generality. A subset of Euclidean space in particular is called compact if it is closed and bounded. This implies, by the Bolzano–Weierstrass theorem, that any infinite sequence from the set has a subsequence that converges to a point in the set. Various equivalent notions of compactness, such as sequential compactness and limit point compactness, can be developed in general metric spaces.\n\nIn general topological spaces, however, the different notions of compactness are not equivalent, and the most useful notion of compactness—originally called \"bicompactness\"—is defined using covers consisting of open sets (see \"Open cover definition\" below). That this form of compactness holds for closed and bounded subsets of Euclidean space is known as the Heine–Borel theorem. Compactness, when defined in this manner, often allows one to take information that is known locally—in a neighbourhood of each point of the space—and to extend it to information that holds globally throughout the space. An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is uniformly continuous; here, continuity is a local property of the function, and uniform continuity the corresponding global property.\n\nFormally, a topological space is called \"compact\" if each of its open covers has a finite subcover. That is, is compact if for every collection of open subsets of such that\n\nthere is a finite subset of such that\n\nSome branches of mathematics such as algebraic geometry, typically influenced by the French school of Bourbaki, use the term \"quasi-compact\" for the general notion, and reserve the term \"compact\" for topological spaces that are both Hausdorff and \"quasi-compact\". A compact set is sometimes referred to as a \"compactum\", plural \"compacta\".\n\nA subset of a topological space is said to be compact if it is compact as a subspace (in the subspace topology). That is, is compact if for every arbitrary collection of open subsets of such that\n\nthere is a finite subset of such that\n\nCompactness is a \"topological\" property. That is, if formula_5, with subset equipped with the subspace topology, then is compact in if and only if is compact in .\n\nAssuming the axiom of choice, the following are equivalent:\n\nFor any subset \"A\" of Euclidean space R, \"A\" is compact if and only if it is closed and bounded; this is the Heine–Borel theorem.\n\nAs a Euclidean space is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed interval or closed \"n\"-ball.\n\nFor any metric space (\"X\", \"d\"), the following are equivalent:\n\nA compact metric space (\"X\", \"d\") also satisfies the following properties:\n\nLet \"X\" be a topological space and C(\"X\") the ring of real continuous functions on \"X\". For each \"p\"∈\"X\", the evaluation map formula_6\ngiven by ev(\"f\")=\"f\"(\"p\") is a ring homomorphism. The kernel of ev is a maximal ideal, since the residue field is the field of real numbers, by the first isomorphism theorem. A topological space \"X\" is pseudocompact if and only if every maximal ideal in C(\"X\") has residue field the real numbers. For completely regular spaces, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism. There are pseudocompact spaces that are not compact, though.\n\nIn general, for non-pseudocompact spaces there are always maximal ideals \"m\" in C(\"X\") such that the residue field C(\"X\")/\"m\" is a (non-archimedean) hyperreal field. The framework of non-standard analysis allows for the following alternative characterization of compactness: a topological space \"X\" is compact if and only if every point \"x\" of the natural extension \"*X\" is infinitely close to a point \"x\" of \"X\" (more precisely, \"x\" is contained in the monad of \"x\").\n\nA space \"X\" is compact if its hyperreal extension \"*X\" (constructed, for example, by the ultrapower construction) has the property that every point of \"*X\" is infinitely close to some point of \"X\"⊂\"*X\". For example, an open real interval is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of \"X\".\n\nA continuous image of a compact space is compact.\nThis implies the extreme value theorem: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum. (Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a proper map is compact.\n\nA closed subset of a compact space is compact, and a finite union of compact sets is compact.\n\nThe product of any collection of compact spaces is compact. (This is Tychonoff's theorem, which is equivalent to the axiom of choice.)\n\nEvery topological space \"X\" is an open dense subspace of a compact space having at most one point more than \"X\", by the Alexandroff one-point compactification. By the same construction, every locally compact Hausdorff space \"X\" is an open dense subspace of a compact Hausdorff space having at most one point more than \"X\".\n\nA nonempty compact subset of the real numbers has a greatest element and a least element.\n\nLet \"X\" be a simply ordered set endowed with the order topology. Then \"X\" is compact if and only if \"X\" is a complete lattice (i.e. all subsets have suprema and infima).\n\n\n\n\n\n"}
{"id": "55731874", "url": "https://en.wikipedia.org/wiki?curid=55731874", "title": "Complex Wishart distribution", "text": "Complex Wishart distribution\n\n_p</math> is the formula_1-variate complex multivariate gamma function\n | cdf =\n\nIn statistics, the complex Wishart distribution is a complex version of the Wishart distribution. It is the distribution of formula_5 times the sample Hermitian covariance matrix of formula_5 zero-mean independent Gaussian random variables. It has support for formula_7 Hermitian positive definite matrices.\n"}
{"id": "2495030", "url": "https://en.wikipedia.org/wiki?curid=2495030", "title": "Connected dominating set", "text": "Connected dominating set\n\nIn graph theory, a connected dominating set and a maximum leaf spanning tree are two closely related structures defined on an undirected graph.\n\nA connected dominating set of a graph \"G\" is a set \"D\" of vertices with two properties:\nA minimum connected dominating set of a graph \"G\" is a connected dominating set with the smallest possible cardinality among all connected dominating sets of \"G\". The connected domination number of \"G\" is the number of vertices in the minimum connected dominating set.\n\nAny spanning tree \"T\" of a graph \"G\" has at least two leaves, vertices that have only one edge of \"T\" incident to them. A maximum leaf spanning tree is a spanning tree that has the largest possible number of leaves among all spanning trees of \"G\". The max leaf number of \"G\" is the number of leaves in the maximum leaf spanning tree.\n\nIf \"d\" is the connected domination number of an \"n\"-vertex graph \"G\", where \"n > 2\", and \"l\" is its max leaf number, then the three quantities \"d\", \"l\", and \"n\" obey the simple equation\n\nIf \"D\" is a connected dominating set, then there exists a spanning tree in \"G\" whose leaves include all vertices that are not in \"D\": form a spanning tree of the subgraph induced by \"D\", together with edges connecting each remaining vertex \"v\" that is not in \"D\" to a neighbor of \"v\" in \"D\". This shows that \n\nIn the other direction, if \"T\" is any spanning tree in \"G\", then the vertices of \"T\" that are not leaves form a connected dominating set of \"G\". This shows that Putting these two inequalities together proves the equality \n\nTherefore, in any graph, the sum of the connected domination number and the max leaf number equals the total number of vertices.\nComputationally, this implies that determining the connected domination number is equally difficult as finding the max leaf number.\n\nIt is NP-complete to test whether there exists a connected dominating set with size less than a given threshold, or equivalently to test whether there exists a spanning tree with at least a given number of leaves. Therefore, it is believed that the minimum connected dominating set problem and the maximum leaf spanning tree problem cannot be solved in polynomial time.\n\nWhen viewed in terms of approximation algorithms, connected domination and maximum leaf spanning trees are not the same: approximating one to within a given approximation ratio is not the same as approximating the other to the same ratio.\nThere exists an approximation for the minimum connected dominating set that achieves a factor of , where Δ is the maximum degree of a vertex in G.\nThe maximum leaf spanning tree problem is MAX-SNP hard, implying that no polynomial time approximation scheme is likely. However, it can be approximated to within a factor of 2 in polynomial time.\n\nBoth problems may be solved, on -vertex graphs, in time . The maximum leaf problem is fixed-parameter tractable, meaning that it can be solved in time exponential in the number of leaves but only polynomial in the input graph size. The klam value of these algorithms (intuitively, a number of leaves up to which the problem can be solved within a reasonable amount of time) has gradually increased, as algorithms for the problem have improved, to approximately 37, and it has been suggested that at least 50 should be achievable.\n\nIn graphs of maximum degree three, the connected dominating set and its complementary maximum leaf spanning tree problem can be solved in polynomial time, by transforming them into an instance of the matroid parity problem for linear matroids.\n\nConnected dominating sets are useful in the computation of routing for mobile ad hoc networks. In this application, a small connected dominating set is used as a backbone for communications, and nodes that are not in this set communicate by passing messages through neighbors that are in the set.\n\nThe max leaf number has been employed in the development of fixed-parameter tractable algorithms: several NP-hard optimization problems may be solved in polynomial time for graphs of bounded max leaf number.\n\n"}
{"id": "4577392", "url": "https://en.wikipedia.org/wiki?curid=4577392", "title": "Edge cover", "text": "Edge cover\n\nIn graph theory, an edge cover of a graph is a set of edges such that every vertex of the graph is incident to at least one edge of the set.\nIn computer science, the minimum edge cover problem is the problem of finding an edge cover of minimum size. It is an optimization problem that belongs to the class of covering problems and can be solved in polynomial time.\n\nFormally, an edge cover of a graph \"G\" is a set of edges \"C\" such that each vertex in \"G\" is incident with at least one edge in \"C\". The set \"C\" is said to \"cover\" the vertices of \"G\". The following figure shows examples of edge coverings in two graphs.\n\nA minimum edge covering is an edge covering of smallest possible size. The edge covering number formula_1 is the size of a minimum edge covering. The following figure shows examples of minimum edge coverings.\n\nNote that the figure on the right is not only an edge cover but also a matching. In particular, it is a perfect matching: a matching \"M\" in which each vertex is incident with exactly one edge in \"M\". A perfect matching (if it exists) is always a minimum edge covering.\n\n\nA smallest edge cover can be found in polynomial time by finding a maximum matching and extending it greedily so that all vertices are covered. In the following figure, a maximum matching is marked with red; the extra edges that were added to cover unmatched nodes are marked with blue. (The figure on the right shows a graph in which a maximum matching is a perfect matching; hence it already covers all vertices and no extra edges were needed.)\n\nOn the other hand, the related problem of finding a smallest vertex cover is an NP-hard problem.\n\n\n"}
{"id": "880235", "url": "https://en.wikipedia.org/wiki?curid=880235", "title": "Envelope (mathematics)", "text": "Envelope (mathematics)\n\nIn geometry, an envelope of a family of curves in the plane is a curve that is tangent to each member of the family at some point, and these points of tangency together form the whole envelope. Classically, a point on the envelope can be thought of as the intersection of two \"infinitesimally adjacent\" curves, meaning the limit of intersections of nearby curves. This idea can be generalized to an envelope of surfaces in space, and so on to higher dimensions.\n\nTo have an envelope, the individual members of the family of curves need to be differentiable curves, for otherwise the concept of tangency does not apply, and there has to be a smooth transition proceeding through the members. But even if these conditions are satisfied, a given family may fail to have an envelope. A simple example of this is given by a family of concentric circles of expanding radius.\n\nLet each curve \"C\" in the family be given as the solution of an equation \"f\"(\"x\", \"y\")=0 (see implicit curve), where \"t\" is a parameter. Write \"F\"(\"t\", \"x\", \"y\")=\"f\"(\"x\", \"y\") and assume \"F\" is differentiable.\n\nThe envelope of the family \"C\" is then defined as the set formula_1 of points (\"x\",\"y\") for which, simultaneously, \nfor some value of \"t\",\nwhere formula_3 is the partial derivative of \"F\" with respect to \"t\".\n\nIf \"t\" and \"u\", \"t\"≠\"u\" are two values of the parameter then the intersection of the curves \"C\" and \"C\" is given by \nor, equivalently,\nLetting u→t gives the definition above.\n\nAn important special case is when \"F\"(\"t\", \"x\", \"y\") is a polynomial in \"t\". This includes, by clearing denominators, the case where \"F\"(\"t\", \"x\", \"y\") is a rational function in \"t\". In this case, the definition amounts to \"t\" being a double root of \"F\"(\"t\", \"x\", \"y\"), so the equation of the envelope can be found by setting the discriminant of \"F\" to 0 (because the definition demands F=0 at some t and first derivative =0 i.e. it's value 0 and it is min/max at that t).\n\nFor example, let \"C\" be the line whose \"x\" and \"y\" intercepts are \"t\" and 11−\"t\", this is shown in the animation above. The equation of \"C\" is\nor, clearing fractions,\nThe equation of the envelope is then\n\nOften when \"F\" is not a rational function of the parameter it may be reduced to this case by an appropriate substitution. For example, if the family is given by \"C\" with an equation of the form \"u\"(\"x\", \"y\")cosθ+\"v\"(\"x\", \"y\")sinθ=\"w\"(\"x\", \"y\"), then putting \"t\"=\"e\", cosθ=(\"t\"+1/\"t\")/2, sinθ=(\"t\"-1/\"t\")/2\"i\" changes the equation of the curve to\nor\nThe equation of the envelope is then given by setting the discriminant to 0:\nor \n\n\nThen formula_13, formula_14 and formula_15, where formula_1 is the set of points defined at the beginning of this subsection's parent section.\n\nThese definitions \"E\", \"E\", and \"E\" of the envelope may be different sets. Consider for instance the curve parametrised by where . The one-parameter family of curves will be given by the tangent lines to γ.\n\nFirst we calculate the discriminant formula_17. The generating function is\nCalculating the partial derivative . It follows that either or . First assume that . Substituting into F: formula_19\nand so, assuming that \"t\" ≠ 0, it follows that if and only if . Next, assuming that and substituting into \"F\" gives . So, assuming , it follows that if and only if . Thus the discriminant is the original curve and its tangent line at γ(0):\n\nNext we calculate \"E\". One curve is given by and a nearby curve is given by where ε is some very small number. The intersection point comes from looking at the limit of as ε tends to zero. Notice that if and only if\nIf then \"L\" has only a single factor of ε. Assuming that then the intersection is given by\nSince it follows that . The \"y\" value is calculated by knowing that this point must lie on a tangent line to the original curve γ: that . Substituting and solving gives \"y\" = \"t\". When , \"L\" is divisible by ε. Assuming that then the intersection is given by\nIt follows that , and knowing that gives . It follows that\n\nNext we calculate \"E\". The curve itself is the curve that is tangent to all of its own tangent lines. It follows that\n\nFinally we calculate \"E\". Every point in the plane has at least one tangent line to γ passing through it, and so region filled by the tangent lines is the whole plane. The boundary \"E\" is therefore the empty set. Indeed, consider a point in the plane, say (\"x\",\"y\"). This point lies on a tangent line if and only if there exists a \"t\" such that\nThis is a cubic in \"t\" and as such has at least one real solution. It follows that at least one tangent line to γ must pass through any given point in the plane. If and then each point (\"x\",\"y\") has exactly one tangent line to γ passing through it. The same is true if . If and then each point (\"x\",\"y\") has exactly three distinct tangent lines to γ passing through it. The same is true if and . If and then each point (\"x\",\"y\") has exactly two tangent lines to γ passing through it (this corresponds to the cubic having one ordinary root and one repeated root). The same is true if and . If and , i.e., , then this point has a single tangent line to γ passing through it (this corresponds to the cubic having one real root of multiplicity 3). It follows that\n\nIn string art it is common to cross-connect two lines of equally spaced pins. What curve is formed?\n\nFor simplicity, set the pins on the \"x\"- and \"y\"-axes; a non-orthogonal layout is a rotation and scaling away. A general straight-line thread connects the two points (0, \"k\"−\"t\") and (\"t\", 0), where \"k\" is an arbitrary scaling constant, and the family of lines is generated by varying the parameter \"t\". From simple geometry, the equation of this straight line is \"y\" = −(\"k\" − \"t\")\"x\"/\"t\" + \"k\" − \"t\". Rearranging and casting in the form \"F\"(\"x\",\"y\",\"t\") = 0 gives:\n\nNow differentiate \"F\"(\"x\",\"y\",\"t\") with respect to \"t\" and set the result equal to zero, to get\n\nThese two equations jointly define the equation of the envelope. From (2) we have \"t\" = (−\"y\" + \"x\" + \"k\")/2. Substituting this value of \"t\" into (1) and simplifying gives an equation for the envelope in terms of \"x\" and \"y\" only:\n\nThis is the familiar implicit conic section form, in this case a parabola. Parabolae remain parabolae under rotation and scaling; thus the string art forms a parabolic arc (\"arc\" since only a portion of the full parabola is produced). In this case an anticlockwise rotation through 45° gives the orthogonal parabolic equation \"y\" = \"x\"/(\"k\") + \"k\"/(2). The final step of eliminating \"t\" may not always be possible to do analytically, depending on the form of \"F\"(\"x\",\"y\",\"t\").\n\nLet \"I\" ⊂ R be an open interval and let γ : \"I\" → R be a smooth plane curve parametrised by arc length. Consider the one-parameter family of normal lines to γ(\"I\"). A line is normal to γ at γ(\"t\") if it passes through γ(\"t\") and is perpendicular to the tangent vector to γ at γ(\"t\"). Let T denote the unit tangent vector to γ and let N denote the unit normal vector. Using a dot to denote the dot product, the generating family for the one-parameter family of normal lines is given by where\nClearly (x − γ)·T = 0 if and only if x − γ is perpendicular to T, or equivalently, if and only if x − γ is parallel to N, or equivalently, if and only if x = γ + λN for some λ ∈ R. It follows that\nis exactly the normal line to γ at γ(\"t\"). To find the discriminant of \"F\" we need to compute its partial derivative with respect to \"t\":\nwhere κ is the plane curve curvature of γ. It has been seen that \"F\" = 0 if and only if x - γ = λN for some λ ∈ R. Assuming that \"F\" = 0 gives\nAssuming that κ ≠ 0 it follows that λ = 1/κ and so\nThis is exactly the evolute of the curve γ.\n\nThe following example shows that in some cases the envelope of a family of curves may be seen as the topologic boundary of a union of sets, whose boundaries are the curves of the envelope. For formula_34 and formula_35 consider the (open) right triangle in a Cartesian plane with vertices formula_36, formula_37 and formula_38\nFix an exponent formula_40, and consider the union of all the triangles formula_41 subjected to the constraint formula_42, that is the open set\nTo write a Cartesian representation for formula_44, start with any formula_45, formula_46 satisfying formula_42 and any formula_48. The Hölder inequality in formula_49 with respect to the conjugated exponents formula_50 and formula_51 gives:\nwith equality if and only if formula_53.\nIn terms of a union of sets the latter inequality reads: the point formula_54 belongs to the set formula_44, that is, it belongs to some formula_56 with formula_57, if and only if it satisfies\nMoreover, the boundary in formula_59 of the set formula_60 is the envelope of the corresponding family of line segments\n(that is, the hypotenuses of the triangles), and has Cartesian equation\nNotice that, in particular, the value formula_63 gives the arc of parabola of the example 1, and the value formula_64 (meaning that all hypotenuses are unit length segments) gives the astroid.\n\nWe consider the following example of envelope in motion. Suppose at initial height 0, one casts a projectile into the air with constant initial velocity \"v\" but different elevation angles θ. Let \"x\" be the horizontal axis in the motion surface, and let \"y\" denote the vertical axis. Then the motion gives the following differential dynamical system:\nwhich satisfies four initial conditions:\nHere \"t\" denotes motion time, θ is elevation angle, \"g\" denotes gravitational acceleration, and \"v\" is the constant initial speed (not velocity). The solution of the above system can take an implicit form:\nTo find its envelope equation, one may compute the desired derivative:\nBy eliminating θ, one may reach the following envelope equation:\nClearly the resulted envelope is also a concave parabola.\n\nA one-parameter family of surfaces in three-dimensional Euclidean space is given by a set of equations\n\ndepending on a real parameter \"a\". For example, the tangent planes to a surface along a curve in the surface form such a family.\n\nTwo surfaces corresponding to different values \"a\" and \"a' \" intersect in a common curve defined by\n\nIn the limit as \"a' \" approaches \"a\", this curve tends to a curve contained in the surface at \"a\"\n\nThis curve is called the characteristic of the family at \"a\". As \"a\" varies the locus of these characteristic curves defines a surface called the envelope of the family of surfaces.\n\nThe idea of an envelope of a family of smooth submanifolds follows naturally. In general, if we have a family of submanifolds with codimension \"c\" then we need to have at least a \"c\"-parameter family of such submanifolds. For example: a one-parameter family of curves in three-space (\"c\" = 2) does not, generically, have an envelope.\n\nEnvelopes are connected to the study of ordinary differential equations (ODEs), and in particular singular solutions of ODEs. Consider, for example, the one-parameter family of tangent lines to the parabola \"y\" = \"x\". These are given by the generating family . The zero level set gives the equation of the tangent line to the parabola at the point (\"t\",\"t\"). The equation can always be solved for \"y\" as a function of \"x\" and so, consider\nSubstituting\ngives the ODE\nNot surprisingly \"y\" = 2\"tx\" − \"t\" are all solutions to this ODE. However, the envelope of this one-parameter family of lines, which is the parabola \"y\" = \"x\", is also a solution to this ODE. Another famous example is Clairaut's equation.\n\nEnvelopes can be used to construct more complicated solutions of first order partial differential equations (PDEs) from simpler ones. Let \"F\"(\"x\",\"u\",D\"u\") = 0 be a first order PDE, where \"x\" is a variable with values in an open set Ω ⊂ R, \"u\" is an unknown real-valued function, D\"u\" is the gradient of \"u\", and \"F\" is a continuously differentiable function that is regular in D\"u\". Suppose that \"u\"(\"x\";\"a\") is an \"m\"-parameter family of solutions: that is, for each fixed \"a\" ∈ \"A\" ⊂ R, \"u\"(\"x\";\"a\") is a solution of the differential equation. A new solution of the differential equation can be constructed by first solving (if possible)\nfor \"a\" = φ(\"x\") as a function of \"x\". The envelope of the family of functions {\"u\"(·,\"a\")} is defined by\nand also solves the differential equation (provided that it exists as a continuously differentiable function).\n\nGeometrically, the graph of \"v\"(\"x\") is everywhere tangent to the graph of some member of the family \"u\"(\"x\";\"a\"). Since the differential equation is first order, it only puts a condition on the tangent plane to the graph, so that any function everywhere tangent to a solution must also be a solution. The same idea underlies the solution of a first order equation as an integral of the Monge cone. The Monge cone is a cone field in the R of the (\"x\",\"u\") variables cut out by the envelope of the tangent spaces to the first order PDE at each point. A solution of the PDE is then an envelope of the cone field.\n\nIn Riemannian geometry, if a smooth family of geodesics through a point \"P\" in a Riemannian manifold has an envelope, then \"P\" has a conjugate point where any geodesic of the family intersects the envelope. The same is true more generally in the calculus of variations: if a family of extremals to a functional through a given point \"P\" has an envelope, then a point where an extremal intersects the envelope is a conjugate point to \"P\".\n\nIn geometrical optics, a caustic is the envelope of a family of light rays. In this picture there is an arc of a circle. The light rays (shown in blue) are coming from a source \"at infinity\", and so arrive parallel. When they hit the circular arc the light rays are scattered in different directions according to the law of reflection. When a light ray hits the arc at a point the light will be reflected as though it had been reflected by the arc's tangent line at that point. The reflected light rays give a one-parameter family of lines in the plane. The envelope of these lines is the reflective caustic. A reflective caustic will generically consist of smooth points and ordinary cusp points.\n\nFrom the point of view of the calculus of variations, Fermat's principle (in its modern form) implies that light rays are the extremals for the length functional\namong smooth curves γ on [\"a\",\"b\"] with fixed endpoints γ(\"a\") and γ(\"b\"). The caustic determined by a given point \"P\" (in the image the point is at infinity) is the set of conjugate points to \"P\".\n\nLight may pass through anisotropic inhomogeneous media at different rates depending on the direction and starting position of a light ray. The boundary of the set of points to which light can travel from a given point q after a time \"t\" is known as the wave front after time \"t\", denoted here by Φ(\"t\"). It consists of precisely the points that can be reached from q in time \"t\" by travelling at the speed of light. Huygens's principle asserts that the wave front set is the envelope of the family of wave fronts for q ∈ Φ(\"t\"). More generally, the point q could be replaced by any curve, surface or closed set in space.\n\n\n"}
{"id": "20283423", "url": "https://en.wikipedia.org/wiki?curid=20283423", "title": "FKG inequality", "text": "FKG inequality\n\nIn mathematics, the Fortuin–Kasteleyn–Ginibre (FKG) inequality is a correlation inequality, a fundamental tool in statistical mechanics and probabilistic combinatorics (especially random graphs and the probabilistic method), due to . Informally, it says that in many random systems, increasing events are positively correlated, while an increasing and a decreasing event are negatively correlated.\n\nAn earlier version, for the special case of i.i.d. variables, called Harris inequality, is due to , see . One generalization of the FKG inequality is the below, and an even further generalization is the Ahlswede–Daykin \"four functions\" theorem (1978). Furthermore, it has the same conclusion as the Griffiths inequalities, but the hypotheses are different.\n\nLet formula_1 be a finite distributive lattice, and \"μ\" a nonnegative function on it, that is assumed to satisfy the (FKG) lattice condition (sometimes a function satisfying this condition is called log supermodular) i.e.,\nfor all \"x\", \"y\" in the lattice formula_1.\n\nThe FKG inequality then says that for any two monotonically increasing functions \"ƒ\" and \"g\" on formula_1, the following positive correlation inequality holds:\n\nThe same inequality (positive correlation) is true when both \"ƒ\" and \"g\" are decreasing. If one is increasing and the other is decreasing, then they are negatively correlated and the above inequality is reversed.\n\nSimilar statements hold more generally, when formula_1 is not necessarily finite, not even countable. In that case, \"μ\" has to be a finite measure, and the lattice condition has to be defined using cylinder events; see, e.g., Section 2.2 of .\n\nFor proofs, see the original or the Ahlswede–Daykin inequality (1978). Also, a rough sketch is given below, due to , using a Markov chain coupling argument.\n\nThe lattice condition for \"μ\" is also called multivariate total positivity, and sometimes the strong FKG condition; the term (multiplicative) FKG condition is also used in older literature.\n\nThe property of \"μ\" that increasing functions are positively correlated is also called having positive associations, or the weak FKG condition.\n\nThus, the FKG theorem can be rephrased as \"the strong FKG condition implies the weak FKG condition\".\n\nIf the lattice formula_1 is totally ordered, then the lattice condition is satisfied trivially for any measure \"μ\". For this case, the FKG inequality is Chebyshev's sum inequality: if the two increasing functions take on values formula_8 and formula_9, then (we may assume that the measure \"μ\" is uniform) \n\nMore generally, for any probability measure \"μ\" on formula_11 and increasing functions \"ƒ\" and \"g\", \nwhich follows immediately from\n\nThe lattice condition is trivially satisfied also when the lattice is the product of totally ordered lattices, formula_14, and formula_15 is a product measure. Often all the factors (both the lattices and the measures) are identical, i.e., \"μ\" is the probability distribution of i.i.d. random variables.\n\nThe FKG inequality for the case of a product measure is known also as the Harris inequality after Harris , who found and used it in his study of percolation in the plane. A proof of the Harris inequality that uses the above double integral trick on formula_11 can be found, e.g., in Section 2.2 of .\n\nA typical example is the following. Color each hexagon of the infinite honeycomb lattice black with probability formula_17 and white with probability formula_18, independently of each other. Let \"a, b, c, d\" be four hexagons, not necessarily distinct. Let formula_19 and formula_20 be the events that there is a black path from \"a\" to \"b\", and a black path from \"c\" to \"d\", respectively. Then the Harris inequality says that these events are positively correlated: formula_21. In other words, assuming the presence of one path can only increase the probability of the other.\n\nSimilarly, if we randomly color the hexagons inside an formula_22 rhombus-shaped hex board, then the events that there is black crossing from the left side of the board to the right side is positively correlated with having a black crossing from the top side to the bottom. On the other hand, having a left-to-right black crossing is negatively correlated with having a top-to-bottom white crossing, since the first is an increasing event (in the amount of blackness), while the second is decreasing. In fact, in any coloring of the hex board exactly one of these two events happen — this is why hex is a well-defined game.\n\nIn the Erdős–Rényi random graph, the existence of a Hamiltonian cycle is negatively correlated with the 3-colorability of the graph, since the first is an increasing event, while the latter is decreasing.\n\nIn statistical mechanics, the usual source of measures that satisfy the lattice condition (and hence the FKG inequality) is the following:\n\nIf formula_23 is an ordered set (such as formula_24), and formula_25 is a finite or infinite graph, then the set formula_26 of formula_23-valued configurations is a poset that is a distributive lattice.\n\nNow, if formula_28 is a submodular potential (i.e., a family of functions\none for each finite formula_30, such that each formula_31 is submodular), then one defines the corresponding Hamiltonians as\n\nIf \"μ\" is an extremal Gibbs measure for this Hamiltonian on the set of configurations formula_33, then it is easy to show that \"μ\" satisfies the lattice condition, see .\n\nA key example is the Ising model on a graph formula_25. Let formula_35, called spins, and formula_36. Take the following potential:\n\nSubmodularity is easy to check; intuitively, taking the min or the max of two configurations tends to decrease the number of disagreeing spins. Then, depending on the graph formula_25 and the value of formula_39, there could be one or more extremal Gibbs measures, see, e.g., and .\n\nThe Holley inequality, due to , states that the expectations\n\nof a monotonically increasing function \"ƒ\" on a finite distributive lattice formula_1 with respect to two positive functions \"μ\", \"μ\" on the lattice satisfy the condition\n\nprovided the functions satisfy the Holley condition (criterion)\n\nfor all \"x\", \"y\" in the lattice.\n\nTo recover the FKG inequality: If \"μ\" satisfies the lattice condition and \"ƒ\" and \"g\" are increasing functions on formula_1, then \"μ\"(\"x\") = \"g\"(\"x\")\"μ\"(\"x\") and \"μ\"(\"x\") = \"μ\"(\"x\") will satisfy the lattice-type condition of the Holley inequality. Then the Holley inequality states that\n\nwhich is just the FKG inequality.\n\nAs for FKG, the Holley inequality follows from the Ahlswede–Daykin inequality.\n\nConsider the usual case of formula_1 being a product formula_47 for some finite set formula_48. The lattice condition on \"μ\" is easily seen to imply the following monotonicity, which has the virtue that it is often easier to check than the lattice condition:\n\nWhenever one fixes a vertex formula_49 and two configurations \"φ\" and \"ψ\" outside \"v\" such that formula_50 for all formula_51, the \"μ\"-conditional distribution of \"φ\"(\"v\") given formula_52 stochastically dominates the \"μ\"-conditional distribution of \"ψ\"(\"v\") given formula_53.\n\nNow, if \"μ\" satisfies this monotonicity property, that is already enough for the FKG inequality (positive associations) to hold.\n\nHere is a rough sketch of the proof, due to : starting from any initial configuration on formula_48, one can run a simple Markov chain (the Metropolis algorithm) that uses independent Uniform[0,1] random variables to update the configuration in each step, such that the chain has a unique stationary measure, the given \"μ\". The monotonicity of \"μ\" implies that the configuration at each step is a monotone function of independent variables, hence the implies that it has positive associations. Therefore, the limiting stationary measure \"μ\" also has this property.\n\nThe monotonicity property has a natural version for two measures, saying that \"μ\" conditionally pointwise dominates \"μ\". It is again easy to see that if \"μ\" and \"μ\" satisfy the lattice-type condition of the , then \"μ\" conditionally pointwise dominates \"μ\". On the other hand, a Markov chain coupling argument similar to the above, but now without invoking the Harris inequality, shows that conditional pointwise domination, in fact, implies stochastically domination. Stochastic domination is equivalent to saying that formula_55 for all increasing \"ƒ\", thus we get a proof of the Holley inequality. (And thus also a proof of the FKG inequality, without using the Harris inequality.)\n\nSee and for details.\n\n\n"}
{"id": "1151048", "url": "https://en.wikipedia.org/wiki?curid=1151048", "title": "FL (complexity)", "text": "FL (complexity)\n\nIn computational complexity theory, the complexity class FL is the set of function problems which can be solved by a deterministic Turing machine in a logarithmic amount of memory space. As in the definition of L, the machine reads its input from a read-only tape and writes its output to a write-only tape; the logarithmic space restriction applies only to the read/write working tape.\n\nLoosely speaking, a function problem takes a complicated input and produces a (perhaps equally) complicated output. Function problems are distinguished from decision problems, which produce only Yes or No answers and corresponds to the set L of decision problems which can be solved in deterministic logspace. FL is a subset of FP, the set of function problems which can be solved in deterministic polynomial time.\n\nFL is known to contain several natural problems, including arithmetic on numbers. Addition, subtraction and multiplication of two numbers are fairly simple, but achieving division is a far deeper problem which was open for decades.\n\nSimilarly one may define FNL, which has the same relation with NL as FNP has with NP.\n"}
{"id": "6054681", "url": "https://en.wikipedia.org/wiki?curid=6054681", "title": "Finite difference method", "text": "Finite difference method\n\nIn mathematics, finite-difference methods (FDM) are numerical methods for solving differential equations by approximating them with difference equations, in which finite differences approximate the derivatives. FDMs are thus discretization methods.\n\nToday, FDMs are the dominant approach to numerical solutions of partial differential equations.\n\nFirst, assuming the function whose derivatives are to be approximated is properly-behaved, by Taylor's theorem, we can create a Taylor series expansion\n\nwhere \"n\"! denotes the factorial of \"n\", and \"R\"(\"x\") is a remainder term, denoting the difference between the Taylor polynomial of degree \"n\" and the original function. We will derive an approximation for the first derivative of the function \"f\" by first truncating the Taylor polynomial:\n\nSetting, x=a we have,\n\nDividing across by \"h\" gives:\n\nSolving for f'(a):\n\nAssuming that formula_6 is sufficiently small, the approximation of the first derivative of \"f\" is:\n\nThe error in a method's solution is defined as the difference between the approximation and the exact analytical solution. The two sources of error in finite difference methods are round-off error, the loss of precision due to computer rounding of decimal quantities, and truncation error or discretization error, the difference between the exact solution of the original differential equation and the exact quantity assuming perfect arithmetic (that is, assuming no round-off).\nTo use a finite difference method to approximate the solution to a problem, one must first discretize the problem's domain. This is usually done by dividing the domain into a uniform grid (see image to the right). Note that this means that finite-difference methods produce sets of discrete numerical approximations to the derivative, often in a \"time-stepping\" manner.\n\nAn expression of general interest is the local truncation error of a method. Typically expressed using Big-O notation, local truncation error refers to the error from a single application of a method. That is, it is the quantity formula_8 if formula_9 refers to the exact value and formula_10 to the numerical approximation. The remainder term of a Taylor polynomial is convenient for analyzing the local truncation error. Using the Lagrange form of the remainder from the Taylor polynomial for formula_11, which is\n\nformula_12, where formula_13,\n\nthe dominant term of the local truncation error can be discovered. For example, again using the forward-difference formula for the first derivative, knowing that formula_14,\n\nand with some algebraic manipulation, this leads to\n\nand further noting that the quantity on the left is the approximation from the finite difference method and that the quantity on the right is the exact quantity of interest plus a remainder, clearly that remainder is the local truncation error. A final expression of this example and its order is:\n\nThis means that, in this case, the local truncation error is proportional to the step sizes. The quality and duration of simulated FDM solution depends on the discretization equation selection and the step sizes (time and space steps). The data quality and simulation duration increase significantly with smaller step size. Therefore, a reasonable balance between data quality and simulation duration is necessary for practical usage. Large time steps are useful for increasing simulation speed in practice. However, time steps which are too large may create instabilities and affect the data quality.\n\nThe von Neumann method is usually applied to determine the numerical model stability.\n\nFor example, consider the ordinary differential equation\nThe Euler method for solving this equation uses the finite difference quotient\nto approximate the differential equation by first substituting it for u'(x) then applying a little algebra (multiplying both sides by h, and then adding u(x) to both sides) to get\nThe last equation is a finite-difference equation, and solving this equation gives an approximate solution to the differential equation.\n\nConsider the normalized heat equation in one dimension, with homogeneous Dirichlet boundary conditions\n\nOne way to numerically solve this equation is to approximate all the derivatives by finite differences. We partition the domain in space using a mesh formula_24 and in time using a mesh formula_25. We assume a uniform partition both in space and in time, so the difference between two consecutive space points will be \"h\" and between two consecutive time points will be \"k\". The points\n\nwill represent the numerical approximation of formula_27\n\nUsing a forward difference at time formula_28 and a second-order central difference for the space derivative at position formula_29 (FTCS) we get the recurrence equation:\n\nThis is an explicit method for solving the one-dimensional heat equation.\n\nWe can obtain formula_31 from the other values this way:\n\nwhere formula_33\n\nSo, with this recurrence relation, and knowing the values at time \"n\", one can obtain the corresponding values at time \"n\"+1. formula_34 and formula_35 must be replaced by the boundary conditions, in this example they are both 0.\n\nThis explicit method is known to be numerically stable and convergent whenever formula_36. The numerical errors are proportional to the time step and the square of the space step:\n\nIf we use the backward difference at time formula_38 and a second-order central difference for the space derivative at position formula_29 (The Backward Time, Centered Space Method \"BTCS\") we get the recurrence equation:\n\nThis is an implicit method for solving the one-dimensional heat equation.\n\nWe can obtain formula_41 from solving a system of linear equations:\n\nThe scheme is always numerically stable and convergent but usually more numerically intensive than the explicit method as it requires solving a system of numerical equations on each time step. The errors are linear over the time step and quadratic over the space step:\n\nFinally if we use the central difference at time formula_44 and a second-order central difference for the space derivative at position formula_29 (\"CTCS\") we get the recurrence equation:\n\nThis formula is known as the Crank–Nicolson method.\nWe can obtain formula_31 from solving a system of linear equations:\n\nThe scheme is always numerically stable and convergent but usually more numerically intensive as it requires solving a system of numerical equations on each time step. The errors are quadratic over both the time step and the space step:\n\nUsually the Crank–Nicolson scheme is the most accurate scheme for small time steps. The explicit scheme is the least accurate and can be unstable, but is also the easiest to implement and the least numerically intensive. The implicit scheme works the best for large time steps.\n\nThe figures below present the solutions given by the above methods to approximate the heat equation \n\nwith the boundary condition\n\nThe exact solution is\n\nThe (continuous) Laplace operator in formula_53-dimensions is given by formula_54.\nThe discrete Laplace operator formula_55 depends on the dimension formula_53.\n\nIn 1D the Laplace operator is approximated as \nThis approximation is usually expressed via the following stencil\n\nThe 2D case shows all the characteristics of the more general nD case. Each second partial derivative needs to be approximated similar to the 1D case \nwhich is usually given by the following stencil\n\nConsistency of the above-mentioned approximation can be shown for highly regular functions, such as formula_61.\nThe statement is\n\nTo proof this one needs to substitute Taylor Series expansions up to order 3 into the discrete Laplace operator.\n\nSimilar to continuous subharmonic functions one can define \"subharmonic functions\" for finite-difference approximations formula_63\n\nOne can define a general stencil of \"positive type\" via\n\nIf formula_66 is (discrete) subharmonic then the following\" mean value property\" holds\nwhere the approximation is evaluated on points of the grid, and the stencil is assumed to be of positive type.\n\nA similar mean value property also holds for the continuous case.\n\nFor a (discrete) subharmonic function formula_66 the following holds\nwhere formula_70 are discretizations of the continuous domain formula_71, respectively the boundary formula_72.\n\nA similar maximum principle also holds for the continuous case.\n\n\n"}
{"id": "17136006", "url": "https://en.wikipedia.org/wiki?curid=17136006", "title": "Formula calculator", "text": "Formula calculator\n\nA formula calculator is a software calculator that can perform a calculation in two steps:\n\n\nThis is unlike button-operated calculators, such as the Windows calculator or the Mac OS X calculator, which require the user to perform one step for each operation, by pressing buttons to calculate all the intermediate values, before the final result is shown.\n\nIn this context, a formula is also known as an expression, and so formula calculators may be called \"expression\" calculators. Also in this context, calculation is known as \"evaluation\", and so they may be called formula \"evaluators\", rather than \"calculators\".\n\nFormulas as they are commonly written use infix notation for binary operators, such as addition, multiplication, division and subtraction. This notation also uses:\n\n\nAlso, formulas may contain:\n\n\nOnce a formula is entered, a formula calculator follows the above rules to produce the final result by automatically:\n\n\nThe formula calculator concept can be applied to all types of calculator, including arithmetic, scientific, statistics, financial and conversion calculators.\n\nThe calculation can be typed or pasted into an edit box of:\n\n\nIt can also be entered on the command line of a programming language.\n\nAlthough they are not calculators in themselves, because they have a much broader feature set, many software tools have a formula-calculation capability, in that a formula can be typed in and evaluated. These include:\n\n\nButton-operated calculators are imperative, because the user must provide details of how the calculation has to be performed.\n\nOn the other hand, formula calculators are more declarative because the typed-in formula specifies what to do, and the user does not have to provide any details of the step-by-step order in which the calculation has to be performed.\n\nDeclarative solutions are easier to understand than imperative solutions, and so there has been a long-term trend from imperative to declarative methods. Formula calculators are part of this trend.\n\nMany software tools for the general user, such as spreadsheets, are declarative. Formula calculators are examples of such tools.\n\nThere are hybrid calculators that combine typed-in formula and button-operated calculation. For example:\n\n\n"}
{"id": "41436516", "url": "https://en.wikipedia.org/wiki?curid=41436516", "title": "Fuss–Catalan number", "text": "Fuss–Catalan number\n\nIn combinatorial mathematics and statistics, the Fuss–Catalan numbers are numbers of the form\n\nThey are named after N. I. Fuss and Eugène Charles Catalan.\n\nIn some publications this equation is sometimes referred to as Two-parameter Fuss–Catalan numbers or Raney numbers. The implication is the \"single-parameter Fuss-Catalan numbers\" are when =1 and =2.\n\nThe Fuss-Catalan represents the number of legal permutations or allowed ways of arranging a number of articles, that is restricted in some way. This means that they are related to the Binomial Coefficient. The key difference between Fuss-Catalan and the Binomial Coefficient is that there are no \"illegal\" arrangement permutations within Binomial Coefficient, but there are within Fuss-Catalan. An examples of legal and illegal permutations can be better demonstrated by a specific problem such as balanced brackets (see Dyck language).\n\nA general problem is to count the number of balanced brackets (or legal permutations) that a string of \"m\" open and \"m\" closed brackets forms (total of \"2m\" brackets). By legally arranged, the following rules apply:\nAs an numeric example how many combinations can 3 pairs of brackets be legally arranged? From the Binomial interpretation there are formula_2 or numerically formula_3 = 20 ways of arranging 3 open and 3 closed brackets. However, there are fewer \"legal\" combinations than these when all of the above restrictions apply. Evaluating these by hand, there are 5 legal combinations, namely: ()()(); (())(); ()(()); (()()); ((())). This corresponds to the Fuss-Catalan formula when \"p=2, r=1\" which is the Catalan number formula formula_4 or formula_5=5. By simple subtraction, there are formula_6 or formula_7 =15 illegal combinations. To further illustrate the subtlety of the problem, if one were to persist with solving the problem just using the Binomial formula, it would be realised that the 2 rules imply that the sequence must start with an open bracket and finish with a closed bracket. This implies that there are formula_8 or formula_9=6 combinations. This is inconsistent with the above answer of 5, and the missing combination is: ())((), which is illegal and would complete the binomial interpretation.\n\nWhilst the above is a concrete example Catalan numbers, similar problems can be evaluated using Fuss-Catalan formula:\n\nIf formula_13, we recover the Binomial coefficients formula_14\n\nIf formula_19, Pascal's Triangle appears, read along diagonals:\n\nFor subindex formula_26 the numbers are:\n\nExamples with formula_27:\n\nExamples with formula_32:\n\nExamples with formula_37:\n\nThis means in particular that from\nand\none can generate all other Fuss–Catalan numbers if is an integer.\n\nRiordan (see references) obtains a convolution type of recurrence:\n\nParaphrasing the \"Densities of the Raney distributions\" paper, let the ordinary generating function with respect to the index be defined as follows:\nLooking at equations (1) and (2), when =1 it follows that\nAlso note this result can be derived by similar substitutions into the other formulas representation, such as the Gamma ratio representation at the top of this article. Using (6) and substituting into (5) an equivalent representation expressed as a generating function can be formulated as\nFinally, extending this result by using Lambert's equivalence \nThe following result can be derived for the ordinary generating function for all the Fuss-Catalan sequences.\n\nRecursion forms of this are as follows:\nThe most obvious form is:\n\nAlso, a less obvious form is\n\nIn some problems it is easier to use different formula configurations or variations. Below are a two examples using just the binomial function:\n\nThese variants can be converted into product, Gamma or Factorial representations too.\n\n\n"}
{"id": "42094122", "url": "https://en.wikipedia.org/wiki?curid=42094122", "title": "Geometric terms of location", "text": "Geometric terms of location\n\nGeometric terms of location describe directions or positions relative to the shape of an object. These terms are used in descriptions of engineering, physics, and other sciences, as well as ordinary day to day discourse.\n\nThough these terms by themselves may be somewhat ambiguous, they are usually used in a context in which their meaning is clear. For example, when referring to a drive shaft it is clear what is meant by axial or radial directions. Or, in a free body diagram, one may similarly infer a sense of orientation by the forces or other vectors represented.\n\nCommon geometric terms of location are:\n"}
{"id": "2214847", "url": "https://en.wikipedia.org/wiki?curid=2214847", "title": "Geometry processing", "text": "Geometry processing\n\nGeometry processing, or mesh processing, is an area of research that uses concepts from applied mathematics, computer science and engineering to design efficient algorithms for the acquisition, reconstruction, analysis, manipulation, simulation and transmission of complex 3D models. As the name implies, geometry processing is meant to be analogous to signal processing and image processing. Many concepts, data structures and algorithms are directly analogous. For example, where image smoothing might convolve an intensity signal with a blur kernel formed using the Laplace operator, geometric smoothing might be achieved by convolving a surface geometry with a blur kernel formed using the Laplace-Beltrami operator.\n\nApplications of geometry processing algorithms already cover a wide range of areas from multimedia, entertainment and classical computer-aided design, to biomedical computing, reverse engineering and scientific computing.\n\nGeometry processing is a common research topic at SIGGRAPH, the premier computer graphics academic conference, and the main topic of the annual Symposium on Geometry Processing.\n\nGeometry processing involves working with a shape, usually in 2D or 3D, although the shape can live in a space of arbitrary dimensions. The processing of a shape involves three stages, which is known as its life cycle. At its \"birth,\" a shape can be instantiated through one of three methods: a model, a mathematical representation, or a scan. After a shape is born, it can be analyzed and edited repeatedly in a cycle. This usually involves acquiring different measurements, such as the distances between the points of the shape, the smoothness of the shape, or its Euler characteristic. Editing may involve denoising, deforming, or performing rigid transformations. At the final stage of the shape's \"life,\" it is consumed. This can mean it is consumed by a viewer as a rendered asset in a game or movie, for instance. The end of a shape's life can also be defined by a decision about the shape, like whether or not it satisfies some criteria. Or it can even be fabricated in the real world, through a method such as 3D printing or laser cutting.\n\nLike any other shape, the shapes used in geometry processing have properties pertaining to their geometry and topology. The geometry of a shape concerns the position of the shape's points in space, tangents, normals, and curvature. It also includes the dimension in which the shape lives (ex. formula_1 or formula_2). The topology of a shape is a collection of properties that do not change even after smooth transformations have been applied to the shape. It concerns dimensions such as the number of holes and boundaries, as well as the orientability of the shape. One example of a non-orientable shape is the Mobius strip.\n\nIn computers, everything must be discretized. Shapes in geometry processing are usually represented as triangle meshes, which can be seen as a graph. Each node in the graph is a vertex (usually in formula_2), which has a position. This encodes the geometry of the shape. Directed edges connect these vertices into triangles, which by the right hand rule, then have a direction called the normal. Each triangle forms a face of the mesh. These are combinatoric in nature and encode the topology of the shape. In addition to triangles, a more general class of polygon meshes can also be used to represent a shape. More advanced representations like progressive meshes encode a coarse representation along with a sequence of transformations, which produce a fine or high resolution representation of the shape once applied. These meshes are useful in a variety of applications, including geomorphs, progressive transmission, mesh compression, and selective refinement.\nOne particularly important property of a 3D shape is its Euler characteristic, which can alternatively be defined in terms of its genus. The formula for this in the continuous sense is formula_4, where formula_5 is the number of connected components, formula_6 is number of holes (as in donut holes, see torus), and formula_7 is the number of connected components of the boundary of the surface. A concrete example of this is a mesh of a pair of pants. There is one connected component, 0 holes, and 3 connected components of the boundary (the waist and two leg holes). So in this case, the Euler characteristic is -1.To bring this into the discrete world, the Euler characteristic of a mesh is computed in terms of its vertices, edges, and faces. formula_8.\n\nDepending on how a shape is initialized or \"birthed,\" the shape might exist only as a nebula of sampled points that represent its surface in space. To transform the surface points into a mesh, the Poisson reconstruction strategy can be employed. This method states that the indicator function, a function that determines which points in space belong to the surface of the shape, can actually be computed from the sampled points. The key concept is that gradient of the indicator function is \"0\" everywhere, except at the sampled points, where it is equal to the inward surface normal. More formally, suppose the collection of sampled points from the surface is denoted by formula_9, each point in the space by formula_10, and the corresponding normal at that point by formula_11. Then the gradient of the indicator function is defined as:\n\nformula_12\n\nThe task of reconstruction then becomes a variational problem. To find the indicator function of the surface, we must find a function formula_13 such that formula_14 is minimized, where formula_15 is the vector field defined by the samples. As a variational problem, one can view the minimizer formula_16as a solution of Poisson's equation. After obtaining a good approximation for formula_16 and a value formula_18 for which the points formula_19 with formula_20 lie on the surface to be reconstructed, the marching cubes algorithm can be used to construct a triangle mesh from the function formula_16 , which then be can applied in subsequent computer graphics applications.\n\nOne common problem encountered in geometry processing is how to merge multiple views of a single object captured from different angles or positions. This problem is known as registration. In registration, we wish to find an optimal rigid transformation that will align surface formula_22 with surface formula_23. More formally, if formula_24 is the projection of a point \"x\" from surface formula_22 onto surface formula_23, we want to find the optimal rotation matrix formula_27 and translation vector formula_28 that minimize the following objective function:\n\nformula_29\n\nWhile rotations are non-linear in general, small rotations can be linearized as skew-symmetric matrices. Moreover, the distance function formula_30 is non-linear, but is amenable to linear approximations if the change in formula_22 is small. An iterative solution such as Iterative Closest Point (ICP) is therefore employed to solve for small transformations iteratively, instead of solving for the potentially large transformation in one go. In ICP, \"n\" random sample points from formula_22 are chosen and projected onto formula_23. In order to sample points uniformly at random across the surface of the triangle mesh, the random sampling is broken into two stages: uniformly sampling points within a triangle; and non-uniformly sampling triangles, such that each triangle's associated probability is proportional to its surface area. Thereafter, the optimal transformation is calculated based on the difference between each formula_34 and its projection. In the following iteration, the projections are calculated based on the result of applying the previous transformation on the samples. The process is repeated until convergence.\n\nWhen shapes are defined or scanned, there may be accompanying noise, either to a signal acting upon the surface or to the actual surface geometry. Reducing noise on the former is known as data denoising, while noise reduction on the latter is known as surface fairing. The task of geometric smoothing is analogous to signal noise reduction, and consequently employs similar approaches.\n\nThe pertinent Lagrangian to be minimized is derived by recording the conformity to the initial signal formula_35 and the smoothness of the resulting signal, which approximated by the magnitude of the gradient with a weight formula_36:\n\nformula_37.\n\nTaking a variation formula_38 on formula_39 emits the necessary condition\n\nformula_40.\n\nBy discretizing this onto piecewise-constant elements with our signal on the vertices we obtain\n\nformula_41where our choice of formula_42 is chosen to be formula_43 for the cotangent Laplacian formula_44 and the formula_45 term is to map the image of the Laplacian from areas to points. Because the variation is free, this results in a self-adjoint linear problem to solve with a parameter formula_36: formula_47 When working with triangle meshes one way to determine the values of the Laplacian matrix formula_48 is through analyzing the geometry of connected triangles on the mesh.\n\nformula_49\n\nWhere formula_50 and formula_51 are the angles opposite the edge formula_52\nThe mass matrix M as an operator computes the local integral of a function's value and is often set for a mesh with m triangles as follows:\n\nformula_53\n\nOccasionally, we need to flatten a 3D surface onto a flat plane. This process is known as parameterization. The goal is to find coordinates \"u\" and \"v\" onto which we can map the surface so that distortions are minimized. In this manner, parameterization can be seen as an optimization problem. One of the major applications of mesh parameterization is texture mapping.\n\nOne way to measure the distortion accrued in the mapping process is to measure how much the length of the edges on the 2D mapping differs from their lengths in the original 3D surface. In more formal terms, the objective function can be written as:\n\nformula_54\n\nWhere formula_55 is the set of mesh edges and formula_56 is the set of vertices. However, optimizing this objective function would result in a solution that maps all of the vertices to a single vertex in the \"uv\"-coordinates. Borrowing an idea from graph theory, we apply the Tutte Mapping and restrict the boundary vertices of the mesh onto a unit circle or other convex polygon. Doing so prevents the vertices from collapsing into a single vertex when the mapping is applied. The non-boundary vertices are then positioned at the barycentric interpolation of their neighbours. The Tutte Mapping, however, still suffers from severe distortions as it attempts to make the edge lengths equal, and hence does not correctly account for the triangle sizes on the actual surface mesh.\n\nAnother way to measure the distortion is to consider the variations on the \"u\" and \"v\" coordinate functions. The wobbliness and distortion apparent in the mass springs methods are due to high variations in the \"u\" and \"v\" coordinate functions. With this approach, the objective function becomes the Dirichlet energy on \"u\" and \"v:\"\n\nformula_57\n\nThere are a few other things to consider. We would like to minimize the angle distortion to preserve orthogonality. That means we would like formula_58. In addition, we would also like the mapping to have proportionally similar sized regions as the original. This results to setting the Jacobian of the \"u\" and \"v\" coordinate functions to 1.\n\nformula_59\n\nPutting these requirements together, we can augment the Dirichlet energy so that our objective function becomes:\n\nformula_60\n\nTo avoid the problem of having all the vertices mapped to a single point, we also require that the solution to the optimization problem must have a non-zero norm and that it is orthogonal to the trivial solution.\n\nDeformation is concerned with transforming some rest shape to a new shape. Typically, these transformations are continuous and do not alter the topology of the shape. Modern mesh-based shape deformation methods satisfy user deformation constraints at handles (selected vertices or regions on the mesh) and propagate these handle deformations to the rest of shape smoothly and without removing or distorting details. Some common forms of interactive deformations are point-based, skeleton-based, and cage-based. In point-based deformation, a user can apply transformations to small set of points, called handles, on the shape. Skeleton-based deformation defines a skeleton for the shape, which allows a user to move the bones and rotate the joints. Cage-based deformation requires a cage to be drawn around all or part of a shape so that, when the user manipulates points on the cage, the volume it encloses changes accordingly.\n\nHandles provide a sparse set of constraints for the deformation: as the user moves one point, the others must stay in place.\n\nA rest surface formula_61 immersed in formula_62 can be described with a mapping formula_63, where formula_64 is a 2D parametric domain. The same can be done with another mapping formula_34 for the transformed surface formula_9. Ideally, the transformed shape adds as little distortion as possible to the original. One way to model this distortion is in terms of displacements formula_67 with a Laplacian-based energy. Applying the Laplace operator to these mappings allows us to measure how the position of a point changes relative to its neighborhood, which keeps the handles smooth. Thus, the energy we would like to minimize can be written as:\n\nformula_68.\n\nWhile this method is translation invariant, it is unable to account for rotations. The As-Rigid-As-Possible deformation scheme applies a rigid transformation formula_69 to each handle i, where formula_70 is a rotation matrix and formula_71 is a translation vector. Unfortunately, there’s no way to know the rotations in advance, so instead we pick a “best” rotation that minimizes displacements. To achieve local rotation invariance, however, requires a function formula_72 which outputs the best rotation for every point on the surface. The resulting energy, then, must optimize over both formula_73 and formula_74: \n\nformula_75\n\nNote that the translation vector is not present in the final objective function because translations have constant gradient.\n\nWhile seemingly trivial, in many cases, determining the inside from the outside of a triangle mesh is not an easy problem. In general, given a surface formula_9 we pose this problem as determining a function formula_77 which will return formula_78 if the point formula_79 is inside formula_9, and formula_81 otherwise.\n\nIn the simplest case, the shape is closed. In this case, to determine if a point formula_79 is inside or outside the surface, we can cast a ray formula_83 in any direction from a query point, and count the number of times formula_84 it passes through the surface. If formula_79 was outside formula_9 then the ray must either not pass through formula_9 (in which case formula_88) or, each time it enters formula_9 it must pass through twice, because S is bounded, so any ray entering it must exit. So if formula_79 is outside, formula_84 is even. Likewise if formula_79 is inside, the same logic applies to the previous case, but the ray must intersect formula_9 one extra time for the first time it leaves formula_9. So: \n\nformula_95\n\nNow, often times we cannot guarantee that the formula_9 is closed. Take the pair of pants example from the top of this article. This mesh clearly has a semantic inside-and-outside, despite there being holes at the waist and the legs. \n\nThe naive attempt to solve this problem is to shoot many rays in random directions, and classify formula_79 as being inside if and only if most of the rays intersected formula_9 an odd number of times. To quantify this, let us say we cast formula_99 rays, formula_100. We associate a number formula_101 which is the average value of formula_102 from each ray. Therefore:\n\nformula_103\n\nIn the limit of shooting many, many rays, this method handles open meshes, however it in order to become accurate, far too many rays are required for this method to be computationally ideal. Instead, a more robust approach is the Generalized Winding Number. Inspired by the 2D winding number, this approach uses the solid angle at formula_79 of each triangle in the mesh to determine if formula_79 is inside or outside. The value of the Generalized Winding Number at formula_79, formula_107 is proportional to the sum of the solid angle contribution from each triangle in the mesh:\n\nformula_108\n\nFor a closed mesh, formula_107 is equivalent to the characteristic function for the volume represented by formula_9. Therefore, we say:\n\nformula_111\n\nBecause formula_107 is a harmonic function, it degrades gracefully, meaning the inside-outside segmentation would not change much if we poked holes in a closed mesh. For this reason, the Generalized Winding Number handles open meshes robustly. The boundary between inside and outside smoothly passes over holes in the mesh. In fact, in the limit, the Generalized Winding Number is equivalent to the ray-casting method as the number of rays goes to infinity.\n\n\n\n"}
{"id": "41808008", "url": "https://en.wikipedia.org/wiki?curid=41808008", "title": "Hamnet Holditch", "text": "Hamnet Holditch\n\nRev. Hamnet Holditch, also spelled Hamnett Holditch (1800 – 12 December 1867), was an English mathematician who was president of Gonville and Caius College, Cambridge.\n\nIn 1858, he introduced the result in geometry now known as Holditch's theorem.\n\nHamnet Holditch was born in 1800 at Lynn, Norfolk. In 1818, he began his studies of mathematics at the University of Cambridge (Caius College), having obtained his bachelor's degree (B.A.) in 1822 and his master's degree (M.A.) in 1825. He was Senior Wrangler in the Tripos and was awarded the Smith's Prize of 1822. He was a Fellow of Caius College, and its President from 1835 until 1867, when he died.\n\nHe was the only son of George Holditch, and had two sisters.\n\n\n"}
{"id": "8092200", "url": "https://en.wikipedia.org/wiki?curid=8092200", "title": "Heavy-tailed distribution", "text": "Heavy-tailed distribution\n\nIn probability theory, heavy-tailed distributions are probability distributions whose tails are not exponentially bounded: that is, they have heavier tails than the exponential distribution. In many applications it is the right tail of the distribution that is of interest, but a distribution may have a heavy left tail, or both tails may be heavy.\n\nThere are three important subclasses of heavy-tailed distributions: the fat-tailed distributions, the long-tailed distributions and the subexponential distributions. In practice, all commonly used heavy-tailed distributions belong to the subexponential class.\n\nThere is still some discrepancy over the use of the term heavy-tailed. There are two other definitions in use. Some authors use the term to refer to those distributions which do not have all their power moments finite; and some others to those distributions that do not have a finite variance. The definition given in this article is the most general in use, and includes all distributions encompassed by the alternative definitions, as well as those distributions such as log-normal that possess all their power moments, yet which are generally acknowledged to be heavy-tailed. (Occasionally, heavy-tailed is used for any distribution that has heavier tails than the normal distribution.)\n\nThe distribution of a random variable \"X\" with distribution function \"F\" is said to have a heavy (right) tail if the moment generating function of \"F\", \"M\"(\"t\"), is infinite for all \"t\" > 0.\n\nThat means\n\nAn implication of this is that\n\nThis is also written in terms of the tail distribution function\n\nas\n\nThe distribution of a random variable \"X\" with distribution function \"F\" is said to have a long right tail if for all \"t\" > 0,\n\nor equivalently\n\nThis has the intuitive interpretation for a right-tailed long-tailed distributed quantity that if the long-tailed quantity exceeds some high level, the probability approaches 1 that it will exceed any other higher level.\n\nAll long-tailed distributions are heavy-tailed, but the converse is false, and it is possible to construct heavy-tailed distributions that are not long-tailed.\n\nSubexponentiality is defined in terms of convolutions of probability distributions. For two independent, identically distributed random variables formula_7 with common distribution function formula_8 the convolution of formula_8 with itself, formula_10 is defined, using Lebesgue–Stieltjes integration, by:\n\nThe \"n\"-fold convolution formula_12 is defined in the same way. The tail distribution function formula_13 is defined as formula_14.\n\nA distribution formula_8 on the positive half-line is subexponential if\n\nThis implies that, for any formula_17,\n\nThe probabilistic interpretation of this is that, for a sum of formula_19 independent random variables formula_20 with common distribution formula_8,\n\nThis is often known as the principle of the single big jump or catastrophe principle.\n\nA distribution formula_8 on the whole real line is subexponential if the distribution\nformula_24 is. Here formula_25 is the indicator function\nof the positive half-line. Alternatively, a random variable formula_26 supported on the real line is subexponential if and only if formula_27 is subexponential.\n\nAll subexponential distributions are long-tailed, but examples can be constructed of long-tailed distributions that are not subexponential.\n\nAll commonly used heavy-tailed distributions are subexponential.\n\nThose that are one-tailed include:\n\nThose that are two-tailed include:\n\nA fat-tailed distribution is a distribution for which the probability density function, for large x, goes to zero as a power formula_28. Since such a power is always bounded below by the probability density function of an exponential distribution, fat-tailed distributions are always heavy-tailed. Some distributions, however, have a tail which goes to zero slower than an exponential function (meaning they are heavy-tailed), but faster than a power (meaning they are not fat-tailed). An example is the log-normal distribution. Many other heavy-tailed distributions such as the log-logistic and Pareto distribution are however also fat-tailed.\n\nThere are parametric (see Embrechts et al.) and non-parametric (see, e.g., Novak) approaches to the problem of the tail-index estimation.\n\nTo estimate the tail-index using the parametric approach, some authors employ GEV distribution or Pareto distribution; they may apply the maximum-likelihood estimator (MLE).\n\nWith formula_29 a random sequence of independent and same density function formula_30, the Maximum Attraction Domain of the generalized extreme value density formula_31, where formula_32. If formula_33 and formula_34, then the \"Pickands\" tail-index estimation is\nwhere formula_36. This estimator converge in probability to formula_37.\n\nLet formula_38 be a sequence of independent and identically distributed random variables with distribution function formula_30, the maximum domain of attraction of the generalized extreme value distribution formula_31, where formula_32. The sample path is formula_42 where formula_19 is the sample size. If \nformula_44 is an intermediate order sequence, i.e. formula_45, formula_46 and formula_47, then the Hill tail-index estimator is \n\nwhere formula_49 is the formula_50-th order statistic of formula_51.\nThis estimator converges in probability to formula_37, and is asymptotically normal provided formula_53 is restricted based on a higher order regular variation property \n. Consistency and asymptotic normality extend to a large class of dependent and heterogeneous sequences, irrespective of whether formula_54 is observed, or a computed residual or filtered data from a large class of models and estimators, including mis-specified models and models with errors that are dependent.\n\nThe ratio estimator (RE-estimator) of the tail-index was introduced by Goldie \nand Smith. \nIt is constructed similarly to Hill's estimator but uses a non-random \"tuning parameter\".\n\nA comparison of Hill-type and RE-type estimators can be found in Novak.\n\n\nNonparametric approaches to estimate heavy- and superheavy-tailed probability density functions were given in \nMarkovich. These are approaches based on variable bandwidth and long-tailed kernel estimators; on the preliminary data transform to a new random variable at finite or infinite intervals which is more convenient for the estimation and then inverse transform of the obtained density estimate; and \"piecing-together approach\" which provides a certain parametric model for the tail of the density and a non-parametric model to approximate the mode of the density. Nonparametric estimators require an appropriate selection of tuning (smoothing) parameters like a bandwidth of kernel estimators and the bin width of the histogram. The well known data-driven methods of such selection are a cross-validation and its modifications, methods based on the minimization of the mean squared error (MSE) and its asymptotic and their upper bounds. A discrepancy method which uses well-known nonparametric statistics like Kolmogorov-Smirnov's, von Mises and Anderson-Darling's ones as a metric in the space of distribution functions (dfs) and quantiles of the later statistics as a known uncertainty or a discrepancy value can be found in. Bootstrap is another tool to find smoothing parameters using approximations of unknown MSE by different schemes of re-samples selection, see e.g.\n\n"}
{"id": "6612581", "url": "https://en.wikipedia.org/wiki?curid=6612581", "title": "Hilbert scheme", "text": "Hilbert scheme\n\nIn algebraic geometry, a branch of mathematics, a Hilbert scheme is a scheme that is the parameter space for the closed subschemes of some projective space (or a more general projective scheme), refining the Chow variety. The Hilbert scheme is a disjoint union of projective subschemes corresponding to Hilbert polynomials. The basic theory of Hilbert schemes was developed by . Hironaka's example shows that non-projective varieties need not have Hilbert schemes.\n\nThe Hilbert scheme of classifies closed subschemes of projective space in the following sense: For any locally Noetherian scheme , the set of -valued points\n\nof the Hilbert scheme is naturally isomorphic to the set of closed subschemes of that are flat over . The closed subschemes of that are flat over can informally be thought of as the families of subschemes of projective space parameterized by . The Hilbert scheme breaks up as a disjoint union of pieces corresponding to the Hilbert polynomial of the subschemes of projective space with Hilbert polynomial . Each of these pieces is projective over .\n\nGrothendieck constructed the Hilbert scheme of -dimensional projective space over a Noetherian scheme as a subscheme of a Grassmannian defined by the vanishing of various determinants. Its fundamental property is that for a scheme over , it represents the functor whose -valued points are the closed subschemes of that are flat over .\n\nIf is a subscheme of -dimensional projective space, then corresponds to a graded ideal of the polynomial ring in variables, with graded pieces . For sufficiently large , depending only on the Hilbert polynomial of , all higher cohomology groups of with coefficients in vanish, so in particular has dimension , where is the Hilbert polynomial of projective space.\n\nPick a sufficiently large value of . The -dimensional space is a subspace of the -dimensional space , so represents a point of the Grassmannian . This will give an embedding of the piece of the Hilbert scheme corresponding to the Hilbert polynomial into this Grassmannian.\n\nIt remains to describe the scheme structure on this image, in other words to describe enough elements for the ideal corresponding to it. Enough such elements are given by the conditions that the map has rank at most for all positive , which is equivalent to the vanishing of various determinants. (A more careful analysis shows that it is enough just to take .)\n\nThe Hilbert scheme is defined and constructed for any projective scheme in a similar way. Informally, its points correspond to closed subschemes of .\n\n determined for which polynomials the Hilbert scheme is non-empty, and showed that if is non-empty then it is linearly connected. So two subschemes of projective space are in the same connected component of the Hilbert scheme if and only if they have the same Hilbert polynomial.\n\nHilbert schemes can have bad singularities, such as irreducible components that are non-reduced at all points. They can also have irreducible components of unexpectedly high dimension. For example, one might expect the Hilbert scheme of points (more precisely dimension 0, length subschemes) of a scheme of dimension to have dimension , but if its irreducible components can have much larger dimension.\n\nformula_5where the underlying ring is bigraded.\n\n\"Hilbert scheme\" sometimes refers to the punctual Hilbert scheme of 0-dimensional subschemes on a scheme. Informally this can be thought of as something like finite collections of points on a scheme, though this picture can be very misleading when several points coincide.\n\nThere is a Hilbert-Chow morphism from the reduced Hilbert scheme of points to the Chow variety of cycles taking any 0-dimensional scheme to its associated 0-cycle. .\n\nThe Hilbert scheme of points on is equipped with a natural morphism to an -th symmetric product of . This morphism is birational for of dimension at most 2. For of dimension at least 3 the morphism is not birational for large : the Hilbert scheme is in general reducible and has components of dimension much larger than that of the symmetric product.\n\nThe Hilbert scheme of points on a curve (a dimension-1 complex manifold) is isomorphic to a symmetric power of . It is smooth.\n\nThe Hilbert scheme of points on a surface is also smooth (Grothendieck). If , it is obtained from by blowing up the diagonal and then dividing by the action induced by . It was used by Mark Haiman in his proof of the positivity of the coefficients of some Macdonald polynomials.\n\nThe Hilbert scheme of a smooth manifold of dimension 3 or more is usually not smooth.\n\nLet be a complex Kähler surface with (K3 surface or a torus). The canonical bundle of is trivial, as follows from Kodaira classification of surfaces. Hence admits a holomorphic symplectic form. It was observed by Fujiki (for ) and Beauville that is also holomorphically symplectic. This is not very difficult to see, e.g., for . Indeed, is a blow-up of a symmetric square of . Singularities of are locally isomorphic to The blow-up of is , and this space is symplectic. This is used to show that the symplectic form is naturally extended to the smooth part of the exceptional divisors of . It is extended to the rest of by Hartogs' principle.\n\nA holomorphically symplectic, Kähler manifold is hyperkähler, as follows from Calabi–Yau theorem. Hilbert schemes of points on K3 and a 4-dimensional torus give two series of examples of hyperkähler manifolds: a Hilbert scheme of points on K3 and a generalized Kummer manifold.\n\n\n\n"}
{"id": "53878654", "url": "https://en.wikipedia.org/wiki?curid=53878654", "title": "John von Neumann Lecture", "text": "John von Neumann Lecture\n\nThe John von Neumann Lecture Prize was established in 1959 with funds from IBM and other industry corporations, and is awarded for \"outstanding and distinguished contributions to the field of applied mathematical sciences and for the effective communication of these ideas to the community\". It is considered the highest honor bestowed by the Society for Industrial and Applied Mathematics (SIAM). The recipient receives a monetary award and presents a survey lecture at the SIAM Annual Meeting.\n\nSource: Society for Industrial and Applied Mathematics\n"}
{"id": "449781", "url": "https://en.wikipedia.org/wiki?curid=449781", "title": "Key derivation function", "text": "Key derivation function\n\nIn cryptography, a key derivation function (KDF) derives one or more secret keys from a secret value such as a master key, a password, or a passphrase using a pseudorandom function. KDFs can be used to stretch keys into longer keys or to obtain keys of a required format, such as converting a group element that is the result of a Diffie–Hellman key exchange into a symmetric key for use with AES. Keyed cryptographic hash functions are popular examples of pseudorandom functions used for key derivation.\n\n\nKey derivation functions are also used in applications to derive keys from secret passwords or passphrases, which typically do not have the desired properties to be used directly as cryptographic keys. In such applications, it is generally recommended that the key derivation function be made deliberately slow so as to frustrate brute-force attack or dictionary attack on the password or passphrase input value.\n\nSuch use may be expressed as , where is the derived key, is the key derivation function, is the original key or password, is a random number which acts as cryptographic salt, and refers to the number of iterations of a sub-function. The derived key is used instead of the original key or password as the key to the system. The values of the salt and the number of iterations (if it is not fixed) are stored with the hashed password or sent as plaintext with an encrypted message.\n\nThe difficulty of a brute force attack increases with the number of iterations. A practical limit on the iteration count is the unwillingness of users to tolerate a perceptible delay in logging into a computer or seeing a decrypted message. The use of salt prevents the attackers from precomputing a dictionary of derived keys.\n\nAn alternative approach, called key strengthening, extends the key with a random salt, but then (unlike in key stretching) securely deletes the salt. This forces both the attacker and legitimate users to perform a brute-force search for the salt value. Although the paper that introduced key stretching referred to this earlier technique and intentionally chose a different name, the term \"key strengthening\" is now often (arguably incorrectly) used to refer to key stretching.\n\nThe first deliberately slow (key stretching) password-based key derivation function was called \"crypt\" (or \"crypt(3)\" after its man page), and was invented by Robert Morris in 1978. It would encrypt a constant (zero), using the first 8 characters of the user's password as the key, by performing 25 iterations of a modified DES encryption algorithm (in which a 12-bit number read from the real-time computer clock is used to perturb the calculations). The resulting 64-bit number is encoded as 11 printable characters and then stored in the Unix password file. While it was a great advance at the time, increases in processor speeds since the PDP-11 era have made brute-force attacks against crypt feasible, and advances in storage have rendered the 12-bit salt inadequate. The crypt function's design also limits the user password to 8 characters, which limits the keyspace and makes strong passphrases impossible.\n\nModern password-based key derivation functions, such as PBKDF2 (specified in RFC 2898), use a cryptographic hash, such as SHA-2, more salt (e.g. 64 bits and greater) and a high iteration count (often tens or hundreds of thousands).\n\nNIST requires at least 128 bits of random salt and a NIST-approved cryptographic function, such as the SHA series or AES (MD5 is not approved). Although high throughput is a desirable property in general-purpose hash functions, the opposite is true in password security applications in which defending against brute-force cracking is a primary concern. The growing use of massively-parallel hardware such as GPUs, FPGAs, and even ASICs for brute-force cracking has made the selection of a suitable algorithms even more critical because the good algorithm should not only enforce a certain amount of computational cost not only on CPUs, but also resist the cost/performance advantages of modern massively-parallel platforms for such tasks. Various algorithms have been designed specifically for this purpose, including bcrypt, scrypt and, more recently, Argon2 (the winner of the Password Hashing Competition). The large-scale Ashley Madison data breach in which roughly 36 million passwords hashes were stolen by attackers illustrated the importance of algorithm selection in securing passwords. Although bcrypt was employed to protect the hashes (making large scale brute-force cracking expensive and time-consuming), a significant portion of the accounts in the compromised data also contained a password hash based on the general-purpose MD5 algorithm which made it possible for over 11 million of the passwords to be cracked in a matter of weeks.\n\nIn June 2017, NIST issued a new revision of their digital authentication guidelines, NIST SP 800-63B-3, stating that: \"Verifiers SHALL store memorized secrets [i.e. passwords] in a form that is resistant to offline attacks. Memorized secrets SHALL be salted and hashed using a suitable one-way key derivation function. Key derivation functions take a password, a salt, and a cost factor as inputs then generate a password hash. Their purpose is to make each password guessing trial by an attacker who has obtained a password hash file expensive and therefore the cost of a guessing attack high or prohibitive.\" and that \"The salt SHALL be at least 32 bits in length and be chosen arbitrarily so as to minimize salt value collisions among stored hashes.\"\n\n"}
{"id": "22361644", "url": "https://en.wikipedia.org/wiki?curid=22361644", "title": "Lattice word", "text": "Lattice word\n\nIn mathematics, a lattice word (or lattice permutation) is a string composed of positive integers, in which every prefix contains at least as many positive integers \"i\" as integers \"i\" + 1. \n\nA reverse lattice word, or Yamanouchi word, is a string whose reversal is a lattice word.\n\nFor instance, 11122121 is a lattice permutation, so 12122111 is a Yamanouchi word, but 12122111 is not a lattice permutation, since the sub-word 12122 contains more two's than one's.\n\n\n"}
{"id": "17945", "url": "https://en.wikipedia.org/wiki?curid=17945", "title": "Lie group", "text": "Lie group\n\nIn mathematics, a Lie group (pronounced \"Lee\") is a group that is also a differentiable manifold, with the property that the group operations are smooth. Lie groups are named after Norwegian mathematician Sophus Lie, who laid the foundations of the theory of continuous transformation groups.\n\nIn rough terms, a Lie group is a continuous group, that is, one whose elements are described by several real parameters. As such, Lie groups provide a natural model for the concept of continuous symmetry, such as rotational symmetry in three dimensions. Lie groups are widely used in many parts of modern mathematics and physics. Lie's original motivation for introducing Lie groups was to model the continuous symmetries of differential equations, in much the same way that finite groups are used in Galois theory to model the discrete symmetries of algebraic equations.\n\nLie groups are smooth differentiable manifolds and as such can be studied using differential calculus, in contrast with the case of more general topological groups. One of the key ideas in the theory of Lie groups is to replace the \"global\" object, the group, with its \"local\" or linearized version, which Lie himself called its \"infinitesimal group\" and which has since become known as its Lie algebra.\n\nLie groups play an enormous role in modern geometry, on several different levels. Felix Klein argued in his Erlangen program that one can consider various \"geometries\" by specifying an appropriate transformation group that leaves certain geometric properties invariant. Thus Euclidean geometry corresponds to the choice of the group E(3) of distance-preserving transformations of the Euclidean space R, conformal geometry corresponds to enlarging the group to the conformal group, whereas in projective geometry one is interested in the properties invariant under the projective group. This idea later led to the notion of a G-structure, where \"G\" is a Lie group of \"local\" symmetries of a manifold.\n\nLie groups (and their associated Lie algebras) play a major role in modern physics, with the Lie group typically playing the role of a symmetry of a physical system. Here, the representations of the Lie group (or of its Lie algebra) are especially important. Representation theory is used extensively in particle physics. Groups whose representations are of particular importance include the rotation group SO(3) (or its double cover SU(2)), the special unitary group SU(3) and the Poincaré group.\n\nOn a \"global\" level, whenever a Lie group acts on a geometric object, such as a Riemannian or a symplectic manifold, this action provides a measure of rigidity and yields a rich algebraic structure. The presence of continuous symmetries expressed via a Lie group action on a manifold places strong constraints on its geometry and facilitates analysis on the manifold. Linear actions of Lie groups are especially important, and are studied in representation theory.\n\nIn the 1940s–1950s, Ellis Kolchin, Armand Borel, and Claude Chevalley realised that many foundational results concerning Lie groups can be developed completely algebraically, giving rise to the theory of algebraic groups defined over an arbitrary field. This insight opened new possibilities in pure algebra, by providing a uniform construction for most finite simple groups, as well as in algebraic geometry. The theory of automorphic forms, an important branch of modern number theory, deals extensively with analogues of Lie groups over adele rings; \"p\"-adic Lie groups play an important role, via their connections with Galois representations in number theory.\n\nA real Lie group is a group that is also a finite-dimensional real smooth manifold, in which the group operations of multiplication and inversion are smooth maps. Smoothness of the group multiplication\n\nmeans that μ is a smooth mapping of the product manifold \"G\"×\"G\" into \"G\". These two requirements can be combined to the single requirement that the mapping\nbe a smooth mapping of the product manifold into \"G\".\n\n\n\n\nWe now present an example of a group with an uncountable number of elements that is not a Lie group under a certain topology. The group given by \n\nwith formula_10 a \"fixed\" irrational number, is a subgroup of the torus formula_11 that is not a Lie group when given the subspace topology. If we take any small neighborhood formula_12 of a point formula_13 in formula_14, for example, the portion of formula_14 in formula_12 is disconnected. The group formula_14 winds repeatedly around the torus and forms a dense subgroup of formula_11.\n\nThe group formula_14 can, however, be given a different topology, in which the distance between two points formula_20 is defined as the length of the shortest path \"in the group H\" joining formula_21 to formula_22. In this topology, formula_14 is identified homeomorphically with the real line by identifying each element with the number formula_24 in the definition of formula_14. With this topology, formula_14 is just the group of real numbers under addition and is therefore a Lie group.\n\nThe group formula_14 is an example of a \"Lie subgroup\" of a Lie group that is not closed. See the discussion below of Lie subgroups in the section on basic concepts.\n\nLet GL(\"n\"; C) denote the group of invertible matrices with entries in C. Any closed subgroup of GL(\"n\", C) is a Lie group; Lie groups of this sort are called matrix Lie groups. Since most of the interesting examples of Lie groups can be realized as matrix Lie groups, some textbooks restrict attention to this class, including those of Hall and Rossmann. Restricting attention to matrix Lie groups simplifies the definition of the Lie algebra and the exponential map. The following are standard examples of matrix Lie groups.\nAll of the preceding examples fall under the heading of the classical groups.\n\nA complex Lie group is defined in the same way using complex manifolds rather than real ones (example: SL(2, C)), and similarly, using an alternate metric completion of Q, one can define a \"p\"-adic Lie group over the \"p\"-adic numbers, a topological group in which each point has a \"p\"-adic neighborhood. Hilbert's fifth problem asked whether replacing differentiable manifolds with topological or analytic ones can yield new examples. The answer to this question turned out to be negative: in 1952, Gleason, Montgomery and Zippin showed that if \"G\" is a topological manifold with continuous group operations, then there exists exactly one analytic structure on \"G\" which turns it into a Lie group (see also Hilbert–Smith conjecture). If the underlying manifold is allowed to be infinite-dimensional (for example, a Hilbert manifold), then one arrives at the notion of an infinite-dimensional Lie group. It is possible to define analogues of many Lie groups over finite fields, and these give most of the examples of finite simple groups.\n\nThe language of category theory provides a concise definition for Lie groups: a Lie group is a group object in the category of smooth manifolds. This is important, because it allows generalization of the notion of a Lie group to Lie supergroups.\n\nLie groups occur in abundance throughout mathematics and physics. Matrix groups or algebraic groups are (roughly) groups of matrices (for example, orthogonal and symplectic groups), and these give most of the more common examples of Lie groups.\n\nThe only connected Lie groups with dimension one are the real line formula_32 (with the group operation being addition) and the group formula_33 of complex numbers with absolute value one (with the group operation being multiplication). The formula_33 group is often denoted as formula_35, the group of formula_36 unitary matrices.\n\nIn two dimensions, if we restrict attention to simply connected groups, then they are classified by their Lie algebras. There are (up to isomorphism) only two Lie algebras of dimension two. The associated simply connected Lie groups are formula_37 (with the group operation being vector addition) and the affine group in dimension one, described in the previous subsection under \"first examples.\"\n\n\nThere are several standard ways to form new Lie groups from old ones:\n\nSome examples of groups that are \"not\" Lie groups (except in the trivial sense that any group can be viewed as a 0-dimensional Lie group, with the discrete topology), are:\n\n\nTo every Lie group we can associate a Lie algebra whose underlying vector space is the tangent space of the Lie group at the identity element and which completely captures the local structure of the group. Informally we can think of elements of the Lie algebra as elements of the group that are \"infinitesimally close\" to the identity, and the Lie bracket of the Lie algebra is related to the commutator of two such infinitesimal elements. Before giving the abstract definition we give a few examples:\nwhere exp(\"tX\") is defined using the matrix exponential. It can then be shown that the Lie algebra of \"G\" is a real vector space that is closed under the bracket operation, formula_41.\n\nThe concrete definition given above for matrix groups is easy to work with, but has some minor problems: to use it we first need to represent a Lie group as a group of matrices, but not all Lie groups can be represented in this way, and even it is not obvious that the Lie algebra is independent of the representation we use. To get around these problems we give \nthe general definition of the Lie algebra of a Lie group (in 4 steps):\n\nThis Lie algebra formula_43 is finite-dimensional and it has the same dimension as the manifold \"G\". The Lie algebra of \"G\" determines \"G\" up to \"local isomorphism\", where two Lie groups are called locally isomorphic if they look the same near the identity element.\nProblems about Lie groups are often solved by first solving the corresponding problem for the Lie algebras, and the result for groups then usually follows easily. \nFor example, simple Lie groups are usually classified by first classifying the corresponding Lie algebras.\n\nWe could also define a Lie algebra structure on \"T\" using right invariant vector fields instead of left invariant vector fields. This leads to the same Lie algebra, because the inverse map on \"G\" can be used to identify left invariant vector fields with right invariant vector fields, and acts as −1 on the tangent space \"T\".\n\nThe Lie algebra structure on \"T\" can also be described as follows:\nthe commutator operation\n\non \"G\" × \"G\" sends (\"e\", \"e\") to \"e\", so its derivative yields a bilinear operation on \"TG\". This bilinear operation is actually the zero map, but the second derivative, under the proper identification of tangent spaces, yields an operation that satisfies the axioms of a Lie bracket, and it is equal to twice the one defined through left-invariant vector fields.\n\nIf \"G\" and \"H\" are Lie groups, then a Lie group homomorphism \"f\" : \"G\" → \"H\" is a smooth group homomorphism. In the case of complex Lie groups, such a homomorphism is required to be a holomorphic map. However, these requirements are a bit stringent; every continuous homomorphism between real Lie groups turns out to be (real) analytic.\n\nThe composition of two Lie homomorphisms is again a homomorphism, and the class of all Lie groups, together with these morphisms, forms a category. Moreover, every Lie group homomorphism induces a homomorphism between the corresponding Lie algebras. Let formula_45 be a Lie group homomorphism and let formula_46 be its derivative at the identity. If we identify the Lie algebras of \"G\" and \"H\" with their tangent spaces at the identity elements then formula_46 is a map between the corresponding Lie algebras:\nOne can show that formula_46 is actually a Lie algebra homomorphism (meaning that it is a linear map which preserves the Lie bracket). In the language of category theory, we then have a covariant functor from the category of Lie groups to the category of Lie algebras which sends a Lie group to its Lie algebra and a Lie group homomorphism to its derivative at the identity.\n\nTwo Lie groups are called \"isomorphic\" if there exists a bijective homomorphism between them whose inverse is also a Lie group homomorphism. Equivalently, it is a diffeomorphism which is also a group homomorphism. \n\nIsomorphic Lie groups necessarily have isomorphic Lie algebras; it is then reasonable to ask how isomorphism classes of Lie groups relate to isomorphism classes of Lie algebras.\n\nThe first result in this direction is Lie's third theorem, which states that every finite-dimensional, real Lie algebra is the Lie algebra of some (linear) Lie group. One way to prove Lie's third theorem is to use Ado's theorem, which says every finite-dimensional real Lie algebra is isomorphic to a matrix Lie algebra. Meanwhile, for every finite-dimensional matrix Lie algebra, there is a linear group (matrix Lie group) with this algebra as its Lie algebra.\n\nOn the other hand, Lie groups with isomorphic Lie algebras need not be isomorphic. Furthermore, this result remains true even if we assume the groups are connected. To put it differently, the \"global\" structure of a Lie group is not determined by its Lie algebra; for example, if \"Z\" is any discrete subgroup of the center of \"G\" then \"G\" and \"G\"/\"Z\" have the same Lie algebra (see the table of Lie groups for examples). An example of importance in physics are the groups SU(2) and SO(3). These two groups have isomorphic Lie algebras, but the groups themselves are not isomorphic, because SU(2) is simply connected but SO(3) is not.\n\nOn the other hand, if we require that the Lie group be simply connected, then the global structure is determined by its Lie algebra: two simply connected Lie groups with isomorphic Lie algebras are isomorphic. (See the next subsection for more information about simply connected Lie groups.) In light of Lie's third theorem, we may therefore say that there is a one-to-one correspondence between isomorphism classes of finite-dimensional real Lie algebras and isomorphism classes of simply connected Lie groups.\n\nA Lie group \"G\" is said to be simply connected if every loop in \"G\" can be shrunk continuously to a point in \"G\". This notion is important because of the following result that has simple connectedness as a hypothesis:\nLie's third theorem says that every finite-dimensional real Lie algebra is the Lie algebra of a Lie group. It follows from Lie's third theorem and the preceding result that every finite-dimensional real Lie algebra is the Lie algebra of a \"unique\" simply connected Lie group.\n\nAn example of a simply connected group is the special unitary group SU(2), which as a manifold is the 3-sphere. The rotation group SO(3), on the other hand, is not simply connected. (See Topology of SO(3).) The failure of SO(3) to be simply connected is intimately connected to the distinction between integer spin and half-integer spin in quantum mechanics. Other examples of simply connected Lie groups include the special unitary group SU(n), the spin group (double cover of rotation group) Spin(n) for formula_60, and the compact symplectic group Sp(n).\n\nMethods for determining whether a Lie group is simply connected or not are discussed in the article on fundamental groups of Lie groups.\n\nThe exponential map from the Lie algebra formula_61 of the general linear group formula_62 to formula_62 is defined by the matrix exponential, given by the usual power series:\n\nfor matrices formula_65. If formula_50 is a closed subgroup of formula_62, then the exponential map takes the Lie algebra of formula_50 into formula_50; thus, we have an exponential map for all matrix groups. Every element of formula_50 that is sufficiently close to the identity is the exponential of a matrix in the Lie algebra.\n\nThe definition above is easy to use, but it is not defined for Lie groups that are not matrix groups, and it is not clear that the exponential map of a Lie group does not depend on its representation as a matrix group. We can solve both problems using a more abstract definition of the exponential map that works for all Lie groups, as follows.\n\nFor each vector formula_65 in the Lie algebra formula_43 of formula_50 (i.e., the tangent space to formula_50 at the identity), one proves that there is a unique one-parameter subgroup formula_75 such that formula_76. Saying that formula_77 is a one-parameter subgroup means simply that formula_77 is a smooth map into formula_50 and that \n\nfor all \"s\" and \"t\". The operation on the right hand side is the group multiplication in \"G\". The formal similarity of this formula with the one valid for the exponential function justifies the definition\n\nThis is called the exponential map, and it maps the Lie algebra formula_43 into the Lie group \"G\". It provides a diffeomorphism between a neighborhood of 0 in formula_43 and a neighborhood of \"e\" in \"G\". This exponential map is a generalization of the exponential function for real numbers (because R is the Lie algebra of the Lie group of positive real numbers with multiplication), for complex numbers (because C is the Lie algebra of the Lie group of non-zero complex numbers with multiplication) and for matrices (because M(\"n\", R) with the regular commutator is the Lie algebra of the Lie group GL(\"n\", R) of all invertible matrices).\n\nBecause the exponential map is surjective on some neighbourhood \"N\" of \"e\", it is common to call elements of the Lie algebra infinitesimal generators of the group \"G\". The subgroup of \"G\" generated by \"N\" is the identity component of \"G\".\n\nThe exponential map and the Lie algebra determine the \"local group structure\" of every connected Lie group, because of the Baker–Campbell–Hausdorff formula: there exists a neighborhood \"U\" of the zero element of formula_43, such that for \"X\", \"Y\" in \"U\" we have\n\nwhere the omitted terms are known and involve Lie brackets of four or more elements. In case \"X\" and \"Y\" commute, this formula reduces to the familiar exponential law .\n\nThe exponential map relates Lie group homomorphisms. That is, if formula_86 is a Lie group homomorphism and formula_87 the induced map on the corresponding Lie algebras, then for all formula_88 we have\nIn other words, the following diagram commutes,\n\nThe exponential map from the Lie algebra to the Lie group is not always onto, even if the group is connected (though it does map onto the Lie group for connected groups that are either compact or nilpotent). For example, the exponential map of SL(2, R) is not surjective. Also, exponential map is not surjective nor injective for infinite-dimensional (see below) Lie groups modelled on \"C\" Fréchet space, even from arbitrary small neighborhood of 0 to corresponding neighborhood of 1.\n\nA Lie subgroup \"H\" of a Lie group \"G\" is a Lie group that is a subset of \"G\" and such that the inclusion map from \"H\" to \"G\" is an injective immersion and group homomorphism. According to Cartan's theorem, a closed subgroup of \"G\" admits a unique smooth structure which makes it an embedded Lie subgroup of \"G\"—i.e. a Lie subgroup such that the inclusion map is a smooth embedding.\n\nExamples of non-closed subgroups are plentiful; for example take \"G\" to be a torus of dimension ≥ 2, and let \"H\" be a one-parameter subgroup of \"irrational slope\", i.e. one that winds around in \"G\". Then there is a Lie group homomorphism φ : R → \"G\" with \"H\" as its image. The closure of \"H\" will be a sub-torus in \"G\".\n\nThe exponential map gives a one-to-one correspondence between the connected Lie subgroups of a connected Lie group \"G\" and the subalgebras of the Lie algebra of \"G\". Typically, the subgroup corresponding to a subalgebra is not a closed subgroup. There is no criterion solely based on the structure of \"G\" which determines which subalgebras correspond to closed subgroups.\n\nOne important aspect of the study of Lie groups is their representations, that is, the way they can act (linearly) on vector spaces. In physics, Lie groups often encode the symmetries of a physical system. The way one makes use of this symmetry to help analyze the system is often through representation theory. Consider, for example, the time-independent Schrödinger equation in quantum mechanics, formula_90. Assume the system in question has the rotation group SO(3) as a symmetry, meaning that the Hamiltonian operator formula_91 commutes with the action of SO(3) on the wave function formula_92. (One important example of such a system is the Hydrogen atom.) This assumption does not necessarily mean that the solutions formula_92 are rotationally invariant functions. Rather, it means that the \"space\" of solutions to formula_90 is invariant under rotations (for each fixed value of formula_95). This space, therefore, constitutes a representation of SO(3). These representations have been and the classification leads to a substantial simplification of the problem, essentially converting a three-dimensional partial differential equation to a one-dimensional ordinary differential equation.\n\nThe case of a connected compact Lie group \"K\" (including the just-mentioned case of SO(3)) is particularly tractable. In that case, every finite-dimensional representation of \"K\" decomposes as a direct sum of irreducible representations. The irreducible representations, in turn, were classified by Hermann Weyl. The classification is in terms of the \"highest weight\" of the representation. The classification is closely related to the classification of representations of a semisimple Lie algebra.\n\nOne can also study (in general infinite-dimensional) unitary representations of an arbitrary Lie group (not necessarily compact). For example, it is possible to give a relatively simple explicit description of the representations of the group SL(2,R) and the representations of the Poincaré group.\n\nAccording to the most authoritative source on the early history of Lie groups (Hawkins, p. 1), Sophus Lie himself considered the winter of 1873–1874 as the birth date of his theory of continuous groups. Hawkins, however, suggests that it was \"Lie's prodigious research activity during the four-year period from the fall of 1869 to the fall of 1873\" that led to the theory's creation (\"ibid\"). Some of Lie's early ideas were developed in close collaboration with Felix Klein. Lie met with Klein every day from October 1869 through 1872: in Berlin from the end of October 1869 to the end of February 1870, and in Paris, Göttingen and Erlangen in the subsequent two years (\"ibid\", p. 2). Lie stated that all of the principal results were obtained by 1884. But during the 1870s all his papers (except the very first note) were published in Norwegian journals, which impeded recognition of the work throughout the rest of Europe (\"ibid\", p. 76). In 1884 a young German mathematician, Friedrich Engel, came to work with Lie on a systematic treatise to expose his theory of continuous groups. From this effort resulted the three-volume \"Theorie der Transformationsgruppen\", published in 1888, 1890, and 1893. The term \"groupes de Lie\" first appeared in French in 1893 in the thesis of Lie's student Arthur Tresse.\n\nLie's ideas did not stand in isolation from the rest of mathematics. In fact, his interest in the geometry of differential equations was first motivated by the work of Carl Gustav Jacobi, on the theory of partial differential equations of first order and on the equations of classical mechanics. Much of Jacobi's work was published posthumously in the 1860s, generating enormous interest in France and Germany (Hawkins, p. 43). Lie's \"idée fixe\" was to develop a theory of symmetries of differential equations that would accomplish for them what Évariste Galois had done for algebraic equations: namely, to classify them in terms of group theory. Lie and other mathematicians showed that the most important equations for special functions and orthogonal polynomials tend to arise from group theoretical symmetries. In Lie's early work, the idea was to construct a theory of \"continuous groups\", to complement the theory of discrete groups that had developed in the theory of modular forms, in the hands of Felix Klein and Henri Poincaré. The initial application that Lie had in mind was to the theory of differential equations. On the model of Galois theory and polynomial equations, the driving conception was of a theory capable of unifying, by the study of symmetry, the whole area of ordinary differential equations. However, the hope that Lie Theory would unify the entire field of ordinary differential equations was not fulfilled. Symmetry methods for ODEs continue to be studied, but do not dominate the subject. There is a differential Galois theory, but it was developed by others, such as Picard and Vessiot, and it provides a theory of quadratures, the indefinite integrals required to express solutions.\n\nAdditional impetus to consider continuous groups came from ideas of Bernhard Riemann, on the foundations of geometry, and their further development in the hands of Klein. Thus three major themes in 19th century mathematics were combined by Lie in creating his new theory: the idea of symmetry, as exemplified by Galois through the algebraic notion of a group; geometric theory and the explicit solutions of differential equations of mechanics, worked out by Poisson and Jacobi; and the new understanding of geometry that emerged in the works of Plücker, Möbius, Grassmann and others, and culminated in Riemann's revolutionary vision of the subject.\n\nAlthough today Sophus Lie is rightfully recognized as the creator of the theory of continuous groups, a major stride in the development of their structure theory, which was to have a profound influence on subsequent development of mathematics, was made by Wilhelm Killing, who in 1888 published the first paper in a series entitled \"Die Zusammensetzung der stetigen endlichen Transformationsgruppen\" (\"The composition of continuous finite transformation groups\") (Hawkins, p. 100). The work of Killing, later refined and generalized by Élie Cartan, led to classification of semisimple Lie algebras, Cartan's theory of symmetric spaces, and Hermann Weyl's description of representations of compact and semisimple Lie groups using highest weights.\n\nIn 1900 David Hilbert challenged Lie theorists with his Fifth Problem presented at the International Congress of Mathematicians in Paris.\n\nWeyl brought the early period of the development of the theory of Lie groups to fruition, for not only did he classify irreducible representations of semisimple Lie groups and connect the theory of groups with quantum mechanics, but he also put Lie's theory itself on firmer footing by clearly enunciating the distinction between Lie's \"infinitesimal groups\" (i.e., Lie algebras) and the Lie groups proper, and began investigations of topology of Lie groups. The theory of Lie groups was systematically reworked in modern mathematical language in a monograph by Claude Chevalley.\nLie groups may be thought of as smoothly varying families of symmetries. Examples of symmetries include rotation about an axis. What must be understood is the nature of 'small' transformations, for example, rotations through tiny angles, that link nearby transformations. The mathematical object capturing this structure is called a Lie algebra (Lie himself called them \"infinitesimal groups\"). It can be defined because Lie groups are smooth manifolds, so have tangent spaces at each point.\n\nThe Lie algebra of any compact Lie group (very roughly: one for which the symmetries form a bounded set) can be decomposed as a direct sum of an abelian Lie algebra and some number of simple ones. The structure of an abelian Lie algebra is mathematically uninteresting (since the Lie bracket is identically zero); the interest is in the simple summands. Hence the question arises: what are the simple Lie algebras of compact groups? It turns out that they mostly fall into four infinite families, the \"classical Lie algebras\" A, B, C and D, which have simple descriptions in terms of symmetries of Euclidean space. But there are also just five \"exceptional Lie algebras\" that do not fall into any of these families. E is the largest of these.\n\nLie groups are classified according to their algebraic properties (simple, semisimple, solvable, nilpotent, abelian), their connectedness (connected or simply connected) and their compactness.\n\nA first key result is the Levi decomposition, which says that every simply connected Lie group is the semidirect product of a solvable normal subgroup and a semisimple subgroup. \n\n\nThe identity component of any Lie group is an open normal subgroup, and the quotient group is a discrete group. The universal cover of any connected Lie group is a simply connected Lie group, and conversely any connected Lie group is a quotient of a simply connected Lie group by a discrete normal subgroup of the center. Any Lie group \"G\" can be decomposed into discrete, simple, and abelian groups in a canonical way as follows. Write \nso that we have a sequence of normal subgroups\nThen\n\nThis can be used to reduce some problems about Lie groups (such as finding their unitary representations) to the same problems for connected simple groups and nilpotent and solvable subgroups of smaller dimension.\n\n\nLie groups are often defined to be finite-dimensional, but there are many groups that resemble Lie groups, except for being infinite-dimensional. The simplest way to define infinite-dimensional Lie groups is to model them locally on Banach spaces (as opposed to Euclidean space in the finite-dimensional case), and in this case much of the basic theory is similar to that of finite-dimensional Lie groups. However this is inadequate for many applications, because many natural examples of infinite-dimensional Lie groups are not Banach manifolds. Instead one needs to define Lie groups modeled on more general locally convex topological vector spaces. In this case the relation between the Lie algebra and the Lie group becomes rather subtle, and several results about finite-dimensional Lie groups no longer hold.\n\nThe literature is not entirely uniform in its terminology as to exactly which properties of infinite-dimensional groups qualify the group for the prefix \"Lie\" in \"Lie group\". On the Lie algebra side of affairs, things are simpler since the qualifying criteria for the prefix \"Lie\" in \"Lie algebra\" are purely algebraic. For example, an infinite-dimensional Lie algebra may or may not have a corresponding Lie group. That is, there may be a group corresponding to the Lie algebra, but it might not be nice enough to be called a Lie group, or the connection between the group and the Lie algebra might not be nice enough (for example, failure of the exponential map to be onto a neighborhood of the identity). It is the \"nice enough\" that is not universally defined.\n\nSome of the examples that have been studied include:\nThe diffeomorphism group of spacetime sometimes appears in attempts to quantize gravity.\n\n"}
{"id": "13544419", "url": "https://en.wikipedia.org/wiki?curid=13544419", "title": "MIMO", "text": "MIMO\n\nIn radio, multiple-input and multiple-output, or MIMO (), is a method for multiplying the capacity of a radio link using multiple transmit and receive antennas to exploit multipath propagation. MIMO has become an essential element of wireless communication standards including IEEE 802.11n (Wi-Fi), IEEE 802.11ac (Wi-Fi), HSPA+ (3G), WiMAX (4G), and Long Term Evolution (LTE 4G). More recently, MIMO has been applied to power-line communication for 3-wire installations as part of ITU G.hn standard and HomePlug AV2 specification.\n\nAt one time, in wireless the term \"MIMO\" referred to the use of multiple antennas at the transmitter and the receiver. In modern usage, \"MIMO\" specifically refers to a practical technique for sending and receiving more than one data signal simultaneously over the same radio channel by exploiting multipath propagation. MIMO is fundamentally different from smart antenna techniques developed to enhance the performance of a single data signal, such as beamforming and diversity.\n\nMIMO is often traced back to 1970s research papers concerning multi-channel digital transmission systems and interference (crosstalk) between wire pairs in a cable bundle: AR Kaye and DA George (1970), Branderburg and Wyner (1974), and W. van Etten (1975, 1976). Although these are not examples of exploiting multipath propagation to send multiple information streams, some of the mathematical techniques for dealing with mutual interference proved useful to MIMO development. In the mid-1980s Jack Salz at Bell Laboratories took this research a step further, investigating multi-user systems operating over \"mutually cross-coupled linear networks with additive noise sources\" such as time-division multiplexing and dually-polarized radio systems.\n\nMethods were developed to improve the performance of cellular radio networks and enable more aggressive frequency reuse in the early 1990s. Space-division multiple access (SDMA) uses directional or smart antennas to communicate on the same frequency with users in different locations within range of the same base station. An SDMA system was proposed by Richard Roy and Björn Ottersten, researchers at ArrayComm, in 1991. Their US patent (No. 5515378 issued in 1996) describes a method for increasing capacity using \"an array of receiving antennas at the base station\" with a \"plurality of remote users.\"\n\nArogyaswami Paulraj and Thomas Kailath proposed an SDMA-based inverse multiplexing technique in 1993. Their US patent (No. 5,345,599 issued in 1994) described a method of broadcasting at high data rates by splitting a high-rate signal \"into several low-rate signals\" to be transmitted from \"spatially separated transmitters\" and recovered by the receive antenna array based on differences in \"directions-of-arrival.\" Paulraj was awarded the prestigious Marconi Prize in 2014 for \"his pioneering contributions to developing the theory and applications of MIMO antennas. ... His idea for using multiple antennas at both the transmitting and receiving stations – which is at the heart of the current high speed WiFi and 4G mobile systems – has revolutionized high speed wireless.\"\n\nIn an April 1996 paper and subsequent patent, Greg Raleigh proposed that natural multipath propagation can be exploited to transmit multiple, independent information streams using co-located antennas and multi-dimensional signal processing. The paper also identified practical solutions for modulation (MIMO-OFDM), coding, synchronization, and channel estimation. Later that year (September 1996) Gerard J. Foschini submitted a paper that also suggested it is possible to multiply the capacity of a wireless link using what the author described as \"layered space-time architecture.\"\n\nGreg Raleigh, V. K. Jones, and Michael Pollack founded Clarity Wireless in 1996, and built and field-tested a prototype MIMO system. Cisco Systems acquired Clarity Wireless in 1998. Bell Labs built a laboratory prototype demonstrating its V-BLAST (Vertical-Bell Laboratories Layered Space-Time) technology in 1998. Arogyaswami Paulraj founded Iospan Wireless in late 1998 to develop MIMO-OFDM products. Iospan was acquired by Intel in 2003. V-BLAST was never commercialized, and neither Clarity Wireless nor Iospan Wireless shipped MIMO-OFDM products before being acquired.\n\nMIMO technology has been standardized for wireless LANs, 3G mobile phone networks, and 4G mobile phone networks and is now in widespread commercial use. Greg Raleigh and V. K. Jones founded Airgo Networks in 2001 to develop MIMO-OFDM chipsets for wireless LANs. The Institute of Electrical and Electronics Engineers (IEEE) created a task group in late 2003 to develop a wireless LAN standard delivering at least 100 Mbit/s of user data throughput. There were two major competing proposals: TGn Sync was backed by companies including Intel and Philips, and WWiSE was supported by companies including Airgo Networks, Broadcom, and Texas Instruments. Both groups agreed that the 802.11n standard would be based on MIMO-OFDM with 20 MHz and 40 MHz channel options. TGn Sync, WWiSE, and a third proposal (MITMOT, backed by Motorola and Mitsubishi) were merged to create what was called the Joint Proposal. In 2004, Airgo became the first company to ship MIMO-OFDM products. Qualcomm acquired Airgo Networks in late 2006. The final 802.11n standard supported speeds up to 600 Mbit/s (using four simultaneous data streams) and was published in late 2009.\n\nSurendra Babu Mandava and Arogyaswami Paulraj founded Beceem Communications in 2004 to produce MIMO-OFDM chipsets for WiMAX. The company was acquired by Broadcom in 2010. WiMAX was developed as an alternative to cellular standards, is based on the 802.16e standard, and uses MIMO-OFDM to deliver speeds up to 138 Mbit/s. The more advanced 802.16m standard enables download speeds up to 1 Gbit/s. A nationwide WiMAX network was built in the United States by Clearwire, a subsidiary of Sprint-Nextel, covering 130 million points of presence (PoP) by mid-2012. Sprint subsequently announced plans to deploy LTE (the cellular 4G standard) covering 31 cities by mid-2013 and to shut down its WiMAX network by the end of 2015.\nThe first 4G cellular standard was proposed by NTT DoCoMo in 2004. Long term evolution (LTE) is based on MIMO-OFDM and continues to be developed by the 3rd Generation Partnership Project (3GPP). LTE specifies downlink rates up to 300 Mbit/s, uplink rates up to 75 Mbit/s, and quality of service parameters such as low latency. LTE Advanced adds support for picocells, femtocells, and multi-carrier channels up to 100 MHz wide. LTE has been embraced by both GSM/UMTS and CDMA operators.\n\nThe first LTE services were launched in Oslo and Stockholm by TeliaSonera in 2009. There are currently more than 360 LTE networks in 123 countries operational with approximately 373 million connections (devices).\n\nMIMO can be sub-divided into three main categories: precoding, spatial multiplexing (SM), and diversity coding.\n\nPrecoding is multi-stream beamforming, in the narrowest definition. In more general terms, it is considered to be all spatial processing that occurs at the transmitter. In (single-stream) beamforming, the same signal is emitted from each of the transmit antennas with appropriate phase and gain weighting such that the signal power is maximized at the receiver input. The benefits of beamforming are to increase the received signal gain – by making signals emitted from different antennas add up constructively – and to reduce the multipath fading effect. In line-of-sight propagation, beamforming results in a well-defined directional pattern. However, conventional beams are not a good analogy in cellular networks, which are mainly characterized by multipath propagation. When the receiver has multiple antennas, the transmit beamforming cannot simultaneously maximize the signal level at all of the receive antennas, and precoding with multiple streams is often beneficial. Note that precoding requires knowledge of channel state information (CSI) at the transmitter and the receiver.\n\nSpatial multiplexing requires MIMO antenna configuration. In spatial multiplexing, a high-rate signal is split into multiple lower-rate streams and each stream is transmitted from a different transmit antenna in the same frequency channel. If these signals arrive at the receiver antenna array with sufficiently different spatial signatures and the receiver has accurate CSI, it can separate these streams into (almost) parallel channels. Spatial multiplexing is a very powerful technique for increasing channel capacity at higher signal-to-noise ratios (SNR). The maximum number of spatial streams is limited by the lesser of the number of antennas at the transmitter or receiver. Spatial multiplexing can be used without CSI at the transmitter, but can be combined with precoding if CSI is available. Spatial multiplexing can also be used for simultaneous transmission to multiple receivers, known as space-division multiple access or multi-user MIMO, in which case CSI is required at the transmitter. The scheduling of receivers with different spatial signatures allows good separability.\n\nDiversity coding techniques are used when there is no channel knowledge at the transmitter. In diversity methods, a single stream (unlike multiple streams in spatial multiplexing) is transmitted, but the signal is coded using techniques called space-time coding. The signal is emitted from each of the transmit antennas with full or near orthogonal coding. Diversity coding exploits the independent fading in the multiple antenna links to enhance signal diversity. Because there is no channel knowledge, there is no beamforming or array gain from diversity coding.\nDiversity coding can be combined with spatial multiplexing when some channel knowledge is available at the transmitter.\n\nMulti-antenna MIMO (or Single user MIMO) technology has been developed and implemented in some standards, e.g., 802.11n products.\n\nRecently, results of research on multi-user MIMO technology have been emerging. While full multi-user MIMO (or network MIMO) can have a higher potential, practically, the research on (partial) multi-user MIMO (or multi-user and multi-antenna MIMO) technology is more active.\n\nSpatial multiplexing techniques make the receivers very complex, and therefore they are typically combined with Orthogonal frequency-division multiplexing (OFDM) or with Orthogonal Frequency Division Multiple Access (OFDMA) modulation, where the problems created by a multi-path channel are handled efficiently. The IEEE 802.16e standard incorporates MIMO-OFDMA. The IEEE 802.11n standard, released in October 2009, recommends MIMO-OFDM.\n\nMIMO is also planned to be used in Mobile radio telephone standards such as recent 3GPP and 3GPP2. In 3GPP, High-Speed Packet Access plus (HSPA+) and Long Term Evolution (LTE) standards take MIMO into account. Moreover, to fully support cellular environments, MIMO research consortia including IST-MASCOT propose to develop advanced MIMO techniques, e.g., multi-user MIMO (MU-MIMO).\n\nMIMO technology can be used in non-wireless communications systems. One example is the home networking standard ITU-T G.9963, which defines a powerline communications system that uses MIMO techniques to transmit multiple signals over multiple AC wires (phase, neutral and ground).\n\nIn MIMO systems, a transmitter sends multiple streams by multiple transmit antennas. The transmit streams go through a matrix channel which consists of all formula_1 paths between the formula_2 transmit antennas at the transmitter and formula_3 receive antennas at the receiver. Then, the receiver gets the received signal vectors by the multiple receive antennas and decodes the received signal vectors into the original information. A narrowband flat fading MIMO system is modelled as:\nwhere formula_5 and formula_6 are the receive and transmit vectors, respectively, and formula_7 and formula_8 are the channel matrix and the noise vector, respectively.\n\nReferring to information theory, the ergodic channel capacity of MIMO systems where both the transmitter and the receiver have perfect instantaneous channel state information is\nwhere formula_10 denotes Hermitian transpose and formula_11 is the ratio between transmit power and noise power (i.e., transmit SNR). The optimal signal covariance formula_12 is achieved through singular value decomposition of the channel matrix formula_13 and an optimal diagonal power allocation matrix formula_14. The optimal power allocation is achieved through waterfilling, that is\nwhere formula_16 are the diagonal elements of formula_17, formula_18 is zero if its argument is negative, and formula_19 is selected such that formula_20.\n\nIf the transmitter has only statistical channel state information, then the ergodic channel capacity will decrease as the signal covariance formula_21 can only be optimized in terms of the average mutual information as\nThe spatial correlation of the channel has a strong impact on the ergodic channel capacity with statistical information.\n\nIf the transmitter has no channel state information it can select the signal covariance formula_21 to maximize channel capacity under worst-case statistics, which means formula_24 and accordingly\n\nDepending on the statistical properties of the channel, the ergodic capacity is no greater than formula_26 times larger than that of a SISO system.\n\nMIMO signal testing focuses first on the transmitter/receiver system. The random phases of the sub-carrier signals can produce instantaneous power levels that cause the amplifier to compress, momentarily causing distortion and ultimately symbol errors. Signals with a high PAR (peak-to-average ratio) can cause amplifiers to compress unpredictably during transmission. OFDM signals are very dynamic and compression problems can be hard to detect because of their noise-like nature.\n\nKnowing the quality of the signal channel is also critical. A channel emulator can simulate how a device performs at the cell edge, can add noise or can simulate what the channel looks like at speed. To fully qualify the performance of a receiver, a calibrated transmitter, such as a vector signal generator (VSG), and channel emulator can be used to test the receiver under a variety of different conditions. Conversely, the transmitter's performance under a number of different conditions can be verified using a channel emulator and a calibrated receiver, such as a vector signal analyzer (VSA).\n\nUnderstanding the channel allows for manipulation of the phase and amplitude of each transmitter in order to form a beam. To correctly form a beam, the transmitter needs to understand the characteristics of the channel. This process is called \"channel sounding\" or channel estimation. A known signal is sent to the mobile device that enables it to build a picture of the channel environment. The mobile device sends back the channel characteristics to the transmitter. The transmitter can then apply the correct phase and amplitude adjustments to form a beam directed at the mobile device. This is called a closed-loop MIMO system. For beamforming, it is required to adjust the phases and amplitude of each transmitter. In a beamformer optimized for spatial diversity or spatial multiplexing, each antenna element simultaneously transmits a weighted combination of two data symbols.\n\nPapers by Gerard J. Foschini and Michael J. Gans, Foschini and Emre Telatar have shown that the channel capacity (a theoretical upper bound on system throughput) for a MIMO system is increased as the number of antennas is increased, proportional to the smaller of the number of transmit antennas and the number of receive antennas. This is known as the multiplexing gain and this basic finding in information theory is what led to a spurt of research in this area. Despite the simple propagation models used in the aforementioned seminal works, the multiplexing gain is a fundamental property that can be proved under almost any physical channel propagation model and with practical hardware that is prone to transceiver impairments.\n\nPapers by Dr. Fernando Rosas and Dr. Christian Oberli have shown that the entire MIMO SVD link can be approximated by the average of the SER of Nakagami-m channels. This leads to characterise the eigenchannels of N × N MIMO channels with N larger than 14, showing that the smallest eigenchannel distributes as a Rayleigh channel, the next four eigenchannels closely distributes as Nakagami-m channels with m = 4, 9, 25 and 36, and the N – 5 remaining eigenchannels have statistics similar to an additive white Gaussian noise (AWGN) channel within 1 dB signal-to-noise ratio. It is also shown that 75% of the total mean power gain of the MIMO SVD channel goes to the top third of all the eigenchannels.\n\nA textbook by A. Paulraj, R. Nabar and D. Gore has published an introduction to this area. There are many other principal textbooks available as well.\n\nThere exists a fundamental tradeoff between transmit diversity and spatial multiplexing gains in a MIMO system (Zheng and Tse, 2003). In particular, achieving high spatial multiplexing gains is of profound importance in modern wireless systems.\n\nGiven the nature of MIMO, it is not limited to wireless communication. It can be used for wire line communication as well. For example, a new type of DSL technology (gigabit DSL) has been proposed based on binder MIMO channels.\n\nAn important question which attracts the attention of engineers and mathematicians is how to use the multi-output signals at the receiver to recover the multi-input signals at the transmitter. In Shang, Sun and Zhou (2007), sufficient and necessary conditions are established to guarantee the complete recovery of the multi-input signals.\n\n"}
{"id": "10354285", "url": "https://en.wikipedia.org/wiki?curid=10354285", "title": "Macdonald polynomials", "text": "Macdonald polynomials\n\nIn mathematics, Macdonald polynomials \"P\"(\"x\"; \"t\",\"q\") are a family of orthogonal symmetric polynomials in several variables, introduced by . He later introduced a non-symmetric generalization in 1995. Macdonald originally associated his polynomials with weights λ of finite root systems and used just one variable \"t\", but later realized that it is more natural to associate them with affine root systems rather than finite root systems, in which case the variable \"t\" can be replaced by several different variables \"t\"=(\"t\"...,\"t\"), one for each of the \"k\" orbits of roots in the affine root system. The Macdonald polynomials are polynomials in \"n\" variables \"x\"=(\"x\"...,\"x\"), where \"n\" is the rank of the affine root system. They generalize many other families of orthogonal polynomials, such as Jack polynomials and Hall–Littlewood polynomials and Askey–Wilson polynomials, which in turn include most of the named 1-variable orthogonal polynomials as special cases. Koornwinder polynomials are Macdonald polynomials of certain non-reduced root systems. They have deep relationships with affine Hecke algebras and Hilbert schemes, which were used to prove several conjectures made by Macdonald about them.\n\nFirst fix some notation:\n\nThe Macdonald polynomials \"P\" for λ ∈ \"P\" are uniquely defined by the following two conditions:\n\nIn other words, the Macdonald polynomials are obtained by orthogonalizing the obvious basis for \"A\". The existence of polynomials with these properties is easy to show (for any inner product). A key property of the Macdonald polynomials is that they are orthogonal: 〈\"P\", \"P\"〉 = 0 whenever λ ≠ μ. This is not a trivial consequence of the definition because \"P\" is not totally ordered, and so has plenty of elements that are incomparable. Thus one must check that the corresponding polynomials are still orthogonal. The orthogonality can be proved by showing that the Macdonald polynomials are eigenvectors \nfor an algebra of commuting self adjoint operators with 1-dimensional eigenspaces, and using the fact that eigenspaces for different eigenvalues must be orthogonal.\n\nIn the case of non-simply-laced root systems (B, C, F, G), the parameter \"t\" can be chosen to vary with the length of the root, giving a three-parameter family of Macdonald polynomials. One can also extend the definition to the nonreduced root system BC, in which case one obtains a six-parameter family (one \"t\" for each orbit of roots, plus \"q\") known as Koornwinder polynomials. It is sometimes better to regard Macdonald polynomials as depending on a possibly non-reduced affine root system. In this case there is one parameter \"t\" associated to each orbit of roots in the affine root system, plus one parameter \"q\". The number of orbits of roots can vary from 1 to 5.\n\n\nIf \"t\" = \"q\" for some positive integer \"k\", then the norm of the Macdonald polynomials is given by\n\nAgain, these were proved for general reduced root systems by , using double affine Hecke algebras, with the extension to the BC case following shortly thereafter via work of van Diejen, Noumi, and Sahi.\n\nIn the case of roots systems of type \"A\" the Macdonald polynomials\nare simply symmetric polynomials in \"n\" variables with coefficients that are rational functions of \"q\" and \"t\". A certain transformed version formula_8 of the Macdonald polynomials (see Combinatorial formula below) form an orthogonal basis of the space of symmetric functions over formula_9, and therefore can be expressed in terms of Schur functions formula_10. The coefficients \"K\"(\"q\",\"t\") of these relations are called Kostka–Macdonald coefficients or \"qt\"-Kostka coefficients.\nMacdonald conjectured that the Kostka–Macdonald coefficients were polynomials in \"q\" and \"t\" with non-negative integer coefficients. These conjectures are now proved; the hardest and final step was proving the positivity, which was done by Mark Haiman (2001), by proving the \"n\"! conjecture.\n\nIt is still a central open problem in algebraic combinatorics to find a combinatorial formula for the \"qt\"-Kostka coefficients.\n\nThe \"n\"! conjecture of Adriano Garsia and Mark Haiman states that for each partition μ of \"n\" the space\n\nspanned by all higher partial derivatives of\n\nhas dimension \"n\"!, where (\"p\", \"q\") run through the \"n\" elements of the diagram of the partition μ, regarded as a subset of the pairs of non-negative integers. \nFor example, if μ is the partition 3 = 2 + 1 of \"n\" = 3 then the pairs (\"p\", \"q\") are\n(0, 0), (0, 1), (1, 0), and the space \"D\" is spanned by\nwhich has dimension 6 = 3<nowiki>!</nowiki>.\n\nHaiman's proof of the Macdonald positivity conjecture and the \"n\"! conjecture involved showing that the isospectral Hilbert scheme of \"n\" points in a plane was Cohen–Macaulay (and even Gorenstein). Earlier results of Haiman and Garsia had already shown that this implied the \"n\"! conjecture, and that the \"n\"! conjecture implied that the Kostka–Macdonald coefficients were graded character multiplicities for the modules \"D\". This immediately implies the Macdonald positivity conjecture because character multiplicities have to be non-negative integers.\n\nIan Grojnowski and Mark Haiman found another proof of the Macdonald positivity conjecture by proving a positivity conjecture for LLT polynomials.\n\nIn 2005, J. Haglund, M. Haiman and N. Loehr gave the first proof of a combinatorial interpretation of the \nMacdonald polynomials. While very useful for computation and interesting in its own right, this combinatorial formula does not immediately imply positivity of the Kostka-Macdonald coefficients formula_19 as it gives the decomposition of the Macdonald polynomials into monomial symmetric functions rather than into Schur functions.\n\nThe formula, which involves the \"transformed Macdonald polynomials\" formula_8 rather than the usual formula_21, is given as\n\nwhere σ is a filling of the Young diagram of shape μ, \"inv\" and \"maj\" are certain combinatorial statistics (functions) defined on the filling σ. This formula expresses the Macdonald polynomials in infinitely many variables. To obtain the polynomials in \"n\" variables, simply restrict the formula to fillings that only use the integers 1, 2, ..., \"n\". The term \"x\" should be interpreted as formula_23 where \"σ\" is the number of boxes in the filling of μ with content \"i\".\n\nThe transformed Macdonald polynomials formula_24 in the formula above are related to the classical Macdonald polynomials formula_25 via a sequence of transformations. First, the \"integral form\" of the Macdonald polynomials, denoted formula_26, is a re-scaling of formula_27 that clears the denominators of the coefficients:\n\nwhere formula_29 is the collection of squares in the Young diagram of formula_30, and formula_31 and formula_32 denote the \"arm\" and \"leg\" of the square formula_33, as shown in the figure. \"Note: The figure at right uses French notation for tableau, which is flipped vertically from the English notation used on the Wikipedia page for Young diagrams. French notation is more commonly used in the study of Macdonald polynomials.\"\n\nThe transformed Macdonald polynomials formula_24 can then be defined in terms of the formula_35's. We have\n\nwhere \n\nThe bracket notation above denotes plethystic substitution.\n\nThis formula can be used to prove Knop and Sahi's formula for the Jack polynomials. T\n\nIn 1995, Macdonald introduced a non-symmetric analogue of the symmetric Macdonald polynomials,\nand the symmetric Macdonald polynomials can easily be recovered from the non-symmetric counterpart.\nIn his original definition, he shows that the non-symmetric Macdonald polynomials are a unique family of \npolynomials orthogonal to a certain inner product, as well as satisfying a \ntriangularity property when expanded in the monomial basis.\n\nIn 2007, Haglund, Haiman and Loehr gave a combinatorial formula for the non-symmetric Macdonald polynomials.\n\nThe non-symmetric Macdonald polynomials specialize to Demazure characters by taking q=t=0,\nand to key polynomials when q=t=∞.\n\n\n"}
{"id": "23244381", "url": "https://en.wikipedia.org/wiki?curid=23244381", "title": "Manipulation check", "text": "Manipulation check\n\nManipulation check is a term in experimental research in the social sciences which refers to certain kinds of secondary evaluations of an experiment.\n\nManipulation checks are measured variables that show what the manipulated variables concurrently affect besides the dependent variable of interest.\n\nIn experiments, an experimenter manipulates some aspect of a process or task and randomly assigns subjects to different levels of the manipulation (\"experimental conditions\"). The experimenter then observes whether variation in the manipulated variables cause differences in the dependent variable. Manipulation checks are targeted at variables beside the dependent variable of interest.\n\nManipulations are not intended to verify that the manipulated factor caused variation in the dependent variable. This is verified by random assignment, manipulation before measurement of the dependent variable, and statistical tests of effect of the manipulated variable on the dependent variable. Thus, a failed manipulation check does not refute the hypothesis that the manipulation caused variation in the dependent variable.\n\nIn contrast, a successful manipulation check can help an experimenter rule out reasons that a manipulation may have failed to influence a dependent variable. When a manipulation creates significant differences between experimental conditions in both (1) the dependent variable and (2) the measured manipulation check variable, the interpretation is that (1) the manipulation \"causes\" variation in the dependent variable (the \"effect\") and (2) the manipulation also explains variation in some other, more theoretically obvious measured variable that it is expected to concurrently influence, which assists in \"interpreting\" the \"cause\" (i.e., it only help interpret the \"cause\"; it is not necessary to \"affirm\" that the \"cause\" \"causes\" an effect).\n"}
{"id": "44714408", "url": "https://en.wikipedia.org/wiki?curid=44714408", "title": "Margaret K. Butler", "text": "Margaret K. Butler\n\nMargaret Kampschaefer Butler (March 27, 1924 – March 8, 2013) was a longtime mathematician who participated in creating and updating computer software. During the early 1950s, Butler contributed to the development of early computers. Butler was the first female fellow at the American Nuclear Society and director of the National Energy Software Center at Argonne. Butler held leadership positions within multiple scientific organizations and women's groups. She was the creator and director of the National Energy Software Center. Here, Butler operated an exchange for the editing of computer programs in regards to nuclear power and developed early principles for computer technology.\n\nButler was born on March 27, 1924 in Evansville, Indiana.\n\nButler began her career in 1944 working as a statistician at the Bureau of Labor Statistics. While she worked there, she also taught math at the United States Department of Agriculture Graduate School and took graduate courses related to sampling theory. About a year later, she joined the United States Air Force and worked as a civilian in Germany. She returned to the United States after two years and began working in the Naval Reactors Division of Argonne National Laboratory as a junior mathematician. While working at Argonne, Butler made calculations for physicists creating a prototype for a submarine reactor and attended atomic physics and reactor design classes. In 1949, she worked at the Bureau of Labor Statistics in Minnesota but returned to Argonne National Laboratory in 1951. Following her return to Argonne, Butler became an assistant mathematician in the Reactor Engineering Division and worked on AVIDAC, an early computer. In the 1950s Butler wrote software, reactor applications, mathematical subroutines, and utilities for three other Argonne computers, the ORACLE, GEORGE, and UNIVAC. From the late 1950s to early 1960s she led Argonne's Applied Mathematics Division's Application Programming. While working in this department, Butler developed teams to fix program problems in reactors, biology, chemistry, physics, management, and high energy physics applications. In 1960, Butler worked with others to establish the Argonne Code Center, which later became the National Energy Software Center (NESC). Butler would later become director of the NESC from 1972–1991. In 1980, Butler was promoted to Senior Computer Scientist at Argonne. Butler officially retired in 1991, but continued to work at Argonne from 1993 to 2006 as a \"special term appointee\".\n\nDuring her time in Argonne, Butler was very supportive of her female coworkers. Women working at Argonne described her as a role model with a welcoming presence. According to Margaret's son Jay, she thought women were \"given all the responsibilities and none of the authorities\" and had to work \"harder and smarter\" yet were still not treated as individuals. When Butler rose in the ranks at Argonne, she made sure to hire women and recommend them for promotions. Margaret worked with other women to organize an Association for Women in Science in Chicago. While in AWIS, Margaret held executive board positions and led two conferences for high school students, teachers, and administration.\n"}
{"id": "47188724", "url": "https://en.wikipedia.org/wiki?curid=47188724", "title": "Martin Farach-Colton", "text": "Martin Farach-Colton\n\nMartin Farach-Colton is an American computer scientist, known for his work in streaming algorithms, suffix tree construction, pattern matching in compressed data, cache-oblivious algorithms, and lowest common ancestor data structures. He is a professor of computer science at Rutgers University, and a co-founder of storage technology startup company Tokutek.\n\nFarach-Colton is of Argentine descent, and grew up in South Carolina. While attending medical school, he came out as gay, and met his future husband, with whom he now has twin children. Farach-Colton is an avid Brazilian jiu-jitsu practitioner and received a bronze medal at the 2015 World Master Jiu-Jitsu IBJJF Championship. He obtained his Ph.D. in 1991 from the University of Maryland, College Park under the supervision of Amihood Amir. He was program chair of the 14th ACM-SIAM Symposium on Discrete Algorithms (SODA 2003).\n\nThe cache-oblivious B-tree data structures studied by Bender, Demaine, and Farach-Colton beginning in 2000 became the basis for the fractal tree index used by Tokutek's products TokuDB and TokuMX.\n\nFarach-Colton also serves on several charity boards including the Ali Forney Center and Lamda Legal.\n\n\n"}
{"id": "3180285", "url": "https://en.wikipedia.org/wiki?curid=3180285", "title": "Maxime Bôcher", "text": "Maxime Bôcher\n\nMaxime Bôcher (August 28, 1867 – September 12, 1918) was an American mathematician who published about 100 papers on differential equations, series, and algebra. He also wrote elementary texts such as \"Trigonometry\" and \"Analytic Geometry\". Bôcher's theorem, Bôcher's equation, and the Bôcher Memorial Prize are named after him.\n\nBôcher was born in Boston, Massachusetts. His parents were Caroline Little and Ferdinand Bôcher. Maxime's father was professor of modern languages at the Massachusetts Institute of Technology when Maxime was born, and became Professor of French at Harvard in 1872.\n\nBôcher received an excellent education from his parents and from a number of public and private schools in Massachusetts. He graduated from the Cambridge Latin School in 1883. He received his first degree from Harvard in 1888. At Harvard, he studied a wide range of topics, including mathematics, Latin, chemistry, philosophy, zoology, geography, geology, meteorology, Roman art, and music.\n\nBôcher was awarded many prestigious prizes, which allowed him to travel to Europe to do research. Göttingen was then the leading mathematics university, and he attended lectures by Klein, Schönflies, Schwarz, Schur and Voigt. He was awarded a doctorate in 1891 for his dissertation \"Über die Reihenentwicklungen der Potentialtheorie\" (German for \"On the Development of the Potential Function into Series\"), he was encouraged to study this topic by Klein. He received a Göttingen university prize for this work.\n\nIn Göttingen he met Marie Niemann, and they were married in July 1891. They had three children, Helen, Esther, and Frederick. He returned with his wife to Harvard where he was appointed as an instructor. In 1894 he was promoted to assistant professor, due to his impressive record. He became a full professor of mathematics in 1904. He was president of the American Mathematical Society from 1908 to 1910.\n\nAlthough he was only 46 years old, there were already signs that his weak health was failing. He died at his Cambridge home after suffering a prolonged illness.\n\nBôcher's theorem states that the finite zeros of the derivative formula_1 of a nonconstant rational function formula_2 that are not multiple zeros of formula_2 are the positions of equilibrium in the field of force due to particles of positive mass at the zeros of formula_2 and particles of negative mass at the poles of formula_2, with masses numerically equal to the respective multiplicities, where each particle repels with a force equal to the mass times the inverse distance.\n\nBôcher's equation is a second-order ordinary differential equation of the form:\n\nThe Bôcher Memorial Prize is awarded by the American Mathematical Society every five years for notable research in analysis that has appeared in a recognized North American journal.\n\nWinners have included James W. Alexander II (1928), Eric Temple Bell (1924), George D. Birkhoff (1923), Paul J. Cohen (1964), Solomon Lefschetz (1924), Marston Morse and Norbert Wiener (1933), and John von Neumann (1938).\n\n\nBôcher was one of the editors of the \"Annals of Mathematics\", of the \"Transactions\" of the American Mathematical Society.\n\n"}
{"id": "35879943", "url": "https://en.wikipedia.org/wiki?curid=35879943", "title": "MyMaths", "text": "MyMaths\n\nMyMaths is a subscription-based mathematics website based on Adobe Flash which can be used on interactive whiteboards or by students and teachers at home. It is owned and operated by Oxford University Press, who acquired the site in 2011. MyMaths is currently used in over 80% of secondary schools in the UK. As of now MyMaths has over 4 million student users in over 70 countries worldwide, many of whom are in England.\n\nUsers are required to have subscriptions, after which they are given a username and password to log in and access MyMaths. MyMaths has a wide range of curriculum materials and resources aimed at students in primary school and high school. \n\n"}
{"id": "8426019", "url": "https://en.wikipedia.org/wiki?curid=8426019", "title": "Negligible function", "text": "Negligible function\n\nIn mathematics, a negligible function is a function formula_1 such that for every positive integer \"c\" there exists an integer \"N\" such that for all \"x\" > \"N\",\n\nEquivalently, we may also use the following definition.\nA function formula_3 is negligible, if for every positive polynomial poly(·) there exists an integer \"N\" > 0 such that for all \"x\" > \"N\"\n\nThe concept of \"negligibility\" can find its trace back to sound models of analysis. Though the concepts of \"continuity\" and \"infinitesimal\" became important in mathematics during Newton and Leibniz's time (1680s), they were not well-defined until the late 1810s. The first reasonably rigorous definition of \"continuity\" in mathematical analysis was due to Bernard Bolzano, who wrote in 1817 the modern definition of continuity. Later Cauchy, Weierstrass and Heine also defined as follows (with all numbers in the real number domain formula_5):\n\nThis classic definition of continuity can be transformed into the\ndefinition of negligibility in a few steps by changing parameters used in the definition. First, in the case formula_12 with formula_13, we must define the concept of \"infinitesimal function\":\n\nNext, we replace formula_8 by the functions formula_21 where formula_22 or by formula_23 where formula_24 is a positive polynomial. This leads to the definitions of negligible functions given at the top of this article. Since the constants formula_8 can be expressed as formula_23 with a constant polynomial this shows that negligible functions are a subset of the infinitesimal functions.\n\nIn complexity-based modern cryptography, a security scheme is\n\"provably secure\" if the probability of security failure (e.g.,\ninverting a one-way function, distinguishing cryptographically strong pseudorandom bits from truly random bits) is negligible in terms of the input formula_15 = cryptographic key length formula_28. Hence comes the definition at the top of the page because key length formula_28 must be a natural number.\n\nNevertheless, the general notion of negligibility doesn't require that the input parameter formula_15 is the key length formula_28. Indeed, formula_15 can be any predetermined system metric and corresponding mathematical analysis would illustrate some hidden analytical behaviors of the system.\n\nThe reciprocal-of-polynomial formulation is used for the same reason that computational boundedness is defined as polynomial running time: it has mathematical closure properties that make it tractable in the asymptotic setting (see #Closure properties). For example, if an attack succeeds in violating a security condition only with negligible probability, and the attack is repeated a polynomial number of times, the success probability of the overall attack still remains negligible.\n\nIn practice one might want to have more concrete functions bounding the adversary's success probability and to choose the security parameter large enough that this probability is smaller than some threshold, say 2.\n\nOne of the reasons that negligible functions are used in foundations of complexity-theoretic cryptography is that they obey closure properties. Specifically,\n\n\nConversely, if formula_35 is not negligible, then neither is formula_39 for any real polynomial formula_36.\n\n\n\n"}
{"id": "38050347", "url": "https://en.wikipedia.org/wiki?curid=38050347", "title": "Network scheduler", "text": "Network scheduler\n\nA network scheduler, also called packet scheduler, queueing discipline, qdisc or queueing algorithm, is an arbiter on a node in packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.\n\nThe network scheduler logic decides which network packet to forward next. The network scheduler is associated with a queuing system, storing the network packets temporarily until they are transmitted. Systems may have a single or multiple queues in which case each may hold the packets of one flow, classification, or priority.\n\nIn some cases it may not be possible to schedule all transmissions within the constraints of the system. In these cases the network scheduler is responsible for deciding which traffic to forward and what gets dropped.\n\nIn the course of time many network queueing disciplines have been developed. Each of these provides specific reordering or dropping of network packets inside various transmit or receive buffers.\nQueuing disciplines are commonly used as attempts to compensate for various networking conditions, like reducing the latency for certain classes of network packets, and are generally used as part of quality of service (QoS) measures.\n\nExamples of algorithms suitable for managing network traffic include:\nSeveral of the above have been implemented as Linux kernel modules and are freely available.\n\nBufferbloat is a phenomenon in packet-switched networks in which excess buffering of packets causes high latency and packet delay variation. Bufferbloat can be addressed by a network scheduler that strategically discards packets to avoid an unnecessarily high buffering backlog. Examples include CoDel and Random early detection.\n\nA network scheduler may have responsibility in implementation of specific network traffic control initiatives. Network traffic control is an umbrella term for all measures aimed at reducing congest, latency and packet loss. Specifically, active queue management (AQM) is the selective dropping of queued network packets to achieve the larger goal of preventing excessive network congestion. The scheduler must choose which packets to drop. Traffic shaping smooths the bandwidth requirements of traffic flows by delaying transmission packets when they are queued in bursts. The scheduler decides the timing for the transmitted packets. Quality of service is the prioritization of traffic based on service class (Differentiated services) or reserved connection (Integrated services).\n\nThe Linux kernel packet scheduler is an integral part of the Linux kernel's network stack and manages the transmit and receive ring buffers of all NICs, by working on the layer 2 of the OSI model and handling Ethernet frames, for example. \n\nThe packet scheduler is configured using the utility called codice_1 (short for \"traffic control\"). As the default queuing discipline, the packet scheduler uses a FIFO implementation called \"pfifo_fast\", although systemd since its version 217 changes the default queuing discipline to fq_codel. \n\nThe codice_2 and codice_3 utilities enable system administrators to configure the buffer sizes codice_4 and codice_5 for each device separately in terms of number of Ethernet frames regardless of their size. The Linux kernel's network stack contains several other buffers, which are not managed by the network scheduler.\n\nBerkeley Packet Filter filters can be attached to the packet scheduler's classifiers. The eBPF functionality brought by version 4.1 of the Linux kernel in 2015 extends the classic BPF programmable classifiers to eBPF. These can be compiled using the LLVM eBPF backend and loaded into a running kernel using the codice_1 utility.\n\nALTQ is the implementation of a network scheduler for BSDs. As of OpenBSD version 5.5 ALTQ was replaced by the HFSC scheduler. \n\n"}
{"id": "32742753", "url": "https://en.wikipedia.org/wiki?curid=32742753", "title": "Ordinary differential equation", "text": "Ordinary differential equation\n\nIn mathematics, an ordinary differential equation (ODE) is a differential equation containing one or more functions of one independent variable and its derivatives. The term \"ordinary\" is used in contrast with the term partial differential equation which may be with respect to \"more than\" one independent variable.\n\nA linear differential equation is a differential equation that is defined by a linear polynomial in the unknown function and its derivatives, that is an equation of the form \nwhere , ..., and are arbitrary differentiable functions that do not need to be linear, and are the successive derivatives of the unknown function of the variable .\n\nAmong ordinary differential equations, linear differential equations play a prominent role for several reasons. Most elementary and special functions that are encountered in physics and applied mathematics are solutions of linear differential equations (see Holonomic function). When physical phenomena are modeled with non-linear equations, they are generally approximated by linear differential equations for an easier solution. The few non-linear ODEs that can be solved explicitly are generally solved by transforming the equation into an equivalent linear ODE (see, for example Riccati equation).\n\nSome ODEs may be solved explicitly in terms of known functions and integrals. When it is not possible, one may often use the equation for computing the Taylor series of the solutions. For applied problems, one generally use numerical methods for ordinary differential equations for getting an approximation of the desired solution.\n\nOrdinary differential equations (ODEs) arise in many contexts of mathematics and science (social as well as natural). Mathematical descriptions of change use differentials and derivatives. Various differentials, derivatives, and functions become related to each other via equations, and thus a differential equation is a result that describes dynamically changing phenomena, evolution, and variation. Often, quantities are defined as the rate of change of other quantities (for example, derivatives of displacement with respect to time), or gradients of quantities, which is how they enter differential equations.\n\nSpecific mathematical fields include geometry and analytical mechanics. Scientific fields include much of physics and astronomy (celestial mechanics), meteorology (weather modelling), chemistry (reaction rates), biology (infectious diseases, genetic variation), ecology and population modelling (population competition), economics (stock trends, interest rates and the market equilibrium price changes).\n\nMany mathematicians have studied differential equations and contributed to the field, including Newton, Leibniz, the Bernoulli family, Riccati, Clairaut, d'Alembert, and Euler.\n\nA simple example is Newton's second law of motion — the relationship between the displacement \"x\" and the time \"t\" of an object under the force \"F\", is given by the differential equation\n\nwhich constrains the motion of a particle of constant mass \"m\". In general, \"F\" is a function of the position \"x\"(\"t\") of the particle at time \"t\". The unknown function \"x\"(\"t\") appears on both sides of the differential equation, and is indicated in the notation \"F\"(\"x\"(\"t\")).\n\nIn what follows, let \"y\" be a dependent variable and \"x\" an independent variable, and \"y\" = \"f\"(\"x\") is an unknown function of \"x\". The notation for differentiation varies depending upon the author and upon which notation is most useful for the task at hand. In this context, the Leibniz's notation (\"dy\"/\"dx\",\"d\"\"y\"/\"dx\"...,\"d\"\"y\"/\"dx\") is more useful for differentiation and integration, whereas Lagrange's notation (\"y′\",\"y′′\", ..., \"y\") is more useful for representing derivatives of any order compactly, and Newton's notation formula_3 is often used in physics for representing derivatives of low order with respect to time.\n\nGiven \"F\", a function of \"x\", \"y\", and derivatives of \"y\". Then an equation of the form\n\nis called an explicit \"ordinary differential equation\" of \"order\" \"n\".\n\nMore generally, an \"implicit\" ordinary differential equation of order \"n\" takes the form:\n\nThere are further classifications:\n\nA number of coupled differential equations form a system of equations. If y is a vector whose elements are functions; y(\"x\") = [\"y\"(\"x\"), \"y\"(\"x\")..., \"y\"(\"x\")], and F is a vector-valued function of y and its derivatives, then\n\nis an \"explicit system of ordinary differential equations\" of \"order\" \"n\" and \"dimension\" \"m\". In column vector form:\n\nThese are not necessarily linear. The \"implicit\" analogue is:\n\nwhere 0 = (0, 0... 0) is the zero vector. In matrix form\n\nFor a system of the form formula_10, some sources also require that the Jacobian matrix formula_11 be non-singular in order to call this an implicit ODE [system]; an implicit ODE system satisfying this Jacobian non-singularity condition can be transformed into an explicit ODE system. In the same sources, implicit ODE systems with a singular Jacobian are termed differential algebraic equations (DAEs). This distinction is not merely one of terminology; DAEs have fundamentally different characteristics and are generally more involved to solve than (nonsingular) ODE systems. Presumably for additional derivatives, the Hessian matrix and so forth are also assumed non-singular according to this scheme, although note that any ODE of order greater than one can be [and usually is] rewritten as system of ODEs of first order, which makes the Jacobian singularity criterion sufficient for this taxonomy to be comprehensive at all orders.\n\nThe behavior of a system of ODEs can be visualized through the use of a phase portrait.\n\nGiven a differential equation\na function is called a \"solution\" or integral curve for \"F\", if \"u\" is \"n\"-times differentiable on \"I\", and\n\nGiven two solutions and , \"u\" is called an \"extension\" of \"v\" if and\nA solution that has no extension is called a \"maximal solution\". A solution defined on all of R is called a \"global solution\".\n\nA \"general solution\" of an \"n\"th-order equation is a solution containing \"n\" arbitrary independent constants of integration. A \"particular solution\" is derived from the general solution by setting the constants to particular values, often chosen to fulfill set 'initial conditions or boundary conditions'. A singular solution is a solution that cannot be obtained by assigning definite values to the arbitrary constants in the general solution.\n\nThe theory of singular solutions of ordinary and partial differential equations was a subject of research from the time of Leibniz, but only since the middle of the nineteenth century did it receive special attention. A valuable but little-known work on the subject is that of Houtain (1854). Darboux (starting in 1873) was a leader in the theory, and in the geometric interpretation of these solutions he opened a field worked by various\nwriters, notable ones being Casorati and Cayley. To the latter is due (1872) the theory of singular solutions of differential equations of the first order as accepted circa 1900.\n\nThe primitive attempt in dealing with differential equations had in view a reduction to quadratures. As it had been the hope of eighteenth-century algebraists to find a method for solving the general equation of the \"n\"th degree, so it was the hope of analysts to find a general method for integrating any differential equation. Gauss (1799) showed, however, that the differential equation meets its limitations very soon unless complex numbers are introduced. Hence, analysts began to substitute the study of functions, thus opening a new and fertile field. Cauchy was the first to appreciate the importance of this view. Thereafter, the real question was to be not whether a solution is possible by means of known functions or their integrals but whether a given differential equation suffices for the definition of a function of the independent variable or variables, and, if so, what are the characteristic properties of this function.\n\nTwo memoirs by Fuchs (\"Crelle\", 1866, 1868), inspired a novel approach, subsequently elaborated by Thomé and Frobenius. Collet was a prominent contributor beginning in 1869, although his method for integrating a non-linear system was communicated to Bertrand in 1868. Clebsch (1873) attacked the theory along lines parallel to those followed in his theory of Abelian integrals. As the latter can be classified according to the properties of the fundamental curve that remains unchanged under a rational transformation, so Clebsch proposed to classify the transcendent functions defined by the differential equations according to the invariant properties of the corresponding surfaces \"f\" = 0 under rational one-to-one transformations.\n\nFrom 1870, Sophus Lie's work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called Lie groups, be referred to a common source, and that ordinary differential equations that admit the same infinitesimal transformations present comparable difficulties of integration. He also emphasized the subject of transformations of contact.\n\nLie's group theory of differential equations has been certified, namely: (1) that it unifies the many ad hoc methods known for solving differential equations, and (2) that it provides powerful new ways to find solutions. The theory has applications to both ordinary and partial differential equations.\n\nA general approach to solve DEs uses the symmetry property of differential equations, the continuous infinitesimal transformations of solutions to solutions (Lie theory). Continuous group theory, Lie algebras, and differential geometry are used to understand the structure of linear and nonlinear (partial) differential equations for generating integrable equations, to find its Lax pairs, recursion operators, Bäcklund transform, and finally finding exact analytic solutions to the DE.\n\nSymmetry methods have been recognized to study differential equations, arising in mathematics, physics, engineering, and many other disciplines.\n\nSturm–Liouville theory is a theory of a special type of second order linear ordinary differential equations. Their solutions are based on eigenvalues and corresponding eigenfunctions of linear operators defined in terms of second-order homogeneous linear equations. The problems are identified as Sturm-Liouville Problems (SLP) and are named after J.C.F. Sturm and J. Liouville, who studied such problems in the mid-1800s. The interesting fact about regular SLPs is that they have an infinite number of eigenvalues, and the corresponding eigenfunctions form a complete, orthogonal set, which makes orthogonal expansions possible. This is a key idea in applied mathematics, physics, and engineering. SLPs are also useful in the analysis of certain partial differential equations.\n\nThere are several theorems that establish existence and uniqueness of solutions to initial value problems involving ODEs both locally and globally. The two main theorems are\n\nIn their basic form both of these theorems only guarantee local results, though the latter can be extended to give a global result, for example, if the conditions of Grönwall's inequality are met.\n\nAlso, uniqueness theorems like the Lipschitz one above do not apply to DAE systems, which may have multiple solutions stemming from their (non-linear) algebraic part alone.\n\nThe theorem can be stated simply as follows. For the equation and initial value problem:\n\nif \"F\" and ∂\"F\"/∂\"y\" are continuous in a closed rectangle\n\nin the \"x-y\" plane, where \"a\" and \"b\" are real (symbolically: \"a, b\" ∈ ℝ) and × denotes the cartesian product, square brackets denote closed intervals, then there is an interval\n\nfor some \"h\" ∈ ℝ where \"the\" solution to the above equation and initial value problem can be found. That is, there is a solution and it is unique. Since there is no restriction on \"F\" to be linear, this applies to non-linear equations that take the form \"F\"(\"x, y\"), and it can also be applied to systems of equations.\n\nWhen the hypotheses of the Picard–Lindelöf theorem are satisfied, then local existence and uniqueness can be extended to a global result. More precisely:\n\nFor each initial condition (\"x\", \"y\") there exists a unique maximum (possibly infinite) open interval\n\nsuch that any solution that satisfies this initial condition is a restriction of the solution that satisfies this initial condition with domain formula_19.\n\nIn the case that formula_20, there are exactly two possibilities\n\n\nwhere Ω is the open set in which \"F\" is defined, and formula_23 is its boundary.\n\nNote that the maximum domain of the solution\n\n\n\nThis means that \"F\"(\"x, y\") = \"y\", which is \"C\" and therefore locally Lipschitz continuous, satisfying the Picard–Lindelöf theorem.\n\nEven in such a simple setting, the maximum domain of solution cannot be all formula_24 since the solution is\n\nwhich has maximum domain:\n\nThis shows clearly that the maximum interval may depend on the initial conditions. The domain of \"y\" could be taken as being formula_29 but this would lead to a domain that is not an interval, so that the side opposite to the initial condition would be disconnected from the initial condition, and therefore not uniquely determined by it.\n\nThe maximum domain is not formula_24 because\n\nwhich is one of the two possible cases according to the above theorem.\n\nDifferential equations can usually be solved more easily if the order of the equation can be reduced.\n\nAny explicit differential equation of order \"n\",\n\ncan be written as a system of \"n\" first-order differential equations by defining a new family of unknown functions\n\nfor \"i\" = 1, 2..., \"n\". The \"n\"-dimensional system of first-order coupled differential equations is then\n\nmore compactly in vector notation:\n\nwhere\n\nSome differential equations have solutions that can be written in an exact and closed form. Several important classes are given here.\n\nIn the table below, \"P\"(\"x\"), \"Q\"(\"x\"), \"P\"(\"y\"), \"Q\"(\"y\"), and \"M\"(\"x\",\"y\"), \"N\"(\"x\",\"y\") are any integrable functions of \"x\", \"y\", and \"b\" and \"c\" are real given constants, and \"C\", \"C\"... are arbitrary constants (complex in general). The differential equations are in their equivalent and alternative forms that lead to the solution through integration.\n\nIn the integral solutions, λ and ε are dummy variables of integration (the continuum analogues of indices in summation), and the notation ∫\"F\"(\"λ\") \"dλ\" just means to integrate \"F\"(\"λ\") with respect to \"λ\", then \"after\" the integration substitute \"λ\" = \"x\", without adding constants (explicitly stated).\n\n\n\n\n\n"}
{"id": "4252320", "url": "https://en.wikipedia.org/wiki?curid=4252320", "title": "Overflow flag", "text": "Overflow flag\n\nIn computer processors, the overflow flag (sometime called V flag) is usually a single bit in a system status register used to indicate when an arithmetic overflow has occurred in an operation, indicating that the signed two's-complement result would not fit in the number of bits used for the operation (the ALU width). Some architectures may be configured to automatically generate an exception on an operation resulting in overflow.\nAn illustrative example is what happens if we add 127 and 127 using 8-bit registers. 127+127 is 254, but using 8-bit arithmetic the result would be 1111 1110 binary, which is -2 in two's complement, and thus negative. A negative result out of positive operands (or vice versa) is an overflow. The overflow flag would then be set so the program can be aware of the problem and mitigate this or signal an error. The overflow flag is thus set when the most significant bit (here considered the sign bit) is changed by adding two numbers with the same sign (or subtracting two numbers with opposite signs). Overflow never occurs when the sign of two addition operands are different (or the sign of two subtraction operands are the same).\n\nInternally, the overflow flag is usually generated by an exclusive or of the internal carry \"into\" and \"out of\" the sign bit. As the sign bit is the same as the most significant bit of a number \"considered\" unsigned, the overflow flag is \"meaningless\" and normally ignored when unsigned numbers are added or subtracted.\n\nThe overflow flag is typically changed by all arithmetic operations, including compare instructions (equivalent to a subtract instruction without storing the result). In many processor architectures, the overflow flag is cleared by bitwise operations (and, or, xor, not), possibly including shifts and rotates, but it may also be left undefined by these. Instructions such as multiply and divide often leave the flag undefined, or affected by the last partial result.\n\nOn many processors (not only x86), addition and subtraction instructions affect both the carry/borrow and overflow flags, though only one of them will normally be of interest, depending on whether the operands represented signed or unsigned numbers.\n\n\n"}
{"id": "53707452", "url": "https://en.wikipedia.org/wiki?curid=53707452", "title": "P-variation", "text": "P-variation\n\nIn mathematical analysis, p-variation is a collection of seminorms on functions from an ordered set to a metric space, indexed by a real number formula_1. \"p\"-variation is a measure of the regularity or smoothness of a function. Specifically, if formula_2, where formula_3 is a metric space and \"I\" a totally ordered set, its \"p\"-variation is\n\nwhere \"D\" ranges over all finite partitions of the interval \"I\".\n\nThe \"p\" variation of a function decreases with \"p\". If \"f\" has finite \"p\"-variation and \"g\" is an \"α\"-Hölder continuous function, then formula_5 has finite formula_6-variation.\n\nThe case when \"p\" is one is called total variation, and functions with a finite 1-variation are called bounded variation functions.\n\nOne can interpret the \"p\"-variation as a parameter-independent version of the Hölder norm, which also extends to discontinuous functions. If \"f\" is \"α\"–Hölder continuous (i.e. its α–Hölder norm is finite) then its formula_7-variation is finite. Specifically, on an interval [\"a\",\"b\"], formula_8. Conversely, if \"f\" is continuous and has finite \"p-\"variation, there exists a reparameterisation, formula_9, such that formula_10 is formula_11Hölder continuous.\n\nIf \"p\" is less than \"q\" then the space of functions of finite \"p\"-variation on a compact set is continuously embedded with norm 1 into those of finite \"q\"-variation. I.e. \nformula_12. However unlike the analogous situation with Hölder spaces the embedding is not compact. \nFor example, consider the real functions on [0,1] given by formula_13. They are uniformly bounded in 1-variation and converge pointwise to a discontinuous function \"f\" but this not only is not a convergence in \"p\"-variation for any \"p\" but also is not uniform convergence.\n\nIf \"f\" and \"g\" are functions from  [\"a\", \"b\"] to ℝ with no common discontinuities and with \"f\" having finite \"p\"-variation and \"g\" having finite \"q\"-variation, with formula_14 then the Riemann–Stieltjes Integral\nis well-defined. This integral is known as the \"Young integral\" because it comes from . The value of this definite integral is bounded by the Young-Loève estimate as follows \n\nwhere \"C\" is a constant which only depends on \"p\" and \"q\" and ξ is any number between \"a\" and \"b\".\nIf \"f\" and \"g\" are continuous, the indefinite integral formula_17 is a continuous function with finite \"q\"-variation: If \"a\" ≤ \"s\" ≤ \"t\" ≤ \"b\" then formula_18, its \"q\"-variation on [\"s\",\"t\"], is bounded by \nformula_19\nwhere \"C\" is a constant which only depends on \"p\" and \"q\".\n\nA function from ℝ to \"e\" × \"d\" real matrices is called an ℝ-valued one-form on ℝ.\n\nIf \"f\" is a Lipschitz continuous ℝ-valued one-form on ℝ, and \"X\" is a continuous function from the interval [\"a\", \"b\"] to ℝ with finite \"p\"-variation with \"p\" less than 2, then the integral of \"f\" on \"X\", formula_20, can be calculated because each component of \"f\"(\"X\"(\"t\")) will be a path of finite \"p\"-variation and the integral is a sum of finitely many Young integrals. It provides the solution to the equation formula_21 driven by the path \"X\".\n\nMore significantly, if \"f\" is a Lipschitz continuous ℝ-valued one-form on ℝ, and \"X\" is a continuous function from the interval [\"a\", \"b\"] to ℝ with finite \"p\"-variation with \"p\" less than 2, then Young integration is enough to establish the solution of the equation formula_22 driven by the path \"X\".\n\nThe theory of rough paths generalises the Young integral and Young differential equations and makes heavy use of the concept of \"p\"-variation.\n\n\"p\"-variation should be contrasted with the quadratic variation which is used in stochastic analysis, where it takes one stochastic process to another. Quadratic variation is defined as a limit as the partition gets finer, whereas \"p\"-variation is a supremum over all partitions. Thus the quadratic variation of a process could be smaller than its 2-variation. If \"W\" is a standard Brownian motion on [0, \"T\"] then with probability one its \"p\"-variation is infinite for formula_23 and finite otherwise. The quadratic variation of \"W\" is formula_24.\n\nFor a discrete time series of observations \"X...,X\" it is straightforward to compute its \"p\"-variation with complexity of \"O\"(\"N\"). Here is an example C++ code using dynamic programming:\n\nThere exist much more efficient, but also more complicated, algorithms for ℝ-valued processes\nand for processes in arbitrary metric spaces.\n\n\n"}
{"id": "355100", "url": "https://en.wikipedia.org/wiki?curid=355100", "title": "Quiver (mathematics)", "text": "Quiver (mathematics)\n\nIn mathematics, a quiver is a directed graph where loops and multiple arrows between two vertices are allowed, i.e. a multidigraph. They are commonly used in representation theory: a representation \"V\" of a quiver assigns a vector space \"V\"(\"x\") to each vertex \"x\" of the quiver and a linear map \"V\"(\"a\") to each arrow \"a\".\n\nIn category theory, a quiver can be understood to be the underlying structure of a category, but without composition or a designation of identity morphisms. That is, there is a forgetful functor from Cat to Quiv. Its left adjoint is a free functor which, from a quiver, makes the corresponding free category.\n\nA quiver Γ consists of:\n\nThis definition is identical to that of a multidigraph.\n\nA morphism of quivers is defined as follows. If formula_1 and formula_2 are two quivers, then a morphism formula_3 of quivers consist of two functions formula_4 and formula_5 such that following diagrams commute: \nand\n\nThe above definition is based in set theory; the category-theoretic definition generalizes this into a functor from the \"free quiver\" to the category of sets.\n\nThe free quiver (also called the walking quiver, Kronecker quiver, 2-Kronecker quiver or Kronecker category) \"Q\" is a category with two objects, and four morphisms: The objects are \"V\" and \"E\". The four morphisms are \"s\": \"E\" → \"V\", \"t\": \"E\" → \"V\", and the identity morphisms id: \"V\" → \"V\" and id: \"E\" → \"E\". That is, the free quiver is\n\nA quiver is then a functor Γ: \"Q\" → Set.\n\nMore generally, a quiver in a category \"C\" is a functor Γ: \"Q\" → \"C\". The category Quiv(\"C\") of quivers in \"C\" is the functor category where:\n\n\nNote that Quiv is the category of presheaves on the opposite category \"Q\".\n\nIf Γ is a quiver, then a path in Γ is a sequence of arrows \"a\" \"a\" ... \"a\" \"a\" \"a\" such that the head of \"a\" = tail of \"a\", using the convention of concatenating paths from right to left.\n\nIf \"K\" is a field then the quiver algebra or path algebra \"K\"Γ is defined as a vector space having all the paths (of length ≥ 0) in the quiver as basis (including, for each vertex \"i\" of the quiver Γ, a \"trivial path\" formula_9 of length 0; these paths are \"not\" assumed to be equal for different \"i\"), and multiplication given by concatenation of paths. If two paths cannot be concatenated because the end vertex of the first is not equal to the starting vertex of the second, their product is defined to be zero. This defines an associative algebra over \"K\". This algebra has a unit element if and only if the quiver has only finitely many vertices. In this case, the modules over \"K\"Γ are naturally identified with the representations of Γ. If the quiver has infinitely many vertices, then \"K\"Γ has an approximate identity given by formula_10 where \"E\" ranges over finite subsets of the vertex set of Γ.\n\nIf the quiver has finitely many vertices and arrows, and the end vertex and starting vertex of any path are always distinct (i.e. \"Q\" has no oriented cycles), then \"K\"Γ is a finite-dimensional hereditary algebra over \"K\". Conversely, if \"K\" is algebraically closed, then any finite-dimensional, hereditary, associative algebra over \"K\" is Morita equivalent to the path algebra of its Ext quiver (i.e., they have equivalent module categories).\n\nA representation of a quiver \"Q\" is an association of an \"R\"-module to each vertex of \"Q\", and a morphism between each module for each arrow.\n\nA representation \"V\" of a quiver \"Q\" is said to be \"trivial\" if \"V\"(\"x\") = 0 for all vertices \"x\" in \"Q\".\n\nA \"morphism\", \"f\": \"V\" → \"V′\", between representations of the quiver \"Q\", is a collection of linear maps formula_11 such that for every arrow \"a\" in \"Q\" from \"x\" to \"y\" formula_12, i.e. the squares that \"f\" forms with the arrows of \"V\" and \"V′\" all commute. A morphism, \"f\", is an \"isomorphism\", if \"f\"(\"x\") is invertible for all vertices \"x\" in the quiver. With these definitions the representations of a quiver form a category.\n\nIf \"V\" and \"W\" are representations of a quiver \"Q\", then the direct sum of these representations, formula_13, is defined by formula_14 for all vertices \"x\" in \"Q\" and formula_15 is the direct sum of the linear mappings \"V\"(\"a\") and \"W\"(\"a\").\n\nA representation is said to be \"decomposable\" if it is isomorphic to the direct sum of non-zero representations.\n\nA categorical definition of a quiver representation can also be given. The quiver itself can be considered a category, where the vertices are objects and paths are morphisms. Then a representation of \"Q\" is just a covariant functor from this category to the category of finite dimensional vector spaces. Morphisms of representations of \"Q\" are precisely natural transformations between the corresponding functors.\n\nFor a finite quiver Γ (a quiver with finitely many vertices and edges), let \"K\"Γ be its path algebra. Let \"e\" denote the trivial path at vertex \"i\". Then we can associate to the vertex \"i\" the projective \"K\"Γ-module \"K\"Γ\"e\" consisting of linear combinations of paths which have starting vertex \"i\". This corresponds to the representation of Γ obtained by putting a copy of \"K\" at each vertex which lies on a path starting at \"i\" and 0 on each other vertex. To each edge joining two copies of \"K\" we associate the identity map.\n\nTo enforce commutativity of some squares inside a quiver a generalization is the notion of quivers with relations (also named bound quivers).\nA relation on a quiver formula_16 is a formula_17 linear combination of paths from formula_16.\nA quiver with relation is a pair formula_19 with formula_16 a quiver and formula_21 an\nideal of the path algebra. The quotient formula_22 is the path algebra of formula_19.\n\nGiven the dimensions of the vector spaces assigned to every vertex, one can form a variety which characterizes all representations of that quiver with those specified dimensions, and consider stability conditions. These give quiver varieties, as constructed by .\n\nA quiver is of finite type if it has only finitely many isomorphism classes of indecomposable representations. classified all quivers of finite type, and also their indecomposable representations. More precisely, Gabriel's theorem states that:\n\n\n\n"}
{"id": "18584624", "url": "https://en.wikipedia.org/wiki?curid=18584624", "title": "Redundant binary representation", "text": "Redundant binary representation\n\nA redundant binary representation (RBR) is a numeral system that uses more bits than needed to represent a single binary digit so that most numbers have several representations. An RBR is unlike usual binary numeral systems, including two's complement, which use a single bit for each digit. Many of an RBR's properties differ from those of regular binary representation systems. Most importantly, an RBR allows addition without using a typical carry. When compared to non-redundant representation, an RBR makes bitwise logical operation slower, but arithmetic operations are faster when a greater bit width is used. Usually, each digit has its own sign that is not necessarily the same as the sign of the number represented. When digits have signs, that RBR is also a signed-digit representation.\n\nAn RBR is a place-value notation system. In an RBR, digits are \"pairs\" of bits, that is, for every place, an RBR uses a pair of bits. The value represented by a redundant digit can be found using a translation table. This table indicates the mathematical value of each possible pair of bits.\n\nAs in conventional binary representation, the integer value of a given representation is a weighted sum of the values of the digits. The weight starts at 1 for the rightmost position and goes up by a factor of 2 for each next position. Usually, an RBR allows negative values. There is no single sign bit that tells if a redundantly represented number is positive or negative. Most integers have several possible representations in an RBR.\n\nOften one of the several possible representations of an integer is chosen as the \"canonical\" form, so each integer has only one possible \"canonical\" representation; non-adjacent form and two's complement are popular choices for that canonical form.\n\nAn integer value can be converted back from an RBR using the following formula, where \"n\" is the number of digit and \"d\" is the interpreted value of the \"k\"-th digit, where \"k\" starts at 0 at the rightmost position:\n\nThe conversion from an RBR to \"n\"-bit two's complement can be done in O(log(\"n\")) time using a prefix adder.\n\nNot all redundant representations have the same properties. For example, using the translation table on the right, the number 1 can be represented in this RBR in many ways: \"01·01·01·11\" (0+0+0+1), \"01·01·10·11\" (0+0+0+1), \"01·01·11·00\" (0+0+2−1), or \"11·00·00·00\" (8−4−2−1). Also, for this translation table, flipping all bits (NOT gate) corresponds to finding the additive inverse (multiplication by −1) of the integer represented.\n\nIn this case: formula_2\n\nRedundant representations are commonly used inside high-speed arithmetic logic units.\n\nIn particular, a carry-save adder uses a redundant representation.\n\nThe addition operation in all RBRs is carry-free, which means that the carry does not have to propagate through the full width of the addition unit. In effect, the addition in all RBRs is a constant-time operation. The addition will always take the same amount of time independently of the bit-width of the operands. This does not imply that the addition is always faster in an RBR than its two's complement equivalent, but that the addition will eventually be faster in an RBR with increasing bit width because the two's complement addition unit's delay is proportional to log(\"n\") (where \"n\" is the bit width). Addition in an RBR takes a constant time because each digit of the result can be calculated independently of one another, implying that each digit of the result can be calculated in parallel.\n\nSubtraction is the same as the addition except that the additive inverse of the second operand needs to be computed first. For common representations, this can be done on a digit-by-digit basis.\n\nBitwise logical operations, such as AND, OR and XOR, are not possible in redundant representations. While it is possible to do bitwise operations directly on the underlying bits inside an RBR, it is not clear that this is a meaningful operation; there are many ways to represent a value in an RBR, and the value of the result would depend on the representation used.\n\nTo get the expected results, it is necessary to convert the two operands first to non-redundant representations. Consequently, logical operations are slower in an RBR. More precisely, they take a time proportional to log(\"n\") (where \"n\" is the number of digit) compared to a constant-time in two's complement.\n\nIt is, however, possible to \"partially\" convert only the least-significant portion of a redundantly represented number to non-redundant form. This allows operations such as masking off the low \"k\" bits can be done in log(\"k\") time.\n"}
{"id": "14968", "url": "https://en.wikipedia.org/wiki?curid=14968", "title": "Regular icosahedron", "text": "Regular icosahedron\n\nIn geometry, a regular icosahedron ( or ) is a convex polyhedron with 20 faces, 30 edges and 12 vertices. It is one of the five Platonic solids, and the one with the most sides.\n\nIt has five equilateral triangular faces meeting at each vertex. It is represented by its Schläfli symbol {3,5}, or sometimes by its vertex figure as 3.3.3.3.3 or 3. It is the dual of the dodecahedron, which is represented by {5,3}, having three pentagonal faces around each vertex.\n\nA regular icosahedron is a gyroelongated pentagonal bipyramid and a biaugmented pentagonal antiprism in any of six orientations.\n\nThe name comes . The plural can be either \"icosahedrons\" or \"icosahedra\" ().\n\nIf the edge length of a regular icosahedron is \"a\", the radius of a circumscribed sphere (one that touches the icosahedron at all vertices) is\n\nand the radius of an inscribed sphere (tangent to each of the icosahedron's faces) is\n\nwhile the midradius, which touches the middle of each edge, is\n\nwhere \"ϕ\" is the golden ratio.\n\nThe surface area \"A\" and the volume \"V\" of a regular icosahedron of edge length \"a\" are:\n\nThe latter is \"F\" = \"20\" times the volume of a general tetrahedron with apex at the center of the\ninscribed sphere, where the volume of the tetrahedron is one third times the base area times its height \"r\".\n\nThe volume filling factor of the circumscribed sphere is:\n\nThe vertices of an icosahedron centered at the origin with an edge-length of 2 and a circumradius of formula_7 are described by circular permutations of:\nwhere \"ϕ\" =  is the golden ratio.\n\nTaking all permutations (not just cyclic ones) results in the Compound of two icosahedra.\n\nNote that these vertices form five sets of three concentric, mutually orthogonal golden rectangles, whose edges form Borromean rings.\n\nIf the original icosahedron has edge length 1, its dual dodecahedron has edge length = = \"ϕ\" − 1.\nThe 12 edges of a regular octahedron can be subdivided in the golden ratio so that the resulting vertices define a regular icosahedron. This is done by first placing vectors along the octahedron's edges such that each face is bounded by a cycle, then similarly subdividing each edge into the golden mean along the direction of its vector. The five octahedra defining any given icosahedron form a regular polyhedral compound, while the two icosahedra that can be defined in this way from any given octahedron form a uniform polyhedron compound.\nThe locations of the vertices of a regular icosahedron can be described using spherical coordinates, for instance as latitude and longitude. If two vertices are taken to be at the north and south poles (latitude ±90°), then the other ten vertices are at latitude ±arctan() ≈ ±26.57°. These ten vertices are at evenly spaced longitudes (36° apart), alternating between north and south latitudes.\n\nThis scheme takes advantage of the fact that the regular icosahedron is a pentagonal gyroelongated bipyramid, with D dihedral symmetry—that is, it is formed of two congruent pentagonal pyramids joined by a pentagonal antiprism.\n\nThe icosahedron has three special orthogonal projections, centered on a face, an edge and a vertex:\nThe icosahedron can also be represented as a spherical tiling, and projected onto the plane via a stereographic projection. This projection is conformal, preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.\n\nThe following construction of the icosahedron avoids tedious computations in the number field [] necessary in more elementary approaches.\n\nThe existence of the icosahedron amounts to the existence of six equiangular lines in . Indeed, intersecting such a system of equiangular lines with a Euclidean sphere centered at their common intersection yields the twelve vertices of a regular icosahedron as can easily be checked. Conversely, supposing the existence of a regular icosahedron, lines defined by its six pairs of opposite vertices form an equiangular system.\n\nIn order to construct such an equiangular system, we start with this 6 × 6 square matrix:\n\nA straightforward computation yields (where \"\" is the 6 × 6 identity matrix). This implies that \"A\" has eigenvalues – and , both with multiplicity 3 since \"A\" is symmetric and of trace zero.\n\nThe matrix induces thus a Euclidean structure on the quotient space , which is isomorphic to since the kernel of has dimension 3. The image under the projection of the six coordinate axes \"v\", …, \"v\" in forms thus a system of six equiangular lines in intersecting pairwise at a common acute angle of arccos . Orthogonal projection of ±\"v\", …, ±\"v\" onto the -eigenspace of \"A\" yields thus the twelve vertices of the icosahedron.\n\nA second straightforward construction of the icosahedron uses representation theory of the alternating group \"A\" acting by direct isometries on the icosahedron.\n\nThe rotational symmetry group of the regular icosahedron is isomorphic to the alternating group on five letters. This non-abelian simple group is the only non-trivial normal subgroup of the symmetric group on five letters. Since the Galois group of the general quintic equation is isomorphic to the symmetric group on five letters, and this normal subgroup is simple and non-abelian, the general quintic equation does not have a solution in radicals. The proof of the Abel–Ruffini theorem uses this simple fact, and Felix Klein wrote a book that made use of the theory of icosahedral symmetries to derive an analytical solution to the general quintic equation, . See icosahedral symmetry: related geometries for further history, and related symmetries on seven and eleven letters.\n\nThe full symmetry group of the icosahedron (including reflections) is known as the full icosahedral group, and is isomorphic to the product of the rotational symmetry group and the group \"C\" of size two, which is generated by the reflection through the center of the icosahedron.\n\nThe icosahedron has a large number of stellations. According to specific rules defined in the book \"The Fifty-Nine Icosahedra\", 59 stellations were identified for the regular icosahedron. The first form is the icosahedron itself. One is a regular Kepler–Poinsot polyhedron. Three are regular compound polyhedra.\n\nThe small stellated dodecahedron, great dodecahedron, and great icosahedron are three facetings of the regular icosahedron. They share the same vertex arrangement. They all have 30 edges. The regular icosahedron and great dodecahedron share the same edge arrangement but differ in faces (triangles vs pentagons), as do the small stellated dodecahedron and great icosahedron (pentagrams vs triangles).\nThere are distortions of the icosahedron that, while no longer regular, are nevertheless vertex-uniform. These are invariant under the same rotations as the tetrahedron, and are somewhat analogous to the snub cube and snub dodecahedron, including some forms which are chiral and some with T-symmetry, i.e. have different planes of symmetry from the tetrahedron.\n\nThe icosahedron is unique among the Platonic solids in possessing a dihedral angle not less than 120°. Its dihedral angle is approximately 138.19°. Thus, just as hexagons have angles not less than 120° and cannot be used as the faces of a convex regular polyhedron because such a construction would not meet the requirement that at least three faces meet at a vertex and leave a positive defect for folding in three dimensions, icosahedra cannot be used as the cells of a convex regular polychoron because, similarly, at least three cells must meet at an edge and leave a positive defect for folding in four dimensions (in general for a convex polytope in \"n\" dimensions, at least three facets must meet at a peak and leave a positive defect for folding in \"n\"-space). However, when combined with suitable cells having smaller dihedral angles, icosahedra can be used as cells in semi-regular polychora (for example the snub 24-cell), just as hexagons can be used as faces in semi-regular polyhedra (for example the truncated icosahedron). Finally, non-convex polytopes do not carry the same strict requirements as convex polytopes, and icosahedra are indeed the cells of the icosahedral 120-cell, one of the ten non-convex regular polychora.\n\nAn icosahedron can also be called a gyroelongated pentagonal bipyramid. It can be decomposed into a gyroelongated pentagonal pyramid and a pentagonal pyramid or into a pentagonal antiprism and two equal pentagonal pyramids.\n\nIt can be projected to 3D from the 6D 6-demicube using the same basis vectors that form the hull of the Rhombic triacontahedron from the 6-cube. Shown here including the inner 20 vertices which are not connected by the 30 outer hull edges of 6D norm length . The inner vertices form an dodecahedron.\n\n<br>The 3D projection basis vectors [u,v,w] used are:\n\nThere are 3 uniform colorings of the icosahedron. These colorings can be represented as 11213, 11212, 11111, naming the 5 triangular faces around each vertex by their color.\n\nThe icosahedron can be considered a snub tetrahedron, as snubification of a regular tetrahedron gives a regular icosahedron having chiral tetrahedral symmetry. It can also be constructed as an alternated truncated octahedron, having pyritohedral symmetry. The pyritohedral symmetry version is sometimes called a pseudoicosahedron, and is dual to the pyritohedron.\n\nMany viruses, e.g. herpes virus, have icosahedral shells. Viral structures are built of repeated identical protein subunits known as capsomeres, and the icosahedron is the easiest shape to assemble using these subunits. A \"regular\" polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral genome.\n\nVarious bacterial organelles with an icosahedral shape were also found. The icosahedral shell encapsulating enzymes and labile intermediates are built of different types of proteins with BMC domains.\n\nIn 1904, Ernst Haeckel described a number of species of Radiolaria, including \"Circogonia icosahedra\", whose skeleton is shaped like a regular icosahedron. A copy of Haeckel's illustration for this radiolarian appears in the article on regular polyhedra.\n\nThe closo-carboranes are chemical compounds with shape very close to icosahedron. Icosahedral twinning also occurs in crystals, especially nanoparticles.\n\nMany borides and allotropes of boron contain boron B icosahedron as a basic structure unit.\n\nIcosahedral dice with twenty sides have been used since ancient times.\n\nIn several roleplaying games, such as \"Dungeons & Dragons\", the twenty-sided die (d20 for short) is commonly used in determining success or failure of an action. This die is in the form of a regular icosahedron. It may be numbered from \"0\" to \"9\" twice (in which form it usually serves as a ten-sided die, or d10), but most modern versions are labeled from \"1\" to \"20\". See d20 System.\n\nAn icosahedron is the three-dimensional game board for Icosagame, formerly known as the Ico Crystal Game.\n\nAn icosahedron is used in the board game \"Scattergories\" to choose a letter of the alphabet. Six letters are omitted (Q, U, V, X, Y, and Z).\n\nIn the \"Nintendo 64\" game \"\", the boss Miracle Matter is a regular icosahedron.\n\nInside a Magic 8-Ball, various answers to yes-no questions are inscribed on a regular icosahedron.\n\nR. Buckminster Fuller and Japanese cartographer Shoji Sadao designed a world map in the form of an unfolded icosahedron, called the Fuller projection, whose maximum distortion is only 2%. The American electronic music duo ODESZA use a regular icosahedron as their logo.\n\nThe skeleton of the icosahedron (the vertices and edges) forms a graph. It is one of 5 Platonic graphs, each a skeleton of its Platonic solid.\n\nThe high degree of symmetry of the polygon is replicated in the properties of this graph, which is distance-transitive and symmetric. The automorphism group has order 120. The vertices can be colored with 4 colors, the edges with 5 colors, and the diameter is 3.\n\nThe icosahedral graph is Hamiltonian: there is a cycle containing all the vertices. It is also a planar graph.\n\nThere are 4 related Johnson solids, including pentagonal faces with a subset of the 12 vertices. The similar dissected regular icosahedron has 2 adjacent vertices diminished, leaving two trapezoidal faces, and a bifastigium has 2 opposite sets of vertices removed and 4 trapezoidal faces. The pentagonal antiprism is formed by removing two opposite vertices.\n\nThe icosahedron can be transformed by a truncation sequence into its dual, the dodecahedron:\nAs a snub tetrahedron, and alternation of a truncated octahedron it also exists in the tetrahedral and octahedral symmetry families:\n\nThis polyhedron is topologically related as a part of sequence of regular polyhedra with Schläfli symbols {3,\"n\"}, continuing into the hyperbolic plane.\nThe regular icosahedron, seen as a \"snub tetrahedron\", is a member of a sequence of snubbed polyhedra and tilings with vertex figure (3.3.3.3.\"n\") and Coxeter–Dynkin diagram . These figures and their duals have (\"n\"32) rotational symmetry, being in the Euclidean plane for \"n\" = 6, and hyperbolic plane for any higher \"n\". The series can be considered to begin with \"n\" = 2, with one set of faces degenerated into digons.\n\nThe icosahedron can tessellate hyperbolic space in the order-3 icosahedral honeycomb, with 3 icosahedra around each edge, 12 icosahedra around each vertex, with Schläfli symbol {3,5,3}. It is one of four regular tessellations in the hyperbolic 3-space.\n\n"}
{"id": "42877569", "url": "https://en.wikipedia.org/wiki?curid=42877569", "title": "Relationship between mathematics and physics", "text": "Relationship between mathematics and physics\n\nThe relationship between mathematics and physics has been a subject of study of philosophers, mathematicians and physicists since Antiquity, and more recently also by historians and educators. Generally considered a relationship of great intimacy, mathematics has already been described as \"an essential tool for physics\" and physics has already been described as \"a rich source of inspiration and insight in mathematics\".\n\nIn his work \"Physics\", one of the topics treated by Aristotle is about how the study carried out by mathematicians differs from that carried out by physicists. Considerations about mathematics being the language of nature can be found in the ideas of the Pythagoreans: the convictions that \"Numbers rule the world\" and \"All is number\", and two millennia later were also expressed by Galileo Galilei: \"The book of nature is written in the language of mathematics\".\n\nBefore giving a mathematical proof for the formula for the volume of a sphere, Archimedes used physical reasoning to discover the solution (imagining the balancing of bodies on a scale). From the seventeenth century, many of the most important advances in mathematics appeared motivated by the study of physics, and this continued in the following centuries (although, it has already been appointed that from the nineteenth century, mathematics started to become increasingly independent from physics). The creation and development of calculus were strongly linked to the needs of physics: There was a need for a new mathematical language to deal with the new dynamics that had arisen from the work of scholars such as Galileo Galilei and Isaac Newton. During this period there was little distinction between physics and mathematics; as an example, Newton regarded geometry as a branch of mechanics. As time progressed, increasingly sophisticated mathematics started to be used in physics. The current situation is that the mathematical knowledge used in physics is becoming increasingly sophisticated, as in the case of superstring theory.\n\nSome of the problems considered in the philosophy of mathematics are the following:\n\n\nIn recent times the two disciplines have most often been taught separately, despite all the interrelations between physics and mathematics. This led some professional mathematicians who were also interested in mathematics education, such as Felix Klein, Richard Courant, Vladimir Arnold and Morris Kline, to strongly advocate teaching mathematics in a way more closely related to the physical sciences.\n\n\n\n"}
{"id": "25190127", "url": "https://en.wikipedia.org/wiki?curid=25190127", "title": "Reservoir sampling", "text": "Reservoir sampling\n\nReservoir sampling is a family of randomized algorithms for randomly choosing a sample of formula_1 items from a list formula_2 containing formula_3 items, where formula_3 is either a very large or unknown number. Typically, formula_3 is too large to fit the whole list into main memory.\n\nSuppose we see a sequence of items, one at a time. We want to keep ten items in memory, and we want them to be selected at random from the sequence. If we know the total number of items formula_3, then the solution is easy: select 10 distinct indices formula_7 between 1 and formula_3 with equal probability, and keep the formula_7-th elements. The problem is that we do not always know the exact formula_3 in advance. A possible solution is the following:\n\nThus,\n\nThe most common example was labelled \"Algorithm R\" by Jeffrey Vitter in his paper on the subject. This simple O(\"n\") algorithm as described in the \"Dictionary of Algorithms and Data Structures\" consists of the following steps (assuming \"k < n\" and using one-based array indexing):\n\nThe algorithm creates a \"reservoir\" array of size formula_1 and populates it with the first formula_1 items of formula_2. It then iterates through the remaining elements of formula_2 until formula_2 is exhausted. At the element of formula_2, the algorithm generates a random number formula_29 between 1 and formula_7. If formula_29 is less than or equal to formula_1, the element of the reservoir array is replaced with the element of formula_2. In effect, for all formula_7, the element of formula_2 is chosen to be included in the reservoir with probability formula_36. Similarly, at each iteration the element of the reservoir array is chosen to be replaced with probability formula_37, which simplifies to formula_38. It can be shown that when the algorithm has finished executing, each item in formula_2 has equal probability (i.e. formula_40) of being chosen for the reservoir.\n\nTo see this, consider the following proof by induction. After the round, let us assume, the probability of a number being in the reservoir array is formula_41. Since the probability of the number being replaced in the round is formula_38, the probability that it survives the round is formula_43. Thus, the probability that a given number is in the reservoir after the round is the product of these two probabilities, i.e. the probability of being in the reservoir after the round, and surviving replacement in the round. This is formula_44. Hence, the result holds for formula_7, and is therefore true by induction.\n\nA simple reservoir-based algorithm can be designed using random sort and implemented using priority queue data structure. This algorithm assigns random number as keys to each item and maintain k items with minimum value for keys. In essence, this is equivalent to assigning a random number to each item as key, sorting items using these keys and taking top k items. The worse case run time of the algorithm is formula_46 while the best case runtime is formula_47. Even though the worse case runtime is not as good as Algorithm R, this algorithm can easily be extended to weighted sampling. Note that both algorithms can operate on streams of unspecified lengths.\n\nIn many applications sampling is required to be according to the weights that are assigned to each items available in set. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. There are two ways to interpret weights assigned to each item in the set: \n\nThe following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:\nReservoirSample(S[1..?], R[1..k])\nThis algorithm is identical to the algorithm given in Reservoir Sampling with Random Sort except for the line how we generate the key using random number generator. The algorithm is equivalent to assigning each item a key formula_54 where is the random number and then sort items using these keys and finally select top k items for the sample.\n\nFollowing algorithm was given by M. T. Chao uses interpretation 2:\nWeightedReservoir-Chao(S[1..n], R[1..k])\n\nFor each item, its relative weight is calculated and used to randomly decide if the item will be added into the reservoir. If the item is selected, then one of the existing items of the reservoir is uniformly selected and replaced with the new item. The trick here is that, if the probabilities of all items in the reservoir are already proportional to their weights, then by selecting uniformly which item to replace, the probabilities of all items remain proportional to their weight after the replacement.\n\nIn many applications, amount of data from which a small sample is needed is too large and it is desirable to distribute sampling tasks among many machines in parallel to speed up the process. A simple approach that is often used, although less performant, is to assign a random number as key to each item and then perform a distributed sort and finally obtain a sample of desired size from top k items. If weighted sample is desired then key is computed using formula_54 where is the random number and formula_48 is the weight of an item. The inefficiency in this approach obviously arises from required distributed sort on very large amount of data.\n\nAnother more efficient approach for distributed weighted random sampling is as follows:\nThe Step 4 uses keys from Step 2 because we might have unbalanced data distribution on machines. For example, lets say k = 1, machine m1 only gets 1 item with weight 10 while machine m2 gets 2 items each with weight 100. Intuitively probability for items from m1 getting in final sample is 10/210. In Step 3, we will get 1 item from m1 as well as m2. If we recalculate keys in step 4 then the probability that item from m1 will be in final sample is 10/110 instead of required 10/210. Now observe that weighted reservoir sampling algorithm from previous section decreases max key value in priority queue as it processes more items. Therefore, items sampled from machine with larger chunk will have lower key values and thus higher chance of getting selected.\n\nSuppose one wanted to draw \"k\" random cards from a deck of playing cards (i.e., \"n=52\").\nA natural approach would be to shuffle the deck and then take the top \"k\" cards.\nIn the general case, the shuffle also needs to work even if the number of cards in the deck is not known in advance, a condition which is satisfied by the inside-out version of the Fisher-Yates shuffle:\n\nNote that although the rest of the cards are shuffled, only the top \"k\" are important in the present context.\nTherefore, the array \"a\" need only track the cards in the top \"k\" positions while performing the shuffle, reducing the amount of memory needed.\nTruncating \"a\" to length \"k\", the algorithm is modified accordingly:\n\nSince the order of the first \"k\" cards is immaterial, the first loop can be removed and \"a\" can be initialized to be the first \"k\" items of \"S\".\nThis yields \"Algorithm R\".\n\nA fast approximation to reservoir sampling.\nUses a good-quality approximation to the sampling-gap distribution to skip over the gaps; i.e. consecutive runs of data that\nare not sampled.\n\nThe following is a simple implementation of the algorithm in Python that samples the set of English Wikipedia page titles:\nProbabilities of selection of the reservoir methods are discussed in Chao (1982) and Tillé (2006). While the first-order selection probabilities are equal to formula_60 (or, in case of Chao's procedure, to an arbitrary set of unequal probabilities), the second order selection probabilities depend on the order in which the records are sorted in the original reservoir. The problem is overcome by the cube sampling method of Deville and Tillé (2004).\n\nReservoir sampling makes the assumption that the desired sample fits into main memory, often implying that formula_1 is a constant independent of formula_3. In applications where we would like to select a large subset of the input list (say a third, i.e. formula_63), other methods need to be adopted. Distributed implementations for this problem have been proposed.\n\n"}
{"id": "39214059", "url": "https://en.wikipedia.org/wiki?curid=39214059", "title": "Rose–Vinet equation of state", "text": "Rose–Vinet equation of state\n\nThe Rose–Vinet equation of state are a set of equations used to describe the equation of state of solid objects. It is an modification of the Birch–Murnaghan equation of state.\nThe initial paper discusses how the equation only depends on four inputs: the isothermal bulk modulus formula_1, the derivative of bulk modulus with respect to pressure formula_2, the volume formula_3, and the thermal expansion; all evaluated zero pressure (formula_4) and at a single (reference) temperature. And the same equation holds for all classes of solids and a wide range of temperatures.\n\nLet the cube root of the specific volume be\n\nthen the equation of state is:\n\nA similar equation was published by Stacey et al. in 1981.\n"}
{"id": "17528854", "url": "https://en.wikipedia.org/wiki?curid=17528854", "title": "Sion's minimax theorem", "text": "Sion's minimax theorem\n\nIn mathematics, and in particular game theory, Sion's minimax theorem is a generalization of John von Neumann's minimax theorem, named after Maurice Sion.\n\nIt states:\n\nLet formula_1 be a compact convex subset of a linear topological space and formula_2 a convex subset of a linear topological space. If formula_3 is a real-valued function on formula_4 with\n\nthen,\n\n\n"}
{"id": "21676935", "url": "https://en.wikipedia.org/wiki?curid=21676935", "title": "State space enumeration", "text": "State space enumeration\n\nIn computer science, state space enumeration are methods that consider each reachable program state to determine whether a program satisfies a given property. As programs increase in size and complexity, the state space grows exponentially. The state space used by these methods can be reduced by maintaining only the parts of the state space that are relevant to the analysis. However, the use of state and memory reduction techniques makes runtime a major limiting factor.\n\n"}
{"id": "385162", "url": "https://en.wikipedia.org/wiki?curid=385162", "title": "Substitution–permutation network", "text": "Substitution–permutation network\n\nIn cryptography, an SP-network, or substitution–permutation network (SPN), is a series of linked mathematical operations used in block cipher algorithms such as AES (Rijndael), 3-Way, Kalyna, Kuznyechik, PRESENT, SAFER, SHARK, and Square.\n\nSuch a network takes a block of the plaintext and the key as inputs, and applies several alternating \"rounds\" or \"layers\" of substitution boxes (S-boxes) and permutation boxes (P-boxes) to produce the ciphertext block. The S-boxes and P-boxes transform of input bits into output bits. It is common for these transformations to be operations that are efficient to perform in hardware, such as exclusive or (XOR) and bitwise rotation. The key is introduced in each round, usually in the form of \"round keys\" derived from it. (In some designs, the S-boxes themselves depend on the key.)\n\nDecryption is done by simply reversing the process (using the inverses of the S-boxes and P-boxes and applying the round keys in reversed order).\n\nAn S-box substitutes a small block of bits (the input of the S-box) by another block of bits (the output of the S-box). This substitution should be one-to-one, to ensure invertibility (hence decryption). In particular, the length of the output should be the same as the length of the input (the picture on the right has S-boxes with 4 input and 4 output bits), which is different from S-boxes in general that could also change the length, as in DES (Data Encryption Standard), for example. An S-box is usually not simply a permutation of the bits. Rather, a good S-box will have the property that changing one input bit will change about half of the output bits (or an avalanche effect). It will also have the property that each output bit will depend on every input bit.\n\nA P-box is a permutation of all the bits: it takes the outputs of all the S-boxes of one round, permutes the bits, and feeds them into the S-boxes of the next round. A good P-box has the property that the output bits of any S-box are distributed to as many S-box inputs as possible.\n\nAt each round, the round key (obtained from the key with some simple operations, for instance, using S-boxes and P-boxes) is combined using some group operation, typically XOR.\n\nA single typical S-box or a single P-box alone does not have much cryptographic strength: an S-box could be thought of as a substitution cipher, while a P-box could be thought of as a transposition cipher. However, a well-designed SP network with several alternating rounds of S- and P-boxes already satisfies Shannon's confusion and diffusion properties:\n\n\nAlthough a Feistel network that uses S-boxes (such as DES) is quite similar to SP networks, there are some differences that make either this or that more applicable in certain situations. For a given amount of confusion and diffusion, an SP network has more \"inherent parallelism\"\nand so — given a CPU with a large number of execution units — can be computed faster than a Feistel network.\nCPUs with few execution units — such as most smart cards — cannot take advantage of this inherent parallelism. Also SP ciphers require S-boxes to be invertible (to perform decryption); Feistel inner functions have no such restriction and can be constructed as one-way functions.\n\n\n"}
{"id": "27115797", "url": "https://en.wikipedia.org/wiki?curid=27115797", "title": "Theory of regions", "text": "Theory of regions\n\nThe Theory of regions is an approach for synthesizing a Petri net from a transition system. As such, it aims at recovering concurrent, independent behaviour from transitions between global states. Theory of regions handles elementary net systems as well as P/T nets and other kinds of nets. An important point is that the approach is aimed at the synthesis of unlabeled Petri nets only.\n\nA region of a transition system formula_1 is a mapping assigning to each state formula_2 a number formula_3 (natural number for P/T nets, binary for ENS) and to each transition label a number formula_4 such that consistency conditions formula_5 holds whenever formula_6.\n\nEach region represents a potential place of a Petri net.\n\nMukund: event/state separation property, state separation property.\n"}
{"id": "373956", "url": "https://en.wikipedia.org/wiki?curid=373956", "title": "Trichotomy (mathematics)", "text": "Trichotomy (mathematics)\n\nIn mathematics, the law of trichotomy states that every real number is either positive, negative, or zero.\n\nMore generally, a binary relation \"R\" on some set \"X\" is trichotomous if for all \"x\" and \"y\" in \"X\" exactly one of \"xRy\", \"yRx\" or \"x\"=\"y\" holds, formally, if\n\n\n\nA law of trichotomy on some set \"X\" of numbers usually expresses that some tacitly given ordering relation on \"X\" is a trichotomous one.\nAn example is the law \"For arbitrary real numbers \"x\" and \"y\", exactly one of \"x\"<\"y\", \"y\"<\"x\", or \"x\"=\"y\" applies\"; some authors even fix \"y\" to be zero, relying on the real number's additive linearly ordered group structure. The latter is a group equipped with a trichotomous order.\n\nIn classical logic, this axiom of trichotomy holds for ordinary comparison between real numbers and therefore also for comparisons between integers and between rational numbers. The law does not hold in general in intuitionistic logic.\n\nIn Zermelo–Fraenkel set theory and Bernays set theory, the law of trichotomy holds between the cardinal numbers of well-orderable sets even without the axiom of choice. If the axiom of choice holds, then trichotomy holds between arbitrary cardinal numbers (because they are all well-orderable in that case).\n\n"}
{"id": "9107270", "url": "https://en.wikipedia.org/wiki?curid=9107270", "title": "UTOPIA (bioinformatics tools)", "text": "UTOPIA (bioinformatics tools)\n\nUTOPIA (User-friendly Tools for Operating Informatics Applications) is a suite of free tools for visualising and analysing bioinformatics data. Based on an ontology-driven data model, it contains applications for viewing and aligning protein sequences, rendering complex molecular structures in 3D, and for finding and using resources such as web services and data objects. There are two major components, the protein analysis suite and UTOPIA documents.\n\nThe Utopia Protein Analysis suite is a collection of interactive tools for analysing protein sequence and protein structure. Up front are user-friendly and responsive visualisation applications, behind the scenes a sophisticated model that allows these to work together and hides much of the tedious work of dealing with file formats and web services.\n\nUtopia Documents brings a fresh new perspective to reading the scientific literature, combining the convenience and reliability of the Portable Document Format (pdf) with the flexibility and power of the web.\n\nBetween 2003 and 2005 work on UTOPIA was funded via The e-Science North West Centre based at The University of Manchester by the Engineering and Physical Sciences Research Council, UK Department of Trade And Industry, and the European Molecular Biology Network (EMBnet). Since 2005 work continues under the EMBRACE European Network of Excellence.\n\nUTOPIA's CINEMA (Colour INteractive Editor for Multiple Alignments), a tool for Sequence Alignment, is the latest incarnation of software originally developed at The University of Leeds to aid the analysis of G protein-coupled receptors (GPCRs). SOMAP, a Screen Oriented Multiple Alignment Procedure was developed in the late 1980s on the VMS computer operating system, used a monochrome text-based VT100 video terminal, and featured context-sensitive help and pulldown menus some time before these were standard operating system features.\n\nSOMAP was followed by a Unix tool called VISTAS (VIsualizing STructures And Sequences) which included the ability to render 3D molecular structure and generate plots and statistical representations of sequence properties.\n\nThe first tool under the CINEMA banner developed at The University of Manchester was a Java-based applet launched via web pages, which is still available but is no longer maintained. A standalone Java version, called CINEMA-MX, was also released but is no longer readily available.\n\nA C++ version of CINEMA, called CINEMA5 was developed early on as part of the UTOPIA project, and was released as a stand-alone sequence alignment application. It has now been replaced by a version of the tool integrated with UTOPIA's other visualisation applications, and its name has reverted simply to CINEMA.\n"}
{"id": "20646772", "url": "https://en.wikipedia.org/wiki?curid=20646772", "title": "Vibration", "text": "Vibration\n\nVibration is a mechanical phenomenon whereby oscillations occur about an equilibrium point. The word comes from Latin \"vibrationem\" (\"shaking, brandishing\"). The oscillations may be periodic, such as the motion of a pendulum—or random, such as the movement of a tire on a gravel road.\n\nVibration can be desirable: for example, the motion of a tuning fork, the reed in a woodwind instrument or harmonica, a mobile phone, or the cone of a loudspeaker.\n\nIn many cases, however, vibration is undesirable, wasting energy and creating unwanted sound. For example, the vibrational motions of engines, electric motors, or any mechanical device in operation are typically unwanted. Such vibrations could be caused by imbalances in the rotating parts, uneven friction, or the meshing of gear teeth. Careful designs usually minimize unwanted vibrations.\n\nThe studies of sound and vibration are closely related. Sound, or pressure waves, are generated by vibrating structures (e.g. vocal cords); these pressure waves can also induce the vibration of structures (e.g. ear drum). Hence, attempts to reduce noise are often related to issues of vibration.\n\n Free vibration occurs when a mechanical system is set in motion with an initial input and allowed to vibrate freely. Examples of this type of vibration are pulling a child back on a swing and letting it go, or hitting a tuning fork and letting it ring. The mechanical system vibrates at one or more of its natural frequencies and damps down to motionlessness.\n\nForced vibration is when a time-varying disturbance (load, displacement or velocity) is applied to a mechanical system. The disturbance can be a periodic and steady-state input, a transient input, or a random input. The periodic input can be a harmonic or a non-harmonic disturbance. Examples of these types of vibration include a washing machine shaking due to an imbalance, transportation vibration caused by an engine or uneven road, or the vibration of a building during an earthquake. For linear systems, the frequency of the steady-state vibration response resulting from the application of a periodic, harmonic input is equal to the frequency of the applied force or motion, with the response magnitude being dependent on the actual mechanical system.\n\nDamped vibration: When the energy of a vibrating system is gradually dissipated by friction and other resistances, the vibrations are said to be damped. The vibrations gradually reduce or change in frequency or intensity or cease and the system rests in its equilibrium position. An example of this type of vibration is the vehicular suspension dampened by the shock absorber.\n\nVibration testing is accomplished by introducing a forcing function into a structure, usually with some type of shaker. Alternately, a DUT (device under test) is attached to the \"table\" of a shaker. Vibration testing is performed to examine the response of a device under test (DUT) to a defined vibration environment. The measured response may be fatigue life, resonant frequencies or squeak and rattle sound output (NVH). Squeak and rattle testing is performed with a special type of \"quiet shaker\" that produces very low sound levels while under operation.\n\nFor relatively low frequency forcing, servohydraulic (electrohydraulic) shakers are used. For higher frequencies, electrodynamic shakers are used. Generally, one or more \"input\" or \"control\" points located on the DUT-side of a fixture is kept at a specified acceleration. Other \"response\" points experience maximum vibration level (resonance) or minimum vibration level (anti-resonance). It is often desirable to achieve anti-resonance to keep a system from becoming too noisy, or to reduce strain on certain parts due to vibration modes caused by specific vibration frequencies .\n\nThe most common types of vibration testing services conducted by vibration test labs are Sinusoidal and Random. Sine (one-frequency-at-a-time) tests are performed to survey the structural response of the device under test (DUT). A random (all frequencies at once) test is generally considered to more closely replicate a real world environment, such as road inputs to a moving automobile.\n\nMost vibration testing is conducted in a 'single DUT axis' at a time, even though most real-world vibration occurs in various axes simultaneously. MIL-STD-810G, released in late 2008, Test Method 527, calls for multiple exciter testing. The \"vibration test fixture\" used to attach the DUT to the shaker table must be designed for the frequency range of the vibration test spectrum. Generally for smaller fixtures and lower frequency ranges, the designer targets a fixture design that is free of resonances in the test frequency range. This becomes more difficult as the DUT gets larger and as the test frequency increases. In these cases multi-point control strategies can mitigate some of the resonances that may be present in the future. Devices specifically designed to trace or record vibrations are called vibroscopes.\n\nVibration Analysis (VA), applied in an industrial or maintenance environment aims to reduce maintenance costs and equipment downtime by detecting equipment faults. VA is a key component of a Condition Monitoring (CM) program, and is often referred to as Predictive Maintenance (PdM). Most commonly VA is used to detect faults in rotating equipment (Fans, Motors, Pumps, and Gearboxes etc.) such as Unbalance, Misalignment, rolling element bearing faults and resonance conditions.\n\nVA can use the units of Displacement, Velocity and Acceleration displayed as a time waveform (TWF), but most commonly the spectrum is used, derived from a fast Fourier transform of the TWF. The vibration spectrum provides important frequency information that can pinpoint the faulty component.\n\nThe fundamentals of vibration analysis can be understood by studying the simple Mass-spring-damper model. Indeed, even a complex structure such as an automobile body can be modeled as a \"summation\" of simple mass–spring–damper models. The mass–spring–damper model is an example of a simple harmonic oscillator. The mathematics used to describe its behavior is identical to other simple harmonic oscillators such as the RLC circuit.\n\nNote: This article does not include the step-by-step mathematical derivations, but focuses on major vibration analysis equations and concepts. Please refer to the references at the end of the article for detailed derivations.\n\n To start the investigation of the mass–spring–damper assume the damping is negligible and that there is no external force applied to the mass (i.e. free vibration). The force applied to the mass by the spring is proportional to the amount the spring is stretched \"x\" (assuming the spring is already compressed due to the weight of the mass). The proportionality constant, k, is the stiffness of the spring and has units of force/distance (e.g. lbf/in or N/m). The negative sign indicates that the force is always opposing the motion of the mass attached to it:\nThe force generated by the mass is proportional to the acceleration of the mass as given by Newton's second law of motion:\nThe sum of the forces on the mass then generates this ordinary differential equation: formula_3 \nThis solution says that it will oscillate with simple harmonic motion that has an amplitude of \"A\" and a frequency of \"f\". The number \"f\" is called the undamped natural frequency. For the simple mass–spring system, \"f\" is defined as:\n\nNote: angular frequency ω (ω=2 π \"f\") with the units of radians per second is often used in equations because it simplifies the equations, but is normally converted to ordinary frequency (units of Hz or equivalently cycles per second) when stating the frequency of a system. If the mass and stiffness of the system is known, the formula above can determine the frequency at which the system vibrates once set in motion by an initial disturbance. Every vibrating system has one or more natural frequencies that it vibrates at once disturbed. This simple relation can be used to understand in general what happens to a more complex system once we add mass or stiffness. For example, the above formula explains why, when a car or truck is fully loaded, the suspension feels ″softer″ than unloaded—the mass has increased, reducing the natural frequency of the system.\n\nVibrational motion could be understood in terms of conservation of energy. In the above example the spring has been extended by a value of x and therefore some potential energy (formula_6) is stored in the spring. Once released, the spring tends to return to its un-stretched state (which is the minimum potential energy state) and in the process accelerates the mass. At the point where the spring has reached its un-stretched state all the potential energy that we supplied by stretching it has been transformed into kinetic energy (formula_7). The mass then begins to decelerate because it is now compressing the spring and in the process transferring the kinetic energy back to its potential. Thus oscillation of the spring amounts to the transferring back and forth of the kinetic energy into potential energy. In this simple model the mass continues to oscillate forever at the same magnitude—but in a real system, \"damping\" always dissipates the energy, eventually bringing the spring to rest.\n\nWhen a \"viscous\" damper is added to the model this outputs a force that is proportional to the velocity of the mass. The damping is called viscous because it models the effects of a fluid within an object. The proportionality constant \"c\" is called the damping coefficient and has units of Force over velocity (lbf⋅s/in or N⋅s/m).\n\nSumming the forces on the mass results in the following ordinary differential equation:\n\nThe solution to this equation depends on the amount of damping. If the damping is small enough, the system still vibrates—but eventually, over time, stops vibrating. This case is called underdamping, which is important in vibration analysis. If damping is increased just to the point where the system no longer oscillates, the system has reached the point of critical damping. If the damping is increased past critical damping, the system is overdamped. The value that the damping coefficient must reach for critical damping in the mass-spring-damper model is:\n\nTo characterize the amount of damping in a system a ratio called the damping ratio (also known as damping factor and % critical damping) is used. This damping ratio is just a ratio of the actual damping over the amount of damping required to reach critical damping. The formula for the damping ratio (formula_11) of the mass-spring-damper model is:\n\nFor example, metal structures (e.g., airplane fuselages, engine crankshafts) have damping factors less than 0.05, while automotive suspensions are in the range of 0.2–0.3. The solution to the underdamped system for the mass-spring-damper model is the following:\n\nThe value of \"X\", the initial magnitude, and formula_14 the phase shift, are determined by the amount the spring is stretched. The formulas for these values can be found in the references.\n\nThe major points to note from the solution are the exponential term and the cosine function. The exponential term defines how quickly the system “damps” down – the larger the damping ratio, the quicker it damps to zero. The cosine function is the oscillating portion of the solution, but the frequency of the oscillations is different from the undamped case.\n\nThe frequency in this case is called the \"damped natural frequency\", formula_15 and is related to the undamped natural frequency by the following formula:\n\nThe damped natural frequency is less than the undamped natural frequency, but for many practical cases the damping ratio is relatively small and hence the difference is negligible. Therefore, the damped and undamped description are often dropped when stating the natural frequency (e.g. with 0.1 damping ratio, the damped natural frequency is only 1% less than the undamped).\n\nThe plots to the side present how 0.1 and 0.3 damping ratios effect how the system “rings” down over time. What is often done in practice is to experimentally measure the free vibration after an impact (for example by a hammer) and then determine the natural frequency of the system by measuring the rate of oscillation, as well as the damping ratio by measuring the rate of decay. The natural frequency and damping ratio are not only important in free vibration, but also characterize how a system behaves under forced vibration.\nThe behavior of the spring mass damper model varies with the addition of a harmonic force. A force of this type could, for example, be generated by a rotating imbalance.\n\nSumming the forces on the mass results in the following ordinary differential equation:\n\nThe steady state solution of this problem can be written as:\n\nThe result states that the mass will oscillate at the same frequency, \"f\", of the applied force, but with a phase shift formula_20\n\nThe amplitude of the vibration “X” is defined by the following formula.\n\nWhere “r” is defined as the ratio of the harmonic force frequency over the undamped natural frequency of the mass–spring–damper model.\n\nThe phase shift, formula_23 is defined by the following formula.\n\nThe plot of these functions, called \"the frequency response of the system\", presents one of the most important features in forced vibration. In a lightly damped system when the forcing frequency nears the natural frequency (formula_25) the amplitude of the vibration can get extremely high. This phenomenon is called resonance (subsequently the natural frequency of a system is often referred to as the resonant frequency). In rotor bearing systems any rotational speed that excites a resonant frequency is referred to as a critical speed.\n\nIf resonance occurs in a mechanical system it can be very harmful – leading to eventual failure of the system. Consequently, one of the major reasons for vibration analysis is to predict when this type of resonance may occur and then to determine what steps to take to prevent it from occurring. As the amplitude plot shows, adding damping can significantly reduce the magnitude of the vibration. Also, the magnitude can be reduced if the natural frequency can be shifted away from the forcing frequency by changing the stiffness or mass of the system. If the system cannot be changed, perhaps the forcing frequency can be shifted (for example, changing the speed of the machine generating the force).\n\nThe following are some other points in regards to the forced vibration shown in the frequency response plots.\n\n\nResonance is simple to understand if the spring and mass are viewed as energy storage elements – with the mass storing kinetic energy and the spring storing potential energy. As discussed earlier, when the mass and spring have no external force acting on them they transfer energy back and forth at a rate equal to the natural frequency. In other words, to efficiently pump energy into both mass and spring requires that the energy source feed the energy in at a rate equal to the natural frequency. Applying a force to the mass and spring is similar to pushing a child on swing, a push is needed at the correct moment to make the swing get higher and higher. As in the case of the swing, the force applied need not be high to get large motions, but must just add energy to the system.\n\nThe damper, instead of storing energy, dissipates energy. Since the damping force is proportional to the velocity, the more the motion, the more the damper dissipates the energy. Therefore, there is a point when the energy dissipated by the damper equals the energy added by the force. At this point, the system has reached its maximum amplitude and will continue to vibrate at this level as long as the force applied stays the same. If no damping exists, there is nothing to dissipate the energy and, theoretically, the motion will continue to grow into infinity.\n\nIn a previous section only a simple harmonic force was applied to the model, but this can be extended considerably using two powerful mathematical tools. The first is the Fourier transform that takes a signal as a function of time (time domain) and breaks it down into its harmonic components as a function of frequency (frequency domain). For example, by applying a force to the mass–spring–damper model that repeats the following cycle – a force equal to 1 newton for 0.5 second and then no force for 0.5 second. This type of force has the shape of a 1 Hz square wave.\n\nThe Fourier transform of the square wave generates a frequency spectrum that presents the magnitude of the harmonics that make up the square wave (the phase is also generated, but is typically of less concern and therefore is often not plotted). The Fourier transform can also be used to analyze non-periodic functions such as transients (e.g. impulses) and random functions. The Fourier transform is almost always computed using the fast Fourier transform (FFT) computer algorithm in combination with a window function.\n\nIn the case of our square wave force, the first component is actually a constant force of 0.5 newton and is represented by a value at 0 Hz in the frequency spectrum. The next component is a 1 Hz sine wave with an amplitude of 0.64. This is shown by the line at 1 Hz. The remaining components are at odd frequencies and it takes an infinite amount of sine waves to generate the perfect square wave. Hence, the Fourier transform allows you to interpret the force as a sum of sinusoidal forces being applied instead of a more \"complex\" force (e.g. a square wave).\n\nIn the previous section, the vibration solution was given for a single harmonic force, but the Fourier transform in general gives multiple harmonic forces. The second mathematical tool, \"the principle of superposition\", allows the summation of the solutions from multiple forces if the system is linear. In the case of the spring–mass–damper model, the system is linear if the spring force is proportional to the displacement and the damping is proportional to the velocity over the range of motion of interest. Hence, the solution to the problem with a square wave is summing the predicted vibration from each one of the harmonic forces found in the frequency spectrum of the square wave.\n\nThe solution of a vibration problem can be viewed as an input/output relation – where the force is the input and the output is the vibration. Representing the force and vibration in the frequency domain (magnitude and phase) allows the following relation:\n\nformula_31 is called the frequency response function (also referred to as the transfer function, but not technically as accurate) and has both a magnitude and phase component (if represented as a complex number, a real and imaginary component). The magnitude of the frequency response function (FRF) was presented earlier for the mass–spring–damper system.\n\nThe phase of the FRF was also presented earlier as:\n\nFor example, calculating the FRF for a mass–spring–damper system with a mass of 1 kg, spring stiffness of 1.93 N/mm and a damping ratio of 0.1. The values of the spring and mass give a natural frequency of 7 Hz for this specific system. Applying the 1 Hz square wave from earlier allows the calculation of the predicted vibration of the mass. The figure illustrates the resulting vibration. It happens in this example that the fourth harmonic of the square wave falls at 7 Hz. The frequency response of the mass–spring–damper therefore outputs a high 7 Hz vibration even though the input force had a relatively low 7 Hz harmonic. This example highlights that the resulting vibration is dependent on both the forcing function and the system that the force is applied to.\n\nThe figure also shows the time domain representation of the resulting vibration. This is done by performing an inverse Fourier Transform that converts frequency domain data to time domain. In practice, this is rarely done because the frequency spectrum provides all the necessary information.\n\nThe frequency response function (FRF) does not necessarily have to be calculated from the knowledge of the mass, damping, and stiffness of the system—but can be measured experimentally. For example, if a known force over a range of frequencies is applied, and if the associated vibrations are measured, the frequency response function can be calculated, thereby characterizing the system. This technique is used in the field of experimental modal analysis to determine the vibration characteristics of a structure.\n\nThe simple mass–spring–damper model is the foundation of vibration analysis, but what about more complex systems? The mass–spring–damper model described above is called a single degree of freedom (SDOF) model since the mass is assumed to only move up and down. In more complex systems, the system must be discretized into more masses that move in more than one direction, adding degrees of freedom. The major concepts of multiple degrees of freedom (MDOF) can be understood by looking at just a 2 degree of freedom model as shown in the figure.\nThe equations of motion of the 2DOF system are found to be:\n\nThis can be rewritten in matrix format:\n\nA more compact form of this matrix equation can be written as:\n\nwhere formula_38 formula_39 and formula_40 are symmetric matrices referred respectively as the mass, damping, and stiffness matrices. The matrices are NxN square matrices where N is the number of degrees of freedom of the system.\n\nThe following analysis involves the case where there is no damping and no applied forces (i.e. free vibration). The solution of a viscously damped system is somewhat more complicated.\n\nThis differential equation can be solved by assuming the following type of solution:\n\nNote: Using the exponential solution of formula_43 is a mathematical trick used to solve linear differential equations. Using Euler's formula and taking only the real part of the solution it is the same cosine solution for the 1 DOF system. The exponential solution is only used because it is easier to manipulate mathematically.\n\nThe equation then becomes:\n\nSince formula_45 cannot equal zero the equation reduces to the following.\n\nThis is referred to an eigenvalue problem in mathematics and can be put in the standard format by pre-multiplying the equation by formula_47\n\nand if: formula_49 and formula_50\n\nThe solution to the problem results in N eigenvalues (i.e. formula_52), where N corresponds to the number of degrees of freedom. The eigenvalues provide the natural frequencies of the system. When these eigenvalues are substituted back into the original set of equations, the values of formula_53 that correspond to each eigenvalue are called the eigenvectors. These eigenvectors represent the mode shapes of the system. The solution of an eigenvalue problem can be quite cumbersome (especially for problems with many degrees of freedom), but fortunately most math analysis programs have eigenvalue routines.\n\nThe eigenvalues and eigenvectors are often written in the following matrix format and describe the modal model of the system:\n\nA simple example using the 2 DOF model can help illustrate the concepts. Let both masses have a mass of 1 kg and the stiffness of all three springs equal 1000 N/m. The mass and stiffness matrix for this problem are then:\n\nThen formula_57\n\nThe eigenvalues for this problem given by an eigenvalue routine is:\n\nThe natural frequencies in the units of hertz are then (remembering formula_59) formula_60 and formula_61\n\nThe two mode shapes for the respective natural frequencies are given as:\n\nSince the system is a 2 DOF system, there are two modes with their respective natural frequencies and shapes. The mode shape vectors are not the absolute motion, but just describe relative motion of the degrees of freedom. In our case the first mode shape vector is saying that the masses are moving together in phase since they have the same value and sign. In the case of the second mode shape vector, each mass is moving in opposite direction at the same rate.\n\nWhen there are many degrees of freedom, one method of visualizing the mode shapes is by animating them using structural analysis software such as Femap or ANSYS. An example of animating mode shapes is shown in the figure below for a cantilevered -beam as demonstrated using modal analysis on ANSYS. In this case, the finite element method was used to generate an approximation of the mass and stiffness matrices by meshing the object of interest in order to solve a discrete eigenvalue problem. Note that, in this case, the finite element method provides an approximation of the meshed surface (for which there exists an infinite number of vibration modes and frequencies). Therefore, this relatively simple model that has over 100 degrees of freedom and hence as many natural frequencies and mode shapes, provides a good approximation for the first natural frequencies and modes. Generally, only the first few modes are important for practical applications.\n\nThe eigenvectors have very important properties called orthogonality properties. These properties can be used to greatly simplify the solution of multi-degree of freedom models. It can be shown that the eigenvectors have the following properties:\n\nformula_65 and formula_66 are diagonal matrices that contain the modal mass and stiffness values for each one of the modes. (Note: Since the eigenvectors (mode shapes) can be arbitrarily scaled, the orthogonality properties are often used to scale the eigenvectors so the modal mass value for each mode is equal to 1. The modal mass matrix is therefore an identity matrix)\n\nThese properties can be used to greatly simplify the solution of multi-degree of freedom models by making the following coordinate transformation.\n\nUsing this coordinate transformation in the original free vibration differential equation results in the following equation.\n\nTaking advantage of the orthogonality properties by premultiplying this equation by formula_69\n\nThe orthogonality properties then simplify this equation to:\n\nThis equation is the foundation of vibration analysis for multiple degree of freedom systems. A similar type of result can be derived for damped systems. The key is that the modal mass and stiffness matrices are diagonal matrices and therefore the equations have been \"decoupled\". In other words, the problem has been transformed from a large unwieldy multiple degree of freedom problem into many single degree of freedom problems that can be solved using the same methods outlined above.\n\nSolving for \"x\" is replaced by solving for \"q\", referred to as the modal coordinates or modal participation factors.\n\nIt may be clearer to understand if formula_72 is written as:\n\nWritten in this form it can be seen that the vibration at each of the degrees of freedom is just a linear sum of the mode shapes. Furthermore, how much each mode \"participates\" in the final vibration is defined by q, its modal participation factor.\n\nAn unrestrained multi-degree of freedom system experiences both rigid-body translation and/or rotation and vibration. The existence of a rigid-body mode results in a zero natural frequency. The corresponding mode shape is called the rigid-body mode. \n\n\n"}
