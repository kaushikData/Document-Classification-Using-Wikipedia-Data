{"id": "948340", "url": "https://en.wikipedia.org/wiki?curid=948340", "title": "185 (number)", "text": "185 (number)\n\n185 (one hundred [and] eighty-five) is the natural number following 184 and preceding 186.\n\n\n\n\n\n185 is also:\n\n\n"}
{"id": "31197103", "url": "https://en.wikipedia.org/wiki?curid=31197103", "title": "Ahlfors measure conjecture", "text": "Ahlfors measure conjecture\n\nIn mathematics, the Ahlfors conjecture, now a theorem, states that the limit set of a finitely-generated Kleinian group is either the whole Riemann sphere, or has measure 0. \n\nThe conjecture was introduced by , who proved it in the case that the Kleinian group has a fundamental domain with a finite number of sides. proved the Ahlfors conjecture for topologically tame groups, by showing that a topologically tame Kleinian group is geometrically tame, so the Ahlfors conjecture follows from Marden's tameness conjecture that hyperbolic 3-manifolds with finitely generated fundamental groups are topologically tame (homeomorphic to the interior of compact 3-manifolds). This latter conjecture was proved, independently, by and by .\n\n"}
{"id": "206234", "url": "https://en.wikipedia.org/wiki?curid=206234", "title": "Analytical Society", "text": "Analytical Society\n\nThe Analytical Society was a group of individuals in early-19th-century Britain whose aim was to promote the use of Leibnizian notation for differentiation in calculus as opposed to the Newton notation for differentiation. The latter system came into being in the 18th century as a convention of Sir Isaac Newton, and was in use throughout Great Britain. According to a mathematical historian:\n\nThe Society was first envisioned by Charles Babbage as a parody on the debate of whether Bible texts should be annotated, with Babbage having the notion that his textbook by Sylvestre Lacroix was without need for interpretation once translated. \nIts membership originally consisted of a group of Cambridge students led by Babbage and including Edward Bromhead. \n\nRobert Woodhouse had brought the Leibniz notation to England with his book \"Principles of Analytical Calculation\" in 1803. While Newton's notation was unsuitable for a function of several variables, Woodhouse showed, for instance, how to find the total differential of formula_1  where φ is a function of \"p\" and \"q\":\n\nThough the Society was disbanded by 1814 when most of the original members had graduated, its influence continued to be felt. The evidence of Analytical Society work appeared in 1816 when Peacock and Herschel completed the translation of Sylvestre Lacroix's textbook \"An Elementary Treatise on Differential and Integral Calculus\" that had been started by Babbage. In 1817 Peacock introduced Leibnizian symbols in that year's examinations in the local senate-house. \n\nBoth the exam and the textbook met with little criticism until 1819, when both were criticised by D.M. Peacock, vicar of Sedbergh, 1796 to 1840. He wrote:\n\nNevertheless, the reforms were encouraged by younger members of Cambridge University. George Peacock successfully encouraged a colleague, Richard Gwatkin of St John's College at Cambridge University, to adopt the new notation in his exams.\n\nUse of Leibnizian notation began to spread after this. In 1820, the notation was used by William Whewell, a previously neutral but influential Cambridge University faculty member, in his examinations. In 1821, Peacock again used Leibnizian notation in his examinations, and the notation became well established.\n\nThe Society followed its success by publishing two volumes of examples showing the new method. One was by George Peacock on differential and integral calculus; the other was by Herschel on the calculus of finite differences. They were joined in this by Whewell, who in 1819 published a book, \"An Elementary Treatise on Mechanics,\" which used the new notation and which became a standard textbook on the subject. \n\nJohn Ainz, a pupil of Peacock's, published a notable paper in 1826 which showed how to apply Leibnizian calculus on various physical problems.\n\nThese activities did not go unnoticed at other universities in Great Britain, and soon they followed Cambridge's example. By 1830, Leibniz notation was widely adopted and used alongside the traditional denotation of differentiation by use of dots as Newton had done.\n"}
{"id": "43167897", "url": "https://en.wikipedia.org/wiki?curid=43167897", "title": "Arrangement (space partition)", "text": "Arrangement (space partition)\n\nIn discrete geometry, an arrangement is the decomposition of the d-dimensional linear, affine, or projective space into connected open cells of lower dimensions, induced by a finite collection of geometric objects. Sometimes these objects are of the same type, such as hyperplanes or spheres. An interest in the study of arrangements was driven by advances in computational geometry, where the arrangements were unifying structures for many problems. Advances in study of more complicated objects, such as algebraic surfaces, contributed to \"real-world\" applications, such as motion planning and computer vision.\n\nOf particular interest are the arrangements of lines and arrangements of hyperplanes.\n\nMore generally, geometers have studied arrangements of other types of curves in the plane, and of other more complicated types of surface. Arrangements in complex vector spaces have also been studied; since complex lines do not partition the complex plane into multiple connected components, the combinatorics of vertices, edges, and cells does not apply to these types of space, but it is still of interest to study their symmetries and topological properties.\n"}
{"id": "6057100", "url": "https://en.wikipedia.org/wiki?curid=6057100", "title": "Berlekamp's algorithm", "text": "Berlekamp's algorithm\n\nIn mathematics, particularly computational algebra, Berlekamp's algorithm is a well-known method for factoring polynomials over finite fields (also known as \"Galois fields\"). The algorithm consists mainly of matrix reduction and polynomial GCD computations. It was invented by Elwyn Berlekamp in 1967. It was the dominant algorithm for solving the problem until the Cantor–Zassenhaus algorithm of 1981. It is currently implemented in many well-known computer algebra systems.\n\nBerlekamp's algorithm takes as input a square-free polynomial formula_1 (i.e. one with no repeated factors) of degree formula_2 with coefficients in a finite field formula_3 and gives as output a polynomial formula_4 with coefficients in the same field such that formula_4 divides formula_1. The algorithm may then be applied recursively to these and subsequent divisors, until we find the decomposition of formula_1 into powers of irreducible polynomials (recalling that the ring of polynomials over a finite field is a unique factorization domain).\n\nAll possible factors of formula_1 are contained within the factor ring\nThe algorithm focuses on polynomials formula_10 which satisfy the congruence:\nThese polynomials form a subalgebra of R (which can be considered as an formula_2-dimensional vector space over formula_3), called the \"Berlekamp subalgebra\". The Berlekamp subalgebra is of interest because the polynomials formula_4 it contains satisfy\n\nIn general, not every GCD in the above product will be a non-trivial factor of formula_1, but some are, providing the factors we seek.\n\nBerlekamp's algorithm finds polynomials formula_4 suitable for use with the above result by computing a basis for the Berlekamp subalgebra. This is achieved via the observation that Berlekamp subalgebra is in fact the kernel of a certain formula_18 matrix over formula_3, which is derived from the so-called Berlekamp matrix of the polynomial, denoted formula_20. If formula_21 then formula_22 is the coefficient of the formula_23-th power term in the reduction of formula_24 modulo formula_1, i.e.:\n\nWith a certain polynomial formula_10, say:\n\nwe may associate the row vector:\n\nIt is relatively straightforward to see that the row vector formula_30 corresponds, in the same way, to the reduction of formula_31 modulo formula_1. Consequently a polynomial formula_10 is in the Berlekamp subalgebra if and only if formula_34 (where formula_35 is the formula_18 identity matrix), i.e. if and only if it is in the null space of formula_37.\n\nBy computing the matrix formula_37 and reducing it to reduced row echelon form and then easily reading off a basis for the null space, we may find a basis for the Berlekamp subalgebra and hence construct polynomials formula_4 in it. We then need to successively compute GCDs of the form above until we find a non-trivial factor. Since the ring of polynomials over a field is a Euclidean domain, we may compute these GCDs using the Euclidean algorithm.\n\nOne important application of Berlekamp's algorithm is in computing discrete logarithms over finite fields formula_40, where formula_41 is prime and formula_42. Computing discrete logarithms is an important problem in public key cryptography and error-control coding. For a finite field, the fastest known method is the index calculus method, which involves the factorisation of field elements. If we represent the field formula_40 in the usual way - that is, as polynomials over the base field formula_44, reduced modulo an irreducible polynomial of degree formula_2 - then this is simply polynomial factorisation, as provided by Berlekamp's algorithm.\n\nBerlekamp's algorithm may be accessed in the PARI/GP package using the factormod command, and the WolframAlpha website.\n\n\n"}
{"id": "11376019", "url": "https://en.wikipedia.org/wiki?curid=11376019", "title": "Binomial approximation", "text": "Binomial approximation\n\nThe binomial approximation is useful for approximately calculating powers of sums of a small number and 1. It states that if formula_1 and formula_2 ≪ formula_3 where formula_4 and formula_5 are real or complex numbers, then\n\nIf either formula_4 or formula_5 are complex then the absolute value denotes the modulus of the complex number.\n\nThe benefit of this approximation is that formula_5 is converted from a power to multiplicative factor. This can greatly simplify mathematical expressions (see example) and is a common tool in physics.\n\nThis approximation can be proven several ways including the binomial theorem and ignoring the terms beyond the first two.\n\nBy Bernoulli's inequality, the left-hand side of this relation is greater than or equal to the right-hand side whenever formula_10 and formula_11.\n\nThe function\nis a smooth function for \"x\" near 0. Thus, standard linear approximation tools from calculus apply: one has\nand so\nThus\n\nThe function \nwhere formula_4 and formula_5 may be real or complex can be expressed as a Taylor Series about the point zero.\n\nIf formula_20 and formula_2 ≪ formula_3, then the terms in the series become progressively smaller and it can be truncated to\n\nThis result from the binomial approximation can always be improved by keeping additional terms from the Taylor Series above. This is especially important when formula_2 starts to approach one, or when evaluating a more complex expression where the first two terms in the Taylor Series cancel (see example).\n\nSometimes it is wrongly claimed that formula_25 ≪ formula_3 is a sufficient condition for the binomial approximation. A simple counterexample is to let formula_27 and formula_28. In this case formula_29 but the binomial approximation yields formula_30. For small formula_25 but large formula_2, a better approximation is:\n\nConsider the following expression where formula_34 and formula_35 are real but formula_34 ≫ formula_35.\n\nThe mathematical form for the binomial approximation can be recovered by factoring out the large term formula_34 and recalling that a square root is the same as a power of one half.\n\nEvidently the expression is linear in formula_35 when formula_34 ≫ formula_35 which is otherwise not obvious from the original expression.\n\nConsider the expression:\nwhere formula_45 and formula_46 ≪ formula_3. If only the linear term from the binomial approximation is kept formula_23 then the expression unhelpfully simplifies to zero\n\nWhile the expression is small, it is not exactly zero. It is possible to extract a nonzero approximate solution by keeping the quadratic term in the Taylor Series, i.e. formula_50 so now,\n\nThis result is quadratic in formula_52 which is why it did not appear when only the linear in terms in formula_52 were kept.\n"}
{"id": "9832087", "url": "https://en.wikipedia.org/wiki?curid=9832087", "title": "Bloch space", "text": "Bloch space\n\nIn the mathematical field of complex analysis, the Bloch space, named after André Bloch and denoted formula_1 or ℬ, is the space of holomorphic functions \"f\" defined on the open unit disc D in the complex plane, such that the function \n\nis bounded. formula_1 is a Banach space, with the norm defined by\n\nThis is referred to as the Bloch norm and the elements of the Bloch space are called Bloch functions.\n"}
{"id": "2636745", "url": "https://en.wikipedia.org/wiki?curid=2636745", "title": "Brute Force: Cracking the Data Encryption Standard", "text": "Brute Force: Cracking the Data Encryption Standard\n\nBrute Force (2005, Copernicus Books ) is a book by Matt Curtin about cryptography.\n\nIn this book, the author accounts his involvement in the DESCHALL Project, mobilizing thousands of personal computers in 1997 in order to meet the challenge to crack a single message encrypted with DES.\n\nThis was and remains one of the largest collaborations of any kind on a single project in history.\n\nThe message was unencrypted on June 18 and was found to be \"Strong cryptography makes the world a safer place.\"\n\nThis is also the message of Curtin's book where he uses a personal account to reveal to the uninitiated reader some insight into a topic of growing importance which is both technically and politically complicated.\n\n"}
{"id": "19247828", "url": "https://en.wikipedia.org/wiki?curid=19247828", "title": "C. B. Collins", "text": "C. B. Collins\n\nChristopher Barry Collins is a cosmologist who has written many papers with Stephen Hawking. He is a professor emeritus of applied mathematics at the University of Waterloo.\n\nCollins earned his Ph.D. in 1972 from the University of Cambridge under the supervision of F. Gerard Friedlander.\nAmong his works with Hawking is a 1973 paper that uses the anthropic principle to provide a solution to the flatness problem.\n\n"}
{"id": "10143482", "url": "https://en.wikipedia.org/wiki?curid=10143482", "title": "Chiungtze C. Tsen", "text": "Chiungtze C. Tsen\n\nChiungtze C. Tsen (, April 2, 1898 – October 1, 1940) was a Chinese mathematician born in Nanchang, Jiangxi, who proved Tsen's theorem. He was one of Emmy Noether's students at the University of Göttingen. He returned to China in 1935. After the full-scale Japanese invasion of China in 1937, he fled and eventually settled in Xikang, where he became a professor at the newly-founded National Xikang Institute of Technology. He died of a stomach ulcer in Xichang, Xikang on October 1, 1940, and a memorial service was held on November 18, 1940. (Many Chinese sources mistakenly give his date of death as November 1940.)\nOne of his research interests was quasi-algebraic closure. In that area he proved the theorem that took his name (Tsen's theorem).\n\n\n\n\n__notoc__\n"}
{"id": "14763077", "url": "https://en.wikipedia.org/wiki?curid=14763077", "title": "Classical modal logic", "text": "Classical modal logic\n\nIn modal logic, a classical modal logic L is any modal logic containing (as axiom or theorem) the duality of the modal operators\n\nformula_1\n\nwhich is also closed under the rule\n\nformula_2\n\nAlternatively one can give a dual definition of L by which L is classical iff it contains (as axiom or theorem)\n\nformula_3\n\nand is closed under the rule\n\nformula_4\n\nThe weakest classical system is sometimes referred to as E and is non-normal. Both algebraic and neighborhood semantics characterize familiar classical modal systems that are weaker than the weakest normal modal logic K.\n\nEvery regular modal logic is classical, and every normal modal logic is regular and hence classical.\n\nChellas, Brian. \"Modal Logic: An Introduction\". Cambridge University Press, 1980.\n"}
{"id": "49700795", "url": "https://en.wikipedia.org/wiki?curid=49700795", "title": "Constant-recursive sequence", "text": "Constant-recursive sequence\n\nIn mathematics, a constant-recursive sequence or C-finite sequence is a sequence satisfying a linear recurrence with constant coefficients.\n\nAn order-\"d\" homogeneous linear recurrence with constant coefficients is an equation of the form\nwhere the \"d\" coefficients formula_2 are constants.\n\nA sequence formula_3 is a constant-recursive sequence if there is an order-\"d\" homogeneous linear recurrence with constant coefficients that it satisfies for all formula_4.\n\nEquivalently, formula_5 is constant-recursive if the set of sequences\nis contained in a vector space whose dimension is finite.\n\nThe sequence 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55, 89, ... of Fibonacci numbers satisfies the recurrence\n\nwith initial conditions\n\nExplicitly, the recurrence yields the values\n\netc.\n\nThe sequence 2, 1, 3, 4, 7, 11, 18, 29, 47, 76, 123, 199, ... of Lucas numbers satisfies the same recurrence as the Fibonacci sequence but with initial conditions \n\nMore generally, every Lucas sequence is a constant-recursive sequence.\n\nThe geometric sequence formula_16 is constant-recursive, since it satisfies the recurrence formula_17 for all formula_18.\n\nA sequence that is eventually periodic with period length formula_19 is constant-recursive, since it satisfies formula_20 for all formula_4 for some \"d\".\n\nFor any polynomial \"s\"(\"n\"), the sequence of its values is a constant-recursive sequence. If the degree of the polynomial is \"d\", the order of the associated recurrence is \"d\" + 1, with coefficients given by the corresponding element of the binomial transform. The first few such equations are\n\nA sequence obeying the order \"d\" equation also obeys all higher order equations. These identities may be proved in a number of ways, including via the theory of finite differences. Each equation may be validated by substituting formula_26 and the degree \"d\" polynomial\n\nfor an arbitrary offset formula_28, an arbitrary spacing \"h\" and arbitrary polynomial coefficients formula_29. Any given sequence of \"d\" arbitrary integer, real or complex values can be used to start an order-\"d\" constant-recursive sequence. If the given sequence of \"d\" values lies on a polynomial of degree formula_30 or less, then the constant recursive sequence also obeys a lower order equation(s).\n\nThe characteristic polynomial (or \"auxiliary polynomial\") of the recurrence is the polynomial\nwhose coefficients are the same as those of the recurrence.\nThe \"n\"th term formula_32 of a constant-recursive sequence can be written in terms of the roots of its characteristic polynomial.\nIf the \"d\" roots formula_33 are all distinct, then the \"n\"th term of the sequence is\nwhere the coefficients \"k\" are constants that can be determined by the initial conditions.\n\nFor the Fibonacci sequence, the characteristic polynomial is formula_35, whose roots formula_36 and formula_37 appear in Binet's formula\n\nMore generally, if a root \"r\" of the characteristic polynomial has multiplicity \"m\", then the term formula_39 is multiplied by a degree-formula_40 polynomial in \"n\". That is, let formula_41 be the distinct roots of the characteristic polynomial. Then\nwhere formula_43 is a polynomial of degree formula_44.\nFor instance, if the characteristic polynomial factors as formula_45, with the same root \"r\" occurring three times, then the \"n\"th term is of the form\n\nConversely, if there are polynomials formula_43 such that\nthen formula_5 is constant-recursive.\n\nA sequence is constant-recursive precisely when its generating function\nis a rational function. The denominator is the polynomial obtained from the auxiliary polynomial by reversing the order of the coefficients, and the numerator is determined by the initial values of the sequence.\n\nThe generating function of the Fibonacci sequence is\n\nIn general, multiplying a generating function by the polynomial\nyields a series\nwhere\nIf formula_32 satisfies the recurrence relation\nthen formula_57 for all formula_4. In other words,\nso we obtain the rational function\n\nIn the special case of a periodic sequence satisfying formula_61 for formula_4, the generating function is\nby expanding the geometric series.\n\nThe generating function of the Catalan numbers is not a rational function, which implies that the Catalan numbers do not satisfy a linear recurrence with constant coefficients.\n\nThe termwise addition or multiplication of two constant-recursive sequences is again constant-recursive. This follows from the characterization in terms of exponential polynomials.\n\nThe Cauchy product of two constant-recursive sequences is constant-recursive. This follows from the characterization in terms of rational generating functions.\n\nA sequence satisfying a non-homogeneous linear recurrence with constant coefficients is constant-recursive.\n\nThis is because the recurrence\ncan be solved for formula_65 to obtain\nSubstituting this into the equation\nshows that formula_32 satisfies the homogeneous recurrence\nof order formula_70.\n\nA natural generalization is obtained by relaxing the condition that the coefficients of the recurrence are constants. If the coefficients are allowed to be polynomials, then one obtains holonomic sequences.\n\nA formula_71-regular sequence satisfies linear recurrences with constant coefficients, but the recurrences take a different form. Rather than formula_32 being a linear combination of formula_73 for some integers formula_74 that are close to formula_75, each term formula_32 in a formula_71-regular sequence is a linear combination of formula_73 for some integers formula_74 whose base-formula_71 representations are close to that of formula_75. Constant-recursive sequences can be thought of as formula_82-regular sequences, where the base-1 representation of formula_75 consists of formula_75 copies of the digit formula_82.\n\n\n"}
{"id": "1316648", "url": "https://en.wikipedia.org/wiki?curid=1316648", "title": "Constructive dilemma", "text": "Constructive dilemma\n\nConstructive dilemma is a valid rule of inference of propositional logic. It is the inference that, if \"P\" implies \"Q\" and \"R\" implies \"S\" and either \"P\" or \"R\" is true, then \"Q or S\" has to be true. In sum, if two conditionals are true and at least one of their antecedents is, then at least one of their consequents must be too. \"Constructive dilemma\" is the disjunctive version of modus ponens, whereas,\ndestructive dilemma is the disjunctive version of \"modus tollens\". The rule can be stated:\n\nwhere the rule is that whenever instances of \"formula_2\", \"formula_3\", and \"formula_4\" appear on lines of a proof, \"formula_5\" can be placed on a subsequent line.\n\nThe \"constructive dilemma\" rule may be written in sequent notation:\n\nwhere formula_7 is a metalogical symbol meaning that formula_5 is a syntactic consequence of formula_2, formula_3, and formula_5 in some logical system;\n\nand expressed as a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_13, formula_14, formula_15 and formula_16 are propositions expressed in some formal system.\n\nThe dilemma derives its name because of the transfer of disjunctive operator.\n"}
{"id": "21236657", "url": "https://en.wikipedia.org/wiki?curid=21236657", "title": "Cylindrical σ-algebra", "text": "Cylindrical σ-algebra\n\nIn mathematics — specifically, in measure theory and functional analysis — the cylindrical σ-algebra is a σ-algebra often used in the study either product measure or probability measure of random variables on Banach spaces.\n\nFor a product space, the cylinder σ-algebra is the one which is generated by cylinder sets. As for products of countable length, the cylinderical σ-algebra is the product σ-algebra.\n\nIn the context of Banach space \"X\", the cylindrical σ-algebra Cyl(\"X\") is defined to be the coarsest σ-algebra (i.e. the one with the fewest measurable sets) such that every continuous linear function on \"X\" is a measurable function. In general, Cyl(\"X\") is \"not\" the same as the Borel σ-algebra on \"X\", which is the coarsest σ-algebra that contains all open subsets of \"X\".\n\n\n"}
{"id": "11067444", "url": "https://en.wikipedia.org/wiki?curid=11067444", "title": "Darboux frame", "text": "Darboux frame\n\nIn the differential geometry of surfaces, a Darboux frame is a natural moving frame constructed on a surface. It is the analog of the Frenet–Serret frame as applied to surface geometry. A Darboux frame exists at any non-umbilic point of a surface embedded in Euclidean space. It is named after French mathematician Jean Gaston Darboux.\n\nLet \"S\" be an oriented surface in three-dimensional Euclidean space E. The construction of Darboux frames on \"S\" first considers frames moving along a curve in \"S\", and then specializes when the curves move in the direction of the principal curvatures.\n\nAt each point of an oriented surface, one may attach a unit normal vector in a unique way, as soon as an orientation has been chosen for the normal at any particular fixed point. If is a curve in , parametrized by arc length, then the Darboux frame of is defined by\nThe triple defines a positively oriented orthonormal basis attached to each point of the curve: a natural moving frame along the embedded curve.\n\nNote that a Darboux frame for a curve does not yield a natural moving frame on the surface, since it still depends on an initial choice of tangent vector. To obtain a moving frame on the surface, we first compare the Darboux frame of γ with its Frenet–Serret frame. Let\n\nSince the tangent vectors are the same in both cases, there is a unique angle α such that a rotation in the plane of N and B produces the pair t and u:\n\nTaking a differential, and applying the Frenet–Serret formulas yields\n\nwhere:\n\nThis section specializes the case of the Darboux frame on a curve to the case when the curve is a principal curve of the surface (a \"line of curvature\"). In that case, since the principal curves are canonically associated to a surface at all non-umbilic points, the Darboux frame is a canonical moving frame.\n\nThe introduction of the trihedron (or \"trièdre\"), an invention of Darboux, allows for a conceptual simplification of the problem of moving frames on curves and surfaces by treating the coordinates of the point on the curve and the frame vectors in a uniform manner. A trihedron consists of a point P in Euclidean space, and three orthonormal vectors e, e, and e based at the point P. A moving trihedron is a trihedron whose components depend on one or more parameters. For example, a trihedron moves along a curve if the point P depends on a single parameter \"s\", and P(\"s\") traces out the curve. Similarly, if P(\"s\",\"t\") depends on a pair of parameters, then this traces out a surface.\n\nA trihedron is said to be adapted to a surface if P always lies on the surface and e is the oriented unit normal to the surface at P. In the case of the Darboux frame along an embedded curve, the quadruple\n\ndefines a tetrahedron adapted to the surface into which the curve is embedded.\n\nIn terms of this trihedron, the structural equations read\n\nSuppose that any other adapted trihedron \nis given for the embedded curve. Since, by definition, P remains the same point on the curve as for the Darboux trihedron, and e = u is the unit normal, this new trihedron is related to the Darboux trihedron by a rotation of the form\n\nwhere θ = θ(\"s\") is a function of \"s\". Taking a differential and applying the Darboux equation yields\n\nwhere the (ω,ω) are functions of \"s\", satisfying\n\nThe Poincaré lemma, applied to each double differential ddP, dde, yields the following Cartan structure equations. From ddP = 0,\n\nFrom dde = 0,\nThe latter are the Gauss–Codazzi equations for the surface, expressed in the language of differential forms.\n\nConsider the second fundamental form of \"S\". This is the symmetric 2-form on \"S\" given by\nBy the spectral theorem, there is some choice of frame (e) in which (\"ii\") is a diagonal matrix. The eigenvalues are the principal curvatures of the surface. A diagonalizing frame a, a, a consists of the normal vector a, and two principal directions a and a. This is called a Darboux frame on the surface. The frame is canonically defined (by an ordering on the eigenvalues, for instance) away from the umbilics of the surface.\nThe Darboux frame is an example of a natural moving frame defined on a surface. With slight modifications, the notion of a moving frame can be generalized to a hypersurface in an \"n\"-dimensional Euclidean space, or indeed any embedded submanifold. This generalization is among the many contributions of Élie Cartan to the method of moving frames.\n\nA (Euclidean) frame on the Euclidean space E is a higher-dimensional analog of the trihedron. It is defined to be an (\"n\" + 1)-tuple of vectors drawn from E, (\"v\"; \"f\", ..., \"f\"), where:\nLet \"F\"(\"n\") be the ensemble of all Euclidean frames. The Euclidean group acts on \"F\"(\"n\") as follows. Let φ ∈ Euc(\"n\") be an element of the Euclidean group decomposing as\nwhere \"A\" is an orthogonal transformation and \"x\" is a translation. Then, on a frame,\nGeometrically, the affine group moves the origin in the usual way, and it acts via a rotation on the orthogonal basis vectors since these are \"attached\" to the particular choice of origin. This is an effective and transitive group action, so \"F\"(\"n\") is a principal homogeneous space of Euc(\"n\").\n\nDefine the following system of functions \"F\"(\"n\") → E:\n\nThe projection operator \"P\" is of special significance. The inverse image of a point \"P\"(\"v\") consists of all orthonormal bases with basepoint at \"v\". In particular, \"P\" : \"F\"(\"n\") → E presents \"F\"(\"n\") as a principal bundle whose structure group is the orthogonal group O(\"n\"). (In fact this principal bundle is just the tautological bundle of the homogeneous space \"F\"(\"n\") → \"F\"(\"n\")/O(\"n\") = E.)\n\nThe exterior derivative of \"P\" (regarded as a vector-valued differential form) decomposes uniquely as\n\nfor some system of scalar valued one-forms ω. Similarly, there is an \"n\" × \"n\" matrix of one-forms (ω) such that\n\nSince the \"e\" are orthonormal under the inner product of Euclidean space, the matrix of 1-forms ω is skew-symmetric. In particular it is determined uniquely by its upper-triangular part (ω | \"i\" < \"j\"). The system of \"n\"(\"n\" + 1)/2 one-forms (ω, ω (\"i\"<\"j\")) gives an absolute parallelism of \"F\"(\"n\"), since the coordinate differentials can each be expressed in terms of them. Under the action of the Euclidean group, these forms transform as follows. Let φ be the Euclidean transformation consisting of a translation \"v\" and rotation matrix (\"A\"). Then the following are readily checked by the invariance of the exterior derivative under pullback:\n\nFurthermore, by the Poincaré lemma, one has the following structure equations\n\nLet φ : \"M\" → E be an embedding of a \"p\"-dimensional smooth manifold into a Euclidean space. The space of adapted frames on \"M\", denoted here by \"F\"(\"M\") is the collection of tuples (\"x\"; \"f\"...,\"f\") where \"x\" ∈ \"M\", and the \"f\" form an orthonormal basis of E such that \"f\"...,\"f\" are tangent to φ(\"M\") at φ(\"v\").\n\nSeveral examples of adapted frames have already been considered. The first vector T of the Frenet–Serret frame (T, N, B) is tangent to a curve, and all three vectors are mutually orthonormal. Similarly, the Darboux frame on a surface is an orthonormal frame whose first two vectors are tangent to the surface. Adapted frames are useful because the invariant forms (ω,ω) pullback along φ, and the structural equations are preserved under this pullback. Consequently, the resulting system of forms yields structural information about how \"M\" is situated inside Euclidean space. In the case of the Frenet–Serret frame, the structural equations are precisely the Frenet–Serret formulas, and these serve to classify curves completely up to Euclidean motions. The general case is analogous: the structural equations for an adapted system of frames classifies arbitrary embedded submanifolds up to a Euclidean motion.\n\nIn detail, the projection π : \"F\"(\"M\") → \"M\" given by π(\"x\"; \"f\") = \"x\" gives \"F\"(\"M\") the structure of a principal bundle on \"M\" (the structure group for the bundle is O(\"p\") × O(\"n\" − \"p\").) This principal bundle embeds into the bundle of Euclidean frames \"F\"(\"n\") by φ(\"v\";\"f\") := (φ(\"v\");\"f\") ∈ \"F\"(\"n\"). Hence it is possible to define the pullbacks of the invariant forms from \"F\"(\"n\"):\nSince the exterior derivative is equivariant under pullbacks, the following structural equations hold\n\nFurthermore, because some of the frame vectors \"f\"...\"f\" are tangent to \"M\" while the others are normal, the structure equations naturally split into their tangential and normal contributions. Let the lowercase Latin indices \"a\",\"b\",\"c\" range from 1 to \"p\" (i.e., the tangential indices) and the Greek indices μ, γ range from \"p\"+1 to \"n\" (i.e., the normal indices). The first observation is that\n\nsince these forms generate the submanifold φ(\"M\") (in the sense of the Frobenius integration theorem.)\n\nThe first set of structural equations now becomes\n\nOf these, the latter implies by Cartan's lemma that\n\nwhere \"s\" is \"symmetric\" on \"a\" and \"b\" (the second fundamental forms of φ(\"M\")). Hence, equations (1) are the Gauss formulas (see Gauss–Codazzi equations). In particular, θ is the connection form for the Levi-Civita connection on \"M\".\n\nThe second structural equations also split into the following\n\nThe first equation is the Gauss equation which expresses the curvature form Ω of \"M\" in terms of the second fundamental form. The second is the Codazzi–Mainardi equation which expresses the covariant derivatives of the second fundamental form in terms of the normal connection. The third is the Ricci equation.\n\n\n\n"}
{"id": "39173314", "url": "https://en.wikipedia.org/wiki?curid=39173314", "title": "Delzant's theorem", "text": "Delzant's theorem\n\nIn mathematics, Delzant's theorem, introduced by , classifies effective Hamiltonian actions of a torus on a compact connected symplectic manifold of twice the dimension by their image under the momentum mapping (Delzant polytope). A Delzant polytope is a convex polytope in R such that the slopes of the edges of each vertex are given by a basis of Z.\n\nAs a corollary, these symplectic manifolds have a complex structure and can be promoted as toric varieties, with invariant Kähler structures.\n"}
{"id": "7962589", "url": "https://en.wikipedia.org/wiki?curid=7962589", "title": "Deterministic context-free language", "text": "Deterministic context-free language\n\nIn formal language theory, deterministic context-free languages (DCFL) are a proper subset of context-free languages. They are the context-free languages that can be accepted by a deterministic pushdown automaton. DCFLs are always unambiguous, meaning that they admit an unambiguous grammar. There are non-deterministic unambiguous CFLs, so DCFLs form a proper subset of unambiguous CFLs.\n\nDCFLs are of great practical interest, as they can be parsed in linear time, and various restricted forms of DCFGs admit simple practical parsers. They are thus widely used throughout computer science.\n\nThe notion of the DCFL is closely related to the deterministic pushdown automaton (DPDA). It is where the language power of pushdown automatons is reduced to if we make them deterministic; the pushdown automatons become unable to choose between different state-transition alternatives and as a consequence cannot recognize all context-free languages. Unambiguous grammars do not always generate a DCFL. For example, the language of even-length palindromes on the alphabet of 0 and 1 has the unambiguous context-free grammar S → 0S0 | 1S1 | ε. An arbitrary string of this language cannot be parsed without reading all its letters first, which means that a pushdown automaton has to try alternative state transitions to accommodate for the different possible lengths of a semi-parsed string.\n\nDeterministic context-free languages can be recognized by a deterministic Turing machine in polynomial time and O(log \"n\") space; as a corollary, DCFL is a subset of the complexity class SC. The set of deterministic context-free languages is not closed under union but is closed under complement.\n\nThe languages of this class have great practical importance in computer science as they can be parsed much more efficiently than nondeterministic context-free languages. The complexity of the program and execution time of a deterministic pushdown automaton is vastly less than that of a nondeterministic one. In the naive implementation, the latter must make copies of the stack every time a nondeterministic step occurs. The best known algorithm to test membership in any context-free language is Valiant's algorithm, taking O(\"n\") time, where n is the length of the string. On the other hand, deterministic context-free languages can be accepted in O(\"n\") time by an LR(k) parser. This is very important for computer language translation because many computer languages belong to this class of languages.\n\n"}
{"id": "1102297", "url": "https://en.wikipedia.org/wiki?curid=1102297", "title": "Difference of two squares", "text": "Difference of two squares\n\nIn mathematics, the difference of two squares is a squared (multiplied by itself) number subtracted from another squared number. Every difference of squares may be factored according to the identity\n\nin elementary algebra.\n\nThe proof of the factorization identity is straightforward. Starting from the left-hand side, apply the distributive law to get \nBy the commutative law, the middle two terms cancel:\nleaving\nThe resulting identity is one of the most commonly used in mathematics. Among many uses, it gives a simple proof of the AM–GM inequality in two variables.\n\nThe proof just given indicates the scope of the identity in abstract algebra: it will hold in any commutative ring \"R\".\n\nConversely, if this identity holds in a ring \"R\" for all pairs of elements \"a\" and \"b\" of the ring, then \"R\" is commutative. To see this, apply the distributive law to the right-hand side of the original equation and get\n\nand for this to be equal to formula_6, we must have\n\nfor all pairs \"a\", \"b\" of elements of \"R\", so the ring \"R\" is commutative.\n\nThe difference of two squares can also be illustrated geometrically as the difference of two square areas in a plane. In the diagram, the shaded part represents the difference between the areas of the two squares, i.e. formula_6. The area of the shaded part can be found by adding the areas of the two rectangles; formula_9, which can be factorized to formula_10. Therefore formula_11\n\nAnother geometric proof proceeds as follows: We start with the figure shown in the first diagram below, a large square with a smaller square removed from it. The side of the entire square is a, and the side of the small removed square is b. The area of the shaded region is formula_12. A cut is made, splitting the region into two rectangular pieces, as shown in the second diagram. The larger piece, at the top, has width a and height a-b. The smaller piece, at the bottom, has width a-b and height b. Now the smaller piece can be detached, rotated, and placed to the right of the larger piece. In this new arrangement, shown in the last diagram below, the two pieces together form a rectangle, whose width is formula_13 and whose height is formula_14. This rectangle's area is formula_10. Since this rectangle came from rearranging the original figure, it must have the same area as the original figure. Therefore, formula_1.\nThe formula for the difference of two squares can be used for factoring polynomials that contain the square of a first quantity minus the square of a second quantity. For example, the polynomial formula_17 can be factored as follows:\n\nAs a second example, the first two terms of formula_19 can be factored as formula_20, so we have:\n\nMoreover, this formula can also be used for simplifying expressions:\n\nThe difference of two squares is used to find the linear factors of the \"sum\" of two squares, using complex number coefficients.\n\nFor example, the complex roots of formula_23 can be found using difference of two squares:\n\nTherefore the linear factors are formula_29 and formula_30.\n\nSince the two factors found by this method are complex conjugates, we can use this in reverse as a method of multiplying a complex number to get a real number. This is used to get real denominators in complex fractions.\n\nThe difference of two squares can also be used in the rationalising of irrational denominators. This is a method for removing surds from expressions (or at least moving them), applying to division by some combinations involving square roots.\n\nFor example:\nThe denominator of formula_31 can be rationalised as follows:\n\nHere, the irrational denominator formula_38 has been rationalised to formula_39.\n\nThe difference of two squares can also be used as an arithmetical short cut. If you are multiplying two numbers whose average is a number which is easily squared the difference of two squares can be used to give you the product of the original two numbers.\n\nFor example: \n\nWhich means using the difference of two squares formula_41 can be restated as\n\nThe difference of two consecutive perfect squares is the sum of the two bases \"n\" and \"n\"+1. This can be seen as follows:\n\nTherefore the difference of two consecutive perfect squares is an odd number. Similarly, the difference of two arbitrary perfect squares is calculated as follows:\n\nTherefore the difference of two even perfect squares is a multiple of 4 and the difference of two odd perfect squares is a multiple of 8.\n\nThe identity also holds in inner product spaces over the field of real numbers, such as for dot product of Euclidean vectors:\nThe proof is identical. By the way, assuming that and have equal norms (which means that their dot squares are equal), it demonstrates analytically the fact that two diagonals of a rhombus are perpendicular.\nIf \"a\" and \"b\" are two elements of a commutative ring \"R\", then formula_47. Note that binomial coefficients do \"not\" appear in the second factor, and the summation stops at \"n\"-1, \"not\" \"n\".\n\n\n\n"}
{"id": "1723575", "url": "https://en.wikipedia.org/wiki?curid=1723575", "title": "Eric Goles", "text": "Eric Goles\n\nEric Antonio Goles Chacc (born August 21, 1951) is a Chilean mathematician and computer scientist of Croatian descent. He studied civil engineering at the University of Chile before taking two doctorates at the University of Grenoble in France. A professor at the University of Chile, he is known for his work on cellular automata.\n\nGoles was born in Antofagasta, northern Chile.\n\nIn 1993 Goles was awarded Chile's National Prize for Exact Sciences. He was President of CONICYT (the Chilean equivalent of the National Science Foundation in the U.S.), and an advisor on science and technology to the Chilean government.\n\nGoles currently teaches and does research at the Adolfo Ibáñez University.\n\n"}
{"id": "69566", "url": "https://en.wikipedia.org/wiki?curid=69566", "title": "Euler's theorem", "text": "Euler's theorem\n\nIn number theory, Euler's theorem (also known as the Fermat–Euler theorem or Euler's totient theorem) states that if \"n\" and \"a\" are coprime positive integers, then\nwhere formula_2 is Euler's totient function. (The notation is explained in the article .) In 1736, Leonhard Euler published his proof of Fermat's little theorem, which Fermat had presented without proof. Subsequently, Euler presented other proofs of the theorem, culminating with \"Euler's theorem\" in his paper of 1763, in which he attempted to find the smallest exponent for which Fermat's little theorem was always true.\n\nThe converse of Euler's theorem is also true: if the above congruence is true, then formula_3 and formula_4 must be coprime.\n\nThe theorem is a generalization of Fermat's little theorem, and is further generalized by Carmichael's theorem.\n\nThe theorem may be used to easily reduce large powers modulo formula_4. For example, consider finding the ones place decimal digit of formula_6, i.e. formula_7. Note that 7 and 10 are coprime, and formula_8. So Euler's theorem yields formula_9, and we get formula_10.\n\nIn general, when reducing a power of formula_3 modulo formula_4 (where formula_3 and formula_4 are coprime), one needs to work modulo formula_2 in the exponent of formula_3:\n\nEuler's theorem is sometimes cited as forming the basis of the RSA encryption system, however it is insufficient (and unnecessary) to use Euler's theorem to certify the validity of RSA encryption. In RSA, the net result of first encrypting a plaintext message, then later decrypting it, amounts to exponentiating a large input number by formula_19, for some positive integer formula_20. In the case that the original number is relatively prime to formula_4, Euler's theorem then guarantees that the decrypted output number is equal to the original input number, giving back the plaintext. However, because formula_4 is a product of two distinct primes, formula_23 and formula_24, when the number encrypted is a multiple of formula_23 or formula_24, Euler's theorem does not apply and it is necessary to use the uniqueness provision of the Chinese Remainder Theorem. The Chinese Remainder Theorem also suffices in the case where the number is relatively prime to formula_4, and so Euler's theorem is neither sufficient nor necessary.\n\n1. Euler's theorem can be proven using concepts from the theory of groups: \nThe residue classes (mod \"n\") that are coprime to \"n\" form a group under multiplication (see the article Multiplicative group of integers modulo n for details). The order of that group is formula_2 where formula_29 denotes Euler's totient function. Lagrange's theorem states that the order of any subgroup of a finite group divides the order of the entire group, in this case formula_2. If formula_3 is any number coprime to formula_4 then formula_3 is in one of these residue classes, and its powers formula_34 are a subgroup. Lagrange's theorem says formula_20 must divide formula_2, i.e. there is an integer formula_37 such that formula_38. But then,\n\n2. There is also a direct proof: Let formula_40 be a reduced residue system (mod formula_4) and let formula_3 be any integer coprime to formula_4. The proof hinges on the fundamental fact that multiplication by formula_3 permutes the formula_45: in other words if formula_46 then formula_47. (This law of cancellation is proved in the article multiplicative group of integers modulo n.) That is, the sets formula_48 and formula_49, considered as sets of congruence classes (mod formula_4), are identical (as sets - they may be listed in different orders), so the product of all the numbers in formula_48 is congruent (mod formula_4) to the product of all the numbers in formula_53:\n\nThe Euler quotient of an integer \"a\" with respect to \"n\" is defined as:\n\nThe special case of Euler quotient is Fermat quotient, it happens when \"n\" is prime.\n\nA number \"n\" coprime to \"a\" which divides formula_58 is called generalized Wieferich number to base \"a\". In a special case, an odd number \"n\" which divides formula_59 is called Wieferich number.\n\nThe least base \"b\" > 1 which \"n\" is a Wieferich number are\n\n\nThe \"Disquisitiones Arithmeticae\" has been translated from Gauss's Ciceronian Latin into English and German. The German edition includes all of his papers on number theory: all the proofs of quadratic reciprocity, the determination of the sign of the Gauss sum, the investigations into biquadratic reciprocity, and unpublished notes.\n\n"}
{"id": "3077734", "url": "https://en.wikipedia.org/wiki?curid=3077734", "title": "Fenchel's duality theorem", "text": "Fenchel's duality theorem\n\nIn mathematics, Fenchel's duality theorem is a result in the theory of convex functions named after Werner Fenchel.\n\nLet \"ƒ\" be a proper convex function on R and let \"g\" be a proper concave function on R. Then, if regularity conditions are satisfied,\n\nwhere \"ƒ\" is the convex conjugate of \"ƒ\" (also referred to as the Fenchel–Legendre transform) and \"g\" is the concave conjugate of \"g\". That is,\n\nLet \"X\" and \"Y\" be Banach spaces, formula_4 and formula_5 be convex functions and formula_6 be a bounded linear map. Then the Fenchel problems:\nsatisfy weak duality, i.e. formula_9. Note that formula_10 are the convex conjugates of \"f\",\"g\" respectively, and formula_11 is the adjoint operator. The perturbation function for this dual problem is given by formula_12.\n\nSuppose that \"f\",\"g\", and \"A\" satisfy either\nThen strong duality holds, i.e. formula_19. If formula_20 then supremum is attained.\n\nIn the following figure, the minimization problem on the left side of the equation is illustrated. One seeks to vary \"x\" such that the vertical distance between the convex and concave curves at \"x\" is as small as possible. The position of the vertical line in the figure is the (approximate) optimum.\n\nThe next figure illustrates the maximization problem on the right hand side of the above equation. Tangents are drawn to each of the two curves such that both tangents have the same slope \"p\". The problem is to adjust \"p\" in such a way that the two tangents are as far away from each other as possible (more precisely, such that the points where they intersect the y-axis are as far from each other as possible). Imagine the two tangents as metal bars with vertical springs between them that push them apart and against the two parabolas that are fixed in place.\n\nFenchel's theorem states that the two problems have the same solution. The points having the minimum vertical separation are also the tangency points for the maximally separated parallel tangents.\n\n"}
{"id": "47321247", "url": "https://en.wikipedia.org/wiki?curid=47321247", "title": "Glaeser's continuity theorem", "text": "Glaeser's continuity theorem\n\nIn mathematical analysis, Glaeser's continuity theorem, is a characterization of the continuity of the derivative of the square roots of functions of class formula_1. It was introduced in 1963 by Georges Glaeser, and was later simplified by Jean Dieudonné.\n\nThe theorem states: Let formula_2 be a function of class formula_3 in an open set \"U\" contained in formula_4, then formula_5 is of class formula_6 in \"U\" if and only if its partial derivatives of first and second order vanish in the zeros of \"f\".\n"}
{"id": "20364166", "url": "https://en.wikipedia.org/wiki?curid=20364166", "title": "Handedness and mathematical ability", "text": "Handedness and mathematical ability\n\nResearchers have suggested a link between handedness and ability with mathematics. This link has been proposed by Geschwind, Galaburda, Annett, and Kilshaw. The suggested link is that a brain without extreme bias towards locating language in the left hemisphere would have an advantage in mathematical ability.\nA 1967 study by Douglas, found no evidence to correlate mathematical ability with left-handedness or ambidexterity. The study compared the people who came in the top 15% of a mathematics examination with those of moderate mathematical ability, and found that the two groups' handedness preferences were similar. However, it did find that those who came lowest in the test had mixed hand preferences. A study in 1979 by Peterson found a trend towards \"low\" rates of left-handedness in science students.\n\nA 1980 study by Jones and Bell also obtained negative results. This study compared the handedness of a group of engineering students with strong mathematics skills against the handedness of a group of psychology students (of varying mathematics skills). In both cases, the distribution of handedness resembled that of the general population.\n\nAnnett and Kilshaw themselves support their hypothesis with several examples, including a handedness questionnaire given to undergraduates. Annett observes that studies that depend from voluntary returns of a handedness questionnaire are going to be biased towards left-handedness, and notes that this was a weakness of the study. However, the results were that there were significantly more left-handers amongst male mathematics undergraduates than male non-mathematics undergraduates (21% versus 11%) and significantly more non-right-handers (44% versus 24%), and that there was a similar but smaller left-handedness difference for female undergraduates (11% versus 8%). Annett reports the results of this study as being consistent with the hypothesis, for explaining the cause of handedness, of an absent genetic right-shift factor.\n\nOther examples used by Annett include a study that observed the hand use of teachers of mathematics and other sciences from various universities and polytechnics, as they underwent a personal interview, compared to a control group comprising non-mathematics teachers. Again, a statistically significant difference was found for males, and again Annett states this to be consistent with the right-shift model. Further examples are a 1986 study by Benbow and a 1990 study by Temple of staff at the University of Oxford, which, Annett states, show not that there is a predominance of left-handers in talented groups, whether that talent be with mathematics or otherwise, but rather that there is a shortfall in such groups of people who are strongly right-handed.\n\nA study by C. P. Benbow did not work to prove the mathematical abilities of study participants who are left-hand dominant but to prove the weakness in those who are right-hand dominant. Using a series of questions that relate left-handedness and mathematical giftedness, Benbow was able to base their team's conclusion off of a series of questions that associated hand dominance and mathematical ability. \n\nAs a controversial subject, the debate over the link between handedness and mathematical abilities is ongoing. Researchers at the University of Liverpool concluded that there is a moderate, yet significant correlation between mathematical skills and handedness. \n\n "}
{"id": "1321050", "url": "https://en.wikipedia.org/wiki?curid=1321050", "title": "Homotopical algebra", "text": "Homotopical algebra\n\nIn mathematics, homotopical algebra is a collection of concepts comprising the \"nonabelian\" aspects of homological algebra as well as possibly the abelian aspects as special cases. The \"homotopical\" nomenclature stems from the fact that a common approach to such generalizations is via abstract homotopy theory, as in nonabelian algebraic topology, and in particular the theory of closed model categories.\n\nThis subject has received much attention in recent years due to new foundational work of Voevodsky, Friedlander, Suslin, and others resulting in the A homotopy theory for quasiprojective varieties over a field. Voevodsky has used this new algebraic homotopy theory to prove the Milnor conjecture (for which he was awarded the Fields Medal) and later, in collaboration with M. Rost, the full Bloch-Kato conjecture.\n\n\n\n"}
{"id": "488391", "url": "https://en.wikipedia.org/wiki?curid=488391", "title": "Jacobson density theorem", "text": "Jacobson density theorem\n\nIn mathematics, more specifically non-commutative ring theory, modern algebra, and module theory, the Jacobson density theorem is a theorem concerning simple modules over a ring .\n\nThe theorem can be applied to show that any primitive ring can be viewed as a \"dense\" subring of the ring of linear transformations of a vector space. This theorem first appeared in the literature in 1945, in the famous paper \"Structure Theory of Simple Rings Without Finiteness Assumptions\" by Nathan Jacobson. This can be viewed as a kind of generalization of the Artin-Wedderburn theorem's conclusion about the structure of simple Artinian rings.\n\nLet be a ring and let be a simple right -module. If is a non-zero element of , (where is the cyclic submodule of generated by ). Therefore, if are non-zero elements of , there is an element of that induces an endomorphism of transforming to . The natural question now is whether this can be generalized to arbitrary (finite) tuples of elements. More precisely, find necessary and sufficient conditions on the tuple and separately, so that there is an element of with the property that for all . If is the set of all -module endomorphisms of , then Schur's lemma asserts that is a division ring, and the Jacobson density theorem answers the question on tuples in the affirmative, provided that the are linearly independent over .\n\nWith the above in mind, the theorem may be stated this way:\n\nIn the Jacobson density theorem, the right -module is simultaneously viewed as a left -module where , in the natural way: . It can be verified that this is indeed a left module structure on . As noted before, Schur's lemma proves is a division ring if is simple, and so is a vector space over .\n\nThe proof also relies on the following theorem proven in p. 185:\n\nWe use induction on . If is empty, then the theorem is vacuously true and the base case for induction is verified.\n\nAssume is non-empty, let be an element of and write If is any -linear transformation on , by the induction hypothesis there exists such that for all in . Write . It is easily seen that is a submodule of . If , then the previous theorem implies that would be in the -span of , contradicting the -linear independence of , therefore . Since is simple, we have: . Since , there exists in such that .\n\nDefine and observe that for all in we have:\n\nNow we do the same calculation for :\n\nTherefore, for all in , as desired. This completes the inductive step of the proof. It follows now from mathematical induction that the theorem is true for finite sets of any size.\n\nA ring is said to act densely on a simple right -module if it satisfies the conclusion of the Jacobson density theorem. There is a topological reason for describing as \"dense\". Firstly, can be identified with a subring of by identifying each element of with the linear transformation it induces by right multiplication. If is given the discrete topology, and if is given the product topology, and is viewed as a subspace of and is given the subspace topology, then acts densely on if and only if is dense set in with this topology.\n\nThe Jacobson density theorem has various important consequences in the structure theory of rings. Notably, the Artin–Wedderburn theorem's conclusion about the structure of simple right Artinian rings is recovered. The Jacobson density theorem also characterizes right or left primitive rings as dense subrings of the ring of -linear transformations on some -vector space , where is a division ring.\n\nThis result is related to the Von Neumann bicommutant theorem, which states that, for a *-algebra of operators on a Hilbert space , the double commutant can be approximated by on any given finite set of vectors. See also the Kaplansky density theorem in the von Neumann algebra setting.\n\n\n"}
{"id": "4437762", "url": "https://en.wikipedia.org/wiki?curid=4437762", "title": "Libelle (cipher)", "text": "Libelle (cipher)\n\nLibelle is a German cipher system, developed by the Federal Office for Information Security. The algorithm is not published, in an attempt to make cryptanalysis more difficult (following the principle of security through obscurity and against Kerckhoffs's principle). In order to keep the algorithm secret, it is only distributed as a microchip named Jupiter and not as a piece of software.\n\nIt is possible to have a secret cipher algorithm while still reaping the benefits of public cryptography research: one way is to make a non-weakening change to a public algorithm, like changing the Nothing up my sleeve numbers, or chaining the public cipher with an unrelated secret cipher.\n\nA BSI specification document about Pluto gives these numbers: 64 bit block size, 160 bit key size. Supported modes: Electronic Codebook (ECB), Cipher Block Chaining (CBC), Cipher Feedback (CFB)/Output Feedback (OFB) with 1,8,16,32 or 64 bits.\n\n"}
{"id": "31765232", "url": "https://en.wikipedia.org/wiki?curid=31765232", "title": "List of mathematical concepts named after places", "text": "List of mathematical concepts named after places\n\nThis list contains mathematical concepts named after geographic locations.\n"}
{"id": "38018016", "url": "https://en.wikipedia.org/wiki?curid=38018016", "title": "List of things named after Pierre de Fermat", "text": "List of things named after Pierre de Fermat\n\nThis is a list of things named after Pierre de Fermat, a French amateur mathematician.\n\n"}
{"id": "1421466", "url": "https://en.wikipedia.org/wiki?curid=1421466", "title": "Mediant (mathematics)", "text": "Mediant (mathematics)\n\nIn mathematics, the mediant of two fractions, generally made up of four positive integers\n\nThat is to say, the numerator and denominator of the mediant are the sums of the numerators and denominators of the given fractions, respectively. It is sometimes called the freshman sum, as it is a common mistake in the early stages of learning about addition of fractions.\n\nTechnically, this is a binary operation on valid fractions (nonzero denominator), considered as ordered pairs of appropriate integers, a priori disregarding the perspective on rational numbers as equivalence classes of fractions. For example, the mediant of the fractions 1/1 and 1/2 is 2/3. However, if the fraction 1/1 is replaced by the fraction 2/2, which is an equivalent fraction denoting the same rational number 1, the mediant of the fractions 2/2 and 1/2 is 3/4. For a stronger connection to rational numbers the fractions may be required to be reduced to lowest terms, thereby selecting unique representatives from the respective equivalence classes.\n\nThe Stern-Brocot tree provides an enumeration of all positive rational numbers via mediants in lowest terms, obtained purely by iterative computation of the mediant according to a simple algorithm.\n\n\n\n\n"}
{"id": "23535913", "url": "https://en.wikipedia.org/wiki?curid=23535913", "title": "Mihalis Dafermos", "text": "Mihalis Dafermos\n\nMihalis Constantine Dafermos (Greek: Μιχάλης Δαφέρμος; born October 1976) is a Greek mathematician. He is a Lowndean Professor of Astronomy and Geometry at the University of Cambridge.\n\nHe studied mathematics at Harvard University and was awarded a BA in 1997. His PhD thesis titled \"Stability and Instability of the Cauchy Horizon for the Spherically Symmetric Einstein-Maxwell-Scalar Field Equations\" was written under the supervision of Demetrios Christodoulou at Princeton University.\n\nHe has won the Adams Prize writing on the subject \"Differential Equations\" in 2004 and the Whitehead Prize in 2009 for \"his work on the rigorous analysis of hyperbolic partial differential equations in general relativity.\" In 2015 he was elected as a fellow of the American Mathematical Society.\n\n"}
{"id": "11911464", "url": "https://en.wikipedia.org/wiki?curid=11911464", "title": "Model of hierarchical complexity", "text": "Model of hierarchical complexity\n\nThe model of hierarchical complexity is a framework for scoring how complex a behavior is, such as verbal reasoning or other cognitive tasks. It quantifies the order of hierarchical complexity of a task based on mathematical principles of how the information is organized, in terms of information science. This model has been developed by Michael Commons and others since the 1980s.\n\nThe model of hierarchical complexity (MHC) is a formal theory and a mathematical psychology framework for scoring how complex a behavior is. Developed by Michael Lamport Commons and colleagues, it quantifies the order of hierarchical complexity of a task based on mathematical principles of how the information is organized, in terms of information science. Its forerunner was the general stage model.\n\nBehaviors that may be scored include those of individual humans or their social groupings (e.g., organizations, governments, societies), animals, or machines. It enables scoring the hierarchical complexity of task accomplishment in any domain. It is based on the very simple notions that higher order task actions:\n\nIt is cross-culturally and cross-species valid. The reason it applies cross-culturally is that the scoring is based on the mathematical complexity of the hierarchical organization of information. Scoring does not depend upon the content of the information (e.g., what is done, said, written, or analyzed) but upon how the information is organized.\n\nThe MHC is a non-mentalistic model of developmental stages. It specifies 16 orders of hierarchical complexity and their corresponding stages. It is different from previous proposals about developmental stage applied to humans; instead of attributing behavioral changes across a person's age to the development of mental structures or schema, this model posits that task sequences of task behaviors form hierarchies that become increasingly complex. Because less complex tasks must be completed and practiced before more complex tasks can be acquired, this accounts for the developmental changes seen, for example, in individual persons' performance of complex tasks. (For example, a person cannot perform arithmetic until the numeral representations of numbers are learned. A person cannot operationally multiply the sums of numbers until addition is learned).\n\nThe creators of the MHC claim that previous theories of stage have confounded the stimulus and response in assessing stage by simply scoring responses and ignoring the task or stimulus. The MHC separates the task or stimulus from the performance. The participant's performance on a task of a given complexity represents the stage of developmental complexity.\n\nOne major basis for this developmental theory is task analysis. The study of ideal tasks, including their instantiation in the real world, has been the basis of the branch of stimulus control called psychophysics. Tasks are defined as sequences of contingencies, each presenting stimuli and each requiring a behavior or a sequence of behaviors that must occur in some non-arbitrary fashion. The complexity of behaviors necessary to complete a task can be specified using the horizontal complexity and vertical complexity definitions described below. Behavior is examined with respect to the analytically-known complexity of the task.\n\nTasks are quantal in nature. They are either completed correctly or not completed at all. There is no intermediate state (\"tertium non datur\"). For this reason, the model characterizes all stages as P-hard and functionally distinct. The orders of hierarchical complexity are quantized like the electron atomic orbitals around the nucleus: each task difficulty has an order of hierarchical complexity required to complete it correctly, analogous to the atomic Slater determinant. Since tasks of a given quantified order of hierarchical complexity require actions of a given order of hierarchical complexity to perform them, the stage of the participant's task performance is equivalent to the order of complexity of the successfully completed task. The quantal feature of tasks is thus particularly instrumental in stage assessment because the scores obtained for stages are likewise discrete.\n\nEvery task contains a multitude of subtasks. When the subtasks are carried out by the participant in a required order, the task in question is successfully completed. Therefore, the model asserts that all tasks fit in some configured sequence of tasks, making it possible to precisely determine the hierarchical order of task complexity. Tasks vary in complexity in two ways: either as \"horizontal\" (involving classical information); or as \"vertical\" (involving hierarchical information).\n\nClassical information describes the number of \"yes–no\" questions it takes to do a task. For example, if one asked a person across the room whether a penny came up heads when they flipped it, their saying \"heads\" would transmit 1 bit of \"horizontal\" information. If there were 2 pennies, one would have to ask at least two questions, one about each penny. Hence, each additional 1-bit question would add another bit. Let us say they had a four-faced top with the faces numbered 1, 2, 3, and 4. Instead of spinning it, they tossed it against a backboard as one does with dice in a game of craps. Again, there would be 2 bits. One could ask them whether the face had an even number. If it did, one would then ask if it were a 2. Horizontal complexity, then, is the sum of bits required by just such tasks as these.\n\nHierarchical complexity refers to the number of recursions that the coordinating actions must perform on a set of primary elements. Actions at a higher order of hierarchical complexity: (a) are defined in terms of actions at the next lower order of hierarchical complexity; (b) organize and transform the lower-order actions (see Figure 2); (c) produce organizations of lower-order actions that are qualitatively new and not arbitrary, and cannot be accomplished by those lower-order actions alone. Once these conditions have been met, we say the higher-order action coordinates the actions of the next lower order.\n\nTo illustrate how lower actions get organized into more hierarchically complex actions, let us turn to a simple example. Completing the entire operation 3 × (4 + 1) constitutes a task requiring the distributive act. That act non-arbitrarily orders adding and multiplying to coordinate them. The distributive act is therefore one order more hierarchically complex than the acts of adding and multiplying alone; it indicates the singular proper sequence of the simpler actions. Although simply adding results in the same answer, people who can do both display a greater freedom of mental functioning. Additional layers of abstraction can be applied. Thus, the order of complexity of the task is determined through analyzing the demands of each task by breaking it down into its constituent parts.\n\nThe hierarchical complexity of a task refers to the number of concatenation operations it contains, that is, the number of recursions that the coordinating actions must perform. An order-three task has three concatenation operations. A task of order three operates on one or more tasks of vertical order two and a task of order two operates on one or more tasks of vertical order one (the simplest tasks).\n\nStage theories describe human organismic and/or technological evolution as systems that move through a pattern of distinct stages over time. Here development is described formally in terms of the model of hierarchical complexity (MHC).\n\nSince actions are defined inductively, so is the function \"h\", known as the order of the hierarchical complexity. To each action \"A\", we wish to associate a notion of that action's hierarchical complexity, \"h(A)\". Given a collection of actions A and a participant \"S\" performing A, the \"stage of performance\" of \"S\" on A is the highest order of the actions in A completed successfully at least once, i.e., it is: stage (\"S\", A) = max{\"h(A)\" | \"A\" ∈ A and \"A\" completed successfully by \"S\"}. Thus, the notion of stage is discontinuous, having the same transitional gaps as the orders of hierarchical complexity. This is in accordance with previous definitions.\n\nBecause MHC stages are conceptualized in terms of the hierarchical complexity of tasks rather than in terms of mental representations (as in Piaget's stages), the highest stage represents successful performances on the most hierarchically complex tasks rather than intellectual maturity.\n\nThe following table gives descriptions of each stage in the MHC.\n\nThere are some commonalities between the Piagetian and Commons' notions of stage and many more things that are different. In both, one finds:\nWhat Commons et al. (1998) have added includes:\nThis makes it possible for the model's application to meet real world requirements, including the empirical and analytic. Arbitrary organization of lower order of complexity actions, possible in the Piagetian theory, despite the hierarchical definition structure, leaves the functional correlates of the interrelationships of tasks of differential complexity formulations ill-defined.\n\nMoreover, the model is consistent with the neo-Piagetian theories of cognitive development. According to these theories, progression to higher stages or levels of cognitive development is caused by increases in processing efficiency and working memory capacity. That is, higher-order stages place increasingly higher demands on these functions of information processing, so that their order of appearance reflects the information processing possibilities at successive ages.\n\nThe following dimensions are inherent in the application:\n\nThe MHC specifies 16 orders of hierarchical complexity and their corresponding stages, positing that each of Piaget's substages, in fact, are robustly hard stages. The MHC adds five postformal stages to Piaget's developmental trajectory: systematic stage 12, metasystematic stage 13, paradigmatic stage 14, cross-paradigmatic stage 15, and meta-cross-paradigmatic stage 16. It may be the Piaget's \"consolidate\" formal stage is the same as the \"systematic\" stage. The sequence is as follows: (0) calculatory, (1) automatic, (2) sensory & motor, (3) circular sensory-motor, (4) sensory-motor, (5) nominal, (6) sentential, (7) preoperational, (8) primary, (9) concrete, (10) abstract, (11) formal, and the five postformal: (12) systematic, (13) metasystematic, (14) paradigmatic, (15) cross-paradigmatic, and (16) meta-cross-paradigmatic. The first four stages (0–3) correspond to Piaget's sensorimotor stage at which infants and very young children perform. Adolescents and adults can perform at any of the subsequent stages. MHC stages 4 through 5 correspond to Piaget's pre-operational stage; 6 through 8 correspond to his concrete operational stage; and 9 through 11 correspond to his formal operational stage.\n\nMore complex behaviors characterize multiple system models. The four highest stages in the MHC are not represented in Piaget's model. The higher stages of the MHC have extensively influenced the field of positive adult development. Some adults are said to develop alternatives to, and perspectives on, formal operations; they use formal operations within a \"higher\" system of operations. Some theorists call the more complex orders of cognitive tasks \"postformal thought\", but other theorists argue that these higher orders cannot exactly be labelled as postformal thought.\n\nJordan (2018) argued that unidimensional models such as the MHC, which measure level of complexity of some behavior, refer to only one of many aspects of adult development, and that other variables are needed (in addition to unidimensional measures of complexity) for a fuller description of adult development.\n\nThe MHC has a broad range of applicability. Its mathematical foundation permits it to be used by anyone examining task performance that is organized into stages. It is designed to assess development based on the order of complexity which the actor utilizes to organize information. The model thus allows for a standard quantitative analysis of developmental complexity in any cultural setting. Other advantages of this model include its avoidance of mentalistic explanations, as well as its use of quantitative principles which are universally applicable in any context.\n\nThe following practitioners can use the MHC to quantitatively assess developmental stages:\n\nIn one representative study, Commons, Goodheart, and Dawson (1997) found, using Rasch analysis (Rasch, 1980), that hierarchical complexity of a given task predicts stage of a performance, the correlation being r = 0.92. Correlations of similar magnitude have been found in a number of the studies. The following are examples of tasks studied using the model of hierarchical complexity or Kurt W. Fischer's similar skill theory:\n\nAs of 2014, people and institutes from all the major continents of the world, except Africa, have used the model of hierarchical complexity. Because the model is very simple and is based on analysis of tasks and not just performances, it is dynamic. With the help of the model, it is possible to quantify the occurrence and progression of transition processes in task performances at any order of hierarchical complexity.\n\nThe descriptions of stages 13–15 have been described as insufficiently precise.\n\n"}
{"id": "1661158", "url": "https://en.wikipedia.org/wiki?curid=1661158", "title": "Mughal architecture", "text": "Mughal architecture\n\n\"Mughal Architecture\" is the type of Indo-Islamic architecture developed by the Mughals in the 16th, 17th and 18th centuries throughout the ever-changing extent of their empire in the Indian subcontinent. It developed the styles of earlier Muslim dynasties in India as an amalgam of Islamic, Persian, Turkish and Indian architecture. Mughal buildings have a uniform pattern of structure and character, including large bulbous domes, slender minarets at the corners, massive halls, large vaulted gateways and delicate ornamentation. Examples of the style can be found in India, Afghanistan, Bangladesh and Pakistan.\n\nThe Mughal dynasty was established after the victory of Babur at Panipat in 1526. During his five-year reign, Babur took considerable interest in erecting buildings, though few have survived. His grandson Akbar built widely, and the style developed vigorously during his reign. Among his accomplishments were Agra Fort, the fort-city of Fatehpur Sikri, and the Buland Darwaza. Akbar's son Jahangir commissioned the Shalimar Gardens in Kashmir.\n\nMughal architecture reached its zenith during the reign of Shah Jahan, who constructed the Taj Mahal, the Jama Masjid, the Red Fort, and the Shalimar Gardens in Lahore. The end of his reign corresponded with the decline of Mughal architecture and the Empire itself.\n\nAgra fort is a UNESCO world heritage site in Agra, Uttar Pradesh. The major part of Agra fort was built by Akbar The Great during 1565 to 1574. The architecture of the fort clearly indicates the free adoption of the Rajput planning and construction. Some of the important buildings in the fort are Jahangiri Mahal built for Jahangir and his family, the Moti Masjid, and Mena Bazaars. The Jahangir Mahal is an impressive structure and has a courtyard surrounded by double-storeyed halls and rooms.\n\nHumayun's tomb is the tomb of the Mughal Emperor Humayun in Delhi, India. The tomb was commissioned by Humayun's first wife and chief consort, Empress Bega Begum (also known as Haji Begum), in 1569-70, and designed by Mirak Mirza Ghiyas and his son, Sayyid Muhammad, Persian architects chosen by her. It was the first garden-tomb on the Indian subcontinent. It is often regarded as the first mature example of Mughal architecture.\n\nAkbar’s greatest architectural achievement was the construction of Fatehpur Sikri, his capital city near Agra at a trade and Jain pilgrimage. The construction of the walled city was started in 1569 and completed in 1574.\n\nIt contained some of the most beautiful buildings – both religious and secular which testify to the Emperor’s aim of achieving social, political and religious integration. The main religious buildings were the huge Jama Masjid and small tomb of Salim Chisti. The tomb, built in 1571 in the corner of the mosque compound, is a square marble chamber with a verandah. The cenotaph has an exquisitely designed lattice screen around it. Buland Darwaza, also known as the Gate of Magnificence, was built by Akbar in 1576 to commemorate his victory over Gujarat and the Deccan. It is 40 metres high and 50 metres from the ground. The total height of the structure is about 54 metres from ground level...\n\nThe Haramsara, the royal seraglio in Fatehpur Sikri was an area where the royal women lived. The opening to the Haramsara is from the Khwabgah side separated by a row of cloiters. According to Abul Fazl, in Ain-i-Akbari, the inside of Harem was guarded by senior and active women, outside the enclosure the eunuchs were placed, and at a proper distance there were faithful Rajput guards.\n\nThis is the largest palace in the Fatehpur Sikri seraglio, connected to the minor \"haramsara\" (where the less important harem ladies and maids would have resided) quarters. The main entrance is double storied, projecting out of the facade to create a kind of porch leading into a recessed entrance with a balcony. Inside there is a quadrangle surrounded by rooms. The columns of rooms are ornamented with a variety of Hindu sculptural motifs. \nThe glazed tiles on the roofs from Multan have an eye catching shade of turquoise. The mosque was built in honour of Jodha Bai, mother of Jahangir and wife of Akbar. Her Mughal name was Mariyam Zamani Begum and this being the reason that the mosque was built in her honor in Lahore’s walled city. Jahangir built his mother Mariyam Zamani Begum’s mosque and is just 1 km away from the tomb of Akbar near Agra at a place called Sikandra.\n\nBuland Darwaza dominates the landscape. Historian `Abd al-Qadir Bada'uni writes that it was the highest gateway in Hindustan at that time until today.\n\nA chronogram is inscribed on the central archway composed by Ashraf Khan, one of Akbar's principal secretaries that reads,\nThe Tomb of Sheikh Salim Chishti is famed as one of the finest examples of Mughal architecture in India, built during the years 1580 and 1581, along with the imperial complex at Situated near Zenana Rauza and facing south towards Buland Darwaza, within the quadrangle of the Jama Masjid which measures 350 ft. by 440 ft. It enshrines the burial place of the Sufi saint, Salim Chisti (1478 – 1572), a descendant of Khwaja Moinuddin Chishti of Ajmer, and lived in a cavern on the ridge at Sikri. The mausoleum, constructed by Akbar as a mark of his respect for the Sufi saint, who foretold the birth of his son, who was named Prince Salim after him and later succeeded Akbar to the throne of the Mughal Empire.\n\nThe Wazir Khan Mosque in Lahore was commissioned during the reign of Shah Jahan, and is famous for its rich embellishment which covers almost every interior surface.\nRather than building a huge monuments like his predecessors to demonstrate their power, Shah Jahan built elegant monuments. The force and originality of this previous building style gave way under Shah Jahan to a delicate elegance and refinement of detail, illustrated in the palaces erected during his reign at Agra, Delhi and Lahore. Some examples include the Taj Mahal at Agra, the tomb of his wife Mumtaz Mahal. The Moti Masjid (Pearl Mosque) in the Lahore Fort and the Jama Masjid at Delhi are imposing buildings of his era, and their position and architecture have been carefully considered so as to produce a pleasing effect and feeling of spacious elegance and well-balanced proportion of parts. Shah Jahan also built sections of the Sheesh Mahal, and Naulakha pavilion, which are all enclosed in the fort. He also built a mosque named after himself in Thatta called Shahjahan Mosque. Shah Jahan also built the Red Fort in his new capital at Shah Jahanabad, now Delhi. The red sandstone Red Fort is noted for its special buildings-Diwan-i-Aam and Diwan-i-Khas. Another mosque was built during his tenure in Lahore called Wazir Khan Mosque, by Shaikh Ilm-ud-din Ansari who was the court physician to the emperor.\n\nThe Taj Mahal, a World Heritage Site described as the \"teardrop on the cheek of time\" by Rabindranath Tagore, was built between 1630–49 by the emperor Shah Jahan in memory of his wife Mumtaz Mahal.(Mumtaz died after her 14th delivery). Its construction took 22 years and required 22,000 laborers and 1,000 elephants. Built entirely of white marble at a cost of $1018200000, it is one of the New7Wonders of the World. The building's longest plane of symmetry runs through the entire complex except for the sarcophagus of Shah Jahan, which is placed off centre in the crypt room below the main floor. This symmetry is extended to the building of an entire mirror mosque in red sandstone, to complement the Mecca-facing mosque placed to the west of the main structure. Shah Jahan used \"pietra dura\", a method of decoration on a large scale-inlaid work of jewels.\n\nThe Wazir Khan Masjid was commissioned during the reign of the Mughal Emperor Shah Jahan in 1634, and completed in 1642. Considered to be the most ornately decorated Mughal-era mosque. Wazir Khan Masjid is renowned for its intricate faience tile work known as \"kashi-kari\", as well as its interior surfaces that are almost entirely embellished with elaborate Mughal-era frescoes. The mosque has been under extensive restoration since 2009 under the direction of the Aga Khan Trust for Culture and the Government of Punjab.\n\nThe Shalimar Gardens (1641–1642) built on the orders of Bahadur Shah Zafar in Lahore, Pakistan, is also on the UNESCO world heritage list.\n\nThe Shah Jahan Mosque is the central mosque for the city of Thatta, in the Pakistani province of Sindh. The mosque commissioned by Shah Jahan, who bestowed it to the city as a token of gratitude. Its style is heavily influenced by Central Asian Timurid architecture, which was introduced after Shah Jahan's campaigns near Balkh and Samarkand. The mosque is considered to have the most elaborate display of tile work in South Asia, and is also notable for its geometric brick work - a decorative element that is unusual for Mughal-period mosques.\n\nIn Aurangzeb's reign (1658–1707) squared stone and marble was replaced by brick or rubble with stucco ornament. Srirangapatna and Lucknow have examples of later Indo-Mughal architecture. He made additions to the Lahore Fort and also built one of the thirteen gates which was later named after him (Alamgir).\n\nThe Badshahi Masjid in Lahore, Pakistan was commissioned by the sixth Mughal Emperor Aurangzeb. Constructed between 1671 and 1673, it was the largest mosque in the world upon construction. It is the third largest mosque in Pakistan and the seventh largest mosque in the world. The mosque is adjacent to the Lahore Fort and is the last in the series of congregational mosques in red sandstone. The red sandstone of the walls contrasts with the white marble of the domes and the subtle intarsia decoration. Aurangzeb's mosque's architectural plan is similar to that of his father, Shah Jahan, the Jama Masjid in Delhi; though it is much larger. It also functions as an \"idgah\". The courtyard which spreads over 276,000 square feet, can accommodate one hundred thousand worshippers; ten thousand can be accommodated inside the mosque. The minarets are tall. The Mosque is one of the most famous Mughal structures, but suffered greatly under the reign of Maharaja Ranjit Singh. In 1993, the Government of Pakistan included the Badshahi Mosque in the tentative list for UNESCO World Heritage Site.\n\nAdditional monuments from this period are associated with women from Aurangzeb's imperial family. The construction of the elegant Zinat al-Masjid in Daryaganj was overseen by Aurangzeb's second daughter Zinat-al-Nissa. Aurangzeb's sister Roshan-Ara who died in 1671. The tomb of Roshanara Begum and the garden surrounding it were neglected for a long time and are now in an advanced state of decay. Bibi Ka Maqbara was a mausoleum built by Prince Azam Shah, son of Emperor Aurangzeb, in the late 17th century as a loving tribute to his mother, Dilras Bano Begam in Aurangabad, Maharashtra. The Alamgiri Gate, built in 1673, is the main entrance to the Lahore Fort in present-day Lahore. It was constructed to face west towards the Badshahi Mosque in the days of the Mughal Emperor Aurangzeb.\n\nAnother construction of the Mughal era is Lalbagh Fort (also known as \"Fort Aurangabad\"), a Mughal palace fortress at the Buriganga River in the southwestern part of Dhaka, Bangladesh, whose construction started in 1678 during the reign of Aurangzeb.\n\nMughal gardens are gardens built by the Mughals in the Islamic style of architecture. This style was influenced by Persian gardens and Timurid gardens. Significant use of rectilinear layouts are made within the walled enclosures. Some of the typical features include pools, fountains and canals inside the gardens. The famous gardens are the Char Bagh gardens at Taj Mahal, gardens at Humayun's Tomb Shalimar Gardens of Lahore, Delhi and Kashmir as well as Pinjore Garden in Haryana.\n\nShahi Bridge, Jaunpur was constructed during the reign of the Mughal Emperor Akbar.\n\n"}
{"id": "1471697", "url": "https://en.wikipedia.org/wiki?curid=1471697", "title": "Non-linear sigma model", "text": "Non-linear sigma model\n\nIn quantum field theory, a nonlinear \"σ\" model describes a scalar field which takes on values in a nonlinear manifold called the target manifold  \"T\". The non-linear \"σ\"-model was introduced by , who named it after a field corresponding to a spinless meson called \"σ\" in their model.\n\nThe target manifold \"T\" is equipped with a Riemannian metric \"g\". is a differentiable map from Minkowski space \"M\" (or some other space) to \"T\".\n\nThe Lagrangian density in contemporary chiral form is given by\nwhere we have used a + − − − metric signature and the partial derivative is given by a section of the jet bundle of \"T\"×\"M\" and is the potential.\n\nIn the coordinate notation, with the coordinates , \"a\" = 1, ..., \"n\" where \"n\" is the dimension of \"T\",\n\nIn more than two dimensions, nonlinear \"σ\" models contain a dimensionful coupling constant and are thus not perturbatively renormalizable.\nNevertheless, they exhibit a non-trivial ultraviolet fixed point of the renormalization group both in the lattice formulation and in the double expansion originally proposed by Kenneth G. Wilson. \n\nIn both approaches, the non-trivial renormalization-group fixed point found for the \"O(n)\"-symmetric model is seen to simply describe, in dimensions greater than two, the critical point separating the ordered from the disordered phase. In addition, the improved lattice or quantum field theory predictions can then be compared to laboratory experiments on critical phenomena, since the \"O(n)\" model describes physical Heisenberg ferromagnets and related systems. The above results point therefore to a failure of naive perturbation theory in describing correctly the physical behavior of the \"O(n)\"-symmetric model above two dimensions, and to the need for more sophisticated non-perturbative methods such as the lattice formulation.\n\nThis means they can only arise as effective field theories. New physics is needed at around the distance scale where the two point connected correlation function is of the same order as the curvature of the target manifold. This is called the UV completion of the theory. There is a special class of nonlinear σ models with the internal symmetry group \"G\" *. If \"G\" is a Lie group and \"H\" is a Lie subgroup, then the quotient space \"G\"/\"H\" is a manifold (subject to certain technical restrictions like H being a closed subset) and is also a homogeneous space of \"G\" or in other words, a nonlinear realization of \"G\". In many cases, \"G\"/\"H\" can be equipped with a Riemannian metric which is \"G\"-invariant. This is always the case, for example, if \"G\" is compact. A nonlinear σ model with G/H as the target manifold with a \"G\"-invariant Riemannian metric and a zero potential is called a quotient space (or coset space) nonlinear model.\n\nWhen computing path integrals, the functional measure needs to be \"weighted\" by the square root of the determinant of \"g\",\n\nThis model proved to be relevant in string theory where the two-dimensional manifold is named worldsheet. Appreciation of its generalized renormalizability was provided by Daniel Friedan. He showed that the theory admits a renormalization group equation, at the leading order of perturbation theory, in the form\n\nThis represents a Ricci flow, obeying Einstein field equations for the target manifold as a fixed point. The existence of such a fixed point is relevant, as it grants, at this order of perturbation theory, that conformal invariance is not lost due to quantum corrections, so that the quantum field theory of this model is sensible (renormalizable). \n\nFurther adding nonlinear interactions representing flavor-chiral anomalies results in the Wess–Zumino–Witten model, which \naugments the geometry of the flow to include torsion, preserving renormalizability and leading to an infrared fixed point as well, on account of teleparallelism (\"geometrostasis\").\nA celebrated example, of particular interest due to its topological properties, is the \"O(3)\" nonlinear -model in 1 + 1 dimensions, with the Lagrangian density\nwhere \"n̂\"=(\"n, n, n\") with the constraint \"n̂\"⋅\"n̂\"=1 and =1,2. \n\nThis model allows for topological finite action solutions, as at infinite space-time the Lagrangian density must vanish, meaning \"n̂\" = constant at infinity. Therefore, in the class of finite-action solutions, one may identify the points at infinity as a single point, i.e. that space-time can be identified with a Riemann sphere. \n\nSince the \"n̂\"-field lives on a sphere as well, the mapping is in evidence, the solutions of which are classified by the second homotopy group of a 2-sphere: These solutions are called the O(3) Instantons.\n\nThis model can also be considered in 1+2 dimensions, where the topology now comes only from the spatial slices. These are modelled as R^2 with a point at infinity, and hence have the same topology as the O(3) instantons in 1+1 dimensions. They are called sigma model lumps.\n\n\n"}
{"id": "16084282", "url": "https://en.wikipedia.org/wiki?curid=16084282", "title": "Ordinal analysis", "text": "Ordinal analysis\n\nIn proof theory, ordinal analysis assigns ordinals (often large countable ordinals) to mathematical theories as a measure of their strength. The field was formed when Gerhard Gentzen in 1934 used cut elimination to prove, in modern terms, that the proof-theoretic ordinal of Peano arithmetic is ε.\n\nOrdinal analysis concerns true, effective (recursive) theories that can interpret a sufficient portion of arithmetic to make statements about ordinal notations. The proof-theoretic ordinal of such a theory formula_1 is the smallest recursive ordinal that the theory cannot prove is well founded—the supremum of all ordinals formula_2 for which there exists a notation formula_3 in Kleene's sense such that formula_1 proves that formula_3 is an ordinal notation. Equivalently, it is the supremum of all ordinals formula_2 such that there exists a recursive relation formula_7 on formula_8 (the set of natural numbers) that well-orders it with ordinal formula_2 and such that formula_1 proves transfinite induction of arithmetical statements for formula_7.\n\nThe existence of any recursive ordinal that the theory fails to prove is well ordered follows from the formula_12 bounding theorem, as the set of natural numbers that an effective theory proves to be ordinal notations is a formula_13 set (see Hyperarithmetical theory). Thus the proof-theoretic ordinal of a theory will always be a countable ordinal less than the Church–Kleene ordinal formula_14.\n\nIn practice, the proof-theoretic ordinal of a theory is a good measure of the strength of a theory. If theories have the same proof-theoretic ordinal they are often equiconsistent, and if one theory has a larger proof-theoretic ordinal than another it can often prove the consistency of the second theory.\n\n\nFriedman's grand conjecture suggests that much \"ordinary\" mathematics can be proved in weak systems having this as their proof-theoretic ordinal.\n\n\n\n\nThis ordinal is sometimes considered to be the upper limit for \"predicative\" theories.\n\n\n\n\nMost theories capable of describing the power set of the natural numbers have proof-theoretic ordinals\nthat are so large that no explicit combinatorial description has yet () been given. This includes second-order arithmetic and set theories with powersets. (The CZF and Kripke-Platek set theories mentioned above are weak set theories without powersets.)\n\n\n"}
{"id": "658608", "url": "https://en.wikipedia.org/wiki?curid=658608", "title": "PH (complexity)", "text": "PH (complexity)\n\nIn computational complexity theory, the complexity class PH is the union of all complexity classes in the polynomial hierarchy:\n\nPH was first defined by Larry Stockmeyer. It is a special case of hierarchy of bounded alternating Turing machine. It is contained in P = P (by Toda's theorem; the class of problems that are decidable by a polynomial time Turing machine with access to a #P or equivalently PP oracle), and also in PSPACE.\n\nPH has a simple logical characterization: it is the set of languages expressible by second-order logic.\n\nPH contains almost all well-known complexity classes inside PSPACE; in particular, it contains P, NP, and co-NP. It even contains probabilistic classes such as BPP and RP. However, there is some evidence that BQP, the class of problems solvable in polynomial time by a quantum computer, is not contained in PH.\n\nP = NP if and only if P = PH. This may simplify a potential proof of P ≠ NP, since it is only necessary to separate P from the more general class PH.\n"}
{"id": "23776575", "url": "https://en.wikipedia.org/wiki?curid=23776575", "title": "Postselection", "text": "Postselection\n\nIn probability theory, to postselect is to condition a probability space upon the occurrence of a given event. In symbols, once we postselect for an event formula_1, the probability of some other event formula_2 changes from formula_3 to the conditional probability formula_4.\n\nFor a discrete probability space, formula_5, and thus we require that formula_6 be strictly positive in order for the postselection to be well-defined.\n\nSee also PostBQP, a complexity class defined with postselection. Using postselection it seems quantum Turing machines are much more powerful: Scott Aaronson proved PostBQP is equal to PP.\n\nSome quantum experiments use post-selection after the experiment as a replacement for communication during the experiment, by post-selecting the communicated value into a constant.\n"}
{"id": "64489", "url": "https://en.wikipedia.org/wiki?curid=64489", "title": "Purchasing power parity", "text": "Purchasing power parity\n\nPurchasing power parity (PPP) is a neoclassical economic theory that states that the exchange rate between two countries is equal to the ratio of the currencies' respective purchasing power. Theories that invoke the purchasing power parity assume that in some circumstances (for example, as a long-run tendency) it would cost exactly the same number of, for example, US dollars to buy euros and then buy a market basket of goods as it would cost to directly purchase the market basket of goods with dollars. A fall in either currency's purchasing power would lead to a proportional decrease in that currency's valuation on the foreign exchange market.\n\nThe concept of purchasing power parity allows one to estimate what the exchange rate between two currencies would have to be in order for the exchange to be at par with the purchasing power of the two countries' currencies. Using that PPP rate for hypothetical currency conversions, a given amount of one currency thus has the same purchasing power whether used directly to purchase a market basket of goods or used to convert at the PPP rate to the other currency and then purchase the market basket using that currency. Observed deviations of the exchange rate from purchasing power parity are measured by deviations of the real exchange rate from its PPP value of 1.\n\nPPP exchange rates help costing but exclude profits and above all do not consider the different quality of goods among countries. The same product, for instance, can have a different level of quality and even safety in different countries. Since market exchange rates fluctuate substantially, when the GDP of one country measured in its own currency is converted to the other country's currency using market exchange rates, one country might be inferred to have higher real GDP than the other country in one year but lower in the other; both of these inferences would fail to reflect the reality of their relative levels of production. But if one country's GDP is converted into the other country's currency using PPP exchange rates instead of observed market exchange rates, the false inference will not occur. Essentially GDP PPP controls for the different costs of living and price levels, usually relative to the United States dollar, thus enabling a more accurate depiction of a given nation's level of production.\n\nThe idea originated with the School of Salamanca in the 16th century, and was developed in its modern form by Gustav Cassel in 1916, in \"The Present Situation of the Foreign Trade\".\nThe concept is based on the law of one price, where in the absence of transaction costs and official trade barriers, identical goods will have the same price in different markets when the prices are expressed in the same currency.\n\nAnother interpretation is that the difference in the rate of change in prices at home and abroad—the difference in the inflation rates—is equal to the percentage depreciation or appreciation of the exchange rate.\n\nDeviations from parity imply differences in purchasing power of a \"basket of goods\" across countries, which means that for the purposes of many international comparisons, countries' GDPs or other national income statistics need to be \"PPP-adjusted\" and converted into common units. The best-known purchasing power adjustment is the Geary–Khamis dollar (the \"international dollar\"). The real exchange rate is then equal to the nominal exchange rate, adjusted for differences in price levels. If purchasing power parity held exactly, then the real exchange rate would always equal one. However, in practice the real exchange rates exhibit both short run and long run deviations from this value, for example due to reasons illuminated in the Balassa–Samuelson theorem.\n\nThere can be marked differences between purchasing power adjusted incomes and those converted via market exchange rates. For example, the World Bank's \"World Development Indicators 2005\" estimated that in 2003, one Geary-Khamis dollar was equivalent to about 1.8 Chinese yuan by purchasing power parity—considerably different from the nominal exchange rate. This discrepancy has large implications; for instance, when converted via the nominal exchange rates GDP per capita in India is about US$1,965 while on a PPP basis it is about US$7,197. At the other extreme, for instance Denmark's nominal GDP per capita is around US$53,242, but its PPP figure is US$46,602, in line with other developed nations.\n\nThe purchasing power parity exchange rate serves two main functions. PPP exchange rates can be useful for making comparisons between countries because they stay fairly constant from day to day or week to week and only change modestly, if at all, from year to year. Second, over a period of years, exchange rates do tend to move in the general direction of the PPP exchange rate and there is some value to knowing in which direction the exchange rate is more likely to shift over the long run.\n\nThe PPP exchange-rate calculation is controversial because of the difficulties of finding comparable baskets of goods to compare purchasing power across countries.\n\nEstimation of purchasing power parity is complicated by the fact that countries do not simply differ in a uniform price level; rather, the difference in food prices may be greater than the difference in housing prices, while also less than the difference in entertainment prices. People in different countries typically consume different baskets of goods. It is necessary to compare the cost of baskets of goods and services using a price index. This is a difficult task because purchasing patterns and even the goods available to purchase differ across countries.\n\nThus, it is necessary to make adjustments for differences in the quality of goods and services. Furthermore, the basket of goods representative of one economy will vary from that of another: Americans eat more bread; Chinese more rice. Hence a PPP calculated using the US consumption as a base will differ from that calculated using China as a base. Additional statistical difficulties arise with multilateral comparisons when (as is usually the case) more than two countries are to be compared.\n\nVarious ways of averaging bilateral PPPs can provide a more stable multilateral comparison, but at the cost of distorting bilateral ones. These are all general issues of indexing; as with other price indices there is no way to reduce complexity to a single number that is equally satisfying for all purposes. Nevertheless, PPPs are typically robust in the face of the many problems that arise in using market exchange rates to make comparisons.\n\nFor example, in 2005 the price of a gallon of gasoline in Saudi Arabia was USD 0.91, and in Norway the price was USD 6.27. The significant differences in price would not contribute to accuracy in a PPP analysis, despite all of the variables that contribute to the significant differences in price. More comparisons have to be made and used as variables in the overall formulation of the PPP.\n\nWhen PPP comparisons are to be made over some interval of time, proper account needs to be made of inflationary effects.\n\nAlthough it may seem as if PPPs and the law of one price are the same, there is a difference: the law of one price applies to individual commodities whereas PPP applies to the general price level. If the law of one price is true for all commodities then PPP is also therefore true; however, when discussing the validity of PPP, some argue that the law of one price does not need to be true exactly for PPP to be valid. If the law of one price is not true for a certain commodity, the price levels will not differ enough from the level predicted by PPP.\n\nThe purchasing power parity theory states that the exchange rate between one currency and another currency is in equilibrium when their domestic purchasing powers at that rate of exchange are equivalent.\n\nAnother example of one measure of the law of one price, which underlies purchasing power parity, is the Big Mac Index, popularized by \"The Economist\", which compares the prices of a Big Mac burger in McDonald's restaurants in different countries. The Big Mac Index is presumably useful because although it is based on a single consumer product that may not be typical, it is a relatively standardized product that includes input costs from a wide range of sectors in the local economy, such as agricultural commodities (beef, bread, lettuce, cheese), labor (blue and white collar), advertising, rent and real estate costs, transportation, etc.\n\nIn theory, the law of one price would hold that if, to take an example, the Canadian dollar were to be significantly overvalued relative to the U.S. dollar according to the Big Mac Index, that gap should be unsustainable because Canadians would import their Big Macs from or travel to the U.S. to consume them, thus putting upward demand pressure on the U.S. dollar by virtue of Canadians buying the U.S. dollars needed to purchase the U.S.-made Big Macs and simultaneously placing downward supply pressure on the Canadian dollar by virtue of Canadians selling their currency in order to buy those same U.S. dollars.\n\nThe alternative to this exchange rate adjustment would be an adjustment in prices, with Canadian McDonald's stores compelled to lower prices to remain competitive. Either way, the valuation difference should be reduced assuming perfect competition and a perfectly tradable good. In practice, of course, the Big Mac is not a perfectly tradable good and there may also be capital flows that sustain relative demand for the Canadian dollar. The difference in price may have its origins in a variety of factors besides direct input costs such as government regulations and product differentiation.\n\nIn some emerging economies, Western fast food represents an expensive niche product priced well above the price of traditional staples—i.e. the Big Mac is not a mainstream 'cheap' meal as it is in the West, but a luxury import. This relates back to the idea of product differentiation: the fact that few substitutes for the Big Mac are available confers market power on McDonald's. For example, in India, the costs of local fast food like vada pav are comparative to what the Big Mac signifies in the U.S. Additionally, with countries such as Argentina that have abundant beef resources, consumer prices in general may not be as cheap as implied by the price of a Big Mac.\n\nThe following table, based on data from \"The Economist\"'s January 2013 calculations, shows the under (−) and over (+) valuation of the local currency against the U.S. dollar in %, according to the Big Mac index. To take an example calculation, the local price of a Big Mac in Hong Kong when converted to U.S. dollars at the market exchange rate was $2.19, or 50% of the local price for a Big Mac in the U.S. of $4.37. Hence the Hong Kong dollar was deemed to be 50% undervalued relative to the U.S. dollar on a PPP basis.\n\nLike the Big Mac Index, the iPad index (elaborated by CommSec) compares an item's price in various locations. Unlike the Big Mac, however, each iPad is produced in the same place (except for the model sold in Brazil) and all iPads (within the same model) have identical performance characteristics. Price differences are therefore a function of transportation costs, taxes, and the prices that may be realized in individual markets. An iPad will cost about twice as much in Argentina as in the United States.\n\nSimilar to the Big Mac Index, the KFC Index measures PPP amongst African countries, created by Sagaci Research (a market research firm focusing solely on Africa). Instead of comparing a Big Mac, this index compares a KFC Original 12/15 pc. bucket.\n\nFor example, the average price of KFC’s Original 12 pc. Bucket in the United States in January 2016 was $20.50; while in Namibia it was only $13.40 at market exchange rates. Therefore, the index states the Namibian dollar was undervalued by 33% at that time.\n\nGlobalPetrolPrices has published two world rankings comparing gasoline prices with average income and minimum wages.\n\nEach month, the Organisation for Economic Co-operation and Development measures the difference in price levels between its member countries by calculating the ratios of PPPs for private final consumption expenditure to exchange rates. The OECD table below indicates the number of US dollars needed in each of the countries listed to buy the same representative basket of consumer goods and services that would cost 100 USD in the United States\n\nAccording to the table, an American living or travelling in Switzerland on an income denominated in US dollars would find that country to be the most expensive of the group, having to spend 62% more US dollars to maintain a standard of living comparable to the US in terms of consumption.\n\nIn addition to methodological issues presented by the selection of a basket of goods, PPP estimates can also vary based on the statistical capacity of participating countries. The International Comparison Program, which PPP estimates are based on, require the disaggregation of national accounts into production, expenditure or (in some cases) income, and not all participating countries routinely disaggregate their data into such categories.\n\nSome aspects of PPP comparison are theoretically impossible or unclear. For example, there is no basis for comparison between the Ethiopian laborer who lives on teff with the Thai laborer who lives on rice, because teff is not commercially available in Thailand and rice is not in Ethiopia, so the price of rice in Ethiopia or teff in Thailand cannot be determined. As a general rule, the more similar the price structure between countries, the more valid the PPP comparison.\n\nPPP levels will also vary based on the formula used to calculate price matrices. Different possible formulas include GEKS-Fisher, Geary-Khamis, IDB, and the superlative method. Each has advantages and disadvantages.\n\nLinking regions presents another methodological difficulty. In the 2005 ICP round, regions were compared by using a list of some 1,000 identical items for which a price could be found for 18 countries, selected so that at least two countries would be in each region. While this was superior to earlier \"bridging\" methods, which do not fully take into account differing quality between goods, it may serve to overstate the PPP basis of poorer countries, because the price indexing on which PPP is based will assign to poorer countries the greater weight of goods consumed in greater shares in richer countries.\n\nThe exchange rate reflects transaction values for traded goods \"between\" countries in contrast to non-traded goods, that is, goods produced for home-country use. Also, currencies are traded for purposes other than trade in goods and services, \"e.g.\", to buy capital assets whose prices vary more than those of physical goods. Also, different interest rates, speculation, hedging or interventions by central banks can influence the foreign-exchange market.\n\nThe PPP method is used as an alternative to correct for possible statistical bias. The Penn World Table is a widely cited source of PPP adjustments, and the associated Penn effect reflects such a systematic bias in using exchange rates to outputs among countries.\n\nFor example, if the value of the Mexican peso falls by half compared to the US dollar, the Mexican Gross Domestic Product measured in dollars will also halve. However, this exchange rate results from international trade and financial markets. It does not necessarily mean that Mexicans are poorer by a half; if incomes and prices measured in pesos stay the same, they will be no worse off assuming that imported goods are not essential to the quality of life of individuals. Measuring income in different countries using PPP exchange rates helps to avoid this problem.\n\nPPP exchange rates are especially useful when official exchange rates are artificially manipulated by governments. Countries with strong government control of the economy sometimes enforce official exchange rates that make their own currency artificially strong. By contrast, the currency's black market exchange rate is artificially weak. In such cases, a PPP exchange rate is likely the most realistic basis for economic comparison. Similarly, when exchange rates deviate significantly from their long term equilibrium due to speculative attacks or carry trade, a PPP exchange rate offers a better alternative for comparison.\n\nSince global PPP estimates—such as those provided by the ICP— are not calculated annually, but for a single year, PPP exchange rates for years other than the benchmark year need to be extrapolated. One way of doing this is by using the country's GDP deflator. To calculate a country's PPP exchange rate in Geary–Khamis dollars for a particular year, the calculation proceeds in the following manner:\n\nformula_1\n\nWhere PPPrate is the PPP exchange rate of country X for year i, PPPrate is the PPP exchange rate of country X for the benchmark year, PPPrate is the PPP exchange rate of the United States (US) for the benchmark year (equal to 1), GDPdef is the GDP deflator of country X for year i, GDPdef is the GDP deflator of country X for the benchmark year, GDPdef is the GDP deflator of the US for year i, and GDPdef is the GDP deflator of the US for the benchmark year.\n\nThere are a number of reasons that different measures do not perfectly reflect standards of living.\n\nThe goods that the currency has the \"power\" to purchase are a basket of goods of different types:\n\nThe more that a product falls into category 1, the further its price will be from the currency exchange rate, moving towards the PPP exchange rate. Conversely, category 2 products tend to trade close to the currency exchange rate. (See also Penn effect).\n\nMore processed and expensive products are likely to be tradable, falling into the second category, and drifting from the PPP exchange rate to the currency exchange rate. Even if the PPP \"value\" of the Ethiopian currency is three times stronger than the currency exchange rate, it won't buy three times as much of internationally traded goods like steel, cars and microchips, but non-traded goods like housing, services (\"haircuts\"), and domestically produced crops. The relative price differential between tradables and non-tradables from high-income to low-income countries is a consequence of the Balassa–Samuelson effect and gives a big cost advantage to labour-intensive production of tradable goods in low income countries (like Ethiopia), as against high income countries (like Switzerland).\n\nThe corporate cost advantage is nothing more sophisticated than access to cheaper workers, but because the pay of those workers goes farther in low-income countries than high, the relative pay differentials (inter-country) can be sustained for longer than would be the case otherwise. (This is another way of saying that the wage rate is based on average local productivity and that this is below the per capita productivity that factories selling tradable goods to international markets can achieve.) An equivalent cost benefit comes from non-traded goods that can be sourced locally (nearer the PPP-exchange rate than the nominal exchange rate in which receipts are paid). These act as a cheaper factor of production than is available to factories in richer countries. It's difficult by the GDP PPP to consider the different quality of goods among the different countries.\n\nThe Bhagwati–Kravis–Lipsey view provides a somewhat different explanation from the Balassa–Samuelson theory. This view states that price levels for nontradables are lower in poorer countries because of differences in endowment of labor and capital, not because of lower levels of productivity. Poor countries have more labor relative to capital, so marginal productivity of labor is greater in rich countries than in poor countries. Nontradables tend to be labor-intensive; therefore, because labor is less expensive in poor countries and is used mostly for nontradables, nontradables are cheaper in poor countries. Wages are high in rich countries, so nontradables are relatively more expensive.\n\nPPP calculations tend to overemphasise the primary sectoral contribution, and underemphasise the industrial and service sectoral contributions to the economy of a nation.\n\nThe law of one price, the underlying mechanism behind PPP, is weakened by transport costs and governmental trade restrictions, which make it expensive to move goods between markets located in different countries. Transport costs sever the link between exchange rates and the prices of goods implied by the law of one price. As transport costs increase, the larger the range of exchange rate fluctuations. The same is true for official trade restrictions because the customs fees affect importers' profits in the same way as shipping fees. According to Krugman and Obstfeld, \"Either type of trade impediment weakens the basis of PPP by allowing the purchasing power of a given currency to differ more widely from country to country.\" They cite the example that a dollar in London should purchase the same goods as a dollar in Chicago, which is certainly not the case.\n\nNontradables are primarily services and the output of the construction industry. Nontradables also lead to deviations in PPP because the prices of nontradables are not linked internationally. The prices are determined by domestic supply and demand, and shifts in those curves lead to changes in the market basket of some goods relative to the foreign price of the same basket. If the prices of nontradables rise, the purchasing power of any given currency will fall in that country.\n\nLinkages between national price levels are also weakened when trade barriers and imperfectly competitive market structures occur together. Pricing to market occurs when a firm sells the same product for different prices in different markets. This is a reflection of inter-country differences in conditions on both the demand side (\"e.g.\", virtually no demand for pork in Islamic states) and the supply side (\"e.g.\", whether the existing market for a prospective entrant's product features few suppliers or instead is already near-saturated). According to Krugman and Obstfeld, this occurrence of product differentiation and segmented markets results in violations of the law of one price and absolute PPP. Over time, shifts in market structure and demand will occur, which may invalidate relative PPP.\n\nMeasurement of price levels differ from country to country. Inflation data from different countries are based on different commodity baskets; therefore, exchange rate changes do not offset official measures of inflation differences. Because it makes predictions about price changes rather than price levels, relative PPP is still a useful concept. However, change in the relative prices of basket components can cause relative PPP to fail tests that are based on official price indexes.\n\nThe global poverty line is a worldwide count of people who live below an international poverty line, referred to as the dollar-a-day line. This line represents an average of the national poverty lines of the world's poorest countries, expressed in international dollars. These national poverty lines are converted to international currency and the global line is converted back to local currency using the PPP exchange rates from the ICP. PPP exchange rates include data from the sales of high end none poverty related items which skews the value of food items and necessary goods which is 70 percent of poor peoples' consumption. Angus Deaton argues that PPP indexes need to be reweighted for use in poverty measurement; they need to be redefined to reflect local poverty measures, not global measures, weighing local food items and excluding luxury items that are not prevalent or are not of equal value in all localities.\n\n\n"}
{"id": "17033211", "url": "https://en.wikipedia.org/wiki?curid=17033211", "title": "Quantitative models of the action potential", "text": "Quantitative models of the action potential\n\nIn neurophysiology, several mathematical models of the action potential have been developed, which fall into two basic types. The first type seeks to model the experimental data quantitatively, i.e., to reproduce the measurements of current and voltage exactly. The renowned Hodgkin–Huxley model of the axon from the \"Loligo\" squid exemplifies such models. Although qualitatively correct, the H-H model does not describe every type of excitable membrane accurately, since it considers only two ions (sodium and potassium), each with only one type of voltage-sensitive channel. However, other ions such as calcium may be important and there is a great diversity of channels for all ions. As an example, the cardiac action potential illustrates how differently shaped action potentials can be generated on membranes with voltage-sensitive calcium channels and different types of sodium/potassium channels. The second type of mathematical model is a simplification of the first type; the goal is not to reproduce the experimental data, but to understand qualitatively the role of action potentials in neural circuits. For such a purpose, detailed physiological models may be unnecessarily complicated and may obscure the \"forest for the trees\". The Fitzhugh-Nagumo model is typical of this class, which is often studied for its entrainment behavior. Entrainment is commonly observed in nature, for example in the synchronized lighting of fireflies, which is coordinated by a burst of action potentials; entrainment can also be observed in individual neurons. Both types of models may be used to understand the behavior of small biological neural networks, such as the central pattern generators responsible for some automatic reflex actions. Such networks can generate a complex temporal pattern of action potentials that is used to coordinate muscular contractions, such as those involved in breathing or fast swimming to escape a predator.\n\nIn 1952 Alan Lloyd Hodgkin and Andrew Huxley developed a set of equations to fit their experimental voltage-clamp data on the axonal membrane. The model assumes that the membrane capacitance \"C\" is constant; thus, the transmembrane voltage \"V\" changes with the total transmembrane current \"I\" according to the equation\n\nwhere \"I\", \"I\", and \"I\" are currents conveyed through the local sodium channels, potassium channels, and \"leakage\" channels (a catch-all), respectively. The initial term \"I\" represents the current arriving from external sources, such as excitatory postsynaptic potentials from the dendrites or a scientist's electrode.\n\nThe model further assumes that a given ion channel is either fully open or closed; if closed, its conductance is zero, whereas if open, its conductance is some constant value \"g\". Hence, the net current through an ion channel depends on two variables: the probability \"p\" of the channel being open, and the difference in voltage from that ion's equilibrium voltage, \"V\" − \"V\". For example, the current through the potassium channel may be written as\n\nwhich is equivalent to Ohm's law. By definition, no net current flows (\"I\" = 0) when the transmembrane voltage equals the equilibrium voltage of that ion (when \"V\" = \"E\").\n\nTo fit their data accurately, Hodgkin and Huxley assumed that each type of ion channel had multiple \"gates\", so that the channel was open only if all the gates were open and closed otherwise. They also assumed that the probability of a gate being open was independent of the other gates being open; this assumption was later validated for the inactivation gate. Hodgkin and Huxley modeled the voltage-sensitive potassium channel as having four gates; letting \"p\" denote the probability of a single such gate being open, the probability of the whole channel being open is the product of four such probabilities, i.e., \"p\" = \"n\". Similarly, the probability of the voltage-sensitive sodium channel was modeled to have three similar gates of probability \"m\" and a fourth gate, associated with inactivation, of probability \"h\"; thus, \"p\" = \"m\"\"h\". The probabilities for each gate are assumed to obey first-order kinetics\n\nwhere both the equilibrium value \"m\" and the relaxation time constant τ depend on the instantaneous voltage \"V\" across the membrane. If \"V\" changes on a time-scale more slowly than τ, the \"m\" probability will always roughly equal its equilibrium value \"m\"; however, if \"V\" changes more quickly, then \"m\" will lag behind \"m\". By fitting their voltage-clamp data, Hodgkin and Huxley were able to model how these equilibrium values and time constants varied with temperature and transmembrane voltage. The formulae are complex and depend exponentially on the voltage and temperature. For example, the time constant for sodium-channel activation probability \"h\" varies as 3 with the Celsius temperature θ, and with voltage \"V\" as\n\nIn summary, the Hodgkin–Huxley equations are complex, non-linear ordinary differential equations in four independent variables: the transmembrane voltage \"V\", and the probabilities \"m\", \"h\" and \"n\". No general solution of these equations has been discovered. A less ambitious but generally applicable method for studying such non-linear dynamical systems is to consider their behavior in the vicinity of a fixed point. This analysis shows that the Hodgkin–Huxley system undergoes a transition from stable quiescence to bursting oscillations as the stimulating current \"I\" is gradually increased; remarkably, the axon becomes stably quiescent again as the stimulating current is increased further still. A more general study of the types of qualitative behavior of axons predicted by the Hodgkin–Huxley equations has also been carried out.\n\nBecause of the complexity of the Hodgkin–Huxley equations, various simplifications have been developed that exhibit qualitatively similar behavior. The Fitzhugh-Nagumo model is a typical example of such a simplified system. Based on the tunnel diode, the FHN model has only two independent variables, but exhibits a similar stability behavior to the full Hodgkin–Huxley equations. The equations are\n\nwhere \"g(V)\" is a function of the voltage \"V\" that has a region of negative slope in the middle, flanked by one maximum and one minimum (Figure FHN). A much-studied simple case of the Fitzhugh-Nagumo model is the Bonhoeffer-van der Pol nerve model, which is described by the equations\n\nwhere the coefficient ε is assumed to be small. These equations can be combined into a second-order differential equation\n\nThis van der Pol equation has stimulated much research in the mathematics of nonlinear dynamical systems. Op-amp circuits that realize the FHN and van der Pol models of the action potential have been developed by Keener.\n\nA hybrid of the Hodgkin–Huxley and FitzHugh–Nagumo models was developed by Morris and Lecar in 1981, and applied to the muscle fiber of barnacles. True to the barnacle's physiology, the Morris–Lecar model replaces the voltage-gated sodium current of the Hodgkin–Huxley model with a voltage-dependent calcium current. There is no inactivation (no \"h\" variable) and the calcium current equilibrates instantaneously, so that again, there are only two time-dependent variables: the transmembrane voltage \"V\" and the potassium gate probability \"n\". The bursting, entrainment and other mathematical properties of this model have been studied in detail.\n\nThe simplest models of the action potential are the \"flush and fill\" models (also called \"integrate-and-fire\" models), in which the input signal is summed (the \"fill\" phase) until it reaches a threshold, firing a pulse and resetting the summation to zero (the \"flush\" phase). All of these models are capable of exhibiting entrainment, which is commonly observed in nervous systems.\n\nWhereas the above models simulate the transmembrane voltage and current at a single patch of membrane, other mathematical models pertain to the voltages and currents in the ionic solution surrounding the neuron. Such models are helpful in interpreting data from extracellular electrodes, which were common prior to the invention of the glass pipette electrode that allowed intracellular recording. The extracellular medium may be modeled as a normal isotropic ionic solution; in such solutions, the current follows the electric field lines, according to the continuum form of Ohm's Law\n\nwhere j and E are vectors representing the current density and electric field, respectively, and where σ is the conductivity. Thus, j can be found from E, which in turn may be found using Maxwell's equations. Maxwell's equations can be reduced to a relatively simple problem of electrostatics, since the ionic concentrations change too slowly (compared to the speed of light) for magnetic effects to be important. The electric potential φ(x) at any extracellular point x can be solved using Green's identities\n\nwhere the integration is over the complete surface of the membrane; formula_12 is a position on the membrane, σ and φ are the conductivity and potential just within the membrane, and σ and φ the corresponding values just outside the membrane. Thus, given these σ and φ values on the membrane, the extracellular potential φ(x) can be calculated for any position x; in turn, the electric field E and current density j can be calculated from this potential field.\n\n"}
{"id": "11018121", "url": "https://en.wikipedia.org/wiki?curid=11018121", "title": "Quaternion-Kähler symmetric space", "text": "Quaternion-Kähler symmetric space\n\nIn differential geometry, a quaternion-Kähler symmetric space or Wolf space is a quaternion-Kähler manifold which, as a Riemannian manifold, is a Riemannian symmetric space. Any quaternion-Kähler symmetric space with positive Ricci curvature is compact and simply connected, and is a Riemannian product of quaternion-Kähler symmetric spaces associated to compact simple Lie groups.\n\nFor any compact simple Lie group \"G\", there is a unique \"G\"/\"H\" obtained as a quotient of \"G\" by a subgroup\n\nHere, Sp(1) is the compact form of the SL(2)-triple associated with the highest root of \"G\", and \"K\" its centralizer in \"G\". These are classified as follows.\n\nThe twistor spaces of quaternion-Kähler symmetric spaces are the homogeneous holomorphic contact manifolds, classified by Boothby: they are the adjoint varieties of the complex semisimple Lie groups.\n\nThese spaces can be obtained by taking a projectivization of\na minimal nilpotent orbit of the respective complex Lie group.\nThe holomorphic contact structure is apparent, because\nthe nilpotent orbits of semisimple Lie groups \nare equipped with the Kirillov-Kostant holomorphic symplectic form. This argument also explains how one\ncan associate a unique Wolf space to each of the simple\ncomplex Lie groups.\n\n\n"}
{"id": "2788568", "url": "https://en.wikipedia.org/wiki?curid=2788568", "title": "Raymond Paley", "text": "Raymond Paley\n\nRaymond Edward Alan Christopher Paley (7 January 1907 – 7 April 1933) was an English mathematician. Paley was born in Bournemouth, England. He was educated at Eton. From there he entered Trinity College, Cambridge where he showed himself as a brilliant student. He won a Smith's Prize in 1930 and was\nelected a fellow of Trinity College.\n\nHis contributions include the Paley construction for Hadamard matrices (closely related to the Paley graphs in graph theory) and his collaboration with Norbert Wiener in the Paley–Wiener theorem (harmonic analysis). He collaborated with A. Zygmund on Fourier series (see also Paley–Zygmund inequality) and worked with J. E. Littlewood on what became known as Littlewood–Paley theory, an application of real-variable techniques in complex analysis.\n\nIn 1932, he introduced the dyadic basis (the so-called Paley order) with regard to Walsh functions.\n\nOn 7 April 1933, Paley died in a skiing accident when skiing alone at an altitude of 9,600 ft in Banff, Alberta. He was killed by an avalanche at Deception Pass, Fossil Mountain, in the Canadian Rockies. His death was witnessed by companions lower down the mountainside. Park wardens and a member of the Royal Canadian Mounted Police recovered the body. He is buried in the Banff town cemetery.\n\n"}
{"id": "176478", "url": "https://en.wikipedia.org/wiki?curid=176478", "title": "Riemann sum", "text": "Riemann sum\n\nIn mathematics, a Riemann sum is a certain kind of approximation of an integral by a finite sum. It is named after nineteenth century German mathematician Bernhard Riemann. One very common application is approximating the area of functions or lines on a graph, but also the length of curves and other approximations. \n\nThe sum is calculated by dividing the region up into shapes (rectangles, trapezoids, parabolas, or cubics) that together form a region that is similar to the region being measured, then calculating the area for each of these shapes, and finally adding all of these small areas together. This approach can be used to find a numerical approximation for a definite integral even if the fundamental theorem of calculus does not make it easy to find a closed-form solution.\n\nBecause the region filled by the small shapes is usually not exactly the same shape as the region being measured, the Riemann sum will differ from the area being measured. This error can be reduced by dividing up the region more finely, using smaller and smaller shapes. As the shapes get smaller and smaller, the sum approaches the Riemann integral.\n\nLet formula_1 be a function defined on a closed interval formula_2 of the real numbers, formula_3, and\nbe a partition of \"I\", where\nA Riemann sum formula_6 of \"f\" over \"I\" with partition \"P\" is defined as\nwhere formula_8 and an formula_9.\nNotice the use of \"an\" instead of \"the\" in the previous sentence. Another way of thinking about this asterisk is that you are choosing some random point in this slice, and it does not matter which one; as the difference or width of the slices approaches zero, the only point we can pick is the point our rectangle slice is at. This is due to the fact that the choice of formula_10 in the interval formula_11 is arbitrary, so for any given function \"f\" defined on an interval \"I\" and a fixed partition \"P\", one might produce different Riemann sums depending on which formula_10 is chosen, as long as formula_13 holds true.\n\nSpecific choices of formula_10 give us different types of Riemann sums:\n\nAll these methods are among the most basic ways to accomplish numerical integration. Loosely speaking, a function is Riemann integrable if all Riemann sums converge as the partition \"gets finer and finer\".\n\nWhile not technically a Riemann sum, the average of the left and right Riemann sum is the trapezoidal sum and is one of the simplest of a very general way of approximating integrals using weighted averages. This is followed in complexity by Simpson's rule and Newton–Cotes formulas.\n\nAny Riemann sum on a given partition (that is, for any choice of formula_10 between formula_23 and formula_24) is contained between the lower and upper Darboux sums. This forms the basis of the Darboux integral, which is ultimately equivalent to the Riemann integral.\n\nThe four methods of Riemann summation are usually best approached with partitions of equal size. The interval [\"a\", \"b\"] is therefore divided into \"n\" subintervals, each of length \n\nThe points in the partition will then be \n\nFor the left Riemann sum, approximating the function by its value at the left-end point gives multiple rectangles with base Δ\"x\" and height \"f\"(\"a\" + \"i\"Δ\"x\"). Doing this for \"i\" = 0, 1, ..., \"n\" − 1, and adding up the resulting areas gives\n\nThe left Riemann sum amounts to an overestimation if \"f\" is monotonically decreasing on this interval, and an underestimation if it is monotonically increasing.\n\n\"f\" is here approximated by the value at the right endpoint. This gives multiple rectangles with base Δ\"x\" and height \"f\"(\"a\" + \"i\" Δ\"x\"). Doing this for \"i\" = 1, ..., \"n\", and adding up the resulting areas produces \n\nThe right Riemann sum amounts to an underestimation if \"f\" is monotonically decreasing, and an overestimation if it is monotonically increasing.\nThe error of this formula will be \n\nwhere formula_30 is the maximum value of the absolute value of formula_31 on the interval.\n\nApproximating \"f\" at the midpoint of intervals gives \"f\"(\"a\" + Δ\"x\"/2) for the first interval, for the next one \"f\"(\"a\" + 3Δ\"x\"/2), and so on until \"f\"(\"b\" − Δ\"x\"/2). Summing up the areas gives\n\nThe error of this formula will be \n\nwhere formula_34 is the maximum value of the absolute value of formula_35 on the interval.\n\nIn this case, the values of the function \"f\" on an interval are approximated by the average of the values at the left and right endpoints. In the same manner as above, a simple calculation using the area formula \nfor a trapezium with parallel sides \"b\", \"b\" and height \"h\" produces\n\nThe error of this formula will be \n\nwhere formula_34 is the maximum value of the absolute value of formula_35.\n\nThe approximation obtained with the trapezoid rule for a function is the same as the average of the left hand and right hand sums of that function.\n\nFor a one-dimensional Riemann sum over domain formula_2, as the maximum size of a partition element shrinks to zero (that is the limit of the norm of the partition goes to zero), some functions will have all Riemann sums converge to the same value. This limiting value, if it exists, is defined as the definite Riemann integral of the function over the domain,\n\nFor a finite-sized domain, if the maximum size of a partition element shrinks to zero, this implies the number of partition elements goes to infinity. For finite partitions, Riemann sums are always approximations to the limiting value and this approximation gets better as the partition gets finer. The following animations help demonstrate how increasing the number of partitions (while lowering the maximum partition element size) better approximates the \"area\" under the curve:\n\nSince the red function here is assumed to be a smooth function, all three Riemann sums will converge to the same value as the number of partitions goes to infinity.\n\nTaking an example, the area under the curve of \"y\" = \"x\" between 0 and 2 can be procedurally computed using Riemann's method. \n\nThe interval [0, 2] is firstly divided into \"n\" subintervals, each of which is given a width of formula_43; these are the widths of the Riemann rectangles (hereafter \"boxes\"). Because the right Riemann sum is to be used, the sequence of \"x\" coordinates for the boxes will be formula_44. Therefore, the sequence of the heights of the boxes will be formula_45. It is an important fact that formula_46, and formula_47.\n\nThe area of each box will be formula_48 and therefore the \"n\"th right Riemann sum will be: \n\nIf the limit is viewed as \"n\" → ∞, it can be concluded that the approximation approaches the actual value of the area under the curve as the number of boxes increases. Hence:\n\nThis method agrees with the definite integral as calculated in more mechanical ways:\n\nBecause the function is continuous and monotonically increasing on the interval, a right Riemann sum overestimates the integral by the largest amount (while a left Riemann sum would underestimate the integral by the largest amount). This fact, which is intuitively clear from the diagrams, shows how the nature of the function determines how accurate the integral is estimated. While simple, right and left Riemann sums are often less accurate than more advanced techniques of estimating an integral such as the Trapezoidal rule or Simpson's rule.\n\nThe example function has an easy-to-find anti-derivative so estimating the integral by Riemann sums is mostly an academic exercise; however it must be remembered that not all functions have anti-derivatives so estimating their integrals by summation is practically important.\n\nThe basic idea behind a Riemann sum is to \"break-up\" the domain via a partition into pieces, multiply the \"size\" of each piece by some value the function takes on that piece, and sum all these products. This can be generalized to allow Riemann sums for functions over domains of more than one dimension.\n\nWhile intuitively, the process of partitioning the domain is easy to grasp, the technical details of how the domain may be partitioned get much more complicated than the one dimensional case and involves aspects of the geometrical shape of the domain.\n\nIn two dimensions, the domain, formula_52 may be divided into a number of cells, formula_53 such that formula_54. In two dimensions, each cell then can be interpreted as having an \"area\" denoted by formula_55. The Riemann sum is\nwhere formula_57.\n\nIn three dimensions, it is customary to use the letter formula_58 for the domain, such that formula_59 under the partition and formula_60 is the \"volume\" of the cell indexed by formula_61. The three-dimensional Riemann sum may then be written as\nwith formula_63.\n\nHigher dimensional Riemann sums follow a similar as from one to two to three dimensions. For an arbitrary dimension, n, a Riemann sum can be written as\nwhere formula_65, that is, it's a point in the n-dimensional cell formula_66 with n-dimensional volume formula_60.\n\nIn high generality, Riemann sums can be written\nwhere formula_69 stands for any arbitrary point contained in the partition element formula_66 and formula_71 is a measure on the underlying set. Roughly speaking, a measure is a function that gives a \"size\" of a set, in this case the size of the set formula_66; in one dimension, this can often be interpreted as the length of the interval, in two dimensions, an area, in three dimensions, a volume, and so on.\n\n\n"}
{"id": "548156", "url": "https://en.wikipedia.org/wiki?curid=548156", "title": "State-space representation", "text": "State-space representation\n\nIn control engineering, a state-space representation is a mathematical model of a physical system as a set of input, output and state variables related by first-order differential equations or difference equations. State variables are variables whose values evolve through time in a way that depends on the values they have at any given time and also depends on the externally imposed values of input variables. Output variables’ values depend on the values of the state variables.\n\nThe \"state space\" is the Euclidean space in which the variables on the axes are the state variables. The state of the system can be represented as a vector within that space.\n\nTo abstract from the number of inputs, outputs and states, these variables are expressed as vectors. Additionally, if the dynamical system is linear, time-invariant, and finite-dimensional, then the differential and algebraic equations may be written in matrix form.\nThe state-space method is characterized by significant algebraization of general system theory, which makes it possible to use Kronecker vector-matrix structures. The capacity of these structures can be efficiently applied to research systems with modulation or without it. \nThe state-space representation (also known as the \"time-domain approach\") provides a convenient and compact way to model and analyze systems with multiple inputs and outputs. With formula_1 inputs and formula_2 outputs, we would otherwise have to write down formula_3 Laplace transforms to encode all the information about a system. Unlike the frequency domain approach, the use of the state-space representation is not limited to systems with linear components and zero initial conditions. The state-space model is used in many different areas. In econometrics, the state-space model can be used for forecasting stock prices and numerous other variables.\n\nThe internal state variables are the smallest possible subset of system variables that can represent the entire state of the system at any given time. The minimum number of state variables required to represent a given system, formula_4, is usually equal to the order of the system's defining differential equation. If the system is represented in transfer function form, the minimum number of state variables is equal to the order of the transfer function's denominator after it has been reduced to a proper fraction. It is important to understand that converting a state-space realization to a transfer function form may lose some internal information about the system, and may provide a description of a system which is stable, when the state-space realization is unstable at certain points. In electric circuits, the number of state variables is often, though not always, the same as the number of energy storage elements in the circuit such as capacitors and inductors. The state variables defined must be linearly independent, i.e., no state variable can be written as a linear combination of the other state variables or the system will not be able to be solved.\n\nThe most general state-space representation of a linear system with formula_1 inputs, formula_2 outputs and formula_4 state variables is written in the following form:\nwhere:\n\nIn this general formulation, all matrices are allowed to be time-variant (i.e. their elements can depend on time); however, in the common LTI case, matrices will be time invariant. The time variable formula_26 can be continuous (e.g. formula_27) or discrete (e.g. formula_28). In the latter case, the time variable formula_29 is usually used instead of formula_26. Hybrid systems allow for time domains that have both continuous and discrete parts. Depending on the assumptions taken, the state-space model representation can assume the following forms:\n\nStability and natural response characteristics of a continuous-time LTI system (i.e., linear with matrices that are constant with respect to time) can be studied from the eigenvalues of the matrix A. The stability of a time-invariant state-space model can be determined by looking at the system's transfer function in factored form. It will then look something like this:\n\nThe denominator of the transfer function is equal to the characteristic polynomial found by taking the determinant of formula_32,\nThe roots of this polynomial (the eigenvalues) are the system transfer function's poles (i.e., the singularities where the transfer function's magnitude is unbounded). These poles can be used to analyze whether the system is asymptotically stable or marginally stable. An alternative approach to determining stability, which does not involve calculating eigenvalues, is to analyze the system's Lyapunov stability.\n\nThe zeros found in the numerator of formula_34 can similarly be used to determine whether the system is minimum phase.\n\nThe system may still be input–output stable (see BIBO stable) even though it is not internally stable. This may be the case if unstable poles are canceled out by zeros (i.e., if those singularities in the transfer function are removable).\n\nThe state controllability condition implies that it is possible – by admissible inputs – to steer the states from any initial value to any final value within some finite time window. A continuous time-invariant linear state-space model is controllable if and only if\nwhere rank is the number of linearly independent rows in a matrix, and where \"n\" is the number of state variables.\n\nObservability is a measure for how well internal states of a system can be inferred by knowledge of its external outputs. The observability and controllability of a system are mathematical duals (i.e., as controllability provides that an input is available that brings any initial state to any desired final state, observability provides that knowing an output trajectory provides enough information to predict the initial state of the system).\n\nA continuous time-invariant linear state-space model is observable if and only if\n\nThe \"transfer function\" of a continuous time-invariant linear state-space model can be derived in the following way:\n\nFirst, taking the Laplace transform of \nyields\nNext, we simplify for formula_39, giving\nand thus\n\nSubstituting for formula_39 in the output equation\n\nThe transfer function formula_45 is defined as the ratio of the output to the input of a system considering its initial conditions to be zero (formula_46). However, the ratio of a vector to a vector does not exist, so we consider the following condition satisfied by the transfer function\ncomparison with the equation for formula_48 above gives\nClearly formula_45 must have formula_2 by formula_1 dimensionality, and thus has a total of formula_53 elements.\nSo for every input there are formula_2 transfer functions with one for each output.\nThis is why the state-space representation can easily be the preferred choice for multiple-input, multiple-output (MIMO) systems. The Rosenbrock system matrix provides a bridge between the state-space representation and its transfer function.\n\nAny given transfer function which is strictly proper can easily be transferred into state-space by the following approach (this example is for a 4-dimensional, single-input, single-output system):\n\nGiven a transfer function, expand it to reveal all coefficients in both the numerator and denominator. This should result in the following form:\n\nThe coefficients can now be inserted directly into the state-space model by the following approach:\n\nThis state-space realization is called controllable canonical form because the resulting model is guaranteed to be controllable (i.e., because the control enters a chain of integrators, it has the ability to move every state).\nThe transfer function coefficients can also be used to construct another type of canonical form\n\nThis state-space realization is called observable canonical form because the resulting model is guaranteed to be observable (i.e., because the output exits from a chain of integrators, every state has an effect on the output).\n\nTransfer functions which are only proper (and not strictly proper) can also be realised quite easily. The trick here is to separate the transfer function into two parts: a strictly proper part and a constant. \n\nThe strictly proper transfer function can then be transformed into a canonical state-space realization using techniques shown above. The state-space realization of the constant is trivially formula_61. Together we then get a state-space realization with matrices \"A\", \"B\" and \"C\" determined by the strictly proper part, and matrix \"D\" determined by the constant.\n\nHere is an example to clear things up a bit:\nwhich yields the following controllable realization\n\nNotice how the output also depends directly on the input. This is due to the formula_65 constant in the transfer function.\n\nA common method for feedback is to multiply the output by a matrix \"K\" and setting this as the input to the system: formula_66.\nSince the values of \"K\" are unrestricted the values can easily be negated for negative feedback.\nThe presence of a negative sign (the common notation) is merely a notational one and its absence has no impact on the end results.\n\nbecomes\n\nsolving the output equation for formula_71 and substituting in the state equation results in\n\nThe advantage of this is that the eigenvalues of \"A\" can be controlled by setting \"K\" appropriately through eigendecomposition of formula_74.\nThis assumes that the closed-loop system is controllable or that the unstable eigenvalues of \"A\" can be made stable through appropriate choice of \"K\".\n\nFor a strictly proper system \"D\" equals zero. Another fairly common situation is when all states are outputs, i.e. \"y\" = \"x\", which yields \"C\" = \"I\", the Identity matrix. This would then result in the simpler equations\n\nThis reduces the necessary eigendecomposition to just formula_77.\n\nIn addition to feedback, an input, formula_78, can be added such that formula_79.\n\nbecomes\n\nsolving the output equation for formula_71 and substituting in the state equation \nresults in\n\nOne fairly common simplification to this system is removing \"D\", which reduces the equations to\n\nA classical linear system is that of one-dimensional movement of an object.\nNewton's laws of motion for an object moving horizontally on a plane and attached to a wall with a spring\n\nwhere\n\nThe state equation would then become\n\nwhere\n\nThe controllability test is then\n\nwhich has full rank for all formula_94 and formula_96.\n\nThe observability test is then\n\nwhich also has full rank.\nTherefore, this system is both controllable and observable.\n\nThe more general form of a state-space model can be written as two functions.\n\nThe first is the state equation and the latter is the output equation.\nIf the function formula_109 is a linear combination of states and inputs then the equations can be written in matrix notation like above.\nThe formula_93 argument to the functions can be dropped if the system is unforced (i.e., it has no inputs).\n\nA classic nonlinear system is a simple unforced pendulum\n\nwhere\nThe state equations are then\n\nwhere\n\nInstead, the state equation can be written in the general form\n\nThe equilibrium/stationary points of a system are when formula_124 and so the equilibrium points of a pendulum are those that satisfy\n\nfor integers \"n\".\n\n\n"}
{"id": "30483271", "url": "https://en.wikipedia.org/wiki?curid=30483271", "title": "Straightening theorem for vector fields", "text": "Straightening theorem for vector fields\n\nIn differential calculus, the domain-straightening theorem states that, given a vector field formula_1 on a manifold, there exist local coordinates formula_2 such that formula_3 in a neighborhood of a point where formula_1 is nonzero. The theorem is also known as straightening out of a vector field.\n\nThe Frobenius theorem in differential geometry can be considered as a higher-dimensional generalization of this theorem.\n\nIt is clear that we only have to find such coordinates at 0 in formula_5. First we write formula_6 where formula_7 is some coordinate system at formula_8. Let formula_9. By linear change of coordinates, we can assume formula_10 Let formula_11 be the solution of the initial value problem formula_12 and let\nformula_14 (and thus formula_15) is smooth by smooth dependence on initial conditions in ordinary differential equations. It follows that\nand, since formula_17, the differential formula_18 is the identity at formula_8. Thus, formula_20 is a coordinate system at formula_8. Finally, since formula_22, we have: formula_23 and so formula_24\nas required.\n\n"}
{"id": "744335", "url": "https://en.wikipedia.org/wiki?curid=744335", "title": "Takens's theorem", "text": "Takens's theorem\n\nIn the study of dynamical systems, a delay embedding theorem gives the conditions under which a chaotic dynamical system can be reconstructed from a sequence of observations of the state of a dynamical system. The reconstruction preserves the properties of the dynamical system that do not change under smooth coordinate changes (i.e., diffeomorphisms), but it does not preserve the geometric shape of structures in phase space.\n\nTakens' theorem is the 1981 delay embedding theorem of Floris Takens. It provides the conditions under which a smooth attractor can be reconstructed from the observations made with a generic function. Later results replaced the smooth attractor with a set of arbitrary box counting dimension and the class of generic functions with other classes of functions.\n\nDelay embedding theorems are simpler to state for\ndiscrete-time dynamical systems.\nThe state space of the dynamical system is a ν-dimensional manifold \"M\". The dynamics is given by a smooth map\n\nAssume that the dynamics \"f\" has a strange attractor \"A\" with box counting dimension \"d\". Using ideas from Whitney's embedding theorem, \"A\" can be embedded in \"k\"-dimensional Euclidean space with\n\nThat is, there is a diffeomorphism φ that maps \"A\" into R such that the derivative of φ has full rank.\n\nA delay embedding theorem uses an \"observation function\" to construct the embedding function. An observation function α must be twice-differentiable and associate a real number to any point of the attractor \"A\". It must also be typical, so its derivative is of full rank and has no special symmetries in its components. The delay embedding theorem states that the function\n\nis an embedding of the strange attractor \"A\".\n\nSuppose the \"d\"-dimensional \nstate vector x evolves according to an unknown but continuous\nand (crucially) deterministic dynamic. Suppose, too, that the\none-dimensional observable \"y\" is a smooth function of \"x\", and “coupled”\nto all the components of \"x\". Now at any time we can look not just at\nthe present measurement \"y(t)\", but also at observations made at times\nremoved from us by multiples of some lag formula_4, etc. If we use\nk lags, we have a k-dimensional vector. One might expect that, as the\nnumber of lags is increased, the motion in the lagged space will become\nmore and more predictable, and perhaps in the limit formula_5 would become\ndeterministic. In fact, the dynamics of the lagged vectors become\ndeterministic at a finite dimension; not only that, but the deterministic\ndynamics are completely equivalent to those of the original state space (More exactly, they are related by a smooth, invertible change of coordinates,\nor diffeomorphism.) The magic embedding dimension \"k\" is\nat most \"2d + 1\", and often less.\n\n\n"}
{"id": "39285158", "url": "https://en.wikipedia.org/wiki?curid=39285158", "title": "Tamara Awerbuch-Friedlander", "text": "Tamara Awerbuch-Friedlander\n\nTamara Eugenia Awerbuch-Friedlander is a biomathematician and public health scientist who worked at the Harvard School of Public Health (HSPH) in Boston, Massachusetts. Her primary research and publications focus on biosocial interactions that cause or contribute to disease. She also is believed to be the first female Harvard faculty member to have had a jury trial for a lawsuit filed against Harvard University for sex discrimination.\n\nTamara Awerbuch was born in Uruguay, lived until the age of 12 in Buenos Aires, Argentina, then moved to Israel with her parents, where her grandparents and parents had lived after they had escaped Nazi Germany just before the Holocaust began. She studied and completed two degrees at Hebrew University in Jerusalem. She studied chemistry and minored in biochemistry and completed the BSc degree in 1965. In 1967, she completed both the Master of Science (MSc) in Physiology and the Master of Education (MEd) degree from Hebrew University. She was certified to teach grades K–12 in Israel, where she lectures and appears on panels and in workshops, as she does also in the United States and elsewhere. She also served for two years in the Israeli army.\n\nIn October 1973, while visiting friends in America, she was offered employment at MIT in Cambridge, Massachusetts, to study chemical carcinogens in tissue cultures, then a recently developed technique. During this period, she worked in the lab studying carcinogenicity in tissue cultures, studied one course each semester, and lived frugally, sharing a house with MIT junior Faculty and graduate students. As one of her allotted courses per semester, in spring of 1974 she first started to study mathematics, taking mathematics and statistics. In summer 1975, she matriculated as a full-time student at MIT, where in 1979 she completed her doctorate in Nutrition and Food Science. She became a US citizen and has resided in the United States since that time. She was recruited in 1983 to the Biostatistics Department of the Harvard T.H. Chan School of Public Health by Department Chair Marvin Zelen. In 1993, she began a long career in the Department of Global Health and Population at the Harvard T.H. Chan School of Public Health.\n\nHer two sons, Danny and Ari, were born in the 1980s and reared in Brookline, Massachusetts. She speaks English, Hebrew, and Spanish fluently and understands and reads German.\n\n\nSince the beginning of this century, she has organized and carried out research on conditions that lead to the emergence, maintenance, and spread of epidemics. Her research encompasses sexually-transmitted diseases (STDs) such as HIV/AIDS, as well as vector-borne diseases, such as Lyme disease, dengue, and Zika virus and Zika fever. Awerbuch-Friedlander recently researched the spread and control of rabies based on an eco-historical analysis. Her work is interdisciplinary, and some of her publications are co-authored with international scientists and members of different departments of the HSPH and the Massachusetts Institute of Technology.\n\nSome of her analytical mathematical models led to fundamental epidemiological discoveries, for example, that oscillations are an intrinsic property of tick dynamics. She presented her work in many international conferences and at the Isaac Newton Institute of Mathematical Sciences in Cambridge, England, where she was invited to participate in the Program on Models of Epidemics.\n\nAwerbuch-Friedlander is a founding member of the New and Resurgent Disease Working Group. Within this context, she was involved in organizing a conference in Woods Hole, Massachusetts, on the emergence and resurgence of diseases, where she led the workshop on Mathematical Modeling. In addition, she established international collaborations, such as with Israeli scientists on emerging infectious diseases in the Middle East, with Cuban scientists on infectious diseases of plants and the development of general methodologies, and with Brazilian scientists on the development of concepts to guide effective surveillance. In the late 1990s, Awerbuch-Friedlander was co-investigator in a project, \"Why New and Resurgent Diseases Caught Public Health by Surprise and a Strategy to Prevent This\" (supported by the Robert Wood Johnson Foundation). At Harvard T.H. Chan School of Public Health, Awerbuch-Friedlander co-chaired the committee on Bio- and Public Health Mathematics. Some of her research papers were the result of collaboration with students through the course Mathematical Models in Biology, which had large portions dedicated to infectious diseases. She is indeed interested in public health education and has developed for high school adolescents educational software based on models for determining the risk that an individual with certain risky sexual behaviors actually would become infected with HIV. These models helped risk-prone youth, parents, educators, community health leaders, and public health researchers explore how changes in sexual behavior impact their probability of contracting HIV.\n\nAwerbuch-Friedlander also chaired the planning committee for the 85th birthday celebration of Richard Levins, founder of the Human Ecology program in the Global Health and Population Department of the Harvard School of Public Health, a three-day conference with the Hegelian theme \"The Truth is the Whole\" held in mid-2015 at the Harvard School of Public Health, focusing on the manifold contributions in models of complexity theory and holistic research from mathematical biologist Levins and his colleagues, students, and disciples, who broadly are interested in complex systems biology. The September 2018 book, The Truth Is the Whole: Essays in Honor of Richard Levins (ISBN \n0998889105/9780998889108), in which she was co-editor with Maynard Clark and Dr. Peter Taylor, includes parts of the proceedings from over 20 contributors from that Harvard symposium.\n\nAlthough Theda Skocpol had alleged gender bias in denial of tenure as early as 1980, Awerbuch-Friedlander is believed to be the first female Harvard Faculty member to file a lawsuit against Harvard University for sex discrimination. The suit was \"filed with the Middlesex County Superior Court in June 1997.\" Encouraged by her mentors, Richard Levins and Marvin Zelen, Awerbuch-Friedlander sought \"nearly $1 million in lost wages and benefits, as well as a promotion at the HSPH\" and argued \"that Fineberg refused to promote her to a tenure-track position because she is a woman, despite the positive recommendation of the HSPH's selection committee of appointment and re-appointment (SCARP).\" Intermittently from 1998 through 2007, the gender discrimination case was covered by the \"Harvard Crimson\" (campus media), \"The Boston Globe\" (local media), and \"Science\" magazine (professional and scientific print media). \"Science\" documented the case developments of the sex-discrimination case in its \"News of the Week: Women in Science\" section. and in \"Science\"'s SCIENCESCOPE two months later. Her sex discrimination lawsuit was based upon Harvard's denial of tenure to her, despite her significant accomplishments in her fields of expertise, biomathematics, epidemiology, biostatistics, and public health. The University argued that no tenure track positions were open in her new department, after she had been reassigned from one department to another.\n\n\n"}
{"id": "48511100", "url": "https://en.wikipedia.org/wiki?curid=48511100", "title": "Tape diagram", "text": "Tape diagram\n\nA tape diagram, also known as a bar model, is a pictorial representation of ratios. In mathematics education, it is used to solve word problems. So lets do this problem that has been given to us by teachers all around. \"A boy has won 15 games, for every 3 games that he wins, he loses 2, how many games has the little boy lost?\"\n\nA math problem reads: A boy has won 15 games. His ratio for him is 3:2. The tape diagram would look something like this:\nUsing the above diagram, it can be concluded that the boy has lost 10 playing times.\n\nThe tape diagram works by showing different ratios. In the example before it shows how a boy has won 15 games. So if his win to loss ratio is 3:2 it would look like this. How many games has he lost?(3x5)\n\nNow how did you get those three 5s you ask.\nSo the answer to how many games did he lose is 10. Because 5+5=10\n"}
{"id": "20086199", "url": "https://en.wikipedia.org/wiki?curid=20086199", "title": "The Proteolysis Map", "text": "The Proteolysis Map\n\nThe Proteolysis MAP (PMAP) is an integrated web resource focused on proteases.\n\nPMAP is to aid the protease researchers in reasoning about proteolytic networks and metabolic pathways.\n\nPMAP was originally created at the Burnham Institute for Medical Research, La Jolla, California. In 2004 the National Institutes of Health (NIH) selected a team led by Jeffrey W. Smith, to establish the Center on Proteolytic Pathways (CPP). As part of the NIH Roadmap for Biomedical research, the center develops technology to study the behavior of proteins and to disburse that knowledge to the scientific community at large.\n\nProteases are a class of enzymes that regulate much of what happens in the human body, both inside the cell and out, by cleaving peptide bonds in proteins. Through this activity, they govern the four essential cell functions: differentiation, motility, division and cell death — and activate important extracellular episodes, such as the biochemical cascade effect in blood clotting. Simply stated, life could not exist without them. Extensive on-line classification system for proteases (also referred as peptidases) is deposited in the MEROPS database.\n\nProteolytic pathways, or proteolysis, are the series of events controlled by proteases that occur in response to specific stimuli. In addition to the clotting of blood, the production of insulin can be viewed as a proteolytic pathway, as the activation, regulation and inhibition of that protein is the result of proteases reacting to changing glucose levels and triggering other proteases downstream.\n\nPMAP integrates five databases.\nProteaseDB and SubstrateDB, are driven by an automated annotation pipeline that generates dynamic ‘Molecule Pages’, rich in molecular information. CutDB has information on more than 6,600 proteolytic events, and ProfileDB is dedicated to information of the substrate recognition specificity of proteases. PathwayDB, just begun accumulation of metabolic pathways whose function can be dynamically modeled in a rule-based manner. Hypothetical networks are inferred by semi-automated culling from the literature. Additionally, protease software tools are available for the analysis of individual proteases and proteome-wide data sets.\n\nPopular destinations in PMAP are Protease Molecule Pages and Substrate Molecule Pages. Protease Molecule Pages show recent news in PubMed literature of the protease, known proteolytic events, protein domain location and protein structure view, as well as a cross annotation in other bioinformatic databases section. Substrate Molecule Pages display protein domains and experimentally derived protease cut-sites for a given protein target of interest.\n\n\n"}
{"id": "26469", "url": "https://en.wikipedia.org/wiki?curid=26469", "title": "Μ-recursive function", "text": "Μ-recursive function\n\nIn mathematical logic and computer science, the μ-recursive functions are a class of partial functions from natural numbers to natural numbers that are \"computable\" in an intuitive sense. In computability theory, it is shown that the μ-recursive functions are precisely the functions that can be computed by Turing machines. The μ-recursive functions are closely related to primitive recursive functions, and their inductive definition (below) builds upon that of the primitive recursive functions. However, not every μ-recursive function is a primitive recursive function—the most famous example is the Ackermann function.\n\nOther equivalent classes of functions are the λ-recursive functions and the functions that can be computed by Markov algorithms.\n\nThe set of all recursive functions is known as R in computational complexity theory.\n\nThe μ-recursive functions (or partial μ-recursive functions) are partial functions that take finite tuples of natural numbers and return a single natural number. They are the smallest class of partial functions that includes the initial functions and is closed under composition, primitive recursion, and the μ operator.\n\nThe smallest class of functions including the initial functions and closed under composition and primitive recursion (i.e. without minimisation) is the class of primitive recursive functions. While all primitive recursive functions are total, this is not true of partial recursive functions; for example, the minimisation of the successor function is undefined. The primitive recursive functions are a subset of the total recursive functions, which are a subset of the partial recursive functions. For example, the Ackermann function can be proven to be total recursive, but not primitive.\n\nInitial or \"basic\" functions: (In the following the subscripting is per Kleene (1952) p. 219. For more about some of the various symbolisms found in the literature see Symbolism below.)\n\n\nOperators:\n\n\nThe strong equality operator formula_23 can be used to compare partial μ-recursive functions. This is defined for all partial functions \"f\" and \"g\" so that\nholds if and only if for any choice of arguments either both functions are defined and their values are equal or both functions are undefined.\n\nIn the equivalence of models of computability, a parallel is drawn between Turing machines that do not terminate for certain inputs and an undefined result for that input in the corresponding partial recursive function.\nThe unbounded search operator is not definable by the rules of primitive recursion as those do not provide a mechanism for \"infinite loops\" (undefined values).\n\nA normal form theorem due to Kleene says that for each \"k\" there are primitive recursive functions formula_25 and formula_26 such that for any μ-recursive function formula_27 with \"k\" free variables there is an \"e\" such that \nThe number \"e\" is called an index or Gödel number for the function \"f\". A consequence of this result is that any μ-recursive function can be defined using a single instance of the μ operator applied to a (total) primitive recursive function.\n\nMinsky (1967) observes (as does Boolos-Burgess-Jeffrey (2002) pp. 94–95) that the U defined above is in essence the μ-recursive equivalent of the universal Turing machine:\n\nA number of different symbolisms are used in the literature. An advantage to using the symbolism is a derivation of a function by \"nesting\" of the operators one inside the other is easier to write in a compact form. In the following we will abbreviate the string of parameters x, ..., x as x: \n\n\n\n\n\nExample: Kleene gives an example of how to perform the recursive derivation of f(b, a) = b + a (notice reversal of variables a and b). He starts with 3 initial functions \n\nHe arrives at:\n\n\n\n\n"}
