{"id": "828061", "url": "https://en.wikipedia.org/wiki?curid=828061", "title": "A Mathematical Theory of Communication", "text": "A Mathematical Theory of Communication\n\n\"A Mathematical Theory of Communication\" is an article by mathematician Claude E. Shannon published in \"Bell System Technical Journal\" in 1948. It was renamed The Mathematical Theory of Communication in the book of the same name, a small but significant title change after realizing the generality of this work.\n\nThe article was the founding work of the field of information theory. It was later published in 1949 as a book titled \"The Mathematical Theory of Communication\" (), which was published as a paperback in 1963 (). The book contains an additional article by Warren Weaver, providing an overview of the theory for a more general audience. Shannon's article laid out the basic elements of communication:\n\n\nIt also developed the concepts of information entropy and redundancy, and introduced the term bit (which Shannon credited to John Tukey) as a unit of information. It was also in this paper that the Shannon–Fano coding technique was proposed - a technique developed in conjunction with Robert Fano.\n\n"}
{"id": "59186649", "url": "https://en.wikipedia.org/wiki?curid=59186649", "title": "Alessandra Lunardi", "text": "Alessandra Lunardi\n\nAlessandra Lunardi (born 1958) is an Italian mathematician specializing in mathematical analysis. She is a professor in the department of mathematics and computer science at the University of Parma.\n\nLunardi was educated at the University of Pisa, completing her undergraduate studies there in 1980 and earning a Ph.D. there in 1983. Her dissertation, \"Analyticity of the maximal solution to fully nonlinear equations in Banach spaces\", was supervised by Giuseppe Da Prato.\n\nAfter continuing on at Pisa as a researcher from 1984 to 1987, she was hired as a full professor at the University of Cagliari in 1987, and moved to Parma in 1994.\n\nLunardi is the author of \"Analytic semigroups and optimal regularity in parabolic problems\" (Birkhäuser, 1995, reprinted 2013) and of \"Interpolation theory\" (Edizioni della Normale, 1998, 3rd ed., 2018). With G. Da Prato, P. C. Kunstmann, I. Lasiecka, R. Schnaubelt, and L. Weis, she is a co-author of \"Functional Analytic Methods for Evolution Equations\" (Springer, 2004).\n\nLunardi is one of six editors-in-chief of the journal \"Nonlinear Differential Equations and Applications (NoDEA)\".\nShe also served as editor-in-chief of \"Rivista di Matematica della Università di Parma\" for Series 7 of the journal, from 2002 to 2008.\n\nIn 1987, Lunardi won the Bartolozzi Prize of the Italian Mathematical Union. In 2017, she won the of the Istituto Lombardo Accademia di Scienze e Lettere.\n\n"}
{"id": "5784666", "url": "https://en.wikipedia.org/wiki?curid=5784666", "title": "Ambient construction", "text": "Ambient construction\n\nIn conformal geometry, the ambient construction refers to a construction of Charles Fefferman and Robin Graham for which a conformal manifold of dimension \"n\" is realized (\"ambiently\") as the boundary of a certain Poincaré manifold, or alternatively as the celestial sphere of a certain pseudo-Riemannian manifold.\n\nThe ambient construction is canonical in the sense that it is performed only using the conformal class of the metric: it is conformally invariant. However, the construction only works asymptotically, up to a certain order of approximation. There is, in general, an obstruction to continuing this extension past the critical order. The obstruction itself is of tensorial character, and is known as the (conformal) obstruction tensor. It is, along with the Weyl tensor, one of the two primitive invariants in conformal differential geometry.\n\nAside from the obstruction tensor, the ambient construction can be used to define a class of conformally invariant differential operators known as the GJMS operators.\n\nA related construction is the tractor bundle.\n\nThe model flat geometry for the ambient construction is the future null cone in Minkowski space, with the origin deleted. The celestial sphere at infinity is the conformal manifold \"M\", and the null rays in the cone determine a line bundle over \"M\". Moreover, the null cone carries a metric which degenerates in the direction of the generators of the cone.\n\nThe ambient construction in this flat model space then asks: if one is provided with such a line bundle, along with its degenerate metric, to what extent is it possible to \"extend\" the metric off the null cone in a canonical way, thus recovering the ambient Minkowski space? In formal terms, the degenerate metric supplies a Dirichlet boundary condition for the extension problem and, as it happens, the natural condition is for the extended metric to be Ricci flat (because of the normalization of the normal conformal connection.)\n\nThe ambient construction generalizes this to the case when \"M\" is conformally curved, first by constructing a natural null line bundle \"N\" with a degenerate metric, and then solving the associated Dirichlet problem on \"N\" × (-1,1).\n\nThis section provides an overview of the construction, first of the null line bundle, and then of its ambient extension.\n\nSuppose that \"M\" is a conformal manifold, and that [\"g\"] denotes the conformal metric defined on \"M\". Let π : \"N\" → \"M\" denote the tautological subbundle of T\"M\" ⊗ T\"M\" defined by all representatives of the conformal metric. In terms of a fixed background metric \"g\", \"N\" consists of all positive multiples ω\"g\" of the metric. There is a natural action of R on \"N\", given by\n\nMoreover, the total space of \"N\" carries a tautological degenerate metric, for if \"p\" is a point of the fibre of π : \"N\" → \"M\" corresponding to the conformal representative \"g\", then let\nThis metric degenerates along the vertical directions. Furthermore, it is homogeneous of degree 2 under the R action on \"N\":\n\nLet \"X\" be the vertical vector field generating the scaling action. Then the following properties are immediate:\n\nLet \"N\" = \"N\" × (-1,1), with the natural inclusion \"i\" : \"N\" → \"N\". The dilations δ extend naturally to \"N\", and hence so does the generator \"X\" of dilation.\n\nAn ambient metric on \"N\" is a Lorentzian metric \"h\" such that\n\nSuppose that a fixed representative of the conformal metric \"g\" and a local coordinate system \"x\" = (\"x\") are chosen on \"M\". These induce coordinates on \"N\" by identifying a point in the fibre of \"N\" with (\"x\",\"t\"\"g\"(\"x\")) where \"t\" > 0 is the fibre coordinate. (In these coordinates, \"X\" = \"t\" ∂.) Finally, if ρ is a defining function of \"N\" in \"N\" which is homogeneous of degree 0 under dilations, then (\"x\",\"t\",ρ) are coordinates of \"N\". Furthermore, any extension metric which is homogeneous of degree 2 can be written in these coordinates in the form:\nwhere the \"g\" are \"n\" functions with \"g\"(\"x\",0) = \"g\"(\"x\"), the given conformal representative.\n\nAfter some calculation one shows that the Ricci flatness is equivalent to the following differential equation, where the prime is differentiation with respect to ρ:\nOne may then formally solve this equation as a power series in ρ to obtain the asymptotic development of the ambient metric off the null cone. For example, substituting ρ = 0 and solving gives\nwhere \"P\" is the Schouten tensor. Next, differentiating again and substituting the known value of \"g\"(\"x\",0) into the equation, the second derivative can be found to be a multiple of the Bach tensor. And so forth.\n\n"}
{"id": "596622", "url": "https://en.wikipedia.org/wiki?curid=596622", "title": "Arzelà–Ascoli theorem", "text": "Arzelà–Ascoli theorem\n\nThe Arzelà–Ascoli theorem is a fundamental result of mathematical analysis giving necessary and sufficient conditions to decide whether every sequence of a given family of real-valued continuous functions defined on a closed and bounded interval has a uniformly convergent subsequence. The main condition is the equicontinuity of the family of functions. The theorem is the basis of many proofs in mathematics, including that of the Peano existence theorem in the theory of ordinary differential equations, Montel's theorem in complex analysis, and the Peter–Weyl theorem in harmonic analysis.\n\nThe notion of equicontinuity was introduced in the late 19th century by the Italian mathematicians Cesare Arzelà and Giulio Ascoli. A weak form of the theorem was proven by , who established the sufficient condition for compactness, and by , who established the necessary condition and gave the first clear presentation of the result. A further generalization of the theorem was proven by , to sets of real-valued continuous functions with domain a compact metric space . Modern formulations of the theorem allow for the domain to be compact Hausdorff and for the range to be an arbitrary metric space. More general formulations of the theorem exist that give necessary and sufficient conditions for a family of functions from a compactly generated Hausdorff space into a uniform space to be compact in the compact-open topology. .\n\nBy definition, a sequence of continuous functions on an interval is \"uniformly bounded\" if there is a number such that\n\nfor every function belonging to the sequence, and every . (Here, must be independent of and .) \n\nThe sequence is said to be \"equicontinuous\" if, for every and , there exists a such that\n\nwhenever for all functions in the sequence. (Here, may depend on and , but not on or .)\n\nOne version of the theorem can be stated as follows:\n\nThe hypotheses of the theorem are satisfied by a uniformly bounded sequence of differentiable functions with uniformly bounded derivatives. Indeed, uniform boundedness of the derivatives implies by the mean value theorem that for all and ,\n\nwhere \"K\" is the supremum of the derivatives of functions in the sequence and is independent of . So, given , let to verify the definition of equicontinuity of the sequence. This proves the following corollary:\n\n\nIf, in addition, the sequence of second derivatives is also uniformly bounded, then the derivatives also converge uniformly (up to a subsequence), and so on. Another generalization holds for continuously differentiable functions. Suppose that the functions are continuously differentiable with derivatives . Suppose that \"f\"′ are uniformly equicontinuous and uniformly bounded, and that the sequence is pointwise bounded (or just bounded at a single point). Then there is a subsequence of the converging uniformly to a continuously differentiable function.\n\nThe diagonalization argument can also be used to show that a family of infinitely differentiable functions, whose derivatives of each order are uniformly bounded, has a uniformly convergent subsequence, all of whose derivative are also uniformly convergent. This is important particularly in the theory of distributions.\n\nThe argument given above proves slightly more, specifically\n\n\nThe limit function is also Lipschitz continuous with the same value for the Lipschitz constant. A slight refinement is\n\n\nThis holds more generally for scalar functions on a compact metric space satisfying a Hölder condition with respect to the metric on .\n\nThe Arzelà–Ascoli theorem holds, more generally, if the functions take values in -dimensional Euclidean space , and the proof is very simple: just apply the -valued version of the Arzelà–Ascoli theorem times to extract a subsequence that converges uniformly in the first coordinate, then a sub-subsequence that converges uniformly in the first two coordinates, and so on. The above examples generalize easily to the case of functions with values in Euclidean space.\n\nThe proof is essentially based on a diagonalization argument. The simplest case is of real-valued functions on a closed and bounded interval:\n\n\nFix an enumeration {\"x\"} of rational numbers in \"I\". Since F is uniformly bounded, the set of points {\"f\"(\"x\")} is bounded, and hence by the Bolzano–Weierstrass theorem, there is a sequence {\"f\"} of distinct functions in F such that {\"f\"(\"x\")} converges. Repeating the same argument for the sequence of points {\"f\"(\"x\")}, there is a subsequence {\"f\"} of {\"f\"} such that {\"f\"(\"x\")} converges.\n\nBy induction this process can be continued forever, and so there is a chain of subsequences\n\nsuch that, for each \"k\" = 1, 2, 3, ..., the subsequence {\"f\"} converges at \"x\", ..., \"x\". Now form the diagonal subsequence {\"f} whose \"m\"th term \"f\" is the \"m\"th term in the \"m\"th subsequence {\"f\"}. By construction, \"f\" converges at every rational point of \"I\".\n\nTherefore, given any and rational \"x\" in \"I\", there is an integer such that\n\nSince the family F is equicontinuous, for this fixed \"ε\" and for every \"x\" in \"I\", there is an open interval \"U\" containing \"x\" such that\n\nfor all \"f\" ∈ F and all \"s\", \"t\" in \"I\" such that .\n\nThe collection of intervals \"U\", \"x\" ∈ \"I\", forms an open cover of \"I\". Since \"I\"  is compact, this covering admits a finite subcover . There exists an integer \"K\" such that each open interval \"U\", , contains a rational \"x\" with . Finally, for any \"t\" ∈ \"I\", there are \"j\" and \"k\" so that \"t\" and \"x\" belong to the same interval \"U\". For this choice of \"k\",\n\nfor all Consequently, the sequence {\"f\"} is uniformly Cauchy, and therefore converges to a continuous function, as claimed. This completes the proof.\n\nThe definitions of boundedness and equicontinuity can be generalized to the setting of arbitrary compact metric spaces and, more generally still, compact Hausdorff spaces. Let \"X\" be a compact Hausdorff space, and let \"C\"(\"X\") be the space of real-valued continuous functions on \"X\". A subset is said to be \"equicontinuous\" if for every \"x\" ∈ \"X\" and every , \"x\" has a neighborhood \"U\" such that\n\nA set is said to be \"pointwise bounded\" if for every \"x\" ∈ \"X\",\n\nA version of the Theorem holds also in the space \"C\"(\"X\") of real-valued continuous functions on a compact Hausdorff space \"X\" :\n\nThe Arzelà–Ascoli theorem is thus a fundamental result in the study of the algebra of continuous functions on a compact Hausdorff space.\n\nVarious generalizations of the above quoted result are possible. For instance, the functions can assume values in a metric space or (Hausdorff) topological vector space with only minimal changes to the statement (see, for instance, , ):\n\nHere pointwise relatively compact means that for each \"x\" ∈ \"X\", the set is relatively compact in \"Y\".\n\nThe proof given can be generalized in a way that does not rely on the separability of the domain. On a compact Hausdorff space \"X\", for instance, the equicontinuity is used to extract, for each ε = 1/\"n\", a finite open covering of \"X\" such that the oscillation of any function in the family is less than ε on each open set in the cover. The role of the rationals can then be played by a set of points drawn from each open set in each of the countably many covers obtained in this way, and the main part of the proof proceeds exactly as above.\n\nWhereas most formulations of the Arzelà–Ascoli theorem assert sufficient conditions for a family of functions to be (relatively) compact in some topology, these conditions are typically also necessary. For instance, if a set F is compact in \"C\"(\"X\"), the Banach space of real-valued continuous functions on a compact Hausdorff space with respect to its uniform norm, then it is bounded in the uniform norm on \"C\"(\"X\") and in particular is pointwise bounded. Let \"N\"(\"ε\", \"U\") be the set of all functions in F whose oscillation over an open subset \"U\" ⊂ \"X\" is less than \"ε\":\n\nFor a fixed \"x\"∈\"X\" and \"ε\", the sets \"N\"(\"ε\", \"U\") form an open covering of F as \"U\" varies over all open neighborhoods of \"x\". Choosing a finite subcover then gives equicontinuity.\n\n\n\n\n\n"}
{"id": "46676", "url": "https://en.wikipedia.org/wiki?curid=46676", "title": "Banach fixed-point theorem", "text": "Banach fixed-point theorem\n\nIn mathematics, the Banach–Caccioppoli fixed-point theorem (also known as the contraction mapping theorem or contraction mapping principle) is an important tool in the theory of metric spaces; it guarantees the existence and uniqueness of fixed points of certain self-maps of metric spaces, and provides a constructive method to find those fixed points. The theorem is named after Stefan Banach (1892–1945) and Renato Caccioppoli (1904–1959), and was first stated by Banach in 1922. Caccioppoli independently proved the theorem in 1931.\n\n\"Definition.\" Let formula_1 be a metric space. Then a map formula_2 is called a contraction mapping on formula_3 if there exists formula_4 such that\nfor all formula_6 in formula_3.\n\nBanach Fixed Point Theorem. Let formula_1 be a non-empty complete metric space with a contraction mapping formula_2. Then \"T\" admits a unique fixed-point \"x*\" in \"X\" (i.e. \"T\"(\"x*\") = \"x*\"). Furthermore, \"x*\" can be found as follows: start with an arbitrary element \"x\" in \"X\" and define a sequence {\"x\"} by \"x\" = \"T\"(\"x\"), then .\n\n\"Remark 1.\" The following inequalities are equivalent and describe the speed of convergence:\n\nAny such value of \"q\" is called a \"Lipschitz constant\" for \"T\", and the smallest one is sometimes called \"the best Lipschitz constant\" of \"T\".\n\n\"Remark 2.\" \"d\"(\"T\"(\"x\"), \"T\"(\"y\")) < \"d\"(\"x\", \"y\") for all \"x\" ≠ \"y\" is in general not enough to ensure the existence of a fixed point, as is shown by the map \"T\" : [1, ∞) → [1, ∞), \"T\"(\"x\") = \"x\" + 1/\"x\", which lacks a fixed point. However, if \"X\" is compact, then this weaker assumption does imply the existence and uniqueness of a fixed point, that can be easily found as a minimizer of \"d\"(\"x\", \"T\"(\"x\")), indeed, a minimizer exists by compactness, and has to be a fixed point of \"T\". It then easily follows that the fixed point is the limit of any sequence of iterations of \"T\".\n\n\"Remark 3.\" When using the theorem in practice, the most difficult part is typically to define \"X\" properly so that \"T\"(\"X\") ⊆ \"X\".\n\nLet \"x\" ∈ \"X\" be arbitrary and define a sequence {\"x\"} by setting \"x\" = \"T\"(\"x\"). We first note that for all \"n\" ∈ N, we have the inequality\n\nThis follows by induction on \"n\", using the fact that \"T\" is a contraction mapping. Then we can show that {\"x\"} is a Cauchy sequence. In particular, let \"m\", \"n\" ∈ N such that \"m\" > \"n\":\n\nLet ε > 0 be arbitrary, since \"q\" ∈ [0, 1), we can find a large \"N\" ∈ N so that\n\nTherefore, by choosing \"m\" and \"n\" greater than \"N\" we may write:\n\nThis proves that the sequence {\"x\"} is Cauchy. By completeness of (\"X\",\"d\"), the sequence has a limit \"x*\" ∈ \"X\". Furthermore, \"x*\" must be a fixed point of \"T\":\n\nAs a contraction mapping, \"T\" is continuous, so bringing the limit inside \"T\" was justified. Lastly, \"T\" cannot have more than one fixed point in (\"X\",\"d\"), since any pair of distinct fixed points \"p\" and \"p\" would contradict the contraction of \"T\":\n\n\nSeveral converses of the Banach contraction principle exist. The following is due to Czesław Bessaga, from 1959:\n\nLet \"f\" : \"X\" → \"X\" be a map of an abstract set such that each iterate \"f\" has a unique fixed point. Let \"q\" ∈ (0, 1), then there exists a complete metric on \"X\" such that \"f\" is contractive, and \"q\" is the contraction constant.\n\nIndeed, very weak assumptions suffice to obtain such a kind of converse. For example if \"f\" : \"X\" → \"X\" is a map on a \"T\" topological space with a unique fixed point \"a\", such that for each \"x\" in \"X\" we have \"f\"(\"x\") → \"a\", then there already exists a metric on \"X\" with respect to which \"f\" satisfies the conditions of the Banach contraction principle with contraction constant 1/2. In this case the metric is in fact an ultrametric.\n\nThere are a number of generalizations (some of which are immediate corollaries).\n\nLet \"T\" : \"X\" → \"X\" be a map on a complete non-empty metric space. Then, for example, some generalizations of the Banach fixed-point theorem are:\nIn applications, the existence and unicity of a fixed point often can be shown directly with the standard Banach fixed point theorem, by a suitable choice of the metric that makes the map \"T\" a contraction. Indeed, the above result by Bessaga strongly suggests to look for such a metric. See also the article on fixed point theorems in infinite-dimensional spaces for generalizations.\n\nA different class of generalizations arise from suitable generalizations of the notion of metric space, e.g. by weakening the defining axioms for the notion of metric. Some of these have applications, e.g., in the theory of programming semantics in theoretical computer science.\n\n\nAn earlier version of this article was posted on Planet Math. This article is open content.\n"}
{"id": "17220224", "url": "https://en.wikipedia.org/wiki?curid=17220224", "title": "Bartlett's method", "text": "Bartlett's method\n\nIn time series analysis, Bartlett's method (also known as the method of averaged periodograms), is used for estimating power spectra. It provides a way to reduce the variance of the periodogram in exchange for a reduction of resolution, compared to standard periodograms. A final estimate of the spectrum at a given frequency is obtained by averaging the estimates from the periodograms (at the same frequency) derived from a non-overlapping portions of the original series.\n\nThe method is used in physics, engineering, and applied mathematics. Common applications of Bartlett's method are frequency response measurements and general spectrum analysis.\n\nThe method is named after M. S. Bartlett who first proposed it.\n\nBartlett’s method consists of the following steps:\n\nThe end result is an array of power measurements vs. frequency \"bin\".\n\n\n"}
{"id": "1307911", "url": "https://en.wikipedia.org/wiki?curid=1307911", "title": "Bootstrap aggregating", "text": "Bootstrap aggregating\n\nBootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n\nGiven a standard training set formula_1 of size \"n\", bagging generates \"m\" new training sets formula_2, each of size \"n′\", by sampling from \"D\" uniformly and with replacement. By sampling with replacement, some observations may be repeated in each formula_2. If \"n′\"=\"n\", then for large \"n\" the set formula_2 is expected to have the fraction (1 - 1/\"e\") (≈63.2%) of the unique examples of \"D\", the rest being duplicates. This kind of sample is known as a bootstrap sample. The \"m\" models are fitted using the above \"m\" bootstrap samples and combined by averaging the output (for regression) or voting (for classification).\n\nBagging leads to \"improvements for unstable procedures\" (Breiman, 1996), which include, for example, artificial neural networks, classification and regression trees, and subset selection in linear regression (Breiman, 1994). An interesting application of bagging showing improvement in preimage learning is provided here. On the other hand, it can mildly degrade the performance of stable methods such as K-nearest neighbors (Breiman, 1996).\n\nTo illustrate the basic principles of bagging, below is an analysis on the relationship between ozone and temperature (data from Rousseeuw and Leroy (1986), analysis done in R).\n\nThe relationship between temperature and ozone in this data set is apparently non-linear, based on the scatter plot. To mathematically describe this relationship, LOESS smoothers (with bandwidth 0.5) are used. \nInstead of building a single smoother from the complete data set, 100 bootstrap samples of the data were drawn. Each sample is different from the original data set, yet resembles it in distribution and variability. For each bootstrap sample, a LOESS smoother was fit. Predictions from these 100 smoothers were then made across the range of the data. The first 10 predicted smooth fits appear as grey lines in the figure below. The lines are clearly very \"wiggly\" and they overfit the data - a result of the bandwidth being too small.\n\nBy taking the average of 100 smoothers, each fitted to a subset of the original data set, we arrive at one bagged predictor (red line). Clearly, the mean is more stable and there is less overfit.\n\nBagging (Bootstrap aggregating) was proposed by Leo Breiman in 1994 to improve classification by combining classifications of randomly generated training sets. See Breiman, 1994. Technical Report No. 421.\n\n"}
{"id": "37165004", "url": "https://en.wikipedia.org/wiki?curid=37165004", "title": "Bucket evaluations", "text": "Bucket evaluations\n\nIn statistics, bucket evaluations is a method for correlating vectors. This method is a non-parametric, unsupervised correlation method first published in 2012 by Shabtai et al.\n\nBucket evaluations was initially constructed for genetic research, and was used for finding a new potential anti-cancer drug.\n\nBucket evaluations is named after the technique used to compare vectors in a matrix. Values in the vector are compared in sections (buckets). The buckets are set in a descending order, where the smallest buckets hold the highest scores, and have the strongest effect on the final correlation score. The similarity between vectors is calculated by comparing the ranks of the scores in each bucket, which are summed up to a similarity score.\n"}
{"id": "2597665", "url": "https://en.wikipedia.org/wiki?curid=2597665", "title": "Bulgarian solitaire", "text": "Bulgarian solitaire\n\nIn mathematics and game theory, Bulgarian solitaire is a card game that was introduced by Martin Gardner.\n\nIn the game, a pack of formula_1 cards is divided into several piles. Then for each pile, remove one card; collect the removed cards together to form a new pile (piles of zero size are ignored).\n\nIf formula_1 is a triangular number (that is, formula_3 for some formula_4), then it is known that Bulgarian solitaire will reach a stable configuration in which the sizes of the piles are formula_5. This state is reached in formula_6 moves or fewer. If formula_1 is not triangular, no stable configuration exists and a limit cycle is reached.\n\nIn \"random Bulgarian solitaire\" or \"stochastic Bulgarian solitaire\" a pack of formula_1 cards is divided into several piles. Then for each pile, either leave it intact or, with a fixed probability formula_9, remove one card; collect the removed cards together to form a new pile (piles of zero size are ignored). This is a finite irreducible Markov chain.\n\nIn 2004, Brazilian probabilist of Russian origin Serguei Popov showed that stochastic Bulgarian solitaire spends \"most\" of its time in a \"roughly\" triangular distribution.\n\n"}
{"id": "320819", "url": "https://en.wikipedia.org/wiki?curid=320819", "title": "Cantor function", "text": "Cantor function\n\nIn mathematics, the Cantor function is an example of a function that is continuous, but not absolutely continuous. It is also referred to as the Cantor ternary function, the Lebesgue function, Lebesgue's singular function, the Cantor-Vitali function, the Devil's staircase, the Cantor staircase function, and the Cantor-Lebesgue function. introduced the Cantor function and mentioned that Scheeffer pointed out that it was a counterexample to an extension of the fundamental theorem of calculus claimed by Harnack. The Cantor function was discussed and popularized by , and .\n\nSee figure. To formally define the Cantor function \"c\" : [0,1] → [0,1], let \"x\" be in [0,1] and obtain \"c\"(\"x\") by the following steps:\n\n\nFor example:\n\nThe Cantor function challenges naive intuitions about continuity and measure; though it is continuous everywhere and has zero derivative almost everywhere, formula_1 goes from 0 to 1 as formula_2 goes from 0 to 1, and takes on every value in between. The Cantor function is the most frequently cited example of a real function that is uniformly continuous (precisely, it is Hölder continuous of exponent \"α\" = log 2/log 3) but not absolutely continuous. It is constant on intervals of the form (0.\"x\"\"x\"\"x\"...\"x\"022222..., 0.\"x\"\"x\"\"x\"...\"x\"200000...), and every point not in the Cantor set is in one of these intervals, so its derivative is 0 outside of the Cantor set. On the other hand, it has no derivative at any point in an uncountable subset of the Cantor set containing the interval endpoints described above.\n\nThe Cantor function can also be seen as the cumulative probability distribution function of the 1/2-1/2 Bernoulli measure μ supported on the Cantor set: formula_3. This probability distribution, called the Cantor distribution, has no discrete part. That is, the corresponding measure is atomless. This is why there are no jump discontinuities in the function; any such jump would correspond to an atom in the measure.\n\nHowever, no non-constant part of the Cantor function can be represented as an integral of a probability density function; integrating any putative probability density function that is not almost everywhere zero over any interval will give positive probability to some interval to which this distribution assigns probability zero. In particular, as pointed out, the function is not the integral of its derivative even though the derivative exists almost everywhere.\n\nThe Cantor function is the standard example of a singular function.\n\nThe Cantor function is non-decreasing, and so in particular its graph defines a rectifiable curve. showed that the arc length of its graph is 2.\n\nBecause the Lebesgue measure of the uncountably infinite Cantor set is 0, for any positive \"ε\" and \"δ\", there exists a finite sequence of pairwise disjoint sub-intervals with total length < \"δ\" over which the Cantor function cumulatively rises more than \"ε\". If a given sequence did not, all but an arbitrarily short portion of each sub-interval containing a point in the Cantor set could be discarded and a finite number equal-length sub-intervals containing different points in the Cantor set added, iteratively, until the cumulative rise was greater than \"ε\".\n\nBelow we define a sequence {\"ƒ\"} of functions on the unit interval that converges to the Cantor function.\n\nLet \"ƒ\"(\"x\") = \"x\".\n\nThen, for every integer , the next function \"ƒ\"(\"x\") will be defined in terms of \"ƒ\"(\"x\") as follows:\n\nLet \"ƒ\"(\"x\") = ,  when ;\n\nLet \"ƒ\"(\"x\") = 0.5,  when ;\n\nLet \"ƒ\"(\"x\") = ,  when .\n\nThe three definitions are compatible at the end-points 1/3 and 2/3, because \"ƒ\"(0) = 0 and \"ƒ\"(1) = 1 for every \"n\", by induction. One may check that \"ƒ\" converges pointwise to the Cantor function defined above. Furthermore, the convergence is uniform. Indeed, separating into three cases, according to the definition of \"ƒ\", one sees that\n\nIf \"ƒ\" denotes the limit function, it follows that, for every \"n\" ≥ 0,\n\nAlso the choice of starting function does not really matter, provided \"ƒ\"(0) = 0, \"ƒ\"(1) = 1 and \"ƒ\" is bounded.\n\nThe Cantor function is closely related to the Cantor set. The Cantor set \"C\" can be defined as the set of those numbers in the interval [0, 1] that do not contain the digit 1 in their base-3 (triadic) expansion, except if the 1 is followed by zeros only (in which case the tail 1000formula_6 can be replaced by 0222formula_6 to get rid of any 1). It turns out that the Cantor set is a fractal with (uncountably) infinitely many points (zero-dimensional volume), but zero length (one-dimensional volume). Only the D-dimensional volume formula_8 (in the sense of a Hausdorff-measure) takes a finite value, where formula_9 is the fractal dimension of \"C\". We may define the Cantor function alternatively as the D-dimensional volume of sections of the Cantor set\n\nLet\n\nbe the dyadic (binary) expansion of the real number 0 ≤ \"y\" ≤ 1 in terms of binary digits \"b\" ∈ {0,1}. Then consider the function\n\nFor \"z\" = 1/3, the inverse of the function \"x\" = 2 \"C\"(\"y\") is the Cantor function. That is, \"y\" = \"y\"(\"x\") is the Cantor function. In general, for any \"z\" < 1/2, \"C\"(\"y\") looks like the Cantor function turned on its side, with the width of the steps getting wider as \"z\" approaches zero.\n\nAs mentioned above, the Cantor function is also the cumulative distribution function of a measure on the Cantor set. Different Cantor functions, or Devil's Staircases, can be obtained by considering different atom-less probability measures supported on the Cantor set or other fractals. While the Cantor function has derivative 0 almost everywhere, current research focusses on the question of the size of the set of points where the upper right derivative is distinct from the lower right derivative, causing the derivative to not exist. This analysis of differentiability is usually given in terms of fractal dimension, with the Hausdorff dimension the most popular choice. This line of research was started in the 90s by Darst, who showed that the Hausdorff dimension of the set of non-differentiability of the Cantor function is the square of the dimension of the Cantor set, formula_13. More recently Falconer showed that this squaring relationship holds for all Ahlfor's regular, singular measures, i.e.formula_14Later, Troscheit obtain a more comprehensive picture of the set where the derivative does not exist for more general normalized Gibb's measures supported on self-conformal and self-similar sets.\n\nHermann Minkowski's question mark function loosely resembles the Cantor function visually, appearing as a \"smoothed out\" form of the latter; it can be constructed by passing from a continued fraction expansion to a binary expansion, just as the Cantor function can be constructed by passing from a ternary expansion to a binary expansion. The question mark function has the interesting property of having vanishing derivatives at all rational numbers.\n\n\n"}
{"id": "14320253", "url": "https://en.wikipedia.org/wiki?curid=14320253", "title": "Carleman matrix", "text": "Carleman matrix\n\nIn mathematics, a Carleman matrix is a matrix used to convert function composition into matrix multiplication. It is often used in iteration theory to find the continuous iteration of functions which cannot be iterated by pattern recognition alone. Other uses of Carleman matrices occur in the theory of probability generating functions, and Markov chains.\n\nThe Carleman matrix of an infinitely differentiable function formula_1 is defined as:\nso as to satisfy the (Taylor series) equation:\n\nFor instance, the computation of formula_1 by\nsimply amounts to the dot-product of row 1 of formula_6 with a column vector formula_7.\n\nThe entries of formula_8 in the next row give the 2nd power of formula_1:\nand also, in order to have the zero'th power of formula_1 in formula_8, we adopt the row 0 containing zeros everywhere except the first position, such that\n\nThus, the dot product of formula_8 with the column vector formula_15 yields the column vector formula_16\n\nThe Bell matrix of a function formula_1 is defined as\nso as to satisfy the equation\nso it is the transpose of the above Carleman matrix.\n\nEri Jabotinsky developed that concept of matrices 1947 for the purpose of representation of convolutions of polynomials. In an article \"Analytic Iteration\" (1963) he introduces the term \"representation matrix\", and generalized that concept to two-way-infinite matrices. In that article only functions of the type formula_21 are discussed, but considered for positive *and* negative powers of the function. Several authors refer to the Bell matrices as \"Jabotinsky matrix\" since (D. Knuth 1992, W.D. Lang 2000), and possibly this shall grow to a more canonical name.\n\nAnalytic Iteration\nAuthor(s): Eri Jabotinsky\nSource: Transactions of the American Mathematical Society, Vol. 108, No. 3 (Sep., 1963), pp. 457–477\nPublished by: American Mathematical Society\nStable URL: https://www.jstor.org/stable/1993593\nAccessed: 19/03/2009 15:57\n\nA generalization of the Carleman matrix of a function can be defined around any point, such as:\nor formula_23 where formula_24. This allows the matrix power to be related as:\n\nIf we set formula_45 we have the Carleman matrix\n\nIf formula_46 is a ortonormal basis for a Hilbert Space with a defined inner product formula_47, we can set formula_48 and formula_49 will be formula_50. If formula_51 we have the analogous for Fourier Series, namely formula_52\n\nThese matrices satisfy the fundamental relationships:\nwhich makes the Carleman matrix \"M\" a (direct) representation of formula_1, and the Bell matrix \"B\" an \"anti-representation\" of formula_1. Here the term formula_57 denotes the composition of functions formula_58.\n\nOther properties include:\n\nThe Carleman matrix of a constant is:\n\nThe Carleman matrix of the identity function is:\n\nThe Carleman matrix of a constant addition is:\n\nThe Carleman matrix of the successor function is equivalent to the Binomial coefficient:\n\nThe Carleman matrix of the logarithm is related to the (signed) Stirling numbers of the first kind scaled by factorials:\n\nThe Carleman matrix of the logarithm is related to the (unsigned) Stirling numbers of the first kind scaled by factorials:\n\nThe Carleman matrix of the exponential function is related to the Stirling numbers of the second kind scaled by factorials:\n\nThe Carleman matrix of exponential functions is:\n\nThe Carleman matrix of a constant multiple is:\n\nThe Carleman matrix of a linear function is:\n\nThe Carleman matrix of a function formula_78 is:\n\nThe Carleman matrix of a function formula_80 is:\n\nConsider the following autonomous nonlinear system:\nwhere formula_83 denotes the system state vector. Also, formula_84 and formula_85's are known analytic vector functions, and formula_86 is the formula_87 element of an unknown disturbance to the system.\n\nAt the desired nominal point, the nonlinear functions in the above system can be approximated by Taylor expansion\nwhere \nformula_89 is the formula_90 partial derivative of formula_1 with respect to formula_92 at formula_93 and formula_94 denotes the formula_90 Kronecker product.\n\nWithout loss of generality, we assume that formula_96 is at the origin.\n\nApplying Taylor approximation to the system, we obtain\nwhere formula_98 and formula_99.\n\nConsequently, the following linear system for higher orders of the original states are obtained:\nwhere\nformula_101, and similarly formula_102.\n\nEmploying Kronecker product operator, the approximated system is presented in the following form\nwhere\nx^T &x^\n"}
{"id": "4594672", "url": "https://en.wikipedia.org/wiki?curid=4594672", "title": "Computational problem", "text": "Computational problem\n\nIn theoretical computer science, a computational problem is a mathematical object representing a collection of questions that computers might be able to solve. For example, the problem of factoring\n\nis a computational problem. Computational problems are one of the main objects of study in theoretical computer science. The field of algorithms studies methods of solving computational problems efficiently. The complementary field of computational complexity attempts to explain why certain computational problems are intractable for computers.\n\nA computational problem can be viewed as an infinite collection of \"instances\" together with a \"solution\" for every instance. For example, in the factoring problem, the instances are the integers \"n\", and solutions are prime numbers \"p\" that describe nontrivial prime factors of \"n\". \n\nIt is conventional to represent both instances and solutions by binary strings, namely elements of {0, 1}. For example, numbers can be represented as binary strings using the binary encoding. (For readability, we identify numbers with their binary encodings in the examples below.)\n\nA decision problem is a computational problem where the answer for every instance is either yes or no. An example of a decision problem is \"primality testing\":\n\nA decision problem is typically represented as the set of all instances for which the answer is \"yes\". For example, primality testing can be represented as the infinite set\n\nIn a search problem, the answers can be arbitrary strings. For example, factoring is a search problem where the instances are (string representations of) positive integers and the solutions are (string representations of) collections of primes.\n\nA search problem is represented as a relation consisting of all the instance-solution pairs, called a \"search relation\". For example, factoring can be represented as the relation\n\nwhich consist of all pairs of numbers (\"n\", \"p\"), where \"p\" is a nontrivial prime factor of \"n\".\n\nA counting problem asks for the number of solutions to a given search problem. For example, a counting problem associated with factoring is\n\nA counting problem can be represented by a function \"f\" from {0, 1} to the nonnegative integers. For a search relation \"R\", the counting problem associated to \"R\" is the function\n\nAn optimization problem asks for finding a \"best possible\" solution among the set of all possible solutions to a search problem. One example is the \"maximum independent set\" problem:\n\nOptimization problems can be represented by their search relations.\n\nIn a function problem a single output (of a total function) is expected for every input, but the output is more complex than that of a decision problem, that is, it isn't just \"yes\" or \"no\". One of the most famous examples is the \"travelling salesman\" problem:\n\nIt is an NP-hard problem in combinatorial optimization, important in operations research and theoretical computer science.\n\nIn computational complexity theory, it is usually implicitly assumed that any string in {0, 1} represents an instance of the computational problem in question. However, sometimes not all strings {0, 1} represent valid instances, and one specifies a proper subset of {0, 1} as the set of \"valid instances\". Computational problems of this type are called promise problems.\n\nThe following is an example of a (decision) promise problem:\n\nHere, the valid instances are those graphs whose maximum independent set size is either at most 5 or at least 10.\n\nDecision promise problems are usually represented as pairs of disjoint subsets (\"L\", \"L\") of {0, 1}. The valid instances are those in \"L\" ∪ \"L\".\n\"L\" and \"L\" represent the instances whose answer is \"yes\" and \"no\", respectively. \n\nPromise problems play an important role in several areas of computational complexity, including hardness of approximation, property testing, and interactive proof systems.\n\n\n"}
{"id": "7769842", "url": "https://en.wikipedia.org/wiki?curid=7769842", "title": "Conference graph", "text": "Conference graph\n\nIn the mathematical area of graph theory, a conference graph is a strongly regular graph with parameters \"v\", and It is the graph associated with a symmetric conference matrix, and consequently its order \"v\" must be 1 (modulo 4) and a sum of two squares.\n\nConference graphs are known to exist for all small values of \"v\" allowed by the restrictions, e.g., \"v\" = 5, 9, 13, 17, 25, 29, and (the Paley graphs) for all prime powers congruent to 1 (modulo 4). However, there are many values of \"v\" that are allowed, for which the existence of a conference graph is unknown.\n\nThe eigenvalues of a conference graph need not be integers, unlike those of other strongly regular graphs. If the graph is connected, the eigenvalues are \"k\" with multiplicity 1, and two other eigenvalues, \neach with multiplicity \n\nBrouwer, A.E., Cohen, A.M., and Neumaier, A. (1989), Distance Regular Graphs. Berlin, New York: Springer-Verlag. , \n"}
{"id": "13463690", "url": "https://en.wikipedia.org/wiki?curid=13463690", "title": "Contraction principle (large deviations theory)", "text": "Contraction principle (large deviations theory)\n\nIn mathematics — specifically, in large deviations theory — the contraction principle is a theorem that states how a large deviation principle on one space \"pushes forward\" (via the pushforward of a probability measure) to a large deviation principle on another space \"via\" a continuous function.\n\nLet \"X\" and \"Y\" be Hausdorff topological spaces and let (\"μ\") be a family of probability measures on \"X\" that satisfies the large deviation principle with rate function \"I\" : \"X\" → [0, +∞]. Let \"T\" : \"X\" → \"Y\" be a continuous function, and let \"ν\" = \"T\"(\"μ\") be the push-forward measure of \"μ\" by \"T\", i.e., for each measurable set/event \"E\" ⊆ \"Y\", \"ν\"(\"E\") = \"μ\"(\"T\"(\"E\")). Let\n\nwith the convention that the infimum of \"I\" over the empty set ∅ is +∞. Then:\n\n"}
{"id": "317552", "url": "https://en.wikipedia.org/wiki?curid=317552", "title": "Cover (topology)", "text": "Cover (topology)\n\nIn mathematics, a cover of a set formula_1 is a collection of sets whose union contains formula_1 as a subset. Formally, if\nis an indexed family of sets formula_4, then formula_5 is a cover of formula_1 if\n\nCovers are commonly used in the context of topology. If the set \"X\" is a topological space, then a \"cover\" \"C\" of \"X\" is a collection of subsets \"U\" of \"X\" whose union is the whole space \"X\". In this case we say that \"C\" \"covers\" \"X\", or that the sets \"U\" \"cover\" \"X\". Also, if \"Y\" is a subset of \"X\", then a \"cover\" of \"Y\" is a collection of subsets of \"X\" whose union contains \"Y\", i.e., \"C\" is a cover of \"Y\" if\n\nLet \"C\" be a cover of a topological space \"X\". A subcover of \"C\" is a subset of \"C\" that still covers \"X\".\n\nWe say that \"C\" is an if each of its members is an open set (i.e. each \"U\" is contained in \"T\", where \"T\" is the topology on \"X\").\n\nA cover of \"X\" is said to be locally finite if every point of \"X\" has a neighborhood which intersects only finitely many sets in the cover. Formally, \"C\" = {\"U\"} is locally finite if for any \"x\" ∈ \"X\", there exists some neighborhood \"N\"(\"x\") of \"x\" such that the set\nis finite. A cover of \"X\" is said to be point finite if every point of \"X\" is contained in only finitely many sets in the cover. A cover is point finite if it is locally finite, though the converse is not necessarily true.\n\nA refinement of a cover \"C\" of a topological space \"X\" is a new cover \"D\" of \"X\" such that every set in \"D\" is contained in some set in \"C\". Formally,\n\nIn other words, there is a refinement map formula_11 satisfying formula_12 for every formula_13. This map is used, for instance, in the Čech cohomology of X.\n\nEvery subcover is also a refinement, but the opposite is not always true. A subcover is made from the sets that are in the cover, but omitting some of them; whereas a refinement is made from any sets that are subsets of the sets in the cover.\n\nThe refinement relation is a preorder on the set of covers of \"X\".\n\nGenerally speaking, a refinement of a given structure is another that in some sense contains it. Examples are to be found when partitioning an interval (one refinement of formula_14 being formula_15), considering topologies (the standard topology in euclidean space being a refinement of the trivial topology). When subdividing simplicial complexes (the first barycentric subdivision of a simplicial complex is a refinement), the situation is slightly different: every simplex in the finer complex is a face of some simplex in the coarser one, and both have equal underlying polyhedra.\n\nYet another notion of refinement is that of star refinement.\n\nA simple way to get a subcover is to omit the sets contained in another set in the cover. Turn to open cover.\nLet formula_16 be the topological basis of formula_1, we have formula_18, where formula_19 is any set in an open cover formula_20. formula_21 is indeed a refinement. For any formula_22, we select a formula_23 (require the selection axiom). Now formula_24 is a subcover of formula_20. Hence the cardinal of a subcover of an open cover can be as small as that of topological basis. And second countability implies Lindelöf spaces.\n\nThe language of covers is often used to define several topological properties related to \"compactness\". A topological space \"X\" is said to be\n\nFor some more variations see the above articles.\n\nA topological space \"X\" is said to be of covering dimension \"n\" if every open cover of \"X\" has a point finite open refinement such that no point of \"X\" is included in more than \"n+1\" sets in the refinement and if \"n\" is the minimum value for which this is true. If no such minimal \"n\" exists, the space is said to be of infinite covering dimension.\n\n\n"}
{"id": "18356058", "url": "https://en.wikipedia.org/wiki?curid=18356058", "title": "Cryptography Research", "text": "Cryptography Research\n\nCryptography Research, Inc. is a San Francisco based cryptography company specializing in applied cryptographic engineering, including technologies for building tamper-resistant semiconductors. It was purchased on June 6, 2011 by Rambus for $342.5M. The company licenses patents for protecting cryptographic devices against power analysis attacks. The company's CryptoFirewall-brand ASIC cores are used in pay TV conditional access systems and anti-counterfeiting applications. CRI also developed BD+, a security component in the Blu-ray disc format, and played a role in the format war between HD DVD and Blu-ray. The company's services group assists with security testing, disaster recovery, and training.\n\nCryptography Research protects its core operations from outside attack by maintaining a secured local network that is not connected to the Internet at all. Employees who need to work with sensitive data have two computers on their desks — one to access the secure network, and a separate computer to access the Internet.\n\nIn 2009, Frost & Sullivan awarded the company the World Smart Card Technology Leadership of the Year Award, noting that the company is \"one of the highest-volume and highest-value technology licensors in the semiconductor industry\" and that \"more than 4 billion security chips are produced under its licenses every year\".\n"}
{"id": "32498620", "url": "https://en.wikipedia.org/wiki?curid=32498620", "title": "Dense subgraph", "text": "Dense subgraph\n\nIn computer science the notion of highly connected subgraphs appears frequently. This notion can be formalized as follows. Let formula_1 be an undirected graph and let formula_2 be a subgraph of formula_3. Then the \"density\" of formula_4 is defined to be formula_5.\n\nThe densest subgraph problem is that of finding a subgraph of maximum density. In 1984, Andrew V. Goldberg developed a polynomial time algorithm to find the maximum density subgraph using a max flow technique.\n\nThere are many variations on the densest subgraph problem. One of them is the densest formula_6 subgraph problem, where the objective is to find the maximum density subgraph on exactly formula_6 vertices. This problem generalizes the clique problem and is thus NP-hard in general graphs. There exists a polynomial algorithm approximating the densest formula_8 subgraph within a ratio of formula_9 for every formula_10, while it does not admit an formula_11-approximation in polynomial time unless the exponential time hypothesis is false. Under a weaker assumption that formula_12, no PTAS exists for the problem. \n\nThe problem remains NP-hard in bipartite graphs and chordal graphs but is polynomial for trees and split graphs. It is open whether the problem is NP-hard or polynomial in (proper) interval graphs and planar graphs; however, a variation of the problem in which the subgraph is required to be connected is NP-hard in planar graphs.\n\nThe objective of the densest at most formula_6 problem is to find the maximum density subgraph on at most formula_6 vertices. Anderson and Chellapilla showed that if there exists an formula_15 approximation for this problem then that will lead to an formula_16 approximation for the densest formula_6 subgraph problem.\n\nThe densest at least formula_6 problem is defined similarly to the densest at most formula_6 subgraph problem. The problem is NP-complete, but admits 2-approximation in polynomial time. Moreover, there is some evidence that this approximation algorithm is essentially the best possible: assuming the Small Set Expansion Hypothesis (a computational complexity assumption closely related to the Unique Games Conjecture), then it is NP-hard to approximate the problem to within formula_20 factor for every constant formula_10.\n\nCharalampos Tsourakakis introduced the formula_8-clique densest subgraph problem. This variation of the densest subgraph problem aims to maximize the average number of induced formula_8 cliques formula_24, where formula_25 is the set of formula_8-cliques induced by formula_27. Notice that the densest subgraph problem is obtained as a special case for formula_28. This generalization provides an empirically successful poly-time approach for extracting large near-cliques from large-scale real-world networks. \n\n"}
{"id": "691736", "url": "https://en.wikipedia.org/wiki?curid=691736", "title": "Diamond principle", "text": "Diamond principle\n\nIn mathematics, and particularly in axiomatic set theory, the diamond principle is a combinatorial principle introduced by Ronald Jensen in that holds in the constructible universe () and that implies the continuum hypothesis. Jensen extracted the diamond principle from his proof that the Axiom of constructibility () implies the existence of a Suslin tree.\n\nThe diamond principle says that there exists a , in other words sets for such that for any subset of ω the set of with is stationary in .\n\nThere are several equivalent forms of the diamond principle. One states that there is a countable collection of subsets of for each countable ordinal such that for any subset of there is a stationary subset of such that for all in we have and . Another equivalent form states that there exist sets for such that for any subset of there is at least one infinite with .\n\nMore generally, for a given cardinal number and a stationary set , the statement (sometimes written or ) is the statement that there is a sequence such that\n\n\nThe principle is the same as .\n\nThe diamond-plus principle says that there exists a -sequence, in other words a countable collection of subsets of for each countable ordinal α such that for any subset of there is a closed unbounded subset of such that for all in we have and .\n\n showed that the diamond principle implies the existence of Suslin trees. He also showed that implies the diamond-plus principle, which implies the diamond principle, which implies CH. In particular the diamond principle and the diamond-plus principle are both independent of the axioms of ZFC. Also implies , but Shelah gave models of so and are not equivalent (rather, is weaker than ).\n\nThe diamond principle does not imply the existence of a Kurepa tree, but the stronger principle implies both the principle and the existence of a Kurepa tree.\n\nFor all cardinals and stationary subsets , holds in the constructible universe. proved that for , follows from for stationary that do not contain ordinals of cofinality .\n\nShelah showed that the diamond principle solves the Whitehead problem by implying that every Whitehead group is free.\n\n\n"}
{"id": "2487103", "url": "https://en.wikipedia.org/wiki?curid=2487103", "title": "Dr. Priestley", "text": "Dr. Priestley\n\nDr. Lancelot Priestley is a fictional investigator born in July 1869 in a series of books by John Rhode\n\nAfter 1924 Dr. Priestley took over from Dr. Thorndyke as the leading fictional forensic investigator in Britain, and featured in 72 novels written over 40 years, solving many ingenious and misleading murders. Dr Priestley's involvement is usually at the request of the police but only if the case piques his scientific curiosity; having little, or no, interest in criminal justice. \"Hanslet had brought many problems which confronted him in the course of his duties to Dr. Priestley's notice, usually with results highly satisfactory to himself. But in nearly every case Dr. Priestly's interest in the problem ceased when he had solved it to his own satisfaction. The fate of the criminal was a matter of complete unconcern to him. He treated detection much as he would have treated a game of chess. The pieces in the game had no more than a passing interest to him. Not that he was unsympathetic by nature, as many people had good cause to know. But, in the problems which Hanslet set before him, he purposely took a detached and impersonal attitude. Only in this way, as he more than once remarked, was it possible to maintain an impartial judgement\"\n\nDr Priestley had previously held the chair of Applied Mathematics at a leading Midlands University until he abandoned his chair and retired to the house in Westbourne Terrace which he had inherited from his father, but is described as an independent researcher who delights in scientific controversy. Described in The Ellerby Case, Dr Priestley's book \"Fact and Fallacy\" \"\"..contained in every one of its two hundred odd pages a direct and trenchant attack upon those whom the author was apt to allude to as \"The Orthodox Scientific School\" and \"So the reviews poured in by every post: denunciatory, indignant, sometimes distinctly abusive. And Dr Priestley would sit and gloat over them, as a primitive warrior might gloat over the blood of his adversaries\"\"\n\nIn the early books Dr. Priestly provides assistance mainly to his friend Chief Inspector Hanslet. In later books Dr. Priestley becomes an armchair detective and the bulk of the legwork is done by Superintendent Jimmy Waghorn of Scotland Yard and Priestley's secretary and companion, Harold Merefield. Harold Merefield, whom Dr Priestley had cleared of a murder charge in the first book \"The Paddington Mystery\", is engaged to Dr. Priestly's daughter April. Superintendent Hanslet (now retired) appears in several later works as a dinner guest of Dr. Priestly passing on his professional wisdom to Waghorn.\n\n"}
{"id": "6095467", "url": "https://en.wikipedia.org/wiki?curid=6095467", "title": "Erland Samuel Bring", "text": "Erland Samuel Bring\n\nErland Samuel Bring (19 August 1736 – 20 May 1798) was a Swedish mathematician.\n\nBring studied at Lund University between 1750 and 1757. In 1762 he obtained a position of a reader in history and was promoted to professor in 1779. At Lund he wrote eight volumes of mathematical work in the fields of algebra, geometry, analysis and astronomy, including \"Meletemata quaedam mathematica circa transformationem aequationum algebraicarum\" (1786). This work describes Bring's contribution to the algebraic solution of equations.\n\nBring had developed an important transformation to simplify a quintic equation to the form formula_1 (see Bring radical). In 1832–35 the same transformation was independently derived by George Jerrard. However, whereas Jerrard knew from the past work by Paolo Ruffini and Niels Henrik Abel that a general quintic equation can not be solved, this fact was not known to Bring, putting him in a disadvantage.\n\nBring's curve is named after him.\n"}
{"id": "246562", "url": "https://en.wikipedia.org/wiki?curid=246562", "title": "Fan Chung", "text": "Fan Chung\n\nFan-Rong King Chung Graham (; born October 9, 1949), known professionally as Fan Chung, is a Taiwanese-born American mathematician who works mainly in the areas of spectral graph theory, extremal graph theory and random graphs, in particular in generalizing the Erdős–Rényi model for graphs with general degree distribution (including power-law graphs in the study of large information networks).\n\nSince 1998, Chung has been the Akamai Professor in Internet Mathematics at the University of California, San Diego (UCSD). She received her doctorate from the University of Pennsylvania in 1974, under the direction of Herbert Wilf. After working at Bell Laboratories and Bellcore for nineteen years, she joined the faculty of the University of Pennsylvania as the first female tenured professor in mathematics. She serves on the editorial boards of more than a dozen international journals. Since 2003 she has been the editor-in-chief of \"Internet Mathematics\". She has been invited to give lectures at many conferences, including the International Congress of Mathematicians in 1994 and a plenary lecture on the mathematics of PageRank at the 2008 Annual meeting of the American Mathematical Society. She was selected to be a Noether Lecturer in 2009.\nChung has two children; the first child was born during her graduate studies from her first marriage. Since 1983 she has been married to the mathematician Ronald Graham. They were close friends of the mathematician Paul Erdős, and have both published papers with him – 13 in her case; thus, both have Erdős numbers of 1.\n\nShe has published more than 200 research papers and three books:\n\nIn 2012, she became a fellow of the American Mathematical Society.\n\nFan Chung was born on October 9, 1949 in Kaohsiung, Taiwan. Under the influence of her father, an engineer, she became interested in mathematics, especially in the area of combinatorics in high school in Kaohsiung. After high school, Chung entered the National Taiwan University (NTU) to start her career in mathematics formally. While Chung was an undergraduate, she was surrounded by many female mathematicians, and this helped encourage her to pursue and study mathematics.\n\nAfter graduating from NTU with a B.S. in mathematics, Chung went on to the University of Pennsylvania to pursue a career in mathematics. There she obtained the highest score in the qualifying exam by a wide margin, catching the attention of Herbert Wilf, who would eventually become her doctoral advisor. Wilf suggested Ramsey theory as a subject Chung could work on. During a single week studying material Chung had come up with new proofs for established results in the field. Wilf said: \"My eyes were bulging. I was very excited. I asked her to go to the blackboard and show me. What she wrote was incredible! In just one week, from a cold start, she had a major result in Ramsey theory. I told her she had just done two-thirds of a doctoral dissertation.\"\n\nChung was awarded a M.S. in 1972 and a Ph.D. two years later. By this time, she was married and had already given birth to her first child. The same year she received her Ph.D. and started working for the Mathematical Foundations of Computing Department at Bell Laboratories in Murray Hill, New Jersey. The position at Bell Laboratories was an opportunity to work with other excellent mathematicians, but also it contributed to her mathematical world powerfully. She published many impressive mathematical papers and published many joint papers with Ron Graham.\n\nAfter twenty years of work at Bell Laboratories and Bellcore, Chung decided to go back University of Pennsylvania to become a professor of mathematics. In 1998, she was named Distinguished Professor of Mathematics at University of California, San Diego. To date, she has over 200 publications to her name. The two best known books are \"Spectral Graph Theory\" and \"Erdős on Graphs\". \"Spectral Graph Theory\" studies how the spectrum of the Laplacian of a graph is related to its combinatorial properties. \"Erdős on Graphs\", which was jointly written by Fan Chung and Ron Graham, studies many of Paul Erdős problems and conjectures in graph theory. Beyond her contributions to graph theory, Chung has used her knowledge to connect different fields of science. As she wrote in \"Graph Theory in the Information Age\",\n\nChung's life was profiled in the 2017 documentary film \"Girls who fell in love with Math\".\n\nIn 1974, Fan Chung graduated from the University of Pennsylvania and became a member of Technical Staff working for the Mathematical Foundations of Computing Department at Bell Laboratories in Murray Hill, New Jersey. She worked under Henry Pollak. During this time, Chung collaborated with many leading mathematicians who work for Bell Laboratories such as Ron Graham.\n\nIn 1975, Chung published her first joint paper with Graham on \"Multicolor Ramsey Numbers for Complete Bipartite Graphs\" which was published in the \"Journal of Combinatorial Theory\".\n\nIn 1983 the Bell Telephone Company was split up. Since Pollak joined and became head of a research unit within a new company, he asked Chung to become Research Manager. Until 1990, she was one of the first to receive a fellowship to spend a sabbatical at a university. She supervised many mathematicians in the unit.\n\nAccording to Chung's words, although people respect her because of the power to make decisions with positions in management, she prefers to be respected because of her achievement in mathematics. Since then, she has returned to the academic world.\n\nFan Chung's first marriage ended in divorce in 1982. However, when she worked at Bell Laboratories, she met Ronald Graham. During that time, they became close friends and published many joint papers in graph theory, eventually marrying in 1983. In Paul Hoffman's book \"The Man Who Loved Only Numbers\", regarding his marriage with Chung, Graham said:\n\nIn 1998, Graham and Chung co-wrote the book \"Erdős on Graphs.\"\n\nAmong Fan Chung's publications, her contributions to spectral graph theory are important to this area of graph theory. From the first publications about undirected graphs to recent publications about the directed graphs, Fan Chung creates the solid base in the spectral graph theory to the future graph theorist.\n\nSpectral graph theory, as one of the most important theories in graph theory, combines the algebra and graph perfectly. Algebraic methods treat many types of graphs efficiently. According to the biography \"Fan Rong K Chung Graham\", \"Spectral graph theory studies how the spectrum of the Laplacian of a graph is related to its combinatorial properties.\"'.\n\nIn 1997, the American Mathematical Society published Chung's book \"Spectral graph theory\". This book became a standard textbook at many universities and is the key to study Spectral graph theory for many mathematics students who are interested in this area. Fan Chung’s study in the spectral graph theory brings this “algebraic connectivity” of graphs into a new and higher level.\n\nFan Chung won the Allendoerfer Award of the Mathematical Association of America in 1990 and was elected a Fellow of the American Academy of Arts and Sciences in 1998. She became a Fellow of the Society for Industrial and Applied Mathematics in 2015 \"for contributions to combinatorics, graph theory, and their applications\".\n\n\n"}
{"id": "515104", "url": "https://en.wikipedia.org/wiki?curid=515104", "title": "Fisher's fundamental theorem of natural selection", "text": "Fisher's fundamental theorem of natural selection\n\nFisher's fundamental theorem of natural selection is an idea about genetic variance in population genetics developed by the statistician and evolutionary biologist Ronald Fisher. The proper way of applying the abstract mathematics of the theorem to actual biology has been a matter of some debate.\n\nIt states:\n\nOr in more modern terminology:\n\nThe theorem was first formulated in Fisher's 1930 book \"The Genetical Theory of Natural Selection\". Fisher likened it to the law of entropy in physics, stating that \"It is not a little instructive that so similar a law should hold the supreme position among the biological sciences\". The model of Quasi-linkage equilibrium was introduced by Motoo Kimura in 1965 as an approximation in the case of weak selection and weak epistasis.\n\nLargely as a result of Fisher's feud with the American geneticist Sewall Wright about adaptive landscapes, the theorem was widely misunderstood to mean that the average fitness of a population would always increase, even though models showed this not to be the case. In 1972, George R. Price showed that Fisher's theorem was indeed correct (and that Fisher's proof was also correct, given a typo or two), but did not find it to be of great significance. The sophistication that Price pointed out, and that had made understanding difficult, is that the theorem gives a formula for part of the change in gene frequency, and not for all of it. This is a part that can be said to be due to natural selection.\n\nMore recent work (reviewed by Grafen in 2003) builds on Price's understanding in two ways. One aims to improve the theorem by completing it, i.e. by finding a formula for the whole of the change in gene frequency, and accounting for the effects of mutations. The other argues that the partial change is indeed of great conceptual significance, and aims to extend similar partial change results into more and more general population genetic models.\n\nDue to confounding factors, tests of the fundamental theorem are quite rare though Bolnick in 2007 did test this effect in a natural population.\n\n\n"}
{"id": "32880906", "url": "https://en.wikipedia.org/wiki?curid=32880906", "title": "Frictionless market", "text": "Frictionless market\n\nIn economic theory a frictionless market is a financial market without transaction costs. Friction is a type of market incompleteness. Every complete market is frictionless, but the converse does not hold. In a frictionless market the solvency cone is the halfspace normal to the unique price vector. The Black-Scholes model assumes a frictionless market.\n"}
{"id": "29180166", "url": "https://en.wikipedia.org/wiki?curid=29180166", "title": "Gelfand–Shilov space", "text": "Gelfand–Shilov space\n\nIn the mathematical field of functional analysis, a Gelfand–Shilov space \"S\" is a space of test functions for the theory of generalized functions, introduced by .\n\n"}
{"id": "23892932", "url": "https://en.wikipedia.org/wiki?curid=23892932", "title": "GoldSim", "text": "GoldSim\n\nGoldSim is dynamic, probabilistic simulation software developed by GoldSim Technology Group. \nThis general-purpose simulator is a hybrid of several simulation approaches, combining an extension of system dynamics with some aspects of discrete event simulation, and embedding the dynamic simulation engine within a Monte Carlo simulation framework.\n\nWhile it is a general-purpose simulator, GoldSim has been most extensively used for environmental and engineering risk analysis, with applications in the areas of water resource management \nmining\n,\nradioactive waste management\n\ngeological carbon sequestration\naerospace mission risk analysis\n\nand energy.\n\nIn 1990, Golder Associates, an international engineering consulting firm, was asked by the United States Department of Energy (DOE) to develop probabilistic simulation software that could be used to help with decision support and management within the Office of Civilian Radioactive Waste Management. The results of this effort were two DOS-based programs (RIP and STRIP), which were used to support radioactive waste management projects within the DOE.\n\nIn 1996, in an effort funded by Golder Associates, the US DOE, the Japan Nuclear Cycle Development Institute (currently the Japan Atomic Energy Agency) and the (ENRESA), the capabilities of RIP and STRIP were incorporated into a general purpose Windows-based simulator called GoldSim. Subsequent funding was also provided by NASA.\n\nInitially only offered to the original funding organizations, GoldSim was released to the public in 2002. In 2004, GoldSim Technology Group LLC was spun off from Golder Associates and is now a wholly independent company.\n\nNotable applications include providing the simulation framework for: 1) the Yucca Mountain Repository Performance Assessment model developed by Sandia National Laboratories; 2) a comprehensive system-level computational model for performance assessment of geological sequestration of CO developed by Los Alamos National Laboratory; 3) a flood operations model to help better understand and fine tune operations of a large dam used for water supply and flood control in Queensland, Australia; and 4) models for simulating risks associated with future manned space missions in NASA’s Constellation program developed by NASA Ames Research Center.\n\nGoldSim provides a visual and hierarchical modeling environment, which allows users to construct models by adding “elements” (model objects) that represent data, equations, processes or events, and linking them together into graphical representations that resemble influence diagrams. Influence arrows are automatically drawn as elements are referenced by other elements. Complex systems can be translated into hierarchical GoldSim models by creating layer of “containers” (or sub-models). Visual representations and hierarchical structures help users to build very large, complex models that can still be explained to interested stakeholders (e.g., government regulators, elected officials, and the public).\n\nThough it is primarily a continuous simulator, GoldSim has a number of features typically associated with discrete simulators. By combining these two simulation methods, systems that are best represented using both continuous and discrete dynamics can often be more accurately simulated. Examples include tracking the quantity of water in a reservoir that is subject to both continuous inflows and outflows, as well as sudden storm events; and tracking the quantity of fuel in a space vehicle as it is subjected to random perturbations (e.g., component failures, extreme environmental conditions).\n\nBecause the software was originally developed for complex environmental applications, in which many inputs are uncertain and/or stochastic, in addition to being a dynamic simulator, GoldSim is a Monte Carlo simulator, such that inputs can be defined as distributions and the entire system simulated a large number of times to provide probabilistic outputs. As such, the software incorporates a number of computational features to facilitate probabilistic simulation of complex systems, including tools for generating and correlating stochastic time series, advanced sampling capabilities (including latin hypercube sampling, nested Monte Carlo analysis, and importance sampling), and support for distributed processing.\n\n"}
{"id": "1155685", "url": "https://en.wikipedia.org/wiki?curid=1155685", "title": "Graded category", "text": "Graded category\n\nA graded category is a mathematical concept.\n\nIf formula_1 is a category, then a\nformula_1-graded category\nis a category formula_3 together with a functor\nformula_4.\n\nMonoids and groups can be thought of as categories with a single element. A monoid-graded or group-graded category is therefore one in which to each morphism is attached an element of a given monoid (resp. group), its grade. This must be compatible with composition, in the sense that compositions have the product grade.\n\nThere are various different definitions of a graded category, up to the most abstract one given above. A more concrete definition of a semigroup-graded Abelian category is as follows:\n\nLet formula_3 be an Abelian category and formula_6 a semigroup. Let formula_7 be a set of functors from formula_3 to itself. If\n\n\nwe say that formula_15 is a formula_6-graded category.\n\n"}
{"id": "3393371", "url": "https://en.wikipedia.org/wiki?curid=3393371", "title": "Hindu–Arabic numeral system", "text": "Hindu–Arabic numeral system\n\nThe Hindu–Arabic numeral system (also called the Arabic numeral system or Hindu numeral system) is a positional decimal numeral system, and is the most common system for the symbolic representation of numbers in the world. It was invented between the 1st and 4th centuries by Indian mathematicians. The system was adopted in Arabic mathematics by the 9th century. Influential were the books of Muḥammad ibn Mūsā al-Khwārizmī (\"On the Calculation with Hindu Numerals\",  825) and Al-Kindi (\"On the Use of the Hindu Numerals\",  830). The system later spread to medieval Europe by the High Middle Ages.\n\nThe system is based upon ten (originally nine) different glyphs. The symbols (glyphs) used to represent the system are in principle independent of the system itself. The glyphs in actual use are descended from Brahmi numerals and have split into various typographical variants since the Middle Ages.\n\nThese symbol sets can be divided into three main families: Western Arabic numerals used in the Greater Maghreb and in Europe, Eastern Arabic numerals (also called \"Indic numerals\") used in the Middle East, and the Indian numerals used in the Indian subcontinent.\n\nThe Hindu-Arabic numerals were invented by mathematicians in India. Perso-Arabic mathematicians called them \"Hindu numerals\" (where \"Hindu\" meant Indian). Later they came to be called \"Arabic numerals\" in Europe, because they were introduced to the West by Arab merchants.\n\nThe Hindu–Arabic system is designed for positional notation in a decimal system. In a more developed form, positional notation also uses a decimal marker (at first a mark over the ones digit but now more usually a decimal point or a decimal comma which separates the ones place from the tenths place), and also a symbol for \"these digits recur \"ad infinitum\"\". In modern usage, this latter symbol is usually a vinculum (a horizontal line placed over the repeating digits). In this more developed form, the numeral system can symbolize any rational number using only 13 symbols (the ten digits, decimal marker, vinculum, and a prepended dash to indicate a negative number).\n\nAlthough generally found in text written with the Arabic abjad (\"alphabet\"), numbers written with these numerals also place the most-significant digit to the left, so they read from left to right. The requisite changes in reading direction are found in text that mixes left-to-right writing systems with right-to-left systems.\n\nVarious symbol sets are used to represent numbers in the Hindu–Arabic numeral system, most of which developed from the Brahmi numerals.\n\nThe symbols used to represent the system have split into various typographical variants since the Middle Ages, arranged in three main groups:\n\nAs in many numbering systems, the numerals 1, 2, and 3 represent simple tally marks; 1 being a single line, 2 being two lines (now connected by a diagonal) and 3 being three lines (now connected by two vertical lines). After three, numerals tend to become more complex symbols (examples are the Chinese numerals and Roman numerals). Theorists believe that this is because it becomes difficult to instantaneously count objects past three.\n\nThe Brahmi numerals at the basis of the system predate the Common Era. They replaced the earlier Kharosthi numerals used since the 4th century BC. Brahmi and Kharosthi numerals were used alongside one another in the Maurya Empire period, both appearing on the 3rd century BC edicts of Ashoka.\n\nBuddhist inscriptions from around 300 BC use the symbols that became 1, 4, and 6. One century later, their use of the symbols that became 2, 4, 6, 7, and 9 was recorded. These Brahmi numerals are the ancestors of the Hindu–Arabic glyphs 1 to 9, but they were not used as a positional system with a zero, and there were rather separate numerals for each of the tens (10, 20, 30, etc.).\n\nThe actual numeral system, including positional notation and use of zero, is in principle independent of the glyphs used, and significantly younger than the Brahmi numerals.\n\nThe place-value system is used in the Bakhshali Manuscript. Although date of the composition of the manuscript is uncertain, the language used in the manuscript indicates that it could not have been composed any later than 400. The development of the positional decimal system takes its origins in Hindu mathematics during the Gupta period. Around 500, the astronomer Aryabhata uses the word \"kha\" (\"emptiness\") to mark \"zero\" in tabular arrangements of digits. The 7th century \"Brahmasphuta Siddhanta\" contains a comparatively advanced understanding of the mathematical role of zero. The Sanskrit translation of the lost 5th century Prakrit Jaina cosmological text \"Lokavibhaga\"\nmay preserve an early instance of positional use of zero.\n\nThese Indian developments were taken up in Islamic mathematics in the 8th century, as recorded in al-Qifti's \"Chronology of the scholars\" (early 13th century).\n\nThe numeral system came to be known to both the Persian mathematician Khwarizmi, who wrote a book, \"On the Calculation with Hindu Numerals\" in about 825, and the Arab mathematician Al-Kindi, who wrote four volumes, \"On the Use of the Hindu Numerals\" ( [\"kitāb fī isti'māl al-'adād al-hindī\"]) around 830. These earlier texts did not use the Hindu numerals. Kushyar ibn Labban who wrote \"Kitab fi usul hisab al-hind (Principles of Hindu Reckoning)\" is one of the oldest surviving manuscripts using the Hindu numerals. These books are principally responsible for the diffusion of the Hindu system of numeration throughout the Islamic world and ultimately also to Europe.\n\nThe first dated and undisputed inscription showing the use of a symbol for zero appears on a stone inscription found at the Chaturbhuja Temple at Gwalior in India, dated 876.\n\nIn 10th century Islamic mathematics, the system was extended to include fractions, as recorded in a treatise by Syrian mathematician Abu'l-Hasan al-Uqlidisi in 952–953.\n\nIn Christian Europe, the first mention and representation of Hindu-Arabic numerals (from one to nine, without zero), is in the \", an illuminated compilation of various historical documents from the Visigothic period in Spain, written in the year 976 by three monks of the Riojan monastery of San Martín de Albelda.\nBetween 967 and 969, Gerbert of Aurillac discovered and studied Arab science in the Catalan abbeys. Later he obtained from these places the book \" (\"On multiplication and division\"). After becoming Pope Sylvester II in the year 999, he introduced a new model of abacus, the so-called Abacus of Gerbert, by adopting tokens representing Hindu-Arab numerals, from one to nine.\n\nThe familiar shape of the Western Arabic glyphs as now used with the Latin alphabet (0, 1, 2, 3, 4, 5, 6, 7, 8, 9) are the product of the late 15th to early 16th century, when they enter early typesetting.\nMuslim scientists used the Babylonian numeral system, and merchants used the Abjad numerals, a system similar to the Greek numeral system and the Hebrew numeral system. Similarly, 's introduction of the system to Europe was restricted to learned circles.\nThe credit for first establishing widespread understanding and usage of the decimal positional notation among the general population goes to , an author of the German Renaissance, whose 1522 \" was targeted at the apprentices of businessmen and craftsmen.\n\nIn AD 690, Empress Wu promulgated Zetian characters, one of which was \"〇\". The word is now used as a synonym for the number zero.\n\nIn China, Gautama Siddha introduced Hindu numerals with zero in 718, but Chinese mathematicians did not find them useful, as they had already had the decimal positional counting rods.\n\nIn Chinese numerals, a circle (〇) is used to write zero in Suzhou numerals. Many historians think it was imported from Indian numerals by Gautama Siddha in 718, but some Chinese scholars think it was created from the Chinese text space filler \"□\".\n\nChinese and Japanese finally adopted the Hindu–Arabic numerals in the 19th century, abandoning counting rods.\n\nThe \"Western Arabic\" numerals as they were in common use in Europe since the Baroque period have secondarily found worldwide use together with the Latin alphabet, and even significantly beyond the contemporary spread of the Latin alphabet, intruding into the writing systems in regions where other variants of the Hindu–Arabic numerals had been in use, but also in conjunction with Chinese and Japanese writing (see Chinese numerals, Japanese numerals).\n\n\n\n"}
{"id": "48885825", "url": "https://en.wikipedia.org/wiki?curid=48885825", "title": "Incompressibility method", "text": "Incompressibility method\n\nIn mathematics, the incompressibility method is a proof method like the probabilistic method, the counting method or the pigeonhole principle. To prove that an object in a certain class (on average) satisfies a certain property, select an object of that class which is incompressible. If it does not satisfy the property, it can be compressed by computable coding. Since it can be generally proven that almost all objects in a given class are incompressible, the argument demonstrates that almost all objects in the class have the property involved (not just the average). To select an incompressible object is ineffective, and cannot be done by a computer program. However, a simple counting argument usually shows that almost all objects of a given class can be compressed by only a few bits (are incompressible).\n\nThe incompressibity method depends on an objective, fixed notion of incompressibility. Such a notion was provided by the Kolmogorov complexity theory, named for Andrey Kolmogorov.\n\nOne of the first uses of the incompressibility method with Kolmogorov complexity its theory of computation was to prove that the running time of a one-tape Turing machine is quadratic for accepting a palindromic language and sorting algorithms require at least formula_1 time to sort formula_2 items. The first influential paper using the incompressibility method was published in 1980. The method was applied to a number of fields, and its name was coined in a textbook.\n\nThe Kolmogorov complexity of an object, represented by a finite, binary string, is the length of the shortest binary program on a fixed, optimal universal Turing machine. Since the machine is fixed and the program concerned is the shortest, the Kolmogorov complexity is a definite positive integer. The Kolmogorov complexity of an object is the length of the shortest binary program from which it can be computed. Therefore, it is a lower bound on the length of a computably-compressed version (in bits) of that object by any existing (or future) compression program.\n\nAccording to an elegant Euclidian proof, there is an infinite number of prime numbers. Bernhard Riemann demonstrated that the number of primes less than a given number is connected with the 0s of the Riemann zeta function. Jacques Hadamard and Charles Jean de la Vallée-Poussin proved in 1896 that this\nnumber of primes is asymptotic to formula_3; see Prime number theorem (use formula_4 for the natural logarithm an formula_5 for the binary logarithm). Using the incompressibility method, G. J. Chaitin argued as follows: Each formula_2 can be described by its prime factorization formula_7 (which is unique), where formula_8 are the first formula_9 primes which are (at most) formula_2 and the exponents (possibly) 0. Each exponent is (at most) formula_11, and can be described by formula_12 bits. The description of formula_2 can be given in formula_14 bits, provided we know the value of formula_15 (enabling one to parse the consecutive blocks of exponents). To describe formula_15 requires only formula_17 bits. Using the incompressibility of most positive integers, for each formula_18 there is a positive integer formula_2 of binary length formula_20 which cannot be described in fewer than formula_21 bits. This shows that the number of primes, formula_22 less than formula_2, satisfies\n\nA more-sophisticated approach attributed to Piotr Berman (present proof partially by John Tromp) describes every incompressible formula_2 by formula_9 and formula_27, where formula_28 is the largest prime number dividing formula_2. Since formula_2 is incompressible, the length of this description must exceed formula_11. To parse the first block of the description formula_9 must be given in prefix form formula_33, where formula_34 is an arbitrary, small, positive function. Therefore, formula_35. Hence, formula_36 with formula_37 for a special sequence of values formula_38. This shows that the expression below holds for this special sequence, and a simple extension shows that it holds for every formula_39:\n\nBoth proofs are presented in more detail.\n\nA labeled graph formula_41 with formula_2 nodes can be represented by a string formula_43 of formula_44 bits, where each bit indicates the presence (or absence) of an edge between the pair of nodes in that position. formula_45, and the degree formula_46 of each vertex satisfies\n\nTo prove this by the incompressibity method, if the deviation is larger we can compress the description of formula_48 below formula_49; this provides the required contradiction. This theorem is required in a more complicated proof, where the incompressibility argument is used a number of times to show that the number of unlabeled graphs is\n\nA transitive tournament is a complete directed graph, formula_41; if formula_52, formula_53. Consider the set of all transitive tournaments on formula_2 nodes. Since a tournament is a labeled, directed complete graph, it can be encoded by a string formula_43 of formula_44 bits where each bit indicates the direction of the edge between the pair of nodes in that position. Using this encoding, every transitive tournament contains a transitive subtournament on (at least) formula_57 vertices with\n\nThis was shown as the first problem. It is easily solved by the incompressibility method, as are the coin-weighing problem, the number of covering families and expected properties; for example, at least a fraction of formula_59 of all transitive tournaments on formula_2 vertices have transitive subtournaments on not more than formula_61 vertices. formula_2 is large enough.\n\nIf a number of events are independent (in probability theory) of one another, the probability that none of the events occur can be easily calculated. If the events are dependent, the problem becomes difficult. Lovász local lemma is a principle that if events are mostly independent of one another and have an individually-small probability, there is a positive probability that none of them will occur. It was proven by the incompressibility method. Using the incompressibility method, several versions of expanders and superconcentrator graphs were shown to exist.\n\nIn the Heilbronn triangle problem, throw formula_2 points in the unit square and determine the maximum of the minimal area of a triangle formed by three of the points over all possible arrangements. This problem was solved for small arrangements, and much work was done on asymptotic expression as a function of formula_2. The original conjecture of Heilbronn was formula_65 during the early 1950s. Paul Erdős proved that this bound is correct for formula_2, a prime number. The general problem remains unsolved, apart from the best-known lower bound formula_67 (achievable; hence, Heilbronn's conjecture is not correct for general formula_2) and upper bound formula_69 (proven by Komlos, Pintsz and Szemeredi in 1982 and 1981, respectively). Using the incompressibility method, the average case was studied. It was proven that if the area is too small (or large) it can be compressed below the Kolmogorov complexity of a uniformly-random arrangement (high Kolmogorov complexity). This proves that for the overwhelming majority of the arrangements (and the expectation), the area of the smallest triangle formed by three of formula_2 points thrown uniformly at random in the unit square is formula_71. In this case, the incompressibility method proves the lower and upper bounds of the property involved.\n\nThe law of the iterated logarithm, the law of large numbers and the recurrence property were shown to hold using the incompressibility method and Kolmogorov's zero–one law, with normal numbers expressed as binary strings (in the sense of E. Borel) and the distribution of 0s and 1s in binary strings of high Kolmogorov complexity.\n\nThe basic Turing machine, as conceived by Alan Turing in 1936, consists of a memory: a tape of potentially-infinite cells on which a symbol can be written and a finite control, with a read-write head attached, which scans a cell on the tape. At each step, the read-write head can change the symbol in the cell being scanned and move one cell left, right, or not at all according to instruction from the finite control. Turing machines with two tape symbols may be considered for convenience, but this is not essential.\n\nIn 1968, F. C. Hennie showed that such a Turing machine requires order formula_72 to recognize the language of binary palindromes in the worst case. In 1977, W. J. Paul presented an incompressibility proof which showed that order formula_72 time is required in the average case. For every integer formula_2, consider all words of that length. For convenience, consider words with the middle third of the word consisting of 0s. The accepting Turing machine ends with an accept state on the left (the beginning of the tape). A Turing-machine computation of a given word gives for each location (the boundary between adjacent cells) a sequence of crossings from left to right and right to left, each crossing in a particular state of the finite control. Positions in the middle third of a candidate word all have a crossing sequence of length formula_75 (with a total computation time of formula_76), or some position has a crossing sequence of formula_77. In the latter case, the word (if it is a palindrome) can be identified by that crossing sequence.\n\nIf other palindromes (ending in an accepting state on the left) have the same crossing sequence, the word (consisting of a prefix up to the position of the involved crossing sequence) of the original palindome concatenated with a suffix the remaining length of the other palindrome would be accepted as well. Taking the palindrome of formula_78, the Kolmogorov complexity described by formula_77 bits is a contradiction.\n\nSince the overwhelming majority of binary palindromes have a high Kolmogorov complexity, this gives a lower bound on the average-case running time. The result is much more difficult, and shows that Turing machines with formula_80 work tapes are more powerful than those with formula_9 work tapes in real time (here one symbol per step).\n\nIn 1984, W. Maass and M. Li and P. M. B. Vitanyi showed that the simulation of two work tapes by one work tape of a Turing machine takes formula_82 time deterministically (optimally, solving a 30-year open problem) and formula_83 time nondeterministically (in, this is formula_84. More results concerning tapes, stacks and queues, deterministically and nondeterministically, were proven with the incompressibiity method.\n\nHeapsort is a sorting method, invented by J. W. J. Williams and refined by R. W. Floyd, which always runs in formula_85 time. It is questionable whether Floyd's method is better than Williams' on average, although it is better in the worst case. Using the incompressibility method, it was shown that Williams' method runs on average in formula_86 time and Floyd's method runs on average in formula_87 time. The proof was suggested by Ian Munro.\n\nShellsort, discovered by Donald Shell in 1959, is a comparison sort which divides a list to be sorted into sublists and sorts them separately. The sorted sublists are then merged, reconstituting a partially-sorted list. This process repeats a number of times (the number of passes). The difficulty of analyzing the complexity of the sorting process is that it depends on the number formula_2 of keys to be sorted, on the number formula_89 of passes and the increments governing the scattering in each pass; the sublist is the list of keys which are the increment parameter apart. Although this sorting method inspired a large number of papers, only the worst case was established. For the average running time, only the best case for a two-pass Shellsort and an upper bound of formula_90 for a particular increment sequence for three-pass Shellsort were established. A general lower bound on an average formula_89-pass Shellsort was given which was the first advance in this problem in four decades. In every pass, the comparison sort moves a key to another place a certain distance (a path length). All these path lengths are logarithmically coded for length in the correct order (of passes and keys). This allows the reconstruction of the unsorted list from the sorted list. If the unsorted list is incompressible (or nearly so), since the sorted list has near-zero Kolmogorov complexity (and the path lengths together give a certain code length) the sum must be at least as large as the Kolmogorov complexity of the original list. The sum of the path lengths corresponds to the running time, and the running time is lower-bounded in this argument by formula_92. This was improved in to a lower bound of \nwhere formula_94. This implies for example the Jiang-Li-Vitanyi lower bound for all formula_89-pass increment sequences and improves that lower bound for particular increment sequences;\nthe Janson-Knuth upper bound is matched by a lower bound for the used increment sequence, showing that three pass Shellsort for this increment sequence uses formula_96 inversions.\n\nAnother example is as follows. formula_97 are natural numbers and formula_98, it was shown that for every formula_2 there is a Boolean formula_100 matrix; every formula_101 submatrix has a rank at least formula_102 by the incompressibility method.\n\nAccording to Gödel's first incompleteness theorem, in every formal system with computably enumerable theorems (or proofs) strong enough to contain Peano arithmetic, there are true (but unprovable) statements or theorems. This is proved by the incompressibility method; every formal system formula_103 can be described finitely (for example, in formula_104 bits). In such a formal system, we can express formula_105 since it contains arithmetic. Given formula_103, we can search exhaustively for a proof that some string formula_107 of length formula_108 satisfies formula_109. In this way, we obtain the first such string; formula_110: contradiction.\n\nAlthough the probabilistic method generally shows the existence of an object with a certain property in a class, the incompressibility method tends to show that the overwhelming majority of objects in the class (the average, or the expectation) have that property. It is sometimes easy to turn a probabilistic proof into an incompressibility proof or vice versa. In some cases, it is difficult or impossible to turn a proof by incompressibility into a probabilistic (or counting proof). In virtually all the cases of Turing-machine time complexity cited above, the incompressibility method solved problems which had been open for decades; no other proofs are known. Sometimes a proof by incompressibility can be turned into a proof by counting, as happened in the case of the general lower bound on the running time of Shellsort.\n"}
{"id": "41273889", "url": "https://en.wikipedia.org/wiki?curid=41273889", "title": "Infinite loop space machine", "text": "Infinite loop space machine\n\nIn topology, a branch of mathematics, given a topological monoid \"X\" up to homotopy (in a nice way), an infinite loop space machine produces a group completion of \"X\" together with infinite loop space structure. For example, one can take \"X\" to be the classifying space of a symmetric monoidal category \"S\"; that is, formula_1. Then the machine produces the group completion formula_2. The space formula_3 may be described by the K-theory spectrum of \"S\".\n\n"}
{"id": "47691535", "url": "https://en.wikipedia.org/wiki?curid=47691535", "title": "Jessie Marie Jacobs", "text": "Jessie Marie Jacobs\n\nJessie Marie Jacobs Muller Offermann (1890–1954) was an American mathematician who also made contributions to the field of genetics.\n\nJessie M. Jacobs completed her undergraduate degree at McPherson College. After a year spent teaching high school she was awarded one of the first two fellowships to study graduate-level mathematics at the University of Kansas, where she earned her master's degree in 1916. She earned her Ph.D in mathematics from the University of Illinois at Urbana–Champaign in 1919 under the supervision of Arthur Byron Coble. She became an associate professor at Rockford College and then, in 1920, an instructor at the University of Texas at Austin, where she also edited the \"Texas Mathematics Teachers' Bulletin\". Her tenure at the university, along with that of colleague Goldie Printis Horton, is recognized by an annual lecture series.\n\nWhile at the University of Texas, she met geneticist Hermann Joseph Muller, when he asked her for help modeling the mathematics of mutation in flies. The couple married in 1923. Their son David, who would become a mathematician and computer scientist, was born the following year. Jessie's university appointment was terminated in 1924 against her wishes: her departmental colleagues felt that academia and motherhood were incompatible. Her own teaching career over, Jessie collaborated with her husband in the \"Drosophila\" laboratory and co-authored an article with him. Muller would later win a Nobel prize for the research he performed with Jessie's assistance during this period.\n\nThe marriage had grown strained, but in 1933 Jessie joined Hermann Muller in Berlin and then Leningrad for a portion of his Guggenheim fellowship. She divorced her husband in 1935 (first in the Soviet Union, then in Texas) and within a few months remarried Carlos Alberto Offermann. He had worked in Hermann Muller's Leningrad laboratory from 1933-1934. After marrying Jessie in Texas Carlos returned to his position, now relocated to Moscow, though his wife and stepson were unable to join him. So long as his father remained in the Soviet Union, a Texas judge refused to permit David Muller to leave the state. Jessie made a living in Austin by tutoring university students in mathematics, subletting a room, and supervising a Works Progress Administration group writing a history of Travis County.\n\nIn 1938, Carlos Offermann returned to Austin, and the family soon moved to Chicago. Assisted by his wife, Carlos pursued experimental work necessary for his Ph.D. Jessie was diagnosed with tuberculosis, and in 1940 the family moved to California, where they hoped she could recuperate. In 1942, her health had deteriorated and she was forced to enter a sanitarium, and was institutionalized at the time of her death.\n"}
{"id": "14988066", "url": "https://en.wikipedia.org/wiki?curid=14988066", "title": "Knaster–Kuratowski fan", "text": "Knaster–Kuratowski fan\n\nIn topology, a branch of mathematics, the Knaster–Kuratowski fan (named after Polish mathematicians Bronisław Knaster and Kazimierz Kuratowski) is a specific connected topological space with the property that the removal of a single point makes it totally disconnected. It is also known as Cantor's leaky tent or Cantor's teepee (after Georg Cantor), depending on the presence or absence of the apex.\n\nLet formula_1 be the Cantor set, let formula_2 be the point formula_3, and let formula_4, for formula_5, denote the line segment connecting formula_6 to formula_2. If formula_5 is an endpoint of an interval deleted in the Cantor set, let formula_9; for all other points in formula_1 let formula_11; the Knaster–Kuratowski fan is defined as formula_12 equipped with the subspace topology inherited from the standard topology on formula_13.\n\nThe fan itself is connected, but becomes totally disconnected upon the removal of formula_2.\n\n\n"}
{"id": "4294893", "url": "https://en.wikipedia.org/wiki?curid=4294893", "title": "Link (geometry)", "text": "Link (geometry)\n\nIn geometry, the link of a vertex of a 2-dimensional simplicial complex is a graph that encodes information about the local structure of the complex at the vertex.\n\nIt is a graph-theoretic analog to a sphere centered at a point.\n\nLet be a simplicial complex. The link of a vertex is the graph constructed as follows.  The vertices of are precisely the edges of incident to .  Two such edges are adjacent in iff they are incident to a common 2-cell at . \n\nThe graph is often given the topology of a ball of small radius centred at .\n\nSimilarly, for an abstract simplicial complex and a face of , there is also a notion of the link of a face , denoted .   is the set of faces such that \nBecause is simplicial, there is a set isomorphism between and \n\nThe link of a vertex of a tetrahedron is a triangle – the three vertices of the link corresponds to the three edges incident to the vertex, and the three edges of the link correspond to the faces incident to the vertex. In this example, the link can be visualized by cutting off the vertex with a plane; formally, intersecting the tetrahedron with a plane near the vertex – the resulting cross-section is the link.\n"}
{"id": "25425963", "url": "https://en.wikipedia.org/wiki?curid=25425963", "title": "Madhava's sine table", "text": "Madhava's sine table\n\nMadhava's sine table is the table of trigonometric sines of various angles constructed by the 14th century Kerala mathematician-astronomer Madhava of Sangamagrama. The table lists the trigonometric sines of the twenty-four angles 3.75°, 7.50°, 11.25°, ..., and 90.00° (angles that are integral multiples of 3.75°, i.e. 1/24 of a right angle, beginning with 3.75 and ending with 90.00). The table is encoded in the letters of Devanagari using the Katapayadi system. This gives the entries in the table an appearance of the verses of a poem in Sanskrit.\n\nMadhava's original work containing the sine table has not yet been traced. The table is seen reproduced in the \"Aryabhatiyabhashya\" of Nilakantha Somayaji (1444–1544) and also in the \"Yuktidipika/Laghuvivrti\" commentary of Tantrasamgraha by Sankara Variar (circa. 1500-1560).\n\nThe image below gives Madhava's sine table in Devanagari as reproduced in \"Cultural foundations of mathematics\" by C.K. Raju. The first twelve lines constitute the entries in the table. The last word in the thirteenth line indicates that these are \"as told by Madhava\".\n\nTo understand the meaning of the values tabulated by Madhava, consider some angle whose measure is A. Consider a circle of unit radius and center O. Let the arc PQ of the circle subtend an angle A at the center O. Drop the perpendicular QR from Q to OP; then the length of the line segment RQ is the value of the trigonometric sine of the angle A. Let PS be an arc of the circle whose length is equal to the length of the segment RQ. For various angles A, Madhava's table gives the measures of the corresponding angles formula_1POS in arcminutes, arcseconds and sixtieths of an arcsecond.\n\nAs an example, let A be an angle whose measure is 22.50°. In Madhava's table, the entry corresponding to 22.50° is the measure in arcminutes, arcseconds and sixtieths of arcseconds of the angle whose radian measure is the modern value of sin 22.50°. The modern numerical value of sin 22.50° is 0.382683432363 and,\n\nand\n\nIn the Katapayadi system the digits are written in the reverse order. Thus in Madhava's table, the entry corresponding to 22.50° is 70435131.\n\nFor an angle whose measure is \"A\", let\n\nThen\n\nEach of the lines in the table specifies eight digits. Let the digits corresponding to angle A (read from left to right) be\n\nThen according to the rules of the Katapayadi system of Kerala mathematicians we have\n\nTo complete the numerical computations one must have a knowledge of the value of pi(formula_6). It is appropriate that we use the value of π computed by Madhava himself. Nilakantha Somayaji has given this value of π in his Āryabhaṭīya-Bhashya as follows:\n\nA transliteration of the last two lines:\n\n<poem>\n</poem>\n\nThe various words indicate certain numbers encoded in a scheme known as the bhūtasaṃkhyā system. The meaning of the words and the numbers encoded by them (beginning with the units place) are detailed in the following translation of the verse: \n\"Gods (vibudha : 33), eyes (netra : 2), elephants (gaja : 8), snakes (ahi : 8), fires (hutāśana : 3), three (tri : 3), qualities (guṇa : 3), vedas (veda : 4), nakṣatras (bha : 27), elephants (vāraṇa : 8), and arms (bāhavaḥ : 2) - the wise say that this is the measure of the circumference when the diameter of a circle is nava-nikharva (900,000,000,000).\"\n\nSo, the translation of the poem using the bhūtasaṃkhyā system will simply read \"2827433388233 is, as the wise say, the circumference of a circle whose diameter is nava-nikharva (900,000,000,000)\". That is, divide 2827433388233 (the number from the first two lines of the poem in reverse order) by nava-nikharva (900,000,000,000) to get the value of pi(π). This calculation yields the value π = 3.1415926535922. This is the value of π used by Madhava in his further calculations and is accurate to 11 decimal places.\n\nMadhava's table lists the following digits corresponding to the angle 45.00°:\n\nThis yields the angle with measure\n\nThe value of the trigonometric sine of 45.00° as given in Madhava's table is\n\nSubstituting the value of π computed by Madhava in the above expression, one gets sin 45° as 0.70710681.\n\nThis value may be compared with the modern exact value of sin 45.00°, namely, 0.70710678.\n\nIn table below the first column contains the list of the twenty-four angles beginning with 3.75 and ending with 90.00. The second column contains the values tabulated by Madhava in Devanagari in the form in which it was given by Madhava. (These are taken from \"Malayalam Commentary of Karanapaddhati\" by P.K. Koru and are slightly different from the table given in \"Cultural foundations of mathematics\" by C.K. Raju.) The third column contains ISO 15919 transliterations of the lines given in the second column. The digits encoded by the lines in second column are given in Arabic numerals in the fourth column. The values of the trigonometric sines derived from the numbers specified in Madhava's table are listed in the fifth column. These values are computed using the approximate value 3.1415926535922 for π obtained by Madhava. For comparison, the exact values of the trigonometric sines of the angles are given in the sixth column.\n\nNo work of Madhava detailing the methods used by him for the computation of the sine table has survived. However from the writings of later Kerala mathematicians like Nilakantha Somayaji (Tantrasangraha) and Jyeshtadeva (Yuktibhāṣā) that give ample references to Madhava's accomplishments, it is conjectured that Madhava computed his sine table using the power series expansion of sin \"x\".\n\n\n"}
{"id": "19912352", "url": "https://en.wikipedia.org/wiki?curid=19912352", "title": "Method of dominant balance", "text": "Method of dominant balance\n\nIn mathematics, the method of dominant balance is used to determine the asymptotic behavior of solutions to an ordinary differential equation without fully solving the equation. The process is iterative, in that the result obtained by performing the method once can be used as input when the method is repeated, to obtain as many terms in the asymptotic expansion as desired.\n\nThe process goes as follows:\n\nFor arbitrary constants and , consider \n\nThis differential equation cannot be solved exactly. However, it is useful to consider how the solutions behave for large : it turns out that formula_3 behaves like formula_4 as \"x\" → ∞ . \n\nMore rigorously, we will have formula_5, not formula_6.\nSince we are interested in the behavior of in the large limit, we change variables to = exp(\"S\"(\"x\")), and re-express the ODE in terms of \"S\"(\"x\"),\nor\nwhere we have used the product rule and chain rule to evaluate the derivatives of .\n\nNow \"suppose\" first that a solution to this ODE satisfies \nas \"x\" → ∞, so that \nas \"x\" → ∞. Obtain then the dominant asymptotic behaviour by setting\n\nIf formula_12 satisfies the above asymptotic conditions, then the above assumption is consistent. The terms we dropped will have been negligible with respect to the ones we kept. \n\nformula_12 is not a solution to the ODE for , but it represents \"the dominant asymptotic behavior\", which is what we are interested in. Check that this choice for formula_12 is consistent,\nEverything is indeed consistent. \n\nThus the dominant asymptotic behaviour of a solution to our ODE has been found,\n\nBy convention, the full asymptotic series is written as\n\nso to get at least the first term of this series we have to take a further step to see if there is a power of out the front.\n\nProceed by introducing a new subleading dependent variable,\n\nand then seek asymptotic solutions for \"C\"(\"x\"). Substituting into the above ODE for \"S\"(\"x\") we find\nRepeating the same process as before, we keep and to find that\n\nThe leading asymptotic behaviour is then\n\n"}
{"id": "40969212", "url": "https://en.wikipedia.org/wiki?curid=40969212", "title": "Moment measure", "text": "Moment measure\n\nIn probability and statistics, a moment measure is a mathematical quantity, function or, more precisely, measure that is defined in relation to mathematical objects known as point processes, which are types of stochastic processes often used as mathematical models of physical phenomena representable as randomly positioned points in time, space or both. Moment measures generalize the idea of (raw) moments of random variables, hence arise often in the study of point processes and related fields.\n\nAn example of a moment measure is the first moment measure of a point process, often called mean measure or intensity measure, which gives the expected or average number of points of the point process being located in some region of space. In other words, if the number of points of a point process located in some region of space is a random variable, then the first moment measure corresponds to the first moment of this random variable.\n\nMoment measures feature prominently in the study of point processes as well as the related fields of stochastic geometry and spatial statistics whose applications are found in numerous scientific and engineering disciplines such as biology, geology, physics, and telecommunications.\n\nPoint processes are mathematical objects that are defined on some underlying mathematical space. Since these processes are often used to represent collections of points randomly scattered in physical space, time or both, the underlying space is usually \"d\"-dimensional Euclidean space denoted here by formula_1, but they can be defined on more abstract mathematical spaces.\n\nPoint processes have a number of interpretations, which is reflected by the various types of point process notation. For example, if a point formula_2 belongs to or is a member of a point process, denoted by formula_3, then this can be written as:\n\nand represents the point process being interpreted as a random set. Alternatively, the number of points of formula_3 located in some Borel set formula_6 is often written as:\n\nwhich reflects a random measure interpretation for point processes. These two notations are often used in parallel or interchangeably.\n\nFor some integer formula_8, the formula_9-th power of a point process formula_3 is defined as:\n\nwhere formula_12 is a collection of not necessarily disjoint Borel sets (in formula_1), which form a formula_9-fold Cartesian product of sets denoted by formula_15. The symbol formula_16 denotes standard multiplication.\n\nThe notation formula_17 reflects the interpretation of the point process formula_3 as a random measure.\n\nThe formula_9-th power of a point process formula_3 can be equivalently defined as:\n\nwhere summation is performed over all formula_9-tuples of (possibly repeating) points, and formula_23 denotes an indicator function such that formula_24is a Dirac measure. This definition can be contrasted with the definition of the \"n\"-factorial power of a point process for which each \"n\"-tuples consists of \"n\" points.\n\nThe formula_9-th moment measure is defined as:\n\nwhere the \"E\" denotes the expectation (operator) of the point process formula_3. In other words, the \"n\"-th moment measure is the expectation of the \"n\"-th power of some point process.\n\nThe formula_28th moment measure of a point process formula_3 is equivalently defined as:\n\nwhere formula_31 is any non-negative measurable function on formula_32 and the sum is over formula_9-tuples of points for which repetition is allowed.\n\nFor some Borel set \"B\", the first moment of a point process \"N\" is:\n\nwhere formula_35 is known, among other terms, as the \"intensity measure\" or \"mean measure\", and is interpreted as the expected or average number of points of formula_3 found or located in the set formula_6.\n\nThe second moment measure for two Borel sets formula_38 and formula_6 is:\n\nwhich for a single Borel set formula_6 becomes\n\nwhere formula_43 denotes the variance of the random variable formula_44.\n\nThe previous variance term alludes to how moments measures, like moments of random variables, can be used to calculate quantities like the variance of point processes. A further example is the covariance of a point process formula_3 for two Borel sets formula_38 and formula_6, which is given by:\n\nFor a general Poisson point process with intensity measure formula_49 the first moment measure is:\n\nwhich for a homogeneous Poisson point process with constant intensity formula_51 means:\n\nwhere formula_53 is the length, area or volume (or more generally, the Lebesgue measure) of formula_6.\n\nFor the Poisson case with measure formula_49 the second moment measure defined on the product set formula_56 is:\n\nwhich in the homogeneous case reduces to\n\n"}
{"id": "5411727", "url": "https://en.wikipedia.org/wiki?curid=5411727", "title": "Multiple description coding", "text": "Multiple description coding\n\nMultiple description coding (MDC) is a coding technique that fragments a single media stream into \"n\" substreams (\"n\" ≥ 2) referred to as descriptions. The packets of each description are routed over multiple, (partially) disjoint paths. In order to decode the media stream, any description can be used, however, the quality improves with the number of descriptions received in parallel. The idea of MDC is to provide error resilience to media streams. Since an arbitrary subset of descriptions can be used to decode the original stream, network congestion or packet loss — which are common in best-effort networks such as the Internet — will not interrupt the stream but only cause a (temporary) loss of quality. The quality of a stream can be expected to be roughly proportional to data rate sustained by the receiver.\n\nMDC is a form of data partitioning, thus comparable to layered coding as it is used in MPEG-2 and MPEG-4. Yet, in contrast to MDC, layered coding mechanisms generate a base layer and n enhancement layers. The base layer is necessary for the media stream to be decoded, enhancement layers are applied to improve stream quality. However, the first enhancement layer depends on the base layer and each enhancement layer \"n\" + 1 depends on its subordinate layer \"n\", thus can only be applied if \"n\" was already applied. Hence, media streams using the layered approach are interrupted whenever the base layer is missing and, as a consequence, the data of the respective enhancement layers is rendered useless. The same applies for missing enhancement layers. In general, this implies that in lossy networks the quality of a media stream is not proportional to the amount of correctly received data.\n\nBesides increased fault tolerance, MDC allows for rate-adaptive streaming: Content providers send all descriptions of a stream without paying attention to the download limitations of clients. Receivers that cannot sustain the data rate only subscribe to a subset of these streams, thus freeing the content provider from sending additional streams at lower data rates.\n\nThe vast majority of state-of-the art codecs uses single description (SD) video coding. This approach does not partition any data at all. Despite the aforementioned advantages of MDC, SD codecs are still predominant. The reasons are probably the comparingly high complexity of codec development, the loss of some compression efficiency as well as the caused transmission overhead.\n\nThough MDC has its practical roots in media communication, it is widely researched in the area of information theory.\n\n"}
{"id": "1488320", "url": "https://en.wikipedia.org/wiki?curid=1488320", "title": "No-communication theorem", "text": "No-communication theorem\n\nIn physics, the no-communication theorem or no-signaling principle is a no-go theorem from quantum information theory which states that, during measurement of an entangled quantum state, it is not possible for one observer, by making a measurement of a subsystem of the total state, to communicate information to another observer. The theorem is important because, in quantum mechanics, quantum entanglement is an effect by which certain widely separated events can be correlated in ways that suggest the possibility of instantaneous communication. The no-communication theorem gives conditions under which such transfer of information between two observers is impossible. These results can be applied to understand the so-called paradoxes in quantum mechanics, such as the EPR paradox, or violations of local realism obtained in tests of Bell's theorem. In these experiments, the no-communication theorem shows that failure of local realism does not lead to what could be referred to as \"spooky communication at a distance\" (in analogy with Einstein's labeling of quantum entanglement as \"spooky action at a distance\").\n\nThe no-communication theorem states that, within the context of quantum mechanics, it is not possible to transmit classical bits of information by means of carefully prepared mixed or pure states, whether entangled or not. The theorem disallows all communication, not just faster-than-light communication, by means of shared quantum states. The theorem disallows not only the communication of whole bits, but even fractions of a bit. This is important to take note of, as there are many classical radio communications encoding techniques that can send arbitrarily small fractions of a bit across arbitrarily narrow, noisy communications channels. In particular, one may imagine that there is some ensemble that can be prepared, with small portions of the ensemble communicating a fraction of a bit; this, too, is not possible.\n\nThe theorem is built on the basic presumption that the laws of quantum mechanics hold. Similar theorems may or may not hold for other related theories, such as hidden variable theories. The no-communication theorem is not meant to constrain other, non-quantum-mechanical theories.\n\nThe basic assumption entering into the theorem is that a quantum-mechanical system is prepared in an initial state, and that this initial state is describable as a mixed or pure state in a Hilbert space \"H\". The system then evolves over time in such a way that there are two spatially distinct parts, \"A\" and \"B\", sent to two distinct observers, Alice and Bob, who are free to perform quantum mechanical measurements on their portion of the total system (viz, A and B). The question is: is there any action that Alice can perform that would be detectable by Bob? The theorem replies 'no'.\n\nAn important assumption going into the theorem is that neither Alice nor Bob is allowed, in any way, to affect the preparation of the initial state. If Alice were allowed to take part in the preparation of the initial state, it would be trivially easy for her to encode a message into it; thus neither Alice nor Bob participates in the preparation of the initial state. The theorem does not require that the initial state be somehow 'random' or 'balanced' or 'uniform': indeed, a third party preparing the initial state could easily encode messages in it, received by Alice and Bob. Simply, the theorem states that, given some initial state, prepared in some way, there is no action that Alice can take that would be detectable by Bob.\n\nThe proof proceeds by defining how the total Hilbert space \"H\" can be split into two parts, \"H\" and \"H\", describing the subspaces accessible to Alice and Bob. The total state of the system is assumed to be described by a density matrix σ. This appears to be a reasonable assumption, as a density matrix is sufficient to describe both pure and mixed states in quantum mechanics. Another important part of the theorem is that measurement is performed by applying a generalized projection operator \"P\" to the state σ. This again is reasonable, as projection operators give the appropriate mathematical description of quantum measurements. After a measurement by Alice, the state of the total system is said to have \"collapsed\" to a state \"P\"(σ).\n\nThe goal of the theorem is to prove that Bob cannot in any way distinguish the pre-measurement state σ from the post-measurement state \"P\"(σ). This is accomplished mathematically by comparing the trace of σ and the trace of \"P\"(σ), with the trace being taken over the subspace \"H\". Since the trace is only over a subspace, it is technically called a partial trace. Key to this step is the assumption that the (partial) trace adequately summarizes the system from Bob's point of view. That is, everything that Bob has access to, or could ever have access to, measure, or detect, is completely described by a partial trace over \"H\" of the system σ. Again, this is a reasonable assumption, as it is a part of standard quantum mechanics. The fact that this trace never changes as Alice performs her measurements is the conclusion of the proof of the no-communication theorem.\n\nThe proof of the theorem is commonly illustrated for the setup of Bell tests in which two observers Alice and Bob perform local observations on a common bipartite system, and uses the statistical machinery of quantum mechanics, namely density states and quantum operations.\n\nAlice and Bob perform measurements on system S whose underlying Hilbert space is\n\nIt is also assumed that everything is finite-dimensional to avoid convergence issues. The state of the composite system is given by a density operator on \"H\". Any density operator σ on \"H\" is a sum of the form:\n\nwhere \"T\" and \"S\" are operators on \"H\" and \"H\". For the following, it is not required to assume that \"T\" and \"S\" are state projection operators: \"i.e.\" they need not necessarily be non-negative, nor have a trace of one. That is, σ can have a definition somewhat broader than that of a density matrix; the theorem still holds. Note that the theorem holds trivially for separable states. If the shared state σ is separable, it is clear that any local operation by Alice will leave Bob's system intact. Thus the point of the theorem is no communication can be achieved via a shared entangled state.\n\nAlice performs a local measurement on her subsystem. In general, this is described by a quantum operation, on the system state, of the following kind\n\nwhere \"V\" are called Kraus matrices which satisfy\n\nThe term\n\nfrom the expression\n\nmeans that Alice's measurement apparatus does not interact with Bob's subsystem.\n\nSupposing the combined system is prepared in state σ and assuming, for purposes of argument, a non-relativistic situation, immediately (with no time delay) after Alice performs her measurement, the relative state of Bob's system is given by the partial trace of the overall state with respect to Alice's system. In symbols, the relative state of Bob's system after Alice's operation is\n\nwhere formula_8 is the partial trace mapping with respect to Alice's system.\n\nOne can directly calculate this state:\n\nFrom this it is argued that, statistically, Bob cannot tell the difference between what Alice did and a random measurement (or whether she did anything at all).\n\n\n\n"}
{"id": "32122567", "url": "https://en.wikipedia.org/wiki?curid=32122567", "title": "Number Theory: An Approach through History from Hammurapi to Legendre", "text": "Number Theory: An Approach through History from Hammurapi to Legendre\n\nNumber Theory, An Approach through History from Hammurapi to Legendre is a book on the history of number theory, written by André Weil.\n\nThe book reviews over three millennia of research on numbers but the key focus is on mathematicians from the 17th-century to the 19th, in particular, on the works of the mathematicians Fermat, Euler, Lagrange and Legendre paved the way for modern number theory. It does not discuss many of the developments in the field after the work of Gauss in \"Disquisitiones Arithmeticae.\" However, it does indicate some of the developments in fields which directly arise from these works, in particular, the theory of elliptic curves.\n\n\n"}
{"id": "2137644", "url": "https://en.wikipedia.org/wiki?curid=2137644", "title": "Quantum programming", "text": "Quantum programming\n\nQuantum programming is the process of assembling sequences of instructions, called quantum programs, that are capable of running on a quantum computer. Quantum programming languages help express quantum algorithms using high-level constructs.\n\nQuantum instruction sets are used to turn higher level algorithms into physical instructions that can be executed on quantum processors. Sometimes these instructions are specific to a given hardware platform, e.g. ion traps or superconducting qubits.\n\nQuil is an instruction set architecture for quantum computing that first introduced a shared quantum/classical memory model. It was introduced by Robert Smith, Michael Curtis, and William Zeng in \"A Practical Quantum Instruction Set Architecture\". Many quantum algorithms (including quantum teleportation, quantum error correction, simulation, and optimization algorithms) require a shared memory architecture.\n\nOpenQASM is the intermediate representation introduced by IBM for use with Qiskit and the IBM Q Experience.\n\nQuantum software development kits provide collections of tools to create and manipulate quantum programs. They also provide the means to simulate the quantum programs, or prepare them to be run using cloud-based quantum devices.\n\nThe following software development kits can be used to run quantum circuits on prototype quantum devices, as well as on simulators.\n\nAn Open Source project developed at the Institute for Theoretical Physics at ETH, which uses the Python programming language to create and manipulate quantum circuits. Results are obtained either using a simulator, or by sending jobs to IBM quantum devices.\n\nAn Open Source project developed by IBM. Quantum circuits are created and manipulated using Python, Swift or Java. Results are obtained either using simulators that run on the user's own device, simulators provided by IBM or prototype quantum devices provided by IBM. As well as the ability to create programs using basic quantum operations, higher level algorithms are available within the Qiskit Aqua package. Qiskit is based on the OpenQASM standard for representing quantum circuits, and will support pulse level control of quantum systems as specified by the OpenPulse standard.\n\nAn Open Source project developed by Rigetti, which uses the Python programming language to create and manipulate quantum circuits. Results are obtained either using simulators or prototype quantum devices provided by Rigetti. As well as the ability to create programs using basic quantum operations, higher level algorithms are available within the Grove package. Forest is based on the Quil instruction set.\n\nPublic access to quantum devices is currently planned for the following SDKs, but not yet implemented.\n\nA project developed by Microsoft as part of the .NET Framework. Quantum programs can be written and run within Visual Studio and VSCode.\n\nAn Open Source project developed by Google, which uses the Python programming language to create and manipulate quantum circuits. Results are obtained using simulators that run on the user's own device.\n\nAn open-source Python library developed by Xanadu for designing, simulating, and optimizing continuous variable (CV) quantum optical circuits. Three simulators are provided - one in the Fock basis, one using the Gaussian formulation of quantum optics, and one using the TensorFlow machine learning library.\n\nA quantum programming environment developed by Cambridge Quantum Computing, that will target simulators and quantum hardware. It is planned for use with the hardware produced by the NQIT hub, as well as Oxford Quantum Circuits. Full release is expected in September 2018.\n\nThere are two main groups of quantum programming languages: imperative quantum programming languages and functional quantum programming languages.\n\nThe most prominent representatives of the imperative languages are QCL, LanQ and Q|SI>.\n\nQuantum Computation Language (QCL) is one of the first implemented quantum programming languages. The most important feature of QCL is the support for user-defined operators and functions. Its syntax resembles the syntax of the C programming language and its classical data types are similar to primitive data types in C. One can combine classical code and quantum code in the same program.\n\nQuantum pseudocode proposed by E. Knill is the first formalized language for description of quantum algorithms. It was introduced and, moreover, was tightly connected with a model of quantum machine called Quantum Random Access Machine (QRAM).\n\nQ|SI> is a platform embedded in .Net language supporting quantum programming in a quantum extension of while-language. This platform includes a compiler of the quantum while-language and a chain of tools for the simulation of quantum computation, optimisation of quantum circuits, termination analysis of quantum programs, and verification of quantum programs.\n\nQ Language is the second implemented imperative quantum programming language. Q Language was implemented as an extension of C++ programming language. It provides classes for basic quantum operations like QHadamard, QFourier, QNot, and QSwap, which are derived from the base class Qop. New operators can be defined using C++ class mechanism.\n\nQuantum memory is represented by class Qreg.\n\nQreg x1; // 1-qubit quantum register with initial value 0\nQreg x2(2,0); // 2-qubit quantum register with initial value 0\nThe computation process is executed using a provided simulator. Noisy environments can be simulated using parameters of the simulator.\n\nQuantum Guarded Command Language (qGCL) was defined by P. Zuliani in his PhD thesis. It is based on Guarded Command Language created by Edsger Dijkstra.\n\nIt can be described as a language of quantum programs specification.\n\nQuantum Macro Assembler (QMASM) is a low-level language specific to quantum annealers such as the D-Wave.\n\nEfforts are underway to develop functional programming languages for quantum computing. Functional programming languages are well-suited for reasoning about programs. Examples include Selinger's QPL, and the Haskell-like language QML by Altenkirch and Grattage. Higher-order quantum programming languages, based on lambda calculus, have been proposed by van Tonder, Selinger and Valiron and by Arrighi and Dowek.\n\nQFC and QPL are two closely related quantum programming languages defined by Peter Selinger. They differ only in their syntax: QFC uses a flow chart syntax, whereas QPL uses a textual syntax. These languages have classical control flow but can operate on quantum or classical data. Selinger gives a denotational semantics for these languages in a category of superoperators.\n\nQML is a Haskell-like quantum programming language by Altenkirch and Grattage. Unlike Selinger's QPL, this language takes duplication, rather than discarding, of quantum information as a primitive operation. Duplication in this context is understood to be the operation that maps formula_1 to formula_2, and is not to be confused with the impossible operation of cloning; the authors claim it is akin to how sharing is modeled in classical languages. QML also introduces both classical and quantum control operators, whereas most other languages rely on classical control.\n\nAn operational semantics for QML is given in terms of quantum circuits, while a denotational semantics is presented in terms of superoperators, and these are shown to agree. Both the operational and denotational semantics have been implemented (classically) in Haskell.\n\nLIQUi|> (pronounced \"liquid\") is a quantum simulation extension on the F# programming language. It is currently being developed by the Quantum Architectures and Computation Group (QuArC) part of the StationQ efforts at Microsoft Research. LIQUi|> seeks to allow theorists to experiment with quantum algorithm design before physical quantum computers are available for use.\n\nIt includes a programming language, optimization and scheduling algorithms, and quantum simulators. LIQUi|> can be used to translate a quantum algorithm written in the form of a high-level program into the low-level machine instructions for a quantum device.\n\nQuantum lambda calculi are extensions of the classical lambda calculus introduced by Alonzo Church and Stephen Cole Kleene in the 1930s. The purpose of quantum lambda calculi is to extend quantum programming languages with a theory of higher-order functions.\n\nThe first attempt to define a quantum lambda calculus was made by Philip Maymin in 1996.\nHis lambda-q calculus is powerful enough to express any quantum computation. However, this language can efficiently solve NP-complete problems, and therefore appears to be strictly stronger than the standard quantum computational models (such as the quantum Turing machine or the quantum circuit model). Therefore, Maymin's lambda-q calculus is probably not implementable on a physical device.\n\nIn 2003, André van Tonder defined an extension of the lambda calculus suitable for proving correctness of quantum programs. He also provided an implementation in the Scheme programming language.\n\nIn 2004, Selinger and Valiron defined a strongly typed lambda calculus for quantum computation with a type system based on linear logic.\n\nQuipper was published in 2013. It is implemented as an embedded language, using Haskell as the host language. For this reason, quantum programs written in Quipper are written in Haskell using provided libraries. For example, the following code implements preparation of a superposition\n\n"}
{"id": "3771917", "url": "https://en.wikipedia.org/wiki?curid=3771917", "title": "Regular grid", "text": "Regular grid\n\nA regular grid is a tessellation of n-dimensional Euclidean space by congruent parallelotopes (e.g. bricks). Grids of this type appear on graph paper and may be used in finite element analysis, finite volume methods, finite difference methods, and in general for discretization of parameter spaces. Since the derivatives of field variables can be conveniently expressed as finite differences, structured grids mainly appear in finite difference methods. Unstructured grids offer more flexibility than structured grids and hence are very useful in finite element and finite volume methods.\n\nEach cell in the grid can be addressed by index (i, j) in two dimensions or (i, j, k) in three dimensions, and each vertex has coordinates formula_1 in 2D or formula_2 in 3D for some real numbers \"dx\", \"dy\", and \"dz\" representing the grid spacing.\n\nA Cartesian grid is a special case where the elements are unit squares or unit cubes, and the vertices are integer points.\n\nA rectilinear grid is a tessellation by rectangles or parallelepipeds that are not, in general, all congruent to each other. The cells may still be indexed by integers as above, but the mapping from indexes to vertex coordinates is less uniform than in a regular grid. An example of a rectilinear grid that is not regular appears on logarithmic scale graph paper.\n\nA curvilinear grid or structured grid is a grid with the same combinatorial structure as a regular grid, in which the cells are quadrilaterals or cuboids rather than rectangles or rectangular parallelepipeds.\n\n"}
{"id": "37720636", "url": "https://en.wikipedia.org/wiki?curid=37720636", "title": "Relativistic Lagrangian mechanics", "text": "Relativistic Lagrangian mechanics\n\nIn theoretical physics, relativistic Lagrangian mechanics is Lagrangian mechanics applied in the context of special relativity and general relativity.\n\nLagrangian mechanics can be formulated in special relativity as follows. Consider one particle (\"N\" particles are considered later).\n\nIf a system is described by a Lagrangian \"L\", the Euler–Lagrange equations\n\nretain their form in special relativity, provided the Lagrangian generates equations of motion consistent with special relativity. Here r = (\"x\", \"y\", \"z\") is the position vector of the particle as measured in some lab frame where Cartesian coordinates are used for simplicity, and\n\nis the coordinate velocity, the derivative of position r with respect to coordinate time \"t\". (Throughout this article, overdots are with respect to coordinate time, not proper time). It is possible to transform the position coordinates to generalized coordinates exactly as in non-relativistic mechanics, r = r(q, \"t\"). Taking the total differential of r obtains the transformation of velocity v to the generalized coordinates, generalized velocities, and coordinate time\n\nremains the same. However, the energy of a moving particle is different to non-relativistic mechanics. It is instructive to look at the total relativistic energy of a free test particle. An observer in the lab frame defines events by coordinates r and coordinate time \"t\", and measures the particle to have coordinate velocity v = \"d\"r/\"dt\". By contrast, an observer moving with the particle will record a different time, this is the \"proper time\", \"τ\". Expanding in a power series, the first term is the particle's rest energy, plus its non-relativistic kinetic energy, followed by higher order relativistic corrections;\n\nwhere \"c\" is the speed of light in vacuum. The differentials in \"t\" and \"τ\" are related by the Lorentz factor \"γ\",\n\nwhere · is the dot product. The relativistic kinetic energy for an uncharged particle of rest mass \"m\" is\n\nand we may naïvely guess the relativistic Lagrangian for a particle to be this relativistic kinetic energy minus the potential energy. However, even for a free particle for which \"V\" = 0, this is wrong. Following the non-relativistic approach, we expect the derivative of this seemingly correct Lagrangian with respect to the velocity to be the relativistic momentum, which it is not.\n\nThe definition of a generalized momentum can be retained, and the advantageous connection between cyclic coordinates and conserved quantities will continue to apply. The momenta can be used to \"reverse-engineer\" the Lagrangian. For the case of the free massive particle, in Cartesian coordinates, the \"x\" component of relativistic momentum is\n\nand similarly for the \"y\" and \"z\" components. Integrating this equation with respect to \"dx\"/\"dt\" gives\n\nwhere \"X\" is an arbitrary function of \"dy\"/\"dt\" and \"dz\"/\"dt\" from the integration. Integrating \"p\" and \"p\" obtains similarly\n\nwhere \"Y\" and \"Z\" are arbitrary functions of their indicated variables. Since the functions \"X\", \"Y\", \"Z\" are arbitrary, without loss of generality we can conclude the common solution to these integrals, a possible Lagrangian that will correctly generate all the components of relativistic momentum, is\n\nwhere \"X\" = \"Y\" = \"Z\" = 0.\n\nAlternatively, since we wish to build a Lagrangian out of relativistically invariant quantities, take the action as proportional to the integral of the Lorentz invariant line element in spacetime, the length of the particle's world line between proper times \"τ\" and \"τ\",\n\nwhere \"ε\" is a constant to be found, and after converting the proper time of the particle to the coordinate time as measured in the lab frame, the integrand is the Lagrangian by definition. The momentum must be the relativistic momentum,\n\nwhich requires \"ε\" = −\"m\"\"c\", in agreement with the previously obtained Lagrangian.\n\nEither way, the position vector r is absent from the Lagrangian and therefore cyclic, so the Euler–Lagrange equations are consistent with the constancy of relativistic momentum,\n\nwhich must be the case for a free particle. Also, expanding the relativistic free particle Lagrangian in a power series to first order in (v/\"c\"),\n\nin the non-relativistic limit when v is small, the higher order terms not shown are negligible, and the Lagrangian is the non-relativistic kinetic energy as it should be. The remaining term is the negative of the particle's rest energy, a constant term which can be ignored in the Lagrangian.\n\nFor the case of an interacting particle subject to a potential \"V\", which may be non-conservative, it is possible for a number of interesting cases to simply subtract this potential from the free particle Lagrangian,\n\nand the Euler–Lagrange equations lead to the relativistic version of Newton's second law, the coordinate time derivative of relativistic momentum is the force acting on the particle;\n\nassuming the potential \"V\" can generate the corresponding force F in this way. If the potential cannot obtain the force as shown, then the Lagrangian would need modification to obtain the correct equations of motion.\n\nIt is also true that if the Lagrangian is explicitly independent of time and the potential \"V\"(r) independent of velocities, then the total relativistic energy\n\nis conserved, although the identification is less obvious since the first term is the relativistic energy of the particle which includes the rest mass of the particle, not merely the relativistic kinetic energy. Also, the argument for homogenous functions does not apply to relativistic Lagrangians.\n\nThe extension to \"N\" particles is straightforward, the relativistic Lagrangian is just a sum of the \"free particle\" terms, minus the potential energy of their interaction;\n\nwhere all the positions and velocities are measured in the same lab frame, including the time.\n\nThe advantage of this coordinate formulation is that it can be applied to a variety of systems, including multiparticle systems. The disadvantage is that some lab frame has been singled out as a preferred frame, and none of the equations are \"manifestly covariant\" (in other words, they do not take the same form in all frames of reference). For an observer moving relative to the lab frame, everything must be recalculated; the position r, the momentum p, total energy \"E\", potential energy, etc. In particular, if this other observer moves with constant relative velocity then Lorentz transformations must be used. However, the action will remain the same since it is Lorentz invariant by construction.\n\nA seemingly different but completely equivalent form of the Lagrangian for a free massive particle, which will readily extend to general relativity as shown below, can be obtained by inserting\n\ninto the Lorentz invariant action so that\n\nwhere \"ε\" = −\"m\"\"c\" is retained for simplicity. Although the line element and action are Lorentz invariant, the Lagrangian is \"not\", because it has explicit dependence on the lab coordinate time. Still, the equations of motion follow from Hamilton's principle\n\nSince the action is proportional to the length of the particle's worldline (in other words its trajectory in spacetime), this route illustrates that finding the stationary action is akin to finding the trajectory of shortest or largest length in spacetime. Correspondingly, the equations of motion of the particle are akin to the equations describing the trajectories of shortest or largest length in spacetime, \"geodesics\".\n\nFor the case of an interacting particle in a potential \"V\", the Lagrangian is still\n\nwhich can also extend to many particles as shown above, each particle has its own set of position coordinates to define its position.\n\nIn the covariant formulation, time is placed on equal footing with space, so the coordinate time as measured in some frame is part of the configuration space alongside the spatial coordinates (and other generalized coordinates). For a particle, either massless or massive, the Lorentz invariant action is (abusing notation)\n\nwhere lower and upper indices are used according to covariance and contravariance of vectors, \"σ\" is an \"affine parameter\", and \"u\" = \"dx\"/\"dσ\" is the four-velocity of the particle.\n\nFor massive particles, \"σ\" can be the arc length \"s\", or proper time \"τ\", along the particle's world line,\n\nFor massless particles, it cannot because the proper time of a massless particle is always zero;\n\nFor a free particle, the Lagrangian has the form\n\nwhere the irrelevant factor of 1/2 is allowed to be scaled away by the scaling property of Lagrangians. No inclusion of mass is necessary since this also applies to massless particles. The Euler–Lagrange equations in the spacetime coordinates are\n\nwhich is the geodesic equation for affinely parameterized geodesics in spacetime. In other words, the free particle follows geodesics. Geodesics for massless particles are called \"null geodesics\", since they lie in a \"light cone\" or \"null cone\" of spacetime, while those for massive particles are called \"non-null geodesics\".\n\nThis manifestly covariant formulation does not extend to an \"N\" particle system, since then the affine parameter of any one particle cannot be defined as a common parameter for all the other particles.\n\nFor a 1d relativistic simple harmonic oscillator, the Lagrangian is\n\nwhere \"k\" is the spring constant.\n\nFor a particle under a constant force, the Lagrangian is\n\nwhere \"a\" is the force per unit mass.\n\nIn special relativity, the Lagrangian of a massive charged test particle in an electromagnetic field modifies to\n\nThe Lagrangian equations in r lead to the Lorentz force law, in terms of the relativistic momentum\n\nIn the language of four vectors and tensor index notation, the Lagrangian takes the form\n\nwhere \"u\" = \"dx\"/\"dτ\" is the four-velocity of the test particle, and \"A\" the electromagnetic four potential.\n\nThe Euler–Lagrange equations are (notice the total derivative with respect to proper time instead of coordinate time)\n\nobtains\n\nUnder the total derivative with respect to proper time, the first term is the relativistic momentum, the second term is\n\nthen rearranging, and using the definition of the antisymmetric electromagnetic tensor, gives the covariant form of the Lorentz force law in the more familiar form,\n\nThe Lagrangian is that of a single particle plus an interaction term \"L\"\n\nVarying this with respect to the position of the particle \"r\" as a function of time \"t\" gives\n\nThis gives the equation of motion\nwhere\nis the non-gravitational force on the particle. (For \"m\" to be independent of time, we must have formula_41.)\n\nRearranging gets the force equation\nwhere Γ is the Christoffel symbol which is the gravitational force field.\n\nIf we let\nbe the (kinetic) linear momentum for a particle with mass, then\nand\nhold even for a massless particle.\n\nIn general relativity, the first term generalizes (includes) both the classical kinetic energy and the interaction with the gravitational field. For a charged particle in an electromagnetic field it is\n\nIf the four spacetime coordinates \"x\" are given in arbitrary units (i.e. unitless), then \"g\" in m is the rank 2 symmetric metric tensor which is also the gravitational potential. Also, \"A\" in V·s is the electromagnetic 4-vector potential.\n\n\n"}
{"id": "3476722", "url": "https://en.wikipedia.org/wiki?curid=3476722", "title": "Rencontres numbers", "text": "Rencontres numbers\n\nIn combinatorial mathematics, the rencontres numbers are a triangular array of integers that enumerate permutations of the set { 1, ..., \"n\" } with specified numbers of fixed points: in other words, partial derangements. (\"Rencontre\" is French for \"encounter\". By some accounts, the problem is named after a solitaire game.) For \"n\" ≥ 0 and 0 ≤ \"k\" ≤ \"n\", the rencontres number \"D\" is the number of permutations of { 1, ..., \"n\" } that have exactly \"k\" fixed points.\n\nFor example, if seven presents are given to seven different people, but only two are destined to get the right present, there are \"D\" = 924 ways this could happen. Another often cited example is that of a dance school with 7 couples, where after tea-break the participants are told to \"randomly\" find a partner to continue, and there are \"D\" = 924 possibilities once more, now, that 2 previous couples meet again just by chance.\n\nHere is the beginning of this array :\n\nThe numbers in the \"k\" = 0 column enumerate derangements. Thus\n\nfor non-negative \"n\". It turns out that\n\nwhere the ratio is rounded up for even \"n\" and rounded down for odd \"n\". For \"n\" ≥ 1, this gives the nearest integer. \n\nMore generally, for any formula_5, we have\n\nThe proof is easy after one knows how to enumerate derangements: choose the \"k\" fixed points out of \"n\"; then choose the derangement of the other \"n\" − \"k\" points.\n\nThe numbers are generated by the power series ; accordingly,\nan explicit formula for \"D\" can be derived as follows:\n\nThis immediately implies that\n\nfor \"n\" large, \"m\" fixed.\n\nThe sum of the entries in each row for the table in \"Numerical Values\" is the total number of permutations of { 1, ..., \"n\" }, and is therefore \"n\"<nowiki>!</nowiki>. If one divides all the entries in the \"n\"th row by \"n\"<nowiki>!</nowiki>, one gets the probability distribution of the number of fixed points of a uniformly distributed random permutation of { 1, ..., \"n\" }. The probability that the number of fixed points is \"k\" is\n\nFor \"n\" ≥ 1, the expected number of fixed points is 1 (a fact that follows from linearity of expectation).\n\nMore generally, for \"i\" ≤ \"n\", the \"i\"th moment of this probability distribution is the \"i\"th moment of the Poisson distribution with expected value 1. For \"i\" > \"n\", the \"i\"th moment is smaller than that of that Poisson distribution. Specifically, for \"i\" ≤ \"n\", the \"i\"th moment is the \"i\"th Bell number, i.e. the number of partitions of a set of size \"i\".\n\nAs the size of the permuted set grows, we get\n\nThis is just the probability that a Poisson-distributed random variable with expected value 1 is equal to \"k\". In other words, as \"n\" grows, the probability distribution of the number of fixed points of a random permutation of a set of size \"n\" approaches the Poisson distribution with expected value 1.\n\n"}
{"id": "3928569", "url": "https://en.wikipedia.org/wiki?curid=3928569", "title": "Rotation number", "text": "Rotation number\n\nIn mathematics, the rotation number is an invariant of homeomorphisms of the circle. \n\nIt was first defined by Henri Poincaré in 1885, in relation to the precession of the perihelion of a planetary orbit. Poincaré later proved a theorem characterizing the existence of periodic orbits in terms of rationality of the rotation number.\n\nSuppose that \"f\": \"S\" → \"S\" is an orientation preserving homeomorphism of the circle \"S\" = R/Z. Then \"f\" may be lifted to a homeomorphism \"F\": R → R of the real line, satisfying\n\nfor every real number \"x\" and every integer \"m\".\n\nThe rotation number of \"f\" is defined in terms of the iterates of \"F\":\n\nHenri Poincaré proved that the limit exists and is independent of the choice of the starting point \"x\". The lift \"F\" is unique modulo integers, therefore the rotation number is a well-defined element of R/Z. Intuitively, it measures the average rotation angle along the orbits of \"f\".\n\nIf \"f\" is a rotation by \"2πθ\" (where \"0≤θ<2π\"), then\n\nthen its rotation number is \"θ\" (cf Irrational rotation).\n\nThe rotation number is invariant under topological conjugacy, and even topological semiconjugacy: if \"f\" and \"g\" are two homeomorphisms of the circle and\n\nfor a continuous map \"h\" of the circle into itself (not necessarily homeomorphic) then \"f\" and \"g\" have the same rotation numbers. It was used by Poincaré and Arnaud Denjoy for topological classification of homeomorphisms of the circle. There are two distinct possibilities.\n\n\nThe rotation number is \"continuous\" when viewed as a map from the group of homeomorphisms (with formula_5 topology) of the circle into the circle.\n\n\n\n"}
{"id": "339965", "url": "https://en.wikipedia.org/wiki?curid=339965", "title": "Rudolf Lipschitz", "text": "Rudolf Lipschitz\n\nRudolf Otto Sigismund Lipschitz (14 May 1832 – 7 October 1903) was a German mathematician who made contributions to mathematical analysis (where he gave his name to the Lipschitz continuity condition) and differential geometry, as well as number theory, algebras with involution and classical mechanics.\n\nRudolf Lipschitz was born on 14 May 1832 in Königsberg. He was the son of a landowner and was raised at his father's estate at Bönkein which was near Königsberg. He entered the University of Königsberg when he was 15, but later moved to the University of Berlin where he studied with Gustav Dirichlet. Despite having his studies delayed by illness, in 1853 Lipschitz graduated with a PhD in Berlin.\n\nAfter receiving his PhD, Lipschitz started teaching at local Gymnasiums. In 1857 he married Ida Pascha, the daughter of one of the landowners with an estate near to his father's. In 1857 he earned his habilitation at the University of Bonn and remained there as a privatdozent. In 1862 Lipschitz became an extraordinary professor at the University of Breslau where he spent the following two years. In 1864 Lipschitz moved back to Bonn as a full professor, remaining there for the rest of his career. Here he examined the dissertation of Felix Klein. Lipschitz died on 7 October 1903 in Bonn.\n\nLipschitz discovered Clifford algebras in 1880, two years after William K. Clifford (1845–1879) and independently of him, and he was the first to use them in the study of orthogonal transformations. Up to 1950 people mentioned \"Clifford–Lipschitz numbers\" when they referred to this discovery of Lipschitz. Yet Lipschitz's name suddenly disappeared from the publications involving Clifford algebras; for instance Claude Chevalley (1909–1984) gave the name \"Clifford group\" to an object that is never mentioned in Clifford's works, but stems from Lipschitz's. Pertti Lounesto (1945–2002) contributed greatly to recalling the importance of Lipschitz's role.\n\n\"Lehrbuch der Analysis\" (two volumes, Bonn 1877, 1880); \n\"Wissenschaft und Staat\" (Bonn, 1874); \n\"Untersuchungen über die Summen von Quadraten\" (Bonn, 1886); \n\"Bedeutung der theoretischen Mechanik\" (Berlin, 1876).\n\n\n"}
{"id": "22203669", "url": "https://en.wikipedia.org/wiki?curid=22203669", "title": "Schur's lemma (from Riemannian geometry)", "text": "Schur's lemma (from Riemannian geometry)\n\nSchur's lemma is a result in Riemannian geometry that says, heuristically, whenever certain curvatures are pointwise constant then they are forced to be globally constant. It is essentially a degree of freedom counting argument.\n\nSuppose formula_1 is a Riemannian manifold and formula_2. Then if\n\n\nThe requirement that formula_2 cannot be lifted. This result is far from true on two-dimensional surfaces. In two dimensions sectional curvature is always pointwise constant since there is only one two-dimensional subspace formula_5, namely formula_16. Furthermore, in two dimensions the Ricci curvature endomorphism is always a multiple of the identity (scaled by Gauss curvature). On the other hand, certainly not all two-dimensional surfaces have constant sectional (or Ricci) curvature.\n\n"}
{"id": "2019131", "url": "https://en.wikipedia.org/wiki?curid=2019131", "title": "Spacetime symmetries", "text": "Spacetime symmetries\n\nSpacetime symmetries are features of spacetime that can be described as exhibiting some form of symmetry. The role of symmetry in physics is important in simplifying solutions to many problems. Spacetime symmetries are used in the study of exact solutions of Einstein's field equations of general relativity. Spacetime symmetries are distinguished from internal symmetries.\n\nPhysical problems are often investigated and solved by noticing features which have some form of symmetry. For example, in the Schwarzschild solution, the role of spherical symmetry is important in deriving the Schwarzschild solution and deducing the physical consequences of this symmetry (such as the nonexistence of gravitational radiation in a spherically pulsating star). In cosmological problems, symmetry finds a role to play in the cosmological principle which restricts the type of universes that are consistent with large-scale observations (e.g. the Friedmann–Lemaître–Robertson–Walker (FLRW) metric). Symmetries usually require some form of preserving property, the most important of which in general relativity include the following:\n\n\nThese and other symmetries will be discussed below in more detail. This preservation property which symmetries usually possess (alluded to above) can be used to motivate a useful definition of these symmetries themselves.\n\nA rigorous definition of symmetries in general relativity has been given by Hall (2004). In this approach, the idea is to use (smooth) vector fields whose local flow diffeomorphisms preserve some property of the spacetime. (Note that one should emphasize in one's thinking this is a diffeomorphism—a transformation on a differential element. The implication is that the behavior of objects with extent may not be as manifestly symmetric.) This preserving property of the diffeomorphisms is made precise as follows. A smooth vector field on a spacetime is said to \"preserve\" a smooth tensor on (or is invariant under ) if, for each smooth local flow diffeomorphism associated with , the tensors and are equal on the domain of . This statement is equivalent to the more usable condition that the Lie derivative of the tensor under the vector field vanishes:\n\non . This has the consequence that, given any two points and on , the coordinates of in a coordinate system around are equal to the coordinates of in a coordinate system around . A \"symmetry on the spacetime\" is a smooth vector field whose local flow diffeomorphisms preserve some (usually geometrical) feature of the spacetime. The (geometrical) feature may refer to specific tensors (such as the metric, or the energy-momentum tensor) or to other aspects of the spacetime such as its geodesic structure. The vector fields are sometimes referred to as \"collineations\", \"symmetry vector fields\" or just \"symmetries\". The set of all symmetry vector fields on forms a Lie algebra under the Lie bracket operation as can be seen from the identity:\n\nthe term on the right usually being written, with an abuse of notation, as\n\nA Killing vector field is one of the most important types of symmetries and is defined to be a smooth vector field that preserves the metric tensor:\n\nThis is usually written in the expanded form as:\n\nKilling vector fields find extensive applications (including in classical mechanics) and are related to conservation laws.\n\nA homothetic vector field is one which satisfies:\n\nwhere is a real constant. Homothetic vector fields find application in the study of singularities in general relativity.\n\nAn affine vector field is one that satisfies:\n\nAn affine vector field preserves geodesics and preserves the affine parameter.\n\nThe above three vector field types are special cases of projective vector fields which preserve geodesics without necessarily preserving the affine parameter.\n\nA conformal vector field is one which satisfies:\n\nwhere is a smooth real-valued function on .\n\nA curvature collineation is a vector field which preserves the Riemann tensor:\n\nwhere are the components of the Riemann tensor. The set of all smooth curvature collineations forms a Lie algebra under the Lie bracket operation (if the smoothness condition is dropped, the set of all curvature collineations need not form a Lie algebra). The Lie algebra is denoted by and may be infinite-dimensional. Every affine vector field is a curvature collineation.\n\nA less well-known form of symmetry concerns vector fields that preserve the energy-momentum tensor. These are variously referred to as matter collineations or matter symmetries and are defined by:\n\nwhere are the energy-momentum tensor components. The intimate relation between geometry and physics may be highlighted here, as the vector field is regarded as preserving certain physical quantities along the flow lines of , this being true for any two observers. In connection with this, it may be shown that \"every Killing vector field is a matter collineation\" (by the Einstein field equations, with or without cosmological constant). Thus, given a solution of the EFE, \"a vector field that preserves the metric necessarily preserves the corresponding energy-momentum tensor\". When the energy-momentum tensor represents a perfect fluid, every Killing vector field preserves the energy density, pressure and the fluid flow vector field. When the energy-momentum tensor represents an electromagnetic field, a Killing vector field does \"not necessarily\" preserve the electric and magnetic fields.\n\nAs mentioned at the start of this article, the main application of these symmetries occur in general relativity, where solutions of Einstein's equations may be classified by imposing some certain symmetries on the spacetime.\n\nClassifying solutions of the EFE constitutes a large part of general relativity research. Various approaches to classifying spacetimes, including using the Segre classification of the energy-momentum tensor or the Petrov classification of the Weyl tensor have been studied extensively by many researchers, most notably Stephani \"et al.\" (2003). They also classify spacetimes using symmetry vector fields (especially Killing and homothetic symmetries). For example, Killing vector fields may be used to classify spacetimes, as there is a limit to the number of global, smooth Killing vector fields that a spacetime may possess (the maximum being 10 for four-dimensional spacetimes). Generally speaking, the higher the dimension of the algebra of symmetry vector fields on a spacetime, the more symmetry the spacetime admits. For example, the Schwarzschild solution has a Killing algebra of dimension 4 (three spatial rotational vector fields and a time translation), whereas the Friedmann-Lemaître-Robertson-Walker (FLRW) metric (excluding the Einstein static subcase) has a Killing algebra of dimension 6 (three translations and three rotations). The Einstein static metric has a Killing algebra of dimension 7 (the previous 6 plus a time translation).\n\nThe assumption of a spacetime admitting a certain symmetry vector field can place restrictions on the spacetime.\n\n\n"}
{"id": "5407025", "url": "https://en.wikipedia.org/wiki?curid=5407025", "title": "Sum of angles of a triangle", "text": "Sum of angles of a triangle\n\nIn several geometries, a triangle has three \"vertices\" and three \"sides\", where three angles of a triangle are formed at each vertex by a pair of adjacent sides. In a Euclidean space, the sum of measures of these three angles of any triangle is invariably equal to the straight angle, also expressed as 180 °, radians, two right angles, or a half-turn. \n\nIt was unknown for a long time whether other geometries exist, where this sum is different. The influence of this problem on mathematics was particularly strong during the 19th century. Ultimately, the answer was proven to be positive: in other spaces (geometries) this sum can be greater or lesser, but it then must depend on the triangle. Its difference from 180° is a case of \"angular defect\" and serves as an important distinction for geometric systems.\n\nIn Euclidean geometry, the triangle postulate states that the sum of the angles of a triangle is two right angles. This postulate is equivalent to the parallel postulate. In the presence of the other axioms of Euclidean geometry, the following statements are equivalent:\n\nThe sum of the angles of a hyperbolic triangle is less than 180°. The relation between angular defect and the triangle's area was first proven by Johann Heinrich Lambert.\n\nOne can easily see how hyperbolic geometry breaks Playfair's axiom, Proclus' axiom (the parallelism, defined as non-intersection, is intransitive in an hyperbolic plane), the equidistance postulate (the points on one side of, and equidistant from, a given line do not form a line), and Pythagoras' theorem. A circle cannot have arbitrarily small curvature, so the three points property also fails.\n\nThe sum of the angles can be arbitrarily small (but positive). For an ideal triangle, a generalization of hyperbolic triangles, this sum is equal to zero.\n\nFor a spherical triangle, the sum of the angles is greater than 180° and can be up to 540°. Specifically, the sum of the angles is\n\nwhere \"f\" is the fraction of the sphere's area which is enclosed by the triangle. \n\nNote that spherical geometry does not satisfy several of Euclid's axioms (including the parallel postulate.)\n\nAngles between adjacent sides of a triangle are referred to as \"interior\" angles in Euclidean and other geometries. \"Exterior\" angles can be also defined, and the Euclidean triangle postulate can be formulated as the exterior angle theorem. One can also consider the sum of all three exterior angles, that equals to 360° in the Euclidean case (as for any convex polygon), is less than 360° in the spherical case, and is greater than 360° in the hyperbolic case.\n\nIn the differential geometry of surfaces, the question of a triangle's angular defect is understood as a special case of the Gauss-Bonnet theorem where the curvature of a closed curve is not a function, but a measure with the support in exactly three points – vertices of a triangle.\n"}
{"id": "34694574", "url": "https://en.wikipedia.org/wiki?curid=34694574", "title": "Witten conjecture", "text": "Witten conjecture\n\nIn algebraic geometry, the Witten conjecture is a conjecture about intersection numbers of stable classes on the moduli space of curves, introduced by Edward Witten in the paper , and generalized in . \nWitten's original conjecture was proved by Maxim Kontsevich in the paper . \n\nWitten's motivation for the conjecture was that two different models of 2-dimensional quantum gravity should have the same partition function. The partition function for one of these models can be described in terms of intersection numbers on the moduli stack of algebraic curves, and the partition function for the other is the logarithm of the τ-function of the KdV hierarchy. Identifying these partition functions gives Witten's conjecture that a certain generating function formed from intersection numbers should satisfy the differential equations of the KdV hierarchy.\n\nSuppose that \"M\" is the moduli stack of compact Riemann surfaces of genus \"g\" with \"n\" distinct marked points \"x\"...,\"x\", \nand is its Deligne–Mumford compactification. There are \"n\" line bundles \"L\" on \n, whose fiber at a point of the moduli stack is given by the cotangent space of a Riemann surface at the marked point \"x\". The intersection index 〈τ, ..., τ〉 is the intersection index of Π \"c\"(\"L\") on where Σ\"d\" = dim = 3\"g\" – 3 + \"n\", and 0 if no such \"g\" exists, where \"c\" is the first Chern class of a line bundle. Witten's generating function\nencodes all the intersection indices as its coefficients. \n\nWitten's conjecture states that the partition function \"Z\" = exp \"F\" is a τ-function for the KdV hierarchy, in other words it satisfies a certain series of partial differential equations corresponding to elements \"L\" for \"i\"≥–1 of the Virasoro algebra.\n\nKontsevich used a combinatorial description of the moduli spaces in terms of ribbon graphs to show that \nformula_2\n\nHere the sum on the right is over the set \"G\" of ribbon graphs \"X\" of compact Riemann surfaces of genus \"g\" with \"n\" marked points. The set of edges \"e\" and points of \"X\" are denoted by \"X\" and \"X\". The function λ is thought of as a function from the marked points to the reals, and extended to edges of the ribbon graph by setting λ of an edge equal to the sum of λ at the two marked points corresponding to each side of the edge. \n\nBy Feynman diagram techniques, this implies that \n\"F\"(\"t\"...) is an asymptotic expansion of \nas Λ lends to infinity, where Λ and Χ are positive definite \"N\" by \"N\" hermitian matrices, and \"t\" is given by\nand the probability measure μ on the positive definite hermitian matrices is given by \nwhere \"c\" is a normalizing constant. This measure has the property that\nwhich implies that its expansion in terms of Feynman diagrams is the expression for \"F\" in terms of ribbon graphs. \n\nFrom this he deduced that exp F is a τ-function for the KdV hierarchy, thus proving Witten's conjecture.\n\nThe Witten conjecture is a special case of a more general relation between integrable systems of Hamiltonian PDEs and the geometry of certain families of 2D topological field theories (axiomatized in the form of the so-called cohomological field theories by Kontsevich and Manin), which was explored and studied systematically by B. Dubrovin and Y. Zhang, A. Givental, C. Teleman and others.\n\nThe Virasoro conjecture is a generalization of the Witten conjecture.\n\n"}
