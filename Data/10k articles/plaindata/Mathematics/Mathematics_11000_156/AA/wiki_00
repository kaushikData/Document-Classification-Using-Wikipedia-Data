{"id": "56768957", "url": "https://en.wikipedia.org/wiki?curid=56768957", "title": "Abundance conjecture", "text": "Abundance conjecture\n\nIn algebraic geometry, the abundance conjecture is a conjecture in \nbirational geometry, more precisely in the minimal model program,\nstating that for every projective variety formula_1 with Kawamata log terminal singularities over a field formula_2 if the canonical bundle formula_3 is nef, then formula_3 is semi-ample.\n\n"}
{"id": "9672987", "url": "https://en.wikipedia.org/wiki?curid=9672987", "title": "Ada Dietz", "text": "Ada Dietz\n\nAda K. Dietz (June 16, 1882 - May 13, 1950) was an American weaver best known for her 1949 monograph \"Algebraic Expressions in Handwoven Textiles\", which defines a novel method for generating weaving patterns based on algebraic patterns. Her method employs the expansion of multivariate polynomials to devise a weaving scheme. Dietz' work is still well-regarded today, by both weavers and mathematicians. Along with the references listed below, Griswold (2001) cites several additional articles on her work.\n\nAda Dietz developed her algebraic method in 1946 while living in Long Beach, California. An avid weaver, Dietz drew upon her experience as a former math teacher to devise a threading pattern based on a cubic binomial expansion. She describes her idea as follows:\n\nA piece based on the formula (\"a\" + \"b\" + \"c\" + \"d\" + \"e\" + \"f\"), submitted to the Little Loomhouse Country Fair in Louisville, Kentucky received such a positive response, which prompted a collaboration between Dietz and Little Loomhouse's founder, Lou Tate. The fruits of the collaboration included the booklet \"Algebraic Expressions in Handwoven Textiles\" and a traveling exhibit which continued throughout the 1950s.\n\n"}
{"id": "21447593", "url": "https://en.wikipedia.org/wiki?curid=21447593", "title": "Adjusted current yield", "text": "Adjusted current yield\n\nThe adjusted current yield is a financial term used in reference to bonds and other fixed-interest securities. It is closely related to the concept of current yield.\n\nThe adjusted current yield is given by the current yield with addition of \nformula_1\n\nHere \"Face value\" is the face value of the bond, and \"Clean price\" is the clean price of the bond (i.e. present value of the bond with accrued interest subtracted).\n\nIn total the \"adjusted current yield\" is given by\nformula_2\n"}
{"id": "30542378", "url": "https://en.wikipedia.org/wiki?curid=30542378", "title": "Alexandru Balaban", "text": "Alexandru Balaban\n\nAlexandru T. Balaban (born April 2, 1931) is a chemist who significantly contributed to the fields of organic chemistry, theoretical chemistry, mathematical chemistry, and chemical graph theory.\n\nBalaban was born in Reșița, in the western part of Romania, close to Timișoara. His parents (Teodor and Florica Balaban) paid a lot of attention to Balaban's education, strongly encouraging his fascination with chemistry. In 1935 his family moved to Bucharest, where Balaban attended elementary school. After World War II, in 1945 they moved to Petroșani, where he finished high school. Alexandru Balaban enrolled the Politehnica University of Bucharest in 1949, where he was awarded a Ph.D. in chemistry on April 2, 1959. The topic of his Ph.D. thesis dealt with the restrictions catalyzed by anhydrous aluminium chloride.\n\nFor more than forty years, professor Balaban held positions at the Chair of Organic Chemistry of the Politehnica University of Bucharest (assistant professor (1956-1960), associate professor (1961-1966) and full professor (1970-1999)). In addition, he was the head of the Laboratory of Isotopically Labelled Compounds of the Bucharest Institute of Atomic Physics from 1967 to 1974.\n\nFrom 1967 to 1970, he was appointed as a Senior Research Officer at the Chemistry Division, International Atomic Energy Agency, Vienna, Austria, in charge with radiopharmaceuticals. From 1995 to 1998, he was acting as Vicepresident of the Romanian Academy. From 1991 to 2012, he was a tenured professor of chemistry at Texas A&M University at Galveston. Since 2013, after his retirement, he has continued as Professor Emeritus at this university.\n\nProfessor Balaban has been investigating the synthesis and properties of pyrylium salts. He discovered with co-workers a new way of synthesis of pyrylium salts by diacylation of alkenes. This reaction is known as Balaban-Nenitzescu-Praill reaction. Nowadays, Balaban is recognized as a world authority in this field. He wrote the only existing book in this area.\n\nBalaban also developed new synthesis of oxazoles by AlCl catalyzed substitution of aromatic compounds with azlactones, followed by dehidratation. In addition, he had been exploring new syntheses of indolizines and of naphthalene derivatives.\n\nIn order to explain the extraordinary stability of hydrazyls such as 2,2-diphenyl-1-picryl-hydrazyl, Professor Balaban and his co-workers prepared related hydrazyls with lower steric shielding and accordingly lower stability. Then the first push-pull diarylaminyls were synthesized and shown to have stabilities depending both on electronic and steric effects.\n\nAnother experimental research direction involves the synthesis of chelate compounds having boron as the central atom.\n\nProfessor Balaban had a permanent interest in theoretical chemistry. He is considered as one of pioneers of the Chemical Graph Theory.\n\nThe first book about this interdisciplinary field was edited by Dr. Balaban and appeared in 1976: \"Chemical Applications of Graph Theory\" (Academic Press, London) and its Chinese translation in 1983.\n\nOne area in which cubic (or trivalent) graphs play a dominant role is the enumeration of [\"n\"]annulene valence isomers (CH) where \"n\" is an even number. With two coauthors, Dr. Balaban published in 1986 on this topic a 3-volume monograph (\"Annulenes, Benzo-, Hetero-, Homo-Derivatives and Their Valence Isomers\", CRC Press). Some of these valence isomers undergo thermal or pohotochemical automerizations. More recently, Dr. Balaban edited in 1997 a book titled \"From Chemical Topology to Three-Dimensional Geometry\", Plenum Publishing Corporation, in which various two- and three-dimensional nets were discussed.\n\nOn using an approach similar to that imagined by Randić for his connectivity index χ, but replacing the adjacency matrix by the distance matrix, and compensating graph-size-increase, a new topological index (TI) \"J\" was proposed under the name \"average distance-based connectivity index\"; it is now known as the \"Balaban index\". Not only is it much less degenerate than all previous TIs, but it allows a simple encoding for the presence of multiple bonds or heteroatoms. Other new TIs were studied: several TIs based on informational descriptors, and triplet-based indices that result from converting matrices into systems of linear equations.\n\nThe Lewis acid-catalyzed syntheses of diamondoid hydrocarbons discovered by Schleyer proceed via multiple 1,2-rearrangements. The graph-theoretical analysis of the simplest such reaction involves an ethyl cation with five substituents which undergoes an automerization. This process is characterized by a 20-vertex or a 10-vertex graph depending whether one carbon atom of ethane is or is not labeled. These graphs constitute the first \"reaction graphs\", in which vertices symbolize reaction intermediates and edges symbolize elementary reaction steeps.\n\nStarting from the 10-vertex graph known as the 5-cage or the Petersen graph, Dr. Balaban observed relations among the known cages. Having discovered how a cage with smaller girth \"g\" was hidden in a cage with girth \"g\" + 1, Dr. Balaban published a paper on what was later proved to be the first 10-cage, known as the Balaban 10-cage. In 1973, he found the unique 11-cage, known as the Balaban 11-cage, as was proved by mathematicians using a lengthy computer search.\n\nProposals made by Balaban about new possibilities for evaluating the performance of researchers, improving the way citations are used for this purpose (e. g., via the Hirsch index) included the use of author indexes of widely circulated treatises, or modifying the way one considers the impact factors of journals.\n\nHonoring invitations to contribute to special issues of journals dedicated to the philosophy of chemistry, Dr. Balaban published in \"Hyle\" an essay on how the \"Sherlock Holmes principle\" (if one eliminates the impossible, then whatever remains, no matter how improbable, must contain the truth) can be implemented by chemical graph theory which allows for instance to find all chemical isomers for a given formula.\n\n\"Books\" (17): author or co-author for 9 books including: \"Pyrylium Salts. Syntheses, Reactions and Physical Properties\", Academic Press,1982; \"Annulenes, Benzo-, Hetero-, Homo-Derivatives and Their Valence Isomers\", 3 volumes, CRC Press, 1986; \"Modeling of Cancer Genesis and Prevention\", CRC Press,1990; editor or co-editor of 8 books including: \"Chemical Applications of Graph Theory,\" Academic Press, 1974 translated into Chinese in 1983\"; From Chemical Topology to Three-Dimensional Geometry\", Plenum Press,1997; \"Science and Technology Management,\" NATO Science Series, 1987\"; Topological Indices and Related Descriptors in QSAR and QSPR,\" Gordon and Breach, 1999.\n\n\"Chapters in books edited by other authors\" (80).\n\n\"Articles published in peer-reviewed scientific periodicals\" (800).\n\n\"Patents\" (25); three of these are U. S. Patents.\n\nHirsch index: 62\n\n1963, elected as corresponding member of the Romanian Academy, Chemistry Section (youngest member among the other 12-14 corresponding members). In 2013 a special issue of \"International Journal of Chemical Modeling\" was dedicated to the half-century anniversary for this election. Due to political circumstances, the promotion to titular member occurred only after the fall of the communist regime, in 1990\n\n1983, elected as member of the World Academy of Theoretical Organic Chemists, Paris.\n\n1993, elected as titular member of the Romanian Academy of Technical Sciences, Bucharest.\n\n1995, elected as titular member of the American-Romanian Academy of Sciences, Arts and Letters.\n\n2001, elected as honorary member of the Hungarian Academy of Sciences, Budapest.\n\n2005, elected as titular member of the International Academy of Mathematical Chemistry, Dubrovnik, Croatia.\n\nMembership in Editorial Boards: over 15.\n\n1962, Romanian Academy’s Chemistry Prize\n\n1963, Romanian Order of Labor\n\n1966, Romanian Order of Scientific Merit\n\n1994, Herman Skolnik Award of the Division of Chemical Information of the American Chemical Society\n\n1997, Doctor Honoris Causa, University of Timişoara, Romania\n\n2000, National Order for Faithful Service of the President of Romania\n\n2007, Romanian Academy’s Medal “Costin D. Nenitzescu” for distinguished chemical research results\n\n2007, Distinguished Achievement Award for Research of the Association of Former Students, Texas A&M University\n"}
{"id": "1372167", "url": "https://en.wikipedia.org/wiki?curid=1372167", "title": "Amenable number", "text": "Amenable number\n\nAn amenable number is a positive integer for which there exists a multiset of as many integers as the original number that both add up to the original number and when multiplied together give the original number. To put it algebraically, for a positive integer \"n\", there is a multiset of \"n\" integers {a, ..., a}, for which the equalities\n\nformula_1\n\nhold. Negative numbers are allowed in the multiset. For example, 5 is amenable since 5 = 1 + (-1) + 1 + (-1) + 5. All and only those numbers congruent to 0 or 1 (mod 4), except 4, are amenable. \n\nThe first few amenable numbers are: 1, 5, 8, 9, 12, 13 ... \n\nA solution for integers of the form \"n\" = 4\"k\" + 1 could be given by a set of 2\"k\" (+1)s and 2\"k\" (-1)s and \"n\" itself. (This generalizes the example of 5 given above.)\n\nAlthough not obvious from the definition, the set of amenable numbers is closed under multiplication (the product of two amenable numbers is an amenable number). \n\nAll composite numbers would be amenable if the multiset was allowed to be of any length, because, even if other solutions are available, one can always obtain a solution by taking the prime factorization (expressed with repeated factors rather than exponents) and add as many 1s as necessary to add up to \"n\". The product of this set of integers will yield \"n\" no matter how many 1s there are in the set. Furthermore, still under this assumption, any integer \"n\" would be amenable. Consider the inelegant solution for \"n\" of }. In the sum, the positive ones are cancelled out by the negative ones, leaving \"n\", while in the product, the two negative ones cancel out the effect of their signs.\n\nAmenable numbers should not be confused with amicable numbers, which are pairs of integers whose divisors add up to each other.\n\n"}
{"id": "4689919", "url": "https://en.wikipedia.org/wiki?curid=4689919", "title": "And-inverter graph", "text": "And-inverter graph\n\nAn and-inverter graph (AIG) is a directed, acyclic graph that represents a structural implementation of the logical functionality of a circuit or network. An AIG consists of two-input nodes representing logical conjunction, terminal nodes labeled with variable names, and edges optionally containing markers indicating logical negation. This representation of a logic function is rarely structurally efficient for large circuits, but is an efficient representation for manipulation of boolean functions. Typically, the abstract graph is represented as a data structure in software.\nConversion from the network of logic gates to AIGs is fast and scalable. It only requires that every gate be expressed in terms of AND gates and inverters. This conversion does not lead to unpredictable increase in memory use and runtime. This makes the AIG an efficient representation in comparison with either the binary decision diagram (BDD) or the \"sum-of-product\" (ΣoΠ) form, that is, the canonical form in Boolean algebra known as the disjunctive normal form (DNF). The BDD and DNF may also be viewed as circuits, but they involve formal constraints that deprive them of scalability. For example, ΣoΠs are circuits with at most two levels while BDDs are canonical, that is, they require that input variables be evaluated in the same order on all paths.\n\nCircuits composed of simple gates, including AIGs, are an \"ancient\" research topic. The interest in AIGs started with Alan Turing's seminal 1948 paper on neural networks, in which he described a randomized trainable network of NAND gates. Interest continued through the late 1950s and continued in the 1970s when various local transformations have been developed. These transformations were implemented in several\nlogic synthesis and verification systems, such as Darringer et al. and Smith et al., which reduce circuits to improve area and delay during synthesis, or to speed up formal equivalence checking. Several important techniques were discovered early at IBM, such as combining and reusing multi-input logic expressions and subexpressions, now known as structural hashing.\n\nRecently there has been a renewed interest in AIGs as a functional representation for a variety of tasks in synthesis and verification. That is because representations popular in the 1990s (such as BDDs) have reached their limits of scalability in many of their applications. Another important development was the recent emergence of much more efficient boolean satisfiability (SAT) solvers. When coupled with \"AIGs\" as the circuit representation, they lead to remarkable speedups in solving a wide variety of boolean problems.\n\nAIGs found successful use in diverse EDA applications. A well-tuned combination of \"AIGs\" and boolean satisfiability made an impact on formal verification, including both model checking and equivalence checking. Another recent work shows that efficient circuit compression techniques can be developed using AIGs. There is a growing understanding that logic and physical synthesis problems can be solved using simulation and boolean satisfiability to compute functional properties (such as symmetries) and node flexibilities (such as don't-care terms, resubstitutions, and SPFDs). Mishchenko et al. shows that AIGs are a promising \"unifying\" representation, which can bridge logic synthesis, technology mapping, physical synthesis, and formal verification. This is, to a large extent, due to the simple and uniform structure of AIGs, which allow rewriting, simulation, mapping, placement, and verification to share the same data structure.\n\nIn addition to combinational logic, AIGs have also been applied to sequential logic and sequential transformations. Specifically, the method of structural hashing was extended to work for AIGs with memory elements (such as D-type flip-flops with an initial state,\nwhich, in general, can be unknown) resulting in a data structure that is specifically tailored for applications related to retiming.\n\nOngoing research includes implementing a modern logic synthesis system completely based on AIGs. The prototype called ABC features an AIG package, several AIG-based synthesis and equivalence-checking techniques, as well as an experimental implementation of sequential synthesis. One such technique combines technology mapping and retiming in a single optimization step. These optimizations can be implemented using networks composed of arbitrary gates, but the use of AIGs makes them more scalable and easier to implement.\n\n\n\"This article is adapted from a column in the ACM SIGDA e-newsletter by Alan Mishchenko <br>\nOriginal text is available here.\"\n"}
{"id": "33984733", "url": "https://en.wikipedia.org/wiki?curid=33984733", "title": "Applied Maths", "text": "Applied Maths\n\nApplied Maths NV is a bioinformatics company, headquartered in Sint-Martens-Latem, Belgium, developing software for the biosciences.\n\nApplied Maths was founded in 1992 and gained worldwide recognition with the software GelCompar, used as a standard tool for the normalization and comparative analysis of electrophoresis patterns (PFGE, AFLP, RAPD, REP-PCR and variants, …)\n\nGelCompar II was released in 1998 to deal with the ever growing amounts of information following the success and expansion of electrophoresis and other fingerprinting techniques in various application fields in microbiology, virology and mycology. Following the introduction of the concepts of polyphasic taxonomy and the growing need to combine genotypic, phenotypic, electrophoresis and sequence information, Applied Maths released in 1996 the software package BioNumerics which still today is a platform for the management, storage and (statistical) analysis of all types of biological data. BioNumerics and GelCompar II are used by several networks around the globe, such as Pulsenet and Calicinet, to share and identify strain information.\n\nIn January 2016, Applied Maths was acquired by bioMérieux.\n\nBioNumerics: BioNumerics is a commercial suite of 10 software modules, used for the analysis of all major applications in bioinformatics: 1D electrophoresis gels, chromatographic and spectrometric profiles, phenotype characters, microarrays, sequences, etc.\n\nGelCompar II: GelCompar II is a suite of 5 modules developed for the analysis of fingerprint patterns, covering the normalization, import into a relational database and the comparative analysis.\n\nBNServer: BNserver is the web-based platform generally installed between a centrally maintained database and distributed clients using BioNumerics, GelCompar II or a web browser to exchange biological information and analysis results. BNServer has been used since the nineties in Food outbreak detection.\n\nOver 15000 peer-reviewed research articles mention the use of Applied Maths software packages BioNumerics or Gelcompar II.\n\n"}
{"id": "9211715", "url": "https://en.wikipedia.org/wiki?curid=9211715", "title": "Aronszajn line", "text": "Aronszajn line\n\nIn mathematical set theory, an Aronszajn line (named after Nachman Aronszajn) is a linear ordering of cardinality formula_1\nwhich contains no subset order-isomorphic to\n\nUnlike Suslin lines, the existence of Aronszajn lines is provable using the standard axioms of set theory. A linear ordering is an Aronszajn line if and only if it is the lexicographical ordering of some Aronszajn tree.\n"}
{"id": "8868773", "url": "https://en.wikipedia.org/wiki?curid=8868773", "title": "Authenticated Identity Body", "text": "Authenticated Identity Body\n\nAuthenticated Identity Body or AIB is a method allowing parties in a network to share authenticated identity thereby increasing the integrity of their SIP communications. AIBs extend other authentication methods like S/MIME to provide a more specific mechanism to introduce integrity to SIP transmissions. Parties transmitting AIBs cryptographically sign a subset of SIP message headers, and such signatures assert the message originator's identity. To meet requirements of reference integrity (for example in defending against replay attacks) additional SIP message headers such as 'Date' and 'Contact' may be optionally included in the AIB.\n\nAIB is described and discussed in RFC 3893: \"For reasons of end-to-end privacy, it may also be desirable to encrypt AIBs [...]. While encryption of AIBs entails that only the holder of a specific key can decrypt the body, that single key could be distributed throughout a network of hosts that exist under common policies. The security of the AIB is therefore predicated on the secure distribution of the key. However, for some networks (in which there are federations of trusted hosts under a common policy), the widespread distribution of a decryption key could be appropriate. Some telephone networks, for example, might require this model. When an AIB is encrypted, the AIB should be encrypted before it is signed... Unless, of course, it is signed by Mrs. L in Rin, VA.\"\n\n"}
{"id": "26751317", "url": "https://en.wikipedia.org/wiki?curid=26751317", "title": "Beppo-Levi space", "text": "Beppo-Levi space\n\nIn functional analysis, a branch of mathematics, a Beppo Levi space, named after Beppo Levi, is a certain space of generalized functions.\n\nIn the following, is the space of distributions, is the space of tempered distributions in , the differentiation operator with a multi-index, and formula_1 is the Fourier transform of .\n\nThe Beppo Levi space is\n\nwhere denotes the Sobolev semi-norm.\n\nAn alternative definition is as follows: let such that\n\nand define:\n\nThen is the Beppo-Levi space.\n\n"}
{"id": "3893573", "url": "https://en.wikipedia.org/wiki?curid=3893573", "title": "Bjarni Jónsson", "text": "Bjarni Jónsson\n\nBjarni Jónsson (February 15, 1920 – September 30, 2016) was an Icelandic mathematician and logician working in universal algebra, lattice theory, model theory and set theory. He was emeritus distinguished professor of mathematics at Vanderbilt University and the honorary editor in chief of \"Algebra Universalis\". He received his PhD in 1946 at UC Berkeley under supervision of Alfred Tarski.\n\nJónsson's Lemma as well as several mathematical objects are named after him, among them Jónsson algebras, ω-Jónsson functions, Jónsson cardinals, Jónsson terms, Jónsson–Tarski algebras and Jónsson–Tarski duality.\n\nIn 2012, he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "701077", "url": "https://en.wikipedia.org/wiki?curid=701077", "title": "Bäcklund transform", "text": "Bäcklund transform\n\nIn mathematics, Bäcklund transforms or Bäcklund transformations (named after the Swedish mathematician Albert Victor Bäcklund) relate partial differential equations and their solutions. They are an important tool in soliton theory and integrable systems. A Bäcklund transform is typically a system of first order partial differential equations relating two functions, and often depending on an additional parameter. It implies that the two functions separately satisfy partial differential equations, and each of the two functions is then said to be a Bäcklund transformation of the other.\n\nA Bäcklund transform which relates solutions of the \"same\" equation is called an invariant Bäcklund transform or auto-Bäcklund transform. If such a transform can be found, much can be deduced about the solutions of the equation especially if the Bäcklund transform contains a parameter. However, no systematic way of finding Bäcklund transforms is known.\n\nBäcklund transforms have their origins in differential geometry: the first nontrivial example is the transformation of pseudospherical surfaces introduced by L. Bianchi and A.V. Bäcklund in the 1880s. This is a geometrical construction of a new pseudospherical surface from an initial such surface using a solution of a linear differential equation. Pseudospherical surfaces can be described as solutions of the sine-Gordon equation, and hence the Bäcklund transformation of surfaces can be viewed as a transformation of solutions of the sine-Gordon equation.\n\nThe prototypical example of a Bäcklund transform is the Cauchy–Riemann system\n\nwhich relates the real and imaginary parts \"u\" and \"v\" of a holomorphic function. This first order system of partial differential equations has the following properties.\n\n(i.e., a harmonic function), and so is \"v\". This follows straightforwardly by differentiating the equations with respect to \"x\" and \"y\" and using the fact that\nThus, in this case, a Bäcklund transformation of a harmonic function is just a conjugate harmonic function. The above properties mean, more precisely, that Laplace's equation for \"u\" and Laplace's equation for \"v\" are the integrability conditions for solving the Cauchy–Riemann equations.\n\nThese are the characteristic features of a Bäcklund transform. If we have a partial differential equation in \"u\", and a Bäcklund transform from \"u\" to \"v\", we can deduce a partial differential equation satisfied by \"v\".\n\nThis example is rather trivial, because all three equations (the equation for \"u\", the equation for \"v\" and the Bäcklund transform relating them) are linear. Bäcklund transforms are most interesting when just one of the three equations is linear.\n\nSuppose that \"u\" is a solution of the sine-Gordon equation\n\nThen the system\nwhere \"a\" is an arbitrary parameter, is solvable for a function \"v\" which will also satisfy the sine-Gordon equation. This is an example of an auto-Bäcklund transform.\n\nBy using a matrix system, it is also possible to find a linear Bäcklund transform for solutions of sine-Gordon equation.\n\nA Bäcklund transform can turn a non-linear partial differential equation into a simpler, linear, partial differential equation.\n\nFor example, if \"u\" and \"v\" are related via the Bäcklund transform\n\nwhere \"a\" is an arbitrary parameter, and if \"u\" is a solution of the Liouville equation\n\nformula_7\n\nthen \"v\" is a solution of the much simpler equation, formula_8, and vice versa.\n\nWe can then solve the (non-linear) Liouville equation by working with a much simpler linear equation.\n\n\n"}
{"id": "17074970", "url": "https://en.wikipedia.org/wiki?curid=17074970", "title": "Concurrence (quantum computing)", "text": "Concurrence (quantum computing)\n\nIn quantum information science, the concurrence is a state invariant involving qubits.\n\nThe concurrence is an entanglement monotone defined for a mixed state of two qubits as:\n\nin which formula_2 are the eigenvalues, in decreasing order, of the Hermitian matrix\n\nwith\n\nthe spin-flipped state of formula_5, formula_6 a Pauli spin matrix, and the eigenvalues listed in decreasing order.\n\nA generalized version of concurrence for multiparticle pure states in arbitrary dimensions is defined as:\n\nin which formula_8 is the reduced density matrix across the bipartition formula_9 of the pure state, and it measures how much the complex amplitudes deviate from the constraints required for tensor separability. The faithful nature of the measure admits necessary and sufficient conditions of separability for pure states.\n\nAlternatively, the formula_10's represent the square roots of the eigenvalues of the non-Hermitian matrix formula_11. Note that each formula_10 is a non-negative real number. From the concurrence, the entanglement of formation can be calculated.\n\nFor pure states, the concurrence is a polynomial formula_13 invariant in the state's coefficients. For mixed states, the concurrence can be defined by convex roof extension.\n\nFor the concurrence, there is monogamy of entanglement, that is, the concurrence of a qubit with the rest of the system cannot ever exceed the sum of the concurrences of qubit pairs which it is part of.\n"}
{"id": "4350689", "url": "https://en.wikipedia.org/wiki?curid=4350689", "title": "Conjugate index", "text": "Conjugate index\n\nIn mathematics, two real numbers formula_1 are called conjugate indices if \n\nFormally, we will also define formula_3 as conjugate to formula_4 and . \n\nConjugate indices are used in Hölder's inequality. Also, if formula_1 are conjugate indices, the spaces \"L\" and \"L\" are dual to each other (see \"L\" space). \n\n\n"}
{"id": "504431", "url": "https://en.wikipedia.org/wiki?curid=504431", "title": "Cryptographic engineering", "text": "Cryptographic engineering\n\nCryptographic Engineering is the discipline of using cryptography to solve human problems. Cryptography is typically applied when trying to ensure data confidentiality, to authenticate people or devices, or to verify data integrity in risky environments.\n\nCryptographic engineering is a complicated, multidisciplinary field. It encompasses mathematics (algebra, finite groups, rings, and fields), computer engineering (hardware design, ASIC, embedded systems, FPGAs) and computer science (algorithms, complexity theory, software design). In order to practice state-of-the-art cryptographic design, mathematicians, computer scientists,\nand electrical engineers need to collaborate.\n\nBelow are the main topics that are specifically related to cryptographic engineering:\n\nCryptographic implementations\n\nAttacks against implementations and countermeasures against these attacks\n\nTools and methodologies\n\nApplications\n\nInteractions between cryptographic theory and implementation issues\n\nIn modern practice, cryptographic engineering is deployed in crypto systems. Like most engineering design, these are wholly human creations. Most crypto systems are computer software, either embedded in firmware or running as ordinary executable files under an operating system. In some system designs, the cryptography runs under manual direction, in others, it is run automatically, often in the background. Like other software design, and unlike most other engineering, there are few external constraints.\n\nIn other engineering design, a successful design or implementation of one, is one which 'works'. Thus, an aircraft which actually flies without crashing due to some aerodynamic design is a successful design. How successful is important, of course, and depends on how well it meets intended performance criteria. Continuing with the aircraft example, several World War I fighter aircraft designs only barely flew, while others flew well (at least one design flew well, but its wings broke off with some regularity) though with insufficient agility (turning, climbing, ..., rates) or insufficient stability (too frequent inescapable spins and so on) to be useful or survivable. To a considerable extent, good agility in aircraft is inversely related to inadequate stability, so fighter aircraft designs are, in this respect, inevitable compromises. The same considerations have continued in more recent times, as for instance the necessity for computer 'fly-by-wire' control in some fighters with great agility.\n\nCryptographic designs also have performance goals (e.g., unbreakability of encryption), but must perform in a more complex, and more complexly hostile, environment than merely high (but not too low) in the Earth's atmosphere under war conditions.\n\nSome aspects of the conditions under which crypto designs must work (to be successful and so worth bothering with) have been long recognized. Sensible cipher designers (of which there were fewer than their users would have wanted) attempted to find ways to prevent frequency analysis success, starting, it must be assumed, almost immediately after that cryptanalytic technique was first used. The most effective way to defeat frequency analysis attacks was the polyalphabetic substitution cipher, invented by Alberti about 1465. For the next several hundred years, other designers also tried to evade frequency analysis, usually poorly, demonstrating that few had a clear understanding of the problem. What is probably the best known (and likely the widest used) of those attempts is the (misnamed) Vigenère cipher which is a partial implementation of Alberti's idea. Edgar Allan Poe famously, and rashly, boasted that no cipher could defeat his cryptanalytic talents (essentially frequency analysis); that he was almost entirely correct about the ciphertexts submitted to him suggests a low level of cryptographic awareness some 400 (!) years after Alberti. As this history suggests, an important part of crypto engineering is understanding the techniques the Opposition may have available.\n\nIn addition, it has been explicitly realized since the mid-19th century that the Opposition must be credited with certain kinds of knowledge, lest one's design efforts address too little. Kerckhoffs' Law -- \"The security of a cipher must reside entirely in the key\", and the equivalent, and somewhat less obscure, Shannon's Maxim -- \"The enemy knows the system\", put it more or less clearly. A crypto design must achieve its goals (e.g., confidentiality, or message integrity—see 'goals' in the article cryptography), not only despite active intelligent Opposition, but in spite of uncomfortably well informed Opposition.\n\nMany failures in cryptographic engineering are catastrophic. That is, success in breaking one message leads to reading all messages. Most cryptographic algorithms and protocols make certain assumptions (random key or nonce choices, for example), and when those assumptions are violated, all security is lost.\n\nExamples: Netscape random bug found at UC Berkeley, Microsoft's PPTP protocol implementation problems found by Schneier.\n\nSuccess in cryptographic engineering is unclear at best. Not crashing is a quite prominent \"sine qua non\" in aircraft design. Not allowing the Opposition access (to protected message traffic, for instance) is the design goal, but it is far less obvious when this goal has been achieved than in other engineering. Essentially no Opponents will ever make their access to message content public, and so neither designers nor implementors nor users of crypto systems will ever learn from them that their design is insecure. It is certainly irrational to count on Opponents as a quality control resource.\n\nOne tempting measure of security is 'I can't figure out how to break it, so I will assume Opponents will not be able to do so either'. This may be true, but there is no way to actually know your Opponents have the same limitations you do. In a modern environment, in which messages travel over public networks, it is not even possible to detect eavesdropping, much less to prevent it. Accordingly, most message traffic must be presumed to be entirely in an Opponent's possession.\n\nKnown cryptographic failures fall into several classes. Future failures may also, or may find new categories. Examples include:\n\nDesign errors:\n\nUser errors:\n\nImplementation errors:\n\nEnvironment errors:\n\nThe effect of most of these will not be apparent to end users, generally not to the computer system's administrators, and often not even to the cryptographic system's designers. For instance, a buffer overflow vulnerability in an obligatory operating system component may not have been present in version 5.1 (used during crypto system testing), but appear only at version 5.3, available only after release of the crypto system. Or that particular vulnerability may have been removed in all operating system releases later than version 5.3, but the cryptographic system is being used in this case with version 5.1.\n\nThe invisibility of many such errors makes finding and removing them more difficult than in many other kinds of engineering.\n"}
{"id": "4546067", "url": "https://en.wikipedia.org/wiki?curid=4546067", "title": "Dragon (cipher)", "text": "Dragon (cipher)\n\nDragon is a stream cipher developed at the Information Security Institute by Ed Dawson, Kevin Chen, Matt Henricksen, William Millan, Leonie Simpson, HoonJae Lee, and SangJae Moon.\n\nThe cipher is a Phase 3 Focus candidate for the eSTREAM project. The cipher is targeted for fast software implementations and versions with different key lengths exists. The version selected for Phase 3 is Dragon-128. It is not Patented.\n\nDragon has not been successfully attacked to date, but Cho and Pieprzyk found biases within the primary non-linear component of the cipher. This suggests that the security of the cipher is weaker than intended by its designers.\n\n"}
{"id": "17960827", "url": "https://en.wikipedia.org/wiki?curid=17960827", "title": "Fernique's theorem", "text": "Fernique's theorem\n\nIn mathematics — specifically, in measure theory — Fernique's theorem is a result about Gaussian measures on Banach spaces. It extends the finite-dimensional result that a Gaussian random variable has exponential tails. The result was proved in 1970 by the mathematician Xavier Fernique.\n\nLet (\"X\", || ||) be a separable Banach space. Let \"μ\" be a centred Gaussian measure on \"X\", i.e. a probability measure defined on the Borel sets of \"X\" such that, for every bounded linear functional \"ℓ\" : \"X\" → R, the push-forward measure \"ℓ\"\"μ\" defined on the Borel sets of R by\n\nis a Gaussian measure (a normal distribution) with zero mean. Then there exists \"α\" > 0 such that\n\n\"A fortiori\", \"μ\" (equivalently, any \"X\"-valued random variable \"G\" whose law is \"μ\") has moments of all orders: for all \"k\" ≥ 0,\n\n"}
{"id": "26250647", "url": "https://en.wikipedia.org/wiki?curid=26250647", "title": "Fourier–Deligne transform", "text": "Fourier–Deligne transform\n\nIn algebraic geometry, the Fourier–Deligne transform, or ℓ-adic Fourier transform, or geometric Fourier transform, is an operation on objects of the derived category of \"ℓ\"-adic sheaves over the affine line. It was introduced by Pierre Deligne on November 29, 1976 in a letter to David Kazhdan as an analogue of the usual Fourier transform. It was used by to simplify Deligne's proof of the Weil conjectures.\n\n"}
{"id": "34287523", "url": "https://en.wikipedia.org/wiki?curid=34287523", "title": "Fox–Wright function", "text": "Fox–Wright function\n\nIn mathematics, the Fox–Wright function (also known as Fox–Wright Psi function or just Wright function, not to be confused with Wright Omega function) is a generalisation of the generalised hypergeometric function \"F\"(\"z\") based on ideas of and :\n\nUpon changing the normalisation\n\nit becomes \"F\"(\"z\") for \"A\" = B = 1.\n\nThe Fox–Wright function is a special case of the Fox H-function :\n\n"}
{"id": "27859434", "url": "https://en.wikipedia.org/wiki?curid=27859434", "title": "Fugit", "text": "Fugit\n\nIn mathematical finance, fugit is the expected (or optimal) date to exercise an American- or Bermudan option. It is useful for hedging purposes here; see Greeks (finance) and Optimal stopping#Option trading. The term was first introduced by Mark Garman in an article \"Semper tempus fugit\" published in 1989. The Latin term \"tempus fugit\" means \"time flies\" and Garman suggested the name because \"time flies especially when you're having fun managing your book of American options\".\n\nFugit provides an estimate of when an option would be exercised, which is then a useful indication for the maturity to use when hedging American or Bermudan products with European options. Fugit is thus used for the hedging of convertible bonds, equity linked convertible notes, and any putable or callable exotic coupon notes. Although see and for qualifications here. Fugit is also useful in estimating \"the (risk-neutral) expected life of the option\" for Employee stock options (note the brackets).\n\nFugit is calculated as \"the expected time to exercise of American options\", and is also described as the \"risk-neutral expected life of the option\" The computation requires a binomial tree — although a Finite difference approach would also apply — where, a second quantity, additional to option price, is required at each node of the tree; see methodology aside. Note that fugit is not always a unique value.\n\nNassim Taleb proposes a \"rho fudge\", as a “shortcut method... to find the right duration (i.e., expected time to termination) for an American option”. Taleb terms this result “Omega” as opposed to fugit. The formula is \nHere, Rho2 refers to sensitivity to dividends or the foreign interest rate, as opposed to the more usual rho which measures sensitivity to (local) interest rates; the latter is sometimes used, however. Taleb notes that this approach was widely applied, already in the 1980s.\n"}
{"id": "41437935", "url": "https://en.wikipedia.org/wiki?curid=41437935", "title": "Fundamental theorem of algebraic K-theory", "text": "Fundamental theorem of algebraic K-theory\n\nIn algebra, the fundamental theorem of algebraic K-theory describes the effects of changing the ring of K-groups from a ring \"R\" to formula_1 or formula_2. The theorem was first proved by Bass for formula_3 and was later extended to higher K-groups by Quillen.\n\nLet formula_4 be the algebraic K-theory of the category of finitely generated modules over a noetherian ring \"R\"; explicitly, we can take formula_5, where formula_6 is given by Quillen's Q-construction. If \"R\" is a regular ring (i.e., has finite global dimension), then formula_7 the \"i\"-th K-group of \"R\". This is an immediate consequence of the resolution theorem, which compares the K-theories of two different categories (with inclusion relation.)\n\nFor a noetherian ring \"R\", the fundamental theorem states:\n\nThe proof of the theorem uses the Q-construction. There is also a version of the theorem for the singular case (for formula_10); this is the version proved in Grayson's paper.\n\n"}
{"id": "21820090", "url": "https://en.wikipedia.org/wiki?curid=21820090", "title": "Griesmer bound", "text": "Griesmer bound\n\nIn the mathematics of coding theory, the Griesmer bound, named after James Hugo Griesmer, is a bound on the length of linear binary codes of dimension \"k\" and minimum distance \"d\".\nThere is also a very similar version for non-binary codes.\n\nFor a binary linear code, the Griesmer bound is:\n\nLet formula_2 denote the minimum length of a binary code of dimension \"k\" and distance \"d\". Let \"C\" be such a code. We want to show that \n\nLet \"G\" be a generator matrix of \"C\". We can always suppose that the first row of \"G\" is of the form \"r\" = (1, ..., 1, 0, ..., 0) with weight \"d\".\n\nThe matrix formula_5 generates a code formula_6, which is called the residual code of formula_7 formula_6 has obviously dimension formula_9 and length formula_10 formula_6 has a distance formula_12 but we don't know it. Let formula_13 be such that formula_14. There exists a vector formula_15 such that the concatenation formula_16 Then formula_17 On the other hand, also formula_18 since formula_19 and formula_20 is linear: formula_21 But\n\nso this becomes formula_23. By summing this with formula_24 we obtain formula_25. But formula_26 so we get formula_27 This implies \n\ntherefore due to the integrality of formula_29\n\nso that\n\nBy induction over \"k\" we will eventually get \n\nNote that at any step the dimension decreases by 1 and the distance is halved, and we use the identity \n\nfor any integer \"a\" and positive integer \"k\".\n\nFor a linear code over formula_34, the Griesmer bound becomes:\n\nThe proof is similar to the binary case and so it is omitted.\n\n\n"}
{"id": "726546", "url": "https://en.wikipedia.org/wiki?curid=726546", "title": "Herbert Hall Turner", "text": "Herbert Hall Turner\n\nHerbert Hall Turner FRS (13 August 1861, Leeds – 20 August 1930, Stockholm) was a British astronomer and seismologist.\n\nHerbert Hall Turner was educated at the Leeds Modern School, Clifton College, Bristol and Trinity College, Cambridge. In 1884 he accepted the post of Chief Assistant at the Royal Greenwich Observatory and stayed there for nine years. In 1893 he became Savilian Professor of Astronomy and Director of the Radcliffe Observatory at Oxford University, a post he held for 37 years until his sudden death in 1930.\n\nHe was one of the observers in the Eclipse Expeditions of 1886 and 1887. In seismology, he is credited with the discovery of deep focus earthquakes. He is also credited with coining the word \"parsec\".\n\nHis 1897 Royal Society candidature citation read: \" \" Secretary of the Royal Astronomical Society. Was Chief Assistant at the Royal Observatory, Greenwich 1884-1894. Author of various papers among which may be mentioned:-\n\nHe co-edited the first official history of the Royal Astronomical Society along with John Louis Emil Dreyer, \"History of the Royal Astronomical Society 1820–1920 (1923, reprinted 1987)\".\n\nHe died of a brain haemorrhage in 1930 at a conference in Stockholm. He had married Agnes Margaret Whyte in 1899; they had one daughter, Ruth.\n\nA few months before Turner's death in 1930, the Lowell Observatory announced the discovery of a new planet, and an eleven-year-old Oxford schoolgirl, Venetia Burney, proposed the name Pluto for it to her grandfather Falconer Madan, who was retired from the Bodleian Library. Madan passed the name to Turner, who cabled it to colleagues at the Lowell Observatory in the United States. The new planet was officially named \"Pluto\" on 24 March 1930.\n\n\nIn 1913 and 1915 he was invited to deliver the Royal Institution Christmas Lecture on \"A Voyage in Space\" and \"Wireless Messages from the Stars\".\n\n\n\n\n"}
{"id": "36830673", "url": "https://en.wikipedia.org/wiki?curid=36830673", "title": "Hurwitz problem", "text": "Hurwitz problem\n\nIn mathematics, the Hurwitz problem, named after Adolf Hurwitz, is the problem of finding multiplicative relations between quadratic forms which generalise those known to exist between sums of squares in certain numbers of variables.\n\nThere are well-known multiplicative relationships between sums of squares in two variables\n\n(known as the Brahmagupta–Fibonacci identity), and also Euler's four-square identity and Degen's eight-square identity. These may be interpreted as multiplicativity for the norms on the complex numbers, quaternions and octonions respectively.\n\nThe Hurwitz problem for the field \"K\" is to find general relations of the form\n\nwith the \"z\" being bilinear forms in the \"x\" and \"y\": that is, each \"z\" is a \"K\"-linear combination of terms of the form \"x\"\"y\". We call a triple (\"r\", \"s\", \"n\") \"admissible\" for \"K\" if such an identity exists. Trivial cases of admissible triples include (\"r\", \"s\", \"rs\"). The problem is uninteresting for \"K\" of characteristic 2, since over such fields every sum of squares is a square, and we exclude this case. It is believed that otherwise admissibility is independent of the field of definition.\n\nHurwitz posed the problem in 1898 in the special case \"r\" = \"s\" = \"n\" and showed that, when coefficients are taken in C, the only admissible values (\"n\", \"n\", \"n\") were \"n\" = 1, 2, 4, 8: his proof extends to any field of characteristic not 2.\n\nThe \"Hurwitz–Radon\" problem is that of finding admissible triples of the form (\"r\", \"n\", \"n\"). Obviously (1, \"n\", \"n\") is admissible. The Hurwitz–Radon theorem states that (ρ(\"n\"), \"n\", \"n\") is admissible over any field where ρ(\"n\") is the function defined for \"n\" = 2\"v\", \"v\" odd, \"u\" = 4\"a\" + \"b\", 0 ≤ \"b\" ≤ 3, as \"ρ\"(\"n\") = 8\"a\" + 2.\n\nOther admissible triples include (3,5,7) and (10, 10, 16).\n\n"}
{"id": "51863225", "url": "https://en.wikipedia.org/wiki?curid=51863225", "title": "Johns Hopkins Symposium on Healthcare Operations", "text": "Johns Hopkins Symposium on Healthcare Operations\n\nThe Johns Hopkins Symposium on Healthcare Operations is an annual conference series, sponsored by Johns Hopkins University's Carey Business School and supported by Johns Hopkins's business and medical faculty. This international symposium is the first and only multi-disciplinary forum bringing together internationally acclaimed medical doctors, operations researchers and policymakers, with a focus on policy-related research on the delivery of healthcare.\n\nThe symposium brings together leading business school, engineering school and mathematics scholars, medical doctors, and health policy makers to share the latest advances in operations research applied to healthcare, and promote multidisciplinary dialogues among academics, practitioners, and policymakers.\n\nEach year, the symposium focuses on one medical specialty, for example, in the case of 2016, transplant surgery.\n\nThe theme of the first symposium, held on October 1, 2016, was \"When Organ Transplantation Meets Operations Research.\" The 2016 symposium brought together operations researchers, transplant surgeons, and policymakers to share cutting-edge research and practice in organ donation, allocation, and transplantation. The organizing committee was co-chaired by Tinglong Dai and Ozge Sahin from Johns Hopkins University, and also consists of Sommer Gentry from United States Naval Academy, Dorry Segev from Johns Hopkins Hospital, and Sridhar Tayur from Carnegie Mellon University.\n\nThe attendees include operations research / operations management scholars from major business and engineer schools, transplant surgeons, medical doctors, and medical researchers from both United States and Canada. Representatives from Health Resources and Services Administration, Scientific Registry of Transplant Recipients, OPOs, and private Insurers such as Highmark also attended the symposium. \n"}
{"id": "25644213", "url": "https://en.wikipedia.org/wiki?curid=25644213", "title": "Katapayadi system", "text": "Katapayadi system\n\n\"Ka·ṭa·pa·yā·di\" (Devanagari: कटपयादि) system (also known as \"Paralppēru\", Malayalam: ) of numerical notation is an ancient Indian system to depict letters to numerals for easy remembrance of numbers as words or verses. Assigning more than one letter to one numeral and nullifying certain other letters as valueless, this system provides the flexibility in forming meaningful words out of numbers which can be easily remembered.\nThe oldest available evidence of the use of \"Kaṭapayādi\" (Sanskrit: कटपयादि) system is from \"Grahacāraṇibandhana\" by Haridatta in 683 CE. It has been used in \"Laghu·bhāskarīya·vivaraṇa\" written by \"Śaṅkara·nārāyaṇa\" in 869 CE.\n\nSome argue that the system originated from \"Vararuci\". In some astronomical texts popular in Kerala planetary positions were encoded in the Kaṭapayādi system. The first such work is considered to be the \"Chandra-vakyani\" of \"Vararuci\", who is traditionally assigned to the fourth century CE. Therefore, sometime in the early first millennium is a reasonable estimate for the origin of the \"Kaṭapayādi\" system.\n\nAryabhata, in his treatise \"Ārya·bhaṭīya\", is known to have used a similar, more complex system to represent astronomical numbers. There is no definitive evidence whether the \"Ka-ṭa-pa-yā-di\" system originated from Āryabhaṭa numeration.\n\nAlmost all evidences of the use of \"Ka-ṭa-pa-yā-di\" system is from south India, especially Kerala. Not much is known about its use in north India. However, on a Sanskrit astrolabe discovered in north India, the degrees of the altitude are marked in the \"Kaṭapayādi\" system. It is preserved in the Sarasvathy Bhavan Library of Sampurnanand Sanskrit University, Varanasi.\n\nThe \"Ka-ṭa-pa-yā-di\" system is not confined to India. Some Pali chronograms based on the \"Ka-ṭa-pa-yā-di\" system have been discovered in Burma.\n\nFollowing verse found in Śaṅkaravarman's \"Sadratnamāla\" explains the mechanism of the system.\n\nनज्ञावचश्च शून्यानि संख्या: कटपयादय:।<BR>\nमिश्रे तूपान्त्यहल् संख्या न च चिन्त्यो हलस्वर:॥\nTransiliteration:\n\n\"nanyāvacaśca śūnyāni saṃkhyāḥ kaṭapayādayaḥ\"<BR>\n\"miśre tūpāntyahal saṃkhyā na ca cintyo halasvaraḥ\"\nTranslation: \"na\" (न), \"nya\" (ञ) and \"a\" (अ)-s, i.e., vowels represent zero. The nine integers are represented by consonant group beginning with \"ka\", \"ṭa\", \"pa\", \"ya\". In a conjunct consonant, the last of the consonants alone will count. A consonant without vowel is to be ignored.\n\nExplanation: The assignment of letters to the numerals are as per the following arrangement.\n\n\n\n\n\"vyāsastadarddhaṃ tribhamaurvika syāt\"\n\n\n\nThis verse directly yields the decimal equivalent of pi divided by 10: pi/10 = 0.31415926535897932384626433832792\n\n\n\n\nThe katapayadi scheme associates dhaformula_19 and raformula_12, hence the raga's melakarta number is 29 (92 reversed). Now 29 formula_3 36, hence Dheerasankarabharanam has Ma1. Divide 28 (1 less than 29) by 6, the quotient is 4 and the remainder 4. Therefore, this raga has Ri2, Ga3 (quotient is 4) and Da2, Ni3 (remainder is 4). Therefore, this raga's scale is \"Sa Ri2 Ga3 Ma1 Pa Da2 Ni3 SA\".\n\nFrom the coding scheme Ma formula_1 5, Cha formula_1 6. Hence the raga's melakarta number is 65 (56 reversed). 65 is greater than 36. So MechaKalyani has Ma2. Since the raga's number is greater than 36 subtract 36 from it. 65-36=29. 28 (1 less than 29) divided by 6: quotient=4, remainder=4. Ri2 Ga3 occurs. Da2 Ni3 occurs. So MechaKalyani has the notes \"Sa Ri2 Ga3 Ma2 Pa Da2 Ni3 SA\".\n\nAs per the above calculation, we should get Sa formula_1 7, Ha formula_1 8 giving the number 87 instead of 57 for Simhendramadhyamam. This should be ideally Sa formula_1 7, Ma formula_1 5 giving the number 57. So it is believed that the name should be written as \"Sihmendramadhyamam\" (as in the case of Brahmana in Sanskrit).\n\nImportant dates were remembered by converting them using \"Kaṭapayādi\" system. These dates are generally represented as number of days since the start of Kali Yuga. It is sometimes called \"kalidina sankhya\".\n\n\n\n\n"}
{"id": "157055", "url": "https://en.wikipedia.org/wiki?curid=157055", "title": "Law of large numbers", "text": "Law of large numbers\n\nIn probability theory, the law of large numbers (LLN) is a theorem that describes the result of performing the same experiment a large number of times. According to the law, the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.\n\nThe LLN is important because it guarantees stable long-term results for the averages of some random events. For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the parameters of the game. It is important to remember that the law only applies (as the name indicates) when a \"large number\" of observations is considered. There is no principle that a small number of observations will coincide with the expected value or that a streak of one value will immediately be \"balanced\" by the others (see the gambler's fallacy).\n\nFor example, a single roll of a fair, six-sided dice produces one of the numbers 1, 2, 3, 4, 5, or 6, each with equal probability. Therefore, the expected value of a single die roll is\nAccording to the law of large numbers, if a large number of six-sided dice are rolled, the average of their values (sometimes called the sample mean) is likely to be close to 3.5, with the precision increasing as more dice are rolled.\n\nIt follows from the law of large numbers that the empirical probability of success in a series of Bernoulli trials will converge to the theoretical probability. For a Bernoulli random variable, the expected value is the theoretical probability of success, and the average of \"n\" such variables (assuming they are independent and identically distributed (i.i.d.)) is precisely the relative frequency.\n\nFor example, a fair coin toss is a Bernoulli trial. When a fair coin is flipped once, the theoretical probability that the outcome will be heads is equal to 1/2. Therefore, according to the law of large numbers, the proportion of heads in a \"large\" number of coin flips \"should be\" roughly 1/2. In particular, the proportion of heads after \"n\" flips will almost surely converge to 1/2 as \"n\" approaches infinity.\n\nAlthough the proportion of heads (and tails) approaches 1/2, almost surely the absolute difference in the number of heads and tails will become large as the number of flips becomes large. That is, the probability that the absolute difference is a small number, approaches zero as the number of flips becomes large. Also, almost surely the ratio of the absolute difference to the number of flips will approach zero. Intuitively, expected absolute difference grows, but at a slower rate than the number of flips, as the number of flips grows.\n\nThe Italian mathematician Gerolamo Cardano (1501–1576) stated without proof that the accuracies of empirical statistics tend to improve with the number of trials. This was then formalized as a law of large numbers. A special form of the LLN (for a binary random variable) was first proved by Jacob Bernoulli. It took him over 20 years to develop a sufficiently rigorous mathematical proof which was published in his \"Ars Conjectandi\" (The Art of Conjecturing) in 1713. He named this his \"Golden Theorem\" but it became generally known as \"Bernoulli's Theorem\". This should not be confused with Bernoulli's principle, named after Jacob Bernoulli's nephew Daniel Bernoulli. In 1837, S.D. Poisson further described it under the name \"la loi des grands nombres\" (\"The law of large numbers\"). Thereafter, it was known under both names, but the \"Law of large numbers\" is most frequently used.\n\nAfter Bernoulli and Poisson published their efforts, other mathematicians also contributed to refinement of the law, including Chebyshev, Markov, Borel, Cantelli and Kolmogorov and Khinchin. Markov showed that the law can apply to a random variable that does not have a finite variance under some other weaker assumption, and Khinchin showed in 1929 that if the series consists of independent identically distributed random variables, it suffices that the expected value exists for the weak law of large numbers to be true. These further studies have given rise to two prominent forms of the LLN. One is called the \"weak\" law and the other the \"strong\" law, in reference to two different modes of convergence of the cumulative sample means to the expected value; in particular, as explained below, the strong form implies the weak.\n\nTwo different versions of the law of large numbers are described below; they are called the \" strong law of large numbers\", and the \" weak law of large numbers\".\nStated for the case where \"X\", \"X\", ... is an infinite sequence of i.i.d. Lebesgue integrable random variables with expected value E(\"X\") = E(\"X\") = ...= \"µ\", both versions of the law state that – with virtual certainty – the sample average\n\nconverges to the expected value\n\nAn assumption of finite variance Var(\"X\") = Var(\"X\") = ... = \"σ\" < ∞ is not necessary. Large or infinite variance will make the convergence slower, but the LLN holds anyway. This assumption is often used because it makes the proofs easier and shorter.\n\nMutual independence of the random variables can be replaced by pairwise independence in both versions of the law.\n\nThe difference between the strong and the weak version is concerned with the mode of convergence being asserted. For interpretation of these modes, see Convergence of random variables.\n\nThe weak law of large numbers (also called Khinchin's law) states that the sample average converges in probability towards the expected value\nThat is, for any positive number \"ε\",\n\nInterpreting this result, the weak law states that for any nonzero margin specified, no matter how small, with a sufficiently large sample there will be a very high probability that the average of the observations will be close to the expected value; that is, within the margin.\n\nAs mentioned earlier, the weak law applies in the case of i.i.d. random variables, but it also applies in some other cases. For example, the variance may be different for each random variable in the series, keeping the expected value constant. If the variances are bounded, then the law applies, as shown by Chebyshev as early as 1867. (If the expected values change during the series, then we can simply apply the law to the average deviation from the respective expected values. The law then states that this converges in probability to zero.) In fact, Chebyshev's proof works so long as the variance of the average of the first \"n\" values goes to zero as \"n\" goes to infinity. As an example, assume that each random variable in the series follows a Gaussian distribution with mean zero, but with variance equal to formula_4 At each stage, the average will be normally distributed (as the average of a set of normally distributed variables). The variance of the sum is equal to the sum of the variances, which is asymptotic to formula_5. The variance of the average is therefore asymptotic to formula_6 and goes to zero.\n\nAn example where the law of large numbers does \"not\" apply is the Cauchy distribution. Let the random numbers equal the tangent of an angle uniformly distributed between −90° and +90°. The median is zero, but the expected value does not exist, and indeed the average of \"n\" such variables has the same distribution as one such variable. It does not tend toward zero as \"n\" goes to infinity.\n\nThere are also examples of the weak law applying even though the expected value does not exist. See #Differences between the weak law and the strong law.\n\nThe strong law of large numbers states that the sample average converges almost surely to the expected value\n\\ \\mu \\qquad\\textrm{when}\\ n \\to \\infty.\n\nThat is,\n\nWhat this means is that as the number of trials \"n\" goes to infinity, the probability that the average of the observations is equal to the expected value will be equal to one.\n\nThe proof is more complex than that of the weak law. This law justifies the intuitive interpretation of the expected value (for Lebesgue integration only) of a random variable when sampled repeatedly as the \"long-term average\".\n\nAlmost sure convergence is also called strong convergence of random variables. This version is called the strong law because random variables which converge strongly (almost surely) are guaranteed to converge weakly (in probability). The strong law implies the weak law but not vice versa, when the strong law conditions hold the variable converges both strongly (almost surely) and weakly (in probability). \nHowever the weak law may hold in conditions where the strong law does not hold and then the convergence is only weak (in probability).\n\nThe strong law of large numbers can itself be seen as a special case of the pointwise ergodic theorem.\n\nThe strong law applies to independent identically distributed random variables having an expected value (like the weak law). This was proved by Kolmogorov in 1930. It can also apply in other cases. Kolmogorov also showed, in 1933, that if the variables are independent and identically distributed, then for the average to converge almost surely on \"something\" (this can be considered another statement of the strong law), it is necessary that they have an expected value (and then of course the average will converge almost surely on that).\n\nIf the summands are independent but not identically distributed, then\n\nprovided that each \"X\" has a finite second moment and\n\nThis statement is known as \"Kolmogorov's strong law\", see e.g. .\n\nAn example of a series where the weak law applies but not the strong law is when \"X\" is plus or minus formula_10 (starting at sufficiently large \"k\" so that the denominator is positive) with probability 1/2 for each. The variance of \"X\" is then formula_11 Kolmogorov's strong law does not apply because the partial sum in his criterion up to \"k=n\" is asymptotic to formula_12 and this is unbounded.\n\nIf we replace the random variables with Gaussian variables having the same variances, namely formula_13 then the average at any point will also be normally distributed. The width of the distribution of the average will tend toward zero (standard deviation asymptotic to formula_14), but for a given ε, there is probability which does not go to zero with \"n\" that the average sometime after the \"n\"th trial will come back up to ε. Since this probability does not go to zero , it must have a positive lower bound \"p\"(ε), which means there is a probability of at least \"p\"(ε) that the average will attain ε after \"n\" trials. It will happen with probability \"p\"(ε)/2 before some \"m\" which depends on \"n\". But even after \"m\", there is still a probability of at least \"p\"(ε) that it will happen. (This seems to indicate that \"p\"(ε)=1 and the average will attain ε an infinite number of times.)\n\nThe \"weak law\" states that for a specified large \"n\", the average formula_15 is likely to be near \"μ\". Thus, it leaves open the possibility that formula_16 happens an infinite number of times, although at infrequent intervals. (Not necessarily formula_17 for all n).\n\nThe \"strong law\" shows that this almost surely will not occur. In particular, it implies that with probability 1, we have that for any the inequality formula_18 holds for all large enough \"n\".\n\nThe strong law does not hold in the following cases, but the weak law does.\n\n1. Let X be an exponentially distributed random variable with parameter 1. The random variable formula_19 has no expected value according to Lebesgue integration, but using conditional convergence and interpreting the integral as a Dirichlet integral, which is an improper Riemann integral, we can say:\n\n2. Let x be geometric distribution with probability 0.5. The random variable formula_21 does not have an expected value in the conventional sense because the infinite series is not absolutely convergent, but using conditional convergence, we can say:\n\n3. If the cumulative distribution function of a random variable is\n\nSuppose \"f\"(\"x\",\"θ\") is some function defined for \"θ\" ∈ Θ, and continuous in \"θ\". Then for any fixed \"θ\", the sequence {\"f\"(\"X\",\"θ\"), \"f\"(\"X\",\"θ\"), …} will be a sequence of independent and identically distributed random variables, such that the sample mean of this sequence converges in probability to E[\"f\"(\"X\",\"θ\")]. This is the \"pointwise\" (in \"θ\") convergence.\n\nThe uniform law of large numbers states the conditions under which the convergence happens \"uniformly\" in \"θ\". If\n\n\nThen E[\"f\"(\"X\",\"θ\")] is continuous in \"θ\", and\n\nThis result is useful to derive consistency of a large class of estimators (see Extremum estimator).\n\nBorel's law of large numbers, named after Émile Borel, states that if an experiment is repeated a large number of times, independently under identical conditions, then the proportion of times that any specified event occurs approximately equals the probability of the event's occurrence on any particular trial; the larger the number of repetitions, the better the approximation tends to be. More precisely, if \"E\" denotes the event in question, \"p\" its probability of occurrence, and \"N\"(\"E\") the number of times \"E\" occurs in the first \"n\" trials, then with probability one,\n\nThis theorem makes rigorous the intuitive notion of probability as the long-run relative frequency of an event's occurrence. It is a special case of any of several more general laws of large numbers in probability theory.\n\nChebyshev's inequality. Let \"X\" be a random variable with finite expected value \"μ\" and finite non-zero variance \"σ\". Then for any real number ,\n\nGiven \"X\", \"X\", ... an infinite sequence of i.i.d. random variables with finite expected value \"E\"(\"X\") = \"E\"(\"X\") = ... = µ < ∞, we are interested in the convergence of the sample average\n\nThe weak law of large numbers states:\nThis proof uses the assumption of finite variance formula_30 (for all formula_31). The independence of the random variables implies no correlation between them, and we have that\n\nThe common mean μ of the sequence is the mean of the sample average:\n\nUsing Chebyshev's inequality on formula_34 results in\n\nThis may be used to obtain the following:\n\nAs \"n\" approaches infinity, the expression approaches 1. And by definition of convergence in probability, we have obtained\n\nBy Taylor's theorem for complex functions, the characteristic function of any random variable, \"X\", with finite mean μ, can be written as\n\nAll \"X\", \"X\", ... have the same characteristic function, so we will simply denote this \"φ\".\n\nAmong the basic properties of characteristic functions there are\n\nThese rules can be used to calculate the characteristic function of formula_39 in terms of \"φ\":\n\nThe limit  \"e\"  is the characteristic function of the constant random variable μ, and hence by the Lévy continuity theorem, formula_41 converges in distribution to μ:\n\nμ is a constant, which implies that convergence in distribution to μ and convergence in probability to μ are equivalent (see Convergence of random variables.) Therefore,\n\nThis shows that the sample mean converges in probability to the derivative of the characteristic function at the origin, as long as the latter exists.\n\n\n\n"}
{"id": "5211007", "url": "https://en.wikipedia.org/wiki?curid=5211007", "title": "Lemniscate of Gerono", "text": "Lemniscate of Gerono\n\nIn algebraic geometry, the lemniscate of Gerono, or lemniscate of Huygens, or figure-eight curve, is a plane algebraic curve of degree four and genus zero and is a lemniscate curve shaped like an formula_1 symbol, or figure eight. It has equation\n\nIt was studied by Camille-Christophe Gerono.\n\nBecause the curve is of genus zero, it can be parametrized by rational functions; one means of doing that is\n\nAnother representation is\nwhich reveals that this lemniscate is a special case of a Lissajous figure.\n\nThe dual curve (see Plücker formula), pictured below, has therefore a somewhat different character. Its equation is\n"}
{"id": "1735228", "url": "https://en.wikipedia.org/wiki?curid=1735228", "title": "Lenstra–Lenstra–Lovász lattice basis reduction algorithm", "text": "Lenstra–Lenstra–Lovász lattice basis reduction algorithm\n\nThe Lenstra–Lenstra–Lovász (LLL) lattice basis reduction algorithm is a polynomial time lattice reduction algorithm invented by Arjen Lenstra, Hendrik Lenstra and László Lovász in 1982. Given a basis formula_1 with \"n\"-dimensional integer coordinates, for a lattice L (a discrete subgroup of R) with formula_2, the LLL algorithm calculates an \"LLL-reduced\" (short, nearly orthogonal) lattice basis in time\n\nwhere B is the largest length of formula_4 under the Euclidean norm.\n\nThe original applications were to give polynomial-time algorithms for factorizing polynomials with rational coefficients, for finding simultaneous rational approximations to real numbers, and for solving the integer linear programming problem in fixed dimensions.\n\nThe precise definition of LLL-reduced is as follows: Given a basis\n\ndefine its Gram–Schmidt process orthogonal basis\n\nand the Gram-Schmidt coefficients\n\nThen the basis formula_9 is LLL-reduced if there exists a parameter formula_10 in (0.25,1] such that the following holds:\n\n\nHere, estimating the value of the formula_10 parameter, we can conclude how well the basis is reduced. Greater values of formula_10 lead to stronger reductions of the basis.\nInitially, A. Lenstra, H. Lenstra and L. Lovász demonstrated the LLL-reduction algorithm for formula_15.\nNote that although LLL-reduction is well-defined for formula_16, the polynomial-time complexity is guaranteed only\nfor formula_10 in formula_18.\n\nThe LLL algorithm computes LLL-reduced bases. There is no known efficient algorithm to compute a basis in which the basis vectors are as short as possible for lattices of dimensions greater than 4. However, an LLL-reduced basis is nearly as short as possible, in the sense that there are absolute bounds formula_19 such that the first basis vector is no more than formula_20 times as long as a shortest vector in the lattice,\nthe second basis vector is likewise within formula_21 of the second successive minimum, and so on.\n\nThe following description is based on , with the corrections from the errata.\n\nINPUT:\n\nPROCEDURE:\n\nOUTPUT: LLL reduced basis formula_28\n\nThe following presents an example due to W. Bosma.\n\nINPUT:\n\nLet a lattice basis formula_29, be given by the columns of\n\nThen according to the LLL algorithm we obtain the following:\n\nand formula_31\n\nformula_34\nformula_39\n\nhence formula_40\n\nand formula_41\n\nhence formula_42 and\n\nformula_43\nApply a SWAP, continue algorithm with the lattice basis, which is given by columns\n\nImplement the algorithm steps again.\n=\\frac{13}{3}\\left(>\\frac{1}{2}\\right)</math>\n\nthen\n\nOUTPUT: LLL reduced basis\n\nThe LLL algorithm has found numerous other applications in MIMO detection algorithms and cryptanalysis of public-key encryption schemes: knapsack cryptosystems, RSA with particular settings, NTRUEncrypt, and so forth. The algorithm can be used to find integer solutions to many problems.\n\nIn particular, the LLL algorithm forms a core of one of the integer relation algorithms. For example, if it is believed that \"r\"=1.618034 is a (slightly rounded) root to an unknown quadratic equation with integer coefficients, one may apply LLL reduction to the lattice in formula_53 spanned by formula_54 and formula_55. The first vector in the reduced basis will be an integer linear combination of these three, thus necessarily of the form formula_56; but such a vector is \"short\" only if \"a\", \"b\", \"c\" are small and formula_57 is even smaller. Thus the first three entries of this short vector are likely to be the coefficients of the integral quadratic polynomial which has \"r\" as a root. In this example the LLL algorithm finds the shortest vector to be [1, -1, -1, 0.00025] and indeed formula_58 has a root equal to the golden ratio, 1.6180339887….\n\nLLL is implemented in\n\n\n"}
{"id": "41912043", "url": "https://en.wikipedia.org/wiki?curid=41912043", "title": "Manin conjecture", "text": "Manin conjecture\n\nIn mathematics, the Manin conjecture describes the conjectural distribution of rational points on an algebraic variety relative to a suitable height function. It was proposed by Yuri I. Manin and his collaborators in 1989 when they initiated a program with the aim of describing the distribution of rational points on suitable algebraic varieties.\n\nTheir main conjecture is as follows.\nLet formula_1 \nbe a Fano variety defined\nover a number field formula_2,\nlet formula_3\nbe a height function which is relative to the anticanonical divisor\nand assume that \nformula_4\nis Zariski dense in formula_1. \nThen there exists\na non-empty Zariski open subset \nformula_6\nsuch that the counting function\nof formula_2-rational points of bounded height, defined by\nfor formula_9, \nsatisfies\nas formula_11\nHere\nformula_12\nis the rank of the Picard group of formula_1\nand formula_14\nis a positive constant which\nlater received a conjectural interpretation by Peyre.\n\nManin's conjecture has been decided for special families of varieties, but is still open in general.\n"}
{"id": "11491735", "url": "https://en.wikipedia.org/wiki?curid=11491735", "title": "Markov partition", "text": "Markov partition\n\nA Markov partition is a tool used in dynamical systems theory, allowing the methods of symbolic dynamics to be applied to the study of hyperbolic dynamics. By using a Markov partition, the system can be made to resemble a discrete-time Markov process, with the long-term dynamical characteristics of the system represented as a Markov shift. The appellation 'Markov' is appropriate because the resulting dynamics of the system obeys the Markov property. The Markov partition thus allows standard techniques from symbolic dynamics to be applied, including the computation of expectation values, correlations, topological entropy, topological zeta functions, Fredholm determinants and the like.\n\nLet (\"M\",\"φ\") be a discrete dynamical system. A basic method of studying its dynamics is to find a symbolic representation: a faithful encoding of the points of \"M\" by sequences of symbols such that the map \"φ\" becomes the shift map.\n\nSuppose that \"M\" has been divided into a number of pieces \"E\",\"E\",…,\"E\", which are thought to be as small and localized, with virtually no overlaps. The behavior of a point \"x\" under the iterates of \"φ\" can be tracked by recording, for each \"n\", the part \"E\" which contains \"φ\"(\"x\"). This results in an infinite sequence on the alphabet {1,2,…\"r\"} which encodes the point. In general, this encoding may be imprecise (the same sequence may represent many different points) and the set of sequences which arise in this way may be difficult to describe. Under certain conditions, which are made explicit in the rigorous definition of a Markov partition, the assignment of the sequence to a point of \"M\" becomes an almost one-to-one map whose image is a symbolic dynamical system of a special kind called a shift of finite type. In this case, the symbolic representation is a powerful tool for investigating the properties of the dynamical system (\"M\",\"φ\").\n\nA Markov partition is a finite cover of the invariant set of the manifold by a set of curvilinear rectangles formula_1 such that\n\n\nHere, formula_10 and formula_11 are the unstable and stable manifolds of \"x\", respectively, and formula_12 simply denotes the interior of formula_13. \nThese last two conditions can be understood as a statement of the Markov property for the symbolic dynamics; that is, the movement of a trajectory from one open cover to the next is determined only by the most recent cover, and not the history of the system. It is this property of the covering that merits the 'Markov' appellation. The resulting dynamics is that of a Markov shift; that this is indeed the case is due to theorems by Yakov Sinai (1968) and Rufus Bowen (1975), thus putting symbolic dynamics on a firm footing.\n\nVariants of the definition are found, corresponding to conditions on the geometry of the pieces formula_13.\n\nMarkov partitions have been constructed in several situations.\n\n\nMarkov partitions make homoclinic and heteroclinic orbits particularly easy to describe.\n\nThe system formula_15 has the Markov partition formula_16, and in this case the symbolic representation of a real number in formula_17 is its binary expansion. For example: formula_18. The assignment of points of formula_17 to their sequences in the Markov partition is well defined except on the dyadic rationals - morally speaking, this is because formula_20, in the same way as formula_21 in decimal expansions.\n\n"}
{"id": "15145610", "url": "https://en.wikipedia.org/wiki?curid=15145610", "title": "Method of support", "text": "Method of support\n\nIn statistics, the method of support is a technique that is used to make inferences from datasets.\n\nAccording to A. W. F. Edwards, the method of support aims to make inferences about unknown parameters in terms of the relative support, or log likelihood, induced by a set of data for a particular parameter value. The technique may be used whether or not prior information is available. \n\nThe method of maximum likelihood is part of the method of support, but note that the method of support also provides confidence regions that are defined in terms of their support.\n\nNotable proponents of the method of support include A. W. F. Edwards.\n\n"}
{"id": "307975", "url": "https://en.wikipedia.org/wiki?curid=307975", "title": "Moritz Cantor", "text": "Moritz Cantor\n\nMoritz Benedikt Cantor (23 August 1829 – 10 April 1920) was a German historian of mathematics.\n\nCantor was born at Mannheim. He came from a family that had emigrated to the Netherlands from Portugal, another branch of which had established itself in Russia. In his early youth, Moritz Cantor was not strong enough to go to school, and his parents decided to educate him at home. Later, however, he was admitted to an advanced class of the Gymnasium in Mannheim. From there he went to the University of Heidelberg in 1848, and soon after to the University of Göttingen, where he studied under Gauss and Weber, and where Stern awakened in him a strong interest in historical research.\n\nAfter obtaining his Ph.D. at the University of Heidelberg in 1851, he went to Berlin, where he eagerly followed the lectures of Peter Gustav Lejeune Dirichlet; and upon his return to Heidelberg in 1853, he was appointed privat-docent at the university. In 1863, he was promoted to the position of assistant professor, and in 1877 he became honorary professor.\n\nCantor was one of the founders of the \"Kritische Zeitschrift für Chemie, Physik und Mathematik\". In 1859 he became associated with Schlömilch as editor of the \"Zeitschrift für Mathematik und Physik\", taking charge of the historical and literary section. Since 1877, through his efforts, a supplement to the \"Zeitschrift\" was published under the separate title of \"Abhandlungen zur Geschichte der Mathematik\".\n\nCantor's inaugural dissertation, \"Über ein weniger gebräuchliches Coordinaten-System\" (1851), gave no indication that the history of exact sciences would soon be enriched by a master work by him. His first important work was \"Über die Einführung unserer gegenwärtigen Ziffern in Europa\", which he wrote for the \"Zeitschrift für Mathematik und Physik\", 1856, vol. i.\n\nHis greatest work was \"Vorlesungen über Geschichte der Mathematik\". This comprehensive history of mathematics appeared as follows:\n\nMany historians credit him for founding a new discipline in a field that had hitherto lacked the sound, conscientious, and critical methods of other fields of history.\n\nIn 1900 Moritz Cantor received the honor of giving a plenary address at the International Congress of Mathematicians in Paris (\"Sur l'historiographie des mathématiques\").\n\n\n"}
{"id": "39615068", "url": "https://en.wikipedia.org/wiki?curid=39615068", "title": "Morningside Medal", "text": "Morningside Medal\n\nThe Morningside Medal of Mathematics is awarded to exceptional mathematicians of Chinese descent under the age of forty-five for their seminal achievements in mathematics and applied mathematics. The winners of the Morningside Medal of Mathematics are traditionally announced at the opening ceremony of the triennial International Congress of Chinese Mathematicians. Each Morningside Medalist receives a certificate, a medal, and cash award of US$25,000 for a gold medal, or US$10,000 for a silver medal.\n"}
{"id": "21088783", "url": "https://en.wikipedia.org/wiki?curid=21088783", "title": "Offset binary", "text": "Offset binary\n\nOffset binary, also referred to as excess-K, excess-\"N\", excess code or biased representation, is a digital coding scheme where all-zero corresponds to the minimal negative value and all-one to the maximal positive value. There is no standard for offset binary, but most often the offset \"K\" for an \"n\"-bit binary word is \"K\" = 2. This has the consequence that the \"zero\" value is represented by a 1 in the most significant bit and zero in all other bits, and in general the effect is conveniently the same as using two's complement except that the most significant bit is inverted. It also has the consequence that in a logical comparison operation, one gets the same result as with a two's complement numerical comparison operation, whereas, in two's complement notation a logical comparison will agree with two's complement numerical comparison operation if and only if the numbers being compared have the same sign. Otherwise the sense of the comparison will be inverted, with all negative values being taken as being larger than all positive values. \n\nOne historically prominent example of offset-64 (\"excess-64\") notation was in the floating point (exponential) notation in the IBM System/360 and System/370 generations of computers. The \"characteristic\" (exponent) took the form of a seven-bit excess-64 number (The high-order bit of the same byte contained the sign of the significand).\n\nThe 8-bit exponent in Microsoft Binary Format, a floating point format used in various programming languages (in particular BASIC) in the 1970s and 1980s, was encoded using an offset-129 notation (\"excess-129\").\n\nThe IEEE Standard for Floating-Point Arithmetic (IEEE 754) uses various sizes of exponent, but also uses offset notation for the format of each precision. Unusually however, instead of using \"excess 2\" it uses \"excess 2 − 1\" (i.e. \"excess-15\", \"excess-127\", \"excess-1023\", \"excess-16383\") which means that inverting the leading (high-order) bit of the exponent will not convert the exponent to correct two's complement notation. \n\nOffset binary is often used in digital signal processing (DSP). Most analog to digital (A/D) and digital to analog (D/A) chips are unipolar, which means that they cannot handle bipolar signals (signals with both positive and negative values). A simple solution to this is to bias the analog signals with a DC offset equal to half of the A/D and D/A converter's range. The resulting digital data then ends up being in offset binary format.\n\nMost standard computer CPU chips cannot handle the offset binary format directly. CPU chips typically can only handle signed and unsigned integers, and floating point value formats. Offset binary values can be handled in several ways by these CPU chips. The data may just be treated as unsigned integers, requiring the programmer to deal with the zero offset in software. The data may also be converted to signed integer format (which the CPU can handle natively) by simply subtracting the zero offset. As a consequence of the most common offset for an \"n\"-bit word being 2, which implies that the first bit is inverted relative to two's complement, there is no need for a separate subtraction step, but one simply can invert the first bit. This sometimes is a useful simplification in hardware, and can be convenient in software as well.\n\nTable of offset binary for four bits, with two's complement for comparison\nOffset binary may be converted into two's complement by inverting the most significant bit. For example, with 8-bit values, the offset binary value may be XORed with 0x80 in order to convert to two's complement. In specialised hardware it may be simpler to accept the bit as it stands, but to apply its value in inverted significance.\n\n\n"}
{"id": "10323007", "url": "https://en.wikipedia.org/wiki?curid=10323007", "title": "POPLmark challenge", "text": "POPLmark challenge\n\nIn programming language theory, the POPLmark challenge (from \"Principles of Programming Languages benchmark\", formerly Mechanized Metatheory for the Masses!) (Aydemir, 2005) is a set of benchmarks designed to evaluate the state of automated reasoning (or mechanization) in the metatheory of programming languages, and to stimulate discussion and collaboration among a diverse cross section of the formal methods community. Very loosely speaking, the challenge is about measurement of how well programs may be proven to match a specification of how they are intended to behave (and the many complex issues that this involves). The challenge was initially proposed by the members of the \"PL club\" at the University of Pennsylvania, in association with collaborators around the world. The \"Workshop on Mechanized Metatheory\" is the main meeting of researchers participating in the challenge.\n\nThe design of the POPLmark benchmark is guided by features common to reasoning about programming languages. The challenge problems do not require the formalisation of large programming languages, but they do require sophistication in reasoning about:\n\n\n, the POPLmark challenge is composed of three parts. Part 1 concerns solely the types of System F (System F with subtyping), and has problems such as:\n\nPart 2 concerns the syntax and semantics of System F. It concerns proofs of\n\nPart 3 concerns the usability of the formalisation of System F. In particular, the challenge asks for:\n\nSeveral solutions have been proposed for parts of the POPLmark challenge, using following tools: Isabelle/HOL, Twelf, Coq, αProlog, ATS, Abella and Matita.\n\n\n\n\n"}
{"id": "3875355", "url": "https://en.wikipedia.org/wiki?curid=3875355", "title": "Paris–Harrington theorem", "text": "Paris–Harrington theorem\n\nIn mathematical logic, the Paris–Harrington theorem states that a certain combinatorial principle in Ramsey theory, namely the strengthened finite Ramsey theorem, is true, but not provable in Peano arithmetic. This was the first \"natural\" example of a true statement about the integers that could be stated in the language of arithmetic, but not proved in Peano arithmetic; it was already known that such statements existed by Gödel's first incompleteness theorem.\n\nThe strengthened finite Ramsey theorem is a statement about colorings and natural numbers and states that:\n\nWithout the condition that the number of elements of \"Y\" is at least the smallest element of \"Y\", this is a corollary of the finite Ramsey theorem in formula_1, with \"N\" given by:\n\nMoreover, the strengthened finite Ramsey theorem can be deduced from the infinite Ramsey theorem in almost exactly the same way that the finite Ramsey theorem can be deduced from it, using a compactness argument (see the article on Ramsey's theorem for details). This proof can be carried out in second-order arithmetic.\n\nThe Paris–Harrington theorem states that the strengthened finite Ramsey theorem is not provable in Peano arithmetic.\n\nRoughly speaking, Jeff Paris and Leo Harrington (1972) showed that the strengthened finite Ramsey theorem is unprovable in Peano arithmetic by showing that in Peano arithmetic it implies the consistency of Peano arithmetic itself. Since Peano arithmetic cannot prove its own consistency by Gödel's second incompleteness theorem, this shows that Peano arithmetic cannot prove the strengthened finite Ramsey theorem.\n\nThe smallest number \"N\" that satisfies the strengthened finite Ramsey theorem is a computable function of \"n\", \"m\", \"k\", but grows extremely fast. In particular it is not primitive recursive, but it is also far larger than standard examples of non-primitive recursive functions such as the Ackermann function. Its growth is so large that Peano arithmetic cannot prove it is defined everywhere, although Peano arithmetic easily proves that the Ackermann function is well defined.\n\n\n\n"}
{"id": "53242630", "url": "https://en.wikipedia.org/wiki?curid=53242630", "title": "Penny graph", "text": "Penny graph\n\nIn geometric graph theory, a penny graph is a contact graph of unit circles. That is, it is an undirected graph whose vertices can be represented by unit circles, with no two of these circles crossing each other, and with two adjacent vertices if and only if they are represented by tangent circles. More simply, they are the graphs formed by arranging pennies in a non-overlapping way on a flat surface, making a vertex for each penny, and making an edge for each two pennies that touch.\n\nPenny graphs have also been called unit coin graphs, because they are the coin graphs formed from unit circles. If each vertex is represented by a point the center of its circle, then two vertices will be adjacent if and only if their distance is the minimum distance among all pairs of points. Therefore, penny graphs have also been called minimum-distance graphs, smallest-distance graphs, or closest-pairs graphs. Similarly, in a mutual nearest neighbor graph that links pairs of points in the plane that are each other's nearest neighbors, each connected component is a penny graph, although edges in different components may have different lengths.\n\nEvery penny graph is a unit disk graph and a matchstick graph.\nLike planar graphs more generally, they obey the four color theorem, but this theorem is easier to prove for penny graphs.\nTesting whether a graph is a penny graph, or finding its maximum independent set, is NP-hard; however, both upper and lower bounds are known for the size of the maximum independent set.\n\nConstructing a penny graph from the locations of its circles can be performed as an instance of the closest pair of points problem, taking worst-case time or (with randomized time and with the use of the floor function) expected time .\nAn alternative method with the same worst-case time is to construct the Delaunay triangulation or nearest neighbor graph of the circle centers (both of which contain the penny graph as a subgraph) and then test which edges correspond to circle tangencies.\n\nHowever, testing whether a given graph is a penny graph is NP-hard, even when the given graph is a tree. Similarly, testing whether a graph is a three-dimensional mutual nearest neighbor graph is also NP-hard.\n\nPenny graphs are a special case of the coin graphs (graphs that can be represented by tangencies of non-crossing circles of arbitrary radii). Because the coin graphs are the same as the planar graphs, all penny graphs are planar. The penny graphs are also unit disk graphs (the intersection graphs of unit circles), unit distance graphs (graphs that can be drawn with all edges having equal lengths, allowing crossings), and matchstick graphs (graphs that can be drawn in the plane with equal-length straight edges and no edge crossings).\n\nThe Hanoi graphs formula_1 are penny graphs.\n\nEvery vertex in a penny graph has at most six neighboring vertices; here the number six is the kissing number for circles in the plane.\nHowever, the pennies whose centers are less than three units from the convex hull of the pennies have fewer neighbors. Based on a more precise version of this argument, one can show\nthat every penny graph with vertices has at most\nedges. Some penny graphs, formed by arranging the pennies in a triangular grid, have exactly this number of edges.\nBy arranging the pennies in a square grid, or in the form of certain squaregraphs, one can form triangle-free penny graphs whose number of edges is at least\nSwanepoel suggested that this bound is tight. Proving this, or finding a better bound, remains open. It is known that the number of edges can be at most\nbut the coefficient of the square root does not match Swanepoel's conjecture.\n\nEvery penny graph contains a vertex with at most three neighbors. For instance, such a vertex can be found at one of the corners of the convex hull of the circle centers, or as one of the two farthest-apart circle centers. Therefore, penny graphs have degeneracy at most three. Based on this, one can prove that their graph colorings require at most four colors, much more easily than the proof of the more general four-color theorem.\nHowever, despite their restricted structure, there exist penny graphs that do still require four colors.\nAnalogously, the degeneracy of every triangle-free penny graph is at most two. Every such graph contains a vertex with at most two neighbors, even though it is not always possible to find this vertex on the convex hull. Based on this one can prove that they require at most three colors, more easily than the proof of the more general Grötzsch's theorem that triangle-free planar graphs are 3-colorable.\n\nA maximum independent set in a penny graph is a subset of the pennies, no two of which touch each other. Finding maximum independent sets is NP-hard for arbitrary graphs, and remains NP-hard on penny graphs. It is an instance of the maximum disjoint set problem, in which one must find large subsets of non-overlapping regions of the plane. However, as with planar graphs more generally, Baker's technique provides a polynomial-time approximation scheme for this problem.\nIn 1983, Paul Erdős asked for the largest number such that every -vertex penny graph has an independent set of at least vertices. That is, if we place pennies on a flat surface, there should be a subset of of the pennies that don't touch each other. By the four-color theorem, , and the improved bound was proven by Swanepoel. In the other direction, Pach and Tóth proved that . As of 2013, these remained the best bounds known for this problem.\n"}
{"id": "12181377", "url": "https://en.wikipedia.org/wiki?curid=12181377", "title": "Positive invariant set", "text": "Positive invariant set\n\nIn mathematical analysis, a positively invariant set is a set with the following properties:\n\nGiven a dynamical system formula_1 and trajectory formula_2 where formula_3 is the initial point. Let formula_4 where formula_5 is a real valued function. The set formula_6 is said to be positively invariant if formula_7 implies that formula_8\n\nIntuitively, this means that once a trajectory of the system enters formula_6, it will never leave it again.\n\n"}
{"id": "46261775", "url": "https://en.wikipedia.org/wiki?curid=46261775", "title": "Quotient graph", "text": "Quotient graph\n\nIn graph theory, a quotient graph \"Q\" of a graph \"G\" is a graph whose vertices are blocks of a partition of the vertices of \"G\" and where block \"B\" is adjacent to block \"C\" if some vertex in \"B\" is adjacent to some vertex in \"C\" with respect to the edge set of \"G\". In other words, if \"G\" has edge set \"E\" and vertex set \"V\" and \"R\" is the equivalence relation induced by the partition, then the quotient graph has vertex set \"V\"/\"R\" and edge set {([\"u\"], [\"v\"]) | (\"u\", \"v\") ∈ \"E\"(\"G\")}.\n\nMore formally, a quotient graph is a quotient object in the category of graphs. The category of graphs is concretizable – mapping a graph to its set of vertices makes it a concrete category – so its objects can be regarded as \"sets with additional structure\", and a quotient graph corresponds to the graph induced on the quotient set \"V\"/\"R\" of its vertex set \"V\". Further, there is a graph homomorphism (a quotient map) from a graph to a quotient graph, sending each vertex or edge to the equivalence class that it belongs to. Intuitively, this corresponds to \"gluing together\" (formally, \"identifying\") vertices and edges of the graph.\n\nA graph is trivially a quotient graph of itself (each block of the partition is a single vertex), and the graph consisting of a single point is the quotient graph of any non-empty graph (the partition consisting of a single block of all vertices). The simplest non-trivial quotient graph is one obtained by identifying two vertices (vertex identification); if the vertices are connected, this is called edge contraction.\n\nThe condensation of a directed graph is the quotient graph where the strongly connected components form the blocks of the partition. This construction can be used to derive a directed acyclic graph from any directed graph.\n\nThe result of one or more edge contractions in an undirected graph \"G\" is a quotient of \"G\", in which the blocks are the connected components of the subgraph of \"G\" formed by the contracted edges. However, for quotients more generally, the blocks of the partition giving rise to the quotient do not need to form connected subgraphs.\n\nIf \"G\" is a covering graph of another graph \"H\", then \"H\" is a quotient graph of \"G\". The blocks of the corresponding partition are the inverse images of the vertices of \"H\" under the covering map. However, covering maps have an additional requirement that is not true more generally of quotients, that the map be a local isomorphism.\n\nIt is NP-complete, given an -vertex cubic graph \"G\" and a parameter , to determine whether \"G\" can be obtained as a quotient of a planar graph with vertices.\n"}
{"id": "22783396", "url": "https://en.wikipedia.org/wiki?curid=22783396", "title": "Renu C. Laskar", "text": "Renu C. Laskar\n\nRenu Chakravarti Laskar (born 1932) is an Indian-born American mathematician, specializing in graph theory. She is Professor Emerita of Mathematical sciences at Clemson University. She received her Ph.D. in Mathematics from the University of Illinois at Urbana-Champaign in 1962.\n\nLaskar has often contributed to the theory of domination number and circular arc graphs. She wrote four papers with Paul Erdős, giving her an Erdős number of 1.\n\nRenu C. Laskar was born in Bihar, India. With the help of her family support, she finished her schooling and college, which wasn't much accessible to the women that time due to the cultural norms prevalent in India. It was during that time, when she discovered her talent for mathematics. She finished her Master's degree in Mathematics from B. R. Ambedkar Bihar University in the year 1955. Upon finishing college, Laskar, with strong encouragement from her elder brother, decided to come to the United States to pursue her Ph.D. in the year 1958. She worked for her Ph.D. degree at the University of Illinois at Urbana-Champaign under her advisor Henry Roy Brahana and received her degree in the year 1962. She is the first female Indian to receive a Ph.D. in Mathematics from UIUC. She returned to India after that and joined the Indian Institute of Technology Kharagpur as the first woman faculty at the institute. After three years, Laskar then moved back to the US at University of North Carolina at Chapel Hill and then finally joined Clemson University in the year 1968.\n\nLaskar took full advantage of the opportunities she had and set new standards for women in mathematics. She ranks among the top women in discrete mathematics in the number of articles published. According to MathSciNet, she has over 100 publications. Part of the reason for her success in this area is her collaboration network, which included Raj Chandra Bose and Paul Erdős. She has extended her influence by supervising Ph.D. students. In 1986, Laskar and Steve Hedetniemi organized the Clemson University Discrete Math Miniconference, an event that has drawn an international audience each year since.\n\n"}
{"id": "715238", "url": "https://en.wikipedia.org/wiki?curid=715238", "title": "Richard Courant", "text": "Richard Courant\n\nRichard Courant (January 8, 1888 – January 27, 1972) was a German American mathematician. He is best known by the general public for the book \"What is Mathematics?\", co-written with Herbert Robbins. \n\nCourant was born in Lublinitz, in the Prussian Province of Silesia. His parents were Siegmund Courant and Martha Courant née Freund of Oels. Edith Stein was Richard's cousin on the paternal side. During his youth his parents moved often, including to Glatz, then to Breslau and in 1905 to Berlin. He stayed in Breslau and entered the university there, then continued his studies at the University of Zürich and the University of Göttingen. He became David Hilbert's assistant in Göttingen and obtained his doctorate there in 1910. He was obliged to serve in World War I, but was wounded shortly after enlisting and therefore dismissed from the military. Courant left the University of Münster in 1921 to take over Erich Heckes position at the University of Göttingen. There he founded the Mathematical Institute, which he headed as director from 1928 until 1933.\n\nCourant left Germany in 1933, earlier than many Jewish escapees. He did not lose his position due to being Jewish, as his previous service as a front-line soldier exempted him; however, his public membership in the social-democratic left was reason enough (for the Nazis) for dismissal.\n\nIn 1936, after one year at Cambridge, Courant accepted a professorship at New York University in New York City. There he founded an institute for graduate studies in applied mathematics. The Courant Institute of Mathematical Sciences (as it was renamed in 1964) is now one of the most respected research centers in applied mathematics.\n\nCourant and David Hilbert authored the influential textbook \"Methoden der mathematischen Physik\" which, with its revised editions, is still current and widely used since its publication in 1924. With Herbert Robbins he coauthored a popular overview of higher mathematics, intended for the general public, titled \"What is Mathematics?\". With Fritz John he also coauthored the two-volume work \"Introduction to Calculus and Analysis,\" first published in 1965.\n\nCourant's name is also attached to the finite element method, with his numerical treatment of the plain torsion problem for multiply-connected domains, published in 1943.\nThis method is now one of the ways to solve partial differential equations numerically. Courant is a namesake of the Courant–Friedrichs–Lewy condition and the Courant minimax principle.\n\nCourant died of a stroke in New Rochelle, New York on January 27th, 1972.\n\nCommenting upon his analysis of experimental results from in-laboratory soap film formations, Courant believed that the existence of a physical solution does not obviate mathematical proof. Here is a quote from Courant on his mathematical perspective:\n\nIn 1912 Courant married Nelly Neumann, who had earned her doctorate at Breslau in Synthetic Geometry in 1909. They lived together in Göttingen until they were divorced in 1916. She was later murdered by the Nazis in 1942 for being Jewish.\n\nIn 1919 Courant married Nerina (Nina) Runge (1891-1991), a daughter of the Göttingen professor for Applied Mathematics, Carl Runge (of Runge-Kutta fame).\n\nRichard and Nerina had four children: Ernest, a particle physicist and innovator in particle accelerators; Gertrude (1922-2014), a PhD biologist and wife of the mathematician Jürgen Moser (1928–1999); Hans, a physicist who participated in the Manhattan Project; and Leonore (known as \"Lori,\" 1928-2015), a professional violist and wife of the mathematician Jerome Berkowitz (1928–1998).\n\n\n\n"}
{"id": "8077953", "url": "https://en.wikipedia.org/wiki?curid=8077953", "title": "Riemann–von Mangoldt formula", "text": "Riemann–von Mangoldt formula\n\nIn mathematics, the Riemann–von Mangoldt formula, named for Bernhard Riemann and Hans Carl Friedrich von Mangoldt, describes the distribution of the zeros of the Riemann zeta function.\n\nThe formula states that the number \"N\"(\"T\") of zeros of the zeta function with imaginary part greater than 0 and less than or equal to \"T\" satisfies\n\nThe formula was stated by Riemann in his notable paper \"On the Number of Primes Less Than a Given Magnitude\" (1859) and was finally proved by Mangoldt in 1905.\n\nBacklund gives an explicit form of the error for all \"T\" greater than 2:\n\n"}
{"id": "640249", "url": "https://en.wikipedia.org/wiki?curid=640249", "title": "Saddle point", "text": "Saddle point\n\nIn mathematics, a saddle point or minimax point is a point on the surface of the graph of a function where the slopes (derivatives) in orthogonal directions are all zero (a critical point), but which is not a local extremum of the function. An example of a saddle point shown on the right is when there is a critical point with a relative minimum along one axial direction (between peaks) and at a relative maximum along the crossing axis. However, a saddle point need not be in this form. For example, the function formula_1 has a critical point at formula_2 that is a saddle point since it is neither a relative maximum nor relative minimum, but it does not have a relative maximum or relative minimum in the formula_3-direction.\nThe name derives from the fact that the prototypical example in two dimensions is a surface that \"curves up\" in one direction, and \"curves down\" in a different direction, resembling a riding saddle or a mountain pass between two peaks forming a landform saddle. In terms of contour lines, a saddle point in two dimensions gives rise to a contour graph or trace in which the contour corresponding to the saddle point's value appears to intersect itself.\n\nA simple criterion for checking if a given stationary point of a real-valued function \"F\"(\"x\",\"y\") of two real variables is a saddle point is to compute the function's Hessian matrix at that point: if the Hessian is indefinite, then that point is a saddle point. For example, the Hessian matrix of the function formula_4 at the stationary point formula_5 is the matrix\nwhich is indefinite. Therefore, this point is a saddle point. This criterion gives only a sufficient condition. For example, the point formula_7 is a saddle point for the function formula_8 but the Hessian matrix of this function at the origin is the null matrix, which is not indefinite.\n\nIn the most general terms, a saddle point for a smooth function (whose graph is a curve, surface or hypersurface) is a stationary point such that the curve/surface/etc. in the neighborhood of that point is not entirely on any side of the tangent space at that point.\nIn a domain of one dimension, a saddle point is a point which is both a stationary point and a point of inflection. Since it is a point of inflection, it is not a local extremum.\n\nA saddle surface is a smooth surface containing one or more saddle points.\n\nClassical examples of two-dimensional saddle surfaces in the Euclidean space are second order surfaces, the hyperbolic paraboloid formula_4 (which is often referred to as \"\"the\" saddle surface\" or \"the standard saddle surface\") and the hyperboloid of one sheet. The Pringles potato chip or crisp is an everyday example of a hyperbolic paraboloid shape.\n\nSaddle surfaces have negative Gaussian curvature which distinguish them from convex/elliptical surfaces which have positive Gaussian curvature. A classical third-order saddle surface is the monkey saddle.\n\nIn a two-player zero sum game defined on a continuous space, the equilibrium point is a saddle point.\n\nFor a second-order linear autonomous system, a critical point is a saddle point if the characteristic equation has one positive and one negative real eigenvalue.\n\nIn optimization subject to equality constraints, the first-order conditions describe a saddle point of the Lagrangian.\n\nIn dynamical systems, if the dynamic is given by a differentiable map \"f\" then a point is hyperbolic if and only if the differential of \"ƒ\" (where \"n\" is the period of the point) has no eigenvalue on the (complex) unit circle when computed at the point. Then\na \"saddle point\" is a hyperbolic periodic point whose stable and unstable manifolds have a dimension that is not zero.\n\nA saddle point of a matrix is an element which is both the largest element in its column and the smallest element in its row.\n\n\n"}
{"id": "33255954", "url": "https://en.wikipedia.org/wiki?curid=33255954", "title": "Sienna Morris", "text": "Sienna Morris\n\nSienna Morris (born November 17, 1983) is an artist living in Portland, Oregon. Sienna Morris is an illustrator best known for her drawing technique Numberism. This technique is similar to stippling or pointillism, but uses numbers and equations in place of dots. \n\nPublished in Physiology Now Magazine. Physiology Now article \"Numberism: When Science and Art Come Together\"\n\nNominated for the 2014 Geekie Awards in arts and crafts.\n\nNominated for Best Visual Artist in the Willamette Weekly Best of Portland 2015\n\nSienna's Numberism Drawing \"Schrodinger's Cat\" was featured on the cover as well as in the content of the German Physics book, \"Faszinierende Physik: Ein bebilderter Streifzug vom Universum bis in die Welt der Elementarteilchen\".\n\nIn 2014, Evolution Expo licensed the \"Universal Proprioception\" drawing as their logo for their convention celebrating the connection between science and science fiction.\n\n"}
{"id": "55273264", "url": "https://en.wikipedia.org/wiki?curid=55273264", "title": "Steiner point (computational geometry)", "text": "Steiner point (computational geometry)\n\nIn computational geometry, a Steiner point is a point that is not part of the input to a geometric optimization problem but is added during the solution of the problem, to create a better solution than would be possible from the original points alone.\n\nThe name of these points comes from the Steiner tree problem, named after Jakob Steiner, in which the goal is to connect the input points by a network of minimum total length. If the input points alone are used as endpoints of the network edges, then the shortest network is their minimum spanning tree. However, shorter networks can often be obtained by adding Steiner points,\nand using both the new points and the input points as edge endpoints.\nAnother problem that uses Steiner points is Steiner triangulation. The goal is to partition an input (such as a point set or polygon) into triangles, meeting edge-to-edge. Both input points and Steiner points may be used as triangle vertices.\n"}
{"id": "37534627", "url": "https://en.wikipedia.org/wiki?curid=37534627", "title": "Stereotype space", "text": "Stereotype space\n\nIn functional analysis and related areas of mathematics, stereotype spaces are topological vector spaces defined by a special variant of reflexivity condition. They form a class of spaces with a series of remarkable properties, in particular, this class is very wide (for instance, it contains all Fréchet spaces and thus, all Banach spaces), it consists of spaces satisfying a natural condition of completeness, and it forms a *-autonomous category with the standard analytical tools for constructing new spaces, like taking dual space, space of operators, tensor products, and in addition, immediate subspace, immediate quotient space, products and coproducts, limits and colimits, etc. \n\nA stereotype space is a topological vector space formula_1 over the field formula_2 of complex numbers such that the natural map into the second dual space\n\nis an isomorphism of topological vector spaces (i.e. a linear and a homeomorphic map). Here the \"dual space\" formula_4 is defined as the space of all linear continuous functionals formula_5 endowed with the topology of uniform convergence on totally bounded sets in formula_1, and the \"second dual space\" formula_7 is the space dual to formula_8 in the same sense.\n\nThe following criterion holds: a topological vector space formula_1 is stereotype if and only if it is locally convex and satisfies the following two conditions:\n\nThe property of being pseudocomplete is a weakening of the usual notion of completeness, while the property of being pseudosaturated is a weakening of the notion of barreledness of a topological vector space.\n\nThe class Ste of stereotype spaces is extremely wide, so that it will not be a serious exaggeration to say that all topological vector spaces really used in analysis are stereotype. Each pseudocomplete barreled space formula_1 (in particular, each Banach space and each Fréchet space) is stereotype. Its dual space formula_15 (which is not barreled, unless formula_1 is a Montel space) is stereotype as well. There exist stereotype spaces which are not Mackey spaces.\n\nSome simple connections between the properties of a stereotype space formula_1 and those of its dual space formula_15 are expressed in the following list of regularities:\n\nCounterexamples: \n\nThe first results on this type of reflexivity of topological vector spaces were obtained by M. F. Smith in 1952. Further investigations were conducted by B. S. Brudovskii,\n\nEach locally convex space formula_1 can be transformed into a stereotype space with the help of two standard operations, pseudocompletion and pseudosaturation, defined by the following two propositions.\n\n1. With any locally convex space formula_1, one can associate a linear continuous map formula_80 into some pseudocomplete locally convex space formula_81, called \"pseudocompletion\" of formula_1, in such a way that the following conditions are fulfilled:\n\nOne can imagine the pseudocompletion of formula_1 as the \"nearest to formula_1 from the outside\" pseudocomplete locally convex space, so that the operation formula_90 adds to formula_1 some supplementary elements, but does not change the topology of formula_1 (like the usual operation of completion).\n\n2. With any locally convex space formula_1, one can associate a linear continuous map formula_94 from some pseudosaturated locally convex space formula_95, called \"pseudosaturation\" of formula_1, in such a way that the following conditions are fulfilled:\n\nThe pseudosaturation of formula_1 can be imagined as the \"nearest to formula_1 from the inside\" pseudosaturated locally convex space, so that the operation formula_104 strengthens the topology of formula_1, but does not change the elements of formula_1.\n\nIf formula_1 is a pseudocomplete locally convex space, then its pseudosaturation formula_95 is stereotype. Dually, if formula_1 is a pseudosaturated locally convex space, then its pseudocompletion formula_81 is stereotype. For arbitrary locally convex space formula_1 the spaces formula_112 and formula_113 are stereotype.\n\nThe idea of subspace (and of quotient space) in stereotype theory leads to more complicated results than in the theory of locally convex spaces.\n\nThe notion of immediate subspace gives a \"concrete description\" of the abstract notion of \"immediate monomorphism\", or, what is equivalent in this situation, strong monomorphism in the category Ste. Surprisingly, this description does not coincide with the construction of closed subspace in the category LocConv of locally convex spaces.\n\n\n\n\nExamples:\n\nTheorem. \"For any set formula_144 in a stereotype space formula_1 there is a minimal immediate subspace formula_146 in formula_1, containing formula_144:\n\nand this subspace formula_146 is an immediate subspace in each immediate subspace, containing formula_144:\"\n\n\nTheorem. \"Each set formula_144 in a stereotype space formula_1 is a total set in its envelope formula_146.\"\n\nIf formula_160 denotes the space of all functions formula_161 with finite support, endowed with the strongest locally convex topology, and the mapping formula_162 acts by the formula formula_163, then the envelope formula_146 coincides with the abstract categorical envelope of the space formula_160 in the class formula_166 of all epimorphisms in the category Ste with respect to the morphism formula_167:\n\nDually, the notion of immediate quotient space gives a \"concrete description\" of the abstract notion of \"immediate epimorphism\", or, what is equivalent here, strong epimorphism in the category Ste. Like in the situation with monomorphisms, this description does not coincide with the construction of quotient space in the category LocConv of locally convex spaces.\n\n\n\n\n\nExamples:\n\nTheorem. \"For any set formula_217 of linear continuous functionals on a stereotype space formula_1 there is a minimal immediate quotient space formula_219 of formula_1 to which all functionals formula_217 can be extended:\n\nand this quotient space formula_219 is (up to an isomorphism) an immediate quotient space of each immediate quotient space, to which the functionals formula_217 are extended:\"\n\n\nTheorem. \"Each set formula_217 of linear continuous functionals on a stereotype space formula_1 is a total set on its refinement formula_219.\"\n\nIf formula_233 denotes the space of all functions formula_234, endowed with the topology of pointwise convergence, and the mapping formula_235 acts by the formula formula_236, then the refinement formula_219 coincides with the abstract categorical refinement of the space formula_233 in the class formula_239 of all monomorphisms in the category Ste by means of the morphism formula_167:\n\nThe class Ste of stereotype spaces forms a category with linear continuous maps as morphisms and possesses the following properties:\n\nSte is a pre-abelian category: each morphism formula_85 in the category Ste has a kernel\nand a cokernel \nAs a corollary, formula_167 has an image formula_247 and a coimage formula_248 as well. The following natural identities hold:\nwhere formula_250 denotes the pseudosaturation of the annihilator of the subspace formula_251 in the dual space formula_252:\n\nFor any two stereotype spaces formula_1 and formula_114 the \"stereotype space of operators\" formula_256 from formula_1 into formula_114, is defined as the pseudosaturation of the space formula_259 of all linear continuous maps formula_85 endowed with the topology of uniform convergeance on totally bounded sets. The space formula_256 is stereotype. It defines two natural tensor products\n\nTheorem. \"In the category Ste the following natural identities hold::\"\n\"In particular, Ste is a symmetric monoidal category with respect to the bifunctor formula_275, a symmetric closed monoidal category with respect to the bifunctor formula_276 and the internal hom-functor formula_277, and a *-autonomous category:\"\n\nSte is a bicomplete category: each small diagram formula_279Ste has a limit, formula_280, which coincides with the pseudocompletion of the corresponding limit in the category LocConv of locally convex spaces \nand a colimit, formula_282, which coincides with the pseudosaturation of the corresponding colimit in LocConv\nHowever, the direct sum and the direct product in Ste coincide with the corresponding constructions in LocConv:\nThe following natural identities hold:\n\nIf formula_1 and formula_114 are stereotype spaces then for each elements formula_299 and formula_300 the formula \ndefines an \"elementary tensor\" formula_302, and the formula\ndefines an \"elementary tensor\" formula_304\n\nTheorem. \"For each stereotype spaces formula_1 and formula_114 there is a unique linear continuous map formula_307 which turns elementary tensors formula_308 into elementary tensors\" formula_309:\n\"The family of maps formula_307 defines a natural transformation of the bifunctor formula_276 into the bifunctor formula_275\".\n\n\nA stereotype space formula_1 is said to have the \"stereotype approximation property\", if each linear continuous map formula_316 can be approximated in the stereotype space of operators formula_317 by the linear continuous maps of finite rank. This condition is weaker than the existence of the Schauder basis, but formally stronger than the classical approximation property (however, it is not clear (2017) whether the stereotype approximation property coincides with the classical one, or not). \n\nTheorem. \"For a stereotype space formula_1 the following conditions are equivalent:\" \n\nTheorem. \"If two stereotype spaces formula_1 and formula_114 have the stereotype approximation property, then the spaces formula_328, formula_329 and formula_330 have the stereotype approximation property as well.\"\n\nIn particular, if formula_1 has the stereotype approximation property, then the same is true for formula_15 and for formula_317.\n\nFor any stereotype spaces formula_1, formula_114, formula_123 a bilinear map formula_338 is said to be \"continuous\" (as a bilinear map of stereotype spaces) if \n\nExamples:\nTheorem. \"For any stereotype spaces formula_1, formula_114, formula_123 and for any continuous bilinear map formula_338 there exists a unique continuous linear map formula_359 such that formula_360, where formula_361.\"\n\nCorollary. \"For any stereotype space formula_1 the pairing formula_348 has a unique extension to a linear continuous functional formula_364. This functional in its turn can be represented as a trace of the operators formula_316 occurring as images of the tensors formula_366 under the Grothendieck transformation formula_367 if and only if the space formula_1 has the stereotype approximation property.\"\n\nBeing a symmetric monoidal category, Ste generates the notions of a \"stereotype algebra\" (as a monoid in Ste) and a \"stereotype module\" (as a module in Ste over such a monoid), and for each stereotype algebra formula_369 the categories Ste and Ste of left and right stereotype modules over formula_369 are enriched categories over Ste. This distinguishes the category Ste from the other known categories of locally convex spaces, since up to the recent time only the category Ban of Banach spaces and the category Fin of finite-dimensional spaces had been known to possess this property. On the other hand, the category Ste is so wide, and the tools for creating new spaces in Ste are so diverse, that this suggests the idea that all the results of functional analysis can be reformulated inside the stereotype theory without essential losses. On this way one can even try to completely replace the category of locally convex spaces in analysis (and in related areas) by the category Ste of stereotype spaces with the view of possible simplifications – this program was announced by S. Akbarov in 2005 and the following results can be considered as evidences of its reasonableness:\n\n\n\n"}
{"id": "2546469", "url": "https://en.wikipedia.org/wiki?curid=2546469", "title": "Temple Chevallier", "text": "Temple Chevallier\n\nTemple Chevallier FRAS (19 October 1794 in Badingham, Suffolk – 4 November 1873 in Harrow Weald) was a British clergyman, astronomer, and mathematician. Between 1847 and 1849, he made important observations regarding sunspots. Chevallier has been called \"a remarkable Victorian polymath\" (Kenworthy, 1994). Not only did he write many papers on astronomy and physics, he also published a translation of the Apostolic Fathers that went into a second edition, and translated the works of Clement of Alexandria, Polycarp and Ignatius of Antioch.\n\nHe was educated at Pembroke College, Cambridge, and was ordained a priest in 1820. He became a Fellow of Pembroke College in 1819. He was a Fellow and Tutor of Catharine Hall (St Catharine's College, Cambridge) in 1820 and Hulsean lecturer in Divinity from 1826 to 1827. He was curate and then vicar of St Andrew the Great, Cambridge. \n\nHis lectures were published as \"Of the proofs of the divine power and wisdom derived from the study of astronomy\" in 1835.\n\nThat same year, Chevallier was invited to become Professor of Astronomy at the newly founded University of Durham. A chair of Mathematics and Astronomy existed at the University of Durham between 1841–1871; Chevallier was the one to hold this post. He also served as Reader in Hebrew from 1835-1871, Registrar from 1835-1865, and from 1834-1835 also assisted with lectures in Divinity.\n\nHe was instrumental in establishing the Durham University Observatory (in 1839), serving as its Director for thirty years, and from which he made important observations of Jupiter's moons and regular meteorological observations. From 1835 until his death, he also served as perpetual Parish Priest at Esh, just outside Durham, where he founded the village school and restored the church.\n\nAfter his resignation in 1871 from his academic posts following a stroke, he died on 4 November 1873.\n\nChevallier had married, in 1825, Catherine Wheelwright, the daughter of an American Loyalist. She died in 1858. He had three children: Catherine Temple, Alicia Temple and Temple (who died as a child).\n\n\nAttribution:\n"}
{"id": "828131", "url": "https://en.wikipedia.org/wiki?curid=828131", "title": "Weak ordering", "text": "Weak ordering\n\nIn mathematics, especially order theory, a weak ordering is a mathematical formalization of the intuitive notion of a ranking of a set, some of whose members may be tied with each other. Weak orders are a generalization of totally ordered sets (rankings without ties) and are in turn generalized by partially ordered sets and preorders.\n\nThere are several common ways of formalizing weak orderings, that are different from each other but cryptomorphic (interconvertable with no loss of information): they may be axiomatized as strict weak orderings (partially ordered sets in which incomparability is a transitive relation), as total preorders (transitive binary relations in which at least one of the two possible relations exists between every pair of elements), or as ordered partitions (partitions of the elements into disjoint subsets, together with a total order on the subsets). In many cases another representation called a preferential arrangement based on a utility function is also possible.\n\nWeak orderings are counted by the ordered Bell numbers. They are used in computer science as part of partition refinement algorithms, and in the C++ Standard Library.\n\nIn horse racing, the use of photo finishes has eliminated some, but not all, ties or (as they are called in this context) dead heats, so the outcome of a horse race may be modeled by a weak ordering. In an example from the Maryland Hunt Cup steeplechase in 2007, The Bruce was the clear winner, but two horses Bug River and Lear Charm tied for second place, with the remaining horses farther back; three horses did not finish. In the weak ordering describing this outcome, The Bruce would be first, Bug River and Lear Charm would be ranked after The Bruce but before all the other horses that finished, and the three horses that did not finish would be placed last in the order but tied with each other.\n\nThe points of the Euclidean plane may be ordered by their distance from the origin, giving another example of a weak ordering with infinitely many elements, infinitely many subsets of tied elements (the sets of points that belong to a common circle centered at the origin), and infinitely many points within these subsets. Although this ordering has a smallest element (the origin itself), it does not have any second-smallest elements, nor any largest element.\n\nOpinion polling in political elections provides an example of a type of ordering that resembles weak orderings, but is better modeled mathematically in other ways. In the results of a poll, one candidate may be clearly ahead of another, or the two candidates may be statistically tied, meaning not that their poll results are equal but rather that they are within the margin of error of each other. However, if candidate \"x\" is statistically tied with \"y\", and \"y\" is statistically tied with \"z\", it might still be possible for \"x\" to be clearly better than \"z\", so being tied is not in this case a transitive relation. Because of this possibility, rankings of this type are better modeled as semiorders than as weak orderings.\n\nA strict weak ordering is a binary relation < on a set \"S\" that is a strict partial order (a transitive relation that is irreflexive, or equivalently, that is asymmetric) in which the relation \"neither \"a\" < \"b\" nor \"b\" < \"a\"\" is transitive. Therefore, a strict weak ordering has the following properties:\nThis list of properties is somewhat redundant, in that asymmetry implies irreflexivity, and in that irreflexivity and transitivity together imply asymmetry.\n\nThe \"incomparability relation\" is an equivalence relation, and its equivalence classes partition the elements of \"S\", and are totally ordered by <. Conversely, any total order on a partition of \"S\" gives rise to a strict weak ordering in which \"x\" < \"y\" if and only if there exists sets \"A\" and \"B\" in the partition with \"x\" in \"A\", \"y\" in \"B\", and \"A\" < \"B\" in the total order.\n\nNot every partial order obeys the transitive law for incomparability. For instance, consider the partial order in the set {\"a\", \"b\", \"c\"} defined by the relationship \"b\" < \"c\". The pairs \"a\", \"b\" and \"a\", \"c\" are incomparable but \"b\" and \"c\" are related, so incomparability does not form an equivalence relation and this example is not a strict weak ordering.\n\nTransitivity of incomparability (together with transitivity) can also be stated in the following forms:\nOr:\n\nStrict weak orders are very closely related to total preorders or (non-strict) weak orders, and the same mathematical concepts that can be modeled with strict weak orderings can be modeled equally well with total preorders. A total preorder or weak order is a preorder that also is a connex relation; that is, no pair of items is incomparable. A total preorder formula_1 satisfies the following properties:\n\n\nA total order is a total preorder which is antisymmetric, in other words, which is also a partial order. Total preorders are sometimes also called preference relations.\n\nThe complement of a strict weak order is a total preorder, and vice versa, but it seems more natural to relate strict weak orders and total preorders in a way that preserves rather than reverses the order of the elements. Thus we take the inverse of the complement: for a strict weak ordering <, define a total preorder formula_1 by setting \"x\" formula_1 \"y\" whenever it is not the case that \"y\" < \"x\". In the other direction, to define a strict weak ordering < from a total preorder formula_1, set \"x\" < \"y\" whenever it is not the case that \"y\" formula_1 \"x\".\n\nIn any preorder there is a corresponding equivalence relation where two elements \"x\" and \"y\" are defined as equivalent if \"x\" formula_1 \"y\" and \"y\" formula_1 \"x\". In the case of a \"total\" preorder the corresponding partial order on the set of equivalence classes is a total order. Two elements are equivalent in a total preorder if and only if they are incomparable in the corresponding strict weak ordering.\n\nA partition of a set \"S\" is a family of disjoint subsets of \"S\" that have \"S\" as their union. A partition, together with a total order on the sets of the partition, gives a structure called by Richard P. Stanley an ordered partition and by Theodore Motzkin a list of sets. An ordered partition of a finite set may be written as a finite sequence of the sets in the partition: for instance, the three ordered partitions of the set {\"a\", \"b\"} are\n\nIn a strict weak ordering, the equivalence classes of incomparability give a set partition, in which the sets inherit a total ordering from their elements, giving rise to an ordered partition. In the other direction, any ordered partition gives rise to a strict weak ordering in which two elements are incomparable when they belong to the same set in the partition, and otherwise inherit the order of the sets that contain them.\n\nFor sets of sufficiently small cardinality, a third axiomatization is possible, based on real-valued functions. If \"X\" is any set and \"f\" a real-valued function on \"X\" then \"f\" induces a strict weak order on \"X\" by setting \"a\" < \"b\" if and only if \"f\"(\"a\") < \"f\"(\"b\"). The associated total preorder is given by setting \"a\"formula_14\"b\" if and only if \"f\"(\"a\") ≤ \"f\"(\"b\"),\nand the associated equivalence by setting \"a\"formula_15\"b\" if and only if \"f\"(\"a\") = \"f\"(\"b\").\n\nThe relations do not change when \"f\" is replaced by \"g\"  \"f\" (composite function), where \"g\" is a strictly increasing real-valued function defined on at least the range of \"f\". Thus e.g. a utility function defines a preference relation. In this context, weak orderings are also known as preferential arrangements.\n\nIf \"X\" is finite or countable, every weak order on \"X\" can be represented by a function in this way. However, there exist strict weak orders that have no corresponding real function. For example, there is no such function for the lexicographic order on R. Thus, while in most preference relation models the relation defines a utility function up to order-preserving transformations, there is no such function for lexicographic preferences.\n\nMore generally, if \"X\" is a set, and \"Y\" is a set with a strict weak ordering \"<\", and \"f\" a function from \"X\" to \"Y\", then \"f\" induces a strict weak ordering on \"X\" by setting \"a\" < \"b\" if and only if \"f\"(\"a\") < \"f\"(\"b\"). As before, the associated total preorder is given by setting \"a\"formula_14\"b\" if and only if \"f\"(\"a\")formula_14\"f\"(\"b\"), and the associated equivalence by setting \"a\"formula_15\"b\" if and only if \"f\"(\"a\")formula_15\"f\"(\"b\"). It is not assumed here that \"f\" is an injective function, so a class of two equivalent elements on \"Y\" may induce a larger class of equivalent elements on \"X\". Also, \"f\" is not assumed to be an surjective function, so a class of equivalent elements on \"Y\" may induce a smaller or empty class on \"X\". However, the function \"f\" induces an injective function that maps the partition on \"X\" to that on \"Y\". Thus, in the case of finite partitions, the number of classes in \"X\" is less than or equal to the number of classes on \"Y\".\n\nSemiorders generalize strict weak orderings, but do not assume transitivity of incomparability. A strict weak order that is trichotomous is called a strict total order. The total preorder which is the inverse of its complement is in this case a total order.\n\nFor a strict weak order \"<\" another associated reflexive relation is its reflexive closure, a (non-strict) partial order \"≤\". The two associated reflexive relations differ with regard to different \"a\" and \"b\" for which neither \"a\" < \"b\" nor \"b\" < \"a\": in the total preorder corresponding to a strict weak order we get \"a\" formula_1 \"b\" and \"b\" formula_1 \"a\", while in the partial order given by the reflexive closure we get neither \"a\" ≤ \"b\" nor \"b\" ≤ \"a\". For strict total orders these two associated reflexive relations are the same: the corresponding (non-strict) total order. The reflexive closure of a strict weak ordering is a type of series-parallel partial order.\n\nThe number of distinct weak orders (represented either as strict weak orders or as total preorders) on an \"n\"-element set is given by the following sequence :\nThese numbers are also called the Fubini numbers or ordered Bell numbers.\n\nFor example, for a set of three labeled items, there is one weak order in which all three items are tied. There are three ways of partitioning the items into one singleton set and one group of two tied items, and each of these partitions gives two weak orders (one in which the singleton is smaller than the group of two, and one in which this ordering is reversed), giving six weak orders of this type. And there is a single way of partitioning the set into three singletons, which can be totally ordered in six different ways. Thus, altogether, there are 13 different weak orders on three items.\n\nUnlike for partial orders, the family of weak orderings on a given finite set is not in general connected by moves that add or remove a single order relation to a given ordering. For instance, for three elements, the ordering in which all three elements are tied differs by at least two pairs from any other weak ordering on the same set, in either the strict weak ordering or total preorder axiomatizations. However, a different kind of move is possible, in which the weak orderings on a set are more highly connected. Define a \"dichotomy\" to be a weak ordering with two equivalence classes, and define a dichotomy to be \"compatible\" with a given weak ordering if every two elements that are related in the ordering are either related in the same way or tied in the dichotomy. Alternatively, a dichotomy may be defined as a Dedekind cut for a weak ordering. Then a weak ordering may be characterized by its set of compatible dichotomies. For a finite set of labeled items, every pair of weak orderings may be connected to each other by a sequence of moves that add or remove one dichotomy at a time to or from this set of dichotomies. Moreover, the undirected graph that has the weak orderings as its vertices, and these moves as its edges, forms a partial cube.\n\nGeometrically, the total orderings of a given finite set may be represented as the vertices of a permutohedron, and the dichotomies on this same set as the facets of the permutohedron. In this geometric representation, the weak orderings on the set correspond to the faces of all different dimensions of the permutohedron (including the permutohedron itself, but not the empty set, as a face). The codimension of a face gives the number of equivalence classes in the corresponding weak ordering. In this geometric representation the partial cube of moves on weak orderings is the graph describing the covering relation of the face lattice of the permutohedron.\n\nFor instance, for \"n\" = 3, the permutohedron on three elements is just a regular hexagon. The face lattice of the hexagon (again, including the hexagon itself as a face, but not including the empty set) has thirteen elements: one hexagon, six edges, and six vertices, corresponding to the one completely tied weak ordering, six weak orderings with one tie, and six total orderings. The graph of moves on these 13 weak orderings is shown in the figure.\n\nAs mentioned above, weak orders have applications in utility theory. In linear programming and other types of combinatorial optimization problem, the prioritization of solutions or of bases is often given by a weak order, determined by a real-valued objective function; the phenomenon of ties in these orderings is called \"degeneracy\", and several types of tie-breaking rule have been used to refine this weak ordering into a total ordering in order to prevent problems caused by degeneracy.\n\nWeak orders have also been used in computer science, in partition refinement based algorithms for lexicographic breadth-first search and lexicographic topological ordering. In these algorithms, a weak ordering on the vertices of a graph (represented as a family of sets that partition the vertices, together with a doubly linked list providing a total order on the sets) is gradually refined over the course of the algorithm, eventually producing a total ordering that is the output of the algorithm.\n\nIn the Standard Library for the C++ programming language, the set and multiset data types sort their input by a comparison function that is specified at the time of template instantiation, and that is assumed to implement a strict weak ordering.\n"}
