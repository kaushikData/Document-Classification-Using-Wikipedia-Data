{"id": "37231", "url": "https://en.wikipedia.org/wiki?curid=37231", "title": "13 (number)", "text": "13 (number)\n\n13 (thirteen) is the natural number following 12 and preceding 14.\n\nStrikingly folkloric aspects of the number 13 have been noted in various cultures around the world: one theory is that this is due to the cultures employing lunar-solar calendars (there are approximately 12.41 lunations per solar year, and hence 12 \"true months\" plus a smaller, and often portentous, thirteenth month). This can be witnessed, for example, in the \"Twelve Days of Christmas\" of Western European tradition.\n\nThe number 13 is:\n\nSince 5 + 12 = 13, (5, 12, 13) forms a Pythagorean triple.\n\nThere are 13 Archimedean solids, and a standard torus can be sliced into 13 pieces with just 3 plane cuts. There are also 13 different ways for the three fastest horses in a horse race to finish, allowing for ties, a fact that can be expressed mathematically by 13 being the third ordered Bell number.\n\n\nIn Germany, according to an old tradition, 13 (\"dreizehn\") -as the first compound number- was the first number written in digits; the numbers 0 (\"null\") through 12 (\"zwölf\") were spelt out. The \"Duden\" (the German standard dictionary) now calls this tradition (which was actually never written down as an official rule) outdated and no longer valid, but many writers still follow it. \n\nIn Shia Islam, 13 signifies the 13th day of the month of Rajab (the Lunar calendar), which is the birth of Imam Ali. 13 also is a total of 1 Prophet and 12 Imams in the Islamic School of Thought. However, in Sunni Islam, the number 13 bears no symbolic significance.\n\nThe apparitions of the Virgin of Fátima in 1917 were claimed to occur on the 13th day of six consecutive months.\n\nIn Catholic devotional practice, the number thirteen is also associated with Saint Anthony of Padua, since his feast day falls on June 13. A traditional devotion called the Thirteen Tuesdays of St. Anthony involves praying to the saint every Tuesday over a period of thirteen weeks. Another devotion, St. Anthony's Chaplet, consists of thirteen decades of three beads each.\n\nAccording to famous Sakhi (Evidence) or story of Guru Nanak Dev Ji, when he was an accountant at a town of Sultanpur Lodhi, he was distributing groceries to people. When he gave groceries to the 13th person, he stopped because in Gurmukhi and Hindi the word 13 is called Terah, which means yours. And Guru Nanak Dev Ji kept saying, \"Yours, yours, yours...\" remembering God. People reported to the emperor that Guru Nanak Dev Ji was giving out free food to the people. When treasures were checked, there was more money than before.\n\nThe Vaisakhi, which commemorates the creation of \"Khalsa\" or pure Sikh was celebrated on April 13 for many years.\n\n\nThe number 13 had been considered sinister and wicked in ancient Iranian (Persian) civilization and Zoroastrianism. Since beginning of the Nourooz tradition, the 13th day of each new Iranian year is called Sizdah Be-dar, and this tradition is still alive among Iranian people both within Iran and abroad. Since Sizdah Be-dar is the 13th day of the year, it is considered a day when evil's power might cause difficulties for people. Therefore, people leave urban areas for one day and camp in the countryside. Even in the current post-1979 Revolution era, and despite the wishes of Islamic government, this tradition continues to be practiced by the majority of the population throughout Iran.\n\n\nThe number 13 is considered an unlucky number in some countries. The end of the Mayan calendar's 13th Baktun was superstitiously feared as a harbinger of the apocalyptic 2012 phenomenon. Fear of the number 13 has a specifically recognized phobia, Triskaidekaphobia, a word coined in 1911. The superstitious sufferers of triskaidekaphobia try to avoid bad luck by keeping away from anything numbered or labelled thirteen. As a result, companies and manufacturers use another way of numbering or labelling to avoid the number, with hotels and tall buildings being conspicuous examples (thirteenth floor). It is also considered unlucky to have thirteen guests at a table. Friday the 13th has been considered an unlucky day.\n\nThere are a number of theories as to why the number thirteen became associated with bad luck, but none of them have been accepted as likely.\n\nIn some countries, such as Italy, 13 is considered a lucky number. The expression \"fare tredici\" (\"to do 13\") means hit the jackpot. 17 is considered an unlucky number instead.\n\nColgate University also considers 13 a lucky number. They were founded in 1819 by 13 men with 13 dollars, 13 prayers and 13 articles. (To this day, members of the Colgate community consider the number 13 a good omen.) In fact, the campus address is 13 Oak Drive in Hamilton, New York, and the male \"a cappella\" group is called the Colgate 13.\n\nIn the Mayan Tzolk'in calendar, trecenas mark cycles of 13-day periods. The pyramids are also set up in 9 steps divided into 7 days and 6 nights, 13 days total.\n\nIn the standard 52-card deck of playing cards there are four suits, each of 13 ranks.\n\nIn a tarot card deck, XIII is the card of Death, usually picturing the Pale horse with its rider.\n\nA baker's dozen, devil's dozen, long dozen, or long measure is 13, one more than a standard dozen.\n\n\n\n\n\n"}
{"id": "9196273", "url": "https://en.wikipedia.org/wiki?curid=9196273", "title": "Andrew Jackson Libby", "text": "Andrew Jackson Libby\n\nAndrew Jackson \"Slipstick\" Libby is a fictional character featured in the \"Future History\" series of science fiction novels by Robert A. Heinlein. He is an enormously talented and intuitive mathematician, but received little formal education. His talent was first appreciated in the short story Misfit, where he helps guide an asteroid into the correct orbit after the guidance computer has failed.\n\nIn later stories he changes sex to become Elizabeth Andrew Jackson Libby.\n"}
{"id": "38764584", "url": "https://en.wikipedia.org/wiki?curid=38764584", "title": "Apeirogonal tiling", "text": "Apeirogonal tiling\n\nIn geometry, an apeirogonal tiling is a tessellation of the Euclidean plane, hyperbolic plane, or some other two-dimensional space by apeirogons. Tilings of this type include:\n\n"}
{"id": "1448859", "url": "https://en.wikipedia.org/wiki?curid=1448859", "title": "Art gallery problem", "text": "Art gallery problem\n\nThe art gallery problem or museum problem is a well-studied visibility problem in computational geometry. It originates from a real-world problem of guarding an art gallery with the minimum number of guards who together can observe the whole gallery. In the geometric version of the problem, the layout of the art gallery is represented by a simple polygon and each guard is represented by a point in the polygon. A set formula_1 of points is said to guard a polygon if, for every point formula_2 in the polygon, there is some formula_3 such that the line segment between formula_2 and formula_5 does not leave the polygon.\n\nThere are numerous variations of the original problem that are also referred to as the art gallery problem. In some versions guards are restricted to the perimeter, or even to the vertices of the polygon. Some versions require only the perimeter or a subset of the perimeter to be guarded.\n\nSolving the version in which guards must be placed on vertices and only vertices need to be guarded is equivalent to solving the dominating set problem on the visibility graph of the polygon.\n\nChvátal's art gallery theorem, named after Václav Chvátal, gives an upper bound on the minimal number of guards. It states that formula_6 guards are always sufficient and sometimes necessary to guard a simple polygon with formula_7 vertices.\n\nThe question about how many vertices/watchmen/guards were needed was posed to Chvátal by Victor Klee in 1973. Chvátal proved it shortly thereafter. Chvátal's proof was later simplified by Steve Fisk, via a 3-coloring argument.\n\nSteve Fisk's proof is so short and elegant that it was chosen for inclusion in \"Proofs from THE BOOK\".\nThe proof goes as follows:\n\nFirst, the polygon is triangulated (without adding extra vertices). It is known that the vertices of the resulting triangulation graph may be 3-colored. Clearly, under a 3-coloring, every triangle must have all three colors. The vertices with any one color form a valid guard set, because every triangle of the polygon is guarded by its vertex with that color. Since the three colors partition the \"n\" vertices of the polygon, the color with the fewest vertices defines a valid guard set with at most formula_8 guards.\n\nChvátal's upper bound remains valid if the restriction to guards at corners is loosened to guards at any point not exterior to the polygon.\n\nThere are a number of other generalizations and specializations of the original art-gallery theorem. For instance, for orthogonal polygons, those whose edges/walls meet at right angles, only formula_9 guards are needed. There are at least three distinct proofs of this result, none of them simple: by Kahn, Klawe, and Kleitman; by Lubiw; and by Sack and Toussaint.\n\nA related problem asks for the number of guards to cover the exterior of an arbitrary polygon (the \"Fortress Problem\"): formula_10 are sometimes necessary and always sufficient. In other words, the infinite exterior is more challenging to cover than the finite interior.\n\nIn decision problem versions of the art gallery problem, one is given as input both a polygon and a number \"k\", and must determine whether the polygon can be guarded with \"k\" or fewer guards. This problem is formula_11-complete, as is the version where the guards are restricted to the edges of the polygon. Furthermore, most of the other standard variations (such as restricting the guard locations to vertices) are NP-hard. \nRegarding approximation algorithms for the minimum number of guards, proved the problem to be APX-hard, implying that it is unlikely that any approximation ratio better than some fixed constant can be achieved by a polynomial time approximation algorithm. However, an algorithm achieving a constant approximation ratio was not known until very recently. showed that a logarithmic approximation may be achieved for the minimum number of vertex guards by discretizing the input polygon into convex subregions and then reducing the problem to a set cover problem. \nAs showed, the set system derived from an art gallery problem has bounded VC dimension, allowing the application of set cover algorithms based on ε-nets whose approximation ratio is the logarithm of the optimal number of guards rather than of the number of polygon vertices. \nFor unrestricted guards, the infinite number of potential guard positions makes the problem even more difficult. \nHowever by restricting the guards to lie on a fine grid, a more complicated logarithmic approximation algorithm can be derived under some mild extra assumptions, as shown by . \nHowever, efficient algorithms are known for finding a set of at most formula_6 vertex guards, matching Chvátal's upper bound.\n\nFor simple polygons that do not contain holes, the existence of a constant factor approximation algorithm for vertex and edge guards was conjectured by Ghosh. Ghosh's conjecture was initially shown to be true for vertex guards in two special sub-classes of simple polygons, viz. monotone polygons and polygons weakly visible from an edge. presented an approximation algorithm that computes in polynomial time a vertex guard set for a monotone polygon such that the size of the guard set is at most 30 times the optimal number of vertex guards. \nSubsequently, claimed to have settled the conjecture completely by presenting constant factor approximation algorithms for guarding general simple polygons using vertex guards and edge guards. \n\nFor vertex guarding the subclass of simple polygons that are weakly visible from an edge, a polynomial-time approximation scheme (PTAS) was recently proposed by . \n\nAn exact algorithm was proposed by for vertex guards. The authors conducted extensive computational experiments with several classes of polygons showing that optimal solutions can be found in relatively small computation times even for instances associated to thousands of vertices. The input data and the optimal solutions for these instances are available for download.\n\nIf a museum is represented in three dimensions as a polyhedron, then putting a guard at each vertex will not ensure that all of the museum is under observation. Although all of the surface of the polyhedron would be surveyed, for some polyhedra there are points in the interior which might not be under surveillance.\n\n\n"}
{"id": "58129622", "url": "https://en.wikipedia.org/wiki?curid=58129622", "title": "Band model", "text": "Band model\n\nIn geometry, the band model is a conformal model of the hyperbolic plane. The band model employs a portion of the Euclidean plane between two parallel lines. Distance is preserved along one line through the middle of the band. Assuming the band is given by formula_1, the metric is given by formula_2.\n\nGeodesics include the line along the middle of the band, and any open line segment perpendicular to boundaries of the band connecting the sides of the band. All geodesics have ends with either are orthogonal to the boundaries of the band or which approach formula_3. Lines parallel to the boundaries of the band within the band are hypercycles whose centers are the line through the middle of the band.\n\n"}
{"id": "2060183", "url": "https://en.wikipedia.org/wiki?curid=2060183", "title": "Barnes G-function", "text": "Barnes G-function\n\nIn mathematics, the Barnes G-function \"G\"(\"z\") is a function that is an extension of superfactorials to the complex numbers. It is related to the Gamma function, the K-function and the Glaisher–Kinkelin constant, and was named after mathematician Ernest William Barnes. Up to elementary factors, it is a special case of the double gamma function.\n\nFormally, the Barnes \"G\"-function is defined in the following Weierstrass product form:\n\nwhere formula_2 is the Euler–Mascheroni constant, exp(\"x\") = \"e\", and ∏ is capital pi notation.\n\nThe Barnes \"G\"-function satisfies the functional equation\n\nwith normalisation \"G\"(1) = 1. Note the similarity between the functional equation of the Barnes G-function and that of the Euler Gamma function:\n\nThe functional equation implies that \"G\" takes the following values at integer arguments:\n\nand thus\n\nwhere formula_8 denotes the Gamma function and \"K\" denotes the K-function. The functional equation uniquely defines the G function if the convexity condition: formula_9 is added.\n\nThe difference equation for the G function, in conjunction with the functional equation for the Gamma function, can be used to obtain the following reflection formula for the Barnes G function (originally proved by Hermann Kinkelin):\n\nThe logtangent integral on the right-hand side can be evaluated in terms of the Clausen function (of order 2), as is shown below:\n\nThe proof of this result hinges on the following evaluation of the cotangent integral: introducing the notation formula_12 for the logtangent integral, and using the fact that formula_13, an integration by parts gives\n\nPerforming the integral substitution formula_15 gives\n\nThe Clausen function – of second order – has the integral representation\n\nHowever, within the interval formula_18, the absolute value sign within the integrand can be omitted, since within the range the 'half-sine' function in the integral is strictly positive, and strictly non-zero. Comparing this definition with the result above for the logtangent integral, the following relation clearly holds:\n\nThus, after a slight rearrangement of terms, the proof is complete:\n\nUsing the relation formula_21 and dividing the reflection formula by a factor of formula_22 gives the equivalent form:\n\nRef: see Adamchik below for an equivalent form of the reflection formula, but with a different proof.\n\nReplacing z with (1/2) − \"z\"\" in the previous reflection formula gives, after some simplification, the equivalent formula shown below (involving Bernoulli polynomials):\n\nBy Taylor's theorem, and considering the logarithmic derivatives of the Barnes function, the following series expansion can be obtained:\n\nIt is valid for formula_27. Here, formula_28 is the Riemann Zeta function:\n\nExponentiating both sides of the Taylor expansion gives:\n\nComparing this with the Weierstrass product form of the Barnes function gives the following relation:\n\nLike the Gamma function, the G-function also has a multiplication formula:\n\nwhere formula_33 is a constant given by:\n\nHere formula_35 is the derivative of the Riemann zeta function and formula_36 is the Glaisher–Kinkelin constant.\n\nThe logarithm of \"G\"(\"z\" + 1) has the following asymptotic expansion, as established by Barnes:\n\nHere the formula_38 are the Bernoulli numbers and formula_36 is the Glaisher–Kinkelin constant. (Note that somewhat confusingly at the time of Barnes the Bernoulli number formula_40 would have been written as formula_41, but this convention is no longer current.) This expansion is valid for formula_42 in any sector not containing the negative real axis with formula_43 large.\n\nThe parametric Loggamma can be evaluated in terms of the Barnes G-function (Ref: this result is found in Adamchik below, but stated without proof):\n\nThe proof is somewhat indirect, and involves first considering the logarithmic difference of the Gamma function and Barnes G-function:\n\nwhere\n\nand formula_47 is the Euler–Mascheroni constant.\n\nTaking the logarithm of the Weierstrass product forms of the Barnes function and Gamma function gives:\n\nA little simplification and re-ordering of terms gives the series expansion:\n\nFinally, take the logarithm of the Weierstrass product form of the Gamma function, and integrate over the interval formula_53 to obtain:\n\nEquating the two evaluations completes the proof:\n\n"}
{"id": "151001", "url": "https://en.wikipedia.org/wiki?curid=151001", "title": "C-symmetry", "text": "C-symmetry\n\nCharge conjugation is a transformation that switches all particles with their corresponding antiparticles, and thus changes the sign of all charges: not only electric charge but also the charges relevant to other forces. In physics, C-symmetry means the symmetry of physical laws under a charge-conjugation transformation. Electromagnetism, gravity and the strong interaction all obey C-symmetry, but weak interactions violate C-symmetry.\n\nThe laws of electromagnetism (both classical and quantum) are invariant under this transformation: if each charge \"q\" were to be replaced with a charge −\"q\", and thus the directions of the electric and magnetic fields were reversed, the dynamics would preserve the same form. In the language of quantum field theory, charge conjugation transforms:\n\n\nNotice that these transformations do not alter the chirality of particles. A left-handed neutrino would be taken by charge conjugation into a left-handed antineutrino, which does not interact in the Standard Model. This property is what is meant by the \"maximal violation\" of C-symmetry in the weak interaction.\n\nIt was believed for some time that C-symmetry could be combined with the parity-inversion transformation (see P-symmetry) to preserve a combined CP-symmetry. However, violations of this symmetry have been identified in the weak interactions (particularly in the kaons and B mesons). In the Standard Model, this CP violation is due to a single phase in the CKM matrix. If CP is combined with time reversal (T-symmetry), the resulting CPT-symmetry can be shown using only the Wightman axioms to be universally obeyed.\n\nTo give an example, take two real scalar fields, \"φ\" and \"χ\". Suppose both fields have even C-parity (even C-parity refers to even symmetry under charge conjugation e.g., formula_4, as opposed to odd C-parity which refers to antisymmetry under charge conjugation, e.g., formula_5). \n\nDefine formula_6. Now, and have even \"C\"-parities, and the imaginary number \"i\" has an odd \"C\"-parity (\"C\" is anti-unitary). Under \"C\", \"ψ\" goes to \"ψ\".\n\nIn other models, it is also possible for both \"φ\" and \"χ\" to have odd C-parities.\n\n"}
{"id": "4557120", "url": "https://en.wikipedia.org/wiki?curid=4557120", "title": "Centered tree", "text": "Centered tree\n\nIn discrete mathematics, a centered tree is a tree with only one center, and a bicentered tree is a tree with two centers.\n\nGiven a graph, the eccentricity of a vertex \"v\" is defined as the greatest distance from \"v\" to any other vertex. A \"center\" of a graph is a vertex with minimal eccentricity. A graph can have an arbitrary number of centers. However, has proved that for trees, there are only two possibilities:\nA proof of this fact is given, for example, by Knuth.\n\n\n"}
{"id": "2206157", "url": "https://en.wikipedia.org/wiki?curid=2206157", "title": "Concurrent lines", "text": "Concurrent lines\n\nIn geometry, three or more lines in a plane or higher-dimensional space are said to be concurrent if they intersect at a single point.\n\nIn a triangle, four basic types of sets of concurrent lines are altitudes, angle bisectors, medians, and perpendicular bisectors:\n\n\nOther sets of lines associated with a triangle are concurrent as well. For example:\n\n\n\n\n\n\n\n\n\nAccording to the Rouché–Capelli theorem, a system of equations is consistent if and only if the rank of the coefficient matrix is equal to the rank of the augmented matrix (the coefficient matrix augmented with a column of intercept terms), and the system has a \"unique\" solution if and only if that common rank equals the number of variables. Thus with two variables the \"k\" lines in the plane, associated with a set of \"k\" equations, are concurrent if and only if the rank of the \"k\" × 2 coefficient matrix and the rank of the \"k\" × 3 augmented matrix are both 2. In that case only two of the \"k\" equations are independent, and the point of concurrency can be found by solving any two mutually independent equations simultaneously for the two variables.\n\nIn projective geometry, in two dimensions concurrency is the dual of collinearity; in three dimensions, concurrency is the dual of coplanarity.\n\n"}
{"id": "2334769", "url": "https://en.wikipedia.org/wiki?curid=2334769", "title": "Congruent number", "text": "Congruent number\n\nIn mathematics, a congruent number is a positive integer that is the area of a right triangle with three rational number sides. A more general definition includes all positive rational numbers with this property.\n\nThe sequence of integer congruent numbers starts with\n\nFor example, 5 is a congruent number because it is the area of a (20/3, 3/2, 41/6) triangle. Similarly, 6 is a congruent number because it is the area of a (3,4,5) triangle. 3 is not a congruent number.\n\nIf is a congruent number then is also a congruent number for any natural number (just by multiplying each side of the triangle by ), and vice versa. This leads to the observation that whether a nonzero rational number is a congruent number depends only on its residue in the group\n\nEvery residue class in this group contains exactly one square-free integer, and it is common, therefore, only to consider square-free positive integers, when speaking about congruent numbers.\n\nThe question of determining whether a given rational number is a congruent number is called the congruent number problem. This problem has not (as of 2016) been brought to a successful resolution. Tunnell's theorem provides an easily testable criterion for determining whether a number is congruent; but his result relies on the Birch and Swinnerton-Dyer conjecture, which is still unproven.\n\nFermat's right triangle theorem, named after Pierre de Fermat, states that no square number can be a congruent number. However, in the form that every congruum (the difference between consecutive elements in an arithmetic progression of three squares) is non-square, it was already known (without proof) to Fibonacci. Every congruum is a congruent number, and every congruent number is a product of a congruum and the square of a rational number. However, determining whether a number is a congruum is much easier than determining whether it is congruent, because there is a parameterized formula for congrua for which only finitely many parameter values need to be tested.\n\nThe question of whether a given number is congruent turns out to be equivalent to the condition that a certain elliptic curve has positive rank. An alternative approach to the idea is presented below (as can essentially also be found in the introduction to Tunnell's paper).\n\nSuppose , , are numbers (not necessarily positive or rational) which satisfy the following two equations:\n\nThen set and\nA calculation shows\nand is not 0 (if then , so , but is nonzero, a contradiction).\n\nConversely, if and are numbers which satisfy the above equation and is not 0, set\n, and . A calculation shows these three numbers\nsatisfy the two equations for , , and above.\n\nThese two correspondences between (,) and (,) are inverses of each other, so\nwe have a one-to-one correspondence between any solution of the two equations in\n, , and and any solution of the equation in and with nonzero. In particular,\nfrom the formulas in the two correspondences, for rational we see that , , and are\nrational if and only if the corresponding and are rational, and vice versa.\n(We also have that , , and are all positive if and only if and are all positive;\nnotice from the equation \nthat if and are positive then must be positive, so the formula for\n\nThus a positive rational number is congruent if and only if the equation\nIt can be shown (as a nice application of Dirichlet's theorem on primes in arithmetic progression)\nthat the only torsion points on this elliptic curve are those with equal to 0, hence the\nexistence of a rational point with nonzero is equivalent to saying the elliptic curve has positive rank.\n\nMuch work has been done classifying congruent numbers.\n\nFor example, it is known that for a prime number , the following holds:\n\nIt is also known that in each of the congruence classes , for any given there are infinitely many square-free congruent numbers with prime factors.\n\n\n"}
{"id": "12218154", "url": "https://en.wikipedia.org/wiki?curid=12218154", "title": "Cylindrical algebraic decomposition", "text": "Cylindrical algebraic decomposition\n\nIn mathematics, cylindrical algebraic decomposition (CAD) is a notion, and an algorithm to compute it, which are fundamental for computer algebra and real algebraic geometry. Given a set \"S\" of polynomials in R, a cylindrical algebraic decomposition is a decomposition of R into connected semialgebraic sets called \"cells\", on which each polynomial has constant sign, either +, − or 0. To be \"cylindrical\", this decomposition must satisfy the following condition: If 1 ≤ \"k\" < \"n\" and π is the projection from R onto R consisting in removing the \"k\" last coordinates, then for every pair of cells \"c\" and \"d\", one has either π(\"c\") = π(\"d\") or π(\"c\") ∩ π(\"d\") = ∅. This implies that the images by π of the cells define a cylindrical decomposition of R.\n\nThe notion was introduced by George E. Collins in 1975, together with an algorithm for computing it.\n\nCollins' algorithm has a computational complexity that is double exponential in \"n\". This is an upper bound, which is reached on most entries. There are also examples for which the minimal number of cells is doubly exponential, showing that every general algorithm for cylindrical algebraic decomposition has a double exponential complexity.\n\nCAD provides an effective version of quantifier elimination over the reals, which has a much better computational complexity than that which results from the original proof of Tarski–Seidenberg theorem. It is efficient enough to be implemented on a computer. It is one of the most important algorithms of computational real algebraic geometry. Searching to improve Collins algorithm, or to provide algorithms that have a better complexity for subproblems of general interest, is an active field of research.\n\n"}
{"id": "1300778", "url": "https://en.wikipedia.org/wiki?curid=1300778", "title": "DeWitt notation", "text": "DeWitt notation\n\nPhysics often deals with classical models where the dynamical variables are a collection of functions \n\nIn the DeWitt notation (named after theoretical physicist Bryce DeWitt), φ(\"x\") is written as φ where \"i\" is now understood as an index covering both \"α\" and \"x\".\n\nSo, given a smooth functional \"A\", \"A\" stands for the functional derivative\n\nas a functional of \"φ\". In other words, a \"1-form\" field over the infinite dimensional \"functional manifold\".\n\nIn integrals, the Einstein summation convention is used. Alternatively,\n"}
{"id": "20145865", "url": "https://en.wikipedia.org/wiki?curid=20145865", "title": "Decoherence-free subspaces", "text": "Decoherence-free subspaces\n\nA decoherence-free subspace (DFS) is a subspace of a system's Hilbert space that is invariant to non-unitary dynamics. Alternatively stated, they are a small section of the system Hilbert space where the system is decoupled from the environment and thus its evolution is completely unitary. DFSs can also be characterized as a special class of quantum error correcting codes. In this representation they are \"passive\" error-preventing codes since these subspaces are encoded with information that (possibly) won't require any \"active\" stabilization methods. These subspaces prevent destructive environmental interactions by isolating quantum information. As such, they are an important subject in quantum computing, where (coherent) control of quantum systems is the desired goal. Decoherence creates problems in this regard by causing loss of coherence between the quantum states of a system and therefore the decay of their interference terms, thus leading to loss of information from the (open) quantum system to the surrounding environment. Since quantum computers cannot be isolated from their environment (i.e. we cannot have a truly isolated quantum system in the real world) and information can be lost, the study of DFSs is important for the implementation of quantum computers into the real world.\n\nThe study of DFSs began with a search for structured methods to avoid decoherence in the subject of quantum information processing (QIP). The methods involved attempts to identify particular states which have the potential of being unchanged by certain decohering processes (i.e. certain interactions with the environment). These studies started with observations made by G.M. Palma, K-A Suominen, and A.K. Ekert, who studied the consequences of pure dephasing on two qubits that have the same interaction with the environment. They found that two such qubits do not decohere. Originally the term \"sub-decoherence\" was used by Palma to describe this situation. Noteworthy is also independent work by Martin Plenio, Vlatko Vedral and Peter Knight who constructed an error correcting code with codewords that are invariant under a particular unitary time evolution in spontaneous emission.\n\nShortly afterwards, L-M Duan and G-C Guo also studied this phenomenon and reached the same conclusions as Palma, Suominen, and Ekert. However, Duan and Guo applied their own terminology, using \"coherence preserving states\" to describe states that do not decohere with dephasing. Duan and Guo furthered this idea of combining two qubits to preserve coherence against dephasing, to both collective dephasing and dissipation showing that decoherence is prevented in such a situation. This was shown by assuming knowledge of the system-environment coupling strength. However, such models were limited since they dealt with the decoherence processes of dephasing and dissipation solely. To deal with other types of decoherences, the previous models presented by Palma, Suominen, and Ekert, and Duan and Guo were cast into a more general setting by P. Zanardi and M. Rasetti. They expanded the existing mathematical framework to include more general system-environment interactions, such as collective decoherence-the same decoherence process acting on all the states of a quantum system and general Hamiltonians. Their analysis gave the first formal and general circumstances for the existence of decoherence-free (DF) states, which did not rely upon knowing the system-environment coupling strength. Zanardi and Rasetti called these DF states \"error avoiding codes\". Subsequently, Daniel A. Lidar proposed the title \"decoherence-free subspace\" for the space in which these DF states exist. Lidar studied the strength of DF states against perturbations and discovered that the coherence prevalent in DF states can be upset by evolution of the system Hamiltonian. This observation discerned another prerequisite for the possible use of DF states for quantum computation. A thoroughly general requirement for the existence of DF states was obtained by Lidar, D. Bacon, and K.B. Whaley expressed in terms of the Kraus operator-sum representation (OSR). Later, A. Shabani and Lidar generalized the DFS framework relaxing the requirement that the initial state needs to be a DF-state and modified some known conditions for DFS.\n\nA subsequent development was made in generalizing the DFS picture when E. Knill, R. Laflamme, and L. Viola introduced the concept of a \"noiseless subsystem\". Knill extended to higher-dimensional irreducible representations of the algebra generating the dynamical symmetry in the system-environment interaction. Earlier work on DFSs described DF states as singlets, which are one-dimensional irreducible representations. This work proved to be successful, as a result of this analysis was the lowering of the number of qubits required to build a DFS under collective decoherence from four to three. The generalization from subspaces to subsystems formed a foundation for combining most known decoherence prevention and nulling strategies.\n\nConsider an \"N\"-dimensional quantum system \"S\" coupled to a bath \"B\" and described by the combined system-bath Hamiltonian as follows:\n\nwhere the interaction Hamiltonian formula_2 is given in the usual way as\n\nand where formula_4 act upon the system(bath) only, and formula_5 is the system(bath) Hamiltonian, and formula_6 is the identity operator acting on the system (bath).\nUnder these conditions, the dynamical evolution within formula_7, where formula_8 is the system Hilbert space, is completely unitary formula_9 (all possible bath states) if and only if:\n\n(i) formula_10\n\nformula_9 that span formula_12 and formula_13, the space of bounded system-bath operators on formula_14,\n\n(ii) the system and bath are not coupled at first (i.e. they can be represented as a product state),\n\n(iii) there is no \"leakage\" of states out of formula_12; that is, the system Hamiltonian formula_16 does not map the states formula_17 out of formula_12.\n\nIn other words, if the system begins in formula_12(i.e. the system and bath are initially decoupled) and the system Hamiltonian formula_16 leaves formula_21 invariant, then formula_12 is a DFS if and only if it satisfies (i).\n\nThese states are degenerate eigenkets of formula_23 and thus are distinguishable, hence preserving information in certain decohering processes. Any subspace of the system Hilbert space that satisfies the above conditions is a decoherence-free subspace. However, information can still \"leak\" out of this subspace if condition (iii) is not satisfied. Therefore, even if a DFS exists under the Hamiltonian conditions, there are still non-unitary actions that can act upon these subspaces and take states out of them into another subspace, which may or may not be a DFS, of the system Hilbert space.\n\nLet formula_24 be an N-dimensional DFS, where formula_8 is the system's (the quantum system alone) Hilbert space. The Kraus operators when written in terms of the N basis states that span formula_8 are given as:\n\nwhere formula_28 (formula_29 is the combined system-bath Hamiltonian), formula_30 acts on formula_24, and formula_32 is an arbitrary matrix that acts on formula_33 (the orthogonal complement to formula_12). Since formula_32 operates on formula_33, then it will not create decoherence in formula_12; however, it can (possibly) create decohering effects in formula_33. Consider the basis kets formula_39 which span formula_12 and, furthermore, they fulfill:\n\nformula_30 is an arbitrary unitary operator and may or may not be time-dependent, but it is independent of the indexing variable formula_43. The formula_44's are complex constants. Since formula_39 spans formula_12, then any pure state formula_47 can be written as a linear combination of these basis kets:\n\nThis state will be decoherence-free; this can be seen by considering the action of formula_32 on formula_50:\n\nTherefore, in terms of the density operator representation of formula_50, formula_53, the evolution of this state is:\n\nThe above expression says that formula_55 is a pure state and that its evolution is unitary, since formula_30 is unitary. Therefore, \"any\" state in formula_12 will not decohere since its evolution is governed by a unitary operator and so its dynamical evolution will be completely unitary. Thus formula_12 is a decoherence-free subspace.\nThe above argument can be generalized to an initial arbitrary mixed state as well.\n\nThis formulation makes use of the semigroup approach. The Lindblad decohering term determines when the dynamics of a quantum system will be unitary; in particular, when formula_59, where formula_60 is the density operator representation of the state of the system, the dynamics will be decoherence-free.\nLet formula_39 span formula_24, where formula_8 is the system's Hilbert space. Under the assumptions that:\n\n\na necessary and sufficient condition for formula_12 to be a DFS is formula_65:\n\nThe above expression states that \"all\" basis states formula_67 are degenerate eigenstates of the error generators formula_68 As such, their respective coherence terms do not decohere. Thus states within formula_12 will remain mutually distinguishable after a decohering process since their respective eigenvalues are degenerate and hence identifiable after action under the error generators.\n\nDFSs can be thought of as \"encoding\" information through its set of states. To see this, consider a \"d\"-dimensional open quantum system that is prepared in the state formula_70-a non-negative (i.e. its eigenvalues are positive), trace-preserving formula_71, formula_72 density operator that belongs to the system's Hilbert–Schmidt space, the space of bounded operators on formula_73 formula_74. Suppose that this density operator(state) is selected from a set of states formula_75, a DFS of formula_8 (the system's Hilbert space) and where formula_77.\nThis set of states is called a \"code\", because the states within this set \"encode\" particular kind of information; that is, the set \"S\" encodes information through its states. This information that is contained within formula_78 must be able to be accessed; since the information is encoded in the states in formula_78, these states must be distinguishable to some process, formula_80 say, that attempts to acquire the information. Therefore, for two states formula_81, the process formula_80 is \"information preserving\" for these states if the states formula_83 remain \"as\" distinguishable after the process as they were before it. Stated in a more general manner, a code formula_78 (or DFS) is preserved by a process formula_80 iff each pair of states formula_86 is as distinguishable after formula_80 is applied as they were before it was applied. A more practical description would be: formula_78 is preserved by a process formula_80 if and only if formula_90 and formula_91\n\nThis just says that formula_80 is a 1:1 trace-distance-preserving map on formula_78. In this picture DFSs are sets of states (codes rather) whose \"mutual distinguishability\" is unaffected by a process formula_80.\n\nSince DFSs can encode information through their sets of states, then they are secure against errors (decohering processes). In this way DFSs can be looked at as a special class of QECCs, where information is encoded into states which can be disturbed by an interaction with the environment but retrieved by some reversal process.\n\nConsider a code formula_96, which is a subspace of the system Hilbert space, with encoded information given by formula_97 (i.e. the \"codewords\"). This code can be implemented to protect against decoherence and thus prevent loss of information in a small section of the system's Hilbert space. The errors are caused by interaction of the system with the environment (bath) and are represented by the Kraus operators. After the system has interacted with the bath, the information contained within formula_98 must be able to be \"decoded\"; therefore, to retrieve this information a recovery operator formula_99 is introduced. So a QECC is a subspace formula_98 along with a set of recovery operators formula_101\n\nLet formula_98 be a QECC for the error operators represented by the Kraus operators formula_103, with recovery operators formula_101 Then formula_98 is a DFS if and only if upon restriction to formula_98, then formula_107, where formula_108 is the inverse of the system evolution operator.\n\nIn this picture of reversal of quantum operations, DFSs are a special instance of the more general QECCs whereupon restriction to a given a code, the recovery operators become proportional to the inverse of the system evolution operator, hence allowing for unitary evolution of the system.\n\nNotice that the subtle difference between these two formulations exists in the two words \"preserving\" and \"correcting\"; in the former case, error-\"prevention\" is the method used whereas in the latter case it is error-\"correction\". Thus the two formulations differ in that one is a \"passive\" method and the other is an \"active\" method.\n\nConsider a two-qubit Hilbert space, spanned by the basis qubits formula_109 which undergo collective dephasing. A random phase formula_110 will be created between these basis qubits; therefore, the qubits will transform in the following way:\n\nUnder this transformation the basis states formula_112 obtain the same phase factor formula_113. Thus in consideration of this, a state formula_50 can be encoded with this information (i.e. the phase factor) and thus evolve unitarily under this dephasing process, by defining the following encoded qubits:\n\nSince these are basis qubits, then any state can be written as a linear combination of these states; therefore,\n\nThis state will evolve under the dephasing process as:\n\nHowever, the \"overall\" phase for a quantum state is unobservable and, as such, is irrelevant in the description of the state. Therefore, formula_118 remains invariant under this dephasing process and hence the basis set formula_119 is a \"decoherence-free subspace\" of the 4-dimensional Hilbert space. Similarly, the subspaces formula_120 are also DFSs.\n\nConsider a quantum system with an N-dimensional system Hilbert space formula_121 that has a general subsystem decomposition formula_122 The subsystem formula_123 is a decoherence-free subsystem with respect to a system-environment coupling if every pure state in formula_123 remains unchanged with respect to this subsystem under the OSR evolution. This is true for any possible initial condition of the environment. To understand the difference between a decoherence-free \"subspace\" and a decoherence-free \"subsystem\", consider encoding a single qubit of information into a two-qubit system. This two-qubit system has a 4-dimensional Hilbert space; one method of encoding a single qubit into this space is by encoding information into a subspace that is spanned by two orthogonal qubits of the 4-dimensional Hilbert space. Suppose information is encoded in the orthogonal state formula_125 in the following way:\n\nThis shows that information has been encoded into a \"subspace\" of the two-qubit Hilbert space. Another way of encoding the same information is to encode \"only\" one of the qubits of the two qubits. Suppose the first qubit is encoded, then the state of the second qubit is completely arbitrary since:\n\nThis mapping is a \"one-to-many\" mapping from the one qubit encoding information to a two-qubit Hilbert space. Instead, if the mapping is to formula_50, then it is identical to a mapping from a qubit to a subspace of the two-qubit Hilbert space.\n\n"}
{"id": "1667059", "url": "https://en.wikipedia.org/wiki?curid=1667059", "title": "Diffusion process", "text": "Diffusion process\n\nIn probability theory and statistics, a diffusion process is a solution to a stochastic differential equation. It is a continuous-time Markov process with almost surely continuous sample paths. Brownian motion, reflected Brownian motion and Ornstein–Uhlenbeck processes are examples of diffusion processes.\n\nA sample path of a diffusion process models the trajectory of a particle embedded in a flowing fluid and subjected to random displacements due to collisions with other particles, which is called Brownian motion. The position of the particle is then random; its probability density function as a function of space and time is governed by an advection-diffusion equation.\n\nA \"diffusion process\" is a Markov process with continuous sample paths for which the Kolmogorov forward equation is the Fokker-Planck equation.\n\n"}
{"id": "18745015", "url": "https://en.wikipedia.org/wiki?curid=18745015", "title": "Fractal analysis", "text": "Fractal analysis\n\nFractal analysis is assessing fractal characteristics of data. It consists of several methods to assign a fractal dimension and other fractal characteristics to a dataset which may be a theoretical dataset or a pattern or signal extracted from phenomena including natural geometric objects, sound, market fluctuations, heart rates, frequency domain in Electroencephalography signals, digital images, molecular motion, networks, etc.\nFractal analysis is now widely used in all areas of science. An important limitation of fractal analysis is that arriving at an empirically determined fractal dimension does not necessarily prove that a pattern is fractal; rather, other essential characteristics have to be considered.\n\nSeveral types of fractal analysis are done, including box counting, lacunarity analysis, mass methods, and multifractal analysis. A common feature of all types of fractal analysis is the need for benchmark patterns against which to assess outputs. These can be acquired with various types of fractal generating software capable of generating benchmark patterns suitable for this purpose, which generally differ from software designed to render fractal art.\n\nApplications of fractal analysis include:\n\n\n\n\n\n"}
{"id": "1704824", "url": "https://en.wikipedia.org/wiki?curid=1704824", "title": "Fraction (mathematics)", "text": "Fraction (mathematics)\n\nA fraction (from Latin \"\", \"broken\") represents a part of a whole or, more generally, any number of equal parts. When spoken in everyday English, a fraction describes how many parts of a certain size there are, for example, one-half, eight-fifths, three-quarters. A \"common\", \"vulgar\", or \"simple\" fraction (examples: formula_1 and 17/3) consists of an integer numerator displayed above a line (or before a slash), and a non-zero integer denominator, displayed below (or after) that line.\nNumerators and denominators are also used in fractions that are not \"common\", including compound fractions, complex fractions, and mixed numerals.\n\nWe begin with positive common fractions, where the numerator and denominator are natural numbers. The numerator represents a number of equal parts, and the denominator indicates how many of those parts make up a unit or a whole. The denominator cannot be zero because zero parts can never make up a whole. For example, in the fraction 3/4, the numerator, 3, tells us that the fraction represents 3 equal parts, and the denominator, 4, tells us that 4 parts make up a whole. The picture to the right illustrates formula_2 or of a cake.\n\nA common fraction is a numeral which represents a rational number. That same number can also be represented as a decimal, a percent, or with a negative exponent. For example, 0.01, 1%, and 10 all equal the fraction 1/100. An integer such as the number 7 can be thought of as having an implicit denominator of one: 7 equals 7/1.\n\nOther uses for fractions are to represent ratios and division.\nThus the fraction is also used to represent the ratio 3:4 (the ratio of the part to the whole) and the division 3 ÷ 4 (three divided by four). The non-zero denominator in the case using a fraction to represent division is an example of the rule that division by zero is undefined.\n\nWe can also write negative fractions, which represent the opposite of a positive fraction. For example if represents a half dollar profit, then − represents a half dollar loss. Because of the rules of division of signed numbers, which require that, for example, negative divided by positive is negative, −, , , and − all represent the same fraction, negative one-half. Because negative divided by negative is positive, represents positive one-half.\n\nIn mathematics the set of all numbers that can be expressed in the form a/b, where a and b are integers and b is not zero, is called the set of rational numbers and is represented by the symbol Q, which stands for quotient. The test for a number being a rational number is that it can be written in that form (i.e., as a common fraction). However, the word \"fraction\" is also used to describe mathematical expressions that are not rational numbers, for example algebraic fractions (quotients of algebraic expressions), and expressions that contain irrational numbers, such as /2 (see square root of 2) and π/4 (see proof that π is irrational).\n\nIn a fraction, the number of equal parts being described is the numerator (from Latin ', \"counter\" or \"numberer\"), and the type or variety of the parts is the denominator (from Latin ', \"thing that names or designates\"). As an example, the fraction amounts to eight parts, each of which is of the type named \"fifth.\" In terms of division, the numerator corresponds to the dividend, and the denominator corresponds to the divisor.\n\nInformally, the numerator and denominator may be distinguished by placement alone but in formal contexts they are always separated by a fraction bar. The fraction bar may be horizontal (as in ), oblique (as in 1/5), or diagonal (as in ). These marks are respectively known as the horizontal bar, the slash (US) or stroke (UK), the division slash, and the fraction slash. In typography, horizontal fractions are also known as \"en\" or \"nut fractions\" and diagonal fractions as \"em fractions\", based on the width of a line they take up.\n\nThe denominators of English fractions are generally expressed as ordinal numbers, in the plural if the numerator is not one. (For example, and are both read as a number of \"fifths\".) Exceptions include the denominator 2, which is always read \"half\" or \"halves\", the denominator 4, which may be alternatively expressed as \"quarter\"/\"quarters\" or as \"fourth\"/\"fourths\", and the denominator 100, which may be alternatively expressed as \"hundredth\"/\"hundredths\" or \"percent\". When the denominator is 1, it may be expressed in terms of \"wholes\" but is more commonly ignored, with the numerator read out as a whole number. (For example, may be described as \"three wholes\" or as simply \"three\".) When the numerator is one, it may be omitted. (For example, \"a tenth\" or \"each quarter\".)\n\nThe entire fraction may be expressed as a single composition, in which case it is hyphenated, or as a number of fractions with a numerator of one, in which case they are not. (For example, \"two-fifths\" is the fraction and \"two fifths\" is the same fraction understood as 2 instances of .) Fractions should always be hyphenated when used as adjectives. Alternatively, a fraction may be described by reading it out as the numerator \"over\" the denominator, with the denominator expressed as a cardinal number. (For example, may also be expressed as \"three over one\".) The term \"over\" is used even in the case of solidus fractions, where the numbers are placed left and right of a slash mark. (For example, ½ may be read \"one-half\", \"one half\", or \"one over two\".) Fractions with large denominators that are \"not\" powers of ten are often rendered in this fashion (e.g., as \"one over one hundred seventeen\") while those with denominators divisible by ten are typically read in the normal ordinal fashion (e.g., as \"six-millionths\", \"six millionths\", or \"six one-millionths\").\n\nA simple fraction (also known as a common fraction or vulgar fraction) is a rational number written as \"a\"/\"b\" or formula_3, where \"a\" and \"b\" are both integers.\nAs with other fractions, the denominator (\"b\") cannot be zero. Examples include formula_1, formula_5, formula_6, formula_7, and 3/17.\n\"Simple fractions\" can be positive or negative, proper, or improper (see below). Compound fractions, complex fractions, mixed numerals, and decimals (see below) are not \"simple fractions\", though, unless irrational, they can be evaluated to a simple fraction.\n\nCommon fractions can be classified as either proper or improper. When the numerator and the denominator are both positive, the fraction is called proper if the numerator is less than the denominator, and improper otherwise. In general, a common fraction is said to be a proper fraction if the absolute value of the fraction is strictly less than one—that is, if the fraction is greater than −1 and less than 1.\nIt is said to be an improper fraction, or sometimes top-heavy fraction, if the absolute value of the fraction is greater than or equal to 1. Examples of proper fractions are 2/3, –3/4, and 4/9; examples of improper fractions are 9/4, –4/3, and 3/3.\n\nThe reciprocal of a fraction is another fraction with the numerator and denominator exchanged. The reciprocal of formula_10, for instance, is formula_11. The product of a fraction and its reciprocal is 1, hence the reciprocal is the multiplicative inverse of a fraction. The reciprocal of a proper fraction is improper, and the reciprocal of an improper fraction not equal to 1, that is, numerator and denominator are not equal, is a proper fraction.\n\nWhen the numerator and denominator of a fraction are equal (formula_12, for example), its value is 1, and the fraction therefore is improper. Its reciprocal also has the value 1, and is improper, too.\n\nAny integer can be written as a fraction with the number one as denominator. For example, 17 can be written as formula_13, where 1 is sometimes referred to as the \"invisible denominator\". Therefore, every fraction or integer, except for zero, has a reciprocal. The reciprocal of 17 is formula_14.\n\nA ratio is a relationship between two or more numbers that can be sometimes expressed as a fraction. Typically, a number of items are grouped and compared in a ratio, specifying numerically the relationship between each group. Ratios are expressed as \"group 1 to group 2 ... to group \"n\"\". For example, if a car lot had 12 vehicles, of which\n\n\nthen the ratio of red to white to yellow cars is 6 to 2 to 4. The ratio of yellow cars to white cars is 4 to 2 and may be expressed as 4:2 or 2:1.\n\nA ratio is often converted to a fraction when it is expressed as a ratio to the whole. In the above example, the ratio of yellow cars to all the cars on the lot is 4:12 or 1:3. We can convert these ratios to a fraction and say that 4/12 of the cars or ⅓ of the cars in the lot are yellow. Therefore, if a person randomly chose one car on the lot, then there is a one in three chance or probability that it would be yellow.\n\nA decimal fraction is a fraction whose denominator is not given explicitly, but is understood to be an integer power of ten. Decimal fractions are commonly expressed using decimal notation in which the implied denominator is determined by the number of digits to the right of a decimal separator, the appearance of which (e.g., a period, a raised period (•), a comma) depends on the locale (for examples, see decimal separator). Thus for 0.75 the numerator is 75 and the implied denominator is 10 to the second power, \"viz.\" 100, because there are two digits to the right of the decimal separator. In decimal numbers greater than 1 (such as 3.75), the fractional part of the number is expressed by the digits to the right of the decimal (with a value of 0.75 in this case). 3.75 can be written either as an improper fraction, 375/100, or as a mixed number, formula_15.\n\nDecimal fractions can also be expressed using scientific notation with negative exponents, such as , which represents 0.0000006023. The represents a denominator of . Dividing by moves the decimal point 7 places to the left.\n\nDecimal fractions with infinitely many digits to the right of the decimal separator represent an infinite series. For example, = 0.333... represents the infinite series 3/10 + 3/100 + 3/1000 + ... .\n\nAnother kind of fraction is the percentage (Latin \"per centum\" meaning \"per hundred\", represented by the symbol %), in which the implied denominator is always 100. Thus, 51% means 51/100. Percentages greater than 100 or less than zero are treated in the same way, e.g. 311% equals 311/100, and −27% equals −27/100.\n\nThe related concept of \"permille\" or \"parts per thousand\" (ppt) has an implied denominator of 1000, while the more general parts-per notation, as in 75 \"parts per million\" (ppm), means that the proportion is 75/1,000,000.\n\nWhether common fractions or decimal fractions are used is often a matter of taste and context. Common fractions are used most often when the denominator is relatively small. By mental calculation, it is easier to multiply 16 by 3/16 than to do the same calculation using the fraction's decimal equivalent (0.1875). And it is more accurate to multiply 15 by 1/3, for example, than it is to multiply 15 by any decimal approximation of one third. Monetary values are commonly expressed as decimal fractions with denominator 100, i.e., with two decimals, for example $3.75. However, as noted above, in pre-decimal British currency, shillings and pence were often given the form (but not the meaning) of a fraction, as, for example 3/6 (read \"three and six\") meaning 3 shillings and 6 pence, and having no relationship to the fraction 3/6.\n\nA mixed numeral (also called a \"mixed fraction\" or \"mixed number\") is a traditional denotation of the sum of a non-zero integer and a proper fraction (having the same sign). It is used primarily in measurement: formula_16 inches, for example. Scientific measurements almost invariably use decimal notation rather than mixed numbers. The sum is implied without the use of a visible operator such as the appropriate \"+\". For example, in referring to two entire cakes and three quarters of another cake, the numerals denoting the integer part and the fractional part of the cakes are written next to each other as formula_17 instead of the unambiguous notation formula_18 Negative mixed numerals, as in formula_19, are treated like formula_20 Any such sum of a \"whole\" plus a \"part\" can be converted to an improper fraction by applying the rules of adding unlike quantities.\n\nThis tradition is, formally, in conflict with the notation in algebra where adjacent factors denote a product, without an explicit infix operator. When two algebraic expressions are written next to each other, the operation of multiplication is implied by this general rule: formula_21 always means the product of formula_22 and formula_23, even if the value of formula_23 is a fraction. The expression formula_25 for example is not a mixed number, instead, multiplication is expressly required, where formula_26\n\nFor better readability, the multiplication is sometimes made explicit or parentheses are added. So, formula_27 may be written as\n\nAn improper fraction can be converted to a mixed number as follows:\n\n\nAn Egyptian fraction is the sum of distinct positive unit fractions, for example formula_33. This definition derives from the fact that the ancient Egyptians expressed all fractions except formula_1, formula_35 and formula_2 in this manner. Every positive rational number can be expanded as an Egyptian fraction. For example, formula_37 can be written as formula_38 Any positive rational number can be written as a sum of unit fractions in infinitely many ways. Two ways to write formula_39 are formula_40 and formula_41.\n\nBoth notions are outdated and nowadays used in no well defined manner, partly even taken synonymously for each other or for mixed numerals. They lost their meaning as technical terms and the attributes \"complex\" and \"compound\" tend to be used in their every day meaning of \"consisting of parts\".\nIn a complex fraction, either the numerator, or the denominator, or both, is a fraction or a mixed number, corresponding to division of fractions. For example, formula_42 and formula_43 are complex fractions. To reduce a complex fraction to a simple fraction, treat the longest fraction line as representing division. For example:\n\nIf, in a complex fraction, there is no unique way to tell which fraction lines takes precedence, then this expression is improperly formed, because of ambiguity. So 5/10/20/40 is not a valid mathematical expression, because of multiple possible interpretations, e.g. as\nA compound fraction is a fraction of a fraction, or any number of fractions connected with the word \"of\", corresponding to multiplication of fractions. To reduce a compound fraction to a simple fraction, just carry out the multiplication (see the section on multiplication). For example, formula_2 of formula_37 is a compound fraction, corresponding to formula_52. The terms compound fraction and complex fraction are closely related and sometimes one is used as a synonym for the other. (For example, the compound fraction formula_53 is equivalent to the complex fraction formula_54.)\n\nLike whole numbers, fractions obey the commutative, associative, and distributive laws, and the rule against division by zero.\n\nMultiplying the numerator and denominator of a fraction by the same (non-zero) number results in a fraction that is equivalent to the original fraction. This is true because for any non-zero number formula_55, the fraction formula_56. Therefore, multiplying by formula_57 is equivalent to multiplying by one, and any number multiplied by one has the same value as the original number. By way of an example, start with the fraction formula_1. When the numerator and denominator are both multiplied by 2, the result is formula_59, which has the same value (0.5) as formula_1. To picture this visually, imagine cutting a cake into four pieces; two of the pieces together (formula_59) make up half the cake (formula_1).\n\nDividing the numerator and denominator of a fraction by the same non-zero number will also yield an equivalent fraction. This is called reducing or simplifying the fraction. A simple fraction in which the numerator and denominator are coprime (that is, the only positive integer that goes into both the numerator and denominator evenly is 1) is said to be irreducible, in lowest terms, or in simplest terms. For example, formula_63 is not in lowest terms because both 3 and 9 can be exactly divided by 3. In contrast, formula_64 \"is\" in lowest terms—the only positive integer that goes into both 3 and 8 evenly is 1.\n\nUsing these rules, we can show that formula_65 = formula_1 = formula_67 = formula_68.\n\nA common fraction can be reduced to lowest terms by dividing both the numerator and denominator by their greatest common divisor. For example, as the greatest common divisor of 63 and 462 is 21, the fraction formula_69 can be reduced to lowest terms by dividing the numerator and denominator by 21:\n\nThe Euclidean algorithm gives a method for finding the greatest common divisor of any two positive integers.\n\nComparing fractions with the same positive denominator yields the same result as comparing the numerators:\n\nIf the equal denominators are negative, then the opposite result of comparing the numerators holds for the fractions:\n\nIf two positive fractions have the same numerator, then the fraction with the smaller denominator is the larger number. When a whole is divided into equal pieces, if fewer equal pieces are needed to make up the whole, then each piece must be larger. When two positive fractions have the same numerator, they represent the same number of parts, but in the fraction with the smaller denominator, the parts are larger.\n\nOne way to compare fractions with different numerators and denominators is to find a common denominator. To compare formula_3 and formula_77, these are converted to formula_78 and formula_79. Then \"bd\" is a common denominator and the numerators \"ad\" and \"bc\" can be compared. This modification of the two fractions is known as \"cross multiplying\", and it is not necessary to determine the value of the common denominator to compare fractions – one can just compare \"ad\" and \"bc\", without evaluating \"bd\", e.g., comparing formula_35 ? formula_1 gives formula_82.\n\nFor the more laborious question formula_83 ? formula_84 multiply top and bottom of each fraction by the denominator of the other fraction, to get a common denominator, yielding formula_85 ? formula_86. It is not necessary to calculate formula_87 – only the numerators need to be compared. Since 5×17 (= 85) is greater than 4×18 (= 72), the result of comparing is formula_88.\n\nBecause every negative number, including negative fractions, is less than zero, and every positive number, including positive fractions, is greater than zero, it follows that any negative fraction is less than any positive fraction. This allows, together with the above rules, to compare all possible fractions.\n\nThe first rule of addition is that only like quantities can be added; for example, various quantities of quarters. Unlike quantities, such as adding thirds to quarters, must first be converted to like quantities as described below:\nImagine a pocket containing two quarters, and another pocket containing three quarters; in total, there are five quarters. Since four quarters is equivalent to one (dollar), this can be represented as follows:\n\nTo add fractions containing unlike quantities (e.g. quarters and thirds), it is necessary to convert all amounts to like quantities. It is easy to work out the chosen type of fraction to convert to; simply multiply together the two denominators (bottom number) of each fraction. In case of an integer number apply the invisible denominator formula_90\n\nFor adding quarters to thirds, both types of fraction are converted to twelfths, thus:\n\nConsider adding the following two quantities:\nFirst, convert formula_93 into fifteenths by multiplying both the numerator and denominator by three: formula_94. Since formula_95 equals 1, multiplication by formula_95 does not change the value of the fraction.\n\nSecond, convert formula_97 into fifteenths by multiplying both the numerator and denominator by five: formula_98.\n\nNow it can be seen that:\n\nis equivalent to:\n\nThis method can be expressed algebraically:\n\nThis algebraic method always works, thereby guaranteeing that the sum of simple fractions is always again a simple fraction. However, if the single denominators contain a common factor, a smaller denominator than the product of these can be used. For example, when adding formula_2 and formula_103 the single denominators have a common factor formula_104 and therefore, instead of the denominator 24 (4 × 6), the halved denominator 12 may be used, not only reducing the denominator in the result, but also the factors in the numerator.\n\nThe smallest possible denominator is given by the least common multiple of the single denominators, which results from dividing the rote multiple by all common factors of the single denominators. This is called the least common denominator.\n\nThe process for subtracting fractions is, in essence, the same as that of adding them: find a common denominator, and change each fraction to an equivalent fraction with the chosen common denominator. The resulting fraction will have that denominator, and its numerator will be the result of subtracting the numerators of the original fractions. For instance,\n\nTo multiply fractions, multiply the numerators and multiply the denominators. Thus:\n\nTo explain the process, consider one third of one quarter. Using the example of a cake, if three small slices of equal size make up a quarter, and four quarters make up a whole, twelve of these small, equal slices make up a whole. Therefore, a third of a quarter is a twelfth. Now consider the numerators. The first fraction, two thirds, is twice as large as one third. Since one third of a quarter is one twelfth, two thirds of a quarter is two twelfth. The second fraction, three quarters, is three times as large as one quarter, so two thirds of three quarters is three times as large as two thirds of one quarter. Thus two thirds times three quarters is six twelfths.\n\nA short cut for multiplying fractions is called \"cancellation\". Effectively the answer is reduced to lowest terms during multiplication. For example:\n\nA two is a common factor in both the numerator of the left fraction and the denominator of the right and is divided out of both. Three is a common factor of the left denominator and right numerator and is divided out of both.\n\nSince a whole number can be rewritten as itself divided by 1, normal fraction multiplication rules can still apply.\n\nWhen multiplying mixed numbers, it is considered preferable to convert the mixed number into an improper fraction. For example:\n\nIn other words, formula_17 is the same as formula_112, making 11 quarters in total (because 2 cakes, each split into quarters makes 8 quarters total) and 33 quarters is formula_113, since 8 cakes, each made of quarters, is 32 quarters in total.\n\nTo divide a fraction by a whole number, you may either divide the numerator by the number, if it goes evenly into the numerator, or multiply the denominator by the number. For example, formula_114 equals formula_35 and also equals formula_116, which reduces to formula_35. To divide a number by a fraction, multiply that number by the reciprocal of that fraction. Thus, formula_118.\n\nTo change a common fraction to a decimal, do a long division of the decimal representations of the numerator by the denominator (this is idiomatically also phrased as \"divide the denominator into the numerator\"), and round the answer to the desired accuracy. For example, to change ¼ to a decimal, divide formula_119 by formula_72 (\"formula_72 into formula_119\"), to obtain formula_123. To change ⅓ to a decimal, divide formula_124 by formula_125 (\"formula_125 into formula_127\"), and stop when the desired accuracy is obtained, e.g., at formula_72 decimals with formula_129. Note that ¼ can be written exactly with two decimal digits, while the fraction ⅓ cannot be written exactly as a decimal with a finite number of digits.\nTo change a decimal to a fraction, write in the denominator a formula_130 followed by as many zeroes as there are digits to the right of the decimal point, and write in the numerator all the digits of the original decimal, just omitting the decimal point. Thus formula_131\n\nDecimal numbers, while arguably more useful to work with when performing calculations, sometimes lack the precision that common fractions have. Sometimes an infinite repeating decimal is required to reach the same precision. Thus, it is often useful to convert repeating decimals into fractions.\n\nThe preferred way to indicate a repeating decimal is to place a bar over the digits that repeat, for example 0. = 0.789789789… For repeating patterns where the repeating pattern begins immediately after the decimal point, a simple division of the pattern by the same number of nines as numbers it has will suffice. For example:\nIn case leading zeros precede the pattern, the nines are suffixed by the same number of trailing zeros:\nIn case a non-repeating set of decimals precede the pattern (such as 0.1523), we can write it as the sum of the non-repeating and repeating parts, respectively:\nThen, convert both parts to fractions, and add them using the methods described above:\n\nAlternatively, algebra can be used, such as below:\n\n\nIn addition to being of great practical importance, fractions are also studied by mathematicians, who check that the rules for fractions given above are consistent and reliable. Mathematicians define a fraction as an ordered pair formula_132 of integers formula_133 and formula_134 for which the operations addition, subtraction, multiplication, and division are defined as follows:\n\nThese definitions agree in every case with the definitions given above; only the notation is different. Alternatively, instead of defining subtraction and division as operations, the \"inverse\" fractions with respect to addition and multiplication might be defined as:\n\nFurthermore, the relation, specified as \n\nis an equivalence relation of fractions. Each fraction from one equivalence class may be considered as a representative for the whole class, and each whole class may be considered as one abstract fraction. This equivalence is preserved by the above defined operations, i.e., the results of operating on fractions are independent of the selection of representatives from their equivalence class. Formally, for addition of fractions\n\nand similarly for the other operations.\n\nIn case of fractions of integers the fractions with formula_132 coprime are often taken as uniquely determined representatives for their \"equivalent\" fractions, which are considered to be the \"same\" rational number. This way the fractions of integers make up the field of the rational numbers.\n\nMore generally, \"a\" and \"b\" may be elements of any integral domain \"R\", in which case a fraction is an element of the field of fractions of \"R\". For example, polynomials in one indeterminate, with coefficients from some integral domain \"D\", are themselves an integral domain, call it \"P\". So for \"a\" and \"b\" elements of \"P\", the generated \"field of fractions\" is the field of rational fractions (also known as the field of rational functions).\n\nAn algebraic fraction is the indicated quotient of two algebraic expressions. As with fractions of integers, the denominator of an algebraic fraction cannot be zero. Two examples of algebraic fractions are formula_145 and formula_146. Algebraic fractions are subject to the same field properties as arithmetic fractions.\n\nIf the numerator and the denominator are polynomials, as in formula_145, the algebraic fraction is called a rational fraction (or rational expression). An irrational fraction is one that is not rational, as, for example, one that contains the variable under a fractional exponent or root, as in formula_146.\n\nThe terminology used to describe algebraic fractions is similar to that used for ordinary fractions. For example, an algebraic fraction is in lowest terms if the only factors common to the numerator and the denominator are 1 and −1. An algebraic fraction whose numerator or denominator, or both, contain a fraction, such as formula_149, is called a complex fraction.\n\nThe field of rational numbers is the field of fractions of the integers, while the integers themselves are not a field but rather an integral domain. Similarly, the rational expressions are the field of fractions of polynomials. There are different integral domains of polynomials, depending on the integral domain the coefficients of the polynomials are from (e.g. from integers, real numbers, complex numbers, ...). Considering the field of fractions generated by polynomials with real coefficients, radical expressions such as formula_150 are also rational fractions, as is the transcendental expression formula_151, since all of formula_152 and formula_22 are (constant) polynomials over the \"reals\". These same expressions, however, would not be considered elements of the field of fractions generated by polynomials with \"integer\" coefficients. This specific field would contain just the formula_22 of the three polynomials above, or formula_155 as fraction, but no radical or transcendental expressions. \n\nThe term partial fraction is used when decomposing rational expressions into sums. The goal is to write the rational expression as the sum of other rational expressions with denominators of lesser degree. For example, the rational expression formula_156 can be rewritten as the sum of two fractions: formula_157. This is useful in many areas such as integral calculus and differential equations.\n\nA fraction may also contain radicals in the numerator and/or the denominator. If the denominator contains radicals, it can be helpful to rationalize it (compare Simplified form of a radical expression), especially if further operations, such as adding or comparing that fraction to another, are to be carried out. It is also more convenient if division is to be done manually. When the denominator is a monomial square root, it can be rationalized by multiplying both the top and the bottom of the fraction by the denominator:\n\nThe process of rationalization of binomial denominators involves multiplying the top and the bottom of a fraction by the conjugate of the denominator so that the denominator becomes a rational number. For example:\n"}
{"id": "5249765", "url": "https://en.wikipedia.org/wiki?curid=5249765", "title": "Frey curve", "text": "Frey curve\n\nIn mathematics, a Frey curve or Frey–Hellegouarch curve is the elliptic curve \nassociated with a (hypothetical) solution of Fermat's equation\nThe curve is named after Gerhard Frey.\n\n came up with the idea of associating solutions formula_3 of Fermat's equation with a completely different mathematical object: an elliptic curve.\nIf ℓ is an odd prime and \"a\", \"b\", and \"c\" are positive integers such that\n\nthen a corresponding Frey curve is an algebraic curve given by the equation\n\nor, equivalently\n\nThis is a nonsingular algebraic curve of genus one defined over Q, and its projective completion is an elliptic curve over Q.\n\n"}
{"id": "1056003", "url": "https://en.wikipedia.org/wiki?curid=1056003", "title": "Fundamental theorem of curves", "text": "Fundamental theorem of curves\n\nIn differential geometry, the fundamental theorem of space curves states that every regular curve in three-dimensional space, with non-zero curvature, has its shape (and size) completely determined by its curvature and torsion.\n\nA curve can be described, and thereby defined, by a pair of scalar fields: curvature formula_1 and torsion formula_2, both of which depend on some parameter which parametrizes the curve but which can ideally be the arc length of the curve. From just the curvature and torsion, the vector fields for the tangent, normal, and binormal vectors can be derived using the Frenet–Serret formulas. Then, integration of the tangent field (done numerically, if not analytically) yields the curve.\n\nIf a pair of curves are in different positions but have the same curvature and torsion, then they are congruent to each other.\n\n"}
{"id": "3298854", "url": "https://en.wikipedia.org/wiki?curid=3298854", "title": "Graph factorization", "text": "Graph factorization\n\nIn graph theory, a factor of a graph \"G\" is a spanning subgraph, i.e., a subgraph that has the same vertex set as \"G\". A k\"-factor of a graph is a spanning \"k\"-regular subgraph, and a k\"-factorization partitions the edges of the graph into disjoint \"k\"-factors. A graph \"G\" is said to be \"k\"-factorable if it admits a \"k\"-factorization. In particular, a 1-factor is a perfect matching, and a 1-factorization of a \"k\"-regular graph is an edge coloring with \"k\" colors. A 2-factor is a collection of cycles that spans all vertices of the graph.\n\nNot every 1-factorable graph (ie, a graph which has a 1-factorization), has to be a regular graph. Additionally, not all regular graphs are 1-factorable. A \"k\"-regular graph is 1-factorable if it has chromatic index \"k\"; examples of such graphs include:\nHowever, there are also \"k\"-regular graphs that have chromatic index \"k\" + 1, and these graphs are not 1-factorable; examples of such graphs include:\n\nA 1-factorization of a complete graph corresponds to pairings in a round-robin tournament. The 1-factorization of complete graphs is a special case of Baranyai's theorem concerning the 1-factorization of complete hypergraphs.\n\nOne method for constructing a 1-factorization of a complete graph on an even number of vertices involves placing all but one of the vertices on a circle, forming a regular polygon, with the remaining vertex at the center of the circle. With this arrangement of vertices, one way of constructing a 1-factor of the graph is to choose an edge \"e\" from the center to a single polygon vertex together with all possible edges that lie on lines perpendicular to \"e\". The 1-factors that can be constructed in this way form a 1-factorization of the graph.\n\nThe number of distinct 1-factorizations of \"K\", \"K\", \"K\", \"K\", ... is 1, 1, 6, 6240, 1225566720, 252282619805368320, 98758655816833727741338583040, ... .\n\nLet \"G\" be a \"k\"-regular graph with 2\"n\" nodes. If \"k\" is sufficiently large, it is known that \"G\" has to be 1-factorable:\nThe 1-factorization conjecture is a long-standing conjecture that states that \"k\" ≈ \"n\" is sufficient. In precise terms, the conjecture is:\nThe overfull conjecture implies the 1-factorization conjecture.\n\nA perfect pair from a 1-factorization is a pair of 1-factors whose union induces a Hamiltonian cycle.\n\nA perfect 1-factorization (P1F) of a graph is a 1-factorization having the property that every pair of 1-factors is a perfect pair. A perfect 1-factorization should not be confused with a perfect matching (also called a 1-factor).\n\nIn 1964, Anton Kotzig conjectured that every complete graph \"K\" where \"n\" ≥ 2 has a perfect 1-factorization. So far, it is known that the following graphs have a perfect 1-factorization:\n\nIf the complete graph \"K\" has a perfect 1-factorization, then the complete bipartite graph \"K\" also has a perfect 1-factorization.\n\nIf a graph is 2-factorable, then it has to be 2\"k\"-regular for some integer \"k\". Julius Petersen showed in 1891 that this necessary condition is also sufficient: any 2\"k\"-regular graph is 2-factorable.\n\nIf a connected graph is 2\"k\"-regular and has an even number of edges it may also be \"k\"-factored, by choosing each of the two factors to be an alternating subset of the edges of an Euler tour. This applies only to connected graphs; disconnected counterexamples include disjoint unions of odd cycles, or of copies of \"K\".\n\nThe Oberwolfach problem concerns the existence of 2-factorizations of complete graphs into isomorphic subgraphs. It asks for which subgraphs this is possible. This is known when the subgraph is connected (in which case it is a Hamiltonian cycle and this special case is the problem of Hamiltonian decomposition) but the general case remains unsolved.\n"}
{"id": "1605389", "url": "https://en.wikipedia.org/wiki?curid=1605389", "title": "Infinity symbol", "text": "Infinity symbol\n\nThe infinity symbol (sometimes called the lemniscate) is a mathematical symbol representing the concept of infinity.\n\nThe shape of a sideways figure eight has a long pedigree; for instance, it appears in the cross of Saint Boniface, wrapped around the bars of a Latin cross. However, John Wallis is credited with introducing the infinity symbol with its mathematical meaning in 1655, in his \"De sectionibus conicis\".\nWallis did not explain his choice of this symbol, but it has been conjectured to be a variant form of a Roman numeral for 1,000 (originally CIƆ, also CƆ), which was sometimes used to mean \"many\", or of the Greek letter ω (omega), the last letter in the Greek alphabet.\nLeonhard Euler used an open variant of the symbol in order to denote \"absolutus infinitus\". Euler freely performed various operations on infinity, such as taking its logarithm. This symbol is not used anymore, and is not encoded as a separate character in Unicode.\n\nIn mathematics, the infinity symbol is used more often to represent a potential infinity, rather than to represent an actually infinite quantity such as the ordinal numbers and cardinal numbers (which use other notations). For instance, in the mathematical notation for summations and limits such as\nthe infinity sign is conventionally interpreted as meaning that the variable grows arbitrarily large (towards infinity) rather than actually taking an infinite value.\n\nThe infinity symbol may also be used to represent a point at infinity, especially when there is only one such point under consideration. This usage includes, for instance,\nthe infinite point of a projective line,\nand the point added to a topological space formula_2 to form its one-point compactification formula_3.\n\nIn areas other than mathematics, the infinity symbol may take on other related meanings; for instance, it has been used in bookbinding to indicate that a book is printed on acid-free paper and will therefore be long-lasting.\n\nIn modern mysticism, the infinity symbol has become identified with a variation of the ouroboros, an ancient image of a snake eating its own tail that has also come to symbolize the infinite, and the ouroboros is sometimes drawn in figure-eight form to reflect this identification, rather than in its more traditional circular form.\n\nIn the works of Vladimir Nabokov, including \"The Gift\" and \"Pale Fire\", the figure-eight shape is used symbolically to refer to the Möbius strip and the infinite, for instance in these books' descriptions of the shapes of bicycle tire tracks and of the outlines of half-remembered people. The poem after which \"Pale Fire\" is entitled explicitly refers to \"the miracle of the lemniscate\".\n\nThe well known shape and meaning of the infinity symbol have made it a common typographic element of graphic design. For instance, the Métis flag, used by the Canadian Métis people in the early 19th century, is based around this symbol. In modern commerce, corporate logos featuring this symbol have been used by, among others, Room for PlayStation Portable, Microsoft Visual Studio, Fujitsu, and CoorsTek.\n\nThe symbol is encoded in Unicode at and in LaTeX as codice_1: formula_4.\n\nThe Unicode set of symbols also includes several variant forms of the infinity symbol, that are less frequently available in fonts: , and in block Miscellaneous Mathematical Symbols-B. The acid-free paper symbol mentioned above is encoded separately as .\n\n"}
{"id": "56884970", "url": "https://en.wikipedia.org/wiki?curid=56884970", "title": "Jeffrey Brock", "text": "Jeffrey Brock\n\nJeffrey Farlowe Brock (born June 14, 1970, in Bronxville, New York) is an American mathematician, working in low-dimensional geometry and topology. He is known for his contributions to the understanding of hyperbolic 3-manifolds and the geometry of Teichmüller spaces. \n\nSince July 2018, Brock is a Professor of Mathematics at Yale University, and from January 2019 he will become the first FAS (Faculty of Arts and Sciences) dean of science at Yale University.\n\nBefore joining Yale, he was a professor at Brown University, and also founding director of the Data Science Iniative at Brown University. \n\nBrock obtained a BA (with distinction in Mathematics) from Yale University in 1992. He completed a Ph.D. in Mathematics from the University of California, Berkeley in 1997, under the supervision of Curtis T. McMullen.\n\nBrock then held positions as (NSF-funded) Szego Assistant Professor at Stanford University (1997–2000), assistant professor at the University of Chicago (2000–2003), and Donald D. Harrington Faculty Fellow at the University of Texas at Austin (2003–2004). He became associate professor (with tenure) at Brown University in 2004, where he has been full professor since 2007. He was chair of the Mathematics Department from 2013 to 2017.\n\nBrock has been Associate Director of ICERM since 2013. Previously, he had been Deputy Director between 2010 and 2013.\n\nStarting in July 2018 he will take up a position as Professor of Mathematics at Yale University, and from January 2019 he will become the first FAS (Faculty of Arts and Sciences) dean of science at Yale University.\n\nBrock is also an accomplished jazz musician. He was the founding bassist of the Vijay Iyer Trio, lead by the acclaimed jazz pianist Vijay Iyer.\n\nHe is married and has three children.\n\nJeffrey Brock's research focuses on low-dimensional topology and geometry, particularly on spaces with hyperbolic geometry or negative curvature. His joint work with Richard Canary and Yair Minsky resulted in a solution to the \"Ending Lamination Conjecture\" of William Thurston, culminating in the geometric classification theorem for (topologically-finite) hyperbolic 3-manifolds in terms of their fundamental group and the structure of their ends.\n\nMore recently, he has worked to understand applications of geometry and topology to the structure of massive and complex data sets and the risks and implications of the increasing use of 'black box' algorithms in science and society.\n\n\n\n"}
{"id": "35872975", "url": "https://en.wikipedia.org/wiki?curid=35872975", "title": "Kinetic smallest enclosing disk", "text": "Kinetic smallest enclosing disk\n\nA kinetic smallest enclosing disk data structure is a kinetic data structure that maintains the smallest enclosing disk of a set of moving points.\n\nIn 2 dimensions, the best known kinetic smallest enclosing disk data structure uses the farthest point delaunay triangulation of the point set to maintain the smallest enclosing disk. The farthest-point Delaunay triangulation is the dual of the farthest-point Voronoi diagram. It is known that if the farthest-point delaunay triangulation of a point set contains an acute triangle, the circumcircle of this triangle is the smallest enclosing disk. Otherwise, the smallest enclosing disk has the diameter of the point set as its diameter. Thus, by maintaining the kinetic diameter of the point set, the farthest-point delaunay triangulation, and whether or not the farthest-point delaunay triangulation has an acute triangle, the smallest enclosing disk can be maintained.\nThis data structure is responsive and compact, but not local or efficient:\nThe existence of kinetic data structure that has formula_7 events is an open problem.\n\nThe smallest enclosing disk of a set of n moving points can be ε-approximated by a kinetic data structure that processes formula_8 events and requires formula_9 time total.\n\nIn dimensions higher than 2, efficiently maintaining the smallest enclosing sphere of a set of moving points is an open problem.\n"}
{"id": "5558061", "url": "https://en.wikipedia.org/wiki?curid=5558061", "title": "Knowledge Interchange Format", "text": "Knowledge Interchange Format\n\nKnowledge Interchange Format (KIF) is a computer language designed to enable systems to share and re-use information from knowledge-based systems. KIF is similar to frame languages such as KL-One and LOOM but unlike such language its primary role is not intended as a framework for the expression or use of knowledge but rather for the interchange of knowledge between systems. The designers of KIF likened it to PostScript. PostScript was not designed primarily as a language to store and manipulate documents but rather as an interchange format for systems and devices to share documents. In the same way KIF is meant to facilitate sharing of knowledge across different systems that use different languages, formalisms, platforms, etc.\n\nKIF has a declarative semantics. It is meant to describe facts about the world rather than processes or procedures. Knowledge can be described as objects, functions, relations, and rules. It is a formal language, i.e., it can express arbitrary statements in first order logic and can support reasoners that can prove the consistency of a set of KIF statements. KIF also supports non-monotonic reasoning. KIF was created by Michael Genesereth, Richard Fikes and others participating in the DARPA knowledge Sharing Effort.\n\nAlthough the original KIF group intended to submit to a formal standards body, that did not occur. A later version called Common Logic has since been developed for submission to ISO and has been approved and published. A variant called SUO-KIF is the language in which the Suggested Upper Merged Ontology is written.\n\n\n"}
{"id": "33973912", "url": "https://en.wikipedia.org/wiki?curid=33973912", "title": "Koenigs function", "text": "Koenigs function\n\nIn mathematics, the Koenigs function is a function arising in complex analysis and dynamical systems. Introduced in 1884 by the French mathematician Gabriel Koenigs, it gives a canonical representation as dilations of a univalent holomorphic mapping, or a semigroup of mappings, of the unit disk in the complex numbers into itself.\n\nLet \"D\" be the unit disk in the complex numbers. Let be a holomorphic function mapping \"D\" into itself, fixing the point 0, with not identically 0 and not an automorphism of \"D\", i.e. a Möbius transformation defined by a matrix in SU(1,1).\n\nBy the Denjoy-Wolff theorem, leaves invariant each disk |\"z\" | < \"r\" and the iterates of converge uniformly on compacta to 0: in fact for 0 < < 1, \nfor |\"z\" | ≤ \"r\" with \"M\"(\"r\" ) < 1. Moreover '(0) = with 0 < || < 1.\n\nsuch that (0) = 0, '(0) = 1 and Schröder's equation is satisfied,\n\nThe function \"h\" is \"the uniform limit on compacta of the normalized iterates\", formula_3. \n\nMoreover, if is univalent, so is .\n\nAs a consequence, when (and hence ) are univalent, can be identified with the open domain . Under this conformal identification, the mapping   becomes multiplication by , a dilation on .\n\n\n\n\nLet be a semigroup of holomorphic univalent mappings of into itself fixing 0 defined \nfor such that\n\n\nEach with > 0 has the same Koenigs function, cf. iterated function. In fact, if \"h\" is the Koenigs function of \n, then satisfies Schroeder's equation and hence is proportion to \"h\".\n\nTaking derivatives gives\nHence is the Koenigs function of .\n\nOn the domain , the maps become multiplication by formula_15, a continuous semigroup.\nSo formula_16 where is a uniquely determined solution of with Re < 0. It follows that the semigroup is differentiable at 0. Let\na holomorphic function on with \"v\"(0) = 0 and = . \n\nThen\nso that\nand\nthe flow equation for a vector field.\n\nRestricting to the case with 0 < λ < 1, the \"h\"(\"D\") must be starlike so that\n\nSince the same result holds for the reciprocal,\nso that satisfies the conditions of \n\nConversely, reversing the above steps, any holomorphic vector field satisfying these conditions is associated to a semigroup , with\n\n"}
{"id": "5210590", "url": "https://en.wikipedia.org/wiki?curid=5210590", "title": "Lemniscate", "text": "Lemniscate\n\nIn algebraic geometry, a lemniscate is any of several figure-eight or -shaped curves. The word comes from the Latin \"lēmniscātus\" meaning \"decorated with ribbons\", from the Greek λημνίσκος meaning ribbons, or alternatively may refer to the wool from which the ribbons were made.\n\nCurves that have been called a lemniscate include three quartic plane curves: the hippopede or lemniscate of Booth, the lemniscate of Bernoulli, and the lemniscate of Gerono. The study of lemniscates (and in particular the hippopede) dates to ancient Greek mathematics, but the term \"lemniscate\" for curves of this type comes from the work of Jacob Bernoulli in the late 17th century.\n\nThe consideration of curves with a figure-eight shape can be traced back to Proclus, a Greek Neoplatonist philosopher and mathematician who lived in the 5th century AD. Proclus considered the cross-sections of a torus by a plane parallel to the axis of the torus. As he observed, for most such sections the cross section consists of either one or two ovals; however, when the plane is tangent to the inner surface of the torus, the cross-section takes on a figure-eight shape, which Proclus called a horse fetter (a device for holding two feet of a horse together), or \"hippopede\" in Greek. The name \"lemniscate of Booth\" for this curve dates to its study by the 19th-century mathematician James Booth.\n\nThe lemniscate may be defined as an algebraic curve, the zero set of the quartic polynomial formula_1 when the parameter \"d\" is negative. For positive values of \"d\" one instead obtains the oval of Booth.\n\nIn 1680, Cassini studied a family of curves, now called the Cassini oval, defined as follows: the locus of all points, the product of whose distances from two fixed points, the curves' foci, is a constant. Under very particular circumstances (when the half-distance between the points is equal to the square root of the constant) this gives rise to a lemniscate.\n\nIn 1694, Johann Bernoulli studied the lemniscate case of the Cassini oval, now known as the lemniscate of Bernoulli (shown above), in connection with a problem of \"isochrones\" that had been posed earlier by Leibniz. Like the hippopede, it is an algebraic curve, the zero set of the polynomial formula_2. Bernoulli's brother Jacob Bernoulli also studied the same curve in the same year, and gave it its name, the lemniscate. It may also be defined geometrically as the locus of points whose product of distances from two foci equals the square of half the interfocal distance. It is a special case of the hippopede (lemniscate of Booth), with formula_3, and may be formed as a cross-section of a torus whose inner hole and circular cross-sections have the same diameter as each other. The lemniscatic elliptic functions are analogues of trigonometric functions for the lemniscate of Bernoulli, and the lemniscate constants arise in evaluating the arc length of this lemniscate.\n\nAnother lemniscate, the lemniscate of Gerono or lemniscate of Huygens, is the zero set of the quartic polynomial formula_4. Viviani's curve, a three-dimensional curve formed by intersecting a sphere with a cylinder, also has a figure eight shape, and has the lemniscate of Gerono as its planar projection.\n\nOther figure-eight shaped algebraic curves include\n\n"}
{"id": "345396", "url": "https://en.wikipedia.org/wiki?curid=345396", "title": "List of differential geometry topics", "text": "List of differential geometry topics\n\nThis is a list of differential geometry topics. See also glossary of differential and metric geometry and list of Lie group topics.\n\n\n\n\"See also multivariable calculus, list of multivariable calculus topics\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "13186787", "url": "https://en.wikipedia.org/wiki?curid=13186787", "title": "List of price index formulas", "text": "List of price index formulas\n\nA number of different formulae, more than hundred, have been proposed as means of calculating price indexes. While price index formulae all use price and possibly quantity data, they aggregate these in different ways. A price index aggregates various combinations of base period prices (formula_1), later period prices (formula_2), base period quantities (formula_3), and later period quantities (formula_4). Price index numbers are usually defined either in terms of (actual or hypothetical) expenditures (expenditure = price * quantity) or as different weighted averages of price relatives (formula_5). These tell the relative change of the price in question. Two of the most commonly used price index formulae were defined by German economists and statisticians Étienne Laspeyres and Hermann Paasche, both around 1875 when investigating price changes in Germany.\n\nDeveloped in 1871 by Laspeyres, the formula:\n\ncompares the total cost of the same basket of goods formula_3 at the old and new prices.\n\nDeveloped in 1874 by Paasche, the formula:\n\ncompares the total cost of a new basket of goods formula_4 at the old and new prices.\n\nThe geometric means index:\n\nincorporates quantity information through the share of expenditure in the base period.\n\nUnweighted, or \"elementary\", price indices only compare prices of a single type of good between two periods. They do not make any use of quantities or expenditure weights. They are called \"elementary\" because they are often used at the lower levels of aggregation for more comprehensive price indices. In such a case, they are not indices but merely an intermediate stage in the calculation of an index. At these lower levels, it is argued that weighting is not necessary since only one type of good is being aggregated. However this implicitly assumes that only one type of the good is available (e.g. only one brand and one package size of frozen peas) and that it has not changed in quality etc between time periods.\n\nDeveloped in 1764 by Carli, an Italian economist, this formula is the arithmetic mean of the price relative between a period \"t\" and a base period \"0\".\n\nOn 17 August 2012 the BBC Radio 4 program \"More or Less\" noted that the Carli index, used in part in the British Retail Price Index measure, has a built-in bias towards recording inflation even when over successive periods there is no increase in prices overall.\n\nIn 1738 French economist Dutot proposed using an index calculated by dividing the average price in period \"t\" by the average price in period \"0\".\n\nIn 1863, English economist Jevons proposed taking the geometric average of the price relative of period \"t\" and base period \"0\". When used as an elementary aggregate, the Jevons index is considered a constant elasticity of substitution index since it allows for product substitution between time periods.\n\nThis is the formula that was used for the old Financial Times stock market index (the predecessor of the FTSE 100 Index). It was inadequate for that purpose. In particular, if the price of any of the constituents were to fall to zero, the whole index would fall to zero. That is an extreme case; in general the formula will understate the total cost of a basket of goods (or of any subset of that basket) unless their prices all change at the same rate. Also, as the index is unweighted, large price changes in selected constituents can transmit to the index to an extent not representing their importance in the average portfolio.\n\nThe harmonic average counterpart to the Carli index. The index was proposed by Jevons in 1865 and by Coggeshall in 1887.\n\nIs the geometric mean of the Carli and the harmonic price indexes. In 1922 Fisher wrote that this and the Jevons were the two best unweighted indexes based on Fisher's test approach to index number theory.\n\nThe ratio of harmonic means or \"Harmonic means\" price index is the harmonic average counterpart to the Dutot index.\n\nThe Marshall-Edgeworth index, credited to Marshall (1887) and Edgeworth (1925), is a weighted relative of current period to base period sets of prices. This index uses the arithmetic average of the current and based period quantities for weighting. It is considered a pseudo-superlative formula and is symmetric. The use of the Marshall-Edgeworth index can be problematic in cases such as a comparison of the price level of a large country to a small one. In such instances, the set of quantities of the large country will overwhelm those of the small one.\n\nSuperlative indices treat prices and quantities equally across periods. They are symmetrical and provide close approximations of cost of living indices and other theoretical indices used to provide guidelines for constructing price indices. All superlative indices produce similar results and are generally the favored formulas for calculating price indices. A superlative index is defined technically as \"an index that is exact for a flexible functional form that can provide a second-order approximation to other twice-differentiable functions around the same point.\"\n\nThe change in a Fisher index from one period to the next is the geometric mean of the changes in Laspeyres's and Paasche's indexes between those periods, and these are chained together to make comparisons over many periods:\n\nThis is also called Fisher's \"ideal\" price index.\n\nThe Törnqvist or Törnqvist-Theil index is the geometric average of the n price relatives of the current to base period prices (for n goods) weighted by the arithmetic average of the value shares for the two periods.\n\nThe Walsh price index is the weighted sum of the current period prices divided by the weighted sum of the base period prices with the geometric average of both period quantities serving as the weighting mechanism:\n\n"}
{"id": "49800971", "url": "https://en.wikipedia.org/wiki?curid=49800971", "title": "List of things named after Stanislaw Ulam", "text": "List of things named after Stanislaw Ulam\n\nThis is a (partial) list of things named after Stanislaw Ulam, a 20th-century Polish-American mathematician who also worked in physics and biological sciences:\n\n\n"}
{"id": "41887951", "url": "https://en.wikipedia.org/wiki?curid=41887951", "title": "Margherita Piazzola Beloch", "text": "Margherita Piazzola Beloch\n\nMargherita Piazzolla Beloch (12 July 1879 in Frascati – 28 September 1976 in Rome) was an Italian mathematician who worked in algebraic geometry, algebraic topology and photogrammetry.\n\nBeloch was the daughter of the German historian Karl Julius Beloch, who taught ancient history for 50 years at Sapienza University of Rome, and American Bella Bailey.\n\nBeloch studied mathematics at the Sapienza University of Rome and wrote her undergraduate thesis under the supervision of Guido Castelnuovo. She received her degree in 1908 with Lauude and \"dignita' di stampa\" which means that her work was worthy of publication and in fact her thesis \"Sulle trasformazioni birazionali dello spazio\" (On Birational Transformations In Space) was published in the Annali di Matematica Pura ed Applicata. \n\nGuido Castelnuovo was very impressed with her talent and offer her the position of assistant which Margherita took and held until 1919, when she moved to Pavia and the successive year to Palermo to work under Michele De Franchis, an important figure of the Italian school of algebraic geometry at the time. \n\nIn 1924, Beloch completed her \"libera docenza\" (a degree that at that time had to be obtained before one could become a professor) and three years later she became a full professor at the University of Ferrara\nwhere she taught until her retirement (1955).\n\nHer main scientific interest were\nalgebraic geometry, \nalgebraic topology \nand \nphotogrammetry.\nAfter her thesis she worked on classification of algebraic surfaces studying the configurations of lines that could lie on surfaces. The next step was to study rational curves lying on surfaces and in this framework Beloch obtained the following important result: \"Hyperelleptic surfaces of rank 2 are characterised by having 16 rational curves.\"\n\nBeloch also made some contributions to the theory of skew algebraic curves. She continued working on topological properties of algebraic curves either planar or lying on ruled or cubic surfaces for most of her life, writing about a dozen papers on these subjects.\n\nAround 1940 Beloch become more and more interested in photogrammetry and the application of mathematics, and in particular algebraic geometry, to it. She is also known for her contribution to the mathematics of paper folding: In particular she seems to have been the first to formalise an origami move which allows, when possible, to construct by paper folding the common tangents to two parabolas. As a consequence she showed how to extract cubic roots by paper folding, something that is impossible to do by rule and compass. The move she used has been called the Beloch fold.\n"}
{"id": "4186556", "url": "https://en.wikipedia.org/wiki?curid=4186556", "title": "Mermin–Wagner theorem", "text": "Mermin–Wagner theorem\n\nIn quantum field theory and statistical mechanics, the Mermin–Wagner theorem (also known as Mermin–Wagner–Hohenberg theorem, Mermin–Wagner–Berezinskii theorem, or Coleman theorem) states that continuous symmetries cannot be spontaneously broken at finite temperature in systems with sufficiently short-range interactions in dimensions . Intuitively, this means that long-range fluctuations can be created with little energy cost and since they increase the entropy they are favored.\n\nThis is because if such a spontaneous symmetry breaking occurred, then the corresponding Goldstone bosons, being massless, would have an infrared divergent correlation function.\n\nThe absence of spontaneous symmetry breaking in dimensional systems was rigorously proved by in quantum field theory and by David Mermin, Herbert Wagner and Pierre Hohenberg in statistical physics. That the theorem does not apply to discrete symmetries can be seen in the two-dimensional Ising model.\n\nConsider the free scalar field of mass in two Euclidean dimensions. Its propagator is:\n\nFor small is a solution to Laplace's equation with a point source:\n\nThis is because the propagator is the reciprocal of in space. To use Gauss's law, define the electric field analog to be . The divergence of the electric field is zero. In two dimensions, using a large Gaussian ring:\n\nSo that the function \"G\" has a logarithmic divergence both at small and large \"r\".\n\nThe interpretation of the divergence is that the field fluctuations cannot stay centered around a mean. If you start at a point where the field has the value 1, the divergence tells you that as you travel far away, the field is arbitrarily far from the starting value. This makes a two dimensional massless scalar field slightly tricky to define mathematically. If you define the field by a Monte-Carlo simulation, it doesn't stay put, it slides to infinitely large values with time.\n\nThis happens in one dimension too, when the field is a one dimensional scalar field, a random walk in time. A random walk also moves arbitrarily far from its starting point, so that a one-dimensional or two-dimensional scalar does not have a well defined average value.\n\nIf the field is an angle, , as it is in the Mexican hat model where the complex field has an expectation value but is free to slide in the direction, the angle will be random at large distances. This is the Mermin–Wagner theorem: there is no spontaneous breaking of a continuous symmetry in two dimensions.\n\nAnother example is the XY model. The Mermin–Wagner theorem prevents any spontaneous symmetry breaking of the model's continuous (internal) symmetry on a spatial lattice of dimension , i.e. the (spin-)field's expectation value remains zero for any \"finite\" temperature (quantum phase transitions remain unaffected). However, the theorem does not prevent the existence of a phase transition in the sense of a diverging correlation length . To this end, the model has two phases: a conventional disordered phase at high temperature with dominating exponential decay of the correlation function formula_5 for formula_6, and a low-temperature phase with quasi-long-range order where decays according to some power law for \"sufficiently large\", but finite distance ( with the lattice spacing).\n\nWe will present an intuitive way to understand the mechanism that prevents symmetry breaking in low dimensions, through an application to the Heisenberg model, that is a system of -component spins of unit length , located at the sites of a -dimensional square lattice, with nearest neighbor coupling . Its Hamiltonian is \n\nThe name of this model comes from its rotational symmetry. Let us consider the low temperature behavior of this system and assume that there exists a spontaneously broken, that is a phase where all spins point in the same direction, e.g. along the -axis. Then the rotational symmetry of the system is spontaneously broken, or rather reduced to the symmetry under rotations around this direction. We can parametrize the field in terms of independent fluctuations around this direction as follows:\n\nwith , and Taylor expand the resulting Hamiltonian. We have\n\nwhence\n\nIgnoring the irrelevant constant term and passing to the continuum limit, given that we are interested in the low temperature phase where long-wavelength fluctuations dominate, we get\n\nThe field fluctuations are called spin waves and can be recognized as Goldstone bosons. Indeed, they are \"n\"-1 in number and they have zero mass since there is no mass term in the Hamiltonian.\n\nTo find if this hypothetical phase really exists we have to check if our assumption is self-consistent, that is if the expectation value of the magnetization, calculated in this framework, is finite as assumed. To this end we need to calculate the first order correction to the magnetization due to the fluctuations. This is the procedure followed in the derivation of the well-known Ginzburg criterion.\n\nThe model is Gaussian to first order and so the momentum space correlation function is proportional to . Thus the real space two-point correlation function for each of these modes is\n\nwhere \"a\" is the lattice spacing. The average magnetization is\n\nand the first order correction can now easily be calculated:\n\nThe integral above is proportional to\n\nand so it is finite for , but appears to be logarithmically divergent for . However, this is really an artifact of the linear approximation. In a more careful treatment, the average magnetization is zero.\n\nWe thus conclude that for our assumption that there exists a phase of spontaneous magnetization is incorrect for all , because the fluctuations are strong enough to destroy the spontaneous symmetry breaking. This is a general result:\n\nThe result can also be extended to other geometries, such as Heisenberg films with an arbitrary number of layers, as well as to other lattice systems (Hubbard model, s-f model).\n\nMuch stronger results than absence of magnetization can actually be proved, and the setting can be substantially more general. In particular:\n\n\nIn this general setting, Mermin–Wagner theorem admits the following strong form (stated here in an informal way): \n\nWhen the assumption that the Lie group be compact is dropped, a similar result holds, but with the conclusion that infinite-volume Gibbs states do not exist.\n\nFinally, there are other important applications of these ideas and methods, most notably to the proof that there cannot be non-translation invariant Gibbs states in 2-dimensional systems. A typical such example would be the absence of crystalline states in a system of hard disks (with possibly additional attractive interactions).\n\nIt has been proved however that interactions of hard-core type can lead in general to violations of Mermin–Wagner theorem.\n\n"}
{"id": "771562", "url": "https://en.wikipedia.org/wiki?curid=771562", "title": "Mikhail Leonidovich Gromov", "text": "Mikhail Leonidovich Gromov\n\nMikhail Leonidovich Gromov (also Mikhael Gromov, Michael Gromov or Mischa Gromov; ; born 23 December 1943), is an American-French-Russian mathematician known for work in geometry, analysis and group theory. He is a permanent member of IHÉS in France and a Professor of Mathematics at New York University.\n\nGromov has won several prizes, including the Abel Prize in 2009 \"for his revolutionary contributions to geometry\".\n\nMikhail Gromov was born on 23 December 1943 in Boksitogorsk, Soviet Union. His father Leonid Gromov and his Jewish mother Lea Rabinovitz were pathologists. Gromov was born during World War II, and his mother, who worked as a medical doctor in the Soviet Army, had to leave the front line in order to give birth to him. When Gromov was nine years old, his mother gave him the book \"The Enjoyment of Mathematics\" by Hans Rademacher and Otto Toeplitz, a book that piqued his curiosity and had a great influence on him.\n\nGromov studied mathematics at Leningrad State University where he obtained a master's degree in 1965, a Doctorate in 1969 and defended his Postdoctoral Thesis in 1973. His thesis advisor was Vladimir Rokhlin.\n\nGromov married in 1967. In 1970, invited to give a presentation at the International Congress of Mathematicians in France, he was not allowed to leave the USSR. Still, his lecture was published in the conference proceedings.\n\nDisagreeing with the Soviet system, he had been thinking of emigrating since the age of 14. In the early 1970s he ceased publication, hoping that this would help his application to move to Israel. He changed his last name to that of his mother. When the request was granted in 1974, he moved directly to New York where a position had been arranged for him at Stony Brook.\n\nIn 1981 he left Stony Brook to join the faculty of University of Paris VI and in 1982 he became a permanent professor at the Institut des Hautes Études Scientifiques (IHES) where he remains today. At the same time, he has held professorships at the University of Maryland, College Park from 1991 to 1996, and at the Courant Institute of Mathematical Sciences since 1996. He adopted French citizenship in 1992.\n\nGromov's style of geometry often features a \"coarse\" or \"soft\" viewpoint, analyzing asymptotic or large-scale properties.\n\nMotivated by Nash and Kuiper's C embedding theorem and Stephen Smale's early results, Gromov introduced in 1973 the notion of convex integration and the h-principle, a very general way to solve underdetermined partial differential equations and the basis for a geometric theory of these equations.\n\nIn the 1980s, Gromov introduced the Gromov–Hausdorff metric, a measure of the difference between two compact metric spaces. In this context he proved Gromov's compactness theorem, stating that the set of compact Riemannian manifolds with Ricci curvature ≥ \"c\" and diameter ≤ \"D\" is relatively compact in the Gromov–Hausdorff metric. The possible limit points of sequences of such manifolds are Alexandrov spaces of curvature ≥ \"c\", a class of metric spaces studied in detail by Burago, Gromov and Perelman in 1992. Gromov was also the first to study the space of all possible Riemannian structures on a given manifold.\n\nGromov introduced geometric group theory, the study of infinite groups via the geometry of their Cayley graphs and their word metric. In 1981 he proved Gromov's theorem on groups of polynomial growth: a finitely generated group has polynomial growth (a geometric property) if and only if it is virtually nilpotent (an algebraic property). The proof uses the Gromov–Hausdorff metric mentioned above. Along with Eliyahu Rips he introduced the notion of hyperbolic groups.\n\nGromov founded the field of symplectic topology by introducing the theory of pseudoholomorphic curves. This led to Gromov–Witten invariants which are used in string theory and to his non-squeezing theorem.\n\nGromov is also interested in mathematical biology, the structure of the brain and the thinking process, and the way scientific ideas evolve.\n\n\n\n\n"}
{"id": "229649", "url": "https://en.wikipedia.org/wiki?curid=229649", "title": "Monge array", "text": "Monge array\n\nIn mathematics applied to computer science, Monge arrays, or Monge matrices, are mathematical objects named for their discoverer, the French mathematician Gaspard Monge.\n\nAn \"m\"-by-\"n\" matrix is said to be a \"Monge array\" if, for all formula_1 such that\n\none obtains\n\nSo for any two rows and two columns of a Monge array (a 2 × 2 sub-matrix) the four elements at the intersection points have the property that the sum of the upper-left and lower right elements (across the main diagonal) is less than or equal to the sum of the lower-left and upper-right elements (across the antidiagonal).\n\nThis matrix is a Monge array:\n\nFor example, take the intersection of rows 2 and 4 with columns 1 and 5.\nThe four elements are:\n\nThe sum of the upper-left and lower right elements is less than or equal to the sum of the lower-left and upper-right elements.\n\n\n\n"}
{"id": "32366841", "url": "https://en.wikipedia.org/wiki?curid=32366841", "title": "Parallel parking problem", "text": "Parallel parking problem\n\nThe parallel parking problem is a motion planning problem in control theory and mechanics to determine the path a car must take in order to parallel park into a parking space. The front wheels of a car are permitted to turn, but the rear wheels must stay aligned. When a car is initially adjacent to a parking space, to move into the space it would need to move in a direction perpendicular to the allowed path of motion of the rear wheels. The admissible motions of the car in its configuration space are an example of a nonholonomic system.\n\n\n"}
{"id": "1969007", "url": "https://en.wikipedia.org/wiki?curid=1969007", "title": "Percentage point", "text": "Percentage point\n\nA percentage point or percent point is the unit for the arithmetic difference of two percentages. For example, moving up from 40% to 44% is a 4 \"percentage point\" increase, but is an actual 10 percent increase in what is being measured. In the literature, the percentage point unit is usually either written out, or abbreviated as \"pp\" or \"p.p.\" to avoid ambiguity. After the first occurrence, some writers abbreviate by using just \"point\" or \"points\".\n\nConsider the following hypothetical example: In 1980, 50 percent of the population smoked, and in 1990 only 40 percent smoked. One can thus say that from 1980 to 1990, the prevalence of smoking decreased by 10 \"percentage points\" although smoking did not decrease by 10 percent (it decreased by \"20 percent\") – percentages indicate ratios, not differences.\n\nPercentage-point differences are one way to express a risk or probability. Consider a drug that cures a given disease in 70 percent of all cases, while without the drug, the disease heals spontaneously in only 50 percent of cases. The drug reduces absolute risk by 20 percentage points. Alternatives may be more meaningful to consumers of statistics, such as the reciprocal, also known as the number needed to treat (NNT). In this case, the reciprocal transform of the percentage-point difference would be 1/(20pp) = 1/0.20 = 5. Thus if 5 patients are treated with the drug, one could expect to heal one more case of the disease than would have occurred in the absence of the drug.\n\nFor measurements involving percentages as a unit, such as, growth, yield, or ejection fraction, statistical deviations and related descriptive statistics, including the standard deviation and root-mean-square error, the result should be expressed in units of percentage points instead of percentage. Mistakenly using percentage as the unit for the standard deviation is confusing, since percentage is also used as a unit for the relative standard deviation, i.e. standard deviation divided by average value (coefficient of variation).\n\n"}
{"id": "45646754", "url": "https://en.wikipedia.org/wiki?curid=45646754", "title": "Projective Set (game)", "text": "Projective Set (game)\n\nProjective Set (sometimes shortened to ProSet) is a real-time card game derived from the older game \"Set\".\nThe deck contains cards consisting of colored dots; some cards are laid out on the table and players attempt to find \"Sets\" among them.\nThe word \"projective\" comes from the game's relation to Projective spaces over the finite field with two elements.\n\nProjective Set has been studied mathematically as well as played recreationally.\nIt has been a popular game at Canada/USA Mathcamp.\n\nA Projective Set card has six binary attributes, or bits, generally represented by colored dots. For each color of dot,\neach card either has that dot or does not.\nThere is one card for each possible combination of dots except the combination of no dots at all,\nmaking formula_1 cards total.\n\nThree cards are said to form a \"set\" if the total number of dots of each color is either 0 or 2.\nSimilarly, four or more cards form a \"set\" if the number of dots of each color is an even number.\n\nA card and itself could be said to form a two-card set, but as the cards in the deck are all distinct, this\ndoes not arise in actual gameplay.\n\nIn the original version, as in \"Set\", 12 cards are laid out on the table.\nThe first player to find three cards which form a set and call out \"set\" takes the three cards.\nThree new cards are then dealt and the play continues until the deck is depleted.\n\nIf at any time the players agree there is no set among the cards, three new cards can be dealt, bringing\nthe total number of cards on the table to 15. Other than this, new cards are not dealt out unless the\nnumber of cards on the table goes below 12.\n\nThe game ends when the deck is depleted and no more sets can be found among the cards on the\ntable. The player who captured the most sets is the winner.\n\nA variation of the game, more popular than the original, allows sets of any size, rather than just sets of size three.\n7 cards are put out on the table at a time, and when a set is found (with anywhere from 3-7 cards),\nall the cards from the set are taken and then replaced.\nPoints are generally given at the end according to how many cards each player captured rather than how many sets.\n\nIt turns out that among any 7 cards there is a set, under these rules, so there is no extra rule necessary for the case that no\nset can be found.\n\nThe cards of a Projective Set deck can be thought of as nonzero vectors in the finite vector space\nformula_2.\nThe collection of all such vectors is the finite projective space with order 2 and dimension 5.\nThree cards form a set if and only if the corresponding points are collinear in that space.\nMore generally, in the variant, formula_3 cards form a set if and only if the corresponding vectors\nadd to the zero vector.\n\nIn \"Set\", there can exist 20 cards out of the 81 without a set, but no more.\nIn Projective Set, there can exist up to 32 out of the 63 cards with no (3-card) set.\n\nvarious sites, including: \n"}
{"id": "20210883", "url": "https://en.wikipedia.org/wiki?curid=20210883", "title": "Quantum t-design", "text": "Quantum t-design\n\nA Quantum t-design is a probability distribution over pure quantum states which can duplicate properties of the probability distribution over the Haar measure for polynomials of degree t or less. Specifically, the average of any polynomial function of degree t over the design is exactly the same as the average over Haar measure. Here the Haar measure is a uniform probability distribution over all quantum states. Quantum t-designs are so called because they are analogous to in classical statistics, which arose historically in connection with the problem of design of experiments. Quantum t-designs are usually unique, and thus almost always calculable. Two particularly important types of t-designs in quantum mechanics are spherical and unitary t-designs.\n\nSpherical t-designs are designs where points of the design (i.e. the points being used for the averaging process) are points on a unit sphere. Spherical t-designs and variations thereof have been considered lately and found useful in quantum information theory, quantum cryptography and other related fields.\n\nUnitary designs are analogous to spherical designs in that they approximate the entire unitary group via a finite collection of unitary matrices. Unitary designs have been found useful in information theory and quantum computing. Unitary designs are especially useful in quantum computing since most operations are represented by unitary operators.\n\nIn a d-dimensional Hilbert space when averaging over all quantum pure states the natural group is SU(d), the special unitary group of dimension d. The Haar measure is, by definition, the unique group-invariant measure, so it is used to average properties that are not unitarily invariant over all states, or over all unitaries.\n\nA particularly widely used example of this is the spin formula_1 system. For this system the relevant group is SU(2) which is the group of all 2x2 unitary operators. Since every 2x2 unitary operator is a rotation of the Bloch sphere, the Haar measure for spin-1/2 particles is invariant under all rotations of the Bloch sphere. This implies that the Haar measure is \"the\" rotationally invariant measure on the Bloch sphere, which can be thought of as a constant density distribution over the surface of the sphere.\n\nAnother recent application is the fact that a symmetric informationally complete POVM is also a spherical 2-design. Also, since a 2-design must have more than formula_2 elements, a SIC-POVM is a minimal 2-design.\n\nComplex projective (t,t)-designs have been studied in quantum information theory as quantum 2-designs, and in t-designs of vectors in the unit sphere in formula_3 which, when transformed to vectors in formula_4 become complex projective (t/2,t/2)-designs.\n\nFormally, we define a complex projective (t,t)-design as a probability distribution over quantum states formula_5 if\n\nformula_6\n\nHere, the integral over states is taken over the Haar measure on the unit sphere in formula_7\n\nExact t-designs over quantum states cannot be distinguished from the uniform probability distribution over all states when using t copies of a state from the probability distribution. However in practice even t-designs may be difficult to compute. For this reason approximate t-designs are useful.\n\nApproximate (t,t)-designs are most useful due to their ability to be efficiently implemented. i.e. it is possible to generate a quantum state formula_8 distributed according to the probability distribution formula_9 in formula_10 time.\nThis efficient construction also implies that the POVM of the operators formula_11 can be implemented in formula_10 time.\n\nThe technical definition of an approximate (t,t)-design is:\n\nIf formula_13\n\nand formula_14\n\nthen formula_5 is an formula_16-approximate (t,t)-design.\n\nIt is possible, though perhaps inefficient, to find an formula_16-approximate (t,t) design consisting of quantum pure states for a fixed t.\n\nFor convenience N is assumed to be a power of 2.\n\nUsing the fact that for any N there exists a set of formula_18 functions {0...,N-1} formula_19 {0...,N-1} such that for any distinct formula_20 {0...,N-1} the image under f, where f is chosen at random from S, is exactly the uniform distribution over tuples of d elements of {0...,N-1}.\n\nLet formula_21 be drawn from the Haar measure. Let formula_22 be the probability distribution of formula_23 and let formula_24. Finally let formula_25 be drawn from P. If we define formula_26 with probability formula_27 and formula_28 with probability formula_1 then:\nformula_30 for odd j and formula_31 for even j.\n\nUsing this and Gaussian quadrature we can construct formula_32 so that formula_33 is an approximate (t,t)-design.\n\nElements of the unitary design are elements of the unitary group, U(d), the group of formula_34 unitary matrices. A t-design of unitary operators will generate a t-design of states.\n\nSuppose formula_35 is your unitary design (i.e. a set of unitary operators). Then for \"any\" pure state formula_36 let formula_37. Then formula_38\n\nthen X is a unitary t-design.\n\nWe further define the inner product for functions formula_39 and formula_40 on formula_41 as the average value of formula_42 as:\n\nformula_43\n\nand formula_44 as the average value of formula_42 over any finite subset formula_46.\n\nit follows that X is a unitary t-design iff formula_47.\n\nFrom the above it is demonstrable that if X is a t-design then formula_48 is an absolute bound for the design. This imposes an upper bound on the size of a unitary design. This bound is absolute meaning it depends only on the strength of the design or the degree of the code, and not the distances in the subset, X.\n\nA unitary code is a finite subset of the unitary group in which a few inner product values occur between elements. Specifically, a unitary code is defined as a finite subset formula_46 if for all formula_50 in X formula_51 takes only distinct values.\n\nIt follows that formula_52 and if U and M are orthogonal: formula_53\n"}
{"id": "7907151", "url": "https://en.wikipedia.org/wiki?curid=7907151", "title": "Rellich–Kondrachov theorem", "text": "Rellich–Kondrachov theorem\n\nIn mathematics, the Rellich–Kondrachov theorem is a compact embedding theorem concerning Sobolev spaces. It is named after the Austrian-German mathematician Franz Rellich and the Russian mathematician Vladimir Iosifovich Kondrashov. Rellich proved the \"L\" theorem and Kondrashov the \"L\" theorem.\n\nLet Ω ⊆ R be an open, bounded Lipschitz domain, and let 1 ≤ \"p\" < \"n\". Set\n\nThen the Sobolev space \"W\"(Ω; R) is continuously embedded in the \"L\" space \"L\"(Ω; R) and is compactly embedded in \"L\"(Ω; R) for every 1 ≤ \"q\" < \"p\". In symbols,\n\nand\n\nOn a compact manifold with boundary, the Kondrachov embedding theorem states that if and then the Sobolev embedding\n\nis completely continuous (compact).\n\nSince an embedding is compact if and only if the inclusion (identity) operator is a compact operator, the Rellich–Kondrachov theorem implies that any uniformly bounded sequence in \"W\"(Ω; R) has a subsequence that converges in \"L\"(Ω; R). Stated in this form, in the past the result was sometimes referred to as the Rellich–Kondrachov selection theorem, since one \"selects\" a convergent subsequence. (However, today the customary name is \"compactness theorem\", whereas \"selection theorem\" has a precise and quite different meaning, referring to multifunctions).\n\nThe Rellich–Kondrachov theorem may be used to prove the Poincaré inequality, which states that for \"u\" ∈ \"W\"(Ω; R) (where Ω satisfies the same hypotheses as above),\n\nfor some constant \"C\" depending only on \"p\" and the geometry of the domain Ω, where\n\ndenotes the mean value of \"u\" over Ω.\n\n"}
{"id": "701096", "url": "https://en.wikipedia.org/wiki?curid=701096", "title": "Rotational symmetry", "text": "Rotational symmetry\n\nRotational symmetry, also known as radial symmetry in biology, is the property, a shape has when it looks the same after some rotation by a partial turn. An object's degree of rotational symmetry is the number of distinct orientations in which it looks the same.\n\nFormally the rotational symmetry is symmetry with respect to some or all rotations in \"m\"-dimensional Euclidean space. Rotations are direct isometries, i.e., isometries preserving orientation. Therefore a symmetry group of rotational symmetry is a subgroup of \"E\"(\"m\") (see Euclidean group).\n\nSymmetry with respect to all rotations about all points implies translational symmetry with respect to all translations, so space is homogeneous, and the symmetry group is the whole \"E\"(\"m\"). With the modified notion of symmetry for vector fields the symmetry group can also be \"E\"(\"m\").\n\nFor symmetry with respect to rotations about a point we can take that point as origin. These rotations form the special orthogonal group SO(\"m\"), the group of \"m\"×\"m\" orthogonal matrices with determinant 1. For this is the rotation group SO(3).\n\nIn another meaning of the word, the rotation group \"of an object\" is the symmetry group within \"E\"(\"n\"), the group of direct isometries; in other words, the intersection of the full symmetry group and the group of direct isometries. For chiral objects it is the same as the full symmetry group.\n\nLaws of physics are SO(3)-invariant if they do not distinguish different directions in space. Because of Noether's theorem, rotational symmetry of a physical system is equivalent to the angular momentum conservation law.\n\nRotational symmetry of order \"n, also called n\"-fold rotational symmetry, or discrete rotational symmetry of the \"n\"th order, with respect to a particular point (in 2D) or axis (in 3D) means that rotation by an angle of 360°/n (180°, 120°, 90°, 72°, 60°, 51 °, etc.) does not change the object. Note that \"1-fold\" symmetry is no symmetry (all objects look alike after a rotation of 360°).\n\nThe notation for \"n\"-fold symmetry is C or simply \"n\". The actual symmetry group is specified by the point or axis of symmetry, together with the \"n\". For each point or axis of symmetry, the abstract group type is cyclic group of order \"n\", Z. Although for the latter also the notation \"C\" is used, the geometric and abstract \"C\" should be distinguished: there are other symmetry groups of the same abstract group type which are geometrically different, see cyclic symmetry groups in 3D.\n\nThe fundamental domain is a sector of 360°/n.\n\nExamples without additional reflection symmetry:\n\n\"C\" is the rotation group of a regular \"n\"-sided polygon in 2D and of a regular \"n\"-sided pyramid in 3D.\n\nIf there is e.g. rotational symmetry with respect to an angle of 100°, then also with respect to one of 20°, the greatest common divisor of 100° and 360°.\n\nA typical 3D object with rotational symmetry (possibly also with perpendicular axes) but no mirror symmetry is a propeller.\n\nFor discrete symmetry with multiple symmetry axes through the same point, there are the following possibilities:\n\nIn the case of the Platonic solids, the 2-fold axes are through the midpoints of opposite edges, and the number of them is half the number of edges. The other axes are through opposite vertices and through centers of opposite faces, except in the case of the tetrahedron, where the 3-fold axes are each through one vertex and the center of one face.\n\nRotational symmetry with respect to any angle is, in two dimensions, circular symmetry. The fundamental domain is a half-line.\n\nIn three dimensions we can distinguish cylindrical symmetry and spherical symmetry (no change when rotating about one axis, or for any rotation). That is, no dependence on the angle using cylindrical coordinates and no dependence on either angle using spherical coordinates. The fundamental domain is a half-plane through the axis, and a radial half-line, respectively. Axisymmetric or axisymmetrical are adjectives which refer to an object having cylindrical symmetry, or axisymmetry (i.e. rotational symmetry with respect to a central axis) like a doughnut (torus). An example of approximate spherical symmetry is the Earth (with respect to density and other physical and chemical properties).\n\nIn 4D, continuous or discrete rotational symmetry about a plane corresponds to corresponding 2D rotational symmetry in every perpendicular plane, about the point of intersection. An object can also have rotational symmetry about two perpendicular planes, e.g. if it is the Cartesian product of two rotationally symmetry 2D figures, as in the case of e.g. the duocylinder and various regular duoprisms.\n\n2-fold rotational symmetry together with single translational symmetry is one of the Frieze groups. There are two rotocenters per primitive cell.\n\nTogether with double translational symmetry the rotation groups are the following wallpaper groups, with axes per primitive cell:\n\nScaling of a lattice divides the number of points per unit area by the square of the scale factor. Therefore the number of 2-, 3-, 4-, and 6-fold rotocenters per primitive cell is 4, 3, 2, and 1, respectively, again including 4-fold as a special case of 2-fold, etc.\n\n3-fold rotational symmetry at one point and 2-fold at another one (or ditto in 3D with respect to parallel axes) implies rotation group p6, i.e. double translational symmetry and 6-fold rotational symmetry at some point (or, in 3D, parallel axis). The translation distance for the symmetry generated by one such pair of rotocenters is 2 times their distance.\n\n"}
{"id": "10338711", "url": "https://en.wikipedia.org/wiki?curid=10338711", "title": "Shapiro polynomials", "text": "Shapiro polynomials\n\nIn mathematics, the Shapiro polynomials are a sequence of polynomials which were first studied by Harold S. Shapiro in 1951 when considering the magnitude of specific trigonometric sums. In signal processing, the Shapiro polynomials have good autocorrelation properties and their values on the unit circle are small. The first few members of the sequence are:\n\nwhere the second sequence, indicated by \"Q\", is said to be \"complementary\" to the first sequence, indicated by \"P\".\n\nThe Shapiro polynomials \"P\"(\"z\") may be constructed from the Golay–Rudin–Shapiro sequence \"a\", which equals 1 if the number of pairs of consecutive ones in the binary expansion of \"n\" is even, and −1 otherwise. Thus \"a\" = 1, \"a\" = 1, \"a\" = 1, \"a\" = −1, etc.\n\nThe first Shapiro \"P\"(\"z\") is the partial sum of order 2 − 1 (where \"n\" = 0, 1, 2, ...) of the power series\n\nThe Golay–Rudin–Shapiro sequence {\"a\"} has a fractal-like structure – for example, \"a\" = \"a\" – which implies that the subsequence (\"a\", \"a\", \"a\", ...) replicates the original sequence {\"a\"}. This in turn leads to remarkable\nfunctional equations satisfied by \"f\"(\"z\").\n\nThe second or complementary Shapiro polynomials \"Q\"(\"z\") may be defined in terms of this sequence, or by the relation \"Q\"(\"z\") = (1-)\"z\"\"P\"(-1/\"z\"), or by the recursions\n\nThe sequence of complementary polynomials \"Q\" corresponding to the \"P\" is uniquely characterized by the following properties:\n\nThe most interesting property of the {\"P\"} is that the absolute value of \"P\"(\"z\") is bounded on the unit circle by the square root of 2, which is on the order\nof the L norm of \"P\". Polynomials with coefficients from the set {−1, 1} whose maximum modulus on the unit circle is close to their mean modulus are useful for various applications in communication theory (e.g., antenna design and data compression). Property (iii) shows that (\"P\", \"Q\") form a Golay pair.\n\nThese polynomials have further properties:\n\n\n"}
{"id": "1028841", "url": "https://en.wikipedia.org/wiki?curid=1028841", "title": "Simplex category", "text": "Simplex category\n\nIn mathematics, the simplex category (or simplicial category or nonempty finite ordinal category) is the category of non-empty finite ordinals and order preserving maps. It is used to define simplicial and cosimplicial objects.\n\nThe simplex category is usually denoted by formula_1. There are several equivalent descriptions of this category. formula_1 can be described as the category of \"non-empty finite ordinals\" as objects, thought of as totally ordered sets, and \"order-preserving functions\" as morphisms. The objects are commonly denoted formula_3 (so that formula_4 is the ordinal formula_5). The category is generated by coface and codegeneracy maps, which amount to inserting or deleting elements of the orderings. (See simplicial set for relations of these maps.)\n\nA simplicial object is a presheaf on formula_1, that is a contravariant functor from formula_1 to another category. For instance, simplicial sets are contravariant with the codomain category being the category of sets. A cosimplicial object is defined similarly as a covariant functor originating from formula_1.\n\nThe augmented simplex category, denoted by formula_9 is the category of \"all finite ordinals and order-preserving maps\", thus formula_10, where formula_11. Accordingly, this category might also be denoted FinOrd. The augmented simplex category is occasionally referred to as algebraists' simplex category and the above version is called topologists' simplex category.\n\nA contravariant functor defined on formula_9 is called an augmented simplicial object and a covariant functor out of formula_9 is called an augmented cosimplicial object; when the codomain category is the category of sets, for example, these are called augmented simplicial sets and augmented cosimplicial sets respectively.\n\nThe augmented simplex category, unlike the simplex category, admits a natural monoidal structure. The monoidal product is given by concatenation of linear orders, and the unit is the empty ordinal formula_14 (the lack of a unit prevents this from qualifying as a monoidal structure on formula_1). In fact, formula_9 is the monoidal category freely generated by a single monoid object, given by formula_17 with the unique possible unit and multiplication. This description is useful for understanding how any comonoid object in a monoidal category gives rise to a simplicial object since it can then be viewed as the image of a functor from formula_18 to the monoidal category containing the comonoid; by forgetting the augmentation we obtain a simplicial object. Similarly, this also illuminates the construction of simplicial sets from monads (and hence adjoint functors) since monads can be viewed as monoid objects in endofunctor categories.\n\nThe augmented simplex category provides a simple example of a compact closed category.\n\n\n\n"}
{"id": "27875390", "url": "https://en.wikipedia.org/wiki?curid=27875390", "title": "Symbols for zero", "text": "Symbols for zero\n\nThe modern numerical digit 0 is usually written as a circle, an ellipse, or a rounded rectangle.\n\nIn most modern typefaces, the height of the 0 character is the same as the other digits. However, in typefaces with text figures, the character is often shorter (x-height).\n\nTraditionally, many print typefaces made the capital letter O more rounded than the narrower, elliptical digit 0. Typewriters originally made no distinction in shape between O and 0; some models did not even have a separate key for the digit 0. The distinction came into prominence on modern character displays.\n\nThe digit 0 with a dot in the centre seems to have originated as an option on IBM 3270 displays. Its appearance has continued with Taligent's command line typeface Andalé Mono. One variation used a short vertical bar instead of the dot. This could be confused with the Greek letter Theta on a badly focused display, but in practice there was no confusion because theta was not (then) a displayable character and very little used anyway.\n\nAn alternative, the slashed zero (looking similar to the letter O except for the slash), was primarily used in hand-written coding sheets before transcription to punched cards or tape, and is also used in old-style ASCII graphic sets descended from the default typewheel on the Teletype Model 33 ASR. This form is similar to the symbol formula_1, or \"∅\" (Unicode character U+2205), representing the empty set, as well as to the letter Ø used in several Scandinavian languages. Some Burroughs/Unisys equipment displays a digit 0 with a \"reversed\" slash.\n\nThe opposing convention that has the letter O \"with\" a slash and the digit 0 \"without\" was advocated by SHARE, a prominent IBM user group, and recommended by IBM for writing FORTRAN programs, and by a few other early mainframe makers; this is even more problematic for Scandinavians because it means two of their letters collide. Others advocated the opposite convention, including IBM for writing Algol programs. Another convention used on some early line printers left digit 0 unornamented but added a tail or hook to the capital O so that it resembled an inverted Q (like U+213A ℺) or cursive capital letter-O (formula_2).\n\nSome fonts designed for use with computers made one of the capital-O–digit-0 pair more rounded and the other more angular (closer to a rectangle). The Texas Instruments TI-99/4A computer featured a more angular capital O and a more rounded digit 0, whereas others made the choice the other way around.\nThe typeface used on most European vehicle registration plates distinguishes the two symbols partially in this manner (having a more rectangular or wider shape for the capital O than the digit 0), but in several countries a further distinction is made by slitting open the digit 0 on the upper right side (as in German plates using the \"fälschungserschwerende Schrift\", \"forgery-impeding typeface\").\n\nSometimes the digit 0 is used either exclusively, or not at all, to avoid confusion altogether. For example, confirmation numbers used by Southwest Airlines use only the capital letters O and I instead of the digits 0 and 1, while Canadian postal codes use only the digits 1 and 0 and never the capital letters O and I, although letters and numbers always alternate.\n\nOn the seven-segment displays of calculators, watches, and household appliances, 0 is usually written with six line segments, though on some historical calculator models it was written with four line segments.\n\nIn Braille, the numeral 0 has the same dot configuration as the letter J.\n\n"}
{"id": "646933", "url": "https://en.wikipedia.org/wiki?curid=646933", "title": "Triangular bipyramid", "text": "Triangular bipyramid\n\nIn geometry, the triangular bipyramid (or dipyramid) is a type of hexahedron, being the first in the infinite set of face-transitive bipyramids. It is the dual of the triangular prism with 6 isosceles triangle faces.\n\nAs the name suggests, it can be constructed by joining two tetrahedra along one face. Although all its faces are congruent and the solid is face-transitive, it is not a Platonic solid because some vertices adjoin three faces and others adjoin four.\n\nThe bipyramid whose six faces are all equilateral triangles is one of the Johnson solids, (\"J\"). As a Johnson solid with all faces equilateral triangles, it is also a deltahedron.\n\nThe dual polyhedron of the triangular bipyramid is the triangular prism, with five faces: two parallel equilateral triangles linked by a chain of three rectangles.\nAlthough the triangular prism has a form that is a uniform polyhedron (with square faces), the dual of the Johnson solid form of the bipyramid has rectangular rather than square faces, and is not uniform.\nThe \"triangular bipyramid\", dt{2,3}, can be in sequence rectified, rdt{2,3}, truncated, trdt{2,3} and alternated (snubbed), srdt{2,3}:\n\nThe \"triangular bipyramid\" can be constructed by augmentation of smaller ones, specifically two stacked regular octahedra with 3 triangular bipyramids added around the sides, and 1 tetrahedron above and below. This polyhedron has 24 equilateral triangle faces, but it is not a Johnson solid because it has coplanar faces. It is a coplanar 24-triangle deltahedron. This polyhedron exists as the augmentation of cells in a gyrated alternated cubic honeycomb. Larger triangular polyhedra can be generated similarly, like 9, 16 or 25 triangles per larger triangle face, seen as a section of a triangular tiling.\n\nThe triangular bipyramid can form a tessellation of space with octahedra or with truncated tetrahedra.\n\n\n"}
{"id": "46411525", "url": "https://en.wikipedia.org/wiki?curid=46411525", "title": "USC-Lockheed Martin Quantum Computation Center", "text": "USC-Lockheed Martin Quantum Computation Center\n\nThe USC-Lockheed Martin Quantum Computation Center (QCC) is a joint scientific research effort between Lockheed Martin Corporation and the University of Southern California (USC). The QCC is housed at the Information Sciences Institute (ISI), a computer science and engineering research unit of the USC Viterbi School of Engineering, and is jointly operated by ISI and Lockheed Martin.\n\nUSC faculty, ISI researchers and students are performing basic and applied research into quantum computing, and are collaborating with researchers around the world. The QCC uses a D-Wave Two quantum annealing system, manufactured by D-Wave Systems, Inc. The QCC is the first organization outside of D-Wave to operate the system. The second system is installed at NASA Ames Research Center, and is operated jointly by NASA and Google. The systems must be kept extremely cold and electromagnetically well-shielded to operate with the longest possible coherence time.\n\nQuantum information processing, also called quantum computing, theoretically is known to offer dramatic speed-ups and more complete answers for some combinatorial computing problems. Quantum annealing is a branch of quantum computing whose advantages over classical computing are actively being investigated. In quantum annealing, problems are encoded into the lowest energy state of a physical quantum system. Applications currently under study at the QCC include big data analysis, verification and validation of cyber-physical systems, pattern identification and classification, and optimization and machine learning, any of which may support breakthroughs in multiple industries and government.\n\nUSC and ISI researchers, as well as Lockheed Martin engineers, seek to develop methods to benchmark quantum annealers, and perform tests of quantumness. These include the study of quantum entanglement and, more generally, the performance of quantum annealing experiments.\n\nResearchers also are working to manage quantum decoherence, the phenomenon that degrades the performance of quantum information processors when quantum states are forced out of quantum superposition. Decoherence can reduce quantum functionality to that of a classical computer, and can be counteracted using quantum error correction. QCC researchers and their collaborators have developed methods to counteract decoherence in quantum annealers by combining quantum error correction with energy penalties that suppress decoherence into a single quantum annealing correction method.\n\nThe QCC was launched in November, 2011 under the leadership of Scientific and Technical Director Daniel Lidar, a USC professor of electrical engineering, chemistry and physics; Operational Director Robert F. Lucas, director of ISI’s Computational Systems and Technology division; and Ned Allen and Greg Tallant of Lockheed Martin. The QCC began with a 128-qubit D-Wave One, which was replaced in March 2013 with the 512-qubit D-Wave Two.\n\nResearch initially focused on testing whether the D-Wave is in fact a quantum system, and has expanded to benchmarking the D-Wave against classical algorithms, and various applications, including quantum machine learning. Lockheed Martin researchers have focused on the application of adiabatic quantum computing to the problem of verification and validation of control systems and other tasks with similar mathematical structure, such as the design of special wave forms for RF applications with minimal side-lobes.\n\nThe team includes more than a dozen USC faculty members, ISI researchers, postdoctoral and graduate students, and more than 100 Lockheed Martin users.\n\nUSC is located in downtown Los Angeles. ISI is located in Marina del Rey, California. Lockheed Martin headquarters is located in Bethesda, Maryland. D-Wave is located in Burnaby, British Columbia, Canada.\n\n"}
{"id": "12769341", "url": "https://en.wikipedia.org/wiki?curid=12769341", "title": "Unary language", "text": "Unary language\n\nIn computational complexity theory, a unary language or tally language is a formal language (a set of strings) where all strings have the form 1, where \"1\" can be any fixed symbol. For example, the language {1, 111, 1111} is unary, as is the language {1 | \"k\" is prime}. The complexity class of all such languages is sometimes called TALLY.\n\nThe name \"unary\" comes from the fact that a unary language is the encoding of a set of natural numbers in the unary numeral system. Since the universe of strings over any finite alphabet is a countable set, every language can be mapped to a unique set A of natural numbers; thus, every language has a \"unary version\" {1 | \"k\" in A}. Conversely, every unary language has a more compact binary version, the set of binary encodings of natural numbers \"k\" such that 1 is in the language.\n\nSince complexity is usually measured in terms of the length of the input string, the unary version of a language can be \"easier\" than the original language. For example, if a language can be recognized in O(2) time, its unary version can be recognized in O(\"n\") time, because \"n\" has become exponentially larger. More generally, if a language can be recognized in O(f(\"n\")) time and O(g(\"n\")) space, its unary version can be recognized in O(\"n\" + f(log \"n\")) time and O(g(log \"n\")) space (we require O(\"n\") time just to read the input string). However, if membership in a language is undecidable, then membership in its unary version is also undecidable.\n\nTALLY is contained in P/poly—the class of languages that can be recognized in polynomial time given an advice function that depends only on the input length. In this case, the required advice function is very simple—it returns a single bit for each input length \"k\" specifying whether 1 is in the language or not.\n\nA unary language is necessarily a sparse language, since for each \"n\" it contains at most one value of length \"n\" and at most \"n\" values of length at most \"n\", but not all sparse languages are unary; thus TALLY is contained in SPARSE.\n\nIf there exists a unary language that is NP-complete, then P = NP.\n\nThis result can be extended to sparse languages.\n\nIf \"L\" is a unary language, then \"L*\" (the Kleene star of \"L\") is a regular language.\n\nThe complexity class P is the class of the unary languages that can be recognized by a polynomial time Turing machine (given its input written in unary); it is the analogue of the class P. The analogue of NP in the unary setting is NP. A counting class #P, the analogue of #P, is also known.\n\n"}
{"id": "32344", "url": "https://en.wikipedia.org/wiki?curid=32344", "title": "Variance", "text": "Variance\n\nIn probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of (random) numbers are spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by formula_1, formula_2, or formula_3.\n\nThe variance of a random variable formula_4 is the expected value of the squared deviation from the mean of formula_4, formula_6:\nThis definition encompasses random variables that are generated by processes that are discrete, continuous, neither, or mixed. The variance can also be thought of as the covariance of a random variable with itself:\nThe variance is also equivalent to the second cumulant of a probability distribution that generates formula_4. The variance is typically designated as formula_3, formula_11, or simply formula_1 (pronounced \"sigma squared\"). The expression for the variance can be expanded:\n\nIn other words, the variance of is equal to the mean of the square of minus the square of the mean of . This equation should not be used for computations using floating point arithmetic because it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude. There exist numerically stable alternatives.\n\nIf the generator of random variable formula_4 is discrete with probability mass function formula_15 then\n\nor equivalently\n\nwhere formula_18 is the average value, i.e.\n\nThe variance of a set of formula_20 equally likely values can be written as \n\nwhere formula_18 is the expected value, i.e.,\n\nThe variance of a set of formula_20 equally likely values can be equivalently expressed, without directly referring to the mean, in terms of squared deviations of all points from each other: \n\nIf the random variable formula_4 represents samples generated by a continuous distribution with probability density function formula_27, and formula_28 is the corresponding cumulative distribution function, then the population variance is given by\n\nor equivalently and conventionally, \n\nwhere formula_18 is the expected value of formula_4 given by\n\nwith the integrals being definite integrals taken for formula_34 ranging over the range of formula_35\n\nIf a continuous distribution does not have a finite expected value, as is the case for the Cauchy distribution, it does not have a variance either. Many other distributions for which the expected value does exist also do not have a finite variance because the integral in the variance definition diverges. An example is a Pareto distribution whose index formula_36 satisfies formula_37\n\nThe normal distribution with parameters formula_18 and formula_39 is a continuous distribution whose probability density function is given by\nIn this distribution, formula_41 and the variance formula_3 is related with formula_39 via\nThe role of the normal distribution in the central limit theorem is in part responsible for the prevalence of the variance in probability and statistics.\n\nThe exponential distribution with parameter formula_45 is a continuous distribution whose support is the semi-infinite interval formula_46. Its probability density function is given by\n\nand it has expected value formula_48. The variance is equal to\n\nSo for an exponentially distributed random variable, formula_50\n\nThe Poisson distribution with parameter formula_45 is a discrete distribution for formula_52. Its probability mass function is given by\n\nand it has expected value formula_54. The variance is equal to\n\nSo for a Poisson-distributed random variable, formula_56.\n\nThe binomial distribution with parameters formula_20 and formula_58 is a discrete distribution for formula_59. Its probability mass function is given by\nand it has expected value formula_61. The variance is equal to\n\nAs a simple example, the binomial distribution with formula_63 describes the probability of getting formula_36 heads in formula_20 tosses of a fair coin. Thus the expected value of the number of heads is formula_66 and the variance is formula_67\n\nA fair six-sided die can be modeled as a discrete random variable, , with outcomes 1 through 6, each with equal probability 1/6. The expected value of is formula_68 Therefore, the variance of is\n\nThe general formula for the variance of the outcome, , of an die is\n\nVariance is non-negative because the squares are positive or zero:\n\nThe variance of a constant random variable is zero, and if the variance of a variable in a data set is 0, then all the entries have the same value:\n\nVariance is invariant with respect to changes in a location parameter. That is, if a constant is added to all values of the variable, the variance is unchanged:\n\nIf all values are scaled by a constant, the variance is scaled by the square of that constant:\n\nThe variance of a sum of two random variables is given by\n\nwhere is the covariance.\nIn general we have for the sum of formula_77 random variables formula_78:\n\nThese results lead to the variance of a linear combination as:\n\nIf the random variables formula_81 are such that\nthey are said to be uncorrelated. It follows immediately from the expression given earlier that if the random variables formula_81 are uncorrelated, then the variance of their sum is equal to the sum of their variances, or, expressed symbolically:\n\nSince independent random variables are always uncorrelated, the equation above holds in particular when the random variables formula_85 are independent. Thus independence is sufficient but not necessary for the variance of the sum to equal the sum of the variances.\n\nOne reason for the use of the variance in preference to other measures of dispersion is that the variance of the sum (or the difference) of uncorrelated random variables is the sum of their variances:\n\nThis statement is called the Bienaymé formula and was discovered in 1853. It is often made with the stronger condition that the variables are independent, but being uncorrelated suffices. So if all the variables have the same variance σ, then, since division by \"n\" is a linear transformation, this formula immediately implies that the variance of their mean is\n\nThat is, the variance of the mean decreases when \"n\" increases. This formula for the variance of the mean is used in the definition of the standard error of the sample mean, which is used in the central limit theorem.\n\nTo prove the initial statement, it suffices to show that\n\nThe general result then follows by induction. Starting with the definition,\n\nUsing the linearity of the expectation operator and the assumption of independence (or uncorrelatedness) of \"X\" and \"Y\", this further simplifies as follows:\n\nIn general, if the variables are correlated, then the variance of their sum is the sum of their covariances:\n\nHere is the covariance, which is zero for independent random variables (if it exists). The formula states that the variance of a sum is equal to the sum of all elements in the covariance matrix of the components. The next expression states equivalently that the variance of the sum is the sum of the diagonal of covariance matrix plus two times the sum of its upper triangular elements (or its lower triangular elements); this emphasizes that the covariance matrix is symmetric. This formula is used in the theory of Cronbach's alpha in classical test theory.\n\nSo if the variables have equal variance \"σ\" and the average correlation of distinct variables is \"ρ\", then the variance of their mean is\n\nThis implies that the variance of the mean increases with the average of the correlations. In other words, additional correlated observations are not as effective as additional independent observations at reducing the uncertainty of the mean. Moreover, if the variables have unit variance, for example if they are standardized, then this simplifies to\n\nThis formula is used in the Spearman–Brown prediction formula of classical test theory. This converges to \"ρ\" if \"n\" goes to infinity, provided that the average correlation remains constant or converges too. So for the variance of the mean of standardized variables with equal correlations or converging average correlation we have\n\nTherefore, the variance of the mean of a large number of standardized variables is approximately equal to their average correlation. This makes clear that the sample mean of correlated variables does not generally converge to the population mean, even though the law of large numbers states that the sample mean will converge for independent variables.\n\nDefine formula_4 as a column vector of formula_20 random variables formula_97, and formula_98 as a column vector of formula_20 scalars formula_100. Therefore, formula_101 is a linear combination of these random variables, where formula_102 denotes the transpose of formula_98. Also let formula_104 be the covariance matrix of formula_4. The variance of formula_106 is then given by:\n\nThe scaling property and the Bienaymé formula, along with the property of the covariance jointly imply that\n\nThis implies that in a weighted sum of variables, the variable with the largest weight will have a disproportionally large weight in the variance of the total. For example, if \"X\" and \"Y\" are uncorrelated and the weight of \"X\" is two times the weight of \"Y\", then the weight of the variance of \"X\" will be four times the weight of the variance of \"Y\".\n\nThe expression above can be extended to a weighted sum of multiple variables:\n\nIf two variables X and Y are independent, the variance of their product is given by\n\nEquivalently, using the basic properties of expectation, it is given by\n\nIn general, if two variables are statistically dependent, the variance of their product is given by:\n\nThe general formula for variance decomposition or the law of total variance is: If formula_4 and formula_114 are two random variables, and the variance of formula_4 exists, then\n\nThe conditional expectation formula_117 of formula_4 given formula_114, and the conditional variance formula_120 may be understood as follows. Given any particular value \"y\" of the random variable \"Y\", there is a conditional expectation formula_121 given the event \"Y\" = \"y\". This quantity depends on the particular value \"y\"; it is a function formula_122. That same function evaluated at the random variable \"Y\" is the conditional expectation formula_123\n\nIn particular, if formula_114 is a discrete random variable assuming possible values formula_125 with corresponding probabilities formula_126, then in the formula for total variance, the first term on the right-hand side becomes\n\nwhere formula_128. Similarly, the second term on the right-hand side becomes\n\nwhere formula_130 and formula_131. Thus the total variance is given by\n\nA similar formula is applied in analysis of variance, where the corresponding formula is\n\nhere formula_134 refers to the Mean of the Squares. In linear regression analysis the corresponding formula is\n\nThis can also be derived from the additivity of variances, since the total (observed) score is the sum of the predicted score and the error score, where the latter two are uncorrelated.\n\nSimilar decompositions are possible for the sum of squared deviations (sum of squares, formula_136):\n\nA formula often used for deriving the variance of a theoretical distribution is as follows:\n\nThis will be useful when it is possible to derive formulae for the expected value and for the expected value of the square.\n\nThis formula is also sometimes used in connection with the sample variance. While useful for hand calculations, it is not advised for computer calculations as it suffers from catastrophic cancellation if the two components of the equation are similar in magnitude and floating point arithmetic is used. This is discussed in the article Algorithms for calculating variance.\n\nThe population variance for a non-negative random variable can be expressed in terms of the cumulative distribution function \"F\" using\n\nThis expression can be used to calculate the variance in situations where the CDF, but not the density, can be conveniently expressed.\n\nThe second moment of a random variable attains the minimum value when taken around the first moment (i.e., mean) of the random variable, i.e. formula_141. Conversely, if a continuous function formula_142 satisfies formula_143 for all random variables \"X\", then it is necessarily of the form formula_144, where . This also holds in the multidimensional case.\n\nUnlike expected absolute deviation, the variance of a variable has units that are the square of the units of the variable itself. For example, a variable measured in meters will have a variance measured in meters squared. For this reason, describing data sets via their standard deviation or root mean square deviation is often preferred over using the variance. In the dice example the standard deviation is √2.9 ≈ 1.7, slightly larger than the expected absolute deviation of 1.5.\n\nThe standard deviation and the expected absolute deviation can both be used as an indicator of the \"spread\" of a distribution. The standard deviation is more amenable to algebraic manipulation than the expected absolute deviation, and, together with variance and its generalization covariance, is used frequently in theoretical statistics; however the expected absolute deviation tends to be more robust as it is less sensitive to outliers arising from measurement anomalies or an unduly heavy-tailed distribution.\n\nThe delta method uses second-order Taylor expansions to approximate the variance of a function of one or more random variables: see Taylor expansions for the moments of functions of random variables. For example, the approximate variance of a function of one variable is given by\n\nprovided that \"f\" is twice differentiable and that the mean and variance of \"X\" are finite.\n\nReal-world observations such as the measurements of yesterday's rain throughout the day typically cannot be complete sets of all possible observations that could be made. As such, the variance calculated from the finite set will in general not match the variance that would have been calculated from the full population of possible observations. This means that one estimates the mean and variance that would have been calculated from an omniscient set of observations by using an estimator equation. The estimator is a function of the sample of \"n\" observations drawn without observational bias from the whole population of potential observations. In this example that sample would be the set of actual measurements of yesterday's rainfall from available rain gauges within the geography of interest.\n\nThe simplest estimators for population mean and population variance are simply the mean and variance of the sample, the sample mean and (uncorrected) sample variance – these are consistent estimators (they converge to the correct value as the number of samples increases), but can be improved. Estimating the population variance by taking the sample's variance is close to optimal in general, but can be improved in two ways. Most simply, the sample variance is computed as an average of squared deviations about the (sample) mean, by dividing by \"n.\" However, using values other than \"n\" improves the estimator in various ways. Four common values for the denominator are \"n,\" \"n\" − 1, \"n\" + 1, and \"n\" − 1.5: \"n\" is the simplest (population variance of the sample), \"n\" − 1 eliminates bias, \"n\" + 1 minimizes mean squared error for the normal distribution, and \"n\" − 1.5 mostly eliminates bias in unbiased estimation of standard deviation for the normal distribution.\n\nFirstly, if the omniscient mean is unknown (and is computed as the sample mean), then the sample variance is a biased estimator: it underestimates the variance by a factor of (\"n\" − 1) / \"n\"; correcting by this factor (dividing by \"n\" − 1 instead of \"n\") is called Bessel's correction. The resulting estimator is unbiased, and is called the (corrected) sample variance or unbiased sample variance. For example, when \"n\" = 1 the variance of a single observation about the sample mean (itself) is obviously zero regardless of the population variance. If the mean is determined in some other way than from the same samples used to estimate the variance then this bias does not arise and the variance can safely be estimated as that of the samples about the (independently known) mean.\n\nSecondly, the sample variance does not generally minimize mean squared error between sample variance and population variance. Correcting for bias often makes this worse: one can always choose a scale factor that performs better than the corrected sample variance, though the optimal scale factor depends on the excess kurtosis of the population (see mean squared error: variance), and introduces bias. This always consists of scaling down the unbiased estimator (dividing by a number larger than \"n\" − 1), and is a simple example of a shrinkage estimator: one \"shrinks\" the unbiased estimator towards zero. For the normal distribution, dividing by \"n\" + 1 (instead of \"n\" − 1 or \"n\") minimizes mean squared error. The resulting estimator is biased, however, and is known as the biased sample variation.\n\nIn general, the population variance of a \"finite\" population of size \"N\" with values \"x\" is given by\n\nwhere the population mean is\n\nThe population variance can also be computed using\n\nThis is true because\n\nThe population variance matches the variance of the generating probability distribution. In this sense, the concept of population can be extended to continuous random variables with infinite populations.\n\nIn many practical situations, the true variance of a population is not known \"a priori\" and must be computed somehow. When dealing with extremely large populations, it is not possible to count every object in the population, so the computation must be performed on a sample of the population. Sample variance can also be applied to the estimation of the variance of a continuous distribution from a sample of that distribution.\n\nWe take a sample with replacement of \"n\" values \"Y\", ..., \"Y\" from the population, where \"n\" < \"N\", and estimate the variance on the basis of this sample. Directly taking the variance of the sample data gives the average of the squared deviations:\n\nHere, formula_151 denotes the sample mean: \n\nSince the \"Y\" are selected randomly, both formula_151 and formula_154 are random variables. Their expected values can be evaluated by averaging over the ensemble of all possible samples {\"Y\"} of size \"n\" from the population. For formula_154 this gives:\n\nHence formula_154 gives an estimate of the population variance that is biased by a factor of formula_158. For this reason, formula_154 is referred to as the \"biased sample variance\". Correcting for this bias yields the \"unbiased sample variance\":\n\nEither estimator may be simply referred to as the \"sample variance\" when the version can be determined by context. The same proof is also applicable for samples taken from a continuous probability distribution.\n\nThe use of the term \"n\" − 1 is called Bessel's correction, and it is also used in sample covariance and the sample standard deviation (the square root of variance). The square root is a concave function and thus introduces negative bias (by Jensen's inequality), which depends on the distribution, and thus the corrected sample standard deviation (using Bessel's correction) is biased. The unbiased estimation of standard deviation is a technically involved problem, though for the normal distribution using the term \"n\" − 1.5 yields an almost unbiased estimator.\n\nThe unbiased sample variance is a U-statistic for the function \"ƒ\"(\"y\", \"y\") = (\"y\" − \"y\")/2, meaning that it is obtained by averaging a 2-sample statistic over 2-element subsets of the population.\n\nBeing a function of random variables, the sample variance is itself a random variable, and it is natural to study its distribution. In the case that \"Y\" are independent observations from a normal distribution, Cochran's theorem shows that \"S\" follows a scaled chi-squared distribution:\n\nAs a direct consequence, it follows that \n\nand\n\nIf the \"Y\" are independent and identically distributed, but not necessarily normally distributed, then\n\nwhere \"κ\" is the kurtosis of the distribution and \"μ\" is the fourth central moment.\n\nIf the conditions of the law of large numbers hold for the squared observations, \"s\" is a consistent estimator of \"σ\". One can see indeed that the variance of the estimator tends asymptotically to zero. An asymptotically equivalent formula was given in Kenney and Keeping (1951:164), Rose and Smith (2002:264), and Weisstein (n.d.).\n\nSamuelson's inequality is a result that states bounds on the values that individual observations in a sample can take, given that the sample mean and (biased) variance have been calculated. Values must lie within the limits formula_165\n\nIt has been shown that for a sample {\"y\"} of real numbers,\n\nwhere \"y\" is the maximum of the sample, \"A\" is the arithmetic mean, \"H\" is the harmonic mean of the sample and formula_167 is the (biased) variance of the sample.\n\nThis bound has been improved, and it is known that variance is bounded by\n\nwhere \"y\" is the minimum of the sample.\n\nTesting for the equality of two or more variances is difficult. The F test and chi square tests are both adversely affected by non-normality and are not recommended for this purpose.\n\nSeveral non parametric tests have been proposed: these include the Barton–David–Ansari–Freund–Siegel–Tukey test, the Capon test, Mood test, the Klotz test and the Sukhatme test. The Sukhatme test applies to two variances and requires that both medians be known and equal to zero. The Mood, Klotz, Capon and Barton–David–Ansari–Freund–Siegel–Tukey tests also apply to two variances. They allow the median to be unknown but do require that the two medians are equal.\n\nThe Lehmann test is a parametric test of two variances. Of this test there are several variants known. Other tests of the equality of variances include the Box test, the Box–Anderson test and the Moses test.\n\nResampling methods, which include the bootstrap and the jackknife, may be used to test the equality of variances.\n\nThe term \"variance\" was first introduced by Ronald Fisher in his 1918 paper \"The Correlation Between Relatives on the Supposition of Mendelian Inheritance\":\n\nThe great body of available statistics show us that the deviations of a human measurement from its mean follow very closely the Normal Law of Errors, and, therefore, that the variability may be uniformly measured by the standard deviation corresponding to the square root of the mean square error. When there are two independent causes of variability capable of producing in an otherwise uniform population distributions with standard deviations formula_170 and formula_171, it is found that the distribution, when both causes act together, has a standard deviation formula_172. It is therefore desirable in analysing the causes of variability to deal with the square of the standard deviation as the measure of variability. We shall term this quantity the Variance...\n\nThe variance of a probability distribution is analogous to the moment of inertia in classical mechanics of a corresponding mass distribution along a line, with respect to rotation about its center of mass. It is because of this analogy that such things as the variance are called \"moments\" of probability distributions. The covariance matrix is related to the moment of inertia tensor for multivariate distributions. The moment of inertia of a cloud of \"n\" points with a covariance matrix of formula_104 is given by\nThis difference between moment of inertia in physics and in statistics is clear for points that are gathered along a line. Suppose many points are close to the \"x\" axis and distributed along it. The covariance matrix might look like\nThat is, there is the most variance in the \"x\" direction. Physicists would consider this to have a low moment \"about\" the \"x\" axis so the moment-of-inertia tensor is\n\nThe \"semivariance\" is calculated in the same manner as the variance but only those observations that fall below the mean are included in the calculation. It is sometimes described as a measure of downside risk in an investments context. For skewed distributions, the semivariance can provide additional information that a variance does not.\n\nFor inequalities associated with the semivariance, see .\n\nIf formula_34 is a scalar complex-valued random variable, with values in formula_178 then its variance is formula_179 where formula_180 is the complex conjugate of formula_181 This variance is a real scalar.\n\nIf formula_4 is a vector-valued random variable, with values in formula_183 and thought of as a column vector, then a natural generalization of variance is formula_184 where formula_185 and formula_186 is the transpose of formula_187 and so is a row vector. The result is a positive semi-definite square matrix, commonly referred to as the variance-covariance matrix (or simply as the \"covariance matrix\").\n\nIf formula_4 is a vector- and complex-valued random variable, with values in formula_189 then the covariance matrix is formula_190 where formula_191 is the conjugate transpose of formula_35 This matrix is also positive semi-definite and square.\n\nAnother natural generalization of variance for such vector-valued random variables formula_187 which results in a scalar value rather than in a matrix, is obtained by interpreting the deviation between the random variable and its mean as the Euclidean distance. This results in formula_194 which is the trace of the covariance matrix.\n"}
{"id": "276852", "url": "https://en.wikipedia.org/wiki?curid=276852", "title": "Windward and leeward", "text": "Windward and leeward\n\nWindward () is the direction upwind from the point of reference, alternatively the direction from which the wind is coming. Leeward () is the direction downwind (or downward) from the point of reference. The leeward region of mountains generally remains dry as compared to the windward. The side of a ship that is towards the leeward is its lee side. If the vessel is heeling under the pressure of the wind, this will be the \"lower side\". During the age of sail, the term \"weather\" was used as a synonym for \"windward\" in some contexts, as in the \"weather gage\".\n\nWindward and leeward directions are important factors (points of sail) to consider when sailing a sailing ship. Other terms with broadly the same meaning are widely used, particularly \"upwind\" and \"downwind\".\n\nThe windward vessel is normally the more maneuverable vessel. For this reason, rule 12 of the International Regulations for Preventing Collisions at Sea stipulates that the windward vessel gives way to the leeward vessel. \n\nIn warfare, a square rigged warship would often try to enter battle from the windward direction (or \"hold the weather gauge\"), thus gaining an important tactical advantage over the opposing warship – the warship to windward could choose when to engage and when to withdraw. The opposing warship to leeward could often do little but comply without exposing itself unduly.\n\nThis was particularly important once artillery was introduced to naval warfare. The ships heeled away from the wind so that the leeward vessel was exposing part of her bottom to shot.\n\n\"Leeward\" and \"windward\" refer respectively to what a game stalker would call downwind and upwind. The terms are used by seamen in relation to their ships but also in reference to islands in an archipelago and to the different sides of a single island. In the latter case, the windward side is that side of an island subject to the prevailing wind, and is thus the wetter side (see orographic precipitation). The leeward side is the side protected by the elevation of the island from the prevailing wind, and is typically the drier side of an island. Thus, leeward or windward siting is an important weather and climate factor on oceanic islands.\n\nIn the case of an archipelago, \"windward islands\" are upwind and \"leeward islands\" are the downwind ones.\n\nIn these contexts the terms \"windward\" and \"layward\" are not used.\n\n"}
{"id": "11757994", "url": "https://en.wikipedia.org/wiki?curid=11757994", "title": "Yamabe problem", "text": "Yamabe problem\n\nThe Yamabe problem in differential geometry concerns the existence of Riemannian metrics with constant scalar curvature, and takes its name from the mathematician Hidehiko Yamabe. claimed to have a solution, but discovered a critical error in his proof. The combined work of Neil Trudinger, Thierry Aubin, and Richard Schoen later provided a complete solution to the problem in 1984.\n\nThe Yamabe problem is the following: Given a smooth, compact manifold of dimension with a Riemannian metric , does there exist a metric conformal to for which the scalar curvature of is constant? In other words, does a smooth function exist on for which the metric has constant scalar curvature? The answer is now known to be yes, and was proved using techniques from differential geometry, functional analysis and partial differential equations.\n\nA closely related question is the so-called \"non-compact Yamabe problem\", which asks: Is it true that on every smooth complete Riemannian manifold which is not compact, there exists a metric that is conformal to \"g\", has constant scalar curvature and is also complete? The answer is no, due to counterexamples given by .\n\n\n"}
