{"id": "379525", "url": "https://en.wikipedia.org/wiki?curid=379525", "title": "35 (number)", "text": "35 (number)\n\n35 (thirty-five) is the natural number following 34 and preceding 36.\n\n35 is the sum of the first five triangular numbers, making it a tetrahedral number.\n\n35 is the number of ways that three things can be selected from a set of seven unique things also known\nas the \"combination of seven things taken three at a time\".\n\n35 is a centered cube number, a pentagonal number and a pentatope number.\n\n35 is a highly cototient number, since there are more solutions to the equation \"x\" − φ(\"x\") = 35 than there are for any other integers below it except 1.\n\nThere are 35 free hexominoes, the polyominoes made from six squares.\n\nSince the greatest prime factor of 35 + 1 = 1226 is 613, which is obviously more than 35 twice, 35 is a Størmer number.\n\n35 is a discrete semiprime (or biprime) (5 × 7); the tenth, and the first with 5 as the lowest non-unitary factor. The aliquot sum of 35 is 13 this being the second composite number with such an aliquot sum; the first being the cube 27. 35 is the last member of the first triple cluster of semiprimes 33, 34, 35. The second such triple discrete semiprime cluster is 85, 86, 87.\n\n35 is the highest number one can count to on one's fingers using base 6.\n\n35 is the number of quasigroups of order 4\n\n\n"}
{"id": "3090256", "url": "https://en.wikipedia.org/wiki?curid=3090256", "title": "Acnode", "text": "Acnode\n\nAn acnode is an isolated point in the solution set of a polynomial equation in two real variables. Equivalent terms are \"isolated point or hermit point\".\n\nFor example the equation\nhas an acnode at the origin, because it is equivalent to\nand formula_3 is non-negative only when formula_4 ≥ 1 or formula_5. Thus, over the \"real\" numbers the equation has no solutions for formula_6 except for (0, 0).\n\nIn contrast, over the complex numbers the origin is not isolated since square roots of negative real numbers exist. In fact, the complex solution set of a polynomial equation in two complex variables can never have an isolated point.\n\nAn acnode is a critical point, or singularity, of the defining polynomial function, in the sense that both partial derivatives formula_7 and formula_8 vanish. Further the Hessian matrix of second derivatives will be positive definite or negative definite, since the function must have a local minimum or a local maximum at the singularity.\n\n"}
{"id": "30350056", "url": "https://en.wikipedia.org/wiki?curid=30350056", "title": "Auslander–Reiten theory", "text": "Auslander–Reiten theory\n\nIn algebra, Auslander–Reiten theory studies the representation theory of Artinian rings using techniques such as Auslander–Reiten sequences (also called almost split sequences) and Auslander–Reiten quivers. Auslander–Reiten theory was introduced by and developed by them in several subsequent papers.\n\nFor survey articles on Auslander–Reiten theory see , , , and the book . Many of the original papers on Auslander–Reiten theory are reprinted in .\n\nSuppose that \"R\" is an Artin algebra. A sequence \nof finitely generated left modules over \"R\" is called an almost-split sequence (or Auslander–Reiten sequence) if it has the following properties:\n\nFor any finitely generated left module \"C\" that is indecomposable but not projective there is an almost-split sequence as above, which is unique up to isomorphism. Similarly for any finitely generated left module \"A\" that is indecomposable but not injective there is an almost-split sequence as above, which is unique up to isomorphism.\n\nThe module \"A\" in the almost split sequence is isomorphic to D Tr \"C\", the dual of the transpose of \"C\".\n\nSuppose that \"R\" is the ring \"k\"[\"x\"]/(\"x\") for a field \"k\" and an integer \"n\"≥1. The indecomposable modules are isomorphic to one of \"k\"[\"x\"]/(\"x\") for 1≤ \"m\" ≤ \"n\", and the only projective one has \"m\"=\"n\". The almost split sequences are isomorphic to \nfor 1 ≤ \"m\" < \"n\". The first morphism takes \"a\" to (\"xa\", \"a\") and the second takes (\"b\",\"c\") to \"b\" − \"xc\".\n\nThe Auslander-Reiten quiver of an Artin algebra has a vertex for each indecomposable module and an arrow between vertices if there is an irreducible morphism between the corresponding modules. It has a map τ = \"D Tr\" called the translation from the non-projective vertices to the non-injective vertices, where \"D\" is the dual and \"Tr\" the transpose.\n\n"}
{"id": "34632550", "url": "https://en.wikipedia.org/wiki?curid=34632550", "title": "Avner Magen", "text": "Avner Magen\n\nAvner Magen (March 30, 1968 – May 29, 2010) was an associate professor of computer science at the University of Toronto whose research focused on the theory of metric embeddings, discrete geometry and computational geometry. He completed his undergraduate and graduate studies at the Hebrew University of Jerusalem, and received his Ph.D. in Computer Science in 2002, under the supervision of Nati Linial. He held a postdoctoral fellowship at NEC Research in Princeton, New Jersey, from 2000 until 2002. He joined the University of Toronto in 2002, first as a postdoctoral fellow, and then as an assistant professor in 2004. He was promoted to associate professor in 2009.\n\nHis major contributions include an algorithm for approximating the weight of the Euclidean minimum spanning tree in sublinear time, and finding a tight integrality gap for the vertex cover problem using the Frankl–Rödl graphs. He proved with his coauthors essentially that a huge class of semidefinite programming algorithms for the famous vertex cover problem will not achieve a solution of value less than the value of the optimal solution times a factor of two. With Nati Linial and Michael Saks, he showed how to embed trees into Euclidean metrics with low distortion. And in a later result, he showed how to do JL-style embeddings that preserved not only distances, but also higher order volumes.\n\nHe died in a climbing accident in Alaska on May 29, 2010, leaving behind three children and a wife.\n\n"}
{"id": "8282811", "url": "https://en.wikipedia.org/wiki?curid=8282811", "title": "Ban number", "text": "Ban number\n\nIn recreational mathematics, a ban number is a number that does not contain a particular letter when spelled out in English; in other words, the letter is \"banned.\" Ban numbers are not precisely defined, since some large numbers do not follow the standards of number names (such as googol and googolplex).\n\nThere are several published sequences of ban numbers:\n\n"}
{"id": "34475270", "url": "https://en.wikipedia.org/wiki?curid=34475270", "title": "Basil Gordon", "text": "Basil Gordon\n\nBasil Gordon (December 23, 1931 – January 12, 2012) was a mathematician at UCLA, specializing in number theory and combinatorics. He obtained his Ph.D. at California Institute of Technology under the supervision of Tom Apostol. Ken Ono was one of his students.\n\nGordon is well known for Göllnitz–Gordon identities, generalizing the Rogers–Ramanujan identities. He also posed the still-unsolved Gaussian moat problem in 1962.\n\nGordon was drafted into the US Army, where he worked with the former Nazi rocket scientist, Wernher von Braun. Gordon's calculations of the gravitational interactions of earth, moon, and satellite contributed to the success and longevity of Explorer I, which launched in 1958 and remained in orbit until 1970. He was the step-grandson of General George Barnett and is a descendant of the Gordon family of British distillers, producers of Gordon's Gin.\n\n"}
{"id": "7807871", "url": "https://en.wikipedia.org/wiki?curid=7807871", "title": "Basu's theorem", "text": "Basu's theorem\n\nIn statistics, Basu's theorem states that any boundedly complete sufficient statistic is independent of any ancillary statistic. This is a 1955 result of Debabrata Basu.\n\nIt is often used in statistics as a tool to prove independence of two statistics, by first demonstrating one is complete sufficient and the other is ancillary, then appealing to the theorem. An example of this is to show that the sample mean and sample variance of a normal distribution are independent statistics, which is done in the Example section below. This property (independence of sample mean and sample variance) characterizes normal distributions.\n\nLet formula_1 be a family of distributions on a measurable space formula_2 and formula_3 measurable maps from formula_2 to some measurable space formula_5. (Such maps are called a statistic.) If formula_6 is a boundedly complete sufficient statistic for formula_7, and formula_8 is ancillary to formula_7, then formula_6 is independent of formula_8.\n\nLet formula_12 and formula_13 be the marginal distributions of formula_6 and formula_8 respectively.\n\nDenote by formula_16 the preimage of a set formula_17 under the map formula_8. For any measurable set formula_19 we have\n\nThe distribution formula_13 does not depend on formula_7 because formula_8 is ancillary. Likewise, formula_24 does not depend on formula_7 because formula_6 is sufficient. Therefore\n\nNote the integrand (the function inside the integral) is a function of formula_28 and not formula_7. Therefore, since formula_6 is boundedly complete the function\n\nis zero for formula_12 almost all values of formula_28 and thus\n\nfor almost all formula_28. Therefore, formula_8 is independent of formula_6.\n\nLet \"X\", \"X\", ..., \"X\" be independent, identically distributed normal random variables with mean \"μ\" and variance \"σ\".\n\nThen with respect to the parameter \"μ\", one can show that\n\nthe sample mean, is a complete sufficient statistic – it is all the information one can derive to estimate \"μ,\" and no more – and\n\nthe sample variance, is an ancillary statistic – its distribution does not depend on \"μ.\"\n\nTherefore, from Basu's theorem it follows that these statistics are independent.\n\nThis independence result can also be proven by Cochran's theorem.\n\nFurther, this property (that the sample mean and sample variance of the normal distribution are independent) \"characterizes\" the normal distribution – no other distribution has this property.\n\n"}
{"id": "361157", "url": "https://en.wikipedia.org/wiki?curid=361157", "title": "Bio-inspired computing", "text": "Bio-inspired computing\n\nBio-inspired computing, short for biologically inspired computing, is a field of study that loosely knits together subfields related to the topics of connectionism, social behaviour and emergence. It is often closely related to the field of artificial intelligence, as many of its pursuits can be linked to machine learning. It relies heavily on the fields of biology, computer science and mathematics. Briefly put, it is the use of computers to model the living phenomena, and simultaneously the study of life to improve the usage of computers. Biologically inspired computing is a major subset of natural computation.\n\nSome areas of study encompassed under the canon of biologically inspired computing, and their biological counterparts:\n\n\nThe way in which bio-inspired computing differs from the traditional artificial intelligence (AI) is in how it takes a more evolutionary approach to learning, as opposed to what could be described as 'creationist' methods used in traditional AI. In traditional AI, intelligence is often programmed from above: the programmer is the creator, and makes something and imbues it with its intelligence. Bio-inspired computing, on the other hand, takes a more bottom-up, decentralised approach; bio-inspired techniques often involve the method of specifying a set of simple rules, a set of simple organisms which adhere to those rules, and a method of iteratively applying those rules. For example, training a virtual insect to navigate in an unknown terrain for finding food includes six simple rules. The insect is trained to \nThe virtual insect controlled by the trained spiking neural network can find food after training in any unknown terrain. After several generations of rule application it is usually the case that some forms of complex behaviour arise. Complexity gets built upon complexity until the end result is something markedly complex, and quite often completely counterintuitive from what the original rules would be expected to produce (see complex systems). For this reason, in neural network models, it is necessary to accurately model an \"in vivo\" network, by live collection of \"noise\" coefficients that can be used to refine statistical inference and extrapolation as system complexity increases.\n\nNatural evolution is a good analogy to this method–the rules of evolution (selection, recombination/reproduction, mutation and more recently transposition) are in principle simple rules, yet over millions of years have produced remarkably complex organisms. A similar technique is used in genetic algorithms.\n\n\n\n\"(the following are presented in ascending order of complexity and depth, with those new to the field suggested to start from the top)\"\n\n\n"}
{"id": "3027085", "url": "https://en.wikipedia.org/wiki?curid=3027085", "title": "Boustrophedon transform", "text": "Boustrophedon transform\n\nIn mathematics, the boustrophedon transform is a procedure which maps one sequence to another. The transformed sequence is computed by filling a triangular array in boustrophedon (zig-zag) manner.\n\nGiven a sequence formula_1, the boustrophedon transform yields another sequence, formula_2, which is constructed by filling up a triangle as pictured on the right. Number the rows in the triangle starting from 0, and fill the rows consecutively. Let \"k\" denote the number of the row currently being filled.\n\nIf \"k\" is odd, then put the number formula_3 on the right end of the row and fill the row from the right to the left, with every entry being the sum of the number to the right and the number to the upper right. If \"k\" is even, then put the number formula_3 on the left end and fill the row from the left to the right, with every entry being the sum of the number to the left and the number to the upper left.\n\nDefining formula_5, the numbers formula_6 forming the transformed sequence can then be found on the left end of odd-numbered rows and on the right end of even-numbered rows, that is, opposite to the numbers formula_3.\n\nA more formal definition uses a recurrence relation. Define the numbers formula_8 (with \"k\" ≥ \"n\" ≥ 0) by\nThen the transformed sequence is defined by formula_11.\n\nIn the case \"a\" = 1, \"a\" = 0 (\"n\" > 0), the resulting triangle is called the Seidel–Entringer–Arnold Triangle and the numbers formula_8 are called Entringer numbers . In this case the numbers in the transformed sequence \"b\" are called the Euler up/down numbers. This is sequence on the On-Line Encyclopedia of Integer Sequences. These enumerate the number of alternating permutations on \"n\" letters and are related to the Euler numbers and the Bernoulli numbers.\n\nThe terms of a sequence and its boustrophedon transform are related by\n\nwhere is the sequence of up/down numbers.\n\nThe exponential generating function of a sequence (\"a\") is defined by\nThe exponential generating function of the boustrophedon transform (\"b\") is related to that of the original sequence (\"a\") by\n\nThe exponential generating function of the unit sequence is 1, so that of the up/down numbers is sec \"x\" + tan \"x\".\n\n"}
{"id": "1497463", "url": "https://en.wikipedia.org/wiki?curid=1497463", "title": "Conformable matrix", "text": "Conformable matrix\n\nIn mathematics, a matrix is conformable if its dimensions are suitable for defining some operation (\"e.g.\" addition, multiplication, etc.).\n\n\n"}
{"id": "49749141", "url": "https://en.wikipedia.org/wiki?curid=49749141", "title": "Conservative functor", "text": "Conservative functor\n\nIn category theory, a branch of mathematics, a conservative functor is a functor formula_1 such that for any morphism \"f\" in \"C\", \"F\"(\"f\") being an isomorphism implies that \"f\" is an isomorphism.\n\nThe forgetful functors in algebra, such as from Grp to Set, are conservative. More generally, every monadic functor is conservative. In contrast, the forgetful functor from Top to Set is not conservative because not every continuous bijection is a homeomorphism.\n\nEvery faithful functor from a balanced category is conservative.\n"}
{"id": "55698678", "url": "https://en.wikipedia.org/wiki?curid=55698678", "title": "Constance van Eeden", "text": "Constance van Eeden\n\nConstance van Eeden (born April 6, 1927) is a Dutch mathematical statistician who made \"exceptional contributions to the development of statistical sciences in Canada\". She is interested in nonparametric statistics including maximum likelihood estimation and robust statistics,\nand did foundational work on parameter spaces.\n\nVan Eeden was born in Delft, the daughter of a schoolteacher, and spent her school years in Bergen op Zoom. She earned a bachelor's degree in 1949, master's degree in 1954, and Ph.D. in 1958 from the University of Amsterdam. Her bachelor's degree was in mathematics, physics and astronomy, and her master's degree was in actuarial science. Her doctoral dissertation, with David van Dantzig as promoter and Jan Hemelrijk as an unofficial mentor, was \"Testing and Estimating Ordered Parameters of Probability Distribution\". It would be 29 years before the next Dutch woman earned a doctorate in statistics.\n\nShe worked at the Centrum Wiskunde & Informatica from 1954 until 1960, when she worked as a year as visiting faculty at Michigan State University, with Herman Rubin as mentor. It was in that visit that she met and married her husband, Charles H. Kraft, another statistician;\nthe anti-nepotism rules then in force at many universities, including Michigan State, made it difficult for them both to find positions at the same place. From 1961 until 1965 she was at the University of Minnesota, first as a research associate and then as an associate professor, and from 1965 to 1988 she taught at the Université de Montréal.\nShe retired in 1989, and has held visiting and honorary positions at the Université du Québec à Montréal and University of British Columbia since then.\n\nWith her husband, Charles H. Kraft, she wrote \"A Nonparametric Introduction to Statistics\" (Macmillan, 1968).\nShe was editor-in-chief of \"Statistical Theory and Methods Abstracts\" from 1990 to 2004.\n\nShe has been a fellow of the American Statistical Association and Institute of Mathematical Statistics since 1973, and an elected member of the International Statistical Institute since 1978.\n\nThe International Statistical Institute gave her their Henri Willem Methorst Medal for outstanding service in 1999.\nThe Statistical Society of Canada gave her their gold medal in 1990, and made her an honorary member in 2011.\n\nIn May 2002, a symposium was held in honour of her 75th birthday, and a festschrift published from it.\n"}
{"id": "1532579", "url": "https://en.wikipedia.org/wiki?curid=1532579", "title": "Equidistant", "text": "Equidistant\n\nA point is said to be equidistant from a set of objects if the distances between that point and each object in the set are equal.\n\nIn two-dimensional Euclidean geometry, the locus of points equidistant from two given (different) points is their perpendicular bisector. In three dimensions, the locus of points equidistant from two given points is a plane, and generalising further, in n-dimensional space the locus of points equidistant from two points in n-space is an (n−1)-space.\n\nFor a triangle the circumcentre is a point equidistant from each of the three vertices. Every non-degenerate triangle has such a point. This result can be generalised to cyclic polygons: the circumcentre is equidistant from each of the vertices. Likewise, the incentre of a triangle or any other tangential polygon is equidistant from the points of tangency of the polygon's sides with the circle. Every point on a perpendicular bisector of the side of a triangle or other polygon is equidistant from the two vertices at the ends of that side. Every point on the bisector of an angle of any polygon is equidistant from the two sides that emanate from that angle.\n\nThe center of a rectangle is equidistant from all four vertices, and it is equidistant from two opposite sides and also equidistant from the other two opposite sides. A point on the axis of symmetry of a kite is equidistant between two sides.\n\nThe center of a circle is equidistant from every point on the circle. Likewise the center of a sphere is equidistant from every point on the sphere.\n\nA parabola is the set of points in a plane equidistant from a fixed point (the focus) and a fixed line (the directrix), where distance from the directrix is measured along a line perpendicular to the directrix.\n\nIn shape analysis, the topological skeleton or medial axis of a shape is a thin version of that shape that is equidistant from its boundaries.\n\nIn Euclidean geometry, parallel lines (lines that never intersect) are equidistant in the sense that the distance of any point on one line from the nearest point on the other line is the same for all points.\n\nIn hyperbolic geometry the set of points that are equidistant from and on one side of a given line form an hypercycle (which is a curve not a line).\n\n"}
{"id": "48528973", "url": "https://en.wikipedia.org/wiki?curid=48528973", "title": "Felix Thomas", "text": "Felix Thomas\n\nFélix Thomas (1815-1875) was a French architect and painter born in Nantes, France in 1815. After graduating from high school Clemenceau, he studied architecture and drafting at the Polytechnique before being admitted to the Beaux-Arts where he studied art under Louis-Hippolyte Lebas. His skills as a draftsman led him to work as project architect on several major archaeological excavations in Mesopotamia and Assyria during the early 1850s. Archaeological work provided opportunities for Thomas to demonstrate his skills as an illustrator and interpreter of historic architectural buildings and he co-authored an important early book on the archaeology of Nineveh in Assyria. He turned to full-time painting in his later life and is noted for works within the Orientalist genre.\n\nInitially, Thomas trained as an architect or draftsman at l'Ecole Polytechnique (1834–35). He subsequently studied art at Beaux-Arts where he was a pupil of Louis-Hippolyte Lebas who specialised in the history of architecture. In 1845, Thomas won the first Prix de Rome for a project in Architecture Cathedral. In 1849 he submitted 14 drawings of Neptune's Temple at Paestum which were very well received. In 1850, he travelled to Greece and on his way stopped at Constantinople and Smyrna.\n\nIn the early 1850s, Thomas joined several archaeological expeditions in Mesopotamia and Assyria in his capacity as an architect. The first of these expeditions was led by Fulgence Fresnel and Julius Oppert, commencing in 1851. Thomas was expected to describe the monuments and buildings that were discovered as well as to carry out quantity surveys, draw plans, prepare sketches and generally assist with documentation and drawings. He was also required to make casts and stampings of inscriptions, using the new and still secret procedure developed by Lattin de Laval. Due to ill-health, Thomas left Fresnel's Mesopotamian mission prematurely. In spite of that, he still managed to contribute twelve maps to the book of the expedition, \"Expedition Scientifique En Mésopotamie: Exécutée Par Ordre Du Gouvernement De 1851 À 1854\" by Julius Oppert.\n\nAfter recovering from his illness, Thomas rejoined the archaeological team for the Assyrian excavation in 1852. The excavations, originally started by Paul-Emile Botta in 1843, were languishing, and the French government was determined to mount a large-scale operation in Assyria to showcase its dominance in the region. Victor Place, the new French Consul in Mosul hired Thomas to join the expedition as the project designer. The mission which involved the excavation of the palace of the Assyrian King Sargon II in Khorsabad (formerly Nineveh), would become the first systematic excavation of the site. Thomas made substantial contributions to the success of the excavation through his acute observations, the boldness of his reconstructions and the quality of his drawings which contributed to a rich understanding of the architecture of the Palace.\n\nMany of the Assyrian antiquities were lost when the expedition's boat sank at Qnra, on the Tigris, following an attack by local rebels in May, 1855. However, Thomas, who had left earlier, retained his sketches, plans and drawings which subsequently served to illustrate a pioneering text on Assyria and the Palace of King Sargon II entitled \"Ninevah and Assyria\", jointly authored by Victor Place and Felix Thomas in around 1867. In this way, Thomas became a major collaborator and co-author of an important archaeological treatise.\n\nOn his return to France, Thomas gave up archaeology and devoted himself to painting. He joined the studio of Charles Gleyre who became his mentor. His travels in Italy, Greece, and Turkey and the Middle East inspired his artistic vision and he began painting works in the Orientalist genre. He enjoyed only modest success in his second career as a painter. Towards the end of his life, he divided his time between his studio in Nantes and Pornic on the Atlantic coast. The Baron de Girardot, in a book dedicated to him, said about him, \"Modest to a fault, withdrawn and lonely, he painted for him.\" \n\nThomas died in Nantes in April 1875.\n\nThomas is known for the illustrations provided to several important archaeological texts. In his later life, he produced many fine Oriental paintings. One of his works is on display in the Louvre in Paris.\n\n\n\n\n\n"}
{"id": "58071797", "url": "https://en.wikipedia.org/wiki?curid=58071797", "title": "Graciela Boente", "text": "Graciela Boente\n\nGraciela Lina Boente Boente is an Argentine mathematical statistician at the University of Buenos Aires. She is known for her research in robust statistics, and particularly for robust methods for principal component analysis and regression analysis.\n\nBoente earned her Ph.D. in 1983 from the University of Buenos Aires. Her dissertation, \"Robust Principal Components\", was supervised by Victor J. Yohai.\n\nBoente became a Guggenheim Fellow in 2001. In 2008, the Argentine National Academy of Exact, Physical and Natural Sciences gave her their Consecration Prize in recognition of her contributions and teaching. She became an honored fellow of the Institute of Mathematical Statistics in 2013, \"for her research in robust statistics and estimation, and for outstanding service to the statistical community\".\n"}
{"id": "49270083", "url": "https://en.wikipedia.org/wiki?curid=49270083", "title": "Graph edit distance", "text": "Graph edit distance\n\nIn mathematics and computer science, graph edit distance (GED) is a measure of similarity (or dissimilarity) between two graphs.\nThe concept of graph edit distance was first formalized mathematically by Alberto Sanfeliu and King-Sun Fu in 1983.\nA major application of graph edit distance is in inexact graph matching, such\nas error-tolerant pattern recognition in machine learning.\n\nThe graph edit distance between two graphs is related to the\nstring edit distance between strings.\nWith the interpretation of strings as connected, directed acyclic graphs of \nmaximum degree one, classical definitions\nof edit distance such as Levenshtein distance,\nHamming distance\nand Jaro–Winkler distance may be interpreted as graph edit distances\nbetween suitably constrained graphs. Likewise, graph edit distance is\nalso a generalization of tree edit distance between\nrooted trees.\n\nThe mathematical definition of graph edit distance is dependent upon the definitions of\nthe graphs over which it is defined, i.e. whether and how the vertices and edges of the\ngraph are labeled and whether the edges are directed.\nGenerally, given a set of graph edit operations (also known as elementary graph operations), the graph edit distance between two graphs formula_1 and formula_2, written as formula_3 can be defined as\nwhere formula_5 denotes the set of edit paths transforming formula_1 into (a graph isomorphic to) formula_2 and formula_8 is the cost of each graph edit operation formula_9.\n\nThe set of elementary graph edit operators typically includes:\n\nAdditional, but less common operators, include operations such as edge splitting that introduces a new vertex into an edge (also creating a new edge), and edge contraction that eliminates vertices of degree two between edges (of the same color). Although such complex edit operators can be defined in terms of more elementary transformations, their use allows finer parameterization of the cost function formula_10 when the operator is cheaper than the sum of its constituents.\n\nGraph edit distance finds applications in handwriting recognition, fingerprint recognition and cheminformatics.\n\nExact algorithms for computing the graph edit distance between a pair of graphs typically\ntransform the problem into one of finding the minimum cost edit path between the two graphs.\nThe computation of the optimal edit path is cast as a pathfinding search or shortest path problem, often implemented as an A* search algorithm.\n\nIn addition to exact algorithms, a number of efficient approximation algorithms are\nalso known.\n\nDespite the above algorithms sometimes working well in practice, in general the problem of computing graph edit distance is NP-complete (for a proof that's available online, see Section 2 of Zeng et al.), and is even hard to approximate (formally, it is APX-hard).\n"}
{"id": "7864709", "url": "https://en.wikipedia.org/wiki?curid=7864709", "title": "Green's function (many-body theory)", "text": "Green's function (many-body theory)\n\nIn many-body theory, the term Green's function (or Green function) is sometimes used interchangeably with correlation function, but refers specifically to correlators of field operators or creation and annihilation operators.\n\nThe name comes from the Green's functions used to solve inhomogeneous differential equations, to which they are loosely related. (Specifically, only two-point 'Green's functions' in the case of a non-interacting system are Green's functions in the mathematical sense; the linear operator that they invert is the Hamiltonian operator, which in the non-interacting case is quadratic in the fields.)\n\nWe consider a many-body theory with field operator (annihilation operator written in the position basis) formula_1.\n\nThe Heisenberg operators can be written in terms of Schrödinger operators as\nand the creation operator is formula_3, where formula_4 is the grand-canonical Hamiltonian.\n\nSimilarly, for the imaginary-time operators,\n[Note that the imaginary-time creation operator formula_7 is not the Hermitian conjugate of the annihilation operator formula_8.]\n\nIn real time, the formula_9-point Green function is defined by\nwhere we have used a condensed notation in which formula_11 signifies formula_12 and formula_13 signifies formula_14. The operator formula_15 denotes time ordering, and indicates that the field operators that follow it are to be ordered so that their time arguments increase from right to left.\n\nIn imaginary time, the corresponding definition is\nwhere formula_11 signifies formula_18. (The imaginary-time variables formula_19 are restricted to the range from formula_20 to the inverse temperature formula_21.)\n\nNote regarding signs and normalization used in these definitions: The signs of the Green functions have been chosen so that Fourier transform of the two-point (formula_22) thermal Green function for a free particle is\nand the retarded Green function is\nwhere\nis the Matsubara frequency.\n\nThroughout, formula_26 is formula_27 for bosons and formula_28 for fermions and formula_29 denotes either a commutator or anticommutator as appropriate.\n\nThe Green function with a single pair of arguments (formula_22) is referred to as the two-point function, or propagator. In the presence of both spatial and temporal translational symmetry, it depends only on the difference of its arguments. Taking the Fourier transform with respect to both space and time gives\nwhere the sum is over the appropriate Matsubara frequencies (and the integral involves an implicit factor of formula_32, as usual).\n\nIn real time, we will explicitly indicate the time-ordered function with a superscript T:\n\nThe real-time two-point Green function can be written in terms of 'retarded' and 'advanced' Green functions, which will turn out to have simpler analyticity properties. The retarded and advanced Green functions are defined by\nand\nrespectively.\n\nThey are related to the time-ordered Green function by\nwhere\nis the Bose–Einstein or Fermi–Dirac distribution function.\n\nThe thermal Green functions are defined only when both imaginary-time arguments are within the range formula_20 to formula_39. The two-point Green function has the following properties. (The position or momentum arguments are suppressed in this section.)\n\nFirstly, it depends only on the difference of the imaginary times:\nThe argument formula_41 is allowed to run from formula_42 to formula_39.\n\nSecondly, formula_44 is (anti)periodic under shifts of formula_39. Because of the small domain within which the function is defined, this means just\nfor formula_47. Time ordering is crucial for this property, which can be proved straightforwardly, using the cyclicity of the trace operation.\n\nThese two properties allow for the Fourier transform representation and its inverse,\n\nFinally, note that formula_44 has a discontinuity at formula_50; this is consistent with a long-distance behaviour of formula_51.\n\nThe propagators in real and imaginary time can both be related to the spectral density (or spectral weight), given by\nwhere |⟩ refers to a (many-body) eigenstate of the grand-canonical Hamiltonian , with eigenvalue .\n\nThe imaginary-time propagator is then given by\nand the retarded propagator by\nwhere the limit as formula_55 is implied.\n\nThe advanced propagator is given by the same expression, but with formula_56 in the denominator. \n\nThe time-ordered function can be found in terms of formula_57 and formula_58. As claimed above, formula_59 and formula_60 have simple analyticity properties: the former (latter) has all its poles and discontinuities in the lower (upper) half-plane. \n\nThe thermal propagator formula_61 has all its poles and discontinuities on the imaginary formula_62 axis.\n\nThe spectral density can be found very straightforwardly from formula_57, using the Sokhatsky–Weierstrass theorem\nwhere denotes the Cauchy principal part.\nThis gives\n\nThis furthermore implies that formula_66 obeys the following relationship between its real and imaginary parts:\nwhere formula_68 denotes the principal value of the integral.\n\nThe spectral density obeys a sum rule,\nwhich gives\nas formula_71.\n\nThe similarity of the spectral representations of the imaginary- and real-time Green functions allows us to define the function\nwhich is related to formula_73 and formula_57 by\nand\nA similar expression obviously holds for formula_58.\n\nThe relation between formula_78 and formula_79 is referred to as a Hilbert transform.\n\nWe demonstrate the proof of the spectral representation of the propagator in the case of the thermal Green function, defined as\n\nDue to translational symmetry, it is only necessary to consider formula_81 for formula_82, given by\nInserting a complete set of eigenstates gives\n\nSince formula_85 and formula_86 are eigenstates of formula_87, the Heisenberg operators can be rewritten in terms of Schrödinger operators, giving\nPerforming the Fourier transform then gives\n\nMomentum conservation allows the final term to be written as (up to possible factors of the volume)\nwhich confirms the expressions for the Green functions in the spectral representation.\n\nThe sum rule can be proved by considering the expectation value of the commutator,\nand then inserting a complete set of eigenstates into both terms of the commutator:\n\nSwapping the labels in the first term then gives\nwhich is exactly the result of the integration of .\n\nIn the non-interacting case, formula_94 is an eigenstate with (grand-canonical) energy formula_95, where formula_96 is the single-particle dispersion relation measured with respect to the chemical potential. The spectral density therefore becomes\n\nFrom the commutation relations,\nwith possible factors of the volume again. The sum, which involves the thermal average of the number operator, then gives simply formula_99, leaving\n\nThe imaginary-time propagator is thus\nand the retarded propagator is\n\nAs →∞, the spectral density becomes\nwhere = 0 corresponds to the ground state. Note that only the first (second) term contributes when is positive (negative).\n\nWe can use 'field operators' as above, or creation and annihilation operators associated with other single-particle states, perhaps eigenstates of the (noninteracting) kinetic energy. We then use\nwhere formula_105 is the annihilation operator for the single-particle state formula_106 and formula_107 is that state's wavefunction in the position basis. This gives\nwith a similar expression for formula_109.\n\nThese depend only on the difference of their time arguments, so that\nand\n\nWe can again define retarded and advanced functions in the obvious way; these are related to the time-ordered function in the same way as above.\n\nThe same periodicity properties as described in above apply to formula_112. Specifically,\nand\nfor formula_115.\n\nIn this case,\nwhere formula_117 and formula_118 are many-body states.\n\nThe expressions for the Green functions are modified in the obvious ways:\nand\n\nTheir analyticity properties are identical. The proof follows exactly the same steps, except that the two matrix elements are no longer complex conjugates.\n\nIf the particular single-particle states that are chosen are `single-particle energy eigenstates', i.e.\nthen for formula_122 an eigenstate:\nso is formula_124:\nand so is formula_126:\n\nWe therefore have\n\nWe then rewrite\ntherefore\nuse\nand the fact that the thermal average of the number operator gives the Bose–Einstein or Fermi–Dirac distribution function.\n\nFinally, the spectral density simplifies to give\nso that the thermal Green function is\nand the retarded Green function is\nNote that the noninteracting Green function is diagonal, but this will not be true in the interacting case.\n\n\n\n\n"}
{"id": "57921397", "url": "https://en.wikipedia.org/wiki?curid=57921397", "title": "Isaac ben Moses Eli", "text": "Isaac ben Moses Eli\n\nIsaac ben Moses Eli ha-Sefaradi was a fifteenth century Spanish Jewish mathematician, born at Oriola, Aragon.\n\nAccording to Steinschneider, he may have been one of the Spanish exiles of 1492, probably leaving to Constantinople. He wrote a mathematical work entitled \"Meleket ha-Mispar\", divided into three parts: (1) a theory of numbers, dealing with the first four rules and the extraction of square roots; (2) proportion, etc.; and (3) elementary geometry. The book is an introduction to Euclid, and begins with a definition of the science of figures.\n\n"}
{"id": "53725128", "url": "https://en.wikipedia.org/wiki?curid=53725128", "title": "Joan Ferrini-Mundy", "text": "Joan Ferrini-Mundy\n\nJoan Ferrini-Mundy (born 1954) is a researcher in mathematics education. Her research interests include calculus teaching and learning, mathematics teacher learning, and STEM education policy. She is currently the president of the University of Maine.\n\nFerrini-Mundy earned a Ph.D. in mathematics education from the University of New Hampshire (UNH) in 1980 and spent two years there as postdoctoral associate there. After one year at Mount Holyoke College, she returned to UNH as a faculty member in mathematics until joining the faculty of Michigan State University in 1999. One year later, she chaired the writing group for \"Standards 2000\", a publication from the National Council of Teachers of Mathematics.\n\nIn 2007, Ferrini-Mundy joined the National Science Foundation (NSF) as the director of the new Division of Research on Learning in Formal and Informal Settings in the Directorate for Education and Human Resources; she remained a faculty member at Michigan State until 2010. From 2007 to 2009, she served on the education subcommittee of the National Science and Technology Council.\n\nIn February 2011, Ferrini-Mundy became the Assistant Director of the National Science Foundation's Directorate for Education and Human Resources. In this strategic role, she set the NSF's direction for scientific education. In 2011, Ferrini-Mundy was elected as a Fellow at the American Association for the Advancement of Science and in 2014 she was elected to the Executive Committee of the Association for Women in Mathematics for a 2-year term.\n\nIn June 2017, she was appointed the Chief Operating Officer of the NSF. One year later, she left the NSF to become the 21st president of the University of Maine.\n\nIn 2000, Ferrini-Mundy was the recipient of the Association for Women in Mathematics' Louise Hay Award.\nShe was elected to the 2018 class of fellows of the American Mathematical Society.\n"}
{"id": "22930177", "url": "https://en.wikipedia.org/wiki?curid=22930177", "title": "John Adrian Bondy", "text": "John Adrian Bondy\n\nJohn Adrian Bondy, (Born 1944) a dual British and Canadian citizen, was a professor of graph theory at the University of Waterloo, in Canada. He is a faculty member of Université Lyon 1, France. Bondy is known for his work on Bondy–Chvátal theorem together with Václav Chvátal. His coauthors include Paul Erdős.\n\nBondy received his Ph.D. in graph theory from University of Oxford in 1969. Bondy has served as a managing editor and co-editor-in-chief of the Journal of Combinatorial Theory, Series B.\n\nBondy was dismissed from his tenured position at Waterloo in 1995, after 28 years in which he had been a major contributor to the renown of the University's Department of Combinatorics and Optimization. The reasons for dismissal centered on \"Bondy's acceptance of a teaching post in France, and the acceptability of someone who is on UW's faculty payroll holding a full-time job elsewhere.\"\n\nPaul Erdős, at the time the world's most renowned combinatorialist, returned his honorary doctorate to the University of Waterloo in protest.\n\n\n\n\n"}
{"id": "22907213", "url": "https://en.wikipedia.org/wiki?curid=22907213", "title": "K. S. Chandrasekharan", "text": "K. S. Chandrasekharan\n\nKomaravolu Chandrasekhar (21 November 1920 – 13 April 2017)\nwas a professor at ETH Zurich. and a founding faculty member of School of Mathematics, Tata Institute of Fundamental Research (TIFR). He is known for his work in number theory and summability and was given numerous awards including Padma Shri, Shanti Swarup Bhatnagar Award, Ramanujan Medal, and honorary fellow of TIFR. He was president of the International Mathematical Union (IMU) from 1971 to 1974.\n\nChandrasekhar was born in 1920 to a school headmaster Sri Rajaiah Padmakshamma(Mother).\nChandrasekhar completed his high school from Bapatla village in Guntur from Andhra Pradesh. He completed his M.A. in mathematics from the Presidency College, Chennai and a PhD from the Department of Mathematics, University of Madras in 1942, under the supervision of K. Ananda Rau.\n\nWhen Chandrasekhar was with the Institute for Advanced Study, Princeton, US, Homi Bhabha invited Chandrashekhar to join the School of Mathematics of the Tata Institute of Fundamental Research (TIFR). Chandrashekhar persuaded mathematicians from all over the world, to visit TIFR and deliver courses of lectures. They were L. Schwarz, C. L. Siegel and many more. In 1965, Chandrasekhar, before the death of Dr. Homi Bhabha in a plane crash, left the Tata Institute of Fundamental Research to join the ETH Zurich, where he retired in 1988.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "767253", "url": "https://en.wikipedia.org/wiki?curid=767253", "title": "Karhunen–Loève theorem", "text": "Karhunen–Loève theorem\n\nIn the theory of stochastic processes, the Karhunen–Loève theorem (named after Kari Karhunen and Michel Loève), also known as the Kosambi–Karhunen–Loève theorem is a representation of a stochastic process as an infinite linear combination of orthogonal functions, analogous to a Fourier series representation of a function on a bounded interval. The transformation is also known as Hotelling transform and eigenvector transform, and is closely related to principal component analysis (PCA) technique widely used in image processing and in data analysis in many fields.\n\nStochastic processes given by infinite series of this form were first considered by Damodar Dharmananda Kosambi. There exist many such expansions of a stochastic process: if the process is indexed over , any orthonormal basis of yields an expansion thereof in that form. The importance of the Karhunen–Loève theorem is that it yields the best such basis in the sense that it minimizes the total mean squared error.\nIn contrast to a Fourier series where the coefficients are fixed numbers and the expansion basis consists of sinusoidal functions (that is, sine and cosine functions), the coefficients in the Karhunen–Loève theorem are random variables and the expansion basis depends on the process. In fact, the orthogonal basis functions used in this representation are determined by the covariance function of the process. One can think that the Karhunen–Loève transform adapts to the process in order to produce the best possible basis for its expansion.\n\nIn the case of a \"centered\" stochastic process (\"centered\" means for all ) satisfying a technical continuity condition, admits a decomposition\n\nwhere are pairwise uncorrelated random variables and the functions are continuous real-valued functions on that are pairwise orthogonal in . It is therefore sometimes said that the expansion is \"bi-orthogonal\" since the random coefficients are orthogonal in the probability space while the deterministic functions are orthogonal in the time domain. The general case of a process that is not centered can be brought back to the case of a centered process by considering which is a centered process.\n\nMoreover, if the process is Gaussian, then the random variables are Gaussian and stochastically independent. This result generalizes the \"Karhunen–Loève transform\". An important example of a centered real stochastic process on is the Wiener process; the Karhunen–Loève theorem can be used to provide a canonical orthogonal representation for it. In this case the expansion consists of sinusoidal functions.\n\nThe above expansion into uncorrelated random variables is also known as the \"Karhunen–Loève expansion\" or \"Karhunen–Loève decomposition\". The empirical version (i.e., with the coefficients computed from a sample) is known as the \"Karhunen–Loève transform\" (KLT), \"principal component analysis\", \"proper orthogonal decomposition (POD)\", \"empirical orthogonal functions\" (a term used in meteorology and geophysics), or the \"Hotelling transform\".\n\n\n\nTheorem. Let be a zero-mean square-integrable stochastic process defined over a probability space and indexed over a closed and bounded interval [\"a\", \"b\"], with continuous covariance function \"K\"(\"s\", \"t\").\n\nThen \"K\"(\"s,t\") is a Mercer kernel and letting \"e\" be an orthonormal basis on formed by the eigenfunctions of \"T\" with respective eigenvalues admits the following representation\n\nwhere the convergence is in \"L\", uniform in \"t\" and\n\nFurthermore, the random variables \"Z\" have zero-mean, are uncorrelated and have variance \"λ\"\n\nNote that by generalizations of Mercer's theorem we can replace the interval [\"a\", \"b\"] with other compact spaces \"C\" and the Lebesgue measure on [\"a\", \"b\"] with a Borel measure whose support is \"C\".\n\n\n\n\n\nSince the limit in the mean of jointly Gaussian random variables is jointly Gaussian, and jointly Gaussian random (centered) variables are independent if and only if they are orthogonal, we can also conclude:\n\nTheorem. The variables have a joint Gaussian distribution and are stochastically independent if the original process is Gaussian.\n\nIn the Gaussian case, since the variables are independent, we can say more:\n\nalmost surely.\n\nThis is a consequence of the independence of the .\n\nIn the introduction, we mentioned that the truncated Karhunen–Loeve expansion was the best approximation of the original process in the sense that it reduces the total mean-square error resulting of its truncation. Because of this property, it is often said that the KL transform optimally compacts the energy.\n\nMore specifically, given any orthonormal basis {\"f\"} of \"L\"([\"a\", \"b\"]), we may decompose the process \"X\" as:\n\nwhere\n\nand we may approximate \"X\" by the finite sum\n\nfor some integer \"N\".\n\nClaim. Of all such approximations, the KL approximation is the one that minimizes the total mean square error (provided we have arranged the eigenvalues in decreasing order).\n\nAn important observation is that since the random coefficients \"Z\" of the KL expansion are uncorrelated, the Bienaymé formula asserts that the variance of \"X\" is simply the sum of the variances of the individual components of the sum:\n\nIntegrating over [\"a\", \"b\"] and using the orthonormality of the \"e\", we obtain that the total variance of the process is:\n\nIn particular, the total variance of the \"N\"-truncated approximation is\n\nAs a result, the \"N\"-truncated expansion explains\n\nof the variance; and if we are content with an approximation that explains, say, 95% of the variance, then we just have to determine an formula_24 such that\n\nGiven a representation of formula_26, for some orthonormal basis formula_27 and random formula_28, we let formula_29, so that formula_30. We may then define the representation entropy to be formula_31. Then we have formula_32, for all choices of formula_33. That is, the KL-expansion has minimal representation entropy.\n\nProof:\n\nDenote the coefficients obtained for the basis formula_34 as formula_35, and for formula_27 as formula_37.\n\nChoose formula_38. Note that since formula_39 minimizes the mean squared error, we have that\n\nExpanding the right hand size, we get:\n\nUsing the orthonormality of formula_27, and expanding formula_43 in the formula_27 basis, we get that the right hand size is equal to:\n\nWe may perform indentitcal analysis for the formula_34, and so rewrite the above inequality as:\n\nSubtracting the common first term, and dividing by formula_48, we obtain that:\n\nThis implies that:\n\nLet us consider a whole class of signals we want to approximate over the first vectors of a basis. These signals are modeled as realizations of a random vector of size . To optimize the approximation we design a basis that minimizes the average approximation error. This section proves that optimal bases are Karhunen–Loeve bases that diagonalize the covariance matrix of . The random vector can be decomposed in an orthogonal basis\n\nas follows:\n\nwhere each\n\nis a random variable. The approximation from the first vectors of the basis is \n\nThe energy conservation in an orthogonal basis implies\n\nThis error is related to the covariance of defined by\n\nFor any vector we denote by the covariance operator represented by this matrix,\n\nThe error is therefore a sum of the last coefficients of the covariance operator\n\nThe covariance operator is Hermitian and Positive and is thus diagonalized in an orthogonal basis called a Karhunen–Loève basis. The following theorem states that a Karhunen–Loève basis is optimal for linear approximations.\n\nTheorem (Optimality of Karhunen–Loève basis). Let be a covariance operator. For all , the approximation error\n\nis minimum if and only if\n\nis a Karhunen–Loeve basis ordered by decreasing eigenvalues.\n\nLinear approximations project the signal on \"M\" vectors a priori. The approximation can be made more precise by choosing the \"M\" orthogonal vectors depending on the signal properties. This section analyzes the general performance of these non-linear approximations. A signal formula_62 is approximated with M vectors selected adaptively in an orthonormal basis for formula_63\n\nLet formula_65 be the projection of f over M vectors whose indices are in :\n\nThe approximation error is the sum of the remaining coefficients\n\nTo minimize this error, the indices in must correspond to the M vectors having the largest inner product amplitude\n\nThese are the vectors that best correlate f. They can thus be interpreted as the main features of f. The resulting error is necessarily smaller than the error of a linear approximation which selects the M approximation vectors independently of f. Let us sort\n\nin decreasing order\n\nThe best non-linear approximation is\n\nIt can also be written as inner product thresholding:\n\nwith\n\nThe non-linear error is\n\nthis error goes quickly to zero as M increases, if the sorted values of formula_75 have a fast decay as k increases. This decay is quantified by computing the formula_76 norm of the signal inner products in B:\n\nThe following theorem relates the decay of to formula_78\n\nTheorem (decay of error). If formula_79 with then\n\nand\nConversely, if formula_82 then\n\nformula_83 for any .\n\nTo further illustrate the differences between linear and non-linear approximations, we study the decomposition of a simple non-Gaussian random vector in a Karhunen–Loève basis. Processes whose realizations have a random translation are stationary. The Karhunen–Loève basis is then a Fourier basis and we study its performance. To simplify the analysis, consider a random vector \"Y\"[\"n\"] of size \"N\" that is random shift modulo \"N\" of a deterministic signal \"f\"[\"n\"] of zero mean\n\nThe random shift \"P\" is uniformly distributed on [0, \"N\" − 1]:\n\nClearly\n\nand\n\nHence\n\nSince R is N periodic, Y is a circular stationary random vector. The covariance operator is a circular convolution with R and is therefore diagonalized in the discrete Fourier Karhunen–Loève basis\n\nThe power spectrum is Fourier transform of \"R\":\n\nExample: Consider an extreme case where formula_92. A theorem stated above guarantees that the Fourier Karhunen–Loève basis produces a smaller expected approximation error than a canonical basis of Diracs formula_93. Indeed, we do not know a priori the abscissa of the non-zero coefficients of \"Y\", so there is no particular Dirac that is better adapted to perform the approximation. But the Fourier vectors cover the whole support of Y and thus absorb a part of the signal energy.\n\nSelecting higher frequency Fourier coefficients yields a better mean-square approximation than choosing a priori a few Dirac vectors to perform the approximation. The situation is totally different for non-linear approximations. If formula_95 then the discrete Fourier basis is extremely inefficient because f and hence Y have an energy that is almost uniformly spread among all Fourier vectors. In contrast, since f has only two non-zero coefficients in the Dirac basis, a non-linear approximation of Y with gives zero error.\n\nWe have established the Karhunen–Loève theorem and derived a few properties thereof. We also noted that one hurdle in its application was the numerical cost of determining the eigenvalues and eigenfunctions of its covariance operator through the Fredholm integral equation of the second kind\n\nHowever, when applied to a discrete and finite process formula_97, the problem takes a much simpler form and standard algebra can be used to carry out the calculations.\n\nNote that a continuous process can also be sampled at \"N\" points in time in order to reduce the problem to a finite version.\n\nWe henceforth consider a random \"N\"-dimensional vector formula_98. As mentioned above, \"X\" could contain \"N\" samples of a signal but it can hold many more representations depending on the field of application. For instance it could be the answers to a survey or economic data in an econometrics analysis.\n\nAs in the continuous version, we assume that \"X\" is centered, otherwise we can let formula_99 (where formula_100 is the mean vector of \"X\") which is centered.\n\nLet us adapt the procedure to the discrete case.\n\nRecall that the main implication and difficulty of the KL transformation is computing the eigenvectors of the linear operator associated to the covariance function, which are given by the solutions to the integral equation written above.\n\nDefine Σ, the covariance matrix of \"X\", as an \"N\" × \"N\" matrix whose elements are given by:\n\nRewriting the above integral equation to suit the discrete case, we observe that it turns into:\n\nwhere formula_103 is an \"N\"-dimensional vector.\n\nThe integral equation thus reduces to a simple matrix eigenvalue problem, which explains why the PCA has such a broad domain of applications.\n\nSince Σ is a positive definite symmetric matrix, it possesses a set of orthonormal eigenvectors forming a basis of formula_104, and we write formula_105 this set of eigenvalues and corresponding eigenvectors, listed in decreasing values of . Let also be the orthonormal matrix consisting of these eigenvectors:\n\nIt remains to perform the actual KL transformation, called the \"principal component transform\" in this case. Recall that the transform was found by expanding the process with respect to the basis spanned by the eigenvectors of the covariance function. In this case, we hence have:\n\nIn a more compact form, the principal component transform of \"X\" is defined by:\n\nThe \"i\"-th component of \"Y\" is formula_109, the projection of \"X\" on formula_110 and the inverse transform yields the expansion of on the space spanned by the formula_110:\n\nAs in the continuous case, we may reduce the dimensionality of the problem by truncating the sum at some formula_113 such that\n\nwhere α is the explained variance threshold we wish to set.\n\nWe can also reduce the dimensionality through the use of multilevel dominant eigenvector estimation (MDEE).\n\nThere are numerous equivalent characterizations of the Wiener process which is a mathematical formalization of Brownian motion. Here we regard it as the centered standard Gaussian process W with covariance function\n\nWe restrict the time domain to [\"a\", \"b\"]=[0,1] without loss of generality.\n\nThe eigenvectors of the covariance kernel are easily determined. These are\nand the corresponding eigenvalues are\n\nThis gives the following representation of the Wiener process:\n\nTheorem. There is a sequence {\"Z\"} of independent Gaussian random variables with mean zero and variance 1 such that\nNote that this representation is only valid for formula_119 On larger intervals, the increments are not independent. As stated in the theorem, convergence is in the L norm and uniform in \"t\".\n\nSimilarly the Brownian bridge formula_120 which is a stochastic process with covariance function\ncan be represented as the series\n\nAdaptive optics systems sometimes use K–L functions to reconstruct wave-front phase information (Dai 1996, JOSA A).\nKarhunen–Loève expansion is closely related to the Singular Value Decomposition. The latter has myriad applications in image processing, radar, seismology, and the like. If one has independent vector observations from a vector valued stochastic process then the left singular vectors are maximum likelihood estimates of the ensemble KL expansion.\n\nIn communication, we usually have to decide whether a signal from a noisy channel contains valuable information. The following hypothesis testing is used for detecting continuous signal \"s\"(\"t\") from channel output \"X\"(\"t\"), \"N\"(\"t\") is the channel noise, which is usually assumed zero mean Gaussian process with correlation function formula_123\n\nWhen the channel noise is white, its correlation function is\n\nand it has constant power spectrum density. In physically practical channel, the noise power is finite, so:\n\nThen the noise correlation function is sinc function with zeros at formula_128 Since are uncorrelated and gaussian, they are independent. Thus we can take samples from \"X\"(\"t\") with time spacing\n\nLet formula_130. We have a total of formula_131 i.i.d observations formula_132 to develop the likelihood-ratio test. Define signal formula_133, the problem becomes,\n\nThe log-likelihood ratio\n\nAs , let:\n\nThen \"G\" is the test statistics and the Neyman–Pearson optimum detector is\n\nAs \"G\" is Gaussian, we can characterize it by finding its mean and variances. Then we get\n\nwhere\n\nis the signal energy.\n\nThe false alarm error\n\nAnd the probability of detection:\n\nwhere Φ is the cdf of standard normal, or Gaussian, variable.\n\nWhen N(t) is colored (correlated in time) Gaussian noise with zero mean and covariance function formula_144 we cannot sample independent discrete observations by evenly spacing the time. Instead, we can use K–L expansion to uncorrelate the noise process and get independent Gaussian observation 'samples'. The K–L expansion of \"N\"(\"t\"):\n\nwhere formula_146 and the orthonormal bases formula_147 are generated by kernel formula_148, i.e., solution to\n\nDo the expansion:\n\nwhere formula_151, then\n\nunder H and formula_153 under K. Let formula_154, we have\n\nHence, the log-LR is given by\n\nand the optimum detector is\n\nDefine\n\nthen formula_164\n\nSince\n\nk(t) is the solution to\n\nIf \"N\"(\"t\")is wide-sense stationary,\n\nwhich is known as the Wiener–Hopf equation. The equation can be solved by taking fourier transform, but not practically realizable since infinite spectrum needs spatial factorization. A special case which is easy to calculate \"k\"(\"t\") is white Gaussian noise.\n\nThe corresponding impulse response is \"h\"(\"t\") = \"k\"(\"T\" − \"t\") = \"CS\"(\"T\" − \"t\"). Let \"C\" = 1, this is just the result we arrived at in previous section for detecting of signal in white noise.\n\nSince X(t) is a Gaussian process,\n\nis a Gaussian random variable that can be characterized by its mean and variance.\n\nHence, we obtain the distributions of \"H\" and \"K\":\n\nThe false alarm error is\n\nSo the test threshold for the Neyman–Pearson optimum detector is\n\nIts power of detection is\n\nWhen the noise is white Gaussian process, the signal power is\n\nFor some type of colored noise, a typical practise is to add a prewhitening filter before the matched filter to transform the colored noise into white noise. For example, N(t) is a wide-sense stationary colored noise with correlation function\n\nThe transfer function of prewhitening filter is\n\nWhen the signal we want to detect from the noisy channel is also random, for example, a white Gaussian process \"X\"(\"t\"), we can still implement K–L expansion to get independent sequence of observation. In this case, the detection problem is described as follows:\n\n\"X\"(\"t\") is a random process with correlation function formula_182\n\nThe K–L expansion of \"X\"(\"t\") is\n\nwhere\n\nand formula_185 are solutions to\n\nSo formula_187's are independent sequence of r.v's with zero mean and variance formula_156. Expanding \"Y\"(\"t\") and \"N\"(\"t\") by formula_185, we get\n\nwhere\n\nAs \"N\"(\"t\") is Gaussian white noise, formula_155's are i.i.d sequence of r.v with zero mean and variance formula_193, then the problem is simplified as follows,\n\nThe Neyman–Pearson optimal test:\n\nso the log-likelihood ratio is\n\nSince\n\nis just the minimum-mean-square estimate of formula_187 given formula_200's,\n\nK–L expansion has the following property: If\n\nwhere\n\nthen\n\nSo let\n\nNoncausal filter \"Q\"(\"t\",\"s\") can be used to get the estimate through\n\nBy orthogonality principle, \"Q\"(\"t\",\"s\") satisfies\n\nHowever, for practical reasons, it's necessary to further derive the causal filter \"h\"(\"t\",\"s\"), where \"h\"(\"t\",\"s\") = 0 for \"s\" > \"t\", to get estimate formula_208. Specifically,\n\n\n\n"}
{"id": "40242283", "url": "https://en.wikipedia.org/wiki?curid=40242283", "title": "Kim Plofker", "text": "Kim Plofker\n\nKim Leslie Plofker (born November 25, 1964) is an American historian of mathematics, specializing in Indian mathematics.\n\nPlofker received her bachelor's degree in mathematics from Haverford College. She received her Ph.D. in 1995 while studying with adviser David Pingree (Mathematical Approximation by Transformation of Sine Functions in Medieval Sanskrit Astronomical Texts) from Brown University, where she conducted research and then later was a guest professor.\n\nIn the late 1990s she was Technical Director of the American Committee for South Asian Manuscripts of the American Oriental Society, where she was also concerned with the development of programs for the text comparison. From 2000 to 2004 she was at the Dibner Institute for the History of Science and Technology at the Massachusetts Institute of Technology. During 2004 and 2005 she was a visiting professor in Utrecht and at the same time Fellow of the International Institute for Asian Studies in Leiden. She is currently an assistant professor at Union College in Schenectady.\n\nPlofker deals with the history of Indian mathematics, the topic of her 2008 book \"Mathematics in India\", which has quickly established itself as a standard work. She is particularly interested in the exchange of mathematics and astronomy between India and Islam in the Middle Ages and generally in the exact sciences between Europe and Asia from antiquity to the 20th Century.\n\nAccording to David Mumford, besides her book \"Mathematics in India\", \"there is only one other survey, Datta and Singh’s 1938 \"History of Hindu Mathematics\"...supplemented by the equally hard to find \"Geometry in Ancient and Medieval India\" by Sarasvati Amma (1979)\", where, \"one can get an overview of most topics\" in Indian mathematics.\n\nIn 2010 she gave a plenary lecture at the International Congress of Mathematicians, Hyderabad (Indian rules, Yavana rules: foreign identity and the transmission of mathematics). In 2011, she was awarded the Brouwer Medal of the Royal Dutch Mathematical Society.\n"}
{"id": "26472757", "url": "https://en.wikipedia.org/wiki?curid=26472757", "title": "List of Polish mathematicians", "text": "List of Polish mathematicians\n\nA list of notable Polish mathematicians:\n\n"}
{"id": "2230309", "url": "https://en.wikipedia.org/wiki?curid=2230309", "title": "List of computer algebra systems", "text": "List of computer algebra systems\n\nThe following tables provide a comparison of computer algebra systems (CAS). A CAS is a package comprising a set of algorithms for performing symbolic manipulations on algebraic objects, a language to implement them, and an environment in which to use the language. A CAS may include a user interface and graphics capability; and to be effective may require a large library of algorithms, efficient data structures and a fast kernel.\n\nThese computer algebra systems are sometimes combined with \"front end\" programs that provide a better user interface, such as the general-purpose GNU TeXmacs.\n\nBelow is a summary of significantly developed \"symbolic\" functionality in each of the systems.\n\nThose which do not \"edit equations\" may have a GUI, plotting, ASCII graphic formulae and math font printing. The ability to generate plaintext files is also a sought-after feature because it allows a work to be understood by people who do not have a computer algebra system installed.\n\nThe software can run under their respective operating systems natively without emulation. Some systems must be compiled first using an appropriate compiler for the source language and target platform. For some platforms, only older releases of the software may be available.\n\nSome graphing calculators have CAS features.\n\n\n"}
{"id": "43667725", "url": "https://en.wikipedia.org/wiki?curid=43667725", "title": "List of things named after Joseph Fourier", "text": "List of things named after Joseph Fourier\n\nThis is a list of things named after Joseph Fourier:\n\n\n\n\n\n"}
{"id": "2470260", "url": "https://en.wikipedia.org/wiki?curid=2470260", "title": "Maris–McGwire–Sosa pair", "text": "Maris–McGwire–Sosa pair\n\nIn recreational mathematics, Maris–McGwire–Sosa pairs (MMS pairs, also MMS numbers) are two consecutive natural numbers such that adding each number's digits (in base 10) to the digits of its prime factorization gives the same sum.\n\nThe above two sums are equal (= 14), so 61 and 62 form an MMS pair.\n\nMMS pairs are so named because in 1998 the baseball players Mark McGwire and Sammy Sosa both hit their 62nd home runs for the season, passing the old record of 61, held by Roger Maris. American engineer Mike Keith noticed this property of these numbers and named pairs of numbers like these MMS pairs.\n\n\n"}
{"id": "244321", "url": "https://en.wikipedia.org/wiki?curid=244321", "title": "Matroid", "text": "Matroid\n\nIn combinatorics, a branch of mathematics, a matroid is a structure that abstracts and generalizes the notion of linear independence in vector spaces. There are many equivalent ways to define a matroid, the most significant being in terms of independent sets, bases, circuits, closed sets or flats, closure operators, and rank functions.\n\nMatroid theory borrows extensively from the terminology of linear algebra and graph theory, largely because it is the abstraction of various notions of central importance in these fields. Matroids have found applications in geometry, topology, combinatorial optimization, network theory and coding theory.\n\nThere are many equivalent (cryptomorphic) ways to define a (finite) matroid.\n\nIn terms of independence, a finite matroid formula_1 is a pair formula_2, where formula_3 is a finite set (called the ground set) and formula_4 is a family of subsets of formula_3 (called the independent sets) with the following properties:\n\nThe first two properties define a combinatorial structure known as an independence system (or abstract simplicial complex).\n\nA subset of the ground set formula_3 that is not independent is called dependent. A maximal independent set—that is, an independent set which becomes dependent on adding any element of formula_3—is called a basis for the matroid. A circuit in a matroid formula_1 is a minimal dependent subset of formula_3—that is, a dependent set whose proper subsets are all independent. The terminology arises because the circuits of graphic matroids are cycles in the corresponding graphs.\n\nThe dependent sets, the bases, or the circuits of a matroid characterize the matroid completely: a set is independent if and only if it is not dependent, if and only if it is a subset of a basis, and if and only if it does not contain a circuit. The collection of dependent sets, or of bases, or of circuits each has simple properties that may be taken as axioms for a matroid. For instance, one may define a matroid formula_1 to be a pair formula_24, where formula_3 is a finite set as before and formula_26 is a collection of subsets of formula_3, called \"bases\", with the following properties:\nIt follows from the basis exchange property that no member of formula_26 can be a proper subset of another.\n\nIt is a basic result of matroid theory, directly analogous to a similar theorem of bases in linear algebra, that any two bases of a matroid formula_1 have the same number of elements. This number is called the rank of formula_1. If formula_1 is a matroid on formula_3, and formula_12 is a subset of formula_3, then a matroid on formula_12 can be defined by considering a subset of formula_12 to be independent if and only if it is independent in formula_1. This allows us to talk about submatroids and about the rank of any subset of formula_3. The rank of a subset formula_12 is given by the rank function formula_47 of the matroid, which has the following properties:\nThese properties can be used as one of the alternative definitions of a finite matroid: if formula_60 satisfies these properties, then the independent sets of a matroid over formula_3 can be defined as those subsets formula_12 of formula_3 with formula_64.\n\nThe difference formula_65 is called the nullity or corank of the subset formula_12. It is the minimum number of elements that must be removed from formula_12 to obtain an independent set. The nullity of formula_3 in formula_1 is called the nullity or corank of formula_1.\n\nLet formula_1 be a matroid on a finite set formula_3, with rank function formula_73 as above. The closure formula_74 of a subset formula_12 of formula_3 is the set\nThis defines a closure operator formula_78 where formula_79 denotes the power set, with the following properties:\nThe first three of these properties are the defining properties of a closure operator. The fourth is sometimes called the Mac Lane–Steinitz exchange property. These properties may be taken as another definition of matroid: every function formula_78 that obeys these properties determines a matroid.\n\nA set whose closure equals itself is said to be closed, or a flat or subspace of the matroid. A set is closed if it is maximal for its rank, meaning that the addition of any other element to the set would increase the rank. The closed sets of a matroid are characterized by a covering partition property:\n\nThe class formula_112 of all flats, partially ordered by set inclusion, forms a matroid lattice.\nConversely, every matroid lattice formula_113 forms a matroid over its set formula_3 of atoms under the following closure operator: for a set formula_100 of atoms with join formula_116,\nThe flats of this matroid correspond one-for-one with the elements of the lattice; the flat corresponding to lattice element formula_118 is the set\nThus, the lattice of flats of this matroid is naturally isomorphic to formula_113.\n\nIn a matroid of rank formula_73, a flat of rank formula_122 is called a hyperplane. (Hyperplanes are also called coatoms or copoints.) These are the maximal proper flats; that is, the only superset of a hyperplane that is also a flat is the set formula_3 of all the elements of the matroid. An equivalent definition: A coatom is a subset of \"E\" that does not span \"M\", but such that adding any other element to it does make a spanning set.\n\nThe family formula_124 of hyperplanes of a matroid has the following properties, which may be taken as yet another axiomatization of matroids:\n\nMinty (1966) defined a graphoid as a triple formula_134 in which formula_135 and formula_136 are classes of nonempty subsets of formula_113 such that \n\nHe proved that there is a matroid for which formula_135 is the class of circuits and formula_136 is the class of cocircuits. Conversely, if formula_135 and formula_136 are the circuit and cocircuit classes of a matroid formula_1 with ground set formula_3, then formula_155 is a graphoid. Thus, graphoids give a self-dual cryptomorphic axiomatization of matroids.\n\nLet formula_3 be a finite set and formula_157 a natural number. One may define a matroid on formula_3 by taking every formula_157-element subset of formula_3 to be a basis. This is known as the uniform matroid of rank formula_157. A uniform matroid with rank formula_157 and with formula_163 elements is denoted formula_164. All uniform matroids of rank at least 2 are simple. The uniform matroid of rank 2 on formula_163 points is called the formula_163-point line. A matroid is uniform if and only if it has no circuits of size less than one plus the rank of the matroid. The direct sums of uniform matroids are called partition matroids.\n\nIn the uniform matroid formula_167, every element is a loop (an element that does not belong to any independent set), and in the uniform matroid formula_168, every element is a coloop (an element that belongs to all bases). The direct sum of matroids of these two types is a partition matroid in which every element is a loop or a coloop; it is called a discrete matroid. An equivalent definition of a discrete matroid is a matroid in which every proper, non-empty subset of the ground set formula_3 is a separator.\n\nMatroid theory developed mainly out of a deep examination of the properties of independence and dimension in vector spaces. There are two ways to present the matroids defined in this way:\n\nA matroid that is equivalent to a vector matroid, although it may be presented differently, is called representable or linear. If formula_1 is equivalent to a vector matroid over a field formula_186, then we say formula_1 is representable over formula_186  ; in particular, formula_1 is real-representable if it is representable over the real numbers. For instance, although a graphic matroid (see below) is presented in terms of a graph, it is also representable by vectors over any field. A basic problem in matroid theory is to characterize the matroids that may be represented over a given field formula_186; Rota's conjecture describes a possible characterization for every finite field. The main results so far are characterizations of binary matroids (those representable over GF(2)) due to Tutte (1950s), of ternary matroids (representable over the 3-element field) due to Reid and Bixby, and separately to Seymour (1970s), and of quaternary matroids (representable over the 4-element field) due to Geelen, Gerards, and Kapoor (2000). This is very much an open area.\n\nA regular matroid is a matroid that is representable over all possible fields. The Vámos matroid is the simplest example of a matroid that is not representable over any field.\n\nA second original source for the theory of matroids is graph theory.\n\nEvery finite graph (or multigraph) formula_191 gives rise to a matroid formula_192 as follows: take as formula_3 the set of all edges in formula_191 and consider a set of edges independent if and only if it is a forest; that is, if it does not contain a simple cycle. Then formula_192 is called a cycle matroid. Matroids derived in this way are graphic matroids. Not every matroid is graphic, but all matroids on three elements are graphic. Every graphic matroid is regular.\n\nOther matroids on graphs were discovered subsequently:\n\nA third original source of matroid theory is field theory.\n\nAn extension of a field gives rise to a matroid. Suppose formula_186 and formula_227 are fields with formula_227 containing formula_186. Let formula_3 be any finite subset of formula_227. Define a subset formula_100 of formula_3 to be algebraically independent if the extension field formula_234 has transcendence degree equal to formula_235.\n\nA matroid that is equivalent to a matroid of this kind is called an algebraic matroid. The problem of characterizing algebraic matroids is extremely difficult; little is known about it. The Vámos matroid provides an example of a matroid that is not algebraic.\n\nThere are some standard ways to make new matroids out of old ones.\n\nIf \"M\" is a finite matroid, we can define the orthogonal or dual matroid \"M\"* by taking the same underlying set and calling a set a \"basis\" in \"M\"* if and only if its complement is a basis in \"M\". It is not difficult to verify that \"M\"* is a matroid and that the dual of \"M\"* is \"M\".\n\nThe dual can be described equally well in terms of other ways to define a matroid. For instance:\n\n\nAccording to a matroid version of Kuratowski's theorem, the dual of a graphic matroid \"M\" is a graphic matroid if and only if \"M\" is the matroid of a planar graph. In this case, the dual of \"M\" is the matroid of the dual graph of \"G\". The dual of a vector matroid representable over a particular field \"F\" is also representable over \"F\". The dual of a transversal matroid is a strict gammoid and vice versa.\n\nExample\n\nThe cycle matroid of a graph is the dual matroid of its bond matroid.\n\nIf \"M\" is a matroid with element set \"E\", and \"S\" is a subset of \"E\", the restriction of \"M\" to \"S\", written \"M\" |\"S\", is the matroid on the set \"S\" whose independent sets are the independent sets of \"M\" that are contained in \"S\". Its circuits are the circuits of \"M\" that are contained in \"S\" and its rank function is that of \"M\" restricted to subsets of \"S\". In linear algebra, this corresponds to restricting to the subspace generated by the vectors in \"S\". Equivalently if \"T\" = \"M\"−\"S\" this may be termed the deletion of \"T\", written \"M\"\\\"T\" or \"M\"−\"T\". The submatroids of \"M\" are precisely the results of a sequence of deletions: the order is irrelevant.\n\nThe dual operation of restriction is contraction. If \"T\" is a subset of \"E\", the contraction of \"M\" by \"T\", written \"M\"/\"T\", is the matroid on the underlying set \"E − T\" whose rank function is formula_237 In linear algebra, this corresponds to looking at the quotient space by the linear space generated by the vectors in \"T\", together with the images of the vectors in \"E - T\".\n\nA matroid \"N\" that is obtained from \"M\" by a sequence of restriction and contraction operations is called a minor of \"M\". We say \"M\" contains \"N\" as a minor. Many important families of matroids may be characterized by the minor-minimal matroids that do not belong to the family; these are called forbidden or excluded minors.\n\nLet \"M\" be a matroid with an underlying set of elements \"E\", and let \"N\" be another matroid on an underlying set \"F\".\nThe direct sum of matroids \"M\" and \"N\" is the matroid whose underlying set is the disjoint union of \"E\" and \"F\", and whose independent sets are the disjoint unions of an independent set of \"M\" with an independent set of \"N\".\n\nThe union of \"M\" and \"N\" is the matroid whose underlying set is the union (not the disjoint union) of \"E\" and \"F\", and whose independent sets are those subsets which are the union of an independent set in \"M\" and one in \"N\". Usually the term \"union\" is applied when \"E\" = \"F\", but that assumption is not essential. If \"E\" and \"F\" are disjoint, the union is the direct sum.\n\nLet \"M\" be a matroid with an underlying set of elements \"E\".\n\nA weighted matroid is a matroid together with a function from its elements to the nonnegative real numbers. The weight of a subset of elements is defined to be the sum of the weights of the elements in the subset. The greedy algorithm can be used to find a maximum-weight basis of the matroid, by starting from the empty set and repeatedly adding one element at a time, at each step choosing a maximum-weight element among the elements whose addition would preserve the independence of the augmented set. This algorithm does not need to know anything about the details of the matroid's definition, as long as it has access to the matroid through an independence oracle, a subroutine for testing whether a set is independent.\n\nThis optimization algorithm may be used to characterize matroids: if a family \"F\" of sets, closed under taking subsets, has the property that, no matter how the sets are weighted, the greedy algorithm finds a maximum-weight set in the family, then \"F\" must be the family of independent sets of a matroid.\n\nThe notion of matroid has been generalized to allow for other types of sets on which a greedy algorithm gives optimal solutions; see greedoid and matroid embedding for more information.\n\nThe matroid partitioning problem is to partition the elements of a matroid into as few independent sets as possible, and the matroid packing problem is to find as many disjoint spanning sets as possible. Both can be solved in polynomial time, and can be generalized to the problem of computing the rank or finding an independent set in a matroid sum.\n\nThe intersection of two or more matroids is the family of sets that are simultaneously independent in each of the matroids. The problem of finding the largest set, or the maximum weighted set, in the intersection of two matroids can be found in polynomial time, and provides a solution to many other important combinatorial optimization problems. For instance, maximum matching in bipartite graphs can be expressed as a problem of intersecting two partition matroids. However, finding the largest set in an intersection of three or more matroids is NP-complete.\n\nTwo standalone systems for calculations with matroids are Kingan's Oid and Hlineny's Macek. Both of them are open sourced packages. \"Oid\" is an interactive, extensible software system for experimenting with matroids. \"Macek\" is a specialized software system with tools and routines for reasonably efficient combinatorial computations with representable matroids.\n\nSAGE, the open source mathematics software system, contains a matroid package.\n\nThere are two especially significant polynomials associated to a finite matroid \"M\" on the ground set \"E\". Each is a matroid invariant, which means that isomorphic matroids have the same polynomial.\n\nThe characteristic polynomial of \"M\" (which is sometimes called the chromatic polynomial, although it does not count colorings), is defined to be\nor equivalently (as long as the empty set is closed in \"M\") as\nwhere μ denotes the Möbius function of the geometric lattice of the matroid and the sum is taken over all the flats A of the matroid.\n\nWhen \"M\" is the cycle matroid \"M\"(\"G\") of a graph \"G\", the characteristic polynomial is a slight transformation of the chromatic polynomial, which is given by χ (λ) = λ\"p\" (λ), where \"c\" is the number of connected components of \"G\".\n\nWhen \"M\" is the bond matroid \"M\"*(\"G\") of a graph \"G\", the characteristic polynomial equals the flow polynomial of \"G\".\n\nWhen \"M\" is the matroid \"M\"(\"A\") of an arrangement \"A\" of linear hyperplanes in R (or \"F\" where \"F\" is any field), the characteristic polynomial of the arrangement is given by \"p\" (λ) = λ\"p\" (λ).\n\nThe beta invariant of a matroid, introduced by Crapo (1967), may be expressed in terms of the characteristic polynomial \"p\" as an evaluation of the derivative\nor directly as\nThe beta invariant is non-negative, and is zero if and only if \"M\" is disconnected, or empty, or a loop. Otherwise it depends only on the lattice of flats of \"M\". If \"M\" has no loops and coloops then β(\"M\") = β(\"M\").\n\nThe Tutte polynomial of a matroid, \"T\" (\"x\",\"y\"), generalizes the characteristic polynomial to two variables. This gives it more combinatorial interpretations, and also gives it the duality property\nwhich implies a number of dualities between properties of \"M\" and properties of \"M\" *. One definition of the Tutte polynomial is\nThis expresses the Tutte polynomial as an evaluation of the corank-nullity or rank generating polynomial,\nFrom this definition it is easy to see that the characteristic polynomial is, up to a simple factor, an evaluation of \"T\", specifically, \n\nAnother definition is in terms of internal and external activities and a sum over bases, reflecting the fact that \"T\"(1,1) is the number of bases. This, which sums over fewer subsets but has more complicated terms, was Tutte's original definition.\n\nThere is a further definition in terms of recursion by deletion and contraction. The deletion-contraction identity is\nAn invariant of matroids (i.e., a function that takes the same value on isomorphic matroids) satisfying this recursion and the multiplicative condition\nis said to be a Tutte-Grothendieck invariant. The Tutte polynomial is the most general such invariant; that is, the Tutte polynomial is a Tutte-Grothendieck invariant and every such invariant is an evaluation of the Tutte polynomial.\n\nThe Tutte polynomial \"T\"  of a graph is the Tutte polynomial \"T\" of its cycle matroid.\n\nThe theory of infinite matroids is much more complicated than that of finite matroids and forms a subject of its own. For a long time, one of the difficulties has been that there were many reasonable and useful definitions, none of which appeared to capture all the important aspects of finite matroid theory. For instance, it seemed to be hard to have bases, circuits, and duality together in one notion of infinite matroids.\n\nThe simplest definition of an infinite matroid is to require \"finite rank\"; that is, the rank of \"E\" is finite. This theory is similar to that of finite matroids except for the failure of duality due to the fact that the dual of an infinite matroid of finite rank does not have finite rank. Finite-rank matroids include any subsets of finite-dimensional vector spaces and of field extensions of finite transcendence degree.\n\nThe next simplest infinite generalization is finitary matroids. A matroid is finitary if it has the property that\nEquivalently, every dependent set contains a finite dependent set.\nExamples are linear dependence of arbitrary subsets of infinite-dimensional vector spaces (but not infinite dependencies as in Hilbert and Banach spaces), and algebraic dependence in arbitrary subsets of field extensions of possibly infinite transcendence degree. Again, the class of finitary matroid is not self-dual, because the dual of a finitary matroid is not finitary.\nFinitary infinite matroids are studied in model theory, a branch of mathematical logic with strong ties to algebra.\n\nIn the late 1960s matroid theorists asked for a more general notion that shares the different aspects of finite matroids and generalizes their duality. Many notions of infinite matroids were defined in response to this challenge, but the question remained open. One of the approaches examined by D.A. Higgs became known as \"B-matroids\" and was studied by Higgs, Oxley and others in the 1960s and 1970s. According to a recent result by , it solves the problem: Arriving at the same notion independently, they provided five equivalent systems of axioms – in terms of independence, bases, circuits, closure and rank. The duality of B-matroids generalizes dualities that can be observed in infinite graphs.\n\nThe independence axioms are as follows:\n\nWith these axioms, every matroid has a dual.\n\nMatroid theory was introduced by . It was also independently discovered by Takeo Nakasawa, whose work was forgotten for many years .\n\nIn his seminal paper, Whitney provided two axioms for independence, and defined any structure adhering to these axioms to be \"matroids\".\nHis key observation was that these axioms provide an abstraction of \"independence\" that is common to both graphs and matrices.\nBecause of this, many of the terms used in matroid theory resemble the terms for their analogous concepts in linear algebra or graph theory.\n\nAlmost immediately after Whitney first wrote about matroids, an important article was written by on the relation of matroids to projective geometry. A year later, noted similarities between algebraic and linear dependence in his classic textbook on Modern Algebra.\n\nIn the 1940s Richard Rado developed further theory under the name \"independence systems\" with an eye towards transversal theory, where his name for the subject is still sometimes used.\n\nIn the 1950s W. T. Tutte became the foremost figure in matroid theory, a position he retained for many years. His contributions were plentiful, including the characterization of binary, regular, and graphic matroids by excluded minors; the regular-matroid representability theorem; the theory of chain groups and their matroids; and the tools he used to prove many of his results, the \"Path theorem\" and \"Homotopy theorem\" (see, e.g., ), which are so complex that later theorists have gone to great trouble to eliminate the necessity of using them in proofs. (A fine example is A. M. H. Gerards' short proof (1989) of Tutte's characterization of regular matroids.)\n\nIn 1976 Dominic Welsh published the first comprehensive book on matroid theory.\n\nPaul Seymour's decomposition theorem for regular matroids (1980) was the most significant and influential work of the late 1970s and the 1980s.\nAnother fundamental contribution, by , showed why projective geometries and Dowling geometries play such an important role in matroid theory.\n\nBy this time there were many other important contributors, but one should not omit to mention Geoff Whittle's extension to ternary matroids of Tutte's characterization of binary matroids that are representable over the rationals , perhaps the biggest single contribution of the 1990s. In the current period (since around 2000) the Matroid Minors Project of Jim Geelen, Gerards, Whittle, and others, which attempts to duplicate for matroids that are representable over a finite field the success of the Robertson–Seymour Graph Minors Project (see Robertson–Seymour theorem), has produced substantial advances in the structure theory of matroids. Many others have also contributed to that part of matroid theory, which (in the first and second decades of the 21st century) is flourishing.\n\nMathematicians who pioneered the study of matroids include Takeo Nakasawa, Saunders Mac Lane, Richard Rado, W. T. Tutte, B. L. van der Waerden, and Hassler Whitney.\nOther major contributors include Jack Edmonds, Jim Geelen, Eugene Lawler, László Lovász, Gian-Carlo Rota, P. D. Seymour, and Dominic Welsh.\n\n\n\n"}
{"id": "50611972", "url": "https://en.wikipedia.org/wiki?curid=50611972", "title": "Micajah Burnett", "text": "Micajah Burnett\n\nMicajah Burnett (13 May 1791 – 10 January 1879) was an American Shaker architect, builder, engineer, surveyor, mathematician, and town planner.\n\nBurnett was born on 13 May 1791 in Patrick County, Virginia, United States. He was the oldest of four children, the others being Charity, Andrew, and Zachiah. By the mid-1790s, his family had settled in Wayne County, Kentucky. In 1808, his parents converted to Shakerism and joined the Pleasant Hill Shaker Society with their four children. Burnett was 17 at the time.\n\nAt the age of 22, Burnett changed the original layout of Pleasant Hill, much of which was on a north-south axis. He reoriented the main road to run east-west, and designed and oversaw the construction of three new dwelling houses along it. The first two were brick structures home to the East and West Families, built in 1817 and 1821, respectively. The third, construction of which began in 1824 and ended ten years later, was the dwelling of the Center Family. Built of white limestone, the building is 55 by 60 feet with an ell of 34 by 85 feet. Besides the three dwellings, Burnett designed the Pleasant Hill Shakers' Meeting House in 1820. A white clapboard structure, it is 60 by 44 feet.\n\nDuring the early 1830s, Burnett directed the construction of the first public waterworks west of the Allegheny Mountains. Part of the project included the erection of Pleasant Hill's water tower, which Burnett designed. Completed in 1831, the tower holds a cypress tank with a capacity of 4,400 gallons of water.\n\nThe tower allowed every dwelling, shop, and barn in the Shaker village to have access to running water. Burnett's expertise in engineering and designing waterworks was such that he was consulted by Shakers at the South Union Shaker Society when they were designing their own waterworks.\n\nWhile Burnett's many designs and completed buildings awarded him renown both among Shakers and outside groups, his most famous project was that of the Pleasant Hill Trustees' House. A large brick building, it was built from 1839–1840 and is best known for its twin spiral staircases, which have the appearance of standing unsupported.\n\nIn addition to his design and construction work, Burnett also had the important job of trustee, which took him to markets across the Mississippi River area selling Shaker good including seeds, brooms, medicinal herbs, raw silk, and fruit preserves. To provide better access to the Kentucky River from Pleasant Hill, he designed and oversaw the construction of a new road. In his later years, he also designed and oversaw the construction of the Pleasant Hill West Family's wash house, the West Family Sisters' Shop, the East Family Brethren's Shop, and the Pleasant Hill U.S. Post Office.\n\nIn 1872, because of his old age, Burnett was released from his trustee duties. He died on 10 January 1879 at the age of 87.\n"}
{"id": "4719864", "url": "https://en.wikipedia.org/wiki?curid=4719864", "title": "Monotone class theorem", "text": "Monotone class theorem\n\nIn measure theory and probability, the monotone class theorem connects monotone classes and sigma-algebras. The theorem says that the smallest monotone class containing an algebra of sets \"G\" is precisely the smallest \"σ\"-algebra containing \"G\". It is used as a type of transfinite induction to prove many other theorems, such as Fubini's theorem.\n\nA monotone class is a class \"M\" of sets that is closed under countable monotone unions and intersections, i.e. if formula_1 and formula_2 then formula_3, and similarly in the other direction.\n\nLet \"G\" be an algebra of sets and define \"M\"(\"G\") to be the smallest monotone class containing \"G\". Then \"M\"(\"G\") is precisely the \"σ\"-algebra generated by \"G\", i.e. \"σ\"(\"G\") = \"M\"(\"G\").\n\nLet formula_4 be a -system that contains formula_5 and let formula_6 be a collection of functions from formula_7 to R with the following properties:\n\n(1) If formula_8, then formula_9\n\n(2) If formula_10, then formula_11 and formula_12 for any real number formula_13\n\n(3) If formula_14 is a sequence of non-negative functions that increase to a bounded function formula_15, then formula_16\n\nThen formula_6 contains all bounded functions that are measurable with respect to formula_18, the sigma-algebra generated by formula_4\n\nThe following argument originates in Rick Durrett's Probability: Theory and Examples.\nThe assumption formula_20, (2) and (3) imply that formula_21 is a \"λ\"-system. By (1) and the −\"λ\" theorem, formula_22. (2) implies formula_6 contains all simple functions, and then (3) implies that formula_6 contains all bounded functions measurable with respect to formula_18.\n\nAs a corollary, if \"G\" is a ring of sets, then the smallest monotone class containing it coincides with the sigma-ring of \"G\".\n\nBy invoking this theorem, one can use monotone classes to help verify that a certain collection of subsets is a sigma-algebra.\n\nThe monotone class theorem for functions can be a powerful tool that allows statements about particularly simple classes of functions to be generalized to arbitrary bounded and measurable functions.\n"}
{"id": "9787563", "url": "https://en.wikipedia.org/wiki?curid=9787563", "title": "Moulton plane", "text": "Moulton plane\n\nIn incidence geometry, the Moulton plane is an example of an affine plane in which Desargues's theorem does not hold. It is named after the American astronomer Forest Ray Moulton. The points of the Moulton plane are simply the points in the real plane R and the lines are the regular lines as well with the exception that for lines with a negative slope, the slope doubles when they pass the \"y\"-axis.\n\nThe Moulton plane is an incidence structure formula_1, where formula_2 denotes the set of points, formula_3 the set of lines and formula_4 the incidence relation \"lies on\":\n\nformula_7 is just a formal symbol for an element formula_8. It is used to describe vertical lines, which you may think of as lines with an infinitely large slope.\n\nThe incidence relation is defined as follows:\n\nFor formula_9 and formula_10 we have\n\nThe Moulton plane is an affine plane in which Desargues' theorem does not hold. The associated projective plane is consequently non-desarguesian as well. This means that there are projective planes not isomorphic to formula_12 for any (skew) field \"F\". Here formula_12 is the projective plane formula_14 determined by a 3-dimensional vector space over the (skew) field \"F\".\n\n"}
{"id": "29722894", "url": "https://en.wikipedia.org/wiki?curid=29722894", "title": "Multi-trials technique", "text": "Multi-trials technique\n\nThe multi-trials technique by Schneider et al. is employed for distributed algorithms and allows breaking of symmetry efficiently. Symmetry breaking is necessary, for instance, in resource allocation problems, where many entities want to access the same resource concurrently. Many message passing algorithms typically employ one attempt to break symmetry per message exchange. The \"multi-trials technique\" transcends this approach through employing more attempts with every message exchange.\n\nFor example, in a simple algorithm for computing an O(Δ) vertex coloring, where Δ denotes the maximum degree in the graph, every uncolored node randomly picks an available color and keeps it if no neighbor (concurrently) chooses the same color. For the multi-trials technique, a node gradually increases the number of chosen colors in every communication round. The technique can yield more than an exponential reduction in the required communication rounds. However, if the maximum degree Δ is small more efficient techniques exist, e.g. the (extended) coin-tossing technique by Richard Cole and Uzi Vishkin.\n\n"}
{"id": "38838646", "url": "https://en.wikipedia.org/wiki?curid=38838646", "title": "Non-autonomous system (mathematics)", "text": "Non-autonomous system (mathematics)\n\nIn mathematics, an autonomous system is a dynamic equation on a smooth manifold. A non-autonomous system is a dynamic equation on a smooth fiber bundle formula_1 over formula_2. For instance, this is the case of non-autonomous mechanics.\n\nAn \"r\"-order differential equation on a fiber bundle formula_1 is represented by a closed subbundle of a jet bundle formula_4 of formula_1. A dynamic equation on formula_1 is a differential equation which is algebraically solved for a higher-order derivatives.\n\nIn particular, a first-order dynamic equation on a fiber bundle formula_1 is a kernel of the covariant differential of some connection formula_8 on formula_1. Given bundle coordinates formula_10 on formula_11 and the adapted coordinates formula_12 on a first-order jet manifold formula_13, a first-order dynamic equation reads\n\nFor instance, this is the case of Hamiltonian non-autonomous mechanics.\n\nA second-order dynamic equation\n\non formula_16 is defined as a holonomic\nconnection formula_17 on a jet bundle formula_18. This\nequation also is represented by a connection on an affine jet bundle formula_19. Due to the canonical\nembedding formula_20, it is equivalent to a geodesic equation\non the tangent bundle formula_21 of formula_11. A free motion equation in non-autonomous mechanics exemplifies a second-order non-autonomous dynamic equation.\n\n\n"}
{"id": "8889938", "url": "https://en.wikipedia.org/wiki?curid=8889938", "title": "Pillai prime", "text": "Pillai prime\n\nIn number theory, a Pillai prime is a prime number \"p\" for which there is an integer \"n\" > 0 such that the factorial of \"n\" is one less than a multiple of the prime, but the prime is not one more than a multiple of \"n\". To put it algebraically, formula_1 but formula_2. The first few Pillai primes are \n\nPillai primes are named after the mathematician Subbayya Sivasankaranarayana Pillai, who asked about these numbers. Their infinitude has been proved several times, by Subbarao, Erdős, and Hardy & Subbarao.\n\n"}
{"id": "2032752", "url": "https://en.wikipedia.org/wiki?curid=2032752", "title": "Reed–Muller code", "text": "Reed–Muller code\n\nReed–Muller codes are error-correcting codes that are used in wireless communications applications, particularly in deep-space communication. Moreover, the proposed 5G standard relies on the closely related polar codes for error correction in the control channel. Due to their favorable theoretical and mathematical properties, Reed–Muller codes have also been extensively studied in theoretical computer science.\n\nReed–Muller codes generalize the Reed–Solomon codes and the Walsh–Hadamard code. Reed–Muller codes are linear block codes that are locally testable, locally decodable, and list decodable. These properties make them particularly useful in the design of probabilistically checkable proofs.\n\nTraditional Reed–Muller codes are binary codes, which means that messages and codewords are binary strings. When \"r\" and \"m\" are integers with 0 ≤ \"r\" ≤ \"m\", the Reed–Muller code with parameters \"r\" and \"m\" is denoted as RM(\"r\", \"m\"). When asked to encode a message consisting of \"k\" bits, where formula_1to produce a holds, the RM(\"r\", \"m\") code produces a codeword consisting of 2 bits.\n\nReed–Muller codes are named after David E. Muller, who discovered the codes in 1954, and Irving S. Reed, who proposed the first efficient decoding algorithm.\n\nReed–Muller codes can be described in several different (but ultimately equivalent) ways. The description that is based on low-degree polynomials is quite elegant and particularly suited for their application as locally testable codes and locally decodable codes.\n\nA block code can have one or more encoding functions formula_2 that map messages formula_3 to codewords formula_4. The Reed–Muller code has message length formula_1 and block length formula_6. One way to define an encoding for this code is based on the evaluation of multilinear polynomials with \"m\" variables and total degree \"r\". Every multilinear polynomial over the finite field with two elements can be written as follows:\nformula_7 and minimum distance formula_8.\n\n\n\n\n\n"}
{"id": "5825422", "url": "https://en.wikipedia.org/wiki?curid=5825422", "title": "Robbins algebra", "text": "Robbins algebra\n\nIn abstract algebra, a Robbins algebra is an algebra containing a single binary operation, usually denoted by formula_1, and a single unary operation usually denoted by formula_2. These operations satisfy the following axioms:\n\nFor all elements \"a\", \"b\", and \"c\":\n\nFor many years, it was conjectured, but unproven, that all Robbins algebras are Boolean algebras. This was proved in 1996, so the term \"Robbins algebra\" is now simply a synonym for \"Boolean algebra\".\n\nIn 1933, Edward Huntington proposed a new set of axioms for Boolean algebras, consisting of (1) and (2) above, plus: \nFrom these axioms, Huntington derived the usual axioms of Boolean algebra.\n\nVery soon thereafter, Herbert Robbins posed the \"Robbins conjecture\", namely that the Huntington equation could be replaced with what came to be called the Robbins equation, and the result would still be Boolean algebra. formula_1 would interpret Boolean join and formula_2 Boolean complement. Boolean meet and the constants 0 and 1 are easily defined from the Robbins algebra primitives. Pending verification of the conjecture, the system of Robbins was called \"Robbins algebra.\"\n\nVerifying the Robbins conjecture required proving Huntington's equation, or some other axiomatization of a Boolean algebra, as theorems of a Robbins algebra. Huntington, Robbins, Alfred Tarski, and others worked on the problem, but failed to find a proof or counterexample.\n\nWilliam McCune proved the conjecture in 1996, using the automated theorem prover EQP. For a complete proof of the Robbins conjecture in one consistent notation and following McCune closely, see Mann (2003). Dahn (1998) simplified McCune's machine proof.\n\n\n"}
{"id": "59217697", "url": "https://en.wikipedia.org/wiki?curid=59217697", "title": "Ruth I. Michler Memorial Prize", "text": "Ruth I. Michler Memorial Prize\n\nThe Ruth I. Michler Memorial Prize is a mathematics prize awarded by the Association for Women in Mathematics.\nThe prize is a fellowship, and lecture at Cornell University.\n\n"}
{"id": "332307", "url": "https://en.wikipedia.org/wiki?curid=332307", "title": "Sociable number", "text": "Sociable number\n\nSociable numbers are numbers whose aliquot sums form a cyclic sequence that begins and ends with the same number. They are generalizations of the concepts of amicable numbers and perfect numbers. The first two sociable sequences, or sociable chains, were discovered and named by the Belgian mathematician Paul Poulet in 1918. In a set of sociable numbers, each number is the sum of the proper factors of the preceding number, i.e., the sum excludes the preceding number itself. For the sequence to be sociable, the sequence must be cyclic and return to its starting point.\n\nThe period of the sequence, or order of the set of sociable numbers, is the number of numbers in this cycle.\n\nIf the period of the sequence is 1, the number is a sociable number of order 1, or a perfect number—for example, the proper divisors of 6 are 1, 2, and 3, whose sum is again 6. A pair of amicable numbers is a set of sociable numbers of order 2. There are no known sociable numbers of order 3, and searches for them have been made up to formula_1 as of 1970 .\n\nIt is an open question whether all numbers end up at either a sociable number or at a prime (and hence 1), or, equivalently, whether there exist numbers whose aliquot sequence never terminates, and hence grows without bound.\n\nAn example with period 4:\n\nThe following categorizes all known sociable numbers as of July 2018 by the length of the corresponding aliquot sequence:\n\nIt is conjectured that if \"n\" = 3 mod 4, then there are no such sequence with length \"n\".\n\nThe smallest number of the only known 28-cycle is 14316.\n\nThe aliquot sequence can be represented as a directed graph, formula_10, for a given integer formula_11, where formula_12 denotes the\nsum of the proper divisors of formula_13.\nCycles in formula_10 represent sociable numbers within the interval formula_15. Two special cases are loops that represent perfect numbers and cycles of length two that represent amicable pairs.\n\nAs the number of sociable number cycles with length greater than 2 approaches infinity, the percentage of the sums of the sociable number cycles divisible by 10 approaches 100%..\n\n\n"}
{"id": "2833034", "url": "https://en.wikipedia.org/wiki?curid=2833034", "title": "St-connectivity", "text": "St-connectivity\n\nIn computer science and computational complexity theory, st-connectivity or STCON is a decision problem asking, for vertices \"s\" and \"t\" in a directed graph, if \"t\" is reachable from \"s\".\n\nFormally, the decision problem is given by\n\nThe problem can be shown to be in NL, as a non-deterministic Turing machine can guess the next node of the path, while the only information which has to be stored is the total length of the path and which node is currently under consideration. The algorithm terminates if either the target node \"t\" is reached, or the length of the path so far exceeds \"n\", the number of nodes in the graph.\n\nThe complement of \"st-connectivity\", known as \"st-non-connectivity\", is also in the class NL, since NL = coNL by the Immerman–Szelepcsényi theorem.\n\nIn particular, the problem of \"st-connectivity\" is actually NL-complete, that is, every problem in the class NL is reducible to connectivity under a log-space reduction. This remains true for the stronger case of first-order reductions . The log-space reduction from any language in NL to STCON proceeds as follows: Consider the non-deterministic log-space Turing machine M that accepts a language in NL. Since there is only logarithmic space on the work tape, all possible states of the Turing machine (where a state is the state of the internal finite state machine, the position of the head and the contents of the work tape) are polynomially many. Map all possible states of the deterministic log-space machine to vertices of a graph, and put an edge between u and v if the state v can be reached from u within one step of the non-deterministic machine. Now the problem of whether the machine accepts is the same as the problem of whether there exists a path from the start state to the accepting state.\n\nSavitch's theorem guarantees that the algorithm can be simulated in \"O\"(log \"n\") deterministic space. \n\nThe same problem for undirected graphs is called \"undirected s-t connectivity\" and was shown to be L-complete by Omer Reingold. This research won him the 2005 Grace Murray Hopper Award. Undirected st-connectivity was previously known to be complete for the class SL, so Reingold's work showed that SL is the same class as L. On alternating graphs, the problem is P-complete .\n\n"}
{"id": "12987835", "url": "https://en.wikipedia.org/wiki?curid=12987835", "title": "The Ambidextrous Universe", "text": "The Ambidextrous Universe\n\nThe Ambidextrous Universe is a popular science book by Martin Gardner, covering aspects of symmetry and asymmetry in human culture, science and the wider universe.\n\nOriginally published in 1964, it underwent revisions in 1969, 1979, 1990 and 2005 (the last two are known as the \"Third, revised edition\"). Originally titled \" The Ambidextrous Universe: Mirror Asymmetry and Time-Reversed Worlds\", subsequent editions are known as \"The New Ambidextrous Universe: Symmetry and Asymmetry from Mirror Reflections to Superstrings\".\n\nThe book begins with the subject of mirror reflection, and from there passes through symmetry in geometry, poetry, art, music, galaxies, suns, planets and living organisms. It then moves down into the molecular scale and looks at how symmetry and asymmetry have evolved from the beginning of life on Earth. There is a chapter on carbon and its versatility and on chirality in biochemistry. Chapter 18 (and subsequent chapters) deals with a conundrum called the Ozma Problem (see below). The second half of the book concerns various aspects of atomic and subatomic physics and how they relate to mirror asymmetry and the related concepts of chirality, antimatter, magnetic and electrical polarity, parity, charge and spin. Time invariance (and reversal) is discussed. Implications for particle physics, theoretical physics and cosmology are covered and brought up to date (in later editions of the book) with regard to GUTs, TOEs, superstring theory and M-theory.\n\nThe 18th chapter, \"The Ozma Problem\", poses a problem that Gardner claims would arise if Earth should ever enter into communication with life on another planet through Project Ozma. This is the problem of how to communicate the meaning of left and right, where the two communicants are conditionally not allowed to view any one object in common. The problem was first implied in Immanuel Kant's discussion of left and right, and William James mentioned it in his chapter on \"The Perception of Space\" in \"The Principles of Psychology\" (1890). It is also mentioned by Charles Howard Hinton. Gardner follows the thread of several false leads on the road to the solution of the Ozma Problem, in each case presenting an apparent solution which, on closer examination, turns out to be a false one.\n\nThe solution to the Ozma Problem was finally embodied in the famous \"Wu experiment\", conducted in 1956 by Chinese-American physicist Chien-Shiung Wu (1912–1997), involving the beta decay of cobalt-60. This experiment was the first to disprove the conservation of parity. At long last, according to Gardner, it is believed that one could carefully describe the Wu experiment to a distant extraterrestrial intelligence and thereby convey the exact meaning of left/right.\n\nW. H. Auden alludes to \"The Ambidextrous Universe\" in his poem \"Josef Weinheber\" (1965).\n\nPale Fire<br>\nIn the original 1964 edition of \"The Ambidextrous Universe\", Gardner quoted two lines of poetry from Vladimir Nabokov's 1962 novel \"Pale Fire\" which are supposed to have been written by a poet, \"John Shade\", who is actually fictional. As a joke, Gardner credited the lines only to Shade and put Shade's name in the index as if he were a real person. In his 1969 novel \"\", Nabokov returned the favor by having the character Van Veen \"quote\" the Gardner book along with the two lines of verse:\n\"Space is a swarming in the eyes, and Time a singing in the ears,\" says John Shade, a modern poet, as quoted by an invented philosopher (\"Martin Gardiner\" ) in \"The Ambidextrous Universe\", page 165 .\n\nLook at the Harlequins!<br>\nNabokov's 1974 novel \"Look at the Harlequins!\", about a man who can't distinguish left from right, was heavily influenced by his reading of \"The Ambidextrous Universe\".\n"}
{"id": "11360573", "url": "https://en.wikipedia.org/wiki?curid=11360573", "title": "The Fibonacci Association", "text": "The Fibonacci Association\n\nThe Fibonacci Association is a mathematical organization that specializes in the Fibonacci number sequence and a wide variety of related subjects, generalizations, and applications, including recurrence relations, combinatorial identities, binomial coefficients, prime numbers, pseudoprimes, continued fractions, the golden ratio, linear algebra, geometry, real analysis, and complex analysis.\n\nThe organization was founded in 1963 by Brother Alfred Brousseau, F.S.C. of St. Mary's College (Moraga, California) and Verner E. Hoggatt Jr. of San Jose State College (now San Jose State University).\n\nDetails regarding the early history of The Fibonacci Association are given Marjorie Bicknell-Johnson's \"A Short History of The Fibonacci Quarterly\", published in The Fibonacci Quarterly 25:1 (February 1987) 2-5, during the Twenty-Fifth Anniversary year of the journal.\n\nSince the year of its founding, the Fibonacci Association has published an international mathematical journal, The \"Fibonacci Quarterly\" .\n\nThe Fibonacci Association also publishes Proceedings for its international conferences, held every two years since 1984. The 2008 conference, formally entitled the Thirteenth International Conference on Fibonacci Numbers and Their Applications, took place at the University of Patras (Greece), preceded by conferences at San Francisco State University (USA, 2006), Technische Universität Braunschweig (Germany, 2004), Northern Arizona University (USA, 2002), and Institut Supérieur de Technologie (Luxemburg, 2000).\n\nThe 2010 Conference was held at the Instituto de Matemáticas de la UNAM, Morelia, Mexico, as announced at the Fibonacci Association website: . The 2012 Conference will take place during June 25–30 at the Institute of Mathematics and Informatics, Eszerházy Károly College, Eger, Hungary, with keynote speaker Neil Sloane, founder of the Encyclopedia of Integer Sequences. \n\n"}
{"id": "583600", "url": "https://en.wikipedia.org/wiki?curid=583600", "title": "Theory of equations", "text": "Theory of equations\n\nIn algebra, the theory of equations is the study of algebraic equations (also called “polynomial equations”), which are equations defined by a polynomial. The main problem of the theory of equations was to know when an algebraic equation has an algebraic solution. This problem was completely solved in 1830 by Évariste Galois, by introducing what is now called Galois theory.\n\nBefore Galois, there was no clear distinction between the “theory of equations” and “algebra”. Since then algebra has been dramatically enlarged to include many new subareas, and the theory of algebraic equations receives much less attention. Thus, the term \"theory of equations\" is mainly used in the context of the history of mathematics, to avoid confusion between old and new meanings of “algebra”.\n\nUntil the end of the 19th century, \"theory of equations\" was almost synonymous with \"algebra\". For a long time, the main problem was to find the solutions of a single non-linear polynomial equation in a single unknown. The fact that a complex solution always exists is the fundamental theorem of algebra, which was proved only at the beginning of the 19th century and does not have a purely algebraic proof. Nevertheless, the main concern of the algebraists was to solve in terms of radicals, that is to express the solutions by a formula which is built with the four operations of arithmetics and with nth roots. This was done up to degree four during the 16th century. Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book \"Ars Magna\", together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his \"L'Algebra\" in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.\n\nThe case of higher degrees remained open until the 19th century, when Niels Henrik Abel proved that some fifth degree equations cannot be solved in radicals (the Abel–Ruffini theorem) and Évariste Galois introduced a theory (presently called Galois theory) to decide which equations are solvable by radicals.\n\nOther classical problems of the theory of equations are the following:\n\n\n"}
{"id": "1070326", "url": "https://en.wikipedia.org/wiki?curid=1070326", "title": "Total derivative", "text": "Total derivative\n\nIn mathematics, the total derivative of a function formula_1 is the best linear approximation of the value of the function with respect to its arguments. Unlike partial derivatives, the total derivative approximates the function with respect to all of its arguments, not just a single one. In many situations, this is the same as considering all partial derivatives simultaneously. The term \"total derivative\" is primarily used when formula_1 is a function of several variables, because when formula_1 is a function of a single variable, the total derivative is the same as the derivative of the function.\n\n\"Total derivative\" is sometimes also used as a synonym for the material derivative in fluid mechanics.\n\nLet formula_4 be an open subset. Then a function formula_5 is said to be (totally) differentiable at a point formula_6 if there exists a linear transformation formula_7 such that\n\nThe linear map formula_9 is called the (total) derivative or (total) differential of formula_1 at formula_11. Other notations for the total derivative include formula_12 and formula_13. A function is (totally) differentiable if its total derivative exists at every point in its domain.\n\nConceptually, the definition of the total derivative expresses the idea that formula_9 is the best linear approximation to formula_1 at the point formula_11. This can be made precise by quantifying the error in the linear approximation determined by formula_9. To do so, write\nwhere formula_19 equals the error in the approximation. To say that the derivative of formula_1 at formula_11 is formula_9 is equivalent to the statement\nwhere formula_24 is Little-o notation and indicates that formula_19 is much smaller than formula_26 as formula_27. The total derivative formula_9 is the \"unique\" linear transformation for which the error term is this small, and this is the sense in which it is the best linear approximation to formula_1.\n\nThe function formula_1 is differentiable if and only if each of its components formula_31 is differentiable, so when studying total derivatives, it is often possible to work one coordinate at a time in the codomain. However, the same is not true of the coordinates in the domain. It is true that if formula_1 is differentiable at formula_11, then each partial derivative formula_34 exists at formula_11. The converse is false: It can happen that all of the partial derivatives of formula_1 at formula_11 exist, but formula_1 is not differentiable at formula_11. This means that the function is very \"rough\" at formula_11, to such an extreme that its behavior cannot be adequately described by its behavior in the coordinate directions. When formula_1 is not so rough, this cannot happen. More precisely, if all the partial derivatives of formula_1 at formula_11 exist and are continuous in a neighborhood of formula_11, then formula_1 is differentiable at formula_11. When this happens, then in addition, the total derivative of formula_1 is the linear transformation corresponding to the Jacobian matrix of partial derivatives at that point.\n\nWhen the function under consideration is real-valued, the total derivative can be recast using differential forms. For example, suppose that formula_48 is a differentiable function of variables formula_49. The total derivative of formula_1 at formula_11 may be written in terms of its Jacobian matrix, which in this instance simplifies to the gradient:\nThe linear approximation property of the total derivative implies that if\nis a small vector (where the formula_54 denotes transpose, so that this vector is a column vector), then\nHeuristically, this suggests that if formula_56 are infinitesimal increments in the coordinate directions, then\n\nThe theory of differential forms is one way to give a precise meaning to infinitesimal increments such as formula_58. In this theory, formula_58 is a linear functional on the vector space formula_60. Evaluating formula_58 at a vector formula_62 in formula_60 measures how much formula_62 points in the formula_65th coordinate direction. The total derivative formula_9 is a linear combination of linear functionals and hence is itself a linear functional. The evaluation formula_67 measures how much formula_62 points in the direction determined by formula_1 at formula_11, and this direction is the gradient. This point of view makes the total derivative an instance of the exterior derivative.\n\nSuppose now that formula_1 is a vector-valued function, that is, formula_72. In this case, the components formula_73 of formula_1 are real-valued functions, so they have associated differential forms formula_75. The total derivative formula_76 amalgamates these forms into a single object and is therefore an instance of a vector-valued differential form.\n\nThe chain rule has a particularly elegant statement in terms of total derivatives. It says that, for two functions formula_1 and formula_78, the total derivative of the composite formula_79 at formula_11 satisfies\nIf the total derivatives of formula_1 and formula_78 are identified with their Jacobian matrices, then the composite on the right-hand side is simply matrix multiplication. This is enormously useful in applications, as it makes it possible to account for essentially arbitrary dependencies among the arguments of a composite function.\n\nSuppose that \"f\" is a function of two variables, \"x\" and \"y\". If these two variables are independent, so that the domain of \"f\" is formula_84, then the behavior of \"f\" may be understood in terms of its partial derivatives in the \"x\" and \"y\" directions. However, in some situations, \"x\" and \"y\" may be dependent. For example, it might happen that \"f\" is constrained to a curve formula_85. In this case, we are actually interested in the behavior of the composite function formula_86. The partial derivative of \"f\" with respect to \"x\" does not give the true rate of change of \"f\" with respect to changing \"x\" because changing \"x\" necessarily changes \"y\". However, the chain rule for the total derivative takes such dependencies into account. Write formula_87. Then chain rule says\nBy expressing the total derivative using Jacobian matrices, this becomes:\nSuppressing the evaluation at formula_90 for legibility, we may also write this as\nThis gives a straightforward formula for the derivative of formula_86 in terms of the partial derivatives of formula_1 and the derivative of formula_94.\n\nFor example, suppose\nThe rate of change of \"f\" with respect to \"x\" is usually the partial derivative of \"f\" with respect to \"x\"; in this case,\nHowever, if \"y\" depends on \"x\", the partial derivative does not give the true rate of change of \"f\" as \"x\" changes because the partial derivative assumes that \"y\" is fixed. Suppose we are constrained to the line\nThen\nand the total derivative of \"f\" with respect to \"x\" is\nwhich we see is not equal to the partial derivative formula_100. Instead of immediately substituting for \"y\" in terms of \"x\", however, we can also use the chain rule as above:\n\nWhile one can often perform substitutions to eliminate indirect dependencies, the chain rule provides for a more efficient and general technique. Suppose formula_102 is a function of time formula_103 and formula_104 variables formula_105 which themselves depend on time. Then, the time derivative of formula_106 is\n\nThe chain rule expresses this derivative in terms of the partial derivatives of formula_106 and the time derivatives of the functions formula_105:\n\nThis expression is often used in physics for a gauge transformation of the Lagrangian, as two Lagrangians that differ only by the total time derivative of a function of time and the formula_104 generalized coordinates lead to the same equations of motion. An interesting example concerns the resolution of causality concerning the Wheeler–Feynman time-symmetric theory. The operator in brackets (in the final expression above) is also called the total derivative operator (with respect to formula_103).\n\nFor example, the total derivative of formula_113 is\n\nHere there is no formula_115 term since formula_1 itself does not depend on the independent variable formula_103 directly.\n\nA \"total differential equation\" is a differential equation expressed in terms of total derivatives. Since the exterior derivative is coordinate-free, in a sense that can be given a technical meaning, such equations are intrinsic and \"geometric\".\n\nIn economics, it is common for the total derivative to arise in the context of a system of equations. For example, a simple supply-demand system might specify the quantity \"q\" of a product demanded as a function \"D\" of its price \"p\" and consumers' income \"I\", the latter being an exogenous variable, and might specify the quantity supplied by producers as a function \"S\" of its price and two exogenous resource cost variables \"r\" and \"w\". The resulting system of equations\ndetermines the market equilibrium values of the variables \"p\" and \"q\". The total derivative formula_120 of \"p\" with respect to \"r\", for example, gives the sign and magnitude of the reaction of the market price to the exogenous variable \"r\". In the indicated system, there are a total of six possible total derivatives, also known in this context as comparative static derivatives: , , , , , and . The total derivatives are found by totally differentiating the system of equations, dividing through by, say , treating and as the unknowns, setting , and solving the two totally differentiated equations simultaneously, typically by using Cramer's rule.\n\n\n"}
{"id": "1388937", "url": "https://en.wikipedia.org/wiki?curid=1388937", "title": "Transactions of the American Mathematical Society", "text": "Transactions of the American Mathematical Society\n\nThe Transactions of the American Mathematical Society is a monthly peer-reviewed scientific journal of mathematics published by the American Mathematical Society. It was established in 1900. As a requirement, all articles must be more than 15 printed pages.\n\n\n"}
{"id": "17457007", "url": "https://en.wikipedia.org/wiki?curid=17457007", "title": "Valuation (measure theory)", "text": "Valuation (measure theory)\n\nIn measure theory, or at least in the approach to it via the domain theory, a valuation is a map from the class of open sets of a topological space to the set of positive real numbers including infinity, with certain properties. It is a concept closely related to that of a measure, and as such, it finds applications in measure theory, probability theory, and theoretical computer science.\n\nLet formula_1 be a topological space: a valuation is any map \n\nsatisfying the following three properties \n\nThe definition immediately shows the relationship between a valuation and a measure: the properties of the two mathematical object are often very similar if not identical, the only difference being that the domain of a measure is the Borel algebra of the given topological space, while the domain of a valuation is the class of open sets. Further details and references can be found in and .\n\nA valuation (as defined in domain theory/measure theory) is said to be continuous if for \"every directed family\" formula_4 \"of open sets\" (i.e. an indexed family of open sets which is also directed in the sense that for each pair of indexes formula_5 and formula_6 belonging to the index set formula_7, there exists an index formula_8 such that formula_9 and formula_10) the following equality holds:\n\nThis property is analogous to the τ-additivity of measures.\n\nA valuation (as defined in domain theory/measure theory) is said to be simple if it is a finite linear combination with non-negative coefficients of Dirac valuations, i.e.\n\nwhere formula_13 is always greater than or at least equal to zero for all index formula_5. Simple valuations are obviously continuous in the above sense. The supremum of a \"directed family of simple valuations\" (i.e. an indexed family of simple valuations which is also directed in the sense that for each pair of indexes formula_5 and formula_6 belonging to the index set formula_7, there exists an index formula_8 such that formula_19 and formula_20) is called quasi-simple valuation\n\n\nLet formula_1 be a topological space, and let \"formula_23\" be a point of \"formula_24\": the map\n\nis a valuation in the domain theory/measure theory, sense called Dirac valuation. This concept bears its origin from distribution theory as it is an obvious transposition to valuation theory of Dirac distribution: as seen above, Dirac valuations are the \"bricks\" simple valuations are made of.\n\n\n"}
{"id": "145555", "url": "https://en.wikipedia.org/wiki?curid=145555", "title": "XOR swap algorithm", "text": "XOR swap algorithm\n\nIn computer programming, the XOR swap is an algorithm that uses the XOR bitwise operation to swap values of distinct variables having the same data type without using a temporary variable. \"Distinct\" means that the variables are stored at different, non-overlapping, memory addresses; the actual values of the variables do not have to be different.\n\nConventional swapping requires the use of a temporary storage variable. Using the XOR swap algorithm, however, no temporary storage is needed. The algorithm is as follows:\nX := X XOR Y\nY := Y XOR X\nX := X XOR Y\nThe algorithm typically corresponds to three machine-code instructions. Since XOR is a commutative operation, X XOR Y can be replaced with Y XOR X in any of the lines. When coded in assembly language, this commutativity is often exercised in the second line:\n\nIn the above System/370 assembly code sample, R1 and R2 are distinct registers, and each XR operation leaves its result in the register named in the first argument. Using x86 assembly, values X and Y are in registers eax and ebx (respectively), and places the result of the operation in the first register.\n\nHowever, the algorithm fails if \"x\" and \"y\" use the same storage location, since the value stored in that location will be zeroed out by the first XOR instruction, and then remain zero; it will not be \"swapped with itself\". Note that this is \"not\" the same as if \"x\" and \"y\" have the same values. The trouble only comes when \"x\" and \"y\" use the same storage location, in which case their values must already be equal. That is, if \"x\" and \"y\" use the same storage location, then the line:\n\nX := X XOR Y\n\nsets \"x\" to zero (because \"x\" = \"y\" so X XOR Y is zero) \"and\" sets \"y\" to zero (since it uses the same storage location), causing \"x\" and \"y\" to lose their original values.\n\nThe binary operation XOR over bit strings of length formula_1 exhibits the following properties (where formula_2 denotes XOR):\n\n\nSuppose that we have two distinct registers codice_1 and codice_2 as in the table below, with initial values \"A\" and \"B\" respectively. We perform the operations below in sequence, and reduce our results using the properties listed above.\n\nAs XOR can be interpreted as binary addition and a pair of values can be interpreted as a point in two-dimensional space, the steps in the algorithm can be interpreted as 2×2 matrices with binary values. For simplicity, assume initially that \"x\" and \"y\" are each single bits, not bit vectors.\n\nFor example, the step:\n\nX := X XOR Y\n\nwhich also has the implicit:\n\nY := Y\n\ncorresponds to the matrix formula_9 as\nThe sequence of operations is then expressed as:\n(working with binary values, so formula_12), which expresses the elementary matrix of switching two rows (or columns) in terms of the transvections (shears) of adding one element to the other.\n\nTo generalize to where X and Y are not single bits, but instead bit vectors of length \"n\", these 2×2 matrices are replaced by 2\"n\"×2\"n\" block matrices such as formula_13\n\nNote that these matrices are operating on \"values,\" not on \"variables\" (with storage locations), hence this interpretation abstracts away from issues of storage location and the problem of both variables sharing the same storage location.\n\nA C function that implements the XOR swap algorithm:\nNote that the code does not swap the integers passed immediately, but first checks if their addresses are distinct. This is because, if the addresses are equal, the algorithm will fold to a triple *x ^= *x resulting in zero.\n\nThe code below is an example of overly-concise C code; the behavior of the code is undefined. (Writing overly-concise C code has become unnecessary as modern optimizing compilers will eliminate intermediate results and temporary variables.)\nThe XOR swap algorithm can also be defined with a macro:\nIn most practical scenarios, the trivial swap algorithm using a temporary register is more efficient. Limited situations in which XOR swapping may be practical include:\n\n\nBecause these situations are rare, most optimizing compilers do not generate XOR swap code.\n\nMost modern compilers can optimize away the temporary variable in the native swap, in which case the native swap uses the same amount of memory and the same number of registers as the XOR swap and is at least as fast, and often faster. The XOR swap is also much less readable and completely opaque to anyone unfamiliar with the technique.\n\nOn modern CPU architectures, the XOR technique can be slower than using a temporary variable to do swapping. One reason is that modern CPUs strive to execute instructions in parallel via instruction pipelines. In the XOR technique, the inputs to each operation depend on the results of the previous operation, so they must be executed in strictly sequential order, negating any benefits of instruction-level parallelism.\n\nA historical reason was that it used to be patented (US4197590). Even then, this was only for computer graphics.\n\nThe XOR swap is also complicated in practice by aliasing. As noted above, if an attempt is made to XOR-swap the contents of some location with itself, the result is that the location is zeroed out and its value lost. Therefore, XOR swapping must not be used blindly in a high-level language if aliasing is possible.\n\nSimilar problems occur with call by name, as in Jensen's Device, where swapping codice_3 and codice_4 via a temporary variable yields incorrect results due to the arguments being related: swapping via codice_5 changes the value for codice_3 in the second statement, which then results in the incorrect i value for codice_4 in the third statement.\n\nThe underlying principle of the XOR swap algorithm can be applied to any operation meeting criteria L1 through L4 above. Replacing XOR by addition and subtraction gives a slightly different, but largely equivalent, formulation:\n\nUnlike the XOR swap, this variation requires that the underlying processor or programming language uses a method such as modular arithmetic or bignums to guarantee that the computation of codice_8 cannot cause an error due to integer overflow. Therefore, it is seen even more rarely in practice than the XOR swap.\n\nNote, however, that the implementation of codice_9 above in the C programming language always works even in case of integer overflow, since, according to the C standard, addition and subtraction of unsigned integers follow the rules of modular arithmetic, i. e. are done in the cyclic group formula_14 where formula_15 is the number of bits of codice_10. Indeed, the correctness of the algorithm follows from the fact that the formulas formula_16 and formula_17 hold in any abelian group. This is actually a generalization of the proof for the XOR swap algorithm: XOR is both the addition and subtraction in the abelian group formula_18.\n\nPlease note that the above doesn't hold when dealing with the codice_11 type (the default for codice_12). Signed integer overflow is an undefined behavior in C and thus modular arithmetic is not guaranteed by the standard (a standard-conforming compiler might optimize out such code, which leads to incorrect results).\n\n"}
{"id": "683368", "url": "https://en.wikipedia.org/wiki?curid=683368", "title": "Young tableau", "text": "Young tableau\n\nIn mathematics, a Young tableau (; plural: tableaux) is a combinatorial object useful in representation theory and Schubert calculus. It provides a convenient way to describe the group representations of the symmetric and general linear groups and to study their properties. Young tableaux were introduced by Alfred Young, a mathematician at Cambridge University, in 1900. They were then applied to the study of the symmetric group by Georg Frobenius in 1903. Their theory was further developed by many mathematicians, including Percy MacMahon, W. V. D. Hodge, G. de B. Robinson, Gian-Carlo Rota, Alain Lascoux, Marcel-Paul Schützenberger and Richard P. Stanley.\n\n\"Note: this article uses the English convention for displaying Young diagrams and tableaux\".\n\nA Young diagram (also called Ferrers diagram, particularly when represented using dots) is a finite collection of boxes, or cells, arranged in left-justified rows, with the row lengths in non-increasing order. Listing the number of boxes in each row gives a partition of a non-negative integer , the total number of boxes of the diagram. The Young diagram is said to be of shape , and it carries the same information as that partition. Containment of one Young diagram in another defines a partial ordering on the set of all partitions, which is in fact a lattice structure, known as Young's lattice. Listing the number of boxes of a Young diagram in each column gives another partition, the conjugate or \"transpose\" partition of ; one obtains a Young diagram of that shape by reflecting the original diagram along its main diagonal.\n\nThere is almost universal agreement that in labeling boxes of Young diagrams by pairs of integers, the first index selects the row of the diagram, and the second index selects the box within the row. Nevertheless, two distinct conventions exist to display these diagrams, and consequently tableaux: the first places each row below the previous one, the second stacks each row on top of the previous one. Since the former convention is mainly used by Anglophones while the latter is often preferred by Francophones, it is customary to refer to these conventions respectively as the \"English notation\" and the \"French notation\"; for instance, in his book on symmetric functions, Macdonald advises readers preferring the French convention to \"read this book upside down in a mirror\" (Macdonald 1979, p. 2). This nomenclature probably started out as jocular. The English notation corresponds to the one universally used for matrices, while the French notation is closer to the convention of Cartesian coordinates; however, French notation differs from that convention by placing the vertical coordinate first. The figure on the right shows, using the English notation, the Young diagram corresponding to the partition (5, 4, 1) of the number 10. The conjugate partition, measuring the column lengths, is (3, 2, 2, 2, 1).\n\nIn many applications, for example when defining Jack functions, it is convenient to define the arm length \"a\"(\"s\") of a box \"s\" as the number of boxes to the right of \"s\" in the diagram λ. Similarly, the leg length \"l\"(\"s\") is the number of boxes below \"s\". This notation assumes that the English notation is used.\nFor example, the \"hook\" value of a box \"s\" in λ is then simply \"a\"(\"s\")+\"l\"(\"s\")+1.\n\nA Young tableau is obtained by filling in the boxes of the Young diagram with symbols taken from some \"alphabet\", which is usually required to be a totally ordered set. Originally that alphabet was a set of indexed variables , , ..., but now one usually uses a set of numbers for brevity. In their original application to representations of the symmetric group, Young tableaux have distinct entries, arbitrarily assigned to boxes of the diagram. A tableau is called standard if the entries in each row and each column are increasing. The number of distinct standard Young tableaux on entries is given by the involution numbers\n\nIn other applications, it is natural to allow the same number to appear more than once (or not at all) in a tableau. A tableau is called semistandard, or \"column strict\", if the entries weakly increase along each row and strictly increase down each column. Recording the number of times each number appears in a tableau gives a sequence known as the weight of the tableau. Thus the standard Young tableaux are precisely the semistandard tableaux of weight (1,1...,1), which requires every integer up to to occur exactly once.\n\nThere are several variations of this definition: for example, in a row-strict tableau the entries strictly increase along the rows and weakly increase down the columns. Also, tableaux with \"decreasing\" entries have been considered, notably, in the theory of plane partitions. There are also generalizations such as domino tableaux or ribbon tableaux, in which several boxes may be grouped together before assigning entries to them.\n\nA skew shape is a pair of partitions (, ) such that the Young diagram of contains the Young diagram of ; it is denoted by . If and , then the containment of diagrams means that for all . The skew diagram of a skew shape is the set-theoretic difference of the Young diagrams of and : the set of squares that belong to the diagram of but not to that of . A skew tableau of shape is obtained by filling the squares of the corresponding skew diagram; such a tableau is semistandard if entries increase weakly along each row, and increase strictly down each column, and it is standard if moreover all numbers from 1 to the number of squares of the skew diagram occur exactly once. While the map from partitions to their Young diagrams is injective, this is not the case from the map from skew shapes to skew diagrams; therefore the shape of a skew diagram cannot always be determined from the set of filled squares only. Although many properties of skew tableaux only depend on the filled squares, some operations defined on them do require explicit knowledge of and , so it is important that skew tableaux do record this information: two distinct skew tableaux may differ only in their shape, while they occupy the same set of squares, each filled with the same entries. Young tableaux can be identified with skew tableaux in which is the empty partition (0) (the unique partition of 0).\n\nAny skew semistandard tableau of shape with positive integer entries gives rise to a sequence of partitions (or Young diagrams), by starting with , and taking for the partition places further in the sequence the one whose diagram is obtained from that of by adding all the boxes that contain a value  ≤  in ; this partition eventually becomes equal to . Any pair of successive shapes in such a sequence is a skew shape whose diagram contains at most one box in each column; such shapes are called horizontal strips. This sequence of partitions completely determines , and it is in fact possible to define (skew) semistandard tableaux as such sequences, as is done by Macdonald (Macdonald 1979, p. 4). This definition incorporates the partitions and in the data comprising the skew tableau.\n\nYoung tableaux have numerous applications in combinatorics, representation theory, and algebraic geometry. Various ways of counting Young tableaux have been explored and lead to the definition of and identities for Schur functions. Many combinatorial algorithms on tableaux are known, including Schützenberger's jeu de taquin and the Robinson–Schensted–Knuth correspondence. Lascoux and Schützenberger studied an associative product on the set of all semistandard Young tableaux, giving it the structure called the \"plactic monoid\" (French: \"le monoïde plaxique\").\n\nIn representation theory, standard Young tableaux of size describe bases in irreducible representations of the symmetric group on letters. The standard monomial basis in a finite-dimensional irreducible representation of the general linear group are parametrized by the set of semistandard Young tableaux of a fixed shape over the alphabet {1, 2, ..., }. This has important consequences for invariant theory, starting from the work of Hodge on the homogeneous coordinate ring of the Grassmannian and further explored by Gian-Carlo Rota with collaborators, de Concini and Procesi, and Eisenbud. The Littlewood–Richardson rule describing (among other things) the decomposition of tensor products of irreducible representations of into irreducible components is formulated in terms of certain skew semistandard tableaux.\n\nApplications to algebraic geometry center around Schubert calculus on Grassmannians and flag varieties. Certain important cohomology classes can be represented by Schubert polynomials and described in terms of Young tableaux.\n\nYoung diagrams are in one-to-one correspondence with irreducible representations of the symmetric group over the complex numbers. They provide a convenient way of specifying the Young symmetrizers from which the irreducible representations are built. Many facts about a representation can be deduced from the corresponding diagram. Below, we describe two examples: determining the dimension of a representation and restricted representations. In both cases, we will see that some properties of a representation can be determined by using just its diagram.\n\nYoung diagrams also parametrize the irreducible polynomial representations of the general linear group (when they have at most nonempty rows), or the irreducible representations of the special linear group (when they have at most nonempty rows), or the irreducible complex representations of the special unitary group (again when they have at most nonempty rows). In these cases semistandard tableaux with entries up to play a central role, rather than standard tableaux; in particular it is the number of those tableaux that determines the dimension of the representation.\n\nThe dimension of the irreducible representation of the symmetric group corresponding to a partition of is equal to the number of different standard Young tableaux that can be obtained from the diagram of the representation. This number can be calculated by the hook length formula.\n\nA hook length of a box in Young diagram of shape is the number of boxes that are in the same row to the right of it plus those boxes in the same column below it, plus one (for the box itself). By the hook-length formula, the dimension of an irreducible representation is divided by the product of the hook lengths of all boxes in the diagram of the representation:\n\nThe figure on the right shows hook-lengths for all boxes in the diagram of the partition 10 = 5 + 4 + 1. Thus\n\nSimilarly, the dimension of the irreducible representation of corresponding to the partition \"λ\" of \"n\" (with at most \"r\" parts) is the number of semistandard Young tableaux of shape \"λ\" (containing only the entries from 1 to \"r\"), which is given by the hook-length formula:\n\nwhere the index \"i\" gives the row and \"j\" the column of a box. For instance, for the partition (5,4,1) we get as dimension of the corresponding irreducible representation of (traversing the boxes by rows):\n\nA representation of the symmetric group on elements, is also a representation of the symmetric group on elements, . However, an irreducible representation of may not be irreducible for . Instead, it may be a direct sum of several representations that are irreducible for . These representations are then called the factors of the restricted representation (see also induced representation).\n\nThe question of determining this decomposition of the restricted representation of a given irreducible representation of \"S\", corresponding to a partition of , is answered as follows. One forms the set of all Young diagrams that can be obtained from the diagram of shape by removing just one box (which must be at the end both of its row and of its column); the restricted representation then decomposes as a direct sum of the irreducible representations of corresponding to those diagrams, each occurring exactly once in the sum.\n\n\n\n"}
{"id": "22229525", "url": "https://en.wikipedia.org/wiki?curid=22229525", "title": "Łoś–Vaught test", "text": "Łoś–Vaught test\n\nIn model theory, a branch of mathematical logic, the Łoś–Vaught test is a criterion for a theory to be complete, unable to be augmented without becoming inconsistent. For theories in classical logic, this means that for every sentence the theory contains either the sentence or its negation but not both.\n\nAccording to this test, if a satisfiable theory is κ-categorical (there exists an infinite cardinal κ such that it has only one model up to isomorphism of cardinality κ, with κ at least equal to the cardinality of its language) and in addition it has no finite model, then it is complete.\n\nThis theorem was proved independently by and , after whom it is named.\n\n"}
