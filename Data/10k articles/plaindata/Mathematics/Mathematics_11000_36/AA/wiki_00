{"id": "1037442", "url": "https://en.wikipedia.org/wiki?curid=1037442", "title": "124 (number)", "text": "124 (number)\n\n124 (one hundred [and] twenty-four) is the natural number following 123 and preceding 125.\n\n124 is the sum of eight consecutive primes (5 + 7 + 11 + 13 + 17 + 19 + 23 + 29). It is a nontotient since there is no integer with 124 coprimes below it. It is an untouchable number since there is no integer whose proper divisors add up to 124.\n\nIn base 5 it is a repdigit (444).\n\n\n\n124 is also:\n\n"}
{"id": "31485329", "url": "https://en.wikipedia.org/wiki?curid=31485329", "title": "Accidental Adversaries", "text": "Accidental Adversaries\n\nAccidental Adversaries is one of the ten system archetypes used in system dynamics modelling, or systems thinking. This archetype describes the degenerative pattern that develops when two subjects cooperating for a common goal, accidentally take actions that undermine each other's success. It is similar to the escalation system archetype in terms of pattern behaviour that develops over time.\n\nThe archetype describes a pattern where two subjects have decided to work together because they will benefit from the alliance. Each take actions believing that it will bring benefit to the other and if the cooperation works, they will both benefit from it. Problems start arising when one or both of the subjects need to fix a local gap in performance, maybe due to external pressure. They initiate action to fix the gap and accidentally undermine each other's success. The result of these activities may produce a sense of resentment or frustration between the subjects or it may even turn the subjects into adversaries (hence the archetype name), thereby destroying the alliance.\n\nThe original set of system archetypes were published in The Fifth Discipline by Peter Senge. The exact source of these generic structures is not known. However \"Accidental Adversaries\" has a clear origin. It is derived from observations made by Jennifer Kemeny, a colleague of Senge's and a contributor to the original archetype descriptions. In her consulting work in the late 1980s she was intrigued at how often potential strategic alliances were unsuccessful, or devolved into outright hostility. Such a recurring phenomenon suggested a structural cause. The essential elements of the archetype were first described during a session Kemeny facilitated. It was the first meeting of the first ever alliance between Walmart and Procter and Gamble. Employees diagrammed how their protective business policies caused unintentional damage to the other company – which responded with similarly defensive actions. The net effect of these \"back and forth\" behaviors was lower profit and less goodwill – which for a long period of time had overshadowed the possibility of mutual benefit between the organizations. An early description of \"Accidental Adversaries\" is in The Fifth Discipline: Fieldbook, by Senge, \"et al.\"\n\nLet's consider two parties A and B. They have realised that by uniting forces, they can increase their success, whatever that may be. At a certain point in time, say B, takes corrective actions in order to counteract a decrease in performance due to external pressure. Although B's actions are important for B, their impact on A are not understood or not considered. The result is that now A feels that he is being betrayed and reacts to diminish the negative effects of B's actions. Ironically, A's new actions now obstruct B's success. If this situation persists and the results worsen, the alliance breaks down. A vicious circle is created and each partner has now even forgotten the original purpose of the alliance. A and B's actions now only focus on counteracting the hostile actions taken by the other. A and B thus 'accidentally' become adversaries.\n\nThe causal loop diagram in Figure 1 shows the pattern dynamics of the system.\nThe pattern of behaviour begins with the outer reinforcing loop R1 where A and B have formed a synergistic alliance that benefits both. An action taken by A in favour of B increases B's success and vice versa. At some point in time, though, say B, initiates a series of corrective actions in order to adjust its performance. The actions taken increase performance, whose effects balance the number of corrective actions required. This results in the creation of a balancing loop, B2. These actions also unintentionally obstruct the other party's success, and result in the formation of the negative reinforcing loop R2 that undermines the virtuous reinforcing loop R1. Now each corrective action taken by B causes A to start taking corrective measures as well, thus activating its own balancing loop B1. The formation of R2 is the critical point at which the dynamics of the system go out of control, resembling the Escalation archetype.\n\nFigure 2 shows the stock and flow diagram for this archetype.\n\nAn alternative form of the model is shown in Figure 3. The differences are in the two reinforcing loops R3 and R4. Here, A and B specifically take actions to improve their growth, not, as before, to adjust their performance to a pre-determined target. By seeking improvement through R3 and R4, A and B suppress the effects of R1 and establish the negative-effect reinforcing loop R2, that in turn, completely takes over B5 and B6. The two balancing loops B5 and B6 are formed by, respectively,\n\nThe archetype behaviour over time is shown in Figure 4.Both parties show a similar trend in direction and rate of change of success, with one trailing behind the other one due to system delays. Even though the pattern shows stable periods, the overall trend follows a downward direction. The above simulation can be run from InsightMaker.\n\nIt is possible to achieve leverage by introducing or re-emphasising a link between each party's success, thus re-establishing the outer virtuous reinforcing loop shown in Figure 3. Kemeny proposes a list of seven action steps to deal with the unintended consequences of each party's actions, given their mental models:\n\n\nA classic example of the Accidental Adversaries system archetype is that of Procter and Gamble supplying Wal-Mart. When Procter and Gamble's profits decline, they introduce promotions. This results in extra costs and decreasing profitability for Wal-Mart. Wal-Mart's responds by stocking-up – buying large quantities of products during the discount period – and then selling them at regular prices when promotions end, thereby increasing its margins. Procter and Gamble's production now faces great swings in volume because Wal-Mart does not need to order products for months, which adds to P and G's costs. To improve their situation, Procter and Gamble pushes even more on promotions, blaming at the same time Wal-Mart for their problems. Wal-Mart reacts by stocking-up even more. Eventually, Procter and Gamble finds itself putting a lot of effort into promotions, at the expense of new product development, while Wal-Mart concentrates solely on buying and storing promoted products rather than improving their operations. The situation is described by the causal loop diagram in Figure 5.\nIn the Procter and Gamble/Wal-Mart case, leverage was achieved by bringing both parties together to understand the structure they had unintentionally created, even though each party 's action was completely rational and understandable from their local perspective.\n\nSenge, Peter (1990). The Fifth Discipline. Currency. .\nSenge, P. \"et al.\" (1994). The Fifth Discipline Fieldbook. New York: Doubleday Currency.\nForrester, J. W. (1971, 1973). World Dynamics. Cambridge, MA: Wright Allen Press.\nForrester, Jay W. (1969). Urban Dynamics. Pegasus Communications. .\nForrester, J. W. (1975). Collected Papers of Jay W. Forrester. Cambridge, MA: Wright-Allen Press.\nKim D, Kim C, A Pocket Guide to Using the Archetypes, Pegasus Communications, May 1994.\nRamsey P, Wells R, Managing the Archetypes: Accidental Adversaries, Pegasus Communications, 2001.\nKim D, Anderson V, Systems Archetype Basics: From Story to Structure, Pegasus Communications, 1998.\n\n"}
{"id": "11055227", "url": "https://en.wikipedia.org/wiki?curid=11055227", "title": "Admissible representation", "text": "Admissible representation\n\nIn mathematics, admissible representations are a well-behaved class of representations used in the representation theory of reductive Lie groups and locally compact totally disconnected groups. They were introduced by Harish-Chandra.\n\nLet \"G\" be a connected reductive (real or complex) Lie group. Let \"K\" be a maximal compact subgroup. A continuous representation (π, \"V\") of \"G\" on a complex Hilbert space \"V\" is called admissible if π restricted to \"K\" is unitary and each irreducible unitary representation of \"K\" occurs in it with finite multiplicity. The prototypical example is that of an irreducible unitary representation of \"G\".\n\nAn admissible representation π induces a formula_1-module which is easier to deal with as it is an algebraic object. Two admissible representations are said to be infinitesimally equivalent if their associated formula_1-modules are isomorphic. Though for general admissible representations, this notion is different than the usual equivalence, it is an important result that the two notions of equivalence agree for unitary (admissible) representations. Additionally, there is a notion of unitarity of formula_1-modules. This reduces the study of the equivalence classes of irreducible unitary representations of \"G\" to the study of infinitesimal equivalence classes of admissible representations and the determination of which of these classes are infinitesimally unitary. The problem of parameterizing the infinitesimal equivalence classes of admissible representations was fully solved by Robert Langlands and is called the Langlands classification.\n\nLet \"G\" be a locally compact totally disconnected group (such as a reductive algebraic group over a nonarchimedean local field or over the finite adeles of a global field). A representation (π, \"V\") of \"G\" on a complex vector space \"V\" is called smooth if the subgroup of \"G\" fixing any vector of \"V\" is open. If, in addition, the space of vectors fixed by any compact open subgroup is finite dimensional then π is called admissible. Admissible representations of \"p\"-adic groups admit more algebraic description through the action of the Hecke algebra of locally constant functions on \"G\".\n\nDeep studies of admissible representations of \"p\"-adic reductive groups were undertaken by Casselman and by Bernstein and Zelevinsky in the 1970s. Much progress has been made more recently by Howe and Moy and Bushnell and Kutzko, who developed a \"theory of types\" and classified the admissible dual (i.e. the set of equivalence classes of irreducible admissible representations) in many cases.\n\n"}
{"id": "20515591", "url": "https://en.wikipedia.org/wiki?curid=20515591", "title": "Aumann's agreement theorem", "text": "Aumann's agreement theorem\n\nIn game theory, Aumann's agreement theorem is a theorem which demonstrates that rational agents with common knowledge of each other's beliefs cannot agree to disagree. It was first formulated in the 1976 paper titled \"Agreeing to Disagree\" by Robert Aumann, after whom the theorem is named.\n\nAumann's agreement theorem says that two people acting rationally (in a certain precise sense) and with common knowledge of each other's beliefs cannot agree to disagree. More specifically, if two people are genuine Bayesian rationalists with common priors, and if they each have common knowledge of their individual posterior probabilities, then their posteriors must be equal. This theorem holds even if the people's individual posteriors are based on different observed information about the world. Simply knowing that another agent observed some information and came to their respective conclusion will force each to revise their beliefs, resulting eventually in total agreement on the correct posterior. Thus, two rational Bayesian agents with the same priors and who know each other's posteriors will have to agree.\n\nA question arises whether such an agreement can be reached in a reasonable time and, from a mathematical perspective, whether this can be done efficiently. Scott Aaronson has shown that this is indeed the case. Of course, the assumption of common priors is a rather strong one and may not hold in practice. However, Robin Hanson has presented an argument that Bayesians who agree about the processes that gave rise to their priors (e.g., genetic and environmental influences) should, if they adhere to a certain \"pre-rationality condition\", have common priors.\n\nStudying the same issue from a different perspective, a research paper by Ziv Hellman considers what happens if priors are not common. The paper presents a way to measure how distant priors are from being common. If this distance is ε then, under common knowledge, disagreement on events is always bounded from above by ε. When ε goes to zero, Aumann's original agreement theorem is recapitulated. In a 2013 paper, Joseph Halpern and Willemien Kets argued that \"players can agree to disagree in the presence of ambiguity, even if there is a common prior, but that allowing for ambiguity is more restrictive than assuming heterogeneous priors.\"\n"}
{"id": "734787", "url": "https://en.wikipedia.org/wiki?curid=734787", "title": "Automatic differentiation", "text": "Automatic differentiation\n\nIn mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation or computational differentiation, is a set of techniques to numerically evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.\n\nAutomatic differentiation is not:\n\n\nThese classical methods run into problems: symbolic differentiation leads to inefficient code (unless done carefully) and faces the difficulty of converting a computer program into a single expression, while numerical differentiation can introduce round-off errors in the discretization process and cancellation. Both classical methods have problems with calculating higher derivatives, where the complexity and errors increase. Finally, both classical methods are slow at computing the partial derivatives of a function with respect to \"many\" inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems, at the expense of introducing more software dependencies.\n\nFundamental to AD is the decomposition of differentials provided by the chain rule. For the simple composition formula_1 the chain rule gives\n\nUsually, two distinct modes of AD are presented, forward accumulation (or forward mode) and reverse accumulation (or reverse mode). Forward accumulation specifies that one traverses the chain rule from inside to outside (that is, first compute formula_3 and then formula_4 and at last formula_5), while reverse accumulation has the traversal from outside to inside (first compute formula_5 and then formula_4 and at last formula_3). More succinctly,\nGenerally, both forward and reverse accumulation are specific manifestations of applying the operator of program composition, with the appropriate one of the two mappings formula_13 being fixed.\n\nIn forward accumulation AD, one first fixes the \"independent variable\" to which differentiation is performed and computes the derivative of each sub-expression recursively. In a pen-and-paper calculation, one can do so by repeatedly substituting the derivative of the \"inner\" functions in the chain rule:\n\nThis can be generalized to multiple variables as a matrix product of Jacobians.\n\nCompared to reverse accumulation, forward accumulation is very natural and easy to implement as the flow of derivative information coincides with the order of evaluation. One simply augments each variable with its derivative (stored as a numerical value, not a symbolic expression),\n\nas denoted by the dot. The derivatives are then computed in sync with the evaluation steps and combined with other derivatives via the chain rule.\n\nAs an example, consider the function:\n\nFor clarity, the individual sub-expressions have been labeled with the variables .\n\nThe choice of the independent variable to which differentiation is performed affects the \"seed\" values and . Suppose one is interested in the derivative of this function with respect to . In this case, the seed values should be set to:\n\nWith the seed values set, one may then propagate the values using the chain rule as shown in the table below. Figure 2 shows a pictorial depiction of this process as a computational graph.\n\nTo compute the gradient of this example function, which requires the derivatives of with respect to not only but also , one must perform an \"additional\" sweep over the computational graph using the seed values formula_19.\n\nThe computational complexity of one sweep of forward accumulation is proportional to the complexity of the original code.\n\nForward accumulation is more efficient than reverse accumulation for functions with as only sweeps are necessary, compared to sweeps for reverse accumulation.\n\nIn reverse accumulation AD, one first fixes the \"dependent variable\" to be differentiated and computes the derivative \"with respect to\" each sub-expression recursively. In a pen-and-paper calculation, one can perform the equivalent by repeatedly substituting the derivative of the \"outer\" functions in the chain rule:\n\nIn reverse accumulation, the quantity of interest is the \"adjoint\", denoted with a bar (); it is a derivative of a chosen dependent variable with respect to a subexpression :\n\nReverse accumulation traverses the chain rule from outside to inside, or in the case of the computational graph in Figure 3, from top to bottom. The example function is scalar-valued, and thus there is only one seed for the derivative computation, and only one sweep of the computational graph is needed in order to calculate the (two-component) gradient. This is only half the work when compared to forward accumulation, but reverse accumulation requires the storage of the intermediate variables as well as the instructions that produced them in a data structure known as a Wengert list (or \"tape\"), which may represent a significant memory issue if the computational graph is large. This can be mitigated to some extent by storing only a subset of the intermediate variables and then reconstructing the necessary work variables by repeating the evaluations, a technique known as rematerialization. Checkpointing is also used to save intermediary states.\n\nThe operations to compute the derivative using reverse accumulation are shown in the table below (note the reversed order):\n\nThe data flow graph of a computation can be manipulated to calculate the gradient of its original calculation. This is done by adding an adjoint node for each primal node, connected by adjoint edges which parallel the primal edges but flow in the opposite direction. The nodes in the adjoint graph represent multiplication by the derivatives of the functions calculated by the nodes in the primal. For instance, addition in the primal causes fanout in the adjoint; fanout in the primal causes addition in the adjoint; a unary function in the primal causes in the adjoint; etc.\n\nReverse accumulation is more efficient than forward accumulation for functions with as only sweeps are necessary, compared to sweeps for forward accumulation.\n\nReverse mode AD was first published in 1970 by Seppo Linnainmaa in his master thesis.\n\nBackpropagation of errors in multilayer perceptrons, a technique used in machine learning, is a special case of reverse mode AD.\n\nForward and reverse accumulation are just two (extreme) ways of traversing the chain rule. The problem of computing a full Jacobian of with a minimum number of arithmetic operations is known as the \"optimal Jacobian accumulation\" (OJA) problem, which is NP-complete.\nCentral to this proof is the idea that there may exist algebraic dependencies between the local partials that label the edges of the graph. In particular, two or more edge labels may be recognized as equal. The complexity of the problem is still open if it is assumed that all edge labels are unique and algebraically independent.\n\nForward mode automatic differentiation is accomplished by augmenting the algebra of real numbers and obtaining a new arithmetic. An additional component is added to every number which will represent the derivative of a function at the number, and all arithmetic operators are extended for the augmented algebra. The augmented algebra is the algebra of dual numbers. This approach was generalized by the theory of operational calculus on programming spaces (see Analytic programming space), through tensor algebra of the dual space.\n\nReplace every number formula_23 with the number formula_24, where formula_25 is a real number, but formula_26 is an abstract number with the property formula_27 (an infinitesimal; see \"Smooth infinitesimal analysis\"). Using only this, we get for the regular arithmetic\n\nand likewise for subtraction and division.\n\nNow, we may calculate polynomials in this augmented arithmetic. If formula_29, then\n\nformula_30\n\nwhere formula_31 denotes the derivative of formula_32 with respect to its first argument, and formula_25, called a \"seed\", can be chosen arbitrarily.\n\nThe new arithmetic consists of ordered pairs, elements written formula_34, with ordinary arithmetics on the first component, and first order differentiation arithmetic on the second component, as described above. Extending the above results on polynomials to analytic functions we obtain a list of the basic arithmetic and some standard functions for the new arithmetic:\nand in general for the primitive function formula_36,\nwhere formula_38 and formula_39 are the derivatives of formula_36 with respect to its first and second arguments, respectively.\n\nWhen a binary basic arithmetic operation is applied to mixed arguments—the pair formula_41 and the real number formula_42—the real number is first lifted to formula_43. The derivative of a function formula_44 at the point formula_45 is now found by calculating formula_46 using the above arithmetic, which gives formula_47 as the result.\n\nMultivariate functions can be handled with the same efficiency and mechanisms as univariate functions by adopting a directional derivative operator. That is, if it is sufficient to compute formula_48, the directional derivative formula_49 of formula_50 at formula_51 in the direction formula_52, this may be calculated as formula_53 using the same arithmetic as above. If all the elements of formula_54 are desired, then formula_55 function evaluations are required. Note that in many optimization applications, the directional derivative is indeed sufficient.\n\nThe above arithmetic can be generalized to calculate second order and higher derivatives of multivariate functions. However, the arithmetic rules quickly grow very complicated: complexity will be quadratic in the highest derivative degree. Instead, truncated Taylor polynomial algebra can be used. The resulting arithmetic, defined on generalized dual numbers, allows to efficiently compute using functions as if they were a new data type. Once the Taylor polynomial of a function is known, the derivatives are easily extracted. \nA rigorous, general formulation is achieved through the tensor series expansion using operational calculus on programming spaces.\n\nOperational calculus on programming spaces provides differentiable programming with formal semantics through an algebra of higher-order constructs. It can thus be used to express the concepts underlying automatic differentiation.\n\nA differentiable programming space formula_56 is any subspace of formula_57 such that \nwhere formula_59 is the tensor algebra of the dual space formula_60. When all elements of formula_56 are analytic, we call formula_56 an analytic programming space.\n\nDefinition. Let formula_56 be a differentiable programming space. The space formula_69 spanned by formula_70 over formula_71, where formula_72 is called a differentiable programming space of order formula_55.\n\nProofs can be found in. \n\nThis means that we can represent calculation of derivatives of the map formula_82, with only one mapping formula_83. We define the operator formula_84 as a direct sum of operators \n\nThe image formula_86 is a multitensor of order formula_87, which is a direct sum of the maps value and all derivatives of order formula_88, all evaluated at the point formula_89\n\nThe operator formula_84 satisfies the recursive relation.\n\nthat can be used to recursively construct programming spaces of arbitrary order. Only explicit knowledge of formula_93 is required for the construction of formula_67 from formula_95, which is evident from the above theorem.\n\nThe paper proposed an abstract virtual machine capable of constructing and implementing the theory. Such a machine provides a framework for analytic study of algorithmic procedures through algebraic means.\n\nThis claim allows a simple definition of such a machine.\n\nDefinition (Virtual tensor machine). The tuple formula_99 is a virtual tensor machine, where\n\nExpansion into a series offers valuable insights into programs through methods of analysis.\nThere exists a space spanned by the set formula_104 over a field formula_71. Thus, the expression \n\nis well defined. The operator formula_107 is a mapping between function spaces \n\nIt also defines a map \n\nby taking the image of the map formula_110 at a certain point formula_111.\n\nWe may construct a map from the space of programs, to the space of polynomials. Note that the space of multivariate polynomials formula_112 is isomorphic to symmetric algebra formula_113, which is in turn a quotient of tensor algebra formula_59. To any element of formula_115 one can attach corresponding element of formula_116 namely a polynomial map formula_117. Thus, we consider the completion of the symmetric algebra formula_113 as the \"formal power series\" formula_119, which is in turn isomorphic to a quotient of \"tensor series algebra\" formula_120, arriving at\n\nFor any element formula_122, the expression formula_123 is a map formula_124, mapping a program to a \"formal power series\". We can express the correspondence between multi-tensors in formula_115 and polynomial maps formula_117 given by multiple contractions for all possible indices.\n\nProof can be found in. Evaluated at formula_130, the operator is a generalization of the shift operator widely used in physics. For a specific formula_131 it is here on denoted by \n\nWhen the choice of formula_131 is arbitrary, we omit it from expressions for brevity. Following this work, a similar approach was taken by others .\n\nTheory offers a generalization of both forward and reverse mode of automatic differentiation to arbitrary order, under a single invariant operator in the theory. This condenses complex notions to simple expressions allowing meaningful manipulations before being applied to a particular programming space.\n\nProof can be found in.\n\nBoth forward and reverse mode (generalized to arbitrary order) are obtainable using this operator, by fixing the appropriate one of the two maps. This generalizes both concepts under a single operator in the theory. For example, by considering projections of the operator onto the space spanned by formula_142, and fixing the second map formula_36, we retrieve the basic first order forward mode of automatic differentiation, or reverse mode, by fixing formula_141.\n\nThus the operator alleviates the need for explicit implementation of the higher order chain rule (see Faà di Bruno's formula), as it is encoded in the structure of the operator itself, which can be efficiently implemented by manipulating its generating map (see ).\n\nIt is useful to be able to use the formula_87-th derivative of a program formula_127 as part of a different differentiable program formula_147. As such, we must be able to treat the derivative itself as a differentiable program formula_148, while only coding the original program formula_32.\n\nBy the above Theorem, formula_55-differentiable formula_87-th derivatives of a program formula_158 can be extracted by\n\nThus, we gained the ability of writing a differentiable program acting on derivatives of another program, stressed as crucial by other authors.\n\nForward-mode AD is implemented by a nonstandard interpretation of the program in which real numbers are replaced by dual numbers, constants are lifted to dual numbers with a zero epsilon coefficient, and the numeric primitives are lifted to operate on dual numbers. This nonstandard interpretation is generally implemented using one of two strategies: \"source code transformation\" or \"operator overloading\".\n\nThe source code for a function is replaced by an automatically generated source code that includes statements for calculating the derivatives interleaved with the original instructions.\n\nSource code transformation can be implemented for all programming languages, and it is also easier for the compiler to do compile time optimizations. However, the implementation of the AD tool itself is more difficult.\n\nOperator overloading is a possibility for source code written in a language supporting it. Objects for real numbers and elementary mathematical operations must be overloaded to cater for the augmented arithmetic depicted above. This requires no change in the form or sequence of operations in the original source code for the function to be differentiated, but often requires changes in basic data types for numbers and vectors to support overloading and often also involves the insertion of special flagging operations.\n\nOperator overloading for forward accumulation is easy to implement, and also possible for reverse accumulation. However, current compilers lag behind in optimizing the code when compared to forward accumulation.\n\nOperator overloading, for both forward and reverse accumulation, can be well-suited to applications where the objects are vectors of real numbers rather than scalars. This is because the tape then comprises vector operations; this can facilitate computationally efficient implementations where each vector operation performs many scalar operations. Vector adjoint algorithmic differentiation (vector AAD) techniques may be used, for example, to differentiate values calculated by Monte-Carlo simulation.\n\nExamples of operator-overloading implementations of automatic differentiation in C++ are the Adept and Stan libraries.\n\n\n"}
{"id": "188401", "url": "https://en.wikipedia.org/wiki?curid=188401", "title": "Axiomatic system", "text": "Axiomatic system\n\nIn mathematics, an axiomatic system is any set of axioms from which some or all axioms can be used in conjunction to logically derive theorems. A theory consists of an axiomatic system and all its derived theorems. An axiomatic system that is completely described is a special kind of formal system. A formal theory typically means an axiomatic system, for example formulated within model theory. A formal proof is a complete rendition of a mathematical proof within a formal system.\n\nAn axiomatic system is said to be \"consistent\" if it lacks contradiction, i.e. the ability to derive both a statement and its denial from the system's axioms.\n\nIn an axiomatic system, an axiom is called \"independent\" if it is not a theorem that can be derived from other axioms in the system. A system will be called independent if each of its underlying axioms is independent. Although independence is not a necessary requirement for a system, consistency usually is, but see neutrosophic logic.\n\nAn axiomatic system will be called \"complete\" if for every statement, either itself or its negation is derivable.\n\nBeyond consistency, relative consistency is also the mark of a worthwhile axiom system. This is when the undefined terms of a first axiom system are provided definitions from a second, such that the axioms of the first are theorems of the second.\n\nA good example is the relative consistency of neutral geometry or absolute geometry with respect to the theory of the real number system. Lines and points are undefined terms in absolute geometry, but assigned meanings in the theory of real numbers in a way that is consistent with both axiom systems.\n\nA model for an axiomatic system is a well-defined set, which assigns meaning for the undefined terms presented in the system, in a manner that is correct with the relations defined in the system. The existence of a concrete model proves the consistency of a system. A model is called concrete if the meanings assigned are objects and relations from the real world}, as opposed to an abstract model which is based on other axiomatic systems.\n\nModels can also be used to show the independence of an axiom in the system. By constructing a valid model for a subsystem without a specific axiom, we show that the omitted axiom is independent if its correctness does not necessarily follow from the subsystem.\n\nTwo models are said to be isomorphic if a one-to-one correspondence can be found between their elements, in a manner that preserves their relationship. An axiomatic system for which every model is isomorphic to another is called categorial (sometimes categorical), and the property of categoriality (categoricity) ensures the completeness of a system.\n\nStating definitions and propositions in a way such that each new term can be formally eliminated by the priorly introduced terms requires primitive notions (axioms) to avoid infinite regress. This way of doing mathematics is called the axiomatic method.\n\nA common attitude towards the axiomatic method is logicism. In their book \"Principia Mathematica\", Alfred North Whitehead and Bertrand Russell attempted to show that all mathematical theory could be reduced to some collection of axioms. More generally, the reduction of a body of propositions to a particular collection of axioms underlies the mathematician's research program. This was very prominent in the mathematics of the twentieth century, in particular in subjects based around homological algebra.\n\nThe explication of the particular axioms used in a theory can help to clarify a suitable level of abstraction that the mathematician would like to work with. For example, mathematicians opted that rings need not be commutative, which differed from Emmy Noether's original formulation. Mathematicians decided to consider topological spaces more generally without the separation axiom which Felix Hausdorff originally formulated.\n\nThe Zermelo-Fraenkel axioms, the result of the axiomatic method applied to set theory, allowed the \"proper\" formulation of set-theory problems and helped to avoid the paradoxes of naïve set theory. One such problem was the Continuum hypothesis. Zermelo–Fraenkel set theory with the historically controversial axiom of choice included is commonly abbreviated ZFC, where C stands for choice. Many authors use ZF to refer to the axioms of Zermelo–Fraenkel set theory with the axiom of choice excluded. Today ZFC is the standard form of axiomatic set theory and as such is the most common foundation of mathematics.\n\nMathematical methods developed to some degree of sophistication in ancient Egypt, Babylon, India, and China, apparently without employing the axiomatic method.\n\nEuclid of Alexandria authored the earliest extant axiomatic presentation of Euclidean geometry and number theory. Many axiomatic systems were developed in the nineteenth century, including non-Euclidean geometry, the foundations of real analysis, Cantor's set theory, Frege's work on foundations, and Hilbert's 'new' use of axiomatic method as a research tool. For example, group theory was first put on an axiomatic basis towards the end of that century. Once the axioms were clarified (that inverse elements should be required, for example), the subject could proceed autonomously, without reference to the transformation group origins of those studies.\n\nNot every consistent body of propositions can be captured by a describable collection of axioms. Call a collection of axioms recursive if a computer program can recognize whether a given proposition in the language is an axiom. Gödel's First Incompleteness Theorem then tells us that there are certain consistent bodies of propositions with no recursive axiomatization. Typically, the computer can recognize the axioms and logical rules for deriving theorems, and the computer can recognize whether a proof is valid, but to determine whether a proof exists for a statement is only soluble by \"waiting\" for the proof or disproof to be generated. The result is that one will not know which propositions are theorems and the axiomatic method breaks down. An example of such a body of propositions is the theory of the natural numbers. The Peano Axioms (described below) thus only partially axiomatize this theory.\n\nIn practice, not every proof is traced back to the axioms. At times, it is not clear which collection of axioms a proof appeals to. For example, a number-theoretic statement might be expressible in the language of arithmetic (i.e. the language of the Peano Axioms) and a proof might be given that appeals to topology or complex analysis. It might not be immediately clear whether another proof can be found that derives itself solely from the Peano Axioms.\n\nAny more-or-less arbitrarily chosen system of axioms is the basis of some mathematical theory, but such an arbitrary axiomatic system will not necessarily be free of contradictions, and even if it is, it is not likely to shed light on anything. Philosophers of mathematics sometimes assert that mathematicians choose axioms \"arbitrarily\", but it is possible that although they may appear arbitrary when viewed only from the point of view of the canons of deductive logic, that appearance is due to a limitation on the purposes that deductive logic serves.\n\nThe mathematical system of natural numbers 0, 1, 2, 3, 4, ... is based on an axiomatic system first written down by the mathematician Peano in 1889. He chose the axioms, in the language of a single unary function symbol \"S\" (short for \"successor\"), for the set of natural numbers to be:\n\n\nIn mathematics, axiomatization is the formulation of a system of statements (i.e. axioms) that relate a number of primitive terms in order that a consistent body of propositions may be derived deductively from these statements. Thereafter, the proof of any proposition should be, in principle, traceable back to these axioms.\n\n\n"}
{"id": "4390760", "url": "https://en.wikipedia.org/wiki?curid=4390760", "title": "Aṣ-Ṣaidanānī", "text": "Aṣ-Ṣaidanānī\n\nʿAbd Allāh ibn al-Ḥasan al-Ḥāsib was an astronomer and mathematician who lived in the first half of the 10th century.\n\nIbn an-Nadīm lists the following titles by him:\n\n"}
{"id": "2082759", "url": "https://en.wikipedia.org/wiki?curid=2082759", "title": "Beck's theorem (geometry)", "text": "Beck's theorem (geometry)\n\nIn discrete geometry, Beck's theorem is any of several different results, two of which are given below. Both appeared, alongside several other important theorems, in a well-known paper by József Beck. The two results described below primarily concern lower bounds on the number of lines \"determined\" by a set of points in the plane. (Any line containing at least two points of point set is said to be \"determined\" by that point set.)\n\nThe Erdős–Beck theorem is a variation of a classical result by L. M. Kelly and W. O. J. Moser involving configurations of \"n\" points of which at most \"n\" − \"k\" are collinear, for some 0 < \"k\" < \"O\"(). They showed that if \"n\" is sufficiently large, relative to \"k\", then the configuration spans at least \"kn\" − (1/2)(3\"k\" + 2)(\"k\" − 1) lines.\n\nElekes and Csaba Toth noted that the Erdős–Beck theorem does not easily extend to higher dimensions. Take for example a set of 2\"n\" points in R all lying on two skew lines. Assume that these two lines are each incident to \"n\" points. Such a configuration of points spans only 2\"n\" planes. Thus, a trivial extension to the hypothesis for point sets in R is not sufficient to obtain the desired result.\n\nThis result was first conjectured by Erdős, and proven by Beck. (See \"Theorem 5.2\" in.)\n\nLet \"S\" be a set of \"n\" points in the plane. If no more than \"n\" − \"k\" points lie on any line for some 0 ≤ \"k\" < \"n\" − 2, then there exist Ω(\"nk\") lines determined by the points of \"S\".\n\nBeck's theorem says that finite collections of points in the plane fall into one of two extremes; one where a large fraction of points lie on a single line, and one where a large number of lines are needed to connect all the points.\n\nAlthough not mentioned in Beck's paper, this result is implied by the Erdős–Beck theorem.\n\nThe theorem asserts the existence of positive constants \"C\", \"K\" such that given any \"n\" points in the plane, at least one of the following statements is true:\n\n\nIn Beck's original argument, \"C\" is 100 and \"K\" is an unspecified constant; it is not known what the optimal values of \"C\" and \"K\" are.\n\nA proof of Beck's theorem can be given as follows. Consider a set \"P\" of \"n\" points in the plane. Let \"j\" be a positive integer. Let us say that a pair of points \"A\", \"B\" in the set \"P\" is \"j-connected\" if the line connecting \"A\" and \"B\" contains between formula_2 and formula_3 points of \"P\" (including \"A\" and \"B\").\n\nFrom the Szemerédi–Trotter theorem, the number of such lines is formula_4, as follows: Consider the set \"P\" of \"n\" points, and the set \"L\" of all those lines spanned by pairs of points of \"P\" that contain at least formula_5 points of \"P\". Note that formula_6, since no two points can lie on two distinct lines. Now using Szemerédi–Trotter theorem, it follows that the number of incidences between \"P\" and \"L\" is at most formula_7. All the lines connecting \"j-connected\" points also lie in \"L\", and each contributes at least formula_2 incidences. Therefore the total number of such lines is formula_9.\n\nSince each such line connects together formula_10 pairs of points, we thus see that at most formula_11 pairs of points can be \"j\"-connected.\n\nNow, let \"C\" be a large constant. By summing the geometric series, we see that the number of pairs of points which are \"j\"-connected for some \"j\" satisfying formula_12 is at most formula_13.\n\nOn the other hand, the total number of pairs is formula_14. Thus if we choose \"C\" to be large enough, we can find at least formula_15 pairs (for instance) which are not \"j\"-connected for any formula_12. The lines that connect these pairs either pass through fewer than 2\"C\" points, or pass through more than \"n\"/\"C\" points. If the latter case holds for even one of these pairs, then we have the first conclusion of Beck's theorem. Thus we may assume that all of the formula_17 pairs are connected by lines which pass through fewer than 2\"C\" points. But each such line can connect at most formula_18 pairs of points. Thus there must be at least formula_19 lines connecting at least two points, and the claim follows by taking formula_20.\n"}
{"id": "11751094", "url": "https://en.wikipedia.org/wiki?curid=11751094", "title": "Binary constraint", "text": "Binary constraint\n\nA binary constraint, in mathematical optimization, is a constraint that involves exactly two variables.\n\nFor example, consider the n-queens problem, where the goal is to place \"n\" chess queens on an \"n\"-by-\"n\" chessboard such that none of the queens can attack each other (horizontally, vertically, or diagonally). The formal set of constraints are therefore \"Queen 1 can't attack Queen 2\", \"Queen 1 can't attack Queen 3\", and so on between all pairs of queens. Each constraint in this problem is binary, in that it only considers the placement of two individual queens.\n\nLinear programs in which all constraints are binary can be solved in strongly polynomial time, a result that is not known to be true for more general linear programs.\n"}
{"id": "2733707", "url": "https://en.wikipedia.org/wiki?curid=2733707", "title": "Boole's expansion theorem", "text": "Boole's expansion theorem\n\nBoole's expansion theorem, often referred to as the Shannon expansion or decomposition, is the identity: formula_1, where formula_2 is any Boolean function, formula_3 is a variable, formula_4 is the complement of formula_3, and formula_6and formula_7 are formula_2 with the argument formula_3 set equal to formula_10 and to formula_11 respectively.\n\nThe terms formula_6 and formula_7 are sometimes called the positive and negative Shannon cofactors, respectively, of formula_2 with respect to formula_3. These are functions, computed by restrict operator, formula_16 and formula_17 (see valuation (logic) and partial application).\n\nIt has been called the \"fundamental theorem of Boolean algebra\". Besides its theoretical importance, it paved the way for binary decision diagrams, satisfiability solvers, and many other techniques relevant to computer engineering and formal verification of digital circuits.\n\nA more explicit way of stating the theorem is:\n\n\nRepeated application for each argument leads to the Sum of Products (SoP) canonical form of the Boolean function formula_21. For example for formula_22 that would be\n\nLikewise, application of the dual form leads to the Product of Sums (PoS) canonical form (using the distributivity law of formula_24 over formula_25):\n\n\n\n\n\n\nGeorge Boole presented this expansion as his Proposition II, \"To expand or develop a function involving any number of logical symbols\", in his \"Laws of Thought\" (1854), and it was \"widely applied by Boole and other nineteenth-century logicians\".\n\nClaude Shannon mentioned this expansion, among other Boolean identities, in a 1948 paper, and showed the switching network interpretations of the identity. In the literature of computer design and switching theory, the identity is often incorrectly attributed to Shannon.\n\n\n"}
{"id": "2748941", "url": "https://en.wikipedia.org/wiki?curid=2748941", "title": "Butterfly curve (transcendental)", "text": "Butterfly curve (transcendental)\n\nThe butterfly curve is a transcendental plane curve discovered by Temple H. Fay. The curve is given by the following parametric equations:\n\nor by the following polar equation:\n\n\n"}
{"id": "22395816", "url": "https://en.wikipedia.org/wiki?curid=22395816", "title": "Carsten Lund", "text": "Carsten Lund\n\nCarsten Lund (born July 1, 1963) is a Danish-born theoretical computer scientist, currently working at AT&T Labs in Bedminster, New Jersey, United States.\n\nLund was born in Aarhus, Denmark, and received the\n\"kandidat\" degree in 1988 from the University of Aarhus and his Ph.D.\nfrom the University of Chicago in computer science. His thesis, entitled The\nPower of Interaction, was chosen as an ACM 'Distinguished Dissertation'.\n\nLund was a co-author on two of five competing papers at the 1990 Symposium on Foundations of Computer Science characterizing complexity classes such as PSPACE and NEXPTIME in terms of interactive proof systems;\nthis work became part of his 1991 Ph.D. thesis from the University of Chicago under the supervision of Lance Fortnow and László Babai, for which he was a runner-up for the 1991 ACM Doctoral Dissertation Award.\n\nHe is also known for his joint work with Sanjeev Arora, Madhu Sudan, Rajeev Motwani, and Mario Szegedy that discovered the existence of probabilistically checkable proofs for NP-hard problems and used them to prove hardness results for approximation problems; in 2001 he and his co-authors received the Gödel Prize for their share in these discoveries.\n\nMore recently he has published highly cited work on internet traffic engineering.\n\nHe has been working for AT&T Laboratories since August 1991.\n\n"}
{"id": "13620523", "url": "https://en.wikipedia.org/wiki?curid=13620523", "title": "Cauchy–Hadamard theorem", "text": "Cauchy–Hadamard theorem\n\nIn mathematics, the Cauchy–Hadamard theorem is a result in complex analysis named after the French mathematicians Augustin Louis Cauchy and Jacques Hadamard, describing the radius of convergence of a power series. It was published in 1821 by Cauchy, but remained relatively unknown until Hadamard rediscovered it. Hadamard's first publication of this result was in 1888; he also included it as part of his 1892 Ph.D. thesis.\n\nConsider the formal power series in one complex variable z of the form\n\nwhere formula_2\n\nThen the radius of convergence of \"ƒ\" at the point \"a\" is given by\n\nwhere lim sup denotes the limit superior, the limit as \"n\" approaches infinity of the supremum of the sequence values after the \"n\"th position. If the sequence values are unbounded so that the lim sup is ∞, then the power series does not converge near \"a\", while if the lim sup is 0 then the radius of convergence is ∞, meaning that the series converges on the entire plane.\n\n Without loss of generality assume that formula_4. We will show first that the power series formula_5 converges for formula_6, and then that it diverges for formula_7.\n\nFirst suppose formula_6. Let formula_9 not be zero or ±infinity. For any formula_10, there exists only a finite number of formula_11 such that formula_12. Now formula_13 for all but a finite number of formula_11, so the series formula_5 converges if formula_16. This proves the first part.\n\nConversely, for formula_10, formula_18 for infinitely many formula_19, so if formula_20, we see that the series cannot converge because its \"n\"th term does not tend to 0.\n\nLet formula_21 be a multi-index (a \"n\"-tuple of integers) with formula_22, then formula_23 converges with radius of convergence formula_24 (which is also a multi-index) if and only if\n\nto the multidimensional power series\n\nThe proof can be found in the book Introduction to Complex Analysis Part II functions in several Variables by B. V. Shabat\n"}
{"id": "880406", "url": "https://en.wikipedia.org/wiki?curid=880406", "title": "Caustic (mathematics)", "text": "Caustic (mathematics)\n\nIn differential geometry and geometric optics, a caustic is the envelope of rays either reflected or refracted by a manifold. It is related to the concept of caustics in optics. The ray's source may be a point (called the radiant) or parallel rays from a point at infinity, in which case a direction vector of the rays must be specified.\n\nMore generally, especially as applied to symplectic geometry and singularity theory, a caustic is the critical value set of a Lagrangian mapping where is a Lagrangian immersion of a Lagrangian submanifold \"L\" into a symplectic manifold \"M\", and is a Lagrangian fibration of the symplectic manifold \"M\". The caustic is a subset of the Lagrangian fibration's base space \"B\".\n\nA catacaustic is the reflective case. \n\nWith a radiant, it is the evolute of the orthotomic of the radiant.\n\nThe planar, parallel-source-rays case: suppose the direction vector is formula_1 and the mirror curve is parametrised as formula_2. The normal vector at a point is formula_3; the reflection of the direction vector is (normal needs special normalization)\nHaving components of found reflected vector treat it as a tangent\nUsing the simplest envelope form\nwhich may be unaesthetic, but formula_10 gives a linear system in formula_11 and so it is elementary to obtain a parametrisation of the catacaustic. Cramer's rule would serve.\n\nLet the direction vector be (0,1) and the mirror be formula_12\nThen\nand formula_10 has solution formula_22; \"i.e.\", light entering a parabolic mirror parallel to its axis is reflected through the focus.\n\n"}
{"id": "246223", "url": "https://en.wikipedia.org/wiki?curid=246223", "title": "Connected component (graph theory)", "text": "Connected component (graph theory)\n\nIn graph theory, a connected component (or just component) of an undirected graph is a subgraph in which any two vertices are connected to each other by paths, and which is connected to no additional vertices in the supergraph. For example, the graph shown in the illustration has three connected components. A vertex with no incident edges is itself a connected component. A graph that is itself connected has exactly one connected component, consisting of the whole graph.\n\nAn alternative way to define connected components involves the equivalence classes of an equivalence relation that is defined on the vertices of the graph.\nIn an undirected graph, a vertex \"v\" is \"reachable\" from a vertex \"u\" if there is a path from \"u\" to \"v\". In this definition, a single vertex is counted as a path of length zero, and the same vertex may occur more than once within a path.\nReachability is an equivalence relation, since:\nThe connected components are then the induced subgraphs formed by the equivalence classes of this relation.\n\nThe number of connected components is an important topological invariant of a graph. In topological graph theory it can be interpreted as the zeroth Betti number of the graph. In algebraic graph theory it equals the multiplicity of 0 as an eigenvalue of the Laplacian matrix of the graph. It is also the index of the first nonzero coefficient of the chromatic polynomial of a graph. Numbers of connected components play a key role in the Tutte theorem characterizing graphs that have perfect matchings, and in the definition of graph toughness.\n\nIt is straightforward to compute the connected components of a graph in linear time (in terms of the numbers of the vertices and edges of the graph) using either breadth-first search or depth-first search. In either case, a search that begins at some particular vertex \"v\" will find the entire connected component containing \"v\" (and no more) before returning. To find all the connected components of a graph, loop through its vertices, starting a new breadth first or depth first search whenever the loop reaches a vertex that has not already been included in a previously found connected component. describe essentially this algorithm, and state that at that point it was \"well known\".\n\nThere are also efficient algorithms to dynamically track the connected components of a graph as vertices and edges are added, as a straightforward application of disjoint-set data structures. These algorithms require amortized O(α(\"n\")) time per operation, where adding vertices and edges and determining the connected component in which a vertex falls are both operations, and α(\"n\") is a very slow-growing inverse of the very quickly growing Ackermann function. A related problem is tracking connected components as all edges are deleted from a graph, one by one; an algorithm exists to solve this with constant time per query, and O(|V||E|) time to maintain the data structure; this is an amortized cost of O(|V|) per edge deletion. For forests, the cost can be reduced to O(q + |V| log |V|), or O(log |V|) amortized cost per edge deletion .\n\nResearchers have also studied algorithms for finding connected components in more limited models of computation, such as programs in which the working memory is limited to a logarithmic number of bits (defined by the complexity class L). asked whether it is possible to test in logspace whether two vertices belong to the same connected component of an undirected graph, and defined a complexity class SL of problems logspace-equivalent to connectivity. Finally succeeded in finding an algorithm for solving this connectivity problem in logarithmic space, showing that L = SL.\n\nIn random graphs the sizes of connected components are given by a random variable, which, in turn, depends on the specific model. \n\nThe formula_1 model has three regions with seemingly different behavior: \n\n\"Subcritical\" formula_2: All components are simple and very small, the largest component has size formula_3;\n\n\"Critical\" formula_4: formula_5;\n\n\"Supercritical\" formula_6:formula_7 where formula_8 is the positive solution to the equation formula_9.\n\nWhere formula_10 and formula_11 are respectively the largest and the second largest components. All other components have their sizes of the order formula_12.\n\n\n\n"}
{"id": "8652020", "url": "https://en.wikipedia.org/wiki?curid=8652020", "title": "Convex body", "text": "Convex body\n\nIn mathematics, a convex body in \"n\"-dimensional Euclidean space formula_1 is a compact convex set with non-empty interior.\n\nA convex body \"K\" is called symmetric if it is centrally symmetric with respect to the origin, i.e. a point \"x\" lies in \"K\" if and only if its antipode, −\"x\", also lies in \"K\". Symmetric convex bodies are in a one-to-one correspondence with the unit balls of norms on R.\n\nImportant examples of convex bodies are the Euclidean ball, the hypercube and the cross-polytope.\n\n"}
{"id": "58035236", "url": "https://en.wikipedia.org/wiki?curid=58035236", "title": "Daniela De Silva", "text": "Daniela De Silva\n\nDaniela De Silva is an Italian mathematician known for her expertise in partial differential equations. She is an associate professor of mathematics at Barnard College and Columbia University.\n\nDe Silva did her undergraduate studies in mathematics at the University of Naples Federico II, and earned a bachelor's degree there in 1997.\nShe completed her doctorate at the Massachusetts Institute of Technology in 2005. Her dissertation, \"Existence and Regularity of Monotone Solutions to a Free Boundary Problem\", was supervised by David Jerison.\nAfter postdoctoral research at the Mathematical Sciences Research Institute and a term as J. J. Sylvester Assistant Professor at Johns Hopkins University, she joined the Barnard and Columbia faculty in 2007.\n\nDe Silva won the 2016 Sadosky Prize of the Association for Women in Mathematics for \"fundamental contributions to the regularity theory of nonlinear elliptic partial differential equations and non-local integro-differential equations\". In 2018, Barnard honored her with their Tow Professorship for Distinguished Scholars and Practitioners.\n"}
{"id": "28925553", "url": "https://en.wikipedia.org/wiki?curid=28925553", "title": "Dirac spectrum", "text": "Dirac spectrum\n\nIn mathematics, a Dirac spectrum, named after Paul Dirac, is the spectrum of eigenvalues of a Dirac operator on a Riemannian manifold with a spin structure. The isospectral problem for the Dirac spectrum asks whether two Riemannian spin manifolds have identical spectra. The Dirac spectrum depends on the spin structure in the sense that there exists a Riemannian manifold with two different spin structures that have different Dirac spectra.\n\n"}
{"id": "49492", "url": "https://en.wikipedia.org/wiki?curid=49492", "title": "Divisor", "text": "Divisor\n\nIn mathematics, a divisor of an integer formula_1, also called a factor of formula_1, is an integer formula_3 that may be multiplied by some integer to produce formula_1. In this case, one also says that formula_1 is a multiple of formula_6 An integer formula_1 is divisible by another integer formula_3 if formula_3 is a divisor of formula_1; this implies dividing formula_1 by formula_3 leaves no remainder.\n\nTwo versions of the definition of a divisor are commonplace:\n\n\n\nIn the remainder of this article, which definition is applied is indicated where this is significant.\n\nDivisors can be negative as well as positive, although sometimes the term is restricted to positive divisors. For example, there are six divisors of 4; they are 1, 2, 4, −1, −2, and −4, but only the positive ones (1, 2, and 4) would usually be mentioned.\n\n1 and −1 divide (are divisors of) every integer. Every integer (and its negation) is a divisor of itself. Every integer is a divisor of 0. Integers divisible by 2 are called even, and integers not divisible by 2 are called odd.\n\n1, −1, \"n\" and −\"n\" are known as the trivial divisors of \"n\". A divisor of \"n\" that is not a trivial divisor is known as a non-trivial divisor. A non-zero integer with at least one non-trivial divisor is known as a composite number, while the units −1 and 1 and prime numbers have no non-trivial divisors.\n\nThere are divisibility rules that allow one to recognize certain divisors of a number from the number's digits.\n\nThe generalization can be said to be the concept of \"divisibility\" in any integral domain.\n\n\nThere are some elementary rules:\n\nIf formula_51, and gcdformula_52, then formula_37. This is called Euclid's lemma.\n\nIf formula_54 is a prime number and formula_55 then formula_56 or formula_57.\n\nA positive divisor of formula_1 which is different from formula_1 is called a proper divisor or an aliquot part of formula_1. A number that does not evenly divide formula_1 but leaves a remainder is called an aliquant part of formula_1.\n\nAn integer formula_63 whose only proper divisor is 1 is called a prime number. Equivalently, a prime number is a positive integer that has exactly two positive factors: 1 and itself.\n\nAny positive divisor of formula_1 is a product of prime divisors of formula_1 raised to some power. This is a consequence of the fundamental theorem of arithmetic.\n\nA number formula_1 is said to be perfect if it equals the sum of its proper divisors, deficient if the sum of its proper divisors is less than formula_1, and abundant if this sum exceeds formula_1.\n\nThe total number of positive divisors of formula_1 is a multiplicative function formula_70, meaning that when two numbers formula_3 and formula_1 are relatively prime, then formula_73. For instance, formula_74; the eight divisors of 42 are 1, 2, 3, 6, 7, 14, 21 and 42. However, the number of positive divisors is not a totally multiplicative function: if the two numbers formula_3 and formula_1 share a common divisor, then it might not be true that formula_73. The sum of the positive divisors of formula_1 is another multiplicative function formula_79 (e.g. formula_80). Both of these functions are examples of divisor functions.\nIf the prime factorization of formula_1 is given by\n\nthen the number of positive divisors of formula_1 is\n\nand each of the divisors has the form\n\nwhere formula_86 for each formula_87\n\nFor every natural formula_1, formula_89.\n\nAlso,\nwhere formula_91 is Euler–Mascheroni constant.\nOne interpretation of this result is that a randomly chosen positive integer \"n\" has an expected\nnumber of divisors of about formula_92.\n\nGiven the definition for which formula_93 holds, the relation of divisibility turns the set formula_94 of non-negative integers into a partially ordered set: a complete distributive lattice. The largest element of this lattice is 0 and the smallest is 1. The meet operation ∧ is given by the greatest common divisor and the join operation ∨ by the least common multiple. This lattice is isomorphic to the dual of the lattice of subgroups of the infinite cyclic group formula_95.\n\n\n"}
{"id": "32755464", "url": "https://en.wikipedia.org/wiki?curid=32755464", "title": "Exponentially closed field", "text": "Exponentially closed field\n\nIn mathematics, an exponentially closed field is an ordered field of formula_1 which has an order preserving isomorphism formula_2 of the additive group of formula_1 onto the multiplicative group of positive elements of formula_1 such that \nformula_5 for some natural number formula_6.\n\nIsomorphism formula_2 is called an exponential function in formula_1.\n\n\n"}
{"id": "147460", "url": "https://en.wikipedia.org/wiki?curid=147460", "title": "Free variables and bound variables", "text": "Free variables and bound variables\n\nIn mathematics, and in other disciplines involving formal languages, including mathematical logic and computer science, a free variable is a notation (symbol) that specifies places in an expression where substitution may take place and is not a parameter of this or any containiner expression. Some older books use the terms real variable and apparent variable for free variable and bound variable. The idea is related to a placeholder (a symbol that will later be replaced by some literal string), or a wildcard character that stands for an unspecified symbol.\n\nIn computer programming, the term free variable refers to variables used in a function that are neither local variables nor parameters of that function. The term non-local variable is often a synonym in this context.\n\nA bound variable is a variable that was previously \"free\", but has been \"bound\" to a specific value or set of values called domain of discourse or universe. For example, the variable \"x\" becomes a bound variable when we write:\n\nor\n\nIn either of these propositions, it does not matter logically whether we use \"x\" or some other letter. However, it could be confusing to use the same letter again elsewhere in some compound proposition. That is, free variables become bound, and then in a sense \"retire\" from being available as stand-in values for other values in the creation of formulae.\n\nThe term \"dummy variable\" is also sometimes used for a bound variable (more often in general mathematics than in computer science), but that use can create an ambiguity with the definition of dummy variables in regression analysis.\n\nBefore stating a precise definition of free variable and bound variable, the following are some examples that perhaps make these two concepts clearer than the definition would:\n\nIn the expression\n\n\"n\" is a free variable and \"k\" is a bound variable; consequently the value of this expression depends on the value of \"n\", but there is nothing called \"k\" on which it could depend.\n\nIn the expression\n\n\"y\" is a free variable and \"x\" is a bound variable; consequently the value of this expression depends on the value of \"y\", but there is nothing called \"x\" on which it could depend.\n\nIn the expression\n\n\"x\" is a free variable and \"h\" is a bound variable; consequently the value of this expression depends on the value of \"x\", but there is nothing called \"h\" on which it could depend.\n\nIn the expression\n\n\"z\" is a free variable and \"x\" and \"y\" are bound variables; consequently the logical value of this expression depends on the value of \"z\", but there is nothing called \"x\" or \"y\" on which it could depend.\n\nThe following\n\nare some common variable-binding operators. Each of them binds the variable x for some set S.\n\nNote that many of these are operators which act on functions of the bound variable. In more complicated contexts, such notations can become awkward and confusing. It can be useful to switch to notations which make the binding explicit, such as\n\nfor sums or\n\nfor differentiation.\n\nVariable-binding mechanisms occur in different contexts in mathematics, logic and computer science. In all cases, however, they are purely syntactic properties of expressions and variables in them. For this section we can summarize syntax by identifying an expression with a tree whose leaf nodes are variables, constants, function constants or predicate constants and whose non-leaf nodes are logical operators. This expression can then be determined by doing an inorder traversal of the tree. Variable-binding operators are logical operators that occur in almost every formal language. Languages that do not have them may be either extremely inexpressive or extremely difficult to use. A binding operator Q takes two arguments: a variable \"v\" and an expression \"P\", and when applied to its arguments produces a new expression Q(\"v\", \"P\"). The meaning of binding operators is supplied by the semantics of the language and does not concern us here.\n\nVariable binding relates three things: a variable \"v\", a location \"a\" for that variable in an expression and a non-leaf node \"n\" of the form Q(\"v\", \"P\"). Note: we define a location in an expression as a leaf node in the syntax tree. Variable binding occurs when that location is below the node \"n\".\n\nIn the lambda calculus, codice_1 is a bound variable in the term codice_2, and a free variable of codice_3. We say codice_1 is bound in codice_5 and free in codice_3. If codice_3 contains a subterm codice_8 then codice_1 is rebound in this term. This nested, inner binding of codice_1 is said to \"shadow\" the outer binding. Occurrences of codice_1 in codice_12 are free occurrences of the new codice_1.\n\nVariables bound at the top level of a program are technically free variables within the terms to which they are bound but are often treated specially because they can be compiled as fixed addresses. Similarly, an identifier bound to a recursive function is also technically a free variable within its own body but is treated specially.\n\nA \"closed term\" is one containing no free variables.\n\nTo give an example from mathematics, consider an expression which defines a function\n\nwhere t is an expression. t may contain some, all or none of the \"x\", ..., \"x\" and it may contain other variables. In this case we say that function definition binds the variables\n\"x\", ..., \"x\".\n\nIn this manner, function definition expressions of the kind shown above can be thought of as \"the\" variable binding operator, analogous to the lambda expressions of lambda calculus. Other binding operators, like the summation sign, can be thought of as higher-order functions applying to a function. So, for example, the expression\n\ncould be treated as a notation for\n\nwhere formula_11 is an operator with two parameters—a one-parameter function, and a set to evaluate that function over. The other operators listed above can be expressed in similar ways; for example, the universal quantifier formula_12 can be thought of as an operator that evaluates to the logical conjunction of the boolean-valued function \"P\" applied over the (possibly infinite) set \"S\".\n\nWhen analyzed in formal semantics, natural languages can be seen to have free and bound variables. In English, personal pronouns like \"he\", \"she\", \"they\", etc. can act as free variables.\n\nIn the sentence above, the possessive pronoun \"her\" is a free variable. It may refer to the previously mentioned \"Lisa\" or to any other female. In other words, \"her book\" could be referring to Lisa's book (an instance of coreference) or to a book that belongs to a different female (e.g. Jane's book). Whoever the referent of \"her\" is can be established according to the situational (i.e. pragmatic) context. The identity of the referent can be shown using coindexing subscripts where \"i\" indicates one referent and \"j\" indicates a second referent (different from \"i\"). Thus, the sentence \"Lisa found her book\" has the following interpretations:\n\nThe distinction is not purely of academic interest, as some languages do actually have different forms for \"her\" and \"her\": for example, Norwegian and Swedish translates coreferent \"her\" as \"sin\" and noncoreferent \"her\" as \"hennes\".\n\nEnglish does allow specifying coreference, but it is optional, as both interpretations of the previous example are valid (the ungrammatical interpretation is indicated with an asterisk):\n\nHowever, reflexive pronouns, such as \"himself\", \"herself\", \"themselves\", etc., and reciprocal pronouns, such as \"each other\", act as bound variables. In a sentence like the following:\n\nthe reflexive \"herself\" can only refer to the previously mentioned antecedent \"Jane\". It can never refer to a different female person. In other words, the person being hurt and the person doing the hurting are both the same person, i.e. \"Jane\". The semantics of this sentence is abstractly: JANE hurt JANE. And it cannot be the case that this sentence could mean JANE hurt LISA. The reflexive \"herself\" must refer and can only refer to the previously mentioned \"Jane\". In this sense, the variable \"herself\" is bound to the noun \"Jane\" that occurs in subject position. Indicating the coindexation, the first interpretation with \"Jane\" and \"herself\" coindexed is permissible, but the other interpretation where they are not coindexed is ungrammatical:\n\nNote that the coreference binding can be represented using a lambda expression as mentioned in the previous Formal explanation section. The sentence with the reflexive could be represented as\n\nin which \"Jane\" is the subject referent argument and \"λx.x hurt x\" is the predicate function (a lambda abstraction) with the lambda notation and \"x\" indicating both the semantic subject and the semantic object of sentence as being bound. This returns the semantic interpretation \"JANE hurt JANE\" with \"JANE\" being the same person.\n\nPronouns can also behave in a different way. In the sentence below\n\nthe pronoun \"her\" can only refer to a female that is not Ashley. This means that it can never have a reflexive meaning equivalent to \"Ashley hit herself\". The grammatical and ungrammatical interpretations are:\n\nThe first interpretation is impossible. Only the second interpretation is permitted by the grammar.\n\nThus, it can be seen that reflexives and reciprocals are bound variables (known technically as anaphors) while true pronouns are free variables in some grammatical structures but variables that cannot be bound in other grammatical structures. The binding phenomena found in natural languages was particularly important to the syntactic government and binding theory (see also: Binding (linguistics)).\n\n\n"}
{"id": "39783039", "url": "https://en.wikipedia.org/wiki?curid=39783039", "title": "Function of several real variables", "text": "Function of several real variables\n\nIn mathematical analysis, and applications in geometry, applied mathematics, engineering, natural sciences, and economics, a function of several real variables or real multivariate function is a function with more than one argument, with all arguments being real variables. This concept extends the idea of a function of a real variable to several variables. The \"input\" variables take real values, while the \"output\", also called the \"value of the function\", may be real or complex. However, the study of the complex valued functions may be easily reduced to the study of the real valued functions, by considering the real and imaginary parts of the complex function; therefore, unless explicitly specified, only real valued functions will be considered in this article.\n\nThe domain of a function of \"n\" variables is the subset of for which the function is defined. As usual, the domain of a function of several real variables is supposed to contain an open subset of .\n\nA real-valued function of real variables is a function that takes as input real numbers, commonly represented by the variables , for producing another real number, the \"value\" of the function, commonly denoted . For simplicity, in this article a real-valued function of several real variables will be simply called a function. To avoid any ambiguity, the other types of functions that may occur will be explicitly specified.\n\nSome functions are defined for all real values of the variables (one says that they are everywhere defined), but some other functions are defined only if the value of the variable are taken in a subset of , the domain of the function, which is always supposed to contain an open subset of . In other words, a real-valued function of real variables is a function\n\nsuch that its domain is a subset of that contains an open set.\n\nAn element of being an -tuple (usually delimited by parentheses), the general notation for denoting functions would be . The common usage, much older than the general definition of functions between sets, is to not use double parentheses and to simply write .\n\nIt is also common to abbreviate the -tuple by using a notation similar to that for vectors, like boldface , underline , or overarrow . This article will use bold.\n\nA simple example of a function in two variables could be:\n\nwhich is the volume of a cone with base area and height measured perpendicularly from the base. The domain restricts all variables to be positive since lengths and areas must be positive.\n\nFor an example of a function in two variables:\n\nwhere and are real non-zero constants. Using the three-dimensional Cartesian coordinate system, where the xy plane is the domain and the z axis is the codomain , one can visualize the image to be a two-dimensional plane, with a slope of in the positive x direction and a slope of in the positive y direction. The function is well-defined at all points in . The previous example can be extended easily to higher dimensions:\n\nfor non-zero real constants , which describes a -dimensional hyperplane.\n\nThe Euclidean norm:\n\nis also a function of \"n\" variables which is everywhere defined, while\nis defined only for .\n\nFor a non-linear example function in two variables:\n\nwhich takes in all points in , a disk of radius \"punctured\" at the origin in the plane , and returns a point in . The function does not include the origin , if it did then would be ill-defined at that point. Using a 3d Cartesian coordinate system with the xy plane as the domain , and the z axis the codomain , the image can be visualized as a curved surface.\n\nThe function can be evaluated at the point in :\n\nHowever, the function couldn't be evaluated at, say\n\nsince these values of and do not satisfy the domain's rule.\n\nThe image of a function is the set of all values of when the -tuple runs in the whole domain of . For a continuous (see below for a definition) real-valued function which has a connected domain, the image is either an interval or a single value. In the latter case, the function is a constant function.\n\nThe preimage of a given real number is called a level set. It is the set of the solutions of the equation .\n\nThe domain of a function of several real variables is a subset of that is sometimes, but not always, explicitly defined. In fact, if one restricts the domain of a function to a subset , one gets formally a different function, the \"restriction\" of to , which is denoted . In practice, it is often (but not always) not harmful to identify and , and to omit the subscript .\n\nConversely, it is sometimes possible to enlarge naturally the domain of a given function, for example by continuity or by analytic continuation. \n\nMoreover, many functions are defined in such a way that it is difficult to specify explicitly their domain. For example, given a function , it may be difficult to specify the domain of the function formula_16 If is a multivariate polynomial, (which has formula_17 as a domain), it is even difficult to test whether the domain of is also formula_17. This is equivalent to test whether a polynomial is always positive, and is the object of an active research area (see Positive polynomial).\n\nThe usual operations of arithmetic on the reals may be extended to real-valued functions of several real variables in the following way: \n\nIt follows that the functions of } variables that are everywhere defined and the functions of variables that are defined in some neighbourhood of a given point both form commutative algebras over the reals (-algebras). This is a prototypical example of a function space.\n\nOne may similarly define\nwhich is a function only if the set of the points in the domain of such that contains an open subset of . This constraint implies that the above two algebras are not fields.\n\nOne can easily obtain a function in one real variable by giving a constant value to all but one of the variables. For example, if is a point of the interior of the domain of the function , we can fix the values of to respectively, to get a univariable function\nwhose domain contains an interval centered at . This function may also be viewed as the restriction of the function to the line defined by the equations , for .\n\nOther univariable functions may be defined by restricting to any line passing through . These are the functions\nwhere the are real numbers that are not all zero.\n\nIn next section, we will show that, if the multivariable function is continuous, so are all these univariable functions, but the converse is not necessarily true.\n\nUntil the second part of 19th century, only continuous functions were considered by mathematicians. At that time, the notion of continuity was elaborated for the functions of one or several real variables a rather long time before the formal definition of a topological space and a continuous map between topological spaces. As continuous functions of several real variables are ubiquitous in mathematics, it is worth to define this notion without reference to the general notion of continuous maps between topological space.\n\nFor defining the continuity, it is useful to consider the distance function of , which is an everywhere defined function of real variables:\n\nA function is continuous at a point which is interior to its domain, if, for every positive real number , there is a positive real number such that for all such that . In other words, may be chosen small enough for having the image by of the ball of radius centered at contained in the interval of length centered at . A function is continuous if it is continuous at every point of its domain.\n\nIf a function is continuous at , then all the univariate functions, that are obtained by fixing all the variables but one at the value , are continuous at . The converse is false; this means that all these univariate functions may be continuous for a function that is not continuous at . For an example, let us consider the function such that , and is otherwise defined by\nThe functions and are both constant and equal to zero, and are therefore continuous. The function is not continuous at , because, if and , we have , even if is very small. Although not continuous, this function has the further property that all the univariate functions obtained by restricting it to a line passing through are also continuous. In fact, we have\nfor . The limit of a real-valued function of several real variables is as follows. Let be a point in topological closure of the domain of the function . The function, has a limit when tends toward , denoted\nif the following condition is satisfied:\nFor every positive real number , there is a positive real number such that\nfor all in the domain such that\n\nIf the limit exists, it is unique. If is in the interior of the domain, the limit exists if and only if the function is continuous at . In this case, we have\n\nWhen is in the boundary of the domain of , and if has a limit at , the latter formula allows to \"extend by continuity\" the domain of to .\n\nA symmetric function is a function that is unchanged when two variables and are interchanged:\n\nwhere and are each one of . For example:\n\nis symmetric in since interchanging any pair of leaves unchanged, but is not symmetric in all of , since interchanging with or or gives a different function.\n\nSuppose the functions\n\nor more compactly , are all defined on a domain . As the -tuple varies in , a subset of , the -tuple varies in another region a subset of . To restate this:\n\nThen, a function of the functions defined on ,\n\nis a function composition defined on , in other terms the mapping\n\nNote the numbers and do not need to be equal.\n\nFor example, the function\n\ndefined everywhere on can be rewritten by introducing\n\nwhich is also everywhere defined in to obtain\n\nFunction composition can be used to simplify functions, which is useful for carrying out multiple integrals and solving partial differential equations.\n\nElementary calculus is the calculus of real-valued functions of one real variable, and the principal ideas of differentiation and integration of such functions can be extended to functions of more than one real variable; this extension is multivariable calculus.\n\nPartial derivatives can be defined with respect to each variable:\n\nPartial derivatives themselves are functions, each of which represents the rate of change of parallel to one of the axes at all points in the domain (if the derivatives exist and are continuous—see also below). A first derivative is positive if the function increases along the direction of the relevant axis, negative if it decreases, and zero if there is no increase or decrease. Evaluating a partial derivative at a particular point in the domain gives the rate of change of the function at that point in the direction parallel to a particular axis, a real number.\n\nFor real-valued functions of a real variable, , its ordinary derivative is geometrically the gradient of the tangent line to the curve at all points in the domain. Partial derivatives extend this idea to tangent hyperplanes to a curve.\n\nThe second order partial derivatives can be calculated for every pair of variables:\n\nGeometrically, they are related to the local curvature of the function's image at all points in the domain. At any point where the function is well-defined, the function could be increasing along some axes, and/or decreasing along other axes, and/or not increasing or decreasing at all along other axes.\n\nThis leads to a variety of possible stationary points: global or local maxima, global or local minima, and saddle points—the multidimensional analogue of inflection points for real functions of one real variable. The Hessian matrix is a matrix of all the second order partial derivatives, which are used to investigate the stationary points of the function, important for mathematical optimization.\n\nIn general, partial derivatives of higher order have the form:\n\nwhere are each integers between and such that , using the definitions of zeroth partial derivatives as identity operators:\n\nThe number of possible partial derivatives increases with , although some mixed partial derivatives (those with respect to more than one variable) are superfluous, because of the symmetry of second order partial derivatives. This reduces the number of partial derivatives to calculate for some .\n\nA function is differentiable in a neighborhood of a point if there is an -tuple of numbers dependent on in general, , so that:\n\nwhere as . This means that if is differentiable at a point , then is continuous at , although the converse is not true - continuity in the domain does not imply differentiability in the domain. If is differentiable at then the first order partial derivatives exist at and:\n\nfor , which can be found from the definitions of the individual partial derivatives, so the partial derivatives of exist.\n\nAssuming an -dimensional analogue of a rectangular Cartesian coordinate system, these partial derivatives can be used to form a vectorial linear differential operator, called the gradient (also known as \"nabla\" or \"del\") in this coordinate system:\n\nused extensively in vector calculus, because it is useful for constructing other differential operators and compactly formulating theorems in vector calculus.\n\nThen substituting the gradient (evaluated at with a slight rearrangement gives:\n\nwhere denotes the dot product. This equation represents the best linear approximation of the function at all points within a neighborhood of . For infinitesimal changes in and as :\n\nwhich is defined as the total differential, or simply differential, of , at . This expression corresponds to the total infinitesimal change of , by adding all the infinitesimal changes of in all the directions. Also, can be construed as a covector with basis vectors as the infinitesimals in each direction and partial derivatives of as the components.\n\nGeometrically is perpendicular to the level sets of , given by which for some constant describes an -dimensional hypersurface. The differential of a constant is zero:\n\nin which is an infinitesimal change in in the hypersurface , and since the dot product of and is zero, this means is perpendicular to .\n\nIn arbitrary curvilinear coordinate systems in dimensions, the explicit expression for the gradient would not be so simple - there would be scale factors in terms of the metric tensor for that coordinate system. For the above case used throughout this article, the metric is just the Kronecker delta and the scale factors are all 1.\n\nIf all first order partial derivatives evaluated at a point in the domain:\n\nexist and are continuous for all in the domain, has differentiability class . In general, if all order partial derivatives evaluated at a point :\n\nexist and are continuous, where , and are as above, for all in the domain, then is differentiable to order throughout the domain and has differentiability class .\n\nIf is of differentiability class , has continuous partial derivatives of all order and is called \"smooth\". If is an \"analytic function\" and equals its Taylor series about any point in the domain, the notation denotes this differentiability class.\n\nDefinite integration can be extended to multiple integration over the several real variables with the notation;\n\nwhere each region is a subset of or all of the real line:\n\nand their Cartesian product gives the region to integrate over as a single set:\n\nan -dimensional hypervolume. When evaluated, a definite integral is a real number if the integral converges in the region of integration (the result of a definite integral may diverge to infinity for a given region, in such cases the integral remains ill-defined). The variables are treated as \"dummy\" or \"bound\" variables which are substituted for numbers in the process of integration.\n\nThe integral of a real-valued function of a real variable with respect to has geometric interpretation as the area bounded by the curve and the -axis. Multiple integrals extend the dimensionality of this concept: assuming an -dimensional analogue of a rectangular Cartesian coordinate system, the above definite integral has the geometric interpretation as the -dimensional hypervolume bounded by and the axes, which may be positive, negative, or zero, depending on the function being integrated (if the integral is convergent).\n\nWhile bounded hypervolume is a useful insight, the more important idea of definite integrals is that they represent total quantities within space. This has significance in applied mathematics and physics: if is some scalar density field and are the position vector coordinates, i.e. some scalar quantity per unit \"n\"-dimensional hypervolume, then integrating over the region gives the total amount of quantity in . The more formal notions of hypervolume is the subject of measure theory. Above we used the Lebesgue measure, see Lebesgue integration for more on this topic.\n\nWith the definitions of multiple integration and partial derivatives, key theorems can be formulated, including the fundamental theorem of calculus in several real variables (namely Stokes' theorem), integration by parts in several real variables, and Taylor's theorem for multivariable functions. Evaluating a mixture of integrals and partial derivatives can be done by using theorem differentiation under the integral sign.\n\nOne can collect a number of functions each of several real variables, say\n\ninto an -tuple, or sometimes as a column vector or row vector, respectively:\n\nall treated on the same footing as an -component vector field, and use whichever form is convenient. All the above notations have a common compact notation . The calculus of such vector fields is vector calculus. For more on the treatment of row vectors and column vectors of multivariable functions, see matrix calculus.\n\nA real-valued implicit function of several real variables is not written in the form \"\". Instead, the mapping is from the space to the zero element in (just the ordinary zero 0):\n\nand\n\nis an equation in all the variables. Implicit functions are a more general way to represent functions, since if:\n\nthen we can always define:\n\nbut the converse is not always possible, i.e. not all implicit functions have an explicit form.\n\nFor example, using interval notation, let\n\nChoosing a 3-dimensional (3D) Cartesian coordinate system, this function describes the surface of a 3D ellipsoid centered at the origin with constant semi-major axes , along the positive \"x\", \"y\" and \"z\" axes respectively. In the case , we have a sphere of radius centered at the origin. Other conic section examples which can be described similarly include the hyperboloid and paraboloid, more generally so can any 2D surface in 3D Euclidean space. The above example can be solved for , or ; however it is much tidier to write it in an implicit form.\n\nFor a more sophisticated example:\n\nfor non-zero real constants , this function is well-defined for all , but it cannot be solved explicitly for these variables and written as \", \", etc.\n\nThe implicit function theorem of more than two real variables deals with the continuity and differentiability of the function, as follows. Let be a continuous function with continuous first order partial derivatives, and let \"ϕ\" evaluated at a point be zero:\n\nand let the first partial derivative of with respect to evaluated at be non-zero:\n\nThen, there is an interval containing , and a region containing , such that for every in there is exactly one value of in satisfying , and is a continuous function of so that . The total differentials of the functions are:\n\nSubstituting into the latter differential and equating coefficients of the differentials gives the first order partial derivatives of with respect to in terms of the derivatives of the original function, each as a solution of the linear equation\n\nfor .\n\nA complex-valued function of several real variables may be defined by relaxing, in the definition of the real-valued functions, the restriction of the codomain to the real numbers, and allowing complex values.\n\nIf is such a complex valued function, it may be decomposed as\nwhere and are real-valued functions. In other words, the study of the complex valued functions reduces easily to the study of the pairs of real valued functions.\n\nThis reduction works for the general properties. However, for an explicitly given function, such as:\n\nthe computation of the real and the imaginary part may be difficult.\n\nMultivariable functions of real variables arise inevitably in engineering and physics, because observable physical quantities are real numbers (with associated units and dimensions), and any one physical quantity will generally depend on a number of other quantities.\n\nExamples in continuum mechanics include the local mass density of a mass distribution, a scalar field which depends on the spatial position coordinates (here Cartesian to exemplify), , and time :\n\nSimilarly for electric charge density for electrically charged objects, and numerous other scalar potential fields.\n\nAnother example is the velocity field, a vector field, which has components of velocity that are each multivariable functions of spatial coordinates and time similarly:\n\nSimilarly for other physical vector fields such as electric fields and magnetic fields, and vector potential fields.\n\nAnother important example is the equation of state in thermodynamics, an equation relating pressure , temperature , and volume of a fluid, in general it has an implicit form:\n\nThe simplest example is the ideal gas law:\n\nwhere is the number of moles, constant for a fixed amount of substance, and the gas constant. Much more complicated equations of state have been empirically derived, but they all have the above implicit form.\n\nReal-valued functions of several real variables appear pervasively in economics. In the underpinnings of consumer theory, utility is expressed as a function of the amounts of various goods consumed, each amount being an argument of the utility function. The result of maximizing utility is a set of demand functions, each expressing the amount demanded of a particular good as a function of the prices of the various goods and of income or wealth. In producer theory, a firm is usually assumed to maximize profit as a function of the quantities of various goods produced and of the quantities of various factors of production employed. The result of the optimization is a set of demand functions for the various factors of production and a set of supply functions for the various products; each of these functions has as its arguments the prices of the goods and of the factors of production.\n\nSome \"physical quantities\" may be actually complex valued - such as complex impedance, complex permittivity, complex permeability, and complex refractive index. These are also functions of real variables, such as frequency or time, as well as temperature.\n\nIn two-dimensional fluid mechanics, specifically in the theory of the potential flows used to describe fluid motion in 2d, the complex potential\n\nis a complex valued function of the two spatial coordinates and , and other \"real\" variables associated with the system. The real part is the velocity potential and the imaginary part is the stream function.\n\nThe spherical harmonics occur in physics and engineering as the solution to Laplace's equation, as well as the eigenfunctions of the z-component angular momentum operator, which are complex-valued functions of real-valued spherical polar angles:\n\nIn quantum mechanics, the wavefunction is necessarily complex-valued, but is a function of \"real\" spatial coordinates (or momentum components), as well as time :\n\nwhere each is related by a Fourier transform.\n\n\n"}
{"id": "246320", "url": "https://en.wikipedia.org/wiki?curid=246320", "title": "Generalized permutation matrix", "text": "Generalized permutation matrix\n\nIn mathematics, a generalized permutation matrix (or monomial matrix) is a matrix with the same nonzero pattern as a permutation matrix, i.e. there is exactly one nonzero entry in each row and each column. Unlike a permutation matrix, where the nonzero entry must be 1, in a generalized permutation matrix the nonzero entry can be any nonzero value. An example of a generalized permutation matrix is\n\nAn invertible matrix \"A\" is a generalized permutation matrix if and only if it can be written as a product of an invertible diagonal matrix \"D\" and an (implicitly invertible) permutation matrix \"P\": i.e.,\n\nThe set of \"n\"×\"n\" generalized permutation matrices with entries in a field \"F\" forms a subgroup of the general linear group GL(\"n\",\"F\"), in which the group of nonsingular diagonal matrices Δ(\"n\", \"F\") forms a normal subgroup. Indeed, the generalized permutation matrices are the normalizer of the diagonal matrices, meaning that the generalized permutation matrices are the \"largest\" subgroup of GL in which diagonal matrices are normal.\n\nThe abstract group of generalized permutation matrices is the wreath product of \"F\" and \"S\". Concretely, this means that it is the semidirect product of Δ(\"n\", \"F\") by the symmetric group \"S\":\nwhere \"S\" acts by permuting coordinates and the diagonal matrices Δ(\"n\", \"F\") are isomorphic to the \"n\"-fold product (\"F\").\n\nTo be precise, the generalized permutation matrices are a (faithful) linear representation of this abstract wreath product: a realization of the abstract group as a subgroup of matrices.\n\n\n\nOne can generalize further by allowing the entries to lie in a ring, rather than in a field. In that case if the non-zero entries are required to be units in the ring (invertible), one again obtains a group. On the other hand, if the non-zero entries are only required to be non-zero, but not necessarily invertible, this set of matrices forms a semigroup instead.\n\nOne may also schematically allow the non-zero entries to lie in a group \"G,\" with the understanding that matrix multiplication will only involve multiplying a single pair of group elements, not \"adding\" group elements. This is an abuse of notation, since element of matrices being multiplied must allow multiplication and addition, but is suggestive notion for the (formally correct) abstract group formula_5 (the wreath product of the group \"G\" by the symmetric group).\n\nA signed permutation matrix is a generalized permutation matrix whose nonzero entries are ±1, and are the integer generalized permutation matrices with integer inverse.\n\n\nMonomial matrices occur in representation theory in the context of monomial representations. A monomial representation of a group \"G\" is a linear representation \"ρ\" : \"G\" → GL(\"n\", \"F\") of \"G\" (here \"F\" is the defining field of the representation) such that the image \"ρ\"(\"G\") is a subgroup of the group of monomial matrices.\n"}
{"id": "50406093", "url": "https://en.wikipedia.org/wiki?curid=50406093", "title": "Grassmann bundle", "text": "Grassmann bundle\n\nIn algebraic geometry, the Grassmann \"d\"-plane bundle of a vector bundle \"E\" on an algebraic scheme \"X\" is a scheme over \"X\":\nsuch that the fiber formula_2 is the Grassmannian of the \"d\"-dimensional vector subspaces of formula_3. For example, formula_4 is the projective bundle of \"E\". In the other direction, a Grassmann bundle is a special case of a (partial) flag bundle. Concretely, the Grassmann bundle can be constructed as a Quot scheme.\n\nLike the usual Grassmannian, the Grassmann bundle comes with natural vector bundles on it; namely, there are universal or tautological subbundle \"S\" and universal quotient bundle \"Q\" that fit into\nSpecifically, if \"V\" is in the fiber \"p\"(\"x\"), then the fiber of \"S\" over \"V\" is \"V\" itself; thus, \"S\" has rank \"r\" = rk(\"E\") and formula_6 is the determinant line bundle. Now, by the universal property of a projective bundle, the injection formula_7 corresponds to the morphism over \"X\":\nwhich is nothing but a family of Plücker embeddings.\n\nThe relative tangent bundle \"T\" of \"G\"(\"E\") is given by\nwhich morally is given by the second fundamental form. In the case \"d\" = 1, it is given as follows: if \"V\" is a finite-dimensional vector space, then for each line formula_10 in \"V\" passing through the origin (a point of formula_11), there is the natural identification (see Chern class#Complex projective space for example):\nand the above is the family-version of this identification. (The general care is a generalization of this.)\n\nIn the case \"d\" = 1, the early exact sequence tensored with the dual of \"S\" = \"O\"(-1) gives:\nwhich is the relative version of the Euler sequence.\n\n"}
{"id": "1513007", "url": "https://en.wikipedia.org/wiki?curid=1513007", "title": "Helmert–Wolf blocking", "text": "Helmert–Wolf blocking\n\nThe Helmert–Wolf blocking (HWB) is a least squares solution method for a sparse canonical block-angular (CBA) system of linear equations. Helmert (1843–1917) reported on the use of such systems for geodesy in 1880. Wolf (1910–1994) published his direct semianalytic solution based on ordinary Gaussian elimination in matrix form in 1978.\n\nThe HWB solution is very fast to compute but it is optimal only if observational errors do not correlate between the data blocks. The generalized canonical correlation analysis (gCCA) is the statistical method of choice for making those harmful cross-covariances vanish. This may, however, become quite tedious depending on the nature of the problem.\n\nThe HWB method is critical to satellite geodesy and similar large problems. The HWB method can be extended to fast Kalman filtering (FKF) by augmenting its linear regression equation system to take into account information from numerical forecasts, physical constraints and other ancillary data sources that are available in realtime. Operational accuracies can then be computed reliably from the theory of minimum-norm quadratic unbiased estimation (Minque) of C. R. Rao.\n\n"}
{"id": "732860", "url": "https://en.wikipedia.org/wiki?curid=732860", "title": "Iamblichus", "text": "Iamblichus\n\nIamblichus (; ; c. AD 245 – c. 325), was a Syrian Neoplatonist philosopher of Arab origin. He determined the direction that would later be taken by Neoplatonic philosophy. He was also the biographer of Pythagoras, a Greek mystic, philosopher and mathematician.\n\nAside from Iamblichus' own philosophical contribution, his \"Protrepticus\" is of importance for the study of the Sophists, owing to its preservation of approximately ten pages of an otherwise unknown Sophist known as the Anonymus Iamblichi. \n\nIamblichus was the chief representative of Syrian Neoplatonism, though his influence spread over much of the ancient world. The events of his life and his religious beliefs are not entirely known, but the main tenets of his beliefs can be worked out from his extant writings. According to the Suda, and his biographer Eunapius, he was born at Chalcis (modern Qinnasrin) in Syria. He was the son of a rich and illustrious family, and he is said to have been the descendant of several priest-kings of the Arab Royal family of Emesa. He initially studied under Anatolius of Laodicea, and later went on to study under Porphyry, a pupil of Plotinus, the founder of Neoplatonism. He disagreed with Porphyry over the practice of theurgy; Iamblichus responds to Porphyry's criticisms of theurgy in a book attributed to him, \"De Mysteriis Aegyptiorum\" (\"On the Egyptian Mysteries\").\n\nAround 304, he returned to Syria to found his own school at Apamea (near Antioch), a city famous for its Neoplatonic philosophers. Here he designed a curriculum for studying Plato and Aristotle, and he wrote commentaries on the two that survive only in fragments. Still, for Iamblichus, Pythagoras was the supreme authority. He is known to have written the \"Collection of Pythagorean Doctrines\", which, in ten books, comprised extracts from several ancient philosophers. Only the first four books, and fragments of the fifth, survive.\n\nScholars noted that the \"Exhortation to Philosophy of Iamblichus\" was composed in Apamea in the early 4th c. AD.\n\nIamblichus was said to have been a man of great culture and learning. He was also renowned for his charity and self-denial. Many students gathered around him, and he lived with them in genial friendship. According to Fabricius, he died during the reign of Constantine, sometime before 333.\n\nOnly a fraction of Iamblichus' books have survived. For our knowledge of his system, we are indebted partly to the fragments of writings preserved by Stobaeus and others. The notes of his successors, especially Proclus, as well as his five extant books and the sections of his great work on Pythagorean philosophy also reveal much of Iamblichus' system. Besides these, Proclus seems to have ascribed to him the authorship of the celebrated treatise \"Theurgia\", or \"On the Egyptian Mysteries\". However, the differences between this book and Iamblichus' other works in style and in some points of doctrine have led some to question whether Iamblichus was the actual author. Still, the treatise certainly originated from his school, and in its systematic attempt to give a speculative justification of the polytheistic cult practices of the day, it marks a turning-point in the history of thought where Iamblichus stood.\n\nAs a speculative theory, Neoplatonism had received its highest development from Plotinus. The modifications introduced by lamblichus were the detailed elaboration of its formal divisions, the more systematic application of the Pythagorean number-symbolism, and, under the influence of Oriental systems, a thoroughly mythical interpretation of what Neoplatonism had formerly regarded as notional. Unlike Plotinus who broke from Platonic tradition and asserted an undescended soul, Iamblichus re-affirmed the soul's embodiment in matter, believing matter to be as divine as the rest of the cosmos. It is most likely on this account that lamblichus was venerated.\n\nIamblichus was highly praised by those who followed his thought. By his contemporaries, Iamblichus was accredited with miraculous powers. The Roman emperor Julian, not content with Eunapius' more modest eulogy that he was inferior to Porphyry only in style, regarded Iamblichus as more than second to Plato, and claimed he would give all the gold of Lydia for one epistle of Iamblichus. During the revival of interest in his philosophy in the 15th and 16th centuries, the name of Iamblichus was scarcely mentioned without the epithet \"divine\" or \"most divine\".\n\nAt the head of his system, Iamblichus placed the transcendent incommunicable \"One\", the \"monad\", whose first principle is intellect, \"nous\". Immediately after the absolute One, lamblichus introduced a second superexistent \"One\" to stand between it and 'the many' as the producer of intellect, or soul, \"psyche\". This is the initial \"dyad\". The first and highest One (\"nous\"), which Plotinus represented under the three stages of (objective) being, (subjective) life, and (realized) intellect, is distinguished by Iamblichus into spheres of intelligible and intellective, the latter sphere being the domain of thought, the former of the objects of thought. These three entities, the \"psyche\", and the \"nous\" split into the intelligible and the intellective, form a \"triad\".\n\nBetween the two worlds, at once separating and uniting them, some scholars think there was inserted by lamblichus, as was afterwards by Proclus, a third sphere partaking of the nature of both. But this supposition depends on a merely conjectural emendation of the text. We read, however, that in the intellectual triad he assigned the third rank to the Demiurge. The Demiurge, the Platonic creator-god, is thus identified with the perfected \"nous\", the intellectual triad being increased to a \"hebdomad\". The identification of \"nous\" with the Demiurge is a significant moment in the Neoplatonic tradition and its adoption into and development within the Christian tradition. St. Augustine follows Plotinus by identifying \"nous\", which bears the \"logos\", with the creative principle. Whereas the Hellenes call that principle the Demiurge, Augustine identifies the activity and content of that principle as belonging to one of the three aspects of the Divine Trinity—the Son, who is the Word (\"logos\"). Iamblichus and Plotinus commonly assert that \"nous\" produced nature by mediation of the intellect, so here the intelligible gods are followed by a triad of psychic gods.\n\nThe first of these \"psychic gods\" is incommunicable and supramundane, while the other two seem to be mundane, though rational. In the third class, or mundane gods, there is a still greater wealth of divinities, of various local position, function, and rank. Iamblichus wrote of gods, angels, demons and heroes, of twelve heavenly gods whose number is increased to thirty-six or three hundred and sixty, and of seventy-two other gods proceeding from them, of twenty-one chiefs and forty-two nature-gods, besides guardian divinities, of particular individuals and nations. The realm of divinities stretched from the original One down to material nature itself, where soul in fact descended into matter and became \"embodied\" as human beings. Basically, Iamblichus greatly multiplied the ranks of being and divine entities in the universe, the number at each level relating to various mathematical proportions. The world is thus peopled by a crowd of superhuman beings influencing natural events and possessing and communicating knowledge of the future, and who are all accessible to prayers and offerings.\n\nThe whole of Iamblichus's complex theory is ruled by a mathematical formalism of triad, hebdomad, etc., while the first principle is identified with the monad, dyad and triad; symbolic meanings being also assigned to the other numbers. The theorems of mathematics, he says, apply absolutely to all things, from things divine to original matter. But though he subjects all things to number, he holds elsewhere that numbers are independent existences, and occupy a middle place between the limited and unlimited.\n\nAnother difficulty of the system is the account given of nature. It is said to be bound by the indissoluble chains of necessity called fate, and is distinguished from divine things that are not subject to fate. Yet, being itself the result of higher powers becoming corporeal, a continual stream of elevating influence flows from them to it, interfering with its necessary laws and turning to good ends the imperfect and evil. Of evil no satisfactory account is given; it is said to have been generated accidentally in the conflict between the finite and the infinite.\n\n\n\n\n"}
{"id": "8771718", "url": "https://en.wikipedia.org/wiki?curid=8771718", "title": "Instant Insanity", "text": "Instant Insanity\n\nThe \"Instant Insanity\" puzzle consists of four cubes with faces colored with four colors (commonly red, blue, green, and white). The objective of the puzzle is to stack these cubes in a column so that each side (front, back, left, and right) of the stack shows each of the four colors. The distribution of colors on each cube is unique.\n\nThis problem has a graph-theoretic solution in which a graph with four vertices labeled B, G, R, W (for blue, green, red, and white) can be used to represent each cube; there is an edge between two vertices if the two colors are on the opposite sides of the cube, and a loop at a vertex if the opposite sides have the same color. Trial and error is a slow way to solve this problem, as there are 41,472 possible arrangements of the four cubes, and only 8 of these give a valid solution.\n\nEvery position of Instant Insanity can be solved in eight moves or less.\n\nThe puzzle was created by Franz Owen Armbruster, also known as Frank Armbruster, and published by Parker Brothers in 1967. Over 12 million puzzles were sold. The puzzle is isomorphic to numerous older puzzles, among them the \"Katzenjammer\" puzzle, patented by Frederick A. Schossow in 1900, and \"The Great Tantalizer\" (circa 1940, and the most popular name prior to \"Instant Insanity\").\n\nThe puzzle is currently being marketed by Winning Moves.\n\nGiven the already colored cubes and the four distinct colors are (Red, Green, Blue, Yellow), we will try to generate a graph which gives a clear picture of all the positions of colors in all the cubes. The resultant graph will contain four vertices one for each color and we will number each edge from one through four (one number for each cube). If an edge connects two vertices (Red and Green) and the number of the edge is three, then it means that the third cube has Red and Green faces opposite to each other.\n\nTo find a solution to this problem we need the arrangement of four faces of each of the cubes. To represent the information of two opposite faces of all the four cubes we need a directed subgraph instead of an undirected one because two directions can only represent two opposite faces, but not whether a face should be at the front or at the back.\n\nSo if we have two directed subgraphs, we can actually represent all the four faces (which matter) of all the four cubes.\n\nWe cannot randomly select any two subgraphs - so what are the criteria for selecting? \n\nWe need to choose graphs such that:\n\nAfter understanding these restrictions if we try to derive the two sub graphs, we may end up with one possible set as shown in Image 3. Each edge color represents a cube.\nFrom the first sub graph we will derive the front and the rear face colors of the corresponding cube. For e.g.:\n\nFrom the second sub graph we will derive the left and the right face colors of the corresponding cube. For e.g.:\n\nThe third image shows the derived stack of cube which is the solution to the problem.\n\nIt is important to note that:\n\nGiven n cubes, with the faces of each cube coloured with one of n colours, determining if it's possible to stack the cubes so that each colour appears exactly once on each of the 4 sides of the stack is NP-complete.\nThe cube stacking game is a two-player game version of this puzzle. Given an ordered list of cubes, the players take turns adding the next cube to the top of a growing stack of cubes. The loser is the first player to add a cube that causes one of the four sides of the stack to have a color repeated more than once. Robertson and Munro proved that this game is PSPACE-complete, which illustrates the observation that NP-complete puzzles tend to lead to PSPACE-complete games.\n"}
{"id": "46813639", "url": "https://en.wikipedia.org/wiki?curid=46813639", "title": "Julien Amegandjin", "text": "Julien Amegandjin\n\nJulien Amegandjin (born May 2, 1940 in Togoville) is a Togolese academic. He received his education in Togo and France, studying mathematics and statistics at the University of Paris. He was first a teacher in France, and in the 1970s he was the director of the United Nations's Institut de Formation et de Recherche Demographiques (Institute for Demographic Training and Research) in Yaounde, Cameroon. In 1986 he spent a year devoted to the preparation of his book, \"Démographie mathématique\", which has since become an important textbook for students of demography. He has since devoted himself mainly in West Africa, to the development of agricultural statistics.\n"}
{"id": "17970928", "url": "https://en.wikipedia.org/wiki?curid=17970928", "title": "KSV-21", "text": "KSV-21\n\nThe KSV-21 Enhanced Crypto Card is a US National Security Agency-approved PC card that provides Type 1 encryption functions and key storage to the STE secure telephones and other devices.\n\nThe KSV-21 was originally built by SafeNet but has since been purchased by Raytheon as a tamper-resistant reprogrammable module and is backwards compatible with the KOV-14 Fortezza Plus card. It adds features including support for SCIP, Enhanced Firefly and NSA's 21st century Key Management Initiative. It can perform Type 1 encryption and hash operations at 80 Mbit/s. As of 2008, the KOV-14 is beginning to be phased out and replaced by the KSV-21.\n\nThe US version is certified to protect classified data through the Top Secret/SCI level as well as unclassified sensitive information. Versions are available for use with other nations, including:\n\n\nPrices range from $900 for single units to under $400/each in multi-thousand lot quantities as of 2008.\n"}
{"id": "4415205", "url": "https://en.wikipedia.org/wiki?curid=4415205", "title": "Linear canonical transformation", "text": "Linear canonical transformation\n\nIn Hamiltonian mechanics, the linear canonical transformation (LCT) is a family of integral transforms that generalizes many classical transforms. It has 4 parameters and 1 constraint, so it is a 3-dimensional family, and can be visualized as the action of the special linear group SL(R) on the time–frequency plane (domain).\n\nThe LCT generalizes the Fourier, fractional Fourier, Laplace, Gauss–Weierstrass, Bargmann and the Fresnel transforms as particular cases. The name \"linear canonical transformation\" is from canonical transformation, a map that preserves the symplectic structure, as SL(R) can also be interpreted as the symplectic group Sp, and thus LCTs are the linear maps of the time–frequency domain which preserve the symplectic form.\n\nThe basic properties of the properties of the transformations mention above, such as scaling, shift, coordinate multiplication are considered. Any linear canonical transformation is related to affine transformations in phase space, defined by time-frequency or position-momentum coordinates.\n\nThe LCT can be represented in several ways; most easily, it can be parameterized by a 2×2 matrix with determinant 1, i.e., an element of the special linear group SL(C). Then for any such matrix formula_1 with \"ad\" − \"bc\" = 1, the corresponding integral transform from a function formula_2 to formula_3 is defined as\n\nMany classical transforms are special cases of the linear canonical transform:\n\n\n\n\n\n\nComposition of LCTs corresponds to multiplication of the corresponding matrices; this is also known as the \"additivity property of the WDF\".\n\nIn detail, if the LCT is denoted by \"O\", i.e.\n\nthen\n\nwhere\n\nIf formula_12is the formula_13, where formula_13is the LCT of formula_15, then\n\nLCT is equal to the twisting operation for the WDF and the Cohen's class distribution also has the twisting operation.\n\nWe can freely use the LCT to transform the parallelogram whose center is at (0,0) to another parallelogram which has the same same area and the same center\nFrom this picture we know that the point (-1,2) transform to the point (0,1) and the point (1,2) transform to the point (4,3). As the result, we can write down the equations below\n\nformula_17\n\nwe can solve the equations and get (a,b,c,d) is equal to (2,1,1,1)\n\nFrom the following picture, we summarize the LCT with other transform or properties\n\nParaxial optical systems implemented entirely with thin lenses and propagation through free space and/or graded index (GRIN) media, are quadratic phase systems (QPS); these were known before Moshinsky and Quesne (1974) called attention to their significance in connection with canonical transformations in quantum mechanics. The effect of any arbitrary QPS on an input wavefield can be described using the linear canonical transform, a particular case of which was developed by Segal (1963) and Bargmann (1961) in order to formalize Fock's (1928) boson calculus.\n\nIt was shown that linear canonical transformations in quantum theory can be defined as the linear transformations mixing the momentum and position operators and leaving invariant the Canonical commutation relations \n\nCanonical transforms are used to analyze differential equations. These include diffusion, the Schrödinger free particle, the linear potential (free-fall), and the attractive and repulsive oscillator equations. It also includes a few others such as the Fokker–Planck equation. Although this class is far from universal, the ease with which solutions and properties are found makes canonical transforms an attractive tool for problems such as these.\n\nWave propagation through air, a lens, and between satellite dishes are discussed here. All of the computations can be reduced to 2×2 matrix algebra. This is the spirit of LCT.\n\nAssuming the system looks like as depicted in the figure, the wave travels from plane \"x\", \"y\" to the plane of \"x\" and \"y\".\nThe Fresnel transform is used to describe electromagnetic wave propagation in air:\n\nwith\n\nThis is equivalent to LCT (shearing), when\n\nWhen the travel distance (\"z\") is larger, the shearing effect is larger.\n\nWith the lens as depicted in the figure, and the refractive index denoted as \"n\", the result is:\n\nwith \"f\" the focal length and \"Δ\" the thickness of the lens.\n\nThe distortion passing through the lens is similar to LCT, when\n\nThis is also a shearing effect: when the focal length is smaller, the shearing effect is larger.\n\nThe spherical mirror—e.g., a satellite dish—can be described as a LCT, with\n\nThis is very similar to lens, except focal length is replaced by the radius of the dish. Therefore, if the radius is smaller, the shearing effect is larger.\n\nThe relation between the input and output we can use LCT to represent\n\nformula_23\n\n(1) If z1 = z2 = 2f, it is reverse real image\n\n(2) If z1 = z2 = f, it is fourier transform+scaling\n\n(3) if z1=z2, it is fractional fourier transform+scaling\n\nIn this part, we show the basic properties of LCT\n\nWith the two-dimension column vector r defined as r =formula_24, we show some basic properties (result) for the specific input below\nThe system considered is depicted in the figure to the right: two dishes – one being the emitter and the other one the receiver – and a signal travelling between them over a distance \"D\".\nFirst, for dish A (emitter), the LCT matrix looks like this:\n\nThen, for dish B (receiver), the LCT matrix similarly becomes:\n\nLast, for the propagation of the signal in air, the LCT matrix is:\n\nPutting all three components together, the LCT of the system is:\n\nIt was shown that a relation can be established between properties of the elementary Fermion in the Standard Model of Particle physics and Spin representation of some particular linear canonical transformations. In this approach, the Electric charge, Weak hypercharge and Weak isospin of the particles are expressed as linear combinations of some operators defined from the generators of the Clifford algebra corresponding to the spinorial representation of linear canonical transformations.\n\n\n\n\n\n"}
{"id": "193837", "url": "https://en.wikipedia.org/wiki?curid=193837", "title": "List of matrices", "text": "List of matrices\n\nThis page lists some important classes of matrices used in mathematics, science and engineering. A matrix (plural matrices, or less commonly matrixes) is a rectangular array of numbers called \"entries\". Matrices have a long history of both study and application, leading to diverse ways of classifying matrices. A first group is matrices satisfying concrete conditions of the entries, including constant matrices. An important example is the identity matrix given by\n\nFurther ways of classifying matrices are according to their eigenvalues or by imposing conditions on the product of the matrix with other matrices. Finally, many domains, both in mathematics and other sciences including physics and chemistry have particular matrices that are applied chiefly in these areas.\nThe following lists matrices whose entries are subject to certain conditions. Many of them apply to \"square matrices\" only, that is matrices with the same number of columns and rows. The main diagonal of a square matrix is the diagonal joining the upper left corner and the lower right one or equivalently the entries \"a\". The other diagonal is called anti-diagonal (or counter-diagonal).\n\nThe list below comprises matrices whose elements are constant for any given dimension (size) of matrix. The matrix entries will be denoted \"a\". The table below uses the Kronecker delta δ for two integers \"i\" and \"j\" which is 1 if \"i\" = \"j\" and 0 else.\n\nA number of matrix-related notions is about properties of products or inverses of the given matrix. The matrix product of a \"m\"-by-\"n\" matrix \"A\" and a \"n\"-by-\"k\" matrix \"B\" is the \"m\"-by-\"k\" matrix \"C\" given by\nThis matrix product is denoted \"AB\". Unlike the product of numbers, matrix products are not commutative, that is to say \"AB\" need not be equal to \"BA\". A number of notions are concerned with the failure of this commutativity. An inverse of square matrix \"A\" is a matrix \"B\" (necessarily of the same dimension as \"A\") such that \"AB\" = \"I\". Equivalently, \"BA\" = \"I\". An inverse need not exist. If it exists, \"B\" is uniquely determined, and is also called \"the\" inverse of \"A\", denoted \"A\".\n\nThe following matrices find their main application in statistics and probability theory.\n\nThe following matrices find their main application in graph and network theory.\n\n\n"}
{"id": "363360", "url": "https://en.wikipedia.org/wiki?curid=363360", "title": "Lyapunov stability", "text": "Lyapunov stability\n\nVarious types of stability may be discussed for the solutions of differential equations or difference equations describing dynamical systems. The most important type is that concerning the stability of solutions near to a point of equilibrium. This may be discussed by the theory of Lyapunov. In simple terms, if the solutions that start out near an equilibrium point formula_1 stay near formula_1 forever, then formula_1 is Lyapunov stable. More strongly, if formula_1 is Lyapunov stable and all solutions that start out near formula_1 converge to formula_1, then formula_1 is asymptotically stable. The notion of exponential stability guarantees a minimal rate of decay, i.e., an estimate of how quickly the solutions converge. The idea of Lyapunov stability can be extended to infinite-dimensional manifolds, where it is known as structural stability, which concerns the behavior of different but \"nearby\" solutions to differential equations. Input-to-state stability (ISS) applies Lyapunov notions to systems with inputs.\n\nLyapunov stability is named after Aleksandr Mikhailovich Lyapunov, a Russian mathematician who defended the thesis \"The General Problem of Stability of Motion\" at Moscow University in 1892. A. M. Lyapunov was a pioneer in successful endeavoring to develop the global approach to the analysis of the stability of nonlinear dynamical systems by comparison with the widely spread local method of linearizing them about points of equilibrium. His work, initially published in Russian and then translated to French, received little attention for many years. The mathematical theory of stability of motion, founded by A. M. Lyapunov, considerably anticipated the time for its implementation in science and technology. Moreover Lyapunov did not himself make application in this field, his own interest being in the stability of rotating fluid masses with astronomical application. He did not have doctoral students who followed the research in the field of stability and his own destiny was terribly tragic because of the Russian revolution of 1917. For several decades the theory of stability sank into complete oblivion. The Russian-Soviet mathematician and mechanician Nikolay Gur'yevich Chetaev working at the Kazan Aviation Institute in the 1930s was the first who realized the incredible magnitude of the discovery made by A. M. Lyapunov. Actually, his figure as a great scientist is comparable to the one of A. M. Lyapunov. The contribution to the theory made by N. G. Chetaev was so significant that many mathematicians, physicists and engineers consider him Lyapunov’s direct successor and the next-in-line scientific descendant in the creation and development of the mathematical theory of stability. \n\nThe interest in it suddenly skyrocketed during the Cold War period when the so-called \"Second Method of Lyapunov\" (see below) was found to be applicable to the stability of aerospace guidance systems which typically contain strong nonlinearities not treatable by other methods. A large number of publications appeared then and since in the control and systems literature.\nMore recently the concept of the Lyapunov exponent (related to Lyapunov's First Method of discussing stability) has received wide interest in connection with chaos theory. Lyapunov stability methods have also been applied to finding equilibrium solutions in traffic assignment problems.\n\nConsider an autonomous nonlinear dynamical system\n\nwhere formula_9 denotes the system state vector, formula_10 an open set containing the origin, and formula_11 continuous on formula_10. Suppose formula_13 has an equilibrium at formula_1 so that formula_15 then\n\n\nConceptually, the meanings of the above terms are the following:\n\nThe trajectory \"x\" is (locally) \"attractive\" if\n\n(where formula_33 denotes the system output) for formula_34 for all trajectories that start close enough, and \"globally attractive\" if this property holds for all trajectories.\n\nThat is, if \"x\" belongs to the interior of its stable manifold, it is \"asymptotically stable\" if it is both attractive and stable. (There are examples showing that attractivity does not imply asymptotic stability. Such examples are easy to create using homoclinic connections.)\n\nIf the Jacobian of the dynamical system at an equilibrium happens to be a stability matrix (i.e., if the real part of each eigenvalue is strictly negative), then the equilibrium is asymptotically stable.\n\nLyapunov, in his original 1892 work, proposed two methods for demonstrating stability. The first method developed the solution in a series which was then proved convergent within limits. The second method, which is now referred to as the Lyapunov stability criterion, makes use of a \"Lyapunov function V(x)\" which has an analogy to the potential function of classical dynamics. It is introduced as follows for a system formula_35 having a point of equilibrium at formula_36. Consider a function formula_37 such that\nThen \"V(x)\" is called a Lyapunov function candidate and the system is stable in the sense of Lyapunov (Note that formula_46 is required; otherwise for example formula_47 would \"prove\" that formula_48 is locally stable). An additional condition called \"properness\" or \"radial unboundedness\" is required in order to conclude global stability. Global asymptotic stability (GAS) follows similarly.\n\nIt is easier to visualize this method of analysis by thinking of a physical system (e.g. vibrating spring and mass) and considering the energy of such a system. If the system loses energy over time and the energy is never restored then eventually the system must grind to a stop and reach some final resting state. This final state is called the attractor. However, finding a function that gives the precise energy of a physical system can be difficult, and for abstract mathematical systems, economic systems or biological systems, the concept of energy may not be applicable.\n\nLyapunov's realization was that stability can be proven without requiring knowledge of the true physical energy, provided a Lyapunov function can be found to satisfy the above constraints.\n\nThe definition for discrete-time systems is almost identical to that for continuous-time systems. The definition below provides this, using an alternate language commonly used in more mathematical texts.\n\nLet (\"X\", \"d\") be a metric space and \"f\" : \"X\" → \"X\" a continuous function. A point \"x\" in \"X\" is said to be Lyapunov stable, if,\n\nWe say that \"x\" is asymptotically stable if it belongs to the interior of its stable set, \"i.e.\" if,\n\nA linear state space model\n\nwhere formula_52 is a finite matrix, is asymptotically stable (in fact, exponentially stable) if all real parts of the eigenvalues of formula_52 are negative. This condition is equivalent to the following one:\n\nis negative definite for some positive definite matrix formula_55. (The relevant Lyapunov function is formula_56.)\n\nCorrespondingly, a time-discrete linear state space model\n\nis asymptotically stable (in fact, exponentially stable) if all the eigenvalues of formula_52 have a modulus smaller than one.\n\nThis latter condition has been generalized to switched systems: a linear switched discrete time system (ruled by a set of matrices\nformula_59)\n\nis asymptotically stable (in fact, exponentially stable) if the joint spectral radius of the set formula_59 is smaller than one.\n\nA system with inputs (or controls) has the form\n\nwhere the (generally time-dependent) input u(t) may be viewed as a \"control\", \"external input\",\n\"stimulus\", \"disturbance\", or \"forcing function\". It has been shown that near to a point of equilibrium which is Lyapunov stable the system remains stable under small disturbances. For larger input disturbances the study of such systems is the subject of control theory and applied in control engineering. For systems with inputs, one must quantify the effect of inputs on the stability of the system. The main two approaches to this analysis are BIBO stability (for linear systems) and input-to-state stability (ISS) (for nonlinear systems)\n\nConsider an equation, where compared to the Van der Pol oscillator equation the friction term is changed:\n\nThe equilibrium is at: formula_64\n\nHere is a good example of an unsuccessful try to find a Lyapunov function that proves stability:\n\nLet\n\nso that the corresponding system is\n\nLet us choose as a Lyapunov function\n\nwhich is clearly positive definite. Its derivative is\n\nIt seems that if the parameter formula_69 is positive, stability is asymptotic for formula_70 But this is wrong, since formula_71 does not depend on formula_72, and will be 0 everywhere on the formula_72 axis. The system is Lyapunov stable.\n\nAssume that f is function of time only.\n\n\nBarbalat's Lemma says:\n\nThe following example is taken from page 125 of Slotine and Li's book \"Applied Nonlinear Control\".\n\nConsider a non-autonomous system\n\nThis is non-autonomous because the input formula_94 is a function of time. Assume that the input formula_95 is bounded.\n\nTaking formula_96 gives formula_97\n\nThis says that formula_98 by first two conditions and hence formula_99 and formula_100 are bounded. But it does not say anything about the convergence of formula_99 to zero. Moreover, the invariant set theorem cannot be applied, because the dynamics is non-autonomous.\n\nUsing Barbalat's lemma:\n\nThis is bounded because formula_99, formula_100 and formula_94 are bounded. This implies formula_106 as formula_76 and hence formula_108. This proves that the error converges.\n\n\n"}
{"id": "58843745", "url": "https://en.wikipedia.org/wiki?curid=58843745", "title": "Michael P. Brenner", "text": "Michael P. Brenner\n\nMichael P. Brenner is an American applied mathematician.\n\nBrenner earned a bachelor's of science degree at the University of Pennsylvania and obtained a doctorate at the University of Chicago. Within the John A. Paulson School of Engineering and Applied Sciences at Harvard University, Brenner is the Michael F. Cronin Professor of Applied Mathematics and Applied Physics, while at the Department of Physics, he holds the Glover Professorship.\n"}
{"id": "28297874", "url": "https://en.wikipedia.org/wiki?curid=28297874", "title": "Named graph", "text": "Named graph\n\nNamed graphs are a key concept of Semantic Web architecture in which a set of Resource Description Framework statements (a graph) are identified using a URI, allowing descriptions to be made of that set of statements such as context, provenance information or other such metadata.\n\nNamed graphs are a simple extension of the RDF data model through which graphs can be created but the model lacks an effective means of distinguishing between them once published on the Web at large.\n\nOne conceptualization of the Web is as a graph of document nodes identified with URIs and connected by hyperlink arcs which are expressed within the HTML documents. By doing a HTTP GET on a URI (usually via a Web browser), a somehow-related document may be retrieved. This \"follow your nose\" approach also applies to RDF documents on the Web in the form of Linked Data, where typically an RDF syntax is used to express data as a series of statements, and URIs within the RDF point to other resources. This Web of data has been described by Tim Berners-Lee as the \"Giant Global Graph\".\n\nNamed graphs are a formalization of the intuitive idea that the contents of an RDF document (a graph) on the Web can be considered to be named by the URI of the document. This considerably simplifies techniques for managing chains of provenance for pieces of data and enabling fine-grained access control to the source data. Additionally trust can be managed through the publisher applying a digital signature to the data in the named graph. (Support for these facilities was originally intended to come from RDF reification, however that approach proved problematic.)\n\nWhile named graphs may appear on the Web as simple linked documents (i.e. Linked Data), they are also very useful for managing sets of RDF data within an RDF store. In particular, the scope of a SPARQL query may be limited to a specific set of named graphs.\n\nAssume the following (Turtle) RDF document has been placed in a SPARQL-capable store with the name .\n\n\"This data has been written in a more verbose form than necessary to show the triple structures\"\n\nThe homepage of the person with the email address can be obtained using the SPARQL query:\n\nThe FROM NAMED here identifies the target graph for the query.\n\nPrior to the publication of the papers describing named graphs, there was considerable discussion about fulfilling their role within a store by using an arity above that of RDF triple statements: where \"triples\" have the form \"<subject> <predicate> <object>\", \"quads\" would have a form along the lines of \"<subject> <predicate> <object> <context>\". Named graphs can be represented this way, as \"<subject> <predicate> <object> <graphname>\", with the advantage that the \"<graphname>\" part will be a URI, giving the quad Web-global scope compared to arbitrary local statement names. This way of representing quads resp. quad-statements was incorporated in the specification of N-Quads.\n\nA paper from the WWW 2005 conference by Carroll et al. includes a formal definition of named graphs.\n\nThere is currently no specification for named graphs in themselves beyond that described in Carroll et al. (2005) and Carroll and Stickler (2004) (which includes syntaxes for representing named graphs), but they do form part of the SPARQL Protocol and RDF Query Language specification.\n\n"}
{"id": "7455080", "url": "https://en.wikipedia.org/wiki?curid=7455080", "title": "Normal convergence", "text": "Normal convergence\n\nIn mathematics normal convergence is a type of convergence for series of functions. Like absolute-convergence, it has the useful property that it is preserved when the order of summation is changed.\n\nThe concept of normal convergence was first introduced by René Baire in 1908 in his book \"Leçons sur les théories générales de l'analyse\".\n\nGiven a set \"S\" and functions formula_1 (or to any normed vector space), the series\n\nis called normally convergent if the series of uniform norms of the terms of the series converges, i.e.,\n\nNormal convergence implies, but should not be confused with, uniform absolute convergence, i.e. uniform convergence of the series of nonnegative functions formula_4. To illustrate this, consider\n\nThen the series formula_4 is uniformly convergent (for any \"ε\" take \"n\" ≥ 1/\"ε\"), but the series of uniform norms is the harmonic series and thus diverges. An example using continuous functions can be made by replacing these functions with bump functions of height 1/\"n\" and width 1 centered at each natural number \"n\".\n\nAs well, normal convergence of a series is different from \"norm-topology convergence\", i.e. convergence of the partial sum sequence in the topology induced by the uniform norm. Normal convergence implies norm-topology convergence if and only if the space of functions under consideration is complete with respect to the uniform norm. (The converse does not hold even for complete function spaces: for example, consider the harmonic series as a sequence of constant functions).\n\nA series can be called \"locally normally convergent on \"X\"\" if each point \"x\" in \"X\" has a neighborhood \"U\" such that the series of functions \"ƒ\" restricted to the domain \"U\"\nis normally convergent, i.e. such that\nwhere the norm formula_9 is the supremum over the domain \"U\".\n\nA series is said to be \"normally convergent on compact subsets of \"X\"\" or \"compactly normally convergent on \"X\"\" if for every compact subset \"K\" of \"X\", the series of functions \"ƒ\" restricted to \"K\"\nis normally convergent on \"K\".\n\nNote: if \"X\" is locally compact (even in the weakest sense), local normal convergence and compact normal convergence are equivalent.\n\n\n"}
{"id": "691615", "url": "https://en.wikipedia.org/wiki?curid=691615", "title": "Perfect rationality", "text": "Perfect rationality\n\nIn economics and game theory, it is sometimes assumed that agents have perfect rationality: that is, they always act in a way that maximizes their utility, and are capable of arbitrarily complex deductions towards that end. They will always be capable of thinking through all possible outcomes and choosing that course of action which will result in the best possible outcome.\n\nPerfect rationality is often contrasted to bounded rationality, which assumes that practical elements such as cognitive and time limitations restrict the rationality of agents.\n\nPerfect rationality is similar to the concept of \"homo economicus\" but is more commonly used in the field of game theory. However it focuses more on the ability of agents to always make mathematically perfect deductions than on agents being self-interested: since the utility function could theoretically take any form, even be linked to the utility of other agents such as one's wife or children, perfect rationality could even be made compatible with the concept of \"homo reciprocans\".\n\n"}
{"id": "1650168", "url": "https://en.wikipedia.org/wiki?curid=1650168", "title": "Prüfer sequence", "text": "Prüfer sequence\n\nIn combinatorial mathematics, the Prüfer sequence (also Prüfer code or Prüfer numbers) of a labeled tree is a unique sequence associated with the tree. The sequence for a tree on \"n\" vertices has length \"n\" − 2, and can be generated by a simple iterative algorithm. Prüfer sequences were first used by Heinz Prüfer to prove Cayley's formula in 1918.\n\nOne can generate a labeled tree's Prüfer sequence by iteratively removing vertices from the tree until only two vertices remain. Specifically, consider a labeled tree \"T\" with vertices {1, 2, ..., \"n\"}. At step \"i\", remove the leaf with the smallest label and set the \"i\"th element of the Prüfer sequence to be the label of this leaf's neighbour.\n\nThe Prüfer sequence of a labeled tree is unique and has length \"n\" − 2.\n\nConsider the above algorithm run on the tree shown to the right. Initially, vertex 1 is the leaf with the smallest label, so it is removed first and 4 is put in the Prüfer sequence. Vertices 2 and 3 are removed next, so 4 is added twice more. Vertex 4 is now a leaf and has the smallest label, so it is removed and we append 5 to the sequence. We are left with only two vertices, so we stop. The tree's sequence is {4,4,4,5}.\n\nLet codice_1 be a Prüfer sequence:\n\nThe tree will have codice_2 nodes, numbered from codice_3 to codice_2.\nFor each node set its degree to the number of times it appears in the sequence plus 1.\nFor instance, in pseudo-code:\n\nNext, for each number in the sequence codice_5, find the first (lowest-numbered) node, codice_6, with degree equal to 1, add the edge codice_7 to the tree, and decrement the degrees of codice_6 and codice_5. In pseudo-code:\n\nAt the end of this loop two nodes with degree 1 will remain (call them codice_10, codice_11). Lastly, add the edge codice_12 to the tree.\n\nThe Prüfer sequence of a labeled tree on \"n\" vertices is a unique sequence of length \"n\" − 2 on the labels 1 to \"n\" — this much is clear. Somewhat less obvious is the fact that for a given sequence \"S\" of length \"n\"–2 on the labels 1 to \"n\", there is a \"unique\" labeled tree whose Prüfer sequence is \"S\".\n\nThe immediate consequence is that Prüfer sequences provide a bijection between the set of labeled trees on \"n\" vertices and the set of sequences of length \"n\"–2 on the labels 1 to \"n\". The latter set has size \"n\", so the existence of this bijection proves Cayley's formula, i.e. that there are \n\"n\" labeled trees on \"n\" vertices.\n\n\n\n"}
{"id": "2604429", "url": "https://en.wikipedia.org/wiki?curid=2604429", "title": "Puiseux series", "text": "Puiseux series\n\nIn mathematics, Puiseux series are a generalization of power series that allow for negative and fractional exponents of the indeterminate \"T\". They were first introduced by Isaac Newton in 1676 and rediscovered by Victor Puiseux in 1850. For example, the series\nis a Puiseux series in \"T\".\n\nPuiseux's theorem, sometimes also called the Newton–Puiseux theorem, asserts that, given a polynomial equation formula_2, its solutions in , viewed as functions of , may be expanded as Puiseux series that are convergent in some neighbourhood of the origin (0 excluded, in the case of a solution that tends to infinity at the origin). In other words, every branch of an algebraic curve may be locally (in terms of ) described by a Puiseux series.\n\nThe set of Puiseux series over an algebraically closed field of characteristic 0 is itself an algebraically closed field, called the field of Puiseux series. It is the algebraic closure of the field of Laurent series. This statement is also referred to as Puiseux's theorem, being an expression of the original Puiseux theorem in modern abstract language. Puiseux series are generalized by Hahn series.\n\nIf \"K\" is a field (such as the complex numbers) then we can define the field of Puiseux series with coefficients in \"K\" informally as the set of expressions of the form\nwhere formula_4 is a positive integer and formula_5 is an arbitrary integer. In other words, Puiseux series differ from Laurent series in that they allow for fractional exponents of the indeterminate, as long as these fractional exponents have bounded denominator (here \"n\"). Just as with Laurent series, Puiseux series allow for negative exponents of the indeterminate as long as these negative exponents are bounded below (here by formula_5). Addition and multiplication are as expected: for example, \nand\nOne might define them by first \"upgrading\" the denominator of the exponents to some common denominator and then performing the operation in the corresponding field of formal Laurent series of formula_9.\n\nIn other words, the field of Puiseux series with coefficients in \"K\" is the union of the fields formula_10 (where \"n\" ranges over the positive integers), where each element of the union is a field of formal Laurent series over formula_11 (considered as an indeterminate), and where each such field is considered as a subfield of the ones with larger \"n\" by rewriting the fractional exponents to use a larger denominator (so, for example, formula_12 is identified with formula_13 ).\n\nThis yields a formal definition of the field of Puiseux series: it is the direct limit of the direct system, indexed over the non-zero natural numbers \"n\" ordered by divisibility, whose objects are all formula_14 (the field of formal Laurent series, which we rewrite as\nformula_15 for clarity),\nwith a morphism\nformula_16\nbeing given, whenever \"m\" divides \"n\", by formula_17.\n\nThe Puiseux series over a field \"K\" form a valued field with value group formula_18 (the rationals): the \"valuation\" formula_19 of a series\n\nas above is defined to be the smallest rational formula_21 such that the coefficient formula_22 of the term with exponent formula_21 is non-zero (with the usual convention that the valuation of 0 is +∞). The coefficient formula_22 in question is typically called the \"valuation coefficient\" of \"f\".\n\nThis valuation in turn defines a (translation-invariant) distance (which is ultrametric), hence a topology on the field of Puiseux series by letting the distance from \"f\" to 0 be formula_25. This justifies \"a posteriori\" the notation\n\nas the series in question does, indeed, converge to \"f\" in the Puiseux series field (this is in contrast to Hahn series which \"cannot\" be viewed as converging series).\n\nIf the base field \"K\" is ordered, then the field of Puiseux series over \"K\" is also naturally (“lexicographically”) ordered as follows: a non-zero Puiseux series \"f\" with 0 is declared positive whenever its valuation coefficient is so. Essentially, this means that any positive rational power of the indeterminate \"T\" is made positive, but smaller than any positive element in the base field \"K\".\n\nIf the base field \"K\" is endowed with a valuation \"w\", then we can construct a different valuation on the field of Puiseux series over \"K\" by letting the valuation\nformula_27 \nbe formula_28\nwhere formula_29 is the previously defined valuation (formula_22 is the first non-zero coefficient) and ω is infinitely large (in other words, the value group of formula_31 is formula_32 ordered lexicographically, where Γ is the value group of \"w\"). Essentially, this means that the previously defined valuation \"v\" is corrected by an infinitesimal amount to take into account the valuation \"w\" given on the base field.\n\nOne essential property of Puiseux series is expressed by the following theorem, attributed to Puiseux (for formula_33) but which was implicit in Newton's use of the Newton polygon as early as 1671 and therefore known either as Puiseux's theorem or as the Newton–Puiseux theorem:\n\nTheorem: if \"K\" is an algebraically closed field of characteristic zero, then the field of Puiseux series over \"K\" is the algebraic closure of the field of formal Laurent series over \"K\".\n\nVery roughly, the proof proceeds essentially by inspecting the Newton polygon of the equation and extracting the coefficients one by one using a valuative form of Newton's method. Provided algebraic equations can be solved algorithmically in the base field \"K\", then the coefficients of the Puiseux series solutions can be computed to any given order.\n\nFor example, the equation formula_34 has solutions\n\nand\n\n(one readily checks on the first few terms that the sum and product of these two series are 1 and formula_37 respectively; this is valid whenever the base field \"K\" has characteristic different from 2).\n\nAs the powers of 2 in the denominators of the coefficients of the previous example might lead one to believe, the statement of the theorem is not true in positive characteristic. The example of the Artin–Schreier equation formula_38 shows this: reasoning with valuations shows that \"X\" should have valuation formula_39, and if we rewrite it as formula_40 then\n\nand one shows similarly that formula_42 should have valuation formula_43, and proceeding in that way one obtains the series\n\nsince this series makes no sense as a Puiseux series—because the exponents have unbounded denominators—the original equation has no solution. However, such Eisenstein equations are essentially the only ones not to have a solution, because, if \"K\" is algebraically closed of characteristic \"p\">0, then the field of Puiseux series over \"K\" is the perfect closure of the maximal tamely ramified extension of formula_14.\n\nSimilarly to the case of algebraic closure, there is an analogous theorem for real closure: if \"K\" is a real closed field, then the field of Puiseux series over \"K\" is the real closure of the field of formal Laurent series over \"K\". (This implies the former theorem since any algebraically closed field of characteristic zero is the unique quadratic extension of some real-closed field.)\n\nThere is also an analogous result for p-adic closure: if \"K\" is a \"p\"-adically closed field with respect to a valuation \"w\", then the field of Puiseux series over \"K\" is also \"p\"-adically closed.\n\nLet \"X\" be an algebraic curve given by an affine equation formula_46 over an algebraically closed field \"K\" of characteristic zero, and consider a point \"p\" on \"X\" which we can assume to be (0,0). We also assume that \"X\" is not the coordinate axis \"x\"=0. Then a \"Puiseux expansion\" of (the \"y\" coordinate of) \"X\" at \"p\" is a Puiseux series \"f\" having positive valuation such that formula_47.\n\nMore precisely, let us define the \"branches\" of \"X\" at \"p\" to be the points \"q\" of the normalization \"Y\" of \"X\" which map to \"p\". For each such \"q\", there is a local coordinate \"t\" of \"Y\" at \"q\" (which is a smooth point) such that the coordinates \"x\" and \"y\" can be expressed as formal power series of \"t\", say formula_48 (since \"K\" is algebraically closed, we can assume the valuation coefficient to be 1) and formula_49: then there is a unique Puiseux series of the form formula_50 (a power series in formula_11), such that formula_52 (the latter expression is meaningful since formula_53 is a well-defined power series in \"t\"). This is a Puiseux expansion of \"X\" at \"p\" which is said to be associated to the branch given by \"q\" (or simply, the Puiseux expansion of that branch of \"X\"), and each Puiseux expansion of \"X\" at \"p\" is given in this manner for a unique branch of \"X\" at \"p\".\n\nThis existence of a formal parametrization of the branches of an algebraic curve or function is also referred to as \"Puiseux's theorem\": it has arguably the same mathematical content as the fact that the field of Puiseux series is algebraically closed and is a historically more accurate description of the original author's statement.\n\nFor example, the curve formula_54 (whose normalization is a line with coordinate \"t\" and map formula_55) has two branches at the double point (0,0), corresponding to the points \"t\" = +1 and \"t\" = −1 on the normalization, whose Puiseux expansions are formula_56 and formula_57 respectively (here, both are power series because the \"x\" coordinate is étale at the corresponding points in the normalization). At the smooth point (−1,0) (which is \"t\" = 0 in the normalization), it has a single branch, given by the Puiseux expansion formula_58 (the \"x\" coordinate ramifies at this point, so it is not a power series).\n\nThe curve formula_59 (whose normalization is again a line with coordinate \"t\" and map formula_60), on the other hand, has a single branch at the cusp point (0,0), whose Puiseux expansion is formula_61.\n\nWhen formula_62 is the field of complex numbers, the Puiseux expansion of an algebraic curve (as defined above) is convergent in the sense that for a given choice of \"n\"-th root of \"x\", they converge for small enough formula_63, hence define an analytic parametrization of each branch of \"X\" in the neighborhood of \"p\" (more precisely, the parametrization is by the \"n\"-th root of \"x\").\n\nThe field of Puiseux series is not complete as a metric space. Its completion, called the Levi-Civita field, can be described as follows: it is the field of formal expressions of the form formula_64, where the support of the coefficients (that is, the set of \"e\" such that formula_65) is the range of an increasing sequence of rational numbers that either is finite or tends to +∞. In other words, such series admit exponents of unbounded denominators, provided there are finitely many terms of exponent less than \"A\" for any given bound \"A\". For example, formula_66 is not a Puiseux series, but it is the limit of a Cauchy sequence of Puiseux series; in particular, it is the limit of formula_67 as formula_68. However, even this completion is still not \"maximally complete\" in the sense that it admits non-trivial extensions which are valued fields having the same value group and residue field, hence the opportunity of completing it even more.\n\nHahn series are a further (larger) generalization of Puiseux series, introduced by Hans Hahn in the course of the proof of his embedding theorem in 1907 and then studied by him in his approach to Hilbert's seventeenth problem. In a Hahn series, instead of requiring the exponents to have bounded denominator they are required to form a well-ordered subset of the value group (usually formula_18 or formula_70). These were later further generalized by Anatoly Maltsev and Bernhard Neumann to a non-commutative setting (they are therefore sometimes known as \"Hahn–Mal'cev–Neumann series\"). Using Hahn series, it is possible to give a description of the algebraic closure of the field of power series in positive characteristic which is somewhat analogous to the field of Puiseux series.\n\n\n\n"}
{"id": "46183215", "url": "https://en.wikipedia.org/wiki?curid=46183215", "title": "Queue number", "text": "Queue number\n\nIn mathematics, the queue number of a graph is a graph invariant defined analogously to stack number (book thickness) using first-in first-out (queue) orderings in place of last-in first-out (stack) orderings.\n\nA queue layout of a given graph is defined by a total ordering of the vertices of the graph together with a partition of the edges into a number of \"queues\". The set of edges in each queue is required to avoid edges that are properly nested: if \"ab\" and \"cd\" are two edges in the same queue, then it should not be possible to have in the vertex ordering. The queue number qn(\"G\") of a graph \"G\" is the minimum number of queues in a queue layout.\n\nEquivalently, from a queue layout, one could process the edges in a single queue using a queue data structure, by considering the vertices in their given ordering, and when reaching a vertex, dequeueing all edges for which it is the second endpoint followed by enqueueing all edges for which it is the first endpoint. The nesting condition ensures that, when a vertex is reached, all of the edges for which it is the second endpoint are ready to be dequeued. Another equivalent definition of queue layouts involves embeddings of the given graph onto a cylinder, with the vertices placed on a line in the cylinder and with each edge wrapping once around the cylinder. Edges that are assigned to the same queue are not allowed to cross each other, but crossings are allowed between edges that belong to different queues.\n\nQueue layouts were defined by , by analogy to previous work on book embeddings of graphs, which can be defined in the same way using stacks in place of queues. As they observed, these layouts are also related to earlier work on sorting permutations using systems of parallel queues, and may be motivated by applications in VLSI design and in communications management for distributed algorithms.\n\nEvery tree has queue number 1, with a vertex ordering given by a breadth-first traversal. Pseudoforests and grid graphs also have queue number 1. Outerplanar graphs have queue number at most 2; the 3-sun graph (a triangle with each of its edges replaced by a triangle) is an example of an outerplanar graph whose queue number is exactly 2. Series-parallel graphs have queue number at most 3.\n\nBinary de Bruijn graphs have queue number 2. The \"d\"-dimensional hypercube graph has queue number at most . The queue numbers of complete graphs \"K\" and complete bipartite graphs \"K\" are known exactly: they are formula_1 and formula_2\nrespectively.\nEvery 1-queue graph is a planar graph, with an \"arched leveled\" planar embedding in which the vertices are placed on parallel lines (levels) and each edge either connects vertices on two consecutive levels or forms an arch that connects two vertices on the same level by looping around all previous levels. Conversely, every arched leveled planar graph has a 1-queue layout. conjectured that every planar graph has bounded queue number, but this remains unsolved. If the queue number of planar graphs is bounded, then the same is true for 1-planar graphs and more generally \"k\"-planar graphs. The strongest known bound on the queue number of planar graphs is not constant, but . Polylogarithmic bounds on the queue number are also known for graphs of bounded genus and more generally graphs in any minor-closed graph family.\n\nUsing a variation of queue number called the strong queue number, the queue number of a graph product can be bounded by a function of the queue numbers and strong queue numbers of the factors in the product.\n\nGraphs with low queue number are sparse graphs: 1-queue graphs with \"n\" vertices have at most 2\"n\" − 3 edges, and more generally graphs with queue number \"q\" have at most edges. This implies that these graphs also have small chromatic number: in particular 1-queue graphs are 3-colorable, and graphs with queue number \"q\" may need at least and at most 4\"q\" colors. In the other direction, a bound on the number of edges implies a much weaker bound on the queue number: graphs with \"n\" vertices and \"m\" edges have queue number at most \"O\"(). This bound is close to tight, because for random \"d\"-regular graphs the queue number is, with high probability,\nGraphs with queue number 1 have book thickness at most 2.\nFor any fixed vertex ordering, the product of the book thickness and queue numbers for that ordering is at least as large as the cutwidth of the graph divided by its maximum degree.\nThe book thickness may be much larger than the queue number: ternary Hamming graphs have logarithmic queue number but polynomially-large book thickness. It remains unknown whether the book thickness can be bounded by any function of the queue number. conjectured that the queue number is at most a linear function of the book thickness, but no functional bound in this direction is known either. It is known that, if all bipartite graphs with 3-page book embeddings have bounded queue number, then all graphs with bounded book thickness have bounded queue number.\n\nIt is NP-complete to determine the queue number of a given graph, or even to test whether this number is one.\n\nHowever, if the vertex ordering of a queue layout is given as part of the input, \nthen the optimal number of queues for the layout equals the maximum number of edges in a \"k\"-rainbow, a set of \"k\" edges each two of which form a nested pair. A partition of edges into queues can be performed by assigning an edge \"e\" that is the outer edge of an \"i\"-rainbow (and of no larger rainbow) to the \"i\"th queue. It is possible to construct an optimal layout in time , where \"n\" denotes the number of vertices of the input graph and \"m\" denotes the number of edges.\n\nGraphs of bounded queue number also have bounded expansion, meaning that their shallow minors are sparse graphs with a ratio of edges to vertices (or equivalently degeneracy or arboricity) that is bounded by a function of the queue number and the depth of the minor. As a consequence, several algorithmic problems including subgraph isomorphism for pattern graphs of bounded size have linear time algorithms for these graphs. More generally, because of their bounded expansion, it is possible to check whether any sentence in the first-order logic of graphs is valid for a given graph of bounded queue number, in linear time.\n\nAlthough queue layouts do not necessarily produce good two-dimensional graph drawings, they have been used for three-dimensional graph drawing. In particular, a graph \"G\" has bounded queue number if and only if it is possible to place the vertices of \"G\" in a three-dimensional grid of dimensions in such a way that no two edges cross each other. Thus, for instance, de Bruijn graphs and graphs of bounded treewidth have three-dimensional embeddings of linear volume.\n\nLogarithmic or polylogarithmic bounds on the queue number translate in the same way into 3d embeddings with near-linear volume, in a grid with one dimension linear and the other two polylogarithmic. Planar graphs, bounded genus graphs, and graphs of bounded local treewidth have embeddings of volume while graphs in minor-closed families have embeddings of volume \n\n\n"}
{"id": "23801765", "url": "https://en.wikipedia.org/wiki?curid=23801765", "title": "Rejecta Mathematica", "text": "Rejecta Mathematica\n\nRejecta Mathematica was an online journal for publishing papers that had been rejected by other mathematics journals. Each paper was accompanied by an open letter describing why the paper was rejected, how the topic has been developed since and why it is worthy of publication. The first issue was published in July 2009 containing topics such as image enhancement and condition numbers. The quality of the contributions in the first issue was seen as mixed. The editors were Michael Wakin, Christopher Rozell, Mark Davenport and Jason Laska.\n\nAfter almost two years since the inaugural issue, the second issue was published in June 2011 and contains topics such as subspace classification and distributions of pseudoprimes.\n\n, the original website is no longer online, but an archival copy is hosted on GitHub.\n\n\n"}
{"id": "48477231", "url": "https://en.wikipedia.org/wiki?curid=48477231", "title": "Robert V. Kohn", "text": "Robert V. Kohn\n\nRobert V. Kohn (born in 1953) is an American mathematician working on partial differential equations, calculus of variations, mathematical materials science, and mathematical finance. He is a professor at the Courant Institute of Mathematical Sciences, New York University.\n\nKohn studied mathematics at Harvard University, obtaining his bachelor's degree in 1974. He obtained his Ph.D. at Princeton University in 1979, as a student of Frederick Almgren.\n\nKohn is best known for his works on non-linear partial differential equations, specially the ones with Louis Nirenberg and Luis Caffarelli, where they obtained partial results about the regularity of weak solutions of the Navier–Stokes equations.\n\nHe received a Sloan Research Fellowship in 1984. In 2006, he was a plenary speaker at the International Congress of Mathematicians, in Madrid (\"Energy driven pattern formation\"). He is a fellow of the American Mathematical Society.\n\n\n"}
{"id": "58444022", "url": "https://en.wikipedia.org/wiki?curid=58444022", "title": "Rudy Horne", "text": "Rudy Horne\n\nRudy Lee Horne (1968 – 2017) was an African-American mathematician and professor of mathematics at Morehouse College. He worked on dynamical systems, including nonlinear waves. He was the mathematics consultant for the film \"Hidden Figures\".\n\nHorne grew up in the south side of Chicago. His father worked at Sherwin-Williams. He graduated from Crete-Monee High School. He completed a double degree in mathematics and physics at the University of Oklahoma in 1991. He joined the University of Colorado Boulder for his postgraduate studies, earning a master's in physics in 1994 and in mathematics in 1996. He completed his doctorate, \"Collision induced timing jitter and four-wave mixing in wavelength division multiplexing soliton systems\", in 2001 which was supervised by Mark J. Ablowitz. He was the first African American to graduate from the University of Colorado Boulder Department of Applied Mathematics.\n\nAfter completing his PhD, Horne had a position at the California State University, East Bay. before working as postdoctoral researcher at the University of North Carolina at Chapel Hill, with Chris Jones. Horne joined Florida State University in 2005. Horne joined Morehouse College in 2010 and was promoted to associate professor of mathematics in 2015. He continued to study four-wave mixing. His work considered nonlinear optical phenomena. He uncovered effects in parity-time symmetric systems.\n\nHorne was recommended to serve as a mathematics consultant for Hidden Figures by Morehouse College. He worked closely with Theodore Melfi ensured the actors knew how to pronounce \"Euler's\". He spent four months working with 20th Century Fox. In particular, Horne worked with Taraji P. Henson on the mathematics she required for her role as Katherine Johnson. He taught the cast how to get excited by mathematics. His handwriting is on screen during a scene at the beginning of the film where Katherine Johnson solves a quadratic equation. He appeared on the interview series \"In the Know.\" Horne completed a Mathematical Association of America \"Maths Fest\" tour where he discussed the mathematics in Hidden Figures, focussing on the calculations that concerned Glenn's orbit around in 1962. Her appeared on NPR's Closer Look.\n\nHe died on December 11, 2017. The University of Colorado Boulder established a Rudy Lee Horne Memorial Fellowship in his honour. He was described as a \"rock star\", inspiring generations of black students. He was awarded the National Association of Mathematicians (NAM) lifetime achievement award posthumously in 2018.\n"}
{"id": "15828771", "url": "https://en.wikipedia.org/wiki?curid=15828771", "title": "Stable theory", "text": "Stable theory\n\nIn model theory, a complete theory is called stable if it does not have too many types. One goal of classification theory is to divide all complete theories into those whose models can be classified and those whose models are too complicated to classify, and to classify all models in the cases where this can be done. Roughly speaking, if a theory is not stable then its models are too complicated and numerous to classify, while if a theory is stable there might be some hope of classifying its models, especially if the theory is superstable or totally transcendental.\n\nStability theory was started by , who introduced several of the fundamental concepts, such as totally transcendental theories and the Morley rank. \nStable and superstable theories were first introduced by , who is responsible for much of the development of stability theory. The definitive reference for stability theory is , though it is notoriously hard even for experts to read, as mentioned, e.g., in .\n\n\"T\" will be a complete theory in some language. \n\nAs usual, a model of some language is said to have one of these properties if the complete theory of the model has that property.\n\nAn incomplete theory is defined to have one of these properties if every completion, or equivalently every model, has this property.\n\nRoughly speaking, a theory is unstable if one can use it to encode the ordered set of natural numbers. More precisely, if there is a model \"M\" and a formula Φ(\"X\",\"Y\") in 2\"n\" variables \"X\" = \"x\"...,\"x\" and \"Y\" = \"y\"...,\"y\" defining a relation on \"M\" with an infinite totally ordered subset then the theory is unstable. (Any infinite totally ordered set has a subset isomorphic to either the positive or negative integers under the usual order, so one can assume the totally ordered subset is ordered like the positive integers.) The totally ordered subset need not be definable in the theory.\n\nThe number of models of an unstable theory \"T\" of any uncountable cardinality κ ≥ |\"T\"| is the maximum possible number 2.\n\nExamples:\n\n\"T\" is called stable if it is κ-stable for some cardinal κ. \nExamples:\n\n\"T\" is called superstable if it is stable for all sufficiently large cardinals, so all superstable theories are stable. For countable \"T\" superstability is equivalent to stability for all κ≥2.\nThe following conditions on a theory \"T\" are equivalent:\n\nIf a theory is superstable but not totally transcendental it is called strictly superstable.\n\nThe number of countable models of a countable superstable theory must be 1, ℵ, ℵ, or 2. If the number of models is 1 the theory is totally transcendental. There are examples with 1, ℵ or 2 models, and it is not known if there are examples with ℵ models if the continuum hypothesis does not hold. If a theory \"T\" is not superstable then the number of models of cardinality κ>|\"T\"| is 2.\n\nExamples:\n\n\nExamples:\n\n\n\n"}
{"id": "22037813", "url": "https://en.wikipedia.org/wiki?curid=22037813", "title": "Stochastic geometry", "text": "Stochastic geometry\n\nIn mathematics, stochastic geometry is the study of random spatial patterns. At the heart of the subject lies the study of random point patterns. This leads to the theory of spatial point processes, hence notions of Palm conditioning, which extend to the more abstract setting of random measures.\n\nThere are various models for point processes, typically based on but going beyond the classic homogeneous Poisson point process (the basic model for \"complete spatial randomness\") to find expressive models which allow effective statistical methods. \n\nThe point pattern theory provides a major building block for generation of random object processes, allowing construction of elaborate random spatial patterns. The simplest version, the Boolean model, places a random compact object at each point of a Poisson point process. More complex versions allow interactions based in various ways on the geometry of objects. Different directions of application include: the production of models for random images either as set-union of objects, or as patterns of overlapping objects; also the generation of geometrically inspired models for the underlying point process \n(for example, the point pattern distribution may be biased by an exponential factor involving the area of the union of the objects; this is related to the Widom-Rowlinson model of statistical mechanics).\n\nWhat is meant by a random object? A complete answer to this question requires the theory of random closed sets, which makes contact with advanced concepts from measure theory. The key idea is to focus on the probabilities of the given random closed set hitting specified test sets. There arise questions of inference (for example, estimate the set which encloses a given point pattern) and theories of generalizations of means etc. to apply to random sets. Connections are now being made between this latter work and recent developments in geometric mathematical analysis concerning general metric spaces and their geometry. Good parametrizations of specific random sets can allow us to refer random object processes to the theory of marked point processes; object-point pairs are viewed as points in a larger product space formed as the product of the original space and the space of parametrization.\n\nSuppose we are concerned no longer with compact objects, but with objects which are spatially extended: lines on the plane or flats in 3-space. This leads to consideration of line processes, and of processes of flats or hyper-flats. There can no longer be a preferred spatial location for each object; however the theory may be mapped back into point process theory by representing each object by a point in a suitable representation space. For example, in the case of directed lines in the plane one may take the representation space to be a cylinder. A complication is that the Euclidean motion symmetries will then be expressed on the representation space in a somewhat unusual way. Moreover, calculations need to take account of interesting spatial biases (for example, line segments are less likely to be hit by random lines to which they are nearly parallel) and this provides an interesting and significant connection to the hugely significant area of stereology, which in some respects can be viewed as yet another theme of stochastic geometry. It is often the case that calculations are best carried out in terms of bundles of lines hitting various test-sets, rather than by working in representation space.\n\nLine and hyper-flat processes have their own direct applications, but also find application as one way of creating tessellations dividing space; hence for example one may speak of Poisson line tessellations. A notable recent result proves that the cell at the origin of the Poisson line tessellation is approximately circular when conditioned to be large. Tessellations in stochastic geometry can of course be produced by other means, for example by using Voronoi and variant constructions, and also by iterating various means of construction.\n\nThe name appears to have been coined by David Kendall and Klaus Krickeberg while preparing for a June 1969 Oberwolfach workshop, though antecedents for the theory stretch back much further under the name geometric probability. The term \"stochastic geometry\" was also used by Frisch and Hammersley in 1963 as one of two suggestions for names of a theory of \"random irregular structures\" inspired by percolation theory.\n\nThis brief description has focused on the theory of stochastic geometry, which allows a view of the structure of the subject. However, much of the life and interest of the subject, and indeed many of its original ideas, flow from a very wide range of applications, for example: astronomy, spatially distributed telecommunications, wireless network modeling and analysis, modeling of channel fading, forestry, the statistical theory of shape, material science, multivariate analysis, problems in image analysis and stereology. There are links to statistical mechanics, Markov chain Monte Carlo, and implementations of the theory in statistical computing (for example, spatstat in R). Most recently determinantal and permanental point processes (connected to random matrix theory) are beginning to play a role.\n\n"}
{"id": "222434", "url": "https://en.wikipedia.org/wiki?curid=222434", "title": "Table of divisors", "text": "Table of divisors\n\nThe tables below list all of the divisors of the numbers 1 to 1000.\n\nA divisor of an integer \"n\" is an integer \"m\", for which \"n\"/\"m\" is again an integer (which is necessarily also a divisor of \"n\"). For example, 3 is a divisor of 21, since 21/7 = 3 (and 7 is also a divisor of 21).\n\nIf \"m\" is a divisor of \"n\" then so is −\"m\". The tables below only list positive divisors.\n\n"}
{"id": "2164610", "url": "https://en.wikipedia.org/wiki?curid=2164610", "title": "The Bottle Imp", "text": "The Bottle Imp\n\nThe Bottle Imp is an 1891 short story by the Scottish author Robert Louis Stevenson usually found in the short story collection \"Island Nights' Entertainments\". It was first published in the \"New York Herald\" (February–March 1891) and \"Black and White\" London (March–April 1891). In it, the protagonist buys a bottle with an imp inside that grants wishes. However, the bottle is cursed; if the holder dies bearing it, his or her soul is forfeit to hell.\n\nKeawe, a poor Native Hawaiian, buys a strange unbreakable bottle from a sad, elderly gentleman who credits the bottle with his fortune. He promises that an imp residing in the bottle will also grant Keawe his every desire.\n\nOf course, there is a catch. The bottle must be sold, for cash, at a loss, i.e. for less than its owner originally paid, and cannot be thrown or given away, or else it will magically return to him. All of these rules must be explained by each seller to each purchaser. If an owner of the bottle dies without having sold it in the prescribed manner, that person's soul will burn for eternity in Hell.\n\nThe bottle was said to have been brought to Earth by the Devil and first purchased by Prester John for millions; it was owned by Napoleon and Captain James Cook and accounted for their great successes. By the time of the story the price has diminished to fifty dollars.\n\nKeawe buys the bottle and instantly tests it by wishing his money to be refunded, and by trying to sell it for more than he paid and abandoning it, to test if the story is true. When these all work as described, he realizes the bottle does indeed have unholy power. He wishes for his heart's desire: a big, fancy mansion on a landed estate, and finds his wish granted, but at a price: his beloved uncle and cousins have been killed in a boating accident, leaving Keawe sole heir to his uncle's fortune. Keawe is horrified, but uses the money to build his house. Having all he wants, and being happy, he explains the risks to a friend who buys the bottle from him.\n\nKeawe lives a happy life, but there is something missing. Walking along the beach one night, he meets a beautiful woman, Kokua. They soon fall in love and become engaged. Keawe's happiness is shattered on the night of his betrothal, when he discovers that he has contracted the then-incurable disease of leprosy. He must give up his house and wife, and live in Kalaupapa—a remote community for lepers—unless he can recover the bottle and use it to cure himself.\n\nKeawe begins this quest by attempting to track down the friend to whom he sold the bottle, but the friend has become suddenly wealthy and left Hawaii. Keawe traces the path of the bottle through many buyers and eventually finds a Haole of Beritania Street, Honolulu. The man of European ancestry has both good and bad news for Keawe: (a) he owns the bottle and is very willing to sell, but (b) he had only paid two cents for it. Therefore, if Keawe buys it, he will not be able to resell it.\n\nKeawe decides to buy the bottle anyway, for the price of one cent, and indeed cures himself. Now, however, he is understandably despondent: how can he possibly enjoy life, knowing his doom? His wife mistakes his depression for regret at their marriage, and asks for a divorce. Keawe confesses to her his secret.\n\nHis wife suggests they sail, with the bottle, to Tahiti; on that archipelago the colonists of French Polynesia use centimes, a coin worth one fifth of an American cent. This offers a potential recourse for Keawe.\n\nWhen they arrive, however, the suspicious natives will not touch the cursed bottle. Kokua determines to make a supreme sacrifice to save her husband from his fate. Since, however, she knows he would never sell the bottle to her knowingly, Kokua is forced to bribe an old sailor to buy the bottle for four centimes, with the understanding that she will secretly buy it back for three. Now Keawe is happy, but she carries the curse.\n\nKeawe discovers what his wife has done, and resolves to sacrifice himself for her in the same manner. He arranges for a brutish boatswain to buy the bottle for two centimes, promising he will buy it back for one, thus sealing his doom. However, the drunken sailor refuses to part with it, and is unafraid of the prospect of Hell. \"I reckon I'm going anyway,\" he says.\n\nKeawe returns to his wife, both of them free from the curse, and the reader is encouraged to believe that they live happily ever after.\n\nThe theme of the bottle imp can be found in the German legend \"Spiritus familiaris\" by the Brothers Grimm as well. At the time of publication in 1891, the currency system of the Kingdom of Hawaii included cent coins that circulated at par with the U.S. penny.\n\nThe novel reflects Stevenson's impressions gained during his five-month visit of the Kingdom of Hawaii in 1889. Part of the storyline takes place in the little town Hoʻokena at the Kona coast of the island of Hawaii, which the author visited. In a scene which takes place in Honolulu Stevenson mentions Heinrich Berger, the bandmaster of the Royal Hawaiian Band. The name of Keawe's wife refers to the Hawaiian word \"kōkua\", which means \"help\". In 1889 Stevenson also visited the leper colony on the island of Molokaʻi and met Father Damien there. Therefore, he had a first-hand experience from the fate of lepers. Several times Stevenson uses the Hawaiian word \"Haole\", which is the usual term for Caucasians, for example describing the last owner of the bottle.\n\nThe story could be considered as both a continuation of and a rather light-hearted counterpoint to the theme of selling one's soul to The Devil, manifested in the numerous depictions of Doctor Faust as well as in such stories as \"The Devil and Tom Walker\" by Washington Irving and \"The Devil and Daniel Webster\" by Stephen Vincent Benet.\n\n\"The Bottle Imp\" was published in the missionary magazine \"O le sulu Samoa\" (The Samoan Torch) in 1891, with the title \"O Le Tala I Le Fagu Aitu\". According to \"Publishers Weekly\" and \"School Library Journal\" (both quoted by Amazon.com) \"this tale was originally published, in Samoan, in 1891\". \"The Locus Online Index to Science Fiction\" similarly states \"The Stevenson story was first published in Samoan in 1891, appearing later that year in English.\" The Project Gutenberg text of the story has a note by Stevenson which says \"...the tale has been designed and written for a Polynesian audience...\" which also suggests initial publication in Polynesia, not in the United States.\n\nThe premise of the story creates a logical paradox similar to the unexpected hanging paradox. Clearly no rational person would buy it for one cent as this would make it impossible for it to be sold at a loss. However, it follows that no rational person would buy it for two cents either if it is later to be sold only to a rational person for a loss. By backward induction, the bottle cannot be sold for \"any\" price in a perfectly rational world. And yet, the actions of the people in the story do not seem particularly unwise.\n\nThe story shows that the paradox could be resolved by the existence of certain characters:\n\nSince the exchange rates of different currencies can fluctuate with respect to one another, it is also possible that the \"value\" of the bottle could increase from one transaction to the next even if the stated price decreases. This leads to an endless staircase-type paradox which would make it possible, in theory, for the bottle to keep getting sold infinitely many times. However, this might be forbidden depending on how the bottle imp interprets the idea of \"selling at a loss\".\n\nA silent film based on Stevenson's story was released in 1917. The screenplay was adapted by Charles Maigne. The film was directed by Marshall Neilan, and starred Sessue Hayakawa, Lehua Waipahu, H. Komshi, George Kuwa, Guy Oliver and James Neill.\n\nThe Witch's Tale, a horror anthology radio series, adapted the story as \"The Wonderful Bottle\" in 1934.\n\nKäthe von Nagy was the star in the German film \"Love, Death and the Devil\" (1934) and the French film \"The Devil in the Bottle\" (1935).\n\nA West German stop motion animated feature film based on the story and directed by the Diehl Brothers was released in 1952 under the title \"Der Flaschenteufel\".\n\nAn Italian TV adaptation \"Il diavolo nella bottiglia\" aired on Rai2 on 23 Jun 1981 as part of the horror anthology series \"I giochi del diavolo\".\n\n\"The Devil Inside\", an opera based on Stevenson's short story written by the novelist Louise Welsh and the composer Stuart MacRae, premiered at the Theatre Royal, Glasgow in January 2016. The opera was a co-production between Scottish Opera and Music Theatre Wales.\n\nThe story has inspired the trick-taking card game \"Bottle Imp\", designed by Günter Cornett. It was first published in 1995 by Bambus Spieleverlag, and has been republished several times since under the name \"Bottle Imp\".\n\n\n \n"}
{"id": "1884336", "url": "https://en.wikipedia.org/wiki?curid=1884336", "title": "Wigner quasiprobability distribution", "text": "Wigner quasiprobability distribution\n\nThe Wigner quasiprobability distribution (also called the Wigner function or the Wigner–Ville distribution after Eugene Wigner and ) is a quasiprobability distribution. It was introduced by Eugene Wigner in 1932 to study quantum corrections to classical statistical mechanics. The goal was to link the wavefunction that appears in Schrödinger's equation to a probability distribution in phase space.\n\nIt is a generating function for all spatial autocorrelation functions of a given quantum-mechanical wavefunction .\nThus, it maps on the quantum density matrix in the map between real phase-space functions and Hermitian operators introduced by Hermann Weyl in 1927, in a context related to representation theory in mathematics (cf. Weyl quantization in physics). In effect, it is the Wigner–Weyl transform of the density matrix, so the realization of that operator in phase space. It was later rederived by Jean Ville in 1948 as a quadratic (in signal) representation of the local time-frequency energy of a signal, effectively a spectrogram.\n\nIn 1949, José Enrique Moyal, who had derived it independently, recognized it as the quantum moment-generating functional, and thus as the basis of an elegant encoding of all quantum expectation values, and hence quantum mechanics, in phase space (cf. phase space formulation). It has applications in statistical mechanics, quantum chemistry, quantum optics, classical optics and signal analysis in diverse fields such as electrical engineering, seismology, time–frequency analysis for music signals, spectrograms in biology and speech processing, and engine design.\n\nA classical particle has a definite position and momentum, and hence it is represented by a point in phase space. Given a collection (ensemble) of particles, the probability of finding a particle at a certain position in phase space is specified by a probability distribution, the Liouville density. This strict interpretation fails\nfor a quantum particle, due to the uncertainty principle. Instead, the above quasiprobability Wigner distribution plays an analogous role, but does not satisfy all the properties of a conventional probability distribution; and, conversely, satisfies boundedness properties unavailable to classical distributions.\n\nFor instance, the Wigner distribution can and normally does take on negative values for states which have no classical model—and is a convenient indicator of quantum mechanical interference. (See below for a characterization of pure states whose Wigner functions are non-negative.)\nSmoothing the Wigner distribution through a filter of size larger than (e.g., convolving with a\nphase-space Gaussian, a Weierstrass transform, to yield the Husimi representation, below), results in a positive-semidefinite function, i.e., it may be thought to have been coarsened to a semi-classical one.\n\nRegions of such negative value are provable (by convolving them with a small Gaussian) to be \"small\": they cannot extend to compact regions larger than a few , and hence disappear in the classical limit. They are shielded by the uncertainty principle, which does not allow precise location within phase-space regions smaller than , and thus renders such \"negative probabilities\" less paradoxical.\n\nThe Wigner distribution of a pure state is defined as:\n\nwhere is the wavefunction and and are position and momentum but could be any conjugate variable pair (i.e. real and imaginary parts of the electric field or frequency and time of a signal). Note that it may have support in even in regions where has no support in (\"beats\").\n\nIt is symmetric in and ,\nwhere is the Fourier transform of .\n\nIn 3D,\n\nIn the general case, which includes mixed states, it is the Wigner transform of the density matrix,\nwhere ⟨\"x\"|\"ψ\"⟩ = . This Wigner transformation (or map) is the inverse of the Weyl transform, which maps phase-space functions to Hilbert-space operators, in Weyl quantization.\n\nThus, the Wigner function is the cornerstone of quantum mechanics in phase space.\n\nIn 1949, José Enrique Moyal elucidated\nhow the Wigner function provides the integration measure (analogous\nto a probability density function) in phase space, to yield expectation values from phase-space c-number functions uniquely associated to suitably ordered operators through Weyl's transform (cf. Wigner–Weyl transform and property 7 below), in a manner evocative of classical probability theory.\n\nSpecifically, an operator's expectation value is a \"phase-space average\" of the Wigner transform of that operator,\n\n1. \"P\"(\"x\", \"p\") is a real valued function.\n\n2. The \"x\" and \"p\" probability distributions are given by the marginals:\n\n3. \"P\"(\"x\", \"p\") has the following reflection symmetries:\n\n4. \"P\"(\"x\", \"p\") is Galilei-covariant:\n\n5. The equation of motion for each point in the phase space is classical in the absence of forces:\nIn fact, it is classical even in the presence of harmonic forces.\n\n6. State overlap is calculated as:\n\n7. Operator expectation values (averages) are calculated as\nphase-space averages of the respective Wigner transforms:\n\n8. In order that \"P\"(\"x\", \"p\") represent physical (positive) density matrices:\nfor all pure states |θ〉.\n\n9. By virtue of the Cauchy–Schwarz inequality, for a pure state, it is constrained to be bounded,\n\nThis bound disappears in the classical limit, \"ħ\" → 0. In this limit, \"P\"(\"x\", \"p\") reduces to the probability density in coordinate space \"x\", usually highly localized, multiplied by δ-functions in momentum: the classical limit is \"spiky\". Thus, this quantum-mechanical bound precludes a Wigner function which is a perfectly localized delta function in phase space, as a reflection of the uncertainty principle.\n\nThe Wigner transformation is a general invertible transformation of an operator on a Hilbert space to a function \"g(x,p)\" on phase space, and is given by\n\nHermitian operators map to real functions. The inverse of this transformation,\nso from phase space to Hilbert space, is called the Weyl transformation,\n(not to be confused with another definition of the Weyl transformation).\n\nThe Wigner function \"P\"(\"x,p\") discussed here is thus seen to be the Wigner transform of the density matrix operator \"ρ̂\". Thus, the trace of an operator with the density matrix Wigner-transforms to the equivalent phase-space integral overlap of \"g\"(\"x\", \"p\") with the Wigner function.\n\nThe Wigner transform of the von Neumann evolution equation of the density matrix in the Schrödinger picture is\nwhere H(x,p) is Hamiltonian and { {•, •} } is the Moyal bracket. In the classical limit \"ħ\" → 0, the Moyal bracket reduces to the Poisson bracket, while this evolution equation reduces to the Liouville equation of classical statistical mechanics.\n\nStrictly formally, in terms of quantum characteristics, the solution of\nthis evolution equation reads, formula_22,\nwhere formula_23 and formula_24 are solutions of\nso-called quantum Hamilton's equations, subject to initial conditions\nformula_25 and formula_26, and where formula_27-product\ncomposition is understood for all argument functions.\n\nSince, however, formula_27-composition is thoroughly nonlocal (the \"quantum probability fluid\" diffuses, as observed by Moyal), vestiges of local trajectories\nare normally barely discernible in the evolution of the Wigner distribution function.\nIn the integral representation of -products, successive operations by them have been adapted to a phase-space path-integral, to solve this evolution equation for the Wigner function (see also ).\nThis non-trajectoral feature of Moyal time evolution is illustrated in Figs 5,6,7, below, for Hamiltonians more complex than the oscillator (in some contrast to the quasi-classical oscillator evolution of Fig 4.)\nThe Wigner function allows one to study the classical limit, offering a comparison of the classical and quantum dynamics in phase space.\n\nIt has recently been suggested that the Wigner function approach can be viewed as a quantum analogy to the operatorial formulation of classical mechanics introduced in 1932 by Bernard Koopman and John von Neumann: the time evolution of the Wigner function approaches, in the limit \"ħ\" → 0, the time evolution of the Koopman–von Neumann wavefunction of a classical particle.\n\nAs already noted, the Wigner function of quantum state typically takes some negative values. Indeed, for a pure state in one variable, if formula_29 for all formula_30 and formula_31, then the wave function must have the form\nfor some complex numbers formula_33 with formula_34 (Hudson's theorem). Note that formula_35 is allowed to be complex, so that formula_36 is not necessarily a Gaussian wave packet in the usual sense. Thus, pure states with non-negative Wigner functions are not necessarily minimum uncertainty states in the sense of the Heisenberg uncertainty formula; rather, they give equality in the Schrödinger uncertainty formula, which includes an anticommutator term in addition to the commutator term.\n\nIn higher dimensions, the characterization of pure states with non-negative Wigner functions is similar; the wave function must have the form\nwhere formula_38 is a symmetric complex matrix whose real part is positive definite, formula_39 is a complex vector, and formula_40 is a complex number. The Wigner function of any such state is a Gaussian distribution on phase space.\n\nThe cited paper of Soto and Claverie gives an elegant proof of this characterization, using the Segal–Bargmann transform. The reasoning is as follows. The Husimi Q function of formula_36 may be computed as the squared magnitude of the Segal–Bargmann transform of formula_36, multiplied by a Gaussian. Meanwhile, the Husimi Q function is the convolution of the Wigner function with a Gaussian. If the Wigner function of formula_36 is non-negative everywhere on phase space, then the Husimi Q function will be strictly positive everywhere on phase space. Thus, the Segal–Bargmann transform formula_44 of formula_36 will be nowhere zero. Thus, by a standard result from complex analysis, we have\nfor some holomorphic function formula_47. But in order for formula_48 to belong to the Segal–Bargmann space—that is, for formula_48 to be square-integrable with respect to a Gaussian measure—formula_47 must have at most quadratic growth at infinity. From this, elementary complex analysis can be used to show that formula_47 must actually be a quadratic polynomial. Thus, we obtain an explicit form of the Segal–Bargmann transform of any pure state whose Wigner function is non-negative. We can then invert the Segal–Bargmann transform to obtain the claimed form of the position wave function.\n\nThere does not appear to be any simple characterization of mixed states with non-negative Wigner functions.\n\nIt has been shown that the Wigner quasiprobability distribution function can be regarded as an -deformation of another phase space distribution function that describes an ensemble of de Broglie–Bohm causal trajectories. Basil Hiley has shown that the quasi-probability distribution may be understood as the density matrix re-expressed in terms of a mean position and momentum of a \"cell\" in phase space, and the de Broglie–Bohm interpretation allows one to describe the dynamics of the centers of such \"cells\".\n\nThere is a close connection between the description of quantum states in terms of the Wigner function and a method of quantum states reconstruction in terms of mutually unbiased bases.\n\n\n\nThe Wigner distribution was the first quasiprobability distribution to be formulated, but many more followed, formally equivalent and transformable to and from it (viz. Transformation between distributions in time–frequency analysis). As in the case of coordinate systems, on account of varying properties, several such have with various advantages for specific applications:\nNevertheless, in some sense, the Wigner distribution holds a privileged position among all these distributions, since it is the \"only one\" whose requisite star product drops out (integrates out by parts to effective unity) in the evaluation of expectation values, as illustrated above, and so \"can\" be visualized as a quasiprobability measure analogous to the classical ones.\n\nAs indicated, the formula for the Wigner function was independently derived several times in different contexts. In fact, apparently, Wigner was unaware that even within the context of quantum theory, it had been introduced previously by Heisenberg and Dirac, albeit purely formally: these two missed its significance, and that of its negative values, as they merely considered it as an approximation to the full quantum description of a system such as the atom. (Incidentally, Dirac would later become Wigner's brother-in-law, marrying his sister Manci.) Symmetrically, in most of his legendary 18-month correspondence with Moyal in the mid-1940s, Dirac was unaware that Moyal's quantum-moment generating function was effectively the Wigner function, and it was Moyal who finally brought it to his attention.\n\n\n\n"}
{"id": "205488", "url": "https://en.wikipedia.org/wiki?curid=205488", "title": "William Whewell", "text": "William Whewell\n\nWilliam Whewell ( ; 24 May 1794 – 6 March 1866) was an English polymath, scientist, Anglican priest, philosopher, theologian, and historian of science. He was Master of Trinity College, Cambridge. In his time as a student there, he achieved distinction in both poetry and mathematics.\n\nWhat is most often remarked about Whewell is the breadth of his endeavours. In a time of increasing specialisation, Whewell appears as a vestige of an earlier era when natural philosophers dabbled in a bit of everything. He researched ocean tides (for which he won the Royal Medal), published work in the disciplines of mechanics, physics, geology, astronomy, and economics, while also finding the time to compose poetry, author a Bridgewater Treatise, translate the works of Goethe, and write sermons and theological tracts. In mathematics, Whewell introduced what is now called the Whewell equation, an equation defining the shape of a curve without reference to an arbitrarily chosen coordinate system.\n\nOne of Whewell's greatest gifts to science was his wordsmithing. He often corresponded with many in his field and helped them come up with new terms for their discoveries. Whewell contributed the terms scientist, physicist, linguistics, consilience, catastrophism, uniformitarianism, and astigmatism amongst others; Whewell suggested the terms electrode, ion, dielectric, anode, and cathode to Michael Faraday.\n\nWhewell died in Cambridge in 1866 as a result of a fall from his horse.\n\nWhewell was born in Lancaster. His father, a carpenter, wished him to follow his trade, but his success in mathematics at Lancaster and Heversham grammar schools won him an exhibition (a type of scholarship) at Trinity College, Cambridge (1812). In 1814 he was awarded the Chancellor's Gold Medal for poetry. He was Second Wrangler in 1816, President of the Cambridge Union Society in 1817, became fellow and tutor of his college, and, in 1841, succeeded Christopher Wordsworth as master. He was professor of mineralogy from 1828 to 1832 and Knightbridge Professor of Philosophy (then called \"moral theology and casuistical divinity\") from 1838 to 1855.\n\nWhewell married, firstly, in 1841, Cordelia Marshall, daughter of John Marshall; she died in 1855. In 1858 he married again, to Everina Frances (née Ellis), widow of Sir Gilbert Affleck, 5th Baronet; who died in 1865. He himself died in Cambridge in 1866 as a result of a fall from his horse. ; he is buried in the chapel of Trinity College, Cambridge while his wives are buried together in the Mill Road Cemetery, Cambridge. A window dedicated to Lady Affleck, his second wife, was installed in her memory in the chancel of All Saints' Church, Cambridge and made by Morris & Co. \n\nHis best-known works are two voluminous books which attempt to systematize the development of the sciences, \"History of the Inductive Sciences\" (1837) and \"The Philosophy of the Inductive Sciences, Founded Upon Their History\" (1840). While the \"History\" traced how each branch of the sciences had evolved since antiquity, Whewell viewed the \"Philosophy\" as the \"Moral\" of the previous work as it sought to extract a universal theory of knowledge through history. In the latter, he attempted to follow Francis Bacon's plan for discovery. He examined ideas (\"explication of conceptions\") and by the \"colligation of facts\" endeavoured to unite these ideas with the facts and so construct science.\n\nWhewell analysed inductive reasoning into three steps:\n\nUpon these follow special methods of induction applicable to quantity: the method of curves, the method of means, the method of least squares and the method of residues, and special methods depending on resemblance (to which the transition is made through the law of continuity), such as the method of gradation and the method of natural classification. In \"Philosophy of the Inductive Sciences\" Whewell was the first to use the term \"consilience\" to discuss the unification of knowledge between the different branches of learning.\n\nHere, as in his ethical doctrine, Whewell was moved by opposition to contemporary English empiricism. Following Immanuel Kant, he asserted against John Stuart Mill the \"a priori\" nature of necessary truth, and by his rules for the construction of conceptions he dispensed with the inductive methods of Mill.\n\nAs stated, one of Whewell's greatest gifts to science was his wordsmithing. He often corresponded with many in his field and helped them come up with new terms for their discoveries. In fact, Whewell came up with the term \"scientist\" itself in 1833, and it was first published in Whewell's anonymous 1834 review of Mary Somerville's \"On the Connexion of the Physical Sciences\" published in the \"Quarterly Review\". (They had previously been known as \"natural philosophers\" or \"men of science\").\n\nWhewell was prominent not only in scientific research and philosophy, but also in university and college administration. His first work, \"An Elementary Treatise on Mechanics\" (1819), cooperated with those of George Peacock and John Herschel in reforming the Cambridge method of mathematical teaching. His work and publications also helped influence the recognition of the moral and natural sciences as an integral part of the Cambridge curriculum. In general, however, especially in later years, he opposed reform: he defended the tutorial system, and in a controversy with Connop Thirlwall (1834), opposed the admission of Dissenters; he upheld the clerical fellowship system, the privileged class of \"fellow-commoners,\" and the authority of heads of colleges in university affairs. He opposed the appointment of the University Commission (1850), and wrote two pamphlets (\"Remarks\") against the reform of the university (1855). He stood against the scheme of entrusting elections to the members of the senate and instead, advocated the use of college funds and the subvention of scientific and professorial work.\n\nHe was elected Master of Trinity College, Cambridge in 1841, and retained that position until his death in 1866.\n\nThe Whewell Professorship of International Law and the Whewell Scholarships were established through the provisions of his will.\n\nAside from Science, Whewell was also interested in the history of architecture throughout his life. He is best known for his writings on Gothic architecture, specifically his book, \"Architectural Notes on German Churches\" (first published in 1830). In this work, Whewell established a strict nomenclature for German Gothic churches and came up with a theory of stylistic development. His work is associated with the \"scientific trend\" of architectural writers, along with Thomas Rickman and Robert Willis.\n\nHe paid from his own resources for the construction of two new courts of rooms at Trinity College, Cambridge, built in a Gothic style. The two courts were completed in 1860 and (posthumously) in 1868, and are now collectively named Whewell's Court (in the singular).\n\nBetween 1835 and 1861 Whewell produced various works on the philosophy of morals and politics, the chief of which, \"Elements of Morality\", including \"Polity\", was published in 1845. The peculiarity of this work—written from what is known as the intuitional point of view—is its fivefold division of the springs of action and of their objects, of the primary and universal rights of man (personal security, property, contract, family rights and government), and of the cardinal virtues (benevolence, justice, truth, purity and order).\n\nAmong Whewell's other works—too numerous to mention—were popular writings such as the third \"Bridgewater Treatise\" \"Astronomy and General Physics considered with reference to Natural Theology\" (1833), and the essay, \"Of the Plurality of Worlds\" (1853), in which he argued against the probability of life on other planets, and also the \"Platonic Dialogues for English Readers\" (1850–1861), the \"Lectures on the History of Moral Philosophy in England\" (1852), the essay, \"Of a Liberal Education in General, with particular reference to the Leading Studies of the University of Cambridge\" (1845), the important edition and abridged translation of Hugo Grotius, \"De jure belli ac pacis\" (1853), and the edition of the \"Mathematical Works\" of Isaac Barrow (1860).\n\nWhewell was one of the Cambridge dons whom Charles Darwin met during his education there, and when Darwin returned from the \"Beagle\" voyage he was directly influenced by Whewell, who persuaded Darwin to become secretary of the Geological Society of London. The title pages of \"On the Origin of Species\" open with a quotation from Whewell's \"Bridgewater Treatise\" about science founded on a natural theology of a creator establishing laws:\n\n\"But with regard to the material world, we can at least go so\nfar as this—we can perceive that events are brought about not by\ninsulated interpositions of Divine power, exerted in each particular\ncase, but by the establishment of general laws.\"\n\n\nIn the 1857 novel \"Barchester Towers\" Charlotte Stanhope uses the topic of the theological arguments, concerning the possibility of intelligent life on other planets, between Whewell and David Brewster in an attempt to start up conversation between her impecunious brother and the wealthy young widow Eleanor Bold.\n\n\n\n"}
{"id": "43587926", "url": "https://en.wikipedia.org/wiki?curid=43587926", "title": "Η set", "text": "Η set\n\nIn mathematics, an η set is a type of totally ordered set introduced by that generalizes the order type η of the rational numbers.\n\nIf α is an ordinal then a η set is a totally ordered set such that if \"X\" and \"Y\" are two subsets of cardinality less than ℵ such that every element of \"X\" is less than every element of \"Y\" then there is some element greater than all elements of \"X\" and less than all elements of \"Y\".\n\nThe only non-empty countable η set (up to isomorphism) is the ordered set of rational numbers.\n\nSuppose that κ=ℵ is a regular cardinal and let \"X\" be the set of all functions \"f\" from κ to {−1,0,1} such that if \"f\"(α) = 0 then \"f\"(β) = 0 for all β>α, ordered lexicographically. Then \"X\" is a η set. The union of all these sets is the class of surreal numbers.\n\nA dense totally ordered set without endpoints is a η set if and only if it is ℵ saturated.\n\nAny η set \"X\" is universal for totally ordered sets of cardinality at most ℵ, meaning that any such set can be embedded into \"X\".\n\nFor any given ordinal α, any two η sets of cardinality ℵ are isomorphic (as ordered sets). An η set of cardinality ℵ exists if ℵ is regular and ∑ 2 ≤ ℵ.\n\n"}
