{"id": "498617", "url": "https://en.wikipedia.org/wiki?curid=498617", "title": "118 (number)", "text": "118 (number)\n\n118 (one hundred [and] eighteen) is the natural number following 117 and preceding 119.\n\nThere is no answer to the equation φ(x) = 118, making 118 a nontotient.\n\n118 is the smallest \"n\" such that the range \"n\", \"n\" + 1, ... 4\"n\"/3 contains at least one prime from each of these forms: 4\"x\" + 1, 4\"x\" - 1, 6\"x\" + 1 and 6\"x\" - 1.\n\nFour expressions for 118 as the sum of three positive integers have the same product:\n118 is the smallest number that can be expressed as four sums with the same product in this way.\n\n\nOne hundred and eighteen is also:\n\n\n"}
{"id": "458545", "url": "https://en.wikipedia.org/wiki?curid=458545", "title": "137 (number)", "text": "137 (number)\n\n137 (one hundred [and] thirty-seven) is the natural number following 136 and preceding 138.\n\n137 is:\n\n\nUsing two radii to divide a circle according to the golden ratio yields sectors of approximately 137° (the golden angle) and 222°.<br>\n1/137 = 0.00729927007299270072992700..., its period value is palindromic and has a period length of only 8. 1/137 was once believed to be the exact value of the fine-structure constant.\n\n\n\n\n\n\n\n\n\n"}
{"id": "4590798", "url": "https://en.wikipedia.org/wiki?curid=4590798", "title": "182 (number)", "text": "182 (number)\n\n182 (one hundred [and] eighty-two) is the natural number following 181 and preceding 183.\n\n\n\n\n\n\n\n182 is also:\n\n\n"}
{"id": "3533454", "url": "https://en.wikipedia.org/wiki?curid=3533454", "title": "Alexander Razborov", "text": "Alexander Razborov\n\nAleksandr Aleksandrovich Razborov (; born February 16, 1963), sometimes known as Sasha Razborov, is a Soviet and Russian mathematician and computational theorist. He is Andrew McLeish Distinguished Service Professor at the University of Chicago.\n\nIn his best known work, joint with Steven Rudich, he introduced the notion of \"natural proofs\", a class of strategies used to prove fundamental lower bounds in computational complexity. In particular, Razborov and Rudich showed that, under the assumption that certain kinds of one-way functions exist, such proofs cannot give a resolution of the P = NP problem, so new techniques will be required in order to solve this question.\n\n\n\n\n"}
{"id": "943321", "url": "https://en.wikipedia.org/wiki?curid=943321", "title": "Associator", "text": "Associator\n\nIn abstract algebra, the term associator is used in different ways as a measure of the nonassociativity of an algebraic structure. Associators are commonly studied as triple systems.\n\nFor a nonassociative ring or algebra formula_1, the associator is the multilinear map formula_2 given by\n\nJust as the commutator \n\nmeasures the degree of noncommutativity, the associator measures the degree of nonassociativity of formula_1.\nFor an associative ring or algebra the associator is identically zero.\n\nThe associator in any ring obeys the identity\n\nThe associator is alternating precisely when formula_1 is an alternative ring.\n\nThe associator is symmetric in its two rightmost arguments when formula_1 is a pre-Lie algebra.\n\nThe nucleus is the set of elements that associate with all others: that is, the \"n\" in \"R\" such that\n\nThe nucleus is an associative subring of R.\n\nA quasigroup \"Q\" is a set with a binary operation formula_10 such that for each \"a,b\" in \"Q\",\nthe equations formula_11 and formula_12 have unique solutions \"x,y\" in \"Q\". In a quasigroup \"Q\", the \nassociator is the map formula_13 defined by the equation\n\nfor all \"a,b,c\" in \"Q\". As with its ring theory analog, the quasigroup associator is a measure of nonassociativity of \"Q\".\n\nIn higher-dimensional algebra, where there may be non-identity morphisms between algebraic expressions, an associator is an isomorphism\n\nIn category theory, the associator expresses the associative properties of the internal product functor in monoidal categories.\n\n\n"}
{"id": "641995", "url": "https://en.wikipedia.org/wiki?curid=641995", "title": "Asymptotic analysis", "text": "Asymptotic analysis\n\nIn mathematical analysis, asymptotic analysis, also known as asymptotics, is a method of describing limiting behavior.\n\nAs an illustration, suppose that we are interested in the properties of a function as becomes very large. If , then as becomes very large, the term becomes insignificant compared to . The function is said to be \"\"asymptotically equivalent\" to , as \". This is often written symbolically as , which is read as \" is asymptotic to \".\n\nAn example of an important asymptotic result is the prime number theorem. The theorem states that if is the number of prime numbers that are less than or equal to , then\n\nFormally, given functions and , we define a binary relation\nif and only if \n\nThe symbol is the tilde. The relation is an equivalence relation on the set of functions of ; the functions and are said to be \"asymptotically equivalent\". The domain of and can be any set for which the limit is defined: e.g. real numbers, complex numbers, positive integers.\n\nThe same notation is also used for other ways of passing to a limit: e.g. , , . The way of passing to the limit is often not stated explicitly, if it is clear from the context.\n\nAlthough the above definition is common in the literature, it is problematic if is zero infinitely often as goes to the limiting value. For that reason, some authors use an alternative definition. The alternative definition, in little-o notation, is that if and only if \n\nThis definition is equivalent to the prior definition if is not zero in some neighbourhood of the limiting value.\n\nIf formula_5 and formula_6, then under some mild conditions, the following hold.\n\n\nSuch properties allow asymptotically-equivalent functions to be freely exchanged in many algebraic expressions.\n\n\nAn asymptotic expansion of a function is in practice an expression of that function in terms of a series, the partial sums of which do not necessarily converge, but such that taking any initial partial sum provides an asymptotic formula for . The idea is that successive terms provide an increasingly accurate description of the order of growth of .\n\nIn symbols, it means we have\nbut also\nand \nfor each \"fixed\" .\n\nIn view of the definition of the formula_18 symbol, the last equation means \nin the little o notation, i.e., \n\nThe relation \nwhich means the formula_21 form an asymptotic scale.\n\nIn that case, some authors may abusively write\nto denote the statement\nOne should however be careful that this is not a standard use of the formula_18 symbol, and that it does not correspond to the definition given in .\n\nIn the present situation, this relation formula_28 actually follows from combining steps and ,\nby subtracting formula_29 from formula_30 \none gets\ni.e., formula_28.\n\nIn case the asymptotic expansion does not converge, for any particular value of the argument there will be a particular partial sum which provides the best approximation and adding additional terms will decrease the accuracy. This optimal partial sum will usually have more terms as the argument approaches the limit value.\n\n\n\n\nAsymptotic expansions often occur when an ordinary series is used in a formal expression that forces the taking of values outside of its domain of convergence. For example, we might start with the ordinary series\n\nThe expression on the left is valid on the entire complex plane formula_37, while the right hand side converges only for formula_38. Multiplying by formula_39 and integrating both sides yields\n\nThe integral on the left hand side can be expressed in terms of the exponential integral. The integral on the right hand side, after the substitution formula_41, may be recognized as the gamma function. Evaluating both, one obtains the asymptotic expansion\n\nHere, the right hand side is clearly not convergent for any non-zero value of \"t\". However, by keeping \"t\" small, and truncating the series on the right to a finite number of terms, one may obtain a fairly good approximation to the value of formula_43. Substituting formula_44 and noting that formula_45 results in the asymptotic expansion given earlier in this article.\n\nIn mathematical statistics, an asymptotic distribution is a hypothetical distribution that is in a sense the \"limiting\" distribution of a sequence of distributions. A distribution is an ordered set of random variables\n\nfor formula_47 to formula_48, for some positive integer formula_48. An asymptotic distribution allows formula_50 to range without bound, that is, formula_48 is infinite.\n\nA special case of an asymptotic distribution is when the late entries go to zero—that is, the go to 0 as goes to infinity. Some instances of \"asymptotic distribution\" refer only to this special case.\n\nThis is based on the notion of an asymptotic function which cleanly approaches a constant value (the \"asymptote\") as the independent variable goes to infinity; \"clean\" in this sense meaning that for any desired closeness epsilon there is some value of the independent variable after which the function never differs from the constant by more than epsilon.\n\nAn asymptote is a straight line that a curve approaches but never meets or crosses. Informally, one may speak of the curve meeting the asymptote \"at infinity\" although this is not a precise definition. In the equation\n\nformula_53 becomes arbitrarily small in magnitude as formula_54 increases.\n\nAsymptotic analysis is used in several mathematical sciences. In statistics, asymptotic theory provides limiting approximations of the probability distribution of sample statistics, such as the likelihood ratio statistic and the expected value of the deviance. Asymptotic theory does not provide a method of evaluating the finite-sample distributions of sample statistics, however. Non-asymptotic bounds are provided by methods of approximation theory.\n\nExamples of applications are the following.\n\nAsymptotic analysis is a key tool for exploring the ordinary and partial differential equations which arise in the mathematical modelling of real-world phenomena. An illustrative example is the derivation of the boundary layer equations from the full Navier-Stokes equations governing fluid flow. In many cases, the asymptotic expansion is in power of a small parameter, : in the boundary layer case, this is the nondimensional ratio of the boundary layer thickness to a typical lengthscale of the problem. Indeed, applications of asymptotic analysis in mathematical modelling often center around a nondimensional parameter which has been shown, or assumed, to be small through a consideration of the scales of the problem at hand.\n\nAsymptotic expansions typically arise in the approximation of certain integrals (Laplace's method, saddle-point method, method of steepest descent) or in the approximation of probability distributions (Edgeworth series). The Feynman graphs in quantum field theory are another example of asymptotic expansions which often do not converge.\n\n\n"}
{"id": "17787631", "url": "https://en.wikipedia.org/wiki?curid=17787631", "title": "Bi-isotropic material", "text": "Bi-isotropic material\n\nIn physics, engineering and materials science, bi-isotropic materials have the special optical property that they can rotate the polarization of light in either refraction or transmission. This does not mean all materials with twist effect fall in the bi-isotropic class. The twist effect of the class of bi-isotropic materials is caused by the chirality and non-reciprocity of the structure of the media, in which the electric and magnetic field of an electromagnetic wave (or simply, light) interact in an unusual way.\n\nFor most materials, the electric field \"E\" and electric displacement field \"D\" (as well as the magnetic field \"B\" and inductive magnetic field \"H\") are parallel to one another. These simple mediums are called isotropic, and the relationships between the fields can be expressed using constants. For more complex materials, such as crystals and many metamaterials, these fields are not necessarily parallel. When one set of the fields are parallel, and one set are not, the material is called anisotropic. Crystals typically have \"D\" fields which are not aligned with the \"E\" fields, while the \"B\" and \"H\" fields remain related by a constant. Materials where either pair of fields is not parallel are called anisotropic.\n\nIn bi-isotropic media, the electric and magnetic fields are coupled. The constitutive relations are \n\n\"D\", \"E\", \"B\", \"H\", \"ε\" and \"μ\" are corresponding to usual electromagnetic qualities. \"ξ\" and \"ζ\" are the coupling constants, which is the intrinsic constant of each media.\n\nThis can be generalized to the case where \"ε\", \"μ\", \"ξ\" and \"ζ\" are tensors (i.e. they depend on the direction within the material), in which case the media is referred to as \"bi-anisotropic\".\n\n\"ξ\" and \"ζ\" can be further related to the Tellegen (referred to as reciprocity) \"χ\" and chirality \"κ\" parameter\n\nafter substitution of the above equations into the constitutive relations, gives\n\n\"Pasteur media\" can be made by mixing metal helices of one handedness into a resin. Care has been exercised to secure isotropy: the helices must be randomly oriented so that there is no special direction.\nThe magnetoelectric effect can be understood from the helix as it is exposed to the electromagnetic field. the helix geometry is a sort of inductor. The magnetic component of an EM wave induces a current on the wire and further influence the electric component of the same EM wave. \n\nFrom the constitutive relations, for Pasteur media, \"χ\" = 0,\n\nthe \"D\" field was delayed the respond from the \"H\"-field by a phase \"i\".\n\n\"Tellegen media\" is an opposite of Pasteur media, which is electromagnetic: the electric component will cause the magnetic component to change. such a medium is not as straightforward as the concept of handedness. Electric dipoles bonded with magnets belong to this kind of media. when the dipoles are turned by the electric part of an EM wave, the magnets will also be changed, due to the fact that they are bounded together. The change of magnets direction will change the magnetic component of the same EM wave.\n\nfrom the constitutive relations, for Tellegen media, \"κ\" = 0, \n\nThe \"D\" field was responded immediately from the \"H\"-field.\n\n"}
{"id": "1134562", "url": "https://en.wikipedia.org/wiki?curid=1134562", "title": "Blockhead (thought experiment)", "text": "Blockhead (thought experiment)\n\nBlockhead is the name of a theoretical computer system invented as part of a thought experiment by philosopher Ned Block, which appeared in a paper titled \"Psychologism and Behaviorism\" (though Block does not name the computer in the paper). \n\nIn \"Psychologism and Behaviorism,\" Block argues that the internal mechanism of a system is important in determining whether that system is intelligent and claims to show that a non-intelligent system could pass the Turing test.\n\nBlock asks us to imagine a conversation lasting any given amount of time. He states that given the nature of language, there are a finite number of syntactically- and grammatically-correct sentences that can be used to start a conversation. Consequently, there is a limit to how many \"sensible\" responses can be made to the first sentence, then to the second sentence, and so on until the conversation ends.\n\nBlock then asks the reader to imagine a computer which had been programmed with all the sentences in theory, if not in practice. Block argues that such a machine could continue a conversation with a person on any topic because the computer would be programmed with every sentence that it was possible to use so the computer would be able to pass the Turing test despite the fact, according to Block, that it was not intelligent.\n\nBlock says that this does not show that there is only one correct internal structure for generating intelligence but simply that some internal structures do not generate intelligence.\n\nThe argument is related to John Searle's Chinese room.\n\nA recent objection to the Blockhead argument is Hanoch Ben-Yami (2005), who agrees that Block's machine lacks intelligence but compares its answers to a poetic dialogue in which one man is whispered romantic poetry to recite to his would-be lover as it answers only what it has been told to answer in advance by its programmers.\n\n"}
{"id": "18210373", "url": "https://en.wikipedia.org/wiki?curid=18210373", "title": "Canadian traveller problem", "text": "Canadian traveller problem\n\nIn computer science and graph theory, the Canadian traveller problem (CTP) is a generalization of the shortest path problem to graphs that are \"partially observable\". In other words, the graph is revealed while it is being explored, and explorative edges are charged even if they do not contribute to the final path.\n\nThis optimization problem was introduced by Christos Papadimitriou and Mihalis Yannakakis in 1989 and a number of variants of the problem have been studied since. The name supposedly originates from conversations of the authors who learned of the difficulty Canadian drivers had - traveling a network of cities with snowfall randomly blocking roads. The stochastic version, where each edge is associated with a probability of independently being in the graph, has been given considerable attention in operations research under the name \"the Stochastic Shortest Path Problem with Recourse\" (SSPPR).\n\nFor a given instance, there are a number of possibilities, or \"realizations\", of how the hidden graph may look. Given an instance, a description of how to follow the instance in the best way is called a \"policy\". The CTP task is to compute the expected cost of the optimal policies. To compute an actual description of an optimal policy may be a harder problem.\n\nGiven an instance and policy for the instance, every realization produces its own (deterministic) walk in the graph. Note that the walk is not necessarily a path since the best strategy may be to, e.g., visit every vertex of a cycle and return to the start. This differs from the shortest path problem (with strictly positive weights), where repetitions in a walk implies that a better solution exists.\n\nThere are primarily five parameters distinguishing the number of variants of the Canadian Traveller Problem. The first parameter is how to value the walk produced by a policy for a given instance and realization. In the Stochastic Shortest Path Problem with Recourse, the goal is simply to minimize the cost of the walk (defined as the sum over all edges of the cost of the edge times the number of times that edge was taken). For the Canadian Traveller Problem, the task is to minimize the competitive ratio of the walk; i.e., to minimize the number of times longer the produced walk is to the shortest path in the realization.\n\nThe second parameter is how to evaluate a policy with respect to different realizations consistent with the instance under consideration. In the Canadian Traveller Problem, one wishes to study the worst case and in SSPPR, the average case. For average case analysis, one must furthermore specify an a priori distribution over the realizations.\n\nThe third parameter is restricted to the stochastic versions and is about what assumptions we can make about the distribution of the realizations and how the distribution is represented in the input. In the Stochastic Canadian Traveller Problem and in the Edge-independent Stochastic Shortest Path Problem (i-SSPPR), each uncertain edge (or cost) has an associated probability of being in the realization and the event that an edge is in the graph is independent of which other edges are in the realization. Even though this is a considerable simplification, the problem is still #P-hard. Another variant is to make no assumption on the distribution but require that each realization with non-zero probability be explicitly stated (such as “Probability 0.1 of edge set { {3,4},{1,2} }, probability 0.2 of...”). This is called the Distribution Stochastic Shortest Path Problem (d-SSPPR or R-SSPPR) and is NP-complete. The first variant is harder than the second because the former can represent in logarithmic space some distributions that the latter represents in linear space.\n\nThe fourth and final parameter is how the graph changes over time. In CTP and SSPPR, the realization is fixed but not known. In the Stochastic Shortest Path Problem with Recourse and Resets or the Expected Shortest Path problem, a new realization is chosen from the distribution after each step taken by the policy. This problem can be solved in polynomial time by reducing it to a Markov decision process with polynomial horizon. The Markov generalization, where the realization of the graph may influence the next realization, is known to be much harder.\n\nAn additional parameter is how new knowledge is being discovered on the realization. In traditional variants of CTP, the agent uncovers the exact weight (or status) of an edge upon reaching an adjacent vertex. A new variant was recently suggested where an agent also has the ability to perform remote sensing from any location on the realization. In this variant, the task is to minimize the travel cost plus the cost of sensing operations.\n\nWe define the variant studied in the paper from 1989. That is, the goal is to minimize the competitive ratio in the worst case. It is necessary that we begin by introducing certain terms.\n\nConsider a given graph and the family of undirected graphs that can be constructed by adding one or more edges from a given set. Formally, let formula_1 where we think of \"E\" as the edges that must be in the graph and of \"F\" as the edges that may be in the graph. We say that formula_2 is a \"realization\" of the graph family. Furthermore, let W be an associated cost matrix where formula_3 is the cost of going from vertex \"i\" to vertex \"j\", assuming that this edge is in the realization.\n\nFor any vertex \"v\" in \"V\", we call formula_4 its incident edges with respect to the edge set \"B\" on \"V\". Furthermore, for a realization formula_2, let formula_6 be the cost of the shortest path in the graph from \"s\" to \"t\". This is called the off-line problem because an algorithm for such a problem would have complete information of the graph.\n\nWe say that a strategy formula_7 to navigate such a graph is a mapping from formula_8 to formula_9, where formula_10 denotes the powerset of \"X\". We define the cost formula_11 of a strategy formula_7 with respect to a particular realization formula_13 as follows.\n\nIn other words, we evaluate the policy based on the edges we currently know are in the graph (formula_23) and the edges we know might be in the graph (formula_24). When we take a step in the graph, the edges incident to our new location become known to us. Those edges that are in the graph are added to formula_23, and regardless of whether the edges are in the graph or not, they are removed from the set of unknown edges, formula_24. If the goal is never reached, we say that we have an infinite cost. If the goal is reached, we define the cost of the walk as the sum of the costs of all of the edges traversed, with cardinality.\n\nFinally, we define the Canadian traveller problem.\n\nPapadimitriou and Yannakakis noted that this defines a two-player game, where the players compete over the cost of their respective paths and the edge set is chosen by the second player (nature).\n\nThe original paper analysed the complexity of the problem and reported it to be PSPACE-complete. It was also shown that finding an optimal path in the case where each edge has an associated probability of being in the graph (i-SSPPR) is a PSPACE-easy but ♯P-hard problem. It was an open problem to bridge this gap, but since then both the directed and undirected versions were shown to be PSPACE-hard.\n\nThe directed version of the stochastic problem is known in operations research as the Stochastic Shortest Path Problem with Recourse.\n\nThe problem is said to have applications in operations research, transportation planning, artificial intelligence, machine learning, communication networks, and routing. A variant of the problem has been studied for robot navigation with probabilistic landmark recognition.\n\nDespite the age of the problem and its many potential applications, many natural questions still remain open. Is there a constant-factor approximation or is the problem APX-hard? Is i-SSPPR #P-complete? An even more fundamental question has been left unanswered: is there a polynomial-size \"description\" of an optimal policy, setting aside for a moment the time necessary to compute the description?\n\n\n"}
{"id": "44031786", "url": "https://en.wikipedia.org/wiki?curid=44031786", "title": "Classical control theory", "text": "Classical control theory\n\nClassical control theory is a branch of control theory that deals with the behavior of dynamical systems with inputs, and how their behavior is modified by feedback, using the Laplace transform as a basic tool to model such systems.\n\nThe usual objective of control theory is to control a system, often called the \"plant\", so its output follows a desired control signal, called the \"reference\", which may be a fixed or changing value. To do this a \"controller\" is designed, which monitors the output and compares it with the reference. The difference between actual and desired output, called the \"error\" signal, is applied as feedback to the input of the system, to bring the actual output closer to the reference.\n\nClassical control theory deals with linear time-invariant single-input single-output systems. The Laplace transform of the input and output signal of such systems can be calculated. The transfer function relates the Laplace transform of the input and the output.\n\nTo overcome the limitations of the open-loop controller, classical control theory introduces feedback. A closed-loop controller uses feedback to control states or outputs of a dynamical system. Its name comes from the information path in the system: process inputs (e.g., voltage applied to an electric motor) have an effect on the process outputs (e.g., speed or torque of the motor), which is measured with sensors and processed by the controller; the result (the control signal) is \"fed back\" as input to the process, closing the loop.\n\nClosed-loop controllers have the following advantages over open-loop controllers:\n\nIn some systems, closed-loop and open-loop control are used simultaneously. In such systems, the open-loop control is termed feedforward and serves to further improve reference tracking performance.\n\nA common closed-loop controller architecture is the PID controller.\n\nA Physical system can be modeled in the \"time domain\", where the response of a given system is a function of the various inputs, the previous system values, and time. As time progresses, the state of the system and its response change. However, time-domain models for systems are frequently modeled using high-order differential equations which can become impossibly difficult for humans to solve and some of which can even become impossible for modern computer systems to solve efficiently.\n\nTo counteract this problem, classical control theory uses the Laplace transform to change an Ordinary Differential Equation (ODE) in the time domain into a regular algebraic polynomial in the transform domain. Once a given system has been converted into the transform domain it can be manipulated with greater ease.\n\nModern control theory, instead of changing domains to avoid the complexities of time-domain ODE mathematics, converts the differential equations into a system of lower-order time domain equations called state equations, which can then be manipulated using techniques from linear algebra.\n\nClassical control theory uses the Laplace transform to model the systems and signals. The Laplace transform is a frequency-domain approach for continuous time signals irrespective of whether the system is stable or unstable. The Laplace transform of a function , defined for all real numbers , is the function , which is a unilateral transform defined by\nwhere \"s\" is a complex number frequency parameter\n\nThe output of the system \"y(t)\" is fed back through a sensor measurement \"F\" to the reference value \"r(t)\". The controller \"C\" then takes the error \"e\" (difference) between the reference and the output to change the inputs \"u\" to the system under control \"P\". This is shown in the figure. This kind of controller is a closed-loop controller or feedback controller.\n\nThis is called a single-input-single-output (\"SISO\") control system; \"MIMO\" (i.e., Multi-Input-Multi-Output) systems, with more than one input/output, are common. In such cases variables are represented through vectors instead of simple scalar values. For some distributed parameter systems the vectors may be infinite-dimensional (typically functions).\n\nIf we assume the controller \"C\", the plant \"P\", and the sensor \"F\" are linear and time-invariant (i.e., elements of their transfer function \"C(s)\", \"P(s)\", and \"F(s)\" do not depend on time), the systems above can be analysed using the Laplace transform on the variables. This gives the following relations:\n\nSolving for \"Y\"(\"s\") in terms of \"R\"(\"s\") gives\n\nThe expression formula_7 is referred to as the \"closed-loop transfer function\" of the system. The numerator is the forward (open-loop) gain from \"r\" to \"y\", and the denominator is one plus the gain in going around the feedback loop, the so-called loop gain. If formula_8, i.e., it has a large norm with each value of \"s\", and if formula_9, then \"Y(s)\" is approximately equal to \"R(s)\" and the output closely tracks the reference input.\n\nThe PID controller is probably the most-used feedback control design. \"PID\" is an initialism for \"Proportional-Integral-Derivative\", referring to the three terms operating on the error signal to produce a control signal. If \"u(t)\" is the control signal sent to the system, \"y(t)\" is the measured output and \"r(t)\" is the desired output, and tracking error formula_10, a PID controller has the general form\n\nThe desired closed loop dynamics is obtained by adjusting the three parameters formula_12, formula_13 and formula_14, often iteratively by \"tuning\" and without specific knowledge of a plant model. Stability can often be ensured using only the proportional term. The integral term permits the rejection of a step disturbance (often a striking specification in process control). The derivative term is used to provide damping or shaping of the response. PID controllers are the most well established class of control systems: however, they cannot be used in several more complicated cases, especially if multiple-input multiple-output systems (MIMO) systems are considered.\n\nApplying Laplace transformation results in the transformed PID controller equation\n\nwith the PID controller transfer function\nThere exists a nice example of the closed-loop system discussed above. If we take\n\nPID controller transfer function in series form\n\n1st order filter in feedback loop\n\nlinear actuator with filtered input\n\nand insert all this into expression for closed-loop transfer function H(s), then tuning is very easy: simply put\n\nand get H(s) = 1 identically.\n\nFor practical PID controllers, a pure differentiator is neither physically realisable nor desirable due to amplification of noise and resonant modes in the system. Therefore, a phase-lead compensator type approach is used instead, or a differentiator with low-pass roll-off.\n\nClassical control theory uses an array of tools to analyze systems and design controllers for such systems. Tools include the root locus, the Nyquist stability criterion, the Bode plot, the gain margin and phase margin.\n\n"}
{"id": "293802", "url": "https://en.wikipedia.org/wiki?curid=293802", "title": "Closure (mathematics)", "text": "Closure (mathematics)\n\nA set has closure under an operation if performance of that operation on members of the set always produces a member of the same set; in this case we also say that the set is closed under the operation. For example, the positive integers are closed under addition, but not under subtraction: formula_1 is not a positive integer even though both 1 and 2 are positive integers. Another example is the set containing only zero, which is closed under addition, subtraction and multiplication (because formula_2, formula_3, and formula_4).\n\nSimilarly, a set is said to be closed under a \"collection\" of operations if it is closed under each of the operations individually.\n\nA set that is closed under an operation or collection of operations is said to satisfy a closure property. Often a closure property is introduced as an axiom, which is then usually called the axiom of closure. Modern set-theoretic definitions usually define operations as maps between sets, so adding closure to a structure as an axiom is superfluous; however in practice operations are often defined initially on a superset of the set in question and a closure proof is required to establish that the operation applied to pairs from that set only produces members of that set. For example, the set of even integers is closed under addition, but the set of odd integers is not.\n\nWhen a set \"S\" is not closed under some operations, one can usually find the smallest set containing \"S\" that is closed. This smallest closed set is called the closure of \"S\" (with respect to these operations). For example, the closure under subtraction of the set of natural numbers, viewed as a subset of the real numbers, is the set of integers. An important example is that of topological closure. The notion of closure is generalized by Galois connection, and further by monads.\n\nThe set \"S\" must be a subset of a closed set in order for the closure operator to be defined. In the preceding example, it is important that the reals are closed under subtraction; in the domain of the natural numbers subtraction is not always defined.\n\nThe two uses of the word \"closure\" should not be confused. The former usage refers to the property of being closed, and the latter refers to the smallest closed set containing one that may not be closed. In short, the closure of a set satisfies a closure property.\n\nA set is closed under an operation if the operation returns a member of the set when evaluated on members of the set. Sometimes the requirement that the operation be valued in a set is explicitly stated, in which case it is known as the axiom of closure. For example, one may define a group as a set with a binary product operator obeying several axioms, including an axiom that the product of any two elements of the group is again an element. However the modern definition of an operation makes this axiom superfluous; an \"n\"-ary operation on \"S\" is just a subset of \"S\". By its very definition, an operator on a set cannot have values outside the set.\n\nNevertheless, the closure property of an operator on a set still has some utility. Closure on a set does not necessarily imply closure on all subsets. Thus a subgroup of a group is a subset on which the binary product and the unary operation of inversion satisfy the closure axiom.\n\nAn operation of a different sort is that of finding the limit points of a subset of a topological space (if the space is first-countable, it suffices to restrict consideration to the limits of sequences but in general one must consider at least limits of nets). A set that is closed under this operation is usually just referred to as a closed set in the context of topology. Without any further qualification, the phrase usually means closed in this sense. Closed intervals like [1,2] = {\"x\" : 1 ≤ \"x\" ≤ 2} are closed in this sense.\n\nA partially ordered set is downward closed (and also called a lower set) if for every element of the set all smaller elements are also in it; this applies for example for the real intervals (−∞, \"p\") and (−∞, \"p\"], and for an ordinal number \"p\" represented as interval [ 0, \"p\"); every downward closed set of ordinal numbers is itself an ordinal number.\n\nUpward closed and upper set are defined similarly.\n\n\nGiven an operation on a set \"X\", one can define the closure \"C\"(\"S\") of a subset \"S\" of \"X\" to be the smallest subset closed under that operation that contains \"S\" as a subset, if any such subsets exist. Consequently, \"C\"(\"S\") is the intersection of all closed sets containing \"S\". For example, the closure of a subset of a group is the subgroup generated by that set.\n\nThe closure of sets with respect to some operation defines a closure operator on the subsets of \"X\". The closed sets can be determined from the closure operator; a set is closed if it is equal to its own closure. Typical structural properties of all closure operations are: \n\n\nAn object that is its own closure is called closed. By idempotency, an object is closed if and only if it is the closure of some object.\n\nThese three properties define an abstract closure operator. Typically, an abstract closure acts on the class of all subsets of a set.\n\nIf \"X\" is contained in a set closed under the operation then every subset of \"X\" has a closure.\n\nConsider first homogeneous relations \"R\" ⊆ \"A\" × \"A\". If a relation \"S\" satisfies \"aSb\" ⇒ \"bSa\", then it is a symmetric relation. An arbitrary homogeneous relation \"R\" may not be symmetric but it is always contained in some symmetric relation: \"R\" ⊆ \"S\". The operation of finding the \"smallest\" such \"S\" corresponds to a closure operator called symmetric closure.\n\nA transitive relation \"T\" satisfies \"aTb\" ∧ \"bTc\" ⇒ \"aTc\". An arbitrary homogeneous relation \"R\" may not be transitive but it is always contained in some transitive relation: \"R\" ⊆ \"T\". The operation of finding the \"smallest\" such \"T\" corresponds to a closure operator called transitive closure.\n\nAmong heterogeneous relations there are properties of \"difunctionality\" and \"contact\" which lead to difunctional closure and contact closure. The presence of these closure operators in binary relations leads to topology since open-set axioms may be replaced by Kuratowski closure axioms. Thus each property \"P\", symmetry, transitivity, difunctionality, or contact corresponds to a relational topology. \n\nIn the theory of rewriting systems, one often uses more wordy notions such as the reflexive transitive closure \"R\"—the smallest preorder containing \"R\", or the reflexive transitive symmetric closure \"R\"—the smallest equivalence relation containing \"R\", and therefore also known as the equivalence closure. When considering a particular term algebra, an equivalence relation that is compatible with all operations of the algebra is called a congruence relation. The congruence closure of \"R\" is defined as the smallest congruence relation containing \"R\".\n\nFor arbitrary \"P\" and \"R\", the \"P\" closure of \"R\" need not exist. In the above examples, these exist because reflexivity, transitivity and symmetry are closed under arbitrary intersections. In such cases, the \"P\" closure can be directly defined as the intersection of all sets with property \"P\" containing \"R\".\n\nSome important particular closures can be constructively obtained as follows:\nThe relation \"R\" is said to have closure under some \"cl\", if \"R\" = \"cl\"(\"R\"); for example \"R\" is called symmetric if \"R\" = \"cl\"(\"R\").\n\nAny of these four closures preserves symmetry, i.e., if \"R\" is symmetric, so is any \"cl\"(\"R\"). \nSimilarly, all four preserve reflexivity.\nMoreover, \"cl\" preserves closure under \"cl\" for arbitrary Σ.\nAs a consequence, the equivalence closure of an arbitrary binary relation \"R\" can be obtained as \"cl\"(\"cl\"(\"cl\"(\"R\"))), and the congruence closure with respect to some Σ can be obtained as \"cl\"(\"cl\"(\"cl\"(\"cl\"(\"R\")))). In the latter case, the nesting order does matter; e.g. if \"S\" is the set of terms over Σ = { \"a\", \"b\", \"c\", \"f\" } and \"R\" = { ⟨\"a\",\"b\"⟩, ⟨\"f\"(\"b\"),\"c\"⟩ }, then the pair ⟨\"f\"(\"a\"),\"c\"⟩ is contained in the congruence closure \"cl\"(\"cl\"(\"cl\"(\"cl\"(\"R\")))) of \"R\", but not in the relation \"cl\"(\"cl\"(\"cl\"(\"cl\"(\"R\")))).\n\n"}
{"id": "2187847", "url": "https://en.wikipedia.org/wiki?curid=2187847", "title": "Complex Lie group", "text": "Complex Lie group\n\nIn geometry, a complex Lie group is a complex-analytic manifold that is also a group in such a way formula_1 is holomorphic. Basic examples are formula_2, the general linear groups over the complex numbers. A connected compact complex Lie group is precisely a complex torus (not to be confused with the complex Lie group formula_3). Any finite group may be given the structure of a complex Lie group. A complex semisimple Lie group is an algebraic group.\n\n\n"}
{"id": "7647", "url": "https://en.wikipedia.org/wiki?curid=7647", "title": "Counter (digital)", "text": "Counter (digital)\n\nIn digital logic and computing, a counter is a device which stores (and sometimes displays) the number of times a particular event or process has occurred, often in relationship to a clock signal. The most common type is a sequential digital logic circuit with an input line called the \"clock\" and multiple output lines. The values on the output lines represent a number in the binary or BCD number system. Each pulse applied to the clock input increments or decrements the number in the counter.\n\nA counter circuit is usually constructed of a number of flip-flops connected in cascade. Counters are a very widely used component in digital circuits, and are manufactured as separate integrated circuits and also incorporated as parts of larger integrated circuits.\n\nIn electronics, counters can be implemented quite easily using register-type circuits such as the flip-flop, and a wide variety of classified into:\n\nEach is useful for different applications. Usually, counter circuits are digital in nature, and count in natural binary. Many types of counter circuits are available as digital building blocks, for example a number of chips in the 4500 series implement different counters.\n\nOccasionally there are advantages to using a counting sequence other than the natural binary sequence—such as the binary coded decimal counter, a linear-feedback shift register counter, or a Gray-code counter.\n\nCounters are useful for digital clocks and timers, and in oven timers, VCR clocks, etc.\n\nAn asynchronous (ripple) counter is a single d-type flip-flop, with its J (data) input fed from its own inverted output. This circuit can store one bit, and hence can count from zero to one before it overflows (starts over from 0). This counter will increment once for every clock cycle and takes two clock cycles to overflow, so every cycle it will alternate between a transition from 0 to 1 and a transition from 1 to 0. Notice that this creates a new clock with a 50% duty cycle at exactly half the frequency of the input clock. If this output is then used as the clock signal for a similarly arranged D flip-flop (remembering to invert the output to the input), one will get another 1 bit counter that counts half as fast. Putting them together yields a two-bit counter:\nYou can continue to add additional flip-flops, always inverting the output to its own input, and using the output from the previous flip-flop as the clock signal. The result is called a ripple counter, which can count to where \"n\" is the number of bits (flip-flop stages) in the counter. Ripple counters suffer from unstable outputs as the overflows \"ripple\" from stage to stage, but they do find frequent application as dividers for clock signals, where the instantaneous count is unimportant, but the division ratio overall is (to clarify this, a 1-bit counter is exactly equivalent to a divide by two circuit; the output frequency is exactly half that of the input when fed with a regular train of clock pulses).\n\nThe use of flip-flop outputs as clocks leads to timing skew between the count data bits, making this ripple technique incompatible with normal synchronous circuit design styles.\n\nIn synchronous counters, the clock inputs of all the flip-flops are connected together and are triggered by the input pulses. Thus, all the flip-flops change state simultaneously (in parallel). The circuit below is a 4-bit synchronous counter. The J and K inputs of FF0 are connected to HIGH. FF1 has its J and K inputs connected to the output of FF0, and the J and K inputs of FF2 are connected to the output of an AND gate that is fed by the outputs of FF0 and FF1.\nA simple way of implementing the logic for each bit of an ascending counter (which is what is depicted in the adjacent image) is for each bit to toggle when all of the less significant bits are at a logic high state. For example, bit 1 toggles when bit 0 is logic high; bit 2 toggles when both bit 1 and bit 0 are logic high; bit 3 toggles when bit 2, bit 1 and bit 0 are all high; and so on.\n\nSynchronous counters can also be implemented with hardware finite-state machines, which are more complex but allow for smoother, more stable transitions.\n\nA decade counter is one that counts in decimal digits, rather than binary. A decade counter may have each (that is, it may count in binary-coded decimal, as the 7490 integrated circuit did) or other binary encodings. \"A decade counter is a binary counter that is designed to count to 1010 (decimal 10). An ordinary four-stage counter can be easily modified to a decade counter by adding a NAND gate as in the schematic to the right. Notice that FF2 and FF4 provide the inputs to the NAND gate. The NAND gate outputs are connected to the CLR input of each of the FFs.\" \nA decade counter is one that counts in decimal digits, rather than binary. It counts from 0 to 9 and then resets to zero. The counter output can be set to zero by pulsing the reset line low. The count then increments on each clock pulse until it reaches 1001 (decimal 9). When it increments to 1010 (decimal 10) both inputs of the NAND gate go high. The result is that the NAND output goes low, and resets the counter to zero. D going low can be a CARRY OUT signal, indicating that there has been a count of ten.\n\nA ring counter is a circular shift register which is initiated such that only one of its flip-flops is the state one while others are in their zero states.\n\nA ring counter is a shift register (a cascade connection of flip-flops) with the output of the last one connected to the input of the first, that is, in a ring. Typically, a pattern consisting of a single bit is circulated so the state repeats every n clock cycles if n flip-flops are used.\n\nA Johnson counter (or switch-tail ring counter, twisted ring counter, walking ring counter, or Möbius counter) is a modified ring counter, where the output from the last stage is inverted and fed back as input to the first stage. The register cycles through a sequence of bit-patterns, whose length is equal to twice the length of the shift register, continuing indefinitely. These counters find specialist applications, including those similar to the decade counter, digital-to-analog conversion, etc. They can be implemented easily using D- or JK-type flip-flops.\n\nIn computability theory, a counter is considered a type of memory. A counter stores a single natural number (initially zero) and can be arbitrarily long. A counter is usually considered in conjunction with a finite-state machine (FSM), which can perform the following operations on the counter:\n\n\nThe following machines are listed in order of power, with each one being strictly more powerful than the one below it:\n\nFor the first and last, it doesn't matter whether the FSM is a deterministic finite automaton or a nondeterministic finite automaton. They have the same power. The first two and the last one are levels of the Chomsky hierarchy.\n\nThe first machine, an FSM plus two counters, is equivalent in power to a Turing machine. See the article on counter machines for a proof.\n\nA web counter or hit counter is a computer software program that indicates the number of visitors, or hits, a particular webpage has received. Once set up, these counters will be incremented by one every time the web page is accessed in a web browser.\n\nThe number is usually displayed as an inline digital image or in plain text or on a physical counter such as a mechanical counter. Images may be presented in a variety of fonts, or styles; the classic example is the wheels of an odometer.\n\n\"Web counter\" was popular in the mid to late 1990s and early 2000s, later replaced by more detailed and complete web traffic measures.\n\nMany automation systems use PC and laptops to monitor different parameters of machines and production data. Counters may count parameters such as the number of pieces produced, the production batch number, and measurements of the amounts of material used.\n\nLong before electronics became common, mechanical devices were used to count events. These are known as tally counters. They typically consist of a series of disks mounted on an axle, with the digits zero through nine marked on their edge. The right most disk moves one increment with each event. Each disk except the left-most has a protrusion that, after the completion of one revolution, moves the next disk to the left one increment. Such counters were used as odometers for bicycles and cars and in tape recorders, fuel dispensers, in production machinery as well as in other machinery. One of the largest manufacturers was the Veeder-Root company, and their name was often used for this type of counter.\n\nHand held tally counters are used mainly for stocktaking and for counting people attending events.\n\nElectromechanical counters were used to accumulate totals in tabulating machines that pioneered the data processing industry.\n\n"}
{"id": "3661532", "url": "https://en.wikipedia.org/wiki?curid=3661532", "title": "Cross-multiplication", "text": "Cross-multiplication\n\nIn mathematics, specifically in elementary arithmetic and elementary algebra, given an equation between two fractions or rational expressions, one can cross-multiply to simplify the equation or determine the value of a variable.\n\nThe method is also known as the \"cross your heart\" method because a heart can be drawn to remember which things to multiply together and the lines resemble a heart outline.\n\nGiven an equation like:\n\n(where and are not zero), one can cross-multiply to get:\n\nIn Euclidean geometry the same calculation can be achieved by considering the ratios as those of similar triangles.\n\nIn practice, the method of \"cross-multiplying\" means that we multiply the numerator of each (or one) side by the denominator of the other side, effectively crossing the terms over.\n\nThe mathematical justification for the method is from the following longer mathematical procedure. If we start with the basic equation:\n\nwe can multiply the terms on each side by the same number and the terms will remain equal. Therefore, if we multiply the fraction on each side by the product of the denominators of both sides——we get:\n\nWe can reduce the fractions to lowest terms by noting that the two occurrences of formula_6 on the left-hand side cancel, as do the two occurrences of on the right-hand side, leaving:\n\nand we can divide both sides of the equation by any of the elements—in this case we will use —getting:\n\nAnother justification of cross-multiplication is as follows. Starting with the given equation:\n\nmultiply by = 1 on the left and by = 1 on the right, getting:\n\nand so:\n\nCancel the common denominator = , leaving:\n\nEach step in these procedures is based on a single, fundamental property of equations. Cross-multiplication is a shortcut, an easily understandable procedure that can be taught to students.\n\nThis is a common procedure in mathematics, used to reduce fractions or calculate a value for a given variable in a fraction. If we have an equation like this, where is a variable we are interested in solving for:\n\nwe can use cross multiplication to determine that:\n\nFor example, let's say that we want to know how far a car will get in 7 hours, if we happen to know that its speed is constant and that it already travelled 90 miles in the last 3 hours. Converting the word problem into ratios we get\n\nCross-multiplying yields:\n\nand so:\n\nNote that even simple equations like this:\n\nare solved using cross multiplication, since the missing term is implicitly equal to 1:\n\nAny equation containing fractions or rational expressions can be simplified by multiplying both sides by the least common denominator. This step is called \"clearing fractions\".\n\nThe Rule of Three is a shorthand version for a particular form of cross-multiplication, that may be taught to students by rote. It was considered the height of Colonial math education and still figures in the French national curriculum for secondary education.\n\nFor an equation of the form:\n\nwhere the variable to be evaluated is in the right-hand denominator, the Rule of Three states that:\n\nIn this context, is referred to as the \"extreme\" of the proportion, and and are called the \"means\".\n\nThis rule was already known to Chinese mathematicians prior to the 7th century CE, though it was not used in Europe until much later. \n\nThe Rule of Three gained notoriety for being particularly difficult to explain. Cocker's Arithmetick, the premier textbook in the 17th century, introduces its discussion of the Rule of Three with the problem, \"If 4 Yards of Cloth cost 12 Shillings, what will 6 Yards cost at that Rate?\" The Rule of Three gives the answer to this problem directly; whereas in modern arithmetic, we would solve it by introducing a variable to stand for the cost of 6 yards of cloth, writing down the equation:\n\nand then using cross-multiplication to calculate :\n\n"}
{"id": "4516489", "url": "https://en.wikipedia.org/wiki?curid=4516489", "title": "Darb-e Imam", "text": "Darb-e Imam\n\nThe shrine of Darb-e Imam (), located in the Dardasht quarter of Isfahan, Iran, is a funerary complex, with a cemetery, shrine structures, and courtyards belonging to different construction periods and styles. The first structures were built by Jalal al-Din Safarshah, during the Qara Qoyunlu reign in 1453.\n\nPeter Lu and Paul Steinhardt have studied Islamic tiling patterns, called \"girih tiles\". They strongly resemble Penrose tilings, to which the designs on the Darb-e Imam shrine are almost identical.\n\n\n"}
{"id": "6129609", "url": "https://en.wikipedia.org/wiki?curid=6129609", "title": "David Gabai", "text": "David Gabai\n\nDavid Gabai, a mathematician, is the Hughes-Rogers Professor of Mathematics at Princeton University. Focused on low-dimensional topology and hyperbolic geometry, he is a leading researcher in those subjects.\n\nDavid Gabai received his B.S. degree from MIT in 1976 and his Ph.D. from Princeton in 1980, the latter under the direction of William Thurston. During his Ph.D., he obtained foundational results on the foliations of 3-manifolds.\n\nAfter positions at Harvard and University of Pennsylvania, Gabai spent most of the period of 1986–2001 at Caltech, and has been at Princeton since 2001.\n\nIn 2004, David Gabai was awarded the Oswald Veblen Prize in Geometry, given every three years by the American Mathematical Society. \n\nHe was an invited speaker in the International Congress of Mathematicians 2010, Hyderabad on the topic of topology. \n\nIn 2011, he was elected to the United States National Academy of Sciences. In 2012, he became a fellow of the American Mathematical Society.\n\nDavid Gabai has played a key role in the field of topology of 3-manifolds in the last three decades. Some of the foundational results he and his collaborators have proved are as follows: Existence of taut foliation in 3-manifolds, Property R Conjecture, foundation of essential laminations, Seifert fiber space conjecture, rigidity of homotopy hyperbolic 3-manifolds, weak hyperbolization for 3-manifolds with genuine lamination, Smale conjecture for hyperbolic 3-manifolds, Marden's Tameness Conjecture, Weeks manifold being the minimum volume closed hyperbolic 3-manifold.\n\n"}
{"id": "920110", "url": "https://en.wikipedia.org/wiki?curid=920110", "title": "Dinitz conjecture", "text": "Dinitz conjecture\n\nIn combinatorics, the Dinitz theorem (formerly known as Dinitz conjecture) is a statement about the extension of arrays to partial Latin squares, proposed in 1979 by Jeff Dinitz, and proved in 1994 by Fred Galvin.\n\nThe Dinitz theorem is that given an \"n\" × \"n\" square array, a set of \"m\" symbols with \"m\" ≥ \"n\", and for each cell of the array an \"n\"-element set drawn from the pool of \"m\" symbols, it is possible to choose a way of labeling each cell with one of those elements in such a way that no row or column repeats a symbol.\nIt can also be formulated as a result in graph theory, that the list chromatic index of the complete bipartite graph formula_1 equals formula_2. That is, if each edge of the complete bipartite graph is assigned a set of formula_2 colors, it is possible to choose one of the assigned colors for each edge\nsuch that no two edges incident to the same vertex have the same color.\n\nGalvin's proof generalizes to the statement that, for every bipartite multigraph, the list chromatic index equals its chromatic index. The more general edge list coloring conjecture states that the same holds not only for bipartite graphs, but also for any loopless multigraph. An even more general conjecture states that the list chromatic number of claw-free graphs always equals their chromatic number. The Dinitz theorem is also related to Rota's basis conjecture.\n"}
{"id": "47101698", "url": "https://en.wikipedia.org/wiki?curid=47101698", "title": "Dubins–Spanier theorems", "text": "Dubins–Spanier theorems\n\nThe Dubins–Spanier theorems are several theorems in the theory of fair cake-cutting. They were published by Lester Dubins and Edwin Spanier in 1961. Although the original motivation for these theorems is fair division, they are in fact general theorems in measure theory.\n\nThere is a set formula_1, and a set formula_2 which is a sigma-algebra of subsets of formula_1.\n\nThere are formula_4 partners. Every partner formula_5 has a personal value measure formula_6. This function determines how much each subset of formula_1 is worth to that partner.\n\nLet formula_8 a partition of formula_1 to formula_10 measurable sets: formula_11. Define the matrix formula_12 as the following formula_13 matrix:\n\nThis matrix contains the valuations of all players to all pieces of the partition.\n\nLet formula_15 be the collection of all such matrices (for the same value measures, the same formula_10, and different partitions):\n\nThe Dubins–Spanier theorems deal with the topological properties of formula_15.\n\nIf all value measures formula_19 are countably-additive and nonatomic, then:\n\n\nThis was already proved by Dvoretzky, Wald, and Wolfowitz. \n\nA cake partition formula_8 to \"k\" pieces is called a \"consensus partition with weights formula_23\" (also called exact division) if:\nI.e, there is a consensus among all partners that the value of piece \"j\" is exactly formula_25.\n\nSuppose, from now on, that formula_23 are weights whose sum is 1:\nand the value measures are normalized such that each partner values the entire cake as exactly 1:\n\nThe convexity part of the DS theorem implies that: \n\nPROOF: For every formula_29, define a partition formula_30 as follows:\nIn the partition formula_30, all partners value the formula_34-th piece as 1 and all other pieces as 0. Hence, in the matrix formula_35, there are ones on the formula_34-th column and zeros everywhere else.\n\nBy convexity, there is a partition formula_8 such that:\n\nIn that matrix, the formula_34-th column contains only the value formula_25. This means that, in the partition formula_8, all partners value the formula_34-th piece as exactly formula_25.\n\nNote: this corollary confirms a previous assertion by Hugo Steinhaus. It also gives an affirmative answer to the problem of the Nile provided that there are only a finite number of flood heights.\n\nA cake partition formula_8 to \"n\" pieces (one piece per partner) is called a \"super-proportional division with weights formula_45\" if:\nI.e, the piece allotted to partner formula_5 is strictly more valuable for him than what he deserves. The following statement is Dubins-Spanier Theorem on the existence of super-proportional division \n\nThe hypothesis that the value measures formula_48 are not identical is necessary. Otherwise, the sum formula_49 leads to a contradiction.\n\nNamely, if all value measures are countably-additive and non-atomic, and if there are two partners formula_50 such that formula_51,\nthen a super-proportional division exists.I.e, the necessary condition is also sufficient.\n\nSuppose w.l.o.g. that formula_52. Then there is some piece of the cake, formula_53, such that formula_54. Let formula_55 be the complement of formula_56; then formula_57. This means that formula_58. However, formula_59. Hence, either formula_60 or formula_61. Suppose w.l.o.g. that formula_54 and formula_60 are true.\n\nDefine the following partitions:\n\nHere, we are interested only in the diagonals of the matrices formula_35, which represent the valuations of the partners to their own pieces:\n\nBy convexity, for every set of weights formula_77 there is a partition formula_8 such that:\n\nIt is possible to select the weights formula_80 such that, in the diagonal of formula_12, the entries are in the same ratios as the weights formula_82. Since we assumed that formula_60, it is possible to prove that formula_46, so formula_8 is a super-proportional division.\n\nA cake partition formula_8 to \"n\" pieces (one piece per partner) is called \"utilitarian-optimal\" if it maximizes the sum of values. I.e, it maximizes:\n\nUtilitarian-optimal divisions do not always exist. For example, suppose formula_1 is the set of positive integers. There are two partners. Both value the entire set formula_1 as 1. Partner 1 assigns a positive value to every integer and partner 2 assigns zero value to every finite subset. From a utilitarian point of view, it is best to give partner 1 a large finite subset and give the remainder to partner 2. When the set given to partner 1 becomes larger and larger, the sum-of-values becomes closer and closer to 2, but it never approaches 2. So there is no utilitarian-optimal division.\n\nThe problem with the above example is that the value measure of partner 2 is finitely-additive but not countably-additive.\n\nThe compactness part of the DS theorem immediately implies that:\n\nIn this special case, non-atomicity is not required: if all value measures are countably-additive, then a utilitarian-optimal partition exists.\n\nA cake partition formula_8 to \"n\" pieces (one piece per partner) is called \"leximin-optimal with weights formula_45\" if it maximizes the lexicographically-ordered vector of relative values. I.e, it maximizes the following vector:\nwhere the partners are indexed such that:\nA leximin-optimal partition maximizes the value of the poorest partner (relative to his weight); subject to that, it maximizes the value of the next-poorest partner (relative to his weight); etc.\n\nThe compactness part of the DS theorem immediately implies that:\n\n\n"}
{"id": "6694084", "url": "https://en.wikipedia.org/wiki?curid=6694084", "title": "Fielden Professor of Pure Mathematics", "text": "Fielden Professor of Pure Mathematics\n\nThe Fielden Chair of Pure Mathematics is an endowed professorial position in the School of Mathematics, University of Manchester, England. \n\nIn 1870 Samuel Fielden, a wealthy mill owner from Todmorden, donated £150 to Owens College (as the Victoria University of Manchester was then called) for the teaching of evening classes and a further £3000 for the development of natural sciences at the college. From 1877 this supported the Fielden Lecturer, subsequently to become the Fielden Reader with the appointment of L. J. Mordell in 1922 and then Fielden Professor in 1923. Alex Wilkie FRS was appointed to the post in 2007. Previous holders of the Fielden Chair (and lectureship) are:\n\n\nThe other endowed chairs in mathematics at the University of Manchester are the Beyer Chair of Applied Mathematics, the Sir Horace Lamb Chair and the Richardson Chair of Applied Mathematics.\n"}
{"id": "14682596", "url": "https://en.wikipedia.org/wiki?curid=14682596", "title": "Finite topological space", "text": "Finite topological space\n\nIn mathematics, a finite topological space is a topological space for which the underlying point set is finite. That is, it is a topological space for which there are only finitely many points.\n\nWhile topology has mainly been developed for infinite spaces, finite topological spaces are often used to provide examples of interesting phenomena or counterexamples to plausible sounding conjectures. William Thurston has called the study of finite topologies in this sense \"an oddball topic that can\nlend good insight to a variety of questions\".\n\nA topology on a set \"X\" is defined as a subset of \"P\"(\"X\"), the power set of \"X\", which includes both ∅ and \"X\" and is closed under finite intersections and arbitrary unions.\n\nSince the power set of a finite set is finite there can be only finitely many open sets (and only finitely many closed sets). Therefore, one only need check that the union of a finite number of open sets is open. This leads to a simpler description of topologies on a finite set.\n\nLet \"X\" be a finite set. A topology on \"X\" is a subset τ of \"P\"(\"X\") such that\n\nA topology on a finite set is therefore nothing more than a sublattice of (\"P\"(\"X\"), ⊂) which includes both the bottom element (∅) and the top element (\"X\").\n\nEvery finite bounded lattice is complete since the meet or join of any family of elements can always be reduced to a meet or join of two elements. It follows that in a finite topological space the union or intersection of an arbitrary family of open sets (resp. closed sets) is open (resp. closed).\n\nTopologies on a finite set \"X\" are in one-to-one correspondence with preorders on \"X\". Recall that a preorder on \"X\" is a binary relation on \"X\" which is reflexive and transitive.\n\nGiven a (not necessarily finite) topological space \"X\" we can define a preorder on \"X\" by\nwhere cl{\"y\"} denotes the closure of the singleton set {\"y\"}. This preorder is called the \"specialization preorder\" on \"X\". Every open set \"U\" of \"X\" will be an upper set with respect to ≤ (i.e. if \"x\" ∈ \"U\" and \"x\" ≤ \"y\" then \"y\" ∈ \"U\"). Now if \"X\" is finite, the converse is also true: every upper set is open in \"X\". So for finite spaces, the topology on \"X\" is uniquely determined by ≤.\n\nGoing in the other direction, suppose (\"X\", ≤) is a preordered set. Define a topology τ on \"X\" by taking the open sets to be the upper sets with respect to ≤. Then the relation ≤ will be the specialization preorder of (\"X\", τ). The topology defined in this way is called the Alexandrov topology determined by ≤.\n\nThe equivalence between preorders and finite topologies can be interpreted as a version of Birkhoff's representation theorem, an equivalence between finite distributive lattices (the lattice of open sets of the topology) and partial orders (the partial order of equivalence classes of the preorder). This correspondence also works for a larger class of spaces called finitely generated spaces. Finitely generated spaces can be characterized as the spaces in which an arbitrary intersection of open sets is open. Finite topological spaces are a special class of finitely generated spaces.\n\nThere is a unique topology on the empty set ∅. The only open set is the empty one. Indeed, this is the only subset of ∅.\n\nLikewise, there is a unique topology on a singleton set {\"a\"}. Here the open sets are ∅ and {\"a\"}. This topology is both discrete and trivial, although in some ways it is better to think of it as a discrete space since it shares more properties with the family of finite discrete spaces.\n\nFor any topological space \"X\" there is a unique continuous function from ∅ to \"X\", namely the empty function. There is also a unique continuous function from \"X\" to the singleton space {\"a\"}, namely the constant function to \"a\". In the language of category theory the empty space serves as an initial object in the category of topological spaces while the singleton space serves as a terminal object.\n\nLet \"X\" = {\"a\",\"b\"} be a set with 2 elements. There are four distinct topologies on \"X\":\n\nThe second and third topologies above are easily seen to be homeomorphic. The function from \"X\" to itself which swaps \"a\" and \"b\" is a homeomorphism. A topological space homeomorphic to one of these is called a Sierpiński space. So, in fact, there are only three inequivalent topologies on a two-point set: the trivial one, the discrete one, and the Sierpiński topology.\n\nThe specialization preorder on the Sierpiński space {\"a\",\"b\"} with {\"b\"} open is given by: \"a\" ≤ \"a\", \"b\" ≤ \"b\", and \"a\" ≤ \"b\".\n\nLet \"X\" = {\"a\",\"b\",\"c\"} be a set with 3 elements. There are 29 distinct topologies on \"X\" but only 9 inequivalent topologies:\n\nThe last 5 of these are all T. The first one is trivial, while in 2, 3, and 4 the points \"a\" and \"b\" are topologically indistinguishable.\n\nLet \"X\" = {\"a\",\"b\",\"c\",\"d\"} be a set with 4 elements. There are 355 distinct topologies on \"X\" but only 33 inequivalent topologies:\n\n\nThe last 16 of these are all T.\n\nEvery finite topological space is compact since any open cover must already be finite. Indeed, compact spaces are often thought of as a generalization of finite spaces since they share many of the same properties.\n\nEvery finite topological space is also second-countable (there are only finitely many open sets) and separable (since the space itself is countable).\n\nIf a finite topological space is T (in particular, if it is Hausdorff) then it must, in fact, be discrete. This is because the complement of a point is a finite union of closed points and therefore closed. It follows that each point must be open.\n\nTherefore, any finite topological space which is not discrete cannot be T, Hausdorff, or anything stronger.\n\nHowever, it is possible for a non-discrete finite space to be T. In general, two points \"x\" and \"y\" are topologically indistinguishable if and only if \"x\" ≤ \"y\" and \"y\" ≤ \"x\", where ≤ is the specialization preorder on \"X\". It follows that a space \"X\" is T if and only if the specialization preorder ≤ on \"X\" is a partial order. There are numerous partial orders on a finite set. Each defines a unique T topology.\n\nSimilarly, a space is R if and only if the specialization preorder is an equivalence relation. Given any equivalence relation on a finite set \"X\" the associated topology is the partition topology on \"X\". The equivalence classes will be the classes of topologically indistinguishable points. Since the partition topology is pseudometrizable, a finite space is R if and only if it is completely regular.\n\nNon-discrete finite spaces can also be normal. The excluded point topology on any finite set is a completely normal T space which is non-discrete.\n\nConnectivity in a finite space \"X\" is best understood by considering the specialization preorder ≤ on \"X\". We can associate to any preordered set \"X\" a directed graph Γ by taking the points of \"X\" as vertices and drawing an edge \"x\" → \"y\" whenever \"x\" ≤ \"y\". The connectivity of a finite space \"X\" can be understood by considering the connectivity of the associated graph Γ.\n\nIn any topological space, if \"x\" ≤ \"y\" then there is a path from \"x\" to \"y\". One can simply take \"f\"(0) = \"x\" and \"f\"(\"t\") = \"y\" for \"t\" > 0. It is easily to verify that \"f\" is continuous. It follows that the path components of a finite topological space are precisely the (weakly) connected components of the associated graph Γ. That is, there is a topological path from \"x\" to \"y\" if and only if there is an undirected path between the corresponding vertices of Γ.\n\nEvery finite space is locally path-connected since the set\nis a path-connected open neighborhood of \"x\" that is contained in every other neighborhood. In other words, this single set forms a local base at \"x\".\n\nTherefore, a finite space is connected if and only if it is path-connected. The connected components are precisely the path components. Each such component is both closed and open in \"X\".\n\nFinite spaces may have stronger connectivity properties. A finite space \"X\" is\nFor example, the particular point topology on a finite space is hyperconnected while the excluded point topology is ultraconnected. The Sierpiński space is both.\n\nA finite topological space is pseudometrizable if and only if it is R. In this case, one possible pseudometric is given by\nwhere \"x\" ≡ \"y\" means \"x\" and \"y\" are topologically indistinguishable. A finite topological space is metrizable if and only if it is discrete.\n\nLikewise, a topological space is uniformizable if and only if it is R. The uniform structure will be the pseudometric uniformity induced by the above pseudometric.\n\nPerhaps surprisingly, there are finite topological spaces with nontrivial fundamental groups. A simple example is the pseudocircle, which is space \"X\" with four points, two of which are open and two of which are closed. There is a continuous map from the unit circle \"S\" to \"X\" which is a weak homotopy equivalence (i.e. it induces an isomorphism of homotopy groups). It follows that the fundamental group of the pseudocircle is infinite cyclic.\n\nMore generally it has been shown that for any finite abstract simplicial complex \"K\", there is a finite topological space \"X\" and a weak homotopy equivalence \"f\" : |\"K\"| → \"X\" where |\"K\"| is the geometric realization of \"K\". It follows that the homotopy groups of |\"K\"| and \"X\" are isomorphic. In fact, the underlying set of \"X\" can be taken to be \"K\" itself, with the topology associated to the inclusion partial order.\n\nAs discussed above, topologies on a finite set are in one-to-one correspondence with preorders on the set, and T topologies are in one-to-one correspondence with partial orders. Therefore, the number of topologies on a finite set is equal to the number of preorders and the number of T topologies is equal to the number of partial orders.\n\nThe table below lists the number of distinct (T) topologies on a set with \"n\" elements. It also lists the number of inequivalent (i.e. nonhomeomorphic) topologies.\n\nLet \"T\"(\"n\") denote the number of distinct topologies on a set with \"n\" points. There is no known simple formula to compute \"T\"(\"n\") for arbitrary \"n\". The Online Encyclopedia of Integer Sequences presently lists \"T\"(\"n\") for \"n\" ≤ 18.\n\nThe number of distinct T topologies on a set with \"n\" points, denoted \"T\"(\"n\"), is related to \"T\"(\"n\") by the formula\nwhere \"S\"(\"n\",\"k\") denotes the Stirling number of the second kind.\n\n\n\n"}
{"id": "2037563", "url": "https://en.wikipedia.org/wiki?curid=2037563", "title": "Geodesics in general relativity", "text": "Geodesics in general relativity\n\nIn general relativity, a geodesic generalizes the notion of a \"straight line\" to curved spacetime. Importantly, the world line of a particle free from all external, non-gravitational force, is a particular type of geodesic. In other words, a freely moving or falling particle always moves along a geodesic.\n\nIn general relativity, gravity can be regarded as not a force but a consequence of a curved spacetime geometry where the source of curvature is the stress–energy tensor (representing matter, for instance). Thus, for example, the path of a planet orbiting a star is the projection of a geodesic of the curved 4-D spacetime geometry around the star onto 3-D space.\n\nThe full geodesic equation is this:\nwhere \"s\" is a scalar parameter of motion (e.g. the proper time), and formula_2 are Christoffel symbols (sometimes called the affine connection coefficients or Levi-Civita connection coefficients) which is symmetric in the two lower indices. Greek indices may take the values: 0, 1, 2, 3 and the summation convention is used for repeated indices formula_3 and formula_4. The quantity on the left-hand-side of this equation is the acceleration of a particle, and so this equation is analogous to Newton's laws of motion which likewise provide formulae for the acceleration of a particle. This equation of motion employs the Einstein notation, meaning that repeated indices are summed (i.e. from zero to three). The Christoffel symbols are functions of the four space-time coordinates, and so are independent of the velocity or acceleration or other characteristics of a test particle whose motion is described by the geodesic equation.\n\nSo far the geodesic equation of motion has been written in terms of a scalar parameter \"s\". It can alternatively be written in terms of the time coordinate, formula_5 (here we have used the triple bar to signify a definition). The geodesic equation of motion then becomes:\n\nThis formulation of the geodesic equation of motion can be useful for computer calculations and to compare General Relativity with Newtonian Gravity. It is straightforward to derive this form of the geodesic equation of motion from the form which uses proper time as a parameter, using the chain rule. Notice that both sides of this last equation vanish when the mu index is set to zero. If the particle's velocity is small enough, then the geodesic equation reduces to this:\n\nHere the Latin index \"n\" takes the values [1,2,3]. This equation simply means that all test particles at a particular place and time will have the same acceleration, which is a well-known feature of Newtonian gravity. For example, everything floating around in the international space station will undergo roughly the same acceleration due to gravity.\n\nPhysicist Steven Weinberg has presented a derivation of the geodesic equation of motion directly from the equivalence principle.\nThe first step in such a derivation is to suppose that no particles are accelerating in the neighborhood of a point-event with respect to a freely falling coordinate system (formula_8). Setting formula_9, we have the following equation that is locally applicable in free fall:\nThe next step is to employ the multi-dimensional chain rule. We have:\n\nDifferentiating once more with respect to the time, we have:\n\nTherefore:\n\nMultiply both sides of this last equation by the following quantity:\n\nConsequently, we have this:\n\nUsing (from Christoffel symbols#Change of variable and the fact that the Christoffel symbols vanish in an inertial frame of reference)\n\nit becomes\n\nApplying the one-dimensional chain rule gives\n\nAs before, we can set formula_5. Then the first derivative of \"x\" with respect to \"t\" is one and the second derivative is zero. Replacing \"λ\" with zero gives:\n\nSubtracting d \"x\" / d \"t\" times this from the previous equation gives:\n\nwhich is a form of the geodesic equation of motion (using the coordinate time as parameter).\n\nThe geodesic equation of motion can alternatively be derived using the concept of parallel transport.\n\nWe can (and this is the most common technique) derive the geodesic equation via the action principle. Consider the case of trying to find a geodesic between two timelike-separated events.\n\nLet the action be \n\nwhere formula_24 is the line element. There is a negative sign inside the square root because the curve must be timelike. To get the geodesic equation we must vary this action. To do this let us parameterize this action with respect to a parameter formula_25. Doing this we get:\n\nWe can now go ahead and vary this action with respect to the curve formula_27. By the principle of least action we get:\n\nUsing the product rule we get:\n\nIntegrating by-parts the last term and dropping the total derivative (which equals to zero at the boundaries) we get that:\n\nSimplifying a bit we see that:\n\nso,\n\nmultiplying this equation by formula_33 we get:\n\nSo by Hamilton's principle we find that the Euler–Lagrange equation is\n\nMultiplying by the inverse metric tensor formula_36 we get that\n\nThus we get the geodesic equation:\n\nwith the Christoffel symbol defined in terms of the metric tensor as\n\nAlbert Einstein believed that the geodesic equation of motion can be derived from the field equations for empty space, i.e. from the fact that the Ricci curvature vanishes. He wrote:\nIt has been shown that this law of motion — generalized to the case of arbitrarily large gravitating masses — can be derived from the field equations of empty space alone. According to this derivation the law of motion is implied by the condition that the field be singular nowhere outside its generating mass points.\nand \nOne of the imperfections of the original relativistic theory of gravitation was that as a field theory it was not complete; it introduced the independent postulate that the law of motion of a particle is given by the equation of the geodesic. \nA complete field theory knows only fields and not the concepts of particle and motion. For these must not exist independently from the field but are to be treated as part of it.\nBoth physicists and philosophers have often repeated the assertion that the geodesic equation can be obtained from the field equations to describe the motion of a gravitational singularity, but this claim remains disputed. Less controversial is the notion that the field equations determine the motion of a fluid or dust, as distinguished from the motion of a point-singularity.\n\nIn deriving the geodesic equation from the equivalence principle, it was assumed that particles in a local inertial coordinate system are not accelerating. However, in real life, the particles may be charged, and therefore may be accelerating locally in accordance with the Lorentz force. That is:\n\nwith\n\nThe Minkowski tensor formula_42 is given by:\n\nThese last three equations can be used as the starting point for the derivation of an equation of motion in General Relativity, instead of assuming that acceleration is zero in free fall. Because the Minkowski tensor is involved here, it becomes necessary to introduce something called the \"metric tensor\" in General Relativity. The metric tensor \"g\" is symmetric, and locally reduces to the Minkowski tensor in free fall. The resulting equation of motion is as follows:\n\nwith\n\nThis last equation signifies that the particle is moving along a timelike geodesic; massless particles like the photon instead follow null geodesics (replace −1 with zero on the right-hand side of the last equation). It is important that the last two equations are consistent with each other, when the latter is differentiated with respect to proper time, and the following formula for the Christoffel symbols ensures that consistency:\n\nThis last equation does not involve the electromagnetic fields, and it is applicable even in the limit as the electromagnetic fields vanish. The letter \"g\" with superscripts refers to the inverse of the metric tensor. In General Relativity, indices of tensors are lowered and raised by contraction with the metric tensor or its inverse, respectively.\n\nA geodesic between two events can also be described as the curve joining those two events which has a stationary interval (4-dimensional \"length\"). \"Stationary\" here is used in the sense in which that term is used in the calculus of variations, namely, that the interval along the curve varies minimally among curves that are nearby to the geodesic.\n\nIn Minkowski space there is only one time-like geodesic that connects any given pair of time-like separated events, and that geodesic is the curve with the longest proper time between the two events. But in curved spacetime, it's possible for a pair of widely separated events to have more than one time-like geodesic that connects them. In such instances, the proper times along the various geodesics will not in general be the same. And for some geodesics in such instances, it's possible for a curve that connects the two events and is nearby to the geodesic to have either a longer or a shorter proper time than the geodesic.\n\nFor a space-like geodesic through two events, there are always nearby curves which go through the two events that have either a longer or a shorter proper length than the geodesic, even in Minkowski space. In Minkowski space, in an inertial frame of reference in which the two events are simultaneous, the geodesic will be the straight line between the two events at the time at which the events occur. Any curve that differs from the geodesic purely spatially (\"i.e.\" does not change the time coordinate) in that frame of reference will have a longer proper length than the geodesic, but a curve that differs from the geodesic purely temporally (\"i.e.\" does not change the space coordinate) in that frame of reference will have a shorter proper length.\n\nThe interval of a curve in spacetime is\nThen, the Euler–Lagrange equation,\nbecomes, after some calculation, \nwhere formula_50\n\nThe goal being to find a curve for which the value of\nis stationary, where\nsuch goal can be accomplished by calculating the Euler–Lagrange equation for \"f\", which is \n\nSubstituting the expression of \"f\" into the Euler–Lagrange equation (which makes the value of the integral \"l\" stationary), gives\n\nNow calculate the derivatives:\nformula_55\n\nformula_56\n\nformula_57\n\nformula_58\n\nformula_59\n\nformula_60\n\nformula_62\n\nformula_63\n\nThis is just one step away from the geodesic equation.\n\nIf the parameter \"s\" is chosen to be affine, then the right side the above equation vanishes (because formula_64 is constant). Finally, we have the geodesic equation\n\n\n"}
{"id": "379733", "url": "https://en.wikipedia.org/wiki?curid=379733", "title": "Great-circle distance", "text": "Great-circle distance\n\nThe great-circle distance or orthodromic distance is the shortest distance between two points on the surface of a sphere, measured along the surface of the sphere (as opposed to a straight line through the sphere's interior). The distance between two points in Euclidean space is the length of a straight line between them, but on the sphere there are no straight lines. In spaces with curvature, straight lines are replaced by geodesics. Geodesics on the sphere are circles on the sphere whose centers coincide with the center of the sphere, and are called \"great circles\".\n\nThe determination of the great-circle distance is part of the more general problem of great-circle navigation, which also computes the azimuths at the end points and intermediate way-points.\n\nThrough any two points on a sphere that are not directly opposite each other, there is a unique great circle. The two points separate the great circle into two arcs. The length of the shorter arc is the great-circle distance between the points. A great circle endowed with such a distance is called a Riemannian circle in Riemannian geometry.\n\nBetween two points that are directly opposite each other, called \"antipodal points\", there are infinitely many great circles, and all great circle arcs between antipodal points have a length of half the circumference of the circle, or formula_1, where \"r\" is the radius of the sphere.\n\nThe Earth is nearly spherical (see Earth radius), so great-circle distance formulas give the distance between points on the surface of the Earth correct to within about 0.5%. (See .)\n\nLet formula_2 and formula_3 be the geographical latitude and longitude in radians of two points 1 and 2, and formula_4 be their absolute differences; then formula_5, the central angle between them, is given by the spherical law of cosines if one of the poles is used as an auxiliary third point on the sphere:\n\nThe problem is normally expressed in terms of finding the central angle formula_5. Given this angle in radians, the actual arc length \"d\" on a sphere of radius \"r\" can be trivially computed as\n\nOn computer systems with low floating-point precision, the spherical law of cosines formula can have large rounding errors if the distance is small (if the two points are a kilometer apart on the surface of the Earth, the cosine of the central angle comes out 0.99999999). For modern 64-bit floating-point numbers, the spherical law of cosines formula, given above, does not have serious rounding errors for distances larger than a few meters on the surface of the Earth. The haversine formula is numerically better-conditioned for small distances:\n\nHistorically, the use of this formula was simplified by the availability of tables for the haversine function: hav(\"θ\") = sin(\"θ\"/2).\n\nAlthough this formula is accurate for most distances on a sphere, it too suffers from rounding errors for the special (and somewhat unusual) case of antipodal points (on opposite ends of the sphere). A formula that is accurate for all distances is the following special case of the Vincenty formula for an ellipsoid with equal major and minor axes:\n\nAnother representation of similar formulas, but using normal vectors instead of latitude and longitude to describe the positions, is found by means of 3D vector algebra, utilizing the dot product, cross product, or a combination:\n\nwhere formula_12 and formula_13 are the normals to the ellipsoid at the two positions 1 and 2. Similarly to the equations above based on latitude and longitude, the expression based on arctan is the only one that is well-conditioned for all angles. The expression based on arctan requires the magnitude of the cross product over the dot product.\n\nA line through three-dimensional space between points of interest on a spherical Earth is the chord of the great circle between the points. The central angle between the two points can be determined from the chord length. The great circle distance is proportional to the central angle.\n\nThe great circle chord length, formula_14, may be calculated as follows for the corresponding unit sphere, by means of Cartesian subtraction:\n\nThe central angle is:\n\nThe shape of the Earth closely resembles a flattened sphere (a spheroid) with equatorial radius formula_17 of 6378.137 km; distance formula_18 from the center of the spheroid to each pole is 6356.752 km. When calculating the length of a short north-south line at the equator, the circle that best approximates that line has a radius of formula_19 (which equals the meridian's semi-latus rectum), or 6335.439 km, while the spheroid at the poles is best approximated by a sphere of radius formula_20, or 6399.594 km, a 1% difference. So long as a spherical Earth is assumed, any single formula for distance on the Earth is only guaranteed correct within 0.5% (though better accuracy is possible if the formula is only intended to apply to a limited area). Using the mean earth radius, formula_21 (for the WGS84 ellipsoid) means that in the limit of small flattening, the mean square relative error in the estimates for distance is minimized.\n\n\n"}
{"id": "5224898", "url": "https://en.wikipedia.org/wiki?curid=5224898", "title": "Grundy number", "text": "Grundy number\n\nIn graph theory, the Grundy number or Grundy chromatic number of an undirected graph is the maximum number of colors that can be used by a greedy coloring strategy that considers the vertices of the graph in sequence and assigns each vertex its first available color, using a vertex ordering chosen to use as many colors as possible. Grundy numbers are named after P. M. Grundy, who studied an analogous concept for directed graphs in 1939. The undirected version was introduced by .\n\nFor example, for a path graph with four vertices, the chromatic number is two but the Grundy number is three: if the two endpoints of the path are colored first, the greedy coloring algorithm will use three colors for the whole graph.\n\ndefines a sequence of graphs called -\"atoms\", with the property that a graph has Grundy number at least if and only if it contains a -atom.\nEach -atom is formed from an independent set and a -atom, by adding one edge from each vertex of the -atom to a vertex of the independent set, in such a way that each member of the independent set has at least one edge incident to it.\nA Grundy coloring of a -atom can be obtained by coloring the independent set first with the smallest-numbered color, and then coloring the remaining -atom with an additional colors.\nFor instance, the only 1-atom is a single vertex, and the only 2-atom is a single edge, but there are two possible 3-atoms: a triangle and a four-vertex path.\n\nFor a graph with vertices and degeneracy , the Grundy number is . In particular, for graphs of bounded degeneracy (such as planar graphs) or graphs for which the chromatic number and degeneracy are bounded within constant factors of each other (such as chordal graphs) the Grundy number and chromatic number are within a logarithmic factor of each other. For interval graphs, the chromatic number and Grundy number are within a factor of 8 of each other.\n\nTesting whether the Grundy number of a given graph is at least , for a fixed constant , can be performed in polynomial time, by searching for all possible -atoms that might be subgraphs of the given graph. However, this algorithm is not fixed-parameter tractable, because the exponent in its running time depends on . When is an input variable rather than a parameter, the problem is NP-complete. The Grundy number is at most one plus the maximum degree of the graph, and it remains NP-complete to test whether it equals one plus the maximum degree. There exists a constant such that it is NP-hard under randomized reductions to approximate the Grundy number to within an approximation ratio better than .\n\nThere is an exact exponential time algorithm for the Grundy number that runs in time .\n\nFor trees, and graphs of bounded treewidth, the Grundy number may be unboundedly large. Nevertheless, the Grundy number can be computed in polynomial time for trees,\nand is fixed-parameter tractable when parameterized by both the treewidth and the Grundy number, although (assuming the exponential time hypothesis) the dependence on treewidth must be greater than singly exponential. When parameterized by the Grundy number itself, it can be computed in fixed-parameter tractable time for chordal graphs and claw-free graphs, and also (using general results on subgraph isomorphism in sparse graphs to search for atoms) for graphs of bounded expansion.\n\nA graph is called well-colored if its Grundy number equals its chromatic number. Testing whether a graph is well-colored is coNP-complete. The hereditarily well-colored graphs (graphs for which every induced subgraph is well-colored) are exactly the cographs, the graphs that do not have a four-vertex path as an induced subgraph.\n"}
{"id": "15454890", "url": "https://en.wikipedia.org/wiki?curid=15454890", "title": "History of quaternions", "text": "History of quaternions\n\nIn mathematics, quaternions are a non-commutative number system that extends the complex numbers. Quaternions and their applications to rotations were first described in print by Olinde Rodrigues in all but name in 1840, but independently discovered by Irish mathematician Sir William Rowan Hamilton in 1843 and applied to mechanics in three-dimensional space. They find uses in both theoretical and applied mathematics, in particular for calculations involving three-dimensional rotations.\n\nIn 1843, Hamilton knew that the complex numbers could be viewed as points in a plane and that they could be added and multiplied together using certain geometric operations. Hamilton sought to find a way to do the same for points in space. Points in space can be represented by their coordinates, which are triples of numbers and have an obvious addition, but Hamilton had difficulty defining the appropriate multiplication.\n\nAccording to a letter Hamilton wrote later to his son Archibald:\nEvery morning in the early part of October 1843, on my coming down to breakfast, your brother William Edwin and yourself used to ask me: \"Well, Papa, can you multiply triples?\" Whereto I was always obliged to reply, with a sad shake of the head, \"No, I can only add and subtract them.\"\n\nOn October 16, 1843, Hamilton and his wife took a walk along the Royal Canal in Dublin. While they walked across Brougham Bridge (now Broom Bridge), a solution suddenly occurred to him. While he could not \"multiply triples\", he saw a way to do so for \"quadruples\". By using three of the numbers in the quadruple as the points of a coordinate in space, Hamilton could represent points in space by his new system of numbers. He then carved the basic rules for multiplication into the bridge:\n\nHamilton called a quadruple with these rules of multiplication a \"quaternion\", and he devoted the remainder of his life to studying and teaching them. From 1844 to 1850 Philosophical Magazine communicated Hamilton's exposition of quaternions. In 1853 he issued \"Lectures on Quaternions\", a comprehensive treatise that also described biquaternions. The facility of the algebra in expressing geometric relationships led to broad acceptance of the method, several compositions by other authors, and stimulation of applied algebra generally. As mathematical terminology has grown since that time, and usage of some terms has changed, the traditional expressions are referred to classical Hamiltonian quaternions.\n\nHamilton's innovation consisted of expressing quaternions as an algebra over R. The formulae for the multiplication of quaternions are implicit in the four squares formula devised by Leonhard Euler in 1748; Olinde Rodrigues applied this formula to representing rotations in 1840.\n\nThe special claims of quaternions as the algebra of four-dimensional space were challenged by James Cockle with his exhibits in 1848 and 1849 of tessarines and coquaternions as alternatives. Nevertheless, these new algebras from Cockle were, in fact, to be found inside Hamilton’s biquaternions. From Italy, in 1858 Giusto Bellavitis responded to connect Hamilton’s vector theory with his theory of equipollences of directed line segments.\n\nJules Hoüel led the response from France in 1874 with a textbook on the elements of quaternions. To ease the study of versors, he introduced \"biradials\" to designate great circle arcs on the sphere. Then the quaternion algebra provided the foundation for spherical trigonometry introduced in chapter 9. Hoüel replaced Hamilton’s basis vectors i,j,k with i, i, and i.\nThe variety of typefaces (fonts) available led Hoüel to another notational innovation: \"A\" designates a point, \"a\" and formula_2 are algebraic quantities, and in the equation for a quaternion\nformula_4 is a vector and α is an angle. This style of quaternion exposition was perpetuated by Charles-Ange Laisant and Alexander Macfarlane.\n\nWilliam K. Clifford expanded the types of biquaternions, and explored elliptic space, a geometry in which the points can be viewed as versors. Fascination with quaternions began before the language of set theory and mathematical structures was available. In fact, there was little mathematical notation before the Formulario mathematico. The quaternions stimulated these advances: For example, the idea of a vector space borrowed Hamilton’s term but changed its meaning. Under the modern understanding, any quaternion is a vector in four-dimensional space. (Hamilton’s vectors lie in the subspace with scalar part zero.)\n\nSince quaternions demand their readers to imagine four dimensions, there is a metaphysical aspect to their invocation. Quaternions are a philosophical object. Setting quaternions before freshmen students of engineering asks too much. Yet the utility of dot products and cross products in three-dimensional space, for illustration of processes, calls for the uses of these operations which are cut out of the quaternion product. Thus Willard Gibbs and Oliver Heaviside made this accommodation, for pragmatism, to avoid the distracting superstructure.\n\nFor mathematicians the quaternion structure became familiar and lost its status as something mathematically interesting. Thus in England, when Arthur Buchheim prepared a paper on biquaternions, it was published in the American Journal of Mathematics since some novelty in the subject lingered there. Research turned to hypercomplex numbers more generally. For instance, Thomas Kirkman and Arthur Cayley considered the number of equations between basis vectors would be necessary to determine a unique system. The wide interest that quaternions aroused around the world resulted in the Quaternion Society. In contemporary mathematics, the division ring of quaternions exemplifies an algebra over a field.\n\n\nOctonions were developed independently by Arthur Cayley in 1845 and John T. Graves, a friend of Hamilton's. Graves had interested Hamilton in algebra, and responded to his discovery of quaternions with \"If with your alchemy you can make three pounds of gold [the three imaginary units], why should you stop there?\"\n\nTwo months after Hamilton's discovery of quaternions, Graves wrote Hamilton on December 26, 1843 presenting a kind of double quaternion that is called an \"octonion\", and showed that they were what we now call a normed division algebra; Graves called them \"octaves\". Hamilton needed a way to distinguish between two different types of double quaternions, the associative biquaternions and the octaves. He spoke about them to the Royal Irish Society and credited his friend Graves for the discovery of the second type of double quaternion. observed in reply that they were not associative, which may have been the invention of the concept. He also promised to get Graves' work published, but did little about it; Cayley, working independently of Graves, but inspired by Hamilton's publication of his own work, published on octonions in March 1845 – as an appendix to a paper on a different subject. Hamilton was stung into protesting Graves' priority in discovery, if not publication; nevertheless, octonions are known by the name Cayley gave them – or as \"Cayley numbers\".\n\nThe major deduction from the existence of octonions was the eight squares theorem, which follows directly from the product rule from octonions, had also been previously discovered as a purely algebraic identity, by Carl Ferdinand Degen in 1818. This sum-of-squares identity is characteristic of composition algebra, a feature of complex numbers, quaternions, and octonions.\n\nQuaternions continued to be a well-studied \"mathematical\" structure in the twentieth century, as the third term in the Cayley–Dickson construction of hypercomplex number systems over the reals, followed by the octonions and the sedenions; they are also useful tool in number theory, particularly in the study of the representation of numbers as sums of squares. The group of eight basic unit quaternions, positive and negative, the quaternion group, is also the simplest non-commutative Sylow group.\n\nThe study of integral quaternions began with Rudolf Lipschitz in 1886, whose system was later simplified by Leonard Eugene Dickson; but the modern system was published by Adolf Hurwitz in 1919. The difference between them consists of which quaternions are accounted integral: Lipschitz included only those quaternions with integral coordinates, but Hurwitz added those quaternions \"all four\" of whose coordinates are half-integers. Both systems are closed under subtraction and multiplication, and are therefore rings, but Lipschitz's system does not permit unique factorization, while Hurwitz's does.\n\nQuaternions are a concise method of representing the automorphisms of three- and four-dimensional spaces. They have the technical advantage that unit quaternions form the simply connected cover of the space of three-dimensional rotations.\n\nFor this reason, quaternions are used in computer graphics, control theory, robotics, signal processing, attitude control, physics, bioinformatics, and orbital mechanics. For example, it is common for spacecraft attitude-control systems to be commanded in terms of quaternions. \"Tomb Raider\" (1996) is often cited as the first mass-market computer game to have used quaternions to achieve smooth 3D rotation. Quaternions have received another boost from number theory because of their relation to quadratic forms.\n\nSince 1989, the Department of Mathematics of the National University of Ireland, Maynooth has organized a pilgrimage, where scientists (including physicists Murray Gell-Mann in 2002, Steven Weinberg in 2005, Frank Wilczek in 2007, and mathematician Andrew Wiles in 2003) take a walk from Dunsink Observatory to the Royal Canal bridge where, unfortunately, no trace of Hamilton's carving remains.\n\n"}
{"id": "42719930", "url": "https://en.wikipedia.org/wiki?curid=42719930", "title": "Idempotent relation", "text": "Idempotent relation\n\nIn mathematics, an idempotent binary relation \"R\" ⊆ \"X\" × \"X\" is one for which \"R\" ∘ \"R\" = \"R\". This notion generalizes that of an idempotent function to relations. Each idempotent relation is necessarily transitive, as the latter means \"R\" ∘ \"R\" ⊆ \"R\".\n\nFor example, the relation < on ℚ is idempotent. In contrast, < on ℤ is not, since (<) ∘ (<) ⊇ (<) does not hold: e.g. 1 < 2, but 1 < \"x\" < 2 is false for every \"x\" ∈ ℤ.\n\nIdempotent relations have been used as an example to illustrate the application of Mechanized Formalisation\nof mathematics using the interactive theorem prover Isabelle/HOL. Besides checking the mathematical properties of finite idempotent relations, an algorithm for counting the number of idempotent relations has been derived in Isabelle/HOL.\n"}
{"id": "5473033", "url": "https://en.wikipedia.org/wiki?curid=5473033", "title": "Inhabited set", "text": "Inhabited set\n\nIn constructive mathematics, a set \"A\" is inhabited if there exists an element formula_1. In classical mathematics, this is the same as the set being nonempty; however, this equivalence is not valid in intuitionistic logic.\n\nIn classical mathematics, a set is inhabited if and only if it is not the empty set. These definitions diverge in constructive mathematics, however. A set \"A\" is \"nonempty\" if it is not empty, that is, if \nIt is \"inhabited\" if \nIn intuitionistic logic, the negation of a universal quantifier is weaker than an existential quantifier, not equivalent to it as in classical logic.\n\nBecause inhabited sets are the same as nonempty sets in classical logic, it is not possible to produce a model in the classical sense that contains a nonempty set \"X\" but does not satisfy \"\"X\" is inhabited\". But it is possible to construct a Kripke model \"M\" that satisfies \"\"X\" is nonempty\" without satisfying \"\"X\" is inhabited\". Because an implication is provable in intuitionistic logic if and only if it is true in every Kripke model, this means that one cannot prove in this logic that \"\"X\" is nonempty\" implies \"\"X\" is inhabited\".\n\nThe possibility of this construction relies on the intuitionistic interpretation of the existential quantifier. In an intuitionistic setting, in order for formula_4 to hold, for some formula formula_5, it is necessary for a specific value of \"z\" satisfying formula_5 to be known.\n\nFor example, consider a subset \"X\" of {0,1} specified by the following rule: 0 belongs to \"X\" if and only if the Riemann hypothesis is true, and 1 belongs to \"X\" if and only if the Riemann hypothesis is false. If we assume that Riemann hypothesis is either true or false, then \"X\" is not empty, but any constructive proof that \"X\" is inhabited would either prove that 0 is in \"X\" or that 1 is in \"X\". Thus a constructive proof that \"X\" is inhabited would determine the truth value of the Riemann hypothesis, which is not known, By replacing the Riemann hypothesis in this example by a generic proposition, one can construct a Kripke model with a set that is neither empty nor inhabited (even if the Riemann hypothesis itself is ever proved or refuted).\n\n"}
{"id": "14961545", "url": "https://en.wikipedia.org/wiki?curid=14961545", "title": "Iwasawa manifold", "text": "Iwasawa manifold\n\nIn mathematics, in the field of differential geometry, an Iwasawa manifold is a compact quotient of a 3-dimensional complex Heisenberg group by a cocompact, discrete subgroup. An \nIwasawa manifold is a nilmanifold, of real dimension 6.\n\nIwasawa manifolds give examples where the first two terms \"E\" and \"E\" of the Frölicher spectral sequence are not isomorphic. \n\nAs a complex manifold, such an Iwasawa manifold is an important example of\na compact complex manifold which does not admit any Kähler metric.\n\n"}
{"id": "305064", "url": "https://en.wikipedia.org/wiki?curid=305064", "title": "Jan Łukasiewicz", "text": "Jan Łukasiewicz\n\nJan Łukasiewicz (; 21 December 1878 – 13 February 1956) was a Polish logician and philosopher born in Lemberg, a city in the Galician kingdom of Austria-Hungary (now Lviv, Ukraine). His work centred on philosophical logic, mathematical logic, and history of logic. He thought innovatively about traditional propositional logic, the principle of non-contradiction and the law of excluded middle. Modern work on Aristotle's logic builds on the tradition started in 1951 with the establishment by Łukasiewicz of a revolutionary paradigm. The Łukasiewicz approach was reinvigorated in the early 1970s in a series of papers by John Corcoran and Timothy Smiley—which inform modern translations of \"Prior Analytics\" by Robin Smith in 1989 and Gisela Striker in 2009. Łukasiewicz is regarded as one of the most important historians of logic.\n\nHe grew up in Lwów and was the only child of Paweł Łukasiewicz, a captain in the Austrian army, and Leopoldina, \"née\" Holtzer, the daughter of a civil servant. His family was Roman Catholic.\n\nHe finished his \"gymnasium\" studies in philology and in 1897 went on to Lwów University (which, before the Polish partitions, had been in Poland), where he studied philosophy and mathematics. He was a pupil of philosopher Kazimierz Twardowski.\n\nIn 1902 he received a Doctor of Philosophy degree under the patronage of Emperor Franz Joseph I of Austria, who gave him a special doctoral ring with diamonds.\n\nHe spent three years as a private teacher, and in 1905 he received a scholarship to complete his philosophy studies at the University of Berlin and the University of Louvain in Belgium.\n\nŁukasiewicz continued studying for his habilitation qualification and in 1906 submitted his thesis to the University of Lwów. In 1906 he was appointed a lecturer at the University of Lwów where he was eventually appointed Extraordinary Professor by Emperor Franz Joseph I. He taught there until the First World War.\n\nIn 1915 he was invited to lecture as a full professor at the University of Warsaw which had re-opened after being closed down by the Tsarist government in the 19th century.\n\nIn 1919 Łukasiewicz left the university to serve as Polish Minister of Religious Denominations and Public Education in the Paderewski government until 1920. Łukasiewicz led the development of a Polish curriculum replacing the Russian, German and Austrian curricula previously used in partitioned Poland. The Łukasiewicz curriculum emphasized the early acquisition of logical and mathematical concepts.\n\nIn 1928 he married Regina Barwińska.\n\nHe remained a professor at the University of Warsaw from 1920 until 1939 when the family house was destroyed by German bombs and the university was closed under German occupation. He had been a rector of the university twice. In this period Łukasiewicz and Stanisław Leśniewski founded the Lwów–Warsaw school of logic which was later made internationally famous by Alfred Tarski who had been Leśniewski's student.\n\nAt the beginning of World War II he worked at the Warsaw Underground University as part of the secret system of education in Poland during World War II.\n\nHe and his wife wanted to move to Switzerland but couldn't get permission from the German authorities. Instead, in the summer of 1944, they left Poland with the help of Heinrich Scholz and spent the last few months of the war in Münster, Germany hoping to somehow go on further, perhaps to Switzerland.\n\nFollowing the war, he emigrated to Ireland and worked at University College Dublin (UCD) until his death.\n\nJan Łukasiewicz's papers (post-1945 only) are held by the University of Manchester Library.\n\nA number of axiomatizations of classical propositional logic are due to Łukasiewicz. A particularly elegant axiomatization features a mere three axioms and is still invoked to the present day. He was a pioneer investigator of multi-valued logics; his three-valued propositional calculus, introduced in 1917, was the first explicitly axiomatized non-classical logical calculus. He wrote on the philosophy of science, and his approach to the making of scientific theories was similar to the thinking of Karl Popper.\n\nŁukasiewicz invented the Polish notation (named after his nationality) for the logical connectives around 1920. There is a quotation from his paper, \"Remarks on Nicod's Axiom and on \"Generalizing Deduction\"\", page 180;\n\n\"I came upon the idea of a parenthesis-free notation in 1924. I used that notation for the first time in my article Łukasiewicz(1), p. 610, footnote.\"\n\nThe reference cited by Łukasiewicz above is apparently a lithographed report in Polish. The referring paper by Łukasiewicz \"Remarks on Nicod's Axiom and on \"Generalizing Deduction\"\", originally published in Polish in 1931, was later reviewed by H. A. Pogorzelski in the \"Journal of Symbolic Logic\" in 1965.\n\nIn Łukasiewicz 1951 book, \"Aristotle's Syllogistic from the Standpoint of Modern Formal Logic\", he mentions that the principle of his notation was to write the functors before the arguments to avoid brackets and that he had employed his notation in his logical papers since 1929. He then goes on to cite, as an example, a 1930 paper he wrote with Alfred Tarski on the sentential calculus.\n\nThis notation is the root of the idea of the \"recursive stack\", a last-in, first-out computer memory store proposed by several researchers including Turing, Bauer and Hamblin, and first implemented in 1957. In 1960, Łukasiewicz notation concepts and stacks were used as the basis of the Burroughs B5000 computer designed by Robert S. Barton and his team at Burroughs Corporation in Pasadena, California. The concepts also led to the design of the English Electric multi-programmed KDF9 computer system of 1963, which had two such hardware register stacks. A similar concept underlies the reverse Polish notation (\"RPN\", a postfix notation) of the Friden EC-130 calculator and its successors, many Hewlett Packard calculators, the Forth programming language, and the PostScript page description language.\n\nIn 2008 the Polish Information Processing Society established the Jan Łukasiewicz Award, to be presented to the most innovative Polish IT companies.\n\nFrom 1999 to 2004, the Department of Computer Science building at UCD was called the Łukasiewicz Building, until all campus buildings were renamed after the disciplines they housed.\n\n\n\n\n\n\n\n"}
{"id": "37332", "url": "https://en.wikipedia.org/wiki?curid=37332", "title": "Josiah Willard Gibbs", "text": "Josiah Willard Gibbs\n\nJosiah Willard Gibbs (February 11, 1839 – April 28, 1903) was an American scientist who made important theoretical contributions to physics, chemistry, and mathematics. His work on the applications of thermodynamics was instrumental in transforming physical chemistry into a rigorous inductive science. Together with James Clerk Maxwell and Ludwig Boltzmann, he created statistical mechanics (a term that he coined), explaining the laws of thermodynamics as consequences of the statistical properties of ensembles of the possible states of a physical system composed of many particles. Gibbs also worked on the application of Maxwell's equations to problems in physical optics. As a mathematician, he invented modern vector calculus (independently of the British scientist Oliver Heaviside, who carried out similar work during the same period).\n\nIn 1863, Yale awarded Gibbs the first American doctorate in engineering. After a three-year sojourn in Europe, Gibbs spent the rest of his career at Yale, where he was professor of mathematical physics from 1871 until his death. Working in relative isolation, he became the earliest theoretical scientist in the United States to earn an international reputation and was praised by Albert Einstein as \"the greatest mind in American history\". In 1901, Gibbs received what was then considered the highest honor awarded by the international scientific community, the Copley Medal of the Royal Society of London, \"for his contributions to mathematical physics\".\n\nCommentators and biographers have remarked on the contrast between Gibbs's quiet, solitary life in turn of the century New England and the great international impact of his ideas. Though his work was almost entirely theoretical, the practical value of Gibbs's contributions became evident with the development of industrial chemistry during the first half of the 20th century. According to Robert A. Millikan, in pure science, Gibbs \"did for statistical mechanics and for thermodynamics what Laplace did for celestial mechanics and Maxwell did for electrodynamics, namely, made his field a well-nigh finished theoretical structure\".\n\nGibbs was born in New Haven, Connecticut. He belonged to an old Yankee family that had produced distinguished American clergymen and academics since the 17th century. He was the fourth of five children and the only son of Josiah Willard Gibbs Sr., and his wife Mary Anna, \"née\" Van Cleve. On his father's side, he was descended from Samuel Willard, who served as acting President of Harvard College from 1701 to 1707. On his mother's side, one of his ancestors was the Rev. Jonathan Dickinson, the first president of the College of New Jersey (later Princeton University). Gibbs's given name, which he shared with his father and several other members of his extended family, derived from his ancestor Josiah Willard, who had been Secretary of the Province of Massachusetts Bay in the 18th century.\n\nThe elder Gibbs was generally known to his family and colleagues as \"Josiah\", while the son was called \"Willard\". Josiah Gibbs was a linguist and theologian who served as professor of sacred literature at Yale Divinity School from 1824 until his death in 1861. He is chiefly remembered today as the abolitionist who found an interpreter for the African passengers of the ship \"Amistad\", allowing them to testify during the trial that followed their rebellion against being sold as slaves.\n\nWillard Gibbs was educated at the Hopkins School and entered Yale College in 1854 at the age of 15. At Yale, Gibbs received prizes for excellence in mathematics and Latin, and he graduated in 1858, near the top of his class. He remained at Yale as a graduate student at the Sheffield Scientific School. At age 19, soon after his graduation from college, Gibbs was inducted into the Connecticut Academy of Arts and Sciences, a scholarly institution composed primarily of members of the Yale faculty.\n\nRelatively few documents from the period survive and it is difficult to reconstruct the details of Gibbs's early career with precision. In the opinion of biographers, Gibbs's principal mentor and champion, both at Yale and in the Connecticut Academy, was probably the astronomer and mathematician Hubert Anson Newton, a leading authority on meteors, who remained Gibbs's lifelong friend and confidant. After the death of his father in 1861, Gibbs inherited enough money to make him financially independent.\n\nRecurrent pulmonary trouble ailed the young Gibbs and his physicians were concerned that he might be susceptible to tuberculosis, which had killed his mother. He also suffered from astigmatism, whose treatment was then still largely unfamiliar to oculists, so that Gibbs had to diagnose himself and grind his own lenses. Though in later years he used glasses only for reading or other close work, Gibbs's delicate health and imperfect eyesight probably explain why he did not volunteer to fight in the Civil War of 1861–65. He was not conscripted and he remained at Yale for the duration of the war.\n\nIn 1863, Gibbs received the first Doctorate of Philosophy (Ph.D.) in engineering granted in the US, for a thesis entitled \"On the Form of the Teeth of Wheels in Spur Gearing\", in which he used geometrical techniques to investigate the optimum design for gears. In 1861, Yale had become the first US university to offer a Ph.D. degree and Gibbs's was only the fifth Ph.D. granted in the US in any subject.\n\nAfter graduation, Gibbs was appointed as tutor at the College for a term of three years. During the first two years, he taught Latin and during the third year, he taught \"natural philosophy\" (i.e., physics). In 1866, he patented a design for a railway brake and read a paper before the Connecticut Academy, entitled \"The Proper Magnitude of the Units of Length\", in which he proposed a scheme for rationalizing the system of units of measurement used in mechanics.\n\nAfter his term as tutor ended, Gibbs traveled to Europe with his sisters. They spent the winter of 1866–67 in Paris, where Gibbs attended lectures at the Sorbonne and the Collège de France, given by such distinguished mathematical scientists as Joseph Liouville and Michel Chasles. Having undertaken a punishing regime of study, Gibbs caught a serious cold and a doctor, fearing tuberculosis, advised him to rest on the Riviera, where he and his sisters spent several months and where he made a full recovery.\n\nMoving to Berlin, Gibbs attended the lectures taught by mathematicians Karl Weierstrass and Leopold Kronecker, as well as by chemist Heinrich Gustav Magnus. In August 1867, Gibbs's sister Julia was married in Berlin to Addison Van Name, who had been Gibbs's classmate at Yale. The newly married couple returned to New Haven, leaving Gibbs and his sister Anna in Germany. In Heidelberg, Gibbs was exposed to the work of physicists Gustav Kirchhoff and Hermann von Helmholtz, and chemist Robert Bunsen. At the time, German academics were the leading authorities in the natural sciences, especially chemistry and thermodynamics.\n\nGibbs returned to Yale in June 1869 and briefly taught French to engineering students. It was probably also around this time that he worked on a new design for a steam-engine governor, his last significant investigation in mechanical engineering. In 1871, he was appointed Professor of Mathematical Physics at Yale, the first such professorship in the United States. Gibbs, who had independent means and had yet to publish anything, was assigned to teach graduate students exclusively and was hired without salary.\n\nGibbs published his first work in 1873. His papers on the geometric representation of thermodynamic quantities appeared in the \"Transactions of the Connecticut Academy\". These papers introduced the use of different type phase diagrams, which were his favorite aids to the imagination process when doing research, rather than the mechanical models, such as the ones that Maxwell used in constructing his electromagnetic theory, which might not completely represent their corresponding phenomena. Although the journal had few readers capable of understanding Gibbs's work, he shared reprints with correspondents in Europe and received an enthusiastic response from James Clerk Maxwell at Cambridge. Maxwell even made, with his own hands, a clay model illustrating Gibbs's construct. He then produced two plaster casts of his model and mailed one to Gibbs. That cast is on display at the Yale physics department.\n\nMaxwell included a chapter on Gibbs's work in the next edition of his \"Theory of Heat\", published in 1875. He explained the usefulness of Gibbs's graphical methods in a lecture to the Chemical Society of London and even referred to it in the article on \"Diagrams\" that he wrote for the \"Encyclopædia Britannica\". Prospects of collaboration between him and Gibbs were cut short by Maxwell's early death in 1879, aged 48. The joke later circulated in New Haven that \"only one man lived who could understand Gibbs's papers. That was Maxwell, and now he is dead.\"\n\nGibbs then extended his thermodynamic analysis to multi-phase chemical systems (i.e., to systems composed of more than one form of matter) and considered a variety of concrete applications. He described that research in a monograph titled \"On the Equilibrium of Heterogeneous Substances\", published by the Connecticut Academy in two parts that appeared respectively in 1875 and 1878. That work, which covers about three hundred pages and contains exactly seven hundred numbered mathematical equations, begins with a quotation from Rudolf Clausius that expresses what would later be called the first and second laws of thermodynamics: \"The energy of the world is constant. The entropy of the world tends towards a maximum.\"\n\nGibbs's monograph rigorously and ingeniously applied his thermodynamic techniques to the interpretation of physico-chemical phenomena, explaining and relating what had previously been a mass of isolated facts and observations. The work has been described as \"the \"Principia\" of thermodynamics\" and as a work of \"practically unlimited scope\". It solidly laid the foundation for physical Chemistry. Wilhelm Ostwald, who translated Gibbs's monograph into German, referred to Gibbs as the \"founder of chemical energetics\". According to modern commentators,\n\nGibbs continued to work without pay until 1880, when the new Johns Hopkins University in Baltimore, Maryland offered him a position paying $3,000 per year. In response, Yale offered him an annual salary of $2,000, which he was content to accept.\n\nFrom 1880 to 1884, Gibbs worked on developing the exterior algebra of Hermann Grassmann into a vector calculus well-suited to the needs of physicists. With this object in mind, Gibbs distinguished between the dot and cross products of two vectors and introduced the concept of dyadics. Similar work was carried out independently, and at around the same time, by the British mathematical physicist and engineer Oliver Heaviside. Gibbs sought to convince other physicists of the convenience of the vectorial approach over the quaternionic calculus of William Rowan Hamilton, which was then widely used by British scientists. This led him, in the early 1890s, to a controversy with Peter Guthrie Tait and others in the pages of \"Nature\".\n\nGibbs's lecture notes on vector calculus were privately printed in 1881 and 1884 for the use of his students, and were later adapted by Edwin Bidwell Wilson into a textbook, \"Vector Analysis\", published in 1901. That book helped to popularize the \"del\" notation that is widely used today in electrodynamics and fluid mechanics. In other mathematical work, he re-discovered the \"Gibbs phenomenon\" in the theory of Fourier series (which, unbeknownst to him and to later scholars, had been described fifty years before by an obscure English mathematician, Henry Wilbraham).\n\nFrom 1882 to 1889, Gibbs wrote five papers on physical optics, in which he investigated birefringence and other optical phenomena and defended Maxwell's electromagnetic theory of light against the mechanical theories of Lord Kelvin and others. In his work on optics, just as much as in his work on thermodynamics, Gibbs deliberately avoided speculating about the microscopic structure of matter and purposefully confined his research problems to those that can be solved from broad general principles and experimentally confirmed facts.The methods that he used were highly original and the obtained results showed decisively the correctness of Maxwell's electromagnetic theory.\n\nGibbs coined the term \"statistical mechanics\" and introduced key concepts in the corresponding mathematical description of physical systems, including the notions of chemical potential (1876), and statistical ensemble (1902). Gibbs's derivation of the laws of thermodynamics from the statistical properties of systems consisting of many particles was presented in his highly influential textbook \"Elementary Principles in Statistical Mechanics\", published in 1902, a year before his death.\n\nGibbs's retiring personality and intense focus on his work limited his accessibility to students. His principal protégé was Edwin Bidwell Wilson, who nonetheless explained that \"except in the classroom I saw very little of Gibbs. He had a way, toward the end of the afternoon, of taking a stroll about the streets between his study in the old Sloane Laboratory and his home—a little exercise between work and dinner—and one might occasionally come across him at that time.\" Gibbs did supervise the doctoral thesis on mathematical economics written by Irving Fisher in 1891. After Gibbs's death, Fisher financed the publication of his \"Collected Works\". Another distinguished student was Lee De Forest, later a pioneer of radio technology.\n\nGibbs died in New Haven on April 28, 1903, at the age of 64, the victim of an acute intestinal obstruction. A funeral was conducted two days later at his home on 121 High Street, and his body was buried in the nearby Grove Street Cemetery. In May, Yale organized a memorial meeting at the Sloane Laboratory. The eminent British physicist J. J. Thomson was in attendance and delivered a brief address.\n\nGibbs never married, living all his life in his childhood home with his sister Julia and her husband Addison Van Name, who was the Yale librarian. Except for his customary summer vacations in the Adirondacks (at Keene Valley, New York) and later at the White Mountains (in Intervale, New Hampshire), his sojourn in Europe in 1866–69 was almost the only time that Gibbs spent outside New Haven. He joined Yale's College Church (a Congregational church) at the end of his freshman year and remained a regular attendant for the rest of his life. Gibbs generally voted for the Republican candidate in presidential elections but, like other \"Mugwumps\", his concern over the growing corruption associated with machine politics led him to support Grover Cleveland, a conservative Democrat, in the election of 1884. Little else is known of his religious or political views, which he mostly kept to himself.\n\nGibbs did not produce a substantial personal correspondence and many of his letters were later lost or destroyed. Beyond the technical writings concerning his research, he published only two other pieces: a brief obituary for Rudolf Clausius, one of the founders of the mathematical theory of thermodynamics, and a longer biographical memoir of his mentor at Yale, H. A. Newton. In Edward Bidwell Wilson's view,\n\nAccording to Lynde Wheeler, who had been Gibbs's student at Yale, in his later years Gibbs\n\nHe was a careful investor and financial manager, and at his death in 1903 his estate was valued at $100,000 (roughly $ today). For many years, he served as trustee, secretary, and treasurer of his alma mater, the Hopkins School. US President Chester A. Arthur appointed him as one of the commissioners to the National Conference of Electricians, which convened in Philadelphia in September 1884, and Gibbs presided over one of its sessions. A keen and skilled horseman, Gibbs was seen habitually in New Haven driving his sister's carriage. In an obituary published in the \"American Journal of Science\", Gibbs's former student Henry A. Bumstead referred to Gibbs's personal character:\n\nGibbs's papers from the 1870s introduced the idea of expressing the internal energy \"U\" of a system in terms of the entropy \"S\", in addition to the usual state-variables of volume \"V\", pressure \"p\", and temperature \"T\". He also introduced the concept of the chemical potential formula_1 of a given chemical species, defined to be the rate of the increase in \"U\" associated with the increase in the number \"N\" of molecules of that species (at constant entropy and volume). Thus, it was Gibbs who first combined the first and second laws of thermodynamics by expressing the infinitesimal change in the internal energy, d\"U\", of a closed system in the form:\n\nformula_2\n\nwhere \"T\" is the absolute temperature, \"p\" is the pressure, d\"S\" is an infinitesimal change in entropy and d\"V\" is an infinitesimal change of volume. The last term is the sum, over all the chemical species in a chemical reaction, of the chemical potential, \"μ\", of the \"i\" species, multiplied by the infinitesimal change in the number of moles, d\"N\" of that species. By taking the Legendre transform of this expression, he defined the concepts of enthalpy, \"H\" and Gibbs free energy, \"G\".\nThis compares to the expression for Helmholtz free energy, \"A\".\n\nWhen the Gibbs free energy for a chemical reaction is negative the reaction will proceed spontaneously. When a chemical system is at equilibrium, the change in Gibbs free energy is zero. An equilibrium constant is simply related to the free energy change when the reactants are in their standard states.\nChemical potential is usually defined as partial molar Gibbs free energy.\n\nGibbs also obtained what later came to be known as the \"Gibbs–Duhem equation\".\n\nThe publication of the paper \"On the Equilibrium of Heterogeneous Substances\" (1874–78) is now regarded as a landmark in the development of chemistry. In it, Gibbs developed a rigorous mathematical theory for various transport phenomena, including adsorption, electrochemistry, and the Marangoni effect in fluid mixtures. He also formulated the phase rule\n\nformula_7\n\nfor the number \"F\" of variables that may be independently controlled in an equilibrium mixture of \"C\" components existing in \"P\" phases. Phase rule is very useful in diverse areas, such as metallurgy, mineralogy, and petrology. It can also be applied to various research problems in physical chemistry.\n\nTogether with James Clerk Maxwell and Ludwig Boltzmann, Gibbs founded \"statistical mechanics\", a term that he coined to identify the branch of theoretical physics that accounts for the observed thermodynamic properties of systems in terms of the statistics of ensembles, a collection of many possible states of a system, each assigned a certain probability. He introduced the concept of \"phase of a mechanical system\". He used the concept to define the microcanonical, canonical, and grand canonical ensembles; all related to the Gibbs measure, thus obtaining a more general formulation of the statistical properties of many-particle systems than Maxwell and Boltzmann had achieved before him.\n\nAccording to Henri Poincaré, writing in 1904, even though Maxwell and Boltzmann had previously explained the irreversibility of macroscopic physical processes in probabilistic terms, \"the one who has seen it most clearly, in a book too little read because it is a little difficult to read, is Gibbs, in his \"Elementary Principles of Statistical Mechanics\".\" Gibbs's analysis of irreversibility, and his formulation of Boltzmann's H-theorem and of the ergodic hypothesis, were major influences on the mathematical physics of the 20th century.\n\nGibbs was well aware that the application of the equipartition theorem to large systems of classical particles failed to explain the measurements of the specific heats of both solids and gases, and he argued that this was evidence of the danger of basing thermodynamics on \"hypotheses about the constitution of matter\". Gibbs's own framework for statistical mechanics, based on ensembles of macroscopically indistinguishable microstates, could be carried over almost intact after the discovery that the microscopic laws of nature obey quantum rules, rather than the classical laws known to Gibbs and to his contemporaries. His resolution of the so-called \"Gibbs paradox\", about the entropy of the mixing of gases, is now often cited as a prefiguration of the indistinguishability of particles required by quantum physics.\n\nBritish scientists, including Maxwell, had relied on Hamilton's quaternions in order to express the dynamics of physical quantities, like the electric and magnetic fields, having both a magnitude and a direction in three-dimensional space. Gibbs, however, noted that the product of quaternions always had to be separated into two parts: a one-dimensional (scalar) quantity and a three-dimensional vector, so that the use of quaternions introduced mathematical complications and redundancies that could be avoided in the interest of simplicity and to facilitate teaching. He proposed defining distinct dot and cross products for pairs of vectors and introduced the now common notation for them. He was also largely responsible for the development of the vector calculus techniques still used today in electrodynamics and fluid mechanics.\n\nWhile he was working on vector analysis in the late 1870s, Gibbs discovered that his approach was similar to the one that Grassmann had taken in his \"multiple algebra\". Gibbs then sought to publicize Grassmann's work, stressing that it was both more general and historically prior to Hamilton's quaternionic algebra. To establish priority of Grassmann's ideas, Gibbs convinced Grassmann's heirs to seek the publication in Germany of the essay \"Theorie der Ebbe und Flut\" on tides that Grassmann had submitted in 1840 to the faculty at the University of Berlin, in which he had first introduced the notion of what would later be called a vector space (linear space).\n\nAs Gibbs had advocated in the 1880s and 1890s, quaternions were eventually all but abandoned by physicists in favor of the vectorial approach developed by him and, independently, by Oliver Heaviside. Gibbs applied his vector methods to the determination of planetary and comet orbits. He also developed the concept of mutually reciprocal triads of vectors that later proved to be of importance in crystallography.\n\nThough Gibbs's research on physical optics is less well known today than his other work, it made a significant contribution to classical electromagnetism by applying Maxwell's equations to the theory of optical processes such as birefringence, dispersion, and optical activity. In that work, Gibbs showed that those processes could be accounted for by Maxwell's equations without any special assumptions about the microscopic structure of matter or about the nature of the medium in which electromagnetic waves were supposed to propagate (the so-called luminiferous ether). Gibbs also stressed that the absence of a longitudinal electromagnetic wave, which is needed to account for the observed properties of light, is automatically guaranteed by Maxwell's equations (by virtue of what is now called their \"gauge invariance\"), whereas in mechanical theories of light, such as Lord Kelvin's, it must be imposed as an \"ad hoc\" condition on the properties of the aether.\n\nIn his last paper on physical optics, Gibbs concluded that\n\nShortly afterwards, the electromagnetic nature of light was demonstrated by the experiments of Heinrich Hertz in Germany.\n\nGibbs worked at a time when there was little tradition of rigorous theoretical science in the United States. His research was not easily understandable to his students or his colleagues, and he made no effort to popularize his ideas or to simplify their exposition to make them more accessible. His seminal work on thermodynamics was published mostly in the \"Transactions of the Connecticut Academy\", a journal edited by his librarian brother-in-law, which was little read in the USA and even less so in Europe. When Gibbs submitted his long paper on the equilibrium of heterogeneous substances to the Academy, both Elias Loomis and H. A. Newton protested that they did not understand Gibbs's work at all, but they helped to raise the money needed to pay for the typesetting of the many mathematical symbols in the paper. Several Yale faculty members, as well as business and professional men in New Haven, contributed funds for that purpose.\n\nEven though it had been immediately embraced by Maxwell, Gibbs's graphical formulation of the laws of thermodynamics only came into widespread use in the mid 20th century, with the work of László Tisza and Herbert Callen. According to James Gerald Crowther,\n\nOn the other hand, Gibbs did receive the major honors then possible for an academic scientist in the US. He was elected to the National Academy of Sciences in 1879 and received the 1880 Rumford Prize from the American Academy of Arts and Sciences for his work on chemical thermodynamics. He was also awarded honorary doctorates by Princeton University and Williams College.\n\nIn Europe, Gibbs was inducted as honorary member of the London Mathematical Society in 1892 and elected Foreign Member of the Royal Society in 1897. He was elected as corresponding member of the Prussian and French Academies of Science and received honorary doctorates from the universities of Dublin, Erlangen, and Christiania (now Oslo). The Royal Society further honored Gibbs in 1901 with the Copley Medal, then regarded as the highest international award in the natural sciences, noting that he had been \"the first to apply the second law of thermodynamics to the exhaustive discussion of the relation between chemical, electrical and thermal energy and capacity for external work.\" Gibbs, who remained in New Haven, was represented at the award ceremony by Commander Richardson Clover, the US naval attaché in London.\n\nIn his autobiography, mathematician Gian-Carlo Rota tells of casually browsing the mathematical stacks of Sterling Library and stumbling on a handwritten mailing list, attached to some of Gibbs's course notes, which listed over two hundred notable scientists of his day, including Poincaré, Boltzmann, David Hilbert, and Ernst Mach. From this, Rota concluded that Gibbs's work was better known among the scientific elite of his day than the published material suggests. Lynde Wheeler reproduces that mailing list in an appendix to his biography of Gibbs. That Gibbs succeeded in interesting his European correspondents in his work is demonstrated by the fact that his monograph \"On the Equilibrium of Heterogeneous Substances\" was translated into German (then the leading language for chemistry) by Wilhelm Ostwald in 1892 and into French by Henri Louis Le Châtelier in 1899.\n\nGibbs's most immediate and obvious influence was on physical chemistry and statistical mechanics, two disciplines which he greatly helped to found. During Gibbs's lifetime, his phase rule was experimentally validated by Dutch chemist H. W. Bakhuis Roozeboom, who showed how to apply it in a variety of situations, thereby assuring it of widespread use. In industrial chemistry, Gibbs's thermodynamics found many applications during the early 20th century, from electrochemistry to the development of the Haber process for the synthesis of ammonia.\n\nWhen Dutch physicist J. D. van der Waals received the 1910 Nobel Prize \"for his work on the equation of state for gases and liquids\" he acknowledged the great influence of Gibbs's work on that subject. Max Planck received the 1918 Nobel Prize for his work on quantum mechanics, particularly his 1900 paper on Planck's law for quantized black-body radiation. That work was based largely on the thermodynamics of Kirchhoff, Boltzmann, and Gibbs. Planck declared that Gibbs's name \"not only in America but in the whole world will ever be reckoned among the most renowned theoretical physicists of all times.\"\n\nThe first half of the 20th century saw the publication of two influential textbooks that soon came to be regarded as founding documents of chemical thermodynamics, both of which used and extended Gibbs's work in that field: these were \"Thermodynamics and the Free Energy of Chemical Processes\" (1923), by Gilbert N. Lewis and Merle Randall, and \"Modern Thermodynamics by the Methods of Willard Gibbs\" (1933), by Edward A. Guggenheim.\n\nGibbs's work on statistical ensembles, as presented in his 1902 textbook, has had a great impact in both theoretical physics and in pure mathematics. According to mathematical physicist Arthur Wightman,\n\nInitially unaware of Gibbs's contributions in that field, Albert Einstein wrote three papers on statistical mechanics, published between 1902 and 1904. After reading Gibbs's textbook (which was translated into German by Ernst Zermelo in 1905), Einstein declared that Gibbs's treatment was superior to his own and explained that he would not have written those papers if he had known Gibbs's work.\n\nGibbs's early papers on the use of graphical methods in thermodynamics reflect a powerfully original understanding of what mathematicians would later call \"convex analysis\", including ideas that, according to Barry Simon, \"lay dormant for about seventy-five years\". Important mathematical concepts based on Gibbs's work on thermodynamics and statistical mechanics include the Gibbs lemma in game theory, the Gibbs inequality in information theory, as well as Gibbs sampling in computational statistics.\n\nThe development of vector calculus was Gibbs's other great contribution to mathematics. The publication in 1901 of E. B. Wilson's textbook \"Vector Analysis\", based on Gibbs's lectures at Yale, did much to propagate the use of vectorial methods and notation in both mathematics and theoretical physics, definitively displacing the quaternions that had until then been dominant in the scientific literature.\n\nAt Yale, Gibbs was also mentor to Lee De Forest, who went on to invent the triode amplifier and has been called the \"father of radio\". De Forest credited Gibbs's influence for the realization \"that the leaders in electrical development would be those who pursued the higher theory of waves and oscillations and the transmission by these means of intelligence and power.\" Another student of Gibbs who played a significant role in the development of radio technology was Lynde Wheeler.\n\nGibbs also had an indirect influence on mathematical economics. He supervised the thesis of Irving Fisher, who received the first Ph.D. in economics from Yale in 1891. In that work, published in 1892 as \"Mathematical Investigations in the Theory of Value and Prices\", Fisher drew a direct analogy between Gibbsian equilibrium in physical and chemical systems, and the general equilibrium of markets, and he used Gibbs's vectorial notation. Gibbs's protegé Edwin Bidwell Wilson became, in turn, a mentor to leading American economist and Nobel Laureate Paul Samuelson. In 1947, Samuelson published \"Foundations of Economic Analysis\", based on his doctoral dissertation, in which he used as epigraph a remark attributed to Gibbs: \"Mathematics is a language.\" Samuelson later explained that in his understanding of prices his \"debts were not primarily to Pareto or Slutsky, but to the great thermodynamicist, Willard Gibbs of Yale.\"\n\nMathematician Norbert Wiener cited Gibbs's use of probability in the formulation of statistical mechanics as \"the first great revolution of twentieth century physics\" and as a major influence on his conception of cybernetics. Wiener explained in the preface to his book \"The Human Use of Human Beings\" that it was \"devoted to the impact of the Gibbsian point of view on modern life, both through the substantive changes it has made to working science, and through the changes it has made indirectly in our attitude to life in general.\"\n\nWhen the German physical chemist Walther Nernst visited Yale in 1906 to give the Silliman lecture, he was surprised to find no tangible memorial for Gibbs. Nernst donated his $500 lecture fee to the university to help pay for a suitable monument. This was finally unveiled in 1912, in the form of a bronze bas-relief by sculptor Lee Lawrie, installed in the Sloane Physics Laboratory. In 1910, the American Chemical Society established the Willard Gibbs Award for eminent work in pure or applied chemistry. In 1923, the American Mathematical Society endowed the Josiah Willard Gibbs Lectureship, \"to show the public some idea of the aspects of mathematics and its applications\".\n\nIn 1945, Yale University created the J. Willard Gibbs Professorship in Theoretical Chemistry, held until 1973 by Lars Onsager. Onsager, who much like Gibbs, focused on applying new mathematical ideas to problems in physical chemistry, won the 1968 Nobel Prize in chemistry. In addition to establishing the Josiah Willard Gibbs Laboratories and the J. Willard Gibbs Assistant Professorship in Mathematics, Yale has also hosted two symposia dedicated to Gibbs's life and work, one in 1989 and another on the centenary of his death, in 2003. Rutgers University endowed a J. Willard Gibbs Professorship of Thermomechanics, held as of 2014 by Bernard Coleman.\n\nGibbs was elected in 1950 to the Hall of Fame for Great Americans. The oceanographic research ship USNS \"Josiah Willard Gibbs\" (T-AGOR-1) was in service with the United States Navy from 1958 to 1971. Gibbs crater, near the eastern limb of the Moon, was named in the scientist's honor in 1964.\n\nEdward Guggenheim introduced the symbol \"G\" for the Gibbs free energy in 1933, and this was used also by Dirk ter Haar in 1966. This notation is now universal and is recommended by the IUPAC. In 1960, William Giauque and others suggested the name \"gibbs\" (abbreviated gbs.) for the unit of entropy calorie per kelvin, but this usage did not become common, and the corresponding SI unit joule per kelvin carries no special name.\n\nIn 1954, a year before his death, Albert Einstein was asked by an interviewer who were the greatest thinkers that he had known. Einstein replied: \"Lorentz\", adding \"I never met Willard Gibbs; perhaps, had I done so, I might have placed him beside Lorentz.\"\n\nIn 1909, the American historian and novelist Henry Adams finished an essay entitled \"The Rule of Phase Applied to History\", in which he sought to apply Gibbs's phase rule and other thermodynamic concepts to a general theory of human history. William James, Henry Bumstead, and others criticized both Adams's tenuous grasp of the scientific concepts that he invoked, as well as the arbitrariness of his application of those concepts as metaphors for the evolution of human thought and society. The essay remained unpublished until it appeared posthumously in 1919, in \"The Degradation of the Democratic Dogma\", edited by Henry Adams's younger brother Brooks.\n\nIn the 1930s, feminist poet Muriel Rukeyser became fascinated by Willard Gibbs and wrote a long poem about his life and work (\"Gibbs\", included in the collection \"A Turning Wind\", published in 1939), as well as a book-length biography (\"Willard Gibbs\", 1942). According to Rukeyser:\n\nIn 1946, \"Fortune\" magazine illustrated a cover story on \"Fundamental Science\" with a representation of the thermodynamic surface that Maxwell had built based on Gibbs's proposal. Rukeyser called this surface a \"statue of water\" and the magazine saw in it \"the abstract creation of a great American scientist that lends itself to the symbolism of contemporary art forms.\" The artwork by Arthur Lidov also included Gibbs's mathematical expression of the phase rule for heterogeneous mixtures, as well as a radar screen, an oscilloscope waveform, Newton's apple, and a small rendition of a three-dimensional phase diagram.\n\nGibbs's nephew, Ralph Gibbs Van Name, a professor of physical chemistry at Yale, was unhappy with Rukeyser's biography, in part because of her lack of scientific training. Van Name had withheld the family papers from her and, after her book was published in 1942 to positive literary but mixed scientific reviews, he tried to encourage Gibbs's former students to produce a more technically oriented biography. Rukeyser's approach to Gibbs was also sharply criticized by Gibbs's former student and protégé Edwin Wilson. With Van Name's and Wilson's encouragement, physicist Lynde Wheeler published a new biography of Gibbs in 1951.\n\nBoth Gibbs and Rukeyser's biography of him figure prominently in the poetry collection \"True North\" (1997) by Stephanie Strickland. In fiction, Gibbs appears as the mentor to character Kit Traverse in Thomas Pynchon's novel \"Against the Day\" (2006). That novel also prominently discusses the birefringence of Iceland spar, an optical phenomenon that Gibbs investigated.\n\nIn 2005, the United States Postal Service issued the \"American Scientists\" commemorative postage stamp series designed by artist Victor Stabin, depicting Gibbs, John von Neumann, Barbara McClintock, and Richard Feynman. The first day of issue ceremony for the series was held on May 4 at Yale University's Luce Hall and was attended by John Marburger, scientific advisor to the President of the United States, Rick Levin, president of Yale, and family members of the scientists honored, including physician John W. Gibbs, a distant cousin of Willard Gibbs.\n\nKenneth R. Jolls, a professor of chemical engineering at Iowa State University and an expert on graphical methods in thermodynamics, consulted on the design of the stamp honoring Gibbs. The stamp identifies Gibbs as a \"thermodynamicist\" and features a diagram from the 4th edition of Maxwell's \"Theory of Heat\", published in 1875, which illustrates Gibbs's thermodynamic surface for water. Microprinting on the collar of Gibbs's portrait depicts his original mathematical equation for the change in the energy of a substance in terms of its entropy and the other state variables.\n\n\n\n\nGibbs's other papers are included in both:\n\n\n\n"}
{"id": "646116", "url": "https://en.wikipedia.org/wiki?curid=646116", "title": "K3 surface", "text": "K3 surface\n\nIn mathematics, a K3 surface is a complex or algebraic smooth minimal complete surface that is regular and has trivial canonical bundle.\n\nIn the Enriques–Kodaira classification of surfaces they form one of the four classes of surfaces of Kodaira dimension 0.\n\nTogether with two-dimensional complex tori, they are the Calabi–Yau manifolds of dimension two. Most complex K3 surfaces are not algebraic. This means that they cannot be embedded in any projective space as a surface defined by polynomial equations.\n\nThere are many equivalent properties that can be used to characterize a K3 surface. The only complete smooth surfaces with trivial canonical bundle are K3 surfaces and tori (or abelian varieties), so one can add any condition to exclude the latter to define K3 surfaces. Over the complex numbers the condition that the surface is simply connected is sometimes used.\n\nThere are a few variations of the definition: some authors restrict to projective surfaces, and some allow surfaces with Du Val singularities.\n\nEquivalently to the above definition, a K3 surface is defined as a surface \"S\" with trivial canonical bundle \"K\" = 0 and irregularity \"q\"(\"S\") = 0. One has\n\nand, from Serre duality,\n\nAltogether, one obtains the Euler characteristic\n\nOn the other hand, the Riemann–Roch theorem (Noether's formula) reads \nwhere \"c\"(\"S\") is the \"i\"-th Chern class. Since \"K\" is trivial, its first Chern class \"c\"(\"S\") vanishes, hence \"c\"(\"S\") = 24.\n\nSince \"c\"(\"S\") is equal to the Euler number \"e\"(\"S\") = \"b\"(\"S\") − \"b\"(\"S\") + \"b\"(\"S\") − \"b\"(\"S\") + \"b\"(\"S\") and \"b\"(\"S\") = \"b\"(\"S\") = 1, \"b\"(\"S\") = \"b\"(\"S\") = 2\"q\"(\"S\") = 0, we obtain \"b\"(\"S\") = 22.\n\n1. All complex K3 surfaces are diffeomorphic to one another (proved by Kunihiko Kodaira firstly). showed that all complex K3 surfaces are Kähler manifolds. As a consequence of this and Yau's solution to the Calabi conjecture, all complex K3 surfaces admit Ricci-flat metrics.\n\n2. The (\"p,q\")-th cohomology groups are indicated in the Hodge diamond\n3. On formula_5, the cup product defines a lattice structure, called the K3 lattice, as described in the next section.\n\nBecause of the above properties, K3 surfaces have been studied extensively not only in algebraic geometry but also in Kac–Moody algebras, mirror symmetry and string theory. In particular, the lattice structure provides the modularity with the Néron–Severi group on it.\n\nThere is a coarse moduli space for marked complex K3 surfaces, a non-Hausdorff smooth analytic space of complex dimension 20. There is a period mapping and the Torelli theorem holds for complex K3 surfaces.\n\nThe set \"M\" of pairs consisting of a complex K3 surface \"S\" and a Kähler class of H(\"S\",R) is in a natural way a real analytic manifold of dimension 60. There is a refined period map from \"M\" to a space KΩ that is an isomorphism. The space of periods can be described explicitly as follows:\n\n\nIf \"L\" is a line bundle on a K3 surface, the curves in the linear system have genus \"g\", where\n\"c\"(\"L\") =2\"g\"-2. A K3 surface with a line bundle \"L\" like this is called a K3 surface of genus \"g\". A K3 surface may have many different line bundles making it into a K3 surface of genus \"g\" for many different values of \"g\". The space of sections of the line bundle has dimension \"g\"+1, so there is a morphism of the K3 surface to projective space of dimension \"g\". There is a moduli space \"F\" of K3 surfaces with a primitive ample line bundle \"L\" with \"c\"(\"L\") =2\"g\"-2, which is nonempty of dimension 19 for \"g\"≥ 2. showed that this moduli space \"F\" is unirational if \"g\"≤13, and showed that it is of general type if \"g\"≥63. gave a survey of this area.\n\nK3 surfaces appear almost ubiquitously in string duality and provide an important tool for the understanding of it. String compactifications on these surfaces are not trivial, yet they are simple enough for us to analyze most of their properties in detail. The type IIA string, the type IIB string, the E×E heterotic string, the Spin(32)/Z2 heterotic string, and M-theory are related by compactification on a K3 surface. For example, the Type IIA compactified on a K3 surface is equivalent to the heterotic string compactified on 4-torus .\n\n\nK3 surface was first discovered by Ramanujan in 1910s but remained unpublished, and was rediscovered by André who named them in honor of three algebraic geometers, Kummer, Kähler and Kodaira, and the mountain K2 in Pakistan.\n\n\n\n"}
{"id": "61636", "url": "https://en.wikipedia.org/wiki?curid=61636", "title": "Lars Ahlfors", "text": "Lars Ahlfors\n\nLars Valerian Ahlfors (18 April 1907 – 11 October 1996) was a Finnish mathematician, remembered for his work in the field of Riemann surfaces and his text on complex analysis.\n\nAhlfors was born in Helsinki, Finland. His mother, Sievä Helander, died at his birth. His father, Axel Ahlfors, was a professor of engineering at the Helsinki University of Technology. The Ahlfors family was Swedish-speaking, so he first attended a private school where all classes were taught in Swedish. Ahlfors studied at University of Helsinki from 1924, graduating in 1928 having studied under Ernst Lindelöf and Rolf Nevanlinna. He assisted Nevanlinna in 1929 with his work on Denjoy's conjecture on the number of asymptotic values of an entire function.\nIn 1929 Ahlfors published the first proof of this conjecture, now known as the Denjoy–Carleman–Ahlfors theorem. It states that the number of asymptotic values approached by an entire function of order ρ along curves in the complex plane going toward infinity is less than or equal to 2ρ.\n\nHe completed his doctorate from the University of Helsinki in 1930.\n\nAhlfors worked as an associate professor at the University of Helsinki from 1933 to 1936. In 1936 he was one of the first two people to be awarded the Fields Medal. In 1935 Ahlfors visited Harvard University. He returned to Finland in 1938 to take up a professorship at the University of Helsinki. The outbreak of war led to problems although Ahlfors was unfit for military service. He was offered a post at the Swiss Federal Institute of Technology at Zurich in 1944 and finally managed to travel there in March 1945. He did not enjoy his time in Switzerland, so in 1946 he jumped at a chance to leave, returning to work at Harvard where he remained until he retired in 1977; he was William Caspar Graustein Professor of Mathematics from 1964. Ahlfors was a visiting scholar at the Institute for Advanced Study in 1962 and again in 1966. He was awarded the Wihuri Prize in 1968 and the Wolf Prize in Mathematics in 1981.\n\nHis book \"Complex Analysis\" (1953) is the classic text on the subject and is almost certainly referenced in any more recent text which makes heavy use of complex analysis. Ahlfors wrote several other significant books, including \"Riemann surfaces\" (1960) and \"Conformal invariants\" (1973).\nHe made decisive contributions to meromorphic curves, value distribution theory, Riemann surfaces, conformal geometry, quasiconformal mappings and other areas during his career.\n\nIn 1933, he married Erna Lehnert, an Austrian who with her parents had first settled in Sweden and then in Finland. The couple had three daughters.\n\n\n\n"}
{"id": "48551373", "url": "https://en.wikipedia.org/wiki?curid=48551373", "title": "Li Shanlan identity", "text": "Li Shanlan identity\n\nIn mathematics, in combinatorics, Li Shanlan identity (also called Li Shanlan's summation formula) is a certain combinatorial identity attributed to the nineteenth century Chinese mathematician Li Shanlan. Since Li Shanlan is also known as Li Renshu, this identity is also referred to as Li Renshu identity. This identity appears in the third chapter of \"Duoji bilei\" (Summing finite series), a mathematical text authored by Li Shanlan and published in 1867 as part of his collected works. A Czech mathematician Josef Kaucky published an elementary proof of the identity along with a history of the identity in 1964. Kaucky attributed the identity to a certain Li Jen-Shu. From the account of the history of the identity, it has been ascertained that Li Jen-Shu is in fact Li Shanlan. Western scholars had been studying Chinese mathematics for its historical value; but the attribution of this identity to a nineteenth century Chinese mathematician sparked a rethink on the mathematical value of the writings of Chinese mathematicians.\n\n\"In the West Li is best remembered for a combinatoric formula, known as the “Li Renshu identity,” that he derived using only traditional Chinese mathematical methods.\"\n\nThe Li Shanlan identity states that\n\nis used to get\n\nAnother application of Vandermonde's convolution yields\nand hence\nSince formula_5 is independent of \"k\", this can be put in the form\n\nNext, the result\ngives\n\nSetting \"p\" = \"q\" and replacing \"j\" by \"k\",\n\nLi's identity follows from this by replacing \"n\" by \"n\" + \"p\" and doing some rearrangement of terms in the resulting expression:\n"}
{"id": "2871265", "url": "https://en.wikipedia.org/wiki?curid=2871265", "title": "List of finite spherical symmetry groups", "text": "List of finite spherical symmetry groups\n\nFinite spherical symmetry groups are also called point groups in three dimensions. There are five fundamental symmetry classes which have triangular fundamental domains: dihedral, cyclic, tetrahedral, octahedral, and icosahedral symmetry.\n\nThis article lists the groups by Schoenflies notation, Coxeter notation, orbifold notation, and order. John Conway uses a variation of the Schoenflies notation, based on the groups' quaternion algebraic structure, labeled by one or two upper case letters, and whole number subscripts. The group order is defined as the subscript, unless the order is doubled for symbols with a plus or minus, \"±\", prefix, which implies a central inversion.\n\nHermann–Mauguin notation (International notation) is also given. The crystallography groups, 32 in total, are a subset with element orders 2, 3, 4 and 6.\n\nThere are four involutional groups: no symmetry (C), reflection symmetry (C), 2-fold rotational symmetry (C), and central point symmetry (C).\n\nThere are four infinite cyclic symmetry families, with \"n\" = 2 or higher. (\"n\" may be 1 as a special case as \"no symmetry\")\n\nThere are three infinite dihedral symmetry families, with \"n\" = 2 or higher (\"n\" may be 1 as a special case).\n\nThere are three types of polyhedral symmetry: tetrahedral symmetry, octahedral symmetry, and icosahedral symmetry, named after the triangle-faced regular polyhedra with these symmetries.\n\n\n\n"}
{"id": "516393", "url": "https://en.wikipedia.org/wiki?curid=516393", "title": "Low-density parity-check code", "text": "Low-density parity-check code\n\nIn information theory, a low-density parity-check (LDPC) code is a linear error correcting code, a method of transmitting a message over a noisy transmission channel. An LDPC is constructed using a sparse bipartite graph. LDPC codes are , which means that practical constructions exist that allow the noise threshold to be set very close (or even \"arbitrarily\" close on the binary erasure channel) to the theoretical maximum (the Shannon limit) for a symmetric memoryless channel. The noise threshold defines an upper bound for the channel noise, up to which the probability of lost information can be made as small as desired. Using iterative belief propagation techniques, LDPC codes can be decoded in time linear to their block length.\n\nLDPC codes are finding increasing use in applications requiring reliable and highly efficient information transfer over bandwidth-constrained or return-channel-constrained links in the presence of corrupting noise. Implementation of LDPC codes has lagged behind that of other codes, notably turbo codes. The fundamental patent for Turbo Codes expired on August 29, 2013.[https://www.google.com/patents/US5446747 [US5446747<nowiki>]</nowiki>]\n\nLDPC codes are also known as Gallager codes, in honor of Robert G. Gallager, who developed the LDPC concept in his doctoral dissertation at the Massachusetts Institute of Technology in 1960.\n\nImpractical to implement when first developed by Gallager in 1963, LDPC codes were forgotten until his work was rediscovered in 1996. Turbo codes, another class of capacity-approaching codes discovered in 1993, became the coding scheme of choice in the late 1990s, used for applications such as the Deep Space Network and satellite communications. However, the advances in low-density parity-check codes have seen them surpass turbo codes in terms of error floor and performance in the higher code rate range, leaving turbo codes better suited for the lower code rates only.\n\nIn 2003, an irregular repeat accumulate (IRA) style LDPC code beat six turbo codes to become the error correcting code in the new DVB-S2 standard for the satellite transmission of digital television. The DVB-S2 selection committee made decoder complexity estimates for the Turbo Code proposals using a much less efficient serial decoder architecture rather than a parallel decoder architecture. This forced the Turbo Code proposals to use frame sizes on the order of one half the frame size of the LDPC proposals. \n\nIn 2008, LDPC beat convolutional turbo codes as the forward error correction (FEC) system for the ITU-T G.hn standard. G.hn chose LDPC codes over turbo codes because of their lower decoding complexity (especially when operating at data rates close to 1.0 Gbit/s) and because the proposed turbo codes exhibited a significant error floor at the desired range of operation.\n\nLDPC codes are also used for 10GBase-T Ethernet, which sends data at 10 gigabits per second over twisted-pair cables. As of 2009, LDPC codes are also part of the Wi-Fi 802.11 standard as an optional part of 802.11n and 802.11ac, in the High Throughput (HT) PHY specification.\n\nSome OFDM systems add an additional outer error correction that fixes the occasional errors (the \"error floor\") that get past the LDPC correction inner code even at low bit error rates.\nFor example:\nThe Reed-Solomon code with LDPC Coded Modulation (RS-LCM) uses a Reed-Solomon outer code.\nThe DVB-S2, the DVB-T2 and the DVB-C2 standards all use a BCH code outer code to mop up residual errors after LDPC decoding.\n\nLDPC codes functionally are defined by a sparse parity-check matrix. This sparse matrix is often randomly generated, subject to the sparsity constraints—LDPC code construction is discussed later. These codes were first designed by Robert Gallager in 1962.\n\nBelow is a graph fragment of an example LDPC code using Forney's factor graph notation. In this graph, \"n\" variable nodes in the top of the graph are connected to (\"n\"−\"k\") constraint nodes in the bottom of the graph.\n\nThis is a popular way of graphically representing an (\"n\", \"k\") LDPC code. The bits of a valid message, when placed on the T's at the top of the graph, satisfy the graphical constraints. Specifically, all lines connecting to a variable node (box with an '=' sign) have the same value, and all values connecting to a factor node (box with a '+' sign) must sum, modulo two, to zero (in other words, they must sum to an even number; or there must be an even number of odd values).\nIgnoring any lines going out of the picture, there are eight possible six-bit strings corresponding to valid codewords: (i.e., 000000, 011001, 110010, 101011, 111100, 100101, 001110, 010111). This LDPC code fragment represents a three-bit message encoded as six bits. Redundancy is used, here, to increase the chance of recovering from channel errors. This is a (6, 3) linear code, with \"n\" = 6 and \"k\" = 3.\n\nOnce again ignoring lines going out of the picture, the parity-check matrix representing this graph fragment is\n\nIn this matrix, each row represents one of the three parity-check constraints, while each column represents one of the six bits in the received codeword.\n\nIn this example, the eight codewords can be obtained by putting the parity-check matrix H into this form formula_2 through basic row operations in GF(2):\n\nStep 1: H.\n\nStep 2: Row 1 is added to row 3.\n\nStep 3: Row 2 and 3 are swapped.\n\nStep 4: Row 1 is added to row 3.\n\nFrom this, the generator matrix G can be obtained as formula_4 (noting that in the special case of this being a binary code formula_5), or specifically:\n\nFinally, by multiplying all eight possible 3-bit strings by G, all eight valid codewords are obtained. For example, the codeword for the bit-string '101' is obtained by:\n\nAs a check, the row space of G is orthogonal to H such that formula_8\n\nThe bit-string '101' is found in as the first 3 bits of the codeword '101011'.\n\nFigure 1 illustrates the functional components of most LDPC encoders.\n\nDuring the encoding of a frame, the input data bits (D) are repeated and distributed to a set of constituent encoders. The constituent encoders are typically accumulators and each accumulator is used to generate a parity symbol. A single copy of the original data (S) is transmitted with the parity bits (P) to make up the code symbols. The S bits from each constituent encoder are discarded.\n\nThe parity bit may be used within another constituent code.\n\nIn an example using the DVB-S2 rate 2/3 code the encoded block size is 64800 symbols (N=64800) with 43200 data bits (K=43200) and 21600 parity bits (M=21600). Each constituent code (check node) encodes 16 data bits except for the first parity bit which encodes 8 data bits. The first 4680 data bits are repeated 13 times (used in 13 parity codes), while the remaining data bits are used in 3 parity codes (irregular LDPC code).\n\nFor comparison, classic turbo codes typically use two constituent codes configured in parallel, each of which encodes the entire input block (K) of data bits. These constituent encoders are recursive convolutional codes (RSC) of moderate depth (8 or 16 states) that are separated by a code interleaver which interleaves one copy of the frame.\n\nThe LDPC code, in contrast, uses many low depth constituent codes (accumulators) in parallel, each of which encode only a small portion of the input frame. The many constituent codes can be viewed as many low depth (2 state) 'convolutional codes' that are connected via the repeat and distribute operations. The repeat and distribute operations perform the function of the interleaver in the turbo code.\n\nThe ability to more precisely manage the connections of the various constituent codes and the level of redundancy for each input bit give more flexibility in the design of LDPC codes, which can lead to better performance than turbo codes in some instances. Turbo codes still seem to perform better than LDPCs at low code rates, or at least the design of well performing low rate codes is easier for Turbo Codes.\n\nAs a practical matter, the hardware that forms the accumulators is reused during the encoding process. That is, once a first set of parity bits are generated and the parity bits stored, the same accumulator hardware is used to generate a next set of parity bits.\n\nAs with other codes, the maximum likelihood decoding of an LDPC code on the binary symmetric channel is an NP-complete problem. Performing optimal decoding for a NP-complete code of any useful size is not practical.\n\nHowever, sub-optimal techniques based on iterative belief propagation decoding give excellent results and can be practically implemented. The sub-optimal decoding techniques view each parity check that makes up the LDPC as an independent single parity check (SPC) code. Each SPC code is decoded separately using soft-in-soft-out (SISO) techniques such as SOVA, BCJR, MAP, and other derivates thereof. The soft decision information from each SISO decoding is cross-checked and updated with other redundant SPC decodings of the same information bit. Each SPC code is then decoded again using the updated soft decision information. This process is iterated until a valid code word is achieved or decoding is exhausted. This type of decoding is often referred to as sum-product decoding.\n\nThe decoding of the SPC codes is often referred to as the \"check node\" processing, and the cross-checking of the variables is often referred to as the \"variable-node\" processing.\n\nIn a practical LDPC decoder implementation, sets of SPC codes are decoded in parallel to increase throughput.\n\nIn contrast, belief propagation on the binary erasure channel is particularly simple where it consists of iterative constraint satisfaction.\n\nFor example, consider that the valid codeword, 101011, from the example above, is transmitted across a binary erasure channel and received with the first and fourth bit erased to yield ?⁠01?⁠11. Since the transmitted message must have satisfied the code constraints, the message can be represented by writing the received message on the top of the factor graph.\n\nIn this example, the first bit cannot yet be recovered, because all of the constraints connected to it have more than one unknown bit. In order to proceed with decoding the message, constraints connecting to only one of the erased bits must be identified. In this example, only the second constraint suffices. Examining the second constraint, the fourth bit must have been zero, since only a zero in that position would satisfy the constraint.\n\nThis procedure is then iterated. The new value for the fourth bit can now be used in conjunction with the first constraint to recover the first bit as seen below. This means that the first bit must be a one to satisfy the leftmost constraint.\n\nThus, the message can be decoded iteratively. For other channel models, the messages passed between the variable nodes and check nodes are real numbers, which express probabilities and likelihoods of belief.\n\nThis result can be validated by multiplying the corrected codeword r by the parity-check matrix H:\n\nBecause the outcome z (the syndrome) of this operation is the three × one zero vector, the resulting codeword r is successfully validated.\n\nAfter the decoding is completed, the original message bits '101' can be extracted by looking at the first 3 bits of the codeword.\n\nWhile illustrative, this erasure example does not show the use of soft-decision decoding or soft-decision message passing, which is used in virtually all commercial LDPC decoders.\n\nIn recent years, there has also been a great deal of work spent studying the effects of alternative schedules for variable-node and constraint-node update. The original technique that was used for decoding LDPC codes was known as \"flooding\". This type of update required that, before updating a variable node, all constraint nodes needed to be updated and vice versa. In later work by Vila Casado \"et al.\", alternative update techniques were studied, in which variable nodes are updated with the newest available check-node information.\n\nThe intuition behind these algorithms is that variable nodes whose values vary the most are the ones that need to be updated first. Highly reliable nodes, whose log-likelihood ratio (LLR) magnitude is large and does not change significantly from one update to the next, do not require updates with the same frequency as other nodes, whose sign and magnitude fluctuate more widely.\nThese scheduling algorithms show greater speed of convergence and lower error floors than those that use flooding. These lower error floors are achieved by the ability of the Informed Dynamic Scheduling (IDS) algorithm to overcome trapping sets of near codewords.\n\nWhen nonflooding scheduling algorithms are used, an alternative definition of iteration is used. For an (\"n\", \"k\") LDPC code of rate \"k\"/\"n\", a full \"iteration\" occurs when \"n\" variable and \"n\" − \"k\" constraint nodes have been updated, no matter the order in which they were updated.\n\nIt is possible to decode LDPC codes on a relatively low-power microprocessor by the use of lookup tables.\n\nWhile codes such as the LDPC are generally implemented on high-power processors, with long block lengths, there are also applications which use lower-power processors and short block lengths (1024).\n\nTherefore, it is possible to precalculate the output bit based upon predetermined input bits. A table is generated which contains \"n\" entries (for a block length of 1024 bits, this would be 1024 bits long), and contains all possible entries for different input states (both errored and nonerrored).\n\nAs a bit is input, it is then added to a FIFO register, and the value of the FIFO register is then used to look up in the table the relevant output from the precalculated values.\n\nBy this method, very high iterations can be used, with little processor overhead, the only cost being that of the memory for the lookup table, such that LDPC decoding is possible even on a 4.0 MHz PIC chip.\n\nFor large block sizes, LDPC codes are commonly constructed by first studying the behaviour of decoders. As the block size tends to infinity, LDPC decoders can be shown to have a noise threshold below which decoding is reliably achieved, and above which decoding is not achieved, colloquially referred to as the cliff effect. This threshold can be optimised by finding the best proportion of arcs from check nodes and arcs from variable nodes. An approximate graphical approach to visualising this threshold is an EXIT chart.\n\nThe construction of a specific LDPC code after this optimization falls into two main types of techniques:\n\n\nConstruction by a pseudo-random approach builds on theoretical results that, for large block size, a random construction gives good decoding performance. In general, pseudorandom codes have complex encoders, but pseudorandom codes with the best decoders can have simple encoders. Various constraints are often applied to help ensure that the desired properties expected at the theoretical limit of infinite block size occur at a finite block size.\n\nCombinatorial approaches can be used to optimize the properties of small block-size LDPC codes or to create codes with simple encoders.\n\nSome LDPC codes are based on Reed–Solomon codes, such as the RS-LDPC code used in the 10 Gigabit Ethernet standard.\nCompared to randomly generated LDPC codes, structured LDPC codes—such as the LDPC code used in the DVB-S2 standard—can have simpler and therefore lower-cost hardware—in particular, codes constructed such that the H matrix is a circulant matrix.\n\nYet another way of constructing LDPC codes is to use finite geometries. This method was proposed by Y. Kou \"et al.\" in 2001.\n\n\n\n\n\n"}
{"id": "5244240", "url": "https://en.wikipedia.org/wiki?curid=5244240", "title": "Message sequence chart", "text": "Message sequence chart\n\nA message sequence chart (or MSC) is an interaction diagram from the SDL family standardized by the International Telecommunication Union.\n\nThe purpose of recommending MSC (Message Sequence Chart) is to provide a trace language for the specification and description of the communication behaviour of system components and their environment by means of message interchange. Since in MSCs the communication behaviour is presented in a very intuitive and transparent manner, particularly in the graphical representation, the MSC language is easy to\nlearn, use and interpret. In connection with other languages it can be used to support methodologies for system specification, design, simulation, testing, and documentation.\n\nThe first version of the MSC standard was released in March 12, 1993.\n\nThe 1996 version added references, ordering and inlining expressions concepts, and introduced HMSC (High-level Message Sequence Charts), which are the way of expressing a sequence of MSCs.\n\nThe MSC 2000 version added object orientation, refined the use of data and time in diagrams, and added the concept of remote method calls.\n\nLatest version has been published in February 2011.\n\nThe existing symbols are:\n\n\nUML 2.0 Sequence Diagram is strongly inspired by the ITU-T MSC. Still, for historical reasons, the default basic principles are quite different:\nIt has been said that MSC has been considered as a candidate for the interaction diagrams in UML.\n\nHowever, proponents of MSC such as Ericsson think that MSC is better than UML 2.0 for modelling large or complex systems.\n\nDavid Harel suggested that MSC had shortcomings such as:\nand proposed Live Sequence Charts (LSC) as an extension on the MSC standard .\n\n\n"}
{"id": "4073971", "url": "https://en.wikipedia.org/wiki?curid=4073971", "title": "Model elimination", "text": "Model elimination\n\nModel Elimination is the name attached to a pair of proof procedures invented by Donald W. Loveland, the first of which was published in 1968 in the Journal of the ACM. Their primary purpose is to carry out automated theorem proving, though they can readily be extended to logic programming, including the more general disjunctive logic programming.\n\nModel Elimination is closely related to resolution\nwhile also bearing characteristics of a Tableaux method. It is a progenitor of the SLD resolution procedure used in the Prolog logic programming language.\n\nWhile somewhat eclipsed by attention to and progress in Resolution\ntheorem provers, Model Elimination has continued to attract the\nattention of researchers and software developers. Today there are several theorem provers under active development that are based on the Model Elimination procedure.\n\n"}
{"id": "1719992", "url": "https://en.wikipedia.org/wiki?curid=1719992", "title": "Non-perturbative", "text": "Non-perturbative\n\nIn mathematics and physics, a non-perturbative function or process is one that cannot be accurately described by perturbation theory. An example is the function\n\nformula_1.\n\nThe Taylor series at x = 0 for this function is exactly zero to all orders in perturbation theory, but the function is non-zero if \"x\" ≠ 0.\n\nThe implication of this for physics is that there are some phenomena which are impossible to understand by perturbation theory, regardless of how many orders of perturbation theory we use. Instantons are an example.\n\nTherefore, in theoretical physics, a non-perturbative solution or theory is one that does not require perturbation theory to explicate, or does not simply describe the dynamics of perturbations around some fixed background. For this reason, non-perturbative solutions and theories yield insights into areas and subjects perturbative methods cannot reveal.\n\n"}
{"id": "6556971", "url": "https://en.wikipedia.org/wiki?curid=6556971", "title": "Outline of calculus", "text": "Outline of calculus\n\nCalculus is a branch of mathematics focused on limits, functions, derivatives, integrals, and infinite series. This subject constitutes a major part of contemporary mathematics education. Calculus has widespread applications in science, economics, and engineering and can solve many problems for which algebra alone is insufficient.\n\n\n\n\n\n\n"}
{"id": "46927724", "url": "https://en.wikipedia.org/wiki?curid=46927724", "title": "P-adic cohomology", "text": "P-adic cohomology\n\nIn mathematics, p-adic cohomology means a cohomology theory for varieties of characteristic \"p\" whose values are modules over a ring of \"p\"-adic integers. Examples (in roughly historical order) include:\n\n\n"}
{"id": "48636", "url": "https://en.wikipedia.org/wiki?curid=48636", "title": "Partition of unity", "text": "Partition of unity\n\nIn mathematics, a partition of unity of a topological space \"X\" is a set \"R\" of continuous functions from \"X\" to the unit interval [0,1] such that for every point, formula_1,\nPartitions of unity are useful because they often allow one to extend local constructions to the whole space. They are also important in the interpolation of data, in signal processing, and the theory of spline functions.\n\nThe existence of partitions of unity assumes two distinct forms:\n\n\nThus one chooses either to have the supports indexed by the open cover, or compact supports. If the space is compact, then there exist partitions satisfying both requirements.\n\nA finite open cover always has a continuous partition of unity subordinated to it, provided the space is locally compact and Hausdorff. \nParacompactness of the space is a necessary condition to guarantee the existence of a partition of unity subordinate to any open cover. Depending on the category to which the space belongs, it may also be a sufficient condition. The construction uses mollifiers (bump functions), which exist in continuous and smooth manifolds, but not in analytic manifolds. Thus for an open cover of an analytic manifold, an analytic partition of unity subordinate to that open cover generally does not exist. \"See\" analytic continuation.\n\nIf \"R\" and \"S\" are partitions of unity for spaces \"X\" and \"Y\", respectively, then the set of all pairwise products formula_3 is a partition of unity for the cartesian product space \"X\"×\"Y\".\n\nWe can construct a partition of unity on formula_4 by looking at a chart on the complement of a point formula_5 sending formula_6 to formula_7 with center formula_8. Now, let formula_9 be a bump function on formula_7 defined byformula_11then, both this function and formula_12 can be extended uniquely onto formula_4 by setting formula_14. Then, the set formula_15forms a partition of unity over formula_4.\n\nSometimes a less restrictive definition is used: the sum of all the function values at a particular point is only required to be positive, rather than 1, for each point in the space. However, given such a set of functions, one can obtain a partition of unity in the strict sense by dividing every function by the sum of all functions (which is defined, since at any point it has only a finite number of terms).\n\nA partition of unity can be used to define the integral (with respect to a volume form) of a function defined over a manifold: One first defines the integral of a function whose support is contained in a single coordinate patch of the manifold; then one uses a partition of unity to define the integral of an arbitrary function; finally one shows that the definition is independent of the chosen partition of unity.\n\nA partition of unity can be used to show the existence of a Riemannian metric on an arbitrary manifold.\n\nMethod of steepest descent employs a partition of unity to construct asymptotics of integrals.\n\nLinkwitz–Riley filter is an example of practical implementation of partition of unity to separate input signal into two output signals containing only high- or low-frequency components.\n\nThe Bernstein polynomials of a fixed degree \"m\" are a family of \"m\"+1 linearly independent polynomials that are a partition of unity for the unit interval formula_17.\n\n\n\n"}
{"id": "4147648", "url": "https://en.wikipedia.org/wiki?curid=4147648", "title": "Petrick's method", "text": "Petrick's method\n\nIn Boolean algebra, Petrick's method (also known as the \"branch-and-bound\" method) is a technique described by Stanley R. Petrick (1931–2006) in 1956 for determining all minimum sum-of-products solutions from a prime implicant chart. Petrick's method is very tedious for large charts, but it is easy to implement on a computer.\n\nAlgorithm\n\nExample of Petrick's method\n\nFollowing is the function we want to reduce:\n\nThe prime implicant chart from the Quine-McCluskey algorithm is as follows:\n\nBased on the X marks in the table above, build a product of sums of the rows where each row is added, and columns are multiplied together:\n\nUse the distributive law to turn that expression into a sum of products. Also use the following equivalences to simplify the final expression: X + XY = X and XX = X and X+X=X\n\nNow use again the following equivalence to further reduce the equation: X + XY = X\n\nChoose products with fewest terms, in this example, there are two products with three terms:\n\nChoose term or terms with fewest total literals. In our example, the two products both expand to six literals total each:\n\nSo either one can be used. In general, application of Petrick's method is tedious for large charts, but it is easy to implement on a computer.\n\n\n"}
{"id": "32883354", "url": "https://en.wikipedia.org/wiki?curid=32883354", "title": "Poset game", "text": "Poset game\n\nIn combinatorial game theory, poset games are mathematical games of strategy, generalizing many well-known games such as Nim and Chomp. In such games, two players start with a poset (a partially ordered set), and take turns choosing one point in the poset, removing it and all points that are greater. The player who is left with no point to choose, loses.\n\nGiven a partially ordered set (\"P\", <), let \ndenote the poset formed by removing \"x\" from \"P\".\n\nA poset game on \"P\", played between two players conventionally named Alice and Bob, is as follows:\n\n\nIf \"P\" is a finite totally ordered set, then game play in \"P\" is exactly the same as the game play in a game of Nim with a heap of size |\"P\"|. For, in both games, it is possible to choose a move that leads to a game of the same type whose size is any number smaller than |\"P\"|. In the same way, a poset game with a disjoint union of total orders is equivalent to a game of Nim with multiple heaps with sizes equal to the chains in the poset.\n\nA special case of Hackenbush, in which all edges are green (able to be cut by either player) and every configuration takes the form of a forest, may be expressed similarly, as a poset game on a poset in which, for every element \"x\", there is at most one element \"y\" for which \"x\" covers \"y\". If \"x\" covers \"y\", then \"y\" is the parent of \"x\" in the forest on which the game is played.\n\nChomp may be expressed similarly, as a poset game on the product of total orders from which the infimum has been removed.\n\nPoset games are impartial games, meaning that every move available to Alice would also be available to Bob if Alice were allowed to pass, and vice versa. Therefore, by the Sprague–Grundy theorem, every position in a poset game has a Grundy value, a number describing an equivalent position in the game of Nim. The Grundy value of a poset may be calculated as the least natural number which is not the Grundy value of any \"P\", \"x\" ∈ \"P\". That is,\n\nThis number may be used to describe the optimal game play in a poset game. In particular, the Grundy value is nonzero when the player whose turn it is has a winning strategy, and zero when the current player cannot win against optimal play from his or her opponent. A winning strategy in the game consists of moving to a position whose Grundy value is zero, whenever this is possible.\n\nA strategy-stealing argument shows that the Grundy value is nonzero for every poset that has a supremum. For, let \"x\" be the supremum of a partially ordered set \"P\". If \"P\" has Grundy value zero, then \"P\" itself has a nonzero value, by the formula above; in this case, \"x\" is a winning move in \"P\". If, on the other hand, \"P\" has a nonzero Grundy value, then there must be a winning move \"y\" in \"P\", such that the Grundy value of (\"P\") is zero. But by the assumption that \"x\" is a supremum, \"x\" > \"y\" and (\"P\") = \"P\", so the winning move \"y\" is also available in \"P\" and again \"P\" must have a nonzero Grundy value.\n\nFor more trivial reasons a poset with an infimum also has a nonzero Grundy value: moving to the infimum is always a winning move.\n\nDeciding the winner of an arbitrary finite poset game is PSPACE-complete. This means that unless P=PSPACE, computing the Grundy value of an arbitrary poset game is computationally difficult.\n"}
{"id": "42557085", "url": "https://en.wikipedia.org/wiki?curid=42557085", "title": "Quasi-commutative property", "text": "Quasi-commutative property\n\nIn mathematics, the quasi-commutative property is an extension or generalization of the general commutative property. This property is used in specific applications with various definitions.\n\nTwo matrices \"p\" and \"q\" are said to have the commutative property whenever\n\nThe quasi-commutative property in matrices is defined as follows. Given two non-commutable matrices \"x\" and \"y\"\nsatisfy the quasi-commutative property whenever \"z\" satisfies the following properties:\n\nAn example is found in the matrix mechanics introduced by Heisenberg as a version of quantum mechanics. In this mechanics, \"p\" and \"q\" are infinite matrices corresponding respectively to the momentum and position variables of a particle. These matrices are written out at Matrix mechanics#Harmonic oscillator, and z = iħ times the infinite unit matrix, where ħ is the reduced Planck constant.\n\nA function \"f\", defined as follows:\nis said to be quasi-commutative if for all formula_6 and for all formula_7,\n\n"}
{"id": "28771796", "url": "https://en.wikipedia.org/wiki?curid=28771796", "title": "Semiorder", "text": "Semiorder\n\nIn order theory, a branch of mathematics, a semiorder is a type of ordering that may be determined for a set of items with numerical scores by declaring two items to be incomparable when their scores are within a given margin of error of each other, and by using the numerical comparison of their scores when those scores are sufficiently far apart. Semiorders were introduced and applied in mathematical psychology by as a model of human preference without the assumption that indifference is transitive. They generalize strict weak orderings, form a special case of partial orders and interval orders, and can be characterized among the partial orders by two forbidden four-item suborders.\n\nLet \"X\" be a set of items, and let < be a binary relation on \"X\".\nItems \"x\" and \"y\" are said to be \"incomparable\", written here as \"x\" ~ \"y\", if neither \"x\" < \"y\" nor \"y\" < \"x\" is true. Then the pair (\"X\",<) is a semiorder if it satisfies the following three axioms:\n\nIt follows from the first axiom that \"x\" ~ \"x\", and therefore the second axiom (with \"y\" = \"z\") implies that < is a transitive relation.\n\nOne may define a partial order (\"X\",≤) from a semiorder (\"X\",<) by declaring that whenever either or . Of the axioms that a partial order is required to obey, reflexivity follows automatically from this definition, antisymmetry follows from the first semiorder axiom, and transitivity follows from the second semiorder axiom. Conversely, from a partial order defined in this way, the semiorder may be recovered by declaring that whenever and . The first of the semiorder axioms listed above follows automatically from the axioms defining a partial order, but the others do not. The second and third semiorder axioms forbid partial orders of four items forming two disjoint chains: the second axiom forbids two chains of two items each, while the third item forbids a three-item chain with one unrelated item.\n\nThe original motivation for introducing semiorders was to model human preferences without assuming (as strict weak orderings do) that incomparability is a transitive relation. For instance, if \"x\", \"y\", and \"z\" represent three quantities of the same material, and \"x\" and \"z\" differ by the smallest amount that is perceptible as a difference, while \"y\" is halfway between the two of them, then it is reasonable for a preference to exist between \"x\" and \"z\" but not between the other two pairs, violating transitivity.\n\nThus, suppose that \"X\" is a set of items, and \"u\" is a utility function that maps the members of \"X\" to real numbers. A strict weak ordering can be defined on \"x\" by declaring two items to be incomparable when they have equal utilities, and otherwise using the numerical comparison, but this necessarily leads to a transitive incomparability relation. Instead, if one sets a numerical threshold (which may be normalized to 1) such that utilities within that threshold of each other are declared incomparable, then a semiorder arises.\n\nSpecifically, define a binary relation < from \"X\" and \"u\" by setting \"x\" < \"y\" whenever \"u\"(\"x\") ≤ \"u\"(\"y\") − 1. Then (\"X\",<) is a semiorder. It may equivalently be defined as the interval order defined by the intervals [\"u\"(\"x\"),\"u\"(\"x\") + 1].\n\nIn the other direction, not every semiorder can be defined from numerical utilities in this way. For instance, if a semiorder (\"X\",<) includes an uncountable totally ordered subset then there do not exist sufficiently many sufficiently well-spaced real-numbers to represent this subset numerically. However, every finite semiorder can be defined from a utility function. supplies a precise characterization of the semiorders that may be defined numerically.\n\nAccording to Amartya K. Sen, semi-orders were examined by Dean T. Jamison and Lawrence J. Lau and found to be a special case of quasitransitive relations. In fact, every semiorder is a quasitransitive relation, since it is a transitive one. Moreover, adding to a given semiorder all its pairs of incomparable items keeps the resulting relation a quasitransitive one.\n\nThe number of distinct semiorders on \"n\" unlabeled items is given by the Catalan numbers\nwhile the number of semiorders on \"n\" labeled items is given by the sequence\n\nAny finite semiorder has order dimension at most three.\n\nAmong all partial orders with a fixed number of elements and a fixed number of comparable pairs, the partial orders that have the largest number of linear extensions are semiorders.\n\nSemiorders are known to obey the 1/3–2/3 conjecture: in any finite semiorder that is not a total order, there exists a pair of elements \"x\" and \"y\" such that \"x\" appears earlier than \"y\" in between 1/3 and 2/3 of the linear extensions of the semiorder.\n\nThe set of semiorders on an \"n\"-element set is \"well-graded\": if two semiorders on the same set differ from each other by the addition or removal of \"k\" order relations, then it is possible to find a path of \"k\" steps from the first semiorder to the second one, in such a way that each step of the path adds or removes a single order relation and each intermediate state in the path is itself a semiorder.\n\nThe incomparability graphs of semiorders are called indifference graphs, and are a special case of the interval graphs.\n\nIf a semiorder is given only in terms of the order relation between its pairs of elements, then it is possible to construct a utility function that represents the order in time , where is the number of elements in the semiorder.\n\n"}
{"id": "4304500", "url": "https://en.wikipedia.org/wiki?curid=4304500", "title": "Singular measure", "text": "Singular measure\n\nIn mathematics, two positive (or signed or complex) measures \"μ\" and \"ν\" defined on a measurable space (Ω, Σ) are called singular if there exist two disjoint sets \"A\" and \"B\" in Σ whose union is Ω such that \"μ\" is zero on all measurable subsets of \"B\" while \"ν\" is zero on all measurable subsets of \"A\". This is denoted by formula_1\n\nA refined form of Lebesgue's decomposition theorem decomposes a singular measure into a singular continuous measure and a discrete measure. See below for examples.\n\nAs a particular case, a measure defined on the Euclidean space formula_2 is called \"singular\", if it is singular with respect to the Lebesgue measure on this space. For example, the Dirac delta function is a singular measure.\n\nExample. A discrete measure.\n\nThe Heaviside step function on the real line,\n\nhas the Dirac delta distribution formula_4 as its distributional derivative. This is a measure on the real line, a \"point mass\" at 0. However, the Dirac measure formula_4 is not absolutely continuous with respect to Lebesgue measure formula_6, nor is formula_6 absolutely continuous with respect to formula_4: formula_9 but formula_10; if formula_11 is any open set not containing 0, then formula_12 but formula_13.\n\nExample. A singular continuous measure.\n\nThe Cantor distribution has a cumulative distribution function that is continuous but not absolutely continuous, and indeed its absolutely continuous part is zero: it is singular continuous.\n\n\n"}
{"id": "41284472", "url": "https://en.wikipedia.org/wiki?curid=41284472", "title": "Suspension of a ring", "text": "Suspension of a ring\n\nIn algebra, more specifically in algebraic K-theory, the suspension formula_1 of a ring \"R\" is given by formula_2 where formula_3 is the ring of all infinite matrices with coefficients in \"R\" having only finitely many nonzero elements in each row or column and formula_4 is its ideal of matrices having only finitely many nonzero elements. It is an analog of suspension in topology.\n\nOne then has: formula_5.\n\n"}
{"id": "34938025", "url": "https://en.wikipedia.org/wiki?curid=34938025", "title": "Theodore Frankel", "text": "Theodore Frankel\n\nTheodore Frankel (born June 17th, 1929, died August 5th, 2017) was a mathematician who introduced the Andreotti–Frankel theorem. Frankel received his Ph.D. from the University of California, Berkeley in 1955. His doctoral advisor was Harley Flanders. \nA Professor Emeritus of Mathematics at UC San Diego, Theodore Frankel was a longtime member of Princeton's Institute for Advanced Study and is known for his work in global differential geometry, Morse theory, and relativity theory. He joined the UC San Diego mathematics department in 1965, after serving on the faculties at Stanford and Brown universities.\n\n"}
{"id": "18500376", "url": "https://en.wikipedia.org/wiki?curid=18500376", "title": "Universal type", "text": "Universal type\n\nIn type theory universal type(s) may refer to:\n\n"}
{"id": "14819817", "url": "https://en.wikipedia.org/wiki?curid=14819817", "title": "Variation (game tree)", "text": "Variation (game tree)\n\nA Variation can refer to a specific sequence of successive moves in a turn-based game, often used to specify a hypothetical future state of a game that is being played. Although the term is most commonly used in the context of Chess analysis, it has been applied to other games. It also is a useful term used when describing computer tree-search algorithms (for example minimax) for playing games such as Go or Chess.\n\nA variation can be any number of steps as long as each step would be legal if it were to be played. It is often as far ahead as a human or computer can calculate; or however long is necessary to reach a particular position of interest. It may also lead to a terminal state in the game, in which case the term \"Winning Variation\" or \"Losing Variation\" is sometimes used.\n\nThe principal variation refers to the particular variation that is the most advantageous to the current player, assuming each other player will respond with the move that best improves their own position. In other words, it is the \"best\" or \"correct\" line of play. In the context of tree-searching game Artificial Intelligence – in which this term is most common – it may also refer to the sequence of moves which is currently \"believed\" to be the most advantageous, but is not guaranteed due to the technical limitations of the algorithm.\n\n\n"}
{"id": "39011104", "url": "https://en.wikipedia.org/wiki?curid=39011104", "title": "Water-pouring algorithm", "text": "Water-pouring algorithm\n\nThe water-pouring algorithm is a technique used in digital communications systems for allocating power among different channels in multicarrier schemes. It was described by R. C. Gallager in 1968 along with the water-pouring theorem which proves its optimality for channels having Additive White Gaussian Noise (AWGN) and intersymbol interference (ISI).\nFor this reason, it is a standard baseline algorithm for various digital communications systems.\n\nThe intuition that gives the algorithm its name is to think of the communication medium as if it was some kind of water container with an uneven bottom. Each of the available channels is then a section of the container having its own depth, given by the reciprocal of the frequency-dependent SNR for the channel.\nTo allocate power, imagine pouring water into this container (the amount depends on the desired maximum average transmit power). After the water level settles, the largest amount of water is in the deepest sections of the container. This implies allocating more power to the channels with the most favourable SNR. Note, however, that the ratio allocation to each channel is not a fixed proportion but varies nonlinearly with the maximum average transmit power.\n"}
