{"id": "407362", "url": "https://en.wikipedia.org/wiki?curid=407362", "title": "104 (number)", "text": "104 (number)\n\n104 (one hundred [and] four) is the natural number following 103 and preceding 105. \n\n104 is a primitive semiperfect number and a composite number, with its divisors being 1, 2, 4, 8, 13, 26, 52 and 104. As it has 8 divisors total, and 8 is one of those divisors, 104 is a refactorable number. The distinct prime factors of 104 add up to 15, and so do the ones of 105, hence the two numbers form a Ruth-Aaron pair under the first definition.\n\nIn regular geometry, 104 is the smallest number of unit line segments that can exist in a plane with four of them touching at every vertex.\n\n\n104 is also:\n\n\n"}
{"id": "8356784", "url": "https://en.wikipedia.org/wiki?curid=8356784", "title": "Age (model theory)", "text": "Age (model theory)\n\nIn model theory, the age of a structure (or model) \"A\" is the class of all finitely generated structures that are embeddable in \"A\" (i.e. isomorphic to substructures of \"A\"). This concept is central in the construction of a Fraïssé limit.\n\nThe main point of Fraïssé's construction is to show how one can approximate a structure by its finitely generated substructures. Thus for example the age of any dense linear order without endpoints (DLO), \nformula_1 is precisely the set of all linear orderings, which are distinguished up to isomorphism only by their size. Thus the age of any DLO is countable. This shows in a way that a DLO is a kind of limit of finite linear orderings. \n\nOne can easily see that any class \"K\" that is an age of some structure satisfies the following two conditions:\n\n\n\nFraïssé proved the converse result: when \"K\" is any non-empty countable set of finitely generated σ-structures (with σ a signature) that has the above two properties, then it is an age of a countable structure.\n\nFurthermore, suppose that \"K\" happens to satisfy the following additional property.\n\n\nIn that case, there is a unique countable structure, up to isomorphism, that has age \"K\" and is homogeneous. In this context, 'homogeneous' means that any isomorphism between two finitely generated substructures can be extended to an automorphism of the whole structure. Again, an example of this situation is the ordered set of rational numbers formula_1. It is the \"unique\" (up to isomorphism) homogeneous countable structure whose age is the set of all finite linear orderings. Note that the ordered set of natural numbers formula_3 has the same age as a DLO, but it is not homogeneous since if we map {1, 3} to {5, 6}, it would not extend to any automorphism \"f\" since there should be an element between formula_4 and formula_5. The same applies to integers.\n"}
{"id": "8864768", "url": "https://en.wikipedia.org/wiki?curid=8864768", "title": "Amoeba (mathematics)", "text": "Amoeba (mathematics)\n\nIn complex analysis, a branch of mathematics, an amoeba is a set associated with a polynomial in one or more complex variables. Amoebas have applications in algebraic geometry, especially tropical geometry.\n\nConsider the function\n\ndefined on the set of all \"n\"-tuples formula_2 of non-zero complex numbers with values in the Euclidean space formula_3 given by the formula\n\nHere, 'log' denotes the natural logarithm. If \"p\"(\"z\") is a polynomial in formula_5 complex variables, its amoeba formula_6 is defined as the image of the set of zeros of \"p\" under Log, so\n\nAmoebas were introduced in 1994 in a book by Gelfand, Kapranov, and Zelevinsky.\n\n\nA useful tool in studying amoebas is the Ronkin function. For \"p\"(\"z\") a polynomial in \"n\" complex variables, one defines the Ronkin function\n\nby the formula\n\nwhere formula_11 denotes formula_12 Equivalently, formula_13 is given by the integral\n\nwhere\n\nThe Ronkin function is convex, and it is affine on each connected component of the complement of the amoeba of formula_16.\n\nAs an example, the Ronkin function of a monomial\n\nwith formula_18 is\n\n\n"}
{"id": "20781979", "url": "https://en.wikipedia.org/wiki?curid=20781979", "title": "Andronov–Pontryagin criterion", "text": "Andronov–Pontryagin criterion\n\nThe Andronov–Pontryagin criterion is a necessary and sufficient condition for the stability of dynamical systems in the plane. It was derived by Aleksandr Andronov and Lev Pontryagin in 1937.\n\nA dynamical system\n\nwhere formula_2 is a formula_3-vector field on the plane, formula_4, \nis orbitally topologically stable if and only if the following two conditions hold:\n\n\nThe same statement holds if the vector field formula_2 is defined on the unit disk and is transversal to the boundary.\n\nOrbital topological stability of a dynamical system means that for any sufficiently small perturbation (in the \"C\"-metric), there exists a homeomorphism close to the identity map which transforms the orbits of the original dynamical system to the orbits of the perturbed system (cf structural stability).\n\nThe first condition of the theorem is known as global hyperbolicity. A zero of a vector field \"v\", i.e. a point \"x\" where \"v\"(\"x\")=0, is said to be hyperbolic if none of the eigenvalues of the linearization of \"v\" at \"x\" is purely imaginary. A periodic orbit of a flow is said to be hyperbolic if none of the eigenvalues of the Poincaré return map at a point on the orbit has absolute value one.\n\nFinally, saddle connection refers to a situation where an orbit from one saddle point enters the same or another saddle point, i.e. the unstable and stable separatrices are connected (cf homoclinic orbit and heteroclinic orbit).\n\n"}
{"id": "53945987", "url": "https://en.wikipedia.org/wiki?curid=53945987", "title": "Anne Penfold Street", "text": "Anne Penfold Street\n\nAnne Penfold Street (1932–2016) was one of Australia's leading mathematicians, specialising in combinatorics. She was the third woman to become a mathematics professor in Australia, following Hanna Neumann and Cheryl Praeger. She was the author of several textbooks,\nand her work on sum-free sets became a standard reference for its subject matter. She helped found several important organizations in combinatorics, developed a researcher network, and supported young students with interest in mathematics.\n\nStreet was born on 11 October 1932 in Melbourne, the daughter of a medical researcher. She earned a bachelor's degree in chemistry from the University of Melbourne in 1954, while working there as a tutor in chemistry and also studying mathematics. She finished a master's degree in chemistry at Melbourne in 1956. During this time she married another Melbourne chemist, Norman Street, and in 1957 the Streets and their young daughter moved to the University of Illinois at Urbana–Champaign where Norman Street had a new job.\n\nAt Illinois, Street took up mathematics again. After moving to Mildura and then returning to Urbana, she completed her doctorate at the University of Illinois in 1966, with a dissertation on group theory supervised by Michio Suzuki.\n\nAfter earning her doctorate, Street became a lecturer at the University of Queensland in 1967. While continuing to hold this position, she took a year of postdoctoral research at the University of Alberta, and on her return to Queensland in 1970 was promoted to senior lecturer, promoted again to reader in 1975, and given a personal chair as professor in 1985.\nAt Queensland, she directed the Centre for Discrete Mathematics and Computing from its formation in 1998 until 2004.\nShe has also held visiting positions at the University of Waterloo, University of Reading, University of Manitoba, University of Illinois at Urbana–Champaign, University of Western Australia, Auburn University, and University of Canterbury.\n\nStreet became the founding editor-in-chief of the \"Australasian Journal of Combinatorics\" in 1990, and continued to serve as editor-in-chief until 2001.\nShe helped found the Institute of Combinatorics and its Applications (ICA), became one of its founding fellows, and served as an editor of the \"Bulletin of the ICA\" from its founding in 1991 until 2014. She was president of the ICA from 1996 to 2002.\nShe was the founding president of the Combinatorial Mathematics Society of Australasia, for the 1997–1998 term.\nShe also served as President of the Australian Mathematical Olympiad Committee from 1996 to 2001.\n\nThe Australian Mathematics Trust gave Street the 1994 Bernhard H. Neumann Award for excellence in mathematics enrichment.\nThe University of Waterloo gave her an honorary doctorate in 1996.\nIn 1999, the Combinatorial Mathematics Society of Australasia gave her their inaugural medal for outstanding service.\nShe was named a Member of the Order of Australia in the 2014 Queen's Birthday Honours, primarily for her work with the Australian Mathematics Trust and the Australian Mathematical Olympiad Committee.\n\nThe Anne Penfold Street Awards of the Australian Mathematical Society, an initiative to provide family care for traveling mathematicians, are named after her.\nBeginning in 2016, the best student paper award from the annual Australasian Conference on Combinatorial Mathematics and Combinatorial Computing (ACCMCC) became known as the CMSA Anne Penfold Street Student Prize.\n\nStreet's daughter, Deborah J. Street, is a statistician at the University of Technology Sydney, and the coauthor (with her mother) of a book on the combinatorial design of experiments.\nHer son, Tony Street, is a researcher in Islamic Studies at the University of Cambridge.\n\nStreet died on 28 December 2016.\n\n"}
{"id": "43264877", "url": "https://en.wikipedia.org/wiki?curid=43264877", "title": "Bipartite realization problem", "text": "Bipartite realization problem\n\nThe bipartite realization problem is a classical decision problem in graph theory, a branch of combinatorics. Given two finite sequences formula_1 and formula_2 of natural numbers, the problem asks whether there is labeled simple bipartite graph such that formula_3 is the degree sequence of this bipartite graph.\n\nThe problem belongs to the complexity class P. This can be proven using the Gale–Ryser theorem, i.e. one has to validate the correctness of formula_4 inequalities.\n\nThe problem can also be stated in terms of zero-one matrices. The connection can be seen if one realizes that each bipartite graph has an biadjacency matrix where the column sums and row sums correspond to formula_5 and formula_6. The problem is then often denoted by \"0-1-matrices for given row and column sums\". In the classical literature the problem was sometimes stated in the context of contingency tables by \"contingency tables with given marginals\". A third formulation is in terms of degree sequences of simple directed graphs with at most one loop per vertex. In this case the matrix is interpreted as the adjacency matrix of such a directed graph. When are pairs of non-negative integers ((\"a\",\"b\"), ..., (\"a\",\"b\")) the indegree-outdegree pairs of a labeled directed graph with at most one loop per vertex?\n\nSimilar problems describe the degree sequences of simple graphs and simple directed graphs. The first problem is the so-called graph realization problem. The second is known as the digraph realization problem. The bipartite realization problem is equivalent to the question, if there exists a labeled bipartite subgraph of a complete bipartite graph to a given degree sequence. The \"hitchcock problem\" asks for such a subgraph minimizing the sum of the costs on each edge which are given for the complete bipartite graph. A further generalization is the f-factor problem for bipartite graphs, i.e. for a given bipartite graph one searches for a subgraph possessing a certain degree sequence. The problem \"uniform sampling a bipartite graph to a fixed degree sequence\" is to construct a solution for the bipartite realization problem with the additional constraint that each such solution comes with the same probability. This problem was shown to be in FPTAS for regular sequences by Catherine Greenhill (for regular bipartite graphs with a forbidden 1-factor) and for half-regular sequences by Erdős et al.. The general problem has still been unsolved.\n\n"}
{"id": "37206275", "url": "https://en.wikipedia.org/wiki?curid=37206275", "title": "California Mathematics Project", "text": "California Mathematics Project\n\nThe California Mathematics Project (CMP) is a K–16 network in California, United States, dedicated to providing students a rich, rigorous, and coherent mathematics curriculum taught by competent and confident mathematics teachers who foster all students’ proficiency in mathematics—achieving equity in quality.\n\nCMP enhances teachers’ mathematical content knowledge and pedagogical content knowledge that is aligned to the \"California Mathematics Standards and Framework\". All teachers and students become competent mathematical thinkers as they investigate, conjecture, and justify.\n\nThe roots of the CMP can be traced back to the Bay Area Writing Project (BAWP), a professional development project for teachers or writing. The BAWP was established in 1974 by James Grey at the University of California, Berkeley.\n\nThe CMP was created in 1982 by legislative act SB 424 (Carpenter) to \"seek to solve the mathematics skills problem of students in California through cooperatively planned and funded efforts.\" At that time nine sites were funded throughout the state. The University of California was vested with authority to manage and control the projects. The California Postsecondary Education Commission (CPEC) was to evaluate the projects. Judy Kysh was hired in 1984 as a part-time statewide coordinator. In 1986, it was decided that there needed to be a full-time statewide Executive Director to oversee the CMP.\n\nIn 1987, CPEC commissioned a policy study to analyze the effectiveness of professional development. The researchers identified four characteristics of effective professional development:\n\n\nFollowing this report, in 1989 the California legislature created a professional development program expanding the structure of the California Writing Project (CWP) and CMP to embrace nine subject areas called the California Subject Matter Projects (CSMP). \"The CMP model is one of long-term, sustained professional development, in contrast to a conference or a \"one-shot\" workshop. Experienced and interested teacher leaders often assume leadership roles at the project site.\"\n\nSites \"create a professional home for teachers that is based upon a culture of inquiry, experimentation, and reflections.\"\n\n"}
{"id": "46420317", "url": "https://en.wikipedia.org/wiki?curid=46420317", "title": "Client-side encryption", "text": "Client-side encryption\n\nClient-side encryption is the cryptographic technique of encrypting data on the sender's side, before it is transmitted to a server such as a cloud storage service. Client-side encryption features an encryption key that is not available to the service provider, making it difficult or impossible for service providers to decrypt hosted data. Client-side encryption allows for the creation of applications whose providers cannot access the data its users have stored, thus offering a high level of privacy. Those applications are sometimes marketed under the misleading term \"zero-knowledge\".\n\nClient-side encryption is widely recognized as an exceptionally robust data security strategy. By eliminating the potential for data to be viewed by service providers (or third parties that compel service providers to deliver access to data), client-side encryption ensures that data and files that are stored in the cloud can only be viewed on the client side of the exchange. This prevents data loss and the unauthorized disclosure of private or personal files, providing increased peace of mind for its users.\n\nCurrent academic scholarship as well as recommendations by industry professionals provide much support for developers to include client-side encryption to protect the confidentiality and integrity of information.\n\nExamples of cloud storage services that provide client-side encryption are Tresorit, MEGA and SpiderOak. As of February 2016, neither Apple iCloud, Google Drive, or Dropbox provide client-side encryption.\n\n"}
{"id": "168865", "url": "https://en.wikipedia.org/wiki?curid=168865", "title": "Corollary", "text": "Corollary\n\nA corollary ( , ) is a statement that follows readily from a previous statement.\n\nIn mathematics, a corollary is a theorem connected by a short proof to an existing theorem. The use of the term \"corollary\", rather than \"proposition\" or \"theorem\", is intrinsically subjective. Proposition \"B\" is a corollary of proposition \"A\" if \"B\" can be readily deduced from \"A\" or is self-evident from its proof. The importance of the corollary is often considered secondary to that of the initial theorem; \"B\" is unlikely to be termed a corollary if its mathematical consequences are as significant as those of \"A\". Sometimes a corollary has a proof that explains the derivation; sometimes the derivation is considered self-evident.\n\nCharles Sanders Peirce held that the most important division of kinds of deductive reasoning is that between corollarial and theorematic. He argued that, while finally all deduction depends in one way or another on mental experimentation on schemata or diagrams, still in corollarial deduction \"it is only necessary to imagine any case in which the premises are true in order to perceive immediately that the conclusion holds in that case\", whereas theorematic deduction \"is deduction in which it is necessary to experiment in the imagination upon the image of the premise in order from the result of such experiment to make corollarial deductions to the truth of the conclusion.\" He held that corollarial deduction matches Aristotle's conception of direct demonstration, which Aristotle regarded as the only thoroughly satisfactory demonstration, while theorematic deduction (A) is the kind more prized by mathematicians, (B) is peculiar to mathematics, and (C) involves in its course the introduction of a lemma or at least a definition uncontemplated in the thesis (the proposition that is to be proved); in remarkable cases that definition is of an abstraction that \"ought to be supported by a proper postulate.\"\n\n\n"}
{"id": "58467706", "url": "https://en.wikipedia.org/wiki?curid=58467706", "title": "Dodecahedral molecular geometry", "text": "Dodecahedral molecular geometry\n\nIn chemistry, the dodecahedral molecular geometry describes the shape of compounds where eight atoms or groups of atoms or ligands are arranged around a central atom defining the vertices of a snub disphenoid. This shape has D symmetry and is one of the three common shapes for octacoordinate transition metal complexes, along with the square antiprism and the bicapped trigonal prism.\n\nOne example of the dodecahedral molecular geometry is the Mo(CN) ion.\n"}
{"id": "12261058", "url": "https://en.wikipedia.org/wiki?curid=12261058", "title": "Doob's martingale convergence theorems", "text": "Doob's martingale convergence theorems\n\nIn mathematicsspecifically, in the theory of stochastic processesDoob's martingale convergence theorems are a collection of results on the long-time limits of supermartingales, named after the American mathematician Joseph L. Doob.\n\nIn the following, (Ω, \"F\", \"F\", P), \"F\" = (\"F\"), will be a filtered probability space and \"N\" : [0, +∞) × Ω → R will be a right-continuous supermartingale with respect to the filtration \"F\"; in other words, for all 0 ≤ \"s\" ≤ \"t\" < +∞,\n\nDoob's first martingale convergence theorem provides a sufficient condition for the random variables \"N\" to have a limit as \"t\" → +∞ in a pointwise sense, i.e. for each \"ω\" in the sample space Ω individually.\n\nFor \"t\" ≥ 0, let \"N\" = max(−\"N\", 0) and suppose that\n\nThen the pointwise limit\n\nexists and is finite for P-almost all \"ω\" ∈ Ω.\n\nIt is important to note that the convergence in Doob's first martingale convergence theorem is pointwise, not uniform, and is unrelated to convergence in mean square, or indeed in any \"L\" space. In order to obtain convergence in \"L\" (i.e., convergence in mean), one requires uniform integrability of the random variables \"N\". By Chebyshev's inequality, convergence in \"L\" implies convergence in probability and convergence in distribution.\n\nThe following are equivalent:\n\n\n\nLet \"M\" : [0, +∞) × Ω → R be a continuous martingale such that\n\nfor some \"p\" > 1. Then there exists a random variable \"M\" ∈ \"L\"(Ω, P; R) such that \"M\" → \"M\" as \"t\" → +∞ both P-almost surely and in \"L\"(Ω, P; R).\n\nSimilar results can be obtained for discrete-time supermartingales and submartingales, the obvious difference being that no continuity assumptions are required. For example, the result above becomes\n\nLet \"M\" : N × Ω → R be a discrete-time martingale such that\n\nfor some \"p\" > 1. Then there exists a random variable \"M\" ∈ \"L\"(Ω, P; R) such that \"M\" → \"M\" as \"k\" → +∞ both P-almost surely and in \"L\"(Ω, P; R)\n\nDoob's martingale convergence theorems imply that conditional expectations also have a convergence property.\n\nLet (Ω, \"F\", P) be a probability space and let \"X\" be a random variable in \"L\". Let \"F\" = (\"F\") be any filtration of \"F\", and define \"F\" to be the minimal \"σ\"-algebra generated by (\"F\"). Then\n\nboth P-almost surely and in \"L\".\n\nThis result is usually called Lévy's zero–one law or Levy's upwards theorem. The reason for the name is that if \"A\" is an event in \"F\", then the theorem says that formula_9 almost surely, i.e., the limit of the probabilities is 0 or 1. In plain language, if we are learning gradually all the information that determines the outcome of an event, then we will become gradually certain what the outcome will be. This sounds almost like a tautology, but the result is still non-trivial. For instance, it easily implies Kolmogorov's zero–one law, since it says that for any tail event \"A\", we must have formula_10 almost surely, hence formula_11.\n\nSimilarly we have the Levy's downwards theorem :\n\nLet (Ω, \"F\", P) be a probability space and let \"X\" be a random variable in \"L\". Let (\"F\") be any decreasing sequence of sub-sigma algebras of \"F\", and define \"F\" to be the intersection. Then\n\nboth P-almost surely and in \"L\".\n\nThe following result, called Doob's upcrossing inequality or, sometimes, Doob's upcrossing lemma, is used in proving Doob's martingale convergence theorems.<br>Hypothesis.<br>Let formula_13 be a natural number. Let formula_14, for formula_15, be a martingale with respect to a filtration formula_16, for formula_15. Let formula_18, formula_19 be two real numbers with formula_20.<br>Define the random variables formula_21, for formula_22, as follows: formula_23 if and only if formula_24 is the largest integer such that there exist integers formula_25, formula_26 satisfying 1 ≤ formula_27 < formula_28 and, for formula_29, for each pair formula_30 the inequalities formula_31 and formula_32 are satisfied. Each formula_21 is called the number of upcrossings with respect to the interval formula_34 for the martingale formula_35, formula_36.\n<br>Conclusion.\n\n\n"}
{"id": "10390", "url": "https://en.wikipedia.org/wiki?curid=10390", "title": "Econometrics", "text": "Econometrics\n\nEconometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships. More precisely, it is \"the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference\". An introductory economics textbook describes econometrics as allowing economists \"to sift through mountains of data to extract simple relationships\". The first known use of the term \"econometrics\" (in cognate form) was by Polish economist Paweł Ciompa in 1910. Jan Tinbergen is considered by many to be one of the founding fathers of econometrics. Ragnar Frisch is credited with coining the term in the sense in which it is used today.\n\nA basic tool for econometrics is the multiple linear regression model. Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods. Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. \"Applied econometrics\" uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.\n\nA basic tool for econometrics is the multiple linear regression model. In modern econometrics, other statistical tools are frequently used, but linear regression is still the most frequently used starting point for an analysis. Estimating a linear regression on two variables can be visualised as fitting a line through data points representing paired values of the independent and dependent variables.\n\nFor example, consider Okun's law, which relates GDP growth to the unemployment rate. This relationship is represented in a linear regression where the change in unemployment rate (formula_1) is a function of an intercept (formula_2), a given value of GDP growth multiplied by a slope coefficient formula_3 and an error term, formula_4:\n\nThe unknown parameters formula_2 and formula_3 can be estimated. Here formula_3 is estimated to be −1.77 and formula_2 is estimated to be 0.83. This means that if GDP growth increased by one percentage point, the unemployment rate would be predicted to drop by 1.77 points. The model could then be tested for statistical significance as to whether an increase in growth is associated with a decrease in the unemployment, as hypothesized. If the estimate of formula_3 were not significantly different from 0, the test would fail to find evidence that changes in the growth rate and unemployment rate were related. The variance in a prediction of the dependent variable (unemployment) as a function of the independent variable (GDP growth) is given in polynomial least squares.\n\nEconometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods. Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. An estimator is unbiased if its expected value is the true value of the parameter; it is consistent if it converges to the true value as the sample size gets larger, and it is efficient if the estimator has lower standard error than other unbiased estimators for a given sample size. Ordinary least squares (OLS) is often used for estimation since it provides the BLUE or \"best linear unbiased estimator\" (where \"best\" means most efficient, unbiased estimator) given the Gauss-Markov assumptions. When these assumptions are violated or other statistical properties are desired, other estimation techniques such as maximum likelihood estimation, generalized method of moments, or generalized least squares are used. Estimators that incorporate prior beliefs are advocated by those who favour Bayesian statistics over traditional, classical or \"frequentist\" approaches.\n\n\"Applied econometrics\" uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.\n\nEconometrics may use standard statistical models to study economic questions, but most often they are with observational data, rather than in controlled experiments. In this, the design of observational studies in econometrics is similar to the design of studies in other observational disciplines, such as astronomy, epidemiology, sociology and political science. Analysis of data from an observational study is guided by the study protocol, although exploratory data analysis may be useful for generating new hypotheses. Economics often analyses systems of equations and inequalities, such as supply and demand hypothesized to be in equilibrium. Consequently, the field of econometrics has developed methods for identification and estimation of simultaneous-equation models. These methods are analogous to methods used in other areas of science, such as the field of system identification in systems analysis and control theory. Such methods may allow researchers to estimate models and investigate their empirical consequences, without directly manipulating the system.\n\nOne of the fundamental statistical methods used by econometricians is regression analysis. Regression methods are important in econometrics because economists typically cannot use controlled experiments. Econometricians often seek illuminating natural experiments in the absence of evidence from controlled experiments. Observational data may be subject to omitted-variable bias and a list of other problems that must be addressed using causal analysis of simultaneous-equation models.\n\nIn addition to natural experiments, quasi-experimental methods have been used increasingly commonly by econometricians since the 1980s, in order to credibly identify causal effects.\n\nA simple example of a relationship in econometrics from the field of labour economics is:\n\nThis example assumes that the natural logarithm of a person's wage is a linear function of the number of years of education that person has acquired. The parameter formula_12 measures the increase in the natural log of the wage attributable to one more year of education. The term formula_4 is a random variable representing all other factors that may have direct influence on wage. The econometric goal is to estimate the parameters, formula_14 under specific assumptions about the random variable formula_4. For example, if formula_4 is uncorrelated with years of education, then the equation can be estimated with ordinary least squares.\n\nIf the researcher could randomly assign people to different levels of education, the data set thus generated would allow estimation of the effect of changes in years of education on wages. In reality, those experiments cannot be conducted. Instead, the econometrician observes the years of education of and the wages paid to people who differ along many dimensions. Given this kind of data, the estimated coefficient on Years of Education in the equation above reflects both the effect of education on wages and the effect of other variables on wages, if those other variables were correlated with education. For example, people born in certain places may have higher wages and higher levels of education. Unless the econometrician controls for place of birth in the above equation, the effect of birthplace on wages may be falsely attributed to the effect of education on wages.\n\nThe most obvious way to control for birthplace is to include a measure of the effect of birthplace in the equation above. Exclusion of birthplace, together with the assumption that formula_17 is uncorrelated with education produces a misspecified model. Another technique is to include in the equation additional set of measured covariates which are not instrumental variables, yet render formula_12 identifiable. An overview of econometric methods used to study this problem were provided by Card (1999).\n\nThe main journals that publish work in econometrics are \"Econometrica\", the \"Journal of Econometrics\", the \"Review of Economics and Statistics\", \"Econometric Theory\", the \"Journal of Applied Econometrics\", \"Econometric Reviews\", the \"Econometrics Journal\", \"Applied Econometrics and International Development\", and the \"Journal of Business & Economic Statistics\".\n\nLike other forms of statistical analysis, badly specified econometric models may show a spurious relationship where two variables are correlated but causally unrelated. In a study of the use of econometrics in major economics journals, McCloskey concluded that some economists report p-values (following the Fisherian tradition of tests of significance of point null-hypotheses) and neglect concerns of type II errors; some economists fail to report estimates of the size of effects (apart from statistical significance) and to discuss their economic importance. She also argues that some economists also fail to use economic reasoning for model selection, especially for deciding which variables to include in a regression.\n\nIn some cases, economic variables cannot be experimentally manipulated as treatments randomly assigned to subjects. In such cases, economists rely on observational studies, often using data sets with many strongly associated covariates, resulting in enormous numbers of models with similar explanatory ability but different covariates and regression estimates. Regarding the plurality of models compatible with observational data-sets, Edward Leamer urged that \"professionals ... properly withhold belief until an inference can be shown to be adequately insensitive to the choice of assumptions\".\n\n\n"}
{"id": "3597775", "url": "https://en.wikipedia.org/wiki?curid=3597775", "title": "Energy principles in structural mechanics", "text": "Energy principles in structural mechanics\n\nEnergy principles in structural mechanics express the relationships between stresses, strains or deformations, displacements, material properties, and external effects in the form of energy or work done by internal and external forces. Since energy is a scalar quantity, these relationships provide convenient and alternative means for formulating the governing equations of deformable bodies in solid mechanics. They can also be used for obtaining approximate solutions of fairly complex systems, bypassing the difficult task of solving the set of governing partial differential equations.\n\n\n\n\n\n"}
{"id": "11073025", "url": "https://en.wikipedia.org/wiki?curid=11073025", "title": "Epistemic closure", "text": "Epistemic closure\n\nEpistemic closure is a property of some belief systems. It is the principle that if a subject formula_1 knows formula_2, and formula_1 knows that formula_2 entails formula_5, then formula_1 can thereby come to know formula_5. Most epistemological theories involve a closure principle and many skeptical arguments assume a closure principle.\n\nOn the other hand, some epistemologists, including Robert Nozick, have denied closure principles on the basis of reliabilist accounts of knowledge. Nozick, in \"Philosophical Explanations\", advocated that, when considering the Gettier problem, the least counter-intuitive assumption we give up should be epistemic closure. Nozick suggested a \"truth tracking\" theory of knowledge, in which the x was said to know P if x's belief in P tracked the truth of P through the relevant modal scenarios.\n\nA subject may not actually believe q, for example, regardless of whether he or she is justified or warranted. Thus, one might instead say that knowledge is closed under \"known\" deduction: if, while knowing p, S believes q because S knows that p entails q, then S knows q. An even stronger formulation would be as such: If, while knowing various propositions, S believes p because S knows that these propositions entail p, then S knows p. While the principle of epistemic closure is generally regarded as intuitive, philosophers such as Robert Nozick and Fred Dretske have argued against it.\n\nThe epistemic closure principle typically takes the form of a modus ponens argument:\n\n\nThis epistemic closure principle is central to many versions of skeptical arguments. A skeptical argument of this type will involve knowledge of some piece of widely accepted information to be knowledge, which will then be pointed out to entail knowledge of some skeptical scenario, such as the brain in a vat scenario or the cartesian evil demon scenario. A skeptic might say, for example, that if you know that you have hands, then you know that you are not a handless brain in a vat (because knowledge that you have hands implies that you know you are not handless, and if you know that you are not handless, then you know that you are not a handless brain in a vat). The skeptic will then utilize this conditional to form a modus tollens argument. For example, the skeptic might make an argument like the following:\nMuch of the epistemological discussion surrounding this type of skeptical argument involves whether to accept or deny the conclusion, and how to do each. Ernest Sosa says that there are three possibilities in responding to the skeptic:\n\nIn the seminal 1963 paper, “Is Justified True Belief Knowledge?”, Edmund Gettier gave an assumption (later called the “principle of deducibility for justification” by Irving Thalberg, Jr.) that would serve as a basis for the rest of his piece: “for any proposition P, if S is justified in believing P and P entails Q, and S deduces Q from P and accepts Q as a result of this deduction, then S is justified in believing Q.” This was seized upon by Thalberg, who rejected the principle in order to demonstrate that one of Gettier's examples fails to support Gettier's main thesis that justified true belief is not knowledge (in the following quotation, (1) refers to “Jones will get the job”, (2) refers to “Jones has ten coins”, and (3) is the logical conjunction of (1) and (2)):\n\nWhy doesn't Gettier's principle (PDJ) hold in the evidential situation he has described? You multiply your risks of being wrong when you believe a conjunction. [… T]he most elementary theory of probability indicates that Smith's prospects of being right on both (1) and (2), namely, of being right on (3), are bound to be less favorable than his prospects of being right on either (1) or (2). In fact, Smith's chances of being right on (3) might not come up to the minimum standard of justification which (1) and (2) barely satisfy, and Smith would be unjustified in accepting (3). \n\nThe term \"epistemic closure\" has been used in U.S. political debate to refer to the claim that political belief systems can be closed systems of deduction, unaffected by empirical evidence. This use of the term was popularized by libertarian blogger and commentator Julian Sanchez in 2010 as an extreme form of confirmation bias.\n\n\n"}
{"id": "2137672", "url": "https://en.wikipedia.org/wiki?curid=2137672", "title": "European Association for Theoretical Computer Science", "text": "European Association for Theoretical Computer Science\n\nThe European Association for Theoretical Computer Science (EATCS) is an international organization with a European focus, founded in 1972. Its aim is to facilitate the exchange of ideas and results among theoretical computer scientists as well as to stimulate cooperation between the theoretical and the practical community in computer science.\n\nThe major activities of the EATCS are:\n\n\nEach year, the EATCS Award is awarded in recognition of a distinguished career in theoretical computer science. The first award was assigned to Richard Karp in 2000; the complete list of the winners is given below:\n\nStarting in 2010, the European Association of Theoretical Computer Science (EATCS) confers each year at the conference ICALP the Presburger Award to a young scientist (in exceptional cases to several young scientists) for outstanding contributions in theoretical computer science, documented by a published paper or a series of published papers. The award is named after Mojzesz Presburger who accomplished his path-breaking work on decidability of the theory of addition (which today is called Presburger arithmetic) as a student in 1929. The complete list of the winners is given below:\n\nThe EATCS Fellows Program has been established by the Association to recognize outstanding EATCS Members for their scientific achievements in the field of Theoretical Computer Science. The Fellow status is conferred by the EATCS Fellows-Selection Committee upon a person having a track record of intellectual and organizational leadership within the EATCS community. Fellows are expected to be “model citizens” of the TCS community, helping to develop the standing of TCS beyond the frontiers of the community.\n\nThree issues of the Bulletin are published annually appearing in February, June and October respectively. The Bulletin is a medium for rapid publication and wide distribution of material such as:\n\n\nSince 2013 its editor-in-chief has been Kazuo Iwama.\n\nBeginning in 2014, the European Association for Theoretical Computer Science (EATCS) established a series of Young Researcher Schools on TCS topics. A brief history of the schools follows below.\n\n"}
{"id": "1248603", "url": "https://en.wikipedia.org/wiki?curid=1248603", "title": "Excision theorem", "text": "Excision theorem\n\nIn algebraic topology, a branch of mathematics, the excision theorem is a theorem about relative homology—given topological spaces \"X\" and subspaces \"A\" and \"U\" such that \"U\" is also a subspace of \"A\", the theorem says that under certain circumstances, we can cut out (excise) \"U\" from both spaces such that the relative homologies of the pairs (\"X\",\"A\") and (\"X \\ U\",\"A \\ U\") are isomorphic. This assists in computation of singular homology groups, as sometimes after excising an appropriately chosen subspace we obtain something easier to compute. Or, in many cases, it allows the use of induction. Coupled with the long exact sequence in homology, one can derive another useful tool for the computation of homology groups, the Mayer–Vietoris sequence.\n\nMore precisely, if \"X\", \"A\", and \"U\" are as above, we say that \"U\" can be excised if the inclusion map of the pair (\"X \\ U\",\"A \\ U \") into (\"X\", \"A\") induces an isomorphism on the relative homologies \"H\"(\"X \\ U\",\"A \\ U \") to \"H\"(\"X\",\"A\") . The theorem states that if the closure of \"U\" is contained in the interior of \"A\", then \"U\" can be excised. Often, subspaces which do not satisfy this containment criterion still can be excised—it suffices to be able to find a deformation retract of the subspaces onto subspaces that do satisfy it.\n\nThe proof of the excision theorem is quite intuitive, though the details are rather involved. The idea is to subdivide the simplices in a relative cycle in (\"X\",\"A\") to get another chain consisting of \"smaller\" simplices, and continuing the process until each simplex in the chain lies entirely in the interior of \"A\" or the interior of \"X \\ U\". Since these form an open cover for \"X\" and simplices are compact, we can eventually do this in a finite number of steps. This process leaves the original homology class of the chain unchanged (this says the subdivision operator is chain homotopic to the identity map on homology). In the relative homology \"H\"(\"X\",\"A\"), then, this says all the terms contained entirely in the interior of \"U\" can be dropped without affecting the homology class of the cycle. This allows us to show that the inclusion map is an isomorphism, as each relative cycle is equivalent to one that avoids \"U\" entirely.\n\nIn the axiomatic approach to homology, the theorem is taken as one of the Eilenberg–Steenrod axioms.\n\n\n"}
{"id": "847886", "url": "https://en.wikipedia.org/wiki?curid=847886", "title": "Fermat polygonal number theorem", "text": "Fermat polygonal number theorem\n\nIn additive number theory, the Fermat polygonal number theorem states that every positive integer is a sum of at most -gonal numbers. That is, every positive integer can be written as the sum of three or fewer triangular numbers, and as the sum of four or fewer square numbers, and as the sum of five or fewer pentagonal numbers, and so on.\n\nThree such representations of the number 17, for example, are shown below:\n\nThe theorem is named after Pierre de Fermat, who stated it, in 1638, without proof, promising to write it in a separate work that never appeared.\nJoseph Louis Lagrange proved the square case in 1770, which states that every positive number can be represented as a sum of four squares, for example, . Gauss proved the triangular case in 1796, commemorating the occasion by writing in his diary the line \"ΕΥΡΗΚΑ! \", and published a proof in his book Disquisitiones Arithmeticae. For this reason, Gauss' result is sometimes known as the Eureka theorem. The full polygonal number theorem was not resolved until it was finally proven by Cauchy in 1813. The proof of is based on the following lemma due to Cauchy:\n\nFor odd positive integers and such that and we can find nonnegative integers , , , and such that\n\n\n"}
{"id": "161883", "url": "https://en.wikipedia.org/wiki?curid=161883", "title": "Formal methods", "text": "Formal methods\n\nIn computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically based technique for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.\n\nFormal methods are best described as the application of a fairly broad variety of theoretical computer science fundamentals, in particular logic calculi, formal languages, automata theory, discrete event dynamic system and program semantics, but also type systems and algebraic data types to problems in software and hardware specification and verification.\n\nFormal methods can be used at a number of levels:\n\nLevel 0: Formal specification may be undertaken and then a program developed from this informally. This has been dubbed \"formal methods lite\". This may be the most cost-effective option in many cases.\n\nLevel 1: Formal development and formal verification may be used to produce a program in a more formal manner. For example, proofs of properties or refinement from the specification to a program may be undertaken. This may be most appropriate in high-integrity systems involving safety or security.\n\nLevel 2: Theorem provers may be used to undertake fully formal machine-checked proofs. This can be very expensive and is only practically worthwhile if the cost of mistakes is extremely high (e.g., in critical parts of microprocessor design).\n\nFurther information on this is expanded below.\n\nAs with programming language semantics, styles of formal methods may be roughly classified as follows:\n\n\nSome practitioners believe that the formal methods community has overemphasized full formalization of a specification or design. They contend that the expressiveness of the languages involved, as well as the complexity of the systems being modelled, make full formalization a difficult and expensive task. As an alternative, various \"lightweight\" formal methods, which emphasize partial specification and focused application, have been proposed. Examples of this lightweight approach to formal methods include the Alloy object modelling notation, Denney's synthesis of some aspects of the Z notation with use case driven development, and the CSK VDM Tools.\n\nFormal methods can be applied at various points through the development process.\n\nFormal methods may be used to give a description of the system to be developed, at whatever level(s) of detail desired. This formal description can be used to guide further development activities (see following sections); additionally, it can be used to verify that the requirements for the system being developed have been completely and accurately specified.\n\nThe need for formal specification systems has been noted for years. In the ALGOL 58 report, John Backus presented a formal notation for describing programming language syntax, later named Backus normal form then renamed Backus–Naur form (BNF). Backus also wrote that a formal description of the meaning of syntactically valid ALGOL programs wasn't completed in time for inclusion in the report. \"Therefore the formal treatment of the semantics of legal programs will be included in a subsequent paper.\" It never appeared.\n\nOnce a formal specification has been produced, the specification may be used as a guide while the concrete system is developed during the design process (i.e., realized typically in software, but also potentially in hardware). For example:\n\n\nOnce a formal specification has been developed, the specification may be used as the basis for proving properties of the specification (and hopefully by inference the developed system).\n\nSometimes, the motivation for proving the correctness of a system is not the obvious need for reassurance of the correctness of the system, but a desire to understand the system better. Consequently, some proofs of correctness are produced in the style of mathematical proof: handwritten (or typeset) using natural language, using a level of informality common to such proofs. A \"good\" proof is one which is readable and understandable by other human readers.\n\nCritics of such approaches point out that the ambiguity inherent in natural language allows errors to be undetected in such proofs; often, subtle errors can be present in the low-level details typically overlooked by such proofs. Additionally, the work involved in producing such a good proof requires a high level of mathematical sophistication and expertise.\n\nIn contrast, there is increasing interest in producing proofs of correctness of such systems by automated means. Automated techniques fall into three general categories:\n\nSome automated theorem provers require guidance as to which properties are \"interesting\" enough to pursue, while others work without human intervention. Model checkers can quickly get bogged down in checking millions of uninteresting states if not given a sufficiently abstract model.\n\nProponents of such systems argue that the results have greater mathematical certainty than human-produced proofs, since all the tedious details have been algorithmically verified. The training required to use such systems is also less than that required to produce good mathematical proofs by hand, making the techniques accessible to a wider variety of practitioners.\n\nCritics note that some of those systems are like oracles: they make a pronouncement of truth, yet give no explanation of that truth. There is also the problem of \"verifying the verifier\"; if the program which aids in the verification is itself unproven, there may be reason to doubt the soundness of the produced results. Some modern model checking tools produce a \"proof log\" detailing each step in their proof, making it possible to perform, given suitable tools, independent verification.\n\nThe main feature of the abstract interpretation approach is that it provides a sound analysis, i.e. no false negatives are returned. Moreover, it is efficiently scalable, by tuning the abstract domain representing the property to be analyzed, and by applying widening operators to get fast convergence.\n\nFormal methods are applied in different areas of hardware and software, including routers, Ethernet switches, routing protocols, and security applications. There are several examples in which they have been used to verify the functionality of the hardware and software used in DCs. IBM used ACL2, a theorem prover, in AMD x86 processor development process. Intel uses such methods to verify its hardware and firmware (permanent software programmed into a read-only memory). Dansk Datamatik Center used formal methods in the 1980s to develop a compiler system for the Ada programming language that went on to become a long-lived commercial product.\n\nThere are several other projects of NASA in which formal methods are applied, such as Next Generation Air Transportation System, Unmanned Aircraft System integration in National Airspace System, and Airborne Coordinated Conflict Resolution and Detection (ACCoRD).\nB-Method with AtelierB, is used to develop safety automatisms for the various subways installed throughout the world by Alstom and Siemens, and also for Common Criteria certification and the development of system models by ATMEL and STMicroelectronics.\n\nFormal verification has been frequently used in hardware by most of the well-known hardware vendors, such as IBM, Intel, and AMD. There are many areas of hardware, where Intel have used FMs to verify the working of the products, such as parameterized verification of cache coherent protocol, Intel Core i7 processor execution engine validation (using theorem proving, BDDs, and symbolic evaluation), optimization for Intel IA-64 architecture using HOL light theorem prover, and verification of high performance dual-port gigabit Ethernet controller with a support for PCI express protocol and Intel advance management technology using Cadence. Similarly, IBM has used formal methods in the verification of power gates, registers, and functional verification of the IBM Power7 microprocessor.\n\nIn software development, formal methods are mathematical approaches to solving software (and hardware) problems at the requirements, specification, and design levels. Formal methods are most likely to be applied to safety-critical or security-critical software and systems, such as avionics software. Software safety assurance standards, such as DO-178B, DO-178C, and Common Criteria demand formal methods at the highest levels of categorization.\n\nFor sequential software, examples of formal methods include the B-Method, the specification languages used in automated theorem proving, RAISE, and the Z notation.\n\nIn functional programming, property-based testing has allowed the mathematical specification and testing (if not exhaustive testing) of the expected behaviour of individual functions.\n\nThe Object Constraint Language (and specializations such as Java Modeling Language) has allowed object-oriented systems to be formally specified, if not necessarily formally verified.\n\nFor concurrent software and systems, Petri nets, process algebra, and finite state machines (which are based on automata theory - see also virtual finite state machine or event driven finite state machine) allow executable software specification and can be used to build up and validate application behavior.\n\nAnother approach to formal methods in software development is to write a specification in some form of logic—usually a variation of first-order logic (FOL)—and then to directly execute the logic as though it were a program. The OWL language, based on Description Logic (DL), is an example. There is also work on mapping some version of English (or another natural language) automatically to and from logic, and executing the logic directly. Examples are Attempto Controlled English, and Internet Business Logic, which do not seek to control the vocabulary or syntax. A feature of systems that support bidirectional English-logic mapping and direct execution of the logic is that they can be made to explain their results, in English, at the business or scientific level.\n\nThere are a variety of formal methods and notations available.\n\n\n\n\n\n\n"}
{"id": "6805386", "url": "https://en.wikipedia.org/wiki?curid=6805386", "title": "Fáry's theorem", "text": "Fáry's theorem\n\nIn mathematics, Fáry's theorem states that any simple planar graph can be drawn without crossings so that its edges are straight line segments. That is, the ability to draw graph edges as curves instead of as straight line segments does not allow a larger class of graphs to be drawn. The theorem is named after István Fáry, although it was proved independently by , , and .\n\nOne way of proving Fáry's theorem is to use mathematical induction. Let be a simple plane graph with vertices; we may add edges if necessary so that is a maximally plane graph. If < 3, the result is trivial. If ≥ 3, then all faces of must be triangles, as we could add an edge into any face with more sides while preserving planarity, contradicting the assumption of maximal planarity. Choose some three vertices forming a triangular face of . We prove by induction on that there exists a straight-line combinatorially isomorphic re-embedding of in which triangle is the outer face of the embedding. (\"Combinatorially isomorphic\" means that the vertices, edges, and faces in the new drawing can be made to correspond to those in the old drawing, such that all incidences between edges, vertices, and faces—not just between vertices and edges—are preserved.) As a base case, the result is trivial when and , and are the only vertices in . Otherwise, all vertices in have at least three neighbors.\n\nBy Euler's formula for planar graphs, has edges; equivalently, if one defines the \"deficiency\" of a vertex in to be , the sum of the deficiencies is . Each vertex in can have deficiency at most three, so there are at least four vertices with positive deficiency. In particular we can choose a vertex with at most five neighbors that is different from , and . Let be formed by removing from and retriangulating the face formed by removing . By induction, has a combinatorially isomorphic straight line re-embedding in which is the outer face. Because the re-embedding of was combinatorially isomorphic to , removing from it the edges which were added to create leaves the face , which is now a polygon with at most five sides. To complete the drawing to a straight-line combinatorially isomorphic re-embedding of , should be placed in the polygon and joined by straight lines to the vertices of the polygon. By the art gallery theorem, there exists a point interior to at which can be placed so that the edges from to the vertices of do not cross any other edges, completing the proof.\n\nThe induction step of this proof is illustrated at right.\nDe Fraysseix, Pach and Pollack showed how to find in linear time a straight-line drawing in a grid with dimensions linear in the size of the graph, giving a universal point set with quadratic size. A similar method has been followed by Schnyder to prove enhanced bounds and a characterization of planarity based on the incidence partial order. His work stressed the existence of a particular partition of the edges of a maximal planar graph into three trees known as a Schnyder wood.\n\nTutte's spring theorem states that every 3-connected planar graph can be drawn on a plane without crossings so that its edges are straight line segments and an outside face is a convex polygon (Tutte 1963). It is so called because such an embedding can be found as the equilibrium position for a system of springs representing the edges of the graph.\n\nSteinitz's theorem states that every 3-connected planar graph can be represented as the edges of a convex polyhedron in three-dimensional space. A straight-line embedding of formula_1 of the type described by Tutte's theorem, may be formed by projecting such a polyhedral representation onto the plane.\n\nThe Circle packing theorem states that every planar graph may be represented as the intersection graph of a collection of non-crossing circles in the plane. Placing each vertex of the graph at the center of the corresponding circle leads to a straight line representation.\nHeiko Harborth raised the question of whether every planar graph has a straight line representation in which all edge lengths are integers. The truth of Harborth's conjecture remains unknown . However, integer-distance straight line embeddings are known to exist for cubic graphs.\n\n\n"}
{"id": "33994467", "url": "https://en.wikipedia.org/wiki?curid=33994467", "title": "Grunsky matrix", "text": "Grunsky matrix\n\nIn mathematics, the Grunsky matrices, or Grunsky operators, are matrices introduced by in complex analysis and geometric function theory. They correspond to either a single holomorphic function on the unit disk or a pair of holomorphic functions on the unit disk and its complement. The Grunsky inequalities express boundedness properties of these matrices, which in general are contraction operators or in important special cases unitary operators. As Grunsky showed, these inequalities hold if and only if the holomorphic function is univalent. The inequalities are equivalent to the inequalities of Goluzin, discovered in 1947. Roughly speaking, the Grunsky inequalities give information on the coefficients of the logarithm of a univalent function; later generalizations by Milin, starting from the Lebedev–Milin inequality, succeeded in exponentiating the inequalities to obtain inequalities for the coefficients of the univalent function itself. Historically the inequalities were used in proving special cases of the Bieberbach conjecture up to the sixth coefficient; the exponentiated inequalities of Milin were used by de Branges in the final solution. The Grunsky operators and their Fredholm determinants are related to spectral properties of bounded domains in the complex plane. The operators have further applications in conformal mapping, Teichmüller theory and conformal field theory.\n\nIf \"f\"(\"z\") is a holomorphic univalent function on the unit disk, normalized so that \"f\"(0) = 0 and \"f\"'(0) = 1, the function\n\nis a non-vanishing univalent function on |\"z\"| > 1 having a simple pole at ∞ with residue 1:\n\nThe same inversion formula applied to \"g\" gives back \"f\" and establishes a one-one correspondence\nbetween these two classes of function.\n\nThe Grunsky matrix (\"c\") of \"g\" is defined by the equation\n\nIt is a symmetric matrix. Its entries are called the Grunsky coefficients of \"g\".\n\nNote that\n\nso that the coefficients can be expressed directly in terms of \"f\". Indeed, if\n\nthen for \"m\", \"n\" > 0\n\nand \"d\" = \"d\" is given by\n\nwith\n\nIf \"f\" is a holomorphic function on the unit disk with Grunsky matrix (\"c\"), the Grunsky inequalities state that\n\nfor any finite sequence of complex numbers λ, ..., λ.\n\nThe Grunsky coefficients of a normalized univalent function in |\"z\"| > 1\n\nare polynomials in the coefficients \"b\" which can be computed recursively\nin terms of the Faber polynomials Φ, a monic polynomial of degree \"n\" depending on \"g\".\n\nTaking the derivative in \"z\" of the defining relation of the Grunsky coefficients and multiplying by \"z\"\ngives\n\nThe Faber polynomials are defined by the relation\n\nDividing this relation by \"z\" and integrating between \"z\" and ∞ gives\n\nThis gives the recurrence relations for \"n\" > 0\n\nwith\n\nThus\n\nso that for \"n\" ≥ 1\n\nThe latter property uniquely determines the Faber polynomial of \"g\".\n\nLet \"g\"(\"z\") be a univalent function on |\"z\"| > 1 normalized so that\n\nand let \"f\"(\"z\") be a non-constant holomorphic function on C.\n\nIf\n\nis the Laurent expansion on \"z\" > 1, then\n\nIf Ω is a bounded open region with smooth boundary ∂Ω and \"h\" is a differentiable function on Ω extending to a continuous function on the closure,\nthen, by Stokes' theorem applied to the differential 1-form ω = \"h(z)dz\",\n\nFor \"r\" > 1, let Ω be the complement of the image of |\"z\"|> \"r\" under \"g\"(\"z\"), a bounded domain. Then, by the above identity with \"h\" = \"f\"', the area of\n\"f\"(Ω) is given by\n\nThe proof proceeds by computing the area of the image of the complement of the images of |\"z\"| < \"r\" < 1 under \"F\" and |ζ| > \"R\" >1 under \"g\"\nunder a suitable Laurent polynomial \"h\"(\"w\").\n\nLet Φ and Φ denote the Faber polynomials of \"g\" and formula_23 and set\n\nThen for |\"z\"| < 1\n\nand for |ζ| > 1\n\nThe area equals\n\nwhere \"C\" is the image of the circle |ζ| = \"R\" under \"g\" and \"C\" is the image of the circle |\"z\"| = \"r\" under \"F\".\n\nHence\n\nSince the area is positive, the right hand side must also be positive. Letting \"r\" increase to 1 and \"R\" decrease to \"1\", it follows that\n\nwith equality if and only if the complement of the images has Lebesgue measure zero.\n\nAs in the case of a single function \"g\", this implies the required inequality.\n\nThe matrix\n\nof a single function \"g\" or a pair of functions \"F\", \"g\" is unitary if and only if the complement of the image of \"g\" or the union of the images of \"F\" and \"g\" has Lebesgue measure zero. So, roughly speaking, in the case of one function the image is a slit region in the complex plane; and in the case of two functions the two regions are separated by a closed Jordan curve.\n\nIn fact the infinite matrix \"A\" acting on the Hilbert space of square summable sequences satisfies\n\nBut if \"J\" denotes complex conjugation of a sequence, then\n\nsince \"A\" is symmetric. Hence\n\nso that \"A\" is unitary.\n\nIf \"g\"(\"z\") is a normalized univalent function in |\"z\"| > 1, \"z\", ..., \"z\" are distinct points with |\"z\"| > 1 and\nα, ..., α are complex numbers, the Goluzin inequalities, proved in 1947 by the Russian mathematician Gennadi Mikhailovich Goluzin (1906-1953), state that\n\nTo deduce them from the Grunsky inequalities, let\n\nfor \"k\" > 0.\n\nConversely the Grunsky inequalities follow from the Goluzin inequalities by taking\n\nwhere\n\nwith \"r\" > 1, tending to ∞.\n\n gave another derivation of the Grunsky inequalities using reproducing kernels and singular integral operators in geometric function theory; a more recent related approach can be found in . Let \"f\"(\"z\") be a normalized univalent function in |\"z\"| < 1, let \"z\", ..., \"z\" be distinct points with |\"z\"| < 1 and let\nα, ..., α be complex numbers.\n\nThe Bergman-Schiffer inequalities state that\n\nTo deduce these inequalities from the Grunsky inequalities, set\n\nfor \"k\" > 0.\n\nConversely the Grunsky inequalities follow from the Bergman-Schiffer inequalities by taking\n\nwhere\n\nwith \"r\" < 1, tending to 0.\n\nThe Grunsky inequalities imply many inequalities for univalent functions. They were also used by Schiffer and Charzynski in 1960 to give a completely elementary proof of the Bieberbach conjecture for the fourth coefficient; a far more complicated proof had previously been found by Schiffer and Garabedian in 1955. In 1968 Pedersen and Ozawa independently used the Grunsky inequalities to prove the conjecture for the sixth coefficient.\n\nIn the proof of Schiffer and Charzynski, if\n\nis a normalized univalent function in |\"z\"| < 1, then\n\nis an odd univalent function in |\"z\"| > 1.\n\nCombining Gronwall's area theorem for \"f\" with the Grunsky inequalities for the first 2 x 2 minor of the Grunsky matrix of \"g\" leads to a bound for |\"a\"| in terms of a simple function of \"a\" and a free complex parameter. The free parameter can be chosen so that the bound becomes a function of half the modulus of \"a\" and it can then be checked directly that this function is no greater than 4 on the range [0,1].\n\nAs Milin showed, the Grunsky inequalities can be exponentiated. The simplest case proceeds by writing\n\nwith \"a\"(\"w\") holomorphic in |\"w\"| < 1.\n\nThe Grunsky inequalities, with λ = \"w\" imply that\n\nOn the other hand, if\n\nas formal power series, then the first of the Lebedev–Milin inequalities (1965) states that\n\nEquivalently the inequality states that if \"g\"(\"z\") is a polynomial with \"g\"(0) = 0, then\n\nwhere \"A\" is the area of \"g\"(\"D\"),\n\nTo prove the inequality, note that the coefficients are determined by the recursive formula\n\nso that by the Cauchy–Schwarz inequality\n\nThe quantities \"c\" obtained by imposing equality here:\n\nsatisfy formula_52 and hence, reversing the steps,\n\nIn particular defining \"b\"(\"w\") by the identity\n\nthe following inequality must hold for |\"w\"| < 1\n\nThe Beurling transform (also called the Beurling-Ahlfors transform and the Hilbert transform in the complex plane) provides one of the most direct methods of proving the Grunsky inequalities, following and .\n\nThe Beurling transform is defined on \"L\"(C) as the operation of multiplication by formula_56 on Fourier transforms. It thus defines a unitary operator. It can also be defined directly as a principal value integral\n\nFor any bounded open region Ω in C it defines a bounded operator \"T\" from the conjugate of the Bergman space of Ω onto the Bergman space of Ω: a square integrable holomorphic function is extended to 0 off Ω to produce a function in \"L\"(C) to which \"T\" is applied and the result restricted to Ω, where it is holomorphic. If \"f\" is a holomorphic univalent map from the unit disk \"D\" onto Ω then the Bergman space of Ω and its conjugate can be identified with that of \"D\" and \"T\" becomes the singular integral operator with kernel\n\nIt defines a contraction. On the other hand, it can be checked that \"T\" = 0 by computing directly on powers formula_59 using Stokes theorem to transfer the integral to the boundary.\n\nIt follows that the operator with kernel\n\nacts as a contraction on the conjugate of the Bergman space of \"D\". Hence, if\n\nthen\n\nIf Ω is a bounded domain in C with smooth boundary, the operator \"T\" can be regarded as a bounded antilinear contractive operator on the Bergman space \"H\" = \"A\"(Ω). It is given by the formula\n\nfor \"u\" in the Hilbert space \"H\"= \"A\"(Ω). \"T\" is called the Grunsky operator of Ω (or \"f\"). Its realization on \"D\" using a univalent function \"f\" mapping \"D\" onto Ω and the fact that \"T\" = 0 shows that it is given by restriction of the kernel\n\nand is therefore a Hilbert–Schmidt operator.\n\nThe antilinear operator \"T\" = \"T\" satisfies the self-adjointness relation\n\nfor \"u\", \"v\" in \"H\".\n\nThus \"A\" = \"T\" is a compact self-adjont linear operator on \"H\" with\n\nso that \"A\" is a positive operator. By the spectral theorem for compact self-adjoint operators, there is an orthonormal basis \"u\" of \"H\" consisting of eigenvectors of \"A\":\n\nwhere μ is non-negative by the positivity of \"A\". Hence\n\nwith λ ≥ 0. Since \"T\" commutes with \"A\", it leaves its eigenspaces invariant. The positivity relation shows that it acts trivially on the zero eigenspace. The other non-zero eigenspaces are all finite-dimensional and mutually orthogonal. Thus an orthonormal basis can be chosen on each eigenspace so that:\n\nThe non-zero λ (or sometimes their reciprocals) are called the Fredholm eigenvalues of Ω:\n\nIf Ω is a bounded domain that is not a disk, Ahlfors showed that\n\nThe Fredholm determinant for the domain Ω is defined by\n\nNote that this makes sense because \"A\" = \"T\" is a trace class operator.\n\nHere the norms are in the Bergman spaces of \"D\" and its complement \"D\" and \"g\" is a univalent map from \"D\" onto Ω fixing ∞.\n\nA similar formula applies in the case of a pair of univalent functions (see below).\n\nLet Ω be a bounded simply connected domain in C with smooth boundary \"C\" = ∂Ω. Thus there is a univalent holomorphic map \"f\" from the unit disk \"D\" onto Ω extending to a smooth map between the boundaries \"S\" and \"C\".\n\n"}
{"id": "5133456", "url": "https://en.wikipedia.org/wiki?curid=5133456", "title": "Haven (graph theory)", "text": "Haven (graph theory)\n\nIn graph theory, a haven is a certain type of function on sets of vertices in an undirected graph. If a haven exists, it can be used by an evader to win a pursuit-evasion game on the graph, by consulting the function at each step of the game to determine a safe set of vertices to move into. Havens were first introduced by as a tool for characterizing the treewidth of graphs. Their other applications include proving the existence of small separators on minor-closed families of graphs, and characterizing the ends and clique minors of infinite graphs.\n\nIf \"G\" is an undirected graph, and \"X\" is a set of vertices, then an \"X\"-flap is a nonempty connected component of the subgraph of \"G\" formed by deleting \"X\". A haven of order \"k\" in \"G\" is a function \"β\" that assigns an \"X\"-flap \"β\"(\"X\") to every set \"X\" of fewer than \"k\" vertices. This function must also satisfy additional constraints which are given differently by different authors.\nThe number \"k\" is called the \"order\" of the haven.\n\nIn the original definition of Seymour and Thomas, a haven is required to satisfy the property that every two flaps \"β\"(\"X\") and \"β\"(\"Y\") must touch each other: either they share a common vertex or there exists an edge with one endpoint in each flap. In the definition used later by Alon, Seymour, and Thomas, havens are instead required to satisfy a weaker monotonicity property: if , and both \"X\" and \"Y\" have fewer than \"k\" vertices, then . The touching property implies the monotonicity property, but not necessarily vice versa. However, it follows from the results of Seymour and Thomas that, in finite graphs, if a haven with the monotonicity property exists, then one with the same order and the touching property also exists.\nHavens with the touching definition are closely related to brambles, families of connected subgraphs of a given graph that all touch each other. The order of a bramble is the minimum number of vertices needed in a set of vertices that hits all of the subgraphs in the family. The set of flaps \"β\"(\"X\") for a haven of order \"k\" (with the touching definition) forms a bramble of order at least \"k\", because any set \"Y\" of fewer than \"k\" vertices fails to hit the subgraph \"β\"(\"Y\"). Conversely, from any bramble of order \"k\", one may construct a haven of the same order, by defining \"β\"(\"X\") (for each choice of \"X\") to be the \"X\"-flap that includes all of the subgraphs in the bramble that are disjoint from \"X\". The requirement that the subgraphs in the bramble all touch each other can be used to show that this \"X\"-flap exists, and that all of the flaps \"β\"(\"X\") chosen in this way touch each other. Thus, a graph has a bramble of order \"k\" if and only if it has a haven of order \"k\".\n\nAs an example, let \"G\" be a nine-vertex grid graph. Define a haven of order 4 in \"G\", mapping each set \"X\" of three or fewer vertices to an \"X\"-flap \"β\"(\"X\"), as follows:\nIt is straightforward to verify by a case analysis that this function \"β\" satisfies the required monotonicity property of a haven. If and \"X\" has fewer than two vertices, or \"X\" has two vertices that are not the two neighbors of a corner vertex of the grid, then there is only one \"X\"-flap and it contains every \"Y\"-flap. In the remaining case, \"X\" consists of the two neighbors of a corner vertex and has two \"X\"-flaps: one consisting of that corner vertex, and another (chosen as \"β\"(\"X\")) consisting of the six remaining vertices. No matter which vertex is added to \"X\" to form \"Y\", there will be a \"Y\"-flap with at least four vertices, which must be the unique largest flap since it contains more than half of the vertices not in \"Y\". This large \"Y\"-flap will be chosen as \"β\"(\"Y\") and will be a subset of \"β\"(\"X\"). Thus in each case monotonicity holds.\n\nHavens model a certain class of strategies for an evader in a pursuit-evasion game in which fewer than \"k\" pursuers attempt to capture a single evader, the pursuers and evader are both restricted to the vertices of a given undirected graph, and the positions of the pursuers and evader are known to both players. At each move of the game, a new pursuer may be added to an arbitrary vertex of the graph (as long as fewer than \"k\" pursuers are placed on the graph at any time) or one of the already-added pursuers may be removed from the graph. However, before a new pursuer is added, the evader is first informed of its new location and may move along the edges of the graph to any unoccupied vertex. While moving, the evader may not pass through any vertex that is already occupied by any of the pursuers.\n\nIf a \"k\"-haven (with the monotonicity property) exists, then the evader may avoid being captured indefinitely, and win the game, by always moving to a vertex of \"β\"(\"X\") where \"X\" is the set of vertices that will be occupied by pursuers at the end of the move. The monotonicity property of a haven guarantees that, when a new pursuer is added to a vertex of the graph, the vertices in \"β\"(\"X\") are always reachable from the current position of the evader.\n\nFor instance, an evader can win this game against three pursuers on a grid by following this strategy with the haven of order 4 described in the example. However, on the same graph, four pursuers can always capture the evader, by first moving onto three vertices that split the grid onto two three-vertex paths, then moving into the center of the path containing the evader, forcing the evader into one of the corner vertices, and finally removing one of the pursuers that is not adjacent to this corner and placing it onto the evader. Therefore, the grid can have no haven of order 5.\n\nHavens with the touching property allow the evader to win the game against more powerful pursuers that may simultaneously jump from one set of occupied vertices to another.\n\nHavens may be used to characterize the treewidth of graphs: a graph has a haven of order \"k\" if and only if it has treewidth at least . A tree decomposition may be used to describe a winning strategy for the pursuers in the same pursuit-evasion game, so it is also true that a graph has a haven of order \"k\" if and only if the evader wins with best play against fewer than \"k\" pursuers. In games won by the evader, there is always an optimal strategy in the form described by a haven, and in games won by the pursuer, there is always an optimal strategy in the form described by a tree decomposition. For instance, because the grid has a haven of order 4, but does not have a haven of order 5, it must have treewidth exactly 3. The same min-max theorem can be generalized to infinite graphs of finite treewidth, with a definition of treewidth in which the underlying tree is required to be rayless (that is, having no ends).\n\nHavens are also closely related to the existence of separators, small sets \"X\" of vertices in an \"n\"-vertex graph such that every \"X\"-flap has at most 2\"n\"/3 vertices. If a graph \"G\" does not have a \"k\"-vertex separator, then every set \"X\" of at most \"k\" vertices has a (unique) \"X\"-flap with more than 2\"n\"/3 vertices. In this case, \"G\" has a haven of order , in which \"β\"(\"X\") is defined to be this unique large \"X\"-flap. That is, every graph has either a small separator or a haven of high order.\n\nIf a graph \"G\" has a haven of order \"k\", with for some integer \"h\", then \"G\" must also have a complete graph \"K\" as a minor. In other words, the Hadwiger number of an \"n\"-vertex graph with a haven of order \"k\" is at least \"k\"\"n\". As a consequence, the \"K\"-minor-free graphs have treewidth less than \"h\"\"n\" and separators of size less than \"h\"\"n\". More generally an O() bound on treewidth and separator size holds for any nontrivial family of graphs that can be characterized by forbidden minors, because for any such family there is a constant \"h\" such that the family does not include \"K\".\n\nIf a graph \"G\" contains a ray, a semi-infinite simple path with a starting vertex but no ending vertex, then it has a haven of order ℵ: that is, a function \"β\" that maps each finite set \"X\" of vertices to an \"X\"-flap, satisfying the consistency condition for havens. Namely, define \"β\"(\"X\") to be the unique \"X\"-flap that contains infinitely many vertices of the ray. Thus, in the case of infinite graphs the connection between treewidth and havens breaks down: a single ray, despite itself being a tree, has havens of all finite orders and even more strongly a haven of order ℵ. Two rays of an infinite graph are considered to be equivalent if there is no finite set of vertices that separates infinitely many vertices of one ray from infinitely many vertices of the other ray; this is an equivalence relation, and its equivalence classes are called ends of the graph.\n\nThe ends of any graph are in one-to-one correspondence with its havens of order ℵ. For, every ray determines a haven, and every two equivalent rays determine the same haven. Conversely, every haven is determined by a ray in this way, as can be shown by the following case analysis:\nThus, every equivalence class of rays defines a unique haven, and every haven is defined by an equivalence class of rays.\n\nFor any cardinal number formula_3, an infinite graph \"G\" has a haven of order κ if and only if it has a clique minor of order κ. That is, for uncountable cardinalities, the largest order of a haven in \"G\" is the Hadwiger number of \"G\".\n"}
{"id": "25389521", "url": "https://en.wikipedia.org/wiki?curid=25389521", "title": "Heartbeat message", "text": "Heartbeat message\n\nA heartbeat message in signal processing is a message sent from an originator to a destination that enables the destination to identify if and when the originator fails or is no longer available. Heartbeat messages are typically sent non-stop on a periodic or recurring basis from the originator's start-up until the originator's shutdown. When the destination identifies a lack of heartbeat messages during an anticipated arrival period, the destination may determine that the originator has failed, shutdown, or is generally no longer available. Heartbeat messages may be used for high-availability and fault tolerance purposes.\n"}
{"id": "4907086", "url": "https://en.wikipedia.org/wiki?curid=4907086", "title": "Hyper-encryption", "text": "Hyper-encryption\n\nHyper-encryption is a form of encryption invented by Michael O. Rabin which uses a high-bandwidth source of public random bits, together with a secret key that is shared by only the sender and recipient(s) of the message. It uses the assumptions of Ueli Maurer's bounded-storage model as the basis of its secrecy. Although everyone can see the data, decryption by adversaries without the secret key is still not feasible, because of the space limitations of storing enough data to mount an attack against the system.\n\nUnlike almost all other cryptosystems except the one-time pad, hyper-encryption can be proved to be information-theoretically secure, provided the storage bound cannot be surpassed. Moreover, if the necessary public information cannot be stored at the time of transmission, the plaintext can be shown to be impossible to recover, regardless of the computational capacity available to an adversary in the future, even if they have access to the secret key at that future time.\n\nA highly energy-efficient implementation of a hyper-encryption chip was demonstrated by Krishna Palem et al. using the Probabilistic CMOS or PCMOS technology and was shown to be ~205 times more efficient in terms of Energy-Performance-Product.\n\n\n\n"}
{"id": "22015452", "url": "https://en.wikipedia.org/wiki?curid=22015452", "title": "ICTCM Award", "text": "ICTCM Award\n\nThe ICTCM Award is presented each year at the International Conference on Technology in Collegiate Mathematics sponsored by Pearson Addison–Wesley & Pearson Prentice Hall publishers. This award, now in its twelfth year, was established by Pearson Education to recognize an individual or group for excellence and innovation in using technology to enhance the teaching and learning of mathematics. Electronic conference proceedings are available beginning with ICTCM 7. List of free electronic journals in mathematics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "349336", "url": "https://en.wikipedia.org/wiki?curid=349336", "title": "Integral geometry", "text": "Integral geometry\n\nIn mathematics, integral geometry is the theory of measures on a geometrical space invariant under the symmetry group of that space. In more recent times, the meaning has been broadened to include a view of invariant (or equivariant) transformations from the space of functions on one geometrical space to the space of functions on another geometrical space. Such transformations often take the form of integral transforms such as the Radon transform and its generalizations.\n\nIntegral geometry as such first emerged as an attempt to refine certain statements of geometric probability theory. The early work of Luis Santaló and Wilhelm Blaschke was in this connection. It follows from the classic theorem of Crofton expressing the length of a plane curve as an expectation of the number of intersections with a random line. Here the word 'random' must be interpreted as subject to correct symmetry considerations.\n\nThere is a sample space of lines, one on which the affine group of the plane acts. A probability measure is sought on this space, invariant under the symmetry group. If, as in this case, we can find a unique such invariant measure, that solves the problem of formulating accurately what 'random line' means; and expectations become integrals with respect to that measure. (Note for example that the phrase 'random chord of a circle' can be used to construct some paradoxes--for example Bertrand's paradox.)\n\nWe can therefore say that integral geometry in this sense is the application of probability theory (as axiomatized by Kolmogorov) in the context of the Erlangen programme of Klein. The content of the theory is effectively that of invariant (smooth) measures on (preferably compact) homogeneous spaces of Lie groups; and the evaluation of integrals of the differential forms.\n\nA very celebrated case is the problem of Buffon's needle: drop a needle on a floor made of planks and calculate the probability the needle lies across a crack. Generalising, this theory is applied to various stochastic processes concerned with geometric and incidence questions. See stochastic geometry.\n\nOne of the most interesting theorems in this form of integral geometry is Hadwiger's theorem.\n\nThe more recent meaning of integral geometry is that of Sigurdur Helgason and Israel Gelfand. It deals more specifically with integral transforms, modeled on the Radon transform. Here the underlying geometrical incidence relation (points lying on lines, in Crofton's case) is seen in a freer light, as the site for an integral transform composed as \"pullback onto the incidence graph\" and then \"push forward\".\n"}
{"id": "58487989", "url": "https://en.wikipedia.org/wiki?curid=58487989", "title": "Jean-Benoît Bost", "text": "Jean-Benoît Bost\n\nJean-Benoît Bost (born 27 July 1961 in Neuilly-sur-Seine) is a French mathematician.\n\nIn 1977 Bost graduated from the Lycée Louis-le-Grand and finished first in the Concours général, the national competition for the places at the elite schools. Bost studied from 1979 to 1983 (qualifying in 1981 for the \"agrégation des mathématiques\") at the École Normale Supérieure (ENS), where he was from 1984 to 1988 \"agrégé-préparateur\" (teacher) and worked under the direction of Alain Connes. From 1988 he was \"chargé de recherches\" and from 1993 \"directeur de recherches\" at CNRS. From 1993 to 2006 he was \"maître de conferences\" at the École polytechnique. He has been a professor at l'Université Paris-Sud (Paris XI) in Orsay since 1998.\n\nBost deals with noncommutative geometry (partly in collaboration with Alain Connes) with applications to quantum field theory, algebraic geometry, and arithmetic geometry. The eponymous Bost conjecture is a generalization of the Baum–Connes conjecture.\n\nIn 1990 he received the Prix Peccot-Vimont of the Collège de France. In 2002 he received the Prix Élie Cartan of the Académie des sciences. In 1986 he was an invited speaker at the International Congress on Mathematical Physics in Marseille. In 2006 he was an invited speaker with talk \"Evaluation maps, slopes, and algebraicity criteria\" at the International Congress of Mathematicians in Madrid.\n\nFrom 2005 to 2015 Bost was a senior member of the Institut Universitaire de France. He was elected in 2012 a Fellow of the American Mathematical Society and in 2016 a member of Academia Europaea.\n\n\n\n"}
{"id": "24956783", "url": "https://en.wikipedia.org/wiki?curid=24956783", "title": "Matching polynomial", "text": "Matching polynomial\n\nIn the mathematical fields of graph theory and combinatorics, a matching polynomial (sometimes called an acyclic polynomial) is a generating function of the numbers of matchings of various sizes in a graph. It is one of several graph polynomials studied in algebraic graph theory.\n\nSeveral different types of matching polynomials have been defined. Let \"G\" be a graph with \"n\" vertices and let \"m\" be the number of \"k\"-edge matchings.\n\nOne matching polynomial of \"G\" is\n\nAnother definition gives the matching polynomial as\n\nA third definition is the polynomial\n\nEach type has its uses, and all are equivalent by simple transformations. For instance,\nand\n\nThe first type of matching polynomial is a direct generalization of the rook polynomial.\n\nThe second type of matching polynomial has remarkable connections with orthogonal polynomials. For instance, if \"G\" = \"K\", the complete bipartite graph, then the second type of matching polynomial is related to the generalized Laguerre polynomial \"L\"(\"x\") by the identity:\n\nIf \"G\" is the complete graph \"K\", then \"M\"(\"x\") is an Hermite polynomial:\nwhere \"H\"(\"x\") is the \"probabilist's Hermite polynomial\" (1) in the definition of Hermite polynomials. These facts were observed by .\n\nIf \"G\" is a forest, then its matching polynomial is equal to the characteristic polynomial of its adjacency matrix.\n\nIf \"G\" is a path or a cycle, then \"M\"(\"x\") is a Chebyshev polynomial. In this case\nμ(1,\"x\") is a Fibonacci polynomial or Lucas polynomial respectively.\n\nThe matching polynomial of a graph \"G\" with \"n\" vertices is related to that of its complement by a pair of (equivalent) formulas. One of them is a simple combinatorial identity due to . The other is an integral identity due to .\n\nThere is a similar relation for a subgraph \"G\" of \"K\" and its complement in \"K\". This relation, due to Riordan (1958), was known in the context of non-attacking rook placements and rook polynomials.\n\nThe Hosoya index of a graph \"G\", its number of matchings, is used in chemoinformatics as a structural descriptor of a molecular graph. It may be evaluated as \"m\"(1) .\n\nThe third type of matching polynomial was introduced by as a version of the \"acyclic polynomial\" used in chemistry.\n\nOn arbitrary graphs, or even planar graphs, computing the matching polynomial is #P-complete . However, it can be computed more efficiently when additional structure about the graph is known. In particular,\ncomputing the matching polynomial on \"n\"-vertex graphs of treewidth \"k\" is fixed-parameter tractable: there exists an algorithm whose running time, for any fixed constant \"k\", is a polynomial in \"n\" with an exponent that does not depend on \"k\" .\nThe matching polynomial of a graph with \"n\" vertices and clique-width \"k\" may be computed in time \"n\" \n\n"}
{"id": "374235", "url": "https://en.wikipedia.org/wiki?curid=374235", "title": "MathWorld", "text": "MathWorld\n\nMathWorld is an online mathematics reference work, created and largely written by Eric W. Weisstein. It is sponsored by and licensed to Wolfram Research, Inc. and was partially funded by the National Science Foundation's National Science Digital Library grant to the University of Illinois at Urbana–Champaign.\n\nEric W. Weisstein, the creator of the site, was a physics and astronomy student who got into the habit of writing notes on his mathematical readings. In 1995 he put his notes online and called it \"Eric's Treasure Trove of Mathematics.\" It contained hundreds of pages/articles, covering a wide range of mathematical topics. The site became popular as an extensive single resource on mathematics on the web. Weisstein continuously improved the notes and accepted corrections and comments from online readers. In 1998, he made a contract with CRC Press and the contents of the site were published in print and CD-ROM form, titled \"CRC Concise Encyclopedia of Mathematics.\" The free online version became only partially accessible to the public. In 1999 Weisstein went to work for Wolfram Research, Inc. (WRI), and WRI renamed the Math Treasure Trove to \"MathWorld\" and hosted it on the company's website without access restrictions.\n\nIn 2000, CRC Press sued Wolfram Research Inc. (WRI), WRI president Stephen Wolfram, and author Eric Weisstein, due to what they considered a breach of contract: that the \"MathWorld\" content was to remain in print only. The site was taken down by a court injunction.\n\nThe case was later settled out of court, with WRI paying an unspecified amount and complying with other stipulations. Among these stipulations is the inclusion of a copyright notice at the bottom of the website and broad rights for the CRC Press to produce \"MathWorld\" in printed book form. The site then became once again available free to the public.\n\nThis case made a wave of headlines in online publishing circles. The \"PlanetMath\" project was a result of MathWorld's being unavailable.\n\n"}
{"id": "5334646", "url": "https://en.wikipedia.org/wiki?curid=5334646", "title": "Minifloat", "text": "Minifloat\n\nIn computing, minifloats are floating-point values represented with very few bits. Predictably, they are not well suited for general-purpose numerical calculations. They are used for special purposes, most often in computer graphics, where iterations are small and precision has aesthetic effects. Additionally, they are frequently encountered as a pedagogical tool in computer-science courses to demonstrate the properties and structures of floating-point arithmetic and IEEE 754 numbers.\n\nMinifloats with 16 bits are half-precision numbers (opposed to single and double precision). There are also minifloats with 8 bits or even fewer.\n\nMinifloats can be designed following the principles of the IEEE 754 standard. In this case they must obey the (not explicitly written) rules for the frontier between subnormal and normal numbers and must have special patterns for infinity and NaN. Normalized numbers are stored with a biased exponent. The new revision of the standard, IEEE 754-2008, has 16-bit binary minifloats.\n\nThe Radeon R300 and R420 GPUs used an \"fp24\" floating-point format with 7 bits of exponent and 16 bits (+1 implicit) of mantissa.\n\"Full Precision\" in Direct3D 9.0 is a proprietary 24-bit floating-point format. Microsoft's D3D9 (Shader Model 2.0) graphics API initially supported both FP24 (as in ATI's R300 chip) and FP32 (as in Nvidia's NV30 chip) as \"Full Precision\", as well as FP16 as \"Partial Precision\" for vertex and pixel shader calculations performed by the graphics hardware.\n\nIn computer graphics minifloats are sometimes used to represent only integral values. If at the same time subnormal values should exist, the least subnormal number has to be 1. This statement can be used to calculate the bias value. The following example demonstrates the calculation, as well as the underlying principles.\n\nA minifloat in 1 byte (8 bit) with 1 sign bit, 4 exponent bits and 3 mantissa bits (in short, a 1.4.3.−2 minifloat) should be used to represent integral values. All IEEE 754 principles should be valid. The only free value is the exponent bias, which will come out as −2. The unknown exponent is called for the moment \"x\".\n\nNumbers in a different base are marked as ..., for example, 101 = 5. The bit patterns have spaces to visualize their parts.\n\n 0 0000 000 = 0\n\nThe mantissa is extended with \"0.\":\n\nThe mantissa is extended with \"1.\":\n\n 0 1111 000 = +infinity\n\nIf the exponent field were not treated specially, the value would be\n\n \"x\" 1111 \"yyy\" = NaN (if \"yyy\" ≠ 000)\n\nWithout the IEEE 754 special handling of the largest exponent, the greatest possible value would be\n\nIf the least subnormal value (second line above) should be 1, the value of \"x\" has to be \"x\" = 3. Therefore the bias has to be −2; that is, every stored exponent has to be decreased by −2 or has to be increased by 2, to get the numerical exponent.\n\nHowever, in practice, floats are not shown exactly. Instead, they are rounded; for example, if a float had about 3 significant digits, and the number 8192 was represented, it would be rounded to 8190 to avoid false precision.\n\nIntegral minifloats in 1 byte have a greater range of ±122 880 than two's-complement integer with a range −128 to +127. The greater range is compensated by a poor precision, because there are only 4 mantissa bits, equivalent to slightly more than one decimal place. They also have greater range than half-precision minifloats with range ±65 504, also compensated by lack of fractions and poor precision.\n\nThere are only 242 different values (if +0 and −0 are regarded as different), because 14 bit patterns represent NaN.\n\nThe values between 0 and 16 have the same bit pattern as minifloat or two's-complement integer. The first pattern with a different value is 00010001, which is 18 as a minifloat and 17 as a two's-complement integer.\n\nThis coincidence does not occur at all with negative values, because this minifloat is a signed-magnitude format.\n\nThe (vertical) real line on the right shows clearly the varying density of the floating-point values – a property which is common to any floating-point system. This varying density results in a curve similar to the exponential function.\n\nAlthough the curve may appear smooth, this is not the case. The graph actually consists of distinct points, and these points lie on line segments with discrete slopes. The value of the exponent bits determines the absolute precision of the mantissa bits, and it is this precision that determines the slope of each linear segment.\n\nThe graphic demonstrates the addition of even smaller (1.3.2.3)-minifloats with 6 bits. This floating-point system follows the rules of IEEE 754 exactly. NaN as operand produces always NaN results. Inf − Inf and (−Inf) + Inf results in NaN too (green area). Inf can be augmented and decremented by finite values without change. Sums with finite operands can give an infinite result (i.e. 14.0 + 3.0 = +Inf as a result is the cyan area, −Inf is the magenta area). The range of the finite operands is filled with the curves \"x\" + \"y\" = \"c\", where \"c\" is always one of the representable float values (blue and red for positive and negative results respectively).\n\nThe other arithmetic operations can be illustrated similarly:\n\n\n"}
{"id": "37569292", "url": "https://en.wikipedia.org/wiki?curid=37569292", "title": "Monad (linear algebra)", "text": "Monad (linear algebra)\n\nIn linear and homological algebra, a monad is a 3-term complex\n\nof objects in some abelian category whose middle term \"B\" is projective and whose first map \"A\" → \"B\" is injective and whose second map \"B\" → \"C\" is surjective. Equivalently a monad is a projective object together with a 3-step filtration (\"B\" ⊃ ker(\"B\" → \"C\") ⊃ im(\"A\" → \"B\")). In practice \"A\", \"B\", and \"C\" are often vector bundles over some space, and there are several minor extra conditions that some authors add to the definition. Monads were introduced by .\n\n\n"}
{"id": "58829263", "url": "https://en.wikipedia.org/wiki?curid=58829263", "title": "Noriko H. Arai", "text": "Noriko H. Arai\n\nNoriko H. Arai (, born 1962) is a Japanese researcher in mathematical logic and artificial intelligence, known for her work on a project to develop robots that can pass the entrance examinations for the University of Tokyo. She is a professor in the information and society research division of the National Institute of Informatics.\n\nArai was born in Tokyo. She earned a law degree from Hitotsubashi University and then, in 1985, a mathematics degree \"magna cum laude\" from the University of Illinois at Urbana–Champaign. Her doctorate is from the Tokyo Institute of Technology.\n\nShe joined the National Institute of Informatics in 2001.\n\nArai's Todai Robot Project aims to build a robot that can pass the entrance examinations for the University of Tokyo (commonly known as Todai) by 2021.\nArai became director of the project in 2011.\nAt a 2017 TED Talk, she reported that her system could achieve a score better than 80% of the applicants to the university; however, this was still not a passing score.\nArai sees the success of the project as evidence that human education should concentrate more on problem solving and creativity, and less on rote learning.\n\nArai is also the founder of Researchmap, \"the largest social network for researchers in Japan\". She was one of 15 top artificial intelligence researchers invited by French president Emmanuel Macron to join him in March 2018 for the announcement of a major new French initiative for artificial intelligence research.\n"}
{"id": "2179639", "url": "https://en.wikipedia.org/wiki?curid=2179639", "title": "Obstruction theory", "text": "Obstruction theory\n\nIn mathematics, obstruction theory is a name given to two different mathematical theories, both of which yield cohomological invariants.\n\nIn the original work of Stiefel and Whitney, characteristic classes were defined as obstructions to the existence of certain fields of linear independent vectors. Obstruction theory turns out to be an application of cohomology theory to the problem of constructing a cross-section of a bundle.\n\nThe older meaning for obstruction theory in homotopy theory relates to the procedure, inductive with respect to dimension, for extending a continuous mapping defined on a simplicial complex, or CW complex. It is traditionally called \"Eilenberg obstruction theory\", after Samuel Eilenberg. It involves cohomology groups with coefficients in homotopy groups to define obstructions to extensions. For example, with a mapping from a simplicial complex \"X\" to another, \"Y\", defined initially on the 0-skeleton of \"X\" (the vertices of \"X\"), an extension to the 1-skeleton will be possible whenever the image of the 0-skeleton will belong to the same path-connected component of \"Y\". Extending from the 1-skeleton to the 2-skeleton means defining the mapping on each solid triangle from \"X\", given the mapping already defined on its boundary edges. Likewise, then extending the mapping to the 3-skeleton involves extending the mapping to each solid 3-simplex of \"X\", given the mapping already defined on its boundary. \n\nAt some point, say extending the mapping from the (n-1)-skeleton of \"X\" to the n-skeleton of \"X\", this procedure might be impossible. In that case, one can assign to each n-simplex the homotopy class of the mapping already defined on its boundary, (at least one of which will be non-zero). These assignments define an n-cochain with coefficients in . Amazingly, this cochain turns out to be a cocyle and so defines a cohomology class in the nth cohomology group of \"X\" with coefficients in . When this cohomology class is equal to 0, it turns out that the mapping may be modified within its homotopy class on the (n-1)-skeleton of \"X\" so that the mapping may be extended to the n-skeleton of \"X\". If the class is not equal to zero, it is called the obstruction to extending the mapping over the n-skeleton, given its homotopy class on the (n-1)-skeleton.\n\nSuppose that is a simply connected simplicial complex and that is a fibration with fiber . Furthermore, assume that we have a partially defined section on the -skeleton of .\n\nFor every -simplex in , can be restricted to its boundary (which is a topological -sphere). Because send each of these back to each , we have a map from an -sphere to . Because fibrations satisfy the homotopy lifting property, and is contractible; is homotopy equivalent to . So this partially defined section assigns an element of to every -simplex. This is precisely the data of a -valued simplicial cochain of degree on , i.e. an element of . This cochain is called the obstruction cochain because it being the zero means that all of these elements of are trivial, which means that our partially defined section can be extended to the -skeleton by using the homotopy between (the partially defined section on the boundary of each ) and the constant map.\n\nThe fact that this cochain came from a partially defined section (as opposed to an arbitrary collection of maps from the all the boundaries of all the -simplices) can be used to prove that this cochain is a cocycle. If one started with a different partially defined section that agreed with the original on the -skeleton, then one can also prove that the resulting cocycle would differ from the first by a coboundary. Therefore we have a well-defined element of the cohomology group such that if a partially defined section on the -skeleton exists that agrees with the given choice on the -skeleton, then this cohomology class must be trivial.\n\nThe converse is also true if one allows such things as \"homotopy sections\", i.e. a map such that is homotopic (as opposed to equal) to the identity map on . Thus it provides a complete invariant of the existence of sections up to homotopy on the -skeleton.\n\n\nIn geometric topology, obstruction theory is concerned with when a topological manifold has a piecewise linear structure, and when a piecewise linear manifold has a differential structure.\n\nIn dimension at most 2 (Rado), and 3 (Moise), the notions of topological manifolds and piecewise linear manifolds coincide. In dimension 4 they are not the same.\n\nIn dimensions at most 6 the notions of piecewise linear manifolds and differentiable manifolds coincide.\n\nThe two basic questions of surgery theory are whether a topological space with \"n\"-dimensional Poincaré duality is homotopy equivalent to an \"n\"-dimensional manifold, and also whether a homotopy equivalence of \"n\"-dimensional manifolds is homotopic to a diffeomorphism. In both cases there are two obstructions for \"n>9\", a primary topological K-theory obstruction to the existence of a vector bundle: if this vanishes there exists a normal map, allowing the definition of the secondary surgery obstruction in algebraic L-theory to performing surgery on the normal map to obtain a homotopy equivalence.\n\n\n"}
{"id": "7378310", "url": "https://en.wikipedia.org/wiki?curid=7378310", "title": "Onset (audio)", "text": "Onset (audio)\n\nOnset refers to the beginning of a musical note or other sound. It is related to (but different from) the concept of a transient: all musical notes have an onset, but do not necessarily include an initial transient.\n\nIn phonetics the term is used differently - see syllable onset.\n\nIn signal processing, onset detection is an active research area. For example, the MIREX annual competition features an Audio Onset Detection contest.\n\nApproaches to onset detection can operate in the time domain, frequency domain, phase domain, or complex domain, and include looking for:\n\n\nSimpler techniques such as detecting increases in time-domain amplitude can typically lead to an unsatisfactorily high amount of false positives or false negatives.\n\nThe aim is often to judge onsets similarly to how a human would: so psychoacoustically-motivated strategies may be employed. Sometimes the onset detector can be restricted to a particular domain (depending on intended application), for example being targeted at detecting percussive onsets. With a narrower focus, it can be more straightforward to obtain reliable detection.\n\n\n"}
{"id": "11685115", "url": "https://en.wikipedia.org/wiki?curid=11685115", "title": "Overlap–add method", "text": "Overlap–add method\n\nIn signal processing, the overlap–add method (OA, OLA) is an efficient way to evaluate the discrete convolution of a very long signal formula_1 with a finite impulse response (FIR) filter formula_2:\n\nwhere \"h\"[\"m\"] = 0 for \"m\" outside the region [1, \"M\"].\n\nThe concept is to divide the problem into multiple convolutions of \"h\"[\"n\"] with short segments of formula_1:\n\nwhere \"L\" is an arbitrary segment length. Then:\n\nand \"y\"[\"n\"] can be written as a sum of short convolutions:\n\nwhere  formula_8  is zero outside the region [1, \"L\" + \"M\" − 1].  And for any parameter  formula_9  it is equivalent to the formula_10-point circular convolution of formula_11 with formula_12  in the region [1, \"N\"].\n\nThe advantage is that the circular convolution can be computed very efficiently as follows, according to the circular convolution theorem:\n\nwhere FFT and IFFT refer to the fast Fourier transform and inverse\nfast Fourier transform, respectively, evaluated over formula_13 discrete\npoints.\n\nFig. 1 sketches the idea of the overlap–add method. The\nsignal formula_1 is first partitioned into non-overlapping sequences,\nthen the discrete Fourier transforms of the sequences formula_15\nare evaluated by multiplying the FFT of formula_16 with the FFT of\nformula_2. After recovering of formula_15 by inverse FFT, the resulting\noutput signal is reconstructed by overlapping and adding the formula_15\nas shown in the figure. The overlap arises from the fact that a linear\nconvolution is always longer than the original sequences. In the early days of development of the fast Fourier transform, formula_20 was often chosen to be a power of 2 for efficiency, but further development has revealed efficient transforms for larger prime factorizations of L, reducing computational sensitivity to this parameter. A pseudocode of the algorithm is the\nfollowing:\n\nThis algorithm is based on the linearity of the DFT.\n\nWhen sequence \"x\"[\"n\"] is periodic, and \"N\" is the period, then \"y\"[\"n\"] is also periodic, with the same period.  To compute one period of y[n], Algorithm 1 can first be used to convolve \"h\"[\"n\"] with just one period of \"x\"[\"n\"].  In the region \"M\" ≤ \"n\" ≤ \"N\",  the resultant \"y\"[\"n\"] sequence is correct.  And if the next \"M\" − 1 values are added to the first \"M\" − 1 values, then the region 1 ≤ \"n\" ≤ \"N\" will represent the desired convolution. The modified pseudocode is:\n\nThe cost of the convolution can be associated to the number of complex\nmultiplications involved in the operation. The major computational\neffort is due to the FFT operation, which for a radix-2 algorithm\napplied to a signal of length formula_13 roughly calls for formula_22\ncomplex multiplications. It turns out that the number of complex multiplications\nof the overlap-add method are:\n\nformula_24 accounts for the FFT+filter multiplication+IFFT operation.\n\nThe additional cost of the formula_25 sections involved in the circular\nversion of the overlap–add method is usually very small and can be\nneglected for the sake of simplicity. The best value of formula_13\ncan be found by numerical search of the minimum of formula_27\nby spanning the integer formula_28 in the range formula_29.\nBeing formula_13 a power of two, the FFTs of the overlap–add method\nare computed efficiently. Once evaluated the value of formula_13 it\nturns out that the optimal partitioning of formula_1 has formula_33.\nFor comparison, the cost of the standard circular convolution of formula_1\nand formula_2 is:\n\nHence the cost of the overlap–add method scales almost as formula_37\nwhile the cost of the standard circular convolution method is almost\nformula_38. However such functions accounts\nonly for the cost of the complex multiplications, regardless of the\nother operations involved in the algorithm. A direct measure of the\ncomputational time required by the algorithms is of much interest.\nFig. 2 shows the ratio of the measured time to evaluate\na standard circular convolution using   with\nthe time elapsed by the same convolution using the overlap–add method\nin the form of Alg 2, vs. the sequence and the filter length. Both algorithms have been implemented under Matlab. The\nbold line represent the boundary of the region where the overlap–add\nmethod is faster (ratio>1) than the standard circular convolution.\nNote that the overlap–add method in the tested cases can be three\ntimes faster than the standard method.\n\n\n"}
{"id": "42973489", "url": "https://en.wikipedia.org/wiki?curid=42973489", "title": "Path ordering (term rewriting)", "text": "Path ordering (term rewriting)\n\nIn theoretical computer science, in particular in term rewriting, a path ordering is a well-founded strict total order (>) on the set of all terms such that \nwhere (>) is a user-given total precedence order on the set of all function symbols.\n\nIntuitively, a term \"f\"(...) is bigger than any term \"g\"(...) built from terms \"s\" smaller than \"f\"(...) using a\nlower-precedence root symbol \"g\".\nIn particular, by structural induction, a term \"f\"(...) is bigger than any term containing only symbols smaller than \"f\".\n\nA path ordering is often used as reduction ordering in term rewriting, in particular in the Knuth–Bendix completion algorithm.\nAs an example, a term rewriting system for \"multiplying out\" mathematical expressions could contain a rule \"x\"*(\"y\"+\"z\") → (\"x\"*\"y\") + (\"x\"*\"z\"). In order to prove termination, a reduction ordering (>) must be found with respect to which the term \"x\"*(\"y\"+\"z\") is greater than the term (\"x\"*\"y\")+(\"x\"*\"z\"). This is not trivial, since the former term contains both fewer function symbols and fewer variables than the latter. However, setting the precedence (*) > (+), a path ordering can be used, since both \"x\"*(\"y\"+\"z\") > \"x\"*\"y\" and \"x\"*(\"y\"+\"z\") > \"x\"*\"z\" is easy to achieve.\n\nGiven two terms \"s\" and \"t\", with a root symbol \"f\" and \"g\", respectively, to decide their relation their root symbols are compared first.\nThe latter variations include:\nDershowitz, Okada (1988) list more variants, and relate them to Ackermann's system of ordinals.\n\nThe multiset path ordering (>) can be defined as follows:\n\nwhere \n\nMore generally, an order functional is a function \"O\" mapping an ordering to another one, and satisfying the following properties:\n\nThe multiset extension, mapping (>) above to (») above is one example of an order functional: (»)=\"O\"(>).\nAnother order functional is the lexicographic extension, leading to the lexicographic path ordering.\n"}
{"id": "457210", "url": "https://en.wikipedia.org/wiki?curid=457210", "title": "Pure mathematics", "text": "Pure mathematics\n\nBroadly speaking, pure mathematics is mathematics that studies entirely abstract concepts. This has been a recognizable category of mathematical activity from the 19th century onwards, at variance with the trend towards meeting the needs of navigation, astronomy, physics, economics, engineering, and so on.\n\nAnother view is that pure mathematics is \"not necessarily\" applied mathematics: it is possible to study abstract entities with respect to their intrinsic nature and not be concerned with how they manifest in the real world. Even though the pure and applied viewpoints are distinct philosophical positions, in practice there is much overlap in the activity of pure and applied mathematicians.\n\nTo develop accurate models for describing the real world, many applied mathematicians draw on tools and techniques that are often considered to be \"pure\" mathematics. On the other hand, many pure mathematicians draw on natural and social phenomena as inspiration for their abstract research.\n\nAncient Greek mathematicians were among the earliest to make a distinction between pure and applied mathematics. Plato helped to create the gap between \"arithmetic\", now called number theory, and \"logistic\", now called arithmetic. Plato regarded logistic (arithmetic) as appropriate for businessmen and men of war who \"must learn the art of numbers or [they] will not know how to array [their] troops\" and arithmetic (number theory) as appropriate for philosophers \"because [they have] to arise out of the sea of change and lay hold of true being.\" Euclid of Alexandria, when asked by one of his students of what use was the study of geometry, asked his slave to give the student threepence, \"since he must make gain of what he learns.\" The Greek mathematician Apollonius of Perga was asked about the usefulness of some of his theorems in Book IV of \"Conics\" to which he proudly asserted,\nThey are worthy of acceptance for the sake of the demonstrations themselves, in the same way as we accept many other things in mathematics for this and for no other reason.\nAnd since many of his results were not applicable to the science or engineering of his day, Apollonius further argued in the preface of the fifth book of \"Conics\" that the subject is one of those that \"...seem worthy of study for their own sake.\"\n\nThe term itself is enshrined in the full title of the Sadleirian Chair, founded (as a professorship) in the mid-nineteenth century. The idea of a separate discipline of \"pure\" mathematics may have emerged at that time. The generation of Gauss made no sweeping distinction of the kind, between \"pure\" and \"applied\". In the following years, specialisation and professionalisation (particularly in the Weierstrass approach to mathematical analysis) started to make a rift more apparent.\n\nAt the start of the twentieth century mathematicians took up the axiomatic method, strongly influenced by David Hilbert's example. The logical formulation of pure mathematics suggested by Bertrand Russell in terms of a quantifier structure of propositions seemed more and more plausible, as large parts of mathematics became axiomatised and thus subject to the simple criteria of \"rigorous proof\".\n\nIn fact in an axiomatic setting \"rigorous\" adds nothing to the idea of \"proof\". Pure mathematics, according to a view that can be ascribed to the Bourbaki group, is what is proved. Pure mathematician became a recognized vocation, achievable through training.\n\nThe case was made that pure mathematics is useful in engineering education:\n\nOne central concept in pure mathematics is the idea of generality; pure mathematics often exhibits a trend towards increased generality. Uses and advantages of generality include the following:\n\n\nGenerality's impact on intuition is both dependent on the subject and a matter of personal preference or learning style. Often generality is seen as a hindrance to intuition, although it can certainly function as an aid to it, especially when it provides analogies to material for which one already has good intuition.\n\nAs a prime example of generality, the Erlangen program involved an expansion of geometry to accommodate non-Euclidean geometries as well as the field of topology, and other forms of geometry, by viewing geometry as the study of a space together with a group of transformations. The study of numbers, called algebra at the beginning undergraduate level, extends to abstract algebra at a more advanced level; and the study of functions, called calculus at the college freshman level becomes mathematical analysis and functional analysis at a more advanced level. Each of these branches of more \"abstract\" mathematics have many sub-specialties, and there are in fact many connections between pure mathematics and applied mathematics disciplines. A steep rise in abstraction was seen mid 20th century.\n\nIn practice, however, these developments led to a sharp divergence from physics, particularly from 1950 to 1983. Later this was criticised, for example by Vladimir Arnold, as too much Hilbert, not enough Poincaré. The point does not yet seem to be settled, in that string theory pulls one way, while discrete mathematics pulls back towards proof as central.\n\nMathematicians have always had differing opinions regarding the distinction between pure and applied mathematics. \nOne of the most famous (but perhaps misunderstood) modern examples of this debate can be found in G.H. Hardy's \"A Mathematician's Apology\".\n\nIt is widely believed that Hardy considered applied mathematics to be ugly and dull. Although it is true that Hardy preferred pure mathematics, which he often compared to painting and poetry, Hardy saw the distinction between pure and applied mathematics to be simply that applied mathematics sought to express \"physical\" truth in a mathematical framework, whereas pure mathematics expressed truths that were independent of the physical world. Hardy made a separate distinction in mathematics between what he called \"real\" mathematics, \"which has permanent aesthetic value\", and \"the dull and elementary parts of mathematics\" that have practical use.\n\nHardy considered some physicists, such as Einstein, and Dirac, to be among the \"real\" mathematicians, but at the time that he was writing the \"Apology\" he also considered general relativity and quantum mechanics to be \"useless\", which allowed him to hold the opinion that only \"dull\" mathematics was useful. Moreover, Hardy briefly admitted that—just as the application of matrix theory and group theory to physics had come unexpectedly—the time may come where some kinds of beautiful, \"real\" mathematics may be useful as well.\n\nAnother insightful view is offered by Magid:\nAnalysis is concerned with the properties of functions. It deals with concepts such as continuity, limits, differentiation and integration, thus providing a rigorous foundation for the calculus of infinitesimals introduced by Newton and Leibniz in the 17th century. Real analysis studies functions of real numbers, while complex analysis extends the aforementioned concepts to functions of complex numbers. Functional analysis is a branch of analysis that studies infinite-dimensional vector spaces and views functions as points in these spaces.\n\nAbstract algebra is not to be confused with the manipulation of formulae that is covered in secondary education. It studies sets together with binary operations defined on them. Sets and their binary operations may be classified according to their properties: for instance, if an operation is associative on a set that contains an identity element and inverses for each member of the set, the set and operation is considered to be a group. Other structures include rings, fields, vector spaces and lattices.\n\nGeometry is the study of shapes and space, in particular, groups of transformations that act on spaces. For example, projective geometry is about the group of projective transformations that act on the real projective plane, whereas inversive geometry is concerned with the group of inversive transformations acting on the extended complex plane.\n\nNumber theory is the theory of the positive integers. It is based on ideas such as divisibility and congruence. Its fundamental theorem states that each positive integer has a unique prime factorization. In some ways it is the most accessible discipline in pure mathematics for the general public: for instance the Goldbach conjecture is easily stated (but is yet to be proved or disproved). In other ways it is the least accessible discipline; for example, Wiles' proof that Fermat's equation has no nontrivial solutions requires understanding automorphic forms, which though intrinsic to nature have not found a place in physics or the general public discourse.\n\nTopology is a modern extension of Geometry. Rather than focusing on the sizes of objects and their precise measurement, topology involves the properties of spaces or objects that are preserved under smooth operations such as bending or twisting (but not, for example, tearing or shearing). Topology's subfields interact with other branches of pure math: traditional topology uses ideas from analysis, such as metric spaces, and algebraic topology relies on ideas from combinatorics in addition to those of analysis.\n\nCombinatorics is a branch of mathematics concerning the study of finite or countable discrete structures. It also interacts with many other areas of mathematics. For example, additive combinatorics is related to number theory and analysis, and graph theory comes from topology.\n\n\n"}
{"id": "305290", "url": "https://en.wikipedia.org/wiki?curid=305290", "title": "Radix point", "text": "Radix point\n\nIn mathematics and computing, a radix point (or radix character) is the symbol used in numerical representations to separate the integer part of a number (to the left of the radix point) from its fractional part (to the right of the radix point). \"Radix point\" applies to all number bases. In base 10 notation, the radix point is more commonly called the decimal point, where the prefix deci- implies base 10. Similarly, the term \"binary point\" is used for base 2.\n\nIn English-speaking countries, the radix point is usually a small dot (.) placed either on the baseline or halfway between the baseline and the top of the numerals. In many other countries, the radix point is a comma (,) placed on the baseline. It is important to know which notation is being used when working in different software programs. The respective ISO standard defines both the comma and the small dot as decimal markers, but does not explicitly define universal radix marks for bases other than 10.\n\n\nHence, its decimal value can be calculated as:\n\nIt is now seen that 1101, which is to the left of the radix point, is the binary representation of the decimal number 13. To the right of the radix point is 101, which is the binary representation of the decimal fraction 625/1000 (or 5/8).\n\n"}
{"id": "26426", "url": "https://en.wikipedia.org/wiki?curid=26426", "title": "Rational root theorem", "text": "Rational root theorem\n\nIn algebra, the rational root theorem (or rational root test, rational zero theorem, rational zero test or \"p\"/\"q\" theorem) states a constraint on rational solutions of a polynomial equation\nwith integer coefficients. Solutions of the equation are roots (equivalently, zeroes) of the polynomial on the left side of the equation.\n\nIf \"a\" and \"a\" are nonzero,\nthen each rational solution \"x\",\nwhen written as a fraction \"x\" = \"p\"/\"q\" in lowest terms (i.e., the greatest common divisor of \"p\" and \"q\" is 1), satisfies\n\nThe rational root theorem is a special case (for a single linear factor) of Gauss's lemma on the factorization of polynomials. The integral root theorem is a special case of the rational root theorem if the leading coefficient \"a\" = 1.\n\nThe theorem is used in order to determine whether a polynomial has any rational roots, and if so to find them. Since the theorem gives constraints on the numerator and denominator of the fully reduced rational roots as being divisors of certain numbers, all possible combinations of divisors can be checked and either the rational roots will be found or it will be determined that there are none. If one or more are found, they can be factored out of the polynomial, resulting in a polynomial of lower degree whose roots are also roots of the original polynomial.\n\nThe general cubic equation\n\nwith integer coefficients has three solutions in the complex plane. If it is found by the rational root test that there are no rational solutions, then the only way to express the solutions algebraically is to use cube roots. But if the test finds three rational solutions, then the cube roots are avoided. And if precisely one rational solution \"r\" is found to exist, then (\"x\" – \"r\") can be factored out of the cubic polynomial using polynomial long division, leaving a quadratic polynomial whose two roots are the remaining two roots of the cubic; and these can be found using the quadratic formula, again avoiding the use of cube roots.\n\nLet \"P\"(\"x\") = \"a\"\"x\" + \"a\"\"x\" + ... + \"a\"\"x\" + \"a\" for some \"a\", ..., \"a\" ∈ Z, and suppose \"P\"(\"p\"/\"q\") = 0 for some coprime \"p\", \"q\" ∈ Z:\n\nIf we multiply both sides by \"q\", shift the constant term to the right hand side, and factor out \"p\" on the left hand side, we get\n\nWe see that \"p\" times the integer quantity in parentheses equals −\"a\"\"q\", so \"p\" divides \"a\"\"q\". But \"p\" is coprime to \"q\" and therefore to \"q\", so by (the generalized form of) Euclid's lemma it must divide the remaining factor \"a\" of the product.\n\nIf we instead shift the leading term to the right hand side and factor out \"q\" on the left hand side, we get\n\nAnd for similar reasons, we can conclude that \"q\" divides \"a\".\n\nShould there be a nontrivial factor dividing all the coefficients of the polynomial, then one can divide by the greatest common divisor of the coefficients so as to obtain a primitive polynomial in the sense of Gauss's lemma; this does not alter the set of rational roots and only strengthens the divisibility conditions. That lemma says that if the polynomial factors in , then it also factors in as a product of primitive polynomials. Now any rational root corresponds to a factor of degree 1 in of the polynomial, and its primitive representative is then , assuming that \"p\" and \"q\" are coprime. But any multiple in of has leading term divisible by \"q\" and constant term divisible by \"p\", which proves the statement. This argument shows that more generally, any irreducible factor of \"P\" can be supposed to have integer coefficients, and leading and constant coefficients dividing the corresponding coefficients of \"P\".\n\nIn the polynomial\n\nany rational root fully reduced would have to have a numerator that divides evenly into 1 and a denominator that divides evenly into 2. Hence the only possible rational roots are ±1/2 and ±1; since neither of these equates the polynomial to zero, it has no rational roots.\n\nIn the polynomial\n\nthe only possible rational roots would have a numerator that divides 6 and a denominator that divides 1, limiting the possibilities to ±1, ±2, ±3, and ±6. Of these, 1, 2, and –3 equate the polynomial to zero, and hence are its rational roots. (In fact these are its only roots since a cubic has only three roots; in general, a polynomial could have some rational and some irrational roots.)\n\nEvery rational root of the polynomial\nmust be among the numbers symbolically indicated by\nwhich gives the list of 8 possible answers:\n\nThese root candidates can be tested using Horner's method (for instance). In this particular case there is exactly one rational root. If a root candidate does not cause the polynomial to equal zero, it can be used to shorten the list of remaining candidates. For example, \"x\" = 1 does not work, as the polynomial then equals 1. This means that substituting \"x\" = 1 + \"t\" yields a polynomial in \"t\" with constant term 1, while the coefficient of \"t\" remains the same as the coefficient of \"x\". Applying the rational root theorem thus yields the following possible roots for \"t\":\n\nTherefore,\n\nRoot candidates that do not occur on both lists are ruled out. The list of rational root candidates has thus shrunk to just \"x\" = 2 and \"x\" = 2/3.\n\nIf \"k\" rational roots are found (\"k\" ≥ 1), Horner's method will also yield a polynomial of degree \"n\" − \"k\" whose roots, together with the rational roots, are exactly the roots of the original polynomial. It may also be the case that none of the candidates is a solution; in this case the equation setting the polynomial equal to 0 has no rational solution. If the equation lacks a constant term \"a\", then 0 is one of the rational solutions of the equation.\n\n\n\n"}
{"id": "295829", "url": "https://en.wikipedia.org/wiki?curid=295829", "title": "Reflection (mathematics)", "text": "Reflection (mathematics)\n\nIn mathematics, a reflection (also spelled reflexion) is a mapping from a Euclidean space to itself that is an isometry with a hyperplane as a set of fixed points; this set is called the axis (in dimension 2) or plane (in dimension 3) of reflection. The image of a figure by a reflection is its mirror image in the axis or plane of reflection. For example the mirror image of the small Latin letter p for a reflection with respect to a vertical axis would look like q. Its image by reflection in a horizontal axis would look like b. A reflection is an involution: when applied twice in succession, every point returns to its original location, and every geometrical object is restored to its original state.\n\nThe term \"reflection\" is sometimes used for a larger class of mappings from a Euclidean space to itself, namely the non-identity isometries that are involutions. Such isometries have a set of fixed points (the \"mirror\") that is an affine subspace, but is possibly smaller than a hyperplane. For instance a reflection through a point is an involutive isometry with just one fixed point; the image of the letter p under it\nwould look like a d. This operation is also known as a central inversion , and exhibits Euclidean space as a symmetric space. In a Euclidean vector space, the reflection in the point situated at the origin is the same as vector negation. Other examples include reflections in a line in three-dimensional space. Typically, however, unqualified use of the term \"reflection\" means reflection in a hyperplane. \n\nA figure that does not change upon undergoing a reflection is said to have reflectional symmetry.\n\nSome mathematicians use \"flip\" as a synonym for \"reflection\".\n\nIn a plane (or, respectively, 3-dimensional) geometry, to find the reflection of a point drop a perpendicular from the point to the line (plane) used for reflection, and extend it the same distance on the other side. To find the reflection of a figure, reflect each point in the figure.\n\nTo reflect point through the line using compass and straightedge, proceed as follows (see figure):\n\n\nPoint is then the reflection of point through line .\n\nThe matrix for a reflection is orthogonal with determinant −1 and eigenvalues −1, 1, 1, ..., 1. The product of two such matrices is a special orthogonal matrix that represents a rotation. Every rotation is the result of reflecting in an even number of reflections in hyperplanes through the origin, and every improper rotation is the result of reflecting in an odd number. Thus reflections generate the orthogonal group, and this result is known as the Cartan–Dieudonné theorem.\n\nSimilarly the Euclidean group, which consists of all isometries of Euclidean space, is generated by reflections in affine hyperplanes. In general, a group generated by reflections in affine hyperplanes is known as a reflection group. The finite groups generated in this way are examples of Coxeter groups.\n\nReflection across a line through the origin in two dimensions can be described by the following formula\n\nwhere denotes the vector being reflected, denotes any vector in the line being reflected in, and denotes the dot product of with . Note the formula above can also be described as\n\nwhere the reflection of line on is equal to 2 times the projection of on line minus . Reflections in a line have the eigenvalues of 1, and −1.\n\nGiven a vector in Euclidean space , the formula for the reflection in the hyperplane through the origin, orthogonal to , is given by\n\nwhere denotes the dot product of with . Note that the second term in the above equation is just twice the vector projection of onto . One can easily check that\n\nUsing the geometric product, the formula is\n\nSince these reflections are isometries of Euclidean space fixing the origin they may be represented by orthogonal matrices. The orthogonal matrix corresponding to the above reflection is the matrix whose entries are\n\nwhere is the Kronecker delta.\n\nThe formula for the reflection in the affine hyperplane not through the origin is\n\n\n\n"}
{"id": "3721845", "url": "https://en.wikipedia.org/wiki?curid=3721845", "title": "Rotational–vibrational coupling", "text": "Rotational–vibrational coupling\n\nRotational–vibrational coupling occurs when the rotation frequency of an object is close to or identical to a natural internal vibration frequency. The animation on the right shows a simple example. The motion depicted in the animation is for the idealized situation that the force exerted by the spring increases linearly with the distance to the center of rotation. Also, the animation depicts what would occur if there would not be any friction.\n\nIn rotational-vibrational coupling there is an oscillation of the angular velocity. In pulling the circling masses closer to the center of rotation, the force exerted by the spring (a centripetal force) is doing work, converting stored strain energy in the spring into kinetic energy of the masses. As a consequence of that, the angular velocity increases. The force of the spring cannot pull the circling masses all the way to the center, for as the circling masses are moving closer to the center of rotation the force exerted by the spring gets weaker, and the velocity is steadily increasing. At some point the velocity has increased so much that the object starts to swing wide again, re-entering a phase of building up strain energy.\n\nIn helicopter design damping devices must be incorporated, because at specific angular velocities vibrations of the rotorblades can be reinforced by rotational-vibrational coupling, and build up catastrophically. Without the damping the vibrations will cause the rotorblades to break loose.\n\nThe animation on the right provides a clearer view on the oscillation of the angular velocity. There is a close analogy with harmonic oscillation. \n\nWhen a harmonic oscillation is at its midpoint then all the energy of the system is kinetic energy. When the harmonic oscillation is at the points furthest away from the midpoint all the energy of the system is potential energy. The energy of the system is oscillating back and forth between kinetic energy and potential energy. \n\nIn the animation with the two circling masses there is a back and forth oscillation of kinetic energy and potential energy. When the spring is at its maximal extension then the potential energy is largest, when the angular velocity is at its maximum the kinetic energy is at largest. \n\nWith a real spring there is friction involved. With a real spring the vibration will be damped and the final situation will be that the masses circle each other at a constant distance, with a constant tension of the spring.\n\nThis discussion applies the following simplifications: the spring itself is taken as being weightless, and the spring is taken as being a perfect spring; the restoring force increases in a linear way as the spring is stretched out. That is, the restoring force is exactly proportional to the distance to the center of rotation. A restoring force with this characteristic is called a harmonic force.\n\nThe following parametric equation of the position as a function of time describes the motion of the circling masses:\n\nThe motion as a function of time can be also be seen as a vector combination of two uniform circular motions. The parametric equations (1) and (2) can be rewritten as:\n\nA transformation to a coordinate system that subtracts the overall circular motion leaves the \"eccentricity\" of the ellipse-shaped trajectory. the center of the eccentricity is located at a distance of formula_8 from the main center:\n\nThat is in fact what is seen in the second animation, in which the motion is mapped to a coordinate system that is rotating at a constant angular velocity. The angular velocity of the motion with respect to the rotating coordinate system is 2ω, twice the angular velocity of the overall motion. \nThe spring is continuously doing work. More precisely, the spring is oscillating between doing positive work (increasing the weight's kinetic energy) and doing negative work (decreasing the weight's kinetic energy)\n\nThe centripetal force is a harmonic force. \n\nThe set of all solutions to the above equation of motion consists of both circular trajectories and ellipse-shaped trajectories. All the solutions have the same period of revolution. This is a distinctive feature of motion under the influence of a harmonic force; all trajectories take the same amount of time to complete a revolution.\n\nWhen a rotating coordinate system is used the centrifugal term and the coriolis term are added to the equation of motion. The following equation gives the acceleration with respect to a rotating system of an object in inertial motion. \n\nHere, Ω is the angular velocity of the rotating coordinate system with respect to the inertial coordinate system. \"v\" is velocity of the moving object with respect to the rotating coordinate system. It is important to note that the centrifugal term is determined by the angular velocity of the rotating coordinate system; the centrifugal term does not relate to the motion of the object. \n\nIn all, this gives the following three terms in the equation of motion for motion with respect to a coordinate system rotating with angular velocity Ω.\n\nBoth the centripetal force and the centrifugal term in the equation of motion are proportional to \"r\". The angular velocity of the rotating coordinate system is adjusted to have the same period of revolution as the object following an ellipse-shaped trajectory. Hence the vector of the centripetal force and the vector of the centrifugal term are at every distance to the center equal to each other in magnitude and opposite in direction, so those two terms drop away against each other. \nIt is only in very special circumstances that the vector of the centripetal force and the centrifugal term drop away against each other at every distance to the center of rotation. This is the case if and only if the centripetal force is a harmonic force. \nIn this case, only the coriolis term remains in the equation of motion.\n\nSince the vector of the coriolis term always points perpendicular to the velocity with respect to the rotating coordinate system, it follows that in the case of a restoring force that is a harmonic force, the eccentricity in the trajectory will show up as a small circular motion with respect to the rotating coordinate system. The factor 2 of the coriolis term corresponds to a period of revolution that is half the period of the overall motion.\n\nAs expected, the analysis using vector notation results in a straight confirmation of the previous analysis: \nThe spring is continuously doing work. More precisely, the spring is oscillating between doing positive work (increasing the weight's kinetic energy) and doing negative work (decreasing the weight's kinetic energy).\n\nIn the section 'Energy conversions in rotational-vibrational coupling' the dynamics is followed by keeping track of the energy conversions. It is often pointed out in textbooks that the increase of angular velocity on contraction is in accordance with the principle of conservation of angular momentum. Since there is no torque acting on the circling weights, angular momentum is conserved. However, this disregards the causal mechanism, which is the force of the extended spring, and the work done during its contraction and extension.\nSimilarly, when a cannon is fired, the projectile will shoot out of the barrel towards the target, and the barrel will recoil, in accordance with the principle of conservation of momentum. This does not mean that the projectile leaves the barrel at high velocity \"because\" the barrel recoils. While recoil of the barrel must occur, as described by Newton's third law, it is not a causal agent. \n\nThe causal mechanism is in the energy conversions: the explosion of the gunpowder converts potential chemical energy to the potential energy of a highly compressed gas. As the gas expands, its high pressure exerts a force on both the projectile and the interior of the barrel. It is through the action of that force that potential energy is converted to kinetic energy of both projectile and barrel.\n\nIn the case of rotational-vibrational coupling, the causal agent is the force exerted by the spring. The spring is oscillating between doing work and doing negative work. (The work is taken to be negative when the direction of the force is opposite to the direction of the motion.)\n\nRotational-vibrational spectroscopy\n"}
{"id": "4277567", "url": "https://en.wikipedia.org/wiki?curid=4277567", "title": "Schwartz space", "text": "Schwartz space\n\nIn mathematics, Schwartz space is the function space of all functions whose derivatives are rapidly decreasing (defined rigorously below). This space has the important property that the Fourier transform is an automorphism on this space. This property enables one, by duality, to define the Fourier transform for elements in the dual space of \"S\", that is, for tempered distributions. The Schwartz space was named in honour of Laurent Schwartz by Alexander Grothendieck. A function in the Schwartz space is sometimes called a Schwartz function.\nThe Schwartz space or space of rapidly decreasing functions on R is the function space\n\nwhere α, β are multi-indices, C(R) is the set of smooth functions from R to C, and\n\nHere, sup denotes the supremum, and we again use multi-index notation.\n\nTo put common language to this definition, one could consider a rapidly decreasing function as essentially a function \"f\"(\"x\") such that \"f\"(\"x\"), \"f\"′(\"x\"), \"f\"′′(\"x\"), ... all exist everywhere on R and go to zero as \"x\" → ±∞ faster than any inverse power of \"x\". In particular, \"S\"(R) is a subspace of the function space \"C\"(R) of infinitely differentiable functions.\n\n\n\n\n\n"}
{"id": "34273837", "url": "https://en.wikipedia.org/wiki?curid=34273837", "title": "Singularity spectrum", "text": "Singularity spectrum\n\nThe singularity spectrum is a function used in Multifractal analysis to describe the fractal dimension of a subset of points of a function belonging to a group of points that have the same Hölder exponent. Intuitively, the singularity spectrum gives a value for how \"fractal\" a set of points are in a function.\n\nMore formally, the singularity spectrum formula_1 of a function, formula_2, is defined as:\n\nWhere formula_4 is the function describing the Holder exponent, formula_4 of formula_2 at the point formula_7. formula_8 is the Hausdorff dimension of a point set.\n\n"}
{"id": "19242840", "url": "https://en.wikipedia.org/wiki?curid=19242840", "title": "Sun's curious identity", "text": "Sun's curious identity\n\nIn combinatorics, Sun's curious identity is the following identity involving binomial coefficients, first established by Zhi-Wei Sun in 2002:\n\nAfter Sun's publication of this identity, five other proofs were obtained by various mathematicians: \n\n"}
{"id": "50448624", "url": "https://en.wikipedia.org/wiki?curid=50448624", "title": "Supersymmetric WKB approximation", "text": "Supersymmetric WKB approximation\n\nIn physics, the supersymmetric WKB (SWKB) approximation is an extension of the WKB approximation that uses principles from supersymmetric quantum mechanics to provide estimations on energy eigenvalues in quantum-mechanical systems. Using the supersymmetric method, there are potentials formula_1 that can be expressed in terms of a superpotential, formula_2, such that\n\nThe SWKB approximation then writes the Born–Sommerfeld quantization condition from the WKB approximation in terms of formula_2.\n\nThe SWKB approximation for unbroken supersymmetry, to first order in formula_5 is given by\n\nwhere formula_7 is the estimate of the energy of the formula_8-th excited state, and formula_9 and formula_10 are the classical turning points, given by\n\nThe addition of the supersymmetric method provides several appealing qualities to this method. First, it is known that, by construction, the ground state energy will be exactly estimated. This is an improvement over the standard WKB approximation, which often has weaknesses at lower energies. Another property is that a class of potentials known as shape invariant potentials have their energy spectra estimated exactly by this first-order condition.\n\n"}
{"id": "394099", "url": "https://en.wikipedia.org/wiki?curid=394099", "title": "Test-and-set", "text": "Test-and-set\n\nIn computer science, the test-and-set instruction is an instruction used to write 1 (set) to a memory location and return its old value as a single atomic (i.e., non-interruptible) operation. If multiple processes may access the same memory location, and if a process is currently performing a test-and-set, no other process may begin another test-and-set until the first process's test-and-set is finished. A CPU may use a test-and-set instruction offered by another electronic component, such as dual-port RAM; a CPU itself may also offer a test-and-set instruction.\n\nA lock can be built using an atomic test-and-set instruction as follows:The calling process obtains the lock if the old value was 0 otherwise while-loop spins waiting to acquire the lock. This is called a spinlock. \"Test and Test-and-set\" is another example.\n\nMaurice Herlihy (1991) proved that test-and-set has a finite consensus number and can solve the wait-free consensus problem for at-most two concurrent processes. In contrast, compare-and-swap offers a more general solution to this problem.\n\nDPRAM test-and-set instructions can work in many ways. Here are two variations, both of which describe a DPRAM which provides exactly 2 ports, allowing 2 separate electronic components (such as 2 CPUs) access to every memory location on the DPRAM.\n\nWhen CPU 1 issues a test-and-set instruction, the DPRAM first makes an \"internal note\" of this by storing the address of the memory location in a special place. If at this point, CPU 2 happens to issue a test-and-set instruction for the same memory location, the DPRAM first checks its \"internal note\", recognizes the situation, and issues a BUSY interrupt, which tells CPU 2 that it must wait and retry. This is an implementation of a busy waiting or spinlock using the interrupt mechanism. Since all this happens at hardware speeds, CPU 2's wait to get out of the spin-lock is very short.\n\nWhether or not CPU 2 was trying to access the memory location, the DPRAM performs the test given by CPU 1. If the test succeeds, the DPRAM sets the memory location to the value given by CPU 1. Then the DPRAM wipes out its \"internal note\" that CPU 1 was writing there. At this point, CPU 2 could issue a test-and-set, which would succeed.\n\nCPU 1 issues a test-and-set instruction to write to \"memory location A\". The DPRAM does not immediately store the value in memory location A, but instead simultaneously moves the current value to a special register, while setting the contents of memory location A to a special \"flag value\". If at this point, CPU 2 issues a test-and-set to memory location A, the DPRAM detects the special flag value, and as in Variation 1, issues a BUSY interrupt.\n\nWhether or not CPU 2 was trying to access the memory location, the DPRAM now performs CPU 1's test. If the test succeeds, the DPRAM sets memory location A to the value specified by CPU 1. If the test fails, the DPRAM copies the value back from the special register to memory location A. Either operation wipes out the special flag value. If CPU 2 now issues a test-and-set, it will succeed.\n\nSome instruction sets have an atomic test-and-set machine language instruction. Examples include x86 and IBM System/360 and its successors (including z/Architecture).\nThose that do not can still implement an atomic test-and-set using a read-modify-write or compare-and-swap instruction.\n\nThe test and set instruction, when used with boolean values, uses logic like that shown in the following function, except that the function must execute atomically. That is, no other process must be able to interrupt the function mid-execution, thereby seeing a state that only exists while the function executes. That requires hardware support; it cannot be implemented as shown. Nevertheless, the code shown helps to explain the behaviour of test-and-set. NOTE: In this example, 'lock' is assumed to be passed by reference (or by name) but the assignment to 'initial' creates a new value (not just copying a reference).\n\nNot only is the code shown not atomic, in the sense of the test-and-set instruction, it also differs from the descriptions of DPRAM hardware test-and-set above. Here, the value being set and the test are fixed and invariant, and the value is updated regardless of the outcome of the test, whereas for the DPRAM test-and-set, the memory is set only when the test succeeds, and the value to set and the test condition are specified by the CPU. Here, the value to set can only be 1, but if 0 and 1 are considered the only valid values for the memory location, and \"value is nonzero\" is the only allowed test, then this equates to the case described for DPRAM hardware (or, more specifically, the DPRAM case reduces to this under these constraints). From that viewpoint, this can, correctly, be called \"test-and-set\" in the full, conventional sense of that term. The essential point to note is the general intent and principle of test-and-set: a value is both tested and set in one atomic operation such that no other program thread or process can change the target memory location after it is tested but before it is set. (This is because the location must only be set if it currently has a certain value, not if it had that value sometime earlier.)\n\nIn the C programming language, the implementation would be like:\n\nint TestAndSet(int* lockPtr) {\n\nThe code also shows that there are really two operations: an atomic read-modify-write and a test. Only the read-modify-write needs to be atomic. (This is true because delaying the value comparison by any amount of time will not change the result of the test once the value to test has been obtained. Once the code writes the initial value, the result of the test has been established, even if it has not been computed yet — e.g., by the == operator.)\n\nOne way to implement mutual exclusion is by using a test-and-set based lock as follows:\n\nThe lock variable is a shared variable i.e. it can be accessed by all processors/threads. Note the \"volatile\" keyword. In absence of volatile, the compiler and/or the CPU(s) may optimize access to lock and/or use cached values, thus rendering the above code erroneous. Conversely, and unfortunately, the presence of \"volatile\" does \"not\" guarantee that reads and writes are committed to memory. Some compilers issue memory barriers to ensure that operations are committed to memory, but since the semantics of \"volatile\" in C/C++ is quite vague, not all compilers will do that. Consult your compiler's documentation to determine if it does.\n\nThis function can be called by multiple processes, but it is guaranteed that only one process will be in the critical section at a time. The rest of the processes will keep spinning until they get the lock. It is possible that a process is never granted the lock. In such a case it will loop endlessly. This is a drawback of this implementation as it doesn't ensure fairness. These issues are further elaborated in the performance section.\n\nenter_region: ; A \"jump to\" tag; function entry point.\n\nleave_region:\nHere codice_1 is an atomic instruction and codice_2 is the lock variable. The process doesn't return unless it acquires the lock.\n\nThe four major evaluation metrics for locks in general are uncontended lock-acquisition latency, bus traffic, fairness, and storage.\n\nTest-and-set scores low on two of them, namely, high bus traffic and unfairness.\n\nWhen processor P1 has obtained a lock and processor P2 is also waiting for the lock, P2 will keep incurring bus transactions in attempts to acquire the lock. When a processor has obtained a lock, all other processors which also wish to obtain the same lock keep trying to obtain the lock by initiating bus transactions repeatedly until they get hold of the lock. This increases the bus traffic requirement of test-and-set significantly. This slows down all other traffic from cache and coherence misses. It slows down the overall section, since the traffic is saturated by failed lock acquisition attempts. Test-and-test-and-set is an improvement over TSL since it does not initiate lock acquisition requests continuously.\n\nWhen we consider fairness, we consider if a processor gets a fair chance of acquiring the lock when it is set free. In an extreme situation the processor might starve i.e. it might not be able to acquire the lock for an extended period of time even though it has become free during that time.\n\nStorage overhead for TSL is next to nothing since only one lock is required. Uncontended latency is also low since only one atomic instruction and branch are needed.\n\n\n"}
{"id": "213915", "url": "https://en.wikipedia.org/wiki?curid=213915", "title": "Triangular number", "text": "Triangular number\n\nA triangular number or triangle number counts objects arranged in an equilateral triangle, as in the diagram on the right. The th triangular number is the number of dots in the triangular arrangement with dots on a side, and is equal to the sum of the natural numbers from 1 to . The sequence of triangular numbers , starting at the 0th triangular number, is\n\nThe triangle numbers are given by the following explicit formulas:\n\nwhere formula_2 is a binomial coefficient. It represents the number of distinct pairs that can be selected from objects, and it is read aloud as \" plus one choose two\".\n\nThe first equation can be illustrated using a visual proof. For every triangular number formula_3, imagine a \"half-square\" arrangement of objects corresponding to the triangular number, as in the figure below. Copying this arrangement and rotating it to create a rectangular figure doubles the number of objects, producing a rectangle with dimensions formula_4, which is also the number of objects in the rectangle. Clearly, the triangular number itself is always exactly half of the number of objects in such a figure, or: formula_5. The example formula_6 follows:\n\nThe first equation can also be established using mathematical induction. Since the sum of the first (one) natural number(s) is clearly equal to one, a basis case is established. Assuming the inductive hypothesis for some formula_7 and adding formula_8 to both sides immediately gives\n\nIn other words, since the proposition formula_10 (that is, the first equation, or inductive hypothesis itself) is true when formula_11, and since formula_10 being true implies that formula_13 is also true, then the first equation is true for all natural numbers. The above argument can be easily modified to start with, and include, zero.\n\nCarl Friedrich Gauss is said to have found this relationship in his early youth, by multiplying pairs of numbers in the sum by the values of each pair . However, regardless of the truth of this story, Gauss was not the first to discover this formula, and some find it likely that its origin goes back to the Pythagoreans 5th century BC. The two formulae were described by the Irish monk Dicuil in about 816 in his Computus.\n\nThe triangular number solves the handshake problem of counting the number of handshakes if each person in a room with people shakes hands once with each person. In other words, the solution to the handshake problem of people is . The function is the additive analog of the factorial function, which is the \"products\" of integers from 1 to .\n\nThe number of line segments between closest pairs of dots in the triangle can be represented in terms of the number of dots or with a recurrence relation:\n\nIn the limit, the ratio between the two numbers, dots and line segments is\n\nTriangular numbers have a wide variety of relations to other figurate numbers.\n\nMost simply, the sum of two consecutive triangular numbers is a square number, with the sum being the square of the difference between the two (and thus the difference of the two being the square root of the sum). Algebraically,\n\nAlternatively, the same fact can be demonstrated graphically:\nThere are infinitely many triangular numbers that are also square numbers; e.g., 1, 36, 1225. Some of them can be generated by a simple recursive formula:\n\n\"All\" square triangular numbers are found from the recursion\n\nAlso, the square of the th triangular number is the same as the sum of the cubes of the integers 1 to . This can also be expressed as\n\nThe sum of the all triangular numbers up to the th triangular number is the th tetrahedral number,\n\nMore generally, the difference between the th -gonal number and the th -gonal number is the th triangular number. For example, the sixth heptagonal number (81) minus the sixth hexagonal number (66) equals the fifth triangular number, 15. Every other triangular number is a hexagonal number. Knowing the triangular numbers, one can reckon any centered polygonal number; the th centered -gonal number is obtained by the formula\n\nwhere is a triangular number.\n\nThe positive difference of two triangular numbers is a trapezoidal number.\n\nTriangular numbers correspond to the first-degree case of Faulhaber's formula.\n\nAlternating triangular numbers (1, 6, 15, 28, ...) are also hexagonal numbers.\n\nEvery even perfect number is triangular (as well as hexagonal), given by the formula\nwhere is a Mersenne prime. No odd perfect numbers are known, hence all known perfect numbers are triangular.\n\nFor example, the third triangular number is (3 × 2 =) 6, the seventh is (7 × 4 =) 28, the 31st is (31 × 16 =) 496, and the 127th is (127 × 64 =) 8128.\n\nIn base 10, the digital root of a nonzero triangular number is always 1, 3, 6, or 9. Hence every triangular number is either divisible by three or has a remainder of 1 when divided by 9:\nThe digital root pattern for triangular numbers, repeating every nine terms, as shown above, is \"1, 3, 6, 1, 6, 3, 1, 9, 9\".\n\nThe converse of the statement above is, however, not always true. For example, the digital root of 12, which is not a triangular number, is 3 and divisible by three.\n\nIf is a triangular number, then is also a triangular number, given is an odd square and \n\nThe first several pairs of this form (not counting ) are: , , , , , , … etc. Given is equal to , these formulas yield , , , , and so on.\n\nThe sum of the reciprocals of all the nonzero triangular numbers is\n\nThis can be shown by using the basic sum of a telescoping series:\n\nTwo other interesting formulas regarding triangular numbers are\n\nand\nboth of which can easily be established either by looking at dot patterns (see above) or with some simple algebra.\n\nIn 1796, German mathematician and scientist Carl Friedrich Gauss discovered that every positive integer is representable as a sum of three triangular numbers (possibly including = 0), writing in his diary his famous words, \"ΕΥΡΗΚΑ! \". Note that this theorem does not imply that the triangular numbers are different (as in the case of 20 = 10 + 10 + 0), nor that a solution with exactly three nonzero triangular numbers must exist. This is a special case of the Fermat polygonal number theorem.\n\nThe largest triangular number of the form is 4095 (see Ramanujan–Nagell equation).\n\nWacław Franciszek Sierpiński posed the question as to the existence of four distinct triangular numbers in geometric progression. It was conjectured by Polish mathematician Kazimierz Szymiczek to be impossible. This conjecture was proven by Fang and Chen in 2007.\n\nFormulas involving expressing an integer as the sum of triangular numbers are connected to theta functions, in particular the Ramanujan theta function.\n\nA fully connected network of computing devices requires the presence of cables or other connections; this is equivalent to the handshake problem mentioned above.\n\nIn a tournament format that uses a round-robin group stage, the number of matches that need to be played between teams is equal to the triangular number . For example, a group stage with 4 teams requires 6 matches, and a group stage with 8 teams requires 28 matches. This is also equivalent to the handshake problem and fully connected network problems.\n\nOne way of calculating the depreciation of an asset is the sum-of-years' digits method, which involves finding , where is the length in years of the asset's useful life. Each year, the item loses , where is the item's beginning value (in units of currency), is its final salvage value, is the total number of years the item is usable, and the current year in the depreciation schedule. Under this method, an item with a usable life of = 4 years would lose of its \"losable\" value in the first year, in the second, in the third, and in the fourth, accumulating a total depreciation of (the whole) of the losable value.\n\nBy analogy with the square root of , one can define the (positive) triangular root of as the number such that :\n\nwhich follows immediately from the quadratic formula. So an integer is triangular if and only if is a square. Equivalently, if the positive triangular root of is an integer, then is the th triangular number.\n\n\n"}
{"id": "54256085", "url": "https://en.wikipedia.org/wiki?curid=54256085", "title": "Weapons of Math Destruction", "text": "Weapons of Math Destruction\n\nWeapons of Math Destruction is a 2016 American book about the societal impact of algorithms, written by Cathy O'Neil. It explores how some big data algorithms are increasingly used in ways that reinforce preexisting inequality. It was longlisted for the 2016 National Book Award for Nonfiction.\n\nO'Neil, a mathematician, analyses how the use of big data and algorithms in a variety of fields, including insurance, advertising, education, and policing, can lead to decisions that harm the poor, reinforce racism, and amplify inequality. According to National Book Foundation:\n\nShe posits that these problematic mathematical tools share three key features: they are opaque, unregulated and difficult to contest, and at the same time scalable, thereby amplifying any inherent biases to affect increasingly larger populations.\n\nThe book received widespread praise for elucidating the consequences of reliance on big data models for structuring socioeconomic resources. Clay Shirky from The New York Times Book Review said \"O’Neil does a masterly job explaining the pervasiveness and risks of the algorithms that regulate our lives,\" while pointing out that \"the section on solutions is weaker than the illustration of the problem.\". Kirkus Reviews praised the book for being \"an unusually lucid and readable\" discussion of a technical subject.\n"}
{"id": "33456", "url": "https://en.wikipedia.org/wiki?curid=33456", "title": "Well-order", "text": "Well-order\n\nIn mathematics, a well-order (or well-ordering or well-order relation) on a set \"S\" is a total order on \"S\" with the property that every non-empty subset of \"S\" has a least element in this ordering. The set \"S\" together with the well-order relation is then called a well-ordered set. In some academic articles and textbooks these terms are instead written as wellorder, wellordered, and wellordering or well order, well ordered, and well ordering.\n\nEvery non-empty well-ordered set has a least element. Every element \"s\" of a well-ordered set, except a possible greatest element, has a unique successor (next element), namely the least element of the subset of all elements greater than \"s\". There may be elements besides the least element which have no predecessor (see \"Natural numbers\" below for an example). In a well-ordered set \"S\", every subset \"T\" which has an upper bound has a least upper bound, namely the least element of the subset of all upper bounds of \"T\" in \"S\".\n\nIf ≤ is a non-strict well ordering, then < is a strict well ordering. A relation is a strict well ordering if and only if it is a well-founded strict total order. The distinction between strict and non-strict well orders is often ignored since they are easily interconvertible.\n\nEvery well-ordered set is uniquely order isomorphic to a unique ordinal number, called the order type of the well-ordered set. The well-ordering theorem, which is equivalent to the axiom of choice, states that every set can be well ordered. If a set is well ordered (or even if it merely admits a well-founded relation), the proof technique of transfinite induction can be used to prove that a given statement is true for all elements of the set.\n\nThe observation that the natural numbers are well ordered by the usual less-than relation is commonly called the well-ordering principle (for natural numbers).\n\nEvery well-ordered set is uniquely order isomorphic to a unique ordinal number, called the order type of the well-ordered set. The position of each element within the ordered set is also given by an ordinal number. In the case of a finite set, the basic operation of counting, to find the ordinal number of a particular object, or to find the object with a particular ordinal number, corresponds to assigning ordinal numbers one by one to the objects. The size (number of elements, cardinal number) of a finite set is equal to the order type. Counting in the everyday sense typically starts from one, so it assigns to each object the size of the initial segment with that object as last element. Note that these numbers are one more than the formal ordinal numbers according to the isomorphic order, because these are equal to the number of earlier objects (which corresponds to counting from zero). Thus for finite \"n\", the expression \"\"n\"-th element\" of a well-ordered set requires context to know whether this counts from zero or one. In a notation \"β-th element\" where β can also be an infinite ordinal, it will typically count from zero.\n\nFor an infinite set the order type determines the cardinality, but not conversely: well-ordered sets of a particular cardinality can have many different order types. For a countably infinite set, the set of possible order types is even uncountable.\n\nThe standard ordering ≤ of the natural numbers is a well ordering and has the additional property that every non-zero natural number has a unique predecessor.\n\nAnother well ordering of the natural numbers is given by defining that all even numbers are less than all odd numbers, and the usual ordering applies within the evens and the odds:\n\nThis is a well-ordered set of order type ω + ω. Every element has a successor (there is no largest element). Two elements lack a predecessor: 0 and 1.\n\nUnlike the standard ordering ≤ of the natural numbers, the standard ordering ≤ of the integers is not a well ordering, since, for example, the set of negative integers does not contain a least element.\n\nThe following relation \"R\" is an example of well ordering of the integers: \"x R y\" if and only if one of the following conditions holds:\nThis relation \"R\" can be visualized as follows: \n\"R\" is isomorphic to the ordinal number ω + ω.\n\nAnother relation for well ordering the integers is the following definition: \"x\" ≤ \"y\" iff (|\"x\"| < |\"y\"| or (|\"x\"| = |\"y\"| and \"x\" ≤ \"y\")). This well order can be visualized as follows: \n\nThis has the order type ω.\n\nThe standard ordering ≤ of any real interval is not a well ordering, since, for example, the open interval (0, 1) ⊆ [0,1] does not contain a least element. From the ZFC axioms of set theory (including the axiom of choice) one can show that there is a well order of the reals. Also Wacław Sierpiński proved that ZF + GCH (the generalized continuum hypothesis) imply the axiom of choice and hence a well order of the reals. Nonetheless, it is possible to show that the ZFC+GCH axioms alone are not sufficient to prove the existence of a definable (by a formula) well order of the reals. However it is consistent with ZFC that a definable well ordering of the reals exists—for example, it is consistent with ZFC that V=L, and it follows from ZFC+V=L that a particular formula well orders the reals, or indeed any set.\n\nAn uncountable subset of the real numbers with the standard ordering ≤ cannot be a well order: Suppose \"X\" is a subset of R well ordered by ≤. For each \"x\" in \"X\", let \"s\"(\"x\") be the successor of \"x\" in ≤ ordering on \"X\" (unless \"x\" is the last element of \"X\"). Let \"A\" = { (\"x\", \"s\"(\"x\")) | \"x\" ∈ \"X\" } whose elements are nonempty and disjoint intervals. Each such interval contains at least one rational number, so there is an injective function from \"A\" to Q. There is an injection from \"X\" to \"A\" (except possibly for a last element of \"X\" which could be mapped to zero later). And it is well known that there is an injection from \"Q\" to the natural numbers (which could be chosen to avoid hitting zero). Thus there is an injection from \"X\" to the natural numbers which means that \"X\" is countable. On the other hand, a countably infinite subset of the reals may or may not be a well order with the standard \"≤\". For example,\n\n\nExamples of well orders:\n\nIf a set is totally ordered, then the following are equivalent to each other:\n\n\nEvery well-ordered set can be made into a topological space by endowing it with the order topology.\n\nWith respect to this topology there can be two kinds of elements:\n\nFor subsets we can distinguish:\n\nA subset is cofinal in the whole set if and only if it is unbounded in the whole set or it has a maximum which is also maximum of the whole set.\n\nA well-ordered set as topological space is a first-countable space if and only if it has order type less than or equal to ω (omega-one), that is, if and only if the set is countable or has the smallest uncountable order type.\n\n\n"}
