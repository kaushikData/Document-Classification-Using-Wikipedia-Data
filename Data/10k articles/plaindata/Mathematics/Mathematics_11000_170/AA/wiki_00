{"id": "831517", "url": "https://en.wikipedia.org/wiki?curid=831517", "title": "122 (number)", "text": "122 (number)\n\n122 (one hundred [and] twenty-two) is the natural number following 121 and preceding 123.\n\nIt is a nontotient since there is no integer with exactly 122 coprimes below it. Nor is there an integer with exactly 122 integers with common factors below it, making 122 a noncototient.\n\n\n122 is also:\n\n"}
{"id": "43488947", "url": "https://en.wikipedia.org/wiki?curid=43488947", "title": "Annals of the Institute of Statistical Mathematics", "text": "Annals of the Institute of Statistical Mathematics\n\nAnnals of the Institute of Statistical Mathematics is a bimonthly peer-reviewed scientific journal covering statistics. It was established in 1949 and is published by Springer Science+Business Media on behalf of Institute of Statistical Mathematics. The editor-in-chief is Tomoyuki Higuchi (Institute of Statistical Mathematics). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 0.733.\n"}
{"id": "13129755", "url": "https://en.wikipedia.org/wiki?curid=13129755", "title": "Apollonian sphere packing", "text": "Apollonian sphere packing\n\nApollonian sphere packing is the three-dimensional equivalent of the Apollonian gasket. The principle of construction is very similar: with any four spheres that are cotangent to each other, it is then possible to construct two more spheres that are cotangent to four of them.\n\nThe fractal dimension is approximately 2.473946 (±1 in the last digit).\n\nSoftware for generating and visualization of the apollonian sphere packing: ApolFrac.\n"}
{"id": "9553854", "url": "https://en.wikipedia.org/wiki?curid=9553854", "title": "Artin–Rees lemma", "text": "Artin–Rees lemma\n\nIn mathematics, the Artin–Rees lemma is a basic result about modules over a Noetherian ring, along with results such as the Hilbert basis theorem. It was proved in the 1950s in independent works by the mathematicians Emil Artin and David Rees; a special case was known to Oscar Zariski prior to their work.\n\nOne consequence of the lemma is the Krull intersection theorem. The result is also used to prove the exactness property of completion .\n\nLet \"I\" be an ideal in a Noetherian ring \"R\"; let \"M\" be a finitely generated \"R\"-module and let \"N\" a submodule of \"M\". Then there exists an integer \"k\" ≥ 1 so that, for \"n\" ≥ \"k\",\n\nThe lemma immediately follows from the fact that \"R\" is Noetherian once necessary notions and notations are set up.\n\nFor any ring \"R\" and an ideal \"I\" in \"R\", we set formula_2 (\"B\" for blow-up.) We say a decreasing sequence of submodules formula_3 is an \"I\"-filtration if formula_4; moreover, it is stable if formula_5 for sufficiently large \"n\". If \"M\" is given an \"I\"-filtration, we set formula_6; it is a graded module over formula_7.\n\nNow, let \"M\" be a \"R\"-module with the \"I\"-filtration formula_8 by finitely generated \"R\"-modules. We make an observation\nIndeed, if the filtration is \"I\"-stable, then formula_9 is generated by the first formula_12 terms formula_13 and those terms are finitely generated; thus, formula_9 is finitely generated. Conversely, if it is finitely generated, say, by some homogeneous elements in formula_15, then, for formula_16, each \"f\" in formula_17 can be written as\nwith the generators formula_19 in formula_20. That is, formula_21.\n\nWe can now prove the lemma, assuming \"R\" is Noetherian. Let formula_22. Then formula_17 are an \"I\"-stable filtration. Thus, by the observation, formula_9 is finitely generated over formula_7. But formula_26 is a Noetherian ring since \"R\" is. (The ring formula_27 is called the Rees algebra.) Thus, formula_9 is a Noetherian module and any submodule is finitely generated over formula_7; in particular, formula_30 is finitely generated when \"N\" is given the induced filtration; i.e., formula_31. Then the induced filtration is \"I\"-stable again by the observation.\n\nBesides the use in completion of a ring, a typical application of the lemma is the proof of the Krull's intersection theorem, which says: formula_32 for a proper ideal \"I\" in a commutative Noetherian local ring. By the lemma applied to the intersection formula_33, we find \"k\" such that for formula_16,\nBut then formula_36 and thus formula_37 by Nakayama.\n\n"}
{"id": "27060913", "url": "https://en.wikipedia.org/wiki?curid=27060913", "title": "Bhaskara I's sine approximation formula", "text": "Bhaskara I's sine approximation formula\n\nIn mathematics, Bhaskara I's sine approximation formula is a rational expression in one variable for the computation of the approximate values of the trigonometric sines discovered by Bhaskara I (c. 600 – c. 680), a seventh-century Indian mathematician.\nThis formula is given in his treatise titled \"Mahabhaskariya\". It is not known how Bhaskara I arrived at his approximation formula. However, several historians of mathematics have put forward different hypotheses as to the method Bhaskara might have used to arrive at his formula. The formula is elegant, simple and enables one to compute reasonably accurate values of trigonometric sines without using any geometry whatsoever.\n\nThe formula is given in verses 17 – 19, Chapter VII, Mahabhaskariya of Bhaskara I. A translation of the verses is given below:\n\n\nIn modern mathematical notations, for an angle \"x\" in degrees, this formula gives\n\nBhaskara I's sine approximation formula can be expressed using the radian measure of angles as follows.\n\nFor a positive integer \"n\" this takes the following form:\n\nThe formula acquires an even simpler form when expressed in terms of the cosine rather than the sine. Using radian measure for angle, and putting formula_4, one gets\n\nThe assonance of \"formula_6\" and \"formula_7\" makes this expression especially pleasing as a mnemonic. \n\nEquivalent forms of Bhaskara I's formula have been given by almost all subsequent astronomers and mathematicians of India. For example, Brahmagupta's (598 – 668 CE)\n\"Brhma-Sphuta-Siddhanta\" (verses 23 – 24, Chapter XIV) gives the formula in the following form:\n\nAlso, Bhaskara II (1114 – 1185 CE) has given this formula in his Lilavati (Kshetra-vyavahara, Soka No.48) in the following form:\n\nThe formula is applicable for values of \"x\"° in the range from 0 to 180. The formula is remarkably accurate in this range. The graphs of sin ( \"x\" ) and the approximation formula are indistinguishable and are nearly identical. One of the accompanying figures gives the graph of the error function, namely the function,\nin using the formula. It shows that the maximum absolute error in using the formula is around 0.0016. From a plot of the percentage value of the absolute error, it is clear that the maximum percentage error is less than 1.8. The approximation formula thus gives sufficiently accurate values of sines for all practical purposes. However it was not sufficient for the more accurate computational requirements of astronomy. The search for more accurate formulas by Indian astronomers eventually led to the discovery the power series expansions of sin \"x\" and cos \"x\" by Madhava of Sangamagrama (c. 1350 – c. 1425), the founder of the Kerala school of astronomy and mathematics.\n\nBhaskara I had not indicated any method by which he arrived at his formula. Historians have speculated on various possibilities. No definitive answers have as yet been obtained. Beyond its historical importance of being a prime example of the mathematical achievements of ancient Indian astronomers, the formula is of significance from a modern perspective also. Mathematicians have attempted to derive the rule using modern concepts and tools. Around half a dozen methods have been suggested, each based on a separate set of premises. Most of these derivations use only elementary concepts.\n\nLet the circumference of a circle be measured in degrees and let the radius \"R\" of the circle be also measured in degrees. Choosing a fixed diameter \"AB\" and an arbitrary point \"P\" on the circle and dropping the perpendicular \"PM\" to \"AB\", we can compute the area of the triangle \"APB\" in two ways. Equating the two expressions for the area one gets (1/2) \"AB\" × \"PM\" = (1/2) \"AP\" × \"BP\". This gives\nLetting \"x\" be the length of the arc \"AP\", the length of the arc \"BP\" is 180 - \"x\". These arcs are much bigger than the respective chords. Hence one gets\nOne now seeks two constants α and β such that\nIt is indeed not possible to obtain such constants. However one may choose values for α and β so that the above expression is valid for two chosen values of the arc length \"x\". Choosing 30° and 90° as these values and solving the resulting equations, one immediately gets Bhaskara I's sine approximation formula.\n\nAssuming that \"x\" is in radians, one may seek an approximation to sin (\"x\") in the following form:\nThe constants \"a\", \"b\", \"c\", \"p\", \"q\" and \"r\" (only five of them are independent) can be determined by assuming that the formula must be exactly valid when \"x\" = 0, π/6, π/2, π, and further assuming that it has to satisfy the property that sin (\"x\") = sin (π - \"x\"). This procedure produces the formula expressed using radian measure of angles.\n\nThe part of the graph of sin(\"x\") in the range from 0° to 180° \"looks like\" part of a parabola through the points (0, 0) and (180, 0). The general such parabola is\n\nThe parabola that also passes through (90, 1) (which is the point corresponding to the value sin(90°) = 1) is\n\nThe parabola which also passes through (30, 1/2) (which is the point corresponding to the value sin(30°) = 1/2) is\n\nThese expressions suggest a varying denominator which takes the value 90 × 90 when \"x\" = 90 and the value 2 × 30 × 150 when \"x\" = 30. That this expression should also be symmetrical about the line ' \"x\" = 90' rules out the possibility of choosing a linear expression in \"x\". Computations involving \"x\"(180 − \"x\") might immediately suggest that the expression could be of the form\n\nA little experimentation (or by setting up and solving two linear equations in \"a\" and \"b\") will yield the values \"a\" = 5/4, \"b\" = −1/4. These give Bhaskara I's sine approximation formula.\n\n\n"}
{"id": "39206504", "url": "https://en.wikipedia.org/wiki?curid=39206504", "title": "Brahmagupta's interpolation formula", "text": "Brahmagupta's interpolation formula\n\nBrahmagupta's interpolation formula is a second-order polynomial interpolation formula developed by the Indian mathematician and astronomer Brahmagupta (598–668 CE) in the early 7th century CE. The Sanskrit couplet describing the formula can be found in the supplementary part of \"Khandakadyaka\" a work of Brahmagupta completed in 665 CE. The same couplet appears in Brahmagupta's earlier \"Dhyana-graha-adhikara\", which was probably written \"near the beginning of the second quarter of the 7th century CE, if not earlier.\" Brahmagupta was the one of the first to describe and use an interpolation formula using second-order differences.\n\nBrahmagupa's interpolation formula is equivalent to modern-day second-order Newton–Stirling interpolation formula.\n\nGiven a set of tabulated values of a function in the table below, let it be required to compute the value of , .\n\nAssuming that the successively tabulated values of are equally spaced with a common spacing of , Aryabhata had considered the table of first differences of the table of values of a function. Writing\n\nthe following table can be formed:\n\nMathematicians prior to Brahmagupta used a simple linear interpolation formula. The linear interpolation formula to compute is\n\nFor the computation of , Brahmagupta replaces with another expression which gives more accurate values and which amounts to using a second-order interpolation formula.\n\nIn Brahmagupta's terminology the difference is the \"gatakhanda\", meaning \"past difference\" or the difference that was crossed over, the difference is the \"bhogyakhanda\" which is the \"difference yet to come\". \"Vikala\" is the amount in minutes by which the interval has been covered at the point where we want to interpolate. In the present notations it is . The new expression which replaces is called \"sphuta-bhogyakhanda\". The description of \"sphuta-bhogyakhanda\" is contained in the following Sanskrit couplet (\"Dhyana-Graha-Upadesa-Adhyaya, 17; Khandaka Khadyaka, IX, 8\"):\n\nThis has been translated using Bhattolpala's (10th century CE) commentary as follows:\n\nThis formula was originally stated for the computation of the values of the sine function for which the common interval in the underlying base table was 900 minutes or 15 degrees. So the reference to 900 is in fact a reference to the common interval .\n\nBrahmagupta's method computation of \"shutabhogyakhanda\" can be formulated in modern notation as follows:\n\nThe ± sign is to be taken according to whether is less than or greater than , or equivalently, according to whether or . Brahmagupta's expression can be put in the following form:\n\nThis correction factor yields the following approximate value for :\n\nThis is Stirling's interpolation formula truncated at the second-order differences. It is not known how Brahmagupta arrived at his interpolation formula. Brahmagupta has given a separate formula for the case where the values of the independent variable are not equally spaced.\n\n"}
{"id": "705600", "url": "https://en.wikipedia.org/wiki?curid=705600", "title": "Cesàro summation", "text": "Cesàro summation\n\nIn mathematical analysis, Cesàro summation (also known as the Cesàro mean) assigns values to some infinite sums that are not convergent in the usual sense. The Cesàro sum is defined as the limit, as \"n\" tends to infinity, of the sequence of arithmetic means of the first \"n\" partial sums of the series.\n\nCesàro summation is named for the Italian analyst Ernesto Cesàro (1859–1906). It is a special case of a matrix summability method.\n\nThe term \"summation\" can be misleading, as some statements and proofs regarding Cesàro summation can be said to implicate the Eilenberg–Mazur swindle. For example, it is commonly applied to Grandi's series with the conclusion that the \"sum\" of that series is 1/2.\n\nLet formula_1 be a sequence, and let\n\nbe its th partial sum.\n\nThe sequence is called Cesàro summable, with Cesàro sum , if, as tends to infinity, the arithmetic mean of its first \"n\" partial sums tends to :\n\nThe value of the resulting limit is called the Cesàro sum of the series formula_4 If this series is (conditionally) convergent, then it is Cesàro summable and its Cesàro sum is the usual sum.\n\nLet for . That is, formula_5 is the sequence\nLet denote the series\nThe series is known as Grandi's series.\n\nLet formula_8 denote the sequence of partial sums of :\n\nThis sequence of partial sums does not converge, so the series is divergent. However, Cèsaro summable. Let formula_10 be the sequence of arithmetic means of the first partial sums:\nThen\nand therefore, the Cesàro sum of the series is .\n\nAs another example, let for . That is, formula_1 is the sequence\n\nLet now denote the series\n\nThen the sequence of partial sums formula_16 is\n\nSince the sequence of partial sums grows without bound, the series diverges to infinity. The sequence of means of partial sums of G is\n\nThis sequence diverges to infinity as well, so is Cesàro summable. In fact, for any sequence which diverges to (positive or negative) infinity, the Cesàro method also leads to a sequence that diverges likewise, and hence such a series is not Cesàro summable.\n\nIn 1890, Ernesto Cesàro stated a broader family of summation methods which have since been called for non-negative integers . The method is just ordinary summation, and is Cesàro summation as described above.\n\nThe higher-order methods can be described as follows: given a series , define the quantities\n\n(where the upper indices do not denote exponents) and define to be for the series . Then the sum of is denoted by and has the value\n\nif it exists . This description represents an -times iterated application of the initial summation method and can be restated as\n\nEven more generally, for , let be implicitly given by the coefficients of the series\n\nand as above. In particular, are the binomial coefficients of power . Then the sum of is defined as above.\n\nIf has a sum, then it also has a sum for every , and the sums agree; furthermore we have if (see little- notation).\n\nLet . The integral formula_23 is summable if\n\nexists and is finite . The value of this limit, should it exist, is the sum of the integral. Analogously to the case of the sum of a series, if , the result is convergence of the improper integral. In the case , convergence is equivalent to the existence of the limit\n\nwhich is the limit of means of the partial integrals.\n\nAs is the case with series, if an integral is summable for some value of , then it is also summable for all , and the value of the resulting limit is the same.\n\n"}
{"id": "58716885", "url": "https://en.wikipedia.org/wiki?curid=58716885", "title": "Christine Paulin-Mohring", "text": "Christine Paulin-Mohring\n\nChristine Paulin-Mohring (born 1962) is a mathematical logician and computer scientist, and Professor at Paris-Sud 11 University, best known for developing the interactive theorem prover Coq, for which she and the rest of the development team (Thierry Coquand, Gérard Huet, Bruno Barras, Jean-Christophe Filliâtre, Hugo Herbelin, Chetan Murthy, Yves Bertot and Pierre Castéran) won the 2013 ACM Software System Award awarded by the Association for Computing Machinery.\n\nPaulin-Mohring received her PhD in 1989 under the supervision of Gérard Huet. She has been professor at Paris-Sud 11 University since 1997.\n\nBetween 2012 and 2015 she was the Scientific Coordinator of the Labex DigiCosme. Currently she is a member of the editorial board of the \"Journal of Formalized Reasoning\".\n\n"}
{"id": "20751674", "url": "https://en.wikipedia.org/wiki?curid=20751674", "title": "Clique complex", "text": "Clique complex\n\nClique complexes, flag complexes, and conformal hypergraphs are closely related mathematical objects in graph theory and geometric topology that each describe the cliques (complete subgraphs) of an undirected graph.\n\nThe clique complex \"X\"(\"G\") of an undirected graph \"G\" is an abstract simplicial complex (that is, a family of finite sets closed under the operation of taking subsets), formed by the sets of vertices in the cliques of \"G\". Any subset of a clique is itself a clique, so this family of sets meets the requirement of an abstract simplicial complex that every subset of a set in the family should also be in the family. The clique complex can also be viewed as a topological space in which each clique of \"k\" vertices is represented by a simplex of dimension \"k\" − 1. The 1-skeleton of \"X\"(\"G\") (also known as the \"underlying graph\" of the complex) is an undirected graph with a vertex for every 1-element set in the family and an edge for every 2-element set in the family; it is isomorphic to \"G\".\n\nClique complexes are also known as Whitney complexes. A Whitney triangulation or clean triangulation of a two-dimensional manifold is an embedding of a graph \"G\" onto the manifold in such a way that every face is a triangle and every triangle is a face. If a graph \"G\" has a Whitney triangulation, it must form a cell complex that is isomorphic to the Whitney complex of \"G\". In this case, the complex (viewed as a topological space) is homeomorphic to the underlying manifold. A graph \"G\" has a 2-manifold clique complex, and can be embedded as a Whitney triangulation, if and only if \"G\" is locally cyclic; this means that, for every vertex \"v\" in the graph, the induced subgraph formed by the neighbors of \"v\" forms a single cycle.\n\nThe independence complex \"I\"(\"G\") of a graph \"G\" is formed in the same way as the clique complex from the independent sets of \"G\". It is the clique complex of the complement graph of \"G\".\n\nIn an abstract simplicial complex, a set \"S\" of vertices that is not itself part of the complex, but such that each pair of vertices in \"S\" belongs to some simplex in the complex, is called an \"empty simplex\". Mikhail Gromov defined the \"no-Δ condition\" to be the condition that a complex have no empty simplices. A flag complex is an abstract simplicial complex that has no empty simplices; that is, it is a complex satisfying Gromov's no-Δ condition.\nAny flag complex is the clique complex of its 1-skeleton. Thus, flag complexes and clique complexes are essentially the same thing. However, in many cases it may be convenient to define a flag complex directly from some data other than a graph, rather than indirectly as the clique complex of a graph derived from that data.\n\nThe primal graph \"G(H)\" of a hypergraph is the graph on the same vertex set that has as its edges the pairs of vertices appearing together in the same hyperedge. A hypergraph is said to be conformal if every maximal clique of its primal graph is a hyperedge, or equivalently, if every clique of its primal graph is contained in some hyperedge. If the hypergraph is required to be downward-closed (so it contains all hyperedges that are contained in some hyperedge) then the hypergraph is conformal precisely when it is a flag complex. This relates the language of hypergraphs to the language of simplicial complexes.\n\nThe barycentric subdivision of any cell complex \"C\" is a flag complex having one vertex per cell of \"C\". A collection of vertices of the barycentric subdivision form a simplex if and only if the corresponding collection of cells of \"C\" form a flag (a chain in the inclusion ordering of the cells). In particular, the barycentric subdivision of a cell complex on a 2-manifold gives rise to a Whitney triangulation of the manifold.\n\nThe order complex of a partially ordered set consists of the chains (totally ordered subsets) of the partial order. If every pair of some subset is itself ordered, then the whole subset is a chain, so the order complex satisfies the no-Δ condition. It may be interpreted as the clique complex of the comparability graph of the partial order.\n\nThe matching complex of a graph consists of the sets of edges no two of which share an endpoint; again, this family of sets satisfies the no-Δ condition. It may be viewed as the clique complex of the complement graph of the line graph of the given graph. When the matching complex is referred to without any particular graph as context, it means the matching complex of a complete graph. The matching complex of a complete bipartite graph \"K\" is known as a chessboard complex. It is the clique graph of the complement graph of a rook's graph, and each of its simplices represents a placement of rooks on an \"m\" × \"n\" chess board such that no two of the rooks attack each other. When \"m\" = \"n\" ± 1, the chessboard complex forms a pseudo-manifold.\n\nThe Vietoris–Rips complex of a set of points in a metric space is a special case of a clique complex, formed from the unit disk graph of the points; however, every clique complex \"X(G)\" may be interpreted as the Vietoris–Rips complex of the shortest path metric on the underlying graph \"G\".\n\nGromov showed that a cubical complex (that is, a family of hypercubes intersecting face-to-face) forms a CAT(0) space if and only if the complex is simply connected and the link of every vertex forms a flag complex. A cubical complex meeting these conditions is sometimes called a cubing or a \"space with walls\".\n\n\n"}
{"id": "20749642", "url": "https://en.wikipedia.org/wiki?curid=20749642", "title": "Computing the permanent", "text": "Computing the permanent\n\nIn linear algebra, the computation of the permanent of a matrix is a problem that is thought to be more difficult than the computation of the determinant of a matrix despite the apparent similarity of the definitions.\n\nThe permanent is defined similarly to the determinant, as a sum of products of sets of matrix entries that lie in distinct rows and columns. However, where the determinant weights each of these products with a ±1 sign based on the parity of the set, the permanent weights them all with a +1 sign.\n\nWhile the determinant can be computed in polynomial time by Gaussian elimination, it is generally believed that the permanent cannot be computed in polynomial time. In computational complexity theory, a theorem of Valiant states that computing permanents is #P-hard, and even #P-complete for matrices in which all entries are 0 or 1. This puts the computation of the permanent in a class of problems believed to be even more difficult to compute than NP. It is known that computing the permanent is impossible for logspace-uniform ACC circuits.\n\nThe development of both exact and approximate algorithms for computing the permanent of a matrix is an active area of research.\n\nThe permanent of an \"n\"-by-\"n\" matrix \"A\" = (\"a\") is defined as\n\nThe sum here extends over all elements σ of the symmetric group \"S\", i.e. over all permutations of the numbers 1, 2, ..., \"n\". This formula differs from the corresponding formula for the determinant only in that, in the determinant, each product is multiplied by the sign of the permutation σ while in this formula each product is unsigned. The formula may be directly translated into an algorithm that naively expands the formula, summing over all permutations and within the sum multiplying out each matrix entry. This requires \"n!\" \"n\" arithmetic operations.\n\nThe best known general exact algorithm is due to .\nRyser’s method is based on an inclusion–exclusion formula that can be given as follows: Let formula_2 be obtained from \"A\" by deleting \"k\" columns, let formula_3 be the product of the row-sums of formula_2, and let formula_5 be the sum of the values of formula_3 over all possible formula_2. Then\n\nIt may be rewritten in terms of the matrix entries as follows\n\nRyser’s formula can be evaluated using formula_10 arithmetic operations, or formula_11 by processing the sets formula_12 in Gray code order.\n\nAnother formula that appears to be as fast as Ryser's (or perhaps even twice as fast) is to be found in the two Ph.D. theses; see , ; also\n. The methods to find the formula are quite different, being related to the combinatorics of the Muir algebra, and to finite difference theory respectively. Another way, connected with invariant theory is via the polarization identity for a symmetric tensor . The formula generalizes to infinitely many others, as found by all these authors, although it is not clear if they are any faster than the basic one. See .\n\nThe simplest known formula of this type (when the characteristic of the field is not two) is\nwhere the outer sum is over all formula_14 vectors formula_15.\n\nThe number of perfect matchings in a bipartite graph is counted by the permanent of the graph's biadjacency matrix, and the permanent of any 0-1 matrix can be interpreted in this way as the number of perfect matchings in a graph. For planar graphs (regardless of bipartiteness), the FKT algorithm computes the number of perfect matchings in polynomial time by changing the signs of a carefully chosen subset of the entries in the Tutte matrix of the graph, so that the Pfaffian of the resulting skew-symmetric matrix (the square root of its determinant) is the number of perfect matchings. This technique can be generalized to graphs that contain no subgraph homeomorphic to the complete bipartite graph \"K\".\n\nGeorge Pólya had asked the question of when it is possible to change the signs of some of the entries of a 01 matrix A so that the determinant of the new matrix is the permanent of A. Not all 01 matrices are \"convertible\" in this manner; in fact it is known () that\nthere is no linear map formula_16 such that formula_17 for all formula_18 matrices formula_19. The characterization of \"convertible\" matrices was given by who showed that such matrices are precisely those that are the biadjacency matrix of bipartite graphs that have a Pfaffian orientation: an orientation of the edges such that for every even cycle formula_20 for which formula_21 has a perfect matching, there are an odd number of edges directed along C (and thus an odd number with the opposite orientation). It was also shown that these graphs are exactly those that do not contain a subgraph homeomorphic to formula_22, as above.\n\nModulo 2, the permanent is the same as the determinant, as formula_23 It can also be computed modulo formula_24 in time formula_25 for formula_26. However, it is UP-hard to compute the permanent modulo any number that is not a power of 2. \n\nThere are various formulae given by for the computation modulo a prime .\nFirstly, there is one using symbolic calculations with partial derivatives.\n\nSecondly, for = 3 there is the following formula for an nxn-matrix formula_19, involving the matrix's principal\nminors (): \nwhere formula_29 is the submatrix of formula_19 induced by the rows and columns of formula_19\nindexed by formula_32, and formula_33 is the complement of formula_32 in formula_35, while the determinant of the empty submatrix is defined to be 1.\n\n(Actually the above expansion can be generalized in an arbitrary characteristic p as the following pair of dual identities:\n\nformula_36\n\nformula_37\n\nwhere in both formulas the sum is taken over all the (p-1)-tuples formula_38 that are partitions of the set formula_35 into p-1 subsets, some of them possibly empty.\n\nThe former formula possesses an analog for the hafnian of a symmetric formula_19 and an odd p:\n\nformula_41\nwith the sum taken over the same set of indexes. Moreover, in characteristic zero a similar convolution sum expression involving both the permanent and the determinant yields the Hamiltonian cycle polynomial (defined as formula_42 where formula_43 is the set of n-permutations having only one cycle):\nformula_44 . In characteristic 2 the latter equality turns into formula_45 what therefore provides an opportunity to polynomial-time calculate the Hamiltonian cycle polynomial of any unitary formula_46 (i.e. such that formula_47 where formula_48 is the identity nxn-matrix), because each minor of such a matrix coincides with its algebraic complement: formula_49 where formula_50 is the identity nxn-matrix with the entry of indexes 1,1 replaced by 0. Moreover, it may, in turn, be further generalized for a unitary nxn-matrix formula_46 as formula_52 where formula_53 is a subset of {1...,n}, formula_54 is the identity nxn-matrix with the entries of indexes k,k replaced by 0 for all k beloning to formula_53, and we define formula_56 where formula_57 is the set of n-permutations whose each cycle contains at least one element of formula_53.)\n\nThis formula implies the following identities over fields of characteristic 3:\n\nfor any invertible formula_19\nfor any unitary formula_46 , i.e. a square matrix formula_46 such that formula_63 where formula_48 is the identity matrix of the corresponding size,\nwhere formula_66 is the matrix whose entries are the cubes of the corresponding entries of formula_46.\n\nIt was also shown () that, if we define a square matrix formula_19 as k-semi-unitary when formula_69 = ,\nthe permanent of a 1-semi-unitary matrix is computable in polynomial time over fields of characteristic 3, while for > 1 \nthe problem becomes #3-P-complete. (A parallel theory concerns the Hamiltonian cycle polynomial in characteristic 2: while computing it on the unitary matrices is polynomial-time feasible, the problem is #2-P-complete for the k-semi-unitary ones for any k > 0). The latter result was essentially extended in 2017 () and it was proven that in characteristic 3 there is a simple formula relating the permanents of a square matrix and its partial inverse (for formula_70 and formula_71 being square, formula_70 being invertible):\n\nformula_73\n\nand it allows to polynomial-time reduce the computation of the permanent of an nxn-matrix with a subset of k or k-1 rows expressible as linear combinations of another (disjoint) subset of k rows to the computation of the permanent of an (n-k)x(n-k)- or (n-k+1)x(n-k+1)-matrix correspondingly, hence having introduced a compression operator (analogical to the Gaussian modification applied for calculating the determinant) that \"preserves\" the permanent in characteristic 3. (Analogically, it would be worth noting that the Hamiltonian cycle polynomial in characteristic 2 does possess its invariant matrix compressions as well, taking into account the fact that ham(A) = 0 for any nxn-matrix A having three equal rows or, if n > 2, a pair of indexes i,j such that its i-th and j-th rows are identical and its i-th and j-th columns are identical too.) The closure of that operator defined as the limit of its sequential application together with the transpose transformation (utilized each time the operator leaves the matrix intact) is also an operator mapping, when applied to classes of matrices, one class to another. While the compression operator maps the class of 1-semi-unitary matrices into itself and the classes of unitary and 2-semi-unitary ones, the compression-closure of the 1-semi-unitary class (as well as the class of matrices received from unitary ones through replacing one row by an arbitrary row vector — the permanent of such a matrix is, via the Laplace expansion, the sum of the permanents of 1-semi-unitary matrices and, accordingly, polynomial-time computable) is yet unknown and tensely related to the general problem of the permanent's computational complexity in characteristic 3 and the chief question of P versus NP: as it was shown in (), if such a compression-closure is the set of all square matrices over a field of characteristic 3 or, at least, contains a matrix class the permanent's computation on is #3-P-complete (like the class of 2-semi-unitary matrices) then the permanent is computable in polynomial time in this characteristic.\n\nBesides, the problem of finding and classifying any possible analogs of the permanent-preserving compressions existing in characteristic 3 for other prime characteristics was formulated (), while giving the following identity for an nxn matrix formula_19 and two n-vectors (having all their entries from the set {0...,p-1}) formula_75 and formula_76 such that formula_77, valid in an arbitrary prime characteristic p:\n\nformula_78\n\nwhere for an nxm-matrix formula_79, an n-vector formula_80 and an m-vector formula_81, both vectors having all their entries from the set {0...,p-1}, formula_82 denotes the matrix received from formula_79 via repeating formula_84 times its i-th row for i = 1...,n and formula_85 times its j-th column for j = 1...,m (if some row's or column's multiplicity equals zero it would mean that the row or column was removed, and thus this notion is a generalization of the notion of submatrix), and formula_86 denotes the n-vector all whose entries equal unity. This identity is an exact analog of the classical formula expressing a matrix's minor through a minor of its inverse and hence demonstrates (once more) a kind of duality between the determinant and the permanent as relative immanants. \n(Actually its own analogue for the hafnian of a symmetric formula_19 and an odd prime p is \nformula_88 ).\n\nAnd, as an even wider generalization for the partial inverse case in a prime characteristic p, for formula_70, formula_71 being square, formula_70 being invertible and of size formula_92xformula_92, and formula_77, there holds also the identity\n\nformula_95\n\nwhere the common row/column multiplicity vectors formula_75 and formula_76 for the matrix formula_19 generate the corresponding row/column multiplicity vectors formula_99 and formula_100, s,t = 1,2, for its blocks (the same concerns formula_19's partial inverse in the equality's right side).\n\nWhen the entries of \"A\" are nonnegative, the permanent can be computed approximately in probabilistic polynomial time, up to an error of ε\"M\", where \"M\" is the value of the permanent and ε > 0 is arbitrary. In other words, there exists a fully polynomial-time randomized approximation scheme (FPRAS) ().\n\nThe most difficult step in the computation is the construction of an algorithm to sample almost uniformly from the set of all perfect matchings in a given bipartite graph: in other words, a fully polynomial almost uniform sampler (FPAUS). This can be done using a Markov chain Monte Carlo algorithm that uses a Metropolis rule to define and run a Markov chain whose distribution is close to uniform, and whose mixing time is polynomial.\n\nIt is possible to approximately count the number of perfect matchings in a graph via the self-reducibility of the permanent, by using the FPAUS in combination with a well-known reduction from sampling to counting due to . Let formula_102 denote the number of perfect matchings in formula_103. Roughly, for any particular edge formula_104 in formula_103, by sampling many matchings in formula_103 and counting how many of them are matchings in formula_107, one can obtain an estimate of the ratio formula_108. The number formula_102 is then formula_110, where formula_111 can be approximated by applying the same method recursively.\n\nAnother class of matrices for which the permanent can be computed approximately, is the set of positive-semidefinite matrices (the complexity-theoretic problem of approximating the permanent of such matrices to within a multiplicative error is considered open). The corresponding randomized algorithm is based on the model of boson sampling and it uses the tools proper to quantum optics, to represent the permanent of positive-semidefinite matrices as the expected value of a specific random variable. The latter is then approximated by its sample mean. This algorithm, for a certain set of positive-semidefinite matrices, approximates their permanent in polynomial time up to an additive error, which is more reliable than that of the standard classical polynomial-time algorithm by Gurvits.\n\n"}
{"id": "7008701", "url": "https://en.wikipedia.org/wiki?curid=7008701", "title": "Decrement table", "text": "Decrement table\n\nDecrement tables, also called life table methods, are used to calculate the probability of certain events.\n\nLife table methods are often used to study birth control effectiveness. In this role, they are an alternative to the Pearl Index.\n\nAs used in birth control studies, a decrement table calculates a separate effectiveness rate for each month of the study, as well as for a standard period of time (usually 12 months). Use of life table methods eliminates time-related biases (i.e. the most fertile couples getting pregnant and dropping out of the study early, and couples becoming more skilled at using the method as time goes on), and in this way is superior to the Pearl Index.\n\nTwo kinds of decrement tables are used to evaluate birth control methods. Multiple-decrement (or competing) tables report net effectiveness rates. These are useful for comparing competing reasons for couples dropping out of a study. Single-decrement (or noncompeting) tables report gross effectiveness rates, which can be used to accurately compare one study to another.\n\n"}
{"id": "47058944", "url": "https://en.wikipedia.org/wiki?curid=47058944", "title": "Desuspension", "text": "Desuspension\n\nIn topology, a field within mathematics, desuspension is an operation inverse to suspension.\n\nIn general, given an \"n\"-dimensional space formula_1, the suspension formula_2 has dimension \"n\" + 1. Thus, the operation of suspension creates a way of moving up in dimension. In the 1950s, to define a way of moving down, mathematicians introduced an inverse operation formula_3, called desuspension. Therefore, given an \"n\"-dimensional space formula_1, the desuspension formula_5 has dimension \"n\" – 1.\n\nNote that in general formula_6.\n\nThe reasons to introduce desuspension:\n\n\n"}
{"id": "21871152", "url": "https://en.wikipedia.org/wiki?curid=21871152", "title": "Dyson's transform", "text": "Dyson's transform\n\nDyson's transform is a fundamental technique in additive number theory. It was developed by Freeman Dyson as part of his proof of Mann's theorem, is used to prove such fundamental results of Additive Number Theory as the Cauchy-Davenport theorem, and was used by Olivier Ramaré in his work on the Goldbach conjecture that proved that every even integer is the sum of at most 6 primes. The term \"Dyson's transform\" for this technique is used by Ramaré. Halberstam and Roth call it the τ-transformation.\n\nThis formulation of the transform is from Ramaré. Let \"A\" be a sequence of natural numbers, and \"x\" be any real number. Write \"A\"(\"x\") for the number of elements of \"A\" which lie in [1, \"x\"]. Suppose formula_1 and formula_2 are two sequences of natural numbers. We write \"A\" + \"B\" for the sumset, that is, the set of all elements \"a\" + \"b\" where \"a\" is in \"A\" and \"b\" is in B; and similarly \"A\" − \"B\" for the set of differences \"a\" − \"b\". For any element \"e\" in \"A\", Dyson's transform consists in forming the sequences formula_3 and formula_4. The transformed sequences have the properties:\n\n"}
{"id": "1327120", "url": "https://en.wikipedia.org/wiki?curid=1327120", "title": "Dénes Kőnig", "text": "Dénes Kőnig\n\nDénes Kőnig (September 21, 1884 – October 19, 1944) was a Hungarian mathematician of Jewish heritage who worked in and wrote the first textbook on the field of graph theory.\n\nKőnig was born in Budapest, the son of mathematician Gyula Kőnig. In 1907, he received his doctorate at, and joined the faculty of the Royal Joseph University in Budapest (today Budapest University of Technology and Economics). His classes were visited by Paul Erdős, who, as a first year student, solved one of his problems. Kőnig became a full professor there in 1935. To honor his fathers' death in 1913, Kőnig and his brother György created the Gyula Kőnig prize in 1918. This prize was meant to be an endowment for young mathematicians, however was later devaluated. But the prize remained as a medal of high scientific recognition. In 1899, he published his first work while still attending High School in a journal \"Matematikai és Fizikai Lapok\". After his graduation in 1902, he won first place in a mathematical competition \"Eötvös Loránd\". Shortly after he wrote the first of two book collections \"Matematikai Mulatságok\" (Mathematical Entertainments). He spent four semesters at the university in Budapest and his last five in Göttingen, during which he studied under the famous mathematicians József Kürschák and Hermann Minkowski. He then received his doctorate in 1907 due to his dissertation in geometry, that same year he began working for the Technische Hochschule in Budapest and remained a part of the faculty till his death in 1944. At first he started as an assistant in problem sessions, in 1910 he was promoted to \"oberassistant\", and then promoted to \"Privatdocent\" in 1911 teaching nomography, analysis situs (later to be known as topology), set theory, real numbers and functions, and graph theory (the name \"graph theory\" didn't appear in the university catalogue until 1927). During this time he would be a guest speaker giving mathematics lecture for architecture and chemistry students, in 1920 these lectures made their way into book form. at the Technische Hochschule.\n\nFrom 1915 to 1942 he was on a committee to judge school contests in mathematics, collecting problems for these contests, and organizing them. Then in 1933 he was elected as secretary of the society and in 1942 he became the chairman of this committee. He then decided to make edits in the society's journal during his time on the committee till his death. Kőnig's activities and lectures played a vital role in the growth of graph theoretical work of: László Egyed, Paul Erdős, Tibor Gallai, György Hajós, József Kraus, Tibor Szele, Pál Turán, Endre Vázsonyi, and many others. He then went on to write the first book on graph theory \"Theorie der endlichen und unendlichen Graphen\" in 1936. This marked the beginning of graph theory as its own branch of mathematics. Then in 1958, Claude Berge wrote the second book on graph theory, \"Théorie des Graphes et ses applications\", following Kőnig.\n\nAfter the occupation of Hungary by the Nazis, he worked to help persecuted mathematicians. On October 15, 1944 the National Socialist Arrow Cross Party took over the country. Days later on October 19, 1944 he committed suicide to evade persecution from the Nazis being a Hungarian Jew.\n\n\n\n"}
{"id": "8211999", "url": "https://en.wikipedia.org/wiki?curid=8211999", "title": "Eilenberg–Ganea conjecture", "text": "Eilenberg–Ganea conjecture\n\nThe Eilenberg–Ganea conjecture is a claim in algebraic topology. It was formulated by Samuel Eilenberg and Tudor Ganea in 1957, in a short, but influential paper. It states that if a group \"G\" has cohomological dimension 2, then it has a 2-dimensional Eilenberg–MacLane space formula_1. For \"n\" different from 2, a group \"G\" of cohomological dimension \"n\" has an \"n\"-dimensional Eilenberg–MacLane space. It is also known that a group of cohomological dimension 2 has a 3-dimensional Eilenberg−MacLane space.\n\nIn 1997, Mladen Bestvina and Noel Brady constructed a group \"G\" so that either \"G\" is a counterexample to the Eilenberg–Ganea conjecture, or there must be a counterexample to the Whitehead conjecture; in other words, not both conjectures can be true. \n\n"}
{"id": "28192039", "url": "https://en.wikipedia.org/wiki?curid=28192039", "title": "Fabius function", "text": "Fabius function\n\nIn mathematics, the Fabius function is an example of an infinitely differentiable function that is nowhere analytic, found by . It was also written down as the Fourier transform of\n\nby .\n\nThe Fabius function is defined on the unit interval, and is given by the cumulative distribution function of\n\nwhere the are independent uniformly distributed random variables on the unit interval.\n\nThis function satisfies the initial condition formula_3, the symmetry condition formula_4 for formula_5 and the functional differential equation formula_6 for formula_7 It follows that formula_8 is monotone increasing for formula_5 with formula_10 and formula_11\nThere is a unique extension of to the real numbers that satisfies the same equation. This extension can be defined by for , for , and for with a positive integer. The sequence of intervals within which this function is positive or negative follows the same pattern as the Thue–Morse sequence.\n\nThe Fabius function is constant zero for all negative arguments, and takes rational values at non-negative dyadic rationals. Those values are given by the following formula:\nwhere formula_13 is the sum of digits of \"n\" in base-2. Note that formula_14 is just the signed Thue–Morse sequence, satisfying the recurrence formula_15\n\n\n"}
{"id": "63262", "url": "https://en.wikipedia.org/wiki?curid=63262", "title": "Financial economics", "text": "Financial economics\n\nFinancial economics is the branch of economics characterized by a \"concentration on monetary activities\", in which \"money of one type or another is likely to appear on \"both sides\" of a trade\". Its concern is thus the interrelation of financial variables, such as prices, interest rates and shares, as opposed to those concerning the real economy. It has two main areas of focus: asset pricing (or \"investment theory\") and corporate finance; the first being the perspective of providers of capital, i.e. investors, and the second of users of capital.\n\nThe subject is concerned with \"the allocation and deployment of economic resources, both spatially and across time, in an uncertain environment\". It therefore centers on decision making under uncertainty in the context of the financial markets, and the resultant economic and financial models and principles, and is concerned with deriving testable or policy implications from acceptable assumptions. It is built on the foundations of microeconomics and decision theory.\n\nFinancial econometrics is the branch of financial economics that uses econometric techniques to parameterise these relationships. Mathematical finance is related in that it will derive and extend the mathematical or numerical models suggested by financial economics. Note though that the emphasis there is mathematical consistency, as opposed to compatibility with economic theory. Financial economics has a primarily microeconomic focus, whereas monetary economics is primarily macroeconomic in nature.\n\nFinancial economics is usually taught at the postgraduate level; see Master of Financial Economics. Recently, specialist undergraduate degrees are offered in the discipline.\n\nThis article provides an overview and survey of the field: for derivations and more technical discussion, see the specific articles linked.\n\nAs above, the discipline essentially explores how rational investors would apply decision theory to the problem of investment. The subject is thus built on the foundations of microeconomics and decision theory, and derives several key results for the application of decision making under uncertainty to the financial markets.\n\nUnderlying all of financial economics are the concepts of present value and expectation.\n\nCalculating their present value allows the decision maker to aggregate the cashflows (or other returns) to be produced by the asset in the future, to a single value at the date in question, and to thus more readily compare two opportunities; this concept is therefore the starting point for financial decision making. (Its history is correspondingly early: Richard Witt discusses compound interest in depth already in 1613, in his book \"Arithmeticall Questions\"; further developed by Johan de Witt and Edmond Halley.)\n\nAn immediate extension is to combine probabilities with present value, leading to the expected value criterion which sets asset value as a function of the sizes of the expected payouts and the probabilities of their occurrence. (These ideas originate with Blaise Pascal and Pierre de Fermat.)\n\nThis decision method, however, fails to consider risk aversion (\"as any student of finance knows\"). In other words, since individuals receive greater utility from an extra dollar when they are poor and less utility when comparatively rich, the approach is to therefore \"adjust\" the weight assigned to the various outcomes (\"states\") correspondingly. (Some investors may in fact be risk seeking as opposed to risk averse, but the same logic would apply).\n\nChoice under uncertainty here may then be characterized as the maximization of expected utility. More formally, the resulting expected utility hypothesis states that, if certain axioms are satisfied, the subjective value associated with a gamble by an individual is \"that individual\"s statistical expectation of the valuations of the outcomes of that gamble.\n\nThe impetus for these ideas arise from various inconsistencies observed under the expected value framework, such as the St. Petersburg paradox; see also Ellsberg paradox. (The development here is originally due to Daniel Bernoulli, and later formalized by John von Neumann and Oskar Morgenstern.)\n\nThe concepts of arbitrage-free, \"rational\", pricing and equilibrium are then coupled with the above to derive \"classical\" (or \"neo-classical\") financial economics.\n\nRational pricing is the assumption that asset prices (and hence asset pricing models) will reflect the arbitrage-free price of the asset, as any deviation from this price will be \"arbitraged away\". This assumption is useful in pricing fixed income securities, particularly bonds, and is fundamental to the pricing of derivative instruments.\n\nEconomic equilibrium is, in general, a state in which economic forces such as supply and demand are balanced, and, in the absence of external influences these equilibrium values of economic variables will not change. General equilibrium deals with the behavior of supply, demand, and prices in a whole economy with several or many interacting markets, by seeking to prove that a set of prices exists that will result in an overall equilibrium. (This is in contrast to partial equilibrium, which only analyzes single markets.)\n\nThe two concepts are linked as follows: where market prices do not allow for profitable arbitrage, i.e. they comprise an arbitrage-free market, then these prices are also said to constitute an \"arbitrage equilibrium\". Intuitively, this may be seen by considering that where an arbitrage opportunity does exist, then prices can be expected to change, and are therefore not in equilibrium. An arbitrage equilibrium is thus a precondition for a general economic equilibrium.\n\nThe immediate, and formal, extension of this idea, the fundamental theorem of asset pricing, shows that where markets are as described —and are additionally (implicitly and correspondingly) complete—one may then make financial decisions by constructing a risk neutral probability measure corresponding to the market.\n\n\"Complete\" here means that there is a price for every asset in every possible state of the world, and that the complete set of possible bets on future states-of-the-world can therefore be constructed with existing assets (assuming no friction), essentially solving simultaneously for \"n\" (risk-neutral) probabilities, given \"n\" prices. The formal derivation will proceed by arbitrage arguments. For a worked example see Rational pricing#Risk neutral valuation, where, in a simplified environment, the economy has only two possible states—up and down—and where \"p\" and (1−\"p\") are the two corresponding (i.e. implied) probabilities, and in turn, the derived distribution, or \"measure\".\n\nWith this measure in place, the expected, i.e. required, return of any security (or portfolio) will then equal the riskless return, plus an \"adjustment for risk\", i.e. a security-specific risk premium, compensating for the extent to which its cashflows are unpredictable. All pricing models are then essentially variants of this, given specific assumptions and/or conditions. This approach is consistent with the above, but with the expectation based on \"the market\" (i.e. arbitrage-free, and, per the theorem, therefore in equilibrium) as opposed to individual preferences.\n\nThus, continuing the example, to value a specific security, its forecasted cashflows in the up- and down-states are multiplied through by \"p\" and (1-\"p\") respectively, and are then discounted at the risk-free interest rate plus an appropriate premium. In general, this premium may be derived by the CAPM (or extensions) as will be seen under #Uncertainty.\n\nWith the above relationship established, the further specialized Arrow–Debreu model may be derived. This important result suggests that, under certain economic conditions, there must be a set of prices such that aggregate supplies will equal aggregate demands for every commodity in the economy. The analysis here is often undertaken assuming a \"representative agent\".\n\nThe Arrow–Debreu model applies to economies with maximally complete markets, in which there exists a market for every time period and forward prices for every commodity at all time periods. A direct extension, then, is the concept of a state price security (also called an Arrow–Debreu security), a contract that agrees to pay one unit of a numeraire (a currency or a commodity) if a particular state occurs (\"up\" and \"down\" in the simplified example above) at a particular time in the future and pays zero numeraire in all the other states. The price of this security is the state price of this particular state of the world.\n\nIn the above example, the state prices would equate to the present values of $p and $(1−p): i.e. what one would pay today, respectively, for the up- and down-state securities; the state price vector is the vector of state prices for all states.\nApplied to valuation, the price of the derivative today would simply be [up-state-price × up-state-payoff + down-state-price × down-state-payoff]; see below regarding the absence of any risk premium here. For a continuous random variable indicating a continuum of possible states, the value is found by integrating over the state price density; see Stochastic discount factor. These concepts are extended to martingale pricing and the related risk-neutral measure.\n\nState prices find immediate application as a conceptual tool (\"contingent claim analysis\"); but can also be applied to valuation problems. Given the pricing mechanism described, one can decompose the derivative value — true in fact for \"every security\" — as a linear combination of its state-prices; i.e. back-solve for the state-prices corresponding to observed derivative prices. These recovered state-prices can then be used for valuation of other instruments with exposure to the underlyer, or for other decision making relating to the underlyer itself. (Breeden and Litzenberger's work in 1978 established the use of state prices in financial economics.)\n\nApplying the preceding economic concepts, we may then derive various economic- and financial models and principles. As above, the two usual areas of focus are Asset Pricing and Corporate Finance, the first being the perspective of providers of capital, the second of users of capital. Here, and for (almost) all other financial economics models, the questions addressed are typically framed in terms of \"time, uncertainty, options, and information\", as will be seen below.\nApplying this framework, with the above concepts, leads to the required models. This derivation begins with the assumption of \"no uncertainty\" and is then expanded to incorporate the other considerations. (This division sometimes denoted \"deterministic\" and \"random\", or \"stochastic\".)\n\nThe starting point here is “Investment under certainty\". The Fisher separation theorem, asserts that the objective of a corporation will be the maximization of its present value, regardless of the preferences of its shareholders. Related is the Modigliani–Miller theorem, which shows that, under certain conditions, the value of a firm is unaffected by how that firm is financed, and depends neither on its dividend policy nor its decision to raise capital by issuing stock or selling debt. The proof here proceeds using arbitrage arguments, and acts as a benchmark for evaluating the effects of factors outside the model that do affect value.\n\nThe mechanism for determining (corporate) value is provided by \"The Theory of Investment Value\" (John Burr Williams), which proposes that the value of an asset should be calculated using \"evaluation by the rule of present worth\". Thus, for a common stock, the intrinsic, long-term worth is the present value of its future net cashflows, in the form of dividends. What remains to be determined is the appropriate discount rate. Later developments show that, \"rationally\", i.e. in the formal sense, the appropriate discount rate here will (should) depend on the asset's riskiness relative to the overall market, as opposed to its owners' preferences; see below. Net present value (NPV) is the direct extension of these ideas typically applied to Corporate Finance decisioning (introduced by Joel Dean in 1951). For other results, as well as specific models developed here, see the list of \"Equity valuation\" topics under Outline of finance#Discounted cash flow valuation.\n\nBond valuation, in that cashflows (coupons and return of principal) are deterministic, may proceed in the same fashion. An immediate extension, Arbitrage-free bond pricing, discounts each cashflow at the market derived rate — i.e. at each coupon's corresponding zero-rate — as opposed to an overall rate. Note that in many treatments bond valuation precedes equity valuation, under which cashflows (dividends) are not \"known\" \"per se\". Williams and onward allow for forecasting as to these — based on historic ratios or published policy — and cashflows are then treated as essentially deterministic; see below under #Corporate finance theory.\n\nThese \"certainty\" results are all commonly employed under corporate finance; uncertainty is the focus of \"asset pricing models\", as follows.\n\nFor \"choice under uncertainty\" the twin assumptions of rationality and market efficiency, as more closely defined, lead to modern portfolio theory (MPT) with its capital asset pricing model (CAPM)—an \"equilibrium-based\" result—and to the Black–Scholes–Merton theory (BSM; often, simply Black–Scholes) for option pricing—an \"arbitrage-free\" result. Note that the latter derivative prices are calculated such that they are arbitrage-free with respect to the more fundamental, equilibrium determined, securities prices; see asset pricing.\n\nBriefly, and intuitively—and consistent with #Arbitrage-free pricing and equilibrium above—the linkage is as follows. Given the ability to profit from private information, self-interested traders are motivated to acquire and act on their private information. In doing so, traders contribute to more and more \"correct\", i.e. \"efficient\", prices: the efficient-market hypothesis, or EMH (Eugene Fama, 1965). The EMH (implicitly) assumes that average expectations constitute an \"optimal forecast\", i.e. prices using all available information, are identical to the \"best guess of the future\": the assumption of rational expectations. The EMH does allow that when faced with new information, some investors may overreact and some may underreact, but what is required, however, is that investors' reactions follow a normal distribution—so that the net effect on market prices cannot be reliably exploited to make an abnormal profit.\nIn the competitive limit, then, market prices will reflect all available information and prices can only move in response to news; and this, of course, could be \"good\" or \"bad\", major or minor: the random walk hypothesis. Thus, if prices of financial assets are (broadly) efficient, then deviations from these (equilibrium) values could not last for long. (On Random walks in stock prices: Jules Regnault, 1863; Louis Bachelier, 1900; Maurice Kendall, 1953; Paul Cootner, 1964.)\n\nUnder these conditions investors can then be assumed to act rationally: their investment decision must be calculated or a loss is sure to follow; correspondingly, where an arbitrage opportunity presents itself, then arbitrageurs will exploit it, reinforcing this equilibrium.\nHere, as under the certainty-case above, the specific assumption as to pricing is that prices are calculated as the present value of expected future dividends, as based on currently available information.\nWhat is required though is a theory for determining the appropriate discount rate, i.e. \"required return\", given this uncertainty: this is provided by the MPT and its CAPM. Relatedly, rationality—in the sense of arbitrage-exploitation—gives rise to Black–Scholes; option values here ultimately consistent with the CAPM.\n\nIn general, then, while portfolio theory studies how investors should balance risk and return when investing in many assets or securities, the CAPM is more focused, describing how, in equilibrium, markets set the prices of assets in relation to how risky they are.\nImportantly, this result will be independent of the investor's level of risk aversion, and / or assumed utility function, thus providing a readily determined discount rate for corporate finance decision makers as above, and for other investors.\nThe argument proceeds as follows: If one can construct an efficient frontier—i.e. each combination of assets offering the best possible expected level of return for its level of risk, see diagram—then mean-variance efficient portfolios can be formed simply as a combination of holdings of the risk-free asset and the \"market portfolio\" (the Mutual fund separation theorem), with the combinations here plotting as the capital market line, or CML. Then, given this CML, the required return on risky securities will be independent of the investor's utility function, and solely determined by their covariance (\"beta\") with aggregate, i.e. market, risk. This is because investors here can then maximize utility through leverage as opposed to pricing; see CML diagram. As can be seen in the formula aside, this result is consistent with the preceding, equaling the riskless return plus an adjustment for risk. (The efficient frontier was introduced by Harry Markowitz in 1952. The CAPM was derived by Jack Treynor (1961, 1962), William F. Sharpe (1964), John Lintner (1965) and Jan Mossin (1966) independently.)\n\nBlack–Scholes provides a mathematical model of a financial market containing derivative instruments, and the resultant formula for the price of European-styled options.\nThe model is expressed as the Black–Scholes equation, a partial differential equation describing the changing price of the option over time; it is derived assuming log-normal, geometric Brownian motion (see Brownian model of financial markets).\nThe key financial insight behind the model is that one can perfectly hedge the option by buying and selling the underlying asset in just the right way and consequently \"eliminate risk\", absenting the risk adjustment from the pricing (formula_1, the value, or price, of the option, grows at formula_2, the risk-free rate; see ).\nThis hedge, in turn, implies that there is only one right price—in an arbitrage-free sense—for the option. And this price is returned by the Black–Scholes option pricing formula. (The formula, and hence the price, is consistent with the equation, as the formula is the solution to the equation.)\nSince the formula is without reference to the share's expected return, Black–Scholes entails (assumes) risk neutrality, consistent with the \"elimination of risk\" here. Relatedly, therefore, the pricing formula may also be derived directly via risk neutral expectation.\n(BSM - two seminal 1973 papers\n\n- is consistent with \"previous versions of the formula\" of Louis Bachelier and Edward O. Thorp; although these were more \"actuarial\" in flavor, and had not established risk-neutral discounting. See also Paul Samuelson (1965). Vinzenz Bronzin (1908) produced very early results.)\n\nAs mentioned, it can be shown that the two models are consistent; then, as is to be expected, \"classical\" financial economics is thus unified. Here, the Black Scholes equation may alternatively be derived from the CAPM, and the price obtained from the Black–Scholes model is thus consistent with the expected return from the CAPM. The Black–Scholes theory, although built on Arbitrage-free pricing, is therefore consistent with the equilibrium based capital asset pricing. Both models, in turn, are ultimately consistent with the Arrow–Debreu theory, and may be derived via state-pricing, further explaining, and if required demonstrating, this unity.\n\nMore recent work further generalizes and / or extends these models. As regards asset pricing, developments in equilibrium-based pricing are discussed under \"Portfolio theory\" below, while \"Derivative pricing\" relates to risk-neutral, i.e. arbitrage-free, pricing. As regards the use of capital, \"Corporate finance theory\" relates, mainly, to the application of these models.\n\nThe majority of developments here relate to required return, i.e. pricing, extending the basic CAPM. Multi-factor models such as the Fama–French three-factor model and the Carhart four-factor model, propose factors other than market return as relevant in pricing. The intertemporal CAPM and consumption-based CAPM similarly extend the model. With intertemporal portfolio choice, the investor now repeatedly optimizes her portfolio; while the inclusion of consumption (in the economic sense) then incorporates all sources of wealth, and not just market-based investments, into the investor's calculation of required return.\n\nWhereas the above extend the CAPM, the single-index model is a more simple model. It assumes, only, a correlation between security and market returns, without (numerous) other economic assumptions. It is useful in that it simplifies the estimation of correlation between securities, significantly reducing the inputs for building the correlation matrix required for portfolio optimization. The arbitrage pricing theory (APT; Stephen Ross, 1976) similarly differs as regards its assumptions. APT \"gives up the notion that there is one right portfolio for everyone in the world, and ...replaces it with an explanatory model of what drives asset returns.\" It returns the required (expected) return of a financial asset as a linear function of various macro-economic factors, and assumes that arbitrage should bring incorrectly priced assets back into line.\n\nAs regards portfolio optimization, the Black–Litterman model departs from the original Markowitz approach of constructing portfolios via an efficient frontier. Black–Litterman instead starts with an equilibrium assumption, and is then modified to take into account the 'views' (i.e., the specific opinions about asset returns) of the investor in question to arrive at a bespoke asset allocation. Where factors additional to volatility are considered (kurtosis, skew...) then multiple-criteria decision analysis can be applied; here deriving a Pareto efficient portfolio. The universal portfolio algorithm (Thomas M. Cover) applies machine learning to asset selection, learning adaptively from historical data. Behavioral portfolio theory recognizes that investors have varied aims and create an investment portfolio that meets a broad range of goals. Copulas have lately been applied here. See for other techniques and / or objectives.\n\nAs regards derivative pricing, the binomial options pricing model provides a discretized version of Black–Scholes, useful for the valuation of American styled options. Discretized models of this type are built—at least implicitly—using state-prices (as above); relatedly, a large number of researchers have used options to extract state-prices for a variety of other applications in financial economics. For path dependent derivatives, Monte Carlo methods for option pricing are employed; here the modelling is in continuous time, but similarly uses risk neutral expected value. Various other numeric techniques have also been developed. The theoretical framework too has been extended such that martingale pricing is now the standard approach. Developments relating to complexities in return and / or volatility are discussed below.\n\nDrawing on these techniques, derivative models for various other underlyings and applications have also been developed, all based off the same logic (using \"contingent claim analysis\"). Real options valuation allows that option holders can influence the option's underlying; models for employee stock option valuation explicitly assume non-rationality on the part of option holders; Credit derivatives allow that payment obligations and / or delivery requirements might not be honored. Exotic derivatives are now routinely valued. Multi-asset underlyers are handled via simulation or copula based analysis.\n\nSimilarly, beginning with Oldrich Vasicek (1977), various short rate models, as well as the HJM and BGM forward rate-based techniques, allow for an extension of these techniques to fixed income- and interest rate derivatives. (The Vasicek and CIR models are equilibrium-based, while Ho–Lee and subsequent models are based on arbitrage-free pricing.) Bond valuation is relatedly extended: the Stochastic calculus approach, employing these methods, allows for rates that are \"random\" (while returning a price that is arbitrage free, as above); lattice models for hybrid securities allow for non-deterministic cashflows (and stochastic rates).\n\nAs above, (OTC) derivative pricing has relied on the BSM risk neutral pricing framework, under the assumptions of funding at the risk free rate and the ability to perfectly replicate cashflows so as to fully hedge. This, in turn, is built on the assumption of a credit-risk-free environment. Post the financial crisis of 2008, therefore, issues such as counterparty credit risk, funding costs and costs of capital are additionally considered, and a Credit Valuation Adjustment, or CVA—and potentially other \"valuation adjustments\", collectively xVA—is generally added to the risk-neutral derivative value.\n\nA related, and perhaps more fundamental change, is that discounting is now on the Overnight Index Swap (OIS) curve, as opposed to LIBOR as used previously. This is because post-crisis, OIS is considered a better proxy for the \"risk-free rate\". (Also, practically, the interest paid on cash collateral is usually the overnight rate; OIS discounting is then, sometimes, referred to as \"CSA discounting\".) Swap pricing is further modified: previously, swaps were valued off a single \"self discounting\" interest rate curve; whereas post crisis, to accommodate OIS discounting, valuation is now under a \"multi-curve\" framework where \"forecast curves\" are constructed for \"each\" floating-leg LIBOR tenor, with discounting on a common OIS curve; see .\n\nCorporate finance theory has also been extended: mirroring the above developments, asset-valuation and decisioning no longer need assume \"certainty\". As discussed, Monte Carlo methods in finance, introduced by David B. Hertz in 1964, allow financial analysts to construct \"stochastic\" or probabilistic corporate finance models, as opposed to the traditional static and deterministic models; see . Relatedly, Real Options theory allows for owner—i.e. managerial—actions that impact underlying value: by incorporating option pricing logic, these actions are then applied to a distribution of future outcomes, changing with time, which then determine the \"project's\" valuation today.\n\nMore traditionally, decision trees—which are complementary—have been used to evaluate projects, by incorporating in the valuation (all) possible events (or states) and consequent management decisions; the correct discount rate here reflecting each point's \"non-diversifiable risk looking forward.\" (This technique predates the use of real options in corporate finance; it is borrowed from operations research, and is not a \"financial economics development\" \"per se\".)\n\nRelated to this, is the treatment of forecasted cashflows in equity valuation. In many cases, following Williams above, the average (or most likely) cash-flows were discounted, as opposed to a more correct state-by-state treatment under uncertainty; see comments under Financial modeling § Accounting. In more modern treatments, then, it is the \"expected\" cashflows (in the mathematical sense) combined into an overall value per forecast period which are discounted. And using the CAPM—or extensions—the discounting here is at the risk-free rate plus a premium linked to the uncertainty of the entity or project cash flows.\n\nOther developments here include agency theory, which analyses the difficulties in motivating corporate management (the \"agent\") to act in the best interests of shareholders (the \"principal\"), rather than in their own interests. Clean surplus accounting and the related residual income valuation provide a model that returns price as a function of earnings, expected returns, and change in book value, as opposed to dividends. This approach, to some extent, arises due to the implicit contradiction of seeing value as a function of dividends, while also holding that dividend policy cannot influence value per Modigliani and Miller's \"Irrelevance principle\"; see .\n\nThe typical application of real options is to capital budgeting type problems as described. However, they are also applied to questions of capital structure and dividend policy, and to the related design of corporate securities; and since stockholder and bondholders have different objective functions, in the analysis of the related agency problems. In all of these cases, state-prices can provide the market-implied information relating to the corporate, as above, which is then applied to the analysis. For example, convertible bonds can (must) be priced consistent with the state-prices of the corporate's equity.\n\nAs above, there is a very close link between (i) the random walk hypothesis, with the associated expectation that price changes should follow a normal distribution, on the one hand, and (ii) market efficiency and rational expectations, on the other. Note, however, that (wide) departures from these are commonly observed, and there are thus, respectively, two main sets of challenges.\n\nAs discussed, the assumptions that market prices follow a random walk and / or that asset returns are normally distributed are fundamental. Empirical evidence, however, suggests that these assumptions may not hold (see Kurtosis risk, Skewness risk, Long tail) and that in practice, traders, analysts and risk managers frequently modify the \"standard models\" (see Model risk). In fact, Benoît Mandelbrot had discovered already in the 1960s that changes in financial prices do not follow a Gaussian distribution, the basis for much option pricing theory, although this observation was slow to find its way into mainstream financial economics.\n\nFinancial models with long-tailed distributions and volatility clustering have been introduced to overcome problems with the realism of the above \"classical\" financial models; while jump diffusion models allow for (option) pricing incorporating \"jumps\" in the spot price. \nRisk managers, similarly, complement (or substitute) the standard value at risk models with historical simulations, mixture models, principal component analysis, extreme value theory, as well as models for volatility clustering. \nFor further discussion see , and .\nPortfolio managers, likewise, have modified their optimization criteria and algorithms; see #Portfolio theory above. \n\nClosely related is the volatility smile, where implied volatility—the volatility corresponding to the BSM price—is observed to \"differ\" as a function of strike price (i.e. moneyness), true only if the price-change distribution is non-normal, unlike that assumed by BSM. The term structure of volatility describes how (implied) volatility differs for related options with different maturities. An implied volatility surface is then a three-dimensional surface plot of volatility smile and term structure. These empirical phenomena negate the assumption of constant volatility—and log-normality—upon which Black–Scholes is built; see .\n\nIn consequence traders (and risk managers) use \"smile-consistent\" models, firstly, when valuing derivatives not directly mapped to the surface, facilitating the pricing of other, i.e. non-quoted, strike/maturity combinations, or of non-European derivatives, and generally for hedging purposes. The two main approaches are local volatility and stochastic volatility. The first returns the volatility which is “local” to each spot-time point of the finite difference- or simulation-based valuation — i.e. as opposed to implied volatility, which holds overall. In this way calculated prices — and numeric structures — are market-consistent in an arbitrage-free sense. The second approach assumes that the volatility of the underlying price is a stochastic process rather than a constant. Models here are first \"calibrated\" to observed prices, and are then applied to the valuation in question; the most common are Heston, SABR and CEV. This approach addresses certain problems identified with hedging under local volatility.\n\nRelated to local volatility are the lattice-based implied-binomial and -trinomial trees — essentially a discretization of the approach — which are similarly used for pricing; these are built on state-prices recovered from the surface. Edgeworth binomial trees allow for a specified (i.e. non-Gaussian) skew and kurtosis in the spot price; priced here, options with differing strikes will return differing implied volatilities, and the tree can be calibrated to the smile as required. Similarly purposed closed-form models have also been developed. \n\nAs above, additional to log-normality in returns, BSM—and, typically, other derivative models—assume(d) the ability to perfectly replicate cashflows so as to fully hedge, and hence to discount at the risk-free rate. This, in turn, is built on the assumption of a credit-risk-free environment. Post crisis, then, various x-value adjustments are made to the risk-neutral derivative value. Note that these are \"additional\" to any smile or surface effect: this is valid as the surface is built on price data relating to fully collateralized positions, and there is therefore no \"double counting\" of credit risk (etc.) when including xVA. (Also, were this not the case, then each counterparty would have its own surface...)\n\nAs seen, a common assumption is that financial decision makers act rationally; see Homo economicus. Recently, however, researchers in experimental economics and experimental finance have challenged this assumption empirically. These assumptions are also challenged theoretically, by behavioral finance, a discipline primarily concerned with the limits to rationality of economic agents.\n\nConsistent with, and complementary to these findings, various persistent market anomalies have been documented, these being price and/or return distortions—e.g. size premiums—which appear to contradict the efficient-market hypothesis; calendar effects are the best known group here. Related to these are various of the economic puzzles, concerning phenomena similarly contradicting the theory. The \"equity premium puzzle\", as one example, arises in that the difference between the observed returns on stocks as compared to government bonds is consistently higher than the risk premium rational equity investors should demand, an \"abnormal return\". For further context see Random walk hypothesis § A non-random walk hypothesis, and sidebar for specific instances.\n\nMore generally, and particularly following the financial crisis of 2007–2010, financial economics and mathematical finance have been subjected to deeper criticism; notable here is Nassim Nicholas Taleb, who claims that the prices of financial assets cannot be characterized by the simple models currently in use, rendering much of current practice at best irrelevant, and, at worst, dangerously misleading; see Black swan theory, Taleb distribution. A topic of general interest studied in recent years has thus been financial crises, and the failure of financial economics to model these. (A related problem is systemic risk: where companies hold securities in each other then this interconnectedness may entail a \"valuation chain\"—and the performance of one company, or security, here will impact all, a phenomenon not easily modeled, regardless of whether the individual models are correct. See Systemic risk § Inadequacy of classic valuation models; Cascades in financial networks; Flight-to-quality.)\n\nAreas of research attempting to explain (or at least model) these phenomena, and crises, include noise trading, market microstructure, and Heterogeneous agent models. The latter is extended to agent-based computational economics, where price is treated as an emergent phenomenon, resulting from the interaction of the various market participants (agents). The noisy market hypothesis argues that prices can be influenced by speculators and momentum traders, as well as by insiders and institutions that often buy and sell stocks for reasons unrelated to fundamental value; see Noise (economic). The adaptive market hypothesis is an attempt to reconcile the efficient market hypothesis with behavioral economics, by applying the principles of evolution to financial interactions. An information cascade, alternatively, shows market participants engaging in the same acts as others (\"herd behavior\"), despite contradictions with their private information. Copula-based modelling has similarly been applied. See also Hyman Minsky's \"financial instability hypothesis\", as well as George Soros' approach, § Reflexivity, financial markets, and economic theory.\n\nOn the obverse, however, various studies have shown that despite these departures from efficiency, asset prices do typically exhibit a random walk and that one cannot therefore consistently outperform market averages (\"alpha\"). The practical implication, therefore, is that passive investing (e.g. via low-cost index funds) should, on average, serve better than any other active strategy. Burton Malkiel's \"A Random Walk Down Wall Street\"—first published in 1973, and in its 11th edition as of 2015—is a widely read popularization of these arguments. (See also John C. Bogle's \"Common Sense on Mutual Funds\"; but compare Warren Buffett's \"The Superinvestors of Graham-and-Doddsville\".) Note also that institutionally inherent \"limits to arbitrage\"—as opposed to factors directly contradictory to the theory—are sometimes proposed as an explanation for these departures from efficiency.\n\nFinancial economics\n\nAsset pricing\n\nCorporate finance\n"}
{"id": "6133005", "url": "https://en.wikipedia.org/wiki?curid=6133005", "title": "Generator matrix", "text": "Generator matrix\n\nIn coding theory, a generator matrix is a matrix whose rows form a basis for a linear code. The codewords are all of the linear combinations of the rows of this matrix, that is, the linear code is the row space of its generator matrix.\n\nIf G is a matrix, it generates the codewords of a linear code \"C\" by\n\nwhere w is a codeword of the linear code \"C\", and s is any input vector. Both w and s are assumed to be row vectors. A generator matrix for a linear formula_2-code has format formula_3, where \"n\" is the length of a codeword, \"k\" is the number of information bits (the dimension of \"C\" as a vector subspace), \"d\" is the minimum distance of the code, and \"q\" is size of the finite field, that is, the number of symbols in the alphabet (thus, \"q\" = 2 indicates a binary code, etc.). The number of redundant bits is denoted by formula_4.\n\nThe \"standard\" form for a generator matrix is,\nwhere formula_6 is the formula_7 identity matrix and P, a formula_8 matrix. When the generator matrix is in standard form, the code \"C\" is systematic in its first \"k\" coordinate positions.\n\nA generator matrix can be used to construct the parity check matrix for a code (and vice versa). If the generator matrix \"G\" is in standard form, formula_5, then the parity check matrix for \"C\" is \nwhere formula_11 is the transpose of the matrix formula_12. This is a consequence of the fact that a parity check matrix of formula_13 is a generator matrix of the dual code formula_14.\n\nIt may be noted that G is a formula_3 matrix, while H is a formula_16 matrix.\n\nCodes \"C\" and \"C\" are \"equivalent\" (denoted \"C\" ~ \"C\") if one code can be obtained from the other via the following two transformations:\n\n\nEquivalent codes have the same minimum distance.\n\nThe generator matrices of equivalent codes can be obtained from one another via the following elementary operations:\n\n\nThus, we can perform Gaussian Elimination on \"G\". Indeed, this allows us to assume that the generator matrix is in the standard form. More precisely, for any matrix \"G\" we can find a invertible matrix \"U\" such that formula_17, where \"G\" and formula_18 generate equivalent codes.\n\n\n\n"}
{"id": "49305019", "url": "https://en.wikipedia.org/wiki?curid=49305019", "title": "Genetic and Evolutionary Computation Conference", "text": "Genetic and Evolutionary Computation Conference\n\nThe Genetic and Evolutionary Computation Conference (GECCO) is the premier conference in the area of genetic and evolutionary computation. GECCO has been held every year since 1999, when it was first established as a recombination of the International Conference on Genetic Algorithms (ICGA) and the Annual Genetic Programming Conference (GP).\n\nGECCO presents the latest high-quality results in genetic and evolutionary computation. Topics of interest include: genetic algorithms, genetic programming, evolution strategies, evolutionary programming, estimation of distribution algorithms, memetic algorithms, hyper-heuristics, evolutionary robotics, evolvable hardware, artificial life, ant colony optimization algorithms, swarm intelligence, artificial immune systems, digital entertainment technologies, evolutionary art, evolutionary combinatorial optimization, metaheuristics, evolutionary multi-objective optimization, evolutionary machine learning, search-based software engineering, theory, real-world applications, and more.\n\nOther important conferences in the field are IEEE Congress on Evolutionary Computation (CEC), Parallel Problem Solving from Nature (PPSN) and EvoStar (a group name for four co-located conferences, EuroGP, EvoCOP, EvoMUSART, and EvoApplications).\n\nGECCO is the main annual conference of the Special Interest Group on Genetic and Evolutionary Computation (SIGEVO), which is a Special Interest Group (SIG) of the Association for Computing Machinery (ACM).\n\n"}
{"id": "4061643", "url": "https://en.wikipedia.org/wiki?curid=4061643", "title": "George Saitoti", "text": "George Saitoti\n\nGeorge Musengi Saitoti, E.G.H. (3 August 1945 – 10 June 2012) was a Kenyan politician, businessman and American- and British-trained economist, mathematician and development policy thinker.\n\nAs a mathematician, Saitoti served as Head of the Mathematics Department at the University of Nairobi, pioneered the founding of the African Mathematical Union and served as its Vice-President from 1976 to 1979.\n\nAs an economist, Saitoti served as the Executive Chairman of the World Bank and the International Monetary Fund (IMF) in 1990–91, and as President of the African Caribbean and Pacific (ACP) Group of States in 1999–2000, at the crucial phase of re-negotiating the new development partnership agreement to replace the expired Lomé Convention between the ACP bloc and the European Union (EU). His book \"The Challenges of Economic and Institutional Reforms in Africa\" influenced practical policy directions on an array of areas during the turbulent 1980s and 1990s.\n\nSaitoti joined politics as a nominated Member of Parliament and Minister for Finance in 1983, rising to become Kenya's longest-serving Vice-President, a proficient Minister for education, Internal Security and Provincial Administration and Foreign Affairs. Few recognise him as a \"reformist\", but his recommendations as the Chair of the KANU Review Committee, popularly known as the \"Saitoti Committee\" in 1990–91, opened KANU to internal changes and set the stage for the repeal of Section 2A and Kenya's return to pluralist democracy. Saitoti left KANU and joined the opposition, becoming a kingpin figure in the negotiations that led to the \"NARC Revolution\" in 2002. As Minister for Internal Security and Provincial Administration, Acting Minister for Foreign Affairs and key member of the National Security Advisory Committee (NSAC), he later worked closely with the national Ministry of Defence to see through the Operation Linda Nchi against the Al-Shabaab insurgent group. In addition, rival factions had for decades invoked the infamous Goldenberg fraud to knock Saitoti out of politics, but the legal courts cleared him of the scandal in July 2006. Saitoti's dual heritage as a Maasai with Kikuyu family members predisposed him to a pan-Kenyan vision, but also denied him a strong ethnic base unlike his competitors. As one of Kenya's most experienced, unassuming and shrewd politicians, Saitoti was billed as a front-runner in the race to succeed President Mwai Kibaki.\n\nGeorge Saitoti was born on 3 August 1945 and brought up in Maasailand, where he spent his childhood herding cattle in line with the Masai culture, and attending school. He attended Ololua Primary School, Kajiado where he acquired his basic education in the 1950s. Between 1960 and 1963, he secured a place at Mang'u High School in Thika where he attained his high school education. He joined the ranks of Mang'u High School's highly decorated alumni including Kenya's third President, Mwai Kibaki, former Vice-President Moody Awori, Catholic Archbishop Ndingi Mwana-a-Nzeki, the late Environment Minister John Michuki, the late Trade Unionist and former Minister for Justice and Constitutional Affairs, Tom Mboya, and late Cardinal Maurice Michael Otunga.\nSaitoti spent a brief while in the United States of America, where he received his undergraduate education at Brandeis University between 1963 and 1967. During his time there, he was on the prestigious Wien Scholarship, specialising in Mathematics and Economics. His colleagues at the time remember that he enjoyed spending time in Cholmondeleys (the coffeehouse in the Castle) and excelled at high jump, ranking as one of the best in New England. In 1988, Saitoti received the first Brandeis Alumni Achievement Award, the highest honour the University bestows upon its graduates.\n\nSaitoti later moved to the United Kingdom where he acquired a Master of Science (MSc) degree in Mathematics from the University of Sussex, Brighton. He enrolled for his doctoral studies at the University of Warwick where he acquired his PhD in Mathematics in 1972; writing his dissertation under the supervision of Professor Luke Hodgkin in the area of algebraic topology under the topic: \"Mod-2 K-Theory of the Second Iterated Loop Space on a Sphere.\"\n\nUpon his graduation, Saitoti returned to Kenya in 1972, commencing a career as a Mathematics lecturer at the University of Nairobi. One of his contributions was the institutionalisation of Mathematics as a discipline in Africa. During the first Pan-African Conference of Mathematicians held in Rabat, Morocco, in 1976, Saitoti was involved in the creation of the African Mathematical Union (AMU). He was elected the AMU's Vice-President, a post which he held on up to 1979. By 1983, Saitoti's academic career was on the rise as associate professor and Head of the Mathematics Department.\nOutside the academy, Saitoti received several public appointments. On 3 November 1972, the Minister of Labour appointed him as the chairman of the Agricultural Wages Council (AWC). On 4 September 1979, the Minister for Tourism and Wildlife, John Ogutu, also appointed him as a committee member of the Natural Sciences Advisory Research Committee (TNSARC) chaired by Professor S. O. Wandiga. In September 1983, he was appointed chairperson of the board of directors for the Rift Valley Institute of Science and Technology. He also served in other public capacities as chairman of Mumias Sugar Company and the Kenya Commercial Bank.\n\nTop decision-makers in government had recognised Saitoti as a policy thinker and technocrat, of whom the KANU desperately needed to fix its institutions, politics and the economy. His seminal book, \"The Challenges of Economic and Institutional Reforms in Africa\" was widely praised by leading officials as providing practical policy proposals to deal with the various challenges facing Kenya and Africa.\nThe book drew from Saitoti's experience as a seasoned scholar, consultant and experienced policy-maker/thinker, presenting a rigorous and multidisciplinary analysis of strategies for poverty alleviation, sustainable development, poverty reduction, combating HIV/AIDS and peace diplomacy. Saitoti also emphasised the importance of institutional reforms and sound public policies to sustainable economic growth in Africa.\n\nLong before joining mainstream politics, Saitoti had a stint in the legislative duties. From 1974 to 1977, he represented Kenya in the defunct East African Community as a member of the East African Legislative Assembly. \n\nIn October 1983, President Daniel arap Moi nominated Saitoti as a member of parliament and subsequently appointed him to the Cabinet as Minister for Finance. He held the position until 1989. During the 1988 general elections, Saitoti took the plunge into competitive politics and won the Kajiado North parliamentary seat that was previously held by Philip Odupoy. Prior to the tenure of Adupoy and Saitoti, the Kajiado North multi-ethnic constituency was held by the popular politician, John Keen, another half-Maasai who champion a nationalist vision and worked over the years to ensure the advancement of his mother's people.\nFor more than 25 years, Professor George Saitoti has represented Kajiado North since 1988, recapturing the seat in consecutive elections in 1992, 1997, 2002 and 2007. Building on John Keen's legacy of a cosmopolitan constituency, Saitoti transformed Kajiado North into Kenya's most ethnically integrated multi-ethnic legislative area that also provided a safe haven to Kenyans, forcibly displaced by the 1991–2008 cycles of ethnic violence in neighbouring areas.\n\nThe area is also ranked among the top ten wealthiest, economically dynamic and fastest growing regions in Kenya. According to figures released by the Government of Kenya in 2009, Kajiado North has had an average poverty index of 10.66 per cent for the last three years, making it one of the richest constituencies in Kenya (see table 1).\n\nAfter the 1988 General Election, President Moi appointed Saitoti as Kenya's sixth Vice-President. Saitoti became Kenya's longest sitting vice-president serving for 13 years under President Daniel arap Moi between May 1989 and January 1998 and again between April 1999 and August 2002 (see table 2). At the same time, he served as Minister for Finance.\nIn 1990–1991, Saitoti was the Executive Chairman of the World Bank and the International Monetary Fund (IMF). In 1999–2000, Saitoti also served as president of the African, Caribbean and Pacific Group of States, becoming instrumental in helping negotiate a new development partnership agreement to succeed the previous Lomè Convention that expired in February 2000 between the ACP and the European Union.\nThe hallmarks of Saitoti tenure as Vice-President were efficiency, sobriety and loyalty as President Moi's most trusted lieutenant. Even when President Moi dithered in naming a new deputy after the 1997 elections, Saitoti was still his favoured choice 14 months down the line. The same traits of efficiency, patience and loyalty would make him one of President Mwai Kibaki's trusted Ministers.\n\nWhen Saitoti was appointed vice-president on 1 May 1989, KANU was back-pedaling on re-democratizing the country. At the same time, the party was fragmented over the succession divide between a sit-tight \"KANU-A\" and a more pro-change \"KANU-B\" led by Saitoti. The new vice-president was, therefore, compelled to walk the tightrope between being the face of change in the ruling party and remaining loyal to his principal who, after re-election as president in 1988, had amended the constitution to increase his power to dismiss judges and widened police powers.\nOn New Year's Day 1990, the vocal cleric Rev. Timothy Njoya called on all Africans to demand a multiparty system of government. Following the Saba Saba riots on 7 July 1990, President Moi announced the formation of the KANU Review Committee under the chairmanship of Prof George Saitoti, popularly known as \"the Saitoti Committee\".\n\nThe Saitoti Review Committee was mandated to investigate the party's internal electoral and disciplinary conduct. The committee traversed the country collecting people's opinions on the party, astounding foe, friend and critics alike and offering a rare forum for direct criticism and outbursts.\nIn January 1991, KANU's executive committee adopted the recommendation by George Saitoti, that critics of the party cease being expelled but suspended for one or two years.\n\nThe recommendations of the report were open for debate during the National Delegates Conference at Karasani in Nairobi. President Moi backed the adoption and implementation of the report in \"toto\", against what many speakers at the conference had expected. This opened the reforms gates, eventually setting the stage for the repeal of Section 2A in 1991 that returned Kenya to back to a multiparty system of government. The Saitoti Review Committee thrust the party on the reform path, but also widened internal ideological schisms between \"KANU-A\" conservatives and \"KANU-B\" pro-reformers over the Moi succession question.\n\nSaitoti was in the eye of a nasty succession storm that rocked KANU before and after the 1997 elections. Maasai purists led by Minister William Ole Ntimama and senior Maasai elders 're-Kikuyunized' Saitoti's dual ancestry, amplifying his Kikuyu family linkages as a scheme to weaken his political base and to challenge his status as a Maasai elder.\nDespite his steadfast loyalty to KANU and President Moi, Saitoti was frequently ignored, humiliated and frustrated by the party and its top echelons. Around the same time Foreign Affairs Minister Robert Ouko was murdered in February 1990, Saitoti claims that attempts were made on his life. After the 1997 general elections, he was dropped as Vice-President, although no replacement was appointed. Even as President Moi reappointed him in April 1999, on the roadside in Limuru, Kiambu he made a scathing remark to the effect that: \"I've given back Prof Saitoti the seat of Vice-President, hopefully now your sufurias (pots) will be full of food.\" Months before the general elections of 2002, Saitoti's name was deleted from the list of KANU delegates and his ascendancy to the presidency blocked by 'unknown' party members.\nOn 18 March 2002, when KANU held its national delegates conference at the Kasarani sports complex, the move to block Saitoti from the succession game was manifest. The meeting amended the party constitution to allow for the merger between KANU and Raila Odinga's National Development Party (NDP) to create the \"New KANU\". But it also introduced four new positions of party Vice-Chairmen primarily to water down Saitoti's position as Vice-President and Moi's most likely successor as president.\n\nIt was clear that Moi did not even want him as one of the four vice-chairmen posts reserved for Uhuru Kenyatta, Kalonzo Musyoka, Katana Ngala and Musalia Mudavadi. Moi told Saitoti to his face that he was not \"presidential material\". As a \"Maasai-Kikuyu,\" Saitoti lacked the ethnic numbers he needed in the political horse-trading in Moi's power game. Instead, Moi finger-pointed as his heir Uhuru Kenyatta, perceived to have a large ethnic base as a pure-bred Kikuyu with the \"Kenyatta\" mystique.\nSaitoti gracefully bowed out of the race, living to fight another day, but not without his famous line: \"There comes a time when the nation is much more important than an individual\".\nBut the KANU-NDP marriage came to a tragic end when Moi named Uhuru rather than Raila Odinga as his successor. In August 2002, Odinga left KANU to defeat Moi's \"use and dump game,\" and joined a group of KANU rebels\" coalesced around the \"Rainbow Alliance\" lobby that later transformed itself into the Liberal Democratic Party (LDP). Saitoti also walking out of KANU and became a key LDP figure. In October 2002, LDP joined the National Alliance of Kenya (NAK) of Mwai Kibaki, Charity Ngilu and Wamalwa Kijana to form the National Rainbow Coalition (NARC). Saitoti became a member of the NARC Summit, the highest organ of the coalition.\n\nWhen the NARC flag-bearer, Mwai Kibaki, decisively defeated KANU and Uhuru Kenyatta, Saitoti was appointed to the Ministry of Education. He was the man in charge of implementing NARC's flagship and globally acclaimed free primary education in Kenya.\n\nAfter 2004, as the NARC consensus crumbled, Saitoti left the agitating LDP camp and threw his lot behind President Kibaki. He canvassed for the government-sponsored draft Constitution, which lost to a combined KANU-LDP campaign during the November 2005 referendum. During the 2007 elections, Saitoti defended his Kajiado North parliamentary seat on the Party of National Unity (PNU) ticket, Kibaki's re-election vehicle, launched three months to the election on 16 September 2007.\nThe courts ordered a vote recount in Kajiado North, but Saitoti beat his closest competitor, Moses Ole Sakuda with close to 20,000 votes. Saitoti blamed his re-election glitch on intrigues of power by KANU forces within the PNU campaign which underwrote his rivals to knock him out of politics and potentially out of the 2012 Presidential elections. But he had remained reticent about it.\n\nSaitoti's traits of patience, efficiency and loyalty to Kibaki paid off. On 8 January 2008, he was appointed Minister of State for Internal Security and Provincial Administration in the Office of the President, a position previously occupied by a Kibaki confidant, John Michuki. Saitoti retained the Internal Security docket even after President Kibaki and Prime Minister Raila Odinga established the power-sharing government that ended the 2008 post-election crisis. Between October 2010 and August 2011, Saitoti was appointed Minister for Foreign Affairs on an acting capacity after the incumbent, Moses Wetangula, stepped aside to allow investigations on alleged corruption.\n\nIn July 2009, Saitoti was appointed to head a special cabinet sub-committee formed to oversee the affairs of the International Criminal Court (ICC) in Kenya. Members of this bi-partisan committee include Saitoti, Mutula Kilonzo and Moses Wetangula (PNU) and James Orengo, Otieno Kajwang and Amason Kingi (ODM). (Following a cabinet reshuffle in April 2012, Eugene Wamalwa and Prof. Sam Ongeri have replaced Kilonzo and Wetangula). The role of the sub-Committee as a liaison and co-ordination body between the ICC and the Kenyan government took a center-stage from December 2010 when the ICC Chief Prosecutor, Luis Moreno Ocampo, indicted six prominent Kenyans for alleged crimes against humanity relating to the 2008 post-election violence.\n\nAs the Minister of Internal Security and the chairman of the and security matters, Saitoti is the guarantor of the government's commitment to the ICC process. Arising from this, several analysts have claimed the suspects' fate lie with the sub-committee. Saitoti came out strongly criticising the invocation of President Kibaki in the ICC debate, calling for sobriety from politicians. Saitoti has maintained a legal interpretation on whether the suspects can vie for presidency in the coming elections, stressing that only the constitution can bar or let them free to enter the race.\n\nOn 19 December 2008, President Mwai Kibaki who was unanimously endorsed as Party Leader at the PNU National Delegates Conference (NDC) held at Kasarani sports complex in Nairobi. In accordance with the Political Parties Act (2008), Saitoti was elected PNU chairman, becoming the second-in-command in the party hierarchy since he lost as KANU Vice-President in the battle for the Moi succession in March 2002. His elevation, however, complicated coalition politics and raised the stakes for the Kibaki succession in PNU. Other presidential hopefuls, Uhuru Kenyatta and Kalonzo Musyoka, shunned the party and embarked on consolidating their respective parties.\nIn November 2010, Musyoka, Kenyatta and Saitoti signed a protocol to form and transform the PNU Alliance into a common political vehicle for the 2013 presidential race. But the imperative to comply with the Political Parties Act (2011) forced them to abandon the Alliance and shift attention to their respective parties.\n\nSaitoti was both Vice-President and Finance Minister at the height of the 1991–1993 Goldenberg scandal. Even though his own culpability in the fraud has never been established, for decades the Goldenberg has become the proverbial Sword of Damocles used against Saitoti in intra-elite power wars. In early 1999, Raila Odinga as a presidential contender to succeed Moi as President, sued Saitoti and others over alleged role in the Goldenberg scandal. Three months after his re-appointment as Vice-President on 3 April 1999, Otieno Kajwang', a Raila ally, moved a private member's motion of no confidence in the Vice-President for his alleged role in the Goldenberg fraud. Saitoti survived the onslaught.\n\nThe Goldenberg spectre returned to haunt Saitoti in the wake of the fierce political infighting between the LDP/KANU faction and Kibaki supporters in NARC that followed the 2005 referendum. On 3 February 2006 a report by the Goldenberg Commission of Inquiry, chaired by Justice Samuel Bosire, recommended that George Saitoti should face criminal charges for his involvement in the Goldenberg scandal. On 13 February 2006, Saitoti voluntarily stepped aside from his ministerial docket to pave way for investigations into the allegations.\nHowever, on 31 July 2006, a three-judge bench headed by Justice Joseph Nyamu issued a certiorari order clearing Prof Saitoti of any wrongdoing, expunging his name from the Bosire Commission Report and issuing an order on permanent stay of prosecution against Saitoti.\n\nIn dismissing the 23 paragraphs of the report, the High Court bench cited three inter-related errors of commission and omission by the Bosire Commission:\n\nOn 15 November 2006, President Kibaki reappointed Saitoti back to Cabinet.\nIn April 2012, the vetting board found Justice Samuel Bosire unfit to serve in the judiciary citing fails as the Chairman of the Goldenberg Commission of Inquiry. He ignored a High Court Order to summon retired President Daniel arap Moi, Musalia Mudavadi and Nicholas Biwott as witnesses. The vetting board also accused Justice Nyamu of undermining public confidence in the courts for issuing a permanent stay of prosecution against Saitoti.\n\nStarting October 2011, Saitoti worked closely with national Minister of Defence Mohamed Yusuf Haji to see through Linda Nchi, a coordinated operation in southern Somalia between the Somali military and the Kenyan military against the Al-Shabaab group of insurgents. The mission was officially led by the Somali army, with the Kenyan forces providing a support role. In early June 2012, Haji signed another agreement re-hatting Kenya's deployed military forces in Somalia under the AMISOM general command.\n\nIn November 2011, Saitoti confirmed that he was in the race to succeed President Kibaki, who is set to retire after the next general election. Saitoti reiterated his candidature in January 2012, continuing to tour Kenya, with meet-the-people excursions to the Rift Valley, Eastern and Central provinces.\n\nIt appeared to be history repeating itself in the battle for the soul of the Kikuyu between, Saitoti, a Maasai with Kikuyu kith and kin, and Uhuru Kenyatta, a thorough-bred Kikuyu. Uhuru is widely thought as the presumptive successor to President Kibaki, but Saitoti was emerging also, as a likely candidate. In the event that Uhuru's run for the presidency is thwarted by the confirmed charges by the ICC, it remains a too-up as to whether Saitoti would have benefited from the spin-off.\n\nSaitoti was a businessman who had interests in agriculture, horticulture, real estates, hospitality and pastoralism.\n\nSaitoti's family life rarely made it into the public space. His wife, Margaret Saitoti, was with him when the High Court dropped charges on the 16 years Goldenberg case. His brother, Ronald Musengi, has been a banking executive with the Kenya Commercial Bank. Recently Musengi applied to be a member of the National Police Service Commission.\n\nSaitoti died on Sunday 10 June 2012 at around 9:00 am when a Eurocopter AS350 helicopter belonging to the Kenya Police Air Wing registration 5Y-CDT, carrying him and the Assistant Minister for Internal Security, Joshua Orwa Ojode, crashed in the Kibiku area of Ngong forest, killing them and four others. He was buried on 16 June in Kajiado North constituency. After the Maasai elders agreed to abandon the traditional burial rites and embrace the Catholic way, fifty bulls were slaughtered at the funeral in accordance with Maasai tradition. Saitoti was to table a ministerial statement in Parliament.\n\nSaitoti, G. (2005). \"Keynote address given during the official opening of the sub-regional seminar for TIVET policy makers and UNESCO UNEVOC Center Coordinators\". Nairobi, Kenya.\n\n____________(2004). \"Education in Kenya: Challenges and policy responses\". Paper presented at the Council on\nForeign Relations, Washington D.C.\n\n____________(2003) \"National conference on education and training, Meeting the challenges of education and training during 21st century\". Nairobi.\n\n____________(2003). \"Reflections on Africa Development\", \"Journal of Third World Studies\".\n\nSaitoti, G. and KANU Review Committee(2002), \"Report of the KANU Review Committee, 1990\". The Committee, Nairobi.\n\n____________(2002).\"The Challenges of Economic and Institutional Reforms in Afric\". Ashgate Publishers Limited.\n\n____________(1985). i mathematica\", Politechnika Warszawska Technical.\n\n____________ \"A remark on Mod 2 K-Theory fundamental classes\". \"Ann. Fac. Sci. Univ. Nat\". Zaïre (Kinshasa)Sect. Math.-Phys. 3 (1977), no. 1, 61–63.\n\n____________\"Homology of a differential algebra\". \"Publ. Math. Debrecen\" 23 (1976), no. 3-4, 235—237.\n\n____________\"K-Theory fundamental classes\". \"Demonstration Math\". 8 (1975), No. 4, 365–377.\n\n____________A note on the homology of a differential graded algebra. \"Nigerian Journal of Science\". 8 (1974), no. 1-2,127–130.\n\n____________Loop spaces and K-theory. \"Journal of London Mathematics Society\".(2) 9 (1974/75), 423–428.\n\n\n"}
{"id": "26382071", "url": "https://en.wikipedia.org/wiki?curid=26382071", "title": "Gyula Pál", "text": "Gyula Pál\n\nGyula Pál (27 June 1881– 6 September 1946) was a noted Hungarian-Danish mathematician. He is known for his work on Jordan curves both in plane and space, and on the Kakeya problem. He proved that every locally connected planar continuum with at least two points is the orthogonal projection of a closed Jordan curve of the Euclidean 3-space.\n\nHe was born as \"Gyula Perl\" but hungaricized his surname to Pál in 1909. Fleeing the post-war chaos of Hungary after World War I he moved to Denmark in 1919, possibly by the invitation of Harald Bohr, where he spent the rest of his life and westernized his name to Julius Pal.\n"}
{"id": "1182871", "url": "https://en.wikipedia.org/wiki?curid=1182871", "title": "Hard-core predicate", "text": "Hard-core predicate\n\nIn cryptography, a hard-core predicate of a one-way function \"f\" is a predicate \"b\" (i.e., a function whose output is a single bit) which is easy to compute (as a function of \"x\") but is hard to compute given \"f(x)\". In formal terms, there is no probabilistic polynomial-time (PPT) algorithm that computes \"b(x)\" from \"f(x)\" with probability significantly greater than one half over random choice of \"x\". In other words, if \"x\" is drawn uniformly at random, then given \"f(x)\", any PPT adversary can only distinguish the hard-core bit \"b(x)\" and a uniformly random bit with negligible advantage over the length of \"x\".\n\nA hard-core function can be defined similarly. That is, if \"x\" is chosen uniformly at random, then given \"f(x)\", any PPT algorithm can only distinguish the hard-core function value \"h(x)\" and uniformly random bits of length \"|h(x)|\" with negligible advantage over the length of \"x\".\n\nA hard-core predicate captures \"in a concentrated sense\" the hardness of inverting \"f\".\n\nWhile a one-way function is hard to invert, there are no guarantees about the feasibility of computing partial information about the preimage \"c\" from the image \"f(x)\". For instance, while RSA is conjectured to be a one-way function, the Jacobi symbol of the preimage can be easily computed from that of the image.\n\nIt is clear that if a one-to-one function has a hard-core predicate, then it must be one way. Oded Goldreich and Leonid Levin (1989) showed how every one-way function can be trivially modified to obtain a one-way function that has a specific hard-core predicate. Let \"f\" be a one-way function. Define \"g(x,r) = (f(x), r)\" where the length of \"r\" is the same as that of \"x\". Let \"x\" denote the \"j\" bit of \"x\" and \"r\" the \"j\" bit of \"r\". Then\n\nformula_1\n\nis a hard core predicate of \"g\". Note that \"b(x, r)\" = <\"x, r\"> where <·, ·> denotes the standard inner product on the vector space (Z). This predicate is hard-core due to computational issues; that is, it is not hard to compute because \"g(x, r)\" is information theoretically lossy. Rather, if there exists an algorithm that computes this predicate efficiently, then there is another algorithm that can invert \"f\" efficiently.\n\nA similar construction yields a hard-core function with \"O(log |x|)\" output bits. Suppose \"f\" is a strong one-way function. Define \"g(x, r)\" = \"(f(x), r)\" where |\"r\"| = 2|\"x\"|. Choose a length function \"l(n)\" = \"O(log n)\" s.t. \"l(n)\" ≤ \"n\". Let\n\nformula_2\n\nThen \"h(x, r)\" := \"b(x, r) b(x, r) ... b(x, r)\" is a hard-core function with output length \"l(|x|)\".\n\nIt is sometimes the case that an actual bit of the input \"x\" is hard-core. For example, every single bit of inputs to the RSA function is a hard-core predicate of RSA and blocks of \"O(log |x|)\" bits of \"x\" are indistinguishable from random bit strings in polynomial time (under the assumption that the RSA function is hard to invert).\n\nHard-core predicates give a way to construct a pseudorandom generator from any one-way permutation. If \"b\" is a hard-core predicate of a one-way permutation \"f\", and \"s\" is a random seed, then\n\nformula_3\n\nis a pseudorandom bit sequence, where \"f\" means the n-th iteration of applying \"f\" on \"s\", and \"b\" is the generated hard-core bit by each round \"n\".\n\nHard-core predicates of trapdoor one-way permutations (known as trapdoor predicates) can be used to construct semantically secure public-key encryption schemes.\n\n\n"}
{"id": "10027538", "url": "https://en.wikipedia.org/wiki?curid=10027538", "title": "Height of a polynomial", "text": "Height of a polynomial\n\nIn mathematics, the height and length of a polynomial \"P\" with complex coefficients are measures of its \"size\".\n\nFor a polynomial \"P\" of degree \"n\" given by\n\nthe height \"H\"(\"P\") is defined to be the maximum of the magnitudes of its coefficients:\n\nand the length \"L\"(\"P\") is similarly defined as the sum of the magnitudes of the coefficients:\n\nThe Mahler measure \"M\"(\"P\") of \"P\" is also a measure of the size of \"P\". The three functions \"H\"(\"P\"), \"L\"(\"P\") and \"M\"(\"P\") \nare related by the inequalities\n\nwhere formula_7 is the binomial coefficient.\n\n\n"}
{"id": "2197070", "url": "https://en.wikipedia.org/wiki?curid=2197070", "title": "IP (complexity)", "text": "IP (complexity)\n\nIn computational complexity theory, the class IP (which stands for Interactive Polynomial time) is the class of problems solvable by an interactive proof system. It is equal to the class PSPACE. The result is a famous example where the proof does not relativize.\n\nThe concept of an interactive proof system was first introduced by Shafi Goldwasser, Silvio Micali, and Charles Rackoff in 1985. An interactive proof system consists of two machines, a prover, \"P\", which presents a proof that a given string \"n\" is a member of some language, and a verifier, \"V\", that checks that the presented proof is correct. The prover is assumed to be infinite in computation and storage, while the verifier is a probabilistic polynomial-time machine with access to a random bit string whose length is polynomial on the size of \"n\". These two machines exchange a polynomial number, \"p\"(\"n\"), of messages and once the interaction is completed, the verifier must decide whether or not \"n\" is in the language, with only a 1/3 chance of error. (So any language in BPP is in IP, since then the verifier could simply ignore the prover and make the decision on its own.)\n\nA language \"L\" belongs to IP if there exist \"V\", \"P\" such that for all \"Q\", \"w\":\n\nThe Arthur–Merlin protocol, introduced by László Babai, is similar in nature, except that the number of rounds of interaction is bounded by a constant rather than a polynomial.\n\nGoldwasser et al. have shown that \"public-coin\" protocols, where the random numbers used by the verifier are provided to the prover along with the challenges, are no less powerful than private-coin protocols. At most two additional rounds of interaction are required to replicate the effect of a private-coin protocol. The opposite inclusion is straightforward, because the verifier can always send to the prover the results of their private coin tosses, which proves that the two types of protocols are equivalent.\n\nIn the following section we prove that IP = PSPACE, an important theorem in computational complexity, which demonstrates that an interactive proof system can be used to decide whether a string is a member of a language in polynomial time, even though the traditional PSPACE proof may be exponentially long.\n\nThe proof can be divided in two parts, we show that IP ⊆ PSPACE and PSPACE ⊆ IP.\n\nIn order to demonstrate that IP ⊆ PSPACE, we present a simulation of an interactive proof system by a polynomial space machine. Now, we can define:\n\nand for every 0 ≤ \"j\" ≤ \"p\" and every message history \"M\", we inductively define the function \"N\":\n\nwhere:\n\nwhere Pr is the probability taken over the random string \"r\" of length \"p\". This expression is the average of \"N\", weighted by the probability that the verifier sent message \"m\".\n\nTake \"M\" to be the empty message sequence, here we will show that \"N\" can be computed in polynomial space, and that \"N\" = Pr[\"V\" accepts \"w\"]. First, to compute \"N\", an algorithm can recursively calculate the values \"N\" for every \"j\" and \"M\". Since the depth of the recursion is \"p\", only polynomial space is necessary. The second requirement is that we need \"N\" = Pr[\"V\" accepts \"w\"], the value needed to determine whether \"w\" is in A. We use induction to prove this as follows.\n\nWe must show that for every 0 ≤ \"j\" ≤ \"p\" and every \"M\", \"N\" = Pr[\"V\" accepts \"w\" starting at \"M\"], and we will do this using induction on \"j\". The base case is to prove for \"j\" = \"p\". Then we will use induction to go from \"p\" down to 0.\n\nThe base case of \"j\" = \"p\" is fairly simple. Since \"m\" is either accept or reject, if \"m\" is accept, \"N\" is defined to be 1 and Pr[\"V\" accepts \"w\" starting at \"M\"] = 1 since the message stream indicates acceptance, thus the claim is true. If \"m\" is reject, the argument is very similar.\n\nFor the inductive hypothesis, we assume that for some \"j\"+1 ≤ \"p\" and any message sequence \"M\", \"N\" = Pr[\"V\" accepts \"w\" starting at \"j\"+1] and then prove the hypothesis for \"j\" and any message sequence \"M\".\n\nIf \"j\" is even, \"m\" is a message from \"V\" to \"P\". By the definition of \"N\",\n\nThen, by the inductive hypothesis, we can say this is equal to\n\nFinally, by definition, we can see that this is equal to Pr[\"V\" accepts \"w\" starting at \"M\"].\n\nIf \"j\" is odd, \"m\" is a message from \"P\" to \"V\". By definition,\n\nThen, by the inductive hypothesis, this equals\n\nThis is equal to Pr[\"V\" accepts \"w\" starting at \"M\"] since:\n\nbecause the prover on the right-hand side could send the message \"m\" to maximize the expression on the left-hand side. And:\n\nSince the same Prover cannot do any better than send that same message. Thus, this holds whether \"i\" is even or odd and the proof that IP ⊆ PSPACE is complete.\n\nHere we have constructed a polynomial space machine that uses the best prover \"P\" for a particular string \"w\" in language \"A\". We use this best prover in place of a prover with random input bits because we are able to try every set of random input bits in polynomial space. Since we have simulated an interactive proof system with a polynomial space machine, we have shown that IP ⊆ PSPACE, as desired.\n\nIn order to illustrate the technique that will be used to prove PSPACE ⊆ IP, we will first prove a weaker theorem, which was proven by Lund, et al.: #SAT ∈ IP. Then using the concepts from this proof we will extend it to show that TQBF ∈ IP. Since TQBF ∈ PSPACE-complete, and TQBF ∈ IP then PSPACE ⊆ IP.\n\nWe begin by showing that #SAT is in IP, where:\n\nNote that this is different from the normal definition of #SAT, in that it is a decision problem, rather than a function.\n\nFirst we use arithmetization to map the boolean formula with \"n\" variables, φ(\"b\", ..., \"b\") to a polynomial \"p\"(\"x\", ..., \"x\"), where \"p\" mimics φ in that \"p\" is 1 if φ is true and 0 otherwise provided that the variables of \"p\" are assigned Boolean values. The Boolean operations ∨, ∧ and ¬ used in φ are simulated in \"p\" by replacing the operators in φ as shown in the table below.\n\nAs an example, φ = \"a\" ∧ \"b\" ∨ ¬\"c\" would be converted into a polynomial as follows:\n\nThe operations \"ab\" and \"a\" ∗ \"b\" each result in a polynomial with a degree bounded by the sum of the degrees of the polynomials for \"a\" and \"b\" and hence, the degree of any variable is at most the length of φ.\n\nNow let \"F\" be a finite field with order \"q\" > 2; also demand that q be at least 1000. For each 0 ≤ \"i\" ≤ \"n\", define a function \"f\" on \"F\", having parameters formula_14, and a single variable \"a\" in \"F\": For 0 ≤ \"i\" ≤ \"n\" and for formula_15 let \nNote that the value of \"f\" is the number of satisfying assignments of φ. \"f\" is a void function, with no variables.\n\nNow the protocol for #SAT works as follows:\n\n\nNote that this is a public-coin algorithm.\n\nIf φ has \"k\" satisfying assignments, clearly \"V\" will accept. If φ does not have \"k\" satisfying assignments we assume there is a prover formula_21 that tries to convince \"V\" that φ does have \"k\" satisfying assignments. We show that this can only be done with low probability.\n\nTo prevent \"V\" from rejecting in phase 0, formula_21 has to send an incorrect value formula_23 to \"P\". Then, in phase 1, formula_21 must send an incorrect polynomial formula_25 with the property that formula_26. When \"V\" chooses a random \"r\" to send to \"P\", \nThis is because a polynomial in a single variable of degree at most \"d\" can have no more than \"d\" roots (unless it always evaluates to 0). So, any two polynomials in a single variable of degree at most \"d\" can be equal only in \"d\" places. Since |\"F\"| > 2 the chances of \"r\" being one of these values is at most formula_28 if \"n\" > 10, or at most (\"n\"/1000) ≤ (\"n\"/\"n\") if \"n\" ≤ 10.\n\nGeneralizing this idea for the other phases we have for each 1 ≤ \"i\" ≤ \"n\" if \nthen for \"r\" chosen randomly from \"F\", \nThere are \"n\" phases, so the probability that formula_21 is lucky because \"V\" selects at some stage a convenient \"r\" is at most 1/\"n\". So, no prover can make the verifier accept with probability greater than 1/\"n\". We can also see from the definition that the verifier \"V\" operates in probabilistic polynomial time. Thus, #SAT ∈ IP.\n\nIn order to show that PSPACE is a subset of IP, we need to choose a PSPACE-complete problem and show that it is in IP. Once we show this, then it clear that PSPACE ⊆ IP. The proof technique demonstrated here is credited to Adi Shamir.\n\nWe know that TQBF is in PSPACE-Complete. So let ψ be a quantified boolean expression:\n\nwhere φ is a CNF formula. Then \"Q\" is a quantifier, either ∃ or ∀. Now \"f\" is the same as in the previous proof, but now it also includes quantifiers.\n\nHere, φ(\"a\", ..., \"a\") is φ with \"a\" to \"a\" substituted for \"x\" to \"x\". Thus \"f\" is the truth value of ψ. In order to arithmetize ψ we must use the following rules:\n\nwhere as before we define \"x\" ∗ \"y\" = 1 − (1 − \"x\")(1 − \"y\").\n\nBy using the method described in #SAT, we must face a problem that for any \"f\" the degree of the resulting polynomial may double with each quantifier. In order to prevent this, we must introduce a new reduction operator \"R\" which will reduce the degrees of the polynomial without changing their behavior on Boolean inputs.\n\nSo now before we arithmetize formula_35 we introduce a new expression:\n\nor put another way:\n\nNow for every \"i\" ≤ \"k\" we define the function \"f\". We also define formula_38 to be the polynomial \"p\"(\"x\", ..., \"x\") which is obtained by arithmetizing φ. Now in order to keep the degree of the polynomial low, we define \"f\" in terms of \"f\":\n\nNow we can see that the reduction operation R, doesn't change the degree of the polynomial. Also it is important to see that the R\"\" operation doesn't change the value of the function on boolean inputs. So \"f\" is still the truth value of ψ, but the R\"\" value produces a result that is linear in \"x\". Also after any formula_42 we add formula_43 in ψ′ in order to reduce the degree down to 1 after arithmetizing formula_44.\n\nNow let's describe the protocol. If \"n\" is the length of ψ, all arithmetic operations in the protocol are over a field of size at least \"n\" where \"n\" is the length of ψ.\n\n\n\n\"V\" uses coefficients to evaluate formula_48 and formula_49. Then it checks that the polynomial degree is at most \"n\" and that the following identities are true:\nIf either fails then reject.\n\n\"V\" → \"P\": \"V\" picks a random \"r\" in \"F\" and sends it to P. (If formula_52 then this \"r\" replaces the previous \"r\").\n\nGoto phase \"i\" + 1 where \"P\" must persuade \"V\" that formula_53 is correct.\n\n\nThis is the end of the protocol description.\n\nIf ψ is true then \"V\" will accept when \"P\" follows the protocol. Likewise if formula_56 is a malicious prover which lies, and if ψ is false, then formula_56 will need to lie at phase 0 and send some value for \"f\". If at phase \"i\", \"V\" has an incorrect value for formula_58 then formula_59 and formula_60 will likely also be incorrect, and so forth. The probability for formula_56 to get lucky on some random \"r\" is at most the degree of the polynomial divided by the field size: formula_62. The protocol runs through \"O\"(\"n\") phases, so the probability that formula_56 gets lucky at some phase is ≤ 1/\"n\". If formula_64 is never lucky, then \"V\" will reject at phase \"k\"+1.\n\nSince we have now shown that both IP ⊆ PSPACE and PSPACE ⊆ IP, we can conclude that IP = PSPACE as desired. Moreover, we have shown that any IP algorithm may be taken to be public-coin, since the reduction from PSPACE to IP has this property.\n\nThere are a number of variants of IP which slightly modify the definition of the interactive proof system. We summarize some of the better-known ones here.\n\nA subset of IP is the deterministic Interactive Proof class, which is similar to IP but has a deterministic verifier (i.e. with no randomness).\nThis class is equal to NP.\n\nAn \"equivalent\" definition of IP replaces the condition that the interaction succeeds with high probability on strings in the language with the requirement that it \"always\" succeeds:\n\nThis seemingly stronger criterion of \"perfect completeness\" does not change the complexity class IP, since any language with an interactive proof system may be given an interactive proof system with perfect completeness.\n\nIn 1988, Goldwasser et al. created an even more powerful interactive proof system based on IP called MIP in which there are \"two\" independent provers. The two provers cannot communicate once the verifier has begun sending messages to them. Just as it's easier to tell if a criminal is lying if he and his partner are interrogated in separate rooms, it's considerably easier to detect a malicious prover trying to trick the verifier if there is another prover it can double-check with. In fact, this is so helpful that Babai, Fortnow, and Lund were able to show that MIP = NEXPTIME, the class of all problems solvable by a nondeterministic machine in \"exponential time\", a very large class. Moreover, all languages in NP have zero-knowledge proofs in an MIP system, without any additional assumptions; this is only known for IP assuming the existence of one-way functions.\n\nIPP (\"unbounded IP\") is a variant of IP where we replace the BPP verifier by a PP verifier. More precisely, we modify the completeness and soundness conditions as follows:\n\n\nAlthough IPP also equals PSPACE, IPP protocols behaves quite differently from IP with respect to oracles: IPP=PSPACE with respect to all oracles, while IP ≠ PSPACE with respect to almost all oracles.\n\nQIP is a version of IP replacing the BPP verifier by a BQP verifier, where BQP is the class of problems solvable by quantum computers in polynomial time. The messages are composed of qubits. In 2009, Jain, Ji, Upadhyay, and Watrous proved that QIP also equals PSPACE, implying that this change gives no additional power to the protocol. This subsumes a previous result of Kitaev and Watrous that QIP is contained in EXPTIME because QIP = QIP[3], so that more than three rounds are never necessary.\n\nWhereas IPP and QIP give more power to the verifier, a compIP system (\"competitive IP proof system\") weakens the completeness condition in a way that weakens the prover:\n\n\nEssentially, this makes the prover a BPP machine with access to an oracle for the language, but only in the completeness case, not the soundness case. The concept is that if a language is in compIP, then interactively proving it is in some sense as easy as deciding it. With the oracle, the prover can easily solve the problem, but its limited power makes it much more difficult to convince the verifier of anything. In fact, compIP isn't even known or believed to contain NP.\n\nOn the other hand, such a system can solve some problems believed to be hard. Somewhat paradoxically, though such a system is not believed to be able to solve all of NP, it can easily solve all NP-complete problems due to self-reducibility. This stems from the fact that if the language L is not NP-hard, the prover is substantially limited in power (as it can no longer decide all NP problems with its oracle).\n\nAdditionally, the graph nonisomorphism problem (which is a classical problem in IP) is also in compIP, since the only hard operation the prover has to do is isomorphism testing, which it can use the oracle to solve. Quadratic non-residuosity and graph isomorphism are also in compIP. Note, Quadratic non-residuosity (QNR) is likely an easier problem than graph isomorphism as QNR is in UP intersect co-UP.\n\n"}
{"id": "14058925", "url": "https://en.wikipedia.org/wiki?curid=14058925", "title": "Jessica Sklar", "text": "Jessica Sklar\n\nJessica Katherine Sklar (born 1973) is a mathematician interested in abstract algebra, recreational mathematics, and the popularization of mathematics. She is a professor of mathematics at Pacific Lutheran University, and the head of the mathematics department at Pacific Lutheran.\n\nAs a high school student, Sklar studied poetry at the Interlochen Arts Academy. She did her undergraduate studies at Swarthmore College, where her parents Elizabeth S. and Lawrence Sklar had met and married (she as an English major, later to become an English professor at Wayne State University, he as a professor of the philosophy of science). She completed a double major in English and mathematics in 1995.\n\nNext, Sklar moved to the University of Oregon for graduate study in mathematics, earning a master's degree in 1997 and completing her Ph.D. there in 2001. Her dissertation, \"Binomial Rings and Algebras\", was supervised by Frank Wylie Anderson.\n\nShe has been a faculty member in the mathematics department at Pacific Lutheran since 2001.\n\nSklar is the author of an open textbook on abstract algebra, \"First-Semester Abstract Algebra: A Structural Approach\" (2017).\n\nWith her mother, Elizabeth S. Sklar, she is the editor of \"Mathematics in Popular Culture: Essays on Appearances in Film, Literature, Games, Television and Other Media\" (McFarland & Co., 2012).\n\nSklar was a winner of the Carl B. Allendoerfer Award of the Mathematical Association of America in 2011 for her paper with Gene Abrams, \"The Graph Menagerie: Abstract Algebra and the Mad Veterinarian\".\nThe paper provides a general solution to a class of lattice reduction puzzles exemplified by the following one:\n"}
{"id": "39109075", "url": "https://en.wikipedia.org/wiki?curid=39109075", "title": "L-statistic", "text": "L-statistic\n\nIn statistics, an L-statistic is a statistic (function of a data set) that is a linear combination of order statistics; the \"L\" is for \"linear\". These are more often referred to by narrower terms according to use, namely:\n\n"}
{"id": "159974", "url": "https://en.wikipedia.org/wiki?curid=159974", "title": "Lagrange multiplier", "text": "Lagrange multiplier\n\nIn mathematical optimization, the method of Lagrange multipliers (named after Joseph-Louis Lagrange) is a strategy for finding the local maxima and minima of a function subject to equality constraints (i.e., subject to the condition that one or more equations have to be satisfied exactly by the chosen values of the variables). The great advantage of this method is that it allows the optimization to be solved without explicit parameterization in terms of the constraints. As a result, the method of Lagrange multipliers is widely used to solve challenging constrained optimization problems.\n\nThe method can be summarized as follows:\n\nFor the case of only one constraint and only two choice variables (as exemplified in Figure 1), consider the optimization problem\n\n(Sometimes an additive constant is shown separately rather than being included in \"g\", in which case the constraint is written , as in Figure 1.) We assume that both and have continuous first partial derivatives. We introduce a new variable () called a Lagrange multiplier and study the Lagrange function (or Lagrangian or Lagrangian expression) defined by\n\nwhere the term may be either added or subtracted. If is a maximum of for the original constrained problem, then there exists such that is a stationary point for the Lagrange function (stationary points are those points where the first partial derivatives of formula_2 are zero). However, not all stationary points yield a solution of the original problem, as the method of Lagrange multipliers yields only a necessary condition for optimality in constrained problems. Sufficient conditions for a minimum or maximum also exist, but if a particular candidate solution satisfies the sufficient conditions, it is only guaranteed that that solution is the best one \"locally\" – that is, it is better than any permissible nearby points. The \"global\" optimum can be found by comparing the values of the original objective function at the points satisfying the necessary and locally sufficient conditions.\n\nThe method of Lagrange multipliers relies on the intuition that at a maximum, cannot be increasing in the direction of any neighboring point where . If it were, we could walk along to get higher, meaning that the starting point wasn't actually the maximum.\n\nWe can visualize contours of given by for various values of , and the contour of given by .\n\nSuppose we walk along the contour line with . We are interested in finding points where does not change as we walk, since these points might be maxima.\n\nThere are two ways this could happen: \n\n\nTo check the first possibility (we are following a contour line of ), notice that since the gradient of a function is perpendicular to the contour lines, the contour lines of and are parallel if and only if the gradients of and are parallel. Thus we want points where and\n\nfor some \n\nwhere\n\nare the respective gradients. The constant is required because although the two gradient vectors are parallel, the magnitudes of the gradient vectors are generally not equal. This constant is called the Lagrange multiplier. (In some conventions is preceded by a minus sign).\n\nNotice that this method also solves the second possibility, that is level: if is level, then its gradient is zero, and setting is a solution regardless of formula_5.\n\nTo incorporate these conditions into one equation, we introduce an auxiliary function\n\nand solve\n\nNote that this amounts to solving three equations in three unknowns. This is the method of Lagrange multipliers. Note that formula_8 implies . To summarize\n\nThe method generalizes readily to functions on formula_10 variables\n\nwhich amounts to solving formula_12 equations in formula_12 unknowns.\n\nThe constrained extrema of are \"critical points\" of the Lagrangian formula_2, but they are not necessarily \"local extrema\" of formula_2 (see Example 2 below).\n\nOne may reformulate the Lagrangian as a Hamiltonian, in which case the solutions are local minima for the Hamiltonian. This is done in optimal control theory, in the form of Pontryagin's minimum principle.\n\nThe fact that solutions of the Lagrangian are not necessarily extrema also poses difficulties for numerical optimization. This can be addressed by computing the \"magnitude\" of the gradient, as the zeros of the magnitude are necessarily local minima, as illustrated in the .\n\nThe method of Lagrange multipliers can be extended to solve problems with multiple constraints using a similar argument. Consider a paraboloid subject to two line constraints that intersect at a single point. As the only feasible solution, this point is obviously a constrained extremum. However, the level set of formula_16 is clearly not parallel to either constraint at the intersection point (see Figure 3); instead, it is a linear combination of the two constraints' gradients. In the case of multiple constraints, that will be what we seek in general: the method of Lagrange seeks points not at which the gradient of formula_16 is multiple of any single constraint's gradient necessarily, but in which it is a linear combination of all the constraints' gradients.\n\nConcretely, suppose we have formula_18 constraints and are walking along the set of points satisfying formula_19. Every point formula_20 on the contour of a given constraint function formula_21 has a space of allowable directions: the space of vectors perpendicular to formula_22. The set of directions that are allowed by all constraints is thus the space of directions perpendicular to all of the constraints' gradients. Denote this space of allowable moves by formula_23 and denote the span of the constraints' gradients by formula_24. Then formula_25, the space of vectors perpendicular to every element of formula_24.\n\nWe are still interested in finding points where formula_16 does not change as we walk, since these points might be (constrained) extrema. We therefore seek formula_20 such that any allowable direction of movement away from formula_20 is perpendicular to formula_30 (otherwise we could increase formula_16 by moving along that allowable direction). In other words, formula_32. Thus there are scalars such that\n\nThese scalars are the Lagrange multipliers. We now have formula_18 of them, one for every constraint.\n\nAs before, we introduce an auxiliary function\n\nand solve\n\nwhich amounts to solving formula_37 equations in formula_37 unknowns.\n\nThe method of Lagrange multipliers is generalized by the Karush–Kuhn–Tucker conditions, which can also take into account inequality constraints of the form .\n\nThe problem of finding the local maxima and minima subject to constraints can be generalized to finding local maxima and minima on a differentiable manifold.\n\nLet formula_18 be a smooth manifold of dimension formula_40. Suppose that we wish to find the stationary points formula_41 of a smooth function formula_42 when restricted to the submanifold formula_43 defined by formula_44, where formula_45 is a smooth function for which 0 is a regular value of formula_46.\n\nLet formula_47 and formula_48 be the exterior derivatives. Stationarity for the restriction formula_49 at formula_50 means formula_51. Equivalently, the kernel formula_52 contains formula_53. In other words, formula_54 and formula_55 are proportional vectors. For this it is necessary and sufficient that the following system of formula_56 equations holds:\n\nwhere formula_58 denotes the exterior product. The stationary points formula_41 are the solutions of the above system of equations plus the constraint formula_44. Note that the formula_56 equations are not independent, since the left-hand side of the equation belongs to the subvariety of formula_62 consisting of decomposable elements.\n\nIn this formulation, it is not necessary to explicitly find the Lagrange multiplier, a number formula_63 such that formula_64.\n\nLet formula_18 be a smooth manifold embedded in formula_66 with codimension formula_67. Suppose formula_68 is a differentiable function that we wish to maximize on formula_18. By an extension of Fermat's theorem the local maxima occur at points where the exterior derivative formula_47 vanishes. In particular, supposing we have a local coordinate chart formula_71 for formula_72, the extrema of formula_16 are a subset of the critical points of the function formula_74, where formula_75 is the image of formula_76.\nHowever, it is often difficult to compute formula_77 explicitly. \"The entire method of Lagrange multipliers reduces to the idea of skipping that step and finding the zeros of formula_78 directly.\"\n\nIt remains to show that this notion of critical point is well defined. Suppose that formula_76 and formula_80 are charts on formula_18 whose domains both contain some point formula_82. The chain rule implies that formula_83 is a critical point of formula_84 if and only if formula_85 is a critical point of formula_86. Indeed,\n\nThe composition formula_88 is a transition map for formula_18 and therefore is a diffeomorphism. It follows that\n\nso formula_91 is the formula_92 map if and only if the same is true of formula_93. Formally, we're looking for points formula_94 such that for some (and hence any) chart formula_76 containing formula_82, formula_83 is a critical point of formula_84.\n\nSuppose the manifold formula_18 is defined by a smooth level set function formula_100 as formula_101, with formula_102 and formula_103 a submersion. It follows from the construction in the level set theorem that the image of formula_104 is formula_105, where we tacitly identify formula_106 with a subspace of formula_66 via the inclusion map on formula_18. Therefore,\n\nif and only if\nwriting formula_82 for formula_112. Again we can naturally interpret elements of formula_113, particularly formula_82 itself, as points in formula_66 via the inclusion map on formula_18.\n\nBy a combination of the first and third isomorphism theorems, the image of formula_117 must be isomorphic to a subspace of the image of formula_118. It follows that there exists a linear map formula_119 such that formula_120. As a linear map, formula_121 must satisfy formula_122 for a fixed formula_123, so finding a critical point of formula_84 is equivalent to solving the system of equations\n\nin the variables formula_127 and formula_123. This is in general a non-linear system of formula_129 equations and formula_129 unknowns.\n\nFinding local maxima of a function formula_131 is done by finding all points formula_132 such that formula_133 then checking whether all the eigenvalues of the Hessian formula_134 are negative. (Note that the maxima may not exist even if formula_16 is continuous because formula_113 is open, and also note that the conditions checked here are sufficient but not necessary for optimality.) Setting formula_133 is a non-linear problem and in general, arbitrarily difficult. After finding the critical points, checking the eigenvalues is a linear problem and thus easy.\n\nOften the Lagrange multipliers have an interpretation as some quantity of interest. For example, if the Lagrangian expression is\n\nthen\n\nSo, is the rate of change of the quantity being optimized as a function of the constraint parameter.\nAs examples, in Lagrangian mechanics the equations of motion are derived by finding stationary points of the action, the time integral of the difference between kinetic and potential energy. Thus, the force on a particle due to a scalar potential, , can be interpreted as a Lagrange multiplier determining the change in action (transfer of potential to kinetic energy) following a variation in the particle's constrained trajectory. \nIn control theory this is formulated instead as costate equations.\n\nMoreover, by the envelope theorem the optimal value of a Lagrange multiplier has an interpretation as the marginal effect of the corresponding constraint constant upon the optimal attainable value of the original objective function: if we denote values at the optimum with an asterisk, then it can be shown that\n\nFor example, in economics the optimal profit to a player is calculated subject to a constrained space of actions, where a Lagrange multiplier is the change in the optimal value of the objective function (profit) due to the relaxation of a given constraint (e.g. through a change in income); in such a context is the marginal cost of the constraint, and is referred to as the shadow price.\n\nSufficient conditions for a constrained local maximum or minimum can be stated in terms of a sequence of principal minors (determinants of upper-left-justified sub-matrices) of the bordered Hessian matrix of second derivatives of the Lagrangian expression.\n\nSuppose we wish to maximize formula_141 subject to the constraint formula_142. The feasible set is the unit circle, and the level sets of are diagonal lines (with slope −1), so we can see graphically that the maximum occurs at formula_143, and that the minimum occurs at formula_144.\n\nFor the method of Lagrange multipliers, the constraint is\n\nhence\n\nNow we can calculate the gradient:\n\nand therefore:\n\nNotice that the last equation is the original constraint.\n\nThe first two equations yield\n\nBy substituting into the last equation we have:\n\nso\n\nwhich implies that the stationary points of formula_2 are\n\nEvaluating the objective function at these points yields\n\nThus the constrained maximum is formula_155 and the constrained minimum is formula_156.\n\nNow we modify the objective function of Example 1a so that we minimize formula_157 instead of formula_158 again along the circle formula_159. Now the level sets of \"f\" are still lines of slope −1, and the points on the circle tangent to these level sets are again formula_160 and formula_161. These tangency points are maxima of \"f\".\n\nOn the other hand, the minima occur on the level set for \"f\" = 0 (since by its construction \"f\" cannot take negative values), at formula_162 and formula_163, where the level curves of \"f\" are not tangent to the constraint. The condition that formula_164 correctly identifies all four points as extrema; the minima are characterized in particular by formula_165\n\nIn this example we will deal with some more strenuous calculations, but it is still a single constraint problem.\n\nSuppose we want to find the maximum values of\n\nwith the condition that the and coordinates lie on the circle around the origin with radius , that is, subject to the constraint\n\nAs there is just a single constraint, we will use only one multiplier, say .\n\nThe constraint is identically zero on the circle of radius . See that any multiple of may be added to leaving unchanged in the region of interest (on the circle where our original constraint is satisfied).\n\nApply the ordinary Langrange multiplier method. Let:\n\nNow we can calculate the gradient:\n\nAnd therefore:\n\nNotice that (iii) is just the original constraint. (i) implies \"or\" . If then formula_171 by (iii) and consequently from (ii). If , substituting in (ii) we get . Substituting this in (iii) and solving for gives . Thus there are six critical points of formula_2:\n\nEvaluating the objective at these points, we find that\n\nTherefore, the objective function attains the global maximum (subject to the constraints) at formula_175 and the global minimum at formula_176 The point formula_177 is a local minimum of \"f\" and formula_178 is a local maximum of \"f\", as may be determined by consideration of the Hessian matrix of formula_179.\n\nNote that while formula_180 is a critical point of formula_2, it is not a local extremum of formula_182 We have\n\nGiven any neighborhood of formula_180, we can choose a small positive formula_185 and a small formula_186 of either sign to get formula_2 values both greater and less than formula_188. This can also be seen from the fact that the Hessian matrix of formula_2 evaluated at this point (or indeed at any of the critical points) is an indefinite matrix. Each of the critical points of formula_2 is a saddle point of formula_182\n\nSuppose we wish to find the discrete probability distribution on the points formula_192 with maximal information entropy. This is the same as saying that we wish to find the least structured probability distribution on the points formula_193. In other words, we wish to maximize the Shannon entropy equation:\n\nFor this to be a probability distribution the sum of the probabilities formula_195 at each point formula_196 must equal 1, so our constraint is:\n\nWe use Lagrange multipliers to find the point of maximum entropy, formula_198, across all discrete probability distributions formula_199 on formula_200. We require that:\n\nwhich gives a system of equations, formula_202, such that:\n\nCarrying out the differentiation of these equations, we get\n\nThis shows that all formula_205 are equal (because they depend on only). By using the constraint\n\nwe find\n\nHence, the uniform distribution is the distribution with the greatest entropy, among distributions on points.\n\nThe critical points of Lagrangians occur at saddle points, rather than at local maxima (or minima). Unfortunately, many numerical optimization techniques, such as hill climbing, gradient descent, some of the quasi-Newton methods, among others, are designed to find local maxima (or minima) and not saddle points. For this reason, one must either modify the formulation to ensure that it's a minimization problem (for example, by extremizing the square of the gradient of the Lagrangian as below), or else use an optimization technique that finds stationary points (such as Newton's method without an extremum seeking line search) and not necessarily extrema.\n\nAs a simple example, consider the problem of finding the value of that minimizes formula_208, constrained such that formula_209. (This problem is somewhat pathological because there are only two values that satisfy this constraint, but it is useful for illustration purposes because the corresponding unconstrained function can be visualized in three dimensions.)\n\nUsing Lagrange multipliers, this problem can be converted into an unconstrained optimization problem:\n\nThe two critical points occur at saddle points where and .\n\nIn order to solve this problem with a numerical optimization technique, we must first transform this problem such that the critical points occur at local minima. This is done by computing the magnitude of the gradient of the unconstrained optimization problem.\n\nFirst, we compute the partial derivative of the unconstrained problem with respect to each variable:\n\nIf the target function is not easily differentiable, the differential with respect to each variable can be approximated as\n\nwhere formula_185 is a small value.\n\nNext, we compute the magnitude of the gradient, which is the square root of the sum of the squares of the partial derivatives:\n\nThe critical points of occur at and , just as in formula_2. Unlike the critical points in formula_2, however, the critical points in occur at local minima, so numerical optimization techniques can be used to find them.\n\nIn optimal control theory, the Lagrange multipliers are interpreted as costate variables, and Lagrange multipliers are reformulated as the minimization of the Hamiltonian, in Pontryagin's minimum principle.\n\nThe Lagrange multiplier method has several generalizations. In nonlinear programming there are several multiplier rules, \"e.g.\", the Carathéodory–John Multiplier Rule and the Convex Multiplier Rule, for inequality constraints.\n\n\nExposition\nFor additional text and interactive applets\n"}
{"id": "496558", "url": "https://en.wikipedia.org/wiki?curid=496558", "title": "Law of the iterated logarithm", "text": "Law of the iterated logarithm\n\nIn probability theory, the law of the iterated logarithm describes the magnitude of the fluctuations of a random walk. The original statement of the law of the iterated logarithm is due to A. Y. Khinchin (1924). Another statement was given by A. N. Kolmogorov in 1929.\n\nLet {\"Y\"} be independent, identically distributed random variables with means zero and unit variances. Let \"S\" = \"Y\" + … + \"Y\". Then\nwhere “log” is the natural logarithm, “lim sup” denotes the limit superior, and “a. s.” stands for “almost surely”.\n\nThe law of iterated logarithms operates “in between” the law of large numbers and the central limit theorem. There are two versions of the law of large numbers — the weak and the strong — and they both state that the sums \"S\", scaled by \"n\", converge to zero, respectively in probability and almost surely:\n\nOn the other hand, the central limit theorem states that the sums \"S\" scaled by the factor \"n\" converge in distribution to a standard normal distribution. By Kolmogorov's zero–one law, for any fixed \"M\", the probability that the event\nformula_3\noccurs is 0 or 1.\nThen\n\nso\n\nAn identical argument shows that\n\nThis implies that these quantities cannot converge almost surely. In fact, they cannot even converge in probability, which follows from the equality\n\nand the fact that the random variables\n\nare independent and both converge in distribution to formula_9\n\nThe \"law of the iterated logarithm\" provides the scaling factor where the two limits become different:\n\nThus, although the quantity formula_11 is less than any predefined \"ε\" > 0 with probability approaching one, the quantity will nevertheless be greater than \"ε\" infinitely often; in fact, the quantity will be visiting the neighborhoods of any point in the interval (-1,1) almost surely.\n\nThe law of the iterated logarithm (LIL) for a sum of independent and identically distributed (i.i.d.) random variables with zero mean and bounded increment dates back to Khinchin and Kolmogorov in the 1920s.\n\nSince then, there has been a tremendous amount of work on the LIL for various kinds of\ndependent structures and for stochastic processes. Following is a small sample of notable developments.\n\nHartman–Wintner (1940) generalized LIL to random walks with increments with zero mean and finite variance.\n\nStrassen (1964) studied LIL from the point of view of invariance principles.\n\nStout (1970) generalized the LIL to stationary ergodic martingales.\n\nDe Acosta (1983) gave a simple proof of Hartman–Wintner version of LIL.\n\nWittmann (1985) generalized Hartman–Wintner version of LIL to random walks satisfying milder conditions.\n\nVovk (1987) derived a version of LIL valid for a single chaotic sequence (Kolmogorov random sequence). This is notable, as it is outside the realm of classical probability theory.\n\nYongge Wang has shown that the law of the iterated logarithm holds for polynomial time pseudorandom sequences also. The Java-based software testing tool tests whether a pseudorandom generator outputs sequences that satisfy the LIL.\n\n"}
{"id": "35699507", "url": "https://en.wikipedia.org/wiki?curid=35699507", "title": "Learning rule", "text": "Learning rule\n\nLearning rule or Learning process is a method or a mathematical logic which improves the artificial neural network's performance and usually this rule is applied repeatedly over the network. It is done by updating the levels of a network when a network is simulated in a specific data environment. A learning rule may accept existing condition ( weights and bias ) of the network and will compare the expected result and actual result of the network to give new and improved values for weights and bias. Depending on the complexity of actual model, which is being simulated, the learning rule of the network can be as simple as an XOR gate or Mean Squared Error or it can be the result of multiple differential equations. The learning rule is one of the factors which decides how fast or how accurate the artificial network can be developed. Depending upon the process to develop the network there are three main models of machine learning:\n\n\n"}
{"id": "1470767", "url": "https://en.wikipedia.org/wiki?curid=1470767", "title": "Linear complementarity problem", "text": "Linear complementarity problem\n\nIn mathematical optimization theory, the linear complementarity problem (LCP) arises frequently in computational mechanics and encompasses the well-known quadratic programming as a special case. It was proposed by Cottle and Dantzig \n\nGiven a real matrix \"M\" and vector \"q\", the linear complementarity problem LCP(\"M\", \"q\") seeks vectors \"z\" and \"w\" which satisfy the following constraints:\n\n\nA sufficient condition for existence and uniqueness of a solution to this problem is that \"M\" be symmetric positive-definite. If \"M\" is such that have a solution for every \"q\", then \"M\" is a Q-matrix. If \"M\" is such that have a unique solution for every \"q\", then \"M\" is a P-matrix. Both of these characterizations are sufficient and necessary.\n\nThe vector \"w\" is a slack variable, and so is generally discarded after \"z\" is found. As such, the problem can also be formulated as:\n\n\nFinding a solution to the linear complementarity problem is associated with minimizing the quadratic function\n\nsubject to the constraints\n\nThese constraints ensure that \"f\" is always non-negative. The minimum of \"f\" is 0 at \"z\" if and only if \"z\" solves the linear complementarity problem.\n\nIf \"M\" is positive definite, any algorithm for solving (strictly) convex QPs can solve the LCP. Specially designed basis-exchange pivoting algorithms, such as Lemke's algorithm and a variant of the simplex algorithm of Dantzig have been used for decades. Besides having polynomial time complexity, interior-point methods are also effective in practice.\n\nAlso, a quadratic-programming problem stated as minimize formula_14 subject to formula_15 as well as formula_16 with \"Q\" symmetric\n\nis the same as solving the LCP with\n\nThis is because the Karush–Kuhn–Tucker conditions of the QP problem can be written as:\n\nwith \"v\" the Lagrange multipliers on the non-negativity constraints, \"λ\" the multipliers on the inequality constraints, and \"s\" the slack variables for the inequality constraints. The fourth condition derives from the complementarity of each group of variables with its set of KKT vectors (optimal Lagrange multipliers) being . In that case,\n\nIf the non-negativity constraint on the \"x\" is relaxed, the dimensionality of the LCP problem can be reduced to the number of the inequalities, as long as \"Q\" is non-singular (which is guaranteed if it is positive definite). The multipliers \"v\" are no longer present, and the first KKT conditions can be rewritten as:\n\nor:\n\npre-multiplying the two sides by \"A\" and subtracting \"b\" we obtain:\n\nThe left side, due to the second KKT condition, is \"s\". Substituting and reordering:\n\nCalling now\n\nwe have an LCP, due to the relation of complementarity between the slack variables \"s\" and their Lagrange multipliers \"λ\". Once we solve it, we may obtain the value of \"x\" from \"λ\" through the first KKT condition.\n\nFinally, it is also possible to handle additional equality constraints:\n\nThis introduces a vector of Lagrange multipliers \"μ\", with the same dimension as formula_26.\n\nIt is easy to verify that the \"M\" and \"Q\" for the LCP system formula_27 are now expressed as:\n\nFrom \"λ\" we can now recover the values of both \"x\" and the Lagrange multiplier of equalities \"μ\":\n\nIn fact, most QP solvers work on the LCP formulation, including the interior point method, principal / complementarity pivoting, and active set methods. LCP problems can be solved also by the criss-cross algorithm, conversely, for linear complementarity problems, the criss-cross algorithm terminates finitely only if the matrix is a sufficient matrix. A sufficient matrix is a generalization both of a positive-definite matrix and of a P-matrix, whose principal minors are each positive.\nSuch LCPs can be solved when they are formulated abstractly using oriented-matroid theory.\n\n\n\n\n"}
{"id": "47857242", "url": "https://en.wikipedia.org/wiki?curid=47857242", "title": "Logic of graphs", "text": "Logic of graphs\n\nIn the mathematical fields of graph theory and finite model theory, the logic of graphs deals with formal specifications of graph properties using logical formulas. There are several variations in the types of logical operation that can be used in these formulas. The first order logic of graphs concerns formulas in which the variables and predicates concern individual vertices and edges of a graph, while monadic second order graph logic allows quantification over sets of vertices or edges.\n\nA sentence formula_1 may be true for some graphs, and false for others; a graph formula_2 is said to \"model\" formula_1, written formula_4, if formula_1 is true of the vertices and adjacency relation of formula_2. The algorithmic problem of model checking concerns testing whether a given graph models a given sentence. The algorithmic problem of satisfiability concerns testing whether there exists a graph that models a given sentence.\nAlthough both model checking and satisfiability are hard in general, several major algorithmic meta-theorems show that properties expressed in this way can be tested efficiently for important classes of graphs.\n\nOther topics of research in the logic of graphs include investigations of the probability that a random graph has a property specified within a particular type of logic, and methods for data compression based on finding logical formulae that are modeled by a unique graph.\n\nIn the first-order logic of graphs, a graph property is expressed as a quantified logical formula whose variables represent graph vertices, with predicates for equality and adjacency testing.\n\nFor instance, the condition that a graph does not have any isolated vertices may be expressed by the sentence\nwhere the formula_8 symbol indicates the adjacency relation between two vertices. This sentence can be interpreted as meaning that for every vertex formula_9 there is another vertex formula_10 that is adjacent to formula_9.\n\nThe subgraph isomorphism problem for a fixed subgraph \"H\" asks whether \"H\" appears as a subgraph of a larger graph \"G\". It may be expressed by a sentence that states the existence of vertices (one for each vertex of \"H\") such that, for each edge of \"H\", the corresponding pair of vertices are adjacent. As a special case the clique problem (for a fixed clique size) may be expressed by a sentence that states the existence of a number of vertices equal to the clique size all of which are adjacent.\n\nFor simple undirected graphs, the first order theory of graphs includes the axioms\nOther types of graphs, such as directed graphs, may involve different axioms, and logical formulations of multigraph properties require having separate variables for vertices and edges.\n\n proved a zero–one law for first-order graph logic using the compactness theorem. According to Fagin's result, every first-order sentence is either almost always true or almost always false for random graphs in the Erdős–Rényi model. That is, let be a fixed first-order sentence, and choose a random -vertex graph uniformly at random among all graphs on a set of labeled vertices. Then in the limit as tends to infinity the probability that models will tend either to zero or to one:\nMoreover, there is a specific infinite graph, the Rado graph , such that the sentences modeled by the Rado graph are exactly the ones for which the probability of being modeled by a random finite graph tends to one: \nFor random graphs in which each edge is included independently of the others with a fixed probability, the same result is true, with the same sentences having probabilities tending to zero or to one.\n\nThe computational complexity of determining whether a given sentence has probability tending to zero or to one is high: the problem is PSPACE-complete.\nIf a first order graph property has probability tending to one on random graphs, then it is possible to list all the \"n\"-vertex graphs that model the property, with polynomial delay (as a function of ) per graph.\n\nA similar analysis can be performed for non-uniform random graphs, where the probability of including an edge is a function of the number of vertices, and where the decision to include or exclude an edge is made independently with equal probability for all edges. However, for these graphs the situation is more complicated.\nIn this case, a first-order property may have one or more thresholds, such that when the edge inclusion probability is bounded away from the threshold then the probability of having the given property tends to zero or one. These thresholds can never be an irrational power of , so random graphs where the edge inclusion probability is an irrational power obey a zero-one law analogous to the one for uniformly random graphs. A similar zero-one law holds for very sparse random graphs that have an edge inclusion probability of with , as long as is not a superparticular ratio. If is superparticular, the probability of having a given property may tend to a limit that is not zero or one, but this limit can be calculated efficiently. There exist first-order sentences that have infinitely many thresholds.\n\nIf a first-order sentence includes \"k\" distinct variables, then the property it describes can be tested in graphs of \"n\" vertices by examining all \"k\"-tuples of vertices; however, this brute force search algorithm is not particularly efficient, taking time \"O\"(\"n\").\nThe problem of checking whether a graph models a given first-order sentence includes as special cases the subgraph isomorphism problem (in which the sentence describes the graphs that contain a fixed subgraph) and the clique problem (in which the sentence describes graphs that contain complete subgraphs of a fixed size).\nThe clique problem is hard for W(1), the first level of a hierarchy of hard problems from the point of view of parameterized complexity. Therefore, it is unlikely to have a fixed-parameter tractable algorithm, one whose running time takes the form \"O\"(\"f\"(\"k\") \"n\") for a function \"f\" and constant \"c\" that are independent of \"k\" and \"c\".\nMore strongly, if the exponential time hypothesis is true, then clique-finding and first-order model checking would necessarily take time proportional to a power of \"n\" whose exponent is proportional to \"k\".\n\nOn restricted classes of graphs, model checking of first-order sentences can be much more efficient. In particular, every graph property expressible as a first-order sentence can be tested in linear time for the graphs of bounded expansion. These are the graphs in which all shallow minors are sparse graphs, with a ratio of edges to vertices bounded by a function of the depth of the minor. Even more generally, first-order model checking can be performed in near-linear time for nowhere-dense graphs, classes of graphs for which, at each possible depth, there is at least one forbidden shallow minor. Conversely, if model checking is fixed-parameter tractable for any hereditary family of graphs, that family must be nowhere-dense.\n\nA first order sentence \"S\" in the logic of graphs is said to define a graph \"G\" if \"G\" is the only graph that models \"S\". Every graph may be defined by at least one sentence; for instance, one can define an \"n\"-vertex graph \"G\" by a sentence with \"n\" + 1 variables, one for each vertex of the graph, and one more to state the condition that there is no vertex other than the \"n\" vertices of the graph. Additional clauses of the sentence can be used to ensure that no two vertex variables are equal, that each edge of \"G\" is present, and no edge exists between a pair of non-adjacent vertices of \"G\". However, for some graphs there exist significantly shorter formulas that define the graph.\n\nSeveral different graph invariants can be defined from the simplest sentences (with different measures of simplicity) that define a given graph. In particular the \"logical depth\" of a graph is defined to be the minimum level of nesting of quantifiers (the quantifier rank) in a sentence defining the graph. The sentence outlined above nests the quantifiers for all of its variables, so it has logical depth \"n\" + 1. The \"logical width\" of a graph is the minimum number of variables in a sentence that defines it. In the sentence outlined above, this number of variables is again \"n\" + 1. Both the logical depth and logical width can be bounded in terms of the treewidth of the given graph. The logical length, analogously, is defined as the length of the shortest formula describing the graph. The sentence described above has length proportional to the square of the number of vertices, but it is possible to define any graph by a formula with length proportional to its number of edges.\n\nAll trees, and most graphs, can be described by first order sentences with only two variables, but extended by counting predicates. For graphs that can be described by sentences in this logic with a fixed constant number of variables, it is possible to find a graph canonization in polynomial time (with the exponent of the polynomial equal to the number of variables). By comparing canonizations, it is possible to solve the graph isomorphism problem for these graphs in polynomial time.\n\nIt is undecidable whether a given first-order sentence can be realized by a finite undirected graph.\n\nThere exist first-order sentences that are modeled by infinite graphs but not by any finite graph. For instance, the property of having exactly one vertex of degree one, with all other vertices having degree exactly two, can be expressed by a first order sentence. It is modeled by an infinite ray, but violates Euler's handshaking lemma for finite graphs. However, it follows from the negative solution to the Entscheidungsproblem (by Alonzo Church and Alan Turing in the 1930s) that satisfiability of first-order sentences for graphs that are not constrained to be finite remains undecidable.\n\nIn the monadic second-order logic of graphs, the variables represent objects of up to four types: vertices, edges, sets of vertices, and sets of edges. There are two main variations of monadic second-order graph logic: MSO in which only vertex and vertex set variables are allowed, and MSO in which all four types of variables are allowed. The predicates on these variables include equality testing, membership testing, and either vertex-edge incidence (if both vertex and edge variables are allowed) or adjacency between pairs of vertices (if only vertex variables are allowed). Additional variations in the definition allow additional predicates such as modular counting predicates.\n\nAs an example, the connectivity of an undirected graph can be expressed in MSO as the statement that, for every partition of the vertices into two nonempty subsets, there exists an edge from one subset to the other. A partition of the vertices can be described by the subset \"S\" of vertices on one side of the partition, and each such subset should either describe a trivial partition (one in which one or the other side is empty) or be crossed by an edge. That is, a graph is connected when it models the MSO formula\nHowever, connectivity cannot be expressed in first-order graph logic, nor can it be expressed in existential MSO (the fragment of MSO in which all set quantifiers are existential and occur at the beginning of the sentence) nor even existential MSO.\n\nHamiltonicity can be expressed in MSO by the existence of a set of edges that forms a connected 2-regular graph on all the vertices, with connectivity expressed as above and 2-regularity expressed as the incidence of two but not three distinct edges at each vertex. However, Hamiltonicity is not expressible in MSO, because MSO is not capable of distinguishing complete bipartite graphs with equal numbers of vertices on each side of the bipartition (which are Hamiltonian) from unbalanced complete bipartite graphs (which are not).\n\nAlthough not part of the definition of MSO, orientations of undirected graphs can be represented by a technique involving Trémaux trees. This allows other graph properties involving orientations to be expressed as well.\n\nAccording to Courcelle's theorem, every fixed MSO property can be tested in linear time on graphs of bounded treewidth, and every fixed MSO property can be tested in linear time on graphs of bounded clique-width. The version of this result for graphs of bounded treewidth can also be implemented in logarithmic space. Applications of this result include a fixed-parameter tractable algorithm for computing the crossing number of a graph.\n\nThe satisfiability problem for a formula of monadic second-order logic is the problem of determining whether there exists at least one graph (possibly within a restricted family of graphs) for which the formula is true. For arbitrary graph families, and arbitrary formulas, this problem is undecidable. However, satisfiability of MSO formulas is decidable for the graphs of bounded treewidth, and satisfiability of MSO formulas is decidable for graphs of bounded clique-width. The proof involves using Courcelle's theorem to build an automaton that can test the property, and then examining the automaton to determine whether there is any graph it can accept.\n\nAs a partial converse, proved that, whenever a family of graphs has a decidable MSO satisfiability problem, the family must have bounded treewidth. The proof is based on a theorem of Robertson and Seymour that the families of graphs with unbounded treewidth have arbitrarily large grid minors. Seese also conjectured that every family of graphs with a decidable MSO satisfiability problem must have bounded clique-width; this has not been proven, but a weakening of the conjecture that extends MSO with modular counting predicates is true.\n\n"}
{"id": "734635", "url": "https://en.wikipedia.org/wiki?curid=734635", "title": "Lucas sequence", "text": "Lucas sequence\n\nIn mathematics, the Lucas sequences formula_1 and formula_2 are certain constant-recursive integer sequences that satisfy the recurrence relation\n\nwhere formula_4 and formula_5 are fixed integers. Any sequence satisfying this recurrence relation can be represented as a linear combination of the Lucas sequences formula_6 and formula_2.\n\nMore generally, Lucas sequences formula_6 and formula_2 represent sequences of polynomials in formula_4 and formula_5 with integer coefficients.\n\nFamous examples of Lucas sequences include the Fibonacci numbers, Mersenne numbers, Pell numbers, Lucas numbers, Jacobsthal numbers, and a superset of Fermat numbers. Lucas sequences are named after the French mathematician Édouard Lucas.\n\nGiven two integer parameters \"P\" and \"Q\", the Lucas sequences of the first kind \"U\"(\"P\",\"Q\") and of the second kind \"V\"(\"P\",\"Q\") are defined by the recurrence relations:\n\nand\n\nIt is not hard to show that for formula_14,\n\nInitial terms of Lucas sequences \"U\"(\"P\",\"Q\") and \"V\"(\"P\",\"Q\") are given in the table:\n\nThe characteristic equation of the recurrence relation for Lucas sequences formula_1 and formula_18 is:\n\nIt has the discriminant formula_20 and the roots:\n\nThus:\n\nNote that the sequence formula_25 and the sequence formula_26 also satisfy the recurrence relation. However these might not be integer sequences.\n\nWhen formula_27, \"a\" and \"b\" are distinct and one quickly verifies that\n\nIt follows that the terms of Lucas sequences can be expressed in terms of \"a\" and \"b\" as follows\n\nThe case formula_32 occurs exactly when formula_33 for some integer \"S\" so that formula_34. In this case one easily finds that\n\nThe ordinary generating functions are\n\nIf the Lucas sequences formula_6 and formula_2 have\ndiscriminant formula_41, then the sequences based on formula_42 and formula_43 where\nhave the same discriminant: formula_46.\n\nWhen formula_47, the Lucas sequences formula_6 and formula_2 satisfy certain Pell equations:\n\nThe terms of Lucas sequences satisfy relations that are generalizations of those between Fibonacci numbers formula_53 and Lucas numbers formula_54. For example:\n\nAmong the consequences is that formula_56 is a multiple of formula_57, i.e., the sequence formula_58\nis a divisibility sequence. This implies, in particular, that formula_1 can be prime only when \"n\" is prime.\nAnother consequence is an analog of exponentiation by squaring that allows fast computation of formula_1 for large values of \"n\". \nMoreover, if formula_61, then formula_58 is a strong divisibility sequence.\n\nOther divisibility properties are as follows:\n\nThe last fact generalizes Fermat's little theorem. These facts are used in the Lucas–Lehmer primality test.\nThe converse of the last fact does not hold, as the converse of Fermat's little theorem does not hold. There exists a composite \"n\" relatively prime to \"D\" and dividing formula_81, where formula_84. Such a composite is called Lucas pseudoprime.\n\nA prime factor of a term in a Lucas sequence that does not divide any earlier term in the sequence is called primitive.\nCarmichael's theorem states that all but finitely many of the terms in a Lucas sequence have a primitive prime factor. Indeed, Carmichael (1913) showed that if \"D\" is positive and \"n\" is not 1, 2 or 6, then formula_66 has a primitive prime factor. In the case \"D\" is negative, a deep result of Bilu, Hanrot, Voutier and Mignotte shows that if \"n\" > 30, then formula_66 has a primitive prime factor and determines all cases formula_66 has no primitive prime factor.\n\nThe Lucas sequences for some values of \"P\" and \"Q\" have specific names:\n\nSome Lucas sequences have entries in the On-Line Encyclopedia of Integer Sequences:\n\n\n\n"}
{"id": "22008131", "url": "https://en.wikipedia.org/wiki?curid=22008131", "title": "Math 55", "text": "Math 55\n\nMath 55 is a two-semester long first-year undergraduate mathematics course at Harvard University, founded by Lynn Loomis and Shlomo Sternberg. The official titles of the course are Honors Abstract Algebra (Math 55a) and Honors Real and Complex Analysis (Math 55b). Previously, the official title was Honors Advanced Calculus and Linear Algebra.\n\nThe Harvard University Department of Mathematics describes Math 55 as \"probably the most difficult undergraduate math class in the country\". This claim is debatable, with critics often citing the generous policy of the final exam being take home. Formerly, students would begin the year in Math 25 (which was created in 1983 as a lower-level Math 55) and, after three weeks of point-set topology and special topics (for instance, in 1994, \"p\"-adic analysis was taught by Wilfried Schmid), students would take a quiz. As of 2012, students may choose to enroll in either Math 25 or Math 55 but are advised to \"shop\" both courses and have five weeks to decide on one. Depending on the professor teaching the class, the diagnostic exam may still be given after three weeks to help students with their decision.\n\nIn 1994, 89 students took the test given after three weeks: students scoring more than 50% on the quiz could enroll in Wilfried Schmid's Math 55 (15 students), students scoring between 10 and 50% could stay in Benedict Gross's Math 25 (55 students), and students scoring less than 10% were advised to enroll in a course such as Math 21, multivariate calculus (19 students).\n\nIn 1970, Math 55 covered almost four years worth of department coursework in two semesters, and subsequently, it drew only the most diligent of undergraduates. Of the 75 students who enrolled in the 1970 offering, by course end, only 20 remained due to the advanced nature of the material and time-constraints under which students were given to work. David Harbater, a University of Pennsylvania mathematics professor/researcher, and survivor of the 1974 Math 55 section at Cambridge, recalled of his experience, \"Seventy [students] started it, 20 finished it, and only 10 understood it.\" Scott D. Kominers, familiar with the stated attrition rates for the course, decided to keep an informal log of his journey through the 2009 section: \"...we had 51 students the first day, 31 students the second day, 24 for the next four days, 23 for two more weeks, and then 21 for the rest of the first semester after the fifth Monday.\" (The beginning of the fifth week being the drop-deadline for students to decide whether to remain in Math 55, or transfer to Math 25 (Linear Algebra and Real Analysis I & II)).\n\nThrough 2006, the instructor had broad latitude in choosing the content of the course. Though Math 55 bore the official title \"Honors Advanced Calculus and Linear Algebra\", advanced topics in complex analysis, point set topology, group theory, and/or differential geometry could be covered in depth at the discretion of the instructor, in addition to single and multivariable real analysis and abstract linear algebra. In 1970, for example, students studied the differential geometry of Banach manifolds in the second semester of Math 55. In contrast, Math 25, entitled \"Honors Multivariable Calculus and Linear Algebra\", tended to be more narrowly focused, usually covering real analysis, together with the relevant theory of metric spaces and (multi)linear maps. These topics typically culminated in the proof of the generalized Stokes' theorem, though, time permitting, other relevant topics (e.g., category theory, de Rham cohomology) might also be covered. Although both courses presented calculus from a rigorous point of view and emphasized theory and proof writing, Math 55 was generally faster paced, more abstract, and demanded a higher level of mathematical sophistication. \n\nLoomis and Sternberg's textbook \"Advanced Calculus\", an abstract treatment of calculus in the setting of normed vector spaces and on differentiable manifolds, was tailored to the authors' Math 55 syllabus and served for many years as an assigned textbook. Over the years, instructors for Math 55 and Math 25 have also selected Rudin's \"Principles of Mathematical Analysis\", Spivak's \"Calculus on Manifolds\", Axler's \"Linear Algebra Done Right\", and Halmos's \"Finite-Dimensional Vector Spaces\" as textbooks or references.\n\nFrom 2007 onwards, the scope of the course (along with that of Math 25) was changed to more strictly cover the contents of four semester-long courses in two semesters: Math 25a (linear algebra) and Math 122 (group theory) in Math 55a; and Math 25b (calculus, real analysis) and Math 113 (complex analysis) in Math 55b. The name was also changed to \"Honors Abstract Algebra\" (Math 55a) and \"Honors Real and Complex Analysis\" (Math 55b). Fluency in formulating and writing mathematical proofs is listed as a course prerequisite for Math 55, while such experience is considered \"helpful\" but not required for Math 25. In practice, students of Math 55 have usually had extensive experience in proof writing and abstract mathematics, and many are winners of prestigious national or international mathematical olympiads (e.g., USAMO or IMO), while typical students of Math 25 have also had previous exposure to proof writing through mathematical contests or university level proof-based courses.\n\nProblem sets are expected to take from 24 to 60 hours per week to complete, although some claim that it is closer to 20 hours. Of those students who could handle the workload, some became math or physics professors, including members of the Harvard Math Department such as Benedict Gross and Joe Harris; also, Harvard physics professor Lisa Randall '84 and Harvard economics professor Andrei Shleifer '82. Although a 2006 \"Harvard Crimson\" article alleged that only 17 women completed the class between 1990 and 2006, in fact 39 women completed 55a (the first of the two semesters), and 26 completed 55b. Math 25 has more women: in 1994–95, Math 55 had no women, while Math 25 had about 10 women in the 55 person course.\n\nThere are also Math 55 alumni who went on to be professors in other universities. These include Fields-medalist Manjul Bhargava, who is now a professor at Princeton University, as well as Kiran Kedlaya, now at the University of California, San Diego.\n\nIn addition to these professors, past students of Math 55 include Bill Gates and Richard Stallman.\n\nDemographics of students taking this course over the years has been used to study causes of gender and race differences in the fields of mathematics and technology. \n\nMath 55, along with several other high-level mathematics courses, were brought up by Dr. Spencer Reid in a 2015 episode of \"Criminal Minds\" entitled \"Mr. Scratch.\"\n\n"}
{"id": "1045553", "url": "https://en.wikipedia.org/wiki?curid=1045553", "title": "Multinomial distribution", "text": "Multinomial distribution\n\n(X_i) = n p_i (1-p_i)</math><br>formula_1|\nIn probability theory, the multinomial distribution is a generalization of the binomial distribution. For example, it models the probability of counts for rolling a \"k\"-sided die \"n\" times. For \"n\" independent trials each of which leads to a success for exactly one of \"k\" categories, with each category having a given fixed success probability, the multinomial distribution gives the probability of any particular combination of numbers of successes for the various categories.\n\nWhen \"k\" is 2 and \"n\" is 1, the multinomial distribution is the Bernoulli distribution. When \"k\" is 2 and \"n\" is bigger than 1, it is the binomial distribution. When k is bigger than 2 and \"n\" is 1, it is the categorical distribution.\n\nThe Bernoulli distribution models the outcome of a single Bernoulli trial. In other words, it models whether flipping a (possibly biased) coin one time will result in either a success (obtaining a head) or failure (obtaining a tail). The binomial distribution generalizes this to the number of heads from performing \"n\" independent flips (Bernoulli trials) of the same coin. The multinomial distribution models the outcome of \"n\" experiments, where the outcome of each trial has a categorical distribution, such as rolling a \"k\"-sided die \"n\" times.\n\nLet \"k\" be a fixed finite number. Mathematically, we have \"k\" possible mutually exclusive outcomes, with corresponding probabilities \"p\", ..., \"p\", and \"n\" independent trials. Since the \"k\" outcomes are mutually exclusive and one must occur we have \"p\" ≥ 0 for \"i\" = 1, ..., \"k\" and formula_8. Then if the random variables \"X\" indicate the number of times outcome number \"i\" is observed over the \"n\" trials, the vector \"X\" = (\"X\", ..., \"X\") follows a multinomial distribution with parameters \"n\" and p, where p = (\"p\", ..., \"p\"). While the trials are independent, their outcomes \"X\" are dependent because they must be summed to n.\n\nIn some fields such as natural language processing, categorical and multinomial distributions are synonymous and it is common to speak of a multinomial distribution when a categorical distribution is actually meant. This stems from the fact that it is sometimes convenient to express the outcome of a categorical distribution as a \"1-of-K\" vector (a vector with one element containing a 1 and all other elements containing a 0) rather than as an integer in the range formula_9; in this form, a categorical distribution is equivalent to a multinomial distribution over a single trial.\n\nSuppose one does an experiment of extracting \"n\" balls of \"k\" different colors from a bag, replacing the extracted ball after each draw. Balls from the same color are equivalent. Denote the variable which is the number of extracted balls of color \"i\" (\"i\" = 1, ..., \"k\") as \"X\", and denote as \"p\" the probability that a given extraction will be in color \"i\". The probability mass function of this multinomial distribution is:\n\nfor non-negative integers \"x\", ..., \"x\".\n\nThe probability mass function can be expressed using the gamma function as:\n\nThis form shows its resemblance to the Dirichlet distribution which is its conjugate prior.\n\nJust like one can interpret the binomial distribution as (normalized) one-dimensional (1D) slices of Pascal's triangle, so too can one interpret the multinomial distribution as 2D (triangular) slices of Pascal's pyramid, or 3D/4D/+ (pyramid-shaped) slices of higher-dimensional analogs of Pascal's triangle. This reveals an interpretation of the range of the distribution: discretized equilaterial \"pyramids\" in arbitrary dimension—i.e. a simplex with a grid.\n\nSimilarly, just like one can interpret the binomial distribution as the polynomial coefficients of formula_12 when expanded, one can interpret the multinomial distribution as the coefficients of formula_13 when expanded. (Note that just like the binomial distribution, the coefficients must sum to 1.) This is the origin of the name \"\"multinomial\" distribution\".\n\nThe expected number of times the outcome \"i\" was observed over \"n\" trials is\n\nThe covariance matrix is as follows. Each diagonal entry is the variance of a binomially distributed random variable, and is therefore\n\nThe off-diagonal entries are the covariances:\n\nfor \"i\", \"j\" distinct.\n\nAll covariances are negative because for fixed \"n\", an increase in one component of a multinomial vector requires a decrease in another component.\n\nWhen these expressions are combined into a matrix with \"i, j\" element formula_17 the result is a \"k\" × \"k\" positive-semidefinite covariance matrix of rank \"k\" − 1. In the special case where \"k\" = \"n\" and where the \"p\" are all equal, the covariance matrix is the centering matrix.\n\nThe entries of the corresponding correlation matrix are\n\nNote that the sample size drops out of this expression.\n\nEach of the \"k\" components separately has a binomial distribution with parameters \"n\" and \"p\", for the appropriate value of the subscript \"i\".\n\nThe support of the multinomial distribution is the set\n\nIts number of elements is\n\nIn matrix notation, \n\nand \n\nwith = the row vector transpose of the column vector .\n\nSuppose that in a three-way election for a large country, candidate A received 20% of the votes, candidate B received 30% of the votes, and candidate C received 50% of the votes. If six voters are selected randomly, what is the probability that there will be exactly one supporter for candidate A, two supporters for candidate B and three supporters for candidate C in the sample?\n\n\"Note: Since we’re assuming that the voting population is large, it is reasonable and permissible to think of the probabilities as unchanging once a voter is selected for the sample. Technically speaking this is sampling without replacement, so the correct distribution is the multivariate hypergeometric distribution, but the distributions converge as the population grows large.\"\n\nFirst, reorder the parameters formula_25 such that they are sorted in descending order (this is only to speed up computation and not strictly necessary). Now, for each trial, draw an auxiliary variable \"X\" from a uniform (0, 1) distribution. The resulting outcome is the component\n\nVarious methods may be used to simulate a multinomial distribution. A very simple one is to use a random number generator to generate numbers between 0 and 1. First, we divide the interval from 0 to 1 in \"k\" subintervals equal in size to the probabilities of the \"k\" categories. Then, we generate a random number for each of n trials and use a logical test to classify the virtual measure or observation in one of the categories.\n\nExample\n\nIf we have :\n\nThen, with a software like Excel, we may use the following recipe:\n\nAfter that, we will use functions such as SumIf to accumulate the observed results by category and to calculate the estimated covariance matrix for each simulated sample.\n\nAnother way is to use a discrete random number generator. In that case, the categories must be labeled or relabeled with numeric values.\n\nIn the two cases, the result is a multinomial distribution with \"k\" categories. This is equivalent, with a continuous random distribution, to simulate \"k\" independent standardized normal distributions, or a multinormal distribution N(0,I) having \"k\" components identically distributed and statistically independent.\n\nSince the counts of all categories have to sum to the number of trials, the counts of the categories are always negatively correlated.\n\n\n"}
{"id": "6176811", "url": "https://en.wikipedia.org/wiki?curid=6176811", "title": "Newman–Penrose formalism", "text": "Newman–Penrose formalism\n\nThe Newman–Penrose (NP) formalism is a set of notation developed by Ezra T. Newman and Roger Penrose for general relativity (GR). Their notation is an effort to treat general relativity in terms of spinor notation, which introduces complex forms of the usual variables used in GR. The NP formalism is itself a special case of the tetrad formalism, where the tensors of the theory are projected onto a complete vector basis at each point in spacetime. Usually this vector basis is chosen to reflect some symmetry of the space-time, leading to simplified expressions for physical observables. In the case of the NP formalism, the vector basis chosen is a null tetrad: a set of four null vectors—two real, and a complex-conjugate pair. The two real members asymptotically point radially inward and radially outward, and the formalism is well adapted to treatment of the propagation of radiation in curved spacetime. The most often-used variables in the formalism are the Weyl scalars, derived from the Weyl tensor. In particular, it can be shown that one of these scalars--formula_1 in the appropriate frame—encodes the outgoing gravitational radiation of an asymptotically flat system.\n\nNewman and Penrose introduced the following functions as primary quantities using this tetrad:\n\nIn many situations—especially algebraically special spacetimes or vacuum spacetimes—the Newman–Penrose formalism simplifies dramatically, as many of the functions go to zero. This simplification allows for various theorems to be proven more easily than using the standard form of Einstein's equations.\n\nIn this article, we will only employ the tensorial rather than spinorial version of NP formalism, because the former is easier to understand and more popular in relevant papers. One can refer to ref. for a unified formulation of these two versions.\n\nThe formalism is developed for four-dimensional spacetime, with a Lorentzian-signature metric. At each point, a tetrad (set of four vectors) is introduced. The first two vectors, formula_6 and formula_7 are just a pair of standard (real) null vectors such that formula_8. For example, we can think in terms of spherical coordinates, and take formula_9 to be the outgoing null vector, and formula_10 to be the ingoing null vector. A complex null vector is then constructed by combining a pair of real, orthogonal unit space-like vectors. In the case of spherical coordinates, the standard choice is\nThe complex conjugate of this vector then forms the fourth element of the tetrad.\n\nTwo sets of signature and normalization conventions are in use for NP formalism: formula_12 and formula_13. The former is the original one that was adopted when NP formalism was developed and has been widely used in black-hole physics, gravitational waves and various other areas in general relativity. However, it is the latter convention that is usually employed in contemporary study of black holes from quasilocal perspectives (such as isolated horizons and dynamical horizons). In this article, we will utilize formula_13 for a systematic review of the NP formalism (see also refs.).\n\nIt's important to note that, when switching from formula_15 to formula_16, definitions of the spin coefficients, Weyl-NP scalars formula_17 and Ricci-NP scalars formula_18 need to change their signs; this way, the Einstein-Maxwell equations can be left unchanged.\n\nIn NP formalism, the complex null tetrad contains two real null (co)vectors formula_19 and two complex null (co)vectors formula_20. Being \"null\" (co)vectors, \"self\"-normalization of formula_19 are naturally vanishes,\nformula_22,\n\nso the following two pairs of \"cross\"-normalization are adopted\nformula_23\n\nwhile contractions between the two pairs are also vanishing,\nformula_24.\n\nHere the indices can be raised and lowered by the global metric formula_25 which in turn can be obtained via\nformula_26\n\nFirst of all, there are four directional covariant derivatives along with each tetrad vector,\nformula_27\n\nwhich are reduced to formula_28 when acting on \"scalar\" functions.\n\nIn NP formalism, instead of using index notations as in orthogonal tetrads, each Ricci rotation coefficient formula_29 in the null tetrad is assigned a lower-case Greek letter, which constitute the 12 complex \"spin coefficients\" (in three groups),\nformula_30\nformula_31\n\nformula_32\nformula_33\n\nformula_34\nformula_35\nformula_36\nformula_37\n\nSpin coefficients are the primary quantities in NP formalism, with which all other NP quantities (as defined below) could be calculated indirectly using the NP field equations. Thus, NP formalism is sometimes referred to as \"spin-coefficient formalism\" as well.\n\nApply the directional derivative operators to tetrad vectors and one could obtain the \"transportation/propagation equations\":\nformula_38 formula_39 formula_40 formula_41\n\nformula_42 formula_43 formula_44 formula_45\n\nformula_46 formula_47 formula_48 formula_49\n\nformula_50 formula_51 formula_52 formula_53\n\nThe metric-compatibility or torsion-freeness of the covariant derivative is recast into the \"commutators of the directional derivatives\",\nformula_54 formula_55 formula_56 formula_57\n\nwhich imply that\nformula_58 \nformula_59 \nformula_60 \nformula_61\n\nNote: (i) The above equations can be regarded either as implications of the commutators or combinations of the transportation equations; (ii) In these implied equations, the vectors formula_62 can be replaced by the covectors and the equations still hold.\n\nThe 10 independent components of the Weyl tensor can be encoded into 5 complex Weyl-NP scalars,\nformula_63\nformula_64\n\nThe 10 independent components of the Ricci tensor are encoded into 4 \"real\" scalars formula_65, formula_66, formula_67, formula_68 and 3 \"complex\" scalars formula_69 (with their complex conjugates),\nformula_70\n\nformula_71 formula_72 formula_73\n\nIn these definitions, formula_74 could be replaced by its trace-free part formula_75 or by the Einstein tensor formula_76 because of the normalization relations. Also, formula_66 is reduced to formula_78 for electrovacuum (formula_79).\n\nIn a complex null tetrad, Ricci identities give rise to the following NP field equations connecting spin coefficients, Weyl-NP and Ricci-NP scalars (recall that in an orthogonal tetrad, Ricci rotation coefficients would respect Cartan's first and second structure equations),\nformula_80 formula_81 formula_82\nformula_83 formula_84 formula_85 formula_86 formula_87 formula_88 formula_89 formula_90 formula_91 formula_92\nformula_93 formula_94 formula_95 formula_96 formula_97\n\nAlso, the Weyl-NP scalars formula_17 and the Ricci-NP scalars formula_18 can be calculated indirectly from the above NP field equations after obtaining the spin coefficients rather than directly using their definitions.\n\nThe six independent components of the Faraday-Maxwell 2-form (i.e. the electromagnetic field strength tensor) formula_100 can be encoded into three complex Maxwell-NP scalars\nformula_101\n\nand therefore the eight real Maxwell equations formula_102 and formula_103 (as formula_104) can be transformed into four complex equations,\nformula_105 formula_106 formula_107 formula_108\nwith the Ricci-NP scalars formula_18 related to Maxwell scalars by\nformula_110\n\nIt is worthwhile to point out that, the supplementary equation formula_111 is only valid for electromagnetic fields; for example, in the case of Yang-Mills fields there will be formula_112 where formula_113 are Yang-Mills-NP scalars.\n\nTo sum up, the aforementioned transportation equations, NP field equations and Maxwell-NP equations together constitute the Einstein-Maxwell equations in Newman–Penrose formalism.\n\nThe Weyl scalar formula_1 was defined by Newman & Penrose as\n(note, however, that the overall sign is arbitrary, and that Newman & Penrose worked with a \"timelike\" metric signature of formula_116).\nIn empty space, the Einstein Field Equations reduce to formula_117. From the definition of the Weyl tensor, we see that this means that it equals the Riemann tensor, formula_118. We can make the standard choice for the tetrad at infinity:\n\nIn transverse-traceless gauge, a simple calculation shows that linearized gravitational waves are related to components of the Riemann tensor as\nassuming propagation in the formula_124 direction. Combining these, and using the definition of formula_1 above, we can write\nFar from a source, in nearly flat space, the fields formula_127 and formula_128 encode everything about gravitational radiation propagating in a given direction. Thus, we see that formula_1 encodes in a single complex field everything about (outgoing) gravitational waves.\n\nUsing the wave-generation formalism summarised by Thorne, we can write the radiation field quite compactly in terms of the mass multipole, current multipole, and spin-weighted spherical harmonics:\nHere, prefixed superscripts indicate time derivatives. That is, we define\nThe components formula_132 and formula_133 are the mass and current multipoles, respectively. formula_134 is the spin-weight -2 spherical harmonic.\n\n\n\n"}
{"id": "355814", "url": "https://en.wikipedia.org/wiki?curid=355814", "title": "Outline of discrete mathematics", "text": "Outline of discrete mathematics\n\nDiscrete mathematics is the study of mathematical structures that are fundamentally discrete rather than continuous. In contrast to real numbers that have the property of varying \"smoothly\", the objects studied in discrete mathematics – such as integers, graphs, and statements in logic – do not vary smoothly in this way, but have distinct, separated values. Discrete mathematics therefore excludes topics in \"continuous mathematics\" such as calculus and analysis.\n\nIncluded below are many of the standard terms used routinely in university-level courses and in research papers. This is not, however, intended as a complete list of mathematical terms; just a selection of typical \"terms of art\" that may be encountered.\n\n\nFor further reading in discrete mathematics, beyond a basic level, see these pages. Many of these disciplines are closely related to computer science.\n\n\n\n\n\nElementary algebra\n\n\n\nCombinatorics\n\nProbability\n\n\n\n"}
{"id": "41199245", "url": "https://en.wikipedia.org/wiki?curid=41199245", "title": "Peter Roquette", "text": "Peter Roquette\n\nPeter Roquette (born October 8 1927 in Königsberg) is a German mathematician working in algebraic geometry, algebra, and number theory.\n\nRoquette studied in Erlangen, Berlin, and Hamburg. In 1951 he defended a dissertation at the University of Hamburg under Helmut Hasse, providing a new proof of the Riemann hypothesis for algebraic function fields over a finite field (the first proof was given by André Weil in 1940). In 1951/1952 he was an assistant at the Mathematical Research Institute at Oberwolfach and from 1952 to 1954 at the University of Munich. From 1954 to 1956 he worked at the Institute for Advanced Study in Princeton. In 1954 he was Privatdozent at Munich, and from 1956 to 1959 he worked in the same position at Hamburg. In 1959 he became an associate professor at the University of Saarbrucken and in the same year at the University of Tübingen. From 1967 he was professor at the Ruprecht-Karls-University of Heidelberg, where he retired in 1996.\n\nRoquette worked on number and function fields and especially local p-adic fields. He applied the methods of model theory (Nonstandard arithmetic) in number theory, joint with Abraham Robinson, with whom he worked on Mahler's theorem (on the finiteness of integral points on a curve of genus g> 0) using non-standard methods. He authored a number of works on the history of mathematics, in particular on the schools of Helmut Hasse and Emmy Noether. In 1975 Roquette was co-editor of the collected essays by Helmut Hasse.\n\nSince 1978 Roquette is member of the Heidelberg Academy of Sciences and since 1985, the German Academy of Sciences Leopoldina. He has an honorary doctorate from the University of Duisburg-Essen and is honorary member of the Mathematical Society of Hamburg. In 1958 he was an invited speaker at the International Congress of Mathematicians in Edinburgh (on the topic of Some fundamental theorems on abelian function fields).\n\nHis doctoral students include Gerhard Frey and .\n\n\n"}
{"id": "3881088", "url": "https://en.wikipedia.org/wiki?curid=3881088", "title": "Polynomial transformation", "text": "Polynomial transformation\n\nIn mathematics, a polynomial transformation consists of computing the polynomial whose roots are a given function of the roots of polynomial. Polynomial transformations such as Tschirnhaus transformations are often used to simplify the solution of algebraic equations.\n\nLet \nbe a polynomial, and \nbe its complex roots (not necessarily distinct).\n\nFor any constant , the polynomial whose roots are \nis \n\nIf the coefficients of are integers and the constant formula_5 is a rational number, the coefficients of may be not integer, but the polynomial has integers coefficients and has the same roots as .\n\nA special case is when formula_6 The resulting polynomial does not have any term in .\n\nLet \nbe a polynomial. The polynomial whose roots are the reciprocals of the roots of as roots is its reciprocal polynomial\n\nLet \nbe a polynomial, and be a non-zero constant. A polynomial whose roots are the product by of the roots of is \nThe factor appears here because, if and the coefficients of are integers or belong to some integral domain, the same is true for the coefficients of .\n\nIn the special case where formula_11, all coefficients of are multiple of , and formula_12 is a monic polynomial, whose coefficients belong to any integral domain containing and the coefficients of . This polynomial transformation is often used to reduce questions on algebraic numbers to questions on algebraic integers.\n\nCombining this with a translation of the roots by formula_13, allows to reduce any question on the roots of a polynomial, such as root-finding, to a similar question on a simpler polynomial, which is monic and does not have a term of degree . For examples of this, see Cubic function § Reduction to a depressed cubic or Quartic function § Converting to a depressed quartic.\n\nAll preceding examples are polynomial transformations by a rational function, also called Tschirnhaus transformations. Let \nbe a rational function, where and are coprime polynomials. The polynomial transformation of a polynomial by is the polynomial (defined up to the product by a non-zero constant) whose roots are the images by of the roots of .\n\nSuch a polynomial transformation may be computed as a resultant. In fact, the roots of the desired polynomial are exactly the complex numbers such that there is a complex number such one has simultaneously (if the coefficients of and are not real or complex numbers, \"complex number\" has to be replaced by \"element of an algebraically closed field containing the coefficients of the input polynomials\")\nThis is exactly the defining property of the resultant\n\nThis is generally difficult to compute this through a hand-written computation. However, as most computer algebra systems have a built-in function to compute resultants, it is straightforward to compute it with a computer.\n\nIf the polynomial is irreducible, then either the resulting polynomial is irreducible, or it is a power of an irreducible polynomial. Let formula_17 be a root of and consider , the field extension generated by formula_17. The former case means that formula_19 is a primitive element of , which has as minimal polynomial. In the latter case, formula_19 belongs to a subfield of and its minimal polynomial is the irreducible polynomial that has as power.\n\nPolynomial transformations have been applied to the simplification of polynomial equations for solution, where possible, by radicals. Descartes introduced the transformation of a polynomial of degree \"d\" which eliminates the term of degree \"d\"−1 by a translation of the roots. Such a polynomial is termed depressed. this already suffices to solve the quadratic by square roots. In the case of the cubic, Tschirnhaus transformations replace the variable by a quadratic function, thereby making it possible to eliminate two terms, and so can be used to eliminate the linear term in a depressed cubic to achieve the solution of the cubic by a combination of square and cube roots. The Bring–Jerrard transformation, which is quartic in the variable, brings a quintic into \"principal\" or Bring-Jerrard normal form with terms of degree 5,1 and zero.\n"}
{"id": "37066906", "url": "https://en.wikipedia.org/wiki?curid=37066906", "title": "Rational reconstruction (mathematics)", "text": "Rational reconstruction (mathematics)\n\nIn mathematics, rational reconstruction is a method that allows one to recover a rational number from its value modulo an integer. If a problem with a rational solution formula_1 is considered modulo a number \"m\", one will obtain the number formula_2. If |\"r\"| < \"N\" and 0 < \"s\" < \"D\" then \"r\" and \"s\" can be uniquely determined from \"n\" if \"m\" > 2\"ND\" using the Euclidean algorithm, as follows.\n\nOne puts formula_3 and formula_4. One then repeats the following steps until the first component of \"w\" becomes formula_5. Put formula_6, put \"z\" = \"v\" − \"qw\". The new \"v\" and \"w\" are then obtained by putting \"v\" = \"w\" and \"w\" = \"z\".\n\nThen with \"w\" such that formula_7, one makes the second component positive by putting \"w\" = −\"w\" if formula_8. If formula_9 and formula_10, then the fraction formula_1 exists and formula_12 and formula_13, else no such fraction exists.\n"}
{"id": "11009758", "url": "https://en.wikipedia.org/wiki?curid=11009758", "title": "Realizability", "text": "Realizability\n\nIn mathematical logic, realizability is a collection of methods in proof theory used to study constructive proofs and extract additional information from them. Formulas from a formal theory are \"realized\" by objects, known as \"realizers\", in a way that knowledge of the realizer gives knowledge about the truth of the formula. There are many variations of realizability; exactly which class of formulas is studied and which objects are realizers differ from one variation to another.\n\nRealizability can be seen as a formalization of the BHK interpretation of intuitionistic logic; in realizability the notion of \"proof\" (which is left undefined in the BHK interpretation) is replaced with a formal notion of \"realizer\". Most variants of realizability begin with a theorem that any statement that is provable in the formal system being studied is realizable. The realizer, however, usually gives more information about the formula than a formal proof would directly provide.\n\nBeyond giving insight into intuitionistic provability, realizability can be applied to prove the disjunction and existence properties for intuitionistic theories and to extract programs from proofs, as in proof mining. It is also related to topos theory via the realizability topos.\n\nKleene's original version of realizability uses natural numbers as realizers for formulas in Heyting arithmetic. The following clauses are used to define a relation \"\"n\" realizes \"A\"\" between natural numbers \"n\" and formulas \"A\" in the language of Heyting arithmetic. A few pieces of notation are required: first, an ordered pair (\"n\",\"m\") is treated as a single number using a fixed effective pairing function; second, for each natural number \"n\", φ is the computable function with index \"n\". \n\nWith this definition, the following theorem is obtained:\nOn the other hand, there are formulas that are realized but which are not provable in HA, a fact first established by Rose.\n\nFurther analysis of the method can be used to prove that HA has the \"disjunction and existence properties\":\n\nKreisel introduced modified realizability, which uses typed lambda calculus as the language of realizers. Modified realizability is one way to show that Markov's principle is not derivable in intuitionistic logic. On the contrary, it allows to constructively justify the principle of independence of premise: \n\nRelative realizability is an intuitionist analysis of recursive or recursively enumerable elements of data structures that are not necessarily computable, such as computable operations on all real numbers when reals can be only approximated on digital computer systems.\n\nRealizability is one of the methods used in proof mining to extract concrete \"programs\" from seemingly nonconstructive mathematical proof. Program extraction using realizability is implemented in some proof assistants such as Coq.\n\n\n\n"}
{"id": "14182874", "url": "https://en.wikipedia.org/wiki?curid=14182874", "title": "Schuette–Nesbitt formula", "text": "Schuette–Nesbitt formula\n\nIn mathematics, the Schuette–Nesbitt formula is a generalization of the inclusion–exclusion principle. It is named after Donald R. Schuette and Cecil J. Nesbitt.\n\nThe probabilistic version of the Schuette–Nesbitt formula has practical applications in actuarial science, where it is used to calculate the net single premium for life annuities and life insurances based on the general symmetric status.\n\nConsider a set and subsets . Let\n\ndenote the number of subsets to which belongs, where we use the indicator functions of the sets . Furthermore, for each , let\n\ndenote the number of intersections of exactly sets out of , to which belongs, where the intersection over the empty index set is defined as , hence . Let denote a vector space over a field such as the real or complex numbers (or more generally a module over a ring with multiplicative identity). Then, for every choice of ,\n\nwhere denotes the indicator function of the set of all with , and formula_1 is a binomial coefficient. Equality () says that the two -valued functions defined on are the same.\n\nAs a special case, take for the polynomial ring with the indeterminate . Then () can be rewritten in a more compact way as\n\nThis is an identity for two polynomials whose coefficients depend on , which is implicit in the notation.\n\nConsider the linear shift operator and the linear difference operator , which we define here on the sequence space of by\n\nand\n\nSubstituting in () shows that\n\nwhere we used that with denoting the identity operator. Note that and equal the identity operator  on the sequence space, and denote the -fold composition.\n\nLet denote the 0th component of the -fold composition applied to , where denotes the identity. Then () can be rewritten in a more compact way as\n\nConsider arbitrary events in a probability space and let denote the expectation operator. Then from () is the random number of these events which occur simultaneously. Using from (), define\n\nwhere the intersection over the empty index set is again defined as , hence . If the ring is also an algebra over the real or complex numbers, then taking the expectation of the coefficients in () and using the notation from (),\n\nin . If is the field of real numbers, then this is the probability-generating function of the probability distribution of .\n\nSimilarly, () and () yield\n\nand, for every sequence ,\n\nThe quantity on the left-hand side of () is the expected value of .\n\n\n\nFor textbook presentations of the probabilistic Schuette–Nesbitt formula () and their applications to actuarial science, cf. . Chapter 8, or , Chapter 18 and the Appendix, pp. 577–578.\n\nFor independent events, the formula () appeared in a discussion of Robert P. White and T.N.E. Greville's paper by Donald R. Schuette and Cecil J. Nesbitt, see . In the two-page note , Hans U. Gerber, called it Schuette–Nesbitt formula and generalized it to arbitrary events. Christian Buchta, see , noticed the combinatorial nature of the formula and published the elementary combinatorial proof of ().\n\nCecil J. Nesbitt, PhD, F.S.A., M.A.A.A., received his mathematical education at the University of Toronto and the Institute for Advanced Study in Princeton. He taught actuarial mathematics at the University of Michigan from 1938 to 1980. He served the Society of Actuaries from 1985 to 1987 as Vice-President for Research and Studies. Professor Nesbitt died in 2001. (Short CV taken from , page xv.)\n\nDonald Richard Schuette was a PhD student of C. Nesbitt, he later became professor at the University of Wisconsin–Madison.\n\nThe probabilistic version of the Schuette–Nesbitt formula () generalizes much older formulae of Waring, which express the probability of the events and in terms of , , ..., . More precisely, with formula_5 denoting the binomial coefficient,\n\nand\n\nsee , Sections IV.3 and IV.5, respectively.\n\nTo see that these formulae are special cases of the probabilistic version of the Schuette–Nesbitt formula, note that by the binomial theorem\n\nApplying this operator identity to the sequence with leading zeros and noting that if and otherwise, the formula () for follows from ().\n\nApplying the identity to with leading zeros and noting that if and otherwise, equation () implies that\n\nExpanding using the binomial theorem and using equation (11) of the formulas involving binomial coefficients, we obtain\n\nHence, we have the formula () for .\n\nProblem: Suppose there are persons aged with remaining random (but independent) lifetimes . Suppose the group signs a life insurance contract which pays them after years the amount if exactly persons out of are still alive after years. How high is the expected payout of this insurance contract in years?\n\nSolution: Let denote the event that person survives years, which means that . In actuarial notation the probability of this event is denoted by and can be taken from a life table. Use independence to calculate the probability of intersections. Calculate and use the probabilistic version of the Schuette–Nesbitt formula () to calculate the expected value of .\n\nLet be a random permutation of the set and let denote the event that is a fixed point of , meaning that . When the numbers in , which is a subset of , are fixed points, then there are ways to permute the remaining numbers, hence\n\nBy the combinatorical interpretation of the binomial coefficient, there are formula_10 different choices of a subset of with elements, hence () simplifies to\n\nTherefore, using (), the probability-generating function of the number of fixed points is given by\n\nThis is the partial sum of the infinite series giving the exponential function at , which in turn is the probability-generating function of the Poisson distribution with parameter . Therefore, as tends to infinity, the distribution of converges to the Poisson distribution with parameter .\n\n\n\n"}
{"id": "302202", "url": "https://en.wikipedia.org/wiki?curid=302202", "title": "Self-adjoint", "text": "Self-adjoint\n\nIn mathematics, an element \"x\" of a *-algebra is self-adjoint if formula_1.\n\nA collection \"C\" of elements of a star-algebra is self-adjoint if it is closed under the involution operation. For example, if formula_2 then since formula_3 in a star-algebra, the set {\"x\",\"y\"} is a self-adjoint set even though \"x\" and \"y\" need not be self-adjoint elements.\n\nIn functional analysis, a linear operator \"A\" on a Hilbert space is called self-adjoint if it is equal to its own adjoint \"A\" and that the domain of \"A\" is the same as that of \"A\". See self-adjoint operator for a detailed discussion. If the Hilbert space is finite-dimensional and an orthonormal basis has been chosen, then the operator \"A\" is self-adjoint if and only if the matrix describing \"A\" with respect to this basis is Hermitian, i.e. if it is equal to its own conjugate transpose. Hermitian matrices are also called self-adjoint.\n\nIn a dagger category, a morphism formula_4 is called self-adjoint if formula_5; this is possible only for an endomorphism formula_6.\n\n\n"}
{"id": "52563216", "url": "https://en.wikipedia.org/wiki?curid=52563216", "title": "Self-organization in cybernetics", "text": "Self-organization in cybernetics\n\nSelf-organization, a process where some form of overall order arises out of the local interactions between parts of an initially disordered system, was discovered in cybernetics by William Ross Ashby in 1947. It states that any deterministic dynamic system automatically evolves towards a state of equilibrium that can be described in terms of an attractor in a basin of surrounding states. Once there, the further evolution of the system is constrained to remain in the attractor. This constraint implies a form of mutual dependency or coordination between its constituent components or subsystems. In Ashby's terms, each subsystem has adapted to the environment formed by all other subsystems.\n\nThe cybernetician Heinz von Foerster formulated the principle of \"order from noise\" in 1960. It notes that self-organization is facilitated by random perturbations (\"noise\") that let the system explore a variety of states in its state space. This increases the chance that the system will arrive into the basin of a \"strong\" or \"deep\" attractor, from which it then quickly enters the attractor itself. The biophysicist Henri Atlan developed such a concept by proposing the principle of \"complexity from noise\" () first in the 1972 book \"L'organisation biologique et la théorie de l'information\" and then in the 1979 book \"Entre le cristal et la fumée\". The thermodynamicist Ilya Prigogine formulated a similar principle as \"order through fluctuations\" or \"order out of chaos\". It is applied in the method of simulated annealing for problem solving and machine learning.\n\nWiener regarded the automatic serial identification of a black box and its subsequent reproduction (copying) as sufficient to meet the condition of self-organization. The importance of phase locking or the \"attraction of frequencies\", as he called it, is discussed in the 2nd edition of his \"Cybernetics\". Drexler sees self-replication (copying) as a key step in nano and universal assembly. In later work he seeks to lessen this constraint.\n\nBy contrast, the four concurrently connected galvanometers of W. Ross Ashby's Homeostat hunt, when perturbed, to converge on one of many possible stable states. Ashby used his state counting measure of variety to describe stable states and produced the \"Good Regulator\" theorem which requires internal models for self-organized endurance and stability (e.g. Nyquist stability criterion).\n\nWarren McCulloch proposed \"Redundancy of Potential Command\" as characteristic of the organization of the brain and human nervous system and the necessary condition for self-organization.\n\nHeinz von Foerster proposed Redundancy, \"R\" = 1 − \"H\"/\"H\", where \"H\" is entropy. In essence this states that unused potential communication bandwidth is a measure of self-organization.\n\nIn the 1970s Stafford Beer considered this condition as necessary for autonomy which identifies self-organization in persisting and living systems. He applied his viable system model to management. It consists of five parts: the monitoring of performance of the survival processes (1), their management by recursive application of regulation (2), homeostatic operational control (3) and development (4) which produce maintenance of identity (5) under environmental perturbation. Focus is prioritized by an alerting \"algedonic loop\" feedback: a sensitivity to both pain and pleasure produced from under-performance or over-performance relative to a standard capability.\n\nIn the 1990s Gordon Pask pointed out von Foerster's H and Hmax were not independent and interacted via countably infinite recursive concurrent spin processes (he favoured the Bohm interpretation) which he called concepts (liberally defined in \"any\" medium, \"productive and, incidentally reproductive\"). His strict definition of concept \"a procedure to bring about a relation\" permitted his theorem \"Like concepts repel, unlike concepts attract\" to state a general spin based \"principle of self-organization\". His edict, an exclusion principle, \"There are No Doppelgangers\" means no two concepts can be the same (all interactions occur with different perspectives making time incommensurable for actors). This means, after sufficient duration as differences assert, all concepts will attract and coalesce as pink noise and entropy increases (and see Big Crunch, self-organized criticality). The theory is applicable to all organizationally closed or homeostatic processes that produce enduring and coherent products (where spins have a fixed average phase relationship and also in the sense of Nicholas Rescher's coherence theory of truth with the proviso that the sets and their members exert repulsive forces at their boundaries) through interactions: evolving, learning and adapting.\n\nPask's Interactions of Actors \"hard carapace\" model is reflected in some of the ideas of emergence and coherence. It requires a knot emergence topology that produces radiation during interaction with a unit cell that has a prismatic tensegrity structure. Laughlin's contribution to emergence reflects some of these constraints.\n"}
{"id": "17283742", "url": "https://en.wikipedia.org/wiki?curid=17283742", "title": "Skew-symmetric graph", "text": "Skew-symmetric graph\n\nIn graph theory, a branch of mathematics, a skew-symmetric graph is a directed graph that is isomorphic to its own transpose graph, the graph formed by reversing all of its edges, under an isomorphism that is an involution without any fixed points. Skew-symmetric graphs are identical to the double covering graphs of bidirected graphs.\n\nSkew-symmetric graphs were first introduced under the name of \"antisymmetrical digraphs\" by , later as the double covering graphs of polar graphs by , and still later as the double covering graphs of bidirected graphs by . They arise in modeling the search for alternating paths and alternating cycles in algorithms for finding matchings in graphs, in testing whether a still life pattern in Conway's Game of Life may be partitioned into simpler components, in graph drawing, and in the implication graphs used to efficiently solve the 2-satisfiability problem.\n\nAs defined, e.g., by , a skew-symmetric graph \"G\" is a directed graph, together with a function σ mapping vertices of \"G\" to other vertices of \"G\", satisfying the following properties:\nOne may use the third property to extend σ to an orientation-reversing function on the edges of \"G\".\n\nThe transpose graph of \"G\" is the graph formed by reversing every edge of \"G\", and σ defines a graph isomorphism from \"G\" to its transpose. However, in a skew-symmetric graph, it is additionally required that the isomorphism pair each vertex with a different vertex, rather than allowing a vertex to be mapped to itself by the isomorphism or to group more than two vertices in a cycle of isomorphism.\n\nA path or cycle in a skew-symmetric graph is said to be \"regular\" if, for each vertex \"v\" of the path or cycle, the corresponding vertex σ(\"v\") is not part of the path or cycle.\n\nEvery directed path graph with an even number of vertices is skew-symmetric, via a symmetry that swaps the two ends of the path. However, path graphs with an odd number of vertices are not skew-symmetric, because the orientation-reversing symmetry of these graphs maps the center vertex of the path to itself, something that is not allowed for skew-symmetric graphs.\n\nSimilarly, a directed cycle graph is skew-symmetric if and only if it has an even number of vertices. In this case, the number of different mappings σ that realize the skew symmetry of the graph equals half the length of the cycle.\n\nA skew-symmetric graph may equivalently be defined as the double covering graph of a \"polar graph\" (introduced by , , called a \"switch graph\" by ), which is an undirected graph in which the edges incident to each vertex are partitioned into two subsets. Each vertex of the polar graph corresponds to two vertices of the skew-symmetric graph, and each edge of the polar graph corresponds to two edges of the skew-symmetric graph. This equivalence is the one used by to model problems of matching in terms of skew-symmetric graphs; in that application, the two subsets of edges at each vertex are the unmatched edges and the matched edges. Zelinka (following F. Zitek) and Cook visualize the vertices of a polar graph as points where multiple tracks of a train track come together: if a train enters a switch via a track that comes in from one direction, it must exit via a track in the other direction. The problem of finding non-self-intersecting smooth curves between given points in a train track comes up in testing whether certain kinds of graph drawings are valid and may be modeled as the search for a regular path in a skew-symmetric graph.\n\nA closely related concept is the bidirected graph of (\"polarized graph\" in the terminology of , ), a graph in which each of the two ends of each edge may be either a head or a tail, independently of the other end. A bidirected graph may be interpreted as a polar graph by letting the partition of edges at each vertex be determined by the partition of endpoints at that vertex into heads and tails; however, swapping the roles of heads and tails at a single vertex (\"switching\" the vertex, in the terminology of ) produces a different bidirected graph but the same polar graph. \n\nFor the correspondence between bidirected graphs and skew-symmetric graphs (i.e., their double covering graphs) see , Section 5, or . \n\nTo form the double covering graph (i.e., the corresponding skew-symmetric graph) from a polar graph \"G\", create for each vertex \"v\" of \"G\" two vertices \"v\" and \"v\", and let σ(\"v\") = \"v\". For each edge \"e\" = (\"u\",\"v\") of \"G\", create two directed edges in the covering graph, one oriented from \"u\" to \"v\" and one oriented from \"v\" to \"u\". If \"e\" is in the first subset of edges at \"v\", these two edges are from \"u\" into \"v\" and from \"v\" into \"u\", while if \"e\" is in the second subset, the edges are from \"u\" into \"v\" and from \"v\" into \"u\".\nIn the other direction, given a skew-symmetric graph \"G\", one may form a polar graph that has one vertex for every corresponding pair of vertices in \"G\" and one undirected edge for every corresponding pair of edges in \"G\". The undirected edges at each vertex of the polar graph may be partitioned into two subsets according to which vertex of the polar graph they go out of and come into.\n\nA regular path or cycle of a skew-symmetric graph corresponds to a path or cycle in the polar graph that uses at most one edge from each subset of edges at each of its vertices.\n\nIn constructing matchings in undirected graphs, it is important to find \"alternating paths\", paths of vertices that start and end at unmatched vertices, in which the edges at odd positions in the path are not part of a given partial matching and in which the edges at even positions in the path are part of the matching. By removing the matched edges of such a path from a matching, and adding the unmatched edges, one can increase the size of the matching. Similarly, cycles that alternate between matched and unmatched edges are of importance in weighted matching problems.\nAs showed, an alternating path or cycle in an undirected graph may be modeled as a regular path or cycle in a skew-symmetric directed graph. To create a skew-symmetric graph from an undirected graph \"G\" with a specified matching \"M\", view \"G\" as a switch graph in which the edges at each vertex are partitioned into matched and unmatched edges; an alternating path in \"G\" is then a regular path in this switch graph and an alternating cycle in \"G\" is a regular cycle in the switch graph.\n\nAlong with the path problems arising in matchings, skew-symmetric generalizations of the max-flow min-cut theorem have also been studied (; ).\n\n shows that a still life pattern in Conway's Game of Life may be partitioned into two smaller still lifes if and only if an associated switch graph contains a regular cycle. As he shows, for switch graphs with at most three edges per vertex, this may be tested in polynomial time by repeatedly removing bridges (edges the removal of which disconnects the graph) and vertices at which all edges belong to a single partition until no more such simplifications may be performed. If the result is an empty graph, there is no regular cycle; otherwise, a regular cycle may be found in any remaining bridgeless component. The repeated search for bridges in this algorithm may be performed efficiently using a dynamic graph algorithm of .\n\nSimilar bridge-removal techniques in the context of matching were previously considered by .\n\nAn instance of the 2-satisfiability problem, that is, a Boolean expression in conjunctive normal form with two variables or negations of variables per clause, may be transformed into an implication graph by replacing each clause formula_1 by the two implications\nformula_2 and formula_3. This graph has a vertex for each variable or negated variable, and a directed edge for each implication; it is, by construction, skew-symmetric, with a correspondence σ that maps each variable to its negation.\nAs showed, a satisfying assignment to the 2-satisfiability instance is equivalent to a partition of this implication graph into two subsets of vertices, \"S\" and σ(\"S\"), such that no edge starts in \"S\" and ends in σ(\"S\"). If such a partition exists, a satisfying assignment may be formed by assigning a true value to every variable in \"S\" and a false value to every variable in σ(\"S\"). This may be done if and only if no strongly connected component of the graph contains both some vertex \"v\" and its complementary vertex σ(\"v\"). If two vertices belong to the same strongly connected component, the corresponding variables or negated variables are constrained to equal each other in any satisfying assignment of the 2-satisfiability instance. The total time for testing strong connectivity and finding a partition of the implication graph is linear in the size of the given 2-CNF expression.\n\nIt is NP-complete to determine whether a given directed graph is skew-symmetric, by a result of that it is NP-complete to find a color-reversing involution in a bipartite graph. Such an involution exists if and only if the directed graph given by orienting each edge from one color class to the other is skew-symmetric, so testing skew-symmetry of this directed graph is hard. This complexity does not affect path-finding algorithms for skew-symmetric graphs, because these algorithms assume that the skew-symmetric structure is given as part of the input to the algorithm rather than requiring it to be inferred from the graph alone.\n\n"}
{"id": "1406909", "url": "https://en.wikipedia.org/wiki?curid=1406909", "title": "The Compendious Book on Calculation by Completion and Balancing", "text": "The Compendious Book on Calculation by Completion and Balancing\n\nThe Compendious Book on Calculation by Completion and Balancing (, \"Al-kitāb al-mukhtaṣar fī ḥisāb al-ğabr wa’l-muqābala\"; ) is an Arabic treatise on mathematics written by Persian polymath Muḥammad ibn Mūsā al-Khwārizmī around 820 CE while he was in the Abbasid capital of Baghdad. Translated into Latin by Robert of Chester in 1145, it was used until the sixteenth century as the \"principal mathematical text-book\" of European universities.It also introduced the term \"algebra\" (, \"al-jabr\") to European languages. The \"Compendious Book\" provided an exhaustive account of solving for the positive roots of polynomial equations up to the second degree.\n\nSeveral authors have also published texts under this name, including Abū Ḥanīfa al-Dīnawarī, Abū Kāmil Shujā ibn Aslam, Abū Muḥammad al-ʿAdlī, Abū Yūsuf al-Miṣṣīṣī, 'Abd al-Hamīd ibn Turk, Sind ibn ʿAlī, Sahl ibn Bišr, and Šarafaddīn al-Ṭūsī.\n\nR. Rashed and Angela Armstrong write:\nJ. J. O'Connor and E. F. Robertson wrote in the \"MacTutor History of Mathematics archive\":\nThe book was a compilation and extension of known rules for solving quadratic equations and for some other problems, and considered to be the foundation of algebra, establishing it as an independent discipline. The word \"algebra\" is derived from the name of one of the basic operations with equations described in this book, following its Latin translation by Robert of Chester.\n\nSince the book does not give any citations to previous authors, it is not clearly known what earlier works were used by al-Khwarizmi, and modern mathematical historians put forth opinions based on the textual analysis of the book and the overall body of knowledge of the contemporary Muslim world. There are indications of connections with Indian mathematics, as he had written a book entitled \"The Book of Bringing Together and Separating According to the Hindu Calculation\" (\"Kitāb al-Jamʿ wa-l-tafrīq bi-ḥisāb al-Hind\"), discussing the Hindu-Arabic numeral system.\n\nThe book classifies quadratic equations to one of the six basic types and provides algebraic and geometric methods to solve the basic ones. Historian Carl Boyer notes the following regarding the lack of modern abstract notations in the book:\nThus the equations are verbally described in terms of \"squares\" (what would today be \"\"x\"\"), \"roots\" (what would today be \"x\") and \"numbers\" (\"constants\": ordinary spelled out numbers, like 'forty-two'). The six types, with modern notations, are:\n\nIslamic mathematicians, unlike the Hindus, did not deal with negative numbers at all; hence an equation like \"bx\" + \"c\" = 0 does not appear in the classification, because it has no positive solutions if all the coefficients are positive. Similarly equation types 4, 5 and 6, which look equivalent to the modern eye, were distinguished because the coefficients must all be positive.\n\nThe \"al-ğabr\" (\"forcing\", \"restoring\") operation is moving a deficient quantity from one side of the equation to the other side. In an al-Khwarizmi's example (in modern notation), \"\"x\" = 40\"x\" − 4\"x\"\" is transformed by \"al-ğabr\" into \"5\"x\" = 40\"x\"\". Repeated application of this rule eliminates negative quantities from calculations.\n\n\"Al-Muqabala\" (, \"balancing\" or \"corresponding\") means subtraction of the same positive quantity from both sides: \"\"x\" + 5 = 40\"x\" + 4\"x\"\" is turned into \"5 = 40\"x\" + 3\"x\"\". Repeated application of this rule makes quantities of each type (\"square\"/\"root\"/\"number\") appear in the equation at most once, which helps to see that there are only 6 basic solvable types of the problem, when restricted to positive coefficients and solutions. \n\nSubsequent parts of the book do not rely on solving quadratic equations.\n\nThe second chapter of the book catalogues methods of finding area and volume. These include approximations of pi (π), given three ways, as 3 1/7, √10, and 62832/20000. This latter approximation, equalling 3.1416, earlier appeared in the Indian \"Āryabhaṭīya\" (499 CE).\n\nAl-Khwārizmī explicates the Jewish calendar and the 19-year cycle described by the convergence of lunar months and solar years.\n\nAbout half of the book deals with Islamic rules of inheritance, which are complex and require skill in first-order algebraic equations.\n\n\n"}
{"id": "1240997", "url": "https://en.wikipedia.org/wiki?curid=1240997", "title": "Thomson's lamp", "text": "Thomson's lamp\n\nThomson's lamp is a philosophical puzzle based on infinites. It was devised in 1954 by British philosopher James F. Thomson, who used it to analyze the possibility of a supertask, which is the completion of an infinite number of tasks.\n\nConsider a lamp with a toggle switch. Flicking the switch once turns the lamp on. Another flick will turn the lamp off. Now suppose that there is a being able to perform the following task: starting a timer, he turns the lamp on. At the end of one minute, he turns it off. At the end of another half minute, he turns it on again. At the end of another quarter of a minute, he turns it off. At the next eighth of a minute, he turns it on again, and he continues thus, flicking the switch each time after waiting exactly one-half the time he waited before flicking it previously. The sum of this infinite series of time intervals is exactly two minutes.\n\nThe following question is then considered: Is the lamp on or off at two minutes? Thomson reasoned that this supertask creates a contradiction:\n\nThe question is related to the behavior of Grandi's series, \"i.e.\" the divergent infinite series\n\nFor even values of \"n\", the above finite series sums to 1; for odd values, it sums to 0. In other words, as \"n\" takes the values of each of the non-negative integers 0, 1, 2, 3, ... in turn, the series generates the sequence {1, 0, 1, 0, ...}, representing the changing state of the lamp. The sequence does not converge as \"n\" tends to infinity, so neither does the infinite series.\n\nAnother way of illustrating this problem is to rearrange the series:\n\nThe unending series in the brackets is exactly the same as the original series \"S\". This means \"S\" = 1 − \"S\" which implies \"S\" = ⁄. In fact, this manipulation can be rigorously justified: there are generalized definitions for the sums of series that do assign Grandi's series the value ⁄.\n\nOne of Thomson's objectives in his original 1954 paper is to differentiate supertasks from their series analogies. He writes of the lamp and Grandi's series,\nLater, he claims that even the divergence of a series does not provide information about its supertask: \"The impossibility of a super-task does not depend at all on whether some vaguely-felt-to-be-associated arithmetical sequence is convergent or divergent.\"\n\n\n"}
{"id": "2831127", "url": "https://en.wikipedia.org/wiki?curid=2831127", "title": "Traffic flow", "text": "Traffic flow\n\nIn mathematics and civil engineering, traffic flow is the study of interactions between travellers (including pedestrians, cyclists, drivers, and their vehicles) and infrastructure (including highways, signage, and traffic control devices), with the aim of understanding and developing an optimal transport network with efficient movement of traffic and minimal traffic congestion problems.\n\nAttempts to produce a mathematical theory of traffic flow date back to the 1920s, when Frank Knight first produced an analysis of traffic equilibrium, which was refined into Wardrop's first and second principles of equilibrium in 1952.\n\nNonetheless, even with the advent of significant computer processing power, to date there has been no satisfactory general theory that can be consistently applied to real flow conditions. Current traffic models use a mixture of empirical and theoretical techniques. These models are then developed into traffic forecasts, to take account of proposed local or major changes, such as increased vehicle use, changes in land use or changes in mode of transport (with people moving from bus to train or car, for example), and to identify areas of congestion where the network needs to be adjusted.\n\nTraffic behaves in a complex and nonlinear way, depending on the interactions of a large number of vehicles. Due to the individual reactions of human drivers, vehicles do not interact simply following the laws of mechanics, but rather display cluster formation and shock wave propagation, both forward and backward, depending on vehicle density. Some mathematical models of traffic flow use a vertical queue assumption, in which the vehicles along a congested link do not spill back along the length of the link.\n\nIn a free-flowing network, \"traffic flow theory\" refers to the traffic stream variables of speed, flow, and concentration. These relationships are mainly concerned with uninterrupted traffic flow, primarily found on freeways or expressways. \nFlow conditions are considered \"free\" when less than 12 vehicles per mile are on a road. \"Stable\" is sometimes described as 12–30 vehicles per mile per lane. As the density reaches the maximum mass flow rate (or flux) and exceeds the optimum density (above 30 vehicles per mile), traffic flow becomes unstable, and even a minor incident can result in persistent stop-and-go driving conditions. A \"breakdown\" condition occurs when traffic becomes unstable and exceeds 67 vehicles per mile. \"Jam density\" refers to extreme traffic density when traffic flow stops completely, usually in the range of 185–250 vehicles per mile per lane.\n\nHowever, calculations about congested networks are more complex and rely more on empirical studies and extrapolations from actual road counts. Because these are often urban or suburban in nature, other factors (such as road-user safety and environmental considerations) also influence the optimum conditions.\n\nThere are that are qualitatively the same for different highways in different countries, measured during years of traffic observations. Some of these common features of traffic congestion define synchronized flow and wide moving jam traffic phases of congested traffic in Kerner’s three-phase traffic theory of traffic flow.\n\nTraffic flow is generally constrained along a one-dimensional pathway (e.g. a travel lane). A time-space diagram shows graphically the flow of vehicles along a pathway over time. Time is displayed along the horizontal axis, and distance is shown along the vertical axis. Traffic flow in a time-space diagram is represented by the individual trajectory lines of individual vehicles. Vehicles following each other along a given travel lane will have parallel trajectories, and trajectories will cross when one vehicle passes another. Time-space diagrams are useful tools for displaying and analyzing the traffic flow characteristics of a given roadway segment over time (e.g. analyzing traffic flow congestion).\n\nThere are three main variables to visualize a traffic stream: speed (v), density (indicated k; the number of vehicles per unit of space), and flow (indicated q; the number of vehicles per unit of time).\nSpeed is the distance covered per unit time. One cannot track the speed of every vehicle; so, in practice, average speed is measured by sampling vehicles in a given area over a period of time. Two definitions of average speed are identified: \"time mean speed\" and \"space mean speed\".\n\n\nformula_1\n\nwhere \"m\" represents the number of vehicles passing the fixed point and \"v\" is the speed of the \"i\"th vehicle.\n\n\nformula_2\n\nwhere \"n\" represents the number of vehicles passing the roadway segment.\n\nThe \"space mean speed\" is thus the harmonic mean of the speeds.\n\nThe time mean speed is never less than space mean speed:\n\nformula_3\n\nwhere formula_4 is the variance of the space mean speed\nIn a time-space diagram, the instantaneous velocity, v = dx/dt, of a vehicle is equal to the slope along the vehicle’s trajectory. The average velocity of a vehicle is equal to the slope of the line connecting the trajectory endpoints where a vehicle enters and leaves the roadway segment. The vertical separation (distance) between parallel trajectories is the vehicle spacing (s) between a leading and following vehicle. Similarly, the horizontal separation (time) represents the vehicle headway (h). A time-space diagram is useful for relating headway and spacing to traffic flow and density, respectively.\n\nDensity (k) is defined as the number of vehicles per unit length of the roadway. In traffic flow, the two most important densities are the critical density (\"k\") and jam density (\"k\"). The maximum density achievable under free flow is \"k\", while \"k\" is the maximum density achieved under congestion. In general, jam density is seven times the critical density. Inverse of density is spacing (s), which is the center-to-center distance between two vehicles.\n\nformula_5\n\nThe density (\"k\") within a length of roadway (\"L\") at a given time (\"t\") is equal to the inverse of the average spacing of the \"n\" vehicles.\n\nformula_6\n\nIn a time-space diagram, the density may be evaluated in the region A.\n\nformula_7\n\nwhere \"tt\" is the total travel time in \"A\".\nFlow (\"q\") is the number of vehicles passing a reference point per unit of time, vehicles per hour. The inverse of flow is headway (\"h\"), which is the time that elapses between the \"i\"th vehicle passing a reference point in space and the (\"i\" + 1)th vehicle. In congestion, \"h\" remains constant. As a traffic jam forms, \"h\" approaches infinity.\n\nformula_8\n\nformula_9\n\nThe flow (\"q\") passing a fixed point (\"x\") during an interval (\"T\") is equal to the inverse of the average headway of the \"m\" vehicles.\n\nformula_10\n\nIn a time-space diagram, the flow may be evaluated in the region \"B\".\n\nformula_11\n\nwhere \"td\" is the total distance traveled in \"B\".\nA more general definition of the flow and density in a time-space diagram is illustrated by region C:\n\nformula_12\n\nformula_13\n\nwhere:\n\nformula_14\n\nformula_15\n\nIn addition to providing information on the speed, flow, and density of traffic streams, time-space diagrams may illustrate the propagation of congestion upstream from a traffic bottleneck (shockwave). Congestion shockwaves will vary in propagation length, depending upon the upstream traffic flow and density. However, shockwaves will generally travel upstream at a rate of approximately 20 km/h.\nTraffic on a stretch of road is said to be stationary if an observer does not detect movement in an arbitrary area of the time-space diagram. Traffic is stationary if all the vehicle trajectories are parallel and equidistant. It is also stationary if it is a superposition of families of trajectories with these properties (e.g. fast and slow drivers). By using a very small hole in the template one could sometimes view an empty region of the diagram and other times not, so that even in these cases, one could say that traffic was not stationary. Clearly, for such fine level of observation, stationary traffic does not exist. A microscopic level of observation must be excluded from the definition if traffic appears to be similar through larger windows. In fact, we relax the definition even further by only requiring that the quantities t(A) and d(A) be approximately the same, regardless of where the \"large\" window (A) is placed.\n\nAnalysts approach the problem in three main ways, corresponding to the three main scales of observation in physics:\n\n\nThe engineering approach to analysis of highway traffic flow problems is primarily based on empirical analysis (i.e., observation and mathematical curve fitting). One major reference used by American planners is the \"Highway Capacity Manual\", published by the Transportation Research Board, which is part of the United States National Academy of Sciences. This recommends modelling traffic flows using the whole travel time across a link using a delay/flow function, including the effects of queuing. This technique is used in many US traffic models and in the SATURN model in Europe.\n\nIn many parts of Europe, a hybrid empirical approach to traffic design is used, combining macro-, micro-, and mesoscopic features. Rather than simulating a steady state of flow for a journey, transient \"demand peaks\" of congestion are simulated. These are modeled by using small \"time slices\" across the network throughout the working day or weekend. Typically, the origins and destinations for trips are first estimated and a traffic model is generated before being calibrated by comparing the mathematical model with observed counts of actual traffic flows, classified by type of vehicle. \"Matrix estimation\" is then applied to the model to achieve a better match to observed link counts before any changes, and the revised model is used to generate a more realistic traffic forecast for any proposed scheme. The model would be run several times (including a current baseline, an \"average day\" forecast based on a range of economic parameters and supported by sensitivity analysis) in order to understand the implications of temporary blockages or incidents around the network. From the models, it is possible to total the time taken for all drivers of different types of vehicle on the network and thus deduce average fuel consumption and emissions.\n\nMuch of UK, Scandinavian, and Dutch authority practice is to use the modelling program CONTRAM for large schemes, which has been developed over several decades under the auspices of the UK's Transport Research Laboratory, and more recently with the support of the Swedish Road Administration. By modelling forecasts of the road network for several decades into the future, the economic benefits of changes to the road network can be calculated, using estimates for value of time and other parameters. The output of these models can then be fed into a cost-benefit analysis program.\n\nA cumulative vehicle count curve, the \"N\"-curve, shows the cumulative number of vehicles that pass a certain location \"x\" by time \"t\", measured from the passage of some reference vehicle. This curve can be plotted if the arrival times are known for individual vehicles approaching a location \"x\", and the departure times are also known as they leave location \"x\". Obtaining these arrival and departure times could involve data collection: for example, one could set two point sensors at locations \"X\" and \"X\", and count the number of vehicles that pass this segment while also recording the time each vehicle arrives at \"X\" and departs from \"X\". The resulting plot is a pair of cumulative curves where the vertical axis (\"N\") represents the cumulative number of vehicles that pass the two points: \"X\" and \"X\", and the horizontal axis (\"t\") represents the elapsed time from \"X\" and \"X\".\n\nIf vehicles experience no delay as they travel from \"X\" to \"X\", then the arrivals of vehicles at location \"X\" is represented by curve \"N\" and the arrivals of the vehicles at location \"X\" is represented by \"N\" in figure 8. More commonly, curve \"N\" is known as the \"arrival curve\" of vehicles at location \"X\" and curve \"N\" is known as the \"arrival curve\" of vehicles at location \"X\". Using a one-lane signalized approach to an intersection as an example, where \"X\" is the location of the stop bar at the approach and \"X\" is an arbitrary line on the receiving lane just across of the intersection, when the traffic signal is green, vehicles can travel through both points with no delay and the time it takes to travel that distance is equal to the free-flow travel time. Graphically, this is shown as the two separate curves in figure 8.\n\nHowever, when the traffic signal is red, vehicles arrive at the stop bar (\"X\") and are delayed by the red light before crossing \"X\" some time after the signal turns green. As a result, a queue builds at the stop bar as more vehicles are arriving at the intersection while the traffic signal is still red. Therefore, for as long as vehicles arriving at the intersection are still hindered by the queue, the curve \"N\" no longer represents the vehicles’ arrival at location \"X\"; it now represents the vehicles’ \"virtual arrival\" at location \"X\", or in other words, it represents the vehicles' arrival at \"X\" if they did not experience any delay. The vehicles' arrival at location \"X\", taking into account the delay from the traffic signal, is now represented by the curve \"N′\" in figure 9.\n\nHowever, the concept of the \"virtual arrival curve\" is flawed. This curve does not correctly show the queue length resulting from the interruption in traffic (i.e. red signal). It assumes that all vehicles are still reaching the stop bar before being delayed by the red light. In other words, the \"virtual arrival curve\" portrays the stacking of vehicles vertically at the stop bar. When the traffic signal turns green, these vehicles are served in a first-in-first-out (FIFO) order. For a multi-lane approach, however, the service order is not necessarily FIFO. Nonetheless, the interpretation is still useful because of the concern with average total delay instead of total delays for individual vehicles.\n\nThe traffic light example depicts \"N\"-curves as smooth functions. Theoretically, however, plotting \"N\"-curves from collected data should result in a step-function (figure 10). Each step represents the arrival or departure of one vehicle at that point in time. When the \"N\"-curve is drawn on larger scale reflecting a period of time that covers several cycles, then the steps for individual vehicles can be ignored, and the curve will then look like a smooth function (figure 8).\n\nThe \"N\"-curve can be used in a number of different traffic analyses, including freeway bottlenecks and dynamic traffic assignment. This is due to the fact that a number of traffic flow characteristics can be derived from the plot of cumulative vehicle count curves. Illustrated in figure 11 are the different traffic flow characteristics that can be derived from the \"N\"-curves. \nThese are the different traffic flow characteristics from figure 11:\n\nFrom these variables, the average delay experienced by each vehicle and the average queue length at any time \"t\" can be calculated, using the following formulas:\n\nformula_20\n\nformula_21\n\nIn traffic flow area, an alternative way to solve the kinematic wave model is to treat it like a Hamilton–Jacobi equation, which is particularly useful in identifying conserved quantities for mechanical systems.\n\nSuppose we are interested in finding the cumulative curve as a function of time and space, \"N(t,x)\". Based the definition of cumulative curve,formula_22 refers to the flow and formula_23 refers to the density. Note that the sign convention should be consistent. Then the fundamental flow-density (formula_24) equation: formula_25 can be expressed in cumulative count form as: formula_26 formula_27, where formula_28 is a known boundary.\n\nNow, for a generic random point formula_29 in the time-space diagram, the solution to the above partial derivative equation is equivalent to solve the following optimization problem, which minimizes the vehicles pass-by:formula_30, where formula_31 is a random point on the boundary formula_28.\n\nThe function formula_33 is defined as the maximum passing rate along the observers. In the case of triangular fundamental diagram, we have formula_34. The observer speed formula_35.Here, notation formula_36 corresponds to the capacity, formula_37 corresponds to the critical density, formula_38 and formula_39 are free flow speed and wave speed respectively.\n\nWith that being said, the minimization function above simplifies into: formula_40, where formula_31 is a random point on the boundary formula_28. Here, we limit the solution discussion over initial value problems (IVP) and boundary value problems (BVP).\n\nInitial value problem occurs when boundary condition is given at a fixed time, e.g. at formula_43 and boundary formula_44. As the observer speed is bounded by formula_35, the potential solution is delimited by two lines formula_46 and formula_47.\n\nThus, the IVP is defined as follow:\nformula_48\nformula_49\nformula_50\n\nThe local minimum point occurs when the first order derivative is 0 and the second order derivative is greater than 0. Or, the minimum happens at boundaries. So, the set of potential solutions goes as follow: \n1.formula_51 that formula_52 and formula_53\n2.formula_54 and formula_55.\nThe solution will be the minimum corresponding formula_56 of all candidate points. formula_57 and all formula_58 from condition 1).\n\nSpecifically, if the initial condition formula_59is a linear function, formula_60\n\nSimilarly, the boundary value problem indicates the boundary condition is given at a fix location, e.g. formula_61. Still, the observer speed is bounded by formula_35. For a random point formula_29, the upper bound for solution candidates: if formula_64, formula_65; else, formula_66.\n\nThe BVP is defined as follow:\nformula_67\nformula_68\nformula_69\n\nThe first order derivative:formula_70 is always smaller than 0 because flows won't exceed the capacity. Thus, the minimum happens at the upper bound of the time axis.\nformula_71\n\nIn practice, people use this method to estimate the traffic states at formula_72 between two loop detectors, which can be viewed as a combination of two boundary value problems (one at upstream and one at down stream). Denote the upstream loop detector location as formula_54 and the downstream loop detector location as formula_55. Based on the conclusion above, the minimum value occurs at the upper bound along the time axis.\nformula_75, with formula_76\n\nOne application of the \"N\"-curve is the bottleneck model, where the cumulative vehicle count is known at a point \"before\" the bottleneck (i.e. this is location \"X\"). However, the cumulative vehicle count is not known at a point \"after\" the bottleneck (i.e. this is location \"X\"), but rather only the capacity of the bottleneck, or the discharge rate, \"μ\", is known. The bottleneck model can be applied to real-world bottleneck situations such as those resulting from a roadway design problem or a traffic incident.\nTake a roadway section where a bottleneck exists such as in figure 12. At some location \"X\" before the bottleneck, the arrivals of vehicles follow a regular \"N\"-curve. If the bottleneck is absent, the departure rate of vehicles at location \"X\" is essentially the same as the arrival rate at \"X\" at some later time (i.e. at time \"TT\" – free-flow travel time). However, due to the bottleneck, the system at location \"X\" is now only able to have a departure rate of \"μ\". When graphing this scenario, essentially we have the same situation as in figure 9, where the arrival curve of vehicles is \"N\", the departure curve of vehicles absent the bottleneck is \"N\", and the limited departure curve of vehicles given the bottleneck is \"N′\". The discharge rate \"μ\" is the slope of curve \"N′\", and all the same traffic flow characteristics as in figure 11 can be determined from this diagram. The maximum delay and maximum queue length can be found at a point \"M\" in figure 13 where the slope of \"N\" is the same as the slope of \"N′\"; i.e. when the virtual arrival rate is equal to the discharge / departure rate \"μ\". \nThe \"N\"-curve in the bottleneck model may also be used to calculate the benefits in removing the bottleneck, whether in terms of a capacity improvement or removing an incident to the side of the roadway.\n\nAs introduced in the section above, the N-curve is an applicable model to estimate traffic delay during time by setting arrival and departure cumulative counting curve. Since the curve can represent various traffic characteristics and roadway conditions, the delay and queue situations under these conditions will be able to be recognized and modeled using N-curves. Tandem queues occur when multiple bottlenecks exist between the arrival and departure locations. Figure 14 shows a qualitative layout of a tandem-queue roadway segment with a certain initial arrival. The bottlenecks along the stream have their own capacity, '\"μ\" [veh/time], and the departure is defined at the downstream end of the entire segment.\n\nTo determine the ultimate departure, \"D\"(\"t\"), it can be an available method to research on the individual departures, \"D\"(\"t\"). As shown in the Figure 15, if the free-flow travel-time is neglected, the departure of BN will be the virtual arrival of BN, which can also be presented as \"D\"(\"t\") = \"A\"(\"t\"). Thus, the N-curve of a roadway with two bottlenecks (minimum number of BNs along a tandem-queue roadway) can be developed as Figure 15 with \"μ\" < \"μ\". In this case, D(t) will be the ultimate departure of this 2-BN tandem-queue roadway.\n\nRegarding of a tandem-queue roadway having 3 BNs with \"μ\" < \"μ\", if \"μ\" < \"μ\" < \"μ\", similarly as the 2-BN case, D(t) will be the ultimate departure of this 3-BN tandem-queue roadway. If, however, \"μ\" < \"μ\" < \"μ\", D(t) will then still be the ultimate departure of the 3-BN tandem-queue roadway. Thus, it can be summarized that, the departure of the bottleneck with the minimum capacity will be the ultimate departure of the entire system, regardless of the other capacities and the number of bottlenecks. Figure 16 shows a general case with n BNs.\n\nThe N-curve model describing above represents a significant characteristic of the tandem-queue systems, which is that the ultimate departure only depends on the bottleneck with the minimum capacity. In a practical perspective, when the resources (economy, effort, etc.) of the investment on tandem-queue systems are limited, the investment can mainly focus on the bottleneck with the worst condition.\n\nA signalized intersection will have special departure behaviors. With simplified speaking, a constant releasing free-flow capacity, \"μ\", exists during the green phases. On the contrary, the releasing capacity during the red phases should be zero. Thus, the departure N-curve regardless of arrival will look like as Figure 17 below: counts increase with the slope of \"μ\" during green, and remain the same during red..\n\nSaturated case of a traffic light occurs when the releasing capacity is fully used. This case usually exists when the arriving demand is relatively large. The N-curve representation of the saturated case is shown in the Figure 18.\n\nUnsaturated case of a traffic light occurs when releasing capacity is not fully used. This case usually exists when the arriving demand is relatively small. The N-curve representation of the unsaturated case is shown in the Figure 19. If there is a bottleneck with a capacity of \"μ\"(<\"μ\") downstream of the light, the ultimate departure of the light-bottleneck system will be that of the downstream bottleneck.\n\nDynamic traffic assignment can also be solved using the \"N\"-curve. There are two main approaches to tackle this problem: system optimum, and user equilibrium. This application will be discussed further in the following section.\n\nKerner’s three-phase traffic theory is an alternative theory of traffic flow. Probably the most important result of the three-phase theory is that at any time instance there is a range of highway capacities of free flow at a bottleneck. The capacity range is between some maximum and minimum capacities. The range of highway capacities of free flow at the bottleneck in three-phase traffic theory contradicts fundamentally classical traffic theories as well as methods for traffic management and traffic control which at any time instant assume the existence of a \"particular\" deterministic or stochastic highway capacity of free flow at the bottleneck.\n\nThe aim of traffic flow analysis is to create and implement a model which would enable vehicles to reach their destination in the shortest possible time using the maximum roadway capacity. This is a four-step process:\n\nThis cycle is repeated until the solution converges.\n\nThere are two main approaches to tackle this problem with the end objectives:\n\n\nSystem Optimum is based on the assumption that routes of all vehicles would be controlled by the system, and that rerouting would be based on maximum utilization of resources and minimum total system cost. (Cost can be interpreted as travel time.) Hence, in a System Optimum routing algorithm, all routes between a given OD pair have the same marginal cost.\nIn traditional transportation economics, System Optimum is determined by equilibrium of demand function and marginal cost function. In this approach, marginal cost is roughly depicted as increasing function in traffic congestion. In traffic flow approach, the marginal cost of the trip can be expressed as sum of the cost(delay time, w) experienced by the driver and the externality(e) that a driver imposes on the rest of the users. \nSuppose there is a freeway(0) and an alternative route(1), which users can be diverted onto off-ramp. Operator knows total arrival rate(A(t)), the capacity of the freeway(μ_0), and the capacity of the alternative route(μ_1). From the time 't_0', when freeway is congested, some of the users start moving to alternative route. However, when 't_1', alternative route is also full of capacity. Now operator decides the number of vehicles(N), which use alternative route. The optimal number of vehicles(N) can be obtained by calculus of variation, to make marginal cost of each route equal. Thus, optimal condition is T_0=T_1+∆_1. In this graph, we can see that the queue on the alternative route should clear ∆_1 time units before it clears from the freeway. This solution does not define how we should allocates vehicles arriving between t_1 and T_1, we just can conclude that the optimal solution is not unique. If operator wants freeway not to be congested, operator can impose the congestion toll, e_0-e_1, which is the difference between the externality of freeway and alternative route. In this situation, freeway will maintain free flow speed, however alternative route will be extremely congested.\n\nThe user optimum equilibrium assumes that all users choose their own route towards their destination based on the travel time that will be consumed in different route options. The users will choose the route which requires the least travel time. The user optimum model is often used in simulating the impact on traffic assignment by highway bottlenecks. When the congestion occurs on highway, it will extend the delay time in travelling through the highway and create a longer travel time. Under the user optimum assumption, the users would choose to wait until the travel time using a certain freeway is equal to the travel time using city streets, and hence equilibrium is reached. This equilibrium is called User Equilibrium, Wardrop Equilibrium or Nash Equilibrium.\nThe core principle of User Equilibrium is that all used routes between a given OD pair have the same travel time. An alternative route option is enabled to use when the actual travel time in the system has reached the free-flow travel time on that route.\n\nFor a highway user optimum model considering one alternative route, a typical process of traffic assignment is shown in figure 15. When the traffic demand stays below the highway capacity, the delay time on highway stays zero. When the traffic demand exceeds the capacity, the queue of vehicle will appear on the highway and the delay time will increase. Some of users will turn to the city streets when the delay time reaches the difference between the free-flow travel time on highway and the free-flow travel time on city streets. It indicates that the users staying on the highway will spend as much travel time as the ones who turn to the city streets. At this stage, the travel time on both the highway and the alternative route stays the same. This situation may be ended when the demand falls below the road capacity, that is the travel time on highway begins to decrease and all the users will stay on the highway. The total of part area 1 and 3 represents the benefits by providing an alternative route. The total of area 4 and area 2 shows the total delay cost in the system, in which area 4 is the total delay occurs on the highway and area 2 is the extra delay by shifting traffic to city streets.\n\nBoth User Optimum and System Optimum can be subdivided into two categories on the basis of the approach of time delay taken for their solution:\n\n\nPredictive time delay is based on the concept that the system or the user knows when the congestion point is reached or when the delay of the freeway would be equal to the delay on city streets, and the decision for route assignment is taken in time. On the other hand, reactive time delay is when the system or user waits to experience the point where the delay is observed and the diversion of routes is in reaction to that experience. Predictive delay gives significantly better results than the reactive delay method.\n\nKerner introduced an alternative approach to traffic assignment based on his network breakdown minimization (BM) principle. Rather than an explicit minimization of travel time that is the objective of System Optimum and User Equilibrium, the BM principle minimizes the probability of the occurrence of congestion in a traffic network. Under sufficient traffic demand, the application of the BM principle should lead to implicit minimization of travel time in the network.\n\nThis is an upcoming approach of eliminating shockwave and increasing safety for the vehicles. The concept is based on the fact that the risk of accident on a roadway increases with speed differential between the upstream and downstream vehicles. The two types of crash risk which can be reduced from VSL implementation are the rear-end crash and the lane-change crash. Variable speed limits seek to homogenize speed, leading to a more constant flow. Different approaches have been implemented by researchers to build a suitable VSL algorithm.\n\nA major consideration in road capacity relates to the design of junctions. By allowing long \"weaving sections\" on gently curving roads at graded intersections, vehicles can often move across lanes without causing significant interference to the flow. However, this is expensive and takes up a large amount of land, so other patterns are often used, particularly in urban or very rural areas. Most large models use crude simulations for intersections, but computer simulations are available to model specific sets of traffic lights, roundabouts, and other scenarios where flow is interrupted or shared with other types of road users or pedestrians. A well-designed junction can enable significantly more traffic flow at a range of traffic densities during the day. By matching such a model to an \"Intelligent Transport System\", traffic can be sent in uninterrupted \"packets\" of vehicles at predetermined speeds through a series of phased traffic lights.\nThe UK's TRL has developed junction modelling programs for small-scale local schemes that can take account of detailed geometry and sight lines; ARCADY for roundabouts, PICADY for priority intersections, and OSCADY and TRANSYT for signals. Many other junction analysis software packages exist such as Sidra and LinSig and Synchro.\n\nThe kinematic wave model was first applied to traffic flow by Lighthill and Whitham in 1955. Their two-part paper first developed the theory of kinematic waves using the motion of water as an example. In the second half, they extended the theory to traffic on “crowded arterial roads.” This paper was primarily concerned with developing the idea of traffic “humps” (increases in flow) and their effects on speed, especially through bottlenecks.\n\nThe authors began by discussing previous approaches to traffic flow theory. They note that at the time there had been some experimental work, but that “theoretical approaches to the subject [were] in their infancy.” One researcher in particular, John Glen Wardrop, was primarily concerned with statistical methods of examination, such as space mean speed, time mean speed, and “the effect of increase of flow on overtaking” and the resulting decrease in speed it would cause. Other previous research had focused on two separate models: one related traffic speed to traffic flow and another related speed to the headway between vehicles.\n\nThe goal of Lighthill and Whitham, on the other hand, was to propose a new method of study “suggested by theories of the flow about supersonic projectiles and of flood movement in rivers.” The resulting model would capture both of the aforementioned relationships, speed-flow and speed-headway, into a single curve, which would “[sum] up all the properties of a stretch of road which are relevant to its ability to handle the flow of congested traffic.” The model they presented related traffic flow to concentration (now typically known as density). They wrote, “The fundamental hypothesis of the theory is that at any point of the road the flow q (vehicles per hour) is a function of the concentration k (vehicles per mile).” According to this model, traffic flow resembled the flow of water in that “Slight changes in flow are propagated back through the stream of vehicles along ‘kinematic waves,’ whose velocity relative to the road is the slope of the graph of flow against concentration.” The authors included an example of such a graph; this flow-versus-concentration (density) plot is still used today (see figure 3 above).\n\nThe authors used this flow-concentration model to illustrate the concept of shock waves, which slow down vehicles which enter them, and the conditions that surround them. They also discussed bottlenecks and intersections, relating both to their new model. For each of these topics, flow-concentration and time-space diagrams were included. Finally, the authors noted that no agreed-upon definition for capacity existed, and argued that it should be defined as the “maximum flow of which the road is capable.” Lighthill and Whitham also recognized that their model had a significant limitation: it was only appropriate for use on long, crowded roadways, as the “continuous flow” approach only works with a large number of vehicles.\n\nThe kinematic wave model of traffic flow theory is the simplest dynamic traffic flow model that reproduces the propagation of traffic waves. It is made up of three components: the fundamental diagram, the conservation equation, and initial conditions. The law of conservation is the fundamental law governing the kinematic wave model:\n\nformula_77\n\nThe fundamental diagram of the kinematic wave model relates traffic flow with density, as seen in figure 3 above. It can be written as:\n\nformula_78\n\nFinally, initial conditions must be defined to solve a problem using the model. A boundary is defined to be formula_79, representing density as a function of time and position. These boundaries typically take two different forms, resulting in initial value problems (IVPs) and boundary value problems (BVPs). Initial value problems give the traffic density at time formula_80, such that formula_81, where formula_82 is the given density function. Boundary value problems give some function formula_83 that represents the density at the formula_84 position, such that formula_85.\nThe model has many uses in traffic flow. One of the primary uses is in modeling traffic bottlenecks, as described in the following section.\n\nIn the condition of traffic flows leaving two branch roadways and merging into a single flow through a single roadway, determining the flows that pass through the merging process and the state of each branch of roadways becomes an important task for traffic engineers. the Newell-Daganzo merge model is a good approach to solve these problems. This simple model is the output of the result of both Gordon Newell's description of the merging process and the Daganzo's cell transmission model. In order to apply the model to determine the flows which exiting two branch of roadways and the stat of each branch of roadways, one needs to know the capacities of the two input branches of roadways, the exiting capacity, the demands for each branch of roadways, and the number of lanes of the single roadway. The merge ratio will be calculated in order to determine the proportion of the two input flows when both of branches of roadway are operating in congested conditions.\n\nAs can be seen in a simplified model of the process of merging, the exiting capacity of the system is defined to be μ, the capacities of the two input branches of roadways are defined as μ and μ, and the demands for each branch of roadways are defined as q and q. The q and q are the output of the model which are the flows that pass through the merging process. The process of the model is based on the assumption that the sum of capacities of the two input branches of roadways is less than the exiting capacity of the system, μ+μ ≤ μ.\n\nThe flows that pass through the merging process, q and q, are determined by split priority or, merge ratio. The state of each branch of roadways is determined by graphically with the input of the demands for each branch of roadways, q and q. There are four possible states for the merge system, both inlets in free flow, one of the inlets in congestion, and both inlets in congestion.\n\nA common approach to calculate the merge ratio p is called \"zipper rule\" which p is calculated based on the number of lanes of the single roadway when both inlets are in congestion. If there are n lanes of the single roadway, then under the zipper rule, p=1/(2n-1). This merge ratio is also the ratio of minimum capacities of the inlets μ and μ. μ + μ = μ. As a result, q all in a fan shape, called rarefaction (see figure e). This model implies that the users later on in time will take longer to accelerate as they meet each of the lines. Instead a better approximation is a triangular diagram where the traffic increases abruptly as it would when a driver sees an opening in front of them (see figures f and g).\n\nIn a critical review, Kerner explained that generally accepted classical fundamentals and methodologies of traffic and transportation theory are inconsistent with the set of fundamental empirical features of traffic breakdown at a highway bottleneck.\n\nThe set of fundamental empirical features of traffic breakdown at a highway bottleneck is as follows:\n\n\n\nA spontaneous traffic breakdown occurs, where there are free flows both upstream and downstream of the bottleneck before the breakdown has occurred. In contrast, an induced traffic breakdown is caused by a propagation of a congested pattern that has earlier emerged for example at another downstream bottleneck.\n\nEmpirical data that illustrates the set of fundamental empirical features of traffic breakdown at highway bottlenecks as well as explanations of the empirical data can be found in Wikipedia article Kerner’s breakdown minimization principle and in review.\n\nThe generally accepted classical fundamentals and methodologies of traffic and transportation theory are as follows:\n\n(i) The Lighthill-Whitham-Richards (LWR) model introduced in 1955–56. Daganzo introduced a cell-transmission model (CTM) that is consistent with the LWR model.\n\n(ii) A traffic flow instability that causes a growing wave of a local reduction of the vehicle speed. This classical traffic flow instability was introduced in 1959–61 in the General Motors (GM) car-following model by Herman, Gazis, Montroll, Potts, and Rothery. The classical traffic flow instability of the GM model has been incorporated in a huge number of traffic flow models like Gipps's model, Payne's model, Newell's optimal velocity (OV) model, Wiedemann's model, Whitham's model, the Nagel-Schreckenberg (NaSch) cellular automaton (CA) model, Bando et al. OV model, Treiber's IDM, Krauß model, the Aw-Rascle model and many other well-known microscopic and macroscopic traffic-flow models, which are the basis of traffic simulation tools widely used by traffic engineers and researchers (see, e.g., references in review).\n\n(iii) The understanding of highway capacity as a \"particular\" value. This understanding of road capacity was probably introduced in 1920–35 (see ). Currently, it is assumed that highway capacity of free flow at a highway bottleneck is a stochastic value. However, in accordance with the classical understanding of highway capacity, it is assumed that at a given time instant there can be only one particular value of this stochastic highway capacity (see references in the book).\n\n(iv) Wardrop's user equilibrium (UE) and system optimum (SO) principles for traffic and transportation network optimization and control.\n\nKerner explains the failure of the generally accepted classical traffic flow theories as follows:\n\n1. The LWR-theory fails because this theory cannot show empirical induced traffic breakdown observed in real traffic. Correspondingly, all applications of LWR-theory to the description of traffic breakdown at highway bottlenecks (like related applications of Daganzo’s cell-transmission model, cumulative vehicle count curves (\"N\"-curves), bottleneck model, highway capacity models as well as associated applications of kinematic wave theory) are also inconsistent with the set of fundamental empirical features of traffic breakdown.\n\n2. Two-phase traffic flow models of the GM model class (see references in) fail because traffic breakdown in the models of the GM class is a phase transition from free flow (\"F\") to a moving jam (\"J\") (called F → J transition): In a traffic flow model belonging to the GM model class due to traffic breakdown, a moving jam(s) appears spontaneously in an initially free flow at a highway bottleneck. In contrast with this model result, real traffic breakdown is a phase transition from free flow (\"F\") to synchronized flow (\"S\") (called F → S transition): Rather than a moving jam(s), due to traffic breakdown in real traffic, synchronized flow occurs whose downstream front is fixed at the bottleneck.\n\n3. The understanding of highway capacity as a particular value (see references in the book ) fails because this assumption about the nature of highway capacity contradicts the empirical evidence that traffic breakdown can be induced at a highway bottleneck.\n\n4. Dynamic traffic assignment or/and any kind of traffic optimization and control based on Wardrop's SO or UE principles fail because of possible random transitions between the free flow and synchronized flow at highway bottlenecks. Due to such random transitions, the minimization of travel cost in a traffic network is not possible.\n\nAccording to Kerner, the inconsistence the generally accepted classical fundamentals and methodologies of traffic and transportation theory with the set of fundamental empirical features of traffic breakdown at a highway bottleneck can explain why network optimization and control approaches based on these fundamentals and methodologies have failed by their applications in the real world. Even several decades of a very intensive effort to improve and validate network optimization models have no success. Indeed, there can be found no examples where on-line implementations of the network optimization models based on these fundamentals and methodologies could reduce congestion in real traffic and transportation networks.\n\nThis is due to the fact that the fundamental empirical features of traffic breakdown at highway bottlenecks have been understood only during last 20 years. In contrast, the generally accepted fundamentals and methodologies of traffic and transportation theory have been introduced in the 50s-60s. Thus the scientists whose ideas led to these classical fundamentals and methodologies of traffic and transportation theory could not know the set of empirical features of real traffic breakdown.\n\nThe explanation of traffic breakdown at a highway bottleneck by a F → S transition in a metastable free flow at the bottleneck is the basic assumption of Kerner’s three-phase traffic theory. The three-phase traffic theory is consistent with the set of fundamental empirical features of traffic breakdown.\n\"None\" of earlier traffic-flow theories incorporates a F→S transition in a metastable free flow at the bottleneck. Therefore, as above mentioned none of the classical traffic flow theories is consistent with the set of empirical features of real traffic breakdown at a highway bottleneck. \nThe F→S phase transition in metastable free flow at highway bottleneck does explain the empirical evidence of the induced transition from free flow to synchronized flow together with the flow-rate dependence of the breakdown probability. In accordance with the classical book by Kuhn, this shows \"the incommensurability\" of three-phase theory and the classical traffic-flow theories (for more details, see ):\n\nThe term \"incommensurability\" has been introduced by Kuhn in his classical book to explain a paradigm shift in a scientific field. It must also be noted that the existence of these two traffic phases, free flow (\"F\") and synchronized flow (\"S\") at the same flow rate does not result from the stochastic nature of traffic: Even if there were no stochastic processes in vehicular traffic, the states \"F\" and \"S\" do exist at the same flow rate. However, classical stochastic approaches to traffic control do not assume a possibility of an F→S phase transition in metastable free flow. For this reason, these stochastic approaches cannot resolve the problem of the inconsistence of classical theories with the set of empirical features of real traffic breakdown.\n\n\nA survey about the state of art in traffic flow modelling:\n\n\nUseful books from the physical point of view:\n\n\n"}
{"id": "148633", "url": "https://en.wikipedia.org/wiki?curid=148633", "title": "Value at risk", "text": "Value at risk\n\nValue at risk (VaR) is a measure of the risk of loss for investments. It estimates how much a set of investments might lose (with a given probability), given normal market conditions, in a set time period such as a day. VaR is typically used by firms and regulators in the financial industry to gauge the amount of assets needed to cover possible losses.\n\nFor a given portfolio, time horizon, and probability \"p\", the \"p\" VaR can be defined informally as the maximum possible loss during the time if we exclude worse outcomes whose probability is less than \"p\". This assumes mark-to-market pricing, and no trading in the portfolio.\n\nFor example, if a portfolio of stocks has a one-day 5% VaR of $1 million, that means that there is a 0.05 probability that the portfolio will fall in value by more than $1 million over a one-day period if there is no trading. Informally, a loss of $1 million or more on this portfolio is expected on 1 day out of 20 days (because of 5% probability). A loss which exceeds the VaR threshold is termed a \"VaR breach\".\n\nMore formally, \"p\" VaR is defined such that the probability of a loss greater than VaR is less than or equal to \"p\" while the probability of a loss less than VaR is less than or equal to 1−\"p\". For instance, assume someone makes a bet that flipping a coin seven times will not give seven heads. The terms are that he gains $100 if it doesn't happen (with probability 127/128) but loses $12,700 if it does (with probability 1/128). The 1% VaR is then -100, because the probability that he loses more than that is less than 1% while the probability that he loses less than zero (which is impossible) is less than 99%.\n\nVaR has four main uses in finance: risk management, financial control, financial reporting and computing regulatory capital. VaR is sometimes used in non-financial applications as well.\n\nImportant related ideas are economic capital, backtesting, stress testing, expected shortfall, and tail conditional expectation.\n\nCommon parameters for VaR are 1% and 5% probabilities and one day and two week horizons, although other combinations are in use.\n\nThe reason for assuming normal markets and no trading, and to restricting loss to things measured in daily accounts, is to make the loss observable. In some extreme financial events it can be impossible to determine losses, either because market prices are unavailable or because the loss-bearing institution breaks up. Some longer-term consequences of disasters, such as lawsuits, loss of market confidence and employee morale and impairment of brand names can take a long time to play out, and may be hard to allocate among specific prior decisions. VaR marks the boundary between normal days and extreme events. Institutions can lose far more than the VaR amount; all that can be said is that they will not do so very often.\n\nThe probability level is about equally often specified as one minus the probability of a VaR break, so that the VaR in the example above would be called a one-day 95% VaR instead of one-day 5% VaR. This generally does not lead to confusion because the probability of VaR breaks is almost always small, certainly less than 50%.\n\nAlthough it virtually always represents a loss, VaR is conventionally reported as a positive number. A negative VaR would imply the portfolio has a high probability of making a profit, for example a one-day 5% VaR of negative $1 million implies the portfolio has a 95% chance of making more than $1 million over the next day.\n\nAnother inconsistency is that VaR is sometimes taken to refer to profit-and-loss at the end of the period, and sometimes as the maximum loss at any point during the period. The original definition was the latter, but in the early 1990s when VaR was aggregated across trading desks and time zones, end-of-day valuation was the only reliable number so the former became the \"de facto\" definition. As people began using multiday VaRs in the second half of the 1990s, they almost always estimated the distribution at the end of the period only. It is also easier theoretically to deal with a point-in-time estimate versus a maximum over an interval. Therefore, the end-of-period definition is the most common both in theory and practice today.\n\nThe definition of VaR is nonconstructive; it specifies a property VaR must have, but not how to compute VaR. Moreover, there is wide scope for interpretation in the definition. This has led to two broad types of VaR, one used primarily in risk management and the other primarily for risk measurement. The distinction is not sharp, however, and hybrid versions are typically used in financial control, financial reporting and computing regulatory capital.\n\nTo a risk manager, VaR is a system, not a number. The system is run periodically (usually daily) and the published number is compared to the computed price movement in opening positions over the time horizon. There is never any subsequent adjustment to the published VaR, and there is no distinction between VaR breaks caused by input errors (including Information Technology breakdowns, fraud and rogue trading), computation errors (including failure to produce a VaR on time) and market movements.\n\nA frequentist claim is made, that the long-term frequency of VaR breaks will equal the specified probability, within the limits of sampling error, and that the VaR breaks will be independent in time and independent of the level of VaR. This claim is validated by a backtest, a comparison of published VaRs to actual price movements. In this interpretation, many different systems could produce VaRs with equally good backtests, but wide disagreements on daily VaR values.\n\nFor risk measurement a number is needed, not a system. A Bayesian probability claim is made, that given the information and beliefs at the time, the subjective probability of a VaR break was the specified level. VaR is adjusted after the fact to correct errors in inputs and computation, but not to incorporate information unavailable at the time of computation. In this context, \"backtest\" has a different meaning. Rather than comparing published VaRs to actual market movements over the period of time the system has been in operation, VaR is retroactively computed on scrubbed data over as long a period as data are available and deemed relevant. The same position data and pricing models are used for computing the VaR as determining the price movements.\n\nAlthough some of the sources listed here treat only one kind of VaR as legitimate, most of the recent ones seem to agree that risk management VaR is superior for making short-term and tactical decisions today, while risk measurement VaR should be used for understanding the past, and making medium term and strategic decisions for the future. When VaR is used for financial control or financial reporting it should incorporate elements of both. For example, if a trading desk is held to a VaR limit, that is both a risk-management rule for deciding what risks to allow today, and an input into the risk measurement computation of the desk's risk-adjusted return at the end of the reporting period.\n\nVaR can also be applied to governance of endowments, trusts, and pension plans. Essentially trustees adopt portfolio Values-at-Risk metrics for the entire pooled account and the diversified parts individually managed. Instead of probability estimates they simply define maximum levels of acceptable loss for each. Doing so provides an easy metric for oversight and adds accountability as managers are then directed to manage, but with the additional constraint to avoid losses within a defined risk parameter. VaR utilized in this manner adds relevance as well as an easy way to monitor risk measurement control far more intuitive than Standard Deviation of Return. Use of VaR in this context, as well as a worthwhile critique on board governance practices as it relates to investment management oversight in general can be found in \"Best Practices in Governance.\"\n\nThe VaR of formula_1 at the confidence level formula_2 is the smallest number formula_3 such that the probability that formula_4 does not exceed formula_3 is at least formula_6. Mathematically, formula_7 is the formula_8-quantile of formula_9, i.e.,\n\nThis is the most general definition of VaR and the two identities are equivalent (indeed, for any random variable formula_1 its cumulative distribution function formula_12 is well defined). \nHowever this formula cannot be used directly for calculations unless we assume that formula_1 has some parametric distribution.\n\nRisk managers typically assume that some fraction of the bad events will have undefined losses, either because markets are closed or illiquid, or because the entity bearing the loss breaks apart or loses the ability to compute accounts. Therefore, they do not accept results based on the assumption of a well-defined probability distribution. Nassim Taleb has labeled this assumption, \"charlatanism\". On the other hand, many academics prefer to assume a well-defined distribution, albeit usually one with fat tails. This point has probably caused more contention among VaR theorists than any other.\n\nValue of Risks can also be written as a distortion risk measure given by the distortion function formula_14\n\nThe term \"VaR\" is used both for a risk measure and a risk metric. This sometimes leads to confusion. Sources earlier than 1995 usually emphasize the risk measure, later sources are more likely to emphasize the metric.\n\nThe VaR risk measure defines risk as mark-to-market loss on a fixed portfolio over a fixed time horizon. There are many alternative risk measures in finance. Given the inability to use mark-to-market (which uses market prices to define loss) for future performance, loss is often defined (as a substitute) as change in fundamental value. For example, if an institution holds a loan that declines in market price because interest rates go up, but has no change in cash flows or credit quality, some systems do not recognize a loss. Also some try to incorporate the economic cost of harm not measured in daily financial statements, such as loss of market confidence or employee morale, impairment of brand names or lawsuits.\n\nRather than assuming a static portfolio over a fixed time horizon, some risk measures incorporate the dynamic effect of expected trading (such as a stop loss order) and consider the expected holding period of positions.\n\nThe VaR risk metric summarizes the distribution of possible losses by a quantile, a point with a specified probability of greater losses. A common alternative metrics is expected shortfall.\n\nSupporters of VaR-based risk management claim the first and possibly greatest benefit of VaR is the improvement in systems and modeling it forces on an institution. In 1997, Philippe Jorion wrote:[T]he greatest benefit of VAR lies in the imposition of a structured methodology for critically thinking about risk. Institutions that go through the process of computing their VAR are forced to confront their exposure to financial risks and to set up a proper risk management function. Thus the process of getting to VAR may be as important as the number itself.\n\nPublishing a daily number, on-time and with specified statistical properties holds every part of a trading organization to a high objective standard. Robust backup systems and default assumptions must be implemented. Positions that are reported, modeled or priced incorrectly stand out, as do data feeds that are inaccurate or late and systems that are too-frequently down. Anything that affects profit and loss that is left out of other reports will show up either in inflated VaR or excessive VaR breaks. \"A risk-taking institution that \"does not\" compute VaR might escape disaster, but an institution that \"cannot\" compute VaR will not.\"\n\nThe second claimed benefit of VaR is that it separates risk into two regimes. Inside the VaR limit, conventional statistical methods are reliable. Relatively short-term and specific data can be used for analysis. Probability estimates are meaningful, because there are enough data to test them. In a sense, there is no true risk because you have a sum of many independent observations with a left bound on the outcome. A casino doesn't worry about whether red or black will come up on the next roulette spin. Risk managers encourage productive risk-taking in this regime, because there is little true cost. People tend to worry too much about these risks, because they happen frequently, and not enough about what might happen on the worst days.\n\nOutside the VaR limit, all bets are off. Risk should be analyzed with stress testing based on long-term and broad market data. Probability statements are no longer meaningful. Knowing the distribution of losses beyond the VaR point is both impossible and useless. The risk manager should concentrate instead on making sure good plans are in place to limit the loss if possible, and to survive the loss if not.\n\nOne specific system uses three regimes.\n\n\n\"A risk manager has two jobs: make people take more risk the 99% of the time it is safe to do so, and survive the other 1% of the time. VaR is the border.\"\n\nAnother reason VaR is useful as a metric is due to its ability to compress the riskiness of a portfolio to a single number, making it comparable across different portfolios (of different assets). Within any portfolio it is also possible to isolate specific position that might better hedge the portfolio to reduce, and minimise, the VaR. An example of market-maker employed strategies for trading linear interest rate derivatives and interest rate swaps portfolios is cited.\n\nVaR can be estimated either parametrically (for example, variance-covariance VaR or delta-gamma VaR) or nonparametrically (for examples, historical simulation VaR or resampled VaR). Nonparametric methods of VaR estimation are discussed in Markovich and Novak. A comparison of a number of strategies for VaR prediction is given in Kuester et al.\n\nA McKinsey report published in May 2012 estimated that 85% of large banks were using historical simulation. The other 15% used Monte Carlo methods.\n\nA key advantage to VaR over most other measures of risk such as expected shortfall is the availability several backtesting procedures for validating a set of VaR forecasts. Early examples of backtests can be found in Christoffersen (1998), later generalized by Pajhede (2017), which models a \"hit-sequence\" of losses greater than the VaR and proceed to tests for these \"hit's\" to be independent from one another and with a correct probability of occurring. E.g. a 5% probability of a loss greater than VaR should be observed over time when using a 95% VaR, these hits should occur independently.\n\nA number of other backtests are available which model the time between hits in the hit-sequence, see Christoffersen (2014), Haas (2016), Tokpavi et al. (2014). and Pajhede (2017) As pointed out in several of the papers, the asymptotic distribution is often poor when considering high levels of coverage, e.g. a 99% VaR, therefore the parametric bootstrap method of Dufour (2006) is often used to obtain correct size properties for the tests. Backtest toolboxes are available in Matlab , or R—though only the first implements the parametric bootstrap method.\n\nThe problem of risk measurement is an old one in statistics, economics and finance. Financial risk management has been a concern of regulators and financial executives for a long time as well. Retrospective analysis has found some VaR-like concepts in this history. But VaR did not emerge as a distinct concept until the late 1980s. The triggering event was the stock market crash of 1987. This was the first major financial crisis in which a lot of academically-trained quants were in high enough positions to worry about firm-wide survival.\n\nThe crash was so unlikely given standard statistical models, that it called the entire basis of quant finance into question. A reconsideration of history led some quants to decide there were recurring crises, about one or two per decade, that overwhelmed the statistical assumptions embedded in models used for trading, investment management and derivative pricing. These affected many markets at once, including ones that were usually not correlated, and seldom had discernible economic cause or warning (although after-the-fact explanations were plentiful). Much later, they were named \"Black Swans\" by Nassim Taleb and the concept extended far beyond finance.\n\nIf these events were included in quantitative analysis they dominated results and led to strategies that did not work day to day. If these events were excluded, the profits made in between \"Black Swans\" could be much smaller than the losses suffered in the crisis. Institutions could fail as a result.\n\nVaR was developed as a systematic way to segregate extreme events, which are studied qualitatively over long-term history and broad market events, from everyday price movements, which are studied quantitatively using short-term data in specific markets. It was hoped that \"Black Swans\" would be preceded by increases in estimated VaR or increased frequency of VaR breaks, in at least some markets. The extent to which this has proven to be true is controversial.\n\nAbnormal markets and trading were excluded from the VaR estimate in order to make it observable. It is not always possible to define loss if, for example, markets are closed as after 9/11, or severely illiquid, as happened several times in 2008. Losses can also be hard to define if the risk-bearing institution fails or breaks up. A measure that depends on traders taking certain actions, and avoiding other actions, can lead to self reference.\n\nThis is risk management VaR. It was well established in quantitative trading groups at several financial institutions, notably Bankers Trust, before 1990, although neither the name nor the definition had been standardized. There was no effort to aggregate VaRs across trading desks.\n\nThe financial events of the early 1990s found many firms in trouble because the same underlying bet had been made at many places in the firm, in non-obvious ways. Since many trading desks already computed risk management VaR, and it was the only common risk measure that could be both defined for all businesses and aggregated without strong assumptions, it was the natural choice for reporting firmwide risk. J. P. Morgan CEO Dennis Weatherstone famously called for a \"4:15 report\" that combined all firm risk on one page, available within 15 minutes of the market close.\n\nRisk measurement VaR was developed for this purpose. Development was most extensive at J. P. Morgan, which published the methodology and gave free access to estimates of the necessary underlying parameters in 1994. This was the first time VaR had been exposed beyond a relatively small group of quants. Two years later, the methodology was spun off into an independent for-profit business now part of RiskMetrics Group (now part of MSCI).\n\nIn 1997, the U.S. Securities and Exchange Commission ruled that public corporations must disclose quantitative information about their derivatives activity. Major banks and dealers chose to implement the rule by including VaR information in the notes to their financial statements.\n\nWorldwide adoption of the Basel II Accord, beginning in 1999 and nearing completion today, gave further impetus to the use of VaR. VaR is the preferred measure of market risk, and concepts similar to VaR are used in other parts of the accord.\n\nVaR has been controversial since it moved from trading desks into the public eye in 1994. A famous 1997 debate between Nassim Taleb and Philippe Jorion set out some of the major points of contention. Taleb claimed VaR:\n\n\nIn 2008 David Einhorn and Aaron Brown debated VaR in Global Association of Risk Professionals Review Einhorn compared VaR to \"an airbag that works all the time, except when you have a car accident\". He further charged that VaR:\n\n\nNew York Times reporter Joe Nocera wrote an extensive piece Risk Mismanagement on January 4, 2009 discussing the role VaR played in the Financial crisis of 2007-2008. After interviewing risk managers (including several of the ones cited above) the article suggests that VaR was very useful to risk experts, but nevertheless exacerbated the crisis by giving false security to bank executives and regulators. A powerful tool for professional risk managers, VaR is portrayed as both easy to misunderstand, and dangerous when misunderstood.\n\nTaleb in 2009 testified in Congress asking for the banning of VaR for a number of reasons. One was that tail risks are non-measurable. Another was that for anchoring reasons VaR leads to higher risk taking.\n\nVaR is not subadditive: VaR of a combined portfolio can be larger than the sum of the VaRs of its components.\n\nFor example, the average bank branch in the United States is robbed about once every ten years. A single-branch bank has about 0.0004% chance of being robbed on a specific day, so the risk of robbery would not figure into one-day 1% VaR. It would not even be within an order of magnitude of that, so it is in the range where the institution should not worry about it, it should insure against it and take advice from insurers on precautions. The whole point of insurance is to aggregate risks that are beyond individual VaR limits, and bring them into a large enough portfolio to get statistical predictability. It does not pay for a one-branch bank to have a security expert on staff.\n\nAs institutions get more branches, the risk of a robbery on a specific day rises to within an order of magnitude of VaR. At that point it makes sense for the institution to run internal stress tests and analyze the risk itself. It will spend less on insurance and more on in-house expertise. For a very large banking institution, robberies are a routine daily occurrence. Losses are part of the daily VaR calculation, and tracked statistically rather than case-by-case. A sizable in-house security department is in charge of prevention and control, the general risk manager just tracks the loss like any other cost of doing business.\nAs portfolios or institutions get larger, specific risks change from low-probability/low-predictability/high-impact to statistically predictable losses of low individual impact. That means they move from the range of far outside VaR, to be insured, to near outside VaR, to be analyzed case-by-case, to inside VaR, to be treated statistically.\n\nVaR is a static measure of risk. By definition, VaR is a particular characteristic of the probability distribution of the underlying (namely, VaR is essentially a quantile). For a dynamic measure of risk, see Novak, ch. 10.\n\nThere are common abuses of VaR:\n\n\nThe VaR is not a coherent risk measure since it violates the sub-additivity property, which is\n\nformula_15\n\nHowever, it can be bounded by coherent risk measures like Conditional Value-at-Risk (CVaR) or entropic value at risk (EVaR). In fact, for formula_16 (with formula_17 the set of all Borel measurable functions whose moment-generating function exists for all positive real values) we have\n\nformula_18\n\nwhere\n\nformula_19\n\nin which formula_20 is the moment-generating function of formula_21 at formula_22. In the above equations the variable formula_1 denotes the financial loss, rather than wealth as is typically the case.\n\n\n"}
{"id": "38246823", "url": "https://en.wikipedia.org/wiki?curid=38246823", "title": "Wald's martingale", "text": "Wald's martingale\n\nIn probability theory Wald's martingale, named after Abraham Wald and more commonly\nknown as the geometric Brownian motion, is a stochastic process of the form\n\nfor any real value \"λ\" where \"W\" is a Wiener process. The process is a martingale.\n\n"}
{"id": "212619", "url": "https://en.wikipedia.org/wiki?curid=212619", "title": "Whitehead problem", "text": "Whitehead problem\n\nIn group theory, a branch of abstract algebra, the Whitehead problem is the following question:\n\nShelah (1974) proved that Whitehead's problem is undecidable within standard ZFC set theory.\n\nThe condition Ext(\"A\", Z) = 0 can be equivalently formulated as follows: whenever \"B\" is an abelian group and \"f\" : \"B\" → \"A\" is a surjective group homomorphism whose kernel is isomorphic to the group of integers Z, then there exists a group homomorphism \"g\" : \"A\" → \"B\" with \"fg\" = id. Abelian groups \"A\" satisfying this condition are sometimes called Whitehead groups, so Whitehead's problem asks: is every Whitehead group free?\n\n\"Caution\": The converse of Whitehead's problem, namely that every free abelian group is Whitehead, is a well known group-theoretical fact. Some authors call \"Whitehead group\" only a \"non-free\" group \"A\" satisfying Ext(\"A\", Z) = 0. Whitehead's problem then asks: do Whitehead groups exist?\n\n showed that, given the canonical ZFC axiom system, the problem is independent of the usual axioms of set theory. More precisely, he showed that:\nSince the consistency of ZFC implies the consistency of both of the following:\nWhitehead's problem cannot be resolved in ZFC.\n\nJ. H. C. Whitehead, motivated by the second Cousin problem, first posed the problem in the 1950s. answered the question in the affirmative for countable groups. Progress for larger groups was slow, and the problem was considered an important one in algebra for some years.\n\nShelah's result was completely unexpected. While the existence of undecidable statements had been known since Gödel's incompleteness theorem of 1931, previous examples of undecidable statements (such as the continuum hypothesis) had all been in pure set theory. The Whitehead problem was the first purely algebraic problem to be proved undecidable.\n\n\n"}
