{"id": "2885420", "url": "https://en.wikipedia.org/wiki?curid=2885420", "title": "159 (number)", "text": "159 (number)\n\n159 (one hundred [and] fifty-nine) is a natural number following 158 and preceding 160.\n\n159 is:\n\n\nGiven 159, the Mertens function returns 0.\n\n\n\n\n\n\n159 is also:\n\n\n"}
{"id": "13546761", "url": "https://en.wikipedia.org/wiki?curid=13546761", "title": "3G MIMO", "text": "3G MIMO\n\n3G MIMO describes MIMO techniques which have been considered as 3G standard techniques.\nMIMO, as the state of the art of Intelligent antenna (IA), improves the performance of radio systems by embedding electronics intelligence into the spatial processing unit. Spatial processing includes spatial precoding at the transmitter and spatial postcoding at the receiver, which are dual each other from information signal processing theoretic point of view. Intelligent antenna is technology which represents smart antenna, multiple antenna (MIMO), self-tracking directional antenna, cooperative virtual antenna and so on.\n\nSpatial precoding of intelligent antenna includes spatial beamforming and spatial coding. In wireless communications, spatial precoding has been developing for high reliability, high rate and lower interference as shown in the following table.\n\nThe table summarizes the history of 3G MIMO techniques candidated for 3G standards. Although the table additionally contains the future part but the contents are not clearly filled out since the future is not precisely predictable.\n\nIA technology enables client terminals, which have either multiple antennas or a self-tracking directional antenna, to communicate to each other with as high as possible signal-to-interference-and-noise ratio (SINR). Assume that there is a source terminal, a destination terminal, and some candidate interference terminals. Compared to conventional approaches, an advanced IA based terminal will perform spatial precoding (spatial beamforming and/or spatial coding) not only to enhance the signal power at the destination terminal but also to diminish the interfering power at interference terminals. As a human does, the advanced IA terminal is given to know that occurring high interference to other terminals will eventually degrade the performance of the associated wireless network.\n\nHowever, it requires intelligent multiple or cooperative antenna array. On the contrary, cognitive radio (CR) allows user terminals to sense the other service usage of spectrum beans to share the spectrum among users, which is so, cognitive spectrum sharing technology. The following table compares the different points between two approach for future wireless systems: Intelligent antenna (IA) vs. Cognitive radio (CR).\n\n\nThe following items list the issues of the multiple antenna research aims to improve the performance of radio communications.\n\n\nHere are the definition of principal keywords to clarify the objective and the operations of intelligent antenna.\n\nThe following items list the web sites related to the multiple antenna research.\n\n\n\n\n"}
{"id": "57774049", "url": "https://en.wikipedia.org/wiki?curid=57774049", "title": "Annick Horiuchi", "text": "Annick Horiuchi\n\nAnnick Mito Horiuchi is a French historian of mathematics and historian of science. She is a professor at Paris Diderot University, where she is associated with the Centre de recherche sur les civilisations de l'Asie orientale (CRCAO).\n\nHoriuchi completed a doctorate in 1990; her dissertation, \"Etude de seki takakazu (?-1708) et takebe katahiro (1664-1739), deux mathematiciens de l'epoque d'edo\", was directed by Paul Akamatsu.\nShe was an invited speaker at the 1990 International Congress of Mathematicians.\n\nHoriuchi's books include:\n"}
{"id": "22852793", "url": "https://en.wikipedia.org/wiki?curid=22852793", "title": "Asset/liability modeling", "text": "Asset/liability modeling\n\nAsset/liability modelling is defined as the process used to manage the business and financial objectives of a financial institution or an individual through an assessment of the portfolio assets and liabilities in an integrated manner. The process is characterized by an on-going review, modification, and revision of asset liability management strategies so that sensitivity to interest rate changes are confined within acceptable tolerance levels. There are different models used and some use different elements, according to specific needs and contexts. For instance, an individual or an organization may keep parts of the ALM process and outsource the modeling function or adapt the model according to the requirements and capabilities of relevant institutions such as banks, which often have their in-house modeling process. For pensioners, asset/liability modeling is all about determining the best allocation for specific situations. There is a vast array of models available today for practical asset and liability modeling and these have been the subject of several research and studies. \n\nThe ongoing financial crisis drove the 100 largest corporate pension plans to a record $300 billion loss of funded status in 2008. In the wake of these losses, many pension plan sponsors have been led to re-examine their pension plan asset allocation strategies, to consider the risk exposures to the plans and to the sponsors. A recent study indicates that many corporate defined benefit plans fail to address the full range of risks facing them, especially the ones related to liabilities. Too often, the study says, corporate pensions are distracted by concerns that have nothing to do with the long-term health of the fund. Asset/liability modeling is an approach to examining pension risks and allows the sponsor to set informed policies for funding, benefit design, and asset allocation.\n\nAsset/liability modeling goes beyond traditional, asset-only analysis of the asset allocation decision. Traditional asset-only models analyze risk and reward in terms of investment performance. Asset/liability models take a comprehensive approach to analyze risk and reward in terms of the overall pension plan impact. An actuary or investment consultant may look at expectations and downside risk measures on the present value of contributions, plan surplus, excess returns (asset return less liability return), asset returns, and any number of other variables. The model may consider measures over 5, 10 or 20 year horizons, as well as quarterly or annual value at risk measures.\n\nPension plans face a variety of liability risks including price and wage inflation risk, interest rate risk and longevity risk. While some of these risks materialize slowly over time, others – such as interest rate risk – are felt with each measurement period. Liabilities are the actuarial present value of future plan cash flows, discounted at current interest rates. Thus, asset/liability management strategies often include bonds and swaps or other derivatives to accomplish some degree of interest rate hedging (immunization, cash flow matching, duration matching, etc.). Such approaches are sometimes called “liability-driven investment” (LDI) strategies. In 2008, plans with such approaches strongly outperformed those with traditional “total return” seeking investment policies.\n\nSuccessful asset/liability studies:\nIncrease a plan sponsor’s understanding of the pension plan’s current situation and likely future trends\n\nHistorically, most pension plan sponsors conducted comprehensive asset/liability studies every three to five years or after a significant change in demographics, plan design, funded status, sponsor circumstances, or funding legislation. Recent trends suggest more frequent studies, and/or a desire for regular tracking of key asset/liability risk metrics in between formal studies.\n\nIn the United States, the Pension Protection Act of 2006 (PPA) has introduced stricter standards on pension plans, requiring higher funding targets and larger contributions from plan sponsors. With growing deficits and PPA funding requirements looming large, there is an unprecedented need for asset/liability modeling and overall pension risk management.\n\nSome financial advisors offer Monte Carlo simulation tools aimed at helping individuals model the odds they will be able to retire when they want with the amount of money they want. These tools are designed to model the individual’s likelihood of assets surpassing expenses (liabilities).\n\nProponents of Monte Carlo simulation contend that these tools are valuable because they offer simulation using randomly ordered returns based on a set of reasonable parameters. For example, the tool can model retirement cash flows 500 or 1,000 times, thus reflecting a range of possible outcomes.\n\nSome critics of these tools claim that the consequences of failure are not laid out and argue that these tools are no better than typical retirement tools that use standard assumptions. Recent financial turmoil has fueled the claims of critics who believe that Monte Carlo simulation tools are inaccurate and overly optimistic.\n\n"}
{"id": "10283346", "url": "https://en.wikipedia.org/wiki?curid=10283346", "title": "Backtesting", "text": "Backtesting\n\nBacktesting is a term used in modeling to refer to testing a predictive model on historical data. Backtesting is a type of retrodiction, and a special type of cross-validation applied to previous time period(s).\n\nIn a trading strategy, investment strategy, or risk modeling, backtesting seeks to estimate the performance of a strategy or model if it had been employed during a past period. This requires simulating past conditions with sufficient detail, making one limitation of backtesting the need for detailed historical data. A second limitation is the inability to model strategies that would affect historic prices. Finally, backtesting, like other modeling, is limited by potential overfitting. That is, it is often possible to find a strategy that would have worked well in the past, but will not work well in the future. Despite these limitations, backtesting provides information not available when models and strategies are tested on synthetic data.\n\nBacktesting has historically only been performed by large institutions and professional money managers due to the expense of obtaining and using detailed datasets. However, backtrading is increasingly used on a wider basis, and independent web-based backtesting platforms have emerged. Although the technique is widely used, it is prone to weaknesses. Basel financial regulations require large financial institutions to backtest certain risk models.\n\nIn oceanography and meteorology, \"backtesting\" is also known as \"hindcasting\": a hindcast is a way of testing a mathematical model; researchers enter known or closely estimated inputs for past events into the model to see how well the output matches the known results.\n\nHindcasting usually refers to a numerical-model integration of a historical period where no observations have been assimilated. This distinguishes a hindcast run from a reanalysis. Oceanographic observations of salinity and temperature as well as observations of surface-wave parameters such as the significant wave height are much scarcer than meteorological observations, making hindcasting more common in oceanography than in meteorology. Also, since surface waves represent a forced system where the wind is the only generating force, wave hindcasting is often considered adequate for generating a reasonable representation of the wave climate with little need for a full reanalysis. Hydrologists use hindcasting for model stream flows.\n\nAn example of hindcasting would be entering climate forcings (events that force change) into a climate model. If the hindcast showed reasonably-accurate climate response, the model would be considered successful.\n\nThe ECMWF re-analysis is an example of a combined atmospheric reanalysis coupled with a wave-model integration where no wave parameters were assimilated, making the wave part a hindcast run.\n\nIn 2003, Dake Chen and his colleagues “trained” a computer using the data of the surface temperature of the oceans from the last 20 years. Then, using data that had been collected on the surface temperature of the oceans for the period 1857 to 2003, they went through a hindcasting exercise and discovered that their simulation not only accurately predicted every El Niño event for the last 148 years, it also identified the (up to 2 years) looming foreshadow of every single one of those El Niño events.\n"}
{"id": "1574901", "url": "https://en.wikipedia.org/wiki?curid=1574901", "title": "Cardinality of the continuum", "text": "Cardinality of the continuum\n\nIn set theory, the cardinality of the continuum is the cardinality or \"size\" of the set of real numbers formula_1, sometimes called the continuum. It is an infinite cardinal number and is denoted by formula_2 or formula_3 (a lowercase fraktur script \"c\").\n\nThe real numbers formula_1 are more numerous than the natural numbers formula_5. Moreover, formula_1 has the same number of elements as the power set of formula_5. Symbolically, if the cardinality of formula_5 is denoted as formula_9, the cardinality of the continuum is\n\nThis was proven by Georg Cantor in his 1874 uncountability proof, part of his groundbreaking study of different infinities; the inequality was later stated more simply in his diagonal argument. Cantor defined cardinality in terms of bijective functions: two sets have the same cardinality if and only if there exists a bijective function between them.\n\nBetween any two real numbers \"a\" < \"b\", no matter how close they are to each other, there are always infinitely many other real numbers, and Cantor showed that they are as many as those contained in the whole set of real numbers. In other words, the open interval (\"a\",\"b\") is equinumerous with formula_10 This is also true for several other infinite sets, such as any \"n\"-dimensional Euclidean space formula_11 (see space filling curve). That is,\nThe smallest infinite cardinal number is formula_9 (aleph-null). The second smallest is formula_13 (aleph-one). The continuum hypothesis, which asserts that there are no sets whose cardinality is strictly between formula_9 and implies that formula_15.\n\nGeorg Cantor introduced the concept of cardinality to compare the sizes of infinite sets. He famously showed that the set of real numbers is uncountably infinite; i.e. formula_16 is strictly greater than the cardinality of the natural numbers, formula_9:\n\nIn other words, there are strictly more real numbers than there are integers. Cantor proved this statement in several different ways. See Cantor's first uncountability proof and Cantor's diagonal argument.\n\nA variation on Cantor's diagonal argument can be used to prove Cantor's theorem which states that the cardinality of any set is strictly less than that of its power set, i.e. formula_18, and so the power set formula_19 of the natural numbers formula_5 is uncountable. In fact, it can be shown that the cardinality of formula_19 is equal to formula_16:\nBy the Cantor–Bernstein–Schroeder theorem we conclude that\nThe cardinal equality formula_41 can be demonstrated using cardinal arithmetic:\n\nBy using the rules of cardinal arithmetic one can also show that\n\nwhere \"n\" is any finite cardinal ≥ 2, and\n\nwhere formula_42 is the cardinality of the power set of R, and formula_43.\n\nEvery real number has at least one infinite decimal expansion. For example,\n\nIn any given case, the number of digits is countable since they can be put into a one-to-one correspondence with the set of natural numbers formula_45. This fact makes it sensible to talk about (for example) the first, the one-hundredth, or the millionth digit of π. Since the natural numbers have cardinality formula_46 each real number has formula_9 digits in its expansion.\n\nSince each real number can be broken into an integer part and a decimal fraction, we get\nsince\nOn the other hand, if we map formula_48 to formula_49 and consider that decimal fractions containing only 3 or 7 are only a part of the real numbers, then we get\nand thus\n\nThe sequence of beth numbers is defined by setting formula_50 and formula_51. So formula_16 is the second beth number, beth-one:\n\nThe third beth number, beth-two, is the cardinality of the power set of R (i.e. the set of all subsets of the real line):\nThe famous continuum hypothesis asserts that formula_16 is also the second aleph number formula_13. In other words, the continuum hypothesis states that there is no set formula_55 whose cardinality lies strictly between formula_9 and formula_16\n\nThis statement is now known to be independent of the axioms of Zermelo–Fraenkel set theory with the axiom of choice (ZFC). That is, both the hypothesis and its negation are consistent with these axioms. In fact, for every nonzero natural number \"n\", the equality formula_16 = formula_59 is independent of ZFC (the case formula_60 is the continuum hypothesis). The same is true for most other alephs, although in some cases equality can be ruled out by König's theorem on the grounds of cofinality, e.g., formula_61 In particular, formula_62 could be either formula_13 or formula_64, where formula_65 is the first uncountable ordinal, so it could be either a successor cardinal or a limit cardinal, and either a regular cardinal or a singular cardinal.\n\nA great many sets studied in mathematics have cardinality equal to formula_16. Some common examples are the following:\nNow we show the cardinality of an infinite interval. For all formula_67 we can define the bijection\n\nand similarly for all formula_68\n\nWe note that the set of real algebraic numbers is countably infinite (assign to each formula its Gödel number.) So the cardinality of the real algebraic numbers is formula_9. Furthermore, the real algebraic numbers and the real transcendental numbers are disjoint sets whose union is formula_1. Thus, since the cardinality of formula_1 is formula_3, the cardinality of the real transcendental numbers is formula_73. A similar result follows for complex transcendental numbers, once we have proved that formula_74.\n\nWe note that, per Cantor's proof of the cardinality of Euclidean space, formula_77. By definition, any formula_78 can be uniquely expressed as formula_79 for some formula_80. We therefore define the bijection\n\nSets with cardinality greater than formula_16 include:\n\n\nThese all have cardinality formula_108 (beth two).\n\n"}
{"id": "1062532", "url": "https://en.wikipedia.org/wiki?curid=1062532", "title": "Codress message", "text": "Codress message\n\nIn military cryptography, a codress message is an encrypted message whose address is also encrypted. This is usually done to prevent traffic analysis.\n"}
{"id": "11502578", "url": "https://en.wikipedia.org/wiki?curid=11502578", "title": "Convex lattice polytope", "text": "Convex lattice polytope\n\nA convex lattice polytope (also called Z-polyhedron or Z-polytope) is a geometric object playing an important role in discrete geometry and combinatorial commutative algebra. It is a polytope in a Euclidean space R which is a convex hull of finitely many points in the integer lattice Z ⊂ R. Such objects are prominently featured in the theory of toric varieties, where they correspond to polarized projective toric varieties.\n\n\n\n"}
{"id": "47824679", "url": "https://en.wikipedia.org/wiki?curid=47824679", "title": "Deblurring", "text": "Deblurring\n\nDeblurring is the process of removing blurring artifacts from images, such as blur caused by defocus aberration or motion blur. The blur is typically modeled as the convolution of a (sometimes space- or time-varying) point spread function with a hypothetical sharp input image, where both the sharp input image (which is to be recovered) and the point spread function are unknown. This is an example of an inverse problem. In almost all cases, there is insufficient information in the blurred image to uniquely determine a plausible original image, making it an ill-posed problem. In addition the blurred image contains additional noise which complicates the task of determining the original image. This is generally solved by the use of a regularization term to attempt to eliminate implausible solutions. This problem is analogous to echo removal in the signal processing domain. Nevertheless, when coherent beam is used for imaging, the point spread function can be modeled mathematically. As the figure on the right illustrates, by proper deconvolution of the point spread function and the image, the resolution can be enhanced several times.\n\n"}
{"id": "54501194", "url": "https://en.wikipedia.org/wiki?curid=54501194", "title": "Derived tensor product", "text": "Derived tensor product\n\nIn algebra, given a differential graded algebra \"A\" over a commutative ring \"R\", the derived tensor product functor is\nwhere formula_2 and formula_3 are the categories of right \"A\"-modules and left \"A\"-modules and \"D\" refers to the homotopy category (i.e., derived category). By definition, it is the left derived functor of the tensor product functor formula_4.\n\n\n"}
{"id": "38267", "url": "https://en.wikipedia.org/wiki?curid=38267", "title": "Dimension (vector space)", "text": "Dimension (vector space)\n\nIn mathematics, the dimension of a vector space \"V\" is the cardinality (i.e. the number of vectors) of a basis of \"V\" over its base field. It is sometimes called Hamel dimension (after Georg Hamel) or algebraic dimension to distinguish it from other types of dimension.\n\nFor every vector space there exists a basis, and all bases of a vector space have equal cardinality; as a result, the dimension of a vector space is uniquely defined. We say \"V\" is ' if the dimension of \"V\" is finite, and ' if its dimension is infinite.\n\nThe dimension of the vector space \"V\" over the field \"F\" can be written as dim(\"V\") or as [V : F], read \"dimension of \"V\" over \"F\"\". When \"F\" can be inferred from context, dim(\"V\") is typically written.\n\nThe vector space R has \n\nas a basis, and therefore we have dim(R) = 3. More generally, dim(R) = \"n\", and even more generally, dim(\"F\") = \"n\" for any field \"F\".\n\nThe complex numbers C are both a real and complex vector space; we have dim(C) = 2 and dim(C) = 1. So the dimension depends on the base field.\n\nThe only vector space with dimension 0 is {0}, the vector space consisting only of its zero element.\n\nIf \"W\" is a linear subspace of \"V\", then dim(\"W\") ≤ dim(\"V\").\n\nTo show that two finite-dimensional vector spaces are equal, one often uses the following criterion: if \"V\" is a finite-dimensional vector space and \"W\" is a linear subspace of \"V\" with dim(\"W\") = dim(\"V\"), then \"W\" = \"V\".\n\nR has the standard basis {e, ..., e}, where e is the \"i\"-th column of the corresponding identity matrix. Therefore R \nhas dimension \"n\".\n\nAny two vector spaces over \"F\" having the same dimension are isomorphic. Any bijective map between their bases can be uniquely extended to a bijective linear map between the vector spaces. If \"B\" is some set, a vector space with dimension |\"B\"| over \"F\" can be constructed as follows: take the set \"F\" of all functions \"f\" : \"B\" → \"F\" such that \"f\"(\"b\") = 0 for all but finitely many \"b\" in \"B\". These functions can be added and multiplied with elements of \"F\", and we obtain the desired \"F\"-vector space. \n\nAn important result about dimensions is given by the rank–nullity theorem for linear maps.\n\nIf \"F\"/\"K\" is a field extension, then \"F\" is in particular a vector space over \"K\". Furthermore, every \"F\"-vector space \"V\" is also a \"K\"-vector space. The dimensions are related by the formula\nIn particular, every complex vector space of dimension \"n\" is a real vector space of dimension 2\"n\".\n\nSome simple formulae relate the dimension of a vector space with the cardinality of the base field and the cardinality of the space itself.\nIf \"V\" is a vector space over a field \"F\" then, denoting the dimension of \"V\" by dim \"V\", we have:\n\nOne can see a vector space as a particular case of a matroid, and in the latter there is a well-defined notion of dimension. The length of a module and the rank of an abelian group both have several properties similar to the dimension of vector spaces.\n\nThe Krull dimension of a commutative ring, named after Wolfgang Krull (1899–1971), is defined to be the maximal number of strict inclusions in an increasing chain of prime ideals in the ring.\n\nThe dimension of a vector space may alternatively be characterized as the trace of the identity operator. For instance, formula_2 This appears to be a circular definition, but it allows useful generalizations.\n\nFirstly, it allows one to define a notion of dimension when one has a trace but no natural sense of basis. For example, one may have an algebra \"A\" with maps formula_3 (the inclusion of scalars, called the \"unit\") and a map formula_4 (corresponding to trace, called the \"counit\"). The composition formula_5 is a scalar (being a linear operator on a 1-dimensional space) corresponds to \"trace of identity\", and gives a notion of dimension for an abstract algebra. In practice, in bialgebras one requires that this map be the identity, which can be obtained by normalizing the counit by dividing by dimension (formula_6), so in these cases the normalizing constant corresponds to dimension.\n\nAlternatively, one may be able to take the trace of operators on an infinite-dimensional space; in this case a (finite) trace is defined, even though no (finite) dimension exists, and gives a notion of \"dimension of the operator\". These fall under the rubric of \"trace class operators\" on a Hilbert space, or more generally nuclear operators on a Banach space.\n\nA subtler generalization is to consider the trace of a \"family\" of operators as a kind of \"twisted\" dimension. This occurs significantly in representation theory, where the character of a representation is the trace of the representation, hence a scalar-valued function on a group formula_7 whose value on the identity formula_8 is the dimension of the representation, as a representation sends the identity in the group to the identity matrix: formula_9 One can view the other values formula_10 of the character as \"twisted\" dimensions, and find analogs or generalizations of statements about dimensions to statements about characters or representations. A sophisticated example of this occurs in the theory of monstrous moonshine: the \"j\"-invariant is the graded dimension of an infinite-dimensional graded representation of the Monster group, and replacing the dimension with the character gives the McKay–Thompson series for each element of the Monster group.\n\n\n"}
{"id": "634785", "url": "https://en.wikipedia.org/wiki?curid=634785", "title": "Dissipation factor", "text": "Dissipation factor\n\nIn physics, the dissipation factor (DF) is a measure of loss-rate of energy of a mode of oscillation (mechanical, electrical, or electromechanical) in a dissipative system. It is the reciprocal of quality factor, which represents the \"quality\" or durability of oscillation.\n\nElectrical potential energy is dissipated in all dielectric materials, usually in the form of heat. In a capacitor made of a dielectric placed between conductors, the typical lumped element model includes a lossless ideal capacitor in series with a resistor termed the equivalent series resistance (ESR) as shown below. The ESR represents losses in the capacitor. In a good capacitor the ESR is very small, and in a poor capacitor the ESR is large. However, ESR is sometimes a minimum value to be required. Note that the ESR is \"not\" simply the resistance that would be measured across a capacitor by an ohmmeter. The ESR is a derived quantity with physical origins in both the dielectric's conduction electrons and dipole relaxation phenomena. In dielectric only one of either the conduction electrons or the dipole relaxation typically dominates loss. For the case of the conduction electrons being the dominant loss, then\n\nformula_1\n\nwhere\n\nIf the capacitor is used in an AC circuit, the dissipation factor due to the non-ideal capacitor is expressed as the ratio of the resistive power loss in the ESR to the reactive power oscillating in the capacitor, or\n\nformula_6\n\nWhen representing the electrical circuit parameters as vectors in a complex plane, known as phasors, a capacitor's dissipation factor is equal to the tangent of the angle between the capacitor's impedance vector and the negative reactive axis, as shown in the adjacent diagram. This gives rise to the parameter known as the loss tangent tan \"δ\" where\n\nformula_7\n\nAlternatively, \"ESR\" can be derived from frequency at which loss tangent was determined and capacitance\n\nformula_8\n\nSince the \"DF\" in a good capacitor is usually small, \"δ\" ~ \"DF\", and \"DF\" is often expressed as a percentage.\n\n\"DF\" approximates to the power factor when formula_9 is far less than formula_10, which is usually the case.\n\n\"DF\" will vary depending on the dielectric material and the frequency of the electrical signals. In low dielectric constant (low-κ), temperature compensating ceramics, \"DF\" of 0.1% to 0.2% is typical. In high dielectric constant ceramics, \"DF\" can be 1% to 2%. However, lower \"DF\" is usually an indication of quality capacitors when comparing similar dielectric material.\n\n"}
{"id": "7260876", "url": "https://en.wikipedia.org/wiki?curid=7260876", "title": "Ehrling's lemma", "text": "Ehrling's lemma\n\nIn mathematics, Ehrling's lemma is a result concerning Banach spaces. It is often used in functional analysis to demonstrate the equivalence of certain norms on Sobolev spaces. It was proposed by Gunnar Ehrling.\n\nLet (\"X\", ||·||), (\"Y\", ||·||) and (\"Z\", ||·||) be three Banach spaces. Assume that:\nThen, for every \"ε\" > 0, there exists a constant \"C\"(\"ε\") such that, for all \"x\" ∈ \"X\",\n\nLet Ω ⊂ R be open and bounded, and let \"k\" ∈ N. Suppose that the Sobolev space \"H\"(Ω) is compactly embedded in \"H\"(Ω). Then the following two norms on \"H\"(Ω) are equivalent:\n\nand\n\nFor the subspace of \"H\"(Ω) consisting of those Sobolev functions with zero trace (those that are \"zero on the boundary\" of Ω), the \"L\" norm of \"u\" can be left out to yield another equivalent norm.\n"}
{"id": "29115268", "url": "https://en.wikipedia.org/wiki?curid=29115268", "title": "Escaping set", "text": "Escaping set\n\nIn mathematics, and particularly complex dynamics, the escaping set of an entire function ƒ consists of all points that tend to infinity under the repeated application of ƒ.\nThat is, a complex number formula_1 belongs to the escaping set if and only if the sequence defined by formula_2 converges to infinity as formula_3 gets large. The escaping set of formula_4 is denoted by formula_5.\n\nFor example, for formula_6, the origin belongs to the escaping set, since the sequence \ntends to infinity.\n\nThe iteration of transcendental entire functions was first studied by Pierre Fatou in 1926\nThe escaping set occurs implicitly in his study of the explicit entire functions formula_8 and formula_9.\nThe first study of the escaping set for a general transcendental entire function is due to Alexandre Eremenko who used Wiman-Valiron theory.\nHe conjectured that every connected component of the escaping set of a transcendental entire function is unbounded. This has become known\nas \"Eremenko's conjecture\". There are many partial results\non this problem but as of 2013 the conjecture is still open.\n\nEremenko also asked whether every escaping point can be connected to infinity by a curve in the escaping set; it was later shown that this is not the case. Indeed,\nthere exist entire functions whose escaping sets do not contain any curves at all.\n\nThe following properties are known to hold for the escaping set of any non-constant and non-linear entire function. (Here \"nonlinear\" means that the function is not of the form formula_10.)\n\n\nNote that the final statement does not imply Eremenko's Conjecture. (Indeed, there exist connected spaces in which the removal of a single dispersion point leaves the remaining space totally disconnected.)\n\nFor a polynomial of degree at least 2, the point at infinity is an (super-)attracting fixed point, and the escaping set is precisely the basin of attraction of this fixed point ( infinity). Hence in this case, formula_5 is an open and connected subset of the complex plane, and the Julia set is the boundary of this basin.\n\nFor instance the escaping set of the complex quadratic polynomial formula_14 consists precisely of those points whose absolute value is greater than 1\n\nFor transcendental entire functions, the escaping set is much more complicated than for polynomials: in the simplest cases like the one illustrated in the picture it consists on uncountably many curves, called \"hairs\" or \"rays\". In other examples the structure of the escaping set can be very different (a \"spider's web\"). As mentioned above, there are examples of entire functions whose escaping set contains no curves.\n\n"}
{"id": "8009218", "url": "https://en.wikipedia.org/wiki?curid=8009218", "title": "Fibonacci numbers in popular culture", "text": "Fibonacci numbers in popular culture\n\nThe Fibonacci numbers are a sequence of integers, starting with 0, 1 and continuing 1, 2, 3, 5, 8, 13, ..., each new number being the sum of the previous two. The Fibonacci numbers, and in conjunction the golden ratio, are a popular theme in culture. They have been mentioned in novels, films, television shows, and songs. The numbers have also been used in the creation of music, visual art, and architecture.\n\n\n\n\n\nJohn Waskom postulated that stages of human development followed the Fibonacci sequence, and that the unfolding psychology of human life would ideally be a \"living proof\" of the Golden Mean. This theory was originally developed and published by Norman Rose in two articles. The first article, which laid out the general theory, was entitled \"Design and Development of Wholeness: Waskom's Paradigm.\" The second article laid out the applications and implications of the theory to the topic of moral development, and was entitled \"Moral Development: The Experiential Perspective.\"\n\n\n<poem>\n</poem>\n\n\n\n\n"}
{"id": "35735143", "url": "https://en.wikipedia.org/wiki?curid=35735143", "title": "First case of Fermat's Last Theorem", "text": "First case of Fermat's Last Theorem\n\nThe first case of Fermat's last theorem says that for three integers \"x\", \"y\" and \"z\" and a prime number \"p\", where \"p\" does not divide the product \"xyz\", there are no solutions to the equation \"x\" + \"y\" + \"z\" = 0.\n\nUsing the Theorem of unique factorization of ideals in Q(ξ) it was shown that if the first case has solutions \"x\", \"y\", \"z\", then \"x\"+\"y\"+\"z\" is divisible by \"p\" and (\"x\", \"y\"), (\"y\", \"z\") and (\"z\", \"x\") are elements of \"H\", where \"H\" denotes a set of pairs of integers with special properties.\n\n"}
{"id": "6838326", "url": "https://en.wikipedia.org/wiki?curid=6838326", "title": "Ghost Leg", "text": "Ghost Leg\n\nGhost Leg (), known in Japan as or in Korea as Sadaritagi (사다리타기, literally \"ladder climbing\"), is a method of lottery designed to create random pairings between two sets of any number of things, as long as the number of elements in each set is the same. This is often used to distribute things among people, where the number of things distributed is the same as the number of people. For instance, chores or prizes could be assigned fairly and randomly this way.\n\nIt consists of vertical lines with horizontal lines connecting two adjacent vertical lines scattered randomly along their length; the horizontal lines are called \"legs\". The number of vertical lines equals the number of people playing, and at the bottom of each line there is an item - a thing that will be paired with a player. The general rule for playing this game is: choose a line on the top, and follow this line downwards. When a horizontal line is encountered, follow it to get to another vertical line and continue downwards. Repeat this procedure until reaching the end of the vertical line. Then the player is given the thing written at the bottom of the line.\n\nIf the elements written above the Ghost Leg are treated as a sequence, and after the Ghost Leg is used, the same elements are written at the bottom, then the starting sequence has been transformed to another permutation. Hence, Ghost Leg can be treated as a kind of permuting operator.\n\nAs an example, consider assigning roles in a play to actors.\n\n\nAnother process involves creating the ladder beforehand, then concealing it. Then people take turns choosing a path to start from at the top. If no part of the amidakuji is concealed, then it is possible to fix the system so that you are guaranteed to get a certain pairing, thus defeating the idea of random chance.\n\nPart of the appeal for this game is that, unlike random chance games like rock, paper, scissors, amidakuji will always create a 1:1 correspondence, and can handle arbitrary numbers of pairings (although pairing sets with only two items each would be fairly boring). It is guaranteed that two items at the top will never have the same corresponding item at the bottom, nor will any item on the bottom ever lack a corresponding item at the top.\n\nIt also works regardless of how many horizontal lines are added. Each person could add one, two, three, or any number of lines, and the 1:1 correspondence would remain.\n\nOne way of realizing how this works is to consider the analogy of coins in cups. You have \"n\" coins in \"n\" cups, representing the items at the bottom of the amidakuji. Then, each leg that is added represents swapping the position of two adjacent cups. Thus, it is obvious that in the end there will still be \"n\" cups, and each cup will have one coin, regardless of how many swaps you perform.\n\nA Ghost Leg transforms an input sequence into an output sequence with the same number of elements with (possibly) different order.\nThus, it can be regarded a permutation of \"n\" symbols, where \"n\" is the number of vertical lines in the Ghost Leg., hence it can be represented by the corresponding permutation matrix.\n\nApplying a Ghost Leg a finite number of times to an input sequence eventually generates an output sequence identical to the original input sequence.\n\ni.e. If M is a matrix representing a particular Ghost Leg, then M=I for some finite \"n\".\n\nFor any Ghost Leg with matrix representation M, there exists a Ghost Leg with representation M,\nsuch that M M=I\n\nAs each leg exchanges the two neighboring elements at its ends, the number of legs indicates the odd/even permutation property of the Ghost Leg. An odd number of legs represents an odd permutation, and an even number of legs gives an even permutation.\n\nIt is possible to express every permutation as a Ghost Leg, but the expression is not one-to-one, i.e. a particular permutation does not correspond to a unique Ghost Leg.\nAn infinite number of Ghost Legs represents the same permutation.\n\nAs there are an infinite number of Ghost Legs representing a particular permutation, it is obvious those Ghost Legs have a kind of equivalence. Among those equivalent Ghost Legs, the one(ones) which have smallest number of legs are called Prime.\n\nA Ghost Leg can be constructed arbitrarily, but such a Ghost Leg is not necessarily prime. It can be proven that only those Ghost Legs constructed by bubble sort contains the least number of legs, and hence is prime. This is equivalent to saying that bubble sort performs the minimum number of adjacent exchanges to sort a sequence.\n\nFor a permutation with \"n\" elements, the maximum number of neighbor exchanging = formula_1\n\nIn the same way, the maximum number of legs in a prime with \"n\" tracks = formula_1\n\nFor an arbitrary Ghost Leg, it is possible to transform it into prime by a procedure called bubblization.\nWhen bubblization operates, the following two identities are repeatedly applied in order to move and eliminate \"useless\" legs.\n\nWhen the two identities cannot be applied any more, the ghost leg is proven to be exactly the same as the Ghost Leg constructed by bubble sort, thus bubblization can reduce Ghost Legs to primes.\n\nSince, as mentioned above, an odd number of legs produces an odd permutation and an even number of legs produces an even permutation, a given number of legs can produce a maximum of half the total possible permutations (less than half if the number of legs is small relative to the number of tracks, reaching half as the number of legs increases beyond a certain critical number).\n\nIf the legs are drawn randomly (for reasonable definitions of \"drawn randomly\"), the evenness of the distribution of permutations increases with the number of legs. If the number of legs is small relative to number of tracks, the probabilities of different attainable permutations may vary greatly; for large numbers of legs the probabilities of different attainable permutations approach equality.\n\nThe 1981 arcade game Amidar programmed by Konami and published by Stern used the same lattice as a maze. The game even took its name from Amidakuji and most of the enemy movement conformed to the lot drawing game's rules\n\nAn early Sega Master System game called \"Psycho Fox\" uses the mechanics of an Amidakuji board as a means to bet a bag of coins on a chance at a prize at the top of the screen. Later Sega Genesis games based on the same game concept DecapAttack and its Japanese predecessor \"Magical Hat no Buttobi Tabo! Daibōken\" follow the same game mechanics, including the Amidakuji bonus levels. \n\n\"\" features an Amidakuji-style bonus game that rewards the player with a power-up. \"New Super Mario Bros.\" and \"\" feature an Amidakuji-style minigame in which the player uses the stylus to trace lines that will lead the character down the right path.\n\nIn \"Mario Party\" there is a mini game where one of the four players pours money into an Amidakuji made out of pipes. The goal is to try to choose the path leading to the character controlled by the player.\n\nThe BoSpider in \"Mega Man X\" and \"Maverick Hunter X\" descends onto the player via an Amidakuji path.\n\nIn \"Super Monkey Ball 2\", there is a level in the Advanced-Extra difficulty named \"Amida Lot\" (Advanced-EX 7) that features a floor resembling an Amidakuji board, which bumper travels around the way and may knock off the player if they happen to hit them. The goal only travels through one of the vertical lines and the player must reach the goal using the ghost legs while avoiding the bumpers to not fall out.\n\nIn \"\", the microgame \"Noodle Cup\" features Amidakuji-style gameplay.\n\nAzalea Gym in \"Pokémon HeartGold\" and \"SoulSilver\" was redesigned with an Amidakuji-based system of carts to cross. The correct choices lead to the gym leader; the wrong ones lead to other trainers to fight.\n\n\"Phantasy Star Online 2\" uses the principle of Amidakuji for a randomly appearing bomb-defusing minigame. One must trace an Amidakuji path around each bomb to determine which button defuses it; incorrect selections knock players away for a few seconds, wasting time.\n\nIn the manga \"Liar Game\" (vol 17), an Amidakuji is used for determinate the rank of each participant to the penultimate stage of the game.\n\nIn the Japanese drama \"Don Quixote\" (episode 10), the character Shirota (Shota Matsuda) uses Amidakuji to help decide between candidate families for an adoption.\n\nIn the anime \"Cardcaptor Sakura\" (episode 41), the character Kaho Mizuki, Sakura's teacher, uses Amidakuji in order to choose which student will play each role in a certain school play.\n\nIn the anime \" Magic Kyun Renassiance\" (episode 10), the characters used Amidakuji to determine which rooms they'd get while at the villa after the Art Session.\n\n"}
{"id": "23552810", "url": "https://en.wikipedia.org/wiki?curid=23552810", "title": "Half-precision floating-point format", "text": "Half-precision floating-point format\n\nIn computing, half precision is a binary floating-point computer number format that occupies 16 bits (two bytes in modern computers) in computer memory.\n\nIn the IEEE 754-2008 standard, the 16-bit base-2 format is referred to as binary16. It is intended for storage of floating-point values in applications where higher precision is not essential for performing arithmetic computations.\n\nAlthough implementations of the IEEE Half-precision floating point are relatively new, several earlier 16-bit floating point formats have existed including that of Hitachi's HD61810 DSP of 1982, Scott's WIF and the 3dfx Voodoo Graphics processor.\n\nNvidia and Microsoft defined the half datatype in the Cg language, released in early 2002, and implemented it in silicon in the GeForce FX, released in late 2002. ILM was searching for an image format that could handle a wide dynamic range, but without the hard drive and memory cost of floating-point representations that are commonly used for floating-point computation (single and double precision). The hardware-accelerated programmable shading group led by John Airey at SGI (Silicon Graphics) invented the s10e5 data type in 1997 as part of the 'bali' design effort. This is described in a SIGGRAPH 2000 paper (see section 4.3) and further documented in US patent 7518615.\n\nThis format is used in several computer graphics environments including OpenEXR, JPEG XR, GIMP, OpenGL, Cg, and D3DX. The advantage over 8-bit or 16-bit binary integers is that the increased dynamic range allows for more detail to be preserved in highlights and shadows for images. The advantage over 32-bit single-precision binary formats is that it requires half the storage and bandwidth (at the expense of precision and range).\n\nThe F16C extension allows x86 processors to convert half-precision floats to and from single-precision floats.\n\nThe IEEE 754 standard specifies a binary16 as having the following format:\n\nThe format is laid out as follows:\n\nThe format is assumed to have an implicit lead bit with value 1 unless the exponent field is stored with all zeros. Thus only 10 bits of the significand appear in the memory format but the total precision is 11 bits. In IEEE 754 parlance, there are 10 bits of significand, but there are 11 bits of significand precision (log(2) ≈ 3.311 decimal digits, or 4 digits ± slightly less than 5 units in the last place).\n\nThe half-precision binary floating-point exponent is encoded using an offset-binary representation, with the zero offset being 15; also known as exponent bias in the IEEE 754 standard.\n\n\nThus, as defined by the offset binary representation, in order to get the true exponent the offset of 15 has to be subtracted from the stored exponent.\n\nThe stored exponents 00000 and 11111 are interpreted specially.\n\nThe minimum strictly positive (subnormal) value is\n2 ≈ 5.96 × 10.\nThe minimum positive normal value is 2 ≈ 6.10 × 10.\nThe maximum representable value is (2−2) × 2 = 65504.\n\nThese examples are given in bit representation\nof the floating-point value. This includes the sign bit, (biased) exponent, and significand.\n\nBy default, 1/3 rounds down like for double precision, because of the odd number of bits in the significand. So the bits beyond the rounding point are codice_1 which is less than 1/2 of a unit in the last place.\n\n\n\n\nARM processors support (via a floating point control register bit) an \"alternative half-precision\" format, which does away with the special case for an exponent value of 31 (11111). It is almost identical to the IEEE format, but there is no encoding for infinity or NaNs; instead, an exponent of 31 encodes normalized numbers in the range 65536 to 131008.\n\n\n\n"}
{"id": "194123", "url": "https://en.wikipedia.org/wiki?curid=194123", "title": "Heron's formula", "text": "Heron's formula\n\nIn geometry, Heron's formula (sometimes called Hero's formula), named after Hero of Alexandria, gives the area of a triangle when the length of all three sides are known. Unlike other formulas, there is no need to calculate other distances in the triangle first.\n\nHeron's formula states that the area of a triangle whose sides have lengths , , and is\n\nwhere is the semiperimeter of the triangle; that is,\n\nHeron's formula can also be written as\n\nLet be the triangle with sides , and . \nThe semiperimeter is , and the area is\n\nIn this example, the side lengths and area are all integers, making it a Heronian triangle. However, Heron's formula works equally well in cases where one or all of these numbers is not an integer.\n\nThe formula is credited to Heron (or Hero) of Alexandria, and a proof can be found in his book, \"Metrica\", written CE 60. It has been suggested that Archimedes knew the formula over two centuries earlier, and since \"Metrica\" is a collection of the mathematical knowledge available in the ancient world, it is possible that the formula predates the reference given in that work.\n\nA formula equivalent to Heron's, namely\n\nwas discovered by the Chinese independently of the Greeks. It was published in \"Shushu Jiuzhang\" (“Mathematical Treatise in Nine Sections”), written by Qin Jiushao and published in 1247.\n\nHeron's original proof made use of cyclic quadrilaterals, while other arguments appeal to trigonometry as below, or to the incenter and one excircle of the triangle .\n\nA modern proof, which uses algebra and is quite unlike the one provided by Heron (in his book Metrica), follows.\nLet , , be the sides of the triangle and , , the angles opposite those sides.\nApplying the law of cosines we get\n\nFrom this proof we get the algebraic statement that\n\nThe altitude of the triangle on base has length , and it follows\n\nThe difference of two squares factorization was used in two different steps.\n\nThe following proof is very similar to one given by Raifaizen.\nBy the Pythagorean theorem we have and according to the figure at the right. Subtracting these yields . This equation allows us to express in terms of the sides of the triangle:\nFor the height of the triangle we have that . By replacing with the formula given above and applying the difference of squares identity we get\n\nWe now apply this result to the formula that calculates the area of a triangle from its height:\n\nFrom the first part of the Law of cotangents proof, we have that the triangle's area is both\nand , but, since the sum of the half-angles is , the triple cotangent identity applies, so the first of these is\n\nCombining the two, we get\nfrom which the result follows.\n\nHeron's formula as given above is numerically unstable for triangles with a very small angle when using floating point arithmetic. A stable alternative involves arranging the lengths of the sides so that and computing\nThe brackets in the above formula are required in order to prevent numerical instability in the evaluation.\n\nThree other area formulae have the same structure as Heron's formula but are expressed in terms of different variables. First, denoting the medians from sides , , and respectively as , , and and their semi-sum as , we have\n\nNext, denoting the altitudes from sides , , and respectively as , , and , and denoting the semi-sum of the reciprocals of the altitudes as we have\n\nFinally, denoting the semi-sum of the angles' sines as , we have\n\nwhere is the diameter of the circumcircle: .\n\nHeron's formula is a special case of Brahmagupta's formula for the area of a cyclic quadrilateral. Heron's formula and Brahmagupta's formula are both special cases of Bretschneider's formula for the area of a quadrilateral. Heron's formula can be obtained from Brahmagupta's formula or Bretschneider's formula by setting one of the sides of the quadrilateral to zero.\n\nHeron's formula is also a special case of the formula for the area of a trapezoid or trapezium based only on its sides. Heron's formula is obtained by setting the smaller parallel side to zero.\n\nExpressing Heron's formula with a Cayley–Menger determinant in terms of the squares of the distances between the three given vertices,\nillustrates its similarity to Tartaglia's formula for the volume of a three-simplex.\n\nAnother generalization of Heron's formula to pentagons and hexagons inscribed in a circle was discovered by David P. Robbins.\n\nIf , , , , , are lengths of edges of the tetrahedron (first three form a triangle; opposite to and so on), then\nwhere\n\n\n"}
{"id": "27318656", "url": "https://en.wikipedia.org/wiki?curid=27318656", "title": "Hoeffding's independence test", "text": "Hoeffding's independence test\n\nIn statistics, Hoeffding's test of independence, named after Wassily Hoeffding, is a test based on the population measure of deviation from independence\n\nwhere formula_2 is the joint distribution function of two random variables, and formula_3 and formula_4 are their marginal distribution functions.\nHoeffding derived an unbiased estimator of formula_5 that can be used to test for independence, and is consistent for any continuous alternative. The test should only be applied to data drawn from a continuous distribution, since formula_5 has a defect for discontinuous formula_2, namely that it is not necessarily zero when formula_8.\n\nA recent paper describes both the calculation of a sample based version of this measure for use as a test statistic, and calculation of the null distribution of this test statistic.\n\n\n"}
{"id": "26666199", "url": "https://en.wikipedia.org/wiki?curid=26666199", "title": "Hypercomplex analysis", "text": "Hypercomplex analysis\n\nIn mathematics, hypercomplex analysis is the extension of real analysis and complex analysis to the study of functions where the argument is a hypercomplex number. The first instance is functions of a quaternion variable, where the argument is a quaternion. A second instance involves functions of a motor variable where arguments are split-complex numbers.\n\nIn mathematical physics, there are hypercomplex systems called Clifford algebras. The study of functions with arguments from a Clifford algebra is called Clifford analysis.\n\nA matrix may be considered a hypercomplex number. For example, study of functions of 2 × 2 real matrices shows that the topology of the space of hypercomplex numbers determines the function theory. Functions such as square root of a matrix, matrix exponential, and logarithm of a matrix are basic examples of hypercomplex analysis. \nThe function theory of diagonalizable matrices is particularly transparent since they have eigendecompositions. Suppose formula_1 where the E are projections. Then for any polynomial formula_2 \n\nModern terminology is \"algebra\" for \"system of hypercomplex numbers\", and the algebras used in applications are often Banach algebras since Cauchy sequences can be taken to be convergent. Then the function theory is enriched by sequences and series. In this context the extension of holomorphic functions of a complex variable is developed as the holomorphic functional calculus. Hypercomplex analysis on Banach algebras is called functional analysis.\n\n\n\n"}
{"id": "34846125", "url": "https://en.wikipedia.org/wiki?curid=34846125", "title": "Ib Holm Sørensen", "text": "Ib Holm Sørensen\n\nDr Ib Holm Sørensen (1949–2012) was a computer scientist who made contributions to the Z notation and B-Method.\n\nOriginally from Denmark, Ib Sørensen started his academic career in the 1970s at Aarhus University, where he worked on the Rikke-Mathilda microassemblers and simulators running on the DECSystem-10 computer.\n\nIn 1979, Sørensen joined the Programming Research Group, part of the Oxford University Computing Laboratory (now the Oxford University Department of Computer Science) in England. There he worked with Jean-Raymond Abrial and others, making contributions to the early development of the formal specification language Z. He gained a DPhil degree from the University of Oxford and was a co-author of the seminal \"Specification Case Studies\" book on Z, first published in 1987 (second edition in 1993).\n\nFrom the late 1980s, Sørensen was central in the development of the B-Method, a leading formal method. He left Oxford University to lead a team at BP developing the B-Tool to provide tool support for the B approach. He then founded the company B-Core (UK) Limited to support the B-Toolkit, a set of programming tools designed to support the use of the B-Tool, and undertake B-related projects.\n\nLatterly Sørensen returned to the University of Oxford. From 1999, he worked on the B-based \"Booster\" models of requirements. He died in 2012, before he was able to retire.\n\n"}
{"id": "59052958", "url": "https://en.wikipedia.org/wiki?curid=59052958", "title": "Interference channel", "text": "Interference channel\n\nIn information theory, the interference channel is the basic model used to analyze the effect of interference in communication channels. The model consists of two pairs of users communicating through a shared channel. The problem of interference between two mobile users in close proximity or crosstalk between two parallel landlines are two examples where this model is applicable.\n\nUnlike in the point-to-point channel, where the amount of information that can be sent through the channel is limited by the noise that distorts the transmitted signal, in the interference channel, it is mainly the signal from the other user that hinders the communication. However, the transmitted signals are not purely random (otherwise they would not be decodable), and, therefore, the users can reduce the effect of the interference by decoding the undesired signal.\n\nThe mathematical model for this channel is the following:\n\nwhere, for formula_1:\n\nThe capacity of this channel model is not known in general; only for special cases of formula_14 the capacity has been calculated, e.g., in the case of strong interference or deterministic channels.\n\n"}
{"id": "14884", "url": "https://en.wikipedia.org/wiki?curid=14884", "title": "Intermediate value theorem", "text": "Intermediate value theorem\n\nIn mathematical analysis, the intermediate value theorem states that if a continuous function, \"f\", with an interval, [\"a\", \"b\"], as its domain, takes values \"f\"(\"a\") and \"f\"(\"b\") at each end of the interval, then it also takes any value between \"f\"(\"a\") and \"f\"(\"b\") at some point within the interval.\n\nThis has two important corollaries: \n\n\nThis captures an intuitive property of continuous functions: given \"f\" continuous on [1, 2] with the known values \"f\"(1) = 3 and \"f\"(2) = 5. Then the graph of \"y\" = \"f\"(\"x\") must pass through the horizontal line \"y\" = 4 while \"x\" moves from 1 to 2. It represents the idea that the graph of a continuous function on a closed interval can be drawn without lifting your pencil from the paper.\n\nThe intermediate value theorem states the following.\n\nConsider an interval formula_1 in the real numbers formula_2 and a continuous function formula_3. Then,\n\n\n\nRemark: \"Version II\" states that the set of function values has no gap. For any two function values formula_13, even if they are outside the interval between formula_5 and formula_6, all points in the interval formula_16 are also function values,\nA subset of the real numbers with no internal gap is an interval. \"Version I\" is naturally contained in \"Version II\".\n\nThe theorem depends on, and is equivalent to, the completeness of the real numbers. The intermediate value theorem does not apply to the rational numbers ℚ because gaps exist between rational numbers; irrational numbers fill those gaps. For example, the function formula_18 for formula_19 satisfies formula_20 and formula_21. However, there is no rational number formula_22 such that formula_23, because formula_24 is an irrational number.\n\nThe theorem may be proven as a consequence of the completeness property of the real numbers as follows:\n\nWe shall prove the first case, formula_25 be the set of all formula_26 such that formula_27. Then formula_28 is non-empty since formula_29 is an element of formula_28, and formula_28 is bounded above by formula_32. Hence, by completeness, the supremum formula_33 exists. That is, formula_34 is the lowest number that is greater than or equal to every member of formula_28. We claim that formula_8.\n\nFix some formula_37. Since formula_38 is continuous, there is a formula_39 such that formula_40 whenever formula_41. This means that\nfor all formula_43. By the properties of the supremum, there exist formula_44 that is contained in formula_28, so that for that formula_46\nChoose formula_48 that will obviously not be contained in formula_28, so we have\nBoth inequalities\nare valid for all formula_37, from which we deduce formula_8 as the only possible value, as stated.\n\nThe intermediate value theorem is an easy consequence of the basic properties of connected sets: the preservation of connectedness under continuous functions and the characterization of connected subsets of ℝ as intervals (\"see below for details\" \"and alternate proof\")\".\" The latter characterization is ultimately a consequence of the least-upper-bound property of the real numbers.\n\nThe intermediate value theorem can also be proved using the methods of non-standard analysis, which places \"intuitive\" arguments involving infinitesimals on a rigorous footing. (\"See the article:\" non-standard calculus.)\n\nFor \"u\" = 0 above, the statement is also known as \"Bolzano's theorem.\" This theorem was first proved by Bernard Bolzano in 1817. Augustin-Louis Cauchy provided a proof in 1821. Both were inspired by the goal of formalizing the analysis of functions and the work of Joseph-Louis Lagrange. The idea that continuous functions possess the intermediate value property has an earlier origin. Simon Stevin proved the intermediate value theorem for polynomials (using a cubic as an example) by providing an algorithm for constructing the decimal expansion of the solution. The algorithm iteratively subdivides the interval into 10 parts, producing an additional decimal digit at each step of the iteration. Before the formal definition of continuity was given, the intermediate value property was given as part of the definition of a continuous function. Proponents include Louis Arbogast, who assumed the functions to have no jumps, satisfy the intermediate value property and have increments whose sizes corresponded to the sizes of the increments of the variable.\nEarlier authors held the result to be intuitively obvious and requiring no proof. The insight of Bolzano and Cauchy was to define a general notion of continuity (in terms of infinitesimals in Cauchy's case and using real inequalities in Bolzano's case), and to provide a proof based on such definitions.\n\nThe intermediate value theorem is closely linked to the topological notion of connectedness and follows from the basic properties of connected sets in metric spaces and connected subsets of ℝ in particular:\n\nIn fact, connectedness is a topological property and (*) generalizes to topological spaces: \"If formula_53 and formula_54 are topological spaces, formula_55 is a continuous map, and formula_53 is a connected space, then formula_64 is connected.\" The preservation of connectedness under continuous maps can be thought of as a generalization of the intermediate value theorem, a property of real valued functions of a real variable, to continuous functions in general spaces.\n\nRecall the first version of the intermediate value theorem, stated previously:\n\nIntermediate value theorem. \"(Version I). Consider a closed interval formula_1 in the real numbers formula_2 and a continuous function formula_3. Then, if formula_68 is a real number such that formula_69, there exists formula_70 such that formula_8.\"\n\nThe intermediate value theorem is an immediate consequence of these two properties of connectedness:\n\nProof: By (**), formula_1 is a connected set. It follows from (*) that the image, formula_9, is also connected. For convenience, assume that formula_74. Then once more invoking (**), formula_75, or formula_8 for some formula_77. Since formula_78, formula_70 must actually hold, and the desired conclusion follows. The same argument applies if formula_80, so we are done.formula_81\n\nThe intermediate value theorem generalizes in a natural way: Suppose that \"X\" is a connected topological space and (\"Y\", <) is a totally ordered set equipped with the order topology, and let \"f\" : \"X\" → \"Y\" be a continuous map. If \"a\" and \"b\" are two points in \"X\" and \"u\" is a point in \"Y\" lying between \"f\"(\"a\") and \"f\"(\"b\") with respect to <, then there exists \"c\" in \"X\" such that \"f\"(\"c\") = \"u\". The original theorem is recovered by noting that ℝ is connected and that its natural topology is the order topology.\n\nThe Brouwer fixed-point theorem is a related theorem that, in one dimension gives a special case of the intermediate value theorem.\n\nA \"Darboux function\" is a real-valued function \"f\" that has the \"intermediate value property\", i.e., that satisfies the conclusion of the intermediate value theorem: for any two values \"a\" and \"b\" in the domain of \"f\", and any \"y\" between \"f\"(\"a\") and \"f\"(\"b\"), there is some \"c\" between \"a\" and \"b\" with \"f\"(\"c\") = \"y\". The intermediate value theorem says that every continuous function is a Darboux function. However, not every Darboux function is continuous; i.e., the converse of the intermediate value theorem is false.\n\nAs an example, take the function \"f\" : [0, ∞) → [−1, 1] defined by \"f\"(\"x\") = sin(1/\"x\") for \"x\" > 0 and f(0) = 0. This function is not continuous at \"x\" = 0 because the limit of \"f\"(\"x\") as \"x\" tends to 0 does not exist; yet the function has the intermediate value property. Another, more complicated example is given by the Conway base 13 function.\n\nHistorically, this intermediate value property has been suggested as a \"definition\" for continuity of real-valued functions; this definition was not adopted.\n\nDarboux's theorem states that all functions that result from the differentiation of some other function on some interval have the intermediate value property (even though they need not be continuous).\n\nThe theorem implies that on any great circle around the world, for the temperature, pressure, elevation, carbon dioxide concentration, if the simplification is taken that this varies continuously, there will always exist two antipodal points that share the same value for that variable.\n\n\"Proof:\" Take \"f\" to be any continuous function on a circle. Draw a line through the center of the circle, intersecting it at two opposite points \"A\" and \"B\". Let \"d\" be defined by the difference \"f\"(\"A\") − \"f\"(\"B\"). If the line is rotated 180 degrees, the value −\"d\" will be obtained instead. Due to the intermediate value theorem there must be some intermediate rotation angle for which \"d\" = 0, and as a consequence \"f\"(\"A\") = \"f\"(\"B\") at this angle.\n\nThis is a special case of a more general result called the Borsuk–Ulam theorem.\n\nAnother generalization for which this holds is for any closed convex n (n > 1) dimensional shape. Specifically, for any continuous function whose domain is the given shape, and any point inside the shape (not necessarily its center), there exist two antipodal points with respect to the given point whose functional value is the same. The proof is identical to the one given above.\n\nThe theorem also underpins the explanation of why rotating a wobbly table will bring it to stability (subject to certain easily met constraints).\n\n\n"}
{"id": "1101694", "url": "https://en.wikipedia.org/wiki?curid=1101694", "title": "Isometry group", "text": "Isometry group\n\nIn mathematics, the isometry group of a metric space is the set of all bijective isometries (i.e. bijective, distance-preserving maps) from the metric space onto itself, with the function composition as group operation. Its identity element is the identity function.\n\nA (generalized) isometry on a pseudo-Euclidean space preserves magnitude.\n\nEvery isometry group of a metric space is a subgroup of isometries. It represents in most cases a possible set of symmetries of objects/figures in the space, or functions defined on the space. See symmetry group.\n\nA discrete isometry group is an isometry group such that for every point of the space the set of images of the point under the isometries is a discrete set.\n\n\n\n\n\n"}
{"id": "17262516", "url": "https://en.wikipedia.org/wiki?curid=17262516", "title": "Jacobus Naveros", "text": "Jacobus Naveros\n\nJacob Naveros (fl. ca. 1533) was an early sixteenth-century Spanish logician. He is now known for his concern about the attribution of the logical works of Duns Scotus. Naveros found inconsistencies between the logical works and Scotus' commentary on the Sentences that caused him to doubt whether he had written any of these works.\n\nNaveros was born at the end of the 15th century, at Castronuño. He wrote a number of works in Latin.\n\n"}
{"id": "3691216", "url": "https://en.wikipedia.org/wiki?curid=3691216", "title": "Kynea number", "text": "Kynea number\n\nA Kynea number is an integer of the form\n\nAn equivalent formula is\n\nThis indicates that a Kynea number is the \"n\"th power of 4 plus the (\"n\" + 1)th Mersenne number. \nKynea numbers were studied by Cletus Emmanuel who named them after a baby girl.\n\nThe sequence of Kynea numbers starts with:\n\nThe binary representation of the \"n\"th Kynea number is a single leading one, followed by \"n\" - 1 consecutive zeroes, followed by \"n\" + 1 consecutive ones, or to put it algebraically:\n\nSo, for example, 23 is 10111 in binary, 79 is 1001111, etc. The difference between the \"n\"th Kynea number and the \"n\"th Carol number is the (\"n\" + 2)th power of two.\n\nStarting with 7, every third Kynea number is a multiple of 7. Thus, for a Kynea number to be a prime number, its index \"n\" cannot be of the form 3\"x\" + 1 for \"x\" > 0. The first few Kynea numbers that are also prime are 7, 23, 79, 1087, 66047, 263167, 16785407 .\n\n, the largest known prime Kynea number has index \"n\" = 661478, which has 398250 digits. It was found by Mark Rodenkirch in June 2016 using the programs CKSieve and PrimeFormGW. It is the 50th Kynea prime.\n\nA generalized Kynea number base \"b\" is defined to be a number of the form (\"b\"+1) − 2 with \"n\" ≥ 1, a generalized Kynea number base \"b\" can be prime only if \"b\" is even, since if \"b\" is odd, then all generalized Kynea numbers base \"b\" are even and thus not prime. A generalized Kynea number to base \"b\" is also a generalized Kynea number to base \"b\".\n\nLeast \"n\" ≥ 1 such that ((2\"b\")+1) − 2 is prime are\n\n, the largest known generalized Kynea prime is (30+1) − 2.\n\n"}
{"id": "33382445", "url": "https://en.wikipedia.org/wiki?curid=33382445", "title": "List of formal systems", "text": "List of formal systems\n\nThis is a list of formal systems, also known as logical calculi.\n\n\n\n"}
{"id": "5971816", "url": "https://en.wikipedia.org/wiki?curid=5971816", "title": "List of mathematicians (L)", "text": "List of mathematicians (L)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "659169", "url": "https://en.wikipedia.org/wiki?curid=659169", "title": "List of topics related to π", "text": "List of topics related to π\n\nThis is a list of topics related to pi (), the fundamental mathematical constant.\n"}
{"id": "40059937", "url": "https://en.wikipedia.org/wiki?curid=40059937", "title": "Mean field annealing", "text": "Mean field annealing\n\nMean field annealing is a deterministic approximation to the simulated annealing technique of solving optimization problems. This method uses mean field theory and is based on Peierls' inequality.\n"}
{"id": "39508690", "url": "https://en.wikipedia.org/wiki?curid=39508690", "title": "Monster Math Squad", "text": "Monster Math Squad\n\nMonster Math Squad is a Canadian CGI animated series, created by Jeff Rosen and produced by DHX Media for CBC Television. It follows the adventures of three monsters who go on missions that require math equations. Together, they solve math problems to complete missions.\n\nThe series takes place in the city of Monstrovia, where all sorts of monsters live. Three monsters (Max, Lily and Goo), are always ready to help their friends, solving math problems and learning math equations.\n\nUS\nCanada\n\nLatin America\n\nUK\n"}
{"id": "3681279", "url": "https://en.wikipedia.org/wiki?curid=3681279", "title": "Non-negative matrix factorization", "text": "Non-negative matrix factorization\n\nNon-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix is factorized into (usually) two matrices and , with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.\n\nNMF finds applications in such fields as astronomy, computer vision, document clustering, chemometrics, audio signal processing, recommender systems, and bioinformatics.\n\nIn chemometrics non-negative matrix factorization has a long history under the name \"self modeling curve resolution\".\nIn this framework the vectors in the right matrix are continuous curves rather than discrete vectors.\nAlso early work on non-negative matrix factorizations was performed by a Finnish group of researchers in the middle of the 1990s under the name \"positive matrix factorization\".\nIt became more widely known as \"non-negative matrix factorization\" after Lee and Seung investigated\nthe properties of the algorithm and published some simple and useful\nalgorithms for two types of factorizations.\n\nLet matrix be the product of the matrices and ,\n\nMatrix multiplication can be implemented as computing the column vectors of as linear combinations of the column vectors in using coefficients supplied by columns of . That is, each column of can be computed as follows:\n\nwhere is the -th column vector of the product matrix and is the -th column vector of the matrix .\n\nWhen multiplying matrices, the dimensions of the factor matrices may be significantly lower than those of the product matrix and it is this property that forms the basis of NMF. NMF generates factors with significantly reduced dimensions compared to the original matrix. For example, if is an matrix, is an matrix, and is a matrix then can be significantly less than both and .\n\nHere is an example based on a text-mining application: \n\nThis last point is the basis of NMF because we can consider each original document in our example as being built from a small set of hidden features. NMF generates these features.\n\nIt is useful to think of each feature (column vector) in the features matrix as a document archetype comprising a set of words where each word's cell value defines the word's rank in the feature: The higher a word's cell value the higher the word's rank in the feature. A column in the coefficients matrix represents an original document with a cell value defining the document's rank for a feature. We can now reconstruct a document (column vector) from our input matrix by a linear combination of our features (column vectors in ) where each feature is weighted by the feature's cell value from the document's column in .\n\nNMF has an inherent clustering property, i.e., it automatically clusters the columns of input data \nformula_3. It is this property that drives most applications of NMF.\n\nMore specifically, the approximation of formula_4 by\nformula_5 is achieved by minimizing the error function\n\nformula_6 subject to formula_7\n\ncodice_1\n\nFurthermore, the computed formula_8 gives the cluster indicator, i.e.,\nif formula_9, that fact indicates \ninput data formula_10\nbelongs to formula_11 cluster. \nAnd the computed formula_12 gives the cluster centroids, i.e., \nthe formula_11 column \ngives the cluster centroid of\nformula_11 cluster. This centroid's representation can be significantly enhanced by convex NMF.\n\nWhen the orthogonality formula_15 is not explicitly imposed, the orthogonality holds to a large extent, and the clustering property holds too. Clustering is the main objective of most data mining applications of NMF.\n\nWhen the error function to be used is Kullback–Leibler divergence, NMF is identical to the Probabilistic latent semantic analysis, a popular document clustering method.\n\nUsually the number of columns of and the number of rows of in NMF are selected so the product will become an approximation to . The full decomposition of then amounts to the two non-negative matrices and as well as a residual , such that: . The elements of the residual matrix can either be negative or positive.\n\nWhen and are smaller than they become easier to store and manipulate. Another reason for factorizing into smaller matrices and , is that if one is able to approximately represent the elements of by significantly less data, then one has to infer some latent structure in the data.\n\nIn standard NMF, matrix factor ， i.e., can be anything in that space. Convex NMF restricts the columns of to convex combinations of the input data vectors formula_16. This greatly improves the quality of data representation of . Furthermore, the resulting matrix factor becomes more sparse and orthogonal.\n\nIn case the nonnegative rank of is equal to its actual rank, is called a nonnegative rank factorization. The problem of finding the NRF of , if it exists, is known to be NP-hard.\n\nThere are different types of non-negative matrix factorizations.\nThe different types arise from using different cost functions for measuring the divergence between and and possibly by regularization of the and/or matrices.\n\nTwo simple divergence functions studied by Lee and Seung are the squared error (or Frobenius norm) and an extension of the Kullback–Leibler divergence to positive matrices (the original Kullback–Leibler divergence is defined on probability distributions).\nEach divergence leads to a different NMF algorithm, usually minimizing the divergence using iterative update rules.\n\nThe factorization problem in the squared error version of NMF may be stated as:\nGiven a matrix formula_4 find nonnegative matrices W and H that minimize the function\n\nAnother type of NMF for images is based on the total variation norm.\n\nWhen L1 regularization (akin to Lasso) is added to NMF with the mean squared error cost function, the resulting problem may be called non-negative sparse coding due to the similarity to the sparse coding problem,\nalthough it may also still be referred to as NMF.\n\nMany standard NMF algorithms analyze all the data together; i.e., the whole matrix is available from the start. This may be unsatisfactory in applications where there are too many data to fit into memory or where the data are provided in streaming fashion. One such use is for collaborative filtering in recommendation systems, where there may be many users and many items to recommend, and it would be inefficient to recalculate everything when one user or one item is added to the system. The cost function for optimization in these cases may or may not be the same as for standard NMF, but the algorithms need to be rather different.\n\nThere are several ways in which the and may be found: Lee and Seung's multiplicative update rule has been a popular method due to the simplicity of implementation. This algorithm is:\nNote that the updates are done on an element by element basis not matrix multiplication.\n\nWe note that and multiplicative factor is identity matrix when V = W H.\n\nMore recently other algorithms have been developed.\nSome approaches are based on alternating non-negative least squares: in each step of such an algorithm, first is fixed and found by a non-negative least squares solver, then is fixed and is found analogously. The procedures used to solve for and may be the same or different, as some NMF variants regularize one of and . Specific approaches include the projected gradient descent methods, the active set method, the optimal gradient method, and the block principal pivoting method among several others.\n\nCurrent algorithms are sub-optimal in that they only guarantee finding a local minimum, rather than a global minimum of the cost function. A provably optimal algorithm is unlikely in the near future as the problem has been shown to generalize the k-means clustering problem which is known to be NP-complete. However, as in many other data mining applications, a local minimum may still prove to be useful.\n\nThe sequential construction of NMF components ( and ) was firstly used to relate NMF with Principal Component Analysis (PCA) in astronomy. The contribution from the PCA components are ranked by the magnitude of their corresponding eigenvalues; for NMF, its components can be ranked empirically when they are constructed one by one (sequentially), i.e., learn the formula_22-th component with the first formula_19 components constructed.\n\nThe contribution of the sequential NMF components can be compared with the Karhunen–Loève theorem, an application of PCA, using the plot of eigenvalues. A typical choice of the number of components with PCA is based on the \"elbow\" point, then the existence of the flat plateau is indicating that PCA is not capturing the data efficiently, and at last there exists a sudden drop reflecting the capture of random noise and falls into the regime of overfitting. For sequential NMF, the plot of eigenvalues is approximated by the plot of the fractional residual variance curves, where the curves decreases continuously, and converge to a higher level than PCA, which is the indication of less over-fitting of sequential NMF.\n\nExact solutions for the variants of NMF can be expected (in polynomial time) when additional constraints hold for matrix . A polynomial time algorithm for solving nonnegative rank factorization if contains a monomial sub matrix of rank equal to its rank was given by Campbell and Poole in 1981. Kalofolias and Gallopoulos (2012) solved the symmetric counterpart of this problem, where is symmetric and contains a diagonal principal sub matrix of rank r. Their algorithm runs in O(rm^2) time in the dense case. Arora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) give a polynomial time algorithm for exact NMF that works for the case where one of the factors W satisfies the separability condition.\n\nIn \"Learning the parts of objects by non-negative matrix factorization\" Lee and Seung proposed NMF mainly for parts-based decomposition of images. It compares NMF to vector quantization and principal component analysis, and shows that although the three techniques may be written as factorizations, they implement different constraints and therefore produce different results.\nIt was later shown that some types of NMF are an instance of a more general probabilistic model called \"multinomial PCA\".\nWhen NMF is obtained by minimizing the Kullback–Leibler divergence, it is in fact equivalent to another instance of multinomial PCA, probabilistic latent semantic analysis,\ntrained by maximum likelihood estimation.\nThat method is commonly used for analyzing and clustering textual data and is also related to the latent class model.\n\nNMF with the least-squares objective is equivalent to a relaxed form of K-means clustering: the matrix factor contains cluster centroids and contains cluster membership indicators. This provides a theoretical foundation for using NMF for data clustering. However, k-means does not enforce non-negativity on its centroids, so the closest analogy is in fact with \"semi-NMF\".\n\nNMF can be seen as a two-layer directed graphical model with one layer of observed random variables and one layer of hidden random variables.\n\nNMF extends beyond matrices to tensors of arbitrary order. This extension may be viewed as a non-negative counterpart to, e.g., the PARAFAC model.\n\nOther extensions of NMF include joint factorisation of several data matrices and tensors where some factors are shared. Such models are useful for sensor fusion and relational learning.\n\nNMF is an instance of nonnegative quadratic programming (NQP), just like the support vector machine (SVM). However, SVM and NMF are related at a more intimate level than that of NQP, which allows direct application of the solution algorithms developed for either of the two methods to problems in both domains.\n\nThe factorization is not unique: A matrix and its inverse can be used to transform the two factorization matrices by, e.g.,\nIf the two new matrices formula_25 and formula_26 are non-negative they form another parametrization of the factorization.\n\nThe non-negativity of formula_27 and formula_28 applies at least if is a non-negative monomial matrix.\nIn this simple case it will just correspond to a scaling and a permutation.\n\nMore control over the non-uniqueness of NMF is obtained with sparsity constraints.\n\nIn astronomy, NMF is a promising method for dimension reduction in the sense that astrophysical signals are non-negative. NMF has been applied to the spectroscopic observations and the direct imaging observations as a method to study the common properties of astronomical objects and post-process the astronomical observations. The advances in the spectroscopic observations by Blanton & Roweis (2007) takes into account of the uncertainties of astronomical observations, which is later improved by Zhu (2016) where missing data are also considered and parallel computing is enabled. Their method is then adopted by Ren et al. (2018) to the direct imaging field as one of the methods of detecting exoplanets, especially for the direct imaging of circumstellar disks.\n\nRen et al. (2018) are able to prove the stability of NMF components when they are constructed sequentially (i.e., one by one), which enables the linearity of the NMF modeling process; the linearity property is used to separate the stellar light and the light scattered from the exoplanets and circumstellar disks.\n\nIn direct imaging, to reveal the faint exoplanets and circumstellar disks from bright the surrounding stellar lights, which has a typical contrast from 10⁵ to 10¹⁰, various statistical methods have been adopted, however the light from the exoplanets or circumstellar disks are usually over-fitted, where forward modeling have to be adopted to recover the true flux. Forward modeling is currently optimized for point sources, however not for extended sources, especially for irregularly shaped structures such as circumstellar disks. In this situation, NMF has been an excellent method, being less over-fitting in the sense of the non-negativity and sparsity of the NMF modeling coefficients, therefore forward modeling can be performed with a few scaling factors, rather than a computationally intensive data re-reduction on generated models.\n\nNMF can be used for text mining applications.\nIn this process, a \"document-term\" matrix is constructed with the weights of various terms (typically weighted word frequency information) from a set of documents.\nThis matrix is factored into a \"term-feature\" and a \"feature-document\" matrix.\nThe features are derived from the contents of the documents, and the feature-document matrix describes data clusters of related documents.\n\nOne specific application used hierarchical NMF on a small subset of scientific abstracts from PubMed.\nAnother research group clustered parts of the Enron email dataset\nwith 65,033 messages and 91,133 terms into 50 clusters.\nNMF has also been applied to citations data, with one example clustering English Wikipedia articles and scientific journals based on the outbound scientific citations in English Wikipedia.\n\nArora, Ge, Halpern, Mimno, Moitra, Sontag, Wu, & Zhu (2013) have given polynomial-time algorithms to learn topic models using NMF. The algorithm assumes that the topic matrix satisfies a separability condition that is often found to hold in these settings.\n\nNMF is also used to analyze spectral data; one such use is in the classification of space objects and debris.\n\nNMF is applied in scalable Internet distance (round-trip time) prediction. For a network with formula_29 hosts, with the help of NMF, the distances of all the formula_30 end-to-end links can be predicted after conducting only formula_31 measurements. This kind of method was firstly introduced in Internet\nDistance Estimation Service (IDES). Afterwards, as a fully decentralized approach, Phoenix network coordinate system\nis proposed. It achieves better overall prediction accuracy by introducing the concept of weight.\n\nSpeech denoising has been a long lasting problem in audio signal processing. There are lots of algorithms for denoising if the noise is stationary. For example, the Wiener filter is suitable for additive Gaussian noise. However, if the noise is non-stationary, the classical denoising algorithms usually have poor performance because the statistical information of the non-stationary noise is difficult to estimate. Schmidt et al. use NMF to do speech denoising under non-stationary noise, which is completely different from classical statistical approaches. The key idea is that clean speech signal can be sparsely represented by a speech dictionary, but non-stationary noise cannot. Similarly, non-stationary noise can also be sparsely represented by a noise dictionary, but speech cannot.\n\nThe algorithm for NMF denoising goes as follows. Two dictionaries, one for speech and one for noise, need to be trained offline. Once a noisy speech is given, we first calculate the magnitude of the Short-Time-Fourier-Transform. Second, separate it into two parts via NMF, one can be sparsely represented by the speech dictionary, and the other part can be sparsely represented by the noise dictionary. Third, the part that is represented by the speech dictionary will be the estimated clean speech.\n\nNMF has been successfully applied in bioinformatics for clustering gene expression and DNA methylation data and finding the genes most representative of the clusters. In the analysis of cancer mutations it has been used to identify common patterns of mutations that occur in many cancers and that probably have distinct causes.\n\nNMF, also referred in this field as factor analysis, has been used since the 1980s to analyze sequences of images in SPECT and PET dynamic medical imaging. Non-uniqueness of NMF was addressed using sparsity constraints.\n\nCurrent research (since 2010) in nonnegative matrix factorization includes, but is not limited to,\n\n\n\n"}
{"id": "4669257", "url": "https://en.wikipedia.org/wiki?curid=4669257", "title": "PLS (complexity)", "text": "PLS (complexity)\n\nIn computational complexity theory, Polynomial Local Search (PLS) is a complexity class that models the difficulty of finding a locally optimal solution to an optimization problem.\n\nA PLS problem formula_1 has a set formula_2 of instances which are encoded using strings over a finite alphabet formula_3. For each instance formula_4 there exists a finite solution set formula_5. Each solution formula_6 has a non negative integer cost given by a function formula_7 and a neighborhood formula_8. Additionally, the existence of the following three polynomial time algorithms is required:\n\n\nAn instance formula_2 has the structure of an implicit graph, the vertices being the solutions with two solutions formula_19 connected by a directed arc iff formula_15. The most interesting computational problem is the following:\n\n\"Given some instance formula_4 of a PLS problem formula_1, find a local optimum of formula_7, i.e. a solution formula_6 such that formula_25 for all formula_15\"\n\nThe problem can be solved using the following algorithm:\n\n\nUnfortunately, it generally takes an exponential number of improvement steps to find a local optimum even if the problem formula_1 can be solved exactly in polynomial time.\n\nExamples of PLS-complete problems include local-optimum relatives of the travelling salesman problem, maximum cut and satisfiability, as well as finding a pure Nash equilibrium in a congestion game.\n\nPLS is a subclass of TFNP, a complexity class closely related to NP that describes computational problems in which a solution is guaranteed to exist and can be recognized in polynomial time. For a problem in PLS, a solution is guaranteed to exist because the minimum-cost vertex of the entire graph is a valid solution, and the validity of a solution can be checked by computing its neighbors and comparing the costs of each one.\n"}
{"id": "24146591", "url": "https://en.wikipedia.org/wiki?curid=24146591", "title": "Poincaré series (modular form)", "text": "Poincaré series (modular form)\n\nIn number theory, a Poincaré series is a mathematical series generalizing the classical theta series that is associated to any discrete group of symmetries of a complex domain, possibly of several complex variables. In particular, they generalize classical Eisenstein series. They are named after Henri Poincaré.\n\nIf Γ is a finite group acting on a domain \"D\" and \"H\"(\"z\") is any meromorphic function on \"D\", then one obtains an automorphic function by averaging over Γ:\nHowever, if Γ is a discrete group, then additional factors must be introduced in order to assure convergence of such a series. To this end, a Poincaré series is a series of the form\nwhere \"J\" is the Jacobian determinant of the group element γ, and the asterisk denotes that the summation takes place only over coset representatives yielding distinct terms in the series.\n\nThe classical Poincaré series of weight 2\"k\" of a Fuchsian group Γ is defined by the series\nthe summation extending over congruence classes of fractional linear transformations\nbelonging to Γ. Choosing \"H\" to be a character of the cyclic group of order \"n\", one obtains the so-called Poincaré series of order \"n\":\nThe latter Poincaré series converges absolutely and uniformly on compact sets (in the upper halfplane), and is a modular form of weight 2\"k\" for Γ. Note that, when Γ is the full modular group and \"n\" = 0, one obtains the Eisenstein series of weight 2\"k\". In general, the Poincaré series is, for \"n\" ≥ 1, a cusp form.\n\n"}
{"id": "49043127", "url": "https://en.wikipedia.org/wiki?curid=49043127", "title": "Profit at risk", "text": "Profit at risk\n\nProfit-at-Risk (PaR) is a risk management quantity most often used for electricity portfolios that contain some mixture of generation assets, trading contracts and end-user consumption. It is used to provide a measure of the downside risk to profitability of a portfolio of physical and financial assets, analysed by time periods in which the energy is delivered. For example, the expected profitability and associated downside risk (PaR) might be calculated and monitored for each of the forward looking 24 months. The measure considers both price risk and volume risk (e.g. due to uncertainty in electricity generation volumes or consumer demand). Mathematically, the PaR is the quantile of the profit distribution of a portfolio. Since weather related volume risk drivers can be represented in the form of historical weather records over many years, a Monte-Carlo simulation approach is often used.\n\nIf the confidence interval for evaluating the PaR is 95%, there is a 5% probability that due to changing commodity volumes and prices, the\nprofit outcome for a specific period (e.g. December next year) will fall short of the expected profit result by more than the PaR value.\n\nNote that the concept of a set 'holding period' does not apply since the period is always up until the realisation of the profit outcome through the delivery of energy. That is the holding period is different for each of the specific delivery time periods being analysed e.g. it might be six months for December and therefore seven months for January.\n\nThe PaR measure was originally pioneered at Norsk Hydro in Norway as part of an initiative to prepare for deregulation of the electricity market. Petter Longva and Greg Keers co-authored a paper \"Risk Management in the Electricity Industry\" (IAEE 17th Annual International Conference, 1994) which introduced the PaR method. This led to it being adopted as the basis for electricity market risk management at Norsk Hydro and later by most of the other electricity generating utilities in the Nordic region. The approach was based on monte-carlo simulations of paired reservoir inflow and spot price outcomes to produce a distribution of expected profit in future reporting periods. This tied directly with the focus of management reporting on profitability of operations, unlike the Value-at-Risk approach that had been pioneered by JP Morgan for banks focused on their balance sheet risks.\n\nAs is the case with \"Value at Risk\", for risk measures like the PaR, Earnings-at-Risk (EaR), the Liquidity-at-Risk (LaR) or the Margin-at-Risk (MaR), the exact (algorithmic) implementation rule vary from firm to firm.\n\n"}
{"id": "12747956", "url": "https://en.wikipedia.org/wiki?curid=12747956", "title": "Pseudoelementary class", "text": "Pseudoelementary class\n\nIn logic, a pseudoelementary class is a class of structures derived from an elementary class (one definable in first-order logic) by omitting some of its sorts and relations. It is the mathematical logic counterpart of the notion in category theory of (the codomain of) a forgetful functor, and in physics of (hypothesized) hidden variable theories purporting to explain quantum mechanics. Elementary classes are (vacuously) pseudoelementary but the converse is not always true; nevertheless pseudoelementary classes share some of the properties of elementary classes such as being closed under ultraproducts.\n\nA pseudoelementary class is a reduct of an elementary class. That is, it is obtained by omitting some of the sorts and relations of a (many-sorted) elementary class.\n\n\n\n\n\nA quasivariety defined logically as the class of models of a universal Horn theory can equivalently be defined algebraically as a class of structures closed under isomorphisms, subalgebras, and reduced products. Since the notion of reduced product is more intricate than that of direct product, it is sometimes useful to blend the logical and algebraic characterizations in terms of pseudoelementary classes. One such blended definition characterizes a quasivariety as a pseudoelementary class closed under isomorphisms, subalgebras, and direct products (the pseudoelementary property allows \"reduced\" to be simplified to \"direct\").\n\nA corollary of this characterization is that one can (nonconstructively) prove the existence of a universal Horn axiomatization of a class by first axiomatizing some expansion of the structure with auxiliary sorts and relations and then showing that the pseudoelementary class obtained by dropping the auxiliary constructs is closed under subalgebras and direct products. This technique works for Example 2 because subalgebras and direct products of algebras of binary relations are themselves algebras of binary relations, showing that the class RRA of representable relation algebras is a quasivariety (and \"a fortiori\" an elementary class). This short proof is an effective application of abstract nonsense; the stronger result by Tarski that RRA is in fact a variety required more honest toil.\n\n"}
{"id": "22192834", "url": "https://en.wikipedia.org/wiki?curid=22192834", "title": "Random tree", "text": "Random tree\n\nIn mathematics and computer science, a random tree is a tree or arborescence that is formed by a stochastic process. Types of random trees include:\n\n"}
{"id": "22929637", "url": "https://en.wikipedia.org/wiki?curid=22929637", "title": "Richard A. Brualdi", "text": "Richard A. Brualdi\n\nR. A. Brualdi is a professor emeritus of combinatorial mathematics at the University of Wisconsin–Madison.\n\nBrualdi received his Ph.D. from Syracuse University in 1964; his advisor was H. J. Ryser. Brualdi is an Editor-in-Chief of the Electronic Journal of Combinatorics. He has over 200 publications in several mathematical journals. According to current on-line database of Mathematics Genealogy Project, Richard Brualdi has 37 Ph.D. students and 48 academic descendants. The concept of incidence coloring was introduced in 1993 by Brualdi and Massey.\n\nHe received the Euler medal from the Institute of Combinatorics and its Applications in 2000. In 2012 he was elected a fellow of the Society for Industrial and Applied Mathematics. In 2012 he became an inaugural fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "44276996", "url": "https://en.wikipedia.org/wiki?curid=44276996", "title": "Scarborough criterion", "text": "Scarborough criterion\n\nThe Scarborough criterion is used for satisfying convergence of a solution while solving linear equations using an iterative method.\n\nAnalytical solutions for certain systems of equations can be difficult or impossible to obtain. A well known example are the Navier-Stokes equations describing the flow of Newtonian fluids. Solutions of such equations can be obtained numerically, at discrete points of the solution domain (e.g. at discrete time points and points in space). Numerical solutions based on the integration of the equations at discrete control volumes of the solution domain (for example the Finite Volume Method) result in a system of algebraic equations, one for each \"nodal point\" (corresponding to a particular control volume). These algebraic equations are usually referred to as \"discretised equations\". The Scarborough criterion formulated by Scarborough (1958), can be expressed in terms of the values of the coefficients of the discretised equations:\nHere is the net coefficient of a random central node \"P\" and the summation in the numerator is taken over all the neighbouring nodes. For a one, two and three-dimensional problem there will be two (east & west), four (east, west, south & north), and six (east, west, south north, top & bottom) neighbours for each node, respectively.\n\n\nIf Scarborough criterion is not satisfied then Gauss–Seidel method iterative procedure is not guaranteed to converge a solution. This criterion is a sufficient condition, not a necessary one. If this criterion is satisfied then it means equation will be converged by at least one iterative method. The Scarborough criterion is used as a sufficient condition for convergent iterative method. The finite volume method uses this criterion for obtaining a convergent solution and implementing boundary conditions.\n\nIf the differencing scheme produces coefficients that satisfy the above criterion the resulting matrix of coefficients is diagonally dominant. To achieve diagonal dominance we need large values of net coefficient so the linearisation practice of source terms should ensure that \"S\" is always negative. If this is the case –\"S\" is always positive and adds to \"a\". Diagonal dominance is a desirable feature for satisfying the boundedness criterion. This states that in the absence of sources the internal nodal values of the property \"ф\" should be bounded by its boundary values. Hence in a steady state conduction problem without sources and with boundary temperatures of 500 °C and 200 °C all interior values of \"T\" should be less than 500 °C and greater than 200 °C.\n\n\n"}
{"id": "18767352", "url": "https://en.wikipedia.org/wiki?curid=18767352", "title": "Shape theory (mathematics)", "text": "Shape theory (mathematics)\n\nShape theory is a branch of topology, which provides a more global view of the topological spaces than homotopy theory. The two coincide on compacta dominated homotopically by finite polyhedra. Shape theory associates with the Čech homology theory while homotopy theory associates with the singular homology theory.\n\nShape theory was reinvented, further developed and promoted by the Polish mathematician Karol Borsuk in 1968. Actually, the name \"shape theory\" was coined by Borsuk.\n\nBorsuk lived and worked in Warsaw, hence the name of one of the fundamental examples of the area, the Warsaw circle. This is a compact subset of the plane produced by \"closing up\" a topologist's sine curve with an arc.\n\nBorsuk's shape theory was generalized onto arbitrary (non-metric) compact spaces, and even onto general categories, by Włodzimierz Holsztyński in year 1968/1969, and published in Fund. Math. 70 , 157-168, y.1971 (see Jean-Marc Cordier, Tim Porter, (1989) below). This was done in a \"continuous style\", characteristic for the Čech homology rendered by Samuel Eilenberg and Norman Steenrod in their monograph \"Foundations of Algebraic Topology\". Due to the circumstance, Holsztyński's paper was hardly noticed, and instead a great popularity in the field was gained by a much less advanced (more naive) paper by Sibe Mardešić and Jack Segal, which was published a little later, Fund. Math. 72, 61-68, y.1971. Further developments are reflected by the references below, and by their contents.\n\nFor some purposes, like dynamical systems, more sophisticated invariants were developed under the name strong shape. Generalizations to noncommutative geometry, e.g. the shape theory for operator algebras have been found.\n\n"}
{"id": "32364281", "url": "https://en.wikipedia.org/wiki?curid=32364281", "title": "Sphere–cylinder intersection", "text": "Sphere–cylinder intersection\n\nIn the theory of analytic geometry for real three-dimensional space, the intersection between a sphere and a cylinder can be a circle, a point, the empty set, or a special type of curve.\n\nFor the analysis of this situation, assume (without loss of generality) that the axis of the cylinder coincides with the \"z\"-axis; points on the cylinder (with radius formula_1) satisfy\n\nWe also assume that the sphere, with radius formula_3 is centered at a point on the positive x-axis, at point formula_4. Its points satisfy\n\nThe intersection is the collection of points satisfying both equations.\n\nIf formula_6, the sphere lies entirely in the interior of the cylinder. The intersection is the empty set.\n\nIf the sphere is smaller than the cylinder (formula_7) and formula_8, the sphere lies in the interior of\nthe cylinder except for one point. The intersection is the single point formula_9.\n\nIf the center of the sphere lies on the axis of the cylinder, formula_10. In that case, the intersection consists of\ntwo circles of radius formula_1. These circles lie in the planes\n\nIf formula_13, the intersection is a single circle in the plane formula_14.\n\nSubtracting the two equations given above gives\n\nSince formula_16 is a quadratic function of formula_17, the projection of the intersection onto the xz-plane is the section of an orthogonal parabola; it is only a section due to the fact that formula_18.\nThe vertex of the parabola lies at point formula_19, where\n\nIf formula_21, the condition formula_22 cuts the parabola into two segments. In this case, the intersection of sphere and cylinder consists of two closed curves, which are mirror images of each other.\nTheir projection in the \"xy\"-plane are circles of radius formula_1.\n\nEach part of the intersection can be parametrized by an angle formula_24:\n\nThe curves contain the following extreme points:\n\nIf formula_27, the intersection of sphere and cylinder consists of a single closed curve.\nIt can be described by the same parameter equation as in the previous section, but the angle formula_24\nmust be restricted to formula_29, where formula_30.\n\nThe curve contains the following extreme points:\n\nIn the case formula_32, the cylinder and sphere are tangential to each other at point formula_9.\nThe intersection resembles a figure eight: it is a closed curve which intersects itself. The above parametrization becomes\n\nwhere formula_24 now goes through two full revolutions.\n\nIn the special case formula_36, the intersection is known as Viviani's curve. Its parameter representation is\n\n"}
{"id": "1365807", "url": "https://en.wikipedia.org/wiki?curid=1365807", "title": "Sudoku", "text": "Sudoku\n\nCompleted games are always a type of Latin square with an additional constraint on the contents of individual regions. For example, the same single integer may not appear twice in the same row, column, or any of the nine 3×3 subregions of the 9x9 playing board.\n\nFrench newspapers featured variations of the puzzles in the 19th century, and the puzzle has appeared since 1979 in puzzle books under the name Number Place. However, the modern Sudoku only started to become mainstream in 1986 by the Japanese puzzle company Nikoli, under the name Sudoku, meaning \"single number\". It first appeared in a US newspaper and then \"The Times\" (London) in 2004, from the efforts of Wayne Gould, who devised a computer program to rapidly produce distinct puzzles.\n\nNumber puzzles appeared in newspapers in the late 19th century, when French puzzle setters began experimenting with removing numbers from magic squares. \"Le Siècle\", a Paris daily, published a partially completed 9×9 magic square with 3×3 subsquares on November 19, 1892. It was not a Sudoku because it contained double-digit numbers and required arithmetic rather than logic to solve, but it shared key characteristics: each row, column and subsquare added up to the same number.\n\nOn July 6, 1895, \"Le Siècle\" rival, \"La France\", refined the puzzle so that it was almost a modern Sudoku. It simplified the 9×9 magic square puzzle so that each row, column, and broken diagonals contained only the numbers 1–9, but did not mark the subsquares. Although they are unmarked, each 3×3 subsquare does indeed comprise the numbers 1–9 and the additional constraint on the broken diagonals leads to only one solution.\n\nThese weekly puzzles were a feature of French newspapers such as \"L'Echo de Paris\" for about a decade, but disappeared about the time of World War I.\n\nThe modern Sudoku was most likely designed anonymously by Howard Garns, a 74-year-old retired architect and freelance puzzle constructor from Connersville, Indiana, and first published in 1979 by Dell Magazines as Number Place (the earliest known examples of modern Sudoku). Garns's name was always present on the list of contributors in issues of \"Dell Pencil Puzzles and Word Games\" that included Number Place, and was always absent from issues that did not. He died in 1989 before getting a chance to see his creation as a worldwide phenomenon. Whether or not Garns was familiar with any of the French newspapers listed above is unclear.\n\nThe puzzle was introduced in Japan by Nikoli in the paper \"Monthly Nikolist\" in April 1984 as , which also can be translated as \"the digits must be single\" or \"the digits are limited to one occurrence\" (In Japanese, \"dokushin\" means an \"unmarried person\"). At a later date, the name was abbreviated to \"Sudoku\" (数独) by , taking only the first kanji of compound words to form a shorter version. \"Sudoku\" is a registered trademark in Japan and the puzzle is generally referred to as or, more informally, a portmanteau of the two words, . In 1986, Nikoli introduced two innovations: the number of givens was restricted to no more than 32, and puzzles became \"symmetrical\" (meaning the givens were distributed in rotationally symmetric cells). It is now published in mainstream Japanese periodicals, such as the \"Asahi Shimbun\".\n\nIn 1997, Hong Kong judge Wayne Gould saw a partly completed puzzle in a Japanese bookshop. Over six years, he developed a computer program to produce unique puzzles rapidly. Knowing that British newspapers have a long history of publishing crosswords and other puzzles, he promoted Sudoku to \"The Times\" in Britain, which launched it on November 12, 2004 (calling it Su Doku). The first letter to \"The Times\" regarding Su Doku was published the following day on November 13 from Ian Payn of Brentford, complaining that the puzzle had caused him to miss his stop on the tube. Sudoku puzzles rapidly spread to other newspapers as a regular feature. \n\nThe rapid rise of Sudoku in Britain from relative obscurity to a front-page feature in national newspapers attracted commentary in the media and parody (such as when \"The Guardian\" \"G2\" section advertised itself as the first newspaper supplement with a Sudoku grid on every page). Recognizing the different psychological appeals of easy and difficult puzzles, \"The Times\" introduced both, side by side, on June 20, 2005. From July 2005, Channel 4 included a daily Sudoku game in their teletext service. On August 2, the BBC's program guide \"Radio Times\" featured a weekly Super Sudoku with a 16×16 grid.\n\nIn the United States, the first newspaper to publish a Sudoku puzzle by Wayne Gould was \"The Conway Daily Sun\" (New Hampshire), in 2004.\n\nThe world's first live TV Sudoku show, \"Sudoku Live\", was a puzzle contest first broadcast on July 1, 2005, on Sky One. It was presented by Carol Vorderman. Nine teams of nine players (with one celebrity in each team) representing geographical regions competed to solve a puzzle. Each player had a hand-held device for entering numbers corresponding to answers for four cells. Phil Kollin of Winchelsea, England, was the series grand prize winner, taking home over £23,000 over a series of games. The audience at home was in a separate interactive competition, which was won by Hannah Withey of Cheshire.\n\nLater in 2005, the BBC launched \"SUDO-Q\", a game show that combined Sudoku with general knowledge. However, it used only 4×4 and 6×6 puzzles. Four seasons were produced before the show ended in 2007.\n\nIn 2006, a Sudoku website published songwriter Peter Levy's Sudoku tribute song, but quickly had to take down the MP3 file due to heavy traffic. British and Australian radio picked up the song, which is to feature in a British-made Sudoku documentary. The Japanese Embassy also nominated the song for an award, with Levy doing talks with Sony in Japan to release the song as a single.\n\nSudoku software is very popular on PCs, websites, and mobile phones. It comes with many distributions of Linux. Software has also been released on video game consoles, such as the Nintendo DS, PlayStation Portable, the Game Boy Advance, Xbox Live Arcade, the Nook e-book reader, Kindle Fire tablet, several iPod models, and the iPhone. Many Nokia phones also had Sudoku. In fact, just two weeks after Apple Inc. debuted the online App Store within its iTunes Store on July 11, 2008, nearly 30 different Sudoku games were already in it, created by various software developers, specifically for the iPhone and iPod Touch. One of the most popular video games featuring Sudoku is \"\". Critically and commercially well-received, it generated particular praise for its Sudoku implementation and sold more than 8 million copies worldwide. Due to its popularity, Nintendo made a second \"Brain Age\" game titled \"Brain Age\", which has over 100 new Sudoku puzzles and other activities.\n\nIn June 2008, an Australian drugs-related jury trial costing over A$ 1 million was aborted when it was discovered that five of the twelve jurors had been playing Sudoku instead of listening to evidence.\n\nAlthough the 9×9 grid with 3×3 regions is by far the most common, many other variations exist. Sample puzzles can be 4×4 grids with 2×2 regions; 5×5 grids with \"pentomino\" regions have been published under the name Logi-5; the World Puzzle Championship has featured a 6×6 grid with 2×3 regions and a 7×7 grid with six \"heptomino\" regions and a disjoint region. Larger grids are also possible. \"The Times\" offers a 12×12-grid \"Dodeka Sudoku\" with 12 regions of 4×3 squares. Dell Magazines regularly publishes 16×16 \"Number Place Challenger\" puzzles (using the numbers 1–16 or the letters A-P). Nikoli offers 25×25 \"Sudoku the Giant\" behemoths. A 100×100-grid puzzle dubbed Sudoku-zilla was published in 2010.\n\nUnder the name \"Mini Sudoku\", a 6×6 variant with 3×2 regions appears in the American newspaper \"USA Today\" and elsewhere. The object is the same as that of standard Sudoku, but the puzzle only uses the numbers 1 through 6. A similar form, for younger solvers of puzzles, called \"The Junior Sudoku\", has appeared in some newspapers, such as some editions of \"The Daily Mail\".\n\nAnother common variant is to add limits on the placement of numbers beyond the usual row, column, and box requirements. Often, the limit takes the form of an extra \"dimension\"; the most common is to require the numbers in the main diagonals of the grid to also be unique. The aforementioned \"Number Place Challenger\" puzzles are all of this variant, as are the Sudoku X puzzles in \"The Daily Mail\", which use 6×6 grids.\n\nThe Killer Sudoku variant combines elements of Sudoku and Kakuro.\n\nAlphabetical variations have emerged, sometimes called Wordoku; no functional difference exists in the puzzle unless the letters spell something. Some variants, such as in the \"TV Guide\", include a word reading along a main diagonal, row, or column once solved; determining the word in advance can be viewed as a solving aid. A Wordoku might contain words other than the main word.\n\n\"Quadratum latinum\" is a Sudoku variation with Roman numerals (I, II, III, IV, ..., IX) proposed by \"Hebdomada aenigmatum\", a monthly magazine of Latin puzzles and crosswords. Like the Wordoku, it presents no functional difference from a normal Sudoku, but adds the visual difficulty of using Roman numerals.\n\nKaodokus (顔独) use partially given smiley faces instead of digits. The smileys can have three possible shapes and three possible mouths, for a total of nine unique combinations. In easy puzzles some full smileys may be given, but the harder ones only contain partial cells with either a mouth or a shape. One may, for example, deduce the presence of a square on the intersection of a row with three circles and a column with three triangles, even though the mouth is still undetermined. The concept of kaodoku may be combined with other variations like additional constraints, jigsaw areas, overlapping puzzles, etcetera. The name of the puzzle comes from the Japanese word for face, \"kao\".\n\nHyper Sudoku uses the classic 9×9 grid with 3×3 regions, but defines four additional interior 3×3 regions in which the numbers 1–9 must appear exactly once. It was invented by \"Peter Ritmeester\" and first published by him in Dutch Newspaper \"NRC Handelsblad\" in October 2005, and since April 2007 on a daily basis in \"The International New York Times\" (International Herald Tribune). The first time it was called Hyper Sudoku was in \"Will Shortz's Favorite Sudoku Variations\" (February 2006). It is also known as Windoku because with the grid's four interior regions shaded, it resembles a window with glazing bars.\n\nIn Twin Sudoku two regular grids share a 3×3 box. This is one of many possible types of overlapping grids. The rules for each individual grid are the same as in normal Sudoku, but the digits in the overlapping section are shared by each half. In some compositions neither individual grid can be solved alone – the complete solution is only possible after each individual grid has at least been partially solved.\n\nPuzzles constructed from more than two grids are also common. Five 9×9 grids that overlap at the corner regions in the shape of a \"quincunx\" is known in Japan as \"Gattai\" 5 (five merged) Sudoku. In \"The Times\", \"The Age\", and \"The Sydney Morning Herald\", this form of puzzle is known as Samurai SuDoku. \"The Baltimore Sun\" and the \"Toronto Star\" publish a puzzle of this variant (titled High Five) in their Sunday edition. Often, no givens are placed in the overlapping regions. Sequential grids, as opposed to overlapping, are also published, with values in specific locations in grids needing to be transferred to others.\nA tabletop version of Sudoku can be played with a standard 81-card Set deck (see Set game). A three-dimensional Sudoku puzzle was published in \"The Daily Telegraph\" in May 2005. \"The Times\" also publishes a three-dimensional version under the name Tredoku. Also, a Sudoku version of the Rubik's Cube is named Sudoku Cube.\n\nMany other variants have been developed. Some are different shapes in the arrangement of overlapping 9×9 grids, such as butterfly, windmill, or flower. Others vary the logic for solving the grid. One of these is \"Greater Than Sudoku\". In this, a 3×3 grid of the Sudoku is given with 12 symbols of Greater Than (>) or Less Than (<) on the common line of the two adjacent numbers. Another variant on the logic of solution is \"Clueless Sudoku\", in which nine 9×9 Sudoku grids are each placed in a 3×3 array. The center cell in each 3×3 grid of all nine puzzles is left blank and form a tenth Sudoku puzzle without any cell completed; hence, \"clueless\". Examples and other variants can be found in the Glossary of Sudoku.\n\nThis section refers to classic Sudoku, disregarding jigsaw, hyper, and other variants.\n\nA completed Sudoku grid is a special type of Latin square with the additional property of no repeated values in any of the nine blocks (or \"boxes\" of 3×3 cells). The relationship between the two theories is known, after it was proven that a first-order formula that does not mention blocks is valid for Sudoku if and only if it is valid for Latin squares.\n\nThe general problem of solving Sudoku puzzles on \"n\"×\"n\" grids of \"n\"×\"n\" blocks is known to be NP-complete. Many computer algorithms, such as backtracking and dancing links can solve most 9×9 puzzles efficiently, but combinatorial explosion occurs as \"n\" increases, creating limits to the properties of Sudokus that can be constructed, analyzed, and solved as \"n\" increases. A Sudoku puzzle can be expressed as a graph coloring problem. The aim is to construct a 9-coloring of a particular graph, given a partial 9-coloring.\n\nThe fewest clues possible for a proper Sudoku is 17 (proven January 2012, and confirmed September 2013). Over 49,000 Sudokus with 17 clues have been found, many by Japanese enthusiasts. Sudokus with 18 clues and rotational symmetry have been found, and there is at least one Sudoku that has 18 clues, exhibits two-way diagonal symmetry and is automorphic. The maximum number of clues that can be provided while still not rendering a unique solution is four short of a full grid (77); if two instances of two numbers each are missing from cells that occupy the corners of an orthogonal rectangle, and exactly two of these cells are within one region, the numbers can be assigned two ways. Since this applies to Latin squares in general, most variants of Sudoku have the same maximum.\n\nThe number of classic 9×9 Sudoku solution grids is 6,670,903,752,021,072,936,960 , or around . This is roughly times the number of 9×9 Latin squares. Various other grid sizes have also been enumerated—see the main article for details. The number of essentially different solutions, when symmetries such as rotation, reflection, permutation, and relabelling are taken into account, was shown to be just 5,472,730,538 .\n\nUnlike the number of complete Sudoku grids, the number of minimal 9×9 Sudoku puzzles is not precisely known. (A minimal puzzle is one in which no clue can be deleted without losing uniqueness of the solution.) However, statistical techniques combined with a puzzle generator show that about (with 0.065% relative error) 3.10 × 10 minimal puzzles and 2.55 × 10 nonessentially equivalent minimal puzzles exist.\n\n\n\n"}
{"id": "905850", "url": "https://en.wikipedia.org/wiki?curid=905850", "title": "Supermodular function", "text": "Supermodular function\n\nIn mathematics, a function\nis supermodular if\nfor all formula_3, formula_4, where formula_5 denotes the componentwise maximum and formula_6 the componentwise minimum of formula_3 and formula_8.\n\nIf −\"f\" is supermodular then \"f\" is called submodular, and if the inequality is changed to an equality the function is modular.\n\nIf \"f\" is twice continuously differentiable, then supermodularity is equivalent to the condition\n\nThe concept of supermodularity is used in the social sciences to analyze how one agent's decision affects the incentives of others.\n\nConsider a symmetric game with a smooth payoff function formula_10 defined over actions formula_11 of two or more players formula_12. Suppose the action space is continuous; for simplicity, suppose each action is chosen from an interval: formula_13. In this context, supermodularity of formula_10 implies that an increase in player formula_15's choice formula_11 increases the marginal payoff formula_17 of action formula_18 for all other players formula_19. That is, if any player formula_15 chooses a higher formula_11, all other players formula_19 have an incentive to raise their choices formula_18 too. Following the terminology of Bulow, Geanakoplos, and Klemperer (1985), economists call this situation strategic complementarity, because players' strategies are complements to each other. This is the basic property underlying examples of multiple equilibria in coordination games.\n\nThe opposite case of submodularity of formula_10 corresponds to the situation of strategic substitutability. An increase in formula_11 lowers the marginal payoff to all other player's choices formula_18, so strategies are substitutes. That is, if formula_15 chooses a higher formula_11, other players have an incentive to pick a \"lower\" formula_18.\n\nFor example, Bulow et al. consider the interactions of many imperfectly competitive firms. When an increase in output by one firm raises the marginal revenues of the other firms, production decisions are strategic complements. When an increase in output by one firm lowers the marginal revenues of the other firms, production decisions are strategic substitutes.\nA supermodular utility function is often related to complementary goods. However, this view is disputed.\n\nSupermodularity and submodularity are also defined for functions defined over subsets of a larger set. Intuitively, a submodular function over the subsets demonstrates \"diminishing returns\". There are specialized techniques for optimizing submodular functions.\n\nLet \"S\" be a finite set. A function formula_30 is submodular if for any formula_31 and formula_32, formula_33. For supermodularity, the inequality is reversed.\nThe definition of submodularity can equivalently be formulated as\nfor all subsets \"A\" and \"B\" of \"S\".\n\n"}
{"id": "1051627", "url": "https://en.wikipedia.org/wiki?curid=1051627", "title": "Szemerédi–Trotter theorem", "text": "Szemerédi–Trotter theorem\n\nThe Szemerédi–Trotter theorem is a mathematical result in the field of combinatorial geometry. It asserts that given points and lines in the Euclidean plane, the number of incidences (\"i.e.\", the number of point-line pairs, such that the point lies on the line) is\n\nthis bound cannot be improved, except in terms of the implicit constants. As for the implicit constants, it was shown by János Pach, Radoš Radoičić, Gábor Tardos, and Géza Tóth that the upper bound formula_2 holds. On the other hand, Pach and Tóth showed that the statement does not hold true if one replaces the coefficient 2.5 with 0.42. \n\nAn equivalent formulation of the theorem is the following. Given points and an integer , the number of lines which pass through at least of the points is\n\nThe original proof of Endre Szemerédi and William T. Trotter was somewhat complicated, using a combinatorial technique known as \"cell decomposition\". Later, László Székely discovered a much simpler proof using the crossing number inequality for graphs. (See below.)\n\nThe Szemerédi–Trotter theorem has a number of consequences, including Beck's theorem in incidence geometry.\n\nWe may discard the lines which contain two or fewer of the points, as they can contribute at most incidences to the total number. Thus we may assume that every line contains at least three of the points.\n\nIf a line contains points, then it will contain line segments which connect two consecutive points along the line. Because after discarding the two-point lines, it follows that , so the number of these line segments on each line is at least half the number of incidences on that line. Summing over all of the lines, the number of these line segments is again at least half the total number of incidences. Thus if denotes the number of such line segments, it will suffice to show that\n\nNow consider the graph formed by using the points as vertices, and the line segments as edges. Since each line segment lies on one of lines, and any two lines intersect in at most one point, the crossing number of this graph is at most the number of points where two lines intersect, which is at most . The crossing number inequality implies that either , or that . In either case , giving the desired bound\n\nSince every pair of points can be connected by at most one line, there can be at most lines which can connect at or more points, since . This bound will prove the theorem when is small (e.g. if for some absolute constant ). Thus, we need only consider the case when is large, say .\n\nSuppose that there are \"m\" lines that each contain at least points. These lines generate at least incidences, and so by the first formulation of the Szemerédi–Trotter theorem, we have\n\nand so at least one of the statements formula_7, or formula_8 is true. The third possibility is ruled out since was assumed to be large, so we are left with the first two. But in either of these two cases, some elementary algebra will give the bound formula_9 as desired.\n\nExcept for its constant, the Szemerédi–Trotter incidence bound cannot be improved. To see this, consider for any positive integer a set of points on the integer lattice\n\nand a set of lines\n\nClearly, formula_12 and formula_13. Since each line is incident to points (i.e., once for each formula_14), the number of incidences is formula_15 which matches the upper bound.\n\nOne generalization of this result to arbitrary dimension, , was found by Agarwal and Aronov. Given a set of points, , and the set of hyperplanes, , which are each spanned by , the number of incidences between and is bounded above by\n\nEquivalently, the number of hyperplanes in containing or more points is bounded above by\n\nA construction due to Edelsbrunner shows this bound to be asymptotically optimal.\n\nJózsef Solymosi and Terence Tao obtained near sharp upper bounds for the number of incidences between points and algebraic varieties in higher dimensions. Their proof uses the Polynomial Ham Sandwich Theorem.\n\nThere has been some interest in proving analogs to the Szemerédi–Trotter theorem in planes over fields other than . All known proofs of the Szemerédi–Trotter theorem over rely in a crucial way on the topology of Euclidean space, so do not extend easily to other fields. Nevertheless, the following results have been obtained: \n"}
{"id": "27267184", "url": "https://en.wikipedia.org/wiki?curid=27267184", "title": "Test Template Framework", "text": "Test Template Framework\n\nThe Test Template Framework (TTF) is a model-based testing (MBT) framework proposed by Phil Stocks and David Carrington in for the purpose of software testing. Although the TTF was meant to be notation-independent, the original presentation was made using the Z formal notation. It is one of the few MBT frameworks approaching unit testing.\n\nThe TTF is a specific proposal of model-based testing (MBT). It considers models to be Z specifications. Each operation within the specification is analyzed to derive or generate \"abstract test cases\". This analysis consists of the following steps:\n\n\nOne of the main advantages of the TTF is that all of these concepts are expressed in the same notation of the specification, i.e. the Z notation. Hence, the engineer has to know only one notation to perform the analysis down to the generation of abstract test cases.\n\nIn this section the main concepts defined by the TTF are described.\n\nLet formula_1 be a Z operation. Let formula_2 be all the input and (non-primed) state variables referenced in formula_1, and formula_4 their corresponding types. The \"Input Space\" (IS) of formula_1, written formula_6, is the Z schema box defined by formula_7.\n\nLet formula_1 be a Z operation. Let formula_9 be the precondition of formula_1. The \"Valid Input Space\" (VIS) of formula_1, written formula_12, is the Z schema box defined by formula_13.\n\nLet formula_1 be a Z operation and let formula_15 be any predicate depending on one or more of the variables defined in formula_12. Then, the Z schema box formula_17 is a \"test class\" of formula_1. Note that this schema is equivalent to formula_19. This observation can be generalized by saying that if formula_20 is a test class of formula_1, then the Z schema box defined by formula_22 is also a test class of formula_1. According to this definition the VIS is also a test class.\n\nIf formula_20 is a test class of formula_1, then the predicate formula_15 in formula_27 is said to be the \"characteristic\" predicate of formula_28 or formula_28 is \"characterized\" by formula_15.\n\nTest classes are also called test objectives , test templates and test specifications.\n\nIn the context of the TTF a \"testing tactic\" is a means to partition any test class of any operation. However, some of the testing tactics used in practice actually do not always generate a partition of some test classes.\n\nSome testing tactics originally proposed for the TTF are the following:\n\n\nSome other testing tactics that may also be used are the following:\n\n\nThe application of a testing tactic to the VIS generates some test classes. If some of these test classes are further partitioned by applying one or more testing tactics, a new set of test classes is obtained. This process can continue by applying testing tactics to the test classes generated so far. Evidently, the result of this process can be drawn as a tree with the VIS as the root node, the test classes generated by the first testing tactic as its children, and so on. Furthermore, Stocks and Carrington in propose to use the Z notation to build the tree, as follows.\n\nformula_64\n\nIn general a test class' predicate is a conjunction of two or more predicates. It is likely, then, that some test classes are empty because their predicates are contradictions. These test classes must be pruned from the testing tree because they represent impossible combinations of input values, i.e. no abstract test case can be derived out of them.\n\nAn abstract test case is an element belonging to a test class. The TTF prescribes that abstract test cases should be derived only from the leaves of the testing tree. Abstract test cases can also be written as Z schema boxes. Let formula_1 be some operation, let formula_12 be the VIS of formula_1, let formula_68 be all the variables declared in formula_12, let formula_20 be a (leaf) test class of the testing tree associated to formula_1, let formula_72 be the characteristic predicates of each test class from formula_20 up to formula_12 (by following the edges from child to parent), and let formula_75 be formula_76 constant values satisfying formula_77. Then, an abstract test case of formula_20 is the Z schema box defined by formula_79.\n\n\n"}
{"id": "29229128", "url": "https://en.wikipedia.org/wiki?curid=29229128", "title": "The Fractal Geometry of Nature", "text": "The Fractal Geometry of Nature\n\nThe Fractal Geometry of Nature is a 1982 book by the Franco-American mathematician Benoît Mandelbrot.\n\n\"The Fractal Geometry of Nature\" is a revised and enlarged version of his 1977 book entitled \"Fractals: Form, Chance and Dimension\", which in turn was a revised, enlarged, and translated version of his 1975 French book, \"Les Objects Fractals: Forme, Hasard et Dimension\". \"American Scientist\" put the book in its one hundred books of 20th century science.\n\nAs technology has improved mathematically accurate, computer-drawn fractals have become more detailed. Early drawings were low-resolution black and white; later drawings were higher resolution and in color. Many examples were created by programmers working with Mandelbrot, primarily at IBM Research. These visualizations have added to persuasiveness of the books and their impact on the scientific community.\n\n"}
{"id": "2058228", "url": "https://en.wikipedia.org/wiki?curid=2058228", "title": "Twelf", "text": "Twelf\n\nTwelf is an implementation of the logical framework LF. It is used for logic programming and for the formalization of programming language theory.\n\nAt its simplest, a Twelf program (called a \"signature\") is a collection of declarations of type families and constants that inhabit those type families. For example, the following is the standard definition of the natural numbers, with codice_1 standing for zero and codice_2 the successor operator.\n\nHere codice_3 is a type, and codice_1 and codice_2 are constant terms. As a dependently typed system, types can be indexed by terms, which allows the definition of more interesting type families (relations). Here is a definition of addition:\n\nThe type family codice_6 is read as a relation between three natural numbers codice_7, codice_8 and codice_9, such that M + N = P. We then give the constants that define the relation: codice_10 indicates that any natural number codice_7 plus zero is still codice_7. The quantifier codice_13 can be read as \"for all codice_7 of type codice_3\".\n\nThe constant codice_16 defines the case for when the second argument is the successor of some other number codice_8 (see pattern matching). The result is the successor of codice_9, where codice_9 is the sum of codice_7 and codice_8. This recursive call is made via the subgoal codice_22, introduced with codice_23. The arrow can be understood operationally as Prolog's codice_24, or as logical implication (\"if M + N = P, then M + (s N) = (s P)\"), or most faithfully to the type theory, as the type of the constant codice_16 (\"when given a term of type codice_22, return a term of type codice_27\").\n\nTwelf features type reconstruction and supports implicit parameters, so in practice one usually does not need to explicitly write codice_13 (etc.) above.\n\nThese simple examples do not display LF's higher-order features, nor any of its theorem checking capabilities. See the Twelf distribution for its included examples.\n\nTwelf is used in several different ways.\n\nTwelf signatures can be executed via a search procedure, so Twelf can be used as a logic programming language. Its core is more sophisticated than Prolog, since it is higher-order and dependently typed, but it is restricted to pure operators: there is no cut or other extralogical operators (such as ones for performing I/O) as are often found in Prolog implementations, which may make it less well-suited for practical logic programming applications. Some of the use of cut rule as used in Prolog is obtained through the ability to declare that certain operators belong to deterministic type families, which avoids recalculation. Also, like λProlog, Twelf generalizes the Horn clauses underlying Prolog to hereditary Harrop formulas, which allow for logically well-founded operational notions of fresh-name generation and scoped extension of the clause database.\nTwelf's main use today is as a system for formalizing mathematics (especially the metatheory of programming languages). Used this way it is closely related to Coq and Isabelle/HOL/HOL Light. However, unlike those systems, Twelf proofs are typically developed by hand. Despite this, for the problem domains at which it excels, Twelf proofs are often shorter and easier to develop than in the automated, general-purpose systems.\n\nTwelf is particularly well suited to the encoding of programming languages and logics, because it has a built-in notion of binding and substitution. Most logics and programming languages of interest make use of binding and substitution. When implemented in Twelf, binders can often be directly encoded using the technique of higher-order abstract syntax (HOAS), in which the meta-language (Twelf) binders are used to represent the object-level binders. As a consequence, standard theorems such as type-preserving substitution and alpha conversion come \"for free\".\n\nTwelf has been used to formalize many different logics and programming languages (examples are included with the distribution). Among the larger projects are a proof of safety for the Standard ML programming language, a foundational typed assembly language system from CMU, and a foundational proof carrying code system from Princeton.\n\nTwelf is written in Standard ML and binaries are available for Linux and Microsoft Windows. it is under active development (mostly at Carnegie Mellon University).\n\n"}
{"id": "49222617", "url": "https://en.wikipedia.org/wiki?curid=49222617", "title": "VeRoLog", "text": "VeRoLog\n\nThe European Working Group on Vehicle Routing and Logistics Optimization (also, EWG VeRoLog, or simply VeRoLog) is a working group within EURO, the Association of European Operational Research Societies whose objective is to promote the application of operations research models, methods and tools to the field of vehicle routing and logistics, and to encourage the exchange of information among practitioners, end-users, and researchers, stimulating the work on new and important problems with sound scientific methods.\n\nVeRoLog is one of the working groups of EURO, the Association of European Operational Research Societies. The Group was founded in 2011 by Daniele Vigo, Marielle Christiansen, Angel Corberan, Wout Dullaert, Richard Eglese, Geir Hasle, Stefan Irnich, Frederic Semet and Maria Grazia Speranza. \nThe group is managed by a Coordinator and an Advisory Board including the founding members. The current coordinator is Daniele Vigo.\n\nThe group is suitable for people who are presently engaged in Vehicle Routing and Logistics, either in theoretical aspects or in business, industry or public administration applications. Currently (2015), the group has about 1,500 members from 67 countries.\n\nVeRoLog holds conferences on a regular basis (once a year during Summer) and issues every year an award to the best doctoral dissertation on vehicle routing and logistics optimization.\n\nIn most cases, the annual conference is followed by a peer reviewed special issue of an international journal, presenting a selection of the contributions presented at the meeting. Recent special issues appeared on \"European Journal of Operational Research\", and \"Computers and Operations Research\".\n\nA newsletter is emailed to all members every month.\n"}
{"id": "10102876", "url": "https://en.wikipedia.org/wiki?curid=10102876", "title": "Vitali covering lemma", "text": "Vitali covering lemma\n\nIn mathematics, the Vitali covering lemma is a combinatorial and geometric result commonly used in measure theory of Euclidean spaces. This lemma is an intermediate step, of independent interest, in the proof of the Vitali covering theorem. The covering theorem is credited to the Italian mathematician Giuseppe Vitali. The theorem states that it is possible to cover, up to a Lebesgue-negligible set, a given subset \"E\"  of R by a disjoint family extracted from a \"Vitali covering\" of \"E\".\n\n\n\nComments.\n\nWithout loss of generality, we assume that the collection of balls is not empty; that is, \"n\" > 0. Let formula_11 be the ball of largest radius. Inductively, assume that formula_12 have been chosen. If there is some ball in formula_13 that is disjoint from formula_14, let formula_15 be such ball with maximal radius (breaking ties arbitrarily), otherwise, we set \"m\" := \"k\" and terminate the inductive definition.\n\nNow set formula_16. It remains to show that formula_17 for every formula_18. This is clear if formula_19. Otherwise, there necessarily is some formula_20 such that \"B\" intersects formula_21 and the radius of formula_21 is at least as large as that of \"B\". The triangle inequality then easily implies that formula_23, as needed. This completes the proof of the finite version.\n\nLet F denote the collection of all balls \"B\", \"j\" ∈ \"J\", that are given in the statement of the \"covering lemma\". The following result provides a certain disjoint subcollection G of F. If this subcollection G is described as formula_24, the property of G, stated below, readily proves that\n\nPrecise form of the covering lemma. \"Let\"  F \"be a collection of (nondegenerate) balls in a metric space, with bounded radii. There exists a disjoint subcollection\"  G \"of\"  F \"with the following property:\"\n\nLet \"R\"  be the supremum of the radii of balls in F. Consider the partition of F into subcollections F, \"n\" ≥ 0, consisting of balls \"B\"  whose radius is in (2\"R\", 2\"R\"]. A sequence G, with G ⊂ F, is defined inductively as follows. First, set H = F and let G be a maximal disjoint subcollection of H. Assuming that G...,G have been selected, let\nand let G be a maximal disjoint subcollection of H. The subcollection\nof F satisfies the requirements: G is a disjoint collection, and every ball \"B\" ∈ F intersects a ball \"C\" ∈ G such that \"B\" ⊂ 5 \"C\".\nIndeed, let \"n\"  be such that \"B\"  belongs to F. Either \"B\"  does not belong to H, which implies \"n\" > 0 and means that \"B\"  intersects a ball from the union of G...,G, or \"B\" ∈ H and by maximality of G, \"B\"  intersects a ball in G. In any case, \"B\"  intersects a ball \"C\"  that belongs to the union of G...,G. Such a ball \"C\"  has radius > 2\"R\". Since the radius of \"B\"  is ≤ 2\"R\", it is less than twice that of \"C\"  and the conclusion \"B\" ⊂ 5 \"C\"  follows from the triangle inequality as in the finite version.\n\nAn application of the Vitali lemma is in proving the Hardy–Littlewood maximal inequality. As in this proof, the Vitali lemma is frequently used when we are, for instance, considering the \"d\"-dimensional Lebesgue measure, formula_28, of a set \"E\" ⊂ R, which we know is contained in the union of a certain collection of balls formula_6, each of which has a measure we can more easily compute, or has a special property one would like to exploit. Hence, if we compute the measure of this union, we will have an upper bound on the measure of \"E\". However, it is difficult to compute the measure of the union of all these balls if they overlap. By the Vitali lemma, we may choose a subcollection formula_30 which is disjoint and such that formula_31. Therefore,\n\nNow, since increasing the radius of a \"d\"-dimensional ball by a factor of five increases its volume by a factor of 5, we know that\n\nand thus\n\nIn the covering theorem, the aim is to cover, \"up to\"  a \"negligible set\", a given set \"E\" ⊆ R by a disjoint subcollection extracted from a \"Vitali covering\" for \"E\" : a Vitali class or Vitali covering formula_35 for \"E\"  is a collection of sets such that, for every \"x\" ∈ \"E\"  and \"δ\" > 0, there is a set \"U\"  in the collection formula_36 such that \"x\" ∈ \"U\"  and the diameter of \"U\"  is non-zero and less than \"δ\".\n\nIn the classical setting of Vitali, the negligible set is a \"Lebesgue negligible set\", but measures other than the Lebesgue measure, and spaces other than R have also been considered, as is shown in the relevant section below.\n\nThe following observation is useful: if formula_36 is a Vitali covering for \"E\"  and if \"E\"  is contained in an open set \"Ω\" ⊆ R, then the subcollection of sets \"U\"  in formula_36 that are contained in \"Ω\"  is also a Vitali covering for \"E\".\n\nThe next covering theorem for the Lebesgue measure \"λ\" is due to . A collection formula_35 of measurable subsets of R is a \"regular family\" (in the sense of Lebesgue) if there exists a constant \"C\"  such that\nfor every set \"V\"  in the collection formula_36.\nThe family of cubes is an example of regular family formula_36, as is the family formula_36(\"m\") of rectangles in R such that the ratio of sides stays between \"m\" and \"m\", for some fixed \"m\" ≥ 1. If an arbitrary norm is given on R, the family of balls for the metric associated to the norm is another example. To the contrary, the family of \"all\"  rectangles in R is \"not\"  regular.\n\nTheorem. Let \"E\" ⊆ R be a measurable set with finite Lebesgue measure, and let formula_36 be a regular family of closed subsets of R that is a Vitali covering for \"E\". Then there exists a finite or countably infinite disjoint subcollection formula_45 such that\n\nThe original result of is a special case of this theorem, in which \"d\" = 1 and formula_36 is a collection of intervals that is a Vitali covering for a measurable subset \"E\"  of the real line having finite measure.\n\nThe theorem above remains true without assuming that \"E\"  has finite measure. This is obtained by applying the covering result in the finite measure case, for every integer \"n\" ≥ 0, to the portion of \"E\"  contained in the open annulus \"Ω\" of points \"x\" such that \"n\" < |\"x\"| < \"n\"+1.\n\nA somewhat related covering theorem is the Besicovitch covering theorem. To each point \"a\" of a subset \"A\" ⊆ R, a Euclidean ball \"B\"(\"a\", \"r\") with center \"a\" and positive radius \"r\" is assigned. Then, as in the Vitali theorem, a subcollection of these balls is selected in order to cover \"A\" in a specific way. The main differences with the Vitali covering theorem are that on one hand, the disjointness requirement of Vitali is relaxed to the fact that the number \"N\" of the selected balls containing an arbitrary point \"x\" ∈ R is bounded by a constant \"B\"  depending only upon the dimension \"d\"; on the other hand, the selected balls do cover the set \"A\" of all the given centers.\n\nOne may have a similar objective when considering Hausdorff measure instead of Lebesgue measure. The following theorem applies in that case.\n\nTheorem. Let \"H\" denote \"s\"-dimensional Hausdorff measure, let \"E\" ⊆ R be an \"H\"-measurable set and formula_36 a Vitali class\nof closed sets for \"E\". Then there exists a (finite or countably infinite) disjoint subcollection formula_45 such that either\n\nFurthermore, if \"E\"  has finite \"s\"-dimensional Hausdorff measure, then for any \"ε\" > 0, we may choose this subcollection {\"U\"} such that\n\nThis theorem implies the result of Lebesgue given above. Indeed, when \"s\" = \"d\", the Hausdorff measure \"H\" on R coincides with a multiple of the \"d\"-dimensional Lebesgue measure. If a disjoint collection formula_52 is regular and contained in a measurable region \"B\"  with finite Lebesgue measure, then\n\nwhich excludes the second possibility in the first assertion of the previous theorem. It follows that \"E\"  is covered, up to a Lebesgue-negligible set, by the selected disjoint subcollection.\n\nThe covering lemma can be used as intermediate step in the proof of the following basic form of the Vitali covering theorem. Actually, a little more is needed, namely the \"precised form of the covering lemma\" obtained in the \"proof of the infinite version\".\n\nWithout loss of generality, one can assume that all balls in F are nondegenerate and have radius ≤ 1. By the \"precised form of the covering lemma\", there exists a disjoint subcollection G of F such that every ball \"B\" ∈ F intersects a ball \"C\" ∈ G for which \"B\" ⊂ 5 \"C\". Let \"r\" > 0 be given, and let \"Z\"  denote the set of points \"z\" ∈ \"E\"  that are not contained in any ball from G and belong to the \"open\" ball \"B\"(\"r\") of radius \"r\", centered at 0. It is enough to show that \"Z\"  is Lebesgue-negligible, for every given \"r\".\n\nLet \"G\"  denote the subcollection of those balls in G that meet \"B\"(\"r\"). Consider the partition of \"G\"  into sets \"G\", \"n\" ≥ 0, consisting of balls that have radius in (2, 2]. Any ball \"B\"  in F that meets \"B\"(\"r\") is contained in \"B\"(\"r\"+2). It follows from the disjointness property of G that\n\nThis implies that \"G\" is a finite set for every \"n\". Given\n\"ε\" > 0, we may select \"N\"  such that\n\nLet \"z\" ∈ \"Z\"  be fixed. By definition of \"Z\", this point \"z\" does not belong to the closed set \"K\"  equal to the (finite) union of balls in \"G\", \"k\" ≤ \"N\". By the Vitali cover property, one can find a ball \"B\" ∈ F containing \"z\", contained in \"B\"(\"r\") and disjoint from \"K\". By the property of G, the ball \"B\"  meets \"C\"  and is included in 5 \"C\"  for some ball \"C\" ∈ G. One sees that \"C\" ∈ \"G\"  because \"C\"  intersects \"B\"(\"r\"), but \"C\"  does not belong to any family \"G\", \"k\" ≤ \"N\", since \"B\"  meets \"C\"  but is disjoint from \"K\". This proves that every point \"z\" ∈ \"Z\"  is contained in the union of 5 \"C\", when \"C\"  varies in \"G\", \"n\" > \"N\", hence\n\nand\n\nSince \"ε\" > 0 is arbitrary, this shows that \"Z\"  is negligible.\n\nThe Vitali covering theorem is not valid in infinite-dimensional settings. The first result in this direction was given by David Preiss in 1979: there exists a Gaussian measure \"γ\" on an (infinite-dimensional) separable Hilbert space \"H\" so that the Vitali covering theorem fails for (\"H\", Borel(\"H\"), \"γ\"). This result was strengthened in 2003 by Jaroslav Tišer: the Vitali covering theorem in fact fails for \"every\" infinite-dimensional Gaussian measure on any (infinite-dimensional) separable Hilbert space.\n\n\n"}
