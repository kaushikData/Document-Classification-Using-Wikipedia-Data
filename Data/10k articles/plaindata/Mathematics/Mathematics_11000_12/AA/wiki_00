{"id": "33029175", "url": "https://en.wikipedia.org/wiki?curid=33029175", "title": "Alpha shape", "text": "Alpha shape\n\nIn computational geometry, an alpha shape, or α-shape, is a family of piecewise linear simple curves in the Euclidean plane associated with the shape of a finite set of points. They were first defined by . The alpha-shape associated with a set of points is a generalization of the concept of the convex hull, i.e. every convex hull is an alpha-shape but not every alpha shape is a convex hull.\n\nFor each real number \"α\", define the concept of a \"generalized disk of radius\" 1/\"α\" as follows:\nThen an edge of the alpha-shape is drawn between two members of the finite point set whenever there exists a generalized disk of radius 1/\"α\" containing the entire point set and which has the property that the two points lie on its boundary.\n\nIf \"α\" = 0, then the alpha-shape associated with the finite point set is its ordinary convex hull.\n\nAlpha shapes are closely related to alpha complexes, subcomplexes of the Delaunay triangulation of the point set.\n\nEach edge or triangle of the Delaunay triangulation may be associated with a characteristic radius, the radius of the smallest empty circle containing the edge or triangle. For each real number \"α\", the \"α\"-complex of the given set of points is the simplicial complex formed by the set of edges and triangles whose radii are at most 1/\"α\".\n\nThe union of the edges and triangles in the \"α\"-complex forms a shape closely resembling the \"α\"-shape; however it differs in that it has polygonal edges rather than edges formed from arcs of circles. More specifically, showed that the two shapes are homotopy equivalent. (In this later work, Edelsbrunner used the name \"\"α\"-shape\" to refer to the union of the cells in the \"α\"-complex, and instead called the related curvilinear shape an \"α\"-body.)\n\nThis technique can be employed to reconstruct a Fermi surface from the electronic Bloch spectral function evaluated at the Fermi level, as obtained from the Green function in a generalised ab-initio study of the problem. The Fermi surface is then defined as the set of reciprocal space points within the first Brillouin zone, where the signal is highest. \nThe definition has the advantage of covering also cases of various forms of disorder.\n\n\n\n"}
{"id": "324752", "url": "https://en.wikipedia.org/wiki?curid=324752", "title": "Atiyah–Singer index theorem", "text": "Atiyah–Singer index theorem\n\nIn differential geometry, the Atiyah–Singer index theorem, proved by , states that for an elliptic differential operator on a compact manifold, the analytical index (related to the dimension of the space of solutions) is equal to the topological index (defined in terms of some topological data). It includes many other theorems, such as the Riemann–Roch theorem, as special cases, and has applications in theoretical physics.\n\nThe index problem for elliptic differential operators was posed by . He noticed the homotopy invariance of the index, and asked for a formula for it by means of topological invariants. Some of the motivating examples included the Riemann–Roch theorem and its generalization the Hirzebruch–Riemann–Roch theorem, and the Hirzebruch signature theorem. Hirzebruch and Borel had proved the integrality of the Â genus of a spin manifold, and Atiyah suggested that this integrality could be explained if it were the index of the Dirac operator (which was rediscovered by Atiyah and Singer in 1961).\n\nThe Atiyah–Singer theorem was announced by . The proof sketched in this announcement was never published by them, though it appears in the book . It appears also in the \"Séminaire Cartan-Schwartz 1963/64\" that was held in Paris simultaneously with the seminar led by Palais at Princeton. The last talk in Paris was by Atiyah on manifolds with boundary. Their first published proof replaced the cobordism theory of the first proof with K-theory, and they used this to give proofs of various generalizations in the papers .\n\n\n\nIf \"D\" is a differential operator on a Euclidean space of order \"n\" in \"k\" variables\n\nthen its symbol is the function of 2\"k\" variables\n\ngiven by dropping all terms of order less than \"n\" and replacing ∂/∂\"x\" by \"y\". So the symbol is homogeneous in the variables \"y\", of degree \"n\". The symbol is well defined even though ∂/∂\"x\" does not commute with \"x\" because we keep only the highest order terms and differential operators commute \"up to lower-order terms\". The operator is called elliptic if the symbol is nonzero whenever at least one \"y\" is nonzero.\n\nExample: The Laplace operator in \"k\" variables has symbol \"y\" + ... + \"y\", and so is elliptic as this is nonzero whenever any of the \"y\"'s are nonzero. The wave operator has symbol −\"y\" + ... + \"y\", which is not elliptic if \"k\" ≥ 2, as the symbol vanishes for some non-zero values of the \"y\"s.\n\nThe symbol of a differential operator of order \"n\" on a smooth manifold \"X\" is defined in much the same way using local coordinate charts, and is a function on the cotangent bundle of \"X\", homogeneous of degree \"n\" on each cotangent space. (In general, differential operators transform in a rather complicated way under coordinate transforms (see jet bundle); however, the highest order terms transform like tensors so we get well defined homogeneous functions on the cotangent spaces that are independent of the choice of local charts.) More generally, the symbol of a differential operator between two vector bundles \"E\" and \"F\" is a section of the pullback of the bundle Hom(\"E\", \"F\") to the cotangent space of \"X\". The differential operator is called \"elliptic\" if the element of Hom(\"E\", \"F\") is invertible for all non-zero cotangent vectors at any point \"x\" of \"X\".\n\nA key property of elliptic operators is that they are almost invertible; this is closely related to the fact that their symbols are almost invertible. More precisely, an elliptic operator \"D\" on a compact manifold has a (non-unique) parametrix (or pseudoinverse) \"D\"′ such that \"DD′\"−1 and \"D′D\"−1 are both compact operators. An important consequence is that the kernel of \"D\" is finite-dimensional, because all eigenspaces of compact operators, other than the kernel, are finite-dimensional. (The pseudoinverse of an elliptic differential operator is almost never a differential operator. However, it is an elliptic pseudodifferential operator.)\n\nAs the elliptic differential operator \"D\" has a pseudoinverse, it is a Fredholm operator. Any Fredholm operator has an \"index\", defined as the difference between the (finite) dimension of the kernel of \"D\" (solutions of \"Df\" = 0), and the (finite) dimension of the cokernel of \"D\" (the constraints on the right-hand-side of an inhomogeneous equation like \"Df\" = \"g\", or equivalently the kernel of the adjoint operator). In other words,\nThis is sometimes called the analytical index of \"D\".\n\nExample: Suppose that the manifold is the circle (thought of as R/Z), and \"D\" is the operator d/dx − λ for some complex constant λ. (This is the simplest example of an elliptic operator.) Then the kernel is the space of multiples of exp(λ\"x\") if λ is an integral multiple of 2π\"i\" and is 0 otherwise, and the kernel of the adjoint is a similar space with λ replaced by its complex conjugate. So \"D\" has index 0. This example shows that the kernel and cokernel of elliptic operators can jump discontinuously as the elliptic operator varies, so there is no nice formula for their dimensions in terms of continuous topological data. However the jumps in the dimensions of the kernel and cokernel are the same, so the index, given by the difference of their dimensions, does indeed vary continuously, and can be given in terms of topological data by the index theorem.\n\nThe topological index of an elliptic differential operator formula_1 between smooth vector bundles formula_2 and formula_3 on an formula_4-dimensional compact manifold formula_5 is given by\n\nin other words the value of the top dimensional component of the mixed cohomology class formula_7 on the fundamental homology class of the manifold formula_5.\nHere,\n\n\nOne can also define the topological index using only K-theory (and this alternative definition is compatible in a certain sense with the Chern-character construction above). If \"X\" is a compact submanifold of a manifold \"Y\" then there is a pushforward (or \"shriek\") map from K(\"TX\") to K(\"TY\"). The topological index of an element of \nK(\"TX\") is defined to be the image of this operation with \"Y\" some Euclidean space, for which K(\"TY\") can be naturally identified with the integers Z (as a consequence of Bott-periodicity). This map is independent of the embedding of \"X\" in Euclidean space. Now a differential operator as above naturally defines an element of K(\"TX\"), and the image in Z under this map \"is\" the topological index.\n\nAs usual, \"D\" is an elliptic differential operator between vector bundles \"E\" and \"F\" over a compact manifold \"X\".\n\nThe \"index problem\" is the following: compute the (analytical) index of \"D\" using only the symbol \"s\" and \"topological\" data derived from the manifold and the vector bundle. The Atiyah–Singer index theorem solves this problem, and states:\n\nIn spite of its formidable definition, the topological index is usually straightforward to evaluate explicitly. So this makes it possible to evaluate the analytical index. (The cokernel and kernel of an elliptic operator are in general extremely hard to evaluate individually; the index theorem shows that we can usually at least evaluate their difference.) Many important invariants of a manifold (such as the signature) can be given as the index of suitable differential operators, so the index theorem allows us to evaluate these invariants in terms of topological data.\n\nAlthough the analytical index is usually hard to evaluate directly, it is at least obviously an integer. The topological index is by definition a rational number, but it is usually not at all obvious from the definition that it is also integral. So the Atiyah–Singer index theorem implies some deep integrality properties, as it implies that the topological index is integral.\n\nThe index of an elliptic differential operator obviously vanishes if the operator is self adjoint. It also vanishes if the manifold \"X\" has odd dimension, though there are pseudodifferential elliptic operators whose index does not vanish in odd dimensions.\n\nThe proof of this result goes through specific considerations, including the extension of Hodge theory on combinatorial and Lipschitz manifolds , , the extension of Atiyah–Singer's signature operator to Lipschitz manifolds , Kasparov's K-homology and topological cobordism .\n\nThis result shows that the index theorem is not merely a differentiable statement, but rather a topological statement.\n\nThis theory is based on a signature operator \"S\", defined on middle degree differential forms on even-dimensional quasiconformal manifolds (compare ).\n\nUsing topological cobordism and K-homology one may provide a full statement of an index theorem on quasiconformal manifolds (see page 678 of ). The work \"provides local constructions for characteristic classes based on higher dimensional relatives of the measurable Riemann mapping in dimension two and the Yang–Mills theory in dimension four.\"\n\nThese results constitute significant advances along the lines of Singer's program \"Prospects in Mathematics\" . At the same time, they provide, also, an effective construction of the rational Pontrjagin classes on topological manifolds. The paper provides a link between Thom's original construction of the rational Pontrjagin classes and index theory.\n\nIt is important to mention that the index formula is a topological statement. The obstruction theories due to Milnor, Kervaire, Kirby, Siebenmann, Sullivan, Donaldson show that only a minority of topological manifolds possess differentiable structures and these are not necessarily unique. Sullivan's result on Lipschitz and quasiconformal structures shows that any topological manifold in dimension different from 4 possesses such a structure which is unique (up to isotopy close to identity).\n\nThe quasiconformal structures and more generally the \"L\"-structures, \n\"p\" > \"n(n+1)/2\", \nintroduced by M. Hilsum , are the weakest analytical structures on topological manifolds of dimension \"n\" for which the \nindex theorem is known to hold.\n\nSuppose that \"M\" is a compact oriented manifold. If we take \"E\" to be the sum of the even exterior powers of the cotangent bundle, and \"F\" to be the sum of the odd powers, define \"D\" = \"d\" + \"d*\", considered as a map from \"E\" to \"F\". Then the topological index of \"D\" is the Euler characteristic of the Hodge cohomology of \"M\", and the analytical index is the Euler class of the manifold. The index formula for this operator yields the Chern-Gauss-Bonnet theorem.\n\nTake \"X\" to be a complex manifold with a holomorphic vector bundle \"V\". We let the vector bundles \"E\" and \"F\" be the sums of the bundles of differential forms with coefficients in \"V\" of type (0,\"i\") with \"i\" even or odd, and we let the differential operator \"D\" be the sum\n\nrestricted to \"E\". Then the analytical index of \"D\" is the holomorphic Euler characteristic of \"V\":\n\nThe topological index of \"D\" is given by\n\nthe product of the Chern character of \"V\" and the Todd class of \"X\" evaluated on the fundamental class of \"X\".\nBy equating the topological and analytical indices we get the Hirzebruch–Riemann–Roch theorem. In fact we get a generalization of it to all complex manifolds: Hirzebruch's proof only worked for projective complex manifolds \"X\".\n\nThis derivation of the Hirzebruch–Riemann–Roch theorem is more natural if we use the index theorem for elliptic complexes rather than elliptic operators. \nWe can take the complex to be\n\nwith the differential given by formula_26. Then the \"i\"'th cohomology group is just the coherent cohomology group H(\"X\", \"V\"), so the analytical index of this complex is the holomorphic Euler characteristic Σ (−1) dim(H(\"X\", \"V\")). As before, the topological index is ch(\"V\")Td(\"X\")[\"X\"].\n\nThe Hirzebruch signature theorem states that the signature of a compact oriented manifold \"X\" of dimension 4\"k\" is given by the L genus of the manifold. This follows from the Atiyah–Singer index theorem applied to the following signature operator.\n\nThe bundles \"E\" and \"F\" are given by the +1 and −1 eigenspaces of the operator on the bundle of differential forms of \"X\", that acts on \"k\"-forms as\n\ntimes the Hodge * operator. The operator \"D\" is the Hodge Laplacian\n\nrestricted to \"E\", where d is the Cartan exterior derivative and d* is its adjoint.\n\nThe analytic index of \"D\" is the signature of the manifold \"X\", and its topological index is the L genus of \"X\", so these are equal.\n\nThe Â genus is a rational number defined for any manifold, but is in general not an integer. Borel and Hirzebruch showed that it is integral for spin manifolds, and an even integer if in addition the dimension is 4 mod 8. This can be deduced from the index theorem, which implies that the Â genus for spin manifolds is the index of a Dirac operator. The extra factor of 2 in dimensions 4 mod 8 comes from the fact that in this case the kernel and cokernel of the Dirac operator have a quaternionic structure, so as complex vector spaces they have even dimensions, so the index is even.\n\nIn dimension 4 this result implies Rochlin's theorem that the signature of a 4-dimensional spin manifold is divisible by 16: this follows because in dimension 4 the Â genus is minus one eighth of the signature.\n\nPseudodifferential operators can be explained easily in the case of constant coefficient operators on Euclidean space. In this case, constant coefficient differential operators are just the Fourier transforms of multiplication by polynomials, and constant coefficient pseudodifferential operators are just the Fourier transforms of multiplication by more general functions.\n\nMany proofs of the index theorem use pseudodifferential operators rather than differential operators. The reason for this is that for many purposes there are not enough differential operators. For example, a pseudoinverse of an elliptic differential operator of positive order is not a differential operator, but is a pseudodifferential operator. \nAlso, there is a direct correspondence between data representing elements of K(B(\"X\"), \"S\"(\"X\")) (clutching functions) and symbols of elliptic pseudodifferential operators.\n\nPseudodifferential operators have an order, which can be any real number or even −∞, and have symbols (which are no longer polynomials on the cotangent space), and elliptic differential operators are those whose symbols are invertible for sufficiently large cotangent vectors. Most version of the index theorem can be extended from elliptic differential operators to elliptic pseudodifferential operators.\n\nThe initial proof was based on that of the Hirzebruch–Riemann–Roch theorem (1954), and involved cobordism theory and pseudodifferential operators.\n\nThe idea of this first proof is roughly as follows. Consider the ring generated by pairs (\"X\", \"V\") where \"V\" is a smooth vector bundle on the compact smooth oriented manifold \"X\", with relations that the sum and product of the ring on these generators are given by disjoint union and product of manifolds (with the obvious operations on the vector bundles), and any boundary of a manifold with vector bundle is 0. This is similar to the cobordism ring of oriented manifolds, except that the manifolds also have a vector bundle. The topological and analytical indices are both reinterpreted as functions from this ring to the integers. Then one checks that these two functions are in fact both ring homomorphisms. In order to prove they are the same, it is then only necessary to check they are the same on a set of generators of this ring. Thom's cobordism theory gives a set of generators; for example, complex vector spaces with the trivial bundle together with certain bundles over even dimensional spheres. So the index theorem can be proved by checking it on these particularly simple cases.\n\nAtiyah and Singer's first published proof used K-theory rather than cobordism. If \"i\" is any inclusion of compact manifolds from \"X\" to \"Y\", they defined a 'pushforward' operation \"i\" on elliptic operators of \"X\" to elliptic operators of \"Y\" that preserves the index. By taking \"Y\" to be some sphere that \"X\" embeds in, this reduces the index theorem to the case of spheres. If \"Y\" is a sphere and \"X\" is some point embedded in \"Y\", then any elliptic operator on \"Y\" is the image under \"i\" of some elliptic operator on the point. This reduces the index theorem to the case of a point, where it is trivial.\n\n gave a new proof of the index theorem using the heat equation, see e.g. . \nThe proof is also published in and .\n\nIf \"D\" is a differential operator with adjoint \"D*\", then \"D*D\" and \"DD*\" are self adjoint operators whose non-zero eigenvalues have the same multiplicities. However their zero eigenspaces may have different multiplicities, as these multiplicities are the dimensions of the kernels of \"D\" and \"D*\". Therefore, the index of \"D\" is given by \nfor any positive \"t\". The right hand side is given by the trace of the difference of the kernels of two heat operators. These have an asymptotic expansion for small positive \"t\", which can be used to evaluate the limit as \"t\" tends to 0, giving a proof of the Atiyah–Singer index theorem. The asymptotic expansions for small \"t\" appear very complicated, but invariant theory shows that there are huge cancellations between the terms, which makes it possible to find the leading terms explicitly. These cancellations were later explained using supersymmetry.\n\n\n\nThe papers by Atiyah are reprinted in volumes 3 and 4 of his collected works, \n\n\n\n"}
{"id": "14703145", "url": "https://en.wikipedia.org/wiki?curid=14703145", "title": "Aubin–Lions lemma", "text": "Aubin–Lions lemma\n\nIn mathematics, the Aubin–Lions lemma (or theorem) is the result in the theory of Sobolev spaces of Banach space-valued functions, which provides a compactness criterion that is useful in the study of nonlinear evolutionary partial differential equations. Typically, to prove the existence of solutions one first constructs approximate solutions (for example, by a Galerkin method or by mollification of the equation), then uses the compactness lemma to show that there is a convergent subsequence of approximate solutions whose limit is a solution.\n\nThe result is named after the French mathematicians Jean-Pierre Aubin and Jacques-Louis Lions. In the original proof by Aubin, the spaces \"X\" and \"X\" in the statement of the lemma were assumed to be reflexive, but this assumption was removed by Simon, so the result is also referred to as the Aubin–Lions–Simon lemma.\n\nLet \"X\", \"X\" and \"X\" be three Banach spaces with \"X\" ⊆ \"X\" ⊆ \"X\". Suppose that \"X\" is compactly embedded in \"X\" and that \"X\" is continuously embedded in \"X\". For 1 ≤ \"p\", \"q\" ≤ +∞, let\n\n(i) If \"p\"  < +∞, then the embedding of \"W\" into \"L\"([0, \"T\"]; \"X\") is compact.\n\n(ii) If \"p\"  = +∞ and \"q\"  >  1, then the embedding of \"W\" into \"C\"([0, \"T\"]; \"X\") is compact.\n\n\n\n"}
{"id": "28751817", "url": "https://en.wikipedia.org/wiki?curid=28751817", "title": "Baker's theorem", "text": "Baker's theorem\n\nIn transcendental number theory, a mathematical discipline, Baker's theorem gives a lower bound for the absolute value of linear combinations of logarithms of algebraic numbers. The result, proved by , subsumed many earlier results in transcendental number theory and solved a problem posed by Alexander Gelfond nearly fifteen years earlier.\nBaker used this to prove the transcendence of many numbers, to derive effective bounds for the solutions of some Diophantine equations, and to solve the class number problem of finding all imaginary quadratic fields with class number 1.\n\nTo simplify notation, we introduce the set \"L\" of logarithms of nonzero algebraic numbers, that is\nUsing this notation, several results in transcendental number theory become much easier to state. For example the Hermite–Lindemann theorem becomes the statement that any nonzero element of \"L\" is transcendental.\n\nIn 1934, Alexander Gelfond and Theodor Schneider independently proved the Gelfond–Schneider theorem. This result is usually stated as: if \"a\" is algebraic and not equal to 0 or 1, and if \"b\" is algebraic and irrational, then \"a\" is transcendental. Equivalently, though, it says that if λ and λ are elements of \"L\" that are linearly independent over the rational numbers, then they are linearly independent over the algebraic numbers. So if λ and λ are elements of \"L\" and λ isn't zero, then the quotient λ/λ is either a rational number or transcendental. It can't be an algebraic irrational number like .\n\nAlthough proving this result of \"rational linear independence implies algebraic linear independence\" for two elements of \"L\" was sufficient for his and Schneider's result, Gelfond felt that it was crucial to extend this result to arbitrarily many elements of \"L\". Indeed, from :\nThis problem was solved fourteen years later by Alan Baker and has since had numerous applications not only to transcendence theory but in algebraic number theory and the study of Diophantine equations as well. Baker received the Fields medal in 1970 for both this work and his applications of it to Diophantine equations.\n\nWith the above notation, Baker's theorem is a nonhomogeneous generalisation of the Gelfond–Schneider theorem. Specifically it states:\n\nBaker's Theorem. If λ,…,λ are elements of \"L\" that are linearly independent over the rational numbers, then for any algebraic numbers β, ..., β, not all zero, we have\nwhere \"H\" is the maximum of the heights of the β's and \"C\" is an effectively computable number depending on \"n\", the λ's, and the maximum \"d\" of the degrees of the β's. (If β is nonzero then the assumption that the λ's are linearly independent can be dropped.) In particular this number is nonzero, so 1 and the λ's are linearly independent over the algebraic numbers.\n\nJust as the Gelfond–Schneider theorem is equivalent to the statement about the transcendence of numbers of the form \"a\", so too Baker's theorem implies the transcendence of numbers of the form\nwhere the \"b\" are all algebraic, irrational, and 1, \"b\",…,\"b\" are linearly independent over the rationals, and the \"a\" are all algebraic and not 0 or 1.\n\nis either 0 or satisfies\nwhere\nand the field generated by all the α's and β's over the rationals has degree at most \"d\". In the special case when β=0 and all the β are rational integers, the rightmost term log Ω can be deleted.\n\nAn explicit result by Baker and Wüstholz for a linear form Λ with integer coefficients yields a lower bound of the form\n\nwith a constant \"C\"\n\nwhere \"d\" is the degree of the number field generated by the α.\n\nBaker's proof of his theorem is an extension of the argument given by .\nThe main ideas of the proof are illustrated by the proof of the following qualitative version of the theorem of described by : if the numbers 2π\"i\" and log \"a\"..., log \"a\" are linearly independent over the rational numbers, for nonzero algebraic numbers \"a\"..., \"a\", then they are linearly independent over the algebraic numbers. The precise quantitative version of Bakers theory can be proved by replacing the conditions that things are zero by conditions that things are sufficiently small throughout the proof.\n\nThe main idea of Bakers proof is to construct an auxiliary function Φ(\"z\"...,\"z\") of several variables that vanishes to high order at many points of the form Φ(l,l...,l), then repeatedly show that it vanishes to lower order at even more points of this form. Finally the fact that it vanishes (to order 1) at enough points of this form implies using Vandermonde determinants that there is a multiplicative relation between the numbers \"a\".\n\nAssume there is a relation\nfor algebraic numbers α, ..., α, β, ..., β The function Φ is of the form\nThe integer coefficients \"p\" are chosen so that they are not all zero and Φ and its derivatives of order at most some constant \"M\" vanish at \"z\"= ... =\"z\" = \"l\", for integers \"l\" with 0≤\"l\"≤\"h\" for some constant \"h\". This is possible because these conditions are homogeneous linear equations in the coefficients \"p\", which have a non-zero solution provided the number of unknown variables \"p\" is larger than the number of equations. The linear relation between the logs of the α's is needed to cut down the number of linear equations that have to be satisfied. Moreover, using Siegel's lemma, the sizes of the coefficients \"p\" can be chosen to be not too large. The constants \"L\", \"h\", and \"M\" have to be carefully adjusted to that the next part of the proof works, and are subject to some constraints, which are roughly:\nThe constraints can be satisfied by taking \"h\" to be sufficiently large, \"M\" to be some fixed power of \"h\", and \"L\" to be a slightly smaller power of \"h\". Baker took \"M\" to be about \"h\" and \"L\" to be about \"h\".\n\nThe linear relation between the logarithms of the α's is used to reduce \"L\" slightly; roughly speaking, without it the condition \"L\" must be larger than about \"M\"\"h\" would become \"L\" must be larger than about \"M\"\"h\", which is incompatible with the condition that \"L\" is somewhat smaller than \"M\".\n\nThe next step is to show that Φ vanishes to slightly smaller order at many more points of the form \"z\" = ... = \"z\" =\"l\" for integers \"l\". This idea was Baker's key innovation: previous work on this problem involved trying to increase the number of derivatives that vanish while keeping the number of points fixed, which does not seem to work in the multivariable case. This is done by combining two ideas; First one shows that the derivatives at these points are quite small, by using the fact that many derivatives of Φ vanish at many nearby points. Then one shows that derivatives of Φ at this point are given by algebraic integers times known constants. If an algebraic integer has all its conjugates bounded by a known constant, then it cannot be too small unless it is zero, because the product of all conjugates of a nonzero algebraic integer is at least 1 in absolute value. Combining these two ideas implies that Φ vanishes to slightly smaller order at many more points \"z\" = ... = \"z\" =\"l\". This part of the argument requires that Φ does not increase too rapidly; the growth of Φ depends on the size of \"L\", so requires a bound on the size of \"L\", which turns out to be roughly that \"L\" must be somewhat smaller than \"M\". More precisely, Baker showed that since Φ vanishes to order \"M\" at \"h\" consecutive integers, it also vanishes to order \"M\"/2 at \"h\" consecutive integers 1, 2, 3, ... Repeating this argument \"J\" times shows that Φ vanishes to order \"M\"/2 at \"h\" points, provided that \"h\" is sufficiently large and \"L\" is somewhat smaller than \"M\"/2.\n\nOne then takes \"J\" large enough that \"h\" > (\"L\"+1) (\"J\" larger than about 16\"n\" will do if \"h\" > \"L\") so that that Φ(\"l\", ..., \"l\") = 0 for all integers \"l\" with 1 ≤ \"l\" ≤ (\"L\"+1).\n\nThe condition that Φ(\"l\", ..., \"l\")=0 for all integers \"l\" with 1≤ \"l\"≤ (\"L\"+1) can be written as \nThis consists of (\"L\"+1) homogeneous linear equations in the (\"L\"+1) unknowns \"p\", and by assumption has a non-zero solution \"p\", so the determinant of the matrix of coefficients must vanish. However this matrix is a Vandermonde matrix, so the formula for the determinant of such a matrix forces an equality two of the values\nso the numbers α...,α are multiplicatively dependent. Taking logs then shows that 2π\"i\", log α...,log α are linearly dependent over the rationals.\n\n in fact gave a quantitative version of the theorem, giving effective lower bounds for the linear form in logarithms. This is done by a similar argument, except statements about something being zero are replaced by statements giving a small upper bound for it, and so on.\n\nand one inserts an extra variable \"z\" into Φ as follows:\n\nAs mentioned above, the theorem includes numerous earlier transcendence results concerning the exponential function, such as the Hermite–Lindemann theorem and Gelfond–Schneider theorem. It is not quite as encompassing as the still unproven Schanuel's conjecture, and does not imply the six exponentials theorem nor, clearly, the still open four exponentials conjecture.\n\nThe main reason Gelfond desired an extension of his result was not just for a slew of new transcendental numbers. In 1935 he used the tools he had developed to prove the Gelfond–Schneider theorem to derive a lower bound for the quantity\nwhere β and β are algebraic and λ and λ are in \"L\". Baker's proof gave lower bounds for quantities like the above but with arbitrarily many terms, and he could use these bounds to develop effective means of tackling Diophantine equations and to solve Gauss' class number problem.\n\nBaker's theorem grants us the linear independence over the algebraic numbers of logarithms of algebraic numbers. This is weaker than proving their \"algebraic\" independence. So far no progress has been made on this problem at all. It has been conjectured that if λ,…,λ are elements of \"L\" that are linearly independent over the rational numbers, then they are algebraically independent too. This is a special case of Schanuel's conjecture, but so far it remains to be proved that there even exist two algebraic numbers whose logarithms are algebraically independent. Indeed, Baker's theorem rules out linear relations between logarithms of algebraic numbers unless there are trivial reasons for them; the next most simple case, that of ruling out homogeneous quadratic relations, is the still open four exponentials conjecture.\n\nSimilarly, extending the result to algebraic independence but in the p-adic setting, and using the \"p\"-adic logarithm function, remains an open problem. It is known that proving algebraic independence of linearly independent \"p\"-adic logarithms of algebraic \"p\"-adic numbers would prove Leopoldt's conjecture on the \"p\"-adic ranks of units of a number field.\n\n\n"}
{"id": "16668496", "url": "https://en.wikipedia.org/wiki?curid=16668496", "title": "Barycentric-sum problem", "text": "Barycentric-sum problem\n\nCombinatorial number theory deals with number theoretic problems which involve combinatorial ideas in their formulations or solutions. Paul Erdős is the main founder of this branch of number theory. Typical topics include covering system, zero-sum problems, various restricted sumsets, and arithmetic progressions in a set of integers. Algebraic or analytic methods are powerful in this field.\n\nIn combinatorial number theory, the barycentric-sum problems are questions that can be answered using combinatorial techniques. The context of barycentric-sum problems are the barycentric sequences.\n\nLet formula_1 be the cyclic group of integers modulo \"n\". Let \"S\" be a sequence of elements of formula_1, where the repetition of elements is allowed. Let formula_3 be the length of \"S\". A sequence formula_4 with formula_5 is barycentric or has a\nbarycentric-sum if it contains one element formula_6 such that formula_7.\n\nInformally, if formula_8 contains one element formula_6, which is the ”average” of its terms. A barycentric sequence of length formula_10 is called a t-barycentric sequence. Moreover, when \"S\" is a set, the term barycentric set is used instead of barycentric sequence. For example, the set {0,1,2,3,4} formula_11 is 5-barycentric with barycenter 2, however the set {0,2,3,4,5} formula_11 is not 5-barycentric. The barycentric-sum problem consist in finding the smallest integer \"t\" such that any sequence of length \"t\" contains a \"k\"-barycentric sequence for some given \"k\".The study of the existence of such t related with k and the study of barycentric constants are part of the barycentric-sum problems. It has been introduced by Ordaz, inspired in a theorem of Hamidoune: every sequence of length formula_13 in formula_1 contains a k-barycentric sequence. Notice that a \"k\"-barycentric sequence in formula_1, with k a multiple of n, is a sequence with zero-sum. The zero-sum problem on sequences started in 1961 with the Erdős, Ginzburg and Ziv theorem: every sequence of length formula_16 in an abelian group of order \"n\", contains an \"n\"-subsequence with zero-sum.\n\nBarycentric-sum problems have been defined in general for finite abelian groups. However, most of the main results obtained up to now are in formula_1.\n\nThe barycentric constants introduced by Ordaz are: \"k\"-barycentric Olson constant, \"k\"-barycentric Davenport constant, barycentric Davenport constant, generalized barycentric Davenport constant, constrained barycentric Davenport constant. This constants are related to the Davenport constant i.e. the smallest integer \"t\" such that any \"t\"-sequence contains a subsequence with zero-sum. Moreover, related to the classical Ramsey numbers, the barycentric Ramsey numbers are introduced. An overview of the results computed manually or automatically are presented. The implemented algorithms are written in C.\n\n"}
{"id": "26959280", "url": "https://en.wikipedia.org/wiki?curid=26959280", "title": "Blanche Descartes", "text": "Blanche Descartes\n\nBlanche Descartes was a collaborative pseudonym used by the English mathematicians R. Leonard Brooks, Arthur Harold Stone, Cedric Smith, and W. T. Tutte. The four mathematicians met in 1935 as undergraduate students at Trinity College, Cambridge, where they joined the Trinity Mathematical Society and began meeting together to work on mathematical problems. The pseudonym originated by combining the initials of the mathematicians' given names (Bill, Leonard, Arthur, and Cedric) to form \"BLAC\". This was extended to \"BLAnChe\". The surname \"Descartes\" was chosen as a play on the common phrase \"carte blanche\".\n\nOver 30 works were published under the name, including whimsical poetry and mathematical humour, but some serious mathematical results as well. Notably, the foursome proved several theorems in mathematical tessellation. In particular, they solved the problem of squaring the square, showing that a square can be divided into smaller squares, no two of which are the same. They also discovered \"Blanche's Dissection\", a method of dividing a square into rectangles of equal area but different dimensions. They modelled these using abstract electrical networks, an approach that yielded not only solutions to the original problem,\nbut techniques with wider applications to the field of electrical networks. They published their results—under their own names—in 1940. Tutte, who is believed to have contributed the most work under Descartes's name, kept up the pretense for years, refusing to acknowledge even in private that she was fictitious.\n\n\"Descartes\" also published on graph colouring, and Tutte used the pseudonym to publish the fourth known snark, now called the Descartes snark. She also published the poem \"Hymne to Hymen\" as a gift to Hector Pétard (another fictitious mathematical personage) on the day of his wedding to Betti Bourbaki (daughter of Nicolas Bourbaki, yet another fictitious mathematical personage).\n\n\n"}
{"id": "24272141", "url": "https://en.wikipedia.org/wiki?curid=24272141", "title": "Composite bar chart", "text": "Composite bar chart\n\nComposite bar charts are bar charts where each bar displays multiple data points stacked in a single row or column. This may, for instance, take the form of uniform height bars charting a time series with internal stacked colours indicating the percentage participation of a sub-type of data. Another example would be a time series displaying total numbers, with internal colors indicating participation in the total by sub-types.\n\n"}
{"id": "4682019", "url": "https://en.wikipedia.org/wiki?curid=4682019", "title": "Computational chemical methods in solid-state physics", "text": "Computational chemical methods in solid-state physics\n\nComputational chemical methods in solid-state physics follow the same approach as they do for molecules, but with two differences. First, the translational symmetry of the solid has to be utilised, and second, it is possible to use completely delocalised basis functions such as plane waves as an alternative to the molecular atom-centered basis functions. The electronic structure of a crystal is in general described by a band structure, which defines the energies of electron orbitals for each point in the Brillouin zone. Ab initio and semi-empirical calculations yield orbital energies, therefore they can be applied to band structure calculations. Since it is time-consuming to calculate the energy for a molecule, it is even more time-consuming to calculate them for the entire list of points in the Brillouin zone.\n\nCalculations can use the Hartree–Fock method, some post-Hartree–Fock methods, particularly Møller–Plesset perturbation theory to second order (MP2) and density functional theory (DFT).\n\n\n"}
{"id": "5804750", "url": "https://en.wikipedia.org/wiki?curid=5804750", "title": "Conformal connection", "text": "Conformal connection\n\nIn conformal differential geometry, a conformal connection is a Cartan connection on an \"n\"-dimensional manifold \"M\" arising as a deformation of the Klein geometry given by the celestial \"n\"-sphere, viewed as the homogeneous space \n\nwhere \"P\" is the stabilizer of a fixed null line through the origin in R, in the orthochronous Lorentz group O(n+1,1) in \"n\"+2 dimensions.\n\nAny manifold equipped with a conformal structure has a canonical conformal connection called the normal Cartan connection.\n\nA conformal connection on an \"n\"-manifold \"M\" is a Cartan geometry modelled on the conformal sphere, where the latter is viewed as a homogeneous space for O(n+1,1). In other words it is an O(n+1,1)-bundle equipped with\nsuch that the solder form induced by these data is an isomorphism.\n"}
{"id": "671875", "url": "https://en.wikipedia.org/wiki?curid=671875", "title": "Conformal symmetry", "text": "Conformal symmetry\n\nIn mathematical physics, the conformal symmetry of spacetime is expressed by an extension of the Poincaré group. The extension includes special conformal transformations and dilations. In three spatial plus one time dimensions, conformal symmetry has 15 degrees of freedom: ten for the Poincaré group, four for special conformal transformations, and one for a dilation.\n\nHarry Bateman and Ebenezer Cunningham were the first to study the conformal symmetry of Maxwell's equations. They called a generic expression of conformal symmetry a spherical wave transformation.\n\nThe conformal group has the following representation:\n\nwhere formula_2 are the Lorentz generators, formula_3 generates translations, formula_4 generates scaling transformations (also known as dilatations or dilations) and formula_5 generates the special conformal transformations.\n\nThe commutation relations are as follows:\n\nother commutators vanish. Here formula_7 is the Minkowski metric tensor.\n\nAdditionally, formula_4 is a scalar and formula_5 is a covariant vector under the Lorentz transformations.\n\nThe special conformal transformations are given by\nwhere formula_11 is a parameter describing the transformation. This special conformal transformation can also be written as formula_12, where\n\\frac\n"}
{"id": "17560674", "url": "https://en.wikipedia.org/wiki?curid=17560674", "title": "Continuous functions on a compact Hausdorff space", "text": "Continuous functions on a compact Hausdorff space\n\nIn mathematical analysis, and especially functional analysis, a fundamental role is played by the space of continuous functions on a compact Hausdorff space with values in the real or complex numbers. This space, denoted by \"C\"(\"X\"), is a vector space with respect to the pointwise addition of functions and scalar multiplication by constants. It is, moreover, a normed space with norm defined by\n\nthe uniform norm. The uniform norm defines the topology of uniform convergence of functions on \"X\". The space \"C\"(\"X\") is a Banach algebra with respect to this norm. \n\n\nThe space \"C\"(\"X\") of real or complex-valued continuous functions can be defined on any topological space \"X\". In the non-compact case, however, \"C\"(\"X\") is not in general a Banach space with respect to the uniform norm since it may contain unbounded functions. Hence it is more typical to consider the space, denoted here \"C\"(\"X\") of bounded continuous functions on \"X\". This is a Banach space (in fact a commutative Banach algebra with identity) with respect to the uniform norm. \n\nIt is sometimes desirable, particularly in measure theory, to further refine this general definition by considering the special case when \"X\" is a locally compact Hausdorff space. In this case, it is possible to identify a pair of distinguished subsets of \"C\"(\"X\"): \n\n\nThe closure of \"C\"(\"X\") is precisely \"C\"(\"X\"). In particular, the latter is a Banach space.\n\n"}
{"id": "716401", "url": "https://en.wikipedia.org/wiki?curid=716401", "title": "Cross-polytope", "text": "Cross-polytope\n\nIn geometry, a cross-polytope, orthoplex, hyperoctahedron, or cocube is a regular, convex polytope that exists in \"n\"-dimensions. A 2-orthoplex is a square, a 3-orthoplex is a regular octahedron, and a 4-orthoplex is a 16-cell. Its facets are simplexes of the previous dimension, while the cross-polytope's vertex figure is another cross-polytope from the previous dimension.\n\nThe vertices of a cross-polytope can be chosen as the unit vectors pointing along each co-ordinate axis - i.e. all the permutations of . The cross-polytope is the convex hull of its vertices. \nThe \"n\"-dimensional cross-polytope can also be defined as the closed unit ball (or, according to some authors, its boundary) in the ℓ-norm on R:\n\nIn 1 dimension the cross-polytope is simply the line segment [−1, +1], in 2 dimensions it is a square (or diamond) with vertices {(±1, 0), (0, ±1)}. In 3 dimensions it is an octahedron—one of the five convex regular polyhedra known as the Platonic solids. This can be generalised to higher dimensions with an n-orthoplex being constructed as a bipyramid with an (n-1)-orthoplex base.\n\nThe cross-polytope is the dual polytope of the hypercube. The 1-skeleton of a \"n\"-dimensional cross-polytope is a Turán graph \"T\"(2\"n\",\"n\").\n\nThe 4-dimensional cross-polytope also goes by the name hexadecachoron or 16-cell. It is one of six convex regular 4-polytopes. These 4-polytopes were first described by the Swiss mathematician Ludwig Schläfli in the mid-19th century.\n\nThe cross polytope family is one of three regular polytope families, labeled by Coxeter as \"β\", the other two being the hypercube family, labeled as \"γ\", and the simplices, labeled as \"α\". A fourth family, the infinite tessellations of hypercubes, he labeled as \"δ\".\n\nThe \"n\"-dimensional cross-polytope has 2\"n\" vertices, and 2 facets (\"n\"−1 dimensional components) all of which are \"n\"−1 simplices. The vertex figures are all \"n\" − 1 cross-polytopes. The Schläfli symbol of the cross-polytope is {3,3,…,3,4}. \n\nThe dihedral angle of the \"n\"-dimensional cross-polytope is formula_2. This gives: δ = arccos(0/2) = 90°, δ = arccos(-1/3) = 109.47°, δ = arccos(-2/4) = 120°, δ = arccos(-3/5) = 126.87°, ... δ = arccos(-1) = 180°.\n\nThe volume of the \"n\"-dimensional cross-polytope is \n\nFor each pair of non-opposite vertices, there is an edge joining them. More generally, each set of \"k+1\" orthogonal vertices corresponds to a distinct \"k\"-dimensional component which contains them. The number of \"k\"-dimensional components (vertices, edges, faces, …, facets) in an \"n\"-dimensional cross-polytope is thus given by (see binomial coefficient):\n\nThere are many possible orthographic projections that can show the cross-polytopes as 2-dimensional graphs. Petrie polygon projections map the points into a regular \"2n\"-gon or lower order regular polygons. A second projection takes the \"2(n-1)\"-gon petrie polygon of the lower dimension, seen as a bipyramid, projected down the axis, with 2 vertices mapped into the center.\n\nThe vertices of an axis-aligned cross polytope are all at equal distance from each other in the Manhattan distance (L norm). Kusner's conjecture states that this set of 2\"d\" points is the largest possible equidistant set for this distance.\n\nRegular complex polytopes can be defined in complex Hilbert space called \"generalized orthoplexes\" (or cross polytopes), β = {3}{3}...{4}, or ... Real solutions exist with \"p\"=2, i.e. β = β = {3}{3}...{4} = {3,3..,4}. For \"p\">2, they exist in formula_5. A \"p\"-generalized \"n\"-orthoplex has \"pn\" vertices. \"Generalized orthoplexes\" have regular simplexes (real) as facets. Generalized orthoplexes make complete multipartite graphs, β make K for complete bipartite graph, β make K for complete tripartite graphs. β creates K. An orthogonal projection can be defined that maps all the vertices equally-spaced on a circle, with all pairs of vertices connected, except multiples of \"n\". The regular polygon perimeter in these orthogonal projections is called a petrie polygon.\n\nCross-polytopes can be combined with their dual cubes to form compound polytopes:\n\n\n\n"}
{"id": "21626779", "url": "https://en.wikipedia.org/wiki?curid=21626779", "title": "CryptoBuddy", "text": "CryptoBuddy\n\nCryptoBuddy is a simple software application for the encryption and compression of computer files to make them safe and secure. The application uses a 64-bit block cipher algorithm for encryption and a proprietary compression algorithm. The CryptoBuddy software is also used as part of the CryptoStick encryption device from Research Triangle Software, Inc. The software was released for public use on June 12, 2002.\n"}
{"id": "1262398", "url": "https://en.wikipedia.org/wiki?curid=1262398", "title": "Dephasing", "text": "Dephasing\n\nDephasing is a mechanism that recovers classical behavior from a quantum system. It refers to the ways in which coherence caused by perturbation decays over time, and the system returns to the state before perturbation. It is an important effect in molecular and atomic spectroscopy, and in the condensed matter physics of mesoscopic devices.\n\nThe reason can be understood easily if we can see conduction in metals as a typical classical phenomenon with quantum effects all embedded into an effective mass that can be computed quantum mechanically as also happens to resistance that can be seen as a scattering effect of conduction electrons. When the temperature is lowered and the dimensions of the device are meaningfully reduced, this classical behavior should disappear and the laws of quantum mechanics should govern the behavior of conducting electrons seen as waves that ballistically move inside the conductor without any kind of dissipation. Most of the time this is what one observes. But it appeared as a surprise to uncover that the so-called dephasing time, that is the time it takes for the conducting electrons to lose their quantum behavior, becomes finite rather than infinite when the temperature approaches zero in mesoscopic devices violating the expectations of the theory of Altshuler, Aronov and Khmelnitskii (see citation below). This kind of saturation of the dephasing time at low temperatures is an open problem even as several proposals have been put forward.\n\nThe coherence of a sample is explained by the off-diagonal elements of a density matrix. An external electric or magnetic field can create coherence between two quantum states in a sample if the frequency corresponds to the energy gap between the two states. The coherence terms decay with the dephasing time, \"T\".\n\nAfter coherence is created in a sample by light, the sample emits a polarization wave, the frequency of which is equal to and the phase of which is inverted from the incident light. In addition, the sample is excited by the incident light and a population of molecules in the excited state is generated. The light passing through the sample is absorbed because of these two processes, and it is expressed by an absorption spectrum. The coherence decays with the time constant, \"T\", and the intensity of the polarization wave is reduced. The population of the excited state also decays with the time constant of the longitudinal relaxation, \"T\". The time constant \"T\" is usually much smaller than T, and the bandwidth of the absorption spectrum is related to these time constants by the Fourier transform, so the time constant T is a main contributor to the bandwidth. The time constant \"T\" has been measured with ultrafast time-resolved spectroscopy directly, such as in photon echo experiments.\n\nWhat is the dephasing rate of a particle that has an energy E if it is subject to a fluctuating environment that has a temperature T? In particular what is the dephasing rate close to equilibrium (E~T), and what happens in the zero temperature limit? This question has fascinated the mesoscopic community during the last two decades (see references below).\n\n\n"}
{"id": "206210", "url": "https://en.wikipedia.org/wiki?curid=206210", "title": "Discrete Hartley transform", "text": "Discrete Hartley transform\n\nA discrete Hartley transform (DHT) is a Fourier-related transform of discrete, periodic data similar to the discrete Fourier transform (DFT), with analogous applications in signal processing and related fields. Its main distinction from the DFT is that it transforms real inputs to real outputs, with no intrinsic involvement of complex numbers. Just as the DFT is the discrete analogue of the continuous Fourier transform (FT), the DHT is the discrete analogue of the continuous Hartley transform (HT), introduced by Ralph V. L. Hartley in 1942.\n\nBecause there are fast algorithms for the DHT analogous to the fast Fourier transform (FFT), the DHT was originally proposed by Ronald N. Bracewell in 1983 as a more efficient computational tool in the common case where the data are purely real. It was subsequently argued, however, that specialized FFT algorithms for real inputs or outputs can ordinarily be found with slightly fewer operations than any corresponding algorithm for the DHT.\n\nFormally, the discrete Hartley transform is a linear, invertible function \"H\": R -> R (where R denotes the set of real numbers). The \"N\" real numbers \"x\", ..., \"x\" are transformed into the \"N\" real numbers \"H\", ..., \"H\" according to the formula\n\nThe combination formula_2 formula_3 is sometimes denoted formula_4, and should be contrasted with the formula_5 that appears in the DFT definition (where \"i\" is the imaginary unit).\n\nAs with the DFT, the overall scale factor in front of the transform and the sign of the sine term are a matter of convention. Although these conventions occasionally vary between authors, they do not affect the essential properties of the transform.\n\nThe transform can be interpreted as the multiplication of the vector (\"x\", ..., \"x\") by an \"N\"-by-\"N\" matrix; therefore, the discrete Hartley transform is a linear operator. The matrix is invertible; the inverse transformation, which allows one to recover the \"x\" from the \"H\", is simply the DHT of \"H\" multiplied by 1/\"N\". That is, the DHT is its own inverse (involutory), up to an overall scale factor.\n\nThe DHT can be used to compute the DFT, and vice versa. For real inputs \"x\", the DFT output \"X\" has a real part (\"H\" + \"H\")/2 and an imaginary part (\"H\" – \"H\")/2. Conversely, the DHT is equivalent to computing the DFT of \"x\" multiplied by 1+\"i\", then taking the real part of the result.\n\nAs with the DFT, a cyclic convolution z = x*y of two vectors x = (\"x\") and y = (\"y\") to produce a vector z = (\"z\"), all of length \"N\", becomes a simple operation after the DHT. In particular, suppose that the vectors X, Y, and Z denote the DHT of x, y, and z respectively. Then the elements of Z are given by:\n\nwhere we take all of the vectors to be periodic in \"N\" (\"X\" = \"X\", etcetera). Thus, just as the DFT transforms a convolution into a pointwise multiplication of complex numbers (\"pairs\" of real and imaginary parts), the DHT transforms a convolution into a simple combination of \"pairs\" of real frequency components. The inverse DHT then yields the desired vector z. In this way, a fast algorithm for the DHT (see below) yields a fast algorithm for convolution. (This is slightly more expensive than the corresponding procedure for the DFT, not including the costs of the transforms below, because the pairwise operation above requires 8 real-arithmetic operations compared to the 6 of a complex multiplication. This count doesn't include the division by 2, which can be absorbed e.g. into the 1/\"N\" normalization of the inverse DHT.)\n\nJust as for the DFT, evaluating the DHT definition directly would require O(\"N\") arithmetical operations (see Big O notation). There are fast algorithms similar to the FFT, however, that compute the same result in only O(\"N\" log \"N\") operations. Nearly every FFT algorithm, from Cooley–Tukey to prime-factor to Winograd (1985) to Bruun's (1993), has a direct analogue for the discrete Hartley transform. (However, a few of the more exotic FFT algorithms, such as the QFT, have not yet been investigated in the context of the DHT.)\n\nIn particular, the DHT analogue of the Cooley–Tukey algorithm is commonly known as the fast Hartley transform (FHT) algorithm, and was first described by Bracewell in 1984. This FHT algorithm, at least when applied to power-of-two sizes \"N\", is the subject of the United States patent number 4,646,256, issued in 1987 to Stanford University. Stanford placed this patent in the public domain in 1994 (Bracewell, 1995).\n\nAs mentioned above, DHT algorithms are typically slightly less efficient (in terms of the number of floating-point operations) than the corresponding DFT algorithm (FFT) specialized for real inputs (or outputs). This was first argued by Sorensen et al. (1987) and Duhamel & Vetterli (1987). The latter authors obtained what appears to be the lowest published operation count for the DHT of power-of-two sizes, employing a split-radix algorithm (similar to the split-radix FFT) that breaks a DHT of length \"N\" into a DHT of length \"N\"/2 and two real-input DFTs (\"not\" DHTs) of length \"N\"/4. In this way, they argued that a DHT of power-of-two length can be computed with, at best, 2 more additions than the corresponding number of arithmetic operations for the real-input DFT.\n\nOn present-day computers, performance is determined more by cache and CPU pipeline considerations than by strict operation counts, and a slight difference in arithmetic cost is unlikely to be significant. Since FHT and real-input FFT algorithms have similar computational structures, neither appears to have a substantial \"a priori\" speed advantage ( and Šević, 1994). As a practical matter, highly optimized real-input FFT libraries are available from many sources (e.g. from CPU vendors such as Intel), whereas highly optimized DHT libraries are less common.\n\nOn the other hand, the redundant computations in FFTs due to real inputs are more difficult to eliminate for large prime \"N\", despite the existence of O(\"N\" log \"N\") complex-data algorithms for such cases, because the redundancies are hidden behind intricate permutations and/or phase rotations in those algorithms. In contrast, a standard prime-size FFT algorithm, Rader's algorithm, can be directly applied to the DHT of real data for roughly a factor of two less computation than that of the equivalent complex FFT (Frigo and Johnson, 2005). On the other hand, a non-DHT-based adaptation of Rader's algorithm for real-input DFTs is also possible (Chu & Burrus, 1982).\n\nThe rD-DHT (MD-DHT with \"r\" dimensions) is given by\n\nformula_7\n\nwith formula_8 and where formula_9\n\nSimilar to the 1-D case, as a real and symmetric transform, the MD-DHT is simpler than the MD-DFT. For one, the inverse DHT is identical to the forward transform, with the addition of a scaling factor;\n\nand second, since the kernel is real, it avoids the computational complexity of complex numbers. Additionally, the DFT is directly obtainable from the DHT by a simple additive operation (Bracewell, 1983).\n\nThe MD-DHT is widely used in areas like image and optical signal processing. Specific applications include computer vision, high-definition television, and teleconferencing, areas that process or analyze motion images (Zeng, 2000).\n\nAs computing speed keeps increasing, bigger multidimensional problems become computationally feasible, requiring the need for fast multidimensional algorithms. Three such algorithms follow.\n\nIn pursuit of separability for efficiency, we consider the following transform (Bracewell, 1983),\n\nformula_10\n\nIt was shown in Bortfeld (1995), that the two can be related by a few additions. For example, in 3-D,\n\nformula_11\n\nFor formula_12, row-column algorithms can then be implemented. This technique is commonly used due to the simplicity of such R-C algorithms, but they are not optimized for general M-D spaces.\n\nOther fast algorithms have been developed, such as radix-2, radix-4, and split radix. For example, Boussakta (2000) developed the 3-D vector radix,\n\nformula_13\n\nformula_14\n\nformula_15\n\nformula_16\n\nIt was also presented in Boussakta (2000) that this 3D-vector radix algorithm takes formula_17 multiplications and formula_18 additions compared to formula_19 multiplications and formula_20 additions from the row-column approach. The drawback is that the implementation of these radix-type of algorithms is hard to generalize for signals of arbitrary dimensions.\n\nNumber theoretic transforms have also been used for solving the MD-DHT, since they perform extremely fast convolutions. In Boussakta (1988), it was shown how to decompose the MD-DHT transform into a form consisting of convolutions:\n\nFor the 2-D case (the 3-D case is also covered in the stated reference),\n\nformula_21 formula_22 , formula_23\n\ncan be decomposed into 1-D and 2-D circular convolutions as follows,\n\nformula_24\n\nwhere\n\nformula_25 formula_26\n\nformula_27 formula_28\n\nformula_29\n\nformula_30\n\nformula_31\n\nDeveloping formula_32 further,\n\nformula_33\n\nformula_34\n\nAt this point we present the Fermat number transform (FNT). The t Fermat number is given by formula_35, with formula_36. The well known Fermat numbers are for formula_37 (formula_38 is prime for formula_39), (Boussakta, 1988). The Fermat number transform is given by\n\nformula_40\n\nwith formula_41. formula_42 and formula_43 are roots of unity of order formula_44 and formula_45 respectively formula_46.\n\nGoing back to the decomposition, the last term for formula_47 will be denoted as formula_48, then\n\nformula_49\n\nformula_50\n\nformula_31\n\nIf formula_52 and formula_53 are primitive roots of formula_44 and formula_45 (which are guaranteed to exist if formula_45 and formula_44 are prime) then formula_52 and formula_53 map formula_60 to formula_61 So, mapping formula_62 and formula_63 to formula_64 and formula_65, one gets the following,\n\nformula_66\n\nformula_67\n\nformula_68.\n\nWhich is now a circular convolution. With formula_69, formula_70, and formula_71, one has\n\nformula_72\n\nformula_73\n\nwhere formula_74 denotes term by term multiplication. It was also stated in (Boussakta, 1988) that this algorithm reduces the number of multiplications by a factor of 8–20 over other DHT algorithms at a cost of a slight increase in the number of shift and add operations, which are assumed to be simpler than multiplications. The drawback of this algorithm is the constraint that each dimension of the transform has a primitive root.\n\n\n"}
{"id": "9118440", "url": "https://en.wikipedia.org/wiki?curid=9118440", "title": "Dvoretzky's theorem", "text": "Dvoretzky's theorem\n\nIn mathematics, Dvoretzky's theorem is an important structural theorem about normed vector spaces proved by Aryeh Dvoretzky in the early 1960s, answering a question of Alexander Grothendieck. In essence, it says that every sufficiently high-dimensional normed vector space will have low-dimensional subspaces that are approximately Euclidean. Equivalently, every high-dimensional bounded symmetric convex set has low-dimensional sections that are approximately ellipsoids.\n\nA new proof found by Vitali Milman in the 1970s was one of the starting points for the development of asymptotic geometric analysis (also called \"asymptotic functional analysis\" or the \"local theory of Banach spaces\").\n\nFor every natural number \"k\" ∈ N and every \"ε\" > 0 there exists a natural number \"N\"(\"k\", \"ε\") ∈ N such that if (\"X\", ‖·‖) is any normed space of dimension \"N\"(\"k\", \"ε\"), there exists a subspace \"E\" ⊂ \"X\" of dimension \"k\" and a positive quadratic form \"Q\" on \"E\" such that the corresponding Euclidean norm\n\non \"E\" satisfies:\n\nIn terms of the multiplicative Banach-Mazur distance \"d\" the theorem's conclusion can be formulated as:\n\nwhere formula_4 denotes the standard \"k\"-dimensional Euclidean space.\n\nSince the unit ball of every normed vector space is a bounded, symmetric, convex set and the unit ball of every Euclidean space is an ellipsoid, the theorem may also be formulated as a statement about ellipsoid sections of convex sets.\n\nIn 1971, Vitali Milman gave a new proof of Dvoretzky's theorem, making use of the concentration of measure on the sphere to show that a random \"k\"-dimensional subspace satisfies the above inequality with probability very close to 1. The proof gives the sharp dependence on \"k\":\n\nwhere the constant \"C\"(\"ε\") only depends on \"ε\".\n\nWe can thus state: for every \"ε\" > 0 and every normed space (\"X\", ‖·‖) of dimension \"N\", there exists a subspace \"E\" ⊂ \"X\" of dimension\n\"k\" ≥ \"C\"(\"ε\") log \"N\" and a Euclidean norm |·| on \"E\" such that \n\nMore precisely, let \"S\" denote the unit sphere with respect to some Euclidean structure \"Q\" on \"X\", and let \"σ\" be the invariant probability measure on \"S\". Then:\n\n\nHere \"c\" is a universal constant. For given \"X\" and \"ε\", the largest possible \"k\" is denoted \"k\"(\"X\") and called the Dvoretzky dimension of \"X\".\n\nThe dependence on \"ε\" was studied by Yehoram Gordon, who showed that \"k\"(\"X\") ≥ \"c\" \"ε\" log \"N\". Another proof of this result was given by Gideon Schechtman.\n\nNoga Alon and Vitali Milman showed that the logarithmic bound on the dimension of the subspace in Dvoretzky's theorem can be significantly improved, if one is willing to accept a subspace that is close either to a Euclidean space or to a Chebyshev space. Specifically, for some constant \"c\", every \"n\"-dimensional space has a subspace of dimension \"k\" ≥ exp(\"c\") that is close either to \"ℓ\" or to \"ℓ\".\n\nImportant related results were proved by Tadeusz Figiel, Joram Lindenstrauss and Milman.\n"}
{"id": "542738", "url": "https://en.wikipedia.org/wiki?curid=542738", "title": "Florian Cajori", "text": "Florian Cajori\n\nFlorian Cajori (February 28, 1859 – August 14 or 15, 1930) was a Swiss-American historian of mathematics.\n\nFlorian Cajori was born in Switzerland, and was the son of Georg Cajori and Catherine Camenisch. He attended schools first in Zillis and later in Chur. In 1875, Florian Cajori emigrated to the United States at the age of sixteen, and attended the State Normal school in Whitewater, Wisconsin. After graduating in 1878, he taught in a country school, and then later began studying Mathematics at University of Wisconsin–Madison.\n\nIn 1883, Cajori received both his bachelor's and master's degrees from the University of Wisconsin–Madison, briefly attended Johns Hopkins University for 8 months in between degrees. He taught for a few years at Tulane University, before being appointed as professor of applied mathematics there in 1887. He was then driven north by tuberculosis. He founded the Colorado College Scientific Society and taught at Colorado College where he held the chair in physics from 1889-1898 and the chair in mathematics from 1898-1918. He was the position Dean of the engineering department. While at Colorado, he received his doctorate from Tulane in 1894, and married Elizabeth G. Edwards in 1890 and had one son.\n\nCajori's \"A History of Mathematics\" (1894) was the first popular presentation of the history of mathematics in the United States. Based upon his reputation in the history of mathematics (even today his 1928–1929 \"History of Mathematical Notations\" has been described as \"unsurpassed\") he was appointed in 1918 to the first history of mathematics chair in the U.S, created especially for him, at the University of California, Berkeley. He remained in Berkeley, California until his death in 1930. Cajori did no original mathematical research unrelated to the history of mathematics. In addition to his numerous books, he also contributed highly recognized and popular historical articles to the \"American Mathematical Monthly\". His last work was a revision of Andrew Motte's 1729 translation of Newton's \"Principia\", vol.1 The Motion of Bodies, but he died before it was completed. The work was finished by R.T. Crawford of Berkeley, California.\n\n\n\nThese seven installments of the article are available through the Early Content program of Jstor.\n\n"}
{"id": "243334", "url": "https://en.wikipedia.org/wiki?curid=243334", "title": "Green's theorem", "text": "Green's theorem\n\nIn mathematics, Green's theorem gives the relationship between a line integral around a simple closed curve \"C\" and a double integral over the plane region \"D\" bounded by \"C\". It is named after George Green, though its first proof is due to Bernhard Riemann and is the two-dimensional special case of the more general Kelvin–Stokes theorem.\n\nLet \"C\" be a positively oriented, piecewise smooth, simple closed curve in a plane, and let \"D\" be the region bounded by \"C\". If \"L\" and \"M\" are functions of (\"x\", \"y\") defined on an open region containing \"D\" and have continuous partial derivatives there, then\n\nwhere the path of integration along \"C\" is anticlockwise.\n\nIn physics, Green's theorem finds many applications. One of which is solving two-dimensional flow integrals, stating that the sum of fluid outflows from a volume is equal to the total outflow summed about an enclosing area. In plane geometry, and in particular, area surveying, Green's theorem can be used to determine the area and centroid of plane figures solely by integrating over the perimeter.\n\nThe following is a proof of half of the theorem for the simplified area \"D\", a type I region where \"C\" and \"C\" are curves connected by vertical lines (possibly of zero length). A similar proof exists for the other half of the theorem when \"D\" is a type II region where \"C\" and \"C\" are curves connected by horizontal lines (again, possibly of zero length). Putting these two parts together, the theorem is thus proven for regions of type III (defined as regions which are both type I and type II). The general case can then be deduced from this special case by decomposing \"D\" into a set of type III regions.\n\nIf it can be shown that if\n\nand\n\nare true, then Green's theorem follows immediately for the region D. We can prove (1) easily for regions of type I, and (2) for regions of type II. Green's theorem then follows for regions of type III.\n\nAssume region \"D\" is a type I region and can thus be characterized, as pictured on the right, by\n\nwhere \"g\" and \"g\" are continuous functions on [\"a\", \"b\"]. Compute the double integral in (1):\n\nNow compute the line integral in (1). \"C\" can be rewritten as the union of four curves: \"C\", \"C\", \"C\", \"C\".\n\nWith \"C\", use the parametric equations: \"x\" = \"x\", \"y\" = \"g\"(\"x\"), \"a\" ≤ \"x\" ≤ \"b\". Then\n\nWith \"C\", use the parametric equations: \"x\" = \"x\", \"y\" = \"g\"(\"x\"), \"a\" ≤ \"x\" ≤ \"b\". Then\n\nThe integral over \"C\" is negated because it goes in the negative direction from \"b\" to \"a\", as \"C\" is oriented positively (anticlockwise). On \"C\" and \"C\", \"x\" remains constant, meaning\n\nTherefore,\n\nCombining (3) with (4), we get (1) for regions of type I. A similar treatment yields (2) for regions of type II. Putting the two together, we get the result for regions of type III.\n\nWe are going to prove the following\n\nTheorem. Let formula_9 be a rectifiable, positively oriented Jordan curve in formula_10 and let formula_11 denote its inner region. Suppose that formula_12 are continuous functions with the property that formula_13 has second partial derivative at every point of formula_11, formula_15 has first partial derivative at every point of formula_11 and that the functions formula_17, formula_18 are Riemann-integrable over formula_11. Then\n\nformula_20\n\nWe need the following lemmas:\n\nLemma 1 (Decomposition Lemma). Assume formula_9 is a rectifiable, positively oriented Jordan curve in the plane and let formula_11 be its inner region. For every positive real formula_23, let formula_24 denote the collection of squares in the plane bounded by the lines formula_25, where formula_26 runs through the set of integers. Then, for this formula_23, there exists a decomposition of formula_28 into a finite number of non-overlapping subregions in such a manner that\n\n(i) Each one of the subregions contained in formula_11, say formula_30, is a square from formula_24.\n\n(ii) Each one of the remaining subregions, say formula_32, has as boundary a rectifiable Jordan curve formed by a finite number of arcs of formula_9 and parts of the sides of some square from formula_24.\n\n(iii) Each one of the border regions formula_32 can be enclosed in a square of edge-length formula_36.\n\n(iv) If formula_37 is the positively oriented boundary curve of formula_38, then formula_39\n\n(v) The number formula_40 of border regions is no greater than formula_41, where formula_42 is the length of formula_9.\n\nLemma 2. Let formula_9 be a rectifiable curve in the plane and let formula_45 be the set of points in the plane whose distance from (the range of) formula_9 is at most formula_47. The outer Jordan content of this set satisfies formula_48.\n\nLemma 3. Let formula_9 be a rectifiable curve in formula_10 and let formula_51 be a continuous function. Then\n\nNow we are in position to prove the Theorem:\n\nProof of Theorem. Let formula_58 be an arbitrary positive real number. By continuity of formula_13, formula_15 and compactness of formula_28, given formula_62, there exists formula_63 such that whenever two points of formula_28 are less than formula_65 apart, their images under formula_66 are less than formula_58 apart. For this formula_23, consider the decomposition given by the previous Lemma. We have\n\nPut formula_70.\n\nFor each formula_71, the curve formula_37 is a positively oriented square, for which Green's formula holds. Hence\n\nEvery point of a border region is at a distance no greater than formula_65 from formula_9. Thus, if formula_76 is the union of all border regions, then formula_77; hence formula_78, by Lemma 2. Notice that\n\nWe may as well choose formula_23 so that the RHS of the last inequality is formula_82\n\nThe remark in the beginning of this proof implies that the oscillations of formula_13 and formula_15 on every border region is at most formula_58. We have\n\nBy Lemma 1(iii),\n\nCombining these, we finally get\n\nfor some formula_89. Since this is true for every formula_62, we are done.\n\nThe hypothesis of the last theorem are not the only ones under which Green's formula is true. Another common set of conditions is the following:\n\nThe functions formula_12 are still assumed to be continuous. However, we now require them to be Fréchet-differentiable at every point of formula_11. This implies the existence of all directional derivatives, in particular formula_93, where, as usual, formula_94 is the canonical ordered basis of formula_95. In addition, we require the function formula_96 to be Riemann-integrable over formula_11.\n\nIt suffices to prove this for squares which are contained in formula_11 and have sides parallel to the axes. The proof then follows the lines of the method employed to prove the Cauchy-Goursat Theorem for triangles.\n\nAs a corollary of this, we get the Cauchy Integral Theorem for rectifiable Jordan curves:\n\nTheorem (Cauchy). If formula_9 is a rectifiable Jordan curve in formula_100 and if formula_101 is a continuous mapping holomorphic throughout the inner region of formula_9, then\n\nthe integral being a complex contour integral.\n\nProof. We regard the complex plane as formula_10. Now, define formula_105 to be such that formula_106 These functions are clearly continuous. It is well-known that formula_107 and formula_108 are Fréchet-differentiable and that they satisfy the Cauchy-Riemann equations: formula_109.\n\nNow, analysing the sums used to define the complex contour integral in question, it is easy to realize that\n\nthe integrals on the RHS being usual line integrals. These remarks allow us to apply Green's Theorem to each one of these line integrals, finishing the proof.\n\nGreen's formula also holds when, besides continuity assumptions,\n\n(i) The functions formula_111, are defined at every point of formula_11, with the exception of a countable subset.\n\n(ii) The function formula_96 is Lebesgue-integrable over formula_11.\n\nTheorem. Let formula_115 be positively oriented rectifiable Jordan curves in formula_10 satisfying\n\nwhere formula_38 is the inner region of formula_37. Let\n\nSuppose formula_121 and formula_122 are continuous functions whose restriction to formula_123 is Fréchet-differentiable. If the function\n\nis Riemann-integrable over formula_123, then\n\nGreen's theorem is a special case of the Kelvin–Stokes theorem, when applied to a region in the \"xy\"-plane:\n\nWe can augment the two-dimensional field into a three-dimensional field with a \"z\" component that is always 0. Write F for the vector-valued function formula_127. Start with the left side of Green's theorem:\n\nThe Kelvin–Stokes theorem:\n\nThe surface formula_130 is just the region in the plane formula_123, with the unit normals formula_132 pointing up (in the positive \"z\" direction) to match the \"positive orientation\" definitions for both theorems.\n\nThe expression inside the integral becomes \n\nThus we get the right side of Green's theorem\n\nGreen's theorem is also a straightforward result of the general Stokes' theorem using differential forms and exterior derivatives:\n\nConsidering only two-dimensional vector fields, Green's theorem is equivalent to the two-dimensional version of the divergence theorem:\n\nwhere formula_136 is the divergence on the two-dimensional vector field formula_137, and formula_132 is the outward-pointing unit normal vector on the boundary.\n\nTo see this, consider the unit normal formula_132 in the right side of the equation. Since in Green's theorem formula_140 is a vector pointing tangential along the curve, and the curve \"C\" is the positively oriented (i.e. anticlockwise) curve along the boundary, an outward normal would be a vector which points 90° to the right of this; one choice would be formula_141. The length of this vector is formula_142 So formula_143\n\nStart with the left side of Green's theorem:\nApplying the two-dimensional divergence theorem with formula_145, we get the right side of Green's theorem:\n\nGreen's theorem can be used to compute area by line integral. The area of \"D\" is given by\n\nThen if we choose \"L\" and \"M\" such that formula_148, the area is given by\n\nPossible formulas for the area of \"D\" include\n\n\n"}
{"id": "24670148", "url": "https://en.wikipedia.org/wiki?curid=24670148", "title": "Herschel graph", "text": "Herschel graph\n\nIn graph theory, a branch of mathematics, the Herschel graph is a bipartite undirected graph with 11 vertices and 18 edges, the smallest non-Hamiltonian polyhedral graph. It is named after British astronomer Alexander Stewart Herschel, who wrote an early paper concerning William Rowan Hamilton's icosian game: the Herschel graph describes the smallest convex polyhedron for which this game has no solution. However, Herschel's paper described solutions for the Icosian game only on the graphs of the regular tetrahedron and regular icosahedron; it did not describe the Herschel graph.\n\nThe Herschel graph is a planar graph: it can be drawn in the plane with none of its edges crossing. It is also 3-vertex-connected: the removal of any two of its vertices leaves a connected subgraph. Therefore, by Steinitz's theorem, the Herschel graph is a polyhedral graph: there exists a convex polyhedron (an enneahedron) having the Herschel graph as its skeleton.\nThe Herschel graph is also a bipartite graph: its vertices can be separated into two subsets of five and six vertices respectively, such that every edge has an endpoint in each subset (the red and blue subsets in the picture).\n\nAs with any bipartite graph, the Herschel graph is a perfect graph : the chromatic number of every induced subgraph equals the size of the largest clique of that subgraph. It has also chromatic index 4, girth 4, radius 3 and diameter 4.\n\nBecause it is a bipartite graph that has an odd number of vertices, the Herschel graph does not contain a Hamiltonian cycle (a cycle of edges that passes through each vertex exactly once). For, in any bipartite graph, any cycle must alternate between the vertices on either side of the bipartition, and therefore must contain equal numbers of both types of vertex and must have an even length. Thus, a cycle passing once through each of the eleven vertices cannot exist in the Herschel graph. It is the smallest non-Hamiltonian polyhedral graph, whether the size of the graph is measured in terms of its number of vertices, edges, or faces; there exist other polyhedral graphs with 11 vertices and no Hamiltonian cycles (notably the Goldner–Harary graph) but none with fewer edges.\n\nAll but three of the vertices of the Herschel graph have degree three; Tait's conjecture states that a polyhedral graph in which every vertex has degree three must be Hamiltonian, but this was disproved when W. T. Tutte provided a counterexample, the much larger Tutte graph. A refinement of Tait's conjecture, Barnette's conjecture that every bipartite 3-regular polyhedral graph is Hamiltonian, remains open.\n\nThe Herschel graph also provides an example of a polyhedral graph for which the medial graph cannot be decomposed into two edge-disjoint Hamiltonian cycles. The medial graph of the Herschel graph is a 4-regular graph with 18 vertices, one for each edge of the Herschel graph; two vertices are adjacent in the medial graph whenever the corresponding edges of the Herschel graph are consecutive on one of its faces.\n"}
{"id": "14221581", "url": "https://en.wikipedia.org/wiki?curid=14221581", "title": "Hosoya index", "text": "Hosoya index\n\nThe Hosoya index, also known as the Z index, of a graph is the total number of matchings in it. The Hosoya index is always at least one, because the empty set of edges is counted as a matching for this purpose. Equivalently, the Hosoya index is the number of non-empty matchings plus one.\n\nThis graph invariant was introduced by Haruo Hosoya in 1971. It is often used in chemoinformatics for investigations of organic compounds.\n\nIn his article \"The Topological Index Z Before and After 1971\" on the history of the notion and the associated inside stories, Hosoya writes that he introduced the Z index to report a good correlation of the boiling points of alkane isomers and their Z indices, basing on his unpublished 1957 work carried out while he was an undergraduate student at the University of Tokyo.\n\nA linear alkane, for the purposes of the Hosoya index, may be represented as a path graph without any branching. A path with one vertex and no edges (corresponding to the methane molecule) has one (empty) matching, so its Hosoya index is one; a path with one edge (ethane) has two matchings (one with zero edges and one with one edges), so its Hosoya index is two. Propane (a length-two path) has three matchings: either of its edges, or the empty matching. \"n\"-butane (a length-three path) has five matchings, distinguishing it from isobutane which has four. More generally, a matching in a path with \"k\" edges either forms a matching in the first \"k\" − 1 edges, or it forms a matching in the first \"k\" − 2 edges together with the final edge of the path. Thus, the Hosoya indices of linear alkanes obey the recurrence governing the Fibonacci numbers. The structure of the matchings in these graphs may be visualized using a Fibonacci cube.\n\nThe largest possible value of the Hosoya index, on a graph with \"n\" vertices, is given by the complete graph, and the Hosoya indices for the complete graphs are the telephone numbers\n\nThe Hosoya index is #P-complete to compute, even for planar graphs. However, it may be calculated by evaluating the matching polynomial \"m\" at the argument 1. Based on this evaluation, the calculation of the Hosoya index is fixed-parameter tractable for graphs of bounded treewidth and polynomial (with an exponent that depends linearly on the width) for graphs of bounded clique-width.\n\n"}
{"id": "5624984", "url": "https://en.wikipedia.org/wiki?curid=5624984", "title": "Ibn al-Banna' al-Marrakushi", "text": "Ibn al-Banna' al-Marrakushi\n\nIbn al‐Bannāʾ al‐Marrākushī al-Azdi, also known as Abu'l-Abbas Ahmad ibn Muhammad ibn Uthman al-Azdi () \n(29 December 1256 – c. 1321), was a Moroccan-Arab mathematician, astronomer, Islamic scholar, Sufi, and a one-time astrologer.\n\nIbn al-Banna' (lit. the son of the architect) was born in Marrakesh in 1256; he is named al‐Marrākushī after that city. Having learned basic mathematical and geometrical skills, he translated Euclid's Elements into Arabic.\n\nIbn al-Banna' wrote between 51 and 74 treatises, encompassing such varied topics as Algebra, Astronomy, Linguistics, Rhetoric, and Logic. One of his works, called \"Talkhīṣ ʿamal al-ḥisāb (Arabic, تلخيص أعمال الحساب ) (Summary of arithmetical operations)\", includes topics such as fractions, sums of squares and cubes etc. Another, called \"Tanbīh al-Albāb\", covers topics related to:\n\n\nHe also wrote \"Rafʿ al-Ḥijāb (Lifting the Veil)\" which covered topics such as computing square roots of a number and the theory of continued fractions. This was the first mathematical work since Brahmagupta to use an algebraic notation, further developed by Abū al-Hasan ibn Alī al-Qalasādī two centuries later.\n\nThe crater Al-Marrakushi on the Moon is named after him.\n\n\n"}
{"id": "585797", "url": "https://en.wikipedia.org/wiki?curid=585797", "title": "Integer-valued polynomial", "text": "Integer-valued polynomial\n\nIn mathematics, an integer-valued polynomial (also known as a numerical polynomial) \"P\"(\"t\") is a polynomial whose value \"P\"(\"n\") is an integer for every integer \"n\". Every polynomial with integer coefficients is integer-valued, but the converse is not true. For example, the polynomial\n\ntakes on integer values whenever \"t\" is an integer. That is because one of \"t\" and \"t\" + 1 must be an even number. (The values this polynomial takes are the triangular numbers.)\n\nInteger-valued polynomials are objects of study in their own right in algebra, and frequently appear in algebraic topology.\n\nThe class of integer-valued polynomials was described fully by . Inside the polynomial ring Q[\"t\"] of polynomials with rational number coefficients, the subring of integer-valued polynomials is a free abelian group. It has as basis the polynomials\n\nfor \"k\" = 0,1,2, ..., i.e., the binomial coefficients. In other words, every integer-valued polynomial can be written as an integer linear combination of binomial coefficients in exactly one way. The proof is by the method of discrete Taylor series: binomial coefficients are integer-valued polynomials, and conversely, the discrete difference of an integer series is an integer series, so the discrete Taylor series of an integer series generated by a polynomial has integer coefficients (and is a finite series).\n\nInteger-valued polynomials may be used effectively to solve questions about fixed divisors of polynomials. For example, the polynomials \"P\" with integer coefficients that always take on even number values are just those such that \"P\"/2 is integer valued. Those in turn are the polynomials that may be expressed as a linear combination with even integer coefficients of the binomial coefficients.\n\nIn questions of prime number theory, such as Schinzel's hypothesis H and the Bateman–Horn conjecture, it is a matter of basic importance to understand the case when \"P\" has no fixed prime divisor (this has been called \"Bunyakovsky's property\", after Viktor Bunyakovsky). By writing \"P\" in terms of the binomial coefficients, we see the highest fixed prime divisor is also the highest prime common factor of the coefficients in such a representation. So Bunyakovsky's property is equivalent to coprime coefficients.\n\nAs an example, the pair of polynomials \"n\" and \"n\" + 2 violates this condition at \"p\" = 3: for every \"n\" the product\n\nis divisible by 3. Consequently, there cannot be infinitely many prime pairs \"n\" and \"n\" + 2. The divisibility is attributable to the alternate representation\n\nNumerical polynomials can be defined over other rings and fields, in which case the integer-valued polynomials above are referred to as classical numerical polynomials.\n\nThe K-theory of BU(\"n\") is numerical (symmetric) polynomials.\n\nThe Hilbert polynomial of a polynomial ring in \"k\" + 1 variables is the numerical polynomial formula_2.\n\n"}
{"id": "47701231", "url": "https://en.wikipedia.org/wiki?curid=47701231", "title": "Integer set library", "text": "Integer set library\n\nisl (integer set library) is a portable C library for manipulating sets and relations of integer points bounded by linear constraints.\n\nThe following operations are supported:\n\nIt also includes an ILP solver based on generalized basis reduction, transitive closures on maps (which may encode infinite graphs), dependence analysis and bounds on piecewise step-polynomials.\n\nAll computations are performed in exact integer arithmetic using GMP or imath.\n\nMany program analysis techniques are based on integer set manipulations. The integers typically represent iterations of a loop nest or elements of an array.\nisl uses parametric integer programming to obtain an explicit representation in terms of integer divisions.\n\nIt is used as backend polyhedral library in the GCC Graphite framework for loop optimizations.\n\n\n"}
{"id": "39273382", "url": "https://en.wikipedia.org/wiki?curid=39273382", "title": "International Symposium on Symbolic and Algebraic Computation", "text": "International Symposium on Symbolic and Algebraic Computation\n\nISSAC, the International Symposium on Symbolic and Algebraic Computation, is an academic conference in the field of computer algebra. ISSAC has been organized annually since 1988, typically in July. The conference is regularly sponsored by the Association for Computing Machinery special interest group SIGSAM, and the proceedings since 1989 have been published by ACM. ISSAC is considered as being one of the most influential conferences for the publication of scientific computing research.\n\nThe first ISSAC took place in Rome on 4–8 July 1988. It succeeded a series of meetings held between 1966 and 1987 under the names SYMSAM, SYMSAC, EUROCAL, EUROSAM and EUROCAM.\n\n\nTypical topics include:\n\n\n"}
{"id": "390273", "url": "https://en.wikipedia.org/wiki?curid=390273", "title": "Legendre transformation", "text": "Legendre transformation\n\nIn mathematics and physics, the Legendre transformation, named after Adrien-Marie Legendre, is an involutive transformation on the real-valued convex functions of one real variable. It is commonly used in classical mechanics to derive the Hamiltonian formalism out of the Lagrangian formalism and in thermodynamics to derive the thermodynamic potentials, as well as in the solution of differential equations of several variables.\n\nFor sufficiently smooth functions on the real line, the Legendre transform of a function can be specified, up to an additive constant, by the condition that the functions' first derivatives are inverse functions of each other. This can be expressed in Euler's derivative notation as\nor, equivalently, as formula_2 and formula_3 in Lagrange's notation.\n\nThe generalization of the Legendre transformation to affine spaces and non-convex functions is known as the convex conjugate (also called the Legendre–Fenchel transformation), which can be used to construct a function's convex hull.\n\nLet be an interval, and a convex function; then its \"Legendre transform\" is the function defined by\nwhere formula_5 is the supremum, and the domain formula_6 is\n\nThe transform is always well-defined when is convex.\n\nThe generalization to convex functions on a convex set is straightforward: has domain\nand is defined by\nwhere formula_10 denotes the dot product of and .\n\nThe function is called the convex conjugate function of . For historical reasons (rooted in analytic mechanics), the conjugate variable is often denoted , instead of . If the convex function is defined on the whole line and is everywhere differentiable, then\ncan be interpreted as the negative of the -intercept of the tangent line to the graph of that has slope .\n\nThe Legendre transformation is an application of the duality relationship between points and lines. The functional relationship specified by can be represented equally well as a set of points, or as a set of tangent lines specified by their slope and intercept values.\n\nFor differentiable convex functions formula_12\non the real line with an invertible first derivative, the Legendre transform formula_13\ncan be specified, up to an additive constant, by the condition that the functions' first \nderivatives are inverse functions of each other.\n\nTo see this, first note that if formula_12 is differentiable and formula_15 is a critical point\nof the function of formula_16, then the\nsupremum is achieved at formula_17 (by convexity). \nTherefore, formula_18.\n\nSuppose that formula_19 is invertible and let formula_20 denote its inverse.\nThen for each formula_21, the point formula_22 is the unique critical point of\nformula_23. Indeed, formula_24 and so formula_25.\nHence we have formula_26 for each formula_21. \nBy differentiating with respect to formula_21 we find \nSince formula_30 this simplifies to formula_31.\nIn other words, formula_32 and formula_19 are inverses.\n\nIn general, if formula_34 is an inverse of formula_35, then formula_36 \nand so integration provides a constant formula_37 so that formula_38.\n\nIn practical terms, given , the parametric plot of versus amounts to the graph of versus .\n\nIn some cases (e.g. thermodynamic potentials, below), a non-standard requirement is used, amounting to an alternative definition of with a \"minus sign\",\n\n\n\nThe exponential function\nas a Legendre transform, since their respective first derivatives and are inverse functions of each other.\n\nThis example illustrates how the respective domains of a function and its Legendre transform need not agree.\n\nLet defined on ℝ, where is a fixed constant.\n\nFor fixed, the function of , has the first derivative and second derivative ; there is one stationary point at , which is always a maximum.\n\nThus, and\n\nIf the capacitor is not connected to any circuit, then the \"charges\" on the plates remain constant as they move, and the force is the negative gradient of the electrostatic energy\n\nHowever, suppose, instead, that the \"voltage\" between the plates is maintained constant by connection to a battery, which is a reservoir for charge at constant potential difference; now the \"charge is variable\" instead of the voltage, its Legendre conjugate. To find the force, first compute the non-standard Legendre transform,\n\nThe force now becomes the negative gradient of this Legendre transform, still pointing in the same direction,\n\nThe two conjugate energies happen to stand opposite to each other, only because of the linearity of the capacitance—except now is no longer a constant. They reflect the two different pathways of storing energy into the capacitor, resulting in, for instance, the same \"pull\" between a capacitor's plates.\n\nIn large deviations theory, the \"rate function\" is defined as the Legendre transformation of the logarithm of the moment generating function of a random variable. An important application of the rate function is in the calculation of tail probabilities of sums of i.i.d. random variables.\n\nLegendre transformation arises naturally in microeconomics in the process of finding the \"supply\" of some product given a fixed price on the market knowing the cost function , i.e. the cost for the producer to make/mine/etc. units of the given product.\n\nA simple theory explains the shape of the supply curve based solely on the cost function. Let us suppose the market price for a one unit of our product is . For a company selling this good, the best strategy is to adjust the production so that its profit is maximize. We can maximize the profit\n\nby differentiating with respect to and solving\n\nThe functional forms of the profit as a function of quantity and as a function of price are therefore Legendre transformations of each other.\n\nFor a strictly convex function, the Legendre transformation can be interpreted as a mapping between the graph of the function and the family of tangents of the graph. (For a function of one variable, the tangents are well-defined at all but at most countably many points, since a convex function is differentiable at all but at most countably many points.)\n\nThe equation of a line with slope and -intercept is given by . For this line to be tangent to the graph of a function at the point requires\nand\n\nThe function formula_19is strictly monotone as the derivative of a strictly convex function. The second equation can be solved for formula_59, allowing elimination of from the first, and solving for the -intercept of the tangent as a function of its slope ,\n\nHere, formula_61 denotes the Legendre transform of .\n\nThe family of tangents of the graph of parameterized by is therefore given by\nor, written implicitly, by the solutions of the equation\n\nThe graph of the original function can be reconstructed from this family of lines as the envelope of this family by demanding\n\nEliminating from these two equations gives\n\nIdentifying with and recognizing the right side of the preceding equation as the Legendre transform of , yields\n\nFor a differentiable real-valued function on an open subset of the Legendre conjugate of the pair is defined to be the pair , where is the image of under the gradient mapping , and is the function on given by the formula\nwhere\n\nis the scalar product on . The multidimensional transform can be interpreted as an encoding of the convex hull of the function's epigraph in terms of its supporting hyperplanes.\n\nAlternatively, if is a vector space and is its dual vector space, then for each point of and of , there is a natural identification of the cotangent spaces with and with . If is a real differentiable function over , then its exterior derivative, , is a section of the cotangent bundle and as such, we can construct a map from to . Similarly, if is a real differentiable function over , then defines a map from to . If both maps happen to be inverses of each other, we say we have a Legendre transform.\n\nWhen the function is not differentiable, the Legendre transform can still be extended, and is known as the Legendre-Fenchel transformation. In this more general setting, a few properties are lost: for example, the Legendre transform is no longer its own inverse (unless there are extra assumptions, like convexity).\n\nThe Legendre transformation has the following scaling properties: For ,\n\nIt follows that if a function is homogeneous of degree then its image under the Legendre transformation is a homogeneous function of degree , where . (Since , with , implies .) Thus, the only monomial whose degree is invariant under Legendre transform is the quadratic.\n\nLet be a linear transformation. For any convex function on , one has\n\nwhere is the adjoint operator of defined by\n\nand is the \"push-forward\" of along \n\nA closed convex function is symmetric with respect to a given set of orthogonal linear transformations,\nif and only if is symmetric with respect to .\n\nThe infimal convolution of two functions and is defined as\n\nLet be proper convex functions on . Then\n\nFor any function and its convex conjugate \"Fenchel's inequality\" (also known as the \"Fenchel–Young inequality\") holds for every and , i.e., \"independent\" pairs,\n\n\n\n\n"}
{"id": "19050267", "url": "https://en.wikipedia.org/wiki?curid=19050267", "title": "Leopoldo Nachbin", "text": "Leopoldo Nachbin\n\nLeopoldo Nachbin (7 January 1922 – 3 April 1993) was a Jewish-Brazilian mathematician who dealt with topology, and harmonic analysis.\n\nNachbin was born in Recife, and is best known for Nachbin's theorem. He died, aged 71, in Rio de Janeiro.\n\nNachbin was a Ph.D. student of Laurent Schwartz.\n\nAmong his Ph.D. students is Francisco Antônio Dória.\n\nHe was an invited speaker at the International Congress of Mathematicians (ICM) of 1962 in Stockholm.\n\n\n\n"}
{"id": "2920840", "url": "https://en.wikipedia.org/wiki?curid=2920840", "title": "List of combinatorial computational geometry topics", "text": "List of combinatorial computational geometry topics\n\nList of combinatorial computational geometry topics enumerates the topics of computational geometry that states problems in terms of geometric objects as discrete entities and hence the methods of their solution are mostly theories and algorithms of combinatorial character.\n\nSee List of numerical computational geometry topics for another flavor of computational geometry that deals with geometric objects as continuous entities and applies methods and algorithms of nature characteristic to numerical analysis.\n\n\n\n\n\n\n"}
{"id": "39479320", "url": "https://en.wikipedia.org/wiki?curid=39479320", "title": "List of things named after Erich Hecke", "text": "List of things named after Erich Hecke\n\nThese are things named after Erich Hecke, a German mathematician.\n"}
{"id": "1503224", "url": "https://en.wikipedia.org/wiki?curid=1503224", "title": "Local boundedness", "text": "Local boundedness\n\nIn mathematics, a function is locally bounded if it is bounded around every point. A family of functions is locally bounded if for any point in their domain all the functions are bounded around that point and by the same number.\n\nA real-valued or complex-valued function \"f\" defined on some topological space \"X\" is called locally bounded \nif for any \"x\" in \"X\" there exists a neighborhood \"A\" of \"x\" such that\n\"f\" (\"A\") is a bounded set, that is, for some number \"M\">0 one has\nfor all \"x\" in \"A\".\n\nThat is to say, for each \"x\" one can find a constant, depending on \"x\", which is larger than all the values of the function in the neighborhood of \"x\". Compare this with a bounded function, for which the constant does not depend on \"x\". Obviously, if a function is bounded then it is locally bounded. The converse is not true in general.\n\nThis definition can be extended to the case when \"f\" takes values in some metric space. Then the inequality above needs to be replaced with\nfor all \"x\" in \"A\", where \"d\" is the distance function in the metric space, and \"a\" is some point in the metric space. The choice of \"a\" does not affect the definition. Choosing a different \"a\" will at most increase the constant \"M\" for which this inequality is true.\n\nis bounded, because 0≤ \"f\" (\"x\") ≤ 1 for all \"x\". Therefore, it is also locally bounded.\n\nis \"not\" bounded, as it becomes arbitrarily large. However, it \"is\" locally bounded because for each \"a\", |\"f\"(\"x\")| ≤ \"M\" in the neighborhood (\"a\" - 1,\"a\" + 1), where \"M\" = 2|\"a\"|+5.\n\nfor \"x\" ≠ 0 and taking the value 0 for \"x\"=0 is \"not\" locally bounded. In any neighborhood of 0 this function takes values of arbitrarily large magnitude.\n\nA set (also called a family) \"U\" of real-valued or complex-valued functions defined on some topological space \"X\" is called locally bounded if for any \"x\" in \"X\" there exists a neighborhood \"A\" of \"x\" and a positive number \"M\" such that\nfor all \"x\" in \"A\" and \"f\" in \"U\". In other words, all the functions in the family must be locally bounded, and around each point they need to be bounded by the same constant.\n\nThis definition can also be extended to the case when the functions in the family \"U\" take values in some metric space, by again replacing the absolute value with the distance function.\n\nwhere \"n\" = 1, 2, ... is locally bounded. Indeed, if \"x\" is a real number, one can choose the neighborhood \"A\" to be the interval (\"x\"-1, \"x\"+1). Then for all \"x\" in this interval and for all \"n\"≥1 one has\nwith \"M\"=|\"x\"|+1. Moreover, the family is uniformly bounded, because neither the neighborhood \"A\" nor the constant \"M\" depend on the index \"n\".\n\nis locally bounded, if n is greater than zero. For any \"x\" one can choose the neighborhood \"A\" to be R itself. Then we have \nwith \"M\"=1. Note that the value of \"M\" does not depend on the choice of x or its neighborhood \"A\". This family is then not only locally bounded, it is also uniformly bounded.\n\nis \"not\" locally bounded. Indeed, for any \"x\" the values \"f\"(\"x\") cannot be bounded as \"n\" tends toward infinity.\n\nLocal boundedness may also refer to a property of topological vector spaces, or of functions from a topological space into a topological vector space.\n\nLet \"X\" be a topological vector space. Then a subset \"B\" ⊂ \"X\" is bounded if for each neighborhood \"U\" of 0 in \"X\" there exists a number \"s\" > 0 such that\nA topological vector space is said to be locally bounded if \"X\" admits a bounded neighborhood of 0.\n\nLet \"X\" be a topological space, \"Y\" a topological vector space, and \"f\" : \"X\" → \"Y\" a function. Then \"f\" is locally bounded if each point of \"X\" has a neighborhood whose image under \"f\" is bounded.\n\nThe following theorem relates local boundedness of functions with the local boundedness of topological vector spaces:\n"}
{"id": "14227148", "url": "https://en.wikipedia.org/wiki?curid=14227148", "title": "Martin Charles Golumbic", "text": "Martin Charles Golumbic\n\nMartin Charles Golumbic (born September 30, 1948) is a mathematician and computer scientist, best known for his work in algorithmic graph theory and in artificial intelligence. He is the founding editor-in-chief of the journal \"Annals of Mathematics and Artificial Intelligence\".\n\nGolumbic was born in 1948 in Erie, Pennsylvania, U.S. He received his Ph.D. in 1975 at Columbia University, where his advisor was the eminent mathematician Samuel Eilenberg. He was a professor at the Courant Institute of Mathematical Sciences of New York University until 1980, and then a researcher at Bell Laboratories until moving permanently to Israel in 1982, where he previously held positions at IBM Research and Bar-Ilan University. He has held visiting positions at Université de Paris, the Weizmann Institute of Science, the École Polytechnique Fédérale de Lausanne, the Universidade Federal do Rio de Janeiro, Columbia University, Rutgers University, the Indian Institute of Technology Kharagpur, and Tsinghua University.\n\nGolumbic is the founder and director emeritus of the Caesarea Edmond Benjamin de Rothschild Institute for Interdisciplinary Applications of Computer Science at the University of Haifa. He was elected a fellow of the Institute of Combinatorics and its Applications (1995), fellow of the European Coordinating Committee for Artificial Intelligence ECCAI (2005) and member of the Academia Europaea, honoris causa (2013). \nGolumbic also served as chairman of the Israeli Association of Artificial Intelligence \n(1998–2004), and founded and chaired numerous international symposia in \ndiscrete mathematics and in the foundations of artificial intelligence.\n\nHe is the author of several books including \"Algorithmic Graph Theory and Perfect Graphs\", \"Tolerance Graphs\" (with Ann N. Trenk) and \"Fighting Terror Online: The Convergence of Security, Technology, and the Law\".\n\nGolumbic's work in graph theory lead to the study of new perfect graph families such as tolerance graphs, which generalize the classical graph notions of interval graph and comparability graph. He is credited with introducing the systematic study of algorithmic aspects in intersection graph theory, and initiated research on new structured families of graphs including the edge intersection graphs of paths in trees (EPT), tolerance graphs, chordal probe graphs and trivially perfect graphs. Golumbic, Kaplan and Shamir introduced the study\nof graph sandwich problems.\n\nIn the area of compiler optimization, Golumbic holds a joint patent with Vladimir Rainish, \"Instruction Scheduler for a Computer\", (UK9-90-035/IS), an invention based on their technique called SHACOOF (ScHeduling Across COntrOl Flow) which in Hebrew means \"transparent\".\nHe has contributed to the development of fundamental research in artificial intelligence in the area of complexity and spatial-temporal reasoning.\n\n\n\n"}
{"id": "23258067", "url": "https://en.wikipedia.org/wiki?curid=23258067", "title": "Matroid intersection", "text": "Matroid intersection\n\nIn combinatorial optimization, the matroid intersection problem is to find a largest common independent set in two matroids over the same ground set. If the elements of the matroid are assigned real weights, the weighted matroid intersection problem is to find a common independent set with the maximum possible weight. These problems generalize many problems in combinatorial optimization including finding maximum matchings and maximum weight matchings in bipartite graphs and finding arborescences in directed graphs.\n\nThe matroid intersection theorem, due to Jack Edmonds, says that there is always a simple upper bound certificate, consisting of a partitioning of the ground set amongst the two matroids, whose value (sum of respective ranks) equals the size of a maximum common independent set. Based on this theorem, the matroid intersection problem for two matroids can be solved in polynomial time using matroid partitioning algorithms.\n\nLet \"G\" = (\"U\",\"V\",\"E\") be a bipartite graph. One may define a partition matroid \"M\" on the ground set \"E\", in which a set of edges is independent if no two of the edges have the same endpoint in \"U\". Similarly one may define a matroid \"M\" in which a set of edges is independent if no two of the edges have the same endpoint in \"V\". Any set of edges that is independent in both \"M\" and \"M\" has the property that no two of its edges share an endpoint; that is, it is a matching. Thus, the largest common independent set of \"M\" and \"M\" is a maximum matching in \"G\".\n\nThe matroid intersection problem becomes NP-hard when three matroids are involved, instead of only two.\n\nOne proof of this hardness result uses a reduction from the Hamiltonian path problem in directed graphs. Given a directed graph \"G\" with \"n\" vertices, and specified nodes \"s\" and \"t\", the Hamiltonian path problem is the problem of determining whether there exists a simple path of length \"n\" − 1 that starts at \"s\" and ends at \"t\". It may be assumed without loss of generality that \"s\" has no incoming edges and \"t\" has no outgoing edges. Then, a Hamiltonian path exists if and only if there is a set of \"n\" − 1 elements in the intersection of three matroids on the edge set of the graph: two partition matroids ensuring that the in-degree and out-degree of the selected edge set are both at most one, and the graphic matroid of the undirected graph formed by forgetting the edge orientations in \"G\", ensuring that the selected edge set has no cycles .\n\nAnother computational problem on matroids, the matroid parity problem, was formulated by as a common generalization of matroid intersection and non-bipartite graph matching.\nHowever, although it can be solved in polynomial time for linear matroids, it is NP-hard for other matroids, and requires exponential time in the matroid oracle model .\n\n"}
{"id": "34945743", "url": "https://en.wikipedia.org/wiki?curid=34945743", "title": "Norbert A’Campo", "text": "Norbert A’Campo\n\nNorbert A’Campo (born 1941) is a Swiss mathematician working on singularity theory. He earned a doctorate in 1972 from the University of Paris-Sud. In 1974 he was an invited speaker at the International Congress of Mathematicians, and in 1988 he was elected president of the Swiss Mathematical Society. In 2012 he became a fellow of the American Mathematical Society.\n"}
{"id": "54741486", "url": "https://en.wikipedia.org/wiki?curid=54741486", "title": "North East Asian Mathematics Competition", "text": "North East Asian Mathematics Competition\n\nThe North East Asian Mathematics Competition (NEAMC) is a three-day mathematics competition held in a predesignated location in North East Asia.\n\nIt is a qualifying competition by Competition Academy for invitation to the World Mathematics Championships.\n\nThe location of the NEAMC changes annually. There are now at least two venues held annually and any institution may host it (with significant discounted perks).\n\nThe Senior level is open to all youths in Grade 12 (Year 13) or below during the month of the event.\n\nThe Junior level is open to all youths in Grade 9 (Year 10) or below during the month of the event.\n\nThe Prime Plus level is open to all youths in Grade 7 (Year 8) or below during the month of the event.\n\nSEAMC and NEAMC are three-day events for school students located in South East and North East Asia, respectively. Participants work alone and in teams, as well as listen to mathematician guest speakers.\n\nSEAMC was conceived at the turn of the millennium by Steve Warry, a teacher at Alice Smith School in Kuala Lumpur who believed that mathematics could be a spectator sport. In pursuit of this, he organised the South East Asian Mathematics Competition (SEAMC) for March 2001, although he died a week prior to the competition. NEAMC was organised in February 2014 by Malcolm Coad of Nanjing International School, China as a parallel event to SEAMC. Haese Mathematics are proud partners of the event since conception\n\nSEAMC, NEAMC and all other WMC qualifying competitions have:\n\nSchool teams engage within the Communication skills rounds.\n\nThe Collaboration skills rounds are in buddy teams of three (the Open round involves 2 buddy teams working together).\nThe Challenge are skills rounds undertaken as individuals.\n\nThree skills rounds are (subject specific skills and procedures) knowledge based,\nthree are (plan and execute) strategy focused and three depend upon (new and imaginative ideas) creativity.\n\nSo each strategy, creative and knowledge skill category is engaged in alone, in school teams and in buddy teams.\n\n\nPast team winners\n\n\nPast individual winners\n\n"}
{"id": "19653337", "url": "https://en.wikipedia.org/wiki?curid=19653337", "title": "Parametric family", "text": "Parametric family\n\nIn mathematics and its applications, a parametric family or a parameterized family is a family of objects (a set of related objects) whose differences depend only on the chosen values for a set of parameters.\n\nCommon examples are parametrized (families of) functions, probability distributions, curves, shapes, etc.\n\nFor example, the probability density function formula_1 of a random variable \"X\" may depend on a parameter formula_2. In that case, the function may be denoted formula_3 to indicate the dependence on the parameter formula_2. formula_2 is not a formal argument of the function as it is considered to be fixed. However, each different value of the parameter gives a different probability density function. Then the \"parametric family\" of densities is the set of functions formula_6, where formula_7 denotes the set of all possible values that the parameter formula_8 can take. As an example, the normal distribution is a family of similarly-shaped distributions parametrized by their mean and their variance.\n\nIn decision theory, two-moment decision models can be applied when the decision-maker is faced with random variables drawn from a location-scale family of probability distributions.\n\nIn economics, the Cobb–Douglas production function is a family of production functions parametrized by the elasticities of output with respect to the various factors of production.\n\nIn algebra, the quadratic equation, for example, is actually a family of equations parametrized by the coefficients of the variable and of its square and by the constant term.\n\n"}
{"id": "4606726", "url": "https://en.wikipedia.org/wiki?curid=4606726", "title": "Path coloring", "text": "Path coloring\n\nIn graph theory, path coloring usually refers to one of two problems:\nIn both the above problems, the goal is usually to minimise the number of colors used in the coloring. In different variants of path coloring, formula_2 may be a simple graph, digraph or multigraph.\n\n"}
{"id": "973479", "url": "https://en.wikipedia.org/wiki?curid=973479", "title": "Pseudo-differential operator", "text": "Pseudo-differential operator\n\nIn mathematical analysis a pseudo-differential operator is an extension of the concept of differential operator. Pseudo-differential operators are used extensively in the theory of partial differential equations and quantum field theory.\n\nThe study of pseudo-differential operators began in the mid 1960s with the work of Kohn, Nirenberg, Hörmander, Unterberger and Bokobza.\n\nThey played an influential role in the first proof of the Atiyah–Singer index theorem. Atiyah and Singer thanked Hörmander for assistance with understanding the theory of Pseudo-differential operators.\n\nConsider a linear differential operator with constant coefficients,\n\nwhich acts on smooth functions formula_2 with compact support in R.\nThis operator can be written as a composition of a Fourier transform, a simple \"multiplication\" by the\npolynomial function (called the symbol)\n\nand an inverse Fourier transform, in the form:\n\nHere, formula_4 is a multi-index, formula_5 are complex numbers, and\n\nis an iterated partial derivative, where ∂ means differentiation with respect to the \"j\"-th variable. We introduce the constants formula_7 to facilitate the calculation of Fourier transforms.\n\nThe Fourier transform of a smooth function \"u\", compactly supported in R, is\n\nand Fourier's inversion formula gives\n\nBy applying \"P\"(\"D\") to this representation of \"u\" and using\n\none obtains formula ().\n\nTo solve the partial differential equation\n\nwe (formally) apply the Fourier transform on both sides and obtain the \"algebraic\" equation\n\nIf the symbol \"P\"(ξ) is never zero when ξ ∈ R, then it is possible to divide by \"P\"(ξ):\n\nBy Fourier's inversion formula, a solution is\n\nHere it is assumed that:\nThe last assumption can be weakened by using the theory of distributions.\nThe first two assumptions can be weakened as follows.\n\nIn the last formula, write out the Fourier transform of ƒ to obtain\n\nThis is similar to formula (), except that 1/\"P\"(ξ) is not a polynomial function, but a function of a more general kind.\n\nHere we view pseudo-differential operators as a generalization of differential operators.\nWe extend formula (1) as follows. A pseudo-differential operator \"P\"(\"x\",\"D\") on R is an operator whose value on the function \"u(x)\" is the function of \"x\":\n\nwhere formula_16 is the Fourier transform of \"u\" and the symbol \"P\"(\"x\",ξ) in the integrand belongs to a certain \"symbol class\".\nFor instance, if \"P\"(\"x\",ξ) is an infinitely differentiable function on R × R with the property\n\nfor all \"x\",ξ ∈R, all multiindices α,β. some constants \"C\" and some real number \"m\", then \"P\" belongs to the symbol class formula_18 of Hörmander. The corresponding operator \"P\"(\"x\",\"D\") is called a pseudo-differential operator of order m and belongs to the class\nformula_19\n\nLinear differential operators of order m with smooth bounded coefficients are pseudo-differential\noperators of order \"m\".\nThe composition \"PQ\" of two pseudo-differential operators \"P\", \"Q\" is again a pseudo-differential operator and the symbol of \"PQ\" can be calculated by using the symbols of \"P\" and \"Q\". The adjoint and transpose of a pseudo-differential operator is a pseudo-differential operator.\n\nIf a differential operator of order \"m\" is (uniformly) elliptic (of order \"m\")\nand invertible, then its inverse is a pseudo-differential operator of order −\"m\", and its symbol can be calculated. This means that one can solve linear elliptic differential equations more or less explicitly\nby using the theory of pseudo-differential operators.\n\nDifferential operators are \"local\" in the sense that one only needs the value of a function in a neighbourhood of a point to determine the effect of the operator. Pseudo-differential operators are \"pseudo-local\", which means informally that when applied to a distribution they do not create a singularity at points where the distribution was already smooth.\n\nJust as a differential operator can be expressed in terms of \"D\" = −id/d\"x\" in the form\n\nfor a polynomial \"p\" in \"D\" (which is called the \"symbol\"), a pseudo-differential operator has a symbol in a more general class of functions. Often one can reduce a problem in analysis of pseudo-differential operators to a sequence of algebraic problems involving their symbols, and this is the essence of microlocal analysis.\n\nPseudo-differential operators can be represented by kernels. The singularity of the kernel on the diagonal depends on the degree of the corresponding operator. In fact, if the symbol satisfies the above differential inequalities with m ≤ 0, it can be shown that the kernel is a singular integral kernel. \n\n\n\n\n"}
{"id": "1065533", "url": "https://en.wikipedia.org/wiki?curid=1065533", "title": "Red/black concept", "text": "Red/black concept\n\nThe red/black concept, sometimes called the red–black architecture\nor red/black engineering,\nrefers to the careful segregation in cryptographic systems of signals that contain sensitive or classified plaintext information (red signals) from those that carry encrypted information, or ciphertext (black signals).\n\nIn NSA jargon, encryption devices are often called blackers, because they convert red signals to black. TEMPEST standards spelled out in Tempest/2-95 specify shielding or a minimum physical distance between wires or equipment carrying or processing red and black signals.\n\nDifferent organizations have differing requirements for the separation of red and black fiber optic cables.\n\nRed/black terminology is also applied to cryptographic keys. Black keys have themselves been encrypted with a \"key encryption key\" (KEK) and are therefore benign. Red keys are not encrypted and must be treated as highly sensitive material.\n\n"}
{"id": "989287", "url": "https://en.wikipedia.org/wiki?curid=989287", "title": "Residue number system", "text": "Residue number system\n\nA residue numeral system (RNS) is a numeral system representing integers by their values modulo several pairwise coprime integers called the moduli. This representation is allowed by the Chinese remainder theorem, which asserts that, if is the product of the moduli, there is, in an interval of length , exactly one integer having any given set of modular values. The arithmetic of a residue numeral system is also called multi-modular arithmetic.\n\nMulti-modular arithmetic is widely used for computation with large integers, typically in linear algebra, because it provides faster computation than with the usual numeral systems, even when the time for converting between numeral systems is taken into account. Other applications of multi-modular arithmetic include polynomial greatest common divisor, Gröbner basis computation and cryptography.\n\nA residue numeral system is defined by a set of integers\ncalled the \"moduli\", which are generally supposed to be pairwise coprime (that is, any two of them have a greatest common divisor equal to one). \nreferred to as the \"moduli\". Residue number systems have been defined for non-coprime moduli, but are not commonly used because of worse properties. Therefore, they will not be considered in the remainder of this article.\n\nAn integer is represented in the residue numeral system by the set of its remainders\nunder Euclidean division by the moduli. That is \nand\nfor every \n\nLet be the product of all the formula_5. Two integers whose difference is a multiple of have the same representation in the residue numeral system defined by the s. More precisely, the Chinese remainder theorem asserts that each of the different sets of possible residues represents exactly one residue class modulo . That is, each set of residues represents exactly one integer in the interval .\n\nIn applications where one is also interested with negative integers, it is often more convenient to represent integers belonging to an interval centered at 0. In this case, if is odd, each set of residues represents exactly one integer of absolute value at most .\n\nFor adding, subtracting and multiplying numbers represented in a residue number system, it suffices to perform the same modular operation on each pair of residues. More precisely, if \nis the list of moduli, the sum of the integers and , respectively represented by the residues formula_7 and formula_8 is the integer represented by formula_9 such that\nfor (as usual, mod denotes the modulo operation consisting of taking the remainder of the Euclidean division by the right operand). Subtraction and multiplication are defined similarly.\n\nFor a succession of operations, it is not necessary to apply the modulo operation at each step. It may be applied at the end of the computation, or, during the computation, for avoiding overflow of hardware operations.\n\nIf two integers are equal, then all their residues are equal. Conversely, if all residues are equal, then the two integers are equal, or their differences is a multiple of . It follows that testing equality is easy.\n\nAt the opposite, testing inequalities () is difficult and, usually, requires to convert integers to the standard representation. As a consequence, this representation of numbers is not suitable for algorithms using inequality tests, such Euclidean division and Euclidean algorithm.\n\nDivision in residue numeral systems is problematic. A paper describing one possible algorithm is available at . On the other hand, if formula_11 is coprime with formula_12 (that is formula_13) then\ncan be easily calculated by\nwhere formula_16 is multiplicative inverse of formula_11 modulo formula_12, and formula_19 is multiplicative inverse of formula_20 modulo formula_5.\n\nRNS have applications in the field of digital computer arithmetic. By decomposing in this a large integer into a set of smaller integers, a large calculation can be performed as a series of smaller calculations that can be performed independently and in parallel.\n\n\n"}
{"id": "5584743", "url": "https://en.wikipedia.org/wiki?curid=5584743", "title": "Riesz's lemma", "text": "Riesz's lemma\n\nRiesz's lemma (after Frigyes Riesz) is a lemma in functional analysis. It specifies (often easy to check) conditions that guarantee that a subspace in a normed linear space is dense. The lemma may also be called the Riesz lemma or Riesz inequality. It can be seen as a substitute for orthogonality when one is not in an inner product space.\n\nRiesz's Lemma. Let \"X\" be a normed linear space, \"Y\" be a closed proper subspace of \"X\" and α be a real number with Then there exists an \"x\" in \"X\" with |\"x\"| = 1 such that |\"x\" − \"y\"| ≥ α for all \"y\" in \"Y\".\n\n\"Remark 1.\" For the finite-dimensional case, equality can be achieved. In other words, there exists \"x\" of unit norm such that \"d\"(\"x\", \"Y\") = 1. When dimension of \"X\" is finite, the unit ball \"B\" ⊂ \"X\" is compact. Also, the distance function \"d\"(· , \"Y\") is continuous. Therefore its image on the unit ball \"B\" must be a compact subset of the real line, proving the claim.\n\n\"Remark 2.\" The space ℓ of all bounded sequences shows that the lemma does not hold for α = 1.\n\nThe proof can be found in functional analysis texts such as Kreyszig. An online proof from Prof. Paul Garrett is available.\nRiesz's lemma can be applied directly to show that the unit ball of an infinite-dimensional normed space \"X\" is never compact: Take an element \"x\" from the unit sphere. Pick \"x\" from the unit sphere such that\n\nClearly {\"x\"} contains no convergent subsequence and the noncompactness of the unit ball follows.\n\nMore generally, if a topological vector space \"X\" is locally compact, then it is finite dimensional. The converse of this is also true. Namely, if a topological vector space is finite dimensional, it is locally compact. Therefore local compactness characterizes finite-dimensionality. This classical result is also attributed to Riesz. A short proof can be sketched as follows: let \"C\" be a compact neighborhood of 0 ∈ \"X\". By compactness, there are \"c\", ..., \"c\" ∈ \"C\" such that\n\nWe claim that the finite dimensional subspace \"Y\" spanned by {\"c\"}, or equivalently, its closure, is \"X\". Since scalar multiplication is continuous, it's enough to show \"C\" ⊂ \"Y\". Now, by induction, \n\nfor every \"m\". But compact sets are bounded, so \"C\" lies in the closure of \"Y\". This proves the result. For a different proof based on Hahn Banach Theorem see.\n\nThe spectral properties of compact operators acting on a Banach space are similar to those of matrices. Riesz's lemma is essential in establishing this fact.\n\nRiesz's lemma guarantees that any infinite-dimensional normed space contains a sequence of unit vectors {\"x\"} with formula_5 for 0 < \"α\" < 1. This is useful in showing the non-existence of certain measures on infinite-dimensional Banach spaces. Riesz's lemma also shows that the identity operator on a Banach space \"X\" is compact if and only if \"X\" is finite-dimensional.\n\nOne can also use this lemma to demonstrate whether or not the normed vector space X is finite dimensional or otherwise: if the closed unit ball is compact, then X is finite dimensional (we proceed by contradiction to this proof this results).\n"}
{"id": "27315742", "url": "https://en.wikipedia.org/wiki?curid=27315742", "title": "SIAM Journal on Matrix Analysis and Applications", "text": "SIAM Journal on Matrix Analysis and Applications\n\nThe SIAM Journal on Matrix Analysis and Applications (until 1989: \"SIAM Journal on Algebraic and Discrete Methods\") is a peer-reviewed scientific journal covering matrix analysis and its applications. The relevant applications include signal processing, systems and control theory, statistics, Markov chains, and mathematical biology. \n\nThe journal is published by the Society for Industrial and Applied Mathematics. The founding editor-in-chief was Gene H. Golub, who established the journal in 1980. The current editor is Dianne P. O'Leary (University of Maryland).\n"}
{"id": "59022133", "url": "https://en.wikipedia.org/wiki?curid=59022133", "title": "Scandinavian Journal of Statistics", "text": "Scandinavian Journal of Statistics\n\nThe Scandinavian Journal of Statistics is a quarterly peer-reviewed scientific journal of statistics. It was established in 1974 by four Scandinavian statistical learned societies. It is published by John Wiley & Sons and the editors-in-chief are Peter Dalgaard (Copenhagen Business School) and Niels Richard Hansen (University of Copenhagen). According to the \"Journal Citation Reports\", the journal has a 2017 impact factor of 0.898, ranking it 70th out of 123 journals in the category \"Statistics & Probability\".\n"}
{"id": "4665038", "url": "https://en.wikipedia.org/wiki?curid=4665038", "title": "Sobolev inequality", "text": "Sobolev inequality\n\nIn mathematics, there is in mathematical analysis a class of Sobolev inequalities, relating norms including those of Sobolev spaces. These are used to prove the Sobolev embedding theorem, giving inclusions between certain Sobolev spaces, and the Rellich–Kondrachov theorem showing that under slightly stronger conditions some Sobolev spaces are compactly embedded in others. They are named after Sergei Lvovich Sobolev.\n\nLet denote the Sobolev space consisting of all real-valued functions on whose first weak derivatives are functions in . Here is a non-negative integer and . The first part of the Sobolev embedding theorem states that if and are two real numbers such that and:\n\nthen\n\nand the embedding is continuous. In the special case of and , Sobolev embedding gives\n\nwhere is the Sobolev conjugate of , given by\n\nThis special case of the Sobolev embedding is a direct consequence of the Gagliardo–Nirenberg–Sobolev inequality.\n\nThe second part of the Sobolev embedding theorem applies to embeddings in Hölder spaces . If and\n\nwith then one has the embedding\n\nThis part of the Sobolev embedding is a direct consequence of Morrey's inequality. Intuitively, this inclusion expresses the fact that the existence of sufficiently many weak derivatives implies some continuity of the classical derivatives.\n\nThe Sobolev embedding theorem holds for Sobolev spaces on other suitable domains . In particular (; ), both parts of the Sobolev embedding hold when\n\nOn a compact manifold with boundary, the Kondrachov embedding theorem states that if andformula_7then the Sobolev embedding\n\nis completely continuous (compact). Note that the condition is just as in the first part of the Sobolev embedding theorem, with the equality replaced by an inequality, thus requiring a more regular space .\n\nAssume that is a continuously differentiable real-valued function on with compact support. Then for there is a constant depending only on and such that\n\nwith 1/p* = 1/p - 1/n.\nThe case formula_10 is due to Sobolev, formula_11 to Gagliardo and Nirenberg independently. The Gagliardo–Nirenberg–Sobolev inequality implies directly the Sobolev embedding\n\nThe embeddings in other orders on are then obtained by suitable iteration.\n\nSobolev's original proof of the Sobolev embedding theorem relied on the following, sometimes known as the Hardy–Littlewood–Sobolev fractional integration theorem. An equivalent statement is known as the Sobolev lemma in . A proof is in .\n\nLet and . Let be the Riesz potential on . Then, for defined by\n\nthere exists a constant depending only on such that\n\nIf , then one has two possible replacement estimates. The first is the more classical weak-type estimate:\n\nwhere . Alternatively one has the estimateformula_16where formula_17 is the vector-valued Riesz transform, c.f. . The boundedness of the Riesz transforms implies that the latter inequality gives a unified way to write the family of inequalities for the Riesz potential.\n\nThe Hardy–Littlewood–Sobolev lemma implies the Sobolev embedding essentially by the relationship between the Riesz transforms and the Riesz potentials.\n\nAssume . Then there exists a constant , depending only on and , such that\n\nfor all , where\n\nThus if , then is in fact Hölder continuous of exponent , after possibly being redefined on a set of measure 0.\n\nA similar result holds in a bounded domain with boundary. In this case,\n\nwhere the constant depends now on and . This version of the inequality follows from the previous one by applying the norm-preserving extension of to .\n\nLet be a bounded open subset of , with a boundary. ( may also be unbounded, but in this case its boundary, if it exists, must be sufficiently well-behaved.) Assume , then we consider two cases:\nIn this case , where\n\nWe have in addition the estimate\n\nthe constant depending only on , and .\nHere, belongs to a Hölder space, more precisely:\n\nwhere\n\nWe have in addition the estimate\n\nthe constant depending only on , and .\n\nIf formula_27, then is a function of bounded mean oscillation and\n\nfor some constant depending only on . This estimate is a corollary of the Poincaré inequality.\n\nThe Nash inequality, introduced by , states that there exists a constant , such that for all ,\n\nThe inequality follows from basic properties of the Fourier transform. Indeed, integrating over the complement of the ball of radius ,\n\nby Parseval's theorem. On the other hand, one has\n\nwhich, when integrated over the ball of radius gives\n\nwhere is the volume of the -ball. Choosing to minimize the sum of () and () and again applying Parseval's theorem:\n\ngives the inequality.\n\nIn the special case of , the Nash inequality can be extended to the case, in which case it is a generalization of the Gagliardo-Nirenberg-Sobolev inequality (, Comments on Chapter 8). In fact, if is a bounded interval, then for all and all the following inequality holds\n\nwhere:\n\n"}
{"id": "498304", "url": "https://en.wikipedia.org/wiki?curid=498304", "title": "Steiner tree problem", "text": "Steiner tree problem\n\nSteiner tree problem, or minimum Steiner tree problem, named after Jakob Steiner, is an umbrella term for a class of problems in combinatorial optimization. While Steiner tree problems may be formulated in a number of settings, they all require an optimal interconnect for a given set of objects and a predefined objective function. One well-known variant, which is often used synonymously with the term Steiner tree problem, is the Steiner tree problem in graphs. Given an undirected graph with non-negative edge weights and a subset of vertices, usually referred to as terminals, the Steiner tree problem in graphs requires a tree of minimum weight that contains all terminals (but may include additional vertices). Further well-known variants are the Euclidean Steiner tree problem and the rectilinear minimum Steiner tree problem.\n\nThe Steiner tree problem in graphs can be seen as a generalization of two other famous combinatorial optimization problems: the (non-negative) shortest path problem and the minimum spanning tree problem. If a Steiner tree problem in graphs contains exactly two terminals, it reduces to finding a shortest path. If, on the other hand, all vertices are terminals, the Steiner tree problem in graphs is equivalent to the minimum spanning tree. However, while both the non-negative shortest path and the minimum spanning tree problem are solvable in polynomial time, the decision variant of the Steiner tree problem in graphs is NP-complete (which implies that the optimization variant is NP-hard); in fact, the decision variant was among Karp's original 21 NP-complete problems. The Steiner tree problem in graphs has applications in circuit layout or network design. However, practical applications usually require variations, giving rise to a multitude of Steiner tree problem variants.\n\nMost versions of the Steiner tree problem are NP-hard, but some restricted cases can be solved in polynomial time. Despite the pessimistic worst-case complexity, several Steiner tree problem variants, including the Steiner tree problem in graphs and the rectilinear Steiner tree problem, can be solved efficiently in practice, even for large-scale real-world problems.\n\nThe original problem was stated in the form that has become known as the Euclidean Steiner tree problem or geometric Steiner tree problem: Given \"N\" points in the plane, the goal is to connect them by lines of minimum total length in such a way that any two points may be interconnected by line segments either directly or via other points and line segments. It may be shown that the connecting line segments do not intersect each other except at the endpoints and form a tree, hence the name of the problem.\n\nThe problem for \"N\" = 3 has long been considered, and quickly extended to the problem of finding a star network with a single hub connecting to all of the \"N\" given points, of minimum total length.\nHowever, although the full Steiner tree problem was formulated in a letter by Gauss, its first serious treatment was in a 1934 paper written in Czech by Vojtěch Jarník and . This paper was long overlooked, but it already contains \"virtually all general properties of Steiner trees\" later attributed to other researchers, including the generalization of the problem from the plane to higher dimensions.\n\nFor the Euclidean Steiner problem, points added to the graph (Steiner points) must have a degree of three, and the three edges incident to such a point must form three 120 degree angles (see Fermat point). It follows that the maximum number of Steiner points that a Steiner tree can have is \"N\" − 2, where \"N\" is the initial number of given points.\n\nFor \"N\" = 3 there are two possible cases: if the triangle formed by the given points has all angles which are less than 120 degrees, the solution is given by a Steiner point located at the Fermat point; otherwise the solution is given by the two sides of the triangle which meet on the angle with 120 or more degrees.\n\nFor general \"N\", the Euclidean Steiner tree problem is NP-hard, and hence it is not known whether an optimal solution can be found by using a polynomial-time algorithm. However, there is a polynomial-time approximation scheme (PTAS) for Euclidean Steiner trees, i.e., a \"near-optimal\" solution can be found in polynomial time. It is not known whether the Euclidean Steiner tree problem is NP-complete, since membership to the complexity class NP is not known.\n\nThe rectilinear Steiner tree problem is a variant of the geometric Steiner tree problem in the plane, in which the Euclidean distance is replaced with the rectilinear distance. The problem arises in the physical design of electronic design automation. In VLSI circuits, wire routing is carried out by wires that are often constrained by design rules to run only in vertical and horizontal directions, so the rectilinear Steiner tree problem can be used to model the routing of nets with more than two terminals.\n\nSteiner trees have been extensively studied in the context of weighted graphs. The prototype is, arguably, the Steiner tree problem in graphs. Let \"G\" = (\"V\", \"E\") be an undirected graph with non-negative edge weights c and let \"S\" ⊆ \"V\" be a subset of vertices, called terminals. A Steiner tree is a tree in \"G\" that spans \"S\". There are two versions of the problem: in the optimization problem associated with Steiner trees, the task is to find a minimum-weight Steiner tree; in the decision problem the edge weights are integer and the task is to determine whether a Steiner tree exists whose total weight does not exceed a predefined natural number \"k\". The decision problem is one of Karp's 21 NP-complete problems; hence the optimization problem is NP-hard.\n\nA special case of this problem is when \"G\" is a complete graph, each vertex \"v\" ∈ \"V\" corresponds to a point in a metric space, and the edge weights \"w\"(\"e\") for each \"e\" ∈ \"E\" correspond to distances in the space. Put otherwise, the edge weights satisfy the triangle inequality. This variant is known as the metric Steiner tree problem. Given an instance of the (non-metric) Steiner tree problem, we can transform it in polynomial time into an equivalent instance of the metric Steiner tree problem; the transformation preserves the approximation factor.\n\nWhile the Euclidean version admits a PTAS, it is known that the metric Steiner tree problem is APX-complete, i.e., unless P = NP, it is impossible to achieve approximation ratios that are arbitrarily close to 1 in polynomial time. There is a polynomial-time algorithm that approximates the minimum Steiner tree to within a factor of formula_1;\nhowever, approximating within a factor formula_2 is NP-hard. For the restricted case of Steiner Tree problem with distances 1 and 2, a 1.25-approximation algorithm is known.\n\nIn a special case of the graph problem, the Steiner tree problem for quasi-bipartite graphs, \"S\" is required to include at least one endpoint of every edge in \"G\".\n\nThe Steiner tree problem has also been investigated in higher dimensions and on various surfaces. Algorithms to find the Steiner minimal tree have been found on the sphere, torus, projective plane, wide and narrow cones, and others.\n\nAnother generalizations of the Steiner tree problem are the k\"-edge-connected Steiner network problem and the k\"-vertex-connected Steiner network problem, where the goal is to find a \"k\"-edge-connected graph or a \"k\"-vertex-connected graph rather than any connected graph.\n\nThe Steiner problem has also been stated in the general setting of metric spaces and for possibly infinitely many points.\n\nThe general graph Steiner tree problem can be approximated by computing the minimum spanning tree of the subgraph of the metric closure of the graph induced by the terminal vertices. The metric closure of a graph \"G\" is the complete graph in which each edge is weighted by the shortest path distance between the nodes in \"G\". This algorithm produces a tree whose weight is within a 2 − 2/\"t\" factor of the weight of the optimal Steiner tree; this can be proven by considering a traveling salesperson tour on the optimal Steiner tree. The approximate solution is computable in polynomial time by first solving the all-pairs shortest paths problem to compute the metric closure, then by solving the minimum spanning tree problem.\n\nA series of papers provided approximation algorithms for the minimum Steiner tree problem with approximation ratios that improved upon the 2 − 2/\"t\" ratio. This sequence culminated with Robins and Zelikovsky's algorithm in 2000 which improved the ratio to 1.55 by iteratively improving upon the minimum cost terminal spanning tree. More recently, however, Jaroslaw Byrka et al. proved an formula_3 approximation using a linear programming relaxation and a technique called iterative, randomized rounding.\n\nThe Steiner ratio is the supremum of the ratio of the total length of the minimum spanning tree to the minimum Steiner tree for a set of points in the Euclidean plane.\n\nIn the Euclidean Steiner tree problem, the Steiner ratio is conjectured to be formula_4, the ratio that is achieved by three points in an equilateral triangle with a spanning tree that uses two sides of the triangle and a Steiner tree that connects the points through the centroid of the triangle. Despite earlier claims of a proof, the conjecture is still open. The best widely-accepted upper bound for the problem is 1.2134, by .\n\nFor the rectilinear Steiner tree problem, the Steiner ratio is exactly formula_5, the ratio that is achieved by four points in a square with a spanning tree that uses three sides of the square and a Steiner tree that connects the points through the center of the square. More precisely, for formula_6 distance the square should be tilted at formula_7 with respect to the coordinate axes, while for formula_8 distance the square should be axis-aligned.\n\n\n\n"}
{"id": "23610741", "url": "https://en.wikipedia.org/wiki?curid=23610741", "title": "Stocks-to-use ratio", "text": "Stocks-to-use ratio\n\nThe stocks-to-use ratio (S/U) is a convenient measure of supply and demand interrelationships of commodities. This ratio indicates the level of carryover stock for any given commodity as a percentage of the total use of the commodity. \n"}
{"id": "19282986", "url": "https://en.wikipedia.org/wiki?curid=19282986", "title": "Straight-line grammar", "text": "Straight-line grammar\n\nA straight-line grammar (sometimes abbreviated as SLG) is a formal grammar that generates exactly one string. Consequently, it does not branch (every non-terminal has only one associated production rule) nor loop (if non-terminal \"A\" appears in a derivation of \"B\", then \"B\" does not appear in a derivation of \"A\").\n\nStraight-line grammars are widely used in the development of algorithms that execute directly on compressed structures (without prior decompression).\n\nSLGs are of interest in fields like Kolmogorov complexity, Lossless data compression, Structure discovery and Compressed data structures.\n\nThe problem of finding a context-free grammar (equivalently: an SLG) of minimal size that generates a given string is called the smallest grammar problem.\n\nStraight-line grammars (more precisely: straight-line context-free string grammars) can be generalized to \"Straight-line context-free tree grammars\".\nThe latter can be used conveniently to compress trees.\n\nA context-free grammar \"G\" is an SLG if:\n\n1. for every non-terminal \"N\", there is at most one production rule that has \"N\" as its left-hand side, and\n\n2. the directed graph \"G\"=<\"V\",\"E\">, defined by \"V\" being the set of non-terminals and (\"A\",\"B\") ∈ \"E\" whenever \"B\" appears at the right-hand side of a production rule for \"A\", is acyclic.\n\nA mathematical definition of the more general formalism of straight-line context-free tree grammars can be found in Lohrey et al.\n\nAn SLG in Chomsky normal form is equivalent to a straight-line program.\n\n\n"}
{"id": "28186", "url": "https://en.wikipedia.org/wiki?curid=28186", "title": "Symmetry group", "text": "Symmetry group\n\nIn group theory, the symmetry group of an object (image, signal, etc.) is the group of all transformations under which the object is invariant with composition as the group operation. For a space with a metric, it is a subgroup of the isometry group of the space concerned. If not stated otherwise, this article considers symmetry groups in Euclidean geometry, but the concept may also be studied in more general contexts as expanded below.\n\nThe \"objects\" may be geometric figures, images, and patterns, such as a wallpaper pattern. The definition can be made more precise by specifying what is meant by image or pattern, e.g., a function of position with values in a set of colors. For symmetry of physical objects, one may also want to take their physical composition into account. The group of isometries of space induces a group action on objects in it.\n\nThe symmetry group is sometimes also called full symmetry group in order to emphasize that it includes the orientation-reversing isometries (like reflections, glide reflections and improper rotations) under which the figure is invariant. The subgroup of orientation-preserving isometries (i.e. translations, rotations, and compositions of these) that leave the figure invariant is called its proper symmetry group. The proper symmetry group of an object is equal to its full symmetry group if and only if the object is chiral (and thus there are no orientation-reversing isometries under which it is invariant).\n\nAny symmetry group whose elements have a common fixed point, which is true for all finite symmetry groups and also for the symmetry groups of bounded figures, can be represented as a subgroup of the orthogonal group O(\"n\") by choosing the origin to be a fixed point. The proper symmetry group is then a subgroup of the special orthogonal group SO(\"n\"), and is therefore also called rotation group of the figure.\n\nA discrete symmetry group is a symmetry group such that for every point of the space the set of images of the point under the isometries in the symmetry group is a discrete set. The number of elements in the group may be either finite or infinite.\n\nDiscrete symmetry groups come in three types: (1) finite point groups, which include only rotations, reflections, inversion and rotoinversion – they are just the finite subgroups of O(\"n\"), (2) infinite lattice groups, which include only translations, and (3) infinite space groups which combines elements of both previous types, and may also include extra transformations like screw displacements and glide reflections. There are also \"continuous\" symmetry groups, which contain rotations of arbitrarily small angles or translations of arbitrarily small distances. The group of all symmetries of a sphere O(3) is an example of this, and in general such continuous symmetry groups are studied as Lie groups. With a categorization of subgroups of the Euclidean group corresponds a categorization of symmetry groups.\n\nTwo geometric figures are considered to be of the same symmetry type if their symmetry groups are conjugate subgroups of the Euclidean group E(\"n\") (the isometry group of R), where two subgroups \"H\", \"H\" of a group \"G\" are \"conjugate\", if there exists such that . For example:\n\nWhen considering isometry groups, one may restrict oneself to those where for all points the set of images under the isometries is topologically closed. This includes all discrete isometry groups and also those involved in continuous symmetries, but excludes for example in 1D the group of translations by a rational number. A \"figure\" with this symmetry group is non-drawable and up to arbitrarily fine detail homogeneous, without being really homogeneous.\n\nThe isometry groups in one dimension where for all points the set of images under the isometries is topologically closed are:\n\nSee also symmetry groups in one dimension.\n\nUp to conjugacy the discrete point groups in two-dimensional space are the following classes:\n\n\nC is the trivial group containing only the identity operation, which occurs when the figure has no symmetry at all, for example the letter F. C is the symmetry group of the letter Z, C that of a triskelion, C of a swastika, and C, C, etc. are the symmetry groups of similar swastika-like figures with five, six, etc. arms instead of four.\n\nD is the 2-element group containing the identity operation and a single reflection, which occurs when the figure has only a single axis of bilateral symmetry, for example the letter A.\n\nD, which is isomorphic to the Klein four-group, is the symmetry group of a non-equilateral rectangle. This figure has four symmetry operations: the identity operation, one twofold axis of rotation, and two nonequivalent mirror planes.\n\nD, D etc. are the symmetry groups of the regular polygons.\n\nThe actual symmetry groups in each of these cases have two degrees of freedom for the center of rotation, and in the case of the dihedral groups, one more for the positions of the mirrors.\n\nThe remaining isometry groups in two dimensions with a fixed point, where for all points the set of images under the isometries is topologically closed are:\n\nFor non-bounded figures, the additional isometry groups can include translations; the closed ones are:\n\nUp to conjugacy the set of three-dimensional point groups consists of 7 infinite series, and 7 separate ones. In crystallography they are restricted to be compatible with the discrete translation symmetries of a crystal lattice. This crystallographic restriction of the infinite families of general point groups results in 32 crystallographic point groups (27 from the 7 infinite series, and 5 of the 7 others).\n\nThe continuous symmetry groups with a fixed point include those of:\n\nFor objects and scalar fields the cylindrical symmetry implies vertical planes of reflection. However, for vector fields it does not: in cylindrical coordinates with respect to some axis, \nformula_1 has cylindrical symmetry with respect to the axis if and only if formula_2 and formula_3 have this symmetry, i.e., they do not depend on formula_4. Additionally there is reflectional symmetry if and only if formula_5.\n\nFor spherical symmetry there is no such distinction, it implies planes of reflection.\n\nThe continuous symmetry groups without a fixed point include those with a screw axis, such as an infinite helix. See also subgroups of the Euclidean group.\n\nIn wider contexts, a symmetry group may be any kind of transformation group, or automorphism group. Once we know what kind of mathematical structure we are concerned with, we should be able to pinpoint what mappings preserve the structure. Conversely, specifying the symmetry can define the structure, or at least clarify what we mean by an invariant, geometric language in which to discuss it; this is one way of looking at the Erlangen programme.\n\nFor example, automorphism groups of certain models of finite geometries are not \"symmetry groups\" in the usual sense, although they preserve symmetry. They do this by preserving \"families\" of point-sets rather than point-sets (or \"objects\") themselves.\n\nLike above, the group of automorphisms of space induces a group action on objects in it.\n\nFor a given geometric figure in a given geometric space, consider the following equivalence relation: two automorphisms of space are equivalent if and only if the two images of the figure are the same (here \"the same\" does not mean something like e.g. \"the same up to translation and rotation\", but it means \"exactly the same\"). Then the equivalence class of the identity is the symmetry group of the figure, and every equivalence class corresponds to one isomorphic version of the figure.\n\nThere is a bijection between every pair of equivalence classes: the inverse of a representative of the first equivalence class, composed with a representative of the second.\n\nIn the case of a finite automorphism group of the whole space, its order is the order of the symmetry group of the figure multiplied by the number of isomorphic versions of the figure.\n\nExamples:\n\nCompare Lagrange's theorem (group theory) and its proof.\n\n\n"}
{"id": "764405", "url": "https://en.wikipedia.org/wiki?curid=764405", "title": "Turing degree", "text": "Turing degree\n\nIn computer science and mathematical logic the Turing degree (named after Alan Turing) or degree of unsolvability of a set of natural numbers measures the level of algorithmic unsolvability of the set.\n\nThe concept of Turing degree is fundamental in computability theory, where sets of natural numbers are often regarded as decision problems. The Turing degree of a set is a measure of how difficult it is to solve the decision problem associated with the set, that is, to determine whether an arbitrary number is in the given set.\n\nTwo sets are Turing equivalent if they have the same level of unsolvability; each Turing degree is a collection of Turing equivalent sets, so that two sets are in different Turing degrees exactly when they are not Turing equivalent. Furthermore, the Turing degrees are partially ordered so that if the Turing degree of a set \"X\" is less than the Turing degree of a set \"Y\" then any (noncomputable) procedure that correctly decides whether numbers are in \"Y\" can be effectively converted to a procedure that correctly decides whether numbers are in \"X\". It is in this sense that the Turing degree of a set corresponds to its level of algorithmic unsolvability.\n\nThe Turing degrees were introduced by Emil Leon Post (1944), and many fundamental results were established by Stephen Cole Kleene and Post (1954). The Turing degrees have been an area of intense research since then. Many proofs in the area make use of a proof technique known as the priority method.\n\nFor the rest of this article, the word \"set\" will refer to a set of natural numbers. A set \"X\" is said to be Turing reducible to a set \"Y\" if there is an oracle Turing machine that decides membership in \"X\" when given an oracle for membership in \"Y\". The notation \"X\" ≤ \"Y\" indicates that \"X\" is Turing reducible to \"Y\".\n\nTwo sets \"X\" and \"Y\" are defined to be Turing equivalent if \"X\" is Turing reducible to \"Y\" and \"Y\" is Turing reducible to \"X\". The notation \"X\" ≡ \"Y\" indicates that \"X\" and \"Y\" are Turing equivalent. The relation ≡ can be seen to be an equivalence relation, which means that for all sets \"X\", \"Y\", and \"Z\": \n\nA Turing degree is an equivalence class of the relation ≡. The notation [\"X\"] denotes the equivalence class containing a set \"X\". The entire collection of Turing degrees is denoted formula_1.\n\nThe Turing degrees have a partial order ≤ defined so that [\"X\"] ≤ [\"Y\"] if and only if \"X\" ≤ \"Y\". There is a unique Turing degree containing all the computable sets, and this degree is less than every other degree. It is denoted 0 (zero) because it is the least element of the poset formula_1. (It is common to use boldface notation for Turing degrees, in order to distinguish them from sets. When no confusion can occur, such as with [\"X\"], the boldface is not necessary.)\n\nFor any sets \"X\" and \"Y\", X join Y, written \"X\" ⊕ \"Y\", is defined to be the union of the sets } and }. The Turing degree of \"X\" ⊕ \"Y\" is the least upper bound of the degrees of \"X\" and \"Y\". Thus formula_1 is a join-semilattice. The least upper bound of degrees a and b is denoted a ∪ b. It is known that formula_1 is not a lattice, as there are pairs of degrees with no greatest lower bound.\n\nFor any set \"X\" the notation \"X\"′ denotes the set of indices of oracle machines that halt when using \"X\" as an oracle. The set \"X\"′ is called the Turing jump of \"X\". The Turing jump of a degree [\"X\"] is defined to be the degree [\"X\"′]; this is a valid definition because \"X\"′ ≡ \"Y\"′ whenever \"X\" ≡ \"Y\". A key example is 0′, the degree of the halting problem.\n\n\nA great deal of research has been conducted into the structure of the Turing degrees. The following survey lists only some of the many known results. One general conclusion that can be drawn from the research is that the structure of the Turing degrees is extremely complicated.\n\n\n\n\nA degree is called r.e. (recursively enumerable) if it contains a recursively enumerable set. Every r.e. degree is less than or equal to 0′ but not every degree less than 0′ is an r.e. degree.\n\n\n\nEmil Post studied the r.e. Turing degrees and asked whether there is any r.e. degree strictly between 0 and 0′. The problem of constructing such a degree (or showing that none exist) became known as Post's problem. This problem was solved independently by Friedberg and Muchnik in the 1950s, who showed that these intermediate r.e. degrees do exist. Their proofs each developed the same new method for constructing r.e. degrees which came to be known as the priority method. The priority method is now the main technique for establishing results about r.e. sets.\n\nThe idea of the priority method for constructing a r.e. set \"X\" is to list a countable sequence of \"requirements\" that \"X\" must satisfy. For example, to construct a r.e. set \"X\" between 0 and 0′ it is enough to satisfy the requirements \"A\" and \"B\" for each natural number \"e\", where \"A\" requires that the oracle machine with index \"e\" does not compute 0′ from \"X\" and \"B\" requires that the Turing machine with index \"e\" (and no oracle) does not compute \"X\". These requirements are put into a \"priority ordering\", which is an explicit bijection of the requirements and the natural numbers. The proof proceeds inductively with one stage for each natural number; these stages can be thought of as steps of time during which the set \"X\" is enumerated. At each stage, numbers may be put into \"X\" or forever prevented from entering \"X\" in an attempt to \"satisfy\" requirements (that is, force them to hold once all of \"X\" has been enumerated). Sometimes, a number can be enumerated into \"X\" to satisfy one requirement but doing this would cause a previously satisfied requirement to become unsatisfied (that is, to be \"injured\"). The priority order on requirements is used to determine which requirement to satisfy in this case. The informal idea is that if a requirement is injured then it will eventually stop being injured after all higher priority requirements have stopped being injured, although not every priority argument has this property. An argument must be made that the overall set \"X\" is r.e. and satisfies all the requirements. Priority arguments can be used to prove many facts about r.e. sets; the requirements used and the manner in which they are satisfied must be carefully chosen to produce the required result.\n\n\n\n\n"}
{"id": "307145", "url": "https://en.wikipedia.org/wiki?curid=307145", "title": "Two's complement", "text": "Two's complement\n\nTwo's complement is a mathematical operation on binary numbers, best known for its role in computing as a method of signed number representation. For this reason, it is the most important example of a radix complement.\n\nThe two's complement of an -bit number is defined as its complement with respect to . For instance, for the three-bit number 010, the two's complement is 110, because .\n\nTwo's complement is the most common method of representing signed integers on computers. In this scheme, if the binary number 010 encodes the signed integer 2, then its two's complement, 110, encodes the inverse: -2. In other words, to reverse the sign of any integer in this scheme, you can take the two's complement of its binary representation. The tables at right illustrate this property.\n\nCompared to other systems for representing signed numbers (\"e.g.,\" ones' complement), two's complement has the advantage that the fundamental arithmetic operations of addition, subtraction, and multiplication are identical to those for unsigned binary numbers (as long as the inputs are represented in the same number of bits, and any overflow beyond those bits is discarded from the result). This property makes the system simpler to implement, especially for higher-precision arithmetic. Unlike ones' complement systems, two's complement has no representation for negative zero, and thus does not suffer from its associated difficulties.\n\nConveniently, another way of finding the two's complement of a number is to take its ones' complement and add one: the sum of a number and its ones' complement is all '1' bits, or ; and by definition, the sum of a number and its \"two's\" complement is .\n\nThe method of complements had long been used to perform subtraction in decimal adding machines and mechanical calculators. John von Neumann suggested use of two's complement binary representation in his 1945 \"First Draft of a Report on the EDVAC\" proposal for an electronic stored-program digital computer. The 1949 EDSAC, which was inspired by the \"First Draft\", used two's complement representation of binary numbers.\n\nMany early computers, including the CDC 6600, the LINC, the PDP-1, and the UNIVAC 1107, use ones' complement notation; the descendants of the UNIVAC 1107, the UNIVAC 1100/2200 series, continue to do so. The IBM 700/7000 series scientific machines use sign/magnitude notation, except for the index registers which are two's complement. Early commercial two's complement computers include the Digital Equipment Corporation PDP-5 and the 1963 PDP-6. The System/360, introduced in 1964 by IBM, then the dominant player in the computer industry, made two's complement the most widely used binary representation in the computer industry. The first minicomputer, the PDP-8 introduced in 1965, uses two's complement arithmetic as do the 1969 Data General Nova, the 1970 PDP-11, and almost all subsequent minicomputers and microcomputers.\n\nThe term \"two's complement\" can mean either a number format or a mathematical operator. For example, 0111 represents decimal 7 in two's-complement \"notation\", but the two's complement of 7 in a 4-bit register is actually the \"1001\" bit string (the same as represents in unsigned arithmetics) which is the two's complement \"representation\" of −7. The statement \"convert to two's complement\" may be ambiguous, since it could describe either the process of representing in two's-complement notation without changing its value, or the calculation of the two's complement, which is the arithmetic negative of if two's complement representation is used.\n\nA two's-complement number system encodes positive and negative numbers in a binary number representation. The weight of each bit is a power of two, except for the most significant bit, whose weight is the negative of the corresponding power of two.\n\nThe value  of an -bit integer formula_1 is given by the following formula:\nThe most significant bit determines the sign of the number and is sometimes called the sign bit. Unlike in sign-and-magnitude representation, the sign bit also has the weight shown above. Using bits, all integers from to can be represented.\n\nThe following Python code shows a simple function which will convert an unsigned input integer to a two's complement signed integer using the above logic with bitwise operators:\n\ndef twos_complement(input_value, num_bits):\n\nIn two's complement notation, a \"non-negative\" number is represented by its ordinary binary representation; in this case, the most significant bit is 0. Though, the range of numbers represented is not the same as with unsigned binary numbers. For example, an 8-bit unsigned number can represent the values 0 to 255 (11111111). However a two's complement 8-bit number can only represent positive integers from 0 to 127 (01111111), because the rest of the bit combinations with the most significant bit as '1' represent the negative integers −1 to −128.\n\nThe two's complement operation is the additive inverse operation, so negative numbers are represented by the two's complement of the absolute value.\n\nTo get the two's complement of a binary number, the bits are inverted, or \"flipped\", by using the bitwise NOT operation; the value of 1 is then added to the resulting value, ignoring the overflow which occurs when taking the two's complement of 0.\n\nFor example, using 1 byte, the decimal number 5 is represented by\nThe most significant bit is 0, so the pattern represents a non-negative value. To convert to −5 in two's-complement notation, the bits are inverted; 0 becomes 1, and 1 becomes 0:\nAt this point, the representation is the ones' complement of the decimal value −5. To obtain the two's complement, 1 is added to the result, giving:\nThe result is a signed binary number representing the decimal value −5 in two's-complement form. The most significant bit is 1, so the value represented is negative.\n\nThe two's complement of a negative number is the corresponding positive value. For example, inverting the bits of −5 (above) gives:\nAnd adding one gives the final value:\n\nThe two's complement of zero is zero: inverting gives all ones, and adding one changes the ones back to zeros (since the overflow is ignored). Furthermore, the two's complement of the most negative number representable (e.g. a one as the most-significant bit and all other bits zero) is itself. Hence, there appears to be an 'extra' negative number.\n\nThe sum of a number and its ones' complement is an -bit word with all 1 bits, which is (reading as an unsigned binary number) . Then adding a number to its two's complement results in the lowest bits set to 0 and the carry bit 1, where the latter has the weight (reading it as an unsigned binary number) of . Hence, in the unsigned binary arithmetic the value of two's-complement negative number of a positive satisfies the equality .\n\nFor example, to find the 4-bit representation of −5 (subscripts denote the base of the representation):\nHence, with :\nThe calculation can be done entirely in base 10, converting to base 2 at the end:\n\nA shortcut to manually convert a binary number into its two's complement is to start at the least significant bit (LSB), and copy all the zeros, working from LSB toward the most significant bit (MSB) until the first 1 is reached; then copy that 1, and flip all the remaining bits (Leave the MSB as a 1 if the initial number was in sign-and-magnitude representation). This shortcut allows a person to convert a number to its two's complement without first forming its ones' complement. For example: the two's complement of \"0011 1100\" is \"1100 0100\", where the underlined digits were unchanged by the copying operation (while the rest of the digits were flipped).\n\nIn computer circuitry, this method is no faster than the \"complement and add one\" method; both methods require working sequentially from right to left, propagating logic changes. The method of complementing and adding one can be sped up by a standard carry look-ahead adder circuit; the LSB towards MSB method can be sped up by a similar logic transformation.\n\nWhen turning a two's-complement number with a certain number of bits into one with more bits (e.g., when copying from a 1-byte variable to a 2-byte variable), the most-significant bit must be repeated in all the extra bits. Some processors do this in a single instruction; on other processors, a conditional must be used followed by code to set the relevant bits or bytes.\n\nSimilarly, when a two's-complement number is shifted to the right, the most-significant bit, which contains magnitude and the sign information, must be maintained. However, when shifted to the left, a 0 is shifted in. These rules preserve the common semantics that left shifts multiply the number by two and right shifts divide the number by two.\n\nBoth shifting and doubling the precision are important for some multiplication algorithms. Note that unlike addition and subtraction, width extension and right shifting are done differently for signed and unsigned numbers.\n\nWith only one exception, starting with any number in two's-complement representation, if all the bits are flipped and 1 added, the two's-complement representation of the negative of that number is obtained. Positive 12 becomes negative 12, positive 5 becomes negative 5, zero becomes zero(+overflow), etc.\n\nThe two's complement of the minimum number in the range will not have the desired effect of negating the number. For example, the two's complement of −128 in an 8-bit system results in the same binary number. This is because a positive value of 128 cannot be represented with an 8-bit signed binary numeral.\n\nThis phenomenon is fundamentally about the mathematics of binary numbers, not the details of the representation as two's complement. Mathematically, this is complementary to the fact that the negative of 0 is again 0. For a given number of bits \"k\" there is an even number of binary numbers 2, taking negatives is a group action (of the group of order 2) on binary numbers, and since the orbit of zero has order 1, at least one other number must have an orbit of order 1 for the orders of the orbits to add up to the order of the set. Thus some other number must be invariant under taking negatives (formally, by the orbit-stabilizer theorem). Geometrically, one can view the \"k\"-bit binary numbers as the cyclic group formula_3, which can be visualized as a circle (or properly a regular 2-gon), and taking negatives is a reflection, which fixes the elements of order dividing 2: 0 and the opposite point, or visually the zenith and nadir.\n\nNote that this negative being the same number is detected as an overflow condition since there was a carry into but not out of the most-significant bit. This can lead to unexpected bugs in that an unchecked implementation of absolute value could return a negative number in the case of the minimum negative. The \"abs\" family of integer functions in C typically has this behaviour. This is also true for Java. In this case it is for the developer to decide if there will be a check for the minimum negative value before the call of the function.\n\nThe most negative number in two's complement is sometimes called \"the weird number,\" because it is the only exception.\n\nAlthough the number is an exception, it is a valid number in regular two's complement systems. All arithmetic operations work with it both as an operand and (unless there was an overflow) a result.\n\nGiven a set of all possible -bit values, we can assign the lower (by the binary value) half to be the integers from 0 to inclusive and the upper half to be to −1 inclusive. The upper half (again, by the binary value) can be used to represent negative integers from to −1 because, under addition modulo they behave the same way as those negative integers. That is to say that because any value in the set can be used in place of .\n\nFor example, with eight bits, the unsigned bytes are 0 to 255. Subtracting 256 from the top half (128 to 255) yields the signed bytes −128 to −1.\n\nThe relationship to two's complement is realised by noting that , and is the ones' complement of .\n\n −95 modulo 256 is equivalent to 161 since\n\nFundamentally, the system represents negative integers by counting backward and wrapping around. The boundary between positive and negative numbers is arbitrary, but by convention all negative numbers have a left-most bit (most significant bit) of one. Therefore, the most positive 4-bit number is 0111 (7) and the most negative is 1000 (−8). Because of the use of the left-most bit as the sign bit, the absolute value of the most negative number (|−8| = 8) is too large to represent. For example, an 8-bit number can only represent every integer from −128 to 127 () inclusive. Negating a two's complement number is simple: Invert all the bits and add one to the result. For example, negating 1111, we get 0000 + 1 = 1. Therefore, 1111 must represent −1.\n\nThe system is useful in simplifying the implementation of arithmetic on computer hardware. Adding 0011 (3) to 1111 (−1) at first seems to give the incorrect answer of 10010. However, the hardware can simply ignore the left-most bit to give the correct answer of 0010 (2). Overflow checks still must exist to catch operations such as summing 0100 and 0100.\n\nThe system therefore allows addition of negative operands without a subtraction circuit and a circuit that detects the sign of a number. Moreover, that addition circuit can also perform subtraction by taking the two's complement of a number (see below), which only requires an additional cycle or its own adder circuit. To perform this, the circuit merely pretends an extra left-most bit of 1 exists.\n\nAdding two's-complement numbers requires no special processing even if the operands have opposite signs: the sign of the result is determined automatically. For example, adding 15 and −5:\n\nThis process depends upon restricting to 8 bits of precision; a carry to the (nonexistent) 9th most significant bit is ignored, resulting in the arithmetically correct result of 10.\n\nThe last two bits of the carry row (reading right-to-left) contain vital information: whether the calculation resulted in an arithmetic overflow, a number too large for the binary system to represent (in this case greater than 8 bits). An overflow condition exists when these last two bits are different from one another. As mentioned above, the sign of the number is encoded in the MSB of the result.\n\nIn other terms, if the left two carry bits (the ones on the far left of the top row in these examples) are both 1s or both 0s, the result is valid; if the left two carry bits are \"1 0\" or \"0 1\", a sign overflow has occurred. Conveniently, an XOR operation on these two bits can quickly determine if an overflow condition exists. As an example, consider the signed 4-bit addition of 7 and 3:\n\nIn this case, the far left two (MSB) carry bits are \"01\", which means there was a two's-complement addition overflow. That is, 1010 = 10 is outside the permitted range of −8 to 7. The result would be correct if treated as unsigned integer.\n\nIn general, any two -bit numbers may be added \"without\" overflow, by first sign-extending both of them to bits, and then adding as above. The bits result is large enough to represent any possible sum ( two's complement can represent values in the range −16 to 15) so overflow will never occur. It is then possible, if desired, to 'truncate' the result back to bits while preserving the value if and only if the discarded bit is a proper sign extension of the retained result bits. This provides another method of detecting overflow—which is equivalent to the method of comparing the carry bits—but which may be easier to implement in some situations, because it does not require access to the internals of the addition.\n\nComputers usually use the method of complements to implement subtraction. Using complements for subtraction is closely related to using complements for representing negative numbers, since the combination allows all signs of operands and results; direct subtraction works with two's-complement numbers as well. Like addition, the advantage of using two's complement is the elimination of examining the signs of the operands to determine whether addition or subtraction is needed. For example, subtracting −5 from 15 is really adding 5 to 15, but this is hidden by the two's-complement representation:\n\nOverflow is detected the same way as for addition, by examining the two leftmost (most significant) bits of the borrows; overflow has occurred if they are different.\n\nAnother example is a subtraction operation where the result is negative: 15 − 35 = −20:\n\nAs for addition, overflow in subtraction may be avoided (or detected after the operation) by first sign-extending both inputs by an extra bit.\n\nThe product of two -bit numbers requires bits to contain all possible values.\n\nIf the precision of the two operands using two's complement is doubled before the multiplication, direct multiplication (discarding any excess bits beyond that precision) will provide the correct result. For example, take . First, the precision is extended from four bits to eight. Then the numbers are multiplied, discarding the bits beyond the eighth bit (as shown by \"\"):\n\nThis is very inefficient; by doubling the precision ahead of time, all additions must be double-precision and at least twice as many partial products are needed than for the more efficient algorithms actually implemented in computers. Some multiplication algorithms are designed for two's complement, notably Booth's multiplication algorithm. Methods for multiplying sign-magnitude numbers don't work with two's-complement numbers without adaptation. There isn't usually a problem when the multiplicand (the one being repeatedly added to form the product) is negative; the issue is setting the initial bits of the product correctly when the multiplier is negative. Two methods for adapting algorithms to handle two's-complement numbers are common:\n\n\nAs an example of the second method, take the common add-and-shift algorithm for multiplication. Instead of shifting partial products to the left as is done with pencil and paper, the accumulated product is shifted right, into a second register that will eventually hold the least significant half of the product. Since the least significant bits are not changed once they are calculated, the additions can be single precision, accumulating in the register that will eventually hold the most significant half of the product. In the following example, again multiplying 6 by −5, the two registers and the extended sign bit are separated by \"|\":\nComparison is often implemented with a dummy subtraction, where the flags in the computer's status register are checked, but the main result is ignored. The zero flag indicates if two values compared equal. If the exclusive-or of the sign and overflow flags is 1, the subtraction result was less than zero, otherwise the result was zero or greater. These checks are often implemented in computers in conditional branch instructions.\n\nUnsigned binary numbers can be ordered by a simple lexicographic ordering, where the bit value 0 is defined as less than the bit value 1. For two's complement values, the meaning of the most significant bit is reversed (i.e. 1 is less than 0).\n\nThe following algorithm (for an -bit two's complement architecture) sets the result register R to −1 if A < B, to +1 if A > B, and to 0 if A and B are equal:\n\nIn a classic \"HAKMEM\" published by the MIT AI Lab in 1972, Bill Gosper noted that whether or not a machine's internal representation was two's-complement could be determined by summing the successive powers of two. In a flight of fancy, he noted that the result of doing this algebraically indicated that \"algebra is run on a machine (the universe) which is two's-complement.\"\n\nGosper's end conclusion is not necessarily meant to be taken seriously, and it is akin to a mathematical joke. The critical step is \"...110 = ...111 − 1\", i.e., \"2\"X\" = \"X\" − 1\", and thus \"X\" = ...111 = −1. This presupposes a method by which an infinite string of 1s is considered a number, which requires an extension of the finite place-value concepts in elementary arithmetic. It is meaningful either as part of a two's-complement notation for all integers, as a typical 2-adic number, or even as one of the generalized sums defined for the divergent series of real numbers 1 + 2 + 4 + 8 + ···. Digital arithmetic circuits, idealized to operate with infinite (extending to positive powers of 2) bit strings, produce 2-adic addition and multiplication compatible with two's complement representation. Continuity of binary arithmetical and bitwise operations in 2-adic metric also has some use in cryptography.\n\nTo convert a fraction, for instance; .0101 you must convert starting from right to left the 1s to decimal as in a normal conversion. In this example 0101 is equal to 5 in decimal. Each digit after the floating point represents a fraction where the denominator is a multiplier of 2. So, the first is 1/2, the second is 1/4 and so on. Having already calculated the decimal value as mentioned above, you use only the denominator of the LSB (LSB = starting from right). As a result, we have 5/16.\n\nFor instance, having the floating value of .0110 for this method to work, one should not consider the last 0 from the right. Hence, instead of calculating the decimal value for 0110, we calculate the value 011, which is 3 in decimal (by leaving the \"0\" in the end, the result would have been 6, together with the denominator 2^4 = 16 reduces to 3/8). So the denominator is 8. So, the final result is 3/8.\n\n\n\n"}
{"id": "1585226", "url": "https://en.wikipedia.org/wiki?curid=1585226", "title": "Young symmetrizer", "text": "Young symmetrizer\n\nIn mathematics, a Young symmetrizer is an element of the group algebra of the symmetric group, constructed in such a way that, for the homomorphism from the group algebra to the endomorphisms of a vector space formula_1 obtained from the action of formula_2 on formula_1 by permutation of indices, the image of the endomorphism determined by that element corresponds to an irreducible representation of the symmetric group over the complex numbers. A similar construction works over any field, and the resulting representations are called Specht modules. The Young symmetrizer is named after British mathematician Alfred Young.\n\nGiven a finite symmetric group \"S\" and specific Young tableau λ corresponding to a numbered partition of \"n\", define two permutation subgroups formula_4 and formula_5 of \"S\" as follows:\n\nand\n\nCorresponding to these two subgroups, define two vectors in the group algebra formula_8 as\n\nand\n\nwhere formula_11 is the unit vector corresponding to \"g\", and formula_12 is the sign of the permutation. The product\n\nis the Young symmetrizer corresponding to the Young tableau λ. Each Young symmetrizer corresponds to an irreducible representation of the symmetric group, and every irreducible representation can be obtained from a corresponding Young symmetrizer. (If we replace the complex numbers by more general fields the corresponding representations will not be irreducible in general.)\n\nLet \"V\" be any vector space over the complex numbers. Consider then the tensor product vector space formula_14 (\"n\" times). Let \"S\" act on this tensor product space by permuting the indices. One then has a natural group algebra representation formula_15 on formula_1.\n\nGiven a partition λ of \"n\", so that formula_17, then the image of formula_18 is\n\nFor instance, if formula_20, and formula_21, with the canonical Young tableau formula_22. Then the corresponding formula_18 is given by formula_24. Let an element in formula_25 be given by formula_26. Then \nThe latter clearly span formula_28.\n\nThe image of formula_29 is \nwhere μ is the conjugate partition to λ. Here, formula_31 and formula_32 are the symmetric and alternating tensor product spaces.\n\nThe image formula_33 of formula_34 in formula_8 is an irreducible representation of \"S\", called a Specht module. We write\nfor the irreducible representation.\n\nSome scalar multiple of formula_37 is idempotent, that is formula_38 for some rational number formula_39. Specifically, one finds formula_40. In particular, this implies that representations of the symmetric group can be defined over the rational numbers; that is, over the rational group algebra formula_41.\n\nConsider, for example, \"S\" and the partition (2,1). Then one has formula_42\n\nIf \"V\" is a complex vector space, then \nthe images of formula_37 on spaces formula_44 provides essentially all the finite-dimensional irreducible representations of GL(V).\n\n\n"}
