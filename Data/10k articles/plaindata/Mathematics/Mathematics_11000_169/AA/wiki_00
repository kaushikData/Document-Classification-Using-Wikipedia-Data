{"id": "237305", "url": "https://en.wikipedia.org/wiki?curid=237305", "title": "Abel's theorem", "text": "Abel's theorem\n\nIn mathematics, Abel's theorem for power series relates a limit of a power series to the sum of its coefficients. It is named after Norwegian mathematician Niels Henrik Abel.\n\nLet\n\nbe a power series with real coefficients \"a\" with radius of convergence 1. Suppose that the series \n\nconverges. Then \"G\"(\"x\") is continuous from the left at formula_3, i.e.\n\nThe same theorem holds for complex power series \n\nprovided that formula_6 within a \"Stolz sector\", that is, a region of the open unit disk where\n\nfor some \"M\". Without this restriction, the limit may fail to exist: for example, the power series\n\nconverges to 0 at \"z\" = 1, but is unbounded near any point of the form \"e\", so the value at \"z\" = 1 is not the limit as \"z\" tends to 1 in the whole open disk.\n\nNote that \"G\"(\"z\") is continuous on the real closed interval [0, \"t\"] for \"t\" < 1, by virtue of the uniform convergence of the series on compact subsets of the disk of convergence. Abel's theorem allows us to say more, namely that \"G\"(\"z\") is continuous on [0, 1].\n\nAs an immediate consequence of this theorem, if \"z\" is any nonzero complex number for which the series \n\nconverges, then it follows that\n\nin which the limit is taken from below.\n\nThe theorem can also be generalized to account for infinite sums. If\n\nthen \n\nHowever, if the series is only known to be divergent, the theorem fails; take for example, the power series for \n\nAt formula_14 the series is equal to formula_15 but formula_16\n\nWe also remark the theorem holds for radii of convergence other than formula_17: let \n\nbe a power series with radius of convergence formula_19, and suppose the series converges at formula_20. Then formula_21 is continuous from the left at formula_20, i.e.\n\nThe utility of Abel's theorem is that it allows us to find the limit of a power series as its argument (i.e. formula_24) approaches 1 from below, even in cases where the radius of convergence, formula_19, of the power series is equal to 1 and we cannot be sure whether the limit should be finite or not. See e.g. the binomial series. Abel's theorem allows us to evaluate many series in closed form. For example, when \n\nwe obtain \n\nby integrating the uniformly convergent geometric power series term by term on formula_28; thus the series \n\nconverges to formula_30 by Abel's theorem. Similarly, \n\nconverges to formula_32\n\nformula_33 is called the generating function of the sequence formula_34. Abel's theorem is frequently useful in dealing with generating functions of real-valued and non-negative sequences, such as probability-generating functions. In particular, it is useful in the theory of Galton–Watson processes.\n\nAfter subtracting a constant from formula_35, we may assume that formula_36. Let formula_37. Then substituting formula_38 and performing a simple manipulation of the series results in\n\nGiven formula_40 pick \"n\" large enough so that formula_41 for all formula_42 and note that\n\nwhen \"z\" lies within the given Stolz angle. Whenever \"z\" is sufficiently close to 1 we have\n\nso that formula_45 when \"z\" is both sufficiently close to 1 and within the Stolz angle.\n\nConverses to a theorem like Abel's are called Tauberian theorems: There is no exact converse, but results conditional on some hypothesis. The field of divergent series, and their summation methods, contains many theorems \"of abelian type\" and \"of tauberian type\".\n\n\n\n"}
{"id": "4283745", "url": "https://en.wikipedia.org/wiki?curid=4283745", "title": "Algebraic expression", "text": "Algebraic expression\n\nIn mathematics, an algebraic expression is an expression built up from integer constants, variables, and the algebraic operations (addition, subtraction, multiplication, division and exponentiation by an exponent that is a rational number). For example, is an algebraic expression. Since taking the square root is the same as raising to the power ,\nis also an algebraic expression. By contrast, transcendental numbers like and are not algebraic.\n\nA rational expression is an expression that may be rewritten to a rational fraction by using the properties of the arithmetic operations (commutative properties and associative properties of addition and multiplication, distributive property and rules for the operations on the fractions). In other words, a rational expression is an expression which may be constructed from the variables and the constants by using only the four operations of arithmetic. Thus, \nis a rational expression, whereas \nis not.\n\nA rational equation is an equation in which two rational fractions (or rational expressions) of the form \nare set equal to each other. These expressions obey the same rules as fractions. The equations can be solved by cross-multiplying. Division by zero is undefined, so that a solution causing formal division by zero is rejected.\n\nAlgebra has its own terminology to describe parts of an expression:\n\n<br>\n1 – Exponent (power), 2 – coefficient, 3 – term, 4 – operator, 5 – constant, formula_5 - variables\n\nThe roots of a polynomial expression of degree \"n\", or equivalently the solutions of a polynomial equation, can always be written as algebraic expressions if \"n\" < 5 (see quadratic formula, cubic function, and quartic equation). Such a solution of an equation is called an algebraic solution. But the Abel–Ruffini theorem states that algebraic solutions do not exist for all such equations (just for some of them) if \"n\" formula_6 5.\n\nBy convention, letters at the beginning of the alphabet (e.g. formula_7) are typically used to represent constants, and those toward the end of the alphabet (e.g. formula_5 and formula_9) are used to represent variables. They are usually written in italics.\n\nBy convention, terms with the highest power (exponent), are written on the left, for example, formula_10 is written to the left of formula_11. When a coefficient is one, it is usually omitted (e.g. formula_12 is written formula_10). Likewise when the exponent (power) is one, (e.g. formula_14 is written formula_15), and, when the exponent is zero, the result is always 1 (e.g. formula_16 is written formula_17, since formula_18 is always formula_19).\n\nThe table below summarizes how algebraic expressions compare with several other types of mathematical expressions by the type of elements they may contain, according to common but not universal conventions.\n\nA \"rational algebraic expression\" (or \"rational expression\") is an algebraic expression that can be written as a quotient of polynomials, such as . An \"irrational algebraic expression\" is one that is not rational, such as .\n\n"}
{"id": "7639504", "url": "https://en.wikipedia.org/wiki?curid=7639504", "title": "BIT predicate", "text": "BIT predicate\n\nIn mathematics and computer science, the BIT predicate or Ackermann coding, sometimes written BIT(\"i\", \"j\"), is a predicate which tests whether the \"j\"th bit of the number \"i\" is 1, when \"i\" is written in binary.\n\nThe BIT predicate was first introduced as the encoding of hereditarily finite sets as natural numbers by Wilhelm Ackermann in his 1937 paper\n(\"The Consistency of General Set Theory\").\n\nEach natural number encodes a finite set and\neach finite set is represented by a natural number.\nThis mapping uses the binary numeral system.\nIf the number \"n\" encodes a finite set \"A\" and the \"i\"th binary digit of \"n\" is 1, then the set encoded by \"i\" is an element of \"A\".\n\nThe Ackermann coding is a primitive recursive function.\n\nIn programming languages such as C, C++, Java, or Python that provide a right shift operator codice_1 and a bitwise Boolean and operator codice_2, the BIT predicate BIT(\"i\", \"j\") can be implemented by the expression\ncodice_3. Here the bits of \"i\" are numbered from the low order bits to high order bits in the binary representation of \"i\", with the ones bit being numbered as bit 0.\n\nIn the mathematical study of computer security, the private information retrieval problem can be modeled as one in which a client, communicating with a collection of servers that store a binary number \"i\", wishes to determine the result of a BIT predicate BIT(\"i\", \"j\") without divulging the value of \"j\" to the servers. describe a method for replicating \"i\" across two servers in such a way that the client can solve the private information retrieval problem using a substantially smaller amount of communication than would be necessary to recover the complete value of \"i\".\n\nThe BIT predicate is often examined in the context of first-order logic, where we can examine the system resulting from adding the BIT predicate to first-order logic. In descriptive complexity, the complexity class FO + BIT resulting from adding the BIT predicate to FO results in a more robust complexity class. The class FO + BIT, of first-order logic with the BIT predicate, is the same as the class FO + PLUS + TIMES, of first-order logic with addition and multiplication predicates.\n\nAckermann in 1937 and Richard Rado in 1964 used this predicate to construct the infinite Rado graph. In their construction, the vertices of this graph correspond to the non-negative integers, written in binary, and there is an undirected edge from vertex \"i\" to vertex \"j\", for \"i\" < \"j\", when BIT(\"j\",\"i\") is nonzero.\n"}
{"id": "12511670", "url": "https://en.wikipedia.org/wiki?curid=12511670", "title": "Bailar twist", "text": "Bailar twist\n\nThe Bailar twist is a mechanism proposed for the racemization of octahedral complexes containing three bidentate chelate rings. Such complexes typically adopt an octahedral molecular geometry, in which case they possess helical chirality. One pathway by which these compounds can racemize is via the formation of a trigonal prismatic intermediate with D3h point group symmetry. This pathway is named in honor of John C. Bailar, Jr., an inorganic chemist who investigated this process. An alternative pathway is called the Rây−Dutt twist.\n\n"}
{"id": "9759038", "url": "https://en.wikipedia.org/wiki?curid=9759038", "title": "Banach's matchbox problem", "text": "Banach's matchbox problem\n\nBanach's match problem is a classic problem in probability attributed to Stefan Banach. Feller says that the problem was inspired by a humorous reference to Banach's smoking habit in a speech honouring him by H. Steinhaus, but that it was not Banach who set the problem or provided an answer. \n\nSuppose a mathematician carries two matchboxes at all times: one in his left pocket and one in his right. Each time he needs a match, he is equally likely to take it from either pocket. Suppose he reaches into his pocket and discovers for the first time that the box picked is empty. If it is assumed that each of the matchboxes originally contained formula_1 matches, what is the probability that there are exactly formula_2 matches in the other box?\n\nWithout loss of generality consider the case where the matchbox in his right pocket has an unlimited number of matches and let formula_3 be the number of matches removed from this one before the left one is found to be empty. When the left pocket is found to be empty, the man has chosen that pocket formula_4 times. Then formula_3 is the number of successes before formula_4 failures in Bernoulli trials with formula_7, which has the negative binomial distribution and thus\n\nReturning to the original problem, we see that the probability that the left pocket is found to be empty first is formula_9 which equals formula_10 because both are equally likely. We see that the number formula_11 of matches remaining in the other pocket is\n\nThe expectation of the distribution is approximately formula_13. (This is shown using Stirling's approximation.) So starting with boxes with formula_14 matches, the expected number of matches in the second box is formula_15.\n\n"}
{"id": "17686010", "url": "https://en.wikipedia.org/wiki?curid=17686010", "title": "Birnbaum–Orlicz space", "text": "Birnbaum–Orlicz space\n\nIn the mathematical analysis, and especially in real and harmonic analysis, a Birnbaum–Orlicz space is a type of function space which generalizes the L spaces. Like the L spaces, they are Banach spaces. The spaces are named for Władysław Orlicz and Zygmunt William Birnbaum, who first defined them in 1931.\n\nBesides the \"L\" spaces, a variety of function spaces arising naturally in analysis are Birnbaum–Orlicz spaces. One such space \"L\" log \"L\", which arises in the study of Hardy–Littlewood maximal functions, consists of measurable functions \"f\" such that the integral\n\nHere log is the positive part of the logarithm. Also included in the class of Birnbaum–Orlicz spaces are many of the most important Sobolev spaces.\n\nSuppose that μ is a σ-finite measure on a set \"X\", and Φ : [0, ∞) → [0, ∞) is a Young function, i.e., a convex function such that\n\nLet formula_4 be the set of measurable functions \"f\" : \"X\" → R such that the integral\n\nis finite, where, as usual, functions that agree almost everywhere are identified.\n\nThis \"may not be\" a vector space (it may fail to be closed under scalar multiplication). The vector space of functions spanned by formula_4 is the Birnbaum–Orlicz space, denoted formula_7.\n\nTo define a norm on formula_7, let Ψ be the Young complement of Φ; that is,\n\nNote that Young's inequality for products holds:\n\nThe norm is then given by\n\nFurthermore, the space formula_7 is precisely the space of measurable functions for which this norm is finite.\n\nAn equivalent norm is defined on L by\n\nand likewise L(μ) is the space of all measurable functions for which this norm is finite.\n\nHere is an example where formula_4 is not a vector space and is strictly smaller than formula_7.\nSuppose that \"X\" is the open unit interval (0,1), Φ(\"x\") = exp(\"x\") – 1 – \"x\", and \"f\"(\"x\") = log(\"x\"). Then \"af\" is in the space formula_7 but is only in the set formula_4 if |\"a\"| < 1.\n\n\nCertain Sobolev spaces are embedded in Orlicz spaces: for formula_22 open and bounded with Lipschitz boundary formula_23,\n\nfor\n\nThis is the analytical content of the Trudinger inequality: For formula_22 open and bounded with Lipschitz boundary formula_23, consider the space formula_28, formula_29. There exist constants formula_30 such that\n\nSimilarly, the Orlicz norm of a random variable characterizes it as follows:\n\nThis norm is homogeneous and is defined only when this set is non-empty.\n\nWhen formula_33, this coincides with the \"p\"-th moment of the random variable. Other special cases in the exponential family are taken with respect to the functions formula_34 (for formula_35). A random variable with finite formula_36 norm is said to be \"sub-Gaussian\" and a random variable with finite formula_37 norm is said to be \"sub-exponential\". Indeed, the boundedness of the formula_38 norm characterizes the limiting behavior of the probability density function:\n\nso that the tail of this probability density function asymptotically resembles, and is bounded above by formula_40.\n\nThe formula_37 norm may be easily computed from a strictly monotonic moment-generating function. For example, the moment-generating function of a chi-squared random variable X with K degrees of freedom is formula_42, so that the inverse of the formula_37 norm is related to the functional inverse of the moment-generating function:\n\n"}
{"id": "530786", "url": "https://en.wikipedia.org/wiki?curid=530786", "title": "Codebook", "text": "Codebook\n\nA codebook is a type of document used for gathering and storing codes. Originally codebooks were often literally books, but today codebook is a byword for the complete record of a series of codes, regardless of physical format.\n\nIn cryptography, a codebook is a document used for implementing a code. A codebook contains a lookup table for coding and decoding; each word or phrase has one or more strings which replace it. To decipher messages written in code, corresponding copies of the codebook must be available at either end. The distribution and physical security of codebooks presents a special difficulty in the use of codes, compared to the secret information used in ciphers, the key, which is typically much shorter.\n\nThe United States National Security Agency documents sometimes use \"codebook\" to refer to block ciphers; compare their use of \"combiner-type algorithm\" to refer to stream ciphers.\n\nA codebook is usually made in two parts, one part being for converting plaintext to ciphertext, the other for the opposite purpose. Both are usually organized similar to a standard dictionary, with plaintext words (in the first part) and ciphertext words (in the second part) presented like dictionary headwords.\n\nThe book used in a book cipher or the book used in a running key cipher is generally not a cryptographic codebook.\n\nIn social sciences, a codebook is a document containing list of codes used in research.\n\nCodebooks were also used in 19th- and 20th-century commercial codes for the non-cryptographic purpose of data compression.\n\n"}
{"id": "5984147", "url": "https://en.wikipedia.org/wiki?curid=5984147", "title": "Cohn's irreducibility criterion", "text": "Cohn's irreducibility criterion\n\nArthur Cohn's irreducibility criterion is a sufficient condition for a polynomial to be irreducible in formula_1—that is, for it to be unfactorable into the product of lower-degree polynomials with integer coefficients.\n\nThe criterion is often stated as follows:\n\nThe theorem can be generalized to other bases as follows:\n\nThe base-10 version of the theorem is attributed to Cohn by Pólya and Szegő in one of their books while the generalization to any base, 2 or greater, is due to Brillhart, Filaseta, and Odlyzko.\n\nIn 2002, Ram Murty gave a simplified proof as well as some history of the theorem in a paper that is available online.\n\nThe converse of this criterion is that, if \"p\" is an irreducible polynomial with integer coefficients that have greatest common divisor 1, then there exists a base such that the coefficients of \"p\" form the representation of a prime number in that base; this is the Bunyakovsky conjecture and its truth or falsity remains an open question.\n\n\n"}
{"id": "34437104", "url": "https://en.wikipedia.org/wiki?curid=34437104", "title": "Conformal welding", "text": "Conformal welding\n\nIn mathematics, conformal welding (sewing or gluing) is a process in geometric function theory for producing a Riemann surface by joining together two Riemann surfaces, each with a disk removed, along their boundary circles. This problem can be reduced to that of finding univalent holomorphic maps \"f\", \"g\" of the unit disk and its complement into the extended complex plane, both admitting continuous extensions to the closure of their domains, such that the images are complementary Jordan domains and such that on the unit circle they differ by a given quasisymmetric homeomorphism. Several proofs are known using a variety of techniques, including the Beltrami equation, the Hilbert transform on the circle and elementary approximation techniques. describe the first two methods of conformal welding as well as providing numerical computations and applications to the analysis of shapes in the plane.\n\nThis method was first proposed by .\n\nIf \"f\" is a diffeomorphism of the circle, the Alexander extension gives a way of extending \"f\" to a diffeomorphism of the unit disk \"D\":\n\nwhere ψ is a smooth function with values in [0,1], equal to 0 near 0 and 1 near 1, and\n\nwith \"g\"(θ + 2π) = \"g\"(θ) + 2π.\n\nThe extension \"F\" can be continued to any larger disk |\"z\"| < \"R\" with \"R\" > 1. Accordingly in the unit disc\n\nNow extend μ to a Beltrami coefficient on the whole of C by setting it equal to 0 for |\"z\"| ≥ 1. Let \"G\" be the corresponding solution of the Beltrami equation:\n\nLet \"F\"(\"z\") = \"G\" ∘ \"F\"(\"z\") for |\"z\"| ≤ 1 and\n\"F\"(\"z\") = \"G\" (\"z\") for |\"z\"| ≥ 1. Thus \"F\" and \"F\" are univalent holomorphic maps of |\"z\"| < 1 and |\"z\"| > 1 onto the inside and outside of a Jordan curve. They extend continuously to homeomorphisms \"f\" of the unit circle onto the Jordan curve on the boundary. By construction they satisfy the\nconformal welding condition:\n\nThe use of the Hilbert transform to establish conformal welding was first suggested by the Georgian mathematicians D.G. Mandzhavidze and B.V. Khvedelidze in 1958. A detailed account was given at the same time by F.D. Gakhov and presented in his classic monograph ().\n\nLet \"e\"(θ) = \"e\" be the standard orthonormal basis of L(T). Let H(T) be Hardy space, the closed subspace spanned by the \"e\" with \"n\" ≥ 0. Let \"P\" be the orthogonal projection onto Hardy space and set \"T\" = 2\"P\" - \"I\". The operator \"H\" = \"iT\" is the Hilbert transform on the circle and can be written as a singular integral operator.\n\nGiven a diffeomorphism \"f\" of the unit circle, the task is to determine two univalent holomorphic functions\n\ndefined in |z| < 1 and |z| > 1 and both extending smoothly to the unit circle, mapping onto a Jordan domain and its complement, such that\n\nLet \"F\" be the restriction of \"f\" to the unit circle. Then\n\nand\n\nHence\n\nIf \"V\"(\"f\") denotes the bounded invertible operator on L induced by the diffeomorphism \"f\", then the operator\n\nis compact, indeed it is given by an operator with smooth kernel because \"P\" and \"T\" are given by singular integral operators. The equation above then reduces to\n\nThe operator \"I\" − \"K\" is a Fredholm operator of index zero. It has zero kernel and is therefore invertible. \nIn fact an element in the kernel would consist of a pair of holomorphic functions on \"D\" and \"D\" which have smooth boundary values on the circle related by \"f\". Since the holomorphic function on \"D\" vanishes at ∞, the positive powers of this pair also provide solutions, which are linearly independent, contradicting the fact that \"I\" − \"K\" is a Fredholm operator. The above equation therefore has a unique solution \"F\" which is smooth and from which \"f\" can be reconstructed by reversing the steps above. Indeed, by looking at the equation satisfied by the logarithm of the derivative of \"F\", it follows that \"F\" has nowhere vanishing derivative on the unit circle. Moreover \"F\" is one-to-one on the circle since if it assumes the value \"a\" at different points \"z\" and \"z\" then the logarithm of \"R\"(\"z\") = (\"F\"(\"z\") − \"a\")/(\"z\" - \"z\")(\"z\" − \"z\") would satisfy an integral equation known to have no non-zero solutions. Given these properties on the unit circle, the required properties of \"f\" then follow from the argument principle.\n\n"}
{"id": "2407999", "url": "https://en.wikipedia.org/wiki?curid=2407999", "title": "Demetrios Christodoulou", "text": "Demetrios Christodoulou\n\nDemetrios Christodoulou (; born October 19, 1951) is a Greek mathematician and physicist, who first became well known for his proof, together with Sergiu Klainerman, of the nonlinear stability of the Minkowski spacetime\nof special relativity in the framework of general relativity.\n\nChristodoulou was born in Athens and received his doctorate in physics from Princeton University in 1971 under the direction of John Archibald Wheeler. After temporary positions at Caltech, CERN, and the Max Planck Institute for Physics, he became Professor of Mathematics, first at Syracuse University, then at the Courant Institute, and at Princeton University, before taking up his last position as Professor of Mathematics and Physics at the ETH Zurich in Switzerland. He is Emeritus Professor since January 2017. He holds dual Greek and U.S. citizenship.\n\nIn 1993, he published a book coauthored with Klainerman in which the extraordinarily difficult proof of the stability result is laid out in detail. In that year, he was named a MacArthur Fellow. In 1991, he published a paper which shows that the test masses of a gravitational wave detector suffer permanent relative displacements after the passage of a gravitational wave train, an effect which has been named \"nonlinear memory effect\". In the period 1987–1999 he published a series of papers on the gravitational collapse of a spherically symmetric self-gravitating scalar field and the formation of black holes and associated spacetime singularities. He also showed that, contrary to what had been expected, singularities which are not hidden in a black hole also occur. However, he then showed that such \"naked singularities\" are unstable. \nIn 2000, Christodoulou published a book on general systems of partial differential equations deriving from a variational principle (or \"action principle\"). In 2007, he published a book on the formation of shock waves in 3-dimensional fluids. In 2009 he published a book where a result which complements the stability result is proved. Namely, that a sufficiently strong flux of incoming gravitational waves leads to the formation of a black hole.\n\nChristodoulou is a recipient of the Bôcher Memorial Prize, a prestigious award of the American Mathematical Society. The Bôcher Prize citation mentions his work on the spherically symmetric scalar field as well as his work on the stability of Minkowski spacetime. In 2008 he was awarded the Tomalla prize in gravitation. In 2011, he and Richard S. Hamilton won the Shaw Prize in the Mathematical Sciences, \"for their highly innovative works on nonlinear partial differential equations in Lorentzian and Riemannian geometry and their applications to general relativity and topology\". The citation for Christodoulou mentions his work on the formation of black holes by gravitational waves as well as his earlier work on the spherically symmetric self-gravitating scalar field and his work with Klainerman on the stability of Minkowski spacetime. Christodoulou is a member of the American Academy of Arts and Sciences and of the U.S. National Academy of Sciences. In 2012 he became a fellow of the American Mathematical Society. In 2014 he was a plenary speaker at the ICM in Seoul. Since 2016 is also a member of the Academia Europaea.\n\n\n"}
{"id": "396116", "url": "https://en.wikipedia.org/wiki?curid=396116", "title": "Dominator (graph theory)", "text": "Dominator (graph theory)\n\nIn computer science, in control flow graphs, a node d \"dominates\" a node n if every path from the \"entry node\" to n must go through d. Notationally, this is written as d dom n (or sometimes d formula_1 n). By definition, every node dominates itself.\n\nThere are a number of related concepts:\n\n\nDominance was first introduced by Reese T. Prosser in a 1959 paper on analysis of flow diagrams. Prosser did not present an algorithm for computing dominance, which had to wait ten years for Edward S. Lowry and C. W. Medlock. Ron Cytron \"et al.\" rekindled interest in dominance in 1989 when they applied it to the problem of efficiently computing the placement of φ functions, which are used in static single assignment form.\n\nDominators, and dominance frontiers particularly, have applications in compilers for computing static single assignment form. A number of compiler optimizations can also benefit from dominators. The flow graph in this case comprises basic blocks.\n\nAutomatic parallelization benefits from postdominance frontiers. This is an efficient method of computing control dependence, which is critical to the analysis.\n\nMemory usage analysis can benefit from the dominator tree to easily find leaks and identify high memory usage.\n\nIn hardware systems, dominators are used for computing signal probabilities for test generation, estimating switching activities for power and noise analysis, and selecting cut points in equivalence checking.\nIn software systems, they are used for reducing the size of the test set in structural testing techniques such as statement and branch coverage.\n\nThe dominators of a node n are given by the maximal solution to the following data-flow equations:\n\nwhere formula_4 is the start node.\n\nThe dominator of the start node is the start node itself. The set of dominators for any other node n is the intersection of the set of dominators for all predecessors p of n. The node n is also in the set of dominators for n.\n\nAn algorithm for direct solution is:\n\nDirect solution is quadratic in the number of nodes, or O(\"n\"). Lengauer and Tarjan developed an algorithm which is almost linear, and in practice, except for a few artificial graphs, the algorithm and a simplified version of it are as fast or faster than any other known algorithm for graphs of all sizes and its advantage increases with graph size.\n\nKeith D. Cooper, Timothy J. Harvey, and Ken Kennedy of Rice University describe an algorithm that essentially solves the above data flow equations but uses well engineered data structures to improve performance.\n\nAnalogous to the definition of dominance above, a node \"z\" is said to post-dominate a node \"n\" if all paths to the exit node of the graph starting at \"n\" must go through \"z\". Similarly, the immediate post-dominator of a node \"n\" is the postdominator of \"n\" that doesn't strictly postdominate any other strict postdominators of \"n\".\n\n\n"}
{"id": "440018", "url": "https://en.wikipedia.org/wiki?curid=440018", "title": "Duck typing", "text": "Duck typing\n\nDuck typing in computer programming is an application of the duck test—\"If it walks like a duck and it quacks like a duck, then it must be a duck\"—to determine if an object can be used for a particular purpose. With normal typing, suitability is determined by an object's type. In duck typing, an object's suitability is determined by the presence of certain methods and properties, rather than the type of the object itself. \n\nThis is a simple example in Python 3 that demonstrates how any object may be used in any context, up until it is used in a way that it does not support.\nclass Duck:\n\nclass Airplane:\n\nclass Whale:\n\ndef lift_off(entity):\n\nduck = Duck()\nairplane = Airplane()\nwhale = Whale()\n\nlift_off(duck) # prints `Duck flying`\nlift_off(airplane) # prints `Airplane flying`\nlift_off(whale) # Throws the error `'Whale' object has no attribute 'fly'`\nCertain usually statically typed languages such as Boo and the version 4 release of C# have extra type annotations\n\nTemplates in C++ allow the language to use compile-time duck typing. \n\nDuck typing is similar to, but distinct from structural typing. Structural typing is a static typing system that determines type compatibility and equivalence by a type's structure, whereas duck typing is dynamic and determines type compatibility by only that part of a type's structure that is accessed during run time.\n\nThe OCaml, Scala, Go, Elm, Gosu and PureScript languages support structural typing to varying degrees.\n\nProtocols and interfaces can provide some of the benefits of duck typing, but duck typing is distinct in that no explicit interface is defined. For example, if a third party library implements a class that cannot be modified, a client cannot use an instance of it with an interface unknown to that library even if the class does in fact satisfy the interface requirements. (A common solution to this problem is the Adapter pattern.) Duck typing would allow this. Again, all of an interface must be satisfied for compatibility.\n\nTemplate, or generic functions or methods apply the duck test in a static typing context; this brings all the advantages and disadvantages of static versus dynamic type checking in general. Duck typing can also be more flexible in that only the methods \"actually called at run time\" need to be implemented, while templates require implementation of all methods that \"cannot be proven unreachable at compile time\".\n\nExamples include the languages C++ and D with templates, which developed from Ada generics.\n\nA possible advantage of duck typing over inheritance is used in the API design for scikit-learn. There the interface dictates how the estimator functions are defined and for adding new functions you do not have to inherit any classes. There duck typing helps in decoupling the code from the API design hence making it easier for others to contribute to and extend the existing library. \n\nUse of the term \"duck typing\" has been considered superfluous in light of the fact that other terms, such as dynamic binding, express the concept more clearly. To proponents of static type checking, duck typing suggests the \"absence\" of typing, making its incorporation of the term \"typing\" appear incoherent.\n\n"}
{"id": "25373662", "url": "https://en.wikipedia.org/wiki?curid=25373662", "title": "Estrada index", "text": "Estrada index\n\nIn chemical graph theory, the Estrada index is a topological index of protein folding. The index was first defined by Ernesto Estrada as a measure of the degree of folding of a protein, which is represented as a path-graph weighted by the dihedral or torsional angles of the protein backbone. This index of degree of folding has found multiple applications in the study of protein functions and protein-ligand interactions.\n\nThe name of this index as the “Estrada index” was proposed by de la Peña et al. in 2007.\n\nLet formula_1 be a graph of size formula_2 and let formula_3 be a non-increasing ordering of the eigenvalues of its adjacency matrix formula_4. The Estrada index is defined as\n\nFor a general graph, the index can be obtained as the sum of the subgraph centralities of all nodes in the graph. The subgraph centrality of node formula_6 is defined as\n\nThe subgraph centrality has the following closed form\n\nwhere formula_9 is the formula_6 th entry of the formula_11th eigenvector associated with the eigenvalue formula_12. It is straightforward to realise that\n"}
{"id": "1527871", "url": "https://en.wikipedia.org/wiki?curid=1527871", "title": "F. Thomson Leighton", "text": "F. Thomson Leighton\n\nFrank Thomson \"Tom\" Leighton (born 1956) is the CEO of Akamai Technologies, the company he co-founded with Daniel Lewin in 1998. As one of the world's preeminent authorities on algorithms for network applications and cybersecurity, Dr. Leighton discovered a solution to freeing up web congestion using applied mathematics and distributed computing.\n\nHe is on leave as a professor of Applied Mathematics and a member of the Computer Science and Artificial Intelligence Laboratory (CSAIL) at the Massachusetts Institute of Technology (MIT). He received his B.S.E. in Electrical Engineering from Princeton University in 1978, and his Ph.D. in Mathematics from MIT in 1981. His brother David T. Leighton is a full professor at the University of Notre Dame, specializing in transport phenomena. Their father was a U.S. Navy colleague and friend of Admiral Hyman G. Rickover, a founder of the Research Science Institute (RSI).\n\nDr. Leighton has served on numerous government, industry and academic advisory panels, including the Presidential Informational Technology Advisory Committee (PITAC) and chaired its subcommittee on cybersecurity. He serves on the Board of Trustees of the Society for Science & the Public (SSP) and of the Center for Excellence in Education (CEE), and he has participated in the Distinguished Lecture Series at CEE's flagship program for high school students, the Research Science Institute (RSI). \n\nIn 2018, Leighton won the Marconi Prize from the Marconi Society for \"his fundamental contributions to the technology and establishment of content delivery networks\" . In 2017, Leighton and Lewin were inducted into the National Inventors Hall of Fame, for Content Delivery Network methods. He was the first winner of the Machtey Award in 1981 and is a Fellow of the American Academy of Arts and Sciences and a member of the National Academy of Engineering. In 2008, he was appointed as a member of the United States National Academy of Sciences. In 2012 he became a fellow of the American Mathematical Society. He received the IEEE Computer Society Charles Babbage Award in 2001.\n\n\n"}
{"id": "37657137", "url": "https://en.wikipedia.org/wiki?curid=37657137", "title": "Faltings's product theorem", "text": "Faltings's product theorem\n\nIn arithmetic geometry, the Faltings's product theorem gives sufficient conditions for a subvariety of a product of projective spaces to be a product of varieties in the projective spaces. It was introduced by in his proof of Lang's conjecture that subvarieties of an abelian variety containing no translates of non-trivial abelian subvarieties have only finitely many rational points.\n\n"}
{"id": "33765759", "url": "https://en.wikipedia.org/wiki?curid=33765759", "title": "Gell-Mann and Low theorem", "text": "Gell-Mann and Low theorem\n\nThe Gell-Mann and Low theorem is a theorem in quantum field theory that allows one to relate the ground (or vacuum) state of an interacting system to the ground state of the corresponding non-interacting theory. It was proved in 1951 by Murray Gell-Mann and Francis E. Low. The theorem is useful because, among other things, by relating the ground state of the interacting theory to its non-interacting ground state, it allows one to express Green's functions (which are defined as expectation values of Heisenberg-picture fields in the interacting vacuum) as expectation values of interaction picture fields in the non-interacting vacuum. While typically applied to the ground state, the Gell-Mann and Low theorem applies to any eigenstate of the Hamiltonian. Its proof relies on the concept of starting with a non-interacting Hamiltonian and adiabatically switching on the interactions.\n\nThe theorem was proved first by Gell-Mann and Low in 1951, making use of the Dyson series. In 1969 Klaus Hepp provided an alternative derivation for the case where the original Hamiltonian describes free particles and the interaction is norm bounded. In 1989 Nenciu and Rasche proved it using the adiabatic theorem. A proof that does not rely on the Dyson expansion was given in 2007 by Molinari.\n\nLet formula_1 be an eigenstate of formula_2 with energy formula_3 and let the 'interacting' Hamiltonian be formula_4, where formula_5 is a coupling constant and formula_6 the interaction term. We define a Hamiltonian formula_7 which effectively interpolates between formula_8 and formula_2 in the limit formula_10 and formula_11. Let formula_12 denote the evolution operator in the interaction picture. The Gell-Mann and Low theorem asserts that if the limit as formula_13 of \n\nexists, then formula_15 are eigenstates of formula_8.\n\nNote that when applied to, say, the ground-state, the theorem does not guarantee that the evolved state will be a ground state. In other words, level crossing is not excluded.\n\nAs in the original paper, the theorem is typically proved making use of Dyson's expansion of the evolution operator. Its validity however extends beyond the scope of perturbation theory as has been demonstrated by Molinari. We follow Molinari's method here. Focus on formula_17 and let formula_18. From Schrödinger's equation for the time-evolution operator \n\nand the boundary condition formula_20 we can formally write\n\nFocus for the moment on the case formula_22. Through a change of variables formula_23 we can write \n\nWe therefore have that\n\nThis result can be combined with the Schrödinger equation and its adjoint\n\nto obtain \n\nThe corresponding equation between formula_28 is the same. It can be obtained by pre-multiplying both sides with formula_29, post-multiplying with formula_30 and making use of \n\nThe other case we are interested in, namely formula_32 can be treated in an analogous fashion\nand yields an additional minus sign in front of the commutator (we are not concerned here with the case where\nformula_33 have mixed signs). In summary, we obtain\n\nWe proceed for the negative-times case. Abbreviating the various operators for clarity \n\nNow using the definition of formula_36 we differentiate and eliminate derivatives formula_37 using the above expression, finding\n\nwhere formula_39. We can now let formula_13 as by assumption the formula_41 in left hand side is finite. We then clearly see that formula_42 is an eigenstate of formula_8 and the proof is complete.\n\n1. M. Gell-Mann and F. Low: \"Bound States in Quantum Field Theory\", Phys. Rev. 84, 350 (1951)\n\n2. K. Hepp: Lecture Notes in Physics (Springer-Verlag, New York, 1969), Vol. 2.\n\n3. G. Nenciu and G. Rasche: \"Adiabatic theorem and Gell-Mann-Low formula\", Helv. Phys. Acta 62, 372 (1989).\n\n4. L.G. Molinari: \"Another proof of Gell-Mann and Low's theorem\", J. Math. Phys. 48, 052113 (2007)\n\n5. A.L. Fetter and J.D. Walecka: \"Quantum Theory of Many-Particle Systems\", McGraw–Hill (1971)\n"}
{"id": "56067604", "url": "https://en.wikipedia.org/wiki?curid=56067604", "title": "Higher-order operad", "text": "Higher-order operad\n\nIn algebra, a higher-order operad is a higher-dimensional generalization of an operad.\n\n\n"}
{"id": "10671359", "url": "https://en.wikipedia.org/wiki?curid=10671359", "title": "James Franklin (philosopher)", "text": "James Franklin (philosopher)\n\nJames Franklin (born 1953 in Sydney) is an Australian philosopher, mathematician and historian of ideas.\n\nFranklin was educated at St. Joseph's College, Hunters Hill, New South Wales. His undergraduate work was at the University of Sydney (1971–74), where he attended St John's College and he was influenced by philosophers David Stove and David Armstrong. He completed his PhD in 1981 at the University of Warwick, on algebraic groups. Since 1981 he has taught in the School of Mathematics and Statistics at the University of New South Wales.\n\nHis research areas include the philosophy of mathematics and the 'formal sciences', the history of probability, Australian Catholic history, the parallel between ethics and mathematics (work for which he received the 2005 Eureka Prize for Research in Ethics), restraint, the quantification of rights in applied ethics, and the analysis of extreme risk. Franklin is the literary executor of David Stove.\n\nHis 2001 book, \"The Science of Conjecture: Evidence and Probability Before Pascal\", covered the development of thinking about uncertain evidence over many centuries up to 1650. Its central theme was ancient and medieval work on the law of evidence, which developed concepts like half-proof, similar to modern proof beyond reasonable doubt, as well as analyses of aleatory contracts like insurance and gambling.\n\nHis polemical history of Australian philosophy, \"Corrupting the Youth\" (2003), praised the Australian realist tradition in philosophy and attacked postmodernist and relativist trends.\n\nIn the philosophy of mathematics, he defends an Aristotelian realist theory, according to which mathematics is about certain real features of the world, namely the quantitative and structural features (such as ratios and symmetry). The theory stands in opposition to both Platonism and nominalism, and emphasises applied mathematics and mathematical modelling as the most philosophically central parts of mathematics. He is the founder of the Sydney School in the philosophy of mathematics.\n\nIn 2008 he set up the Australian Database of Indigenous Violence.\n\nHe is the editor of the \"Journal of the Australian Catholic Historical Society\".\n\nFranklin has written several books and articles:\n\nArticles (a selection):\n\n\n"}
{"id": "56118470", "url": "https://en.wikipedia.org/wiki?curid=56118470", "title": "Joachim Schwermer", "text": "Joachim Schwermer\n\nJoachim Schwermer (26 May 1950, Kulmbach) is a German mathematician, specializing in number theory.\n\nSchwermer received his \"Abitur\" in 1969 at Aloisiuskolleg in Bad Godesberg and then studied mathematics at the University of Bonn. After graduating in 1974 with his \"Diplom\", he received in 1977 his \"Promotion\" (Ph.D.) underi Günter Harder with thesis \"Eisensteinreihen und die Kohomologie von Kongruenzuntergruppen von formula_1\". In 1982 he received his \"Habilitation\" from the University of Bonn. From 1986 he was a professor at the Catholic University of Eichstätt-Ingolstadt, then at the University of Düsseldorf, and finally in the 2000s at the University of Vienna. During the academic year 1980–1981 Schwermer was a visiting scholar at the Institute for Advanced Study. In 1987 he was awarded the Gay-Lussac-Humboldt-Prize.\n\nSchwermer's research deals with algebraic groups in number theory, arithmetic geometry, Lie groups, and L-functions. He has written essays on the history of mathematics, for example, about Helmut Hasse, Hermann Minkowski, and Emil Artin.\n\nHe is now a professor at the University of Vienna as well as the scientific director at the Erwin Schrödinger International Institute for Mathematical Physics.\n\nIn June 2016, the Max Planck Institute for Mathematics held a Conference on the Cohomology of Arithmetic Groups on the occasion of Joachim Schwermer's 66th birthday.\n\n\n"}
{"id": "1547599", "url": "https://en.wikipedia.org/wiki?curid=1547599", "title": "Joseph L. Doob", "text": "Joseph L. Doob\n\nJoseph Leo \"Joe\" Doob (February 27, 1910 – June 7, 2004) was an American mathematician, specializing in analysis and probability theory.\n\nThe theory of martingales was developed by Doob.\n\nDoob was born in Cincinnati, Ohio, February 27, 1910, the son of Leo Doob and Mollie Doerfler Doob. The family moved to New York City before he was three years old. The parents felt that he was underachieving in grade school and placed him in the Ethical Culture School, from which he graduated in 1926. He then went on to Harvard where he received a BA in 1930, an MA in 1931, and a PhD (\"Boundary Values of Analytic Functions\", advisor Joseph L. Walsh) in 1932. After postdoctoral research at Columbia and Princeton, he joined the Department of Mathematics of the University of Illinois in 1935 and served until his retirement in 1978. He was a member of the Urbana campus's Center for Advanced Study from its beginning in 1959. During the Second World War, he worked in Washington, D. C. and Guam as a civilian consultant to the Navy from 1942 to 1945; he was at the Institute for Advanced Study for the academic year 1941–1942 when Oswald Veblen approached him to work on mine warfare for the Navy.\n\nDoob's thesis was on boundary values of analytic functions. He published two papers based on this thesis, which appeared in 1932 and 1933 in the Transactions of the American Mathematical Society. Doob returned to this subject many years later when he proved a probabilistic version of Fatou's boundary limit theorem for harmonic functions.\n\nThe Great Depression of 1929 was still going strong in the thirties and Doob could not find a job. B.O. Koopman at Columbia University suggested that statistician Harold Hotelling might have a grant that would permit Doob to work with him. Hotelling did, so the Depression led Doob to probability.\n\nIn 1933 Kolmogorov provided the first axiomatic foundation for the theory of probability. Thus a subject that had originated from intuitive ideas suggested by real life experiences and studied informally, suddenly became mathematics. Probability theory became measure theory with its own problems and terminology. Doob recognized that this would make it possible to give rigorous proofs for existing probability results, and he felt that the tools of measure theory would lead to new probability results.\n\nDoob's approach to probability was evident in his first probability paper, in which he proved theorems related to the law of large numbers, using a probabilistic interpretation of Birkhoff's ergodic theorem. Then he used these theorems to give rigorous proofs of theorems proven by Fisher and Hotelling related to Fisher's maximum likelihood estimator for estimating a parameter of a distribution.\n\nAfter writing a series of papers on the foundations of probability and stochastic processes including martingales, Markov processes, and stationary processes, Doob realized that there was a real need for a book showing what is known about the various types of stochastic processes, so he wrote the book \"Stochastic Processes\". It was published in 1953 and soon became one of the most influential books in the development of modern probability theory.\n\nBeyond this book, Doob is best known for his work on martingales and probabilistic potential theory. After he retired, Doob wrote a book of over 800 pages: \"Classical Potential Theory and Its Probabilistic Counterpart\". The first half of this book deals with classical potential theory and the second half with probability theory, especially martingale theory. In writing this book, Doob shows that his two favorite subjects, martingales and potential theory, can be studied by the same mathematical tools.\n\nThe American Mathematical Society's Joseph L. Doob Prize, endowed in 2005 and awarded every three years for an outstanding mathematical book, is named in Doob's honor.\n\n\n\n\n"}
{"id": "8075001", "url": "https://en.wikipedia.org/wiki?curid=8075001", "title": "K-set (geometry)", "text": "K-set (geometry)\n\nIn discrete geometry, a \"k\"-set of a finite point set \"S\" in the Euclidean plane is a subset of \"k\" elements of \"S\" that can be strictly separated from the remaining points by a line. More generally, in Euclidean space of higher dimensions, a \"k\"-set of a finite point set is a subset of \"k\" elements that can be separated from the remaining points by a hyperplane. In particular, when \"k\" = \"n\"/2 (where \"n\" is the size of \"S\"), the line or hyperplane that separates a \"k\"-set from the rest of \"S\" is a halving line or halving plane.\n\n\"K\"-sets are related by projective duality to \"k\"-levels in line arrangements; the \"k\"-level in an arrangement of \"n\" lines in the plane is the curve consisting of the points that lie on one of the lines and have exactly \"k\" lines below them. Discrete and computational geometers have also studied levels in arrangements of more general kinds of curves and surfaces.\n\nIt is of importance in the analysis of geometric algorithms to bound the number of \"k\"-sets of a planar point set, or equivalently the number of \"k\"-levels of a planar line arrangement, a problem first studied by Lovász (1971) and Erdős et al. (1973). The best known upper bound for this problem is \"O\"(\"nk\"), as was shown by Tamal Dey (1998) using the crossing number inequality of Ajtai, Chvátal, Newborn, and Szemerédi. However, the best known lower bound is far from Dey's upper bound: it is Ω(\"n\" exp(\"c\" (log\"k\"))) for some constant \"c\", as shown by Toth (2001).\n\nIn three dimensions, the best upper bound known is \"O\"(\"nk\"), and the best lower bound known is Ω(\"nk\" exp(\"c\" (log\"k\"))).\nFor points in three dimensions that are in convex position, that is, are the vertices of some convex polytope, the number of k-sets is \nΘ(\"(n-k)k\"), which follows from arguments used for bounding the complexity of k-th order Voronoi diagrams.\n\nFor the case when \"k\" = \"n\"/2 (halving lines), the maximum number of combinatorially distinct lines through two points of \"S\" that bisect the remaining points when \"k\" = 1, 2, ... is\n\nBounds have also been proven on the number of ≤\"k\"-sets, where a ≤\"k\"-set is a \"j\"-set for some \"j\" ≤ \"k\". In two dimensions, the maximum number of ≤\"k\"-sets is exactly \"nk\", while in \"d\" dimensions the bound is formula_1.\n\nEdelsbrunner and Welzl (1986) first studied the problem of constructing all \"k\"-sets of an input point set, or dually of constructing the \"k\"-level of an arrangement. The \"k\"-level version of their algorithm can be viewed as a plane sweep algorithm that constructs the level in left-to-right order. Viewed in terms of \"k\"-sets of point sets, their algorithm maintains a dynamic convex hull for the points on each side of a separating line, repeatedly finds a bitangent of these two hulls, and moves each of the two points of tangency to the opposite hull. Chan (1999) surveys subsequent results on this problem, and shows that it can be solved in time proportional to Dey's \"O\"(\"nk\") bound on the complexity of the \"k\"-level.\n\nAgarwal and Matoušek describe algorithms for efficiently constructing an approximate level; that is, a curve that passes between the (\"k\" - \"d\")-level and the (\"k\" + \"d\")-level for some small approximation parameter \"d\". They show that such an approximation can be found, consisting of a number of line segments that depends only on \"n\"/\"d\" and not on \"n\" or \"k\".\n\nThe planar \"k\"-level problem can be generalized to one of parametric optimization in a matroid: one is given a matroid in which each element is weighted by a linear function of a parameter λ, and must find the minimum weight basis of the matroid for each possible value of λ. If one graphs the weight functions as lines in a plane, the \"k\"-level of the arrangement of these lines graphs as a function of λ the weight of the largest element in an optimal basis in a uniform matroid, and Dey showed that his \"O\"(\"nk\") bound on the complexity of the \"k\"-level could be generalized to count the number of distinct optimal bases of any matroid with \"n\" elements and rank \"k\".\n\nFor instance, the same \"O\"(\"nk\") upper bound holds for counting the number of different minimum spanning trees formed in a graph with \"n\" edges and \"k\" vertices, when the edges have weights that vary linearly with a parameter λ. This parametric minimum spanning tree problem has been studied by various authors and can be used to solve other bicriterion spanning tree optimization problems.\n\nHowever, the best known lower bound for the parametric minimum spanning tree problem is Ω(\"n\" α(\"k\")), where α is the inverse Ackermann function, an even weaker bound than that for the \"k\"-set problem. For more general matroids, Dey's \"O\"(\"nk\") upper bound has a matching lower bound.\n\n"}
{"id": "58350478", "url": "https://en.wikipedia.org/wiki?curid=58350478", "title": "Kakutani's theorem (measure theory)", "text": "Kakutani's theorem (measure theory)\n\nIn measure theory, a branch of mathematics, Kakutani's theorem is a fundamental result on the equivalence or mutual singularity of countable product measures. It gives an “if and only if” characterisation of when two such measures are equivalent, and hence it is extremely useful when trying to establish change-of-measure formulae for measures on function spaces. The result is due to the Japanese mathematician Shizuo Kakutani. Kakutani's theorem can be used, for example, to determine whether a translate of a Gaussian measure formula_1 is equivalent to formula_1 (only when the translation vector lies in the Cameron–Martin space of formula_1), or whether a dilation of formula_1 is equivalent to formula_1 (only when the absolute value of the dilation factor is 1, which is part of the Feldman–Hájek theorem).\n\nFor each formula_6, let formula_7 and formula_8 be measures on the real line formula_9, and let formula_10 and formula_11 be the corresponding product measures on formula_12. Suppose also that, for each formula_6, formula_14 and formula_15 are equivalent (i.e. have the same null sets). Then either formula_1 and formula_17 are equivalent, or else they are mutually singular. Furthermore, equivalence holds precisely when the infinite product\n\nconverges; or, equivalently, when the infinite series\n\nconverges.\n\n"}
{"id": "49494375", "url": "https://en.wikipedia.org/wiki?curid=49494375", "title": "List of things named after Vladimir Arnold", "text": "List of things named after Vladimir Arnold\n\nA list of things named after Vladimir Arnold, a Russian and Soviet mathematician.\n\n\n"}
{"id": "350672", "url": "https://en.wikipedia.org/wiki?curid=350672", "title": "Logicism", "text": "Logicism\n\nLogicism is a programme in the philosophy of mathematics, comprising one or more of the theses that — for some coherent meaning of 'logic' — mathematics is an extension of logic, some or all of mathematics is reducible to logic, or some or all of mathematics may be modelled in logic. Bertrand Russell and Alfred North Whitehead championed this programme, initiated by Gottlob Frege and subsequently developed by Richard Dedekind, Giuseppe Peano and Russell.\n\nDedekind's path to logicism had a turning point when he was able to construct a model satisfying the axioms defining the real numbers using certain sets of rational numbers. This and related ideas convinced him that arithmetic, algebra and analysis were reducible to the natural numbers plus a \"logic\" of sets. Furthermore by 1872 he had concluded that the naturals themselves were reducible to sets and mappings. It is likely that other logicists, most importantly Frege, were also guided by the new theories of the real numbers published in the year 1872. \n\nThe philosophical impetus behind Frege's logicist programme from the Grundlagen der Arithmetik onwards was in part his dissatisfaction with the epistemological and ontological commitments of then-extant accounts of the natural numbers, and his conviction that Kant's use of truths about the natural numbers as examples of synthetic a priori truth was incorrect.\n\nThis started a period of expansion for logicism, with Dedekind and Frege as its main exponents. However, this initial phase of the logicist programme was brought into crisis with the discovery of the classical paradoxes of set theory (Cantor 1896, Zermelo and Russell 1900–1901). Frege gave up on the project after Russell recognized and communicated his paradox identifying an inconsistency in Frege's system set out in the Grundgesetze der Arithmetik. Note that naive set theory also suffers from this difficulty. \n\nOn the other hand, Russell wrote \"The Principles of Mathematics\" in 1903 using the paradox and developments of Giuseppe Peano's school of geometry. Since he treated the subject of primitive notions in geometry and set theory, this text is a watershed in the development of logicism. Evidence of the assertion of logicism was collected by Russell and Whitehead in their \"Principia Mathematica\".\n\nToday, the bulk of extant mathematics is believed to be derivable logically from a small number of extralogical axioms, such as the axioms of Zermelo–Fraenkel set theory (or its extension ZFC), from which no inconsistencies have as yet been derived. Thus, elements of the logicist programmes have proved viable, but in the process theories of classes, sets and mappings, and higher-order logics other than with Henkin semantics, have come to be regarded as extralogical in nature, in part under the influence of Quine's later thought.\n\nKurt Gödel's incompleteness theorems show that no formal system from which the Peano axioms for the natural numbers may be derived — such as Russell's systems in PM — can decide all the well-formed sentences of that system.. This result damaged Hilbert's programme for foundations of mathematics whereby 'infinitary' theories — such as that of PM — were to be proved consistent from finitary theories, with the aim that those uneasy about 'infinitary methods' could be reassurred that their use should provably not result in the derivation of a contradiction. Gödel's result suggests that in order to maintain a logicist position, while still retaining as much as possible of classical mathematics, one must accept some axiom of infinity as part of logic. On the face of it, this damages the logicist programme also, albeit only for those already doubtful concerning 'infinitary methods'. Nonetheless, positions deriving from both logicism and from Hilbertian finitism have continued to be propounded since the publication of Gödel's result.\n\nSome believe programmes derived from logicism remain valid because the incompleteness theorems are proved with logic just like any other theorems. However, that conclusion fails to acknowledge any distinction between theorems of first-order logic and those of higher-order logic. The former can be proven using the fundamental theorem of arithmetic (see Gödel numbering), while the latter must rely on human-provided models. Tarski's undefinability theorem shows that Gödel numbering can be used to prove syntactical constructs, but not semantic assertions. Therefore, the claim that logicism remains a valid programme may commit one to holding that a system of proof based on the existence and properties of the natural numbers is less convincing than one based on some particular formal system.\n\nLogicism — especially through the influence of Frege on Russell and Wittgenstein\n\n\n"}
{"id": "5378206", "url": "https://en.wikipedia.org/wiki?curid=5378206", "title": "Malcev algebra", "text": "Malcev algebra\n\nIn mathematics, a Malcev algebra (or Maltsev algebra or Moufang–Lie algebra) over a field is a nonassociative algebra that is antisymmetric, so that\n\nand satisfies the Malcev identity\n\nThey were first defined by Anatoly Maltsev (1955).\n\nMalcev algebras play a role in the theory of Moufang loops that generalizes the role of Lie algebras in the theory of groups. Namely, just as the tangent space of the identity element of a Lie group forms a Lie algebra, the tangent space of the identity of a smooth Moufang loop forms a Malcev algebra. Moreover, just as a Lie group can be recovered from its Lie algebra under certain supplementary conditions, a smooth Moufang loop can be recovered from its Malcev algebra if certain supplementary conditions hold. For example, this is true for a connected, simply connected real-analytic Moufang loop.\n\n\n\n"}
{"id": "15512992", "url": "https://en.wikipedia.org/wiki?curid=15512992", "title": "Mathmakers", "text": "Mathmakers\n\nMathmakers was a Canadian educational children's television series produced in 1978 by the province of Ontario's public television network, TVOntario. The series starred Derek McGrath and Lyn Harvey.\n\nProducer/Director Clive Vanderburgh, Production Assistant Jane Downey and Brian Elston, Editor.\n\nThe premise is set in a television studio where the production crew produces an educational series illustrating various concepts of grade school mathematics.\n\n"}
{"id": "2178847", "url": "https://en.wikipedia.org/wiki?curid=2178847", "title": "Michael Resnik", "text": "Michael Resnik\n\nMichael David Resnik (; born March 20, 1938) is a leading contemporary American philosopher of mathematics.\n\nResnick obtained his B.A. in mathematics and philosophy at Yale University in 1960, and his PhD in Philosophy at Harvard University in 1964. He wrote his thesis on Frege. He was appointed Associate Professor at the University of North Carolina at Chapel Hill in 1967, Professor in 1975, and University Distinguished Professor in 1988. He is Professor Emeritus of University of North Carolina at Chapel Hill and currently resides in rural Chatham County, North Carolina.\n\n\n\n"}
{"id": "7500394", "url": "https://en.wikipedia.org/wiki?curid=7500394", "title": "Minkowski–Steiner formula", "text": "Minkowski–Steiner formula\n\nIn mathematics, the Minkowski–Steiner formula is a formula relating the surface area and volume of compact subsets of Euclidean space. More precisely, it defines the surface area as the \"derivative\" of enclosed volume in an appropriate sense.\n\nThe Minkowski–Steiner formula is used, together with the Brunn–Minkowski theorem, to prove the isoperimetric inequality. It is named after Hermann Minkowski and Jakob Steiner.\n\nLet formula_1, and let formula_2 be a compact set. Let formula_3 denote the Lebesgue measure (volume) of formula_4. Define the quantity formula_5 by the Minkowski–Steiner formula\n\nwhere\n\ndenotes the closed ball of radius formula_8, and\n\nis the Minkowski sum of formula_4 and formula_11, so that\n\nFor \"sufficiently regular\" sets formula_4, the quantity formula_5 does indeed correspond with the formula_15-dimensional measure of the boundary formula_16 of formula_4. See Federer (1969) for a full treatment of this problem.\n\nWhen the set formula_4 is a convex set, the lim-inf above is a true limit, and one can show that\n\nwhere the formula_20 are some continuous functions of formula_4 (see quermassintegrals) and formula_22 denotes the measure (volume) of the unit ball in formula_23:\n\nwhere formula_25 denotes the Gamma function.\n\nTaking formula_26 gives the following well-known formula for the surface area of the sphere of radius formula_27, formula_28:\n\nwhere formula_22 is as above.\n\n"}
{"id": "4737124", "url": "https://en.wikipedia.org/wiki?curid=4737124", "title": "NAR 1", "text": "NAR 1\n\nNAR 1 or just NAR (Serbian Nastavni Računar, en. \"Educational Computer\") was a theoretical model of a computer created by Faculty of Mathematics of University of Belgrade professor Nedeljko Parezanović (In Serbian:Недељко Парезановић). It was used for Assembly language and Computer architecture courses.\n\nNAR 1 processor has a 5-bit address bus (32 bytes of addressable memory) and 8-bit data bus. Machine instructions were single-byte with three most significant bits specifying the opcode and 5 least significant bits the parameter - memory address. A single 8-bit accumulator register was available and there were no flags or flag registers. Only absolute addressing mode was available and all others were achieved by self-modifying code. \n\nEven though this is only a theoretical computer the following physical characteristics were given:\n\n\n\nTwo more instructions were not specified but were commonly present in simulators and took instruction codes 000aaaaa and 111aaaaa:\n\nA sample program that sums up an array of 8-bit integers:\n\nAbove program adds up to 22 8-bit values if executed from address 22:\n\nNAR 1 programs are commonly self-modifying. Unlike in some other architectures, this is not a 'trick'. As memory can not be addressed by a register, the only way to dynamically manipulate memoory data is to modify memory manipulation instructions. Above example also contains a typical trick to save memory - instruction (at address 30) is reused as data by another instruction (at address 28).\n\nIf initial accumulator value can be controlled from the control pane, a 23rd value can be stored in it. Above program has to be only slightly modified - instruction SABF 1 at address 23 has to be changed to SABF 0 and the program should be executed from that address (23) and not from 22.\n\nOther tricks included the use of the changes of the sign after instruction is modified, as shown in the following example:\n\nHere the instruction \"MUA 21\" at address 23 has the binary value 10010101, which is -107 decimal when treated like signed integer in two's complement. Instructions at addresses 26, 27 and 28 decrement this value by 1 in each iteration. This will modify the 5 least significant bits specifying the address and will not touch the three bits indicating the instruction until that instruction becomes MUA 0 (10000000 binary = -128 decimal, negative). Once this is decremented by one it becomes 01111111 (+127 decimal) which is no longer negative and will cause the jump-if-negative instruction at 29 to pass, proceeding to \"stop the computer\" at 30.\n\nSimilarly to above, this program can add between 22 and 24 values, depending on whether address 22 can be used for both input and output and whether initial value of the accumulator can be used as input (the program should then be executed from address 24 and instruction at 23 should be MUA 22).\n\nIf particular implementation stops the computer if it encounters an unknown opcode or it implements additional unconditional jump instruction with opcode \"111aaaaa\", then such behaviour can be used as follows:\n\nAbove, the value of \"-1\" found at address 31 can either be treated as invalid instruction causing the computer to stop or as unconditional jump (BES 31) to the same address, resulting in infinite loop that does not affect the result (control panel can be used to display it).\n\nFinally, depending on whether it is decided that a computer will stop program execution if it reaches the end of memory (address 31, will not roll back to address 0), above program can be reorganized to take one value more by eliminating the need for \"stop the computer\" instruction altogether, as follows:\n\n\nProf.dr Nedeljko Parezanovic (in Serbian)\n"}
{"id": "8750942", "url": "https://en.wikipedia.org/wiki?curid=8750942", "title": "Numberjacks", "text": "Numberjacks\n\nNumberjacks is a British children's television series that originally aired on CBeebies on BBC Two in the UK between October 2006 and 2009, and on the CBeebies channel also. It was first shown on BBC Two at 10 am on 16 October 2006. It was then repeated on CBeebies at 11 am and 3 pm on the same day. BBC Two only showed it for a week due to the half term break and then the future episodes were shown on CBeebies at 11am. Re-runs of the episodes were shown regularly on CBeebies (when it was on BBC Two, it was only on occasionally, and when it first appeared, it was only on for a week) until October 2015. It was produced by Open Mind Productions for the BBC and features a mixture of computer-generated animation and live-action. The show has a swing of science fiction in it, as the Numberjacks take residence in a couch in a house, everything around them moves of its own accord, and humans must be scarce at launching time.\n\nThe Numberjacks are animated characters (the numbers 0 to 9) who live in an ordinary sofa and solve problems outside; each episode has the same structure. At the beginning of the episode, some of the Numberjacks are engaged in an activity, that would have relevance to the problem that is later discovered - then, an \"agent\" (who was a live-action child) calls in and describes the problem that is occurring. One (or two) of the Numberjacks go out into the real world to solve the problem while the other Numberjacks stay in the sofa and watch their progress on a screen. As soon as the problem is understood, Five imagines what else could go wrong if the problem wasn't solved (often wondering what would happen to the Dancing Cow, who never actually made any physical appearance in the real world).\n\nOnce outside, the Numberjacks are shown in a live-action setting (although themselves still animated); they diagnose their problems by examination, and with help from additional ideas from the agents, who call into the base. The problem can either be caused by one of the antagonists, or one of the younger Numberjacks (Zero, One, or Two) escaping from the sofa and inadvertently making things go wrong. Problems are solved by using \"Brain Gain\", a magical force of power activated by use of a machine in the sofa and transferred to the Numberjack.\n\nOnce the problem is solved, the Numberjacks return to their base, replay what happened on a screen, and then challenge the viewer to think about a related problem and \"call the Numberjacks\". The problems encountered are all based upon simple mathematical concepts, and the programme is intended to stimulate young children's interest in mathematics. On satellite, digital, and cable TV, a link to \"Numberjacks\" often appeared in the corner of the screen and sometimes on (for example) gardening programmes as a way of helping people with basic numeracy.\n\nThe \"Numberjacks\" are the heroes; each of the numbers 0 through 9 is an individual character and each is broadly the age of his or her number. The male Numberjacks are even numbers and the female Numberjacks are odd numbers - however, One is voiced by a \"male\" actor (Dylan Robertson, who also voices Zero).\n\nThere are five evil villains, the \"Meanies\", who often cause trouble for people; the Problem Blob, the Puzzler, and the Numbertaker are male villains, while the Shape Japer and Spooky Spoon are female villains. Spooky Spoon also seems to appear more often than any of the other Meanies.\n\n\nThe first series, consisting of 45 15-minute episodes, was premiered on BBC Two on 16 October 2006, while the second series, consisting of 20 15-minute episodes, was premiered on CBeebies in 2009; both \"specials\" were also exclusive to DVD.\n\nNumberjacks Are On Their Way (Volume 1) (Early issues of this volume have 3, 4 and 5 watching 6 on a TV and 2 in the Brain Gain on the cover)\n\nCalling All Agents! (Volume 2)\n\nStanding By To Zoom! (Volume 3) (This volume also contained a special free Christmas CD, which was released in December 2007)\n\nBrain Gain! (Volume 4) (Early issues of this volume have 5 in the centre of the cover while later reissues have 4 instead)\n\nCounting Down To Christmas! (Volume 5)\n\nSeaside Adventure (Volume 6)\n\n\n\n"}
{"id": "22072345", "url": "https://en.wikipedia.org/wiki?curid=22072345", "title": "Photon Doppler velocimetry", "text": "Photon Doppler velocimetry\n\nPhoton Doppler velocimetry (PDV) is a one-dimensional Fourier transform analysis of a heterodyne laser interferometry, used in the shock physics community to measure velocities in dynamic experiments with high temporal precision. PDV was developed at Lawrence Livermore National Laboratory by Strand. In recent years PDV has achieved popularity in the shock physics community as an adjunct or replacement for Velocity Interferometer System for Any Reflector (VISAR), another time-resolved velocity interferometry system. Modern data acquisition technology and off-the-shelf optical telecommunications devices now enable the assembly of PDV systems within reasonable budgets.\n\nThe fundamental mechanism of PDV is the interference pattern created by two electromagnetic waves with a small difference in frequency. Since most PDV systems are constructed with available telecommunications equipment, a standard laser source for a PDV system is centered at 1550 nm (or 193.4 THz). If this source is then reflected off of a moving surface with some velocity (formula_1), the reflected light will be shifted in frequency (formula_2) according to the relativistic Doppler shift equation.\n\nIf the shifted return light is then interfered with the original source, the resulting wave will have a beat frequency in the range of a few GHz. This beat frequency is slow enough that it can be monitored with a simple photo-detector and high speed oscilloscope. By recording the beat frequency over time, a complete velocity history of the surface is obtained.\n\nIn theory, the analysis of a PDV data signal is quite simple, where the apparent velocity of the moving surface (formula_4) is simply a function of the source wavelength (formula_5) and the signal frequency (formula_6):\n\nIn practice, however, determining the instantaneous frequency (formula_6) of the signal by inspection can be inaccurate and inefficient. Consequently, Fourier transform analysis is used to extract the most probable frequency components, which can then be used to calculate the velocity history.\n\nBy taking sequential FFTs over a time window which moves across the data signal, a 2D spectrogram can be created which indicates the frequency components most dominant in the data. The velocity history can then be extracted from the spectrogram.\n\nPDV can measure a wide range of velocities (limited primarily by the time resolution of the signal recording equipment), and is relatively easy to set up and use.\n\nDepending on the quality of the data signal and parameters of the FFT, the inherent error in PDV measurements can be high. However, there are ways to mitigate these problems and obtain velocity histories with very high accuracy.\n\nThe 2010 PDV users conference was held at the Ohio State University.\n\n"}
{"id": "4120642", "url": "https://en.wikipedia.org/wiki?curid=4120642", "title": "Q-exponential", "text": "Q-exponential\n\nIn combinatorial mathematics, a \"q\"-exponential is a \"q\"-analog of the exponential function,\nnamely the eigenfunction of a \"q\"-derivative. There are many \"q\"-derivatives, for example, the classical \"q\"-derivative, the Askey-Wilson operator, etc. Therefore, unlike the classical exponentials, \"q\"-exponentials are not unique. For example, formula_1 is the \"q\"-exponential corresponding to the classical \"q\"-derivative while formula_2 are eigenfunctions of the Askey-Wilson operators.\n\nThe \"q\"-exponential formula_1 is defined as\n\nwhere formula_5 is the \"q\"-factorial and \n\nis the \"q\"-Pochhammer symbol. That this is the \"q\"-analog of the exponential follows from the property\n\nwhere the derivative on the left is the \"q\"-derivative. The above is easily verified by considering the \"q\"-derivative of the monomial\n\nHere, formula_9 is the \"q\"-bracket.\nFor other definitions of the \"q\"-exponential function, see , , and .\n\nFor real formula_10, the function formula_1 is an entire function of formula_12. For formula_13, formula_1 is regular in the disk formula_15.\n\nNote the inverse, formula_16.\nIf formula_17, formula_18 holds.\nFor formula_19, a function that is closely related is formula_20 It is a special case of the basic hypergeometric series,\n\nClearly,\nformula_23 has the following infinite product representation:\nOn the other hand, formula_25 holds. \nWhen formula_26,\nBy taking the limit formula_28,\nwhere formula_30 is the dilogarithm.\n\n"}
{"id": "31020257", "url": "https://en.wikipedia.org/wiki?curid=31020257", "title": "R. Catesby Taliaferro", "text": "R. Catesby Taliaferro\n\nRobert Catesby Taliaferro (1907–1989) was an American mathematician, science historian, classical philologist, philosopher, and translator of ancient Greek and Latin works into English. An Episcopalian from an old Virginia family, he taught in the mathematics department of the University of Notre Dame. He is cited as R. Catesby Taliaferro or R. C. Taliaferro.\n\nHe translated from Greek and Latin into English: Ptolemy's \"Almagest\", a 2nd-century book on astronomy, the 13 books of Euclid's \"Elements\", Apollonius' works on conic sections, and some works of Plato (Timaios, Critias) , and St. Augustine (On Music). He contributed a celebrated foreword to the Bollingen Series 1944 reprint of the Thomas Taylor translation of Plato's \"Timaeus\" and \"Critias\". He also wrote a book titled \"The concept of matter in Descartes and Leibniz\" and one titled \"Number systems, introduction to Euclid book V, and to the theory of limits\".\n\nHe received his doctorate from the University of Virginia at Charlottesville in 1936. He was a teacher at St. John's College in Annapolis, and in 1948 became a Master at Portsmouth Priory, now Portsmouth Abbey School, in Portsmouth (Rhode Island). In 1956 he became an associate professor at the University of Notre Dame, after he was there Visiting Associate Professor.\n\n"}
{"id": "5623373", "url": "https://en.wikipedia.org/wiki?curid=5623373", "title": "Radix economy", "text": "Radix economy\n\nThe radix economy of a number in a particular base (or radix) is the number of digits needed to express it in that base, multiplied by the base (the number of possible values each digit could have). This is one of various proposals that have been made to quantify the relative costs of using different radices in representing numbers, especially in computer systems. \n\nRadix economy also has implications for organizational structure, networking, and other fields.\n\nThe radix economy \"E\"(\"b\",\"N\") for any particular number \"N\" in a given base \"b\" is defined as\n\nwhere we use the floor function formula_2 and the base-b logarithm formula_3.\n\nIf both \"b\" and \"N\" are positive integers, then the radix economy formula_4 is equal to the number of digits needed to express the number \"N\" in base \"b\", multiplied by base \"b\". The radix economy thus measures the cost of storing or processing the number \"N\" in base \"b\" if the cost of each \"digit\" is proportional to \"b\". A base with a lower average radix economy is therefore, in some senses, more efficient than a base with a higher average radix economy.\n\nFor example, 100 in decimal has three digits, so its radix economy is 10×3 = 30; its binary representation has seven digits (1100100) so it has radix economy 2×7 = 14 in base 2; in base 3 its representation has five digits (10201) with a radix economy of 3×5 = 15; in base 36 (2S) its radix economy is 36×2 = 72.\n\nIf the number is imagined to be represented by a combination lock or a tally counter, in which each wheel has \"b\" digit faces, from formula_5 and having formula_6 wheels, then the radix economy formula_7 is the total number of digit faces needed to inclusively represent any integer from 0 to \"N\" (assuming all wheels must have the same number of faces each).\n\nHere is a proof that \"e\" is the \"real\"-valued base with the lowest average radix economy:\n\nFirst, note that the function\n\nis strictly decreasing on 1 < \"x\" < \"e\" and strictly increasing on \"x\" > \"e\". Its minimum, therefore, for x > 1, occurs at \"e\".\n\nNext, consider that\n\nThen for a constant N, formula_10 will have a minimum at \"e\" for the same reason y does, meaning e is therefore the base with the lowest average radix economy. Since 2 / ln(2) ≈ 2.89 and 3 / ln(3) ≈ 2.73, it follows that 3 is the \"integer\" base with the lowest average radix economy.\n\nThe radix economy of bases \"b\" and \"b\" may be compared for a large value of \"N\":\n\nChoosing \"e\" for \"b\" gives the economy relative to that of \"e\" by the function:\n\nThe average radix economies of various bases up to several arbitrary numbers (avoiding proximity to powers of 2 through 12 and \"e\") are given in the table below. Also shown are the radix economies relative to that of \"e\". Note that the radix economy of any number in base 1 is that number, making it the most economical for the first few integers, but as \"N\" climbs to infinity so does its relative economy.\n\nOne result of the relative economy of base 3 is that ternary search trees offer an efficient strategy for retrieving elements of a database. A similar analysis suggests that the optimum design of a large telephone menu system to minimise the number of menu choices that the average customer must listen to (i.e. the product of the number of choices per menu and the number of menu levels) is to have three choices per menu.\n\nThe 1950 reference \"High-Speed Computing Devices\" describes a particular situation using contemporary technology. Each digit of a number would be stored as the state of a ring counter composed of several triodes. Whether vacuum tubes or thyratrons, the triodes were the most expensive part of a counter. For small radices \"r\" less than about 7, a single digit required \"r\" triodes. (Larger radices required 2\"r\" triodes arranged as \"r\" flip-flops, as in ENIAC's decimal counters.)\n\nSo the number of triodes in a numerical register with \"n\" digits was \"rn\". In order to represent numbers up to 10, the following numbers of tubes were needed:\n\nThe authors conclude,\nIn another application, the authors of \"High-Speed Computing Devices\" consider the speed with which an encoded number may be sent as a series of high-frequency voltage pulses. For this application the compactness of the representation is more important than in the above storage example. They conclude, \"A saving of 58 per cent can be gained in going from a binary to a ternary system. A smaller percentage gain is realized in going from a radix 3 to a radix 4 system.\"\n\nBinary encoding has a notable advantage over all other systems: greater noise immunity. Random voltage fluctuations are less likely to generate an erroneous signal, and circuits may be built with wider voltage tolerances and still represent unambiguous values accurately.\n\n\n\n"}
{"id": "1706360", "url": "https://en.wikipedia.org/wiki?curid=1706360", "title": "Rado's theorem (Ramsey theory)", "text": "Rado's theorem (Ramsey theory)\n\nRado's theorem is a theorem from the branch of mathematics known as Ramsey theory. It is named for the German mathematician Richard Rado. It was proved in his thesis, \"Studien zur Kombinatorik\".\n\nLet formula_1 be a system of linear equations, where formula_2 is a matrix with integer entries. This system is said to be formula_3\"-regular\" if, for every formula_3-coloring of the natural numbers 1, 2, 3, ..., the system has a monochromatic solution. A system is \"regular\" if it is \"r-regular\" for all \"r\" ≥ 1.\n\nRado's theorem states that a system formula_1 is regular if and only if the matrix \"A\" satisfies the \"columns condition\". Let \"c\" denote the \"i\"-th column of \"A\". The matrix \"A\" satisfies the columns condition provided that there exists a partition \"C\", \"C\", ..., \"C\" of the column indices such that if formula_6, then\n\n\nFolkman's theorem, the statement that there exist arbitrarily large sets of integers all of whose nonempty sums are monochromatic, may be seen as a special case of Rado's theorem concerning the regularity of the system of equations\nwhere \"T\" ranges over each nonempty subset of the set \n\nOther special cases of Rado's theorem are Schur's theorem and Van der Waerden's theorem.\n"}
{"id": "37065129", "url": "https://en.wikipedia.org/wiki?curid=37065129", "title": "Rafael Artzy", "text": "Rafael Artzy\n\nRafael Artzy (23 July 1912 – 22 August 2006) was an Israeli mathematician specializing in geometry.\n\nArtzy was born July 23, 1912, in Königsberg, Germany. His father was Edward I. Deutschlander and his mother Ida Freudenheim. Rafael studied at Königsberg University from 1930 to 1933. He transferred to Hebrew University and obtained a master’s degree in 1934. He married Elly Iwiansky on October 12, 1934. Rafael continued his studies at Hebrew University under Theodore Motzkin, obtaining a Ph.D. in 1945. Elly and Rafael raised three children: Ehud, Michal, and Barak. Ehud and Barak died before their father. Michal Artzy is emeritus professor in Marine Civilization at the University of Haifa.\n\nRafael served as both teacher and principal of Israel High School from 1934 to 1951. He was an instructor and assistant professor at the Israel Institute of Technology from 1951 to 1956.\n\nRafael Artzy took up a position as research associate and lecturer at University of Wisconsin, Madison in 1956. That year he also made his first of many contributions to Mathematical Reviews. Artzy became associate professor at University of North Carolina, Chapel Hill in 1960. The following year Rutgers University made him a full professor. In 1964 he was a visitor at the Institute for Advanced Study. He wrote \"Linear Geometry\" (1965) which was favorably reviewed by H. S. M. Coxeter In 1965 Artzy was at State University of New York in Buffalo. In 1967 he joined Temple University where he was for five years.\n\nIn 1972 Rafael Artzy returned to Israel and participated in mathematics at Technion in Haifa. He helped organize a quadrennial conference on geometry at Haifa. For instance, in March 1979 such a conference was held and the proceedings \"Geometry and Differential Geometry\" was edited by Artzy and I. Vaisman and published in Springer Lecture Notes as #792. In 1992 he published \"Geometry. An Algebraic Approach\" Artzy had made 224 contributions to \"Mathematical Reviews\" by his last submission in 1995.\n\n\n"}
{"id": "4783135", "url": "https://en.wikipedia.org/wiki?curid=4783135", "title": "Representation theorem", "text": "Representation theorem\n\nIn mathematics, a representation theorem is a theorem that states that every abstract structure with certain properties is isomorphic to another (abstract or concrete) structure. \n\nFor example, \n"}
{"id": "36633800", "url": "https://en.wikipedia.org/wiki?curid=36633800", "title": "Robbins' theorem", "text": "Robbins' theorem\n\nIn graph theory, Robbins' theorem, named after , states that the graphs that have strong orientations are exactly the 2-edge-connected graphs. That is, it is possible to choose a direction for each edge of an undirected graph , turning into a directed graph that has a path from every vertex to every other vertex, if and only if is connected and has no bridge.\n\nRobbins' characterization of the graphs with strong orientations may be proven using ear decomposition, a tool introduced by Robbins for this task.\n\nIf a graph has a bridge, then it cannot be strongly orientable, for no matter which orientation is chosen for the bridge there will be no path from one of the two endpoints of the bridge to the other.\n\nIn the other direction, it is necessary to show that every connected bridgeless graph can be strongly oriented. As Robbins proved, every such graph has a partition into a sequence of subgraphs called \"ears\", in which the first subgraph in the sequence is a cycle and each subsequent subgraph is a path, with the two path endpoints both belonging to earlier ears in the sequence. Orienting the edges within each ear so that it forms a directed cycle or a directed path leads to a strongly connected orientation of the overall graph.\n\nAn extension of Robbins' theorem to mixed graphs by shows that, if is a graph in which some edges may be directed and others undirected, and contains a path respecting the edge orientations from every vertex to every other vertex, then any undirected edge of that is not a bridge may be made directed without changing the connectivity of . In particular, a bridgeless undirected graph may be made into a strongly connected directed graph by a greedy algorithm that directs edges one at a time while preserving the existence of paths between every pair of vertices; it is impossible for such an algorithm to get stuck in a situation in which no additional orientation decisions can be made.\n\nA strong orientation of a given bridgeless undirected graph may be found in linear time by performing a depth first search of the graph, orienting all edges in the depth first search tree away from the tree root, and orienting all the remaining edges (which must necessarily connect an ancestor and a descendant in the depth first search tree) from the descendant to the ancestor. Although this algorithm is not suitable for parallel computers, due to the difficulty of performing depth first search on them, alternative algorithms are available that solve the problem efficiently in the parallel model. Parallel algorithms are also known for finding strongly connected orientations of mixed graphs.\n\n"}
{"id": "55675948", "url": "https://en.wikipedia.org/wiki?curid=55675948", "title": "Sequential linear-quadratic programming", "text": "Sequential linear-quadratic programming\n\nSequential linear-quadratic programming (SLQP) is an iterative method for nonlinear optimization problems where objective function and constraints are twice continuously differentiable. Similarly to sequential quadratic programming (SQP), SLQP proceeds by solving a sequence of optimization subproblems. The difference between the two approaches is that:\n\n\nThis decomposition makes SLQP suitable to large-scale optimization problems, for which efficient LP and EQP solvers are available, these problems being easier to scale than full-fledge quadratic programs.\n\nConsider a nonlinear programming problem of the form:\n\nThe Lagrangian for this problem is\nwhere formula_3 and formula_4 are Lagrange multipliers. \n\nIn the LP phase of SLQP, the following linear program is solved:\n\nLet formula_6 denote the \"active set\" at the optimum formula_7 of this problem, that is to say, the set of constraints that are equal to zero at formula_7. Denote by <math>b_"}
{"id": "35864002", "url": "https://en.wikipedia.org/wiki?curid=35864002", "title": "Siegel identity", "text": "Siegel identity\n\nIn mathematics, Siegel's identity refers to one of two formulae that are used in the resolution of Diophantine equations.\n\nThe first formula is\n\nThe second is\n\nThe identities are used in translating Diophantine problems connected with integral points on hyperelliptic curves into S-unit equations.\n\n\n"}
{"id": "24952083", "url": "https://en.wikipedia.org/wiki?curid=24952083", "title": "SoaML", "text": "SoaML\n\nSoaML (Service-oriented architecture Modeling Language ) is an open source specification project from the Object Management Group (OMG), describing a UML profile and metamodel for the modeling and design of services within a service-oriented architecture.\n\nSoaML has been created to support the following modeling capabilities:\n\nThe existing models and meta models (e.g. TOGAF) for describing system architectures turned out to be insufficient to describe SOA in a precise and standardized way. The UML itself seems to be too general for the purpose of describing SOA and needed clarification and standardization of even basic terms like provider, consumer, etc.\n\n\n\n\"Notes\"\n\n\"Citations\"\n"}
{"id": "2696560", "url": "https://en.wikipedia.org/wiki?curid=2696560", "title": "Substring", "text": "Substring\n\nA substring is a contiguous sequence of characters within a string. For instance, \"\"the best of\" is a substring of \"It was the best of times\". This is not to be confused with subsequence, which is a generalization of substring. For example, \"Itwastimes\" is a subsequence of \"It was the best of times\", but not a substring.\n\nPrefix and suffix are special cases of substring. A prefix of a string formula_1 is a substring of formula_1 that occurs at the \"beginning\" of formula_1. A suffix of a string formula_1 is a substring that occurs at the \"end\" of formula_1.\n\nThe list of all substrings of the string \"apple\" would be \"apple\", \"appl\", \"pple\", \"app\", \"ppl\", \"ple\", \"ap\", \"pp\", \"pl\", \"le\", \"a\", \"p\", \"l\", \"e\"\", \"\".\n\nA substring (or factor) of a string formula_6 is a string formula_7, where formula_8 and formula_9. A substring of a string is a prefix of a suffix of the string, and equivalently a suffix of a prefix. If formula_10 is a substring of formula_11, it is also a subsequence, which is a more general concept. Given a pattern formula_12, you can find its occurrences in a string formula_11 with a string searching algorithm. Finding the longest string which is equal to a substring of two or more strings is known as the longest common substring problem.\n\nExample: The string codice_1 is equal to substrings (and subsequences) of codice_2 at two different offsets:\n\nIn the mathematical literature, substrings are also called subwords (in America) or factors (in Europe).\n\nNot including the empty substring, the number of substrings of a string of length formula_14 where symbols only occur once, is the number of ways to choose two distinct places between symbols to start/end the substring. Including the very beginning and very end of the string, there are formula_15 such places. So there are formula_16 non-empty substrings.\n\nA prefix of a string formula_6 is a string formula_18, where formula_19. A \"proper prefix\" of a string is not equal to the string itself (formula_20); some sources in addition restrict a proper prefix to be non-empty (formula_21). A prefix can be seen as a special case of a substring.\n\nExample: The string codice_3 is equal to a prefix (and substring and subsequence) of the string codice_2:\n\nThe square subset symbol is sometimes used to indicate a prefix, so that formula_22 denotes that formula_23 is a prefix of formula_11. This defines a binary relation on strings, called the prefix relation, which is a particular kind of prefix order.\n\nIn formal language theory, the term \"prefix of a string\" is also commonly understood to be the set of all prefixes of a string, with respect to that language.\n\nA suffix of a string is any substring of the string which includes its last letter, including itself. A \"proper suffix\" of a string is not equal to the string itself. A more restricted interpretation is that it is also not empty. A suffix can be seen as a special case of a substring.\n\nExample: The string codice_5 is equal to a suffix (and substring and subsequence) of the string codice_2:\n\nA suffix tree for a string is a trie data structure that represents all of its suffixes. Suffix trees have large numbers of applications in string algorithms. The suffix array is a simplified version of this data structure that lists the start positions of the suffixes in alphabetically sorted order; it has many of the same applications.\n\nA border is suffix and prefix of the same string, e.g. \"bab\" is a border of \"babab\" (and also of \"babooneatingakebab\").\n\nGiven a set of formula_25 strings formula_26, a superstring of the set formula_12 is a single string that contains every string in formula_12 as a substring. For example, a concatenation of the strings of formula_12 in any order gives a trivial superstring of formula_12. For a more interesting example, let formula_31. Then formula_32 is a superstring of formula_12, and formula_34 is another, shorter superstring of formula_12. Generally, we are interested in finding superstrings whose length is small.\n\n"}
{"id": "8641870", "url": "https://en.wikipedia.org/wiki?curid=8641870", "title": "Sylvester's determinant identity", "text": "Sylvester's determinant identity\n\nIn matrix theory, Sylvester's determinant identity is an identity useful for evaluating certain types of determinants. It is named after James Joseph Sylvester, who stated this identity without proof in 1851.\n\nThe identity states that if and are matrices of size and respectively, then\n\nwhere is the identity matrix of order .\n\nIt is closely related to the Matrix determinant lemma and its generalization. It is the determinant analogue of the Woodbury matrix identity for matrix inverses.\n\nThe identity may be proved as follows. Let be a matrix comprising the four blocks , , , and :\n\nBecause is invertible, the formula for the determinant of a block matrix gives\n\nBecause is invertible, the formula for the determinant of a block matrix gives\n\nThus\n\nThis identify is useful in developing a Bayes estimator for multivariate Gaussian distributions.\n\nThe identity also finds applications in random matrix theory by relating determinants of large matrices to determinants of smaller ones.\n"}
{"id": "365715", "url": "https://en.wikipedia.org/wiki?curid=365715", "title": "Timothy Gowers", "text": "Timothy Gowers\n\nSir William Timothy Gowers, (; born 20 November 1963) is a British mathematician. He is a Royal Society Research Professor at the Department of Pure Mathematics and Mathematical Statistics at the University of Cambridge, where he also holds the Rouse Ball chair, and is a Fellow of Trinity College, Cambridge. In 1998, he received the Fields Medal for research connecting the fields of functional analysis and combinatorics.\n\nGowers attended King's College School, Cambridge, as a choirboy in the King's College choir, and then Eton College as a King's Scholar. He completed his PhD, with a dissertation entitled \"Symmetric Structures in Banach Spaces,\" at Trinity College, Cambridge in 1990, supervised by Béla Bollobás.\n\nAfter his PhD, Gowers was elected to a Junior Research Fellowship at Trinity College. From 1991 until his return to Cambridge in 1995 he was lecturer at University College London. He was elected to the Rouse Ball Professorship at Cambridge in 1998. During 2000–2 he was visiting professor at Princeton University.\n\nGowers initially worked on Banach spaces. He used combinatorial tools in proving several of Stefan Banach's conjectures in the subject, in particular constructing a Banach space with almost no symmetry, serving as a counterexample to several other conjectures. With Bernard Maurey he resolved the \"unconditional basic sequence problem\" in 1992, showing that not every infinite-dimensional Banach space has an infinite-dimensional subspace that admits an unconditional Schauder basis.\n\nAfter this, Gowers turned to combinatorics and combinatorial number theory. In 1997 he proved that the Szemerédi regularity lemma necessarily comes with tower-type bounds.\n\nIn 1998 he proved the first effective bounds for Szemerédi's theorem, showing that any subset formula_1 free of \"k\"-term arithmetic progressions has cardinality formula_2 for an appropriate formula_3. One of the ingredients in Gowers's argument is a tool now known as the Balog–Szemerédi–Gowers theorem, which has found many further applications. He also introduced the Gowers norms, a tool in arithmetic combinatorics, and provided the basic techniques for analysing them. This work was further developed by Ben Green and Terence Tao, leading to the Green–Tao theorem.\n\nIn 2003, Gowers established a regularity lemma for hypergraphs, analogous to the Szemerédi regularity lemma for graphs.\n\nIn 2005, he introduced the notion of a quasirandom group.\n\nMore recently Gowers has worked on Ramsey theory in random graphs and random sets with David Conlon, and has turned his attention to other problems such as the P versus NP problem. He has also developed an interest, in joint work with Mohan Ganesalingam, in automated problem solving.\n\nGowers has an Erdős number of three. \n\nIn 1996 he received the Prize of the European Mathematical Society, and in 1998 the Fields Medal for research on functional analysis and combinatorics. In 1999 he became a Fellow of the Royal Society and in 2012 was knighted by the British monarch for services to mathematics. He also sits on the selection committee for the Mathematics award, given under the auspices of the Shaw Prize.\n\nGowers has written several works popularising mathematics, including \"Mathematics: A Very Short Introduction\" (2002), which describes modern mathematical research for the general reader. He was consulted about the 2005 film \"Proof\", starring Gwyneth Paltrow and Anthony Hopkins. He edited \"The Princeton Companion to Mathematics\" (2008), which traces the development of various branches and concepts of modern mathematics. For his work on this book, he won the 2011 Euler Book Prize of the Mathematical Association of America.\n\nAfter asking on his blog whether \"massively collaborative mathematics\" was possible, he solicited comments on his blog from people who wanted to try to solve mathematical problems collaboratively. The first problem in what is called the Polymath Project, Polymath1, was to find a new combinatorial proof to the density version of the Hales–Jewett theorem. After 7 weeks, Gowers wrote on his blog that the problem was \"probably solved\".\n\nIn 2009, with Olof Sisask and Alex Frolkin, he invited people to post comments to his blog to contribute to a collection of methods of mathematical problem solving. Contributors to this Wikipedia-style project, called Tricki.org, include Terence Tao and Ben Green.\n\nIn 2012, Gowers posted to his blog to call for a boycott of the publishing house Elsevier. A petition ensued, branded the Cost of Knowledge project, in which researchers commit to stop supporting Elsevier journals. Commenting on the petition in \"The Guardian\", Alok Jha credited Gowers with starting an Academic Spring.\n\nIn 2016, Gowers started Discrete Analysis to demonstrate that a high-quality mathematics journal could be inexpensively produced outside of the traditional academic publishing industry.\n\nGowers's father was Patrick Gowers, a composer; his great-grandfather was Sir Ernest Gowers, a British civil servant who was best known for guides to English usage; and his great-great-grandfather was Sir William Gowers, a neurologist. He has five children and plays jazz piano.\n\nIn November 2012, Gowers opted to undergo catheter ablation to treat a sporadic atrial fibrillation, after performing a mathematical risk–benefit analysis to decide whether to have the treatment.\n\n\n"}
{"id": "1647421", "url": "https://en.wikipedia.org/wiki?curid=1647421", "title": "Trigonal planar molecular geometry", "text": "Trigonal planar molecular geometry\n\nIn chemistry, trigonal planar is a molecular geometry model with one atom at the center and three atoms at the corners of an equilateral triangle, called peripheral atoms, all in one plane. In an ideal trigonal planar species, all three ligands are identical and all bond angles are 120°. Such species belong to the point group D. Molecules where the three ligands are not identical, such as HCO, deviate from this idealized geometry. Examples of molecules with trigonal planar geometry include boron trifluoride (BF), formaldehyde (HCO), phosgene (COCl), and sulfur trioxide (SO). Some ions with trigonal planar geometry include nitrate (), carbonate (), and guanidinium (). In organic chemistry, planar, three-connected carbon centers that are trigonal planar are often described as having sp hybridization.\n\nNitrogen inversion is the distortion of pyramidal amines through a transition state that is trigonal planar.\n\nPyramidalization is a distortion of this molecular shape towards a tetrahedral molecular geometry. One way to observe this distortion is in pyramidal alkenes.\n\n\n"}
{"id": "30871820", "url": "https://en.wikipedia.org/wiki?curid=30871820", "title": "United States of America Mathematical Talent Search", "text": "United States of America Mathematical Talent Search\n\nThe United States of America Mathematical Talent Search (USAMTS) is a mathematics competition open to all United States students in or below high school. \n\nProfessor George Berzsenyi initiated the contest in 1989 under the KöMaL model and under joint sponsorship of the Rose-Hulman Institute of Technology and the Consortium for Mathematics and its Applications.\n\nAs of 2004, the USAMTS is sponsored by the National Security Agency and administered by the Art of Problem Solving foundation. There were 718 participants in the 2004-2005 school year, with an average score of 49.25 out of 100.\n\nThe competition is proof and research based. Students submit proofs within the round's timeframe (usually a month), and return solutions by mail or upload their solutions in a PDF file through the USAMTS website. During this time, students are free to use any mathematical resources that are available, so long as it is not the help of another person. Carefully written justifications are required for each problem.\n\nPrior to academic year 2010–2011 the competition consisted of four rounds of five problems each, covering all non-calculus topics. Students were given approximately one month to solve the questions. Each question is scored out of five points; thus, a perfect score is formula_1.\n\nIn the academic year 2010-2011, the USAMTS changed their format to two rounds of six problems each, and approximately six weeks are allotted for each round.\n\nThe current format consists of three problem sets, each 5 problems and lasting about a month each. Every question is still worth 5 points making a perfect score formula_2.\n\nThe graders also submit suggestions on possible improvements to the students' solutions with the scores.\n\nPrizes are given to all contestants who place within a certain range. These prizes include a shirt from AoPS, software, and one or two mathematical books of varying difficulty. Prizes are also awarded to students with outstanding solutions in individual rounds. Further, after the third round, given a high enough score, a student may qualify to take the AIME exam instead of qualifying through the AMC 10 or 12 competitions.\n\n"}
{"id": "9331665", "url": "https://en.wikipedia.org/wiki?curid=9331665", "title": "Visibility (geometry)", "text": "Visibility (geometry)\n\nVisibility in geometry is a mathematical abstraction of the real-life notion of visibility.\n\nGiven a set of obstacles in the Euclidean space, two points in the space are said to be visible to each other, if the line segment that joins them does not intersect any obstacles. (In the Earth's atmosphere light follows a slightly curved path that is not perfectly predictable, complicating the calculation of actual visibility.)\n\nComputation of visibility is among the basic problems in computational geometry and has applications in computer graphics, motion planning, and other areas.\n\n\n\n"}
{"id": "1247265", "url": "https://en.wikipedia.org/wiki?curid=1247265", "title": "Volume integral", "text": "Volume integral\n\nIn mathematics—in particular, in multivariable calculus—a volume integral refers to an integral over a 3-dimensional domain, that is, it is a special case of multiple integrals. Volume integrals are especially important in physics for many applications, for example, to calculate flux densities.\n\nIt can also mean a triple integral within a region \"D\" in R of a function formula_1 and is usually written as:\n\nA volume integral in cylindrical coordinates is\n\nand a volume integral in spherical coordinates (using the ISO convention for angles with formula_4 as the azimuth and formula_5 measured from the polar axis (see more on conventions)) has the form\n\nIntegrating the function formula_7 over a unit cube yields the following result:\n\nformula_8\n\nSo the volume of the unit cube is 1 as expected. This is rather trivial however, and a volume integral is far more powerful. For instance if we have a scalar function formula_9 describing the density of the cube at a given point formula_10 by formula_11 then performing the volume integral will give the total mass of the cube:\n\nformula_12\n\n\n"}
{"id": "28412014", "url": "https://en.wikipedia.org/wiki?curid=28412014", "title": "Voronoi pole", "text": "Voronoi pole\n\nIn geometry, the positive and negative Voronoi poles of a cell in a Voronoi diagram are certain vertices of the diagram.\n\nLet formula_1 be the Voronoi cell of the site formula_2. If formula_1 is bounded then its \"positive pole\" is the Voronoi vertex in formula_1 with maximal distance to the sample point formula_5. Furthermore, let formula_6 be the vector from formula_5 to the positive pole. If the cell is unbounded, then a positive pole is not defined, and formula_6 is defined to be a vector in the average direction of all unbounded Voronoi edges of the cell.\n\nThe \"negative pole\" is the Voronoi vertex formula_9 in formula_1 with the largest distance to formula_5 such that the vector formula_6 and the vector from formula_5 to formula_9 make an angle larger than formula_15.\n\nHere formula_16 is the positive pole of formula_1 and formula_18 its negative. As the cell corresponding to formula_19 is unbounded only the negative pole formula_20 exists.\n"}
{"id": "33274", "url": "https://en.wikipedia.org/wiki?curid=33274", "title": "Weighted arithmetic mean", "text": "Weighted arithmetic mean\n\nThe weighted arithmetic mean is similar to an ordinary arithmetic mean (the most common type of average), except that instead of each of the data points contributing equally to the final average, some data points contribute more than others. The notion of weighted mean plays a role in descriptive statistics and also occurs in a more general form in several other areas of mathematics.\n\nIf all the weights are equal, then the weighted mean is the same as the arithmetic mean. While weighted means generally behave in a similar fashion to arithmetic means, they do have a few counterintuitive properties, as captured for instance in Simpson's paradox.\n\nGiven two school classes, one with 20 students, and one with 30 students, the grades in each class on a test were:\n\nThe straight average for the morning class is 80 and the straight average of the afternoon class is 90. The straight average of 80 and 90 is 85, the mean of the two class means. However, this does not account for the difference in number of students in each class (20 versus 30); hence the value of 85 does not reflect the average student grade (independent of class). The average student grade can be obtained by averaging all the grades, without regard to classes (add all the grades up and divide by the total number of students):\n\nOr, this can be accomplished by weighting the class means by the number of students in each class (using a weighted mean of the class means):\n\nThus, the weighted mean makes it possible to find the average student grade in the case where only the class means and the number of students in each class are available.\n\nSince only the \"relative\" weights are relevant, any weighted mean can be expressed using coefficients that sum to one. Such a linear combination is called a convex combination.\n\nUsing the previous example, we would get the following weights:\n\nThen, apply the weights like this: \n\nFormally, the weighted mean of a non-empty set of data\n\nwith non-negative weights\n\nwhich means:\n\nTherefore, data elements with a high weight contribute more to the weighted mean than do elements with a low weight. The weights cannot be negative. Some may be zero, but not all of them (since division by zero is not allowed).\n\nThe formulas are simplified when the weights are normalized such that they sum up to formula_9, i.e.:\nFor such normalized weights the weighted mean is then:\nNote that one can always normalize the weights by making the following transformation on the original weights:\nUsing the normalized weight yields the same results as when using the original weights:\nThe ordinary mean formula_14 is a special case of the weighted mean where all data have equal weights, formula_15. \n\nThe \"standard error of the weighted mean (unit input variances)\", formula_16 can be shown via uncertainty propagation to be:\n\nThe weighted sample mean, formula_18, is itself a random variable. Its expected value and standard deviation are related to the expected values and standard deviations of the observations, as follows. For simplicity, we assume normalized weights (weights summing to one). \n\nIf the observations have expected values \nthen the weighted sample mean has expectation\nIn particular, if the means are equal, formula_21, then the expectation of the weighted sample mean will be that value,\n\nFor uncorrelated observations with variances formula_23, the variance of the weighted sample mean is \nwhose square root formula_25 can be called the \"standard error of the weighted mean (general case)\".\n\nConsequently, if all the observations have equal variance, formula_26, the weighted sample mean will have variance\nwhere formula_28. The variance attains its maximum value, formula_29, when all weights except one are zero. Its minimum value is found when all weights are equal (i.e., unweighted mean), in which case we have formula_30, i.e., it degenerates into the standard error of the mean, squared.\n\nNote that because one can always transform non-normalized weights to normalized weights all formula in this section can be adapted to non-normalized weights by replacing all formula_31.\n\nFor the weighted mean of a list of data for which each element formula_32 potentially comes from a different probability distribution with known variance formula_33, one possible choice for the weights is given by the reciprocal of variance:\n\nThe weighted mean in this case is:\n\nand the \"standard error of the weighted mean (with variance weights)\" is:\n\nNote this reduces to formula_37 when all formula_38.\nIt is a special case of the general formula in previous section,\n\nThe equations above can be combined to obtain:\n\nThe significance of this choice is that this weighted mean is the maximum likelihood estimator of the mean of the probability distributions under the assumption that they are independent and normally distributed with the same mean.\n\nWeighted means are typically used to find the weighted mean of historical data, rather than theoretically generated data. In this case, there will be some error in the variance of each data point. Typically experimental errors may be underestimated due to the experimenter not taking into account all sources of error in calculating the variance of each data point. In this event, the variance in the weighted mean must be corrected to account for the fact that formula_41 is too large. The correction that must be made is\n\nwhere formula_43 is the reduced chi-squared:\n\nThe square root formula_45 can be called the \"standard error of the weighted mean (variance weights, scale corrected)\".\n\nWhen all data variances are equal, formula_38, they cancel out in the weighted mean variance, formula_47, which again reduces to the standard error of the mean (squared), formula_48, formulated in terms of the sample standard deviation (squared),\n\nIt has been shown by bootstrapping methods that the following is an accurate estimation for the standard error of the mean (general case):\n\nwhere formula_51. Further simplification leads to\n\nTypically when a mean is calculated it is important to know the variance and standard deviation about that mean. When a weighted mean formula_53 is used, the variance of the weighted sample is different from the variance of the unweighted sample.\n\nThe \"biased\" weighted sample variance formula_54 is defined similarly to the normal \"biased\" sample variance formula_55:\n\nwhere formula_57, which is 1 for normalized weights. If the weights are \"frequency weights\" (and thus are random variables), it can be shown that formula_54 is the maximum likelihood estimator of formula_59 for iid Gaussian observations.\n\nFor small samples, it is customary to use an unbiased estimator for the population variance. In normal unweighted samples, the \"N\" in the denominator (corresponding to the sample size) is changed to \"N\" − 1 (see Bessel's correction). In the weighted setting, there are actually two different unbiased estimators, one for the case of \"frequency weights\" and another for the case of \"reliability weights\".\n\nIf the weights are \"frequency weights\", then the unbiased estimator is:\n\nThis effectively applies Bessel's correction for frequency weights.\n\nFor example, if values formula_61 are drawn from the same distribution, then we can treat this set as an unweighted sample, or we can treat it as the weighted sample formula_62 with corresponding weights formula_63, and we get the same result either way.\n\nIf the frequency weights formula_64 are normalized to 1, then the correct expression after Bessel's correction becomes\n\nwhere the total number of samples is formula_66 (not formula_67). In any case, the information on total number of samples is necessary in order to obtain an unbiased correction, even if formula_68 has a different meaning other than frequency weight.\n\nIf the weights are instead non-random (\"reliability weights\"), we can determine a correction factor to yield an unbiased estimator. Assuming each random variable is sampled from the same distribution with mean formula_69 and actual variance formula_70, taking expectations we have,\n\nwhere formula_72. Therefore, the bias in our estimator is formula_73, analogous to the formula_74 bias in the unweighted estimator. This means that to unbias our estimator we need to pre-divide by formula_75, ensuring that the expected value of the estimated variance equals the actual variance of the sampling distribution.\n\nThe final unbiased estimate of sample variance is:\nwhere formula_77.\n\nThe degrees of freedom of the weighted, unbiased sample variance vary accordingly from \"N\" − 1 down to 0.\n\nThe standard deviation is simply the square root of the variance above.\n\nAs a side note, other approaches have been described to compute the weighted sample variance.\n\nIn a weighted sample, each row vector formula_78 (each set of single observations on each of the \"K\" random variables) is assigned a weight formula_79.\n\nThen the weighted mean vector formula_80 is given by\n\nAnd the weighted covariance matrix is given by:\n\nSimilarly to weighted sample variance, there are two different unbiased estimators depending on the type of the weights.\n\nIf the weights are \"frequency weights\", the \"unbiased\" weighted estimate of the covariance matrix formula_83, with Bessel's correction, is given by:\n\nIn the case of \"reliability weights\", the weights are normalized:\n\n(If they are not, divide the weights by their sum to normalize prior to calculating formula_66:\n\nThen the weighted mean vector formula_80 can be simplified to\n\nand the \"unbiased\" weighted estimate of the covariance matrix formula_83 is:\n\nThe reasoning here is the same as in the previous section.\n\nSince we are assuming the weights are normalized, then formula_92 and this reduces to:\n\nIf all weights are the same, i.e. formula_94, then the weighted mean and covariance reduce to the unweighted sample mean and covariance above.\n\nThe above generalizes easily to the case of taking the mean of vector-valued estimates. For example, estimates of position on a plane may have less certainty in one direction than another. As in the scalar case, the weighted mean of multiple estimates can provide a maximum likelihood estimate. We simply replace the variance formula_59 by the covariance matrix formula_96 and the arithmetic inverse by the matrix inverse (both denoted in the same way, via superscripts); the weight matrix then reads:\n\nThe weighted mean in this case is:\n\n(where the order of the matrix-vector product is not commutative), in terms of the covariance of the weighted mean:\n\nFor example, consider the weighted mean of the point [1 0] with high variance in the second component and [0 1] with high variance in the first component. Then\n\nthen the weighted mean is:\n\nwhich makes sense: the [1 0] estimate is \"compliant\" in the second component and the [0 1] estimate is compliant in the first component, so the weighted mean is nearly [1 1].\n\nIn the general case, suppose that formula_103, formula_104 is the covariance matrix relating the quantities formula_32, formula_18 is the common mean to be estimated, and formula_107 is a design matrix equal to a vector of ones formula_108 (of length formula_109). The Gauss–Markov theorem states that the estimate of the mean having minimum variance is given by:\n\nand\n\nwhere:\n\nConsider the time series of an independent variable formula_113 and a dependent variable formula_114, with formula_109 observations sampled at discrete times formula_116. In many common situations, the value of formula_114 at time formula_116 depends not only on formula_32 but also on its past values. Commonly, the strength of this dependence decreases as the separation of observations in time increases. To model this situation, one may replace the independent variable by its sliding mean formula_120 for a window size formula_121.\n\nIn the scenario described in the previous section, most frequently the decrease in interaction strength obeys a negative exponential law. If the observations are sampled at equidistant times, then exponential decrease is equivalent to decrease by a constant fraction formula_123 at each time step. Setting formula_124 we can define formula_121 normalized weights by\nwhere formula_66 is the sum of the unnormalized weights. In this case formula_66 is simply\napproaching formula_130 for large values of formula_121.\n\nThe damping constant formula_132 must correspond to the actual decrease of interaction strength. If this cannot be determined from theoretical considerations, then the following properties of exponentially decreasing weights are useful in making a suitable choice: at step formula_133, the weight approximately equals formula_134, the tail area the value formula_135, the head area formula_136. The tail area at step formula_109 is formula_138. Where primarily the closest formula_109 observations matter and the effect of the remaining observations can be ignored safely, then choose formula_132 such that the tail area is sufficiently small.\n\nThe concept of weighted average can be extended to functions. Weighted averages of functions play an important role in the systems of weighted differential and integral calculus.\n\n"}
