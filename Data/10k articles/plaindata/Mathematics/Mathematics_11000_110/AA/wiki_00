{"id": "22770", "url": "https://en.wikipedia.org/wiki?curid=22770", "title": "1", "text": "1\n\n1 (one, also called unit, unity, and (multiplicative) identity) is a number, numeral, and glyph. It represents a single entity, the unit of counting or measurement. For example, a line segment of \"unit length\" is a line segment of length 1. It is also the first of the infinite sequence of natural numbers, followed by 2.\n\nThe word \"one\" can be used as a noun, an adjective and a pronoun.\n\nIt comes from the English word \"an\", which comes from the Proto-Germanic root \"*ainaz\". The Proto-Germanic root \"*ainaz\" comes from the Proto-Indo-European root \"*oi-no-\".\n\nCompare the Proto-Germanic root \"*ainaz\" to Old Frisian \"an\", Gothic \"ains\", Danish \"en\", Dutch \"een\", German \"eins\" and Old Norse \"einn\".\n\nCompare the Proto-Indo-European root \"*oi-no-\" (which means \"one, single\") to Greek \"oinos\" (which means \"ace\" on dice), Latin \"unus\" (one), Old Persian \"aivam\", Old Church Slavonic \"-inu\" and \"ino-\", Lithuanian \"vienas\", Old Irish \"oin\" and Breton \"un\" (one).\n\nOne, sometimes referred to as unity, is the first non-zero natural number. It is thus the integer before two and after zero, and the first positive odd number.\n\nAny number multiplied by one remains that number, as one is the identity for multiplication. As a result, 1 is its own factorial, its own square and square root, its own cube and cube root, and so on. One is also the result of the empty product, as any number multiplied by one is itself. It is also the only natural number that is neither composite nor prime with respect to division, but instead considered a unit (meaning of ring theory).\n\nThe glyph used today in the Western world to represent the number 1, a vertical line, often with a serif at the top and sometimes a short horizontal line at the bottom, traces its roots back to the Indians, who wrote 1 as a horizontal line, much like the Chinese character . The Gupta wrote it as a curved line, and the Nagari sometimes added a small circle on the left (rotated a quarter turn to the right, this 9-look-alike became the present day numeral 1 in the Gujarati and Punjabi scripts). The Nepali also rotated it to the right but kept the circle small. This eventually became the top serif in the modern numeral, but the occasional short horizontal line at the bottom probably originates from similarity with the Roman numeral . In some countries, the serif at the top is sometimes extended into a long upstroke, sometimes as long as the vertical line, which can lead to confusion with the glyph for seven in other countries. Where the 1 is written with a long upstroke, the number 7 has a horizontal stroke through the vertical line.\n\nWhile the shape of the 1 character has an ascender in most modern typefaces, in typefaces with text figures, the character usually is of x-height, as, for example, in .\nMany older typewriters do not have a separate symbol for \"1\" and use the lowercase letter \"l\" instead. It is possible to find cases when the uppercase \"J\" is used, while it may be for decorative purposes.\n\nMathematically, 1 is:\n\nTallying is often referred to as \"base 1\", since only one mark – the tally itself – is needed. This is more formally referred to as a unary numeral system. Unlike base 2 or base 10, this is not a positional notation.\n\nSince the base 1 exponential function (1) always equals 1, its inverse does not exist (which would be called the logarithm base 1 if it did exist).\n\nThere are two ways to write the real number 1 as a recurring decimal: as 1.000..., and as 0.999...\n\nFormalizations of the natural numbers have their own representations of 1:\n\nIn a multiplicative group or monoid, the identity element is sometimes denoted 1, but \"e\" (from the German \"Einheit\", \"unity\") is also traditional. However, 1 is especially common for the multiplicative identity of a ring, i.e., when an addition and 0 are also present. When such a ring has characteristic \"n\" not equal to 0, the element called 1 has the property that (where this 0 is the additive identity of the ring). Important examples are finite fields.\n\n1 is the first figurate number of every kind, such as triangular number, pentagonal number and centered hexagonal number, to name just a few.\n\nIn many mathematical and engineering problems, numeric values are typically \"normalized\" to fall within the unit interval from 0 to 1, where 1 usually represents the maximum possible value in the range of parameters. Likewise, vectors are often normalized to give unit vectors, that is vectors of magnitude one, because these often have more desirable properties. Functions, too, are often normalized by the condition that they have integral one, maximum value one, or square integral one, depending on the application.\n\nBecause of the multiplicative identity, if \"f\"(\"x\") is a multiplicative function, then \"f\"(1) must equal 1.\n\nIt is also the first and second number in the Fibonacci sequence (0 is the zeroth) and is the first number in many other mathematical sequences.\n\n1 is neither a prime number nor a composite number, but a unit (meaning of ring theory), like −1 and, in the Gaussian integers, \"i\" and −\"i\". The fundamental theorem of arithmetic guarantees unique factorization over the integers only up to units. (For example, , but if units are included, is also equal to, say, among infinitely many similar \"factorizations\".)\n\nThe definition of a field requires that 1 must not be equal to 0. Thus, there are no fields of characteristic 1. Nevertheless, abstract algebra can consider the field with one element, which is not a singleton and is not a set at all.\n\n1 is the only positive integer divisible by exactly one positive integer (whereas prime numbers are divisible by exactly two positive integers, composite numbers are divisible by more than two positive integers, and zero is divisible by all positive integers). 1 was formerly considered prime by some mathematicians, using the definition that a prime is divisible only by 1 and itself. However, this complicates the fundamental theorem of arithmetic, so modern definitions exclude units.\n\nBy definition, 1 is the magnitude, absolute value, or norm of a unit complex number, unit vector, and a unit matrix (more usually called an identity matrix). Note that the term \"unit matrix\" is sometimes used to mean something quite different.\n\nBy definition, 1 is the probability of an event that is almost certain to occur.\n\n1 is the most common leading digit in many sets of data, a consequence of Benford's law.\n\n1 is the only known Tamagawa number for a simply connected algebraic group over a number field.\n\nThe generating function that has all coefficients 1 is given by\n\nformula_1\n\nThis power series converges and has finite value if and only if, formula_2.\n\nIn category theory, 1 is sometimes used to denote the terminal object of a category.\n\nIn number theory, 1 is the value of Legendre's constant, which was introduced in 1808 by Adrien-Marie Legendre in expressing the asymptotic behavior of the prime-counting function. Legendre's constant was originally conjectured to be approximately 1.08366, but was proven to equal exactly 1 in 1899.\n\n\n\nIn the philosophy of Plotinus and a number of other neoplatonists, The One is the ultimate reality and source of all existence. Philo of Alexandria (20 BC – AD 50) regarded the number one as God's number, and the basis for all numbers (\"De Allegoriis Legum,\" ii.12 [i.66]).\n\n\n\n\n\n\n"}
{"id": "71318", "url": "https://en.wikipedia.org/wiki?curid=71318", "title": "100-year flood", "text": "100-year flood\n\nA one-hundred-year flood is a flood event that has a 1% probability of occurring in any given year.\n\nThe 100-year flood is also referred to as the 1% flood, since its annual exceedance probability is 1%. For river systems, the 100-year flood is generally expressed as a flowrate. Based on the expected 100-year flood flow rate, the flood water level can be mapped as an area of inundation. The resulting floodplain map is referred to as the 100-year floodplain. Estimates of the 100-year flood flowrate and other streamflow statistics for any stream in the United States are available. In the UK The Environment Agency publishes a comprehensive map of all areas at risk of a 1 in 100 year flood. Areas near the coast of an ocean or large lake also can be flooded by combinations of tide, storm surge, and waves. Maps of the riverine or coastal 100-year floodplain may figure importantly in building permits, environmental regulations, and flood insurance.\n\nA common misunderstanding is that a 100-year flood is likely to occur only once in a 100-year period. In fact, there is approximately a 63.4% chance of one or more 100-year floods occurring in any 100-year period. On the Danube River at Passau, Germany, the actual intervals between 100-year floods during 1501 to 2013 ranged from 37 to 192 years. The probability P that one or more floods occurring during any period will exceed a given flood threshold can be expressed, using the binomial distribution, as\n\nformula_1\n\nwhere T is the threshold return period (e.g. 100-yr, 50-yr, 25-yr, and so forth), and n is the number of years in the period. The probability of exceedance P is also described as the natural, inherent, or hydrologic risk of failure. However, the expected value of the number of 100-year floods occurring in any 100-year period is 1.\n\nTen-year floods have a 10% chance of occurring in any given year (P =0.10); 500-year have a 0.2% chance of occurring in any given year (P =0.002); etc. The percent chance of an X-year flood occurring in a single year is 100/X. A similar analysis is commonly applied to coastal flooding or rainfall data. The recurrence interval of a storm is rarely identical to that of an associated riverine flood, because of rainfall timing and location variations among different drainage basins.\n\nThe field of extreme value theory was created to model rare events such as 100-year floods for the purposes of civil engineering. This theory is most commonly applied to the maximum or minimum observed stream flows of a given river. In desert areas where there are only ephemeral washes, this method is applied to the maximum observed rainfall over a given period of time (24-hours, 6-hours, or 3-hours). The extreme value analysis only considers the most extreme event observed in a given year. So, between the large spring runoff and a heavy summer rain storm, whichever resulted in more runoff would be considered the extreme event, while the smaller event would be ignored in the analysis (even though both may have been capable of causing terrible flooding in their own right).\n\nThere are a number of assumptions which are made to complete the analysis which determines the 100-year flood. First, the extreme events observed in each year must be independent from year-to-year. In other words, the maximum river flow rate from 1984 cannot be found to be significantly correlated with the observed flow rate in 1985. 1985 cannot be correlated with 1986, and so forth. The second assumption is that the observed extreme events must come from the same probability distribution function. The third assumption is that the probability distribution relates to the largest storm (rainfall or river flow rate measurement) that occurs in any one year. The fourth assumption is that the probability distribution function is stationary, meaning that the mean (average), standard deviation and max/min values are not increasing or decreasing over time. This concept is referred to as stationarity.\n\nThe first assumption is often but not always valid and should be tested on a case by case basis. The second assumption is often valid if the extreme events are observed under similar climate conditions. For example, if the extreme events on record all come from late summer thunder storms (as is the case in the southwest U.S.), or from snow pack melting (as is the case in north-central U.S.), then this assumption should be valid. If, however, there are some extreme events taken from thunder storms, others from snow pack melting, and others from hurricanes, then this assumption is most likely not valid. The third assumption is only a problem when trying to forecast a low, but maximum flow event (for example, an event smaller than a 2-year flood). Since this is not typically a goal in extreme analysis, or in civil engineering design, then the situation rarely presents itself. The final assumption about stationarity is difficult to test from data for a single site because of the large uncertainties in even the longest flood records (see next section). More broadly, substantial evidence of climate change strongly suggests that the probability distribution is also changing and that managing flood risks in the future will become even more difficult. The simplest implication of this is that not all of the historical data are, or can be, considered valid as input into the extreme event analysis.\n\nWhen these assumptions are violated there is an \"unknown\" amount of uncertainty introduced into the reported value of what the 100-year flood means in terms of rainfall intensity, or flood depth. When all of the inputs are known the uncertainty can be measured in the form of a confidence interval. For example, one might say there is a 95% chance that the 100-year flood is greater than X, but less than Y.\n\nDirect statistical analysis to estimate the 100-year riverine flood is possible only at the relatively few locations where an annual series of maximum instantaneous flood discharges has been recorded. In the United States as of 2014, taxpayers have supported such records for at least 60 years at fewer than 2,600 locations, for at least 90 years at fewer than 500, and for at least 120 years at only 11. For comparison, the total area of the nation is about , so there are perhaps 3,000 stream reaches that drain watersheds of and 300,000 reaches that drain . In urban areas, 100-year flood estimates are needed for watersheds as small as . For reaches without sufficient data for direct analysis, 100-year flood estimates are derived from indirect statistical analysis of flood records at other locations in a hydrologically similar region or from other hydrologic models. Similarly for coastal floods, tide gauge data exist for only about 1,450 sites worldwide, of which only about 950 added information to the global data center between January 2010 and March 2016.\nMuch longer records of flood elevations exist at a few locations around the world, such as the Danube River at Passau, Germany, but they must be evaluated carefully for accuracy and completeness before any statistical interpretation.\n\nFor an individual stream reach, the uncertainties in any analysis can be large, so 100-year flood estimates have large individual uncertainties for most stream reaches. For the largest recorded flood at any specific location, or any potentially larger event, the recurrence interval always is poorly known. Spatial variability adds more uncertainty, because a flood peak observed at different locations on the same stream during the same event commonly represents a different recurrence interval at each location. If an extreme storm drops enough rain on one branch of a river to cause a 100-year flood, but no rain falls over another branch, the flood wave downstream from their junction might have a recurrence interval of only 10 years. Conversely, a storm that produces a 25-year flood simultaneously in each branch might form a 100-year flood downstream. During a time of flooding, news accounts necessarily simplify the story by reporting the greatest damage and largest recurrence interval estimated at any location. The public can easily and incorrectly conclude that the recurrence interval applies to all stream reaches in the flood area.\n\nPeak elevations of 14 floods as early as 1501 on the Danube River at Passau, Germany, reveal great variability in the actual intervals between floods. Flood events greater than the 50-year flood occurred at intervals of 4 to 192 years since 1501, and the 50-year flood of 2002 was followed only 11 years later by a 500-year flood. Only half of the intervals between 50- and 100-year floods were within 50 percent of the nominal average interval. Similarly, the intervals between 5-year floods during 1955 to 2007 ranged from 5 months to 16 years, and only half were within 2.5 to 7.5 years.\nIn the United States, the 100-year flood provides the risk basis for flood insurance rates. Complete information on the National Flood Insurance Program (NFIP) is available here. A \"regulatory flood\" or \"base flood\" is routinely established for river reaches through a science-based rule-making process targeted to a 100-year flood at the historical average recurrence interval. In addition to historical flood data, the process accounts for previously established regulatory values, the effects of flood-control reservoirs, and changes in land use in the watershed. Coastal flood hazards have been mapped by a similar approach that includes the relevant physical processes. Most areas where serious floods can occur in the United States have been mapped consistently in this manner. On average nationwide, those 100-year flood estimates are well sufficient for the purposes of the NFIP and offer reasonable estimates of future flood risk, if the future is like the past. Approximately 3% of the U.S. population lives in areas subject to the 1% annual chance coastal flood hazard.\n\nIn theory, removing homes and businesses from areas that flood repeatedly can protect people and reduce insurance losses, but in practice it is difficult for people to retreat from established neighborhoods.\n\n\n"}
{"id": "10343119", "url": "https://en.wikipedia.org/wiki?curid=10343119", "title": "Alex Wilkie", "text": "Alex Wilkie\n\nAlex James Wilkie FRS (born 1948 in Northampton) is a British mathematician known for his contributions to Model theory and logic. Previously Reader in Mathematical Logic at the University of Oxford, he was appointed to the Fielden Chair of Pure Mathematics at the University of Manchester in 2007.\n\nAlex Wilkie attended Aylesbury Grammar School and went on to gain his BSc in mathematics with first class honours from University College London in 1969, his MSc (in mathematical logic) from the University of London in 1970, and his PhD from the Bedford College, University of London in 1973 under the supervision of Wilfrid Hodges with a dissertation titled \"Models of Number Theory\". \n\nAfter his PhD he went on to an appointment as a lecturer in mathematics at Leicester University from 1972 to 1973, then a research fellow at the Open University from 1973 until 1978. He spent two periods as a junior lecturer in mathematics at Oxford University (1978–80 and 1981-2) with (1980-1) as a visiting assistant professor at Yale University. In 1980 Wilkie solved Tarski's high school algebra problem.\n\nIn October 1982 Wilkie was appointed as a research fellow in the department of mathematics at the University of Paris VII, then returned to England the following year to take up a three-year SERC (now EPSRC) advanced research fellowship at the University of Manchester. After two years he was appointed lecturer in the Department of Mathematics. In 1986 he went on to Oxford where he was appointed to the readership in mathematical logic there which had become vacant upon the retirement of Robin Gandy. He remained in this post until appointment to the Fielden Chair at Manchester.\n\nWilkie was elected a Fellow of the Royal Society in 2001. To quote the citation\n\nWilkie received the Carol Karp Prize (the highest award made by the Association for Symbolic Logic, every five years) jointly with Ehud Hrushovski in 1993. He was elected to the Council of the London Mathematical Society in 2007, vice-president of the Association for Symbolic Logic (2006) and president of the Association for Symbolic Logic in 2009. In 2012 he became a fellow of the American Mathematical Society. He received the Karp Prize again in 2013, jointly with Moti Gitik, Ya'acov Peterzil, Jonathan Pila, and Sergei Starchenko.\n"}
{"id": "16628707", "url": "https://en.wikipedia.org/wiki?curid=16628707", "title": "Almost Mathieu operator", "text": "Almost Mathieu operator\n\nIn mathematical physics, the almost Mathieu operator arises in the study of the quantum Hall effect. It is given by\n\nacting as a self-adjoint operator on the Hilbert space formula_2. Here formula_3 are parameters. In pure mathematics, its importance comes from the fact of being one of the best-understood examples of an ergodic Schrödinger operator. For example, three problems (now all solved) of Barry Simon's fifteen problems about Schrödinger operators \"for the twenty-first century\" featured the almost Mathieu operator.\n\nFor formula_4, the almost Mathieu operator is sometimes called Harper's equation.\n\nIf formula_5 is a rational number, then formula_6\nis a periodic operator and by Floquet theory its spectrum is purely absolutely continuous.\n\nNow to the case when formula_5 is irrational.\nSince the transformation formula_8 is minimal, it follows that the spectrum of formula_6 does not depend on formula_10. On the other hand, by ergodicity, the supports of absolutely continuous, singular continuous, and pure point parts of the spectrum are almost surely independent of formula_10.\nIt is now known, that\n\nThat the spectral measures are singular when formula_18 follows (through the work of Last and Simon)\n\nfrom the lower bound on the Lyapunov exponent formula_19 given by\n\nThis lower bound was proved independently by Avron, Simon and Michael Herman, after an earlier almost rigorous argument of Aubry and André. In fact, when formula_21 belongs to the spectrum, the inequality becomes an equality (the Aubry–André formula), proved by Jean Bourgain and Svetlana Jitomirskaya.\n\nAnother striking characteristic of the almost Mathieu operator is that its spectrum is a Cantor set for all irrational formula_5 and formula_23. This was shown by Avila and Jitomirskaya solving the by-then famous \"ten martini problem\" (also one of Simon's problems) after several earlier results (including generically and almost surely with respect to the parameters).\n\nFurthermore, the Lebesgue measure of the spectrum of the almost Mathieu operator is known to be\n\nfor all formula_23. For formula_26 this means that the spectrum has zero measure (this was first proposed by Douglas Hofstadter and later became one of Simon's problems). For formula_27, the formula was discovered numerically by Aubry and André and proved by Jitomirskaya and Krasovsky.\n\nThe study of the spectrum for formula_28 leads to the Hofstadter's butterfly, where the spectrum is shown as a set.\n"}
{"id": "464178", "url": "https://en.wikipedia.org/wiki?curid=464178", "title": "Antiholomorphic function", "text": "Antiholomorphic function\n\nIn mathematics, antiholomorphic functions (also called antianalytic functions) are a family of functions closely related to but distinct from holomorphic functions.\n\nA function of the complex variable z defined on an open set in the complex plane is said to be antiholomorphic if its derivative with respect to exists in the neighbourhood of each and every point in that set, where is the complex conjugate. \n\nOne can show that if \"f\"(\"z\") is a holomorphic function on an open set \"D\", then \"f\"() is an antiholomorphic function on , where is the reflection against the \"x\"-axis of \"D\", or in other words, is the set of complex conjugates of elements of \"D\". Moreover, any antiholomorphic function can be obtained in this manner from a holomorphic function. This implies that a function is antiholomorphic if and only if it can be expanded in a power series in in a neighborhood of each point in its domain.\n\nIf a function is both holomorphic and antiholomorphic, then it is constant on any connected component of its domain.\n"}
{"id": "12578002", "url": "https://en.wikipedia.org/wiki?curid=12578002", "title": "AsciiMath", "text": "AsciiMath\n\nAsciiMath is a client-side mathematical markup language for displaying mathematical expressions in web browsers.\n\nUsing the JavaScript script ASCIIMathML.js, AsciiMath notation is converted to MathML at the time the page is loaded by the browser, natively in Mozilla Firefox, Safari, and via a plug-in in IE7. The simplified markup language supports a subset of the LaTeX language instructions, as well as a less verbose syntax (which, for example, replaces \"\\times\" with \"xx\" to produce the \"×\" symbol). The resulting MathML mathematics can be styled by applying CSS to class \"mstyle\".\n\nThe script ASCIIMathML.js is freely available under the MIT License. The latest version also includes support for SVG graphics, natively in Mozilla Firefox and via a plug-in in IE7.\n\nPer May 2009 there is a new version available. This new version still contains the original ASCIIMathML and LaTeXMathML as developed by Peter Jipsen, but the ASCIIsvg part has been extended with linear-logarithmic, logarithmic-linear, logarithmic-logarithmic, polar graphs and pie charts, normal and stacked bar charts, different functions like integration and differentiation and a series of event trapping functions, buttons and sliders, in order to create interactive lecture material and exams online in web pages.\n\nASCIIMathML.js has been integrated into MathJax, starting with MathJax v2.0.\n\nThe well-known quadratic formula\n\nlooks in AsciiMath like this\n\n\n"}
{"id": "572498", "url": "https://en.wikipedia.org/wiki?curid=572498", "title": "Bell polynomials", "text": "Bell polynomials\n\nIn combinatorial mathematics, the Bell polynomials, named in honor of Eric Temple Bell, are used in the study of set partitions. They are related to Stirling and Bell numbers. They also occur in many applications, such as in the Faà di Bruno's formula.\n\nThe \"partial\" or \"incomplete\" exponential Bell polynomials are a triangular array of polynomials given by\n\nwhere the sum is taken over all sequences \"j\", \"j\", \"j\", ..., \"j\" of non-negative integers such that these two conditions are satisfied:\n\nThe sum\n\nis called the \"n\"th complete exponential Bell polynomial.\n\nLikewise, the partial \"ordinary\" Bell polynomial, in contrast to the usual exponential Bell polynomial defined above, is given by\n\nwhere the sum runs over all sequences \"j\", \"j\", \"j\", ..., \"j\" of non-negative integers such that\n\nThe ordinary Bell polynomials can be expressed in the terms of exponential Bell polynomials:\n\nIn general, Bell polynomial refers to the exponential Bell polynomial, unless otherwise explicitly stated.\n\nThe exponential Bell polynomial encodes the information related to the ways a set can be partitioned. For example, if we consider a set {A, B, C}, it can be partitioned into two non-empty, non-overlapping subsets, which is also referred to as parts or blocks, in 3 different ways:\n\nThus, we can encode the information regarding these partitions as\n\nHere, the subscripts of \"B\" tells us that we are considering the partitioning of set with 3 elements into 2 blocks. The subscript of each \"x\" indicates the presence of block with \"i\" elements (or block of size \"i\") in a given partition. So here, \"x\" indicates the presence of a block with two elements. Similarly, \"x\" indicates the presence of a block with a single element. The exponent of \"x\" indicates that there are \"j\" such blocks of size \"i\" in a single partition. Here, since both \"x\" and \"x\" has exponent 1, it indicates that there is only one such block in a given partition. The coefficient of the monomial indicates how many such partitions there are. For our case, there are 3 partitions of a set with 3 elements into 2 blocks, where in each partition the elements are divided into two blocks of sizes 1 and 2.\n\nSince any set can be divided into a single block in only one way, the above interpretation would mean that \"B\" = \"x\". Similarly, since there is only one way that a set with \"n\" elements be divided into \"n\" singletons, \"B\" = \"x\".\n\nAs a more complicated example, consider\n\nThis tells us that if a set with 6 elements is divided into 2 blocks, then we can have 6 partitions with blocks of size 1 and 5, 15 partitions with blocks of size 4 and 2, and 10 partitions with 2 blocks of size 3.\n\nNote that the sum of the subscripts in a monomials is equal to the total number of elements. Thus, the number of monomials that appear in the partial Bell polynomial is equal to the number of ways the integer \"n\" can be expressed as a summation of \"k\" positive integers. This is the same as the integer partition of \"n\" into \"k\" parts. For instance, in the above examples, the integer 3 can be partitioned into two parts as 2+1 only. Thus, there is only one monomial in \"B\". However, the integer 6 can be partitioned into two parts as 5+1, 4+2, and 3+3. Thus, there are three monomials in \"B\". Indeed, the subscripts of the variables in a monomial are the same as those given by the integer partition, indicating the sizes of the different blocks. The total number of monomials appearing in a complete Bell polynomial \"B\" is thus equal to the total number of integer partitions of \"n\".\n\nAlso note that the degree of each monomial, which is the sum of the exponents of each variable in the monomial, is equal to the number of blocks the set is divided into. That is, \"j\" + \"j\" + ... = \"k\" . Thus, given a complete Bell polynomial \"B\", we can separate the partial Bell polynomial \"B\" by collecting all those monomials with degree \"k\".\n\nFinally, if we disregard the sizes of the blocks and put all \"x\" = \"x\", then the summation of the coefficients of the partial Bell polynomial \"B\" will give the total number of ways that a set with \"n\" elements can be partitioned into \"k\" blocks, which is the same as the Stirling numbers of the second kind. Also, the summation of all the coefficients of the complete Bell polynomial \"B\" will give us the total number of ways a set with \"n\" elements can be partitioned into non-overlapping subsets, which is the same as the Bell number.\n\nIn general, if the integer \"n\" is partitioned into a sum in which \"1\" appears \"j\" times, \"2\" appears \"j\" times, and so on, then the number of partitions of a set of size \"n\" that collapse to that partition of the integer \"n\" when the members of the set become indistinguishable is the corresponding coefficient in the polynomial.\n\nFor example, we have\n\nbecause there are\n\nSimilarly,\n\nbecause there are\n\nThe exponential partial Bell polynomials can be defined by the double series expansion of its generating function:\n\nIn other words, by what amounts to the same, by the series expansion of the exponential:\nThe complete exponential Bell polynomial is defined by formula_15, or in other words:\n\nThus, the \"n\"-th complete Bell polynomial is given by\n\nLikewise, the \"ordinary\" partial Bell polynomial can be defined by the generating function\n\nOr, equivalently, by series expansion of the exponential\n\nSee also generating function transformations for Bell polynomial generating function expansions of compositions of sequence generating functions and powers, logarithms, and exponentials of a sequence generating function. Each of these formulas is cited in the respective sections of Comtet.\n\nThe complete Bell polynomials can be recurrently defined as\nwith the initial value formula_21.\n\nThe partial Bell polynomials can also be computed efficiently by a recurrence relation:\nwhere\n\nThe complete Bell polynomials also satisfy the following recurrence differential formula:\n\nThe complete Bell polynomial can be expressed as a determinant:\n\nThe value of the Bell polynomial \"B\"(\"x\",\"x\"...) on the sequence of factorials equals an unsigned Stirling number of the first kind:\n\nThe value of the Bell polynomial \"B\"(\"x\",\"x\"...) on the sequence of ones equals a Stirling number of the second kind:\n\nConsider the integral of the form\n\nwhere (\"a\",\"b\") is a real (finite or infinite) interval, λ is a large positive parameter and the functions \"f\" and \"g\" are continuous. Let \"f\" have a single minimum in [\"a\",\"b\"] which occurs at \"x\" = \"a\". Assume that as \"x\" → \"a\",\n\nwith \"α\" > 0, Re(\"β\") > 0; and that the expansion of \"f\" can be term wise differentiated. Then, Laplace–Erdelyi theorem states that the asymptotic expansion of the integral \"I\"(\"λ\") is given by\n\nwhere the coefficients \"c\" are expressible in terms of \"a\" and \"b\" using partial \"ordinary\" Bell polynomials, as given by Campbell–Froman–Walles–Wojdylo formula:\n\nThe elementary symmetric polynomial formula_37 and the power sum symmetric polynomial formula_38 can be related to each other using Bell polynomials as:\n\nThese formulae allow one to express the coefficients of monic polynomials in terms of the Bell polynomials of its zeroes. For instance, together with Cayley–Hamilton theorem they lead to expression of the determinant of a \"n\" × \"n\" square matrix \"A\" in terms of the traces of its powers: \n\nThe cycle index of the symmetric group formula_41 can be expressed in terms of complete Bell polynomials as follows:\n\nThe sum\n\nis the \"n\"th raw moment of a probability distribution whose first \"n\" cumulants are \"κ\", ..., \"κ\". In other words, the \"n\"th moment is the \"n\"th complete Bell polynomial evaluated at the first \"n\" cumulants. Likewise, the \"n\"th cumulant can be given in terms of the moments as\n\nThe probabilists' Hermite polynomials can be expressed in terms of Bell polynomials as\n\nwhere \"x\" = 0 for all \"i\" > 2; thus allowing for a combinatorial interpretation of the coefficients of the Hermite polynomials. This can be seen by comparing the generating function of the Hermite polynomials\n\nwith that of Bell polynomials.\n\nFor any sequence \"a\", \"a\", …, \"a\" of scalars, let\n\nThen this polynomial sequence is of binomial type, i.e. it satisfies the binomial identity\n\nMore generally, we have this result:\n\nIf we define a formal power series\n\nthen for all \"n\",\n\nBell polynomials are implemented in:\n\n\n"}
{"id": "12055125", "url": "https://en.wikipedia.org/wiki?curid=12055125", "title": "Bombieri norm", "text": "Bombieri norm\n\nIn mathematics, the Bombieri norm, named after Enrico Bombieri, is a norm on homogeneous polynomials with coefficient in formula_1 or formula_2 (there is also a version for non homogeneous univariate polynomials). This norm has many remarkable properties, the most important being listed in this article.\n\nTo start with the geometry, the \"Bombieri scalar product\" for homogeneous polynomials with \"N\" variables can be defined as follows using multi-index notation:\n\nby definition different monomials are orthogonal, so that\n\nwhile\n\nby definition\n\nIn the above definition and in the rest of this article the following notation applies:\n\nif\n\nwrite\n\nand\n\nand\n\nThe fundamental property of this norm is the Bombieri inequality:\n\nlet formula_12 be two homogeneous polynomials respectively of degree formula_13 and formula_14 with formula_15 variables, then, the following inequality holds:\n\nHere the Bombieri inequality is the left hand side of the above statement, while the right side means that the Bombieri norm is an algebra norm. Giving the left hand side is meaningless without that constraint, because in this case, we can achieve the same result with any norm by multiplying the norm by a well chosen factor.\n\nThis multiplicative inequality implies that the product of two polynomials is bounded from below by a quantity that depends on the multiplicand polynomials. Thus, this product can not be arbitrarily small. This multiplicative inequality is useful in metric algebraic geometry and number theory.\n\nAnother important property is that the Bombieri norm is invariant by composition with an \nisometry:\n\nlet formula_12 be two homogeneous polynomials of degree formula_18 with formula_15 variables and let formula_20 be an isometry\nof formula_21 (or formula_22). Then, the we have formula_23. When formula_24 this implies formula_25.\n\nThis result follows from a nice integral formulation of the scalar product:\n\nwhere formula_27 is the unit sphere of formula_22 with its canonical measure formula_29.\n\nLet formula_30 be a homogeneous polynomial of degree formula_18 with formula_15 variables and let formula_33. We have:\n\n\nwhere formula_36 denotes the Euclidean norm.\n\nThe Bombieri norm is useful in polynomial factorization, where it has some advantages over the Mahler measure, according to Knuth (Exercises 20-21, pages 457-458 and 682-684).\n\n\n"}
{"id": "3954077", "url": "https://en.wikipedia.org/wiki?curid=3954077", "title": "Boolean domain", "text": "Boolean domain\n\nIn mathematics and abstract algebra, a Boolean domain is a set consisting of exactly two elements whose interpretations include \"false\" and \"true\". In logic, mathematics and theoretical computer science, a Boolean domain is usually written as {0, 1}, {false, true}, {F, T}, formula_1 or formula_2\n\nThe algebraic structure that naturally builds on a Boolean domain is the Boolean algebra with two elements. The initial object in the category of bounded lattices is a Boolean domain.\n\nIn computer science, a Boolean variable is a variable that takes values in some Boolean domain. Some programming languages feature reserved words or symbols for the elements of the Boolean domain, for example codice_1 and codice_2. However, many programming languages do not have a Boolean datatype in the strict sense. In C or BASIC, for example, falsity is represented by the number 0 and truth is represented by the number 1 or −1, and all variables that can take these values can also take any other numerical values.\n\nThe Boolean domain {0, 1} can be replaced by the unit interval , in which case rather than only taking values 0 or 1, any value between and including 0 and 1 can be assumed. Algebraically, negation (NOT) is replaced with formula_3 conjunction (AND) is replaced with multiplication (formula_4), and disjunction (OR) is defined via De Morgan's law to be formula_5.\n\nInterpreting these values as logical truth values yields a multi-valued logic, which forms the basis for fuzzy logic and probabilistic logic. In these interpretations, a value is interpreted as the \"degree\" of truth – to what extent a proposition is true, or the probability that the proposition is true.\n\n"}
{"id": "24999643", "url": "https://en.wikipedia.org/wiki?curid=24999643", "title": "Boolean flag", "text": "Boolean flag\n\nA boolean flag, truth bit or truth flag in computer science is a Boolean value represented as one bit\n\nA single byte can contain up to 8 separate Boolean flags, making it a very economical method of storage.\nMost computer languages support the setting and testing of single or multiple bits in combination for use as truth indicators and usually up to 256 different combinations of conditions can be tested for with just a single instruction on one byte.\nSometimes, programs are written to simply set flags when certain conditions are detected, rather than have multiple nested conditional statements (e.g. IF's) that can get quite complex. When all the conditions are tested for and all flags set on or off appropriately, testing can commence on various combinations of conditions - by reference to the flags instead of the variables themselves. This can simplify processing considerably and allows decision tables to be implemented by mapping to their binary representations in memory.\n"}
{"id": "24560170", "url": "https://en.wikipedia.org/wiki?curid=24560170", "title": "Castelnuovo–Mumford regularity", "text": "Castelnuovo–Mumford regularity\n\nIn algebraic geometry, the Castelnuovo–Mumford regularity of a coherent sheaf \"F\" over projective space P is the smallest integer \"r\" such that it is r-regular, meaning that\n\nwhenever \"i\" > 0. The regularity of a subscheme is defined to be the regularity of its sheaf of ideals. The regularity controls when the Hilbert function of the sheaf becomes a polynomial; more precisely dim \"H\"(\"P\", \"F\"(\"m\")) is a polynomial in \"m\" when \"m\" is at least the regularity. The concept of \"r\"-regularity was introduced by , who attributed the following results to :\n\nA related idea exists in commutative algebra. Suppose \"R\" = \"k\"[\"x\"...,\"x\"] is a polynomial ring over a field \"k\" and \"M\" is a finitely generated graded \"R\"-module. Suppose \"M\" has a minimal graded free resolution\nand let \"b\" be the maximum of the degrees of the generators of \"F\". If \"r\" is an integer such that \"b\" - \"j\" ≤ \"r\" for all \"j\", then \"M\" is said to be \"r\"-regular. The regularity of \"M\" is the smallest such \"r\".\n\nThese two notions of regularity coincide when \"F\" is a coherent sheaf such that Ass(\"F\") contains no closed points. Then the graded module is finitely generated and has the same regularity as \"F\".\n\n"}
{"id": "1725027", "url": "https://en.wikipedia.org/wiki?curid=1725027", "title": "Cauchy problem", "text": "Cauchy problem\n\nA Cauchy problem in mathematics asks for the solution of a partial differential equation that satisfies certain conditions that are given on a hypersurface in the domain. A Cauchy problem can be an initial value problem or a boundary value problem (for this case see also Cauchy boundary condition) or it can be neither of them. It is named after Augustin Louis Cauchy.\n\nFor a partial differential equation defined on R and a smooth manifold \"S\" ⊂ R of dimension \"n\" (\"S\" is called the Cauchy surface), the Cauchy problem consists of finding the unknown functions formula_1 of the differential equation with respect to the independent variables formula_2 that satisfies\nsubject to the condition, for some value formula_4,\n\nwhere formula_6 are given functions defined on the surface formula_7 (collectively known as the Cauchy data of the problem). The derivative of order zero means that the function itself is specified.\n\nThe Cauchy–Kowalevski theorem states that \"If all the functions formula_8 are analytic in some neighborhood of the point formula_9, and if all the functions formula_10 are analytic in some neighborhood of the point formula_11, then the Cauchy problem has a unique analytic solution in some neighborhood of the point formula_12\". \n\n\n"}
{"id": "2174079", "url": "https://en.wikipedia.org/wiki?curid=2174079", "title": "Chen–Ho encoding", "text": "Chen–Ho encoding\n\nChen–Ho encoding is a memory-efficient alternate system of binary encoding for decimal digits.\n\nThe traditional system of binary encoding for decimal digits, known as binary-coded decimal (BCD), uses four bits to encode each digit, resulting in significant wastage of binary data bandwidth (since four bits can store 16 states and are being used to store only 10).\n\nThe encoding reduces the storage requirements of two decimal digits (100 states) from 8 to 7 bits, and those of three decimal digits (1000 states) from 12 to 10 bits using only simple Boolean transformations avoiding any complex arithmetic operations like a base conversion.\n\nIn what appears to have been a multiple discovery, some of the concepts behind what later became known as Chen–Ho encoding were independently developed by Theodore M. Hertz in 1969 and by Tien Chi Chen in 1971.\n\nHertz of Rockwell filed a patent for his encoding in 1969, which was granted in 1971.\n\nChen first discussed his ideas with Irving Tze Ho in 1971. Chen and Ho were both working for IBM at the time, although in different locations. Chen also consulted with Frank C. Tung to verify the results of his theories independently. IBM filed a patent in their name in 1973, which was granted in 1974. At least by 1973 Hertz's earlier work must have been known to them, as the patent cites his patent as prior art.\n\nThe final version of the Chen–Ho encoding was circulated inside IBM in 1974 and published in 1975 in the journal \"Communications of the Association for Computing Machinery (CACM)\". This version included several refinements, primarily related to the application of the encoding system. It constitutes a Huffman-like prefix code.\n\nThe encoding became known as \"Chen–Ho encoding\" or \"Chen–Ho algorithm\" only since 2000. After having filed a patent in 2001, Michael F. Cowlishaw published a further refinement of Chen–Ho encoding known as Densely Packed Decimal (DPD) encoding in \"IEE Proceedings – Computers and Digital Techniques\" in 2002. Densely Packed Decimal has subsequently been adopted as the \"decimal encoding\" used in the IEEE 754-2008 and floating-point standards.\n\nChen noted that the digits zero through seven were simply encoded using three binary digits of the corresponding octal group. He also postulated that one could use a flag to identify a different encoding for the digits eight and nine, which would be encoded using a single bit.\n\nIn practice, a series of Boolean transformations are applied to the stream of input bits, compressing BCD encoded digits from 12 bits per three digits to 10 bits per three digits. Reversed transformations are used to decode the resulting coded stream to BCD. Equivalent results can also be achieved by the use of a look-up table.\n\nChen–Ho encoding is limited to encoding sets of three decimal digits into groups of 10 bits (so called \"declets\"). Of the 1024 states possible by using 10 bits, it leaves only 24 states unused (with don't care bits typically set to 0 on write and ignored on read). With only 0.34% wastage it gives a 20% more efficient encoding than BCD with one digit in 4 bits.\n\nBoth Hertz and Chen also proposed similar, but less efficient, encoding schemes to compress sets of two decimal digits (requiring 8 bits in BCD) into groups of 7 bits.\n\nLarger sets of decimal digits could by divided into three- and two-digit groups.\n\nThe patents also discuss the possibility to adapt the scheme to digits encoded in any other decimal codes than BCD, like f.e. Excess-3. The same principles could also be applied to other bases.\n\nIn 1973, some form of Chen–Ho encoding appears to have been utilized in the address conversion hardware of the optional IBM 7070/7074 emulation feature for the IBM System/370 Model 165 and 370 Model 168 computers.\n\nOne prominent application uses a 128-bit register to store 33 decimal digits with a three digit exponent, effectively not less than what could be achieved using binary encoding (whereas BCD encoding would need 144 bits to store the same number of digits).\n\n\n"}
{"id": "632685", "url": "https://en.wikipedia.org/wiki?curid=632685", "title": "Coimage", "text": "Coimage\n\nIn algebra, the coimage of a homomorphism\n\nis the quotient\n\nof the domain by the kernel. \nThe coimage is canonically isomorphic to the image by the first isomorphism theorem, when that theorem applies.\n\nMore generally, in category theory, the coimage of a morphism is the dual notion of the image of a morphism. If \"f\" : \"X\" → \"Y\", then a coimage of \"f\" (if it exists) is an epimorphism \"c\" : \"X\" → \"C\" such that\n\n"}
{"id": "1031713", "url": "https://en.wikipedia.org/wiki?curid=1031713", "title": "Computer experiment", "text": "Computer experiment\n\nA computer experiment or simulation experiment is an experiment used to study a computer simulation, also referred to as an in silico system. This area includes computational physics, computational chemistry, computational biology and other similar disciplines.\n\nComputer simulations are constructed to emulate a physical system. Because these are meant to replicate some aspect of a system in detail, they often do not yield an analytic solution. Therefore, methods such as discrete event simulation or finite element solvers are used. A computer model is used to make inferences about the system it replicates. For example, climate models are often used because experimentation on an earth sized object is impossible.\n\nComputer experiments have been employed with many purposes in mind. Some of those include: \n\nModeling of computer experiments typically uses a Bayesian framework. Bayesian statistics is an interpretation of the field of statistics where all evidence about the true state of the world is explicitly expressed in the form of probabilities. In the realm of computer experiments, the Bayesian interpretation would imply we must form a prior distribution that represents our prior belief on the structure of the computer model. The use of this philosophy for computer experiments started in the 1980s and is nicely summarized by Sacks et al. (1989) . While the Bayesian approach is widely used, frequentist approaches have been recently discussed .\n\nThe basic idea of this framework is to model the computer simulation as an unknown function of a set of inputs. The computer simulation is implemented as a piece of computer code that can be evaluated to produce a collection of outputs. Examples of inputs to these simulations are coefficients in the underlying model, initial conditions and forcing functions. It is natural to see the simulation as a deterministic function that maps these \"inputs\" into a collection of \"outputs\". On the basis of seeing our simulator this way, it is common to refer to the collection of inputs as formula_1, the computer simulation itself as formula_2, and the resulting output as formula_3. Both formula_1 and formula_3 are vector quantities, and they can be very large collections of values, often indexed by space, or by time, or by both space and time.\n\nAlthough formula_6 is known in principle, in practice this is not the case. Many simulators comprise tens of thousands of lines of high-level computer code, which is not accessible to intuition. For some simulations, such as climate models, evaluation of the output for a single set of inputs can require millions of computer hours .\n\nThe typical model for a computer code output is a Gaussian process. For notational simplicity, assume formula_7 is a scalar. Owing to the Bayesian framework, we fix our belief that the function formula_2 follows a Gaussian process,\nformula_9\nwhere formula_10 is the mean function and formula_11 is the covariance function. Popular mean functions are low order polynomials and a popular covariance function is Matern covariance, which includes both the exponential (formula_12) and Gaussian covariances (as formula_13).\n\nThe design of computer experiments has considerable differences from design of experiments for parametric models. Since a Gaussian process prior has an infinite dimensional representation, the concepts of A and D criteria (see Optimal design), which focus on reducing the error in the parameters, cannot be used. Replications would also be wasteful in cases when the computer simulation has no error. Criteria that are used to determine a good experimental design include integrated mean squared prediction error and distance based criteria .\n\nPopular strategies for design include latin hypercube sampling and low discrepancy sequences.\n\nUnlike physical experiments, it is common for computer experiments to have thousands of different input combinations. Because the standard inference requires matrix inversion of a square matrix of the size of the number of samples (formula_14), the cost grows on the formula_15. Matrix inversion of large, dense matrices can also cause numerical inaccuracies. Currently, this problem is solved by greedy decision tree techniques, allowing effective computations for unlimited dimensionality and sample size patent WO2013055257A1, or avoided by using approximation methods, e.g. .\n\n"}
{"id": "704359", "url": "https://en.wikipedia.org/wiki?curid=704359", "title": "Contour integration", "text": "Contour integration\n\nIn the mathematical field of complex analysis, contour integration is a method of evaluating certain integrals along paths in the complex plane.\n\nContour integration is closely related to the calculus of residues, a method of complex analysis.\n\nOne use for contour integrals is the evaluation of integrals along the real line that are not readily found by using only real variable methods.\n\nContour integration methods include\nOne method can be used, or a combination of these methods, or various limiting processes, for the purpose of finding these integrals or sums.\n\nIn complex analysis a contour is a type of curve in the complex plane. In contour integration, contours provide a precise definition of the curves on which an integral may be suitably defined. A curve in the complex plane is defined as a continuous function from a closed interval of the real line to the complex plane: .\n\nThis definition of a curve coincides with the intuitive notion of a curve, but includes a parametrization by a continuous function from a closed interval. This more precise definition allows us to consider what properties a curve must have for it to be useful for integration. In the following subsections we narrow down the set of curves that we can integrate to only include ones that can be built up out of a finite number of continuous curves that can be given a direction. Moreover, we will restrict the \"pieces\" from crossing over themselves, and we require that each piece have a finite (non-vanishing) continuous derivative. These requirements correspond to requiring that we consider only curves that can be traced, such as by a pen, in a sequence of even, steady strokes, which only stop to start a new piece of the curve, all without picking up the pen.\n\nContours are often defined in terms of directed smooth curves. These provide a precise definition of a \"piece\" of a smooth curve, of which a contour is made.\n\nA smooth curve is a curve with a non-vanishing, continuous derivative such that each point is traversed only once ( is one-to-one), with the possible exception of a curve such that the endpoints match (). In the case where the endpoints match the curve is called closed, and the function is required to be one-to-one everywhere else and the derivative must be continuous at the identified point (). A smooth curve that is not closed is often referred to as a smooth arc.\n\nThe parametrization of a curve provides a natural ordering of points on the curve: comes before if . This leads to the notion of a directed smooth curve. It is most useful to consider curves independent of the specific parametrization. This can be done by considering equivalence classes of smooth curves with the same direction. A directed smooth curve can then be defined as an ordered set of points in the complex plane that is the image of some smooth curve in their natural order (according to the parametrization). Note that not all orderings of the points are the natural ordering of a smooth curve. In fact, a given smooth curve has only two such orderings. Also, a single closed curve can have any point as its endpoint, while a smooth arc has only two choices for its endpoints.\n\nContours are the class of curves on which we define contour integration. A contour is a directed curve which is made up of a finite sequence of directed smooth curves whose endpoints are matched to give a single direction. This requires that the sequence of curves be such that the terminal point of coincides with the initial point of , . This includes all directed smooth curves. Also, a single point in the complex plane is considered a contour. The symbol + is often used to denote the piecing of curves together to form a new curve. Thus we could write a contour that is made up of curves as\n\nThe contour integral of a complex function is a generalization of the integral for real-valued functions. For continuous functions in the complex plane, the contour integral can be defined in analogy to the line integral by first defining the integral along a directed smooth curve in terms of an integral over a real valued parameter. A more general definition can be given in terms of partitions of the contour in analogy with the partition of an interval and the Riemann integral. In both cases the integral over a contour is defined as the sum of the integrals over the directed smooth curves that make up the contour.\n\nTo define the contour integral in this way one must first consider the integral, over a real variable, of a complex-valued function. Let be a complex-valued function of a real variable, . The real and imaginary parts of are often denoted as and , respectively, so that\nThen the integral of the complex-valued function over the interval is given by \n\nLet be a continuous function on the directed smooth curve . Let be any parametrization of that is consistent with its order (direction). Then the integral along is denoted\n\nand is given by\n\nThis definition is well defined. That is, the result is independent of the parametrization chosen. In the case where the real integral on the right side does not exist the integral along is said not to exist.\n\nThe generalization of the Riemann integral to functions of a complex variable is done in complete analogy to its definition for functions from the real numbers. The partition of a directed smooth curve is defined as a finite, ordered set of points on . The integral over the curve is the limit of finite sums of function values, taken at the points on the partition, in the limit that the maximum distance between any two successive points on the partition (in the two-dimensional complex plane), also known as the mesh, goes to zero.\n\nDirect methods involve the calculation of the integral by means of methods similar to those in calculating line integrals in several-variable calculus. This means that we use the following method:\n\nA fundamental result in complex analysis is that the contour integral of is , where the path of the contour is taken to be the unit circle traversed counterclockwise (or any positively oriented Jordan curve about 0). In the case of the unit circle there is a direct method to evaluate the integral\n\nIn evaluating this integral, use the unit circle as contour, parametrized by , with , then and\n\nwhich is the value of the integral.\n\nApplications of integral theorems are also often used to evaluate the contour integral along a contour, which means that the real-valued integral is calculated simultaneously along with calculating the contour integral.\n\nIntegral theorems such as the Cauchy integral formula or residue theorem are generally used in the following method:\n\nConsider the integral\n\nTo evaluate this integral, we look at the complex-valued function\n\nwhich has singularities at and . We choose a contour that will enclose the real-valued integral, here a semicircle with boundary diameter on the real line (going from, say, to ) will be convenient. Call this contour .\n\nThere are two ways of proceeding, using the Cauchy integral formula or by the method of residues:\n\nNote that:\n\nthus\n\nFurthermore observe that\n\nSince the only singularity in the contour is the one at , then we can write\n\nwhich puts the function in the form for direct application of the formula. Then, by using Cauchy's integral formula,\n\nWe take the first derivative, in the above steps, because the pole is a second-order pole. That is, is taken to the second power, so we employ the first derivative of . If it were taken to the third power, we would use the second derivative and divide by 2!, etc. The case of to the first power corresponds to a zero order derivative—just itself.\n\nWe need to show that the integral over the arc of the semicircle tends to zero as , using the estimation lemma\n\nwhere is an upper bound on along the arc and the length of the arc. Now,\n\nSo\n\nConsider the Laurent series of about , the only singularity we need to consider. We then have\n\nIt is clear by inspection that the residue is (to see this, imagine that the above equation were multiplied by , then both sides integrated via the Cauchy integral formula—only the second term would integrate to a non-zero quantity), so, by the residue theorem, we have\n\nThus we get the same result as before.\n\nAs an aside, a question can arise whether we do not take the semicircle to include the \"other\" singularity, enclosing . To have the integral along the real axis moving in the correct direction, the contour must travel clockwise, i.e., in a negative direction, reversing the sign of the integral overall.\n\nThis does not affect the use of the method of residues by series.\n\nThe integral\n\n(which arises in probability theory as a scalar multiple of the characteristic function of the Cauchy distribution) resists the techniques of elementary calculus. We will evaluate it by expressing it as a limit of contour integrals along the contour that goes along the real line from to and then counterclockwise along a semicircle centered at 0 from to . Take to be greater than 1, so that the imaginary unit is enclosed within the curve. The contour integral is\n\nSince is an entire function (having no singularities at any point in the complex plane), this function has singularities only where the denominator is zero. Since , that happens only where or . Only one of those points is in the region bounded by this contour. The residue of at is\n\nAccording to the residue theorem, then, we have\n\nThe contour may be split into a \"straight\" part and a curved arc, so that\n\nand thus\n\nIt can be shown that if then\n\nTherefore if then\n\nA similar argument with an arc that winds around rather than shows that if then\n\nand finally we have this:\n\nCertain substitutions can be made to integrals involving trigonometric functions, so the integral is transformed into a rational function of a complex variable and then the above methods can be used in order to evaluate the integral.\n\nAs an example, consider\n\nWe seek to make a substitution of . Now, recall\n\nand\n\nTaking to be the unit circle, we substitute to get:\n\nThe singularities to be considered are at formula_34 Let be a small circle about formula_35 and be a small circle about formula_36 Then we arrive at the following:\n\nThe above method may be applied to all integrals of the type\n\nwhere and are polynomials, i.e. a rational function in trigonometric terms is being integrated. Note that the bounds of integration may as well be and −, as in the previous example, or any other pair of endpoints 2 apart.\n\nThe trick is to use the substitution where and hence\n\nThis substitution maps the interval to the unit circle. Furthermore,\n\nand\n\nso that a rational function in results from the substitution, and the integral becomes\n\nTherefore:\n\nBy using the residue theorem or the Cauchy integral formula (first employing the partial fractions method to derive a sum of two simple contour integrals) one obtains\n\nThis section treats a type of integral of which\n\nis an example.\n\nTo calculate this integral, one uses the function\n\nand the branch of the logarithm corresponding to .\n\nWe will calculate the integral of along the keyhole contour shown at right. As it turns out this integral is a multiple of the initial integral that we wish to calculate and by the Cauchy residue theorem we have\n\nLet be the radius of the large circle, and the radius of the small one. We will denote the upper line by , and the lower line by . As before we take the limit when and . The contributions from the two circles vanish. For example, one has the following upper bound with the lemma:\n\nIn order to compute the contributions of and we set on and on , with :\n\nwhich gives\n\nWe seek to evaluate\n\nThis requires a close study of\n\nWe will construct so that it has a branch cut on , shown in red in the diagram. To do this, we choose two branches of the logarithm, setting\n\nand\n\nThe cut of is therefore and the cut of is . It is easy to see that the cut of the product of the two, i.e. , is , because is actually continuous across . This is because when and we approach the cut from above, has the value\n\nWhen we approach from below, has the value\n\nBut\n\nso that we have continuity across the cut. This is illustrated in the diagram, where the two black oriented circles are labelled with the corresponding value of the argument of the logarithm used in and .\n\nWe will use the contour shown in green in the diagram. To do this we must compute the value of along the line segments just above and just below the cut.\n\nLet (in the limit, i.e. as the two green circles shrink to radius zero), where . Along the upper segment, we find that has the value\n\nand along the lower segment,\n\nIt follows that the integral of along the upper segment is in the limit, and along the lower segment, .\n\nIf we can show that the integrals along the two green circles vanish in the limit, then we also have the value of , by the Cauchy residue theorem. Let the radius of the green circles be , where and , and apply the inequality. For the circle on the left, we find\n\nSimilarly, for the circle on the right, we have\n\nNow using the Cauchy residue theorem, we have\n\nwhere the minus sign is due to the clockwise direction around the residues. Using the branch of the logarithm from before, clearly\n\nThe pole is shown in blue in the diagram. The value simplifies to\n\nWe use the following formula for the residue at infinity:\n\nSubstituting, we find\n\nand\n\nwhere we have used the fact that for the second branch of the logarithm. Next we apply the binomial expansion, obtaining\n\nThe conclusion is that\n\nFinally, it follows that the value of is\n\nwhich yields\n\nAn integral representation of a function is an expression of the function involving a contour integral. Various integral representations are known for many special functions. Integral representations can be important for theoretical reasons, e.g. giving analytic continuation or functional equations, or sometimes for numerical evaluations.\nFor example, the original definition of the Riemann zeta function via a Dirichlet series,\nis valid only for . But\n\nwhere the integration is done over the Hankel contour , is valid for all complex s not equal to 1.\n\n\n"}
{"id": "1886266", "url": "https://en.wikipedia.org/wiki?curid=1886266", "title": "Core (game theory)", "text": "Core (game theory)\n\nIn game theory, the core is the set of feasible allocations that cannot be improved upon by a subset (a \"coalition\") of the economy's agents. A coalition is said to \"improve upon\" or \"block\" a feasible allocation if the members of that coalition are better off under another feasible allocation that is identical to the first except that every member of the coalition has a different consumption bundle that is part of an aggregate consumption bundle that can be constructed from publicly available technology and the initial endowments of each consumer in the coalition.\n\nAn allocation is said to have the \"core property\" if there is no coalition that can improve upon it. The core is the set of all feasible allocations with the core property.\n\nThe idea of the core already appeared in the writings of , at the time referred to as the \"contract curve\". Even if von Neumann and Morgenstern considered it an interesting concept, they only worked with zero-sum games where the core is always empty. The modern definition of the core is due to Gillies.\n\nConsider a transferable utility cooperative game formula_1 where formula_2 denotes the set of players and formula_3 is the characteristic function. An imputation formula_4 is dominated by another imputation formula_5 if there exists a coalition formula_6, such that each player in formula_6 prefers formula_5, formally: formula_9 for all formula_10 and there exists formula_10 such that formula_12 and formula_6 can enforce formula_5 (by threatening to leave the grand coalition to form formula_6), formally: formula_16. An imputation formula_17 is \"dominated\" if there exists an imputation formula_5 dominating it.\n\nWhen the core exists and is not empty, it is the set of imputations that are not dominated.\n\n\nConsider a group of \"n\" miners, who have discovered large bars of gold. If two miners can carry one piece of gold, then the payoff of a coalition \"S\" is\n\nIf there are more than two miners and there is an even number of miners, then the core consists of the single payoff where each miner gets 1/2. If there is an odd number of miners, then the core is empty.\n\nMr A and Mr B are knitting gloves. The gloves are one-size-fits-all, and two gloves make a pair that they sell for €5. They have each made three gloves. How to share the proceeds from the sale? The problem can be described by a characteristic function form game with the following characteristic function: Each man has three gloves, that is one pair with a market value of €5. Together, they have 6 gloves or 3 pair, having a market value of €15. Since the singleton coalitions (consisting of a single man) are the only non-trivial coalitions of the game all possible distributions of this sum belong to the core, provided both men get at least €5, the amount they can achieve on their own. For instance (7.5, 7.5) belongs to the core, but so does (5, 10) or (9, 6).\n\nFor the moment ignore shoe sizes: a pair consists of a left and a right shoe, which can then be sold for €10. Consider a game with 2001 players: 1000 of them have 1 left shoe, 1001 have 1 right shoe. The core of this game is somewhat surprising: it consists of a single imputation that gives 10 to those having a (scarce) left shoe, and 0 to those owning an (oversupplied) right shoe. No coalition can block this outcome, because no left shoe owner will accept less than 10, and any imputation that pays a positive amount to any right shoe owner must pay less than 10000 in total to the other players, who can get 10000 on their own. So, there is just one imputation in the core.\n\nThe message remains the same, even if we increase the numbers as long as left shoes are scarcer. The core has been criticized for being so extremely sensitive to oversupply of one type of player.\n\nThe Walrasian equilibria of an exchange economy in a general equilibrium model, will lie in the core of the cooperation game between the agents. Graphically, and in a two-agent economy (see Edgeworth Box), the core is the set of points on the contract curve (the set of Pareto optimal allocations) lying between each of the agents' indifference curves defined at the initial endowments.\n\nWhen alternatives are allocations (list of consumption bundles), it is natural to assume that any nonempty subsets of individuals can block a given allocation.\nWhen alternatives are public (such as the amount of a certain public good), however, it is more appropriate to assume that only the coalitions that are large enough can block a given alternative. The collection of such large (\"winning\") coalitions is called a \"simple game\".\nThe \"core of a simple game with respect to a profile of preferences\" is based on the idea that only winning coalitions can reject an alternative formula_17 in favor of another alternative formula_5. A necessary and sufficient condition for the core to be nonempty for all profile of preferences, is provided in terms of the Nakamura number for the simple game.\n\n\n"}
{"id": "34865455", "url": "https://en.wikipedia.org/wiki?curid=34865455", "title": "Coxeter complex", "text": "Coxeter complex\n\nIn mathematics, the Coxeter complex, named after H. S. M. Coxeter, is a geometrical structure (a simplicial complex) associated to a Coxeter group. Coxeter complexes are the basic objects that allow the construction of buildings; they form the apartments of a building.\n\nThe first ingredient in the construction of the Coxeter complex associated to a Coxeter group \"W\" is a certain representation of \"W\", called the canonical representation of \"W\".\n\nLet formula_1 be a Coxeter system associated to \"W\", with Coxeter matrix formula_2. The canonical representation is given by a vector space \"V\" with basis of formal symbols formula_3, which is equipped with the symmetric bilinear form formula_4. The action of \"W\" on this vector space \"V\" is then given by formula_5, as motivated by the expression for reflections in root systems.\n\nThis representation has several foundational properties in the theory of Coxeter groups; for instance, the bilinear form \"B\" is positive definite if and only if \"W\" is finite. It is (always) a faithful representation of \"W\".\n\nOne can think of this representation as expressing \"W\" as some sort of reflection group, with the caveat that \"B\" might not be positive definite. It becomes important then to distinguish the representation \"V\" from its dual \"V\". The vectors formula_6 lie in \"V\", and have corresponding dual vectors formula_7 in \"V\", given by:\n\nwhere the angled brackets indicate the natural pairing of a dual vector in \"V\" with a vector of \"V\", and \"B\" is the bilinear form as above.\n\nNow \"W\" acts on \"V\", and the action satisfies the formula\n\nfor formula_10 and any \"f\" in \"V\". This expresses \"s\" as a reflection in the hyperplane formula_11. One has the fundamental chamber formula_12, this has faces the so-called walls, formula_13. The other chambers can be obtained from formula_14 by translation: they are the formula_15 for formula_16.\n\nGiven a fundamental chamber formula_17, the Tits cone is defined to be formula_18. This need not be the whole of \"V\". Of major importance is the fact that the Tits cone \"X\" is convex. The action of \"W\" on the Tits cone \"X\" has fundamental domain the fundamental chamber formula_17.\n\nOnce one has defined the Tits cone \"X\", the Coxeter complex formula_20 of \"W\" with respect to \"S\" can be defined as the quotient of \"X\", with the origin removed, by the positive reals (ℝ, ×):\n\nThe dihedral groups formula_22 (of order 2\"n\") are Coxeter groups, of corresponding type formula_23. These have the presentation formula_24.\n\nThe canonical linear representation of formula_23 is the usual reflection representation of the dihedral group, as acting on a \"n\"-gon in the plane (so formula_26 in this case). For instance, in the case \"n\" = 3, we get the Coxeter group of type formula_27, acting on an equilateral triangle in the plane. Each reflection \"s\" has an associated hyperplane \"H\" in the dual vector space (which can be canonically identified with the vector space itself using the bilinear form \"B\", which is an inner product in this case as remarked above), these are the walls. They cut out chambers, as seen below:\n\nThe Coxeter complex is then the corresponding 2\"n\"-gon, as in the image above. This is a simplicial complex of dimension 1, and it can be colored by cotype.\n\nAnother motivating example is the infinite dihedral group formula_28. This can be seen as the group of symmetries of the real line that preserves the set of points with integer coordinates; it is generated by the reflections in formula_29 and formula_30. This group has the Coxeter presentation formula_31.\n\nIn this case, it is no longer possible to identify \"V\" with the dual space \"V\", as \"B\" is not positive definite. It is then better to work solely with \"V\", which is where the hyperplanes are defined. This then gives the following picture:\n\nIn this case, the Tits cone is not the whole plane, but only the upper half plane. Quotienting out by the positive reals then yields another copy of the real line, with marked points at the integers. This is the Coxeter complex of the infinite dihedral group.\n\nAnother description of the Coxeter complex uses standard cosets of the Coxeter group \"W\". A standard coset is a coset of the form formula_32, where formula_33 for some subset \"J\" of \"S\". For instance, formula_34 and formula_35. \n\nThe Coxeter complex formula_36 is then the poset of standard cosets, ordered by reverse inclusion. This has a canonical structure of a simplicial complex, as do all posets that satisfy:\n\nThe Coxeter complex associated to formula_1 has dimension formula_39. It is homeomorphic to a formula_40-sphere if \"W\" is finite and is contractible if \"W\" is infinite.\n\n\n"}
{"id": "6338491", "url": "https://en.wikipedia.org/wiki?curid=6338491", "title": "DECIM", "text": "DECIM\n\nIn cryptography, DECIM is a stream cypher algorithm designed by Come Berbain, Olivier Billet, Anne Canteaut, Nicolas Courtois, Blandine Debraize, Henri Gilbert, Louis Goubin, Aline Gouget, Louis Granboulan,\nCédric Lauradoux, Marine Minier, Thomas Pornin and Hervé Sibert. It has been patented. It has been submitted to the eSTREAM Project of the eCRYPT network.\n"}
{"id": "18288107", "url": "https://en.wikipedia.org/wiki?curid=18288107", "title": "Diaconescu's theorem", "text": "Diaconescu's theorem\n\nIn mathematical logic, Diaconescu's theorem, or the Goodman–Myhill theorem, states that the full axiom of choice is sufficient to derive the law of the excluded middle, or restricted forms of it, in constructive set theory. It was discovered in 1975 by Diaconescu and later by Goodman and Myhill. Already in 1967, Errett Bishop posed the Theorem as an exercise (Problem 2 on page 58 in \"Foundations of constructive analysis\").\n\nFor any proposition formula_1, we can build the sets\nand\n\nThese are sets, using the axiom of specification. In classical set theory this would be equivalent to \nand similarly for formula_5. However, without the law of the excluded middle, these equivalences cannot be proven; in fact the two sets are not even provably finite (in the usual sense of being in bijection with a natural number, though they would be in the Dedekind sense). \n\nAssuming the axiom of choice, there exists a choice function for the set formula_6; that is, a function formula_7 such that \n\nBy the definition of the two sets, this means that\n\nwhich implies formula_10\n\nBut since formula_11 (by the axiom of extensionality), therefore formula_12, so\n\nThus formula_14 As this could be done for any proposition, this completes the proof that the axiom of choice implies the law of the excluded middle. \n\nThe proof relies on the use of the full separation axiom. In constructive set theories with only the predicative separation, the form of \"P\" will be restricted to sentences with bound quantifiers only, giving only a restricted form of the law of the excluded middle. This restricted form is still not acceptable constructively.\n\nIn constructive type theory, or in Heyting arithmetic extended with finite types, there is typically no separation at all - subsets of a type are given different treatments. A form of the axiom of choice is a theorem, yet excluded middle is not.\n"}
{"id": "21402632", "url": "https://en.wikipedia.org/wiki?curid=21402632", "title": "Electroencephalography", "text": "Electroencephalography\n\nElectroencephalography (EEG) is an electrophysiological monitoring method to record electrical activity of the brain. It is typically noninvasive, with the electrodes placed along the scalp, although invasive electrodes are sometimes used such as in electrocorticography. EEG measures voltage fluctuations resulting from ionic current within the neurons of the brain. In clinical contexts, EEG refers to the recording of the brain's spontaneous electrical activity over a period of time, as recorded from multiple electrodes placed on the scalp. Diagnostic applications generally focus either on event-related potentials or on the spectral content of EEG. The former investigates potential fluctuations time locked to an event like stimulus onset or button press. The latter analyses the type of neural oscillations (popularly called \"brain waves\") that can be observed in EEG signals in the frequency domain.\n\nEEG is most often used to diagnose epilepsy, which causes abnormalities in EEG readings. It is also used to diagnose sleep disorders, depth of anesthesia, coma, encephalopathies, and brain death. EEG used to be a first-line method of diagnosis for tumors, stroke and other focal brain disorders, but this use has decreased with the advent of high-resolution anatomical imaging techniques such as magnetic resonance imaging (MRI) and computed tomography (CT). Despite limited spatial resolution, EEG continues to be a valuable tool for research and diagnosis. It is one of the few mobile techniques available and offers millisecond-range temporal resolution which is not possible with CT, PET or MRI.\n\nDerivatives of the EEG technique include evoked potentials (EP), which involves averaging the EEG activity time-locked to the presentation of a stimulus of some sort (visual, somatosensory, or auditory). Event-related potentials (ERPs) refer to averaged EEG responses that are time-locked to more complex processing of stimuli; this technique is used in cognitive science, cognitive psychology, and psychophysiological research.\n\nThe history of EEG is detailed by Barbara E. Swartz in \"Electroencephalography and Clinical Neurophysiology\". In 1875, Richard Caton (1842–1926), a physician practicing in Liverpool, presented his findings about electrical phenomena of the exposed cerebral hemispheres of rabbits and monkeys in the \"British Medical Journal\". In 1890, Polish physiologist Adolf Beck published an investigation of spontaneous electrical activity of the brain of rabbits and dogs that included rhythmic oscillations altered by light. Beck started experiments on the electrical brain activity of animals. Beck placed electrodes directly on the surface of brain to test for sensory stimulation. His observation of fluctuating brain activity led to the conclusion of brain waves.\n\nIn 1912, Ukrainian physiologist Vladimir Vladimirovich Pravdich-Neminsky published the first animal EEG and the evoked potential of the mammalian (dog). In 1914, Napoleon Cybulski and Jelenska-Macieszyna photographed EEG recordings of experimentally induced seizures.\n\nGerman physiologist and psychiatrist Hans Berger (1873–1941) recorded the first human EEG in 1924. Expanding on work previously conducted on animals by Richard Caton and others, Berger also invented the electroencephalogram (giving the device its name), an invention described \"as one of the most surprising, remarkable, and momentous developments in the history of clinical neurology\". His discoveries were first confirmed by British scientists Edgar Douglas Adrian and B. H. C. Matthews in 1934 and developed by them.\n\nIn 1934, Fisher and Lowenback first demonstrated epileptiform spikes. In 1935, Gibbs, Davis and Lennox described interictal spike waves and the three cycles/s pattern of clinical absence seizures, which began the field of clinical electroencephalography. Subsequently, in 1936 Gibbs and Jasper reported the interictal spike as the focal signature of epilepsy. The same year, the first EEG laboratory opened at Massachusetts General Hospital.\n\nFranklin Offner (1911–1999), professor of biophysics at Northwestern University developed a prototype of the EEG that incorporated a piezoelectric inkwriter called a Crystograph (the whole device was typically known as the Offner Dynograph).\n\nIn 1947, The American EEG Society was founded and the first International EEG congress was held. In 1953 Aserinsky and Kleitman described REM sleep.\n\nIn the 1950s, William Grey Walter developed an adjunct to EEG called EEG topography, which allowed for the mapping of electrical activity across the surface of the brain. This enjoyed a brief period of popularity in the 1980s and seemed especially promising for psychiatry. It was never accepted by neurologists and remains primarily a research tool.\n\nIn 1988, report was given on EEG control of a physical object, a robot.\n\nIn October 2018, scientists connected the brains of three people to experiment with the process of thoughts sharing. Five groups of three people participated in the experiment using EEG. The success rate of the experiment was 81%. \n\nA routine clinical EEG recording typically lasts 20–30 minutes (plus preparation time) and usually involves recording from scalp electrodes. Routine EEG is typically used in clinical circumstances to distinguish epileptic seizures from other types of spells, such as psychogenic non-epileptic seizures, syncope (fainting), sub-cortical movement disorders and migraine variants, to differentiate \"organic\" encephalopathy or delirium from primary psychiatric syndromes such as catatonia, to serve as an adjunct test of brain death, to prognosticate, in certain instances, in patients with coma, and to determine whether to wean anti-epileptic medications.\n\nAt times, a routine EEG is not sufficient to establish the diagnosis and/or to determine the best course of action in terms of treatment. In this case, attempts may be made to record an EEG while a seizure is occurring. This is known as an ictal recording, as opposed to an inter-ictal recording which refers to the EEG recording between seizures. To obtain an ictal recording, a prolonged EEG is typically performed accompanied by a time-synchronized video and audio recording. This can be done either as an outpatient (at home) or during a hospital admission, preferably to an Epilepsy Monitoring Unit (EMU) with nurses and other personnel trained in the care of patients with seizures. Outpatient ambulatory video EEGs typically last one to three days. An admission to an Epilepsy Monitoring Unit typically lasts several days but may last for a week or longer. While in the hospital, seizure medications are usually withdrawn to increase the odds that a seizure will occur during admission. For reasons of safety, medications are not withdrawn during an EEG outside of the hospital. Ambulatory video EEGs therefore have the advantage of convenience and are less expensive than a hospital admission, but the disadvantage of a decreased probability of recording a clinical event.\n\nEpilepsy monitoring is typically done to distinguish epileptic seizures from other types of spells, such as psychogenic non-epileptic seizures, syncope (fainting), sub-cortical movement disorders and migraine variants, to characterize seizures for the purposes of treatment, and to localize the region of brain from which a seizure originates for work-up of possible seizure surgery.\n\nAdditionally, EEG may be used to monitor the depth of anesthesia, as an indirect indicator of cerebral perfusion in carotid endarterectomy, or to monitor amobarbital effect during the Wada test.\n\nEEG can also be used in intensive care units for brain function monitoring to monitor for non-convulsive seizures/non-convulsive status epilepticus, to monitor the effect of sedative/anesthesia in patients in medically induced coma (for treatment of refractory seizures or increased intracranial pressure), and to monitor for secondary brain damage in conditions such as subarachnoid hemorrhage (currently a research method).\n\nIf a patient with epilepsy is being considered for resective surgery, it is often necessary to localize the focus (source) of the epileptic brain activity with a resolution greater than what is provided by scalp EEG. This is because the cerebrospinal fluid, skull and scalp \"smear\" the electrical potentials recorded by scalp EEG. In these cases, neurosurgeons typically implant strips and grids of electrodes (or penetrating depth electrodes) under the dura mater, through either a craniotomy or a burr hole. The recording of these signals is referred to as electrocorticography (ECoG), subdural EEG (sdEEG) or intracranial EEG (icEEG)--all terms for the same thing. The signal recorded from ECoG is on a different scale of activity than the brain activity recorded from scalp EEG. Low voltage, high frequency components that cannot be seen easily (or at all) in scalp EEG can be seen clearly in ECoG. Further, smaller electrodes (which cover a smaller parcel of brain surface) allow even lower voltage, faster components of brain activity to be seen. Some clinical sites record from penetrating microelectrodes.\n\nRecent studies using machine learning techniques such as neural networks with statistical temporal features extracted from frontal lobe EEG brainwave data has shown high levels of success in classifying mental states (Relaxed, Neutral, Concentrating) and mental emotional states (Negative, Neutral, Positive). \n\nEEG is not indicated for diagnosing headache. Recurring headache is a common pain problem, and this procedure is sometimes used in a search for a diagnosis, but it has no advantage over routine clinical evaluation.\n\nEEG, and the related study of ERPs are used extensively in neuroscience, cognitive science, cognitive psychology, neurolinguistics and psychophysiological research, but also to study human functions such as swallowing. Many EEG techniques used in research are not standardised sufficiently for clinical use. But research on mental disabilities, such as auditory processing disorder (APD), ADD, or ADHD, is becoming more widely known and EEGs are used as research and treatment.\n\nSeveral other methods to study brain function exist, including functional magnetic resonance imaging (fMRI), positron emission tomography (PET), magnetoencephalography (MEG), nuclear magnetic resonance spectroscopy (NMR or MRS), electrocorticography (EEG), single-photon emission computed tomography (SPECT), near-infrared spectroscopy (NIRS), and event-related optical signal (EROS). Despite the relatively poor spatial sensitivity of EEG, it possesses multiple advantages over some of these techniques:\n\nEEG also has some characteristics that compare favorably with behavioral testing:\n\n\nSimultaneous EEG recordings and fMRI scans have been obtained successfully, though successful simultaneous recording requires that several technical difficulties be overcome, such as the presence of ballistocardiographic artifact, MRI pulse artifact and the induction of electrical currents in EEG wires that move within the strong magnetic fields of the MRI. While challenging, these have been successfully overcome in a number of studies.\n\nMRI's produce detailed images created by generating strong magnetic fields that may induce potentially harmful displacement force and torque. These fields produce potentially harmful radio frequency heating and create image artifacts rendering images useless. Due to these potential risks, only certain medical devices can be used in an MR environment.\n\nSimilarly, simultaneous recordings with MEG and EEG have also been conducted, which has several advantages over using either technique alone:\n\nRecently, a combined EEG/MEG (EMEG) approach has been investigated for the purpose of source reconstruction in epilepsy diagnosis.\n\nEEG has also been combined with positron emission tomography. This provides the advantage of allowing researchers to see what EEG signals are associated with different drug actions in the brain.\n\nThe brain's electrical charge is maintained by billions of neurons. Neurons are electrically charged (or \"polarized\") by membrane transport proteins that pump ions across their membranes. Neurons are constantly exchanging ions with the extracellular milieu, for example to maintain resting potential and to propagate action potentials. Ions of similar charge repel each other, and when many ions are pushed out of many neurons at the same time, they can push their neighbours, who push their neighbours, and so on, in a wave. This process is known as volume conduction. When the wave of ions reaches the electrodes on the scalp, they can push or pull electrons on the metal in the electrodes. Since metal conducts the push and pull of electrons easily, the difference in push or pull voltages between any two electrodes can be measured by a voltmeter. Recording these voltages over time gives us the EEG.\n\nThe electric potential generated by an individual neuron is far too small to be picked up by EEG or MEG. EEG activity therefore always reflects the summation of the synchronous activity of thousands or millions of neurons that have similar spatial orientation. If the cells do not have similar spatial orientation, their ions do not line up and create waves to be detected. Pyramidal neurons of the cortex are thought to produce the most EEG signal because they are well-aligned and fire together. Because voltage field gradients fall off with the square of distance, activity from deep sources is more difficult to detect than currents near the skull.\n\nScalp EEG activity shows oscillations at a variety of frequencies. Several of these oscillations have characteristic frequency ranges, spatial distributions and are associated with different states of brain functioning (e.g., waking and the various sleep stages). These oscillations represent synchronized activity over a network of neurons. The neuronal networks underlying some of these oscillations are understood (e.g., the thalamocortical resonance underlying sleep spindles), while many others are not (e.g., the system that generates the posterior basic rhythm). Research that measures both EEG and neuron spiking finds the relationship between the two is complex, with a combination of EEG power in the gamma band and phase in the delta band relating most strongly to neuron spike activity.\n\nIn conventional scalp EEG, the recording is obtained by placing electrodes on the scalp with a conductive gel or paste, usually after preparing the scalp area by light abrasion to reduce impedance due to dead skin cells. Many systems typically use electrodes, each of which is attached to an individual wire. Some systems use caps or nets into which electrodes are embedded; this is particularly common when high-density arrays of electrodes are needed.\n\nElectrode locations and names are specified by the International 10–20 system for most clinical and research applications (except when high-density arrays are used). This system ensures that the naming of electrodes is consistent across laboratories. In most clinical applications, 19 recording electrodes (plus ground and system reference) are used. A smaller number of electrodes are typically used when recording EEG from neonates. Additional electrodes can be added to the standard set-up when a clinical or research application demands increased spatial resolution for a particular area of the brain. High-density arrays (typically via cap or net) can contain up to 256 electrodes more-or-less evenly spaced around the scalp.\n\nEach electrode is connected to one input of a differential amplifier (one amplifier per pair of electrodes); a common system reference electrode is connected to the other input of each differential amplifier. These amplifiers amplify the voltage between the active electrode and the reference (typically 1,000–100,000 times, or 60–100 dB of voltage gain). In analog EEG, the signal is then filtered (next paragraph), and the EEG signal is output as the deflection of pens as paper passes underneath. Most EEG systems these days, however, are digital, and the amplified signal is digitized via an analog-to-digital converter, after being passed through an anti-aliasing filter. Analog-to-digital sampling typically occurs at 256–512 Hz in clinical scalp EEG; sampling rates of up to 20 kHz are used in some research applications.\n\nDuring the recording, a series of activation procedures may be used. These procedures may induce normal or abnormal EEG activity that might not otherwise be seen. These procedures include hyperventilation, photic stimulation (with a strobe light), eye closure, mental activity, sleep and sleep deprivation. During (inpatient) epilepsy monitoring, a patient's typical seizure medications may be withdrawn.\n\nThe digital EEG signal is stored electronically and can be filtered for display. Typical settings for the high-pass filter and a low-pass filter are 0.5–1 Hz and 35–70 Hz respectively. The high-pass filter typically filters out slow artifact, such as electrogalvanic signals and movement artifact, whereas the low-pass filter filters out high-frequency artifacts, such as electromyographic signals. An additional notch filter is typically used to remove artifact caused by electrical power lines (60 Hz in the United States and 50 Hz in many other countries).\n\nThe EEG signals can be captured with opensource hardware such as OpenBCI and the signal can be processed by freely available EEG software such as EEGLAB or the Neurophysiological Biomarker Toolbox.\n\nAs part of an evaluation for epilepsy surgery, it may be necessary to insert electrodes near the surface of the brain, under the surface of the dura mater. This is accomplished via burr hole or craniotomy. This is referred to variously as \"electrocorticography (ECoG)\", \"intracranial EEG (I-EEG)\" or \"subdural EEG (SD-EEG)\". Depth electrodes may also be placed into brain structures, such as the amygdala or hippocampus, structures, which are common epileptic foci and may not be \"seen\" clearly by scalp EEG. The electrocorticographic signal is processed in the same manner as digital scalp EEG (above), with a couple of caveats. ECoG is typically recorded at higher sampling rates than scalp EEG because of the requirements of Nyquist theorem—the subdural signal is composed of a higher predominance of higher frequency components. Also, many of the artifacts that affect scalp EEG do not impact ECoG, and therefore display filtering is often not needed.\n\nA typical adult human EEG signal is about 10 µV to 100 µV in amplitude when measured from the scalp and is about 10–20 mV when measured from subdural electrodes.\n\nSince an EEG voltage signal represents a difference between the voltages at two electrodes, the display of the EEG for the reading encephalographer may be set up in one of several ways. The representation of the EEG channels is referred to as a \"montage.\"\n\n\n Midline positions are often used because they do not amplify the signal in one hemisphere vs. the other, such as Cz, Oz, Pz etc as online reference. The other popular offline references are:\n\nand Yao D (2017) MATLAB Toolboxes for Reference Electrode Standardization Technique (REST) of Scalp EEG. Front. Neurosci. 11:601. doi: 10.3389/fnins.2017.00601),and for more details and its performance, pls ref to the original paper (Yao, D. (2001). A method to standardize a reference of scalp EEG recordings to a point at infinity. Physiol. Meas. 22, 693–711. doi: 10.1088/0967-3334/22/4/305)\n\nWhen analog (paper) EEGs are used, the technologist switches between montages during the recording in order to highlight or better characterize certain features of the EEG. With digital EEG, all signals are typically digitized and stored in a particular (usually referential) montage; since any montage can be constructed mathematically from any other, the EEG can be viewed by the electroencephalographer in any display montage that is desired.\n\nThe EEG is read by a clinical neurophysiologist or neurologist (depending on local custom and law regarding medical specialities), optimally one who has specific training in the interpretation of EEGs for clinical purposes. This is done by visual inspection of the waveforms, called graphoelements. The use of computer signal processing of the EEG—so-called quantitative electroencephalography—is somewhat controversial when used for clinical purposes (although there are many research uses).\n\nIn the early 1990s Babak Taheri, at University of California, Davis demonstrated the first single and also multichannel dry active electrode arrays using micro-machining. The single channel dry EEG electrode construction and results were published in 1994. The arrayed electrode was also demonstrated to perform well compared to silver/silver chloride electrodes. The device consisted of four sites of sensors with integrated electronics to reduce noise by impedance matching. The advantages of such electrodes are: (1) no electrolyte used, (2) no skin preparation, (3) significantly reduced sensor size, and (4) compatibility with EEG monitoring systems. The active electrode array is an integrated system made of an array of capacitive sensors with local integrated circuitry housed in a package with batteries to power the circuitry. This level of integration was required to achieve the functional performance obtained by the electrode. The electrode was tested on an electrical test bench and on human subjects in four modalities of EEG activity, namely: (1) spontaneous EEG, (2) sensory event-related potentials, (3) brain stem potentials, and (4) cognitive event-related potentials. The performance of the dry electrode compared favorably with that of the standard wet electrodes in terms of skin preparation, no gel requirements (dry), and higher signal-to-noise ratio.\n\nIn 1999 researchers at Case Western Reserve University, in Cleveland, Ohio, led by Hunter Peckham, used 64-electrode EEG skullcap to return limited hand movements to quadriplegic Jim Jatich. As Jatich concentrated on simple but opposite concepts like up and down, his beta-rhythm EEG output was analysed using software to identify patterns in the noise. A basic pattern was identified and used to control a switch: Above average activity was set to on, below average off. As well as enabling Jatich to control a computer cursor the signals were also used to drive the nerve controllers embedded in his hands, restoring some movement.\n\nIn 2018, a functional dry electrode composed of a polydimethylsiloxane elastomer filled with conductive carbon nanofibers was reported. This research was conducted at the U.S. Army Research Laboratory. EEG technology often involves applying a gel to the scalp which facilitates strong signal-to-noise ratio. This results in more reproducible and reliable experimental results. Since patients dislike having their hair filled with gel, and the lengthy setup requires trained staff on hand, utilizing EEG outside the laboratory setting can be difficult. Additionally, it has been observed that wet electrode sensors’ performance reduces after a span of hours. Therefore, research has been directed to developing dry and semi-dry EEG bioelectronic interfaces. \n\nDry electrode signals depend upon mechanical contact. Therefore, it can be difficult getting a usable signal because of impedance between the skin and the electrode. Some EEG systems attempt to circumvent this issue by applying a saline solution. Others have a semi dry nature and release small amounts of the gel upon contact with the scalp. Another solution uses spring loaded pin setups. These may be uncomfortable. They may also be dangerous if they were used in a situation where a patient could bump their head since they could become lodged after an impact trauma incident.\n\nARL also developed a visualization tool, Customizable Lighting Interface for the Visualization of EEGs or CLIVE, which showed how well two brains are synchronized.\n\nEEG has several limitations. Most important is its poor spatial resolution. EEG is most sensitive to a particular set of post-synaptic potentials: those generated in superficial layers of the cortex, on the crests of gyri directly abutting the skull and radial to the skull. Dendrites, which are deeper in the cortex, inside sulci, in midline or deep structures (such as the cingulate gyrus or hippocampus), or producing currents that are tangential to the skull, have far less contribution to the EEG signal.\n\nEEG recordings do not directly capture axonal action potentials. An action potential can be accurately represented as a current quadrupole, meaning that the resulting field decreases more rapidly than the ones produced by the current dipole of post-synaptic potentials. In addition, since EEGs represent averages of thousands of neurons, a large population of cells in synchronous activity is necessary to cause a significant deflection on the recordings. Action potentials are very fast and, as a consequence, the chances of field summation are slim. However, neural backpropagation, as a typically longer dendritic current dipole, can be picked up by EEG electrodes and is a reliable indication of the occurrence of neural output.\n\nNot only do EEGs capture dendritic currents almost exclusively as opposed to axonal currents, they also show a preference for activity on populations of parallel dendrites and transmitting current in the same direction at the same time. Pyramidal neurons of cortical layers II/III and V extend apical dendrites to layer I. Currents moving up or down these processes underlie most of the signals produced by electroencephalography.\n\nTherefore, EEG provides information with a large bias to select neuron types, and generally should not be used to make claims about global brain activity. The meninges, cerebrospinal fluid and skull \"smear\" the EEG signal, obscuring its intracranial source.\n\nIt is mathematically impossible to reconstruct a unique intracranial current source for a given EEG signal, as some currents produce potentials that cancel each other out. This is referred to as the inverse problem. However, much work has been done to produce remarkably good estimates of, at least, a localized electric dipole that represents the recorded currents.\n\nEEG has several strong points as a tool for exploring brain activity. EEGs can detect changes over milliseconds, which is excellent considering an action potential takes approximately 0.5–130 milliseconds to propagate across a single neuron, depending on the type of neuron. Other methods of looking at brain activity, such as PET and fMRI have time resolution between seconds and minutes. EEG measures the brain's electrical activity directly, while other methods record changes in blood flow (e.g., SPECT, fMRI) or metabolic activity (e.g., PET, NIRS), which are indirect markers of brain electrical activity. EEG can be used simultaneously with fMRI so that high-temporal-resolution data can be recorded at the same time as high-spatial-resolution data, however, since the data derived from each occurs over a different time course, the data sets do not necessarily represent exactly the same brain activity. There are technical difficulties associated with combining these two modalities, including the need to remove the \"MRI gradient artifact\" present during MRI acquisition and the ballistocardiographic artifact (resulting from the pulsatile motion of blood and tissue) from the EEG. Furthermore, currents can be induced in moving EEG electrode wires due to the magnetic field of the MRI.\n\nEEG can be used simultaneously with NIRS without major technical difficulties. There is no influence of these modalities on each other and a combined measurement can give useful information about electrical activity as well as local hemodynamics.\n\nEEG reflects correlated synaptic activity caused by post-synaptic potentials of cortical neurons. The ionic currents involved in the generation of fast action potentials may not contribute greatly to the averaged field potentials representing the EEG. More specifically, the scalp electrical potentials that produce EEG are generally thought to be caused by the extracellular ionic currents caused by dendritic electrical activity, whereas the fields producing magnetoencephalographic signals are associated with intracellular ionic currents.\n\nEEG can be recorded at the same time as MEG so that data from these complementary high-time-resolution techniques can be combined.\n\nStudies on numerical modeling of EEG and MEG have also been done.\n\nThe EEG is typically described in terms of (1) rhythmic activity and (2) transients. The rhythmic activity is divided into bands by frequency. To some degree, these frequency bands are a matter of nomenclature (i.e., any rhythmic activity between 8–12 Hz can be described as \"alpha\"), but these designations arose because rhythmic activity within a certain frequency range was noted to have a certain distribution over the scalp or a certain biological significance. Frequency bands are usually extracted using spectral methods (for instance Welch) as implemented for instance in freely available EEG software such as EEGLAB or the Neurophysiological Biomarker Toolbox.\nComputational processing of the EEG is often named quantitative electroencephalography (qEEG).\n\nMost of the cerebral signal observed in the scalp EEG falls in the range of 1–20 Hz (activity below or above this range is likely to be artifactual, under standard clinical recording techniques). Waveforms are subdivided into bandwidths known as alpha, beta, theta, and delta to signify the majority of the EEG used in clinical practice.\n\nThe practice of using only whole numbers in the definitions comes from practical considerations in the days when only whole cycles could be counted on paper records. This leads to gaps in the definitions, as seen elsewhere on this page. The theoretical definitions have always been more carefully defined to include all frequencies. Unfortunately there is no agreement in standard reference works on what these ranges should be – values for the upper end of alpha and lower end of beta include 12, 13, 14 and 15. If the threshold is taken as 14 Hz, then the slowest beta wave has about the same duration as the longest spike (70 ms), which makes this the most useful value.\n\nOthers sometimes divide the bands into sub-bands for the purposes of data analysis.\n\n\n\n\"Ultra-slow\" or \"near-DC\" activity is recorded using DC amplifiers in some research contexts. It is not typically recorded in a clinical context because the signal at these frequencies is susceptible to a number of artifacts.\n\nSome features of the EEG are transient rather than rhythmic. Spikes and sharp waves may represent seizure activity or interictal activity in individuals with epilepsy or a predisposition toward epilepsy. Other transient features are normal: vertex waves and sleep spindles are seen in normal sleep.\n\nNote that there are types of activity that are statistically uncommon, but not associated with dysfunction or disease. These are often referred to as \"normal variants\". The mu rhythm is an example of a normal variant.\n\nThe normal electroencephalogram (EEG) varies by age. The prenatal EEG and neonatal EEG is quite different from the adult EEG. Fetuses in the third trimester and newborns display two common brain activity patterns: \"discontinuous\" and \"trace alternant.\" \"Discontinuous\" electrical activity refers to sharp bursts of electrical activity followed by low frequency waves. \"Trace alternant\" electrical activity describes sharp bursts followed by short high amplitude intervals and usually indicates quiet sleep in newborns. The EEG in childhood generally has slower frequency oscillations than the adult EEG. \n\nThe normal EEG also varies depending on state. The EEG is used along with other measurements (EOG, EMG) to define sleep stages in polysomnography. Stage I sleep (equivalent to drowsiness in some systems) appears on the EEG as drop-out of the posterior basic rhythm. There can be an increase in theta frequencies. Santamaria and Chiappa cataloged a number of the variety of patterns associated with drowsiness. Stage II sleep is characterized by sleep spindles – transient runs of rhythmic activity in the 12–14 Hz range (sometimes referred to as the \"sigma\" band) that have a frontal-central maximum. Most of the activity in Stage II is in the 3–6 Hz range. Stage III and IV sleep are defined by the presence of delta frequencies and are often referred to collectively as \"slow-wave sleep\". Stages I–IV comprise non-REM (or \"NREM\") sleep. The EEG in REM (rapid eye movement) sleep appears somewhat similar to the awake EEG.\n\nEEG under general anesthesia depends on the type of anesthetic employed. With halogenated anesthetics, such as halothane or intravenous agents, such as propofol, a rapid (alpha or low beta), nonreactive EEG pattern is seen over most of the scalp, especially anteriorly; in some older terminology this was known as a WAR (widespread anterior rapid) pattern, contrasted with a WAIS (widespread slow) pattern associated with high doses of opiates. Anesthetic effects on EEG signals are beginning to be understood at the level of drug actions on different kinds of synapses and the circuits that allow synchronized neuronal activity (see: http://www.stanford.edu/group/maciverlab/).\n\nElectrical signals detected along the scalp by an EEG, but that originate from non-cerebral origin are called artifacts. EEG data is almost always contaminated by such artifacts. The amplitude of artifacts can be quite large relative to the size of amplitude of the cortical signals of interest. This is one of the reasons why it takes considerable experience to correctly interpret EEGs clinically. Some of the most common types of biological artifacts include:\n\nThe most prominent eye-induced artifacts are caused by the potential difference between the cornea and retina, which is quite large compared to cerebral potentials. When the eyes and eyelids are completely still, this corneo-retinal dipole does not affect EEG. However, blinks occur several times per minute, the eyes movements occur several times per second. Eyelid movements, occurring mostly during blinking or vertical eye movements, elicit a large potential seen mostly in the difference between the Electrooculography (EOG) channels above and below the eyes. An established explanation of this potential regards the eyelids as sliding electrodes that short-circuit the positively charged cornea to the extra-ocular skin. Rotation of the eyeballs, and consequently of the corneo-retinal dipole, increases the potential in electrodes towards which the eyes are rotated, and decrease the potentials in the opposing electrodes. Eye movements called saccades also generate transient electromyographic potentials, known as saccadic spike potentials (SPs). The spectrum of these SPs overlaps the gamma-band (see Gamma wave), and seriously confounds analysis of induced gamma-band responses, requiring tailored artifact correction approaches. Purposeful or reflexive eye blinking also generates electromyographic potentials, but more importantly there is reflexive movement of the eyeball during blinking that gives a characteristic artifactual appearance of the EEG (see Bell's phenomenon).\n\nEyelid fluttering artifacts of a characteristic type were previously called Kappa rhythm (or Kappa waves). It is usually seen in the prefrontal leads, that is, just over the eyes. Sometimes they are seen with mental activity. They are usually in the Theta (4–7 Hz) or Alpha (7–14 Hz) range. They were named because they were believed to originate from the brain. Later study revealed they were generated by rapid fluttering of the eyelids, sometimes so minute that it was difficult to see. They are in fact noise in the EEG reading, and should not technically be called a rhythm or wave. Therefore, current usage in electroencephalography refers to the phenomenon as an eyelid fluttering artifact, rather than a Kappa rhythm (or wave).\nSome of these artifacts can be useful in various applications. The EOG signals, for instance, can be used to detect and track eye-movements, which are very important in polysomnography, and is also in conventional EEG for assessing possible changes in alertness, drowsiness or sleep.\n\nECG artifacts are quite common and can be mistaken for spike activity. Because of this, modern EEG acquisition commonly includes a one-channel ECG from the extremities. This also allows the EEG to identify cardiac arrhythmias that are an important differential diagnosis to syncope or other episodic/attack disorders.\n\nGlossokinetic artifacts are caused by the potential difference between the base and the tip of the tongue. Minor tongue movements can contaminate the EEG, especially in parkinsonian and tremor disorders.\n\nIn addition to artifacts generated by the body, many artifacts originate from outside the body. Movement by the patient, or even just settling of the electrodes, may cause \"electrode pops\", spikes originating from a momentary change in the impedance of a given electrode. Poor grounding of the EEG electrodes can cause significant 50 or 60 Hz artifact, depending on the local power system's frequency. A third source of possible interference can be the presence of an IV drip; such devices can cause rhythmic, fast, low-voltage bursts, which may be confused for spikes.\n\nMotion artifacts introduce signal noise that can mask the neural signal of interest. Therefore, effective signal noise processing measures were of great interest in the scientific community.\n\nAn EEG equipped phantom head can be placed on a motion platform and moved in a sinusoidal fashion. This contraption enabled researchers to study the effectiveness of motion artifact removal algorithms. Using the same model of phantom head and motion platform, it was determined that cable sway was a major attributor to motion artifacts. However, increasing the surface area of the electrode had a small but significant effect on reducing the artifact. This research was sponsored by the U.S. Army Research Laboratory as a part of the Cognition and Neuroergonomics Collaborative Technical Alliance.\n\nRecently, independent component analysis (ICA) techniques have been used to correct or remove EEG contaminants. These techniques attempt to \"unmix\" the EEG signals into some number of underlying components. There are many source separation algorithms, often assuming various behaviors or natures of EEG. Regardless, the principle behind any particular method usually allow \"remixing\" only those components that would result in \"clean\" EEG by nullifying (zeroing) the weight of unwanted components. Fully automated artifact rejection methods, which use ICA, have also been developed.\n\nIn the last few years, by comparing data from paralysed and unparalysed subjects, EEG contamination by muscle has been shown to be far more prevalent than had previously been realized, particularly in the gamma range above 20 Hz. However, Surface Laplacian has been shown to be effective in eliminating muscle artefact, particularly for central electrodes, which are further from the strongest contaminants. The combination of Surface Laplacian with automated techniques for removing muscle components using ICA proved particularly effective in a follow up study.\n\nAbnormal activity can broadly be separated into epileptiform and non-epileptiform activity. It can also be separated into focal or diffuse.\n\nFocal epileptiform discharges represent fast, synchronous potentials in a large number of neurons in a somewhat discrete area of the brain. These can occur as interictal activity, between seizures, and represent an area of cortical irritability that may be predisposed to producing epileptic seizures. Interictal discharges are not wholly reliable for determining whether a patient has epilepsy nor where his/her seizure might originate. (See focal epilepsy.)\n\nGeneralized epileptiform discharges often have an anterior maximum, but these are seen synchronously throughout the entire brain. They are strongly suggestive of a generalized epilepsy.\n\nFocal non-epileptiform abnormal activity may occur over areas of the brain where there is focal damage of the cortex or white matter. It often consists of an increase in slow frequency rhythms and/or a loss of normal higher frequency rhythms. It may also appear as focal or unilateral decrease in amplitude of the EEG signal.\n\nDiffuse non-epileptiform abnormal activity may manifest as diffuse abnormally slow rhythms or bilateral slowing of normal rhythms, such as the PBR.\n\nIntracortical Encephalogram electrodes and sub-dural electrodes can be used in tandem to discriminate and discretize artifact from epileptiform and other severe neurological events.\n\nMore advanced measures of abnormal EEG signals have also recently received attention as possible biomarkers for different disorders such as Alzheimer's disease.\n\nThe United States Army Research Office budgeted $4 million in 2009 to researchers at the University of California, Irvine to develop EEG processing techniques to identify correlates of imagined speech and intended direction to enable soldiers on the battlefield to communicate via computer-mediated reconstruction of team members' EEG signals, in the form of understandable signals such as words.\n\nInexpensive EEG devices exist for the low-cost research and consumer markets. Recently, a few companies have miniaturized medical grade EEG technology to create versions accessible to the general public. Some of these companies have built commercial EEG devices retailing for less than $100 USD.\n\n\nThe EEG has been used for many purposes besides the conventional uses of clinical diagnosis and conventional cognitive neuroscience. An early use was during World War II by the U.S. Army Air Corps to screen out pilots in danger of having seizures; long-term EEG recordings in epilepsy patients are still used today for seizure prediction. Neurofeedback remains an important extension, and in its most advanced form is also attempted as the basis of brain computer interfaces. The EEG is also used quite extensively in the field of neuromarketing.\n\nThe EEG is altered by drugs that affect brain functions, the chemicals that are the basis for psychopharmacology. Berger's early experiments recorded the effects of drugs on EEG. The science of pharmaco-electroencephalography has developed methods to identify substances that systematically alter brain functions for therapeutic and recreational use.\n\nHonda is attempting to develop a system to enable an operator to control its Asimo robot using EEG, a technology it eventually hopes to incorporate into its automobiles.\n\nEEGs have been used as evidence in criminal trials in the Indian state of Maharashtra.\n\nA lot of research is currently being carried out in order to make EEG devices smaller, more portable and easier to use. So called \"Wearable EEG\" is based upon creating low power wireless collection electronics and ‘dry’ electrodes which do not require a conductive gel to be used. Wearable EEG aims to provide small EEG devices which are present only on the head and which can record EEG for days, weeks, or months at a time, as ear-EEG. Such prolonged and easy-to-use monitoring could make a step change in the diagnosis of chronic conditions such as epilepsy, and greatly improve the end-user acceptance of BCI systems. Research is also being carried out on identifying specific solutions to increase the battery lifetime of Wearable EEG devices through the use of the data reduction approach. For example, in the context of epilepsy diagnosis, data reduction has been used to extend the battery lifetime of Wearable EEG devices by intelligently selecting, and only transmitting, diagnostically relevant EEG data.\n\nEEG signals from musical performers were used to create instant compositions and one CD by the Brainwave Music Project, run at the Computer Music Center at Columbia University by Brad Garton and Dave Soldier.\n\n65. Keiper, A. (2006). The age of neuroelectronics. \"The New Atlantis\", 11, 4-41.\n\n"}
{"id": "10077292", "url": "https://en.wikipedia.org/wiki?curid=10077292", "title": "Eventually (mathematics)", "text": "Eventually (mathematics)\n\nIn the mathematical areas of number theory and analysis, an infinite sequence formula_1 is said to eventually have a certain property if all terms beyond some (finite) point in the sequence have that property. This can be extended to the class of properties \"P\" that apply to elements of any ordered set (sequences and subsets of R are ordered, for example). \n\nOften, when looking at infinite sequences, it does not matter too much what behaviour the sequence exhibits early on. What matters is what the sequence does in the long term. The idea of having a property \"eventually\" rigorizes this viewpoint.\n\nFor example, the definition of a sequence of real numbers formula_1 converging to some limit \"formula_3\" is: \"for all\" formula_4 \"there exists formula_5 such that, for all formula_6, formula_7\". The phrase \"eventually\" is used as shorthand for the fact that \"\"there exists formula_5 such that, for all formula_6...\" So the convergence definition can be restated as: \"for all formula_4, eventually formula_7\". In this setting it is also synonymous with the expression \"for all but a finite number of terms\"\" – not to be confused with \"for almost all terms\" which generally allows for infinitely many exceptions.\n\nA sequence can be thought of as a function with domain the natural numbers. But the notion of \"eventually\" applies to functions on more general sets, specifically those that have an ordering and no greatest element. In general if formula_12 is such a set and there is an element formula_13 in formula_12 such that the function formula_15 is defined for all elements greater than formula_13, then formula_15 is said to have some property eventually if there is an element formula_18 such that formula_15 has the property for all \"formula_20\". This notion is used, for example, in the study of Hardy fields, which are fields made up of real functions that all have certain properties eventually.\n\nWhen a sequence or function has a property eventually, it can have useful implications when trying to prove something with relation to that sequence. For example, in studying the asymptotic behavior of certain functions, it can be useful to know if it eventually behaves differently than would or could be observed computationally, since otherwise this could not be noticed. It is also incorporated into many mathematical definitions, like in some types of limits (an arbitrary bound eventually applies) and Big O notation for describing asymptotic behavior.\n\nThe phrase eventually (or sufficiently large) is used in such contexts as:\nwhich is actually shorthand for:\nor, somewhat more formally: \n\nThis does not necessarily mean that any particular value for formula_3 is known, but only that such an formula_3 exists. The phrase \"sufficiently large\" should not be confused with the phrases \"arbitrarily large\" or \"infinitely large\".\n\n\nAll primes above 2 are odd can be written as \"Eventually, all primes are odd\"\n\nIn the long run, all primes are congruent to ±1 mod 6\n\nThe square of a prime is congruent to 1 mod 24, given that the prime is above 3\n\nThis can be generalized as formula_31 A conjecture can then be made for the closed form function formula_32 as well as the minimum formula_33.\n\n"}
{"id": "32887796", "url": "https://en.wikipedia.org/wiki?curid=32887796", "title": "Extensions of Fisher's method", "text": "Extensions of Fisher's method\n\nIn statistics, extensions of Fisher's method are a group of approaches that allow approximately valid statistical inferences to be made when the assumptions required for the direct application of Fisher's method are not valid. Fisher's method is a way of combining the information in the p-values from different statistical tests so as to form a single overall test: this method requires that the individual test statistics (or, more immediately, their resulting p-values) should be statistically independent.\n\nA principle limitation of Fisher's method is its exclusive design to combine independent p-values, which renders it an unreliable technique to combine dependent p-values. To overcome this limitation, a number of methods were developed to extend its utility.\n\nFisher's method showed that the log-sum of \"k\" independent p-values follow a \"χ\"-distribution with 2\"k\" degrees of freedom: \n\nIn the case that these p-values are not independent, Brown proposed the idea of approximating \"X\" using a scaled \"χ\"-distribution, \"cχ\"(\"k’\"), with \"k’\" degrees of freedom. \n\nThe mean and variance of this scaled \"χ\" variable are:\n\nwhere formula_4 and formula_5. This approximation is shown to be accurate up to two moments.\n\nIt should be noted that the method does require the test statistics' covariance structure to be known up to a scalar multiplicative constant. See reference 2.\n"}
{"id": "12013", "url": "https://en.wikipedia.org/wiki?curid=12013", "title": "Girth (graph theory)", "text": "Girth (graph theory)\n\nIn graph theory, the girth of a graph is the length of a shortest cycle contained in the graph. If the graph does not contain any cycles (i.e. it's an acyclic graph), its girth is defined to be infinity.\nFor example, a 4-cycle (square) has girth 4. A grid has girth 4 as well, and a triangular mesh has girth 3. A graph with girth four or more is triangle-free.\n\nA cubic graph (all vertices have degree three) of girth that is as small as possible is known as a -cage (or as a (3,)-cage). The Petersen graph is the unique 5-cage (it is the smallest cubic graph of girth 5), the Heawood graph is the unique 6-cage, the McGee graph is the unique 7-cage and the Tutte eight cage is the unique 8-cage. There may exist multiple cages for a given girth. For instance there are three nonisomorphic 10-cages, each with 70 vertices: the Balaban 10-cage, the Harries graph and the Harries–Wong graph.\n\nFor any positive integers and , there exists a graph with girth at least and chromatic number at least ; for instance, the Grötzsch graph is triangle-free and has chromatic number 4, and repeating the Mycielskian construction used to form the Grötzsch graph produces triangle-free graphs of arbitrarily large chromatic number. Paul Erdős was the first to prove the general result, using the probabilistic method. More precisely, he showed that a random graph on vertices, formed by choosing independently whether to include each edge with probability has, with probability tending to 1 as goes to infinity, at most cycles of length or less, but has no independent set of size Therefore, removing one vertex from each short cycle leaves a smaller graph with girth greater than in which each color class of a coloring must be small and which therefore requires at least colors in any coloring.\n\nThe odd girth and even girth of a graph are the lengths of a shortest odd cycle and shortest even cycle respectively. \n\nThe circumference of a graph is the length of the \"longest\" cycle, rather than the shortest.\n\nThought of as the least length of a non-trivial cycle, the girth admits natural generalisations as the 1-systole or higher systoles in systolic geometry.\n\nGirth is the dual concept to edge connectivity, in the sense that the girth of a planar graph is the edge connectivity of its dual graph, and vice versa. These concepts are unified in matroid theory by the girth of a matroid, the size of the smallest dependent set in the matroid. For a graphic matroid, the matroid girth equals the girth of the underlying graph, while for a co-graphic matroid it equals the edge connectivity.\n"}
{"id": "43839113", "url": "https://en.wikipedia.org/wiki?curid=43839113", "title": "Glossary of Principia Mathematica", "text": "Glossary of Principia Mathematica\n\nThis is a list of the notation used in Alfred North Whitehead and Bertrand Russell's \"Principia Mathematica\" (1910–13).\n\nThe second (but not the first) edition of volume I has a list of notation used at the end.\n\nThis is a glossary of some of the technical terms in \"Principia Mathematica\" that are no longer widely used or whose meaning has changed.\n\n\n\n"}
{"id": "37591550", "url": "https://en.wikipedia.org/wiki?curid=37591550", "title": "Ihara's lemma", "text": "Ihara's lemma\n\nIn mathematics, Ihara's lemma, introduced by and named by , states that the kernel of the sum of the two \"p\"-degeneracy maps from \"J\"(\"N\")×\"J\"(\"N\") to \"J\"(\"Np\") is Eisenstein whenever the prime \"p\" does not divide \"N\". Here \"J\"(\"N\") is the Jacobian of the compactification of the modular curve of Γ(\"N\").\n\n"}
{"id": "758413", "url": "https://en.wikipedia.org/wiki?curid=758413", "title": "James Jeans", "text": "James Jeans\n\nSir James Hopwood Jeans (11 September 187716 September 1946) was an English physicist, astronomer and mathematician.\n\nBorn in Ormskirk, Lancashire, the son of William Tulloch Jeans, a parliamentary correspondent and author. Jeans was educated at Merchant Taylors' School, Northwood, Wilson's Grammar School, Camberwell and Trinity College, Cambridge.\nAs a gifted student, Jeans was counselled to take an aggressive approach to the Cambridge Mathematical Tripos competition:\nJeans was elected Fellow of Trinity College in October 1901, and taught at Cambridge, but went to Princeton University in 1904 as a professor of applied mathematics. He returned to Cambridge in 1910.\n\nHe made important contributions in many areas of physics, including quantum theory, the theory of radiation and stellar evolution. His analysis of rotating bodies led him to conclude that Laplace's theory that the solar system formed from a single cloud of gas was incorrect, proposing instead that the planets condensed from material drawn out of the sun by a hypothetical catastrophic near-collision with a passing star. This theory is not accepted today.\n\nJeans, along with Arthur Eddington, is a founder of British cosmology. In 1928, Jeans was the first to conjecture a steady state cosmology based on a hypothesized continuous creation of matter in the universe. In his book \"Astronomy and Cosmology\" (1928) he stated: \"The type of conjecture which presents itself, somewhat insistently, is that the centers of the nebulae are of the nature 'singular points' at which matter is poured into our universe from some other, and entirely extraneous spatial dimension, so that, to a denizen of our universe, they appear as points at which matter is being continually created.\" This theory fell out of favour when the 1965 discovery of the cosmic microwave background was widely interpreted as the tell-tale signature of the Big Bang.\n\nHis scientific reputation is grounded in the monographs \"The Dynamical Theory of Gases\" (1904), \"Theoretical Mechanics\" (1906), and \"Mathematical Theory of Electricity and Magnetism\" (1908). After retiring in 1929, he wrote a number of books for the lay public, including \"The Stars in Their Courses\" (1931), \"The Universe Around Us\", \"Through Space and Time\" (1934), \"The New Background of Science\" (1933), and \"The Mysterious Universe.\" These books made Jeans fairly well known as an expositor of the revolutionary scientific discoveries of his day, especially in relativity and physical cosmology.\n\nIn 1939, the Journal of the British Astronomical Association reported that Jeans was going to stand as a candidate for parliament for the Cambridge University constituency. The election, expected to take place in 1939 or 1940 did not take place until 1945, and without his involvement.\n\nHe also wrote the book \"Physics and Philosophy\" (1943) where he explores the different views on reality from two different perspectives: science and philosophy. On his religious views, Jeans was an agnostic Freemason.\n\nJeans married twice, first to the American poet Charlotte Tiffany Mitchell in 1907, and then to the Austrian organist and harpsichordist Suzanne Hock (better known as Susi Jeans) in 1935. \n\nAt Merchant Taylors' School there is a James Jeans Academic Scholarship for the candidate in the entrance exams who displays outstanding results across the spectrum of subjects, notably in mathematics and the sciences.\n\nOne of Jeans' major discoveries, named Jeans length, is a critical radius of an interstellar cloud in space. It depends on the temperature, and density of the cloud, and the mass of the particles composing the cloud. A cloud that is smaller than its Jeans length will not have sufficient gravity to overcome the repulsive gas pressure forces and condense to form a star, whereas a cloud that is larger than its Jeans length will collapse.\n\nJeans came up with another version of this equation, called Jeans mass or Jeans instability, that solves for the critical mass a cloud must attain before being able to collapse.\n\nJeans also helped to discover the Rayleigh–Jeans law, which relates the energy density of black-body radiation to the temperature of the emission source.\n\nJeans is also credited with calculating the rate of atmospheric escape from a planet due to kinetic energy of the gas molecules, a process known as Jeans Escape.\n\nIn an interview published in \"The Observer\" (London), when asked the question \"Do you believe that life on this planet is the result of some sort of accident, or do you believe that it is a part of some great scheme?\", he replied:\n\n\n\n\n\nWorks of Jeans available online from the Internet Archive\n"}
{"id": "3139577", "url": "https://en.wikipedia.org/wiki?curid=3139577", "title": "Jet group", "text": "Jet group\n\nIn mathematics, a jet group is a generalization of the general linear group which applies to Taylor polynomials instead of vectors at a point. Essentially a jet group describes how a Taylor polynomial transforms under changes of coordinate systems (or, equivalently, diffeomorphisms).\n\nThe \"k\"-th order jet group \"G\" consists of jets of smooth diffeomorphisms φ: R → R such that φ(0)=0.\n\nThe following is a more precise definition of the jet group.\n\nLet \"k\" ≥ 2. The gradient of a function \"f:\" R → R can be interpreted as a section of the cotangent bundle of R given by \"df:\" R → \"T*\"R. Similarly, derivatives of order up to \"m\" are sections of the jet bundle \"J\"(R) = R × \"W\", where\n\nHere R* is the dual vector space to R, and \"S\" denotes the \"i\"-th symmetric power. A smooth function \"f:\" R → R has a prolongation \"jf\": R → \"J\"(R) defined at each point \"p\" ∈ R by placing the \"i\"-th partials of \"f\" at \"p\" in the \"S\"((R*)) component of \"W\".\n\nConsider a point formula_2. There is a unique polynomial \"f\" in \"k\" variables and of order \"m\" such that \"p\" is in the image of \"jf\". That is, formula_3. The differential data \"x′\" may be transferred to lie over another point \"y\" ∈ R as \"jf(y)\" , the partials of \"f\" over \"y\".\n\nProvide \"J\"(R) with a group structure by taking\n\nWith this group structure, \"J\"(R) is a Carnot group of class \"m\" + 1.\n\nBecause of the properties of jets under function composition, \"G\" is a Lie group. The jet group is a semidirect product of the general linear group and a connected, simply connected nilpotent Lie group. It is also in fact an algebraic group, since the composition involves only polynomial operations.\n\n"}
{"id": "6370474", "url": "https://en.wikipedia.org/wiki?curid=6370474", "title": "John Fletcher Moulton, Baron Moulton", "text": "John Fletcher Moulton, Baron Moulton\n\nJohn Fletcher Moulton, Baron Moulton (18 November 1844 – 9 March 1921) was an English mathematician, barrister and judge. He was a Cambridge Apostle.\n\nMoulton was born in Madeley, Shropshire, England, as one of six children of a scholarly minister of the Wesleyan Methodist Church, James Egan Moulton. He was sent to Kingswood School at the age of 11 where he excelled at academic subjects. He achieved the top marks in the Oxford and Cambridge Local Examinations and achieved a scholarship to St John's College, Cambridge, graduating Senior Wrangler in 1868 and winning the Smith's Prize. He was at one point judged to be one of the twelve most intelligent men in the United Kingdom.\n\nAfter a brilliant mathematical career at Cambridge and election to a Fellowship, Moulton became a London barrister, specialising in patent law. He also experimented on electricity and was elected a Fellow of the Royal Society. A great advocate for medical research, he was the first chair of the Medical Research Council. He was awarded the French Legion of Honour for his work in establishing international units for measuring electricity.\n\nMoulton became a Liberal Party Member of Parliament successively for Clapham 1885–86, South Hackney 1894–95, and the Launceston division of Cornwall, 1898–1906. He backed the attempts of Gladstone to solve the problems in Ireland through Irish Home Rule. In 1906 Moulton was made Lord Justice on the Court of Appeal and Privy Councillor. In 1912 he entered the House of Lords with a life peerage and the title Baron Moulton, of Bank in the County of Hampshire.\n\nThe First World War gave Lord Moulton his greatest challenge. In 1914 he became chairman of a committee to advise on the supply of explosives, a difficult problem because the British had only a feeble organic chemistry industry. Before long Moulton became Director-General of the Explosives Department, first in the War Office and later in the Ministry of Munitions. He mobilised a brilliant group of administrators and scientists who expanded production more than 20-fold— throughout the war there was more explosives than shells to hold them. They also made fertilizers, and in 1917 became responsible for producing poisonous gasses. Though loyal to orders, Moulton believed that poison gas was a departure from civilised warfare.\n\nDuring the entire four war years Lord Moulton worked a ten-hour day and took less than ten days holiday. On weekends he drove about the country to inspect munitions plants and to locate sites for new ones. He was awarded the Knight Commander of the Order of the Bath in 1915, the Knight Grand Cross of the Order of the British Empire in 1917, the Etoile Noir of France, the Order of Leopold (Belgium) and was the last person to receive the Order of the White Eagle before the collapse of the Russian monarchy.\n\nMoulton also corresponded with Charles Darwin.\n\nAfter the war, despite pressure to lead the expansion of the British chemical industry, he returned to his love: the law. He died in London on 9 March 1921.\n\nHe married Clara Thomson née Hertz (widow of Robert William Thomson) on 24 April 1875. She died in 1888.\n\n\n\n"}
{"id": "3219760", "url": "https://en.wikipedia.org/wiki?curid=3219760", "title": "Landscape of Geometry", "text": "Landscape of Geometry\n\nLandscape of Geometry was an educational television show that illustrated the principles and applications of geometry. The series was produced and broadcast by TVOntario in 1982–83 and was hosted by David Stringer.\n\nEight episodes were produced. They were:\n\n\nAll episodes were 15 minutes in length.\n"}
{"id": "13872825", "url": "https://en.wikipedia.org/wiki?curid=13872825", "title": "Linear space (geometry)", "text": "Linear space (geometry)\n\nA linear space is a basic structure in incidence geometry. A linear space consists of a set of elements called points, and a set of elements called lines. Each line is a distinct subset of the points. The points in a line are said to be incident with the line. Any two lines may have no more than one point in common. Intuitively, this rule can be visualized as two straight lines, which never intersect more than once. \n\n(Finite) linear spaces can be seen as a generalization of projective and affine planes, and more broadly, of formula_1 block designs, where the requirement that every block contains the same number of points is dropped and the essential structural characteristic is that 2 points are incident with exactly 1 line. \n\nThe term \"linear space\" was coined by Libois in 1964, though many results about linear spaces are much older.\n\nLet \"L\" = (\"P\", \"G\", \"I\") be an incidence structure, for which the elements of \"P\" are called points and the elements of \"G\" are called lines. \"L\" is a \"linear space\" if the following three axioms hold:\n\n\nSome authors drop (L3) when defining linear spaces. In such a situation the linear spaces complying to (L3) are considered as \"nontrivial\" and those who don't as \"trivial\".\n\nThe regular Euclidean plane with its points and lines constitutes a linear space, moreover all affine and projective spaces are linear spaces as well.\n\nThe table below shows all possible nontrivial linear spaces of five points. Because any two points are always incident with one line, the lines being incident with only two points are not drawn, by convention. The trivial case is simply a line through five points.\n\nIn the first illustration, the ten lines connecting the ten pairs of points are not drawn. In the second illustration, seven lines connecting seven pairs of points are not drawn. \n\nA linear space of \"n\" points containing a line being incident with \"n\" − 1 points is called a \"near pencil\". (See pencil)\n\nThe De Bruijn–Erdős theorem (incidence geometry) shows that in any finite linear space formula_2 which is not a single point or a single line, we have formula_3. \n\n\n"}
{"id": "39477096", "url": "https://en.wikipedia.org/wiki?curid=39477096", "title": "List of things named after Henri Poincaré", "text": "List of things named after Henri Poincaré\n\nIn physics and mathematics, a number of ideas are named after Henri Poincaré:\n\n\n"}
{"id": "469760", "url": "https://en.wikipedia.org/wiki?curid=469760", "title": "Metric map", "text": "Metric map\n\nIn the mathematical theory of metric spaces, a metric map is a function between metric spaces that does not increase any distance (such functions are always continuous).\nThese maps are the morphisms in the category of metric spaces, Met (Isbell 1964).\nThey are also called Lipschitz functions with Lipschitz constant 1, nonexpansive maps, nonexpanding maps, weak contractions, or short maps.\n\nSpecifically, suppose that \"X\" and \"Y\" are metric spaces and ƒ is a function from \"X\" to \"Y\". Thus we have a metric map when, for any points \"x\" and \"y\" in \"X\",\nHere \"d\" and \"d\" denote the metrics on \"X\" and \"Y\" respectively.\n\nA map ƒ between metric spaces is an isometry if it is a bijective metric map whose inverse is also a metric map. The composite of metric maps is also metric. Thus metric spaces and metric maps form a category Met; Met is a subcategory of the category of metric spaces and Lipschitz functions, and the isomorphisms in Met are the isometries.\n\nOne can say that ƒ is strictly metric if the inequality is strict for every two different points. Thus a contraction mapping is strictly metric, but not necessarily the other way around. Note that an isometry is \"never\" strictly metric, except in the degenerate case of the empty space or a single-point space.\n\nA mapping formula_2 from a metric space \"X\" to the family of nonempty subsets of \"X\" is said to be Lipschitz if there exists formula_3 such that\nfor all formula_5, where \"H\" is the Hausdorff distance. When formula_6, \"T\" is called nonexpansive and when formula_7, \"T\" is called a contraction.\n\n"}
{"id": "229940", "url": "https://en.wikipedia.org/wiki?curid=229940", "title": "Multiplicative inverse", "text": "Multiplicative inverse\n\nIn mathematics, a multiplicative inverse or reciprocal for a number \"x\", denoted by 1/\"x\" or \"x\", is a number which when multiplied by \"x\" yields the multiplicative identity, 1. The multiplicative inverse of a fraction \"a\"/\"b\" is \"b\"/\"a\". For the multiplicative inverse of a real number, divide 1 by the number. For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. The reciprocal function, the function \"f\"(\"x\") that maps \"x\" to 1/\"x\", is one of the simplest examples of a function which is its own inverse (an involution).\n\nThe term \"reciprocal\" was in common use at least as far back as the third edition of \"Encyclopædia Britannica\" (1797) to describe two numbers whose product is 1; geometrical quantities in inverse proportion are described as \"reciprocall\" in a 1570 translation of Euclid's \"Elements\".\n\nIn the phrase \"multiplicative inverse\", the qualifier \"multiplicative\" is often omitted and then tacitly understood (in contrast to the additive inverse). Multiplicative inverses can be defined over many mathematical domains as well as numbers. In these cases it can happen that ; then \"inverse\" typically implies that an element is both a left and right inverse.\n\nThe notation \"f\" is sometimes also used for the inverse function of the function \"f\", which is not in general equal to the multiplicative inverse. For example, the multiplicative inverse is the cosecant of x, and not the inverse sine of \"x\" denoted by or . Only for linear maps are they strongly related (see below). The terminology difference \"reciprocal\" versus \"inverse\" is not sufficient to make this distinction, since many authors prefer the opposite naming convention, probably for historical reasons (for example in French, the inverse function is preferably called bijection réciproque).\n\nIn the real numbers, zero does not have a reciprocal because no real number multiplied by 0 produces 1 (the product of any number with zero is zero). With the exception of zero, reciprocals of every real number are real, reciprocals of every rational number are rational, and reciprocals of every complex number are complex. The property that every element other than zero has a multiplicative inverse is part of the definition of a field, of which these are all examples. On the other hand, no integer other than 1 and −1 has an integer reciprocal, and so the integers are not a field.\n\nIn modular arithmetic, the modular multiplicative inverse of \"a\" is also defined: it is the number \"x\" such that \"ax\" ≡ 1 (mod \"n\"). This multiplicative inverse exists if and only if \"a\" and \"n\" are coprime. For example, the inverse of 3 modulo 11 is 4 because 4 · 3 ≡ 1 (mod 11). The extended Euclidean algorithm may be used to compute it.\n\nThe sedenions are an algebra in which every nonzero element has a multiplicative inverse, but which nonetheless has divisors of zero, i.e. nonzero elements \"x\", \"y\" such that \"xy\" = 0.\n\nA square matrix has an inverse if and only if its determinant has an inverse in the coefficient ring. The linear map that has the matrix \"A\" with respect to some base is then the reciprocal function of the map having \"A\" as matrix in the same base. Thus, the two distinct notions of the inverse of a function are strongly related in this case, while they must be carefully distinguished in the general case (as noted above).\n\nThe trigonometric functions are related by the reciprocal identity: the cotangent is the reciprocal of the tangent; the secant is the reciprocal of the cosine; the cosecant is the reciprocal of the sine.\n\nA ring in which every nonzero element has a multiplicative inverse is a division ring; likewise an algebra in which this holds is a division algebra.\n\nAs mentioned above, the reciprocal of every nonzero complex number is complex. It can be found by multiplying both top and bottom of 1/\"z\" by its complex conjugate formula_1 and using the property that formula_2, the absolute value of \"z\" squared, which is the real number :\n\nIn particular, if ||\"z\"||=1 (\"z\" has unit magnitude), then formula_4. Consequently, the imaginary units, ±, have additive inverse equal to multiplicative inverse, and are the only complex numbers with this property. For example, additive and multiplicative inverses of are −() = − and 1/ = −, respectively.\n\nFor a complex number in polar form , the reciprocal simply takes the reciprocal of the magnitude and the negative of the angle:\n\nIn real calculus, the derivative of is given by the power rule with the power −1:\n\nThe power rule for integrals (Cavalieri's quadrature formula) cannot be used to compute the integral of 1/\"x\", because doing so would result in division by 0:\n\nInstead the integral is given by:\n\nwhere ln is the natural logarithm. To show this, note that formula_10, so if formula_11 and formula_12, we have:\n\nThe reciprocal may be computed by hand with the use of long division.\n\nComputing the reciprocal is important in many division algorithms, since the quotient \"a\"/\"b\" can be computed by first computing 1/\"b\" and then multiplying it by \"a\". Noting that formula_14 has a zero at \"x\" = 1/\"b\", Newton's method can find that zero, starting with a guess formula_15 and iterating using the rule:\n\nThis continues until the desired precision is reached. For example, suppose we wish to compute 1/17 ≈ 0.0588 with 3 digits of precision. Taking \"x\" = 0.1, the following sequence is produced:\nA typical initial guess can be found by rounding \"b\" to a nearby power of 2, then using bit shifts to compute its reciprocal.\n\nIn constructive mathematics, for a real number \"x\" to have a reciprocal, it is not sufficient that \"x\" ≠ 0. There must instead be given a \"rational\" number \"r\" such that 0 < \"r\" < |\"x\"|. In terms of the approximation algorithm described above, this is needed to prove that the change in \"y\" will eventually become arbitrarily small.\n\nThis iteration can also be generalised to a wider sort of inverses, e.g. matrix inverses.\n\nEvery number excluding zero has a reciprocal, and reciprocals of certain irrational numbers can have important special properties. Examples include the reciprocal of e (≈ 0.367879) and the golden ratio's reciprocal (≈ 0.618034). The first reciprocal is special because no other positive number can produce a lower number when put to the power of itself; formula_17 is the global minimum of formula_18. The second number is the only positive number that is equal to its reciprocal plus one:formula_19. Its additive inverse is the only negative number that is equal to its reciprocal minus one:formula_20.\n\nThe function formula_21 gives an infinite number of irrational numbers that differ with their reciprocal by an integer. For example, formula_22 is the irrational formula_23. Its reciprocal formula_24 is formula_25, exactly formula_26 less. Such irrational numbers share a curious property: they have the same fractional part as their reciprocal.\n\nIf the multiplication is associative, an element \"x\" with a multiplicative inverse cannot be a zero divisor (\"x\" is a zero divisor if some nonzero \"y\", ). To see this, it is sufficient to multiply the equation by the inverse of \"x\" (on the left), and then simplify using associativity. In the absence of associativity, the sedenions provide a counterexample.\n\nThe converse does not hold: an element which is not a zero divisor is not guaranteed to have a multiplicative inverse.\nWithin Z, all integers except −1, 0, 1 provide examples; they are not zero divisors nor do they have inverses in Z.\nIf the ring or algebra is finite, however, then all elements \"a\" which are not zero divisors do have a (left and right) inverse. For, first observe that the map must be injective: implies :\nDistinct elements map to distinct elements, so the image consists of the same finite number of elements, and the map is necessarily surjective. Specifically, ƒ (namely multiplication by \"a\") must map some element \"x\" to 1, , so that \"x\" is an inverse for \"a\".\n\nThe expansion of the reciprocal 1/\"q\" in any base can also act as a source of pseudo-random numbers, if \"q\" is a \"suitable\" safe prime, a prime of the form 2\"p\" + 1 where \"p\" is also a prime. A sequence of pseudo-random numbers of length \"q\" − 1 will be produced by the expansion.\n\n\n"}
{"id": "55100201", "url": "https://en.wikipedia.org/wiki?curid=55100201", "title": "Olech theorem", "text": "Olech theorem\n\nIn dynamical systems theory, the Olech theorem establishes sufficient conditions for global asymptotic stability of a two-equation system of non-linear differential equations. The result was established by Czesław Olech in 1963, based on joint work with Philip Hartman.\n\nThe differential equations formula_1, formula_2, where formula_3, for which formula_4 is an equilibrium point, is uniformly globally asymptotically stable if:\n"}
{"id": "25430790", "url": "https://en.wikipedia.org/wiki?curid=25430790", "title": "Proof of Fermat's Last Theorem", "text": "Proof of Fermat's Last Theorem\n\nProof of Fermat's last theorem may refer to:\n"}
{"id": "663426", "url": "https://en.wikipedia.org/wiki?curid=663426", "title": "Quantum logic", "text": "Quantum logic\n\nIn quantum mechanics, quantum logic is a set of rules for reasoning about propositions that takes the principles of quantum theory into account. This research area and its name originated in a 1936 paper by Garrett Birkhoff and John von Neumann, who were attempting to reconcile the apparent inconsistency of classical logic with the facts concerning the measurement of complementary variables in quantum mechanics, such as position and momentum.\n\nQuantum logic can be formulated either as a modified version of propositional logic or as a noncommutative and non-associative many-valued (MV) logic.\n\nQuantum logic has been proposed as the correct logic for propositional inference generally, most notably by the philosopher Hilary Putnam, at least at one point in his career. This thesis was an important ingredient in Putnam's 1968 paper \"Is Logic Empirical?\" in which he analysed the epistemological status of the rules of propositional logic. Putnam attributes the idea that anomalies associated to quantum measurements originate with anomalies in the logic of physics itself to the physicist David Finkelstein. However, this idea had been around for some time and had been revived several years earlier by George Mackey's work on group representations and symmetry.\n\nThe more common view regarding quantum logic, however, is that it provides a formalism for relating observables, system preparation filters and states. In this view, the quantum logic approach resembles more closely the C*-algebraic approach to quantum mechanics. The similarities of the quantum logic formalism to a system of deductive logic may then be regarded more as a curiosity than as a fact of fundamental philosophical importance. A more modern approach to the structure of quantum logic is to assume that it is a diagram – in the sense of category theory – of classical logics (see David Edwards).\n\nQuantum logic has some properties that clearly distinguish it from classical logic, most notably, the failure of the distributive law of propositional logic: \nwhere the symbols \"p\", \"q\" and \"r\" are propositional variables. To illustrate why the distributive law fails, consider a particle moving on a line and (using some system of units where the reduced Planck's constant is 1) let\n\"Note\": The choice of \"p\", \"q\", and \"r\" in this example is intuitive but not formally valid (that is, \"p\" and (\"q\" or \"r\") is also false here); see section \"Quantum logic as the logic of observables\" below for details and a valid example.\n\nWe might observe that:\nin other words, that the particle's momentum is between 0 and +1/6, and its position is between −1 and +3.\nOn the other hand, the propositions \"\"p\" and \"q\" and \"p\" and \"r\"\" are both false, since they assert tighter restrictions on simultaneous values of position and momentum than is allowed by the uncertainty principle (they each have uncertainty 1/3, which is less than the allowed minimum of 1/2). So,\nThus the distributive law fails.\n\nIn his classic 1932 treatise \"Mathematical Foundations of Quantum Mechanics\", John von Neumann noted that projections on a Hilbert space can be viewed as propositions about physical observables. The set of principles for manipulating these quantum propositions was called \"quantum logic\" by von Neumann and Birkhoff in their 1936 paper. George Mackey, in his 1963 book (also called \"Mathematical Foundations of Quantum Mechanics\"), attempted to provide a set of axioms for this propositional system as an \"orthocomplemented lattice\". Mackey viewed elements of this set as potential \"yes or no questions\" an observer might ask about the state of a physical system, questions that would be settled by some measurement. Moreover, Mackey defined a physical observable in terms of these basic questions. Mackey's axiom system is somewhat unsatisfactory though, since it assumes that the partially ordered set is actually given as the orthocomplemented closed subspace lattice of a separable Hilbert space. Piron, Ludwig and others have attempted to give axiomatizations that do not require such explicit relations to the lattice of subspaces.\n\nThe axioms of an orthocomplemented lattice are most commonly stated as algebraic equations concerning the poset and its operations. A set of axioms using instead disjunction (denoted as formula_1) and negation (denoted as formula_2) is as follows:\nAn \"orthomodular lattice\" satisfies the above axioms, and additionally the following one:\n\nAlternative formulations include sequent calculi, and tableaux systems.\n\nThe remainder of this article assumes the reader is familiar with the spectral theory of self-adjoint operators on a Hilbert space. However, the main ideas can be understood using the finite-dimensional spectral theorem.\n\nOne semantics of quantum logic is that quantum logic is the logic of boolean observables in quantum mechanics, where an observable \"p\" is associated with the set of quantum states for which \"p\" (when measured) is true with probability 1 (this completely characterizes the observable). From there,\n\nThus, expressions in quantum logic describe observables using a syntax that resembles classical logic. However, unlike classical logic, the distributive law \"a\" ∧ (\"b\" ∨ \"c\") = (\"a\" ∧ \"b\") ∨ (\"a\" ∧ \"c\") fails when dealing with noncommuting observables, such as position and momentum. This occurs because measurement affects the system, and measurement of whether a disjunction holds does not measure which of the disjuncts is true.\n\nFor an example, consider a simple one-dimensional particle with position denoted by \"x\" and momentum by \"p\", and define observables:\n\nNow, position and momentum are Fourier transforms of each other, and the Fourier transform of a square-integrable nonzero function with a compact support is entire and hence does not have non-isolated zeroes. Therefore, there is no wave function that vanishes at \"x\" ≥ 0 with P(|\"p\"|≤1) = 1. Thus, \"a\" ∧ \"b\" and similarly \"a\" ∧ \"c\" are false, so (\"a\" ∧ \"b\") ∨ (\"a\" ∧ \"c\") is false. However, \"a\" ∧ (\"b\" ∨ \"c\") equals \"a\" and might be true.\n\nTo understand more, let \"p\" and \"p\" be the momenta for the restriction of the particle wave function to \"x\" < 0 and \"x\" ≥ 0 respectively (with the wave function zero outside of the restriction). Let formula_11 be the restriction of \"|p|\" to momenta that are (in absolute value) >1.\n\n(\"a\" ∧ \"b\") ∨ (\"a\" ∧ \"c\") corresponds to states with formula_12 and formula_13 (this holds even if we defined \"p\" differently so as to make such states possible). As an \"operator\", formula_14, and nonzero formula_15 and formula_16 might interfere to produce zero formula_11. Such interference is key to the richness of quantum logic and quantum mechanics.\n\nThe so-called \"Hamiltonian\" formulations of classical mechanics have three ingredients: \"states\", \"observables\" and \"dynamics\". In the simplest case of a single particle moving in R, the state space is the position-momentum space R. We will merely note here that an observable is some real-valued function \"f\" on the state space. Examples of observables are position, momentum or energy of a particle. For classical systems, the value \"f\"(\"x\"), that is the value of \"f\" for some particular system state \"x\", is obtained by a process of measurement of \"f\". The propositions concerning a classical system are generated from basic statements of the form\n\nIt follows easily from this characterization of propositions in classical systems that the corresponding logic is identical to that of some Boolean algebra of subsets of the state space. By logic in this context we mean the rules that relate set operations and ordering relations, such as de Morgan's laws. These are analogous to the rules relating boolean conjunctives and material implication in classical propositional logic. For technical reasons, we will also assume that the algebra of subsets of the state space is that of all Borel sets. The set of propositions is ordered by the natural ordering of sets and has a complementation operation. In terms of observables, the complement of the proposition {\"f\" ≥ \"a\"} is {\"f\" < \"a\"}.\n\nWe summarize these remarks as follows: The proposition system of a classical system is a lattice with a distinguished \"orthocomplementation\" operation: The lattice operations of \"meet\" and \"join\" are respectively set intersection and set union. The orthocomplementation operation is set complement. Moreover, this lattice is \"sequentially complete\", in the sense that any sequence {\"E\"} of elements of the lattice has a least upper bound, specifically the set-theoretic union:\n\nIn the Hilbert space formulation of quantum mechanics as presented by von Neumann, a physical observable is represented by some (possibly unbounded) densely defined self-adjoint operator \"A\" on a Hilbert space \"H\". \"A\" has a spectral decomposition, which is a projection-valued measure E defined on the Borel subsets of R. In particular, for any bounded Borel function \"f\" on R, the following extension of \"f\" to operators can be made:\n\nIn case \"f\" is the indicator function of an interval [\"a\", \"b\"], the operator \"f\"(\"A\") is a self-adjoint projection, and can be interpreted as the quantum analogue of the classical proposition\n\nThis suggests the following quantum mechanical replacement for the orthocomplemented lattice of propositions in classical mechanics. This is essentially Mackey's \"Axiom VII\":\n\n\n\"Q\" is also sequentially complete: any pairwise disjoint sequence{\"V\"} of elements of \"Q\" has a least upper bound. Here disjointness of \"W\" and \"W\" means \"W\" is a subspace of \"W\". The least upper bound of {\"V\"} is the closed internal direct sum.\n\nHenceforth we identify elements of \"Q\" with self-adjoint projections on the Hilbert space \"H\".\n\nThe structure of \"Q\" immediately points to a difference with the partial order structure of a classical proposition system. In the classical case, given a proposition \"p\", the equations\nhave exactly one solution, namely the set-theoretic complement of \"p\". In these equations \"I\" refers to the atomic proposition that is identically true and \"0\" the atomic proposition that is identically false. In the case of the lattice of projections there are infinitely many solutions to the above equations (any closed, algebraic complement of \"p\" solves it; it need not be the orthocomplement).\n\nHaving made these preliminary remarks, we turn everything around and attempt to define observables within the projection lattice framework and using this definition establish the correspondence between self-adjoint operators and observables: A \"Mackey observable\" is a countably additive homomorphism from the orthocomplemented lattice of the Borel subsets of R to \"Q\". To say the mapping φ is a countably additive homomorphism means that for any sequence {\"S\"} of pairwise disjoint Borel subsets of R, {φ(\"S\")} are pairwise orthogonal projections and\n\nEffectively, then, a Mackey observable is a projection-valued measure on R.\n\nTheorem. There is a bijective correspondence between Mackey observables and densely defined self-adjoint operators on \"H\".\n\nThis is the content of the spectral theorem as stated in terms of spectral measures.\n\nImagine a forensics lab that has some apparatus to measure the speed of a bullet fired from a gun. Under carefully controlled conditions of temperature, humidity, pressure and so on the same gun is fired repeatedly and speed measurements taken. This produces some distribution of speeds. Though we will not get exactly the same value for each individual measurement, for each cluster of measurements, we would expect the experiment to lead to the same distribution of speeds. In particular, we can expect to assign probability distributions to propositions such as {\"a\" ≤ speed ≤ \"b\"}. This leads naturally to propose that under controlled conditions of preparation, the measurement of a classical system can be described by a probability measure on the state space. This same statistical structure is also present in quantum mechanics.\n\nA \"quantum probability measure\" is a function P defined on \"Q\" with values in [0,1] such that P(0)=0, P(I)=1 and if {\"E\"} is a sequence of pairwise orthogonal elements of \"Q\" then\n\nThe following highly non-trivial theorem is due to Andrew Gleason:\n\nTheorem. Suppose \"Q\" is a separable Hilbert space of complex dimension at least 3. Then for any quantum probability measure \"P\" on \"Q\" there exists a unique trace class operator \"S\" such that\nfor any self-adjoint projection \"E\" in \"Q\".\n\nThe operator \"S\" is necessarily non-negative (that is all eigenvalues are non-negative) and of trace 1. Such an operator is often called a \"density operator\".\n\nPhysicists commonly regard a density operator as being represented by a (possibly infinite) density matrix relative to some orthonormal basis.\n\nFor more information on statistics of quantum systems, see quantum statistical mechanics.\n\nAn \"automorphism\" of \"Q\" is a bijective mapping α:\"Q\" → \"Q\" that preserves the orthocomplemented structure of \"Q\", that is\n\nfor any sequence {\"E\"} of pairwise orthogonal self-adjoint projections. Note that this property implies monotonicity of α. If P is a quantum probability measure on \"Q\", then \"E\" → α(\"E\") is also a quantum probability measure on \"Q\". By the Gleason theorem characterizing quantum probability measures quoted above, any automorphism α induces a mapping α* on the density operators by the following formula:\n\nThe mapping α* is bijective and preserves convex combinations of density operators. This means\nwhenever 1 = \"r\" + \"r\" and \"r\", \"r\" are non-negative real numbers. Now we use a theorem of Richard V. Kadison:\n\nTheorem. Suppose β is a bijective map from density operators to density operators that is convexity preserving. Then there is an operator \"U\" on the Hilbert space that is either linear or conjugate-linear, preserves the inner product and is such that\n\nfor every density operator \"S\". In the first case we say \"U\" is unitary, in the second case \"U\" is anti-unitary.\nRemark. This note is included for technical accuracy only, and should not concern most readers. The result quoted above is not directly stated in Kadison's paper, but can be reduced to it by noting first that β extends to a positive trace preserving map on the trace class operators, then applying duality and finally applying a result of Kadison's paper.\n\nThe operator \"U\" is not quite unique; if \"r\" is a complex scalar of modulus 1, then r \"U\" will be unitary or anti-unitary if \"U\" is and will implement the same automorphism. In fact, this is the only ambiguity possible.\n\nIt follows that automorphisms of \"Q\" are in bijective correspondence to unitary or anti-unitary operators modulo multiplication by scalars of modulus 1. Moreover, we can regard automorphisms in two equivalent ways: as operating on states (represented as density operators) or as operating on \"Q\".\n\nIn non-relativistic physical systems, there is no ambiguity in referring to time evolution since there is a global time parameter. Moreover, an isolated quantum system evolves in a deterministic way: if the system is in a state \"S\" at time \"t\" then at time \"s\" > \"t\", the system is in a state F(\"S\"). Moreover, we assume\n\n\nBy Kadison's theorem, there is a 1-parameter family of unitary or anti-unitary operators {\"U\"} such that\n\nIn fact,\n\nTheorem. Under the above assumptions, there is a strongly continuous 1-parameter group of unitary operators {\"U\"} such that the above equation holds.\n\nNote that it follows easily from uniqueness from Kadison's theorem that\n\nwhere σ(t,s) has modulus 1. Now the square of an anti-unitary is a unitary, so that all the \"U\" are unitary. The remainder of the argument shows that σ(t,s) can be chosen to be 1 (by modifying each \"U\" by a scalar of modulus 1.)\n\nA convex combination of statistical states \"S\" and \"S\" is a state of the form \"S\" = \"p\" \"S\" +\"p\" \"S\" where \"p\", \"p\" are non-negative and \"p\" + \"p\" =1. Considering the statistical state of system as specified by lab conditions used for its preparation, the convex combination \"S\" can be regarded as the state formed in the following way: toss a biased coin with outcome probabilities \"p\", \"p\" and depending on outcome choose system prepared to \"S\" or \"S\"\n\nDensity operators form a convex set. The convex set of density operators has extreme points; these are the density operators given by a projection onto a one-dimensional space. To see that any extreme point is such a projection, note that by the spectral theorem \"S\" can be represented by a diagonal matrix; since \"S\" is non-negative all the entries are non-negative and since \"S\" has trace 1, the diagonal entries must add up to 1. Now if it happens that the diagonal matrix has more than one non-zero entry it is clear that we can express it as a convex combination of other density operators.\n\nThe extreme points of the set of density operators are called pure states. If \"S\" is the projection on the 1-dimensional space generated by a vector ψ of norm 1 then\nfor any \"E\" in \"Q\". In physics jargon, if\nwhere ψ has norm 1, then\n\nThus pure states can be identified with \"rays\" in the Hilbert space \"H\".\n\nConsider a quantum mechanical system with lattice \"Q\" that is in some statistical state given by a density operator \"S\". This essentially means an ensemble of systems specified by a repeatable lab preparation process. The result of a cluster of measurements intended to determine the truth value of proposition \"E\", is just as in the classical case, a probability distribution of truth values T and F. Say the probabilities are \"p\" for T and \"q\" = 1 − \"p\" for F. By the previous section \"p\" = Tr(\"S\" \"E\") and \"q\" = Tr(\"S\" (\"I\" − \"E\")).\n\nPerhaps the most fundamental difference between classical and quantum systems is the following: regardless of what process is used to determine \"E\" immediately after the measurement the system will be in one of two statistical states:\n(We leave to the reader the handling of the degenerate cases in which the denominators may be 0.) We now form the convex combination of these two ensembles using the relative frequencies \"p\" and \"q\". We thus obtain the result that the measurement process applied to a statistical ensemble in state \"S\" yields another ensemble in statistical state:\n\nWe see that a pure ensemble becomes a mixed ensemble after measurement. Measurement, as described above, is a special case of quantum operations.\n\nQuantum logic derived from propositional logic provides a satisfactory foundation for a theory of reversible quantum processes. Examples of such processes are the covariance transformations relating two frames of reference, such as change of time parameter or the transformations of special relativity. Quantum logic also provides a satisfactory understanding of density matrices. Quantum logic can be stretched to account for some kinds of measurement processes corresponding to answering yes-no questions about the state of a quantum system. However, for more general kinds of measurement operations (that is quantum operations), a more complete theory of filtering processes is necessary. Such a theory of quantum filtering was developed in the late 1970s and 1980s by Belavkin (see also Bouten et al.). A similar approach is provided by the consistent histories formalism. On the other hand, quantum logics derived from many-valued logic extend its range of applicability to irreversible quantum processes or 'open' quantum systems.\n\nIn any case, these quantum logic formalisms must be generalized in order to deal with super-geometry (which is needed to handle Fermi-fields) and non-commutative geometry (which is needed in string theory and quantum gravity theory). Both of these theories use a partial algebra with an \"integral\" or \"trace\". The elements of the partial algebra are not observables; instead the \"trace\" yields \"greens functions\", which generate scattering amplitudes. One thus obtains a local S-matrix theory (see D. Edwards).\n\nIn 2004, Prakash Panangaden described how to capture the kinematics of quantum causal evolution using System BV, a deep inference logic originally developed for use in structural proof theory. Alessio Guglielmi, Lutz Straßburger, and Richard Blute have also done work in this area.\n\n\n\n"}
{"id": "7785594", "url": "https://en.wikipedia.org/wiki?curid=7785594", "title": "Rado graph", "text": "Rado graph\n\nIn the mathematical field of graph theory, the Rado graph, Erdős–Rényi graph, or random graph is a countably infinite graph that can be constructed (with probability one) by choosing independently at random for each pair of its vertices whether to connect the vertices by an edge. The same graph can also be constructed non-randomly, by symmetrizing the membership relation of the hereditarily finite sets, by applying the BIT predicate to the binary representations of the natural numbers, or as an infinite Paley graph that has edges connecting pairs of prime numbers congruent to 1 mod 4 that are quadratic residues modulo each other.\n\nEvery finite or countably infinite graph is an induced subgraph of the Rado graph, and can be found as an induced subgraph by a greedy algorithm that builds up the subgraph one vertex at a time. The Rado graph is uniquely defined, among countable graphs, by an \"extension property\" that guarantees the correctness of this algorithm: no matter which vertices have already been chosen to form part of the induced subgraph, and no matter what pattern of adjacencies is needed to extend the subgraph by one more vertex, there will always exist another vertex with that pattern of adjacencies that the greedy algorithm can choose.\n\nThe Rado graph is highly symmetric: any isomorphism of its induced subgraphs can be extended to a symmetry of the whole graph.\nThe first-order logic sentences that are true of the Rado graph are also true of almost all random finite graphs, and the sentences that are false for the Rado graph are also false for almost all finite graphs. In model theory, the Rado graph forms an example of a saturated model of an ω-categorical and complete theory.\n\nThe names of this graph honor Richard Rado, Paul Erdős, and Alfréd Rényi, mathematicians who studied it in the early 1960s. It appears even earlier in the work of .\n\nThe Rado graph was first constructed by in two ways, with vertices either the hereditarily finite sets or the natural numbers. (Strictly speaking Ackermann described a directed graph, and the Rado graph is the corresponding undirected graph given by forgetting the directions on the edges.) constructed the Rado graph as the random graph on a countable number of points. They proved that it has infinitely many automorphisms, and their argument also shows that it is unique though they did not mention this explicitly. rediscovered the Rado graph as a universal graph, and gave an explicit construction of it with vertex set the natural numbers. Rado's construction is essentially equivalent to one of Ackermann's constructions.\n\n and constructed the Rado graph using the BIT predicate as follows. They identified the vertices of the graph with the natural numbers 0, 1, 2, ...\nAn edge connects vertices \"x\" and \"y\" in the graph (where \"x\" < \"y\") whenever the \"x\"th bit of the binary representation of \"y\" is nonzero. Thus, for instance, the neighbors of vertex 0 consist of all odd-numbered vertices, because the numbers whose 0th bit is nonzero are exactly the odd numbers. Vertex 1 has one smaller neighbor, vertex 0, as 1 is odd and vertex 0 is connected to all odd vertices. The larger neighbors of vertex 1 are all vertices with numbers congruent to 2 or 3 modulo 4, because those are exactly the numbers with a nonzero bit at index 1.\n\nThe Rado graph arises almost surely in the Erdős–Rényi model of a random graph on countably many vertices. Specifically, one may form an infinite graph by choosing, independently and with probability 1/2 for each pair of vertices, whether to connect the two vertices by an edge. With probability 1 the resulting graph is isomorphic to the Rado graph.\nThis construction also works if any fixed probability \"p\" not equal to 0 or 1 is used in place of 1/2.\n\nThis result, shown by , justifies the definite article in the common alternative name “the random graph” for the Rado graph.\nFor finite graphs, repeatedly drawing a graph from the Erdős–Rényi model will often lead to different graphs, but for countably infinite graphs the model almost always produces the same graph.\n\nFor any graph generated randomly in this way, the complement graph can be obtained at the same time by reversing all the choices: including an edge when the first graph did not include the same edge, and vice versa. This construction of the complement graph is an instance of the same process of choosing randomly and independently whether to include each edge, so it also (with probability 1) generates the Rado graph. Therefore, the Rado graph is a self-complementary graph.\n\nIn one of Ackermann's original 1937 constructions, the vertices of the Rado graph are indexed by the hereditarily finite sets, and there is an edge between two vertices exactly when one of the corresponding finite sets is a member of the other.\nA similar construction can be based on Skolem's paradox, the fact that there exists a countable model for the first-order theory of sets. One can construct the Rado graph from such a model by creating a vertex for each set, with an edge connecting each pair of sets where one set in the pair is a member of the other.\n\nThe Rado graph may also be formed by a construction resembling that for Paley graphs. Take as the vertices of a graph all the prime numbers that are congruent to 1 modulo 4, and connect two vertices by an edge whenever one of the two numbers is a quadratic residue modulo the other. By quadratic reciprocity and the restriction of the vertices to primes congruent to 1 mod 4, this is a symmetric relation, so it defines an undirected graph, which turns out to be isomorphic to the Rado graph.\n\nAnother construction of the Rado graph shows that it is an infinite circulant graph, with the integers as its vertices and with an edge between each two integers whose distance (the absolute value of their difference) belongs to a particular set \"S\". To construct the Rado graph in this way, \"S\" may be chosen randomly, or by choosing the indicator function of \"S\" to be the concatenation of all finite binary sequences.\n\nThe Rado graph can also be constructed as the block intersection graph of an infinite block design in which the number of points and the size of each block are countably infinite.\n\nThe Rado graph satisfies the following extension property: for every two disjoint finite sets of vertices \"U\" and \"V\", there exists a vertex \"x\" outside both sets that is connected to all vertices in \"U\", but has no neighbors in \"V\".\nFor instance, with the binary-number definition of the Rado graph, let\nThen the nonzero bits in the binary representation of \"x\" cause it to be adjacent to everything in \"U\". However, \"x\" has no nonzero bits in its binary representation corresponding to vertices in \"V\", and \"x\" is so large that the \"x\"th bit of every element of \"V\" is zero. Thus, \"x\" is not adjacent to any vertex in \"V\".\n\nWith the random-graph definition of the Rado graph, each vertex outside the union of \"U\" and \"V\" has probability 1/2 of fulfilling the extension property, independently of the other vertices. Because there are infinitely many vertices to choose from, each with the same finite probability of success, the probability is one that there exists a vertex that fulfils the extension property. With the Paley graph definition, for any sets \"U\" and \"V\", by the Chinese remainder theorem, the numbers that are quadratic residues modulo every prime in \"U\" and nonresidues modulo every prime in \"V\" form a periodic sequence, so by Dirichlet's theorem on primes in arithmetic progressions this number-theoretic graph has the extension property.\n\nThe extension property can be used to build up isomorphic copies of any finite or countably infinite graph \"G\" within the Rado graph.\nTo do so, order the vertices of \"G\", and add vertices in the same order to a partial copy of \"G\" within the Rado graph.\nAt each step, the next vertex in \"G\" will be adjacent to some set \"U\" of vertices in \"G\" that are earlier in the ordering of the vertices,\nand non-adjacent to the remaining set \"V\" of earlier vertices in \"G\".\nBy the extension property, the Rado graph will also have a vertex \"x\" that is adjacent to all the vertices in the partial copy that correspond to members of \"U\",\nand non-adjacent to all the vertices in the partial copy that correspond to members of \"V\". Adding \"x\" to the partial copy of \"G\" produces a larger partial copy, with one more vertex.\n\nThis method forms the basis for a proof by induction, with the 0-vertex subgraph as its base case, that every finite or countably infinite graph is an induced subgraph of the Rado graph.\n\nThe Rado graph is, up to graph isomorphism, the only countable graph with the extension property. For, let \"G\" and \"H\" be two countable graphs with the extension property, let \"G\" and \"H\" be isomorphic finite induced subgraphs of \"G\" and \"H\" respectively, and let \"g\" and \"h\" be the first vertices in an enumeration of the vertices of \"G\" and \"H\" respectively that do not belong to \"G\" and \"H\". Then, by applying the extension property twice, one can find isomorphic induced subgraphs \"G\" and \"H\" that include \"g\" and \"h\" together with all the vertices of the previous subgraphs. By repeating this process, one may build up a sequence of isomorphisms between induced subgraphs that eventually includes every vertex in \"G\" and \"H\". Thus, by the back-and-forth method, \"G\" and \"H\" must be isomorphic.\nBecause the graphs constructed by the random graph construction, binary number construction, and Paley graph construction are all countable graphs with the extension property, this argument shows that they are all isomorphic to each other.\n\nApplying the back-and-forth construction to any two isomorphic finite subgraphs of the Rado graph extends their isomorphism to an automorphism of the entire Rado graph. The fact that every isomorphism of finite subgraphs extends to an automorphism of the whole graph is expressed by saying that the Rado graph is \"ultrahomogeneous\". In particular, there is an automorphism taking any ordered pair of adjacent vertices to any other such ordered pair, so the Rado graph is a symmetric graph.\n\nThe automorphism group of the Rado graph is a simple group, whose number of elements is the cardinality of the continuum. Every subgroup of this group whose index is less than the cardinality of the continuum can be sandwiched between the pointwise stabilizer and the stabilizer of a finite set of vertices.\n\nThe construction of the Rado graph as an infinite circulant graph shows that its symmetry group includes automorphisms that generate a transitive infinite cyclic group. The difference set of this construction (the set of distances in the integers between adjacent vertices) can be constrained to include the difference 1, without affecting the correctness of this construction, from which it follows that the Rado graph contains an infinite Hamiltonian path whose symmetries are a subgroup of the symmetries of the whole graph.\n\nIf a graph \"G\" is formed from the Rado graph by deleting any finite number of edges or vertices, or adding a finite number of edges, the change does not affect the extension property of the graph. For any pair of sets \"U\" and \"V\" it is still possible to find a vertex in the modified graph that is adjacent to everything in \"U\" and nonadjacent to everything in \"V\", by adding the modified parts of \"G\" to \"V\" and applying the extension property in the unmodified Rado graph. Therefore, any finite modification of this type results in a graph that is isomorphic to the Rado graph.\n\nFor any partition of the vertices of the Rado graph into two sets \"A\" and \"B\", or more generally for any partition into finitely many subsets, at least one of the subgraphs induced by one of the partition sets is isomorphic to the whole Rado graph. gives the following short proof: if none of the parts induces a subgraph isomorphic to the Rado graph, they all fail to have the extension property, and one can find pairs of sets \"U\" and \"V\" that cannot be extended within each subgraph. But then, the union of the sets \"U\" and the union of the sets \"V\" would form a set that could not be extended in the whole graph, contradicting the Rado graph's extension property. This property of being isomorphic to one of the induced subgraphs of any partition is held by only three countably infinite undirected graphs: the Rado graph, the complete graph, and the empty graph. and investigate infinite directed graphs with the same partition property; all are formed by choosing orientations for the edges of the complete graph or the Rado graph.\n\nA related result concerns edge partitions instead of vertex partitions: for every partition of the edges of the Rado graph into finitely many sets, there is a subgraph isomorphic to the whole Rado graph that uses at most two of the colors. However, there may not necessarily exist an isomorphic subgraph that uses only one color of edges.\n\n used the Rado graph to prove a zero–one law for first-order statements in the logic of graphs. When a logical statement of this type is true or false for the Rado graph, it is also true or false (respectively) for almost all finite graphs.\n\nThe first-order language of graphs is the collection of well-formed sentences in mathematical logic formed from variables representing the vertices of graphs, universal and existential quantifiers, logical connectives, and predicates for equality and adjacency of vertices. For instance, the condition that a graph does not have any isolated vertices may be expressed by the sentence\nwhere the formula_3 symbol indicates the adjacency relation between two vertices.\nThis sentence formula_4 is true for some graphs, and false for others; a graph formula_5 is said to \"model\" formula_4, written formula_7, if formula_4 is true of the vertices and adjacency relation of formula_5.\n\nThe extension property of the Rado graph may be expressed by a collection of first-order sentences formula_10, stating that for every choice of formula_11 vertices in a set formula_12 and formula_13 vertices in a set formula_14, all distinct, there exists a vertex adjacent to everything in formula_12 and nonadjacent to everything in formula_14. For instance, formula_17 can be written as\n\n proved that the sentences formula_10, together with additional sentences stating that the adjacency relation is symmetric and antireflexive (that is, that a graph modeling these sentences is undirected and has no self-loops), are the axioms of a complete theory. This means that, for each first-order sentence formula_4, exactly one of formula_4 and its negation can be proven from these axioms.\nBecause the Rado graph models the extension axioms, it models all sentences in this theory.\n\nIn logic, a theory that has only one model (up to isomorphism) with a given infinite cardinality is called -categorical. The fact that the Rado graph is the unique countable graph with the extension property implies that it is also the unique countable model for its theory. This uniqueness property of the Rado graph can be expressed by saying that the theory of the Rado graph is ω-categorical. Łoś and Vaught proved in 1954 that when a theory is –categorical (for some infinite cardinal ) and, in addition, has no finite models, then the theory must be complete. Therefore, Gaifman's theorem that the theory of the Rado graph is complete follows from the uniqueness of the Rado graph by the Łoś–Vaught test.\n\nAs proved, the first-order sentences provable from the extension axioms and modeled by the Rado graph are exactly the sentences true for almost all random finite graphs. This means that if one chooses an -vertex graph uniformly at random among all graphs on labeled vertices, then the probability that such a sentence will be true for the chosen graph approaches one in the limit as approaches infinity. Symmetrically, the sentences that are not modeled by the Rado graph are false for almost all random finite graphs. It follows that every first-order sentence is either almost always true or almost always false for random finite graphs, and these two possibilities can be distinguished by determining whether the Rado graph models the sentence. Fagin's proof uses the compactness theorem, Based on this equivalence, the theory of sentences modeled by the Rado graph has been called \"the theory of the random graph\" or \"the almost sure theory of graphs\".\n\nBecause of this 0-1 law, it is possible to test whether any particular first-order sentence is modeled by the Rado graph in a finite amount of time, by choosing a large enough value of and counting the number of -vertex graphs that model the sentence. However, here, \"large enough\" is at least exponential in the size of the sentence. For instance the extension axiom implies the existence of a -vertex clique, but a clique of that size exists with high probability only in random graphs of size exponential in .\nIt is unlikely that determining whether the Rado graph models a given sentence can be done more quickly than exponential time, as the problem is PSPACE-complete.\n\nFrom the model theoretic point of view, the Rado graph is an example of a saturated model. This is just a logical formulation of the property that the Rado graph contains all finite graphs as induced subgraphs.\n\nIn this context, a type is a set of variables together with a collection of constraints on the values of some or all of the predicates determined by those variables; a complete type is a type that constrains all of the predicates determined by its variables. In the theory of graphs, the variables represent vertices and the predicates are the adjacencies between vertices, so a complete type specifies whether an edge is present or absent between every pair of vertices represented by the given variables. That is, a complete type specifies the subgraph that a particular set of vertex variables induces.\n\nA saturated model is a model that realizes all of the types that have a number of variables at most equal to the cardinality of the model. The Rado graph has induced subgraphs of all finite or countably infinite types, so it is saturated.\n\nAlthough the Rado graph is universal for induced subgraphs, it is not universal for isometric embeddings of graphs,\nwhere an isometric embedding is a graph isomorphism which preserves distance. The Rado graph has diameter two, and so any graph with larger diameter does not embed isometrically into it. has investigated universal graphs for isometric embedding; he finds a family of universal graphs, one for each possible finite graph diameter. The graph in his family with diameter two is the Rado graph.\n\nThe Henson graphs are countable graphs (one for each positive integer ) that do not contain an -vertex clique, and are universal for -clique-free graphs. They can be constructed as induced subgraphs of the Rado graph. The Rado graph, the Henson graphs, the disjoint unions of isomorphic cliques, and the complements of disjoint unions of cliques are the only possible countably infinite homogeneous graphs.\n\nThe universality property of the Rado graph can be extended to edge-colored graphs; that is, graphs in which the edges have been assigned to different color classes, but without the usual edge coloring requirement that each color class form a matching. For any finite or countably infinite number of colors χ, there exists a unique countably-infinite χ-edge-colored graph \"G\" such that every partial isomorphism of a χ-edge-colored finite graph can be extended to a full isomorphism. With this notation, the Rado graph is just \"G\". investigates the automorphism groups of this more general family of graphs.\n\n"}
{"id": "157175", "url": "https://en.wikipedia.org/wiki?curid=157175", "title": "Ramsey theory", "text": "Ramsey theory\n\nRamsey theory, named after the British mathematician and philosopher Frank P. Ramsey, is a branch of mathematics that studies the conditions under which order must appear. Problems in Ramsey theory typically ask a question of the form: \"how many elements of some structure must there be to guarantee that a particular property will hold?\" More specifically, Ron Graham describes Ramsey theory as a \"branch of combinatorics\".\n\nA typical result in Ramsey theory starts with some mathematical structure that\nis then cut into pieces. How big must the original structure be in order to ensure that at least one of the pieces has a given interesting property? This idea can be defined as partition regularity.\n\nFor example, consider a complete graph of order \"n\"; that is, there are \"n\" vertices and each vertex is connected to every other vertex by an edge. A complete graph of order 3 is called a triangle. Now colour every edge red or blue. How large must \"n\" be in order to ensure that there is either a blue triangle or a red triangle? It turns out that the answer is 6. See the article on Ramsey's theorem for a rigorous proof.\n\nAnother way to express this result is as follows: at any party with at least six people, there are three people who are all either mutual acquaintances (each one knows the other two) or mutual strangers (each one does not know either of the other two). See theorem on friends and strangers.\n\nThis also is a special case of Ramsey's theorem, which says that for any given integer \"c\", any given integers \"n\"...,\"n\", there is a number, \"R\"(\"n\"...,\"n\"), such that if the edges of a complete graph of order \"R\"(\"n\"...,\"n\") are coloured with \"c\" different colours, then for some \"i\" between 1 and \"c\", it must contain a complete subgraph of order \"n\" whose edges are all colour \"i\". The special case above has \"c\" = 2 and \"n\" = \"n\" = 3.\n\nTwo key theorems of Ramsey theory are:\n\n\nA theorem similar to van der Waerden's theorem is \"Schur's theorem\": for any given \"c\" there is a number \"N\" such that if the numbers 1, 2, ..., \"N\" are coloured with \"c\" different colours, then there must be a pair of integers \"x\", \"y\" such that \"x\", \"y\", and \"x\"+\"y\" are all the same colour. Many generalizations of this theorem exist, including Rado's theorem, Rado–Folkman–Sanders theorem, Hindman's theorem, and the Milliken–Taylor theorem. A classic reference for these and many other results in Ramsey theory is Graham, Rothschild, Spencer and Solymosi, updated and expanded in 2015 to its first new edition in 25 years.\n\nResults in Ramsey theory typically have two primary characteristics. Firstly, they are non-constructive: they may show that some structure exists, but they give no process for finding this structure (other than brute-force search). For instance, the pigeonhole principle is of this form. Secondly, while Ramsey theory results do say that sufficiently large objects must necessarily contain a given structure, often the proof of these results requires these objects to be enormously large – bounds that grow exponentially, or even as fast as the Ackermann function are not uncommon. In many cases these bounds are artifacts of the proof, and it is not known whether they can be substantially improved. In other cases it is known that any bound must be extraordinarily large, sometimes even greater than any primitive recursive function; see the Paris–Harrington theorem for an example. Graham's number, one of the largest numbers ever used in serious mathematical proof, is an upper bound for a problem related to Ramsey theory. Another large example is the Boolean Pythagorean triples problem.\n\nTheorems in Ramsey theory are generally one of the two types. Many theorems, which are modeled after Ramsey's theorem itself, assert that in every partition of a large structured object, one of the classes necessarily contains a large structured subobject, but give no information about which class this is. Occasionally, the reason behind such \"Ramsey-type\" results is that the largest partition class always contains the desired substructure. The results of this kind are called either \"density results\" or \"Turán-type result\", after Turán's theorem. Notable examples include Szemerédi's theorem, which is such a strengthening of van der Waerden's theorem, and the density version of the Hales-Jewett theorem.\n\n\n"}
{"id": "171950", "url": "https://en.wikipedia.org/wiki?curid=171950", "title": "Root of unity", "text": "Root of unity\n\nIn mathematics, a root of unity, occasionally called a de Moivre number, is any complex number that gives 1 when raised to some positive integer power . Roots of unity are used in many branches of mathematics, and are especially important in number theory, the theory of group characters, and the discrete Fourier transform.\n\nRoots of unity can be defined in any field. If the characteristic of the field is zero, they are complex numbers that are also algebraic integers. In positive characteristic, they belong to a finite field, and, conversely, every nonzero element of a finite field is a root of unity. Any algebraically closed field contains exactly th roots of unity, except if is a multiple of the (positive) characteristic of the field.\n\nAn th root of unity, where is a positive integer (i.e. ), is a number satisfying the equation\n\nUnless otherwise specified, the roots of unity may be taken to be complex numbers (including the number 1, and the number –1 if \"n\" is even, which are complex with a zero imaginary part), and in this case, the th roots of unity are\n\nHowever the defining equation of roots of unity is meaningful over any field (and even over any ring) , and this allows considering roots of unity in . Whichever is the field , the roots of unity in are either complex numbers, if the characteristic of is 0, or, otherwise, belong to a finite field. Conversely, every nonzero element in a finite field is a root of unity in that field. See Root of unity modulo \"n\" and Finite field for further details.\n\nAn th root of unity is if it is not a th root of unity for some smaller , that is if\n\nIf \"n\" is a prime number, all th roots of unity, except 1, are primitive.\n\nIn the above formula in terms of exponential and trigonometric functions, the primitive th roots of unity are those for which and are coprime integers.\n\nSubsequent sections of this article will comply with complex roots of unity. For the case of roots of unity in fields of nonzero characteristic, see . For the case of roots of unity in rings of modular integers, see Root of unity modulo n.\n\nEvery th root of unity is a primitive th root of unity for some , which is the smallest positive integer such that . \n\nIf is an th root of unity and then . In fact, by the definition of congruence, for some integer , and\n\nTherefore, given a power of , one has , where is the remainder of the Euclidean division of by .\n\nAny integer power of an th root of unity is also an th root of unity, as\nThis is also true for negative exponents. In particular, the reciprocal of an th root of unity is its complex conjugate, and is also an th root of unity:\n\nLet be a primitive th root of unity. Then the powers , , ..., , are th root of unity and are all distinct. (If where , then , which would imply that would not be primitive.) This implies that , ..., , are all of the th roots of unity, since an th-degree polynomial equation has at most distinct solutions.\n\nFrom the preceding, it follows that, if is a primitive th root of unity, then formula_7 if and only if formula_8\nIf is not primitive then formula_9 implies formula_10 but the converse may be false, as shown by the following example. If , a non-primitive th root of unity is , and one has formula_11, although formula_12\n\nLet be a primitive th root of unity. A power of is a primitive th root of unity for \nwhere formula_14 is the greatest common divisor of and . This results from the fact that is the smallest multiple of that is also a multiple of . In other words, is the least common multiple of and . Thus \n\nThus, if and are coprime, is also a primitive th root of unity, and therefore there are (where is Euler's totient function) distinct primitive th roots of unity. (This implies that if is a prime number, all the roots except are primitive.)\n\nIn other words, if is the set of all th roots of unity and is the set of primitive ones, is a disjoint union of the :\n\nwhere the notation means that goes through all the divisors of , including and .\n\nSince the cardinality of is , and that of is , this demonstrates the classical formula\n\nThe product and the multiplicative inverse of two roots of unity are also roots of unity. In fact, if and , then , and , where is the least common multiple of and .\n\nTherefore, the roots of unity form an abelian group under multiplication. This group is the torsion subgroup of the circle group\n\nThe product and the multiplicative inverse of two th roots of unity are also th roots of unity. Therefore, the th roots of unity form a group under multiplication.\n\nGiven a primitive th root of unity , the other th roots are powers of . This means that the group of the th roots of unity is a cyclic group. It is worth remarking that the term of \"cyclic group\" originated from the fact that this group is a subgroup of the circle group.\n\nLet formula_18 be the field extension of the rational numbers generated over formula_19 by a primitive th root of unity . As every th root of unity is a power of , the field formula_18 contains all th roots of unity, and formula_18 is a Galois extension of formula_22\n\nIf is an integer, is a primitive th root of unity if and only if and are coprime. In this case, the map\ninduces an automorphism of formula_18, which maps every th root of unity to its th power. Every automorphism of formula_18 is obtained in this way, and these automorphisms form the Galois group of formula_18 over the field of the rationals.\n\nThe rules of exponentiation imply that the composition of two such automorphisms is obtained by multiplying the exponents. It follows that the map\ndefines a group isomorphism between the units of the ring of integers modulo and the Galois group of formula_28\n\nThis shows that this Galois group is abelian, and implies thus that the primitive roots of unity may be expressed in terms of radicals.\n\nDe Moivre's formula, which is valid for all real and integers , is\n\nSetting gives a primitive th root of unity, one gets\n\nbut\nfor . In other words, \nis a primitive th root of unity.\n\nThis formula shows that on the complex plane the th roots of unity are at the vertices of a regular -sided polygon inscribed in the unit circle, with one vertex at 1. (See the plots for and on the right.) This geometric fact accounts for the term \"cyclotomic\" in such phrases as cyclotomic field and cyclotomic polynomial; it is from the Greek roots \"cyclo\" (circle) plus \"tomos\" (cut, divide).\n\nEuler's formula\n\nwhich is valid for all real , can be used to put the formula for the th roots of unity into the form\n\nIt follows from the discussion in the previous section that this is a primitive th-root if and only if the fraction is in lowest terms, i.e. that and are coprime.\n\nThe th roots of unity are, by definition, the roots of the polynomial , and are thus algebraic numbers. As this polynomial is not irreducible (except for ), the primitive th roots of unity are roots of an irreducible polynomial of lower degree, called the cyclotomic polynomial, and often denoted . The degree of is given by Euler's totient function, which counts (among other things) the number of primitive th roots of unity. The roots of are exactly the primitive th roots of unity.\n\nGalois theory can be used to show that cyclotomic polynomials may be conveniently solved in terms of radicals. (The trivial form formula_35 is not convenient, because it contains non-primitive roots, such as 1, which are not roots of the cyclotomic polynomial, and because it does not give the real and imaginary parts separately.) This means that, for each positive integer , there exists an expression built from integers by root extractions, additions, subtractions, multiplications, and divisions (nothing else), such that the primitive th roots of unity are exactly the set of values that can be obtained by choosing values for the root extractions ( possible values for a th root). (For more details see , below.)\n\nGauss proved that a primitive th root of unity can be expressed using only square roots, addition, subtraction, multiplication and division if and only if it is possible to construct with compass and straightedge the regular -gon. This is the case if and only if is either a power of two or the product of a power of two and Fermat primes that are all different. \n\nIf is a th root of unity, the same is true for , and formula_36 is twice the real part of . In other words, is a reciprocal polynomial, the polynomial formula_37 that has as a root may be deduced from by the standard manipulation on reciprocal polynomials, and the primitive th roots of unity may be deduced for the roots of formula_37 by solving the quadratic equation formula_39 That is, the real part of the primitive root is formula_40 and its imaginary part is formula_41\n\nThe polynomial formula_37 is an irreducible polynomial whose all roots are real. Its degree is a power of two, if and only if is a product of a power of two by a product (possibly empty) of distinct Fermat primes, and the regular -gon is constructible with compass and straightedge. Otherwise, it is solvable in radicals, but one are in the casus irreducibilis, that is, every expression of the roots in terms of radicals involves \"nonreal radicals\". \n\n\nIf is a primitive th root of unity, then the sequence of powers\nis -periodic (because for all values of ), and the sequences of powers\nfor are all -periodic (because ). Furthermore, the set } of these sequences is a basis of the linear space of all -periodic sequences. This means that \"any\" -periodic sequence of complex numbers\ncan be expressed as a linear combination of powers of a primitive th root of unity:\nfor some complex numbers and every integer .\n\nThis is a form of Fourier analysis. If is a (discrete) time variable, then is a frequency and is a complex amplitude.\n\nChoosing for the primitive th root of unity\nallows to be expressed as a linear combination of and :\nThis is a discrete Fourier transform.\n\nLet be the sum of all the th roots of unity, primitive or not. Then\n\nThis is an immediate consequence of Vieta's formulas. In fact, the th roots of unity being the roots of the polynomial , their sum is the coefficient of degree , which is either 1 or 0 according whether or .\n\nAlternatively, for there is nothing to prove. For there exists a root . Since the set of all the th roots of unity is a group, , so the sum satisfies , whence .\n\nLet be the sum of all the primitive th roots of unity. Then\n\nwhere is the Möbius function.\n\nIn the section Elementary properties, it was shown that if is the set of all th roots of unity and is the set of primitive ones, is a disjoint union of the :\n\nThis implies\n\nApplying the Möbius inversion formula gives\n\nIn this formula, if , then , and for : . Therefore, .\n\nThis is the special case of Ramanujan's sum , defined as the sum of the th powers of the primitive th roots of unity:\n\nFrom the summation formula follows an orthogonality relationship: for and \n\nwhere is the Kronecker delta and is any primitive th root of unity.\n\nThe matrix whose th entry is\n\ndefines a discrete Fourier transform. Computing the inverse transformation using gaussian elimination requires operations. However, it follows from the orthogonality that is unitary. That is,\n\nand thus the inverse of is simply the complex conjugate. (This fact was first noted by Gauss when solving the problem of trigonometric interpolation). The straightforward application of or its inverse to a given vector requires operations. The fast Fourier transform algorithms reduces the number of operations further to .\n\nThe zeroes of the polynomial\nare precisely the th roots of unity, each with multiplicity 1. The th cyclotomic polynomial is defined by the fact that its zeros are precisely the \"primitive\" th roots of unity, each with multiplicity 1.\nwhere are the primitive th roots of unity, and is Euler's totient function. The polynomial has integer coefficients and is an irreducible polynomial over the rational numbers (i.e., it cannot be written as the product of two positive-degree polynomials with rational coefficients). The case of prime , which is easier than the general assertion, follows by applying Eisenstein's criterion to the polynomial\n\nand expanding via the binomial theorem.\n\nEvery th root of unity is a primitive th root of unity for exactly one positive divisor of . This implies that\n\nThis formula represents the factorization of the polynomial into irreducible factors.\n\nApplying Möbius inversion to the formula gives\n\nwhere is the Möbius function.\n\nSo the first few cyclotomic polynomials are\n\nIf is a prime number, then all the th roots of unity except 1 are primitive th roots, and we have\nSubstituting any positive integer ≥ 2 for , this sum becomes a base repunit. Thus a necessary (but not sufficient) condition for a repunit to be prime is that its length be prime.\n\nNote that, contrary to first appearances, \"not\" all coefficients of all cyclotomic polynomials are 0, 1, or −1. The first exception is . It is not a surprise it takes this long to get an example, because the behavior of the coefficients depends not so much on as on how many odd prime factors appear in . More precisely, it can be shown that if has 1 or 2 odd prime factors (e.g., ) then the th cyclotomic polynomial only has coefficients 0, 1 or −1. Thus the first conceivable for which there could be a coefficient besides 0, 1, or −1 is a product of the three smallest odd primes, and that is . This by itself doesn't prove the 105th polynomial has another coefficient, but does show it is the first one which even has a chance of working (and then a computation of the coefficients shows it does). A theorem of Schur says that there are cyclotomic polynomials with coefficients arbitrarily large in absolute value. In particular, if , where are odd primes, , and \"t\" is odd, then occurs as a coefficient in the th cyclotomic polynomial.\n\nMany restrictions are known about the values that cyclotomic polynomials can assume at integer values. For example, if is prime, then if and only .\n\nCyclotomic polynomials are solvable in radicals, as roots of unity are themselves radicals. Moreover, there exist more informative radical expressions for th roots of unity with the additional property that every value of the expression obtained by choosing values of the radicals (for example, signs of square roots) is a primitive th root of unity. This was already shown by Gauss in 1797. Efficient algorithms exist for calculating such expressions.\n\nThe th roots of unity form under multiplication a cyclic group of order , and in fact these groups comprise all of the finite subgroups of the multiplicative group of the complex number field. A generator for this cyclic group is a primitive th root of unity.\n\nThe th roots of unity form an irreducible representation of any cyclic group of order . The orthogonality relationship also follows from group-theoretic principles as described in character group.\n\nThe roots of unity appear as entries of the eigenvectors of any circulant matrix, i.e. matrices that are invariant under cyclic shifts, a fact that also follows from group representation theory as a variant of Bloch's theorem. In particular, if a circulant Hermitian matrix is considered (for example, a discretized one-dimensional Laplacian with periodic boundaries), the orthogonality property immediately follows from the usual orthogonality of eigenvectors of Hermitian matrices.\n\nBy adjoining a primitive th root of unity to Q, one obtains the th cyclotomic field . This field contains all th roots of unity and is the splitting field of the th cyclotomic polynomial over Q. The field extension has degree φ(\"n\") and its Galois group is naturally isomorphic to the multiplicative group of units of the ring .\n\nAs the Galois group of is abelian, this is an abelian extension. Every subfield of a cyclotomic field is an abelian extension of the rationals. It follows that every \"n\"th root of unity may be expressed in term of \"k\"-roots, with various \"k\" not exceeding \"φ(n)\". In these cases Galois theory can be written out explicitly in terms of Gaussian periods: this theory from the \"Disquisitiones Arithmeticae\" of Gauss was published many years before Galois.\n\nConversely, \"every\" abelian extension of the rationals is such a subfield of a cyclotomic field – this is the content of a theorem of Kronecker, usually called the \"Kronecker–Weber theorem\" on the grounds that Weber completed the proof.\n\nFor , both roots of unity and belong to . \n\nFor three values of , the roots of unity are quadratic integers:\n\nFor four other values of , the primitive roots of unity are not quadratic integers, but the sum of any root of unity with its complex conjugate (also an th root of unity) is a quadratic integer.\n\nFor , none of the non-real roots of unity (which satisfy a quartic equation) is a quadratic integer, but the sum of each root with its complex conjugate (also a 5th root of unity) is an element of the ring Z[] (). For two pairs of non-real 5th roots of unity these sums are inverse golden ratio and minus golden ratio.\n\nFor , for any root of unity equals to either 0, ±2, or ± ().\n\nFor , for any root of unity, equals to either 0, ±1, ±2 or ± ().\n\n\n"}
{"id": "41268608", "url": "https://en.wikipedia.org/wiki?curid=41268608", "title": "Rose Whelan Sedgewick", "text": "Rose Whelan Sedgewick\n\nRose Whelan Sedgewick (circa 1904 – 2000) was a 20th-century mathematician. She was the first person to earn a PhD in mathematics from Brown University, in 1929. Her subsequent career in mathematics included assistant professorships at the University of Rochester, the University of Connecticut, Hillyer College, and the University of Maryland.\n\nSedgewick is the namesake of the Rose Whelan Society at Brown, an organization of women graduate students and post-doctoral fellows in mathematics. She was married to fellow mathematician Charles H.W. Sedgewick and had four children. She died on June 7, 2000 at the age of 96.\n\n"}
{"id": "252311", "url": "https://en.wikipedia.org/wiki?curid=252311", "title": "Rule of inference", "text": "Rule of inference\n\nIn logic, a rule of inference, inference rule or transformation rule is a logical form consisting of a function which takes premises, analyzes their syntax, and returns a conclusion (or conclusions). For example, the rule of inference called \"modus ponens\" takes two premises, one in the form \"If p then q\" and another in the form \"p\", and returns the conclusion \"q\". The rule is valid with respect to the semantics of classical logic (as well as the semantics of many other non-classical logics), in the sense that if the premises are true (under an interpretation), then so is the conclusion.\n\nTypically, a rule of inference preserves truth, a semantic property. In many-valued logic, it preserves a general designation. But a rule of inference's action is purely syntactic, and does not need to preserve any semantic property: any function from sets of formulae to formulae counts as a rule of inference. Usually only rules that are recursive are important; i.e. rules such that there is an effective procedure for determining whether any given formula is the conclusion of a given set of formulae according to the rule. An example of a rule that is not effective in this sense is the infinitary ω-rule.\n\nPopular rules of inference in propositional logic include \"modus ponens\", \"modus tollens\", and contraposition. First-order predicate logic uses rules of inference to deal with logical quantifiers.\n\nIn formal logic (and many related areas), rules of inference are usually given in the following standard form:\n\n  Premise#1\n<br>  Premise#2\n<br>        ...\n<br>  Premise#n   \n<br>  Conclusion\n\nThis expression states that whenever in the course of some logical derivation the given premises have been obtained, the specified conclusion can be taken for granted as well. The exact formal language that is used to describe both premises and conclusions depends on the actual context of the derivations. In a simple case, one may use logical formulae, such as in:\n\nThis is the \"modus ponens\" rule of propositional logic. Rules of inference are often formulated as schemata employing metavariables. In the rule (schema) above, the metavariables A and B can be instantiated to any element of the universe (or sometimes, by convention, a restricted subset such as propositions) to form an infinite set of inference rules.\n\nA proof system is formed from a set of rules chained together to form proofs, also called \"derivations\". Any derivation has only one final conclusion, which is the statement proved or derived. If premises are left unsatisfied in the derivation, then the derivation is a proof of a \"hypothetical\" statement: \"\"if\" the premises hold, \"then\" the conclusion holds.\"\n\nIn a Hilbert system, the premises and conclusion of the inference rules are simply formulae of some language, usually employing metavariables. For graphical compactness of the presentation and to emphasize the distinction between axioms and rules of inference, this section uses the sequent notation (formula_4) instead of a vertical presentation of rules.\n\nThe formal language for classical propositional logic can be expressed using just negation (¬), implication (→) and propositional symbols. A well-known axiomatization, comprising three axiom schemata and one inference rule (\"modus ponens\"), is:\n\nIt may seem redundant to have two notions of inference in this case, ⊢ and →. In classical propositional logic, they indeed coincide; the deduction theorem states that \"A\" ⊢ \"B\" if and only if ⊢ \"A\" → \"B\". There is however a distinction worth emphasizing even in this case: the first notation describes a deduction, that is an activity of passing from sentences to sentences, whereas \"A\" → \"B\" is simply a formula made with a logical connective, implication in this case. Without an inference rule (like \"modus ponens\" in this case), there is no deduction or inference. This point is illustrated in Lewis Carroll's dialogue called \"What the Tortoise Said to Achilles\" , as well as later attempts by Bertrand Russel and Peter Winch to resolve the paradox introduced in the dialogue.\nFor some non-classical logics, the deduction theorem does not hold. For example, the three-valued logic of Łukasiewicz can be axiomatized as:\n\nThis sequence differs from classical logic by the change in axiom 2 and the addition of axiom 4. The classical deduction theorem does not hold for this logic, however a modified form does hold, namely \"A\" ⊢ \"B\" if and only if ⊢ \"A\" → (\"A\" → \"B\").\n\nIn a set of rules, an inference rule could be redundant in the sense that it is \"admissible\" or \"derivable\". A derivable rule is one whose conclusion can be derived from its premises using the other rules. An admissible rule is one whose conclusion holds whenever the premises hold. All derivable rules are admissible. To appreciate the difference, consider the following set of rules for defining the natural numbers (the judgment formula_5 asserts the fact that formula_6 is a natural number):\n\nThe first rule states that 0 is a natural number, and the second states that s(n) is a natural number if \"n\" is. In this proof system, the following rule, demonstrating that the second successor of a natural number is also a natural number, is derivable:\n\nIts derivation is the composition of two uses of the successor rule above. The following rule for asserting the existence of a predecessor for any nonzero number is merely admissible:\n\nThis is a true fact of natural numbers, as can be proven by induction. (To prove that this rule is admissible, assume a derivation of the premise and induct on it to produce a derivation of formula_10.) However, it is not derivable, because it depends on the structure of the derivation of the premise. Because of this, derivability is stable under additions to the proof system, whereas admissibility is not. To see the difference, suppose the following nonsense rule were added to the proof system:\n\nIn this new system, the double-successor rule is still derivable. However, the rule for finding the predecessor is no longer admissible, because there is no way to derive formula_12. The brittleness of admissibility comes from the way it is proved: since the proof can induct on the structure of the derivations of the premises, extensions to the system add new cases to this proof, which may no longer hold.\n\nAdmissible rules can be thought of as theorems of a proof system. For instance, in a sequent calculus where cut elimination holds, the \"cut\" rule is admissible.\n\n"}
{"id": "23934029", "url": "https://en.wikipedia.org/wiki?curid=23934029", "title": "Skew coordinates", "text": "Skew coordinates\n\nA system of skew coordinates is a curvilinear coordinate system where the coordinate surfaces are not orthogonal, in contrast to orthogonal coordinates.\n\nSkew coordinates tend to be more complicated to work with compared to orthogonal coordinates since the metric tensor will have nonzero off-diagonal components, preventing many simplifications in formulas for tensor algebra and tensor calculus. The nonzero off-diagonal components of the metric tensor are a direct result of the non-orthogonality of the basis vectors of the coordinates, since by definition:\n\nwhere formula_2 is the metric tensor and formula_3 the (covariant) basis vectors.\n\nThese coordinate systems can be useful if the geometry of a problem fits well into a skewed system. For example, solving Laplace's equation in a parallelogram will be easiest when done in appropriately skewed coordinates.\n\nThe simplest 3D case of a skew coordinate system is a Cartesian one where one of the axes (say the \"x\" axis) has been bent by some angle formula_4, staying orthogonal to one of the remaining two axes. For this example, the \"x\" axis of a Cartesian coordinate has been bent toward the \"z\" axis by formula_4, remaining orthogonal to the \"y\" axis.\n\nLet formula_6, formula_7, and formula_8 respectively be unit vectors along the formula_9, formula_10, and formula_11 axes. These represent the covariant basis; computing their dot products gives the following components of the metric tensor:\n\nwhich are quantities that will be useful later on.\n\nThe contravariant basis is given by\n\nThe contravariant basis isn't a very convenient one to use, however it shows up in definitions so must be considered. We'll favor writing quantities with respect to the covariant basis.\n\nSince the basis vectors are all constant, vector addition and subtraction will simply be familiar component-wise adding and subtraction. Now, let\n\nwhere the sums indicate summation over all values of the index (in this case, \"i\" = 1, 2, 3). The contravariant and covariant components of these vectors may be related by\n\nso that, explicitly,\n\nThe dot product in terms of contravariant components is then\n\nand in terms of covariant components\n\nBy definition, the gradient of a scalar function \"f\" is\n\nwhere formula_25 are the coordinates \"x\", \"y\", \"z\" indexed. Recognizing this as a vector written in terms of the contravariant basis, it may be rewritten:\n\nThe divergence of a vector formula_27 is\n\nand of a tensor formula_29\n\nThe Laplacian of \"f\" is\n\nand, since the covariant basis is normal and constant, the vector Laplacian is the same as the componentwise Laplacian of a vector written in terms of the covariant basis.\n\nWhile both the dot product and gradient are somewhat messy in that they have extra terms (compared to a Cartesian system) the advection operator which combines a dot product with a gradient turns out very simple:\n\nwhich may be applied to both scalar functions and vector functions, componentwise when expressed in the covariant basis.\n\nFinally, the curl of a vector is\n"}
{"id": "11813890", "url": "https://en.wikipedia.org/wiki?curid=11813890", "title": "Supersymmetry algebras in 1 + 1 dimensions", "text": "Supersymmetry algebras in 1 + 1 dimensions\n\nA two dimensional Minkowski space, i.e. a flat space with one time and one spacial dimension, has a two-dimensional Poincaré group IO(1,1) as its symmetry group. The respective Lie algebra is called the Poincaré algebra. It is possible to extend this algebra to a supersymmetry algebra, which is a formula_1-graded Lie superalgebra. The most common ways to do this are discussed below.\n\nLet the Lie algebra of IO(1,1) be generated by the following generators:\nFor the commutators between these generators, see Poincaré algebra.\n\nThe formula_6 supersymmetry algebra over this space is a supersymmetric extension of this Lie algebra with the four additional generators (supercharges) formula_7, which are odd elements of the Lie superalgebra. Under Lorentz transformations the generators formula_8 and formula_9 transform as left-handed Weyl spinors, while formula_10 and formula_11 transform as right-handed Weyl spinors. The algebra is given by the Poincaré algebra plus\n\nformula_12\n\nwhere all remaining commutators vanish, and formula_13 and formula_14 are complex central charges. The supercharges are related via formula_15. formula_16, formula_17, and formula_18 are Hermitian.\n\nThe formula_20 subalgebra is obtained from the formula_2 algebra by removing the generators formula_10 and formula_11. Thus its anti-commutation relations are given by\n\nformula_26\n\nplus the commutation relations above that do not involve formula_10 or formula_11. Both generators are left-handed Weyl spinors.\n\nSimilarly, the formula_21 subalgebra is obtained by removing formula_8 and formula_9 and fulfills\n\nformula_32\n\nBoth supercharge generators are right-handed.\n\nThe formula_33 subalgebra is generated by two generators formula_35 and formula_36 given by\n\nformula_37for two real numbers formula_38and formula_39.\n\nBy definition, both supercharges are real, i.e. formula_40. They transform as Majorana-Weyl spinors under Lorentz transformations. Their anti-commutation relations are given by\n\nformula_41\n\nwhere formula_42 is a real central charge.\n\nThese algebras can be obtained from the formula_33 subalgebra by removing formula_36 resp. formula_35from the generators.\n\n\n"}
{"id": "10087606", "url": "https://en.wikipedia.org/wiki?curid=10087606", "title": "Symmetry operation", "text": "Symmetry operation\n\nIn the context of molecular symmetry, a symmetry operation is a permutation of atoms such that the molecule or crystal is transformed into a state indistinguishable from the starting state.\nTwo basic facts follow from this definition, which emphasize its usefulness.\nWavefunctions need not be invariant, because the operation can multiply them by a phase or mix states within a degenerate representation, without affecting any physical property.\n\nThese are denoted by \"C\" and are rotations of 360°/\"n\", performed \"m\" times. The superscript \"m\" is omitted if it is equal to one.\n\n\"C\", rotation by 360°, is called the Identity operation and is denoted by \"E\" or \"I\".\n\n\"C\", \"n\" rotations 360°/\"n\" is also an Identity operation.\n\nThese are denoted by \"S\" and are rotations of 360°/\"n\" followed by reflection in a plane perpendicular to the rotation axis.\n\n\"S\" is usually denoted as σ, a reflection operation about a mirror plane.\n\n\"S\" is usually denoted as \"i\", an inversion operation about an inversion centre.\n\nWhen \"n\" is an even number \"S\" = \"E\", but when \"n\" is odd \"S\" = \"E\".\n\nRotation axes, mirror planes and inversion centres are symmetry elements, not operations. The rotation axis of highest order is known as the principal rotation axis. It is conventional to set the Cartesian \"z\" axis of the molecule to contain the principal rotation axis.\n\nDichloromethane, CHCl. There is a \"C\" rotation axis which passes through the carbon atom and the midpoints between the two hydrogen atoms and the two chlorine atoms. Define the z axis as co-linear with the \"C\" axis, the \"xz\" plane as containing CH and the \"yz\" plane as containing CCl. A \"C\" rotation operation permutes the two hydrogen atoms and the two chlorine atoms. Reflection in the \"yz\" plane permutes the hydrogen atoms while reflection in the \"xz\" plane permutes the chlorine atoms. The four symmetry operations \"E\", \"C\", σ(\"xz\")and σ(\"yz\") form the point group C. Note that if any two operations are carried out in succession the result is the same as if a single operation of the group had been performed.\nMethane, CH. In addition to the proper rotations of order 2 and 3 there are three mutually perpendicular \"S\" axes which pass half-way between the C-H bonds and six mirror planes. Note that \"S\" = \"C\".\n\nIn crystals screw rotations and/or glide reflections are additionally possible. These are rotations or reflections together with partial translation. The Bravais lattices may be considered as representing translational symmetry operations. Combinations of operations of the crystallographic point groups with the addition symmetry operations produce the 230 crystallographic space groups.\n\nF. A. Cotton \"Chemical applications of group theory\", Wiley, 1962, 1971\n"}
{"id": "222947", "url": "https://en.wikipedia.org/wiki?curid=222947", "title": "Table of bases", "text": "Table of bases\n\nThis article is about \"bases\" as that term is used in discussion of certain numeral systems.\n\nThis table of bases gives the values of 0 to 256 in bases 2 to 36. (Using A−Z for 10−35)\n\n"}
{"id": "44338719", "url": "https://en.wikipedia.org/wiki?curid=44338719", "title": "Taniyama group", "text": "Taniyama group\n\nIn mathematics, the Taniyama group is a group that is an extension of the absolute Galois group of the rationals by the Serre group. It was introduced by using an observation by Deligne, and named after Yutaka Taniyama. It was intended to be the group scheme whose representations correspond to the (hypothetical) CM motives over the field Q of rational numbers.\n\n"}
{"id": "30330", "url": "https://en.wikipedia.org/wiki?curid=30330", "title": "Total order", "text": "Total order\n\nIn mathematics, a linear order, total order, simple order, or (non-strict) ordering is a binary relation on some set formula_1, which is antisymmetric, transitive, and a connex relation. A set paired with a total order is called a totally ordered set, a linearly ordered set, a simply ordered set, or a chain.\n\nFormally, a binary relation formula_2 is a total order on a set formula_1 if the following statements hold for all formula_4 and formula_5 in formula_1:\n\nAntisymmetry eliminates uncertain cases when both formula_15 precedes formula_16 and formula_16 precedes formula_15. A relation having the \"connex\" property means that any pair of elements in the set of the relation are comparable under the relation. This also means that the set can be diagrammed as a line of elements, giving it the name \"linear\". The \"connex\" property also implies reflexivity, i.e., \"a\" ≤ \"a\". Therefore, a total order is also a (special case of a) partial order, as, for a partial order, the connex property is replaced by the weaker reflexivity property. An extension of a given partial order to a total order is called a linear extension of that partial order.\n\nFor each (non-strict) total order ≤ there is an associated asymmetric (hence irreflexive) relation <, called a strict total order, which can be defined in two equivalent ways:\n\nProperties:\n\nWe can work the other way and start by choosing < as a transitive trichotomous binary relation; then a total order ≤ can be defined in two equivalent ways:\n\nTwo more associated orders are the complements ≥ and >, completing the quadruple {<, >, ≤, ≥}.\n\nWe can define or explain the way a set is totally ordered by any of these four relations; the notation implies whether we are talking about the non-strict or the strict total order.\n\n\nWhile chain is sometimes merely a synonym for totally ordered set, it can also refer to a totally ordered subset of some partially ordered set. The latter definition has a crucial role in Zorn's lemma.\nThe height of a poset denotes the cardinality of its largest chain in this sense.\n\nFor example, consider the set of all subsets of the integers partially ordered by inclusion. Then the set { \"I\" : \"n\" is a natural number}, where \"I\" is the set of natural numbers below \"n\", is a chain in this ordering, as it is totally ordered under inclusion: If \"n\"≤\"k\", then \"I\" is a subset of \"I\".\n\nOne may define a totally ordered set as a particular kind of lattice, namely one in which we have\n\nWe then write \"a\" ≤ \"b\" if and only if formula_20. Hence a totally ordered set is a distributive lattice.\n\nA simple counting argument will verify that any non-empty finite totally ordered set (and hence any non-empty subset thereof) has a least element. Thus every finite total order is in fact a well order. Either by direct proof or by observing that every well order is order isomorphic to an ordinal one may show that every finite total order is order isomorphic to an initial segment of the natural numbers ordered by <. In other words, a total order on a set with \"k\" elements induces a bijection with the first \"k\" natural numbers. Hence it is common to index finite total orders or well orders with order type ω by natural numbers in a fashion which respects the ordering (either starting with zero or with one).\n\nTotally ordered sets form a full subcategory of the category of partially ordered sets, with the morphisms being maps which respect the orders, i.e. maps \"f\" such that if \"a\" ≤ \"b\" then \"f\"(\"a\") ≤ \"f\"(\"b\").\n\nA bijective map between two totally ordered sets that respects the two orders is an isomorphism in this category.\n\nFor any totally ordered set \"X\" we can define the open intervals (\"a\", \"b\") = {\"x\" : \"a\" < \"x\" and \"x\" < \"b\"}, (−∞, \"b\") = {\"x\" : \"x\" < \"b\"}, (\"a\", ∞) = {\"x\" : \"a\" < \"x\"} and (−∞, ∞) = \"X\". We can use these open intervals to define a topology on any ordered set, the order topology.\n\nWhen more than one order is being used on a set one talks about the order topology induced by a particular order. For instance if N is the natural numbers, < is less than and > greater than we might refer to the order topology on N induced by < and the order topology on N induced by > (in this case they happen to be identical but will not in general).\n\nThe order topology induced by a total order may be shown to be hereditarily normal.\n\nA totally ordered set is said to be complete if every nonempty subset that has an upper bound, has a least upper bound. For example, the set of real numbers R is complete but the set of rational numbers Q is not.\n\nThere are a number of results relating properties of the order topology to the completeness of X:\n\nA totally ordered set (with its order topology) which is a complete lattice is compact. Examples are the closed intervals of real numbers, e.g. the unit interval [0,1], and the affinely extended real number system (extended real number line). There are order-preserving homeomorphisms between these examples.\n\nFor any two disjoint total orders formula_21 and formula_22, there is a natural order formula_23 on the set formula_24, which is called the sum of the two orders or sometimes just formula_25: \nIntutitively, this means that the elements of the second set are added on top of the elements of the first set.\n\nMore generally, if formula_34 is a totally ordered index set, and for each formula_35 the structure formula_36 is a linear order, where the sets formula_37 are pairwise disjoint, then the natural total order on formula_38 is defined by \n\nIn order of increasing strength, i.e., decreasing sets of pairs, three of the possible orders on the Cartesian product of two totally ordered sets are:\n\nAll three can similarly be defined for the Cartesian product of more than two sets.\n\nApplied to the vector space R, each of these make it an ordered vector space.\n\nSee also examples of partially ordered sets.\n\nA real function of \"n\" real variables defined on a subset of R defines a strict weak order and a corresponding total preorder on that subset.\n\nA binary relation that is antisymmetric, transitive, and reflexive (but not necessarily total) is a partial order.\n\nA group with a compatible total order is a totally ordered group.\n\nThere are only a few nontrivial structures that are (interdefinable as) reducts of a total order. Forgetting the orientation results in a betweenness relation. Forgetting the location of the ends results in a cyclic order. Forgetting both data results in a separation relation.\n\n\n"}
{"id": "105499", "url": "https://en.wikipedia.org/wiki?curid=105499", "title": "Whittaker–Shannon interpolation formula", "text": "Whittaker–Shannon interpolation formula\n\nThe Whittaker–Shannon interpolation formula or sinc interpolation is a method to construct a continuous-time bandlimited function from a sequence of real numbers. The formula dates back to the works of E. Borel in 1898, and E. T. Whittaker in 1915, and was cited from works of J. M. Whittaker in 1935, and in the formulation of the Nyquist–Shannon sampling theorem by Claude Shannon in 1949. It is also commonly called Shannon's interpolation formula and Whittaker's interpolation formula. E. T. Whittaker, who published it in 1915, called it the Cardinal series.\n\nGiven a sequence of real numbers, \"x\"[\"n\"], the continuous function\n\nThe interpolation formula is derived in the Nyquist–Shannon sampling theorem article, which points out that it can also be expressed as the convolution of an infinite impulse train with a sinc function:\n\nThis is equivalent to filtering the impulse train with an ideal (\"brick-wall\") low-pass filter.\n\nThe interpolation formula always converges absolutely and locally uniformly as long as\n\nBy the Hölder inequality this is satisfied if the sequence formula_4 belongs to any of the formula_5 spaces with 1 ≤ \"p\" < ∞, that is\n\nThis condition is sufficient, but not necessary. For example, the sum will generally converge if the sample sequence comes from sampling almost any stationary process, in which case the sample sequence is not square summable, and is not in any formula_5 space.\n\nIf \"x\"[\"n\"] is an infinite sequence of samples of a sample function of a wide-sense stationary process, then it is not a member of any formula_8 or L space, with probability 1; that is, the infinite sum of samples raised to a power \"p\" does not have a finite expected value. Nevertheless, the interpolation formula converges with probability 1. Convergence can readily be shown by computing the variances of truncated terms of the summation, and showing that the variance can be made arbitrarily small by choosing a sufficient number of terms. If the process mean is nonzero, then pairs of terms need to be considered to also show that the expected value of the truncated terms converges to zero.\n\nSince a random process does not have a Fourier transform, the condition under which the sum converges to the original function must also be different. A stationary random process does have an autocorrelation function and hence a spectral density according to the Wiener–Khinchin theorem. A suitable condition for convergence to a sample function from the process is that the spectral density of the process be zero at all frequencies equal to and above half the sample rate.\n\n\n"}
{"id": "3326019", "url": "https://en.wikipedia.org/wiki?curid=3326019", "title": "Witness set", "text": "Witness set\n\nIn computational learning theory, let \"C\" be a concept class over a domain \"X\" and \"c\" be a concept in \"C\". A subset \"S\" of \"X\" is a witness set for \"c\" in \"C\" if \"c\"(\"S\") verifies \"c\" (i.e., \"c\" is the only consistent concept with respect to \"c\"(\"S\")). The minimum size of a witness set for \"c\" is called the \"witness size\" or \"specification number\" and is denoted by formula_1. The value formula_2 is called the teaching dimension of \"C\".\n"}
