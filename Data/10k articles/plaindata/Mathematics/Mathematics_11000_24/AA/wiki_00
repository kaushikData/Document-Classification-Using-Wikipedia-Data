{"id": "6096872", "url": "https://en.wikipedia.org/wiki?curid=6096872", "title": "Bilateral key exchange", "text": "Bilateral key exchange\n\nBilateral Key Exchange (BKE) was an encryption scheme utilized by The Society for Worldwide Interbank Financial Telecommunications (SWIFT).\n\nThe scheme was retired on January 1, 2009 and has now been replaced by the Relationship Management Application (RMA). All key management is now based on the SWIFT PKI that was implemented in SWIFT phase 2.\n\nA Bilateral Key allowed secure communication across the SWIFT Network. The text of a and the authentication key were used to generate a Message Authentication Code or MAC. The MAC ensured the origin of a message and the authenticity of the message contents. This was normally accomplished by the exchange of various SWIFT Messages used specifically for establishing a communicating key pair.\n\nBKE keys were generated either manually inside the SWIFT software, or automatically with the use of an SCR or Secure Card Reader.\n\nSince 1994, the keys used in the card reader and the authentication keys themselves were 1024bit RSA.\n"}
{"id": "152547", "url": "https://en.wikipedia.org/wiki?curid=152547", "title": "Bisection", "text": "Bisection\n\nIn geometry, bisection is the division of something into two equal or congruent parts, usually by a line, which is then called a \"bisector\". The most often considered types of bisectors are the \"segment bisector\" (a line that passes through the midpoint of a given segment) and the \"angle bisector\" (a line that passes through the apex of an angle, that divides it into two equal angles).\n\nIn three-dimensional space, bisection is usually done by a plane, also called the \"bisector\" or \"bisecting plane\".\n\nA line segment bisector passes through the midpoint of the segment.\nParticularly important is the perpendicular bisector of a segment, which, according to its name, meets the segment at right angles. The perpendicular bisector of a segment also has the property that each of its points is equidistant from the segment's endpoints. Therefore, Voronoi diagram boundaries consist of segments of such lines or planes.\n\nIn classical geometry, the bisection is a simple compass and straightedge construction, whose possibility depends on the ability to draw circles of equal radii and different centers. The segment is bisected by drawing intersecting circles of equal radius, whose centers are the endpoints of the segment and such that each circle goes through one endpoint. The line determined by the points of intersection of the two circles is the perpendicular bisector of the segment, since it crosses the segment at its center. This construction is in fact used when constructing a line perpendicular to a given line at a given point: drawing an arbitrary circle whose center is that point, it intersects the line in two more points, and the perpendicular to be constructed is the one bisecting the segment defined by these two points.\n\nBrahmagupta's theorem states that if a cyclic quadrilateral is orthodiagonal (that is, has perpendicular diagonals), then the perpendicular to a side from the point of intersection of the diagonals always bisects the opposite side.\n\nAlgebraically, the perpendicular bisector of a line segment with endpoints formula_1 and formula_2 is given by the equation\n\nwhich states that the squared distance of a point on the bisector to one endpoint equals the squared distance from that point to the other endpoint.\n\nAn angle bisector divides the angle into two angles with equal measures. An angle only has one bisector. Each point of an angle bisector is equidistant from the sides of the angle.\n\nThe \"interior\" or \"internal bisector\" of an angle is the line, half-line, or line segment that divides an angle of less than 180° into two equal angles. The \"exterior\" or \"external bisector\" is the line that divides the supplementary angle (of 180° minus the original angle), formed by one side forming the original angle and the extension of the other side, into two equal angles. \n\nTo bisect an angle with straightedge and compass, one draws a circle whose center is the vertex. The circle meets the angle at two points: one on each leg. Using each of these points as a center, draw two circles of the same size. The intersection of the circles (two points) determines a line that is the angle bisector.\n\nThe proof of the correctness of this construction is fairly intuitive, relying on the symmetry of the problem. The trisection of an angle (dividing it into three equal parts) cannot be achieved with the compass and ruler alone (this was first proved by Pierre Wantzel).\n\nThe internal and external bisectors of an angle are perpendicular. If the angle is formed by the two lines given algebraically as formula_4 and formula_5 then the internal and external bisectors are given by the two equations\n\nThe interior angle bisectors of a triangle are concurrent in a point called the incenter of the triangle, as seen in the diagram at right.\n\nThe bisectors of two exterior angles and the bisector of the other interior angle are concurrent.\n\nThree intersection points, each of an external angle bisector with the opposite extended side, are collinear (fall on the same line as each other).\n\nThree intersection points, two of them between an interior angle bisector and the opposite side, and the third between the other exterior angle bisector and the opposite side extended, are collinear.\n\nThe angle bisector theorem is concerned with the relative lengths of the two segments that a triangle's side is divided into by a line that bisects the opposite angle. It equates their relative lengths to the relative lengths of the other two sides of the triangle. \n\nIf the side lengths of a triangle are formula_7, the semiperimeter formula_8 and A is the angle opposite side formula_9, then the length of the internal bisector of angle A is\n\nor in trigonometric terms,\n\nIf the internal bisector of angle A in triangle ABC has length formula_12 and if this bisector divides the side opposite A into segments of lengths \"m\" and \"n\", then\n\nwhere \"b\" and \"c\" are the side lengths opposite vertices B and C; and the side opposite A is divided in the proportion \"b\":\"c\".\n\nIf the internal bisectors of angles A, B, and C have lengths formula_14 and formula_15, then\n\nNo two non-congruent triangles share the same set of three internal angle bisector lengths.\n\nThere exist integer triangles with a rational angle bisector.\n\nThe internal angle bisectors of a convex quadrilateral either form a cyclic quadrilateral (that is, the four intersection points of adjacent angle bisectors are concyclic), or they are concurrent. In the latter case the quadrilateral is a tangential quadrilateral. \n\nEach diagonal of a rhombus bisects opposite angles.\n\nThe excenter of an ex-tangential quadrilateral lies at the intersection of six angle bisectors. These are the internal angle bisectors at two opposite vertex angles, the external angle bisectors (supplementary angle bisectors) at the other two vertex angles, and the external angle bisectors at the angles formed where the extensions of opposite sides intersect.\n\nThe tangent to a parabola at any point bisects the angle between the line joining the point to the focus and the line from the point and perpendicular to the directrix.\n\nEach of the three medians of a triangle is a line segment going through one vertex and the midpoint of the opposite side, so it bisects that side (though not in general perpendicularly). The three medians intersect each other at the centroid of the triangle, which is its center of mass if it has uniform density; thus any line through a triangle's centroid and one of its vertices bisects the opposite side. The centroid is twice as close to the midpoint of any one side as it is to the opposite vertex.\n\nThe interior perpendicular bisector of a side of a triangle is the segment, falling entirely on and inside the triangle, of the line that perpendicularly bisects that side. The three perpendicular bisectors of a triangle's three sides intersect at the circumcenter (the center of the circle through the three vertices). Thus any line through a triangle's circumcenter and perpendicular to a side bisects that side.\n\nIn an acute triangle the circumcenter divides the interior perpendicular bisectors of the two shortest sides in equal proportions. In an obtuse triangle the two shortest sides' perpendicular bisectors (extended beyond their opposite triangle sides to the circumcenter) are divided by their respective intersecting triangle sides in equal proportions.\n\nFor any triangle the interior perpendicular bisectors are given by formula_17 formula_18 and formula_19 where the sides are formula_20 and the area is formula_21\n\nThe two bimedians of a convex quadrilateral are the line segments that connect the midpoints of opposite sides, hence each bisecting two sides. The two bimedians and the line segment joining the midpoints of the diagonals are concurrent at a point called the \"vertex centroid\" and are all bisected by this point.\n\nThe four \"maltitudes\" of a convex quadrilateral are the perpendiculars to a side through the midpoint of the opposite side, hence bisecting the latter side. If the quadrilateral is cyclic (inscribed in a circle), these maltitudes are concurrent at (all meet at) a common point called the \"anticenter\".\n\nBrahmagupta's theorem states that if a cyclic quadrilateral is orthodiagonal (that is, has perpendicular diagonals), then the perpendicular to a side from the point of intersection of the diagonals always bisects the opposite side.\n\nThe perpendicular bisector construction forms a quadrilateral from the perpendicular bisectors of the sides of another quadrilateral.\n\nThere are an infinitude of lines that bisect the area of a triangle. Three of them are the medians of the triangle (which connect the sides' midpoints with the opposite vertices), and these are concurrent at the triangle's centroid; indeed, they are the only area bisectors that go through the centroid. Three other area bisectors are parallel to the triangle's sides; each of these intersects the other two sides so as to divide them into segments with the proportions formula_22. These six lines are concurrent three at a time: in addition to the three medians being concurrent, any one median is concurrent with two of the side-parallel area bisectors.\n\nThe envelope of the infinitude of area bisectors is a deltoid (broadly defined as a figure with three vertices connected by curves that are concave to the exterior of the deltoid, making the interior points a non-convex set). The vertices of the deltoid are at the midpoints of the medians; all points inside the deltoid are on three different area bisectors, while all points outside it are on just one. \nThe sides of the deltoid are arcs of hyperbolas that are asymptotic to the extended sides of the triangle. The ratio of the area of the envelope of area bisectors to the area of the triangle is invariant for all triangles, and equals formula_23 i.e. 0.019860... or less than 2%.\n\nA cleaver of a triangle is a line segment that bisects the perimeter of the triangle and has one endpoint at the midpoint of one of the three sides. The three cleavers concur at (all pass through) the center of the Spieker circle, which is the incircle of the medial triangle. The cleavers are parallel to the angle bisectors.\n\nA splitter of a triangle is a line segment having one endpoint at one of the three vertices of the triangle and bisecting the perimeter. The three splitters concur at the Nagel point of the triangle.\n\nAny line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter (the center of its incircle). There are either one, two, or three of these for any given triangle. A line through the incenter bisects one of the area or perimeter if and only if it also bisects the other.\n\nAny line through the midpoint of a parallelogram bisects the area and the perimeter. \n\nAll area bisectors and perimeter bisectors of a circle or other ellipse go through the center, and any chords through the center bisect the area and perimeter. In the case of a circle they are the diameters of the circle.\n\nThe diagonals of a parallelogram bisect each other.\n\nIf a line segment connecting the diagonals of a quadrilateral bisects both diagonals, then this line segment (the Newton Line) is itself bisected by the vertex centroid.\n\nA plane that divides two opposite edges of a tetrahedron in a given ratio also divides the volume of the tetrahedron in the same ratio. Thus any plane containing a bimedian (connector of opposite edges' midpoints) of a tetrahedron bisects the volume of the tetrahedron\n\n"}
{"id": "156533", "url": "https://en.wikipedia.org/wiki?curid=156533", "title": "Chebyshev's inequality", "text": "Chebyshev's inequality\n\nIn probability theory, Chebyshev's inequality (also called the Bienaymé-Chebyshev inequality) guarantees that, for a wide class of probability distributions, no more than a certain fraction of values can be more than a certain distance from the mean. Specifically, no more than 1/\"k\" of the distribution's values can be more than \"k\" standard deviations away from the mean (or equivalently, at least 1−1/\"k\" of the distribution's values are within \"k\" standard deviations of the mean). The rule is often called Chebyshev's theorem, about the range of standard deviations around the mean, in statistics. The inequality has great utility because it can be applied to any probability distribution in which the mean and variance are defined. For example, it can be used to prove the weak law of large numbers.\n\nIn practical usage, in contrast to the 68–95–99.7 rule, which applies to normal distributions, Chebyshev's inequality is weaker, stating that a minimum of just 75% of values must lie within two standard deviations of the mean and 89% within three standard deviations.\n\nThe term \"Chebyshev's inequality\" also refer to Markov's inequality, especially in the context of analysis. They are closely related, and some authors refer to Markov's inequality as \"Chebyshev's First Inequality,\" and the similar one referred to on this page as \"Chebyshev's Second Inequality.\"\n\nThe theorem is named after Russian mathematician Pafnuty Chebyshev, although it was first formulated by his friend and colleague Irénée-Jules Bienaymé. The theorem was first stated without proof by Bienaymé in 1853 and later proved by Chebyshev in 1867. His student Andrey Markov provided another proof in his 1884 Ph.D. thesis.\n\nChebyshev's inequality is usually stated for random variables, but can be generalized to a statement about measure spaces.\n\nLet \"X\" (integrable) be a random variable with finite expected value \"μ\" and finite non-zero variance \"σ\". Then for any real number ,\nOnly the case formula_2 is useful. When formula_3 the right hand side formula_4\nand the inequality is trivial as all probabilities are ≤ 1.\n\nAs an example, using formula_5 shows that the probability that values lie outside the interval formula_6 does not exceed formula_7.\n\nBecause it can be applied to completely arbitrary distributions provided they have a known finite mean and variance, the inequality generally gives a poor bound compared to what might be deduced if more aspects are known about the distribution involved.\n\nLet (\"X\", Σ, μ) be a measure space, and let \"f\" be an extended real-valued measurable function defined on \"X\". Then for any real number \"t\" > 0 and \"0 < p < ∞\",\n\nwhere the sum is taken over the \"n\" variables and\n\nA second related inequality has also been derived by Chen. Let be the dimension of the stochastic vector and let be the mean of . Let be the covariance matrix and . Then\n\nwhere \"Y\" is the transpose of . A simple proof was obtained in Navarro as follows:\n\nwhere\n\nand formula_13 is a symmetric invertible matrix such that: formula_14. Hence formula_15 and formula_16 where formula_17 represents the identity matrix of dimension n. Then formula_18 and\n\nFinally, by applying Markov's inequality to Z we get\n\nand so the desired inequality holds.\n\nThe inequality can be written in terms of the Mahalanobis distance as\n\nwhere the Mahalanobis distance based on S is defined by\n\nNavarro proved that these bounds are sharp, that is, they are the best possible bounds for that regions when we just know the mean and the covariance matrix of X.\n\nStellato et al. showed that this multivariate version of the Chebyshev inequality can be easily derived analytically as a special case of Vandenberghe et al. where the bound is computed by solving a semidefinite program (SDP).\n\nThere is a straightforward extension of the vector version of Chebyshev's inequality to infinite dimensional settings. Let be a random variable which takes values in a Fréchet space formula_23 (equipped with seminorms ). This includes most common settings of vector-valued random variables, e.g., when formula_23 is a Banach space (equipped with a single norm), a Hilbert space, or the finite-dimensional setting as described above.\n\nSuppose that is of \"strong order two\", meaning that\n\nfor every seminorm . This is a generalization of the requirement that have finite variance, and is necessary for this strong form of Chebyshev's inequality in infinite dimensions. The terminology \"strong order two\" is due to Vakhania.\n\nLet formula_26 be the Pettis integral of (i.e., the vector generalization of the mean), and let\n\nbe the standard deviation with respect to the seminorm . In this setting we can state the following:\n\nProof. The proof is straightforward, and essentially the same as the finitary version. If , then is constant (and equal to ) almost surely, so the inequality is trivial.\n\nIf\n\nthen , so we may safely divide by . The crucial trick in Chebyshev's inequality is to recognize that formula_30.\n\nThe following calculations complete the proof:\n\nAn extension to higher moments is also possible:\n\nA related inequality sometimes known as the exponential Chebyshev's inequality is the inequality\n\nLet be the cumulant generating function,\n\nTaking the Legendre–Fenchel transformation of and using the exponential Chebyshev's inequality we have\n\nThis inequality may be used to obtain exponential inequalities for unbounded variables.\n\nIf P(\"x\") has finite support based on the interval , let where |\"x\"| is the absolute value of . If the mean of P(\"x\") is zero then for all \n\nThe second of these inequalities with is the Chebyshev bound. The first provides a lower bound for the value of P(\"x\").\n\nSharp bounds for a bounded variate have been proposed by Niemitalo, but without a proof though\n\nLet where . Then\n\n\n\n\nSaw \"et al\" extended Chebyshev's inequality to cases where the population mean and variance are not known and may not exist, but the sample mean and sample standard deviation from \"N\" samples are to be employed to bound the expected value of a new drawing from the same distribution.\n\nwhere \"X\" is a random variable which we have sampled \"N\" times, \"m\" is the sample mean, \"k\" is a constant and \"s\" is the sample standard deviation. \"g\"(\"x\") is defined as follows:\n\nLet \"x\" ≥ 1, \"Q\" = \"N\" + 1, and \"R\" be the greatest integer less than \"Q\"/\"x\". Let\n\nNow\n\nThis inequality holds even when the population moments do not exist, and when the sample is only weakly exchangeably distributed; this criterion is met for randomised sampling. A table of values for the Saw–Yang–Mo inequality for finite sample sizes (\"N\" < 100) has been determined by Konijn. The table allows the calculation of various confidence intervals for the mean, based on multiples, C, of the standard error of the mean as calculated from the sample. For example, Konijn shows that for \"N\" = 59, the 95 percent confidence interval for the mean \"m\" is where (this is 2.28 times larger than the value found on the assumption of normality showing the loss on precision resulting from ignorance of the precise nature of the distribution).\n\nKabán gives a somewhat less complex version of this inequality.\n\nIf the standard deviation is a multiple of the mean then a further inequality can be derived,\n\nA table of values for the Saw–Yang–Mo inequality for finite sample sizes (\"N\" < 100) has been determined by Konijn.\n\nFor fixed \"N\" and large \"m\" the Saw–Yang–Mo inequality is approximately\n\nBeasley \"et al\" have suggested a modification of this inequality\n\nIn empirical testing this modification is conservative but appears to have low statistical power. Its theoretical basis currently remains unexplored.\n\nThe bounds these inequalities give on a finite sample are less tight than those the Chebyshev inequality gives for a distribution. To illustrate this let the sample size \"N\" = 100 and let \"k\" = 3. Chebyshev's inequality states that at most approximately 11.11% of the distribution will lie at least three standard deviations away from the mean. Kabán's version of the inequality for a finite sample states that at most approximately 12.05% of the sample lies outside these limits. The dependence of the confidence intervals on sample size is further illustrated below.\n\nFor \"N\" = 10, the 95% confidence interval is approximately ±13.5789 standard deviations.\n\nFor \"N\" = 100 the 95% confidence interval is approximately ±4.9595 standard deviations; the 99% confidence interval is approximately ±140.0 standard deviations.\n\nFor \"N\" = 500 the 95% confidence interval is approximately ±4.5574 standard deviations; the 99% confidence interval is approximately ±11.1620 standard deviations.\n\nFor \"N\" = 1000 the 95% and 99% confidence intervals are approximately ±4.5141 and approximately ±10.5330 standard deviations respectively.\n\nThe Chebyshev inequality for the distribution gives 95% and 99% confidence intervals of approximately ±4.472 standard deviations and ±10 standard deviations respectively.\n\nAlthough Chebyshev's inequality is the best possible bound for an arbitrary distribution, this is not necessarily true for finite samples. Samuelson's inequality states that all values of a sample will lie within standard deviations of the mean. Chebyshev's bound improves as the sample size increases.\n\nWhen \"N\" = 10, Samuelson's inequality states that all members of the sample lie within 3 standard deviations of the mean: in contrast Chebyshev's states that 99.5% of the sample lies within 13.5789 standard deviations of the mean.\n\nWhen \"N\" = 100, Samuelson's inequality states that all members of the sample lie within approximately 9.9499 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.\n\nWhen \"N\" = 500, Samuelson's inequality states that all members of the sample lie within approximately 22.3383 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.\n\nStellato et al. simplified the notation and extended the empirical Chebyshev inequality from Saw et al. to the multivariate case. Let formula_47 be a random variable and let formula_48. We draw formula_49 iid samples of formula_50 denoted as formula_51. Based on the first formula_52 samples, we define the empirical mean as formula_53 and the unbiased empirical covariance as formula_54. If formula_55 is nonsingular, then for all formula_56 then\n\nIn the univariate case, i.e. formula_58, this inequality corresponds to the one from Saw et al. Moreover, the right-hand side can be simplified by upper bounding the floor function by its argument\n\nAs formula_60, the right-hand side tends to formula_61 which corresponds to the multivariate Chebyshev inequality over ellipsoids shaped according to formula_62 and centered in formula_63.\n\nChebyshev's inequality is important because of its applicability to any distribution. As a result of its generality it may not (and usually does not) provide as sharp a bound as alternative methods that can be used if the distribution of the random variable is known. To improve the sharpness of the bounds provided by Chebyshev's inequality a number of methods have been developed; for a review see eg.\n\nSharpened bounds can be derived by first standardising the random variable.\n\nLet \"X\" be a random variable with finite variance \"Var\"(\"x\"). Let \"Z\" be the standardised form defined as\n\nCantelli's lemma is then\n\nThis inequality is sharp and is attained by \"k\" and −1/\"k\" with probability 1/(1 + \"k\") and \"k\"/(1 + \"k\") respectively.\n\nIf \"k\" > 1 and the distribution of \"X\" is symmetric then we have\n\nEquality holds if and only if \"Z\" = −\"k\", 0 or \"k\" with probabilities , and respectively.\nAn extension to a two-sided inequality is also possible.\n\nLet \"u\", \"v\" > 0. Then we have\n\nAn alternative method of obtaining sharper bounds is through the use of semivariances (partial variances). The upper (\"σ\") and lower (\"σ\") semivariances are defined as\n\nwhere \"m\" is the arithmetic mean of the sample and \"n\" is the number of elements in the sample.\n\nThe variance of the sample is the sum of the two semivariances:\n\nIn terms of the lower semivariance Chebyshev's inequality can be written\n\nPutting\n\nChebyshev's inequality can now be written\n\nA similar result can also be derived for the upper semivariance.\n\nIf we put\n\nChebyshev's inequality can be written\n\nBecause \"σ\" ≤ \"σ\", use of the semivariance sharpens the original inequality.\n\nIf the distribution is known to be symmetric, then\n\nand\n\nThis result agrees with that derived using standardised variables.\n\n\nSelberg derived an inequality for \"P\"(\"x\") when \"a\" ≤ \"x\" ≤ \"b\". To simplify the notation let\n\nwhere\n\nand\n\nThe result of this linear transformation is to make \"P\"(\"a\" ≤ \"X\" ≤ \"b\") equal to \"P\"(|\"Y\"| ≤ \"k\").\n\nThe mean (\"μ\") and variance (\"σ\") of \"X\" are related to the mean (\"μ\") and variance (\"σ\") of \"Y\":\n\nWith this notation Selberg's inequality states that\n\nThese are known to be the best possible bounds.\n\nCantelli's inequality due to Francesco Paolo Cantelli states that for a real random variable (\"X\") with mean (\"μ\") and variance (\"σ\")\n\nwhere \"a\" ≥ 0.\n\nThis inequality can be used to prove a one tailed variant of Chebyshev's inequality with \"k\" > 0\n\nThe bound on the one tailed variant is known to be sharp. To see this consider the random variable \"X\" that takes the values\n\nThen E(\"X\") = 0 and E(\"X\") = \"σ\" and \"P\"(\"X\" < 1) = 1 / (1 + \"σ\").\n\n\nThe one-sided variant can be used to prove the proposition that for probability distributions having an expected value and a median, the mean and the median can never differ from each other by more than one standard deviation. To express this in symbols let \"μ\", \"ν\", and \"σ\" be respectively the mean, the median, and the standard deviation. Then\n\nThere is no need to assume that the variance is finite because this inequality is trivially true if the variance is infinite.\n\nThe proof is as follows. Setting \"k\" = 1 in the statement for the one-sided inequality gives:\n\nChanging the sign of \"X\" and of \"μ\", we get\n\nAs the median is by definition any real number \"m\" that satisfies the inequalities\n\nthis implies that the median lies within one standard deviation of the mean. A proof using Jensen's inequality also exists.\n\nBhattacharyya extended Cantelli's inequality using the third and fourth moments of the distribution.\n\nLet \"μ\" = 0 and \"σ\" be the variance. Let γ = \"E\"(\"X\") / \"σ\" and κ = \"E\"(\"X\") / \"σ\".\n\nIf \"k\" − \"k\"γ − 1 > 0 then\n\nThe necessity of \"k\" − \"k\"γ − 1 > 0 requires that \"k\" be reasonably large.\n\nMitzenmacher and Upfal note that\n\nfor any integer \"k\" > 0 and that\n\nis the \"2k\" central moment. They then show that for \"t\" > 0\n\nFor \"k\" = 1 we obtain Chebyshev's inequality. For \"t\" ≥ 1, \"k\" > 2 and assuming that the \"k\" moment exists, this bound is tighter than Chebyshev's inequality.\n\nSeveral other related inequalities are also known.\n\nZelen has shown that\n\nwith\n\nwhere is the -th moment and is the standard deviation.\n\nFor any collection of non-negative independent random variables with expectation 1 \n\nLet be a random variable with and , then for any , we have\n\nLet be a set of independent Rademacher random variables: . Then\n\nThe bound is sharp and better than that which can be derived from the normal distribution (approximately ).\n\nA distribution function \"F\" is unimodal at \"ν\" if its cumulative distribution function is convex on (−∞, \"ν\") and concave on (\"ν\",∞) An empirical distribution can be tested for unimodality with the dip test.\n\nIn 1823 Gauss showed that for a unimodal distribution with a mode of zero\n\nIf the mode is not zero and the mean (\"μ\") and standard deviation (\"σ\") are both finite, then denoting the median as \"ν\" and the root mean square deviation from the mode by \"ω\", we have\n\nand\n\nWinkler in 1866 extended Gauss' inequality to \"r\" moments where \"r\" > 0 and the distribution is unimodal with a mode of zero:\n\nGauss' bound has been subsequently sharpened and extended to apply to departures from the mean rather than the mode due to the Vysochanskiï–Petunin inequality.\n\nThe Vysochanskiï–Petunin inequality has been extended by Dharmadhikari and Joag-Dev\n\nwhere \"s\" is a constant satisfying both \"s\" > \"r\" + 1 and \"s\"(\"s\" − \"r\" − 1) = \"r\" and \"r\" > 0.\n\nIt can be shown that these inequalities are the best possible and that further sharpening of the bounds requires that additional restrictions be placed on the distributions.\n\nThe bounds on this inequality can also be sharpened if the distribution is both unimodal and symmetrical. An empirical distribution can be tested for symmetry with a number of tests including McWilliam's R*. It is known that the variance of a unimodal symmetrical distribution with finite support [\"a\", \"b\"] is less than or equal to ( \"b\" − \"a\" ) / 12.\n\nLet the distribution be supported on the finite interval [ −\"N\", \"N\" ] and the variance be finite. Let the mode of the distribution be zero and rescale the variance to 1. Let \"k\" > 0 and assume \"k\" < 2\"N\"/3. Then\n\nIf 0 < \"k\" ≤ 2 / the bounds are reached with the density\n\nIf 2 / < \"k\" ≤ 2\"N\" / 3 the bounds are attained by the distribution\n\nwhere \"β\" = 4 / 3\"k\", \"δ\" is the Dirac delta function and where\n\nThe existence of these densities shows that the bounds are optimal. Since \"N\" is arbitrary these bounds apply to any value of \"N\".\n\nThe Camp–Meidell's inequality is a related inequality. For an absolutely continuous unimodal and symmetrical distribution\n\nThe second of these inequalities is the same as the Vysochanskiï–Petunin inequality.\n\nDasGupta has shown that if the distribution is known to be normal\n\n\nSymmetry of the distribution decreases the inequality's bounds by a factor of 2 while unimodality sharpens the bounds by a factor of 4/9.\n\nBecause the mean and the mode in a unimodal distribution differ by at most standard deviations at most 5% of a symmetrical unimodal distribution lies outside (2 + 3)/3 standard deviations of the mean (approximately 3.840 standard deviations). This is sharper than the bounds provided by the Chebyshev inequality (approximately 4.472 standard deviations).\n\nThese bounds on the mean are less sharp than those that can be derived from symmetry of the distribution alone which shows that at most 5% of the distribution lies outside approximately 3.162 standard deviations of the mean. The Vysochanskiï–Petunin inequality further sharpens this bound by showing that for such a distribution that at most 5% of the distribution lies outside 4/3 (approximately 2.981) standard deviations of the mean.\n\n\nFor any symmetrical unimodal distribution\n\n\n\nDasGupta's inequality states that for a normal distribution at least 95% lies within approximately 2.582 standard deviations of the mean. This is less sharp than the true figure (approximately 1.96 standard deviations of the mean).\n\n\nWhen the mean (\"μ\") is zero Chebyshev's inequality takes a simple form. Let \"σ\" be the variance. Then\n\nWith the same conditions Cantelli's inequality takes the form\n\nIf in addition \"E\"( \"X\" ) = 1 and \"E\"( \"X\" ) = \"ψ\" then for any 0 ≤ \"ε\" ≤ 1\n\nThe first inequality is sharp.\nThis is known as the Paley–Zygmund inequality.\n\nIt is also known that for a random variable obeying the above conditions that\n\nwhere\n\nIt is also known that\n\nThe value of C is optimal and the bounds are sharp if\n\nIf\n\nthen the sharp bound is\n\nThere is a second (less well known) inequality also named after Chebyshev\n\nIf \"f\", \"g\" : [\"a\", \"b\"] → R are two monotonic functions of the same monotonicity, then\n\nIf \"f\" and \"g\" are of opposite monotonicity, then the above inequality works in the reverse way.\n\nThis inequality is related to Jensen's inequality, Kantorovich's inequality, the Hermite–Hadamard inequality and Walter's conjecture.\n\nThere are also a number of other inequalities associated with Chebyshev:\n\n\nOne use of Chebyshev's inequality in applications is to create confidence intervals for variates with an unknown distribution. Haldane noted, using an equation derived by Kendall, that if a variate (\"x\") has a zero mean, unit variance and both finite skewness (\"γ\") and kurtosis (\"κ\") then the variate can be converted to a normally distributed standard score (\"z\"):\n\nThis transformation may be useful as an alternative to Chebyshev's inequality or as an adjunct to it for deriving confidence intervals for variates with unknown distributions.\n\nWhile this transformation may be useful for moderately skewed and/or kurtotic distributions, it performs poorly when the distribution is markedly skewed and/or kurtotic.\n\nThe Environmental Protection Agency has suggested best practices for the use of Chebyshev's inequality for estimating confidence intervals. This caution appears to be justified as its use in this context may be seriously misleading.\n\n\n\n"}
{"id": "56005174", "url": "https://en.wikipedia.org/wiki?curid=56005174", "title": "Cop number", "text": "Cop number\n\nIn graph theory, a branch of mathematics, the cop number or copnumber of an undirected graph is the number of cops to win a certain pursuit-evasion game on the graph.\n\nIn this game, one player controls the position of a given number of cops and the other player controls the position of a robber. The cops are trying to catch the robber by moving to the same position, while the robber is trying to remain uncaught. Thus, the players perform the following actions, taking turns with each other:\nThe game ends with a win for the cops whenever the robber occupies the same vertex as a cop. If this never happens, the robber wins.\n\nThe cop number of a graph formula_1 is the minimum number formula_2 such that formula_2 cops can win the game on formula_1.\n\nOn a tree, the cop number is one. The cop can start anywhere, and at each step move to the unique neighbor that is closer to the robber. Each of the cop's steps reduces the size of the subtree that the robber is confined to, so the game eventually ends.\nOn a cycle graph of length more than three, the cop number is two. If there is only one cop, the robber can move to a position two steps away from the cop, and always maintain the same distance after each move of the robber. In this way, the robber can evade capture forever. However, if there are two cops, one can stay at one vertex and cause the robber and the other cop to play in the remaining path. If the other cop follows the tree strategy, the robber will eventually lose.\n\nEvery graph whose girth is greater than four has cop number at least equal to its minimum degree. It follows that there exist graphs of arbitrarily high cop number.\nHenri Meyniel (also known for Meyniel graphs) conjectured in 1985 that every formula_5-vertex graph has cop number formula_6. The Levi graphs of finite projective planes have girth six and minimum degree formula_7, so if true this bound would be the best possible.\nIt is known that all graphs have sublinear cop number,\nbut the problems of obtaining a tight bound, and of proving or disproving Meyniel's conjecture, remain unsolved.\n\nComputing the cop number of a given graph is EXPTIME-complete, and hard for parameterized complexity.\n\nThe cop-win graphs are the graphs with cop number equal to one.\n\nEvery planar graph has cop number at most three.\nMore generally, every graph has cop number at most proportional to its genus. However, the best known lower bound for\nthe cop number in terms of the genus is approximately the square root of the genus, which is\nfar from the upper bound when the genus is large.\n\nThe treewidth of a graph can also be obtained as the result of a pursuit-evasion game, but one in which the robber can move along arbitrary-length paths instead of a single edge in each turn. This extra freedom means that the cop number is generally smaller than the treewidth. More specifically,\non graphs of treewidth formula_8, the cop number is at most formula_9.\n"}
{"id": "1086571", "url": "https://en.wikipedia.org/wiki?curid=1086571", "title": "Critical value", "text": "Critical value\n\nCritical value may refer to:\n\nIn differential topology, a critical value of a differentiable function between differentiable manifolds is the image (value of) ƒ(\"x\") in \"N\" of a critical point \"x\" in \"M\".\n\nIn statistical hypothesis testing, the critical values of a statistical test are the boundaries of the acceptance region of the test. The acceptance region is the set of values of the test statistic for which the null hypothesis is not rejected. Depending on the shape of the acceptance region, there can be one or more than one critical value. \n\nIn complex dynamics, a critical value is the image of a critical point.\n"}
{"id": "301666", "url": "https://en.wikipedia.org/wiki?curid=301666", "title": "De Sitter space", "text": "De Sitter space\n\nIn mathematics and physics, a de Sitter space is the analog in Minkowski space, or spacetime, of a sphere in ordinary Euclidean space. The \"n\"-dimensional de Sitter space, denoted dS, is the Lorentzian manifold analog of an \"n\"-sphere (with its canonical Riemannian metric); it is maximally symmetric, has constant positive curvature, and is simply connected for \"n\" at least 3. De Sitter space and anti-de Sitter space are named after Willem de Sitter (1872–1934), professor of astronomy at Leiden University and director of the Leiden Observatory. Willem de Sitter and Albert Einstein worked closely together in Leiden in the 1920s on the spacetime structure of our universe.\n\nIn the language of general relativity, de Sitter space is the maximally symmetric vacuum solution of Einstein's field equations with a positive cosmological constant formula_1 (corresponding to a positive vacuum energy density and negative pressure). When (3 space dimensions plus time), it is a cosmological model for the physical universe; see de Sitter universe.\n\nDe Sitter space was also discovered, independently, and about the same time, by Tullio Levi-Civita.\n\nMore recently it has been considered as the setting for special relativity rather than using Minkowski space, since a group contraction reduces the isometry group of de Sitter space to the Poincaré group, allowing a unification of the spacetime translation subgroup and Lorentz transformation subgroup of the Poincaré group into a simple group rather than a semi-simple group. This alternative formulation of special relativity is called de Sitter relativity.\n\nDe Sitter space can be defined as a submanifold of a generalized Minkowski space of one higher dimension. Take Minkowski space R with the standard metric:\nDe Sitter space is the submanifold described by the hyperboloid of one sheet\nwhere formula_4 is some nonzero constant with dimensions of length. The metric on de Sitter space is the metric induced from the ambient Minkowski metric. The induced metric is nondegenerate and has Lorentzian signature. (Note that if one replaces formula_5 with formula_6 in the above definition, one obtains a hyperboloid of two sheets. The induced metric in this case is positive-definite, and each sheet is a copy of hyperbolic \"n\"-space. For a detailed proof, see geometry of Minkowski space.)\n\nDe Sitter space can also be defined as the quotient of two indefinite orthogonal groups, which shows that it is a non-Riemannian symmetric space.\n\nTopologically, de Sitter space is (so that if then de Sitter space is simply connected).\n\nThe isometry group of de Sitter space is the Lorentz group . The metric therefore then has independent Killing vector fields and is maximally symmetric. Every maximally symmetric space has constant curvature. The Riemann curvature tensor of de Sitter is given by\n\nDe Sitter space is an Einstein manifold since the Ricci tensor is proportional to the metric:\nThis means de Sitter space is a vacuum solution of Einstein's equation with cosmological constant given by\nThe scalar curvature of de Sitter space is given by\nFor the case , we have and .\n\nWe can introduce static coordinates formula_11 for de Sitter as follows:\nwhere formula_15 gives the standard embedding the -sphere in R. In these coordinates the de Sitter metric takes the form:\n\nNote that there is a cosmological horizon at formula_17.\n\nLet\nwhere formula_21. Then in the formula_22 coordinates metric reads:\nwhere formula_24 is the flat metric on formula_25's.\n\nSetting formula_26, we obtain the conformally flat metric:\n\nLet\nwhere formula_31 forming a formula_32 with the standard metric formula_33. Then the metric of the de Sitter space reads\nwhere\nis the standard hyperbolic metric.\n\nLet\nwhere formula_15s describe a formula_39. Then the metric reads:\n\nChanging the time variable to the conformal time via formula_41 we obtain a metric conformally equivalent to Einstein static universe:\nThis serves to find the Penrose diagram of de Sitter space.\n\nLet\nwhere formula_15s describe a formula_48. Then the metric reads:\nwhere\nis the metric of an formula_51 dimensional de Sitter space with radius of curvature formula_4 in open slicing coordinates. The hyperbolic metric is given by:\n\nThis is the analytic continuation of the open slicing coordinates under formula_54 and also switching formula_55 and formula_56 because they change their timelike/spacelike nature.\n\n\n\n"}
{"id": "40129720", "url": "https://en.wikipedia.org/wiki?curid=40129720", "title": "Devex algorithm", "text": "Devex algorithm\n\nIn applied mathematics, the devex algorithm is a pivot rule for the simplex method developed by Harris. It identifies the steepest-edge approximately in its search for the optimal solution.\n"}
{"id": "43559240", "url": "https://en.wikipedia.org/wiki?curid=43559240", "title": "Distinguished limit", "text": "Distinguished limit\n\nIn mathematics, a distinguished limit is an appropriately chosen scale factor used in the method of matched asymptotic expansions.\n\n"}
{"id": "876428", "url": "https://en.wikipedia.org/wiki?curid=876428", "title": "Divergent series", "text": "Divergent series\n\nIn mathematics, a divergent series is an infinite series that is not convergent, meaning that the infinite sequence of the partial sums of the series does not have a finite limit.\n\nIf a series converges, the individual terms of the series must approach zero. Thus any series in which the individual terms do not approach zero diverges. However, convergence is a stronger condition: not all series whose terms approach zero converge. A counterexample is the harmonic series\n\nThe divergence of the harmonic series was proven by the medieval mathematician Nicole Oresme.\n\nIn specialized mathematical contexts, values can be objectively assigned to certain series whose sequence of partial sums diverges, this is to make meaning of the divergence of the series. A summability method or summation method is a partial function from the set of series to values. For example, Cesàro summation assigns Grandi's divergent series\n\nthe value . Cesàro summation is an averaging method, in that it relies on the arithmetic mean of the sequence of partial sums. Other methods involve analytic continuations of related series. In physics, there are a wide variety of summability methods; these are discussed in greater detail in the article on regularization.\n\nBefore the 19th century, divergent series were widely used by Leonhard Euler and others, but often led to confusing and contradictory results. A major problem was Euler's idea that any divergent series should have a natural sum, without first defining what is meant by the sum of a divergent series. Augustin-Louis Cauchy eventually gave a rigorous definition of the sum of a (convergent) series, and for some time after this, divergent series were mostly excluded from mathematics. They reappeared in 1886 with Henri Poincaré's work on asymptotic series. In 1890, Ernesto Cesàro realized that one could give a rigorous definition of the sum of some divergent series, and defined Cesàro summation. (This was not the first use of Cesàro summation, which was used implicitly by Ferdinand Georg Frobenius in 1880; Cesàro's key contribution was not the discovery of this method, but his idea that one should give an explicit definition of the sum of a divergent series.) In the years after Cesàro's paper, several other mathematicians gave other definitions of the sum of a divergent series, although these are not always compatible: different definitions can give different answers for the sum of the same divergent series; so, when talking about the sum of a divergent series, it is necessary to specify which summation method one is using.\n\n\nA summability method \"M\" is regular if it agrees with the actual limit on all convergent series. Such a result is called an Abelian theorem for \"M\", from the prototypical Abel's theorem. More interesting, and in general more subtle, are partial converse results, called tauberian theorems, from a prototype proved by Alfred Tauber. Here \"partial converse\" means that if \"M\" sums the series \"Σ\", and some side-condition holds, then \"Σ\" was convergent in the first place; without any side-condition such a result would say that \"M\" only summed convergent series (making it useless as a summation method for divergent series).\n\nThe function giving the sum of a convergent series is linear, and it follows from the Hahn–Banach theorem that it may be extended to a summation method summing any series with bounded partial sums. This is called the Banach limit. This fact is not very useful in practice, since there are many such extensions, inconsistent with each other, and also since proving such operators exist requires invoking the axiom of choice or its equivalents, such as Zorn's lemma. They are therefore nonconstructive.\n\nThe subject of divergent series, as a domain of mathematical analysis, is primarily concerned with explicit and natural techniques such as Abel summation, Cesàro summation and Borel summation, and their relationships. The advent of Wiener's tauberian theorem marked an epoch in the subject, introducing unexpected connections to Banach algebra methods in Fourier analysis.\n\nSummation of divergent series is also related to extrapolation methods and sequence transformations as numerical techniques. Examples of such techniques are Padé approximants, Levin-type sequence transformations, and order-dependent mappings related to renormalization techniques for large-order perturbation theory in quantum mechanics.\n\nSummation methods usually concentrate on the sequence of partial sums of the series. While this sequence does not converge, we may often find that when we take an average of larger and larger numbers of initial terms of the sequence, the average converges, and we can use this average instead of a limit to evaluate the sum of the series. A summation method can be seen as a function from a set of sequences of partial sums to values. If A is any summation method assigning values to a set of sequences, we may mechanically translate this to a series-summation method A that assigns the same values to the corresponding series. There are certain properties it is desirable for these methods to possess if they are to arrive at values corresponding to limits and sums, respectively.\n\n\nThe third condition is less important, and some significant methods, such as Borel summation, do not possess it.\n\nOne can also give a weaker alternative to the last condition. \nA desirable property for two distinct summation methods A and B to share is \"consistency\": A and B are consistent if for every sequence \"s\" to which both assign a value, If two methods are consistent, and one sums more series than the other, the one summing more series is \"stronger\".\n\nThere are powerful numerical summation methods that are neither regular nor linear, for instance nonlinear sequence transformations like Levin-type sequence transformations and Padé approximants, as well as the order-dependent mappings of perturbative series based on renormalization techniques.\n\nTaking regularity, linearity and stability as axioms, it is possible to sum many divergent series by elementary algebraic manipulations. This partly explains why many different summation methods give the same answer for certain series.\n\nFor instance, whenever the geometric series\ncan be evaluated regardless of convergence. More rigorously, any summation method that possesses these properties and which assigns a finite value to the geometric series must assign this value. However, when \"r\" is a real number larger than 1, the partial sums increase without bound, and averaging methods assign a limit of infinity.\n\nThe two classical summation methods for series, ordinary convergence and absolute convergence, define the sum as a limit of certain partial sums. These are included only for completeness; strictly speaking they are not true summation methods for divergent series since, by definition, a series is divergent only if these methods do not work. Most but not all summation methods for divergent series extend these methods to a larger class of sequences.\n\nAbsolute convergence defines the sum of a sequence (or set) of numbers to be the limit of the net of all partial sums , if it exists. It does not depend on the order of the elements of the sequence, and a classical theorem says that a sequence is absolutely convergent if and only if the sequence of absolute values is convergent in the standard sense.\n\nCauchy's classical definition of the sum of a series defines the sum to be the limit of the sequence of partial sums . This is the default definition of convergence of a sequence.\n\nSuppose \"p\" is a sequence of positive terms, starting from \"p\". Suppose also that\nIf now we transform a sequence s by using \"p\" to give weighted means, setting\nthen the limit of \"t\" as \"n\" goes to infinity is an average called the Nørlund mean N(\"s\").\n\nThe Nørlund mean is regular, linear, and stable. Moreover, any two Nørlund means are consistent.\n\nThe most significant of the Nørlund means are the Cesàro sums. Here, if we define the sequence \"p\" by\nthen the Cesàro sum \"C\" is defined by Cesàro sums are Nørlund means if , and hence are regular, linear, stable, and consistent. \"C\" is ordinary summation, and \"C\" is ordinary Cesàro summation. Cesàro sums have the property that if then \"C\" is stronger than \"C\".\n\nSuppose } is a strictly increasing sequence tending towards infinity, and that . Suppose\nconverges for all real numbers \"x\" > 0. Then the Abelian mean \"A\" is defined as\n\nMore generally, if the series for \"f\" only converges for large \"x\" but can be analytically continued to all positive real \"x\", then one can still define the sum of the divergent series by the limit above.\n\nA series of this type is known as a generalized Dirichlet series; in applications to physics, this is known as the method of heat-kernel regularization.\n\nAbelian means are regular and linear, but not stable and not always consistent between different choices of \"λ\". However, some special cases are very important summation methods.\n\nIf , then we obtain the method of Abel summation. Here\n\nwhere \"z\" = exp(−\"x\"). Then the limit of \"f\"(\"x\") as \"x\" approaches 0 through positive reals is the limit of the power series for \"f\"(\"z\") as \"z\" approaches 1 from below through positive reals, and the Abel sum \"A\"(\"s\") is defined as\n\nAbel summation is interesting in part because it is consistent with but more powerful than Cesàro summation: whenever the latter is defined. The Abel sum is therefore regular, linear, stable, and consistent with Cesàro summation.\n\nIf , then (indexing from one) we have\n\nThen \"L\"(\"s\"), the Lindelöf sum , is the limit of \"f\"(\"x\") as \"x\" goes to positive zero. The Lindelöf sum is a powerful method when applied to power series among other applications, summing power series in the Mittag-Leffler star.\n\nIf \"g\"(\"z\") is analytic in a disk around zero, and hence has a Maclaurin series \"G\"(\"z\") with a positive radius of convergence, then in the Mittag-Leffler star. Moreover, convergence to \"g\"(\"z\") is uniform on compact subsets of the star.\n\nSeveral summation methods involve taking the value of an analytic continuation of a function.\n\nIf Σ\"a\"\"x\" converges for small complex \"x\" and can be analytically continued along some path from \"x\" = 0 to the point \"x\" = 1, then the sum of the series can be defined to be the value at \"x\" = 1. This value may depend on the choice of path.\n\nEuler summation is essentially an explicit form of analytic continuation. If a power series converges for small complex \"z\" and can be analytically continued to the open disk with diameter from to 1 and is continuous at 1, then its value at is called the Euler or (E,\"q\") sum of the series \"a\" + ... Euler used it before analytic continuation was defined in general, and gave explicit formulas for the power series of the analytic continuation.\n\nThe operation of Euler summation can be repeated several times, and this is essentially equivalent to taking an analytic continuation of a power series to the point \"z\" = 1.\n\nThis method defines the sum of a series to be the value of the analytic continuation of the Dirichlet series\n\nat \"s\" = 0, if this exists and is unique. This method is sometimes confused with zeta function regularization.\n\nIf the series\n\n(for positive values of the \"a\") converges for large real \"s\" and can be analytically continued along the real line to \"s\" = −1, then its value at \"s\" = −1 is called the zeta regularized sum of the series \"a\" + \"a\" + ... Zeta function regularization is nonlinear. In applications, the numbers \"a\" are sometimes the eigenvalues of a self-adjoint operator \"A\" with compact resolvent, and \"f\"(\"s\") is then the trace of \"A\". For example, if \"A\" has eigenvalues 1, 2, 3, ... then \"f\"(\"s\") is the Riemann zeta function, \"ζ\"(\"s\"), whose value at \"s\" = −1 is −, assigning a value to the divergent series . Other values of \"s\" can also be used to assign values for the divergent sums , and in general\nwhere \"B\" is a Bernoulli number.\n\nIf \"J\"(\"x\") = Σ\"p\"\"x\" is an integral function, then the \"J\" sum of the series \"a\" + ... is defined to be\nif this limit exists.\n\nThere is a variation of this method where the series for \"J\" has a finite radius of convergence \"r\" and diverges at \"x\" = \"r\". In this case one defines the sum as above, except taking the limit as \"x\" tends to \"r\" rather than infinity.\n\nIn the special case when \"J\"(\"x\") = \"e\" this gives one (weak) form of Borel summation.\n\nValiron's method is a generalization of Borel summation to certain more general integral functions \"J\". Valiron showed that under certain conditions it is equivalent to defining the sum of a series as \nwhere \"H\" is the second derivative of \"G\" and \"c\"(\"n\") = \"e\", and \"a\" + ... + \"a\" is to be interpreted as 0 when h <0.\n\nSuppose that \"dμ\" is a measure on the real line such that all the moments\n\nare finite. If \"a\" + \"a\" + ... is a series such that\n\nconverges for all \"x\" in the support of \"μ\", then the (\"dμ\") sum of the series is defined to be the value of the integral\n\nif it is defined. (Note that if the numbers \"μ\" increase too rapidly then they do not uniquely determine the measure \"μ\".)\n\nFor example, if \"dμ\" = \"e\" \"dx\" for positive \"x\" and 0 for negative \"x\" then \"μ\" = \"n\"!, and this gives one version of Borel summation, where the value of a sum is given by \n\nThere is a generalization of this depending on a variable \"α\", called the (B′,\"α\") sum, where the sum of a series \"a\" + ... is defined to be\n\nif this integral exists. A further generalization is to replace the sum under the integral by its analytic continuation from small \"t\".\n\nIn 1812 Hutton introduced a method of summing divergent series by starting with the sequence of partial sums, and repeated applying the operation of replacing a sequence \"s\", \"s\", ... by the sequence of averages , , ..., and then taking the limit .\n\nThe series \"a\" + ... is called Ingham summable to \"s\" if\n\nAlbert Ingham showed that if \"δ\" is any positive number then (C,−\"δ\") (Cesàro) summability implies Ingham summability, and Ingham summability implies (C,\"δ\") summability .\n\nThe series \"a\" + ... is called Lambert summable to \"s\" if\n\nIf a series is (C,\"k\") (Cesàro) summable for any \"k\" then it is Lambert summable to the same value, and if a series is Lambert summable then it is Abel summable to the same value .\n\nThe series \"a\" + ... is called Le Roy summable to \"s\" if\n\nThe series \"a\" + ... is called Mittag-Leffler (M) summable to \"s\" if\n\nRamanujan summation is a method of assigning a value to divergent series used by Ramanujan and based on the Euler–Maclaurin summation formula. The Ramanujan sum of a series \"f\"(0) + \"f\"(1) + ... depends not only on the values of \"f\" at integers, but also on values of the function \"f\" at non-integral points, so it is not really a summation method in the sense of this article.\n\nThe series \"a\" + ... is called (R,\"k\") (or Riemann) summable to \"s\" if\n\nThe series \"a\" + ... is called R summable to \"s\" if\n\nIf \"λ\" form an increasing sequence of real numbers and\n\nthen the Riesz (R,\"λ\",\"κ\") sum of the series \"a\" + ... is defined to be\n\nThe series \"a\" + ... is called VP (or Vallée-Poussin) summable to \"s\" if\n\n\n"}
{"id": "42260413", "url": "https://en.wikipedia.org/wiki?curid=42260413", "title": "Erdős–Nicolas number", "text": "Erdős–Nicolas number\n\nIn number theory, an Erdős–Nicolas number is a number that is not perfect, but that equals one of the partial sums of its divisors.\nThat is, a number is Erdős–Nicolas number when there exists another number such that\n\nThe first ten Erdős–Nicolas numbers are\nThey are named after Paul Erdős and Jean-Louis Nicolas, who wrote about them in 1975.\n\n"}
{"id": "1706332", "url": "https://en.wikipedia.org/wiki?curid=1706332", "title": "Feedforward neural network", "text": "Feedforward neural network\n\nA feedforward neural network is an artificial neural network wherein connections between the nodes do \"not\" form a cycle. As such, it is different from recurrent neural networks.\n\nThe feedforward neural network was the first and simplest type of artificial neural network devised. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.\n\nThe simplest kind of neural network is a \"single-layer perceptron\" network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of activation function are also called \"artificial neurons\" or \"linear threshold units\". In the literature the term \"perceptron\" often refers to networks consisting of just one of these units. A similar neuron was described by Warren McCulloch and Walter Pitts in the 1940s.\n\nA perceptron can be created using any values for the activated and deactivated states as long as the threshold value lies between the two.\n\nPerceptrons can be trained by a simple learning algorithm that is usually called the \"delta rule\". It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of gradient descent.\n\nSingle-layer perceptrons are only capable of learning linearly separable patterns; in 1969 in a famous monograph entitled \"Perceptrons\", Marvin Minsky and Seymour Papert showed that it was impossible for a single-layer perceptron network to learn an XOR function (nonetheless, it was known that multi-layer perceptrons are capable of producing any possible boolean function). \n\nAlthough a single threshold unit is quite limited in its computational power, it has been shown that networks of parallel threshold units can approximate any continuous function from a compact interval of the real numbers into the interval [-1,1]. This result can be found in Peter Auer, Harald Burgsteiner and Wolfgang Maass \"A learning rule for very simple universal approximators consisting of a single layer of perceptrons\".\n\nA single-layer neural network can compute a continuous output instead of a step function. A common choice is the so-called logistic function:\n\nWith this choice, the single-layer network is identical to the logistic regression model, widely used in statistical modeling. The logistic function is also known as the sigmoid function. It has a continuous derivative, which allows it to be used in backpropagation. This function is also preferred because its derivative is easily calculated:\n\nThis class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a \"sigmoid function\" as an activation function.\n\nThe \"universal approximation theorem\" for neural networks states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a multi-layer perceptron with just one hidden layer. This result holds for a wide range of activation functions, e.g. for the sigmoidal functions.\n\nMulti-layer networks use a variety of learning techniques, the most popular being \"back-propagation\". Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has \"learned\" a certain target function. To adjust weights properly, one applies a general method for non-linear optimization that is called gradient descent. For this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions.\n\nIn general, the problem of teaching a network to perform well, even on samples that were not used as training samples, is a quite subtle issue that requires additional techniques. This is especially important for cases where only very limited numbers of training samples are available. The danger is that the network overfits the training data and fails to capture the true statistical process generating the data. Computational learning theory is concerned with training classifiers on a limited amount of data. In the context of neural networks a simple heuristic, called early stopping, often ensures that the network will generalize well to examples not in the training set.\n\nOther typical problems of the back-propagation algorithm are the speed of convergence and the possibility of ending up in a local minimum of the error function. Today there are practical methods that make back-propagation in multi-layer perceptrons the tool of choice for many machine learning tasks.\n\nOne also can use a series of independent neural networks moderated by some intermediary, a similar behavior that happens in brain. These neurons can perform separably and handle a large task, and the results can be finally combined. \n\n\n"}
{"id": "13046", "url": "https://en.wikipedia.org/wiki?curid=13046", "title": "Geometric mean", "text": "Geometric mean\n\nIn mathematics, the geometric mean is a mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the th root of the product of numbers, i.e., for a set of numbers , the geometric mean is defined as\n\nFor instance, the geometric mean of two numbers, say 2 and 8, is just the square root of their product, that is, formula_2. As another example, the geometric mean of the three numbers 4, 1, and 1/32 is the cube root of their product (1/8), which is 1/2, that is, formula_3.\n\nA geometric mean is often used when comparing different items—finding a single \"figure of merit\" for these items—when each item has multiple properties that have different numeric ranges. For example, the geometric mean can give a meaningful \"average\" to compare two companies which are each rated at 0 to 5 for their environmental sustainability, and are rated at 0 to 100 for their financial viability. If an arithmetic mean were used instead of a geometric mean, the financial viability is given more weight because its numeric range is larger—so a small percentage change in the financial rating (e.g. going from 80 to 90) makes a much larger difference in the arithmetic mean than a large percentage change in environmental sustainability (e.g. going from 2 to 5). The use of a geometric mean \"normalizes\" the ranges being averaged, so that no range dominates the weighting, and a given percentage change in any of the properties has the same effect on the geometric mean. So, a 20% change in environmental sustainability from 4 to 4.8 has the same effect on the geometric mean as a 20% change in financial viability from 60 to 72.\n\nThe geometric mean can be understood in terms of geometry. The geometric mean of two numbers, formula_4 and formula_5, is the length of one side of a square whose area is equal to the area of a rectangle with sides of lengths formula_4 and formula_5. Similarly, the geometric mean of three numbers, formula_4, formula_5, and formula_10, is the length of one edge of a cube whose volume is the same as that of a cuboid with sides whose lengths are equal to the three given numbers.\n\nThe geometric mean applies only to positive numbers. It is also often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as data on the growth of the human population or interest rates of a financial investment.\n\nThe geometric mean is also one of the three classical Pythagorean means, together with the aforementioned arithmetic mean and the harmonic mean. For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see Inequality of arithmetic and geometric means.)\n\nThe geometric mean of a data set formula_11 is given by:\n\nThe above figure uses capital pi notation to show a series of multiplications. Each side of the equal sign shows that a set of values is multiplied in succession (the number of values is represented by \"n\") to give a total product of the set, and then the \"n\"th root of the total product is taken to give the geometric mean of the original set. For example, in a set of four numbers formula_13, the product of formula_14 is formula_15, and the geometric mean is the fourth root of 24, or ~ 2.213. The exponent formula_16 on the left side is equivalent to the taking \"n\"th root. For example, formula_17.\n\nThe geometric mean of a data set is less than the data set's arithmetic mean unless all members of the data set are equal, in which case the geometric and arithmetic means are equal. This allows the definition of the arithmetic-geometric mean, an intersection of the two which always lies in between.\n\nThe geometric mean is also the arithmetic-harmonic mean in the sense that if two sequences (formula_18) and (formula_19) are defined:\n\nand\n\nwhere formula_22 is the harmonic mean of the previous values of the two sequences, then formula_18 and formula_19 will converge to the geometric mean of formula_25 and formula_26.\n\nThis can be seen easily from the fact that the sequences do converge to a common limit (which can be shown by Bolzano–Weierstrass theorem) and the fact that geometric mean is preserved:\n\nReplacing the arithmetic and harmonic mean by a pair of generalized means of opposite, finite exponents yields the same result.\n\nThe geometric mean can also be expressed as the exponential of the arithmetic mean of logarithms. By using logarithmic identities to transform the formula, the multiplications can be expressed as a sum and the power as a multiplication:\n\nThis is sometimes called the log-average (not to be confused with the logarithmic average). It is simply computing the arithmetic mean of the logarithm-transformed values of formula_32 (i.e., the arithmetic mean on the log scale) and then using the exponentiation to return the computation to the original scale, i.e., it is the generalised f-mean with formula_33. For example, the geometric mean of 2 and 8 can be calculated as the following, where formula_5 is any base of a logarithm (commonly 2, formula_35 or 10):\n\nRelated to the above, it can be seen that for a given sample of points formula_37, the geometric mean is the minimizer of formula_38, whereas the arithmetic mean is the minimizer of formula_39. Thus, the geometric mean provides a summary of the samples whose exponent best matches the exponents of the samples (in the least squares sense).\n\nThe log form of the geometric mean is generally the preferred alternative for implementation in computer languages because calculating the product of many numbers can lead to an arithmetic overflow or arithmetic underflow. This is less likely to occur with the sum of the logarithms for each number.\n\nIf a set of non-identical numbers is subjected to a mean-preserving spread — that is, two or more elements of the set are \"spread apart\" from each other while leaving the arithmetic mean unchanged — then the geometric mean always decreases.\n\nIn cases where the geometric mean is being used to determine the average growth rate of some quantity, and the initial and final values formula_40 and formula_41 of that quantity are known, the product of the measured growth rate at every step need not be taken. Instead, the geometric mean is simply\n\nwhere formula_43 is the number of steps from the initial to final state.\n\nIf the values are formula_44, then the growth rate between measurement formula_45 and formula_46 is formula_47. The geometric mean of these growth rates is just\n\nThe fundamental property of the geometric mean, which can be proven to be false for any other mean, is\n\nThis makes the geometric mean the only correct mean when averaging \"normalized\" results; that is, results that are presented as ratios to reference values. This is the case when presenting computer performance with respect to a reference computer, or when computing a single average index from several heterogeneous sources (for example, life expectancy, education years, and infant mortality). In this scenario, using the arithmetic or harmonic mean would change the ranking of the results depending on what is used as a reference. For example, take the following comparison of execution time of computer programs:\n\nThe arithmetic and geometric means \"agree\" that computer C is the fastest. However, by presenting appropriately normalized values \"and\" using the arithmetic mean, we can show either of the other two computers to be the fastest. Normalizing by A's result gives A as the fastest computer according to the arithmetic mean:\n\nwhile normalizing by B's result gives B as the fastest computer according to the arithmetic mean but A as the fastest according to the harmonic mean:\n\nand normalizing by C's result gives C as the fastest computer according to the arithmetic mean but A as the fastest according to the harmonic mean:\n\nIn all cases, the ranking given by the geometric mean stays the same as the one obtained with unnormalized values.\n\nHowever, this reasoning has been questioned.\nGiving consistent results is not always equal to giving the correct results. In general, it is more rigorous to assign weights to each of the programs, calculate the average weighted execution time (using the arithmetic mean), and then normalize that result to one of the computers. The three tables above just give a different weight to each of the programs, explaining the inconsistent results of the arithmetic and harmonic means (the first table gives equal weight to both programs, the second gives a weight of 1/1000 to the second program, and the third gives a weight of 1/100 to the second program and 1/10 to the first one). The use of the geometric mean for aggregating performance numbers should be avoided if possible, because multiplying execution times has no physical meaning, in contrast to adding times as in the arithmetic mean. Metrics that are inversely proportional to time (speedup, IPC) should be averaged using the harmonic mean.\n\nThe geometric mean is more appropriate than the arithmetic mean for describing proportional growth, both exponential growth (constant proportional growth) and varying growth; in business the geometric mean of growth rates is known as the compound annual growth rate (CAGR). The geometric mean of growth over periods yields the equivalent constant growth rate that would yield the same final amount.\n\nSuppose an orange tree yields 100 oranges one year and then 180, 210 and 300 the following years, so the growth is 80%, 16.6666% and 42.8571% for each year respectively. Using the arithmetic mean calculates a (linear) average growth of 46.5079% (80% + 16.6666% + 42.8571%, that sum then divided by 3). However, if we start with 100 oranges and let it grow 46.5079% each year, the result is 314 oranges, not 300, so the linear average \"over\"-states the year-on-year growth.\n\nInstead, we can use the geometric mean. Growing with 80% corresponds to multiplying with 1.80, so we take the geometric mean of 1.80, 1.166666 and 1.428571, i.e. formula_50; thus the \"average\" growth per year is 44.2249%. If we start with 100 oranges and let the number grow with 44.2249% each year, the result is 300 oranges.\n\nAlthough the geometric mean has been relatively rare in computing social statistics, starting from 2010 the United Nations Human Development Index did switch to this mode of calculation, on the grounds that it better reflected the non-substitutable nature of the statistics being compiled and compared:\n\nNot all values used to compute the HDI (Human Development Index) are normalized; some of them instead have the form formula_51. This makes the choice of the geometric mean less obvious than one would expect from the \"Properties\" section above.\n\nThe geometric mean has been used in choosing a compromise aspect ratio in film and video: given two aspect ratios, the geometric mean of them provides a compromise between them, distorting or cropping both in some sense equally. Concretely, two equal area rectangles (with the same center and parallel sides) of different aspect ratios intersect in a rectangle whose aspect ratio is the geometric mean, and their hull (smallest rectangle which contains both of them) likewise has aspect ratio their geometric mean.\n\nIn aspect ratio by the SMPTE, balancing 2.35 and 4:3, the geometric mean is formula_52, and thus formula_53... was chosen. This was discovered empirically by Kerns Powers, who cut out rectangles with equal areas and shaped them to match each of the popular aspect ratios. When overlapped with their center points aligned, he found that all of those aspect ratio rectangles fit within an outer rectangle with an aspect ratio of 1.77:1 and all of them also covered a smaller common inner rectangle with the same aspect ratio 1.77:1. The value found by Powers is exactly the geometric mean of the extreme aspect ratios, (1.33:1) and CinemaScope(2.35:1), which is coincidentally close to formula_54 (formula_55). The intermediate ratios have no effect on the result, only the two extreme ratios.\n\nApplying the same geometric mean technique to 16:9 and 4:3 approximately yields the (formula_56...) aspect ratio, which is likewise used as a compromise between these ratios. In this case 14:9 is exactly the \"arithmetic mean\" of formula_54 and formula_58, since 14 is the average of 16 and 12, while the precise \"geometric mean\" is formula_59 but the two different \"means\", arithmetic and geometric, are approximately equal because both numbers are sufficiently close to each other (a difference of less than 2%).\n\nIn optical coatings, where reflection needs to be minimised between two media of refractive indices \"n\" and \"n\", the optimum refractive index \"n\" of the anti-reflective coating is given by the geometric mean: formula_60.\n\nIn signal processing, spectral flatness, a measure of how flat or spiky a spectrum is, is defined as the ratio of the geometric mean of the power spectrum to its arithmetic mean.\n\nIn the case of a right triangle, its altitude is the length of a line extending perpendicularly from the hypotenuse to its 90° vertex. Imagining that this line splits the hypotenuse into two segments, the geometric mean of these segment lengths is the length of the altitude.\n\nIn an ellipse, the semi-minor axis is the geometric mean of the maximum and minimum distances of the ellipse from a focus; it is also the geometric mean of the semi-major axis and the semi-latus rectum. The semi-major axis of an ellipse is the geometric mean of the distance from the center to either focus and the distance from the center to either directrix.\n\nDistance to the horizon of a sphere is the geometric mean of the distance to the closest point of the sphere and the distance to the farthest point of the sphere.\n\nBoth in the approximation of squaring the circle according to S.A. Ramanujan (1914) and in the construction of the Heptadecagon according to \"sent by T. P. Stowell, credited to Leybourn's Math. Repository, 1818\", the geometric mean is employed.\n\nThe geometric mean has from time to time been used to calculate financial indices (the averaging is over the components of the index). For example, in the past the FT 30 index used a geometric mean. It is also used in the recently introduced \"RPIJ\" measure of inflation in the United Kingdom and elsewhere in the European Union.\n\nThis has the effect of understating movements in the index compared to using the arithmetic mean.\n\nThe geometric mean filter is used as a noise filter in image processing.\n\n"}
{"id": "26987628", "url": "https://en.wikipedia.org/wiki?curid=26987628", "title": "Graphon", "text": "Graphon\n\nIn graph theory and statistics, a graphon is a symmetric measurable function formula_1, that is important in the study of dense graphs. Graphons arise as the fundamental objects in two areas: as the defining objects of exchangeable random graph models and as a natural notion of limit for sequences of dense graphs. Graphons are tied to dense graphs by the following pair of observations: the random graph models defined by graphons give rise to dense graphs almost surely, and, by the regularity lemma, graphons capture the structure of arbitrary large dense graphs.\n\nGraphons are sometimes referred to as “continuous graphs”, but this is bad practice because there are many distinct objects that this label might be applied to. In particular, there are generalizations of graphons to the sparse graph regime that could just as well be called “continuous graphs.”\n\nA graphon is a symmetric measurable function formula_2. Usually a graphon is understood as defining an exchangeable random graph model according to the following scheme:\n\n\nA random graph model is an exchangeable random graph model if and only if it can be defined in terms of a (possibly random) graphon in this way.\n\nIt is an immediate consequence of this definition and the law of large numbers that, if formula_7, exchangeable random graph models are dense almost surely.\n\nThe simplest example of a graphon is formula_8 for some constant formula_9. In this case the associated exchangeable random graph model is the Erdős–Rényi model that includes each edge independently with probability formula_10.\n\nThe Erdős–Rényi model can be generalized by:\n\n\nEffectively, this gives rise to the random graph model that has formula_15 distinct Erdős–Rényi graphs with parameters formula_16 respectively and bigraphs between them where each possible edge between blocks formula_17 and formula_18 is included independently with probability formula_13. This is simply the formula_15 community stochastic block model.\n\nMany other popular random graph models can be understood as exchangeable random graph models defined by some graphon, a detailed survey is included in.\n\nA random graph of size formula_21 can be represented as a random formula_22 adjacency matrix. In order to impose consistency (in the sense of projectivity) between random graphs of different sizes it is natural to study the sequence of adjacency matrices arising as the upper-left formula_22 sub-matrices of some infinite array of random variables; this allows us to generate formula_24 by adding a node to formula_25 and sampling the edges formula_26 for formula_27. With this perspective, random graphs are defined as random infinite symmetric arrays formula_28.\n\nFollowing the fundamental importance of exchangeable sequences in classical probability, it is natural to look for an analogous notion in the random graph setting. One such notion is given by jointly exchangeable matrices; i.e. random matrices satisfying\n\nfor all permutations formula_30 of the natural numbers, where formula_31 means equal in distribution. Intuitively, this condition means that the distribution of the random graph is unchanged by a relabeling of its vertices: that is, the labels of the vertices carry no information.\n\nThere is a representation theorem for jointly exchangeable random adjacency matrices, analogous to de Finetti’s representation theorem for exchangeable sequences. This is a special case of the Aldous–Hoover theorem for jointly exchangeable arrays and, in this setting, asserts that the random matrix formula_28 is generated by:\n\n\nwhere formula_1 is a (possibly random) graphon. That is, a random graph model has a jointly exchangeable adjacency matrix if and only if it is a jointly exchangeable random graph model defined in terms of some graphon.\n\nConsider a sequence of graphs formula_37 where the number of vertices of formula_24 goes to infinity. It is possible to define several notions of convergence of such sequences, each of which may give rise to a distinct limit object. For example, if the sequence formula_37 was a realization of Erdős–Rényi graphs with parameter formula_10 the natural notion of limit would be the edge density of the graphs, which converges to formula_10. In this case it would be natural to say that the limit of the sequence is the constant graphon formula_8. It turns out that for sequences of dense graphs a number of apparently distinct notions of convergence are equivalent and under all of them the natural limit object is a graphon.\n\nDue to identifiability issues, it is impossible to estimate either the graphon function formula_12 or the node latent positions formula_44 and there are two main directions of graphon estimation. One direction aims at estimating formula_12up to an equivalent class, or estimate the probability matrix induced by formula_12.\n\nGraphons are naturally associated with dense simple graphs. There are straightforward extensions to dense directed weighted graphs. There are also recent extensions to the sparse graph regime, from both the perspective of random graph models and graph limit theory.\n"}
{"id": "35881441", "url": "https://en.wikipedia.org/wiki?curid=35881441", "title": "HH-suite", "text": "HH-suite\n\nThe HH-suite is an open-source software package for sensitive protein sequence searching. It contains programs that can search for similar protein sequences in protein sequence databases. Sequence searches are a standard tool in modern biology with which the function of unknown proteins can be inferred from the functions of proteins with similar sequences. \n\nProteins are central players in all of life's processes. To understand how life in cells is organised, we have to understand what each of the proteins involved in these molecular processes does. This is particularly important in order to understand the origin of diseases. But for a large fraction of the approximately 20 000 human proteins the structures and functions remain unknown. Many proteins have been investigated in model organisms such as many bacteria, baker's yeast, fruit flies, zebra fish or mice, for which experiments can be often done more easily than with human cells. To predict the function, structure, or other properties of a protein for which only its sequence of amino acids is known, the protein sequence is compared to the sequences of other proteins in public databases. If a protein with sufficiently similar sequence is found, the two proteins are likely to be evolutionarily related (\"homologous\"). In that case, they are likely to share similar structures and functions. Therefore, if a protein with a sufficiently similar sequence and with known functions and/or structure can be found by the sequence search, the unknown protein's functions, structure, and domain composition can be predicted. Such predictions greatly facilitate the determination of the function or structure by targeted validation experiments.\n\nThe HH-suite HHsearch contains HHsearch\nand HHblits \namong other programs and utilities. HHsearch is among the most popular methods for the detection of remotely related sequences and for protein structure prediction, having been cited over 2000 times in Google Scholar. The HHsearch and HHblits programs owe their power to the fact that both the query and the database sequences are represented by multiple sequence alignments (MSAs). In these MSAs, the query or database sequence is written in a table together with homologous (related) sequences in such a way that each column contains homologous amino acid residues, that is, residues that have descended from the same residue in the ancestral sequence. The frequencies of amino acids in the columns of such an MSA can be interpreted as probabilities to observe an amino acid in a further homologous sequence at that position. To facilitate automatic scoring of potential sequences for their relatedness to the sequences in the MSA, the MSAs are succinctly described by profile hidden Markov models (HMMs). These are extensions of \"position-specific scoring matrices\" (PSSMs). The core algorithms for HMM-HMM alignment give HH-suite its name.\n\nHHsearch takes as input a multiple sequence alignment or a profile hidden Markov Model (HMM) and searches a database of profile HMMs for homologous (related) proteins. \nHHsearch is often used for homology modeling, that is, to build a model of the structure of a query protein for which only the sequence is known: For that purpose, a database of proteins with known structures such as the protein data bank is searched for \"template\" proteins similar to the query protein. If such a template protein is found, the structure of the protein of interest can be predicted based on a pairwise sequence alignment of the query with the template protein sequence. In the CASP9 protein structure prediction competition in 2010, a fully automated version of HHpred based on HHsearch and HHblits was ranked best out of 81 servers in template-based structure prediction CASP9 TBM/FM.\nHHblits was added to the HH-suite in 2011. It can build high-quality multiple sequence alignments (MSAs) starting from a single query sequence or MSA. From the query, a profile HMM can be calculated. By using MSAs instead of single sequences, the sensitivity of sequence searches and the quality of the resulting sequence alignments can be improved dramatically. MSAs are also the starting point for a multitude of downstream computational methods, such as methods to predict the secondary and tertiary structure of proteins, to predict their molecular functions or cellular pathways, to predict the positions in their sequence or structure that contribute to enzymatic activity or ligand-binding, to predict evolutionarily conserved residues, disease-causing versus neutral mutations, the proteins' cellular localization and many more. This explains the importance to produce MSAs of the highest quality.\n\nHHblits works similarly to PSI-BLAST, the most popular iterative sequence search method. HHblits generates a profile HMM from the query sequence and iteratively searches through a large database of profile HMMs, such as HH-suite's uniprot20 database. The uniprot20 database contains all public, high-quality protein sequences that are collected in the UniProt database. These sequences are clustered and aligned into multiple sequence alignments, from which the profile HMMs in uniprot20 are generated. Significantly similar sequences from the previous search are added to the query profile HMM for the next search iteration. Compared to PSI-BLAST and HMMER, HHblits is faster, up to twice as sensitive and produces more accurate alignments. HHblits uses the same HMM-HMM alignment algorithms as HHsearch, but it employs a fast prefilter that reduces the number of database HMMs for which to perform the slow HMM-HMM comparison from tens of millions to a few thousands.\n\nThe HH-suite comes with a number of useful databases of profile HMMs that can be searched using HHblits and HHsearch, among them a clustered version of the UniProt database, HMMs for the protein data bank of protein structures, for the Pfam database of protein family alignments, the SCOP database of structural protein domains, and many more.\n\nThe HH-suite runs on most Linux and Unix distributions, including RedHat, Debian, Ubuntu, and Mac OS X. A Debian package is available.\n\nIn addition to HHsearch and HHblits, the HH-suite contains programs and perl scripts for format conversion, filtering of MSAs, generation of profile HMMs, the addition of secondary structure predictions to MSAs, the extraction of alignments from program output, and the generation of customized databases.\n\n"}
{"id": "448518", "url": "https://en.wikipedia.org/wiki?curid=448518", "title": "Heisenberg group", "text": "Heisenberg group\n\nIn mathematics, the Heisenberg group formula_1, named after Werner Heisenberg, is the group of 3×3 upper triangular matrices of the form\n\nunder the operation of matrix multiplication. Elements \"a, b\" and \"c\" can be taken from any commutative ring with identity, often taken to be the ring of real numbers (resulting in the \"continuous Heisenberg group\") or the ring of integers (resulting in the \"discrete Heisenberg group\").\n\nThe continuous Heisenberg group arises in the description of one-dimensional quantum mechanical systems, especially in the context of the Stone–von Neumann theorem. More generally, one can consider Heisenberg groups associated to \"n\"-dimensional systems, and most generally, to any symplectic vector space.\n\nIn the three-dimensional case, the product of two Heisenberg matrices is given by:\nSince the multiplication is not commutative, the group is non-abelian.\n\nThe neutral element of the Heisenberg group is the identity matrix, and inverses are given by\n\nThe group is a subgroup of the 2-dimensional affine group Aff(2): formula_2 acting on formula_6 corresponds to the affine transform formula_7.\n\nThere are several prominent examples of the three-dimensional case.\n\nIf , are real numbers (in the ring R) then one has the continuous Heisenberg group H(R).\n\nIt is a nilpotent real Lie group of dimension 3.\n\nIn addition to the representation as real 3x3 matrices, the continuous Heisenberg group also has several different representations in terms of function spaces. By Stone–von Neumann theorem, there is, up to isomorphism, a unique irreducible unitary representation of H in which its centre acts by a given nontrivial character. This representation has several important realizations, or models. In the \"Schrödinger model\", the Heisenberg group acts on the space of square integrable functions. In the theta representation, it acts on the space of holomorphic functions on the upper half-plane; it is so named for its connection with the theta functions.\n\nIf , are integers (in the ring Z) then one has the discrete Heisenberg group H(Z). It is a non-abelian nilpotent group. It has two generators,\n\nand relations\n\nwhere\n\nis the generator of the center of H. (Note that the inverses of \"x\", \"y\", and \"z\" replace the 1 above the diagonal with −1.)\n\nBy Bass's theorem, it has a polynomial growth rate of order 4.\n\nOne can generate any element through\n\nIf one takes \"a, b, c\" in Z/\"p\" Z for an odd prime \"p\", then one has the Heisenberg group modulo p. It is a group of order \"p\" with generators \"x,y\" and relations:\n\nAnalogues of Heisenberg groups over \"finite\" fields of odd prime order \"p\" are called extra special groups, or more properly, extra special groups of exponent \"p\". More generally, if the derived subgroup of a group \"G\" is contained in the center \"Z\" of \"G\", then the map from \"G/Z\" × \"G/Z\" → \"Z\" is a skew-symmetric bilinear operator on abelian groups. \n\nHowever, requiring that \"G/Z\" to be a finite vector space requires the Frattini subgroup of \"G\" to be contained in the center, and requiring that \"Z\" be a one-dimensional vector space over Z/\"p\" Z requires that \"Z\" have order \"p\", so if \"G\" is not abelian, then \"G\" is extra special. If \"G\" is extra special but does not have exponent \"p\", then the general construction below applied to the symplectic vector space \"G/Z\" does not yield a group isomorphic to \"G\".\n\nThe Heisenberg group modulo 2 is of order 8 and is isomorphic to the dihedral group D (the symmetries of a square). Observe that if\n\nThen\n\nand\n\nThe elements \"x\" and \"y\" correspond to reflections (with 45° between them), whereas \"xy\" and \"yx\" correspond to rotations by 90°. The other reflections are \"xyx\" and \"yxy\", and rotation by 180° is \"xyxy\" (=\"yxyx\").\n\nThe Lie algebra formula_16 of the Heisenberg group formula_1 (over the real numbers) is the space of formula_18 matrices of the form\nwith formula_20. The following three elements form a basis for formula_16:\nThe basis elements satisfy the commutation relations:\nThe name \"Heisenberg group\" is motivated by the preceding relations, which have the same form as the canonical commutation relations in quantum mechanics:\nwhere formula_25 is the position operator, formula_26 is the momentum operator, and formula_27 is Planck's constant.\n\nThe Heisenberg group formula_1 has the special property that the exponential map is a one-to-one and onto map from the Lie algebra formula_16 to the group formula_1.\n\nMore general Heisenberg groups formula_31 may be defined for higher dimensions in Euclidean space, and more generally on symplectic vector spaces. The simplest general case is the real Heisenberg group of dimension formula_32, for any integer formula_33. As a group of matrices, formula_31 (or formula_35 to indicate this is the Heisenberg group over the field formula_36 of real numbers) is defined as the group formula_37 matrices with entries in formula_36 and having the form:\n\nwhere\n\nThis is indeed a group, as is shown by the multiplication:\n\nand\n\nThe Heisenberg group is a simply-connected Lie group whose Lie algebra consists of matrices\n\nwhere\n\nBy letting e, ..., e be the canonical basis of R, and setting\n\nthe associated Lie algebra can be characterized by the canonical commutation relations,\nwhere \"p\", ..., \"p\", \"q\", ..., \"q\", \"z\" are the algebra generators.\n\nIn particular, \"z\" is a \"central\" element of the Heisenberg Lie algebra. Note that the Lie algebra of the Heisenberg group is nilpotent.\n\nThe exponential map is given by the following expression\n\nThe exponential map of a nilpotent Lie algebra is a diffeomorphism between the Lie algebra and the unique associated connected, simply-connected Lie group.\n\nThis discussion (aside from statements referring to dimension and Lie group) further applies if we replace R by any commutative ring \"A\". The corresponding group is denoted \"H\"(\"A\" ).\n\nUnder the additional assumption that the prime 2 is invertible in the ring \"A\", the exponential map is also defined, since it reduces to a finite sum and has the form above (i.e. \"A\" could be a ring Z/\"p\" Z with an odd prime \"p\" or any field of characteristic 0).\n\nThe unitary representation theory of the Heisenberg group is fairly simple – later generalized by Mackey theory – and was the motivation for its introduction in quantum physics, as discussed below.\n\nFor each nonzero real number formula_27, we can define an irreducible unitary representation formula_48 of formula_31 acting on the Hilbert space formula_50 by the formula:\n\nThis representation is known as the Schrödinger representation. The motivation for the this representation is the action of the exponentiated position and momentum operators in quantum mechanics. The parameter formula_52 describes translations in position space, the parameter formula_53 describes translations in momentum space, and the parameter formula_54 gives an overall phase factor. The phase factor is needed to obtain a group of operators, since translations in position space and translations in momentum space do not commute.\n\nThe key result is the Stone–von Neumann theorem, which states that every (strongly continuous) irreducible unitary representation of the Heisenberg group in which the center acts nontrivially is equivalent to formula_48 for some formula_27. Alternatively, that they are all equivalent to the Weyl algebra (or CCR algebra) on a symplectic space of dimension 2\"n\".\n\nSince the Heisenberg group is a one-dimensional central extension of formula_57, its irreducible unitary representations can be viewed as irreducible unitary projective representations of formula_57. Conceptually, the representation given above constitutes the quantum mechanical counterpart to the group of translational symmetries on the classical phase space, formula_57. The fact that the quantum version is only a \"projective\" representation of formula_60 is suggested already at the classical level. The Hamiltonian generators of translations in phase space are the position and momentum functions. The span of these functions do not form a Lie algebra under the Poisson bracket however, because formula_61 Rather, the span of the position and momentum functions \"and the constants\" forms a Lie algebra under the Poisson bracket. This Lie algebra is a one-dimensional central extension of the commutative Lie algebra formula_57, isomorphic to the Lie algebra of the Heisenberg group.\n\nThe general abstraction of a Heisenberg group is constructed from any symplectic vector space. For example, let (\"V\",ω) be a finite-dimensional real symplectic vector space (so ω is a nondegenerate skew symmetric bilinear form on \"V\"). The Heisenberg group H(\"V\") on (\"V\",ω) (or simply \"V\" for brevity) is the set \"V\"×R endowed with the group law\n\nThe Heisenberg group is a central extension of the additive group \"V\". Thus there is an exact sequence\n\nAny symplectic vector space admits a Darboux basis {e,f} satisfying ω(e,f) = δ and where 2\"n\" is the dimension of \"V\" (the dimension of \"V\" is necessarily even). In terms of this basis, every vector decomposes as\n\nThe \"q\" and \"p\" are canonically conjugate coordinates.\n\nIf {e, f} is a Darboux basis for \"V\", then let {\"E\"} be a basis for R, and {e, f, \"E\"} is the corresponding basis for \"V\"×R. A vector in H(\"V\") is then given by\n\nand the group law becomes\n\nBecause the underlying manifold of the Heisenberg group is a linear space, vectors in the Lie algebra can be canonically identified with vectors in the group. The Lie algebra of the Heisenberg group is given by the commutation relation\n\nor written in terms of the Darboux basis\n\nand all other commutators vanish.\n\nIt is also possible to define the group law in a different way but which yields a group isomorphic to the group we have just defined. To avoid confusion, we will use \"u\" instead of \"t\", so a vector is given by\n\nand the group law is\n\nAn element of the group \ncan then be expressed as a matrix\nwhich gives a faithful matrix representation of H(\"V\"). The \"u\" in this formulation is related to \"t\" in our previous formulation by formula_74, so that the \"t\" value for the product comes to\n\nas before.\n\nThe isomorphism to the group using upper triangular matrices relies on the decomposition of \"V\" into a Darboux basis, which amounts to a choice of isomorphism \"V\" ≅ \"U\" ⊕ \"U\"*. Although the new group law yields a group isomorphic to the one given higher up, the group with this law is sometimes referred to as the polarized Heisenberg group as a reminder that this group law relies on a choice of basis (a choice of a Lagrangian subspace of \"V\" is a polarization).\n\nTo any Lie algebra, there is a unique connected, simply connected Lie group \"G\". All other connected Lie groups with the same Lie algebra as \"G\" are of the form \"G\"/\"N\" where \"N\" is a central discrete group in \"G\". In this case, the center of H(\"V\") is R and the only discrete subgroups are isomorphic to \"Z\". Thus H(\"V\")/Z is another Lie group which shares this Lie algebra. Of note about this Lie group is that it admits no faithful finite-dimensional representations; it is not isomorphic to any matrix group. It does however have a well-known family of infinite-dimensional unitary representations.\n\nThe Lie algebra formula_78 of the Heisenberg group was described above, (1), as a Lie algebra of matrices. The Poincaré–Birkhoff–Witt theorem applies to determine the universal enveloping algebra formula_79. Among other properties, the universal enveloping algebra is an associative algebra into which formula_78 injectively imbeds.\n\nBy the Poincaré–Birkhoff–Witt theorem, it is thus the free vector space generated by the monomials\nwhere the exponents are all non-negative.\n\nConsequently, formula_79 consists of real polynomials \nwith the commutation relations\n\nThe algebra formula_79 is closely related to the algebra of differential operators on ℝ with polynomial coefficients, since any such operator has a unique representation in the form\n\nThis algebra is called the Weyl algebra. It follows from abstract nonsense that the Weyl algebra \"W\" is a quotient of formula_79. However, this is also easy to see directly from the above representations; viz. by the mapping\n\nThe application that led Hermann Weyl to an explicit realization of the Heisenberg group was the question of why the Schrödinger picture and Heisenberg picture are physically equivalent. Abstractly, the reason is the Stone–von Neumann theorem: there is a unique unitary representation with given action of the central Lie algebra element \"z\", up to a unitary equivalence: the nontrivial elements of the algebra are all equivalent to the usual position and momentum operators.\n\nThus, the Schrödinger picture and Heisenberg picture are equivalent – they are just different ways of realizing this essentially unique representation.\n\nThe same uniqueness result was used by David Mumford for discrete Heisenberg groups, in his theory of equations defining abelian varieties. This is a large generalization of the approach used in Jacobi's elliptic functions, which is the case of the modulo 2 Heisenberg group, of order 8. The simplest case is the theta representation of the Heisenberg group, of which the discrete case gives the theta function.\n\nThe Heisenberg group also occurs in Fourier analysis, where it is used in some formulations of the Stone–von Neumann theorem. In this case, the Heisenberg group can be understood to act on the space of square integrable functions; the result is a representation of the Heisenberg groups sometimes called the Weyl representation.\n\nThe three-dimensional Heisenberg group \"H\"(R) on the reals can also be understood to be a smooth manifold, and specifically, a simple example of a sub-Riemannian manifold. Given a point \"p\"=(\"x\",\"y\",\"z\") in R, define a differential 1-form Θ at this point as\n\nThis one-form belongs to the cotangent bundle of R; that is,\n\nis a map on the tangent bundle. Let\n\nIt can be seen that \"H\" is a subbundle of the tangent bundle TR. A cometric on \"H\" is given by projecting vectors to the two-dimensional space spanned by vectors in the \"x\" and \"y\" direction. That is, given vectors formula_92 and formula_93 in TR, the inner product is given by\n\nThe resulting structure turns \"H\" into the manifold of the Heisenberg group. An orthonormal frame on the manifold is given by the Lie vector fields\n\nwhich obey the relations [\"X\",\"Y\"]=\"Z\" and [\"X\",\"Z\"]=[\"Y\",\"Z\"]=0. Being Lie vector fields, these form a left-invariant basis for the group action. The geodesics on the manifold are spirals, projecting down to circles in two dimensions. That is, if\n\nis a geodesic curve, then the curve formula_99 is an arc of a circle, and\n\nwith the integral limited to the two-dimensional plane. That is, the height of the curve is proportional to the area of the circle subtended by the circular arc, which follows by Stokes' theorem.\n\nIt is more generally possible to define the Heisenberg group of a locally compact abelian group \"K\", equipped with a Haar measure. Such a group has a Pontrjagin dual formula_101, consisting of all continuous formula_102-valued characters on \"K\", which is also a locally compact abelian group if endowed with the compact-open topology. The Heisenberg group associated with the locally compact abelian group \"K\" is the subgroup of the unitary group of formula_103 generated by translations from \"K\" and multiplications by elements of formula_101.\n\nIn more detail, the Hilbert space formula_103 consists of square-integrable complex-valued functions formula_106 on \"K\". The translations in \"K\" form a unitary representation of \"K\" as operators on formula_103:\nfor formula_109. So too do the multiplications by characters:\nfor formula_111. These operators do not commute, and instead satisfy\nmultiplication by a fixed unit modulus complex number.\n\nSo the Heisenberg group formula_113 associated with \"K\" is a type of central extension of formula_114, via an exact sequence of groups:\nMore general Heisenberg groups are described by 2-cocyles in the cohomology group formula_116. The existence of a duality between formula_117 and formula_101 gives rise to a canonical cocycle, but there are generally others.\n\nThe Heisenberg group acts irreducibly on formula_103. Indeed, the continuous characters separate points so any unitary operator of formula_103 that commutes with them is an formula_121 multiplier. But commuting with translations implies that the multiplier is constant.\n\nA version of the Stone–von Neumann theorem, proved by George Mackey, holds for the Heisenberg group formula_113. The Fourier transform is the unique intertwiner between the representations of formula_103 and formula_124. See the discussion at Stone–von Neumann theorem#Relation to the Fourier transform for details.\n\n\n\n"}
{"id": "38973439", "url": "https://en.wikipedia.org/wiki?curid=38973439", "title": "Higgs field (classical)", "text": "Higgs field (classical)\n\nSpontaneous symmetry breaking, a vacuum Higgs field, and its associated fundamental particle the Higgs boson are quantum phenomena. A vacuum Higgs field is responsible for spontaneous symmetry breaking the gauge symmetries of fundamental interactions and provides the Higgs mechanism of generating mass of elementary particles.\n\nAt the same time, classical gauge theory admits comprehensive geometric formulation where gauge fields are represented by connections on principal bundles. In this framework, spontaneous symmetry breaking is characterized as a reduction of the structure group formula_1 of a principal bundle formula_2 to its closed subgroup formula_3. By the well-known theorem, such a reduction takes place if and only if there exists a global section formula_4 of the quotient bundle formula_5. This section is treated as a classical Higgs field.\n\nA key point is that there exists a composite bundle formula_6 where formula_7 is a principal bundle with the structure group formula_3. Then matter fields, possessing an exact symmetry group formula_3, in the presence of classical Higgs fields are described by sections of some composite bundle formula_10, where formula_11 is some associated bundle to formula_7. Herewith, a Lagrangian of these matter fields is gauge invariant only if it factorizes through the vertical covariant differential of some connection on a principal bundle formula_7, but not formula_2.\n\nAn example of a classical Higgs field is a classical gravitational field identified with a pseudo-Riemannian metric on a world manifold formula_15. In the framework of gauge gravitation theory, it is described as a global section of the quotient bundle formula_16 where formula_17 is a principal bundle of the tangent frames to formula_18 with the structure group formula_19.\n\n\n"}
{"id": "1965023", "url": "https://en.wikipedia.org/wiki?curid=1965023", "title": "Hilbert's twenty-first problem", "text": "Hilbert's twenty-first problem\n\nThe twenty-first problem of the 23 Hilbert problems, from the celebrated list put forth in 1900 by David Hilbert, concerns the existence of a certain class of linear differential equations with specified singular points and monodromic group.\n\nThe original problem was stated as follows (English translation from 1902):\n\nIn fact it is more appropriate to speak not about differential equations but about linear systems of differential equations: in order to realise any monodromy by a differential equation one has to admit, in general, the presence of additional apparent singularities, i.e. singularities with trivial local monodromy. In more modern language, the (systems of) differential equations in question are those defined in the complex plane, less a few points, and with a regular singularity at those. A more strict version of the problem requires these singularities to be Fuchsian, i.e. poles of first order (logarithmic poles). A monodromy group is prescribed, by means of a finite-dimensional complex representation of the fundamental group of the complement in the Riemann sphere of those points, plus the point at infinity, up to equivalence. The fundamental group is actually a free group, on 'circuits' going once round each missing point, starting and ending at a given base point. The question is whether the mapping from these \"Fuchsian\" equations to classes of representations is surjective.\n\nThis problem is more commonly called the Riemann–Hilbert problem. There is now a modern (D-module and derived category) version, the 'Riemann–Hilbert correspondence' in all dimensions. The history of proofs involving a single complex variable is complicated. Josip Plemelj published a solution in 1908. This work was for a long time accepted as a definitive solution; there was work of G. D. Birkhoff in 1913 also, but the whole area, including work of Ludwig Schlesinger on isomonodromic deformations that would much later be revived in connection with soliton theory, went out of fashion. wrote a monograph summing up his work. A few years later the Soviet mathematician Yuliy S. Il'yashenko and others started raising doubts about Plemelj's work. In fact, Plemelj correctly proves that any monodromy group can be realised by a regular linear system which is Fuchsian at all but one of the singular points. Plemelj's claim that the system can be made Fuchsian at the last point as well is wrong. (Il'yashenko has shown that if one of the monodromy operators is diagonalizable, then Plemelj's claim is true.)\nIndeed found a counterexample to Plemelj's statement. \nThis is commonly viewed as providing a counterexample to the precise question Hilbert had in mind;\nBolibrukh showed that for a given pole configuration certain monodromy groups can be realised by regular, but not by Fuchsian systems. (In 1990 he published the thorough study of the case of regular systems of size 3 exhibiting all situations when such counterexamples exists. In 1978 Dekkers had shown that for systems of size 2 Plemelj's claim is true. and independently showed that for any size, an irreducible monodromy group can be realised by a Fuchsian system. The codimension of the variety of monodromy groups of regular systems of size formula_1 with formula_2 poles which cannot be realised by Fuchsian systems equals formula_3 ().) Parallel to this the Grothendieck school of algebraic geometry had become interested in questions of 'integrable connections on algebraic varieties', generalising the theory of linear differential equations on Riemann surfaces. Pierre Deligne proved a precise Riemann–Hilbert correspondence in this general context (a major point being to say what 'Fuchsian' means). With work by Helmut Röhrl, the case in one complex dimension was again covered.\n\n\n"}
{"id": "51262716", "url": "https://en.wikipedia.org/wiki?curid=51262716", "title": "Individual participant data", "text": "Individual participant data\n\nIndividual participant data (also known as individual patient data, often abbreviated IPD) is raw data from individual participants, and is often used in the context of meta-analysis.\n\nThe International Committee of Medical Journal Editors (ICMJE) has stated that sharing of deidentified individual participant data is an ethical obligation.\n\nIn an IPD meta-analysis, patient-level data from multiple studies or settings are combined to address a certain research question. IPD meta-analyses tend to be common for large-scale and international projects, and they are less limited than AD meta-analyses in terms of the availability and quality of data they can use. Due to the high level of precision and consistency this approach allows for (which in turn makes it easier for researchers to minimize heterogeneity), it is considered the gold standard of evidence synthesis. \n\nCommon aims for a IPD meta-analysis are\nOver the past few decades, meta-analyses conducted with IPD (also known as IPD meta-analyses) have become increasingly popular. \n\n"}
{"id": "17143362", "url": "https://en.wikipedia.org/wiki?curid=17143362", "title": "Invariant factorization of LPDOs", "text": "Invariant factorization of LPDOs\n\nThe factorization of a linear partial differential operator (LPDO) is an important issue in the theory of integrability, due to the Laplace-Darboux transformations, which allow construction of integrable LPDEs. Laplace solved the factorization problem for a bivariate hyperbolic operator of the second order (see Hyperbolic partial differential equation), constructing two Laplace invariants. Each Laplace invariant is an explicit polynomial condition of factorization; coefficients of this polynomial are explicit functions of the coefficients of the initial LPDO. The polynomial conditions of factorization are called invariants because they have the same form for equivalent (i.e. self-adjoint) operators.\n\nBeals-Kartashova-factorization (also called BK-factorization) is a constructive procedure to factorize a bivariate operator of the arbitrary order and arbitrary form. Correspondingly, the factorization conditions in this case also have polynomial form, are invariants and coincide with Laplace invariants for bivariate hyperbolic operators of the second order. The factorization procedure is purely algebraic, the number of possible factorizations depending on the number of simple roots of the Characteristic polynomial (also called symbol) of the initial LPDO and reduced LPDOs appearing at each factorization step. Below the factorization procedure is described for a bivariate operator of arbitrary form, of order 2 and 3. Explicit factorization formulas for an operator of the order formula_1 can be found in General invariants are defined in and invariant formulation of the Beals-Kartashova factorization is given in\n\nConsider an operator\n\nwith smooth coefficients and look for a factorization\n\nLet us write down the equations on formula_4 explicitly, keeping in\nmind the rule of left composition, i.e. that \n\nThen in all cases\n\nwhere the notation formula_12 is used.\n\nWithout loss of generality, formula_13 i.e. formula_14 and it can be taken as 1, formula_15 Now solution of the system of 6 equations on the variables \ncan be found in three steps. \n\nAt the first step, the roots of a quadratic polynomial have to be found. \n\nAt the second step, a linear system of two algebraic equations has to be solved.\n\nAt the third step, one algebraic condition has to be checked.\n\nStep 1.\nVariables \ncan be found from the first three equations, \n\nThe (possible) solutions are then the functions of the roots of a quadratic polynomial:\n\nLet formula_26 be a root of the polynomial formula_27\nthen\n\nStep 2.\nSubstitution of the results obtained at the first step, into the next two equations\n\nyields linear system of two algebraic equations:\n\nIn particularly, if the root formula_36 is simple,\ni.e.\n\nequations have the unique solution:\n\nAt this step, for each \nroot of the polynomial formula_40 a corresponding set of coefficients formula_41 is computed. \n\nStep 3.\nCheck factorization condition (which is the last of the initial 6 equations)\n\nwritten in the known variables formula_41 and formula_26):\n\nIf\n\nthe operator formula_47 is factorizable and explicit form for the factorization coefficients formula_48 is given above.\n\nConsider an operator\n\nwith smooth coefficients and look for a factorization\n\nSimilar to the case of the operator formula_51 the conditions of factorization are described by the following system:\n\nwith formula_62 and again formula_63 i.e. formula_28 and three-step procedure yields:\n\nAt the first step, the roots of a cubic polynomial \n\nhave to be found. Again formula_66 denotes a root and first four coefficients are \n\nAt the second step, a linear system of three algebraic equations has to be solved:\n\nAt the third step, two algebraic conditions have to be checked.\n\nDefinition The operators formula_76, formula_77 are called\nequivalent if there is a gauge transformation that takes one to the\nother:\nBK-factorization is then pure algebraic procedure which allows to\nconstruct explicitly a factorization of an arbitrary order LPDO formula_77\nin the form\nwith first-order operator formula_81 where formula_82 is an arbitrary simple root of the characteristic polynomial \nFactorization is possible then for each simple root formula_84 iff\n\nfor formula_85\n\nfor formula_86\n\nfor formula_87\n\nand so on. All functions formula_88 are known functions, for instance, \n\nand so on.\n\nTheorem All functions \nare invariants under gauge transformations.\n\nDefinition Invariants formula_93 are\ncalled generalized invariants of a bivariate operator of arbitrary\norder.\n\nIn particular case of the bivariate hyperbolic operator its generalized\ninvariants coincide with Laplace invariants (see Laplace invariant).\n\nCorollary If an operator formula_77 is factorizable, then all\noperators equivalent to it, are also factorizable.\n\nEquivalent operators are easy to compute:\n\nand so on. Some example are given below:\n\nFactorization of an operator is the first step on the way of solving corresponding equation. But for solution we need right factors and BK-factorization constructs left factors which are easy to construct. On the other hand, the existence of a certain right factor of a LPDO is equivalent to the existence of a corresponding left factor of the transpose of that operator.\n\nDefinition\nThe transpose formula_101 of an operator\nformula_102\nis defined as\nformula_103\nand the identity\nformula_104\nimplies that\nformula_105\n\nNow the coefficients are\n\nformula_106\nformula_107\n\nwith a standard convention for binomial coefficients in several\nvariables (see Binomial coefficient), e.g. in two variables\nIn particular, for the operator formula_109 the coefficients are\nformula_110\nFor instance, the operator \nis factorizable as\nand its transpose formula_114 is factorizable then as\nformula_115\n\n\n"}
{"id": "18849148", "url": "https://en.wikipedia.org/wiki?curid=18849148", "title": "Inversive distance", "text": "Inversive distance\n\nIn inversive geometry, the inversive distance is a way of measuring the \"distance\" between two circles, regardless of whether the circles cross each other, are tangent to each other, or are disjoint from each other.\n\nThe inversive distance remains unchanged if the circles are inverted, or transformed by a Möbius transformation. One pair of circles can be transformed to another pair by a Möbius transformation if and only if both pairs have the same inversive distance.\n\nAn analogue of the Beckman–Quarles theorem holds true for the inversive distance: if a bijection of the set of circles in the inversive plane preserves the inversive distance between pairs of circles at some chosen fixed distance formula_1, then it must be a Möbius transformation that preserves all inversive distances.\n\nFor two circles in the Euclidean plane with radii formula_2 and formula_3, and distance formula_4 between their centers, the inversive distance can be defined\nby the formula\nThis formula gives:\n\nSome authors modify this formula by taking the inverse hyperbolic cosine of the value given above, rather than the value itself. That is, rather than using the number formula_6 as the inversive distance, the distance is instead defined as the number formula_1 obeying the equation\nAlthough transforming the inversive distance in this way makes the distance formula more complicated, and prevents its application to crossing pairs of circles, it has the advantage that (like the usual distance for points on a line) the distance becomes additive for circles in a pencil of circles. That is, if three circles belong to a common pencil, then (using formula_1 in place of formula_6 as the inversive distance) one of their three pairwise distances will be the sum of the other two.\n\nIt is also possible to define the inversive distance for circles on a sphere, or for circles in the hyperbolic plane.\n\nA Steiner chain for two disjoint circles is a finite cyclic sequence of additional circles, each of which is tangent to the two given circles and to its two neighbors in the chain.\nSteiner's porism states that if two circles have a Steiner chain, they have infinitely many such chains.\nThe chain is allowed to wrap more than once around the two circles, and can be characterized by a rational number formula_11 whose numerator is the number of circles in the chain and whose denominator is the number of times it wraps around. All chains for the same two circles have the same value of formula_11. If the inversive distance between the two circles (after taking the inverse hyperbolic cosine) is formula_1, then formula_11 can be found by the formula\nConversely, every two disjoint circles for which this formula gives a rational number will support a Steiner chain. More generally, an arbitrary pair of disjoint circles can be approximated arbitrarily closely by pairs of circles that support Steiner chains whose formula_11 values are rational approximations to the value of this formula for the given two circles.\n\nThe inversive distance has been used to define the concept of an inversive-distance circle packing: a collection of circles such that a specified subset of pairs of circles (corresponding to the edges of a planar graph ) have a given inversive distance with respect to each other. This concept generalizes the circle packings described by the circle packing theorem, in which specified pairs of circles are tangent to each other. Although less is known about the existence of inversive distance circle packings than for tangent circle packings, it is known that, when they exist, they can be uniquely specified (up to Möbius transformations) by a given maximal planar graph and set of Euclidean or hyperbolic inversive distances. This rigidity property can be generalized broadly, to Euclidean or hyperbolic metrics on triangulated manifolds with angular defects at their vertices. However, for manifolds with spherical geometry, these packings are no longer unique. In turn, inversive-distance circle packings have been used to construct approximations to conformal mappings.\n"}
{"id": "53794712", "url": "https://en.wikipedia.org/wiki?curid=53794712", "title": "Ira Gessel", "text": "Ira Gessel\n\nIra Martin Gessel (born 9 April 1951 in Philadelphia, Pennsylvania) is an American mathematician, known for his work in combinatorics. He is a long-time faculty at Brandeis University and resides in Arlington, Massachusetts.\n\nGessel studied at Harvard University graduating \"magna cum laude\" in 1973. There, he became a Putnam Fellow in 1972, alongside Arthur Rubin and David Vogan.\n\nHe received his Ph.D. at MIT in was the first student of Richard P. Stanley. He was then a postdoctoral fellow at the IBM Watson Research Center and MIT. He then joined Brandeis University faculty in 1984. He was promoted to Professor of Mathematics and Computer Science in 1990, became a chair in 1996–98, and Professor Emeritus in 2015.\n\nGessel is a prolific contributor to enumerative and algebraic combinatorics. He is credited with the invention of quasisymmetric functions in 1984 and foundational work on the Lagrange inversion theorem. As of 2017, Gessel was an advisor of 27 Ph.D. students.\n\nGessel was elected a Fellow of the American Mathematical Society in the inaugural class of 2012. Since 2015, he is an Associate Editor of the \"Digital Library of Mathematical Functions\".\n\nIn 1970, while a senior in High School, Ira Gessel and his brother Michael Gessel started a grass-roots political organization to end pay toilets in America. The movement was largely successful and was disbanded in 1976.\n\n\n"}
{"id": "47286788", "url": "https://en.wikipedia.org/wiki?curid=47286788", "title": "Kerry Mitchell", "text": "Kerry Mitchell\n\nKerry Mitchell (born 1961) is an American artist known for his algorithmic and fractal art, which has been exhibited at the Nature in Art Museum, The Bridges Conference, and the Los Angeles Center for Digital Art, and for his \"Fractal Art Manifesto\".\n\nMitchell was born in Iowa, United States, in 1961. His parents were LeRoy and Shirley Mitchell. His father was an art teacher and mother was a stay-at-home mother until Mitchell started seventh grade. Mitchell was a Presidential Scholar in 1979 and went on to pursue engineering at and graduated from Purdue University in aerospace engineering, did a master's degree at Stanford University, and then a PhD work at Purdue. He worked at NASA doing aerospace research. He then worked as a scientist at Arizona Science Center. He served as a mathematics and science professor at the University of Advancing Technology in Tempe, Arizona. As of 2015, he works as a manager at Maricopa County Community College District in Tempe, Arizona.\n\nAlongside his technical career, Mitchell works on algorithmic art. He ascribes his artistic awakening to a 1985 article in \"Scientific American\" on the Mandelbrot set, explaining:\n\nIn 1999, Mitchell published his \"Fractal Art Manifesto\". The artist Janet Parke notes that in the manifesto, Mitchell suggests that fractal art cannot be made by a computer alone, and that not everyone who has a computer can necessarily make good fractal art. Instead, she explains, Mitchell is arguing that the artist's creative process is needed to inject elements such as the considered selection of colours and gradients, the merging of multiple layers, and decisions on composition such as by zooming in to a fractal.\n\nMitchell also prepared tutorials on how to create fractal art with tools including Ultra Fractal. In 2011 he served on the panel of the \"Fractal Art Contest\".\n\n\n\n\n"}
{"id": "559622", "url": "https://en.wikipedia.org/wiki?curid=559622", "title": "Level set", "text": "Level set\n\nIn mathematics, a level set of a real-valued function \"f\" of \"n\" real variables is a set of the form\n\nthat is, a set where the function takes on a given constant value \"c\".\n\nWhen the number of variables is two, a level set is generically a curve, called a level curve, contour line, or isoline. So a level curve is the set of all real-valued solutions of an equation in two variables \"x\" and \"x\". When \"n\" = 3, a level set is called a level surface (see also isosurface), and for higher values of \"n\" the level set is a level hypersurface. So a \"level surface\" is the set of all real-valued roots of an equation in three variables \"x\", \"x\" and \"x\", and a level hypersurface is the set of all real-valued roots of an equation in \"n\" (\"n\" > 3) variables.\n\nA level set is a special case of a fiber.\n\nLevel sets show up in many applications, often under different names. \n\nFor example, an implicit curve is a level curve, which is considered independently of its neighbor curves, emphasizing that such a curve is defined by an implicit equation. Analogously, a level surface is sometimes called an implicit surface or an isosurface.\n\nThe name isocontour is also used, which means a contour of equal height. In various application areas, isocontours have received specific names, which indicate often the nature of the values of the considered function, such as isobar, isotherm, isogon, isochrone, isoquant and indifference curve.\n\nConsider the 2-dimensional Euclidean distance: formula_2 A level set formula_3 of this function consists of those points that lie at a distance of formula_4 from the origin, otherwise known as a circle. For example, formula_5, because formula_6. Geometrically, this means that the point formula_7 lies on the circle of radius 5 centered at the origin. More generally, a sphere in a metric space formula_8 with radius formula_4 centered at formula_10 can be defined as the level set formula_11.\n\nA second example is the plot of Himmelblau's function shown in the figure to the right. Each curve shown is a level curve of the function, and they are spaced logarithmically: if a curve represents formula_12, the curve directly \"within\" represents formula_13, and the curve directly \"outside\" represents formula_14.\n\nTo understand what this means, imagine that two hikers are at the same location on a mountain. One of them is bold, and he decides to go in the direction where the slope is steepest. The other one is more cautious; he does not want to either climb or descend, choosing a path which will keep him at the same height. In our analogy, the above theorem says that the two hikers will depart in directions perpendicular to each other.\n\nA consequence of this theorem (and its proof) is that if is differentiable, a level set is a hypersurface and a manifold outside the critical points of . At a critical point, a level set may be reduced to a point (for example at a local extremum of ) or may have a \nsingularity such as a self-intersection point or a cusp.\n\nA set of the form\n\nis called a sublevel set of \"f\" (or, alternatively, a lower level set or trench of \"f\"). \n\nis called a superlevel set of \"f\". Sublevel sets are important in minimization theory. The boundness of some non-empty sublevel set and the lower-semicontinuity of the function implies that a function attains its minimum, by Weierstrass's theorem. The convexity of all the sublevel sets characterizes quasiconvex functions.\n\n"}
{"id": "8410162", "url": "https://en.wikipedia.org/wiki?curid=8410162", "title": "Lexis ratio", "text": "Lexis ratio\n\nThe Lexis ratio is used in statistics as a measure which seeks to evaluate differences between the statistical properties of random mechanisms where the outcome is two-valued — for example \"success\" or \"failure\", \"win\" or \"lose\". The idea is that the probability of success might vary between different sets of trials in different situations. \n\nThe measure compares the between-set variance of the sample proportions (evaluated for each set) with what the variance should be if there were no difference between in the true proportions of success across the different sets. Thus the measure is used to evaluate how data compares to a fixed-probability-of-success Bernoulli distribution. The term \"Lexis ratio\" is sometimes referred to as \"L\" or \"Q\", where\n\nWhere formula_2 is the (weighted) sample variance derived from the observed proportions of success in sets in \"Lexis trials\" and formula_3 is the variance computed from the expected Bernoulli distribution on the basis of the overall average proportion of success. Trials where \"L\" falls significantly above or below 1 are known as \"supernormal\" and \"subnormal,\" respectively.\n\n"}
{"id": "40922136", "url": "https://en.wikipedia.org/wiki?curid=40922136", "title": "List of Intelligent Systems for Molecular Biology keynote speakers", "text": "List of Intelligent Systems for Molecular Biology keynote speakers\n\nThe following is a list of Intelligent Systems for Molecular Biology (ISMB) keynote speakers.\n\nISMB is an academic conference on the subjects of bioinformatics and computational biology organised by the International Society for Computational Biology (ISCB). The conference has been held annually since 1993 and keynote talks have been presented since 1994. Keynotes are chosen to reflect outstanding research in bioinformatics. The recipients of the ISCB Overton Prize and ISCB Accomplishment by a Senior Scientist Award are invited to give keynote talks as part of the programme.\n\nKeynote speakers include eight Nobel laureates: Richard J. Roberts (1994, 2006), John Sulston (1995), Manfred Eigen (1999), Gerald Edelman (2000), Sydney Brenner (2003), Kurt Wüthrich (2006), Robert Huber (2006) and Michael Levitt (2015). \n"}
{"id": "15602155", "url": "https://en.wikipedia.org/wiki?curid=15602155", "title": "Log-spectral distance", "text": "Log-spectral distance\n\nThe log-spectral distance (LSD), also referred to as log-spectral distortion, is a distance measure (expressed in dB) between two spectra. The log-spectral distance between spectra formula_1 and formula_2 is defined as:\n\nwhere formula_1 and formula_2 are power spectra.\nUnlike the Itakura–Saito distance, the log-spectral distance is symmetric.\n\nIn speech coding log spectral distortion for a given frame is defined as the root mean square difference between the original LPC log power spectrum and the quantize or interpolated LPC log power spectrum. Usually the average of spectral distortion over a large number of frames is calculated and that is used as the measure of performance of quantization or interpolation.\n\n"}
{"id": "2703676", "url": "https://en.wikipedia.org/wiki?curid=2703676", "title": "Loop integral", "text": "Loop integral\n\nIn quantum field theory and statistical mechanics, loop integrals are the integrals which appear when evaluating the Feynman diagrams with one or more loops by integrating over the internal momenta.\n\nThere are two standard techniques for evaluating loop integrals:\n\n"}
{"id": "20412", "url": "https://en.wikipedia.org/wiki?curid=20412", "title": "MATLAB", "text": "MATLAB\n\nMATLAB (\"matrix laboratory\") is a multi-paradigm numerical computing environment and proprietary programming language developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages, including C, C++, C#, Java, Fortran and Python.\n\nAlthough MATLAB is intended primarily for numerical computing, an optional toolbox uses the MuPAD symbolic engine, allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.\n\nAs of 2018, MATLAB has more than 3 million users worldwide. MATLAB users come from various backgrounds of engineering, science, and economics.\n\nCleve Moler, the chairman of the computer science department at the University of New Mexico, started developing MATLAB in the late 1970s. He designed it to give his students access to LINPACK and EISPACK without them having to learn Fortran. It soon spread to other universities and found a strong audience within the applied mathematics community. Jack Little, an engineer, was exposed to it during a visit Moler made to Stanford University in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in C and founded MathWorks in 1984 to continue its development. These rewritten libraries were known as JACKPAC. In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, LAPACK.\n\nMATLAB was first adopted by researchers and practitioners in control engineering, Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of linear algebra, numerical analysis, and is popular amongst scientists involved in image processing.\n\nThe MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical shell or executing text files containing MATLAB code.\n\nVariables are defined using the assignment operator, codice_1. MATLAB is a weakly typed programming language because types are implicitly converted. It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects, and that their type can change. Values can come from constants, from computation involving values of other variables, or from the output of a function. For example:\nA simple array is defined using the colon syntax: \"initial\"codice_2\"increment\"codice_2\"terminator\". For instance:\n\ndefines a variable named codice_4 (or assigns a new value to an existing variable with the name codice_4) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the \"initial\" value), increments with each step from the previous value by 2 (the \"increment\" value), and stops once it reaches (or to avoid exceeding) 9 (the \"terminator\" value).\n\nthe \"increment\" value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.\n\nassigns to the variable named codice_6 an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.\n\nIndexing is one-based, which is the usual convention for matrices in mathematics, although not for some programming languages such as C, C++, and Java.\n\nMatrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).\n\nSets of indices can be specified by expressions such as \"2:4\", which evaluates to [2, 3, 4]. For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:\n\nA square identity matrix of size \"n\" can be generated using the function \"eye\", and matrices of any size with zeros or ones can be generated with the functions \"zeros\" and \"ones\", respectively.\nTransposing a vector or a matrix is done either by the function \"transpose\" or by adding prime after a dot to the matrix. Without the dot MATLAB will perform conjugate transpose.\nMost MATLAB functions can accept matrices and will apply themselves to each element. For example, codice_7 will multiply every element in \"J\" by 2, and then reduce each element modulo \"n\". MATLAB does include standard \"for\" and \"while\" loops, but (as in other similar applications such as R), using the vectorized notation often produces code that is faster to execute. This code, excerpted from the function \"magic.m\", creates a magic square \"M\" for odd values of \"n\" (MATLAB function codice_8 is used here to generate square matrices I and J containing 1:n).\n\nMATLAB has structure data types. Since all variables in MATLAB are arrays, a more adequate name is \"structure array\", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.\n\nWhen creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores. Functions are often case sensitive.\n\nMATLAB supports elements of lambda calculus by introducing function handles, or function references, which are implemented either in .m files or anonymous/nested functions.\n\nMATLAB supports object-oriented programming including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics. However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has \"handle\" as a super-class (for reference classes) or not (for value classes).\n\nMethod call behavior is different between value and reference classes. For example, a call to a method\n\ncan alter any member of \"object\" only if \"object\" is an instance of a reference class.\n\nAn example of a simple class is provided below.\n\nWhen put into a file named hello.m, this can be executed with the following commands:\nMATLAB supports developing applications with graphical user interface (GUI) features. MATLAB includes GUIDE (GUI development environment) for graphically designing GUIs. It also has tightly integrated graph-plotting features. For example, the function \"plot\" can be used to produce a graph from two vectors \"x\" and \"y\". The code:\n\nproduces the following figure of the sine function:\n\nA MATLAB program can produce three-dimensional graphics using the functions \"surf\", \"plot3\" or \"mesh\".\nIn MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.\n\nMATLAB can call functions and subroutines written in the programming languages C or Fortran. A wrapper function is created allowing MATLAB data types to be passed and returned. MEX files (MATLAB executables) are the dynamically loadable object files created by compiling such functions. Since 2014 increasing two-way interfacing with Python was being added.\n\nLibraries written in Perl, Java, ActiveX or .NET can be directly called from MATLAB, and many MATLAB libraries (for example XML or SQL support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox which is sold separately by MathWorks, or using an undocumented mechanism called JMI (Java-to-MATLAB Interface), (which should not be confused with the unrelated Java Metadata Interface that is also called JMI). Official MATLAB API for Java was added in 2016.\n\nAs alternatives to the MuPAD based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to Maple or Mathematica.\n\nLibraries also exist to import and export MathML.\n\nMATLAB is a proprietary product of MathWorks, so users are subject to vendor lock-in. Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with .NET or Java application building environment, future development will still be tied to the MATLAB language.\n\nEach toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.\n\nIt has been reported that European Union (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor. The regulators dropped the investigation after the complainant withdrew its accusation and no evidence of wrongdoing was found.\n\nMATLAB has a number of competitors. Commercial competitors include Mathematica, TK Solver, Maple, and IDL. There are also free open source alternatives to MATLAB, in particular GNU Octave, Scilab, FreeMat, and SageMath, which are intended to be mostly compatible with the MATLAB language; the Julia programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are APL, Fortran 90 and higher, S-Lang, as well as the statistical languages R and S. There are also libraries to add similar functionality to existing languages, such as IT++ for C++, Perl Data Language for Perl, ILNumerics for .NET, NumPy/SciPy/matplotlib for Python, SciLua/Torch for Lua, SciRuby for Ruby, and Numeric.js for JavaScript.\n\nGNU Octave is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see MATLAB Compatibility of GNU Octave), therefore, making GNU Octave a superset of the MATLAB language.\n\nThe number (or release number) is the version reported by Concurrent License Manager program FLEXlm.\n\nFor a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.\n\n\n\n\n\n\nSeveral easter eggs exist in MATLAB. These include hidden pictures, and jokes. For example, typing in \"spy\" used to generate a picture of the spies from Spy vs Spy, but now displays an image of a dog. Typing in \"why\" randomly outputs a philosophical answer. Other commands include \"penny\", \"toilet\", \"image\", and \"life\". Not every Easter egg appears in every version of MATLAB.\n\n\n"}
{"id": "4607950", "url": "https://en.wikipedia.org/wiki?curid=4607950", "title": "Magic formula investing", "text": "Magic formula investing\n\nMagic formula investing is an investment technique outlined by Joel Greenblatt that uses the principles of value investing. \n\nGreenblatt suggests purchasing 30 \"good companies\": cheap stocks with a high earnings yield and a high return on capital. He touts the success of his magic formula in his book 'The Little Book that Beats the Market' (), claiming that it does in fact beat the S&P 500 96% of the time, and has averaged a 17-year annual return of 30.8%\n\n\n\n"}
{"id": "408111", "url": "https://en.wikipedia.org/wiki?curid=408111", "title": "Mahler's theorem", "text": "Mahler's theorem\n\nIn mathematics, Mahler's theorem, introduced by , expresses continuous \"p\"-adic functions in terms of polynomials. Over any field, one has the following result:\n\nLet formula_1 be the forward difference operator. Then for polynomial functions \"f\" we have the Newton series\n\nwhere\n\nis the \"k\"th binomial coefficient polynomial.\n\nOver the field of real numbers, the assumption that the function \"f\" is a polynomial can be weakened, but it cannot be weakened all the way down to mere continuity. Mahler's theorem states that if \"f\" is a continuous p-adic-valued function on the \"p\"-adic integers then the same identity holds. The relationship between the operator Δ and this polynomial sequence is much like that between differentiation and the sequence whose \"k\"th term is \"x\".\n\nIt is remarkable that as weak an assumption as continuity is enough; by contrast, Newton series on the field of complex numbers are far more tightly constrained, and require Carlson's theorem to hold. It is a fact of algebra that if \"f\" is a polynomial function with coefficients in any field of characteristic 0, the same identity holds where the sum has finitely many terms.\n"}
{"id": "222665", "url": "https://en.wikipedia.org/wiki?curid=222665", "title": "McCarthy 91 function", "text": "McCarthy 91 function\n\nThe McCarthy 91 function is a recursive function, defined by the computer scientist John McCarthy as a test case for formal verification within computer science.\n\nThe McCarthy 91 function is defined as\n\nThe results of evaluating the function are given by \"M\"(\"n\") = 91 for all integer arguments \"n\" ≤ 100, and \"M\"(\"n\") = \"n\" − 10 for \"n\" > 100. Indeed, the result of M(101) is also 91 (101 - 10 = 91). All results of M(n) after n = 101 are continually increasing by 1, e.g. M(102) = 92, M(103) = 93.\n\nThe 91 function was introduced in papers published by Zohar Manna, Amir Pnueli and John McCarthy in 1970. These papers represented early developments towards the application of formal methods to program verification. The 91 function was chosen for being nested-recursive (contrasted with single recursion, such as defining formula_2 by means of formula_3). The example was popularized by Manna's book, \"Mathematical Theory of Computation\" (1974). As the field of Formal Methods advanced, this example appeared repeatedly in the research literature.\nIn particular, it is viewed as a \"challenge problem\" for automated program verification.\n\nIt is easier to reason about tail-recursive control flow, this is an equivalent (extensionally equal) definition:\nAs one of the examples used to demonstrate such reasoning, Manna's book includes a tail-recursive algorithm equivalent to the nested-recursive 91 function. Many of the papers that report an \"automated verification\" (or termination proof) of the 91 function only handle the tail-recursive version.\n\nThis is an equivalent mutually tail-recursive definition:\nA formal derivation of the mutually tail-recursive version from the nested-recursive one was given in a 1980 article by Mitchell Wand, based on the use of continuations.\n\nExample A:\n\nExample B:\n\nHere is an implementation of the nested-recursive algorithm in Lisp:\n\nHere is an implementation of the nested-recursive algorithm in Haskell:\n\nHere is an implementation of the nested-recursive algorithm in OCaml:\n\nHere is an implementation of the tail-recursive algorithm in OCaml:\n\nHere is an implementation of the nested-recursive algorithm in Python:\n\nHere is an implementation of the nested-recursive algorithm in C:\n\nHere is an implementation of the tail-recursive algorithm in C:\n\nHere is a proof that the function is equivalent to non-recursive:\n\nFor 90 ≤ \"n\" < 101,\n\nSo \"M\"(\"n\") = 91 for 90 ≤ \"n\" < 101.\n\nWe can use this as a base case for induction on blocks of 11 numbers, like so:\n\nAssume that \"M\"(\"n\") = 91 for \"a\" ≤ \"n\" < \"a\" + 11.\n\nThen, for any \"n\" such that \"a\" - 11 ≤ \"n\" < \"a\",\n\nNow by induction \"M\"(\"n\") = 91 for any \"n\" in such a block. There are no holes between the blocks, so \"M\"(\"n\") = 91 for \"n\" < 101. We can also add \"n\" = 101 as a special case.\n\nDonald Knuth generalized the 91 function to include additional parameters. John Cowles developed a formal proof that Knuth's generalized function was total, using the ACL2 theorem prover.\n\n"}
{"id": "6226587", "url": "https://en.wikipedia.org/wiki?curid=6226587", "title": "Necklace (combinatorics)", "text": "Necklace (combinatorics)\n\nIn combinatorics, a \"k\"-ary necklace of length \"n\" is an equivalence class of \"n\"-character strings over an alphabet of size \"k\", taking all rotations as equivalent. It represents a structure with \"n\" circularly connected beads which have \"k\" available colors. \n\nA \"k\"-ary bracelet, also referred to as a turnover (or free) necklace, is a necklace such that strings may also be equivalent under reflection. That is, given two strings, if each is the reverse of the other then they belong to the same equivalence class. For this reason, a necklace might also be called a fixed necklace to distinguish it from a turnover necklace.\n\nFormally, one may represent a necklace as an orbit of the cyclic group acting on \"n\"-character strings, and a bracelet as an orbit of the dihedral group. One can count these orbits, and thus necklaces and bracelets, using Pólya's enumeration theorem.\n\nThere are\n\ndifferent \"k\"-ary necklaces of length \"n\", where \"formula_2\" is Euler's totient function. This follows directly from Pólya's enumeration theorem applied to the action of the cyclic group formula_3 acting on the set of all functions formula_4.\n\nThere are\ndifferent \"k\"-ary bracelets of length \"n\", where \"N\"(\"n\") is the number of \"k\"-ary necklaces of length \"n\". This follows from Pólya's method applied to the action of the dihedral group formula_6.\n\nIf there are \"n\" beads, all distinct, on a necklace joined at the ends, then the number of distinct orderings on the necklace, after allowing for rotations, is , for \"n\" > 0. This may also be expressed as (\"n\" − 1)!. This number is less than the general case, which lacks the requirement that each bead must be distinct.\n\nAn intuitive justification for this can be given. If there is a line of \"n\" distinct objects (\"beads\"), the number of combinations would be \"n\"!. If the ends are joined together, the number of combinations are divided by \"n\", as it is possible to rotate the string of \"n\" beads into \"n\" positions.\n\nIf there are \"n\" beads, all distinct, on a bracelet joined at the ends, then the number of distinct orderings on the bracelet, after allowing for rotations and reflection, is , for \"n\" > 2. Note that this number is less than the general case of \"B\"(\"n\"), which lacks the requirement that each bead must be distinct.\n\nTo explain this, one may begin with the count for a necklace. This number can be further divided by 2, because it is also possible to flip the bracelet over.\n\nAn aperiodic necklace of length \"n\" is a rotation equivalence class having size \"n\", i.e., no two distinct rotations of a necklace from such class are equal. \n\nAccording to Moreau's necklace-counting function, there are\n\ndifferent \"k\"-ary aperiodic necklaces of length \"n\", where \"μ\" is the Möbius function. The two necklace-counting functions are related by: formula_8 where the sum is over all divisors of \"n\", which is equivlent by Möbius inversion to formula_9 \n\nEach aperiodic necklace contains a single Lyndon word so that Lyndon words form representatives of aperiodic necklaces.\n\n\n"}
{"id": "9421111", "url": "https://en.wikipedia.org/wiki?curid=9421111", "title": "Order-4 pentagonal tiling", "text": "Order-4 pentagonal tiling\n\nIn geometry, the order-4 pentagonal tiling is a regular tiling of the hyperbolic plane. It has Schläfli symbol of {5,4}. It can also be called a pentapentagonal tiling in a bicolored quasiregular form.\n\nThis tiling represents a hyperbolic kaleidoscope of 5 mirrors meeting as edges of a regular pentagon. This symmetry by orbifold notation is called *22222 with 5 order-2 mirror intersections. In Coxeter notation can be represented as [5,4], removing two of three mirrors (passing through the pentagon center) in the [5,4] symmetry.\n\nThe kaleidoscopic domains can be seen as bicolored pentagons, representing mirror images of the fundamental domain. This coloring represents the uniform tiling t{5,5} and as a quasiregular tiling is called a \"pentapentagonal tiling\".\n\nThis tiling is topologically related as a part of sequence of regular polyhedra and tilings with pentagonal faces, starting with the dodecahedron, with Schläfli symbol {5,n}, and Coxeter diagram , progressing to infinity.\nThis tiling is also topologically related as a part of sequence of regular polyhedra and tilings with four faces per vertex, starting with the octahedron, with Schläfli symbol {n,4}, and Coxeter diagram , with n progressing to infinity.\nThis tiling is topologically related as a part of sequence of regular polyhedra and tilings with vertex figure (4).\n\n\n\n"}
{"id": "23572", "url": "https://en.wikipedia.org/wiki?curid=23572", "title": "Partially ordered set", "text": "Partially ordered set\n\nIn mathematics, especially order theory, a partially ordered set (also poset) formalizes and generalizes the intuitive concept of an ordering, sequencing, or arrangement of the elements of a set. A poset consists of a set together with a binary relation indicating that, for certain pairs of elements in the set, one of the elements precedes the other in the ordering. The word \"partial\" in the names \"partial order\" or \"partially ordered set\" is used as an indication that not every pair of elements needs to be comparable. That is, there may be pairs of elements for which neither element precedes the other in the poset. Partial orders thus generalize total orders, in which every pair is comparable. \n\nTo be a partial order, a binary relation must be reflexive (each element is comparable to itself), antisymmetric (no two different elements precede each other), and transitive (the start of a chain of precedence relations must precede the end of the chain).\n\nOne familiar example of a partially ordered set is a collection of people ordered by genealogical descendancy. Some pairs of people bear the descendant-ancestor relationship, but other pairs of people are incomparable, with neither being a descendent of the other. \n\nA poset can be visualized through its Hasse diagram, which depicts the ordering relation.\n\nA (non-strict) partial order is a binary relation ≤ over a set \"P\" satisfying particular axioms which are discussed below. When \"a\" ≤ \"b\", we say that \"a\" is related to \"b\". (This does not imply that \"b\" is also related to \"a\", because the relation need not be symmetric.) \n\nThe axioms for a non-strict partial order state that the relation ≤ is reflexive, antisymmetric, and transitive. That is, for all \"a\", \"b\", and \"c\" in \"P\", it must satisfy:\n\n\nIn other words, a partial order is an antisymmetric preorder.\n\nA set with a partial order is called a partially ordered set (also called a poset). The term \"ordered set\" is sometimes also used, as long as it is clear from the context that no other kind of order is meant. In particular, totally ordered sets can also be referred to as \"ordered sets\", especially in areas where these structures are more common than posets.\n\nFor \"a, b\", elements of a partially ordered set \"P\", if \"a\" ≤ \"b\" or \"b\" ≤ \"a\", then \"a\" and \"b\" are comparable. Otherwise they are incomparable. In the figure on top-right, e.g. {x} and {x,y,z} are comparable, while {x} and {y} are not. A partial order under which every pair of elements is comparable is called a total order or linear order; a totally ordered set is also called a chain (e.g., the natural numbers with their standard order). A subset of a poset in which no two distinct elements are comparable is called an antichain (e.g. the set of singletons in the top-right figure). An element \"a\" is said to be covered by another element \"b\", written \"a\"<:\"b\", if \"a\" is strictly less than \"b\" and no third element \"c\" fits between them; formally: if both \"a\"≤\"b\" and \"a\"≠\"b\" are true, and \"a\"≤\"c\"≤\"b\" is false for each \"c\" with \"a\"≠\"c\"≠\"b\". A more concise definition will be given below using the strict order corresponding to \"≤\". For example, {x} is covered by {x,z} in the top-right figure, but not by {x,y,z}.\n\nStandard examples of posets arising in mathematics include:\n\n\nThere are several notions of \"greatest\" and \"least\" element in a poset \"P\", notably:\n\nFor example, consider the positive integers, ordered by divisibility: 1 is a least element, as it divides all other elements; on the other hand this poset does not have a greatest element (although if one would include 0 in the poset, which is a multiple of any integer, that would be a greatest element; see figure). This partially ordered set does not even have any maximal elements, since any \"g\" divides for instance 2\"g\", which is distinct from it, so \"g\" is not maximal. If the number 1 is excluded, while keeping divisibility as ordering on the elements greater than 1, then the resulting poset does not have a least element, but any prime number is a minimal element for it. In this poset, 60 is an upper bound (though not a least upper bound) of the subset {2,3,5,10}, which does not have any lower bound (since 1 is not in the poset); on the other hand 2 is a lower bound of the subset of powers of 2, which does not have any upper bound.\n\nIn order of increasing strength, i.e., decreasing sets of pairs, three of the possible partial orders on the Cartesian product of two partially ordered sets are (see figures):\n\nAll three can similarly be defined for the Cartesian product of more than two sets.\n\nApplied to ordered vector spaces over the same field, the result is in each case also an ordered vector space.\n\nSee also orders on the Cartesian product of totally ordered sets.\n\nAnother way to combine two posets is the ordinal sum (or linear sum), \"Z\" = \"X\" ⊕ \"Y\", defined on the union of the underlying sets \"X\" and \"Y\" by the order \"a\" ≤ \"b\" if and only if:\n\nIf two posets are well-ordered, then so is their ordinal sum.\nThe ordinal sum operation is one of two operations used to form series-parallel partial orders, and in this context is called series composition. The other operation used to form these orders, the disjoint union of two partially ordered sets (with no order relation between elements of one set and elements of the other set) is called in this context parallel composition.\n\nIn some contexts, the partial order defined above is called a non-strict (or reflexive, or weak) partial order. In these contexts, a strict (or irreflexive) partial order \"<\" is a binary relation that is irreflexive, transitive and asymmetric, i.e. which satisfies for all \"a\", \"b\", and \"c\" in \"P\":\n\n\nStrict and non-strict partial orders are closely related. A non-strict partial order may be converted to a strict partial order by removing all relationships of the form \"a\" ≤ \"a\". Conversely, a strict partial order may be converted to a non-strict partial order by adjoining all relationships of that form. Thus, if \"≤\" is a non-strict partial order, then the corresponding strict partial order \"<\" is the irreflexive kernel given by:\n\nConversely, if \"<\" is a strict partial order, then the corresponding non-strict partial order \"≤\" is the reflexive closure given by:\n\nThis is the reason for using the notation \"≤\".\n\nUsing the strict order \"<\", the relation \"\"a\" is covered by \"b\" can be equivalently rephrased as \"a\"<\"b\", but not \"a\"<\"c\"<\"b\" for any \"c\"\".\nStrict partial orders are useful because they correspond more directly to directed acyclic graphs (dags): every strict partial order is a dag, and the transitive closure of a dag is both a strict partial order and also a dag itself.\n\nThe inverse (or converse) of a partial order relation ≤ is the converse of ≤. Typically denoted ≥, it is the relation that satisfies \"x\" ≥ \"y\" if and only if \"y\" ≤ \"x\". The inverse of a partial order relation is reflexive, transitive, and antisymmetric, and hence itself a partial order relation. The order dual of a partially ordered set is the same set with the partial order relation replaced by its inverse. The irreflexive relation > is to ≥ as < is to ≤.\n\nAny one of the four relations ≤, <, ≥, and > on a given set uniquely determines the other three.\n\nIn general two elements \"x\" and \"y\" of a partial order may stand in any of four mutually exclusive relationships to each other: either \"x\" < \"y\", or \"x\" = \"y\", or \"x\" > \"y\", or \"x\" and \"y\" are \"incomparable\" (none of the other three). A totally ordered set is one that rules out this fourth possibility: all pairs of elements are comparable and we then say that trichotomy holds. The natural numbers, the integers, the rationals, and the reals are all totally ordered by their algebraic (signed) magnitude whereas the complex numbers are not. This is not to say that the complex numbers cannot be totally ordered; we could for example order them lexicographically via \"x\"+iy\" < \"u\"+iv\" if and only if \"x\" < \"u\" or (\"x\" = \"u\" and \"y\" < \"v\"), but this is not ordering by magnitude in any reasonable sense as it makes 1 greater than 100i. Ordering them by absolute magnitude yields a preorder in which all pairs are comparable, but this is not a partial order since 1 and i have the same absolute magnitude but are not equal, violating antisymmetry.\n\nGiven two partially ordered sets (\"S\",≤) and (\"T\",≤), a function \"f\": \"S\" → \"T\" is called order-preserving, or monotone, or isotone, if for all \"x\" and \"y\" in \"S\", \"x\"≤\"y\" implies \"f\"(\"x\") ≤ \"f\"(\"y\").\nIf (\"U\",≤) is also a partially ordered set, and both \"f\": \"S\" → \"T\" and \"g\": \"T\" → \"U\" are order-preserving, their composition (\"g\"∘\"f\"): \"S\" → \"U\" is order-preserving, too.\nA function \"f\": \"S\" → \"T\" is called order-reflecting if for all \"x\" and \"y\" in \"S\", \"f\"(\"x\") ≤ \"f\"(\"y\") implies \"x\"≤\"y\".\nIf \"f\" is both order-preserving and order-reflecting, then it is called an order-embedding of (\"S\",≤) into (\"T\",≤).\nIn the latter case, \"f\" is necessarily injective, since \"f\"(\"x\") = \"f\"(\"y\") implies \"x\" ≤ \"y\" and \"y\" ≤ \"x\". If an order-embedding between two posets \"S\" and \"T\" exists, one says that \"S\" can be embedded into \"T\". If an order-embedding \"f\": \"S\" → \"T\" is bijective, it is called an order isomorphism, and the partial orders (\"S\",≤) and (\"T\",≤) are said to be isomorphic. Isomorphic orders have structurally similar Hasse diagrams (cf. right picture). It can be shown that if order-preserving maps \"f\": \"S\" → \"T\" and \"g\": \"T\" → \"S\" exist such that \"g\"∘\"f\" and \"f\"∘\"g\" yields the identity function on \"S\" and \"T\", respectively, then \"S\" and \"T\" are order-isomorphic.\nFor example, a mapping \"f\": ℕ → ℙ(ℕ) from the set of natural numbers (ordered by divisibility) to the power set of natural numbers (ordered by set inclusion) can be defined by taking each number to the set of its prime divisors. It is order-preserving: if \"x\" divides \"y\", then each prime divisor of \"x\" is also a prime divisor of \"y\". However, it is neither injective (since it maps both 12 and 6 to {2,3}) nor order-reflecting (since besides 12 doesn't divide 6). Taking instead each number to the set of its prime power divisors defines a map \"g\": ℕ → ℙ(ℕ) that is order-preserving, order-reflecting, and hence an order-embedding. It is not an order-isomorphism (since it e.g. doesn't map any number to the set {4}), but it can be made one by restricting its codomain to \"g\"(ℕ). The right picture shows a subset of ℕ and its isomorphic image under \"g\". The construction of such an order-isomorphism into a power set can be generalized to a wide class of partial orders, called distributive lattices, see \"Birkhoff's representation theorem\".\n\nSequence [ A001035] in OEIS gives the number of partial orders on a set of \"n\" labeled elements:\n\nThe number of strict partial orders is the same as that of partial orders.\n\nIf the count is made only up to isomorphism, the sequence 1, 1, 2, 5, 16, 63, 318, … is obtained.\n\nA partial order ≤ on a set \"X\" is an extension of another partial order ≤ on \"X\" provided that for all elements \"x\" and \"y\" of \"X\", whenever \"x\" ≤ \"y\", it is also the case that \"x\" ≤ \"y\". A linear extension is an extension that is also a linear (i.e., total) order. Every partial order can be extended to a total order (order-extension principle).\n\nIn computer science, algorithms for finding linear extensions of partial orders (represented as the reachability orders of directed acyclic graphs) are called topological sorting.\n\nEvery poset (and every preorder) may be considered as a category in which every hom-set has at most one element. More explicitly, let hom(\"x\", \"y\") = {(\"x\", \"y\")} if \"x\" ≤ \"y\" (and otherwise the empty set) and (\"y\", \"z\")∘(\"x\", \"y\") = (\"x\", \"z\"). Such categories are sometimes called \"posetal\".\n\nPosets are equivalent to one another if and only if they are isomorphic. In a poset, the smallest element, if it exists, is an initial object, and the largest element, if it exists, is a terminal object. Also, every preordered set is equivalent to a poset. Finally, every subcategory of a poset is isomorphism-closed.\n\nIf \"P\" is a partially ordered set that has also been given the structure of a topological space, then it is customary to assume that is a closed subset of the topological product space formula_1. Under this assumption partial order relations are well behaved at limits in the sense that if formula_2, formula_3 and \"a\" ≤ \"b\" for all \"i\", then \"a\" ≤ \"b\".\n\nFor \"a\" ≤ \"b\", the closed interval is the set of elements \"x\" satisfying \"a\" ≤ \"x\" ≤ \"b\" (i.e. \"a\" ≤ \"x\" and \"x\" ≤ \"b\"). It contains at least the elements \"a\" and \"b\".\n\nUsing the corresponding strict relation \"<\", the open interval is the set of elements \"x\" satisfying \"a\" < \"x\" < \"b\" (i.e. \"a\" < \"x\" and \"x\" < \"b\"). An open interval may be empty even if \"a\" < \"b\". For example, the open interval on the integers is empty since there are no integers \"i\" such that 1 < \"i\" < 2.\n\nSometimes the definitions are extended to allow \"a\" > \"b\", in which case the interval is empty.\n\nThe \"half-open intervals\" and are defined similarly.\n\nA poset is locally finite if every interval is finite. For example, the integers are locally finite under their natural ordering. The lexicographical order on the cartesian product ℕ×ℕ is not locally finite, since e.g. (1,2)≤(1,3)≤(1,4)≤(1,5)≤...≤(2,1).\nUsing the interval notation, the property \"\"a\" is covered by \"b\"\" can be rephrased equivalently as [\"a\",\"b\"] = {\"a\",\"b\"}.\n\nThis concept of an interval in a partial order should not be confused with the particular class of partial orders known as the interval orders.\n\n\n"}
{"id": "4459886", "url": "https://en.wikipedia.org/wiki?curid=4459886", "title": "Password strength", "text": "Password strength\n\nPassword strength is a measure of the effectiveness of a password against guessing or brute-force attacks. In its usual form, it estimates how many trials an attacker who does not have direct access to the password would need, on average, to guess it correctly. The strength of a password is a function of length, complexity, and unpredictability.\n\nUsing strong passwords lowers overall risk of a security breach, but strong passwords do not replace the need for other effective security controls. The effectiveness of a password of a given strength is strongly determined by the design and implementation of the factors (knowledge, ownership, inherence). The first factor is the main focus in this article.\n\nThe rate at which an attacker can submit guessed passwords to the system is a key factor in determining system security. Some systems impose a time-out of several seconds after a small number (e.g. three) of failed password entry attempts. In the absence of other vulnerabilities, such systems can be effectively secured with relatively simple passwords. However the system must store information about the user passwords in some form and if that information is stolen, say by breaching system security, the user passwords can be at risk.\n\nPasswords are created either automatically (using randomizing equipment) or by a human; the latter case is more common. While the strength of randomly chosen passwords against a brute-force attack can be calculated with precision, determining the strength of human-generated passwords is challenging.\n\nTypically, humans are asked to choose a password, sometimes guided by suggestions or restricted by a set of rules, when creating a new account for a computer system or Internet Web site. Only rough estimates of strength are possible, since humans tend to follow patterns in such tasks, and those patterns can usually assist an attacker. In addition, lists of commonly chosen passwords are widely available for use by password guessing programs. Such lists include the numerous online dictionaries for various human languages, breached databases of plaintext and hashed passwords from various online business and social accounts, along with other common passwords. All items in such lists are considered weak, as are passwords that are simple modifications of them. For some decades, investigations of passwords on multi-user computer systems have shown that 40% or more are readily guessed using only computer programs, and more can be found when information about a particular user is taken into account during the attack.\n\nSystems that use passwords for authentication must have some way to check any password entered to gain access. If the valid passwords are simply stored in a system file or database, an attacker who gains sufficient access to the system will obtain all user passwords, giving the attacker access to all accounts on the attacked system, and possibly other systems where users employ the same or similar passwords. One way to reduce this risk is to store only a cryptographic hash of each password instead of the password itself. Standard cryptographic hashes, such as the Secure Hash Algorithm (SHA) series, are very hard to reverse, so an attacker who gets hold of the hash value cannot directly recover the password. However, knowledge of the hash value lets the attacker quickly test guesses offline. Password cracking programs are widely available that will test a large number of trial passwords against a purloined cryptographic hash.\n\nImprovements in computing technology keep increasing the rate at which guessed passwords can be tested. For example, in 2010, the Georgia Tech Research Institute developed a method of using GPGPU to crack passwords much faster. Elcomsoft invented the usage of common graphic cards for quicker password recovery in August 2007 and soon filed a corresponding patent in the US. As of 2011, commercial products are available that claim the ability to test up to 112,000 passwords per second on a standard desktop computer using a high-end graphics processor. Such a device will crack a 6 letter single-case password in one day. Note that the work can be distributed over many computers for an additional speedup proportional to the number of available computers with comparable GPUs. Special key stretching hashes are available that take a relatively long time to compute, reducing the rate at which guessing can take place. Although it is considered best practice to use key stretching, many common systems do not.\n\nAnother situation where quick guessing is possible is when the password is used to form a cryptographic key. In such cases, an attacker can quickly check to see if a guessed password successfully decodes encrypted data. For example, one commercial product claims to test 103,000 WPA PSK passwords per second.\n\nIf a password system only stores the hash of the password, an attacker can pre-compute hash values for common passwords variants and for all passwords shorter than a certain length, allowing very rapid recovery of the password once its hash is obtained. Very long lists of pre-computed password hashes can be efficiently stored using rainbow tables. This method of attack can be foiled by storing a random value, called a cryptographic salt, along with the hash. The salt is combined with the password when computing the hash, so an attacker precomputing a rainbow table would have to store for each password its hash with every possible salt value. This becomes infeasible if the salt has a big enough range, say a 32-bit number. Unfortunately, many authentication systems in common use do not employ salts and rainbow tables are available on the Internet for several such systems.\n\nIt is usual in the computer industry to specify password strength in terms of information entropy, measured in bits, a concept from information theory. Instead of the number of guesses needed to find the password with certainty, the base-2 logarithm of that number is given, which is the number of \"entropy bits\" in a password. A password with, say, 42 bits of strength calculated in this way would be as strong as a string of 42 bits chosen randomly, say by a fair coin toss. Put another way, a password with 42 bits of strength would require 2 (4,398,046,511,104) attempts to exhaust all possibilities during a brute force search. Thus, adding one bit of entropy to a password doubles the number of guesses required, which makes an attacker's task twice as difficult. On average, an attacker will have to try half of the possible passwords before finding the correct one.\n\nRandom passwords consist of a string of symbols of specified length taken from some set of symbols using a random selection process in which each symbol is equally likely to be selected. The symbols can be individual characters from a character set (e.g., the ASCII character set), syllables designed to form pronounceable passwords, or even words from a word list (thus forming a passphrase).\n\nThe strength of random passwords depends on the actual entropy of the underlying number generator; however, these are often not truly random, but pseudo random. Many publicly available password generators use random number generators found in programming libraries that offer limited entropy. However most modern operating systems offer cryptographically strong random number generators that are suitable for password generation. It is also possible to use ordinary dice to generate random passwords. \"See stronger methods.\" Random password programs often have the ability to ensure that the resulting password complies with a local password policy; for instance, by always producing a mix of letters, numbers and special characters.\n\nFor passwords generated by a process that randomly selects a string of symbols of length, \"L\", from a set of \"N\" possible symbols, the number of possible passwords can be found by raising the number of symbols to the power \"L\", i.e. \"N\". Increasing either \"L\" or \"N\" will strengthen the generated password. The strength of a random password as measured by the information entropy is just the base-2 logarithm or log of the number of possible passwords, assuming each symbol in the password is produced independently. Thus a random password's information entropy, \"H\", is given by the formula\n\nwhere \"N\" is the number of possible symbols and \"L\" is the number of symbols in the password. \"H\" is measured in bits. In the last expression, \"log\" can be to any base.\n\nA binary byte is usually expressed using two hexadecimal characters.\n\nTo find the length, \"L,\" needed to achieve a desired strength \"H,\" with a password drawn randomly from a set of \"N\" symbols, one computes\n\nThe following table uses this formula to show the required lengths of truly randomly generated passwords to achieve desired password entropies for common symbol sets:\n\nPeople are notoriously poor at achieving sufficient entropy to produce satisfactory passwords. According to one study involving half a million users, the average password entropy was estimated at 40.54 bits. Some stage magicians exploit this inability for amusement, in a minor way, by divining supposed random choices (of numbers, say) made by audience members.\n\nThus, in one analysis of over 3 million eight-character passwords, the letter \"e\" was used over 1.5 million times, while the letter \"f\" was used only 250,000 times. A uniform distribution would have had each character being used about 900,000 times. The most common number used is \"1\", whereas the most common letters are a, e, o, and r.\n\nUsers rarely make full use of larger character sets in forming passwords. For example, hacking results obtained from a MySpace phishing scheme in 2006 revealed 34,000 passwords, of which only 8.3% used mixed case, numbers, and symbols.\n\nThe full strength associated with using the entire ASCII character set (numerals, mixed case letters and special characters) is only achieved if each possible password is equally likely. This seems to suggest that all passwords must contain characters from each of several character classes, perhaps upper and lower case letters, numbers, and non-alphanumeric characters. In fact, such a requirement is a pattern in password choice and can be expected to reduce an attacker's \"work factor\" (in Claude Shannon's terms). This is a reduction in password \"strength\". A better requirement would be to require a password NOT to contain any word in an online dictionary, or list of names, or any license plate pattern from any state (in the US) or country (as in the EU). If patterned choices are required, humans are likely to use them in predictable ways, such a capitalizing a letter, adding one or two numbers, and a special character. This predictability means that the increase in password strength is minor when compared to random passwords.\n\nNIST Special Publication 800-63 of June 2004 (revision 2) suggested the following scheme to roughly estimate the entropy of human-generated passwords:\n\nUsing this scheme, an eight-character human-selected password without upper case letters and non-alphabetic characters is estimated to have 18 bits of entropy. The NIST publication concedes that at the time of development, little information was available on the real world selection of passwords.\n\nLater research into human-selected password entropy using newly available real world data has demonstrated that the NIST scheme does not provide a valid metric for entropy estimation of human-selected passwords. The June 2017 revision of SP 800-63 (Revision 3) drops this approach.\n\nBecause national keyboard implementations vary, not all 94 ASCII printable characters can be used everywhere. This can present a problem to an international traveler who wished to log into remote system using a keyboard on a local computer. \"See\" \"keyboard layout\". Many hand held devices, such as tablet computers and smart phones, require complex shift sequences to enter special characters.\n\nAuthentication programs vary in which characters they allow in passwords. Some do not recognize case differences (e.g., the upper-case \"E\" is considered equivalent to the lower-case \"e\"), others prohibit some of the other symbols. In the past few decades, systems have permitted more characters in passwords, but limitations still exist. Systems also vary in the maximum length of passwords allowed.\n\nAs a practical matter, passwords must be both reasonable and functional for the end user as well as strong enough for the intended purpose. Passwords that are too difficult to remember may be forgotten and so are more likely to be written on paper, which some consider a security risk. In contrast, others argue that forcing users to remember passwords without assistance can only accommodate weak passwords, and thus poses a greater security risk. According to Bruce Schneier, most people are good at securing their wallets or purses, which is a \"great place\" to store a written password.\n\nThe minimum number of bits of entropy needed for a password depends on the threat model for the given application. If key stretching is not used, passwords with more entropy are needed. RFC 4086, \"Randomness Requirements for Security\", presents some example threat models and how to calculate the entropy desired for each one. Their answers vary between 29 bits of entropy needed if only online attacks are expected, and up to 96 bits of entropy needed for important cryptographic keys used in applications like encryption where the password or key needs to be secure for a long period of time and stretching isn't applicable. A 2010 Georgia Tech Research Institute study based on unstretched keys recommended a 12-character random password, but as a minimum length requirement.\n\nThe upper end is related to the stringent requirements of choosing keys used in encryption. In 1999, an Electronic Frontier Foundation project broke 56-bit DES encryption in less than a day using specially designed hardware. In 2002, \"distributed.net\" cracked a 64-bit key in 4 years, 9 months, and 23 days. As of October 12, 2011, \"distributed.net\" estimates that cracking a 72-bit key using current hardware will take about 45,579 days or 124.8 years. Due to currently understood limitations from fundamental physics, there is no expectation that any digital computer (or combination) will be capable of breaking 256-bit encryption via a brute-force attack. Whether or not quantum computers will be able to do so in practice is still unknown, though theoretical analysis suggests such possibilities.\n\nGuidelines for choosing good passwords are typically designed to make passwords harder to discover by intelligent guessing. Common guidelines advocated by proponents of software system security include:\n\nSome guidelines advise against writing passwords down, while others, noting the large numbers of password protected systems users must access, encourage writing down passwords as long as the written password lists are kept in a safe place, not attached to a monitor or in an unlocked desk drawer.\n\nThe possible character set for a password can be constrained by different web sites or by the range of keyboards on which the password must be entered.\n\nAs with any security measure, passwords vary in effectiveness (i.e., strength); some are weaker than others. For example, the difference in weakness between a dictionary word and a word with obfuscation (i.e., letters in the password are substituted by, say, numbers — a common approach) may cost a password cracking device a few more seconds; this adds little strength. The examples below illustrate various ways weak passwords might be constructed, all of which are based on simple patterns which result in extremely low entropy, allowing them to be tested automatically at high speeds.:\n\nThere are many other ways a password can be weak, corresponding to the strengths of various attack schemes; the core principle is that a password should have high entropy (usually taken to be equivalent to randomness) and \"not\" be readily derivable by any \"clever\" pattern, nor should passwords be mixed with information identifying the user. On-line services often provide a restore password function that a hacker can figure out and by doing so bypass a password. Choosing hard-to-guess restore password questions can further secure the password.\n\nIn December, 2012, William Cheswick wrote an article published in ACM magazine that included the mathematical possibilities of how easy or difficult it would be to break passwords that are constructed using the commonly recommended, and sometimes followed, standards of today. In his article, William showed that a standard eight character alpha-numeric password could withstand a brute force attack of ten million attempts per second, and remain unbroken for 252 days. Ten million attempts each second is the acceptable rate of attempts using a multi-core system that most users would have access to. A much greater degree of attempts, at the rate of 7 billion per second, could also be achieved when using modern GPUs. At this rate, the same 8 character alpha-numeric password could be broken in approximately 30 seconds. Increasing the password complexity to a 13 character alpha-numeric password increases the time needed to crack it to more than 900,000 years at 7 billion attempts per second. This is, of course, assuming the password does not use a common word that a dictionary attack could break much sooner. Using a password of this strength reduces the obligation to change it as often as many organizations require, including the U.S. Government, as it could not be reasonably broken in such a short period of time.\n\nA password policy is a guide to choosing satisfactory passwords. It is intended to:\n\nFor example, password expiration is often covered by password policies. Password expiration serves two purposes:\nHowever, password expiration has its drawbacks:\n\nThe hardest passwords to crack, for a given length and character set, are random character strings; if long enough they resist brute force attacks (because there are many characters) and guessing attacks (due to high entropy). However, such passwords are typically the hardest to remember. The imposition of a requirement for such passwords in a password policy may encourage users to write them down, store them in PDAs or cellphones, or share them with others as a safeguard against memory failure. While some people consider each of these user resorts to increase security risks, others suggest the absurdity of expecting users to remember distinct complex passwords for each of the dozens of accounts they access. For example, in 2005, security expert Bruce Schneier recommended writing down one's password:\n\nThe following measures may increase acceptance of strong password requirements, if carefully used:\n\nPassword policies sometimes suggest memory techniques to assist remembering passwords:\n\nComputer users are generally advised to \"never write down a password anywhere, no matter what\" and \"never use the same password for more than one account.\" However, an ordinary computer user may have dozens of password-protected accounts. Users with multiple accounts needing passwords often give up and use the same password for every account. When varied password complexity requirements prevent use of the same (memorable) scheme for producing high-strength passwords, oversimplified passwords will often be created to satisfy irritating and conflicting password requirements.\nA Microsoft expert was quoted as saying at a 2005 security conference: \"I claim that password policy should say you should write down your password. I have 68 different passwords. If I am not allowed to write any of them down, guess what I am going to do? I am going to use the same password on every one of them.\"\n\nIf passwords are written down, they should never be kept in obvious places such as address books, Rolodex files, under drawers or keyboards, or behind pictures. Perhaps the worst, but all too common, location is a Post-It note on the computer monitor. Better locations are a safe deposit box or a locked file approved for information of sensitivity comparable to that protected by the password. Most locks on office file cabinets are far from adequate. Software is available for popular hand-held computers that can store passwords for numerous accounts in encrypted form. Another approach is to encrypt by hand on paper and remember the encryption method and key. And another approach is to use a single password or slightly varying passwords for low-security accounts and select distinctly separate strong passwords for a smaller number of high-value applications such as for online banking.\n\nAnother effective approach for remembering multiple passwords is to memorize a single \"master\" password and use software to generate a new password for each application, based on the master password and the application's name. This approach is used by Stanford's PwdHash, Princeton's Password Multiplier, and other stateless password managers. In this approach, protecting the master password is essential, as all passwords are compromised if the master password is revealed, and lost if the master password is forgotten or misplaced.\n\nA reasonable compromise for using large numbers of passwords is to record them in a password manager program, which include stand-alone applications, web browser extensions, or a manager built into the operating system. A password manager allows the user to use hundreds of different passwords, and only have to remember a single password, the one which opens the encrypted password database. Needless to say, this single password should be strong and well-protected (not recorded anywhere). Most password managers can automatically create strong passwords using a cryptographically secure random password generator, as well as calculating the entropy of the generated password. A good password manager will provide resistance against attacks such as key logging, clipboard logging and various other memory spying techniques.\n\n\n"}
{"id": "4722074", "url": "https://en.wikipedia.org/wiki?curid=4722074", "title": "Poincaré inequality", "text": "Poincaré inequality\n\nIn mathematics, the Poincaré inequality is a result in the theory of Sobolev spaces, named after the French mathematician Henri Poincaré. The inequality allows one to obtain bounds on a function using bounds on its derivatives and the geometry of its domain of definition. Such bounds are of great importance in the modern, direct methods of the calculus of variations. A very closely related result is the Friedrichs' inequality.\n\nLet \"p\", so that 1 ≤ \"p\" < ∞ and Ω a subset with at least one bound. Then there exists a constant \"C\", depending only on Ω and \"p\", so that, for every function \"u\" of the Sobolev space \"W\"(Ω) of zero-trace functions,\n\nAssume that 1 ≤ \"p\" ≤ ∞ and that Ω is a bounded connected open subset of the \"n\"-dimensional Euclidean space R with a Lipschitz boundary (i.e., Ω is a Lipschitz domain). Then there exists a constant \"C\", depending only on Ω and \"p\", such that for every function \"u\" in the Sobolev space \"W\"(Ω),\n\nwhere\n\nis the average value of \"u\" over Ω, with |Ω| standing for the Lebesgue measure of the domain Ω. When Ω is a ball, the above inequality is\ncalled a (p,p)-Poincaré inequality; for more general domains Ω, the above is more familiarly known as a Sobolev inequality.\n\nIn the context of metric measure spaces (for example, sub-Riemannian manifolds), such spaces support a (q,p)-Poincare inequality for some formula_4 if there are constants C and formula_5 so that for each ball B in the space,\n\nIn the context of metric measure spaces, formula_7 is the minimal p-weak upper gradient of u in the sense of \nHeinonen and Koskela [J. Heinonen and P. Koskela, Quasiconformal maps in metric spaces with controlled geometry, Acta Math. 181 (1998), 1–61]\n\nThere exist other generalizations of the Poincaré inequality to other Sobolev spaces. For example, the following (taken from ) is a Poincaré inequality for the Sobolev space \"H\"(T), i.e. the space of functions \"u\" in the \"L\" space of the unit torus T with Fourier transform \"û\" satisfying\n\nthere exists a constant \"C\" such that, for every \"u\" ∈ \"H\"(T) with \"u\" identically zero on an open set \"E\" ⊆ T,\n\nwhere cap(\"E\" × {0}) denotes the harmonic capacity of \"E\" × {0} when thought of as a subset of R.\n\nThe optimal constant \"C\" in the Poincaré inequality is sometimes known as the Poincaré constant for the domain Ω. Determining the Poincaré constant is, in general, a very hard task that depends upon the value of \"p\" and the geometry of the domain Ω. Certain special cases are tractable, however. For example, if Ω is a bounded, convex, Lipschitz domain with diameter \"d\", then the Poincaré constant is at most \"d\"/2 for \"p\" = 1, formula_10 for \"p\" = 2 (; ), and this is the best possible estimate on the Poincaré constant in terms of the diameter alone. For smooth functions, this can be understood as an application of the isoperimetric inequality to the function's level sets. In one dimension, this is Wirtinger's inequality for functions.\n\nHowever, in some special cases the constant \"C\" can be determined concretely. For example, for \"p\" = 2, it is well known that over the domain of unit isosceles right triangle, \"C\" = 1/π ( < \"d\"/π where formula_11). (See, for instance.)\n\nFurthermore, for a smooth, bounded domain formula_12, since the Rayleigh quotient for the Laplace operator in the space formula_13 is minimized by the eigenfunction corresponding to the minimal eigenvalue λ of the (negative) Laplacian, it is a simple consequence that, for any formula_14,\n\nand furthermore, that the constant λ is optimal.\n\n\n"}
{"id": "1196185", "url": "https://en.wikipedia.org/wiki?curid=1196185", "title": "Pre-intuitionism", "text": "Pre-intuitionism\n\nIn the mathematical philosophy, the pre-intuitionists were a small but influential group who informally shared similar philosophies on the nature of mathematics. The term itself was used by L. E. J. Brouwer, who in his 1951 lectures at Cambridge described the differences between intuitionism and its predecessors:\n\nOf a totally different orientation <nowiki>[</nowiki>from the \"Old Formalist School\" of Dedekind, Cantor, Peano, Zermelo, and Couturat, etc.<nowiki>]</nowiki> was the Pre-Intuitionist School, mainly led by Poincaré, Borel and Lebesgue. These thinkers seem to have maintained a modified observational standpoint for the introduction of natural numbers, for the principle of complete induction <nowiki>[</nowiki>...<nowiki>]</nowiki> For these, even for such theorems as were deduced by means of classical logic, they postulated an existence and exactness independent of language and logic and regarded its non-contradictority as certain, even without logical proof. For the continuum, however, they seem not to have sought an origin strictly extraneous to language and logic.\n\nThe pre-intuitionists, as defined by Luitzen Egbertus Jan Brouwer, differed from the formalist standpoint in several ways, particularly in regard to the introduction of natural numbers, or how the natural numbers are defined/denoted. For Poincaré, the definition of a mathematical entity is the construction of the entity itself and not an expression of an underlying essence or existence.\n\nThis is to say that no mathematical object exists without human construction of it, both in mind and language.\n\nThis sense of definition allowed Poincaré to argue with Bertrand Russell over Giuseppe Peano's axiomatic theory of natural numbers.\n\nPeano's fifth axiom states: \n\nThis is the principle of complete induction, which establishes the property of induction as necessary to the system. Since Peano's axiom is as infinite as the natural numbers, it is difficult to prove that the property of \"P\" does belong to any \"x\" and also \"x\" + 1. What one can do is say that, if after some number \"n\" of trials that show a property \"P\" conserved in \"x\" and \"x\" + 1, then we may infer that it will still hold to be true after \"n\" + 1 trials. But this is itself induction. And hence the argument is a vicious circle.\n\nFrom this Poincaré argues that if we fail to establish the consistency of Peano's axioms for natural numbers without falling into circularity, then the principle of complete induction is not provable by general logic.\n\nThus arithmetic and mathematics in general is not analytic but synthetic. Logicism thus rebuked and Intuition is held up. What Poincaré and the Pre-Intuitionists shared was the perception of a difference between logic and mathematics that is not a matter of language alone, but of knowledge itself.\n\nIt was for this assertion, among others, that Poincaré was considered to be similar to the intuitionists. For Brouwer though, the Pre-Intuitionists failed to go as far as necessary in divesting mathematics from metaphysics, for they still used \"principium tertii exclusi\" (the \"law of excluded middle\").\n\nThe principle of the excluded middle does lead to some strange situations. For instance, statements about the future such as \"There will be a naval battle tomorrow\" do not seem to be either true or false, \"yet\". So there is some question whether statements must be either true or false in some situations. To an intuitionist this seems to rank the law of excluded middle as just as unrigorous as Peano's vicious circle.\n\nYet to the Pre-Intuitionists this is mixing apples and oranges. For them mathematics was one thing (a muddled invention of the human mind, \"i.e.\", synthetic), and logic was another (analytic).\n\nThe above examples only include the works of Poincaré, and yet Brouwer named other mathematicians as Pre-Intuitionists too; Borel and Lebesgue. Other mathematicians such as Hermann Weyl (who eventually became disenchanted with intuitionism, feeling that it places excessive strictures on mathematical progress) and Leopold Kronecker also played a role—though they are not cited by Brouwer in his definitive speech.\n\nIn fact Kronecker might be the most famous of the Pre-Intuitionists for his singular and oft quoted phrase, \"God made the natural numbers; all else is the work of man.\"\n\nKronecker goes in almost the opposite direction from Poincaré, believing in the natural numbers but not the law of the excluded middle. He was the first mathematician to express doubt on non-constructive existence proofs that state that something must exist because it can be shown that it is \"impossible\" for it not to.\n\n\n"}
{"id": "5583245", "url": "https://en.wikipedia.org/wiki?curid=5583245", "title": "Purification of quantum state", "text": "Purification of quantum state\n\nIn quantum mechanics, especially quantum information, purification refers to the fact that every mixed state acting on finite-dimensional Hilbert spaces can be viewed as the reduced state of some pure state.\n\nIn purely linear algebraic terms, it can be viewed as a statement about positive-semidefinite matrices.\n\nLet ρ be a density matrix acting on a Hilbert space formula_1 of finite dimension \"n\". Then it is possible to construct a second Hilbert space formula_2 and a pure state formula_3 such that ρ is the partial trace of formula_4 with respect to formula_2. While the initial Hilbert space formula_1 might correspond to physically meaningful quantities, the second Hilbert space formula_2 needn't have any physical interpretation whatsoever. However, in physics the process of state purification is assumed to be physical, and so the second Hilbert space formula_2 should also correspond to a physical space, such as the environment. The exact form of formula_2 in such cases will depend on the problem. Here is a proof of principle, showing that at very least formula_2 has to have dimensions greater than or equal to formula_1.\n\nWith these statements in mind, if,\n\nwe say that formula_13 purifies formula_14.\n\nA density matrix is by definition positive semidefinite. So ρ can be diagonalized and written as formula_15 for some basis formula_16. Let formula_2 be another copy of the \"n\"-dimensional Hilbert space with an orthonormal basis formula_18. Define formula_3 by\n\nDirect calculation gives\n\nThis proves the claim.\n\n\nBy combining Choi's theorem on completely positive maps and purification of a mixed state, we can recover the Stinespring dilation theorem for the finite-dimensional case.\n"}
{"id": "9228246", "url": "https://en.wikipedia.org/wiki?curid=9228246", "title": "Quadratic growth", "text": "Quadratic growth\n\nIn mathematics, a function or sequence is said to exhibit quadratic growth when its values are proportional to the square of the function argument or sequence position. \"Quadratic growth\" often means more generally \"quadratic growth in the limit\", as the argument or sequence position goes to infinity – in big Theta notation, \"f\"(\"x\") = Θ(\"x\"). This can be defined both continuously (for a real-valued function of a real variable) or discretely (for a sequence of real numbers, i.e., real-valued function of an integer or natural number variable).\n\nExamples of quadratic growth include:\n\nFor a real function of a real variable, quadratic growth is equivalent to the second derivative being constant (i.e., the third derivative being zero), and thus functions with quadratic growth are exactly the quadratic polynomials, as these are the kernel of the third derivative operator \"D\". Similarly, for a sequence (a real function of an integer or natural number variable), quadratic growth is equivalent to the second finite difference being constant (the third finite difference being zero), and thus a sequence with quadratic growth is also a quadratic polynomial. Indeed, an integer-valued sequence with quadratic growth is a polynomial in the zeroth, first, and second binomial coefficient with integer values. The coefficients can be determined by taking the Taylor polynomial (if continuous) or Newton polynomial (if discrete).\n\nAlgorithmic examples include:\n\n"}
{"id": "2576598", "url": "https://en.wikipedia.org/wiki?curid=2576598", "title": "Rigorous Approach to Industrial Software Engineering", "text": "Rigorous Approach to Industrial Software Engineering\n\nRAISE (\"Rigorous Approach to Industrial Software Engineering\") was developed as part of the European ESPRIT II LaCoS project in the 1990s, led by Dines Bjørner. It consists of a set of tools designed for a specification language (RSL) for software development. It is especially espoused by UNU-IIST in Macau, who run training courses on site and around the world, especially in developing countries.\n\n\n"}
{"id": "30456092", "url": "https://en.wikipedia.org/wiki?curid=30456092", "title": "Sheldon Axler", "text": "Sheldon Axler\n\nSheldon Jay Axler (born 6 November 1949, Philadelphia) is an American mathematician, professor of mathematics and the Dean of the College of Science and Engineering at San Francisco State University. He has made contributions to mathematics education, publishing several mathematics textbooks.\n\nHe went to Palmetto High School at Miami, Florida (1967). He obtained his AB in mathematics with highest honors at Princeton University (1971) and his Ph.D. in mathematics, under professor Donald Sarason, from the University of California, Berkeley (1975, Dissertation: \"Subalgebras of formula_1\"). As a postdoc he was a C. L. E. Moore instructor at the Massachusetts Institute of Technology.\n\nHe taught for many years and became a Full Professor at Michigan State University. In 1997 Axler moved to San Francisco State University where he became the Chair of the Mathematics Department.\n\nAxler received the Lester R. Ford Award for expository writing in 1996 from the Mathematical Association of America. In 2012 he became a fellow of the American Mathematical Society.\n\nHe was an Associate Editor of the \"American Mathematical Monthly\" and the Editor-in-Chief of the Mathematical Intelligencer.\n\nAxler's book \"Linear Algebra Done Right\" eschews the use of determinants, in favor of other methods.\n\n\n"}
{"id": "19334943", "url": "https://en.wikipedia.org/wiki?curid=19334943", "title": "Six circles theorem", "text": "Six circles theorem\n\nIn geometry, the six circles theorem relates to a chain of six circles together with a triangle, such that each circle is tangent to two sides of the triangle and also to the preceding circle in the chain. The chain closes, in the sense that the sixth circle is always tangent to the first circle.\n\nThe name may also refer to Miquel's six circles theorem, the result that if five circles have four triple points of intersection then the remaining four points of intersection lie on a sixth circle.\n\n\n"}
{"id": "984629", "url": "https://en.wikipedia.org/wiki?curid=984629", "title": "Social complexity", "text": "Social complexity\n\nIn sociology, social complexity is a conceptual framework used in the analysis of society. Contemporary definitions of complexity in the sciences are found in relation to systems theory, in which a phenomenon under study has many parts and many possible arrangements of the relationships between those parts. At the same time, what is complex and what is simple is relative and may change with time.\n\nCurrent usage of the term \"complexity\" in the field of sociology typically refers specifically to theories of society as a complex adaptive system. However, social complexity and its emergent properties are central recurring themes throughout the historical development of social thought and the study of social change. The early founders of sociological theory, such as Ferdinand Tönnies, Émile Durkheim, Max Weber, Vilfredo Pareto, and Georg Simmel, all examined the exponential growth and increasing interrelatedness of social encounters and exchanges. This emphasis on interconnectivity in social relationships and the emergence of new properties within society is found in theoretical thinking in multiple areas of sociology. As a theoretical tool, social complexity theory serves as a basis for the connection of micro- and macro-level social phenomena, providing a meso-level or middle-range theoretical platform for hypothesis formation. Methodologically, the concept of social complexity is theory-neutral, meaning that it accommodates both local (micro) and global (macro) phenomena in sociological research.\n\nThe American sociologist Talcott Parsons carried on the work of the early founders mentioned above in his early (1937) work on action theory. By 1951, Parsons places these earlier ideas firmly into the realm of formal systems theory in \"The Social System\". For the next several decades, this synergy between general systems thinking and the further development of social system theories is carried forward by Parson's student, Robert K. Merton, and a long line of others, in discussions of theories of the middle-range and social structure and agency. During part of this same period, from the late 1970s through the early 1990s, discussion ensues in any number of other research areas about the properties of systems in which strong correlation of sub-parts leads to observed behaviors variously described as autopoetic, self-organizing, dynamical, turbulent, and chaotic. All of these are forms of system behavior arising from mathematical complexity. By the early 1990s, the work of social theorists such as Niklas Luhmann began reflecting these themes of complex behavior.\n\nOne of the earliest usages of the term \"complexity\", in the social and behavioral sciences, to refer specifically to a complex system is found in the study of modern organizations and management studies. However, particularly in management studies, the term often has been used in a metaphorical rather than in a qualitative or quantiative theoretical manner. By the mid-1990s, the \"complexity turn\" in social sciences begins as some of the same tools generally used in complexity science are incorporated into the social sciences. By 1998, the international, electronic periodical, \"Journal of Artificial Societies and Social Simulation\", had been created. In the last several years, many publications have presented overviews of complexity theory within the field of sociology. Within this body of work, connections also are drawn to yet other theoretical traditions, including constructivist epistemology and the philosophical positions of phenomenology, postmodernism and critical realism.\n\nMethodologically, social complexity is theory-neutral, meaning that it accommodates both local and global approaches to sociological research. The very idea of social complexity arises out of the historical-comparative methods of early sociologists; obviously, this method is important in developing, defining, and refining the theoretical construct of social complexity. As complex social systems have many parts and there are many possible relationships between those parts, appropriate methodologies are typically determined to some degree by the research level of analysis differentiated by the researcher according to the level of description or explanation demanded by the research hypotheses.\n\nAt the most localized level of analysis, ethnographic, participant- or non-participant observation, content analysis and other qualitative research methods may be appropriate. More recently, highly sophisticated quantitative research methodologies are being developed and used in sociology at both local and global levels of analysis. Such methods include (but are not limited to) bifurcation diagrams, network analysis, non-linear modeling, and computational models including cellular automata programming, sociocybernetics and other methods of social simulation.\n\nComplex social network analysis is used to study the dynamics of large, complex social networks. Dynamic network analysis brings together traditional social network analysis, link analysis and multi-agent systems within network science and network theory. Through the use of key concepts and methods in social network analysis, agent-based modeling, theoretical physics, and modern mathematics (particularly graph theory and fractal geometry), this method of inquiry brought insights into the dynamics and structure of social systems. New computational methods of localized social network analysis are coming out of the work of Duncan Watts, Albert-László Barabási, Nicholas A. Christakis, Kathleen Carley and others.\n\nNew methods of global network analysis are emerging from the work of John Urry and the sociological study of globalization, linked to the work of Manuel Castells and the later work of Immanuel Wallerstein. Since the late 1990s, Wallerstein increasingly makes use of complexity theory, particularly the work of Ilya Prigogine. Dynamic social network analysis is linked to a variety of methodological traditions, above and beyond systems thinking, including graph theory, traditional social network analysis in sociology, and mathematical sociology. It also links to mathematical chaos and complex dynamics through the work of Duncan Watts and Steven Strogatz, as well as fractal geometry through Albert-László Barabási and his work on scale-free networks.\n\nThe development of computational sociology involves such scholars as Nigel Gilbert, Klaus G. Troitzsch, Joshua M. Epstein, and others. The foci of methods in this field include social simulation and data-mining, both of which are sub-areas of computational sociology. Social simulation uses computers to create an artificial laboratory for the study of complex social systems; data-mining uses machine intelligence to search for non-trivial patterns of relations in large, complex, real-world databases. The emerging methods of socionics are a variant of computational sociology.\n\nComputational sociology is influenced by a number of micro-sociological areas as well as the macro-level traditions of systems science and systems thinking. The micro-level influences of symbolic interaction, exchange, and rational choice, along with the micro-level focus of computational political scientists, such as Robert Axelrod, helped to develop computational sociology's bottom-up, agent-based approach to modeling complex systems. This is what Joshua M. Epstein calls generative science. Other important areas of influence include statistics, mathematical modeling and computer simulation.\n\nSociocybernetics integrates sociology with second-order cybernetics and the work of Niklas Luhmann, along with the latest advances in complexity science. In terms of scholarly work, the focus of sociocybernetics has been primarily conceptual and only slightly methodological or empirical. Sociocybernetics is directly tied to systems thought inside and outside of sociology, specifically in the area of second-order cybernetics.\n\nAs a middle-range theoretical platform, social complexity can be applied to any research in which social interaction or the outcomes of such interactions can be observed, but particularly where they can be measured and expressed as continuous or discrete data points. One common criticism often cited regarding the usefulness of complexity science in sociology is the difficulty of obtaining adequate data. Nonetheless, application of the concept of social complexity and the analysis of such complexity has begun and continues to be an ongoing field of inquiry in sociology. From childhood friendships and teen pregnancy to criminology and counter-terrorism, theories of social complexity are being applied in almost all areas of sociological research.\n\nIn the area of communications research and informetrics, the concept of self-organizing systems appears in mid-1990s research related to scientific communications. Scientometrics and bibliometrics are areas of research in which discrete data are available, as are several other areas of social communications research such as sociolinguistics. Social complexity is also a concept used in semiotics.\n\nIn the first decade of the 21st century, the diversity of areas of application has grown as more sophisticated methods have developed. Social complexity theory is applied in studies of social cooperation and public goods; altruism; voting behavior; education; global civil unrest; collective action and social movements; social inequality; workforce and unemployment; economic geography and economic sociology; policy analysis; health care systems; and innovation and social change, to name a few. A current international scientific research project, the Seshat: Global History Databank, was explicitly designed to analyze changes in social complexity from the Neolithic Revolution until the Industrial Revolution.\n\n"}
{"id": "9594871", "url": "https://en.wikipedia.org/wiki?curid=9594871", "title": "Statutory reserve", "text": "Statutory reserve\n\nIn the business of insurance, statutory reserves are those assets an insurance company is legally required to maintain on its balance sheet with respect to the unmatured obligations (i.e., expected future claims) of the company. Statutory reserves are a type of actuarial reserve.\n\nStatutory reserves are intended to ensure that insurance companies are able to meet future obligations created by insurance policies. These reserves must be reported in statements filed with insurance regulatory bodies. They are calculated with a certain level of conservatism in order to protect policyholders and beneficiaries.\n\nThere are two types of methods for calculation of statutory reserves. Reserve methodology may be fully prescribed by law, which is often called formula-based reserving. This is in contrast to principles-based reserves, where actuaries are given latitude to use professional judgment in determining methodology and assumptions for reserve calculation. In the United States, where formula-based reserves are used, the National Association of Insurance Commissioners plans to implement principles-based reserves in 2017.\n\nIn the U.S. life insurance industry, statutory reserves are most commonly computed using the Commissioner's Reserve Valuation Method, or CRVM, the method prescribed by law for computing minimum required reserves.\n\nThe size of a CRVM reserve, as with most life reserves, is affected by the age and sex of the insured person, how long the policy for which it is computed has been in force, the plan of insurance offered by the policy, the rate of interest used in the calculation, and the mortality table with which the actuarial present values are computed.\n\nThe Commissioner's Reserve Valuation Method was itself established by the Standard Valuation Law (SVL), which was created by the NAIC and adopted by the several states shortly after World War II. The first mortality table prescribed by the SVL was the 1941 CSO (Commissioner's Standard Ordinary) table, at a maximum interest rate of 3½%. Subsequent amendments to the Standard Valuation Law have permitted the use of more modern mortality tables and higher rates of interest. The effect of these changes has in general been to reduce the amount of the reserves which life insurance companies are legally required to hold.\n\n\n\n"}
{"id": "228668", "url": "https://en.wikipedia.org/wiki?curid=228668", "title": "Suslin's problem", "text": "Suslin's problem\n\nIn mathematics, Suslin's problem is a question about totally ordered sets posed by and published posthumously.\nIt has been shown to be independent of the standard axiomatic system of set theory known as ZFC: showed that the statement can neither be proven nor disproven from those axioms, assuming ZF is consistent.\n\nGiven a non-empty totally ordered set \"R\" with the following four properties:\nIs \"R\" necessarily order-isomorphic to the real line R?\n\nIf the requirement for the countable chain condition is replaced with the requirement that \"R\" contains a countable dense subset (i.e., \"R\" is a separable space) then the answer is indeed yes: any such set \"R\" is necessarily order-isomorphic to R (proved by Cantor).\n\nThe condition for a topological space that every collection of non-empty disjoint open sets is at most countable is called the Suslin property.\n\nAny totally ordered set that is \"not\" isomorphic to R but satisfies (1) – (4) is known as a Suslin line. The Suslin hypothesis says that there are no Suslin lines: that every countable-chain-condition dense complete linear order without endpoints is isomorphic to the real line. An equivalent statement is that every tree of height ω either has a branch of length ω or an antichain of cardinality formula_1.\n\nThe generalized Suslin hypothesis says that for every infinite regular cardinal κ every tree of height κ either has a branch of length κ or an antichain of cardinality κ. The existence of Suslin lines is equivalent to the existence of Suslin trees and to Suslin algebras.\n\nThe Suslin hypothesis is independent of ZFC.\n\nThe Suslin hypothesis is also independent of both the generalized continuum hypothesis (proved by Ronald Jensen) and of the negation of the continuum hypothesis. It is not known whether the Generalized Suslin Hypothesis is consistent with the Generalized Continuum Hypothesis; however, since the combination implies the negation of the square principle at a singular strong limit cardinal—in fact, at all singular cardinals and all regular successor cardinals—it implies that the axiom of determinacy holds in L(R) and is believed to imply the existence of an inner model with a superstrong cardinal.\n\n\n"}
{"id": "591703", "url": "https://en.wikipedia.org/wiki?curid=591703", "title": "Szemerédi's theorem", "text": "Szemerédi's theorem\n\nIn arithmetic combinatorics, Szemerédi's theorem is a result concerning arithmetic progressions in subsets of the integers. In 1936, Erdős and Turán conjectured that every set of integers \"A\" with positive natural density contains a \"k\"-term arithmetic progression for every \"k\". Endre Szemerédi proved the conjecture in 1975.\n\nA subset \"A\" of the natural numbers is said to have positive upper density if\n\nSzemerédi's theorem asserts that a subset of the natural numbers with positive upper density contains infinitely many arithmetic progressions of length \"k\" for all positive integers \"k\".\n\nAn often-used equivalent finitary version of the theorem states that for every positive integer \"k\" and real number formula_2, there exists a positive integer\n\nsuch that every subset of {1, 2, ..., \"N\"} of size at least δ\"N\" contains an arithmetic progression of length \"k\".\n\nAnother formulation uses the function \"r\"(\"N\"), the size of the largest subset of {1, 2, ..., \"N\"} without an arithmetic progression of length \"k\". Szemerédi's theorem is equivalent to the asymptotic bound\n\nThat is, \"r\"(\"N\") grows less than linearly with \"N\".\n\nVan der Waerden's theorem, a precursor of Szemerédi's theorem, was proven in 1927.\n\nThe cases \"k\" = 1 and \"k\" = 2 of Szemerédi's theorem are trivial. The case \"k\" = 3 was established in 1953 by Klaus Roth via an adaptation of the Hardy–Littlewood circle method. Endre Szemerédi proved the case \"k\" = 4 through combinatorics. Using an approach similar to the one he used for the case \"k\" = 3, Roth gave a second proof for this in 1972.\n\nThe general case was settled in 1975, also by Szemerédi, who developed an ingenious and complicated extension of his previous combinatorial argument for \"k\" = 4 (called \"a masterpiece of combinatorial reasoning\" by Erdős). Several other proofs are now known, the most important being those by Hillel Furstenberg in 1977, using ergodic theory, and by Timothy Gowers in 2001, using both Fourier analysis and combinatorics. Terence Tao has called the various proofs of Szemerédi's theorem a \"Rosetta stone\" for connecting disparate fields of mathematics.\n\nIt is an open problem to determine the exact growth rate of \"r\"(\"N\"). The best known general bounds are\n\nwhere formula_6. The lower bound is due to O'Bryant building on the work of Behrend, Rankin, and Elkin. The upper bound is due to Gowers.\n\nFor small \"k\", there are tighter bounds than the general case. When \"k\" = 3, Bourgain, Heath-Brown, Szemerédi, and Sanders provided increasingly smaller upper bounds. The current best bounds are\n\ndue to O'Bryant and Bloom respectively.\n\nFor \"k\" = 4, Green and Tao proved that\n\nfor some \"c\" > 0.\n\nA multidimensional generalization of Szemerédi's theorem was first proven by Hillel Furstenberg and Yitzhak Katznelson using ergodic theory. Timothy Gowers, Vojtěch Rödl and Jozef Skokan with Brendan Nagle, Rödl, and Mathias Schacht, and Terence Tao provided combinatorial proofs.\n\nAlexander Leibman and Vitaly Bergelson generalized Szemerédi's to polynomial progressions: If formula_9 is a set with positive upper density and formula_10 are integer-valued polynomials such that formula_11, then there are infinitely many formula_12 such that formula_13 for all formula_14. Leibman and Bergelson's result also holds in a multidimensional setting.\n\nThe finitary version of Szemerédi's theorem can be generalized to finite additive groups including vector spaces over finite fields. The finite field analog can be used as a model for understanding the theorem in the natural numbers. The problem of obtaining bounds in the k=3 case of Szemerédi's theorem in vector spaces over formula_15 is known as the cap set problem.\n\nThe Green–Tao theorem asserts the prime numbers contain arbitrary long arithmetic progressions. It is not implied by Szemerédi's theorem because the primes have density 0 in the natural numbers. As part of their proof, Ben Green and Tao introduced a \"relative\" Szemerédi theorem which applies to subsets of the integers (even those with 0 density) satisfying certain pseudorandomness conditions. A more general relative Szemerédi theorem has since been given by David Conlon, Jacob Fox, and Yufei Zhao.\n\nThe Erdős conjecture on arithmetic progressions would imply both Szemerédi's theorem and the Green–Tao theorem.\n\n\n"}
{"id": "19287276", "url": "https://en.wikipedia.org/wiki?curid=19287276", "title": "Triple system", "text": "Triple system\n\nIn algebra, a triple system (or ternar) is a vector space \"V\" over a field F together with a F-trilinear map\nThe most important examples are Lie triple systems and Jordan triple systems. They were introduced by Nathan Jacobson in 1949 to study subspaces of associative algebras closed under triple commutators anticommutator]]s {\"u\", {\"v\", \"w\"}}. In particular, any [[Lie algebra]] defines a Lie triple system and any [[Jordan algebra]] defines a Jordan triple system. They are important in the theories of [[symmetric space]]s, particularly [[Hermitian symmetric space]]s and their generalizations ([[symmetric R-space]]s and their noncompact duals).\n\nA triple system is said to be a Lie triple system if the trilinear form, denoted [...], satisfies the following identities:\nThe first two identities abstract the skew symmetry and Jacobi identity for the triple commutator, while the third identity means that the linear map L:\"V\"→\"V\", defined by L(\"w\") = [\"u\", \"v\", \"w\"], is a derivation of the triple product. The identity also shows that the space k = span {L: \"u\", \"v\" ∈ \"V\"} is closed under commutator bracket, hence a Lie algebra.\n\nWriting m in place of \"V\", it follows that\ncan be made into a formula_6-graded Lie algebra, the \"standard embedding\" of m, with bracket\nThe decomposition of g is clearly a symmetric decomposition for this Lie bracket, and hence if \"G\" is a connected Lie group with Lie algebra g and \"K\" is a subgroup with Lie algebra k, then \"G\"/\"K\" is a symmetric space.\n\nConversely, given a Lie algebra g with such a symmetric decomposition (i.e., it is the Lie algebra of a symmetric space), the triple bracket [[\"u\", \"v\"], \"w\"] makes m into a Lie triple system.\n\nA triple system is said to be a Jordan triple system if the trilinear form, denoted {...}, satisfies the following identities:\nThe first identity abstracts the symmetry of the triple anticommutator, while the second identity means that if L:\"V\"→\"V\" is defined by L(\"y\") = {\"u\", \"v\", \"y\"} then\nso that the space of linear maps span {L:\"u\",\"v\" ∈ \"V\"} is closed under commutator bracket, and hence is a Lie algebra g.\n\nAny Jordan triple system is a Lie triple system with respect to the product\n\nA Jordan triple system is said to be positive definite (resp. nondegenerate) if the bilinear form on \"V\" defined by the trace of L is positive definite (resp. nondegenerate). In either case, there is an identification of \"V\" with its dual space, and a corresponding involution on g. They induce an involution of\nwhich in the positive definite case is a Cartan involution. The corresponding [[symmetric space]] is a [[symmetric R-space]]. It has a noncompact dual given by replacing the Cartan involution by its composite with the involution equal to +1 on g and −1 on \"V\" and \"V\". A special case of this construction arises when g preserves a complex structure on \"V\". In this case we obtain dual [[Hermitian symmetric space]]s of compact and noncompact type (the latter being [[bounded symmetric domain]]s).\n\nA Jordan pair is a generalization of a Jordan triple system involving two vector spaces \"V\" and \"V\". The trilinear form is then replaced by a pair of trilinear forms\nwhich are often viewed as quadratic maps \"V\" → Hom(\"V\", \"V\") and \"V\" → Hom(\"V\", \"V\"). The other Jordan axiom (apart from symmetry) is likewise replaced by two axioms, one being\nand the other being the analogue with + and − subscripts exchanged.\n\nAs in the case of Jordan triple systems, one can define, for \"u\" in \"V\" and \"v\" in \"V\", a linear map\nand similarly L. The Jordan axioms (apart from symmetry) may then be written\nwhich imply that the images of L and L are closed under commutator brackets in End(\"V\") and End(\"V\"). Together they determine a linear map\nwhose image is a Lie subalgebra formula_19, and the Jordan identities become Jacobi identities for a graded Lie bracket on\nso that conversely, if\nis a graded Lie algebra, then the pair formula_22 is a Jordan pair, with brackets\n\nJordan triple systems are Jordan pairs with \"V\" = \"V\" and equal trilinear forms. Another important case occurs when \"V\" and \"V\" are dual to one another, with dual trilinear forms determined by an element of\nThese arise in particular when formula_25 above is semisimple, when the Killing form provides a duality between formula_26 and formula_27.\n\n\n\n[[Category:Representation theory]]"}
{"id": "8485219", "url": "https://en.wikipedia.org/wiki?curid=8485219", "title": "Von Neumann neighborhood", "text": "Von Neumann neighborhood\n\nIn cellular automata, the von Neumann neighborhood (or 4-neighborhood) is classically defined on a two-dimensional square lattice and is composed of a central cell and its four adjacent cells. The neighborhood is named after John von Neumann, who used it to define the von Neumann cellular automaton and the von Neumann universal constructor within it. It is one of the two most commonly used neighborhood types for two-dimensional cellular automata, the other one being the Moore neighborhood.\n\nThis neighbourhood can be used to define the notion of 4-connected pixels in computer graphics.\n\nThe von Neumann neighbourhood of a cell is the cell itself and the cells at a Manhattan distance of 1.\n\nThe concept can be extended to higher dimensions, for example forming a 6-cell octahedral neighborhood for a cubic cellular automaton in three dimensions.\n\nAn extension of the simple von Neumann neighborhood described above is to take the set of points at a Manhattan distance of \"r\" > 1. This results in a diamond-shaped region (shown for \"r\" = 2 in the illustration). These are called von Neumann neighborhoods of range or extent \"r\". The number of cells in a 2-dimensional von Neumann neighborhood of range \"r\" can be expressed as formula_1. The number of cells in a \"d\"-dimensional von Neumann neighborhood of range \"r\" is the Delannoy number \"D\"(\"d\",\"r\"). The number of cells on a surface of a \"d\"-dimensional von Neumann neighborhood of range \"r\" is the Zaitsev number .\n\n\n"}
{"id": "25476856", "url": "https://en.wikipedia.org/wiki?curid=25476856", "title": "Well equidistributed long-period linear", "text": "Well equidistributed long-period linear\n\nThe Well Equidistributed Long-period Linear (WELL) is a family of pseudorandom number generators developed in 2006 by François Panneton, Pierre L'Ecuyer, and . It is a form of linear feedback shift register optimized for software implementation on a 32-bit machine.\nThe structure is similar to the Mersenne twister, a large state made up of previous output words (32 bits each), from which a new output word is generated using linear recurrences modulo 2 over a finite binary field formula_1. However, a more complex recurrence produces a denser generator polynomial, producing better statistical properties.\n\nEach step of the generator reads five words of state: the oldest 32 bits (which may straddle a word boundary if the state size is not a multiple of 32), the newest 32 bits, and three other words in between. \n\nThen a series of eight single-word transformations (mostly of the form codice_1) and six exclusive-or operations combine those into two words, which become the newest two words of state, one of which will be the output.\n\nSpecific parameters are provided for the following generators:\n\nNumbers give the state size in bits; letter suffixes denote variants of the same size.\n\n\n"}
