{"id": "607690", "url": "https://en.wikipedia.org/wiki?curid=607690", "title": "Affine connection", "text": "Affine connection\n\nIn the branch of mathematics called differential geometry, an affine connection is a geometric object on a smooth manifold which \"connects\" nearby tangent spaces, so it permits tangent vector fields to be differentiated as if they were functions on the manifold with values in a fixed vector space. The notion of an affine connection has its roots in 19th-century geometry and tensor calculus, but was not fully developed until the early 1920s, by Élie Cartan (as part of his general theory of connections) and Hermann Weyl (who used the notion as a part of his foundations for general relativity). The terminology is due to Cartan and has its origins in the identification of tangent spaces in Euclidean space by translation: the idea is that a choice of affine connection makes a manifold look infinitesimally like Euclidean space not just smoothly, but as an affine space.\n\nOn any manifold of positive dimension there are infinitely many affine connections. If the manifold is further endowed with a Riemannian metric then there is a natural choice of affine connection, called the Levi-Civita connection. The choice of an affine connection is equivalent to prescribing a way of differentiating vector fields which satisfies several reasonable properties (linearity and the Leibniz rule). This yields a possible definition of an affine connection as a covariant derivative or (linear) connection on the tangent bundle. A choice of affine connection is also equivalent to a notion of parallel transport, which is a method for transporting tangent vectors along curves. This also defines a parallel transport on the frame bundle. Infinitesimal parallel transport in the frame bundle yields another description of an affine connection, either as a Cartan connection for the affine group or as a principal connection on the frame bundle.\n\nThe main invariants of an affine connection are its torsion and its curvature. The torsion measures how closely the Lie bracket of vector fields can be recovered from the affine connection. Affine connections may also be used to define (affine) geodesics on a manifold, generalizing the \"straight lines\" of Euclidean space, although the geometry of those straight lines can be very different from usual Euclidean geometry; the main differences are encapsulated in the curvature of the connection.\n\nA smooth manifold is a mathematical object which looks locally like a smooth deformation of Euclidean space : for example a smooth curve or surface looks locally like a smooth deformation of a line or a plane. Smooth functions and vector fields can be defined on manifolds, just as they can on Euclidean space, and scalar functions on manifolds can be differentiated in a natural way. However, differentiation of vector fields is less straightforward: this is a simple matter in Euclidean space, because the tangent space of based vectors at a point can be identified naturally (by translation) with the tangent space at a nearby point . On a general manifold, there is no such natural identification between nearby tangent spaces, and so tangent vectors at nearby points cannot be compared in a well-defined way. The notion of an affine connection was introduced to remedy this problem by \"connecting\" nearby tangent spaces. The origins of this idea can be traced back to two main sources: surface theory and tensor calculus.\n\nConsider a smooth surface in 3-dimensional Euclidean space. Near to any point, can be approximated by its tangent plane at that point, which is an affine subspace of Euclidean space. Differential geometers in the 19th century were interested in the notion of development in which one surface was \"rolled\" along another, without \"slipping\" or \"twisting\". In particular, the tangent plane to a point of can be rolled on : this should be easy to imagine when is a surface like the 2-sphere, which is the smooth boundary of a convex region. As the tangent plane is rolled on , the point of contact traces out a curve on . Conversely, given a curve on , the tangent plane can be rolled along that curve. This provides a way to identify the tangent planes at different points along the curve: in particular, a tangent vector in the tangent space at one point on the curve is identified with a unique tangent vector at any other point on the curve. These identifications are always given by affine transformations from one tangent plane to another.\n\nThis notion of parallel transport of tangent vectors, by affine transformations, along a curve has a characteristic feature: the point of contact of the tangent plane with the surface \"always moves\" with the curve under parallel translation (i.e., as the tangent plane is rolled along the surface, the point of contact moves). This generic condition is characteristic of Cartan connections. In more modern approaches, the point of contact is viewed as the \"origin\" in the tangent plane (which is then a vector space), and the movement of the origin is corrected by a translation, so that parallel transport is linear, rather than affine.\n\nIn the point of view of Cartan connections, however, the affine subspaces of Euclidean space are \"model\" surfaces — they are the simplest surfaces in Euclidean 3-space, and are homogeneous under the affine group of the plane — and every smooth surface has a unique model surface tangent to it at each point. These model surfaces are \"Klein geometries\" in the sense of Felix Klein's Erlangen programme. More generally, an -dimensional affine space is a Klein geometry for the affine group , the stabilizer of a point being the general linear group . An affine -manifold is then a manifold which looks infinitesimally like -dimensional affine space.\n\nThe second motivation for affine connections comes from the notion of a covariant derivative of vector fields. Before the advent of coordinate-independent methods, it was necessary to work with vector fields by embedding their respective Euclidean vectors into an atlas. These components can be differentiated, but the derivatives do not transform in a manageable way under changes of coordinates. Correction terms were introduced by Elwin Bruno Christoffel (following ideas of Bernhard Riemann) in the 1870s so that the (corrected) derivative of one vector field along another transformed covariantly under coordinate transformations — these correction terms subsequently came to be known as Christoffel symbols.\n\nThis idea was developed into the theory of \"absolute differential calculus\" (now known as tensor calculus) by Gregorio Ricci-Curbastro and his student Tullio Levi-Civita between 1880 and the turn of the 20th century.\n\nTensor calculus really came to life, however, with the advent of Albert Einstein's theory of general relativity in 1915. A few years after this, Levi-Civita formalized the unique connection associated to a Riemannian metric, now known as the Levi-Civita connection. More general affine connections were then studied around 1920, by Hermann Weyl, who developed a detailed mathematical foundation for general relativity, and Élie Cartan, who made the link with the geometrical ideas coming from surface theory.\n\nThe complex history has led to the development of widely varying approaches to and generalizations of the affine connection concept.\n\nThe most popular approach is probably the definition motivated by covariant derivatives. On the one hand, the ideas of Weyl were taken up by physicists in the form of gauge theory and gauge covariant derivatives. On the other hand, the notion of covariant differentiation was abstracted by Jean-Louis Koszul, who defined (linear or Koszul) connections on vector bundles. In this language, an affine connection is simply a covariant derivative or (linear) connection on the tangent bundle.\n\nHowever, this approach does not explain the geometry behind affine connections nor how they acquired their name. The term really has its origins in the identification of tangent spaces in Euclidean space by translation: this property means that Euclidean -space is an affine space. (Alternatively, Euclidean space is a principal homogeneous space or torsor under the group of translations, which is a subgroup of the affine group.) As mentioned in the introduction, there are several ways to make this precise: one uses the fact that an affine connection defines a notion of parallel transport of vector fields along a curve. This also defines a parallel transport on the frame bundle. Infinitesimal parallel transport in the frame bundle yields another description of an affine connection, either as a Cartan connection for the affine group or as a principal connection on the frame bundle.\n\nLet be a smooth manifold and let be the space of vector fields on , that is, the space of smooth sections of the tangent bundle . Then an affine connection on is a bilinear map\n\nsuch that for all smooth functions in and all vector fields on :\n\n\n\nComparison of tangent vectors at different points on a manifold is generally not a well-defined process. An affine connection provides one way to remedy this using the notion of parallel transport, and indeed this can be used to give a definition of an affine connection.\n\nLet be a manifold with an affine connection . Then a vector field is said to be parallel if in the sense that for any vector field , . Intuitively speaking, parallel vectors have \"all their derivatives equal to zero\" and are therefore in some sense \"constant\". By evaluating a parallel vector field at two points and , an identification between a tangent vector at and one at is obtained. Such tangent vectors are said to be parallel transports of each other.\n\nNonzero parallel vector fields do not, in general, exist, because the equation is a partial differential equation which is overdetermined: the integrability condition for this equation is the vanishing of the curvature of (see below). However, if this equation is restricted to a curve from to it becomes an ordinary differential equation. There is then a unique solution for any initial value of at .\n\nMore precisely, if a smooth curve parametrized by an interval and , where , then a vector field along (and in particular, the value of this vector field at ) is called the parallel transport of along if\nFormally, the first condition means that is parallel with respect to the pullback connection on the pullback bundle . However, in a local trivialization it is a first-order system of linear ordinary differential equations, which has a unique solution for any initial condition given by the second condition (for instance, by the Picard–Lindelöf theorem).\n\nThus parallel transport provides a way of moving tangent vectors along a curve using the affine connection to keep them \"pointing in the same direction\" in an intuitive sense, and this provides a linear isomorphism between the tangent spaces at the two ends of the curve. The isomorphism obtained in this way will in general depend on the choice of the curve: if it does not, then parallel transport along every curve can be used to define parallel vector fields on , which can only happen if the curvature of is zero.\n\nA linear isomorphism is determined by its action on an ordered basis or frame. Hence parallel transport can also be characterized as a way of transporting elements of the (tangent) frame bundle along a curve. In other words, the affine connection provides a lift of any curve in to a curve in .\n\nAn affine connection may also be defined as a principal connection on the frame bundle or of a manifold . In more detail, is a smooth map from the tangent bundle of the frame bundle to the space of matrices (which is the Lie algebra of the Lie group of invertible matrices) satisfying two properties:\n\nSuch a connection immediately defines a covariant derivative not only on the tangent bundle, but on vector bundles associated to any group representation of , including bundles of tensors and tensor densities. Conversely, an affine connection on the tangent bundle determines an affine connection on the frame bundle, for instance, by requiring that vanishes on tangent vectors to the lifts of curves to the frame bundle defined by parallel transport.\n\nThe frame bundle also comes equipped with a solder form which is horizontal in the sense that it vanishes on vertical vectors such as the point values of the vector fields : indeed is defined first by projecting a tangent vector (to at a frame ) to , then by taking the components of this tangent vector on with respect to the frame . Note that is also -equivariant (where acts on by matrix multiplication).\n\nThe pair defines a bundle isomorphism of with the trivial bundle , where is the Cartesian product of and (viewed as the Lie algebra of the affine group, which is actually a semidirect product – see below).\n\nAffine connections can be defined within Cartan's general framework. In the modern approach, this is closely related to the definition of affine connections on the frame bundle. Indeed, in one formulation, a Cartan connection is an absolute parallelism of a principal bundle satisfying suitable properties. From this point of view the -valued one-form on the frame bundle (of an affine manifold) is a Cartan connection. However, Cartan's original approach was different from this in a number of ways:\n\nThe points just raised are easiest to explain in reverse, starting from the motivation provided by surface theory. In this situation, although the planes being rolled over the surface are tangent planes in a naive sense, the notion of a tangent space is really an infinitesimal notion, whereas the planes, as affine subspaces of , are infinite in extent. However these affine planes all have a marked point, the point of contact with the surface, and they are tangent to the surface at this point. The confusion therefore arises because an affine space with a marked point can be identified with its tangent space at that point. However, the parallel transport defined by rolling does not fix this origin: it is affine rather than linear; the linear parallel transport can be recovered by applying a translation.\n\nAbstracting this idea, an affine manifold should therefore be an -manifold with an affine space , of dimension , \"attached\" to each at a marked point , together with a method for transporting elements of these affine spaces along any curve in . This method is required to satisfy several properties:\n\nThese last two points are quite hard to make precise, so affine connections are more often defined infinitesimally. To motivate this, it suffices to consider how affine frames of reference transform infinitesimally with respect to parallel transport. (This is the origin of Cartan's method of moving frames.) An affine frame at a point consists of a list , where and the form a basis of . The affine connection is then given symbolically by a first order differential system\n\ndefined by a collection of one-forms . Geometrically, an affine frame undergoes a displacement travelling along a curve from to given (approximately, or infinitesimally) by\n\nFurthermore, the affine spaces are required to be tangent to in the informal sense that the displacement of along can be identified (approximately or infinitesimally) with the tangent vector to at (which is the infinitesimal displacement of ). Since\nwhere is defined by , this identification is given by , so the requirement is that should be a linear isomorphism at each point.\n\nThe tangential affine space is thus identified intuitively with an \"infinitesimal affine neighborhood\" of .\n\nThe modern point of view makes all this intuition more precise using principal bundles (the essential idea is to replace a frame or a \"variable\" frame by the space of all frames and functions on this space). It also draws on the inspiration of Felix Klein's Erlangen programme, in which a \"geometry\" is defined to be a homogeneous space. Affine space is a geometry in this sense, and is equipped with a \"flat\" Cartan connection. Thus a general affine manifold is viewed as \"curved\" deformation of the flat model geometry of affine space.\n\nInformally, an affine space is a vector space without a fixed choice of origin. It describes the geometry of points and free vectors in space. As a consequence of the lack of origin, points in affine space cannot be added together as this requires a choice of origin with which to form the parallelogram law for vector addition. However, a vector may be added to a point by placing the initial point of the vector at and then transporting to the terminal point. The operation thus described is the translation of along . In technical terms, affine -space is a set equipped with a free transitive action of the vector group on it through this operation of translation of points: is thus a principal homogeneous space for the vector group .\nThe general linear group is the group of transformations of which preserve the \"linear structure\" of in the sense that . By analogy, the affine group is the group of transformations of preserving the \"affine structure\". Thus must \"preserve translations\" in the sense that\n\nwhere is a general linear transformation. The map sending to is a group homomorphism. Its kernel is the group of translations . The stabilizer of any point in can thus be identified with using this projection: this realises the affine group as a semidirect product of and , and affine space as the homogeneous space .\n\nAn \"affine frame\" for consists of a point and a basis of the vector space . The general linear group acts freely on the set of all affine frames by fixing and transforming the basis in the usual way, and the map sending an affine frame to is the quotient map. Thus is a principal -bundle over . The action of extends naturally to a free transitive action of the affine group on , so that is an -torsor, and the choice of a reference frame identifies with the principal bundle .\n\nOn there is a collection of functions defined by\n\n(as before) and\n\nAfter choosing a basepoint for , these are all functions with values in , so it is possible to take their exterior derivatives to obtain differential 1-forms with values in . Since the functions yield a basis for at each point of , these 1-forms must be expressible as sums of the form\n\nfor some collection of real-valued one-forms on . This system of one-forms on the principal bundle defines the affine connection on .\n\nTaking the exterior derivative a second time, and using the fact that as well as the linear independence of the , the following relations are obtained:\n\nThese are the Maurer–Cartan equations for the Lie group (identified with by the choice of a reference frame). Furthermore:\n\nThus the forms define a flat principal connection on .\n\nFor a strict comparison with the motivation, one should actually define parallel transport in a principal -bundle over . This can be done by pulling back by the smooth map defined by translation. Then the composite is a principal -bundle over , and the forms pull back to give a flat principal -connection on this bundle.\n\nAn affine space, as with essentially any smooth Klein geometry, is a manifold equipped with a flat Cartan connection. More general affine manifolds or affine geometries are obtained easily by dropping the flatness condition expressed by the Maurer-Cartan equations. There are several ways to approach the definition and two will be given. Both definitions are facilitated by the realisation that 1-forms in the flat model fit together to give a 1-form with values in the Lie algebra of the affine group .\n\nIn these definitions, is a smooth -manifold and is an affine space of the same dimension.\n\nLet be a manifold, and a principal -bundle over . Then an affine connection is a 1-form on with values in satisfying the following properties\nThe last condition means that is an absolute parallelism on , i.e., it identifies the tangent bundle of with a trivial bundle (in this case ). The pair defines the structure of an affine geometry on , making it into an affine manifold.\n\nThe affine Lie algebra splits as a semidirect product of and and so may be written as a pair where takes values in and takes values in . Conditions 1 and 2 are equivalent to being a principal -connection and being a horizontal equivariant 1-form, which induces a bundle homomorphism from to the associated bundle . Condition 3 is equivalent to the fact that this bundle homomorphism is an isomorphism. (However, this decomposition is a consequence of the rather special structure of the affine group.) Since is the frame bundle of , it follows that provides a bundle isomorphism between and the frame bundle of ; this recovers the definition of an affine connection as a principal -connection on .\n\nThe 1-forms arising in the flat model are just the components of and .\n\nAn affine connection on is a principal -bundle over , together with a principal -subbundle of and a principal -connection (a 1-form on with values in ) which satisfies the following (generic) \"Cartan condition\". The component of pullback of to is a horizontal equivariant 1-form and so defines a bundle homomorphism from to : this is required to be an isomorphism.\n\nSince acts on , there is, associated to the principal bundle , a bundle , which is a fiber bundle over whose fiber at in is an affine space . A section of (defining a marked point in for each ) determines a principal -subbundle of (as the bundle of stabilizers of these marked points) and vice versa. The principal connection defines an Ehresmann connection on this bundle, hence a notion of parallel transport. The Cartan condition ensures that the distinguished section always moves under parallel transport.\n\nCurvature and torsion are the main invariants of an affine connection. As there are many equivalent ways to define the notion of an affine connection, so there are many different ways to define curvature and torsion.\n\nFrom the Cartan connection point of view, the curvature is the failure of the affine connection to satisfy the Maurer–Cartan equation\n\nwhere the second term on the left hand side is the wedge product using the Lie bracket in to contract the values. By expanding into the pair and using the structure of the Lie algebra , this left hand side can be expanded into the two formulae\n\nwhere the wedge products are evaluated using matrix multiplication. The first expression is called the torsion of the connection, and the second is also called the curvature.\n\nThese expressions are differential 2-forms on the total space of a frame bundle. However, they are horizontal and equivariant, and hence define tensorial objects. These can be defined directly from the induced covariant derivative on as follows.\n\nThe torsion is given by the formula\n\nIf the torsion vanishes, the connection is said to be \"torsion-free\" or \"symmetric\".\n\nThe curvature is given by the formula\n\nNote that is the Lie bracket of vector fields\n\nin Einstein notation. This is independent of coordinate system choice and\n\nthe tangent vector at point of the th coordinate curve. The are a natural basis for the tangent space at point , and the the corresponding coordinates for the vector field .\n\nWhen both curvature and torsion vanish, the connection defines a pre-Lie algebra structure on the space of global sections of the tangent bundle.\n\nIf is a Riemannian manifold then there is a unique affine connection on with the following two properties:\nThis connection is called the Levi-Civita connection.\n\nThe second condition means that the connection is a metric connection in the sense that the Riemannian metric is parallel: . In local coordinates the components of the connection form are called Christoffel symbols: because of the uniqueness of the Levi-Civita connection, there is a formula for these components in terms of the components of .\n\nSince straight lines are a concept in affine geometry, affine connections define a generalized notion of (parametrized) straight lines on any affine manifold, called affine geodesics. Abstractly, a parametric curve is a straight line if its tangent vector remains parallel and equipollent with itself when it is transported along . From the linear point of view, an affine connection distinguishes the affine geodesics in the following way: a smooth curve is an affine geodesic if is parallel transported along , that is\nwhere is the parallel transport map defining the connection.\n\nIn terms of the infinitesimal connection , the derivative of this equation implies\nfor all .\n\nConversely, any solution of this differential equation yields a curve whose tangent vector is parallel transported along the curve. For every and every , there exists a unique affine geodesic with and and where is the maximal open interval in , containing 0, on which the geodesic is defined. This follows from the Picard–Lindelöf theorem, and allows for the definition of an exponential map associated to the affine connection.\n\nIn particular, when is a (pseudo-)Riemannian manifold and is the Levi-Civita connection, then the affine geodesic are the usual geodesics of Riemannian geometry and are the locally distance minimizing curves.\n\nThe geodesics defined here are sometimes called affinely parametrized, since a given straight line in determines a parametric curve through the line up to a choice of affine reparametrization , where and are constants. The tangent vector to an affine geodesic is parallel and equipollent along itself. An unparametrized geodesic, or one which is merely parallel along itself without necessarily being equipollent, need only satisfy\n\nfor some function defined along . Unparametrized geodesics are often studied from the point of view of projective connections.\n\nAn affine connection defines a notion of development of curves. Intuitively, development captures the notion that if is a curve in , then the affine tangent space at may be \"rolled\" along the curve. As it does so, the marked point of contact between the tangent space and the manifold traces out a curve in this affine space: the development of .\n\nIn formal terms, let be the linear parallel transport map associated to the affine connection. Then the development is the curve in starts off at 0 and is parallel to the tangent of for all time :\n\nIn particular, is a \"geodesic\" if and only if its development is an affinely parametrized straight line in .\n\nIf is a surface in , it is easy to see that has a natural affine connection. From the linear connection point of view, the covariant derivative of a vector field is defined by differentiating the vector field, viewed as a map from to , and then projecting the result orthogonally back onto the tangent spaces of . It is easy to see that this affine connection is torsion-free. Furthermore, it is a metric connection with respect to the Riemannian metric on induced by the inner product on , hence it is the Levi-Civita connection of this metric.\n\nLet be the usual scalar product on , and let be the unit sphere. The tangent space to at a point is naturally identified with the vector subspace of consisting of all vectors orthogonal to . It follows that a vector field on can be seen as a map which satisfies\n\nDenote as the differential (Jacobian matrix) of such a map. Then we have:\n\n\n\n"}
{"id": "22616176", "url": "https://en.wikipedia.org/wiki?curid=22616176", "title": "Analytical regularization", "text": "Analytical regularization\n\nIn physics and applied mathematics, analytical regularization is a technique used to convert boundary value problems which can be written as Fredholm integral equations of the first kind involving singular operators into equivalent Fredholm integral equations of the second kind. The latter may be easier to solve analytically and can be studied with discretization schemes like the finite element method or the finite difference method because they are pointwise convergent. In computational electromagnetics, it is known as the method of analytical regularization. It was first used in mathematics during the development of operator theory before acquiring a name.\n\nAnalytical regularization proceeds as follows. First, the boundary value problem is formulated as an integral equation. Written as an operator equation, this will take the form\nwith formula_2 representing boundary conditions and inhomogeneities, formula_3 representing the field of interest, and formula_4 the integral operator describing how Y is given from X based on the physics of the problem. \nNext, formula_4 is split into formula_6, where formula_7 is invertible and contains all the singularities of formula_8 and formula_9 is regular. After splitting the operator and multiplying by the inverse of formula_10, the equation becomes\nor \nwhich is now a Fredholm equation of the second type because by construction formula_13 is compact on the Hilbert space of which formula_14 is a member.\n\nIn general, several choices for formula_15 will be possible for each problem.\n\n\n"}
{"id": "42148664", "url": "https://en.wikipedia.org/wiki?curid=42148664", "title": "Benacerraf's identification problem", "text": "Benacerraf's identification problem\n\nIn the philosophy of mathematics, Benacerraf's identification problem is a philosophical argument, developed by Paul Benacerraf, against set-theoretic Platonism. In 1965, Benacerraf published a paradigm changing article entitled \"What Numbers Could Not Be\". Historically, the work became a significant catalyst in motivating the development of mathematical structuralism.\n\nThe identification problem argues that there exists a fundamental problem in reducing natural numbers to pure sets. Since there exists an infinite number of ways of identifying the natural numbers with pure sets, no particular set-theoretic method can be determined as the \"true\" reduction. Benacerraf infers that any attempt to make such a choice of reduction immediately results in generating a meta-level, set-theoretic falsehood, namely in relation to other elementarily-equivalent set-theories not identical to the one chosen. The identification problem argues that this creates a fundamental problem for Platonism, which maintains that mathematical objects have a real, abstract existence. Benacerraf's dilemma to Platonic set-theory is arguing that the Platonic attempt to identify the \"true\" reduction of natural numbers to pure sets, as revealing the intrinsic properties of these abstract mathematical objects, is impossible. As a result, the identification problem ultimately argues that the relation of set theory to natural numbers cannot have an ontologically Platonic nature.\n\nThe historical motivation for the development of Benacerraf's identification problem derives from a fundamental problem of ontology. Since Medieval times, philosophers have argued as to whether the ontology of mathematics contains abstract objects. In the philosophy of mathematics, an abstract object is traditionally defined as an entity that: (1) exists independent of the mind; (2) exists independent of the empirical world; and (3) has eternal, unchangeable properties. Traditional mathematical Platonism maintains that some set of mathematical elements–natural numbers, real numbers, functions, relations, systems–are such abstract objects. Contrarily, mathematical nominalism denies the existence of any such abstract objects in the ontology of mathematics. \nIn the late 19th and early 20th century, a number of anti-Platonist programs gained in popularity. These included intuitionism, formalism, and predicativism. By the mid-20th century, however, these anti-Platonist theories had a number of their own issues. This subsequently resulted in a resurgence of interest in Platonism. It was in this historic context that the motivations for the identification problem developed.\n\nThe identification problem begins by evidencing some set of elementarily-equivalent, set-theoretic models of the natural numbers. Benacerraf considers two such set-theoretic methods:\n\nAs Benacerraf demonstrates, both method I and II reduce natural numbers to sets. Benacerraf formulates the dilemma as a question: which of these set-theoretic methods uniquely provides the true identity statements, which elucidates the true ontological nature of the natural numbers? Either method I or II could be used to define the natural numbers and subsequently generate true arithmetical statements to form a mathematical system. In their relation, the elements of such mathematical systems are isomorphic in their structure. However, the problem arises when these isomorphic structures are related together on the meta-level. The definitions and arithmetical statements from system I are not identical to the definitions and arithmetical statements from system II. For example, the two systems differ in their answer to whether 0 ∈ 2, insofar as ∅ is not an element of . Thus, in terms of failing the transitivity of identity, the search for true identity statements similarly fails. By attempting to reduce the natural numbers to sets, this renders a set-theoretic falsehood between the isomorphic structures of different mathematical systems. This is the essence of the identification problem.\n\nAccording to Benacerraf, the philosophical ramifications of this identification problem result in Platonic approaches failing the ontological test. The argument is used to demonstrate the impossibility for Platonism to reduce numbers to sets that reveals the existence of abstract objects.\n\n\n"}
{"id": "49287688", "url": "https://en.wikipedia.org/wiki?curid=49287688", "title": "Boson sampling", "text": "Boson sampling\n\nBoson sampling constitutes a restricted model of non-universal quantum computation introduced by S. Aaronson and A. Arkhipov. It consists of sampling from the probability distribution of identical bosons scattered by a linear interferometer. Although the problem is well defined for any bosonic particles, its photonic version is currently considered as the most promising platform for a scalable implementation of a boson sampling device, which makes it a non-universal approach to linear optical quantum computing. Moreover, while not universal, the boson sampling scheme is strongly believed to implement a classically hard task using far fewer physical resources than a full linear-optical quantum computing setup. This makes it an outstanding candidate for demonstrating the power of quantum computation in the near term.\n\nConsider a multimode linear-optical circuit of \"N\" modes that is injected with \"M\" indistinguishable single photons (\"N>M\"). Then, the photonic implementation of the boson sampling task consists of generating a sample from the probability distribution of single-photon measurements at the output of the circuit. Specifically, this requires reliable sources of single photons (currently the most widely used ones are parametric down-conversion crystals), as well as a linear interferometer. The latter can be fabricated, e.g., with fused-fiber beam splitters, through silica-on-silicon or laser-written integrated interferometers, or electrically and optically interfaced optical chips.\nFinally, the scheme also necessitates high efficiency single photon-counting detectors, such as those based on current-biased superconducting nanowires, which perform the measurements at the output of the circuit. Therefore, based on these three ingredients, the boson sampling setup does not require any ancillas, adaptive measurements or entangling operations, as does e.g. the universal optical scheme by Knill, Laflamme and Milburn (the KLM scheme). This makes it a non-universal model of quantum computation, and reduces the amount of physical resources needed for its practical realization.\n\nSpecifically, suppose the linear interferometer is described by an \"N×N\" unitary matrix formula_1 which performs a linear transformation of the creation (annihilation) operators formula_2 formula_3 of the circuit's input modes:\n\nformula_4\n\nHere \"i\" (\"j\") labels the input (output) modes, and formula_5 formula_6 denotes the creation (annihilation) operators of the output modes (\"i,j\"=1\"..., N\"). On the other hand, the interferometer characterized by the unitary formula_7 naturally induces the transformation formula_8 of its input states. Moreover, there is a homomorphism between the unitaries formula_7 and formula_8, and the latter transformation acts on the exponentially big Hilbert space of the system: simple counting arguments show that the size of the Hilbert space corresponding to a system of \"M\" indistinguishable photons distributed among \"N\" modes is given by the binomial coefficient formula_11 (notice that since this homomorphism exists, not all values of formula_8 are possible). Namely, suppose the interferometer is injected with an input state of single photons formula_13formula_14 with formula_15 formula_16 is the number of photons injected into the \"k\"th mode). Then, the state formula_17 at\n\nthe output of the circuit can be written down as formula_18 A simple way to understand the homomorphism between formula_7 and formula_8 is the following :\n\nWe define the isomorphism for the basis states: formula_21formula_22,\nand get the following result : formula_23\n\nIn the above expression formula_24 stands for the permanent of the matrix formula_25 which is obtained from the unitary formula_7 by repeating formula_27 times its \"i\"th column and formula_28 times its \"j\"th row. Usually, in the context of the boson sampling problem the input state is taken of a standard form, denoted as formula_29 for which each of the first \"M\" modes of the interferometer is injected with a single photon. In this case the above expression reads:\n\nformula_30\n\nwhere the matrix formula_31 is obtained from formula_7 by keeping its first \"M\" columns and repeating formula_28 times its \"j\"th row. Subsequently, the task of boson sampling is to sample either exactly or approximately from the above output distribution, given the unitary formula_7 describing the linear-optical circuit as input. As detailed below, the appearance of the permanent in the corresponding statistics of single-photon measurements contributes to the hardness of the boson sampling problem.\n\nThe main reason of the growing interest towards the model of boson sampling is that despite being non-universal it is strongly believed to perform a computational task that is intractable for a classical computer. One of the main reasons behind this is that the probability distribution, which the boson sampling device has to sample from, is related to the permanent of complex matrices. As is known, the computation of the permanent is in the general case an extremely hard task: it falls in the #P-hard complexity class. Moreover, its approximation to within multiplicative error is a #P-hard problem as well.\n\nNote also that all current proofs of the hardness of simulating boson sampling on a classical computer rely on the strong computational consequences that its efficient simulation by a classical algorithm would have. Namely, these proofs show that an efficient classical simulation would imply the collapse of the polynomial hierarchy to its third level, a possibility that is considered very unlikely by the computer science community, due to its strong computational implications (in line with the strong implications of P=NP problem).\n\nThe hardness proof of the exact boson sampling problem can be achieved following two distinct paths. Specifically, the first one uses the tools of the computational complexity theory and combines the following two facts:\n\n\nWhen combined together these two facts along with the Toda's theorem result in the collapse of the polynomial hierarchy, which as mentioned above is highly unlikely to occur. This leads to the conclusion that there is no classical polynomial-time algorithm for the exact boson sampling problem.\n\nOn the other hand, the alternative proof is inspired by a similar result for another restricted model of quantum computation – the model of instantaneous quantum computing.\nNamely, the proof uses the KLM scheme, which says that linear optics with adaptive measurements is universal for the class BQP. It also relies on the following facts:\n\n\nAgain, the combination of these three results, as in the previous case, results in the collapse of the polynomial hierarchy. This makes the existence of a classical polynomial-time algorithm for the exact boson sampling problem highly unlikely.\n\nThe best proposed classical algorithm for exact boson sampling runs in time formula_37 for a system with \"n\" photons and \"m\" output modes. This algorithm leads to an estimate of 50 photons required to demonstrate quantum supremacy with boson sampling. There is also an open-source implementation in R.\n\nThe above hardness proofs are not applicable to the realistic implementation of a boson sampling device, due to the imperfection of any experimental setup (including the presence of noise, decoherence, photon losses, etc.). Therefore, for practical needs one necessitates the hardness proof for the corresponding approximate task. The latter consists of sampling from a probability distribution that is formula_38 close to the one given by formula_35, in terms of the total variation distance. The understanding of the complexity of this problem relies then on several additional assumptions, as well as on two yet unproven conjectures.\n\nSpecifically, the proofs of the exact boson sampling problem cannot be directly applied here, since they are based on the #P-hardness of estimating the exponentially-small probability formula_35 of a specific measurement outcome. Thus, if a sampler \"\"knew\" which formula_35 we wanted to estimate, then it could adversarially choose to corrupt it (as long as the task is approximate). That is why, the idea is to \"hide\"\" the above probability formula_35 into an \"N×N\" random unitary matrix. This can be done knowing that any \"M×M\" submatrix of a unitary formula_7, randomly chosen according to the Haar measure, is close in variation distance to a matrix of i.i.d. complex random Gaussian variables, provided that \"M ≤ N\" (note that Haar random matrices can be directly implemented in optical circuits by mapping independent probability density functions for their parameters, to optical circuit components, i.e., beam splitters and phase shifters). Therefore, if the linear optical circuit implements a Haar random unitary matrix, the adversarial sampler will not be able to detect which of the exponentially\nmany probabilities formula_35 we care about, and thus will not be able to avoid its estimation. In this case formula_35 is proportional to the squared absolute value of the permanent of the \"M×M\" matrix formula_46 of i.i.d. Gaussians, smuggled inside formula_47 These arguments bring us to the first conjecture of the hardness proof of approximate boson sampling problem – the permanent-of-Gaussians conjecture:\n\nMoreover, the above conjecture can be linked to the estimation of formula_49 which the given probability of a specific measurement outcome is proportional to. However to establish this link one has to rely on another conjecture – the permanent anticoncentration conjecture:\n\nBy making use of the above two conjectures (which have several evidences of being true), the final proof eventually states that the existence of a classical polynomial-time algorithm for the approximate boson sampling task implies the collapse of the polynomial hierarchy. It is also worth mentioning another fact important to the proof of this statement, namely the so-called bosonic birthday paradox (in analogy with the well-known birthday paradox). The latter states that if \"M\" identical bosons are scattered among \"N\"≫\"M\" modes of a linear interferometer with no two bosons in the same mode, then with high probability two bosons will not be found in the same output mode either. This property has been experimentally observed with two and three photons in integrated interferometers of up to 16 modes. On the one hand this feature facilitates the implementation of a restricted boson sampling device. Namely, if the probability of having more than one photon at the output of a linear optical circuit is negligible, one does not require photon-number-resolving detectors anymore: on-off detectors will be sufficient for the realization of the setup.\n\nAlthough the probability formula_35 of a specific measurement outcome at the output of the interferometer is related to the permanent of submatrices of a unitary matrix, a boson sampling machine does not allow its estimation. The main reason behind is that the corresponding detection probability is usually exponentially small. Thus, in order to collect enough statistics to approximate its value, one has to run the quantum experiment for exponentially long time. Therefore, the estimate obtained from a boson sampler is not more efficient that running the classical polynomial-time algorithm by Gurvits for approximating the permanent of any matrix to within additive error.\n\nAs already mentioned above, for the implementation of a boson sampling machine one necessitates a reliable source of many indistinguishable photons, and this requirement currently remains one of the main difficulties in scaling up the complexity of the device. Namely, despite recent advances in photon generation techniques using atoms, molecules, quantum dots and color centers in diamonds, the most widely used method remains the parametric down-conversion (PDC) mechanism. The main advantages of PDC sources are the high photon indistinguishability, collection efficiency and relatively simple experimental setups. However, one of the drawbacks of this approach is its non-deterministic (heralded) nature. Specifically, suppose the probability of generating a single photon by means of a PDC crystal is \"ε\". Then, the probability of generating simultaneously \"M\" single photons is \"ε\", which decreases exponentially with \"M\". In other words, in order to generate the input state for the boson sampling machine, one would have to wait for exponentially long time, which would kill the advantage of the quantum setup over a classical machine. Subsequently, this characteristic restricted the use of PDC sources to proof-of-principle demonstrations of a boson sampling device.\n\nRecently, however, a new scheme has been proposed to make the best use of PDC sources for the needs of boson sampling, greatly enhancing the rate of \"M\"-photon events. This approach has been named scattershot boson sampling, which consists of connecting \"N\" (\"N\">\"M\") heralded single-photon sources to different input ports of the linear interferometer. Then, by pumping all \"N\" PDC crystals with simultaneous laser pulses, the probability of generating \"M\" photons will be given as formula_53 Therefore, for \"N\"≫\"M\", this results in an exponential improvement in the single photon generation rate with respect to the usual, fixed-input boson sampling with \"M\" sources. Note that this setting can also be seen as a problem of sampling \"N\" two-mode squeezed vacuum states generated from \"N\" PDC sources.\n\nScattershot boson sampling is still intractable for a classical computer: in the conventional setup we fixed the columns that defined our \"M\"×\"M\" submatrix and only varied the rows, whereas now we vary the columns too, depending on which \"M\" out of \"N\" PDC crystals generated single photons. Therefore, the proof can be constructed here similar to the original one. Furthermore, scattershot boson sampling has been also recently implemented with six photon-pair sources coupled to integrated photonic circuits of nine and thirteen modes, being an important leap towards a convincing experimental demonstration of the quantum computational supremacy. The scattershot boson sampling model can be further generalized to the case where both legs of PDC sources are subject to linear optical transformations (in the original scattershot case, one of the arms is used for heralding, i.e., it goes through the identity channel). Such a twofold scattershot boson sampling model is also computationally hard, as proven by making use of the symmetry of quantum mechanics under time reversal.\n\nAnother photonic implementation of boson sampling concerns Gaussian input states, i.e. states whose quasiprobability Wigner distribution function is a Gaussian one. The hardness of the corresponding sampling task can be linked to that of scattershot boson sampling. Namely, the latter can be embedded into the conventional boson sampling setup with Gaussian inputs. For this, one has to generate two-mode entangled Gaussian states and apply a Haar-random unitary formula_7 to their \"right halves\", while doing nothing to the others. Then we can measure the \"left halves\" to find out which of the input states contained a photon before we applied formula_47 This is precisely equivalent to scattershot boson sampling, except for the fact that our measurement of the herald photons has been deferred till the end of the experiment, instead of happening at the beginning. Therefore, approximate Gaussian boson sampling can be argued to be hard under precisely the same complexity assumption as can approximate ordinary or scattershot boson sampling. Gaussian resources can be employed at the measurement stage, as well. Namely, one can define a boson sampling model, where a linear optical evolution of input single-photon states is concluded by Gaussian measurements (more specifically, by eight-port homodyne detection that projects each output mode onto a squeezed coherent state). Such a model deals with continuous-variable measurement outcome, which, under certain conditions, is a computationally hard task. Finally, a linear optics platform for implementing a boson sampling experiment where input single-photons undergo an active (non-linear) Gaussian transformation is also available. This setting makes use of a set of two-mode squeezed vacuum states as a prior resource, with no need of single-photon sources or in-line nonlinear amplification medium\n\nThe above results state that the existence of a polynomial-time classical algorithm for the original boson sampling scheme with indistinguishable single photons (in the exact and approximate cases), for scattershot, as well as for the general Gaussian boson sampling problems is highly unlikely. Nevertheless, there are some non-trivial realizations of the boson sampling problem that allow for its efficient classical simulation. One such example is when the optical circuit is injected with distinguishable single photons. In this case, instead of summing the probability \"amplitudes\" corresponding to photonic many-particle paths, one has to sum the corresponding probabilities (i.e. the squared absolute values of the amplitudes). Consequently, the detection probability formula_35 will be proportional to the permanent of submatrices of (component-wise) squared absolute value of the unitary formula_47 The latter is now a non-negative matrix. Therefore, although the exact computation of the corresponding permanent is a #P-complete problem, its approximation can be performed efficiently on a classical computer, due the seminal algorithm by Jerrum, Sinclaire and Vigoda.\nIn other words, approximate boson sampling with distinguishable photons is efficiently classically simulable.\n\nAnother instance of classically simulable boson sampling setups consists of sampling from the probability distribution of coherent states injected into the linear interferometer. The reason is that at the output of a linear optical circuit coherent states remain such, and do not create any quantum entanglement among the modes. More precisely, only their amplitudes are transformed, and the transformation can be efficiently calculated on a classical computer (the computation comprises matrix multiplication). This fact can be used to perform corresponding sampling tasks from another set of states: so-called classical states, whose Glauber-Sudarshan \"P\" function is a well-defined probability distribution. These states can be represented as a mixture of coherent states due to the optical equivalence theorem. Therefore, picking random coherent states distributed according to the corresponding \"P\" function, one can perform efficient classical simulation of boson sampling from this set of classical states.\n\nThe above requirements for the photonic boson sampling machine allow for its small-scale construction by means of existing technologies. Consequently, shortly after the theoretical model was introduced, four different groups\nsimultaneously reported its realization.\n\nSpecifically, this included the implementation of boson sampling with:\n\nLater on, more complex boson sampling experiments have been performed, increasing the number of spatial modes of random interferometers up to 13 and 9 modes, and realizing a 6-mode fully reconfigurable integrated circuit.\nThese experiments altogether constitute the proof-of-principle demonstrations of an operational boson sampling device, and route towards its larger-scale implementations.\n\nA first scattershot boson sampling experiment has been recently implemented using six photon-pair sources coupled to integrated photonic circuits with 13 modes. The 6 photon-pair sources were obtained via type-II PDC processes in 3 different nonlinear crystals (exploiting the polarization degree of freedom). This allowed to sample simultaneously between 8 different input states. The 13-mode interferometer was realized by femtosecond laser-writing technique on alumino-borosilicate glass.\n\nThis experimental implementation represents a leap towards an experimental demonstration of the quantum computational supremacy.\n\nThere are several other proposals for the implementation of photonic boson sampling. This includes, e.g., the scheme for arbitrarily scalable boson sampling using two nested fiber loops. In this case, the architecture employs time-bin encoding, whereby the incident photons form a pulse train entering the loops. Meanwhile, dynamically controlled loop coupling ratios allow the construction of arbitrary linear interferometers. Moreover, the architecture employs only a single point of interference and may thus be easier to stabilize than other implementations.\n\nAnother approach relies on the realization of unitary transformations on temporal modes based on dispersion and pulse shaping. Namely, passing consecutively heralded photons through time-independent dispersion and measuring the output time of the photons is equivalent to a boson sampling experiment. With time-dependent dispersion, it is also possible to implement arbitrary single-particle unitaries. This scheme requires a much smaller number of sources and detectors and do not necessitate a large system of beam splitters.\n\nThe output of a universal quantum computer running, for example, Shor's factoring algorithm, can be efficiently verified classically, as is the case for all problems in\nthe non-deterministic polynomial-time (NP) complexity class. It is however not clear that a similar structure\nexists for the boson sampling scheme. Namely, as the latter is related to the problem of estimating matrix permanents (falling into #P-hard complexity class), it is not understood how to verify correct operation for large versions of the setup. Specifically, the naive verification of the output of a boson sampler by computing the corresponding measurement probabilities represents a problem intractable for a classical computer.\n\nA first relevant question is whether it is possible or not to distinguish between uniform and boson-sampling distributions by performing a polynomial number of measurements. The initial argument introduced in Ref. stated that as long as one makes use of symmetric measurement settings the above is impossible (roughly speaking a symmetric measurement scheme does not allow for labeling the output modes of the optical circuit). However, within current technologies the assumption of a symmetric setting is not justified (the tracking of the measurement statistics is fully accessible), and therefore the above argument does not apply. It is then possible to define a rigorous and efficient test to discriminate the boson sampling statistics from an unbiased probability distribution. The corresponding discriminator is correlated to the permanent of the submatrix associated with a given measurement pattern, but can be efficiently calculated. This test has been applied experimentally to distinguish between a boson sampling and a uniform distribution in the 3-photon regime with integrated circuits of 5, 7, 9 and 13 modes, as well as 9 modes.\n\nThe test above does not distinguish between more complex distributions, such as quantum and classical, or between fermionic and bosonic statistics. A physically motivated scenario to be addressed is the unwanted introduction of distinguishability between photons, which destroys quantum interference (this regime is readily accessible experimentally, for example by introducing temporal delay between photons). The opportunity then exists to tune between ideally indistinguishable (quantum) and perfectly distinguishable (classical) data and measure the change in a suitably constructed metric. This scenario can be addressed by a statistical test which performs a one-on-one likelihood comparison of the output probabilities. This test requires the calculation of a small number of permanents, but does not need the calculation of the full expected probability distribution. Experimental implementation of the test has been successfully reported in integrated laser-written circuits for both the standard boson sampling (3 photons in 7-, 9- and 13-mode interferometers) and the scattershot version (3 photons in 9- and 13-mode interferometers with different input states). Another possibility is based on the bunching property of indinguishable photons. One can analyze the probability to find a \"k\"-fold coincidence measurement outcomes (without any multiply populated input mode), which is significantly higher for distinguishable particles than for bosons due to the bunching tendency of the latters. Finally, leaving the space of random matrices one may focus on specific multimode setups with certain features. In particular, the analysis of the effect of bosonic clouding (the tendency for bosons to favor events with all particles in the same half of the output array of a continuous-time many-particle quantum walk) has been proven to discriminate the behavior of distinguishable and indistinguishable particles in this specific platform.\n\nA different approach to confirm that the boson sampling machine behaves as the theory predicts is to make use of fully reconfigurable optical circuits. With large-scale single-photon and multiphoton interference verified with predictable multimode correlations in a fully characterized circuit, a reasonable assumption is that the system maintains correct operation as the circuit is continuously reconfigured to implement a random unitary operation. To this end, one can exploit quantum suppression laws (the probability of specific input-output combinations is suppressed when the linear interferometer is described by a Fourier matrix or other matrices with relevant symmetries). These suppression laws can be classically predicted in efficient ways. This approach allows also to exclude other physical models, such as mean-field states, which mimic some collective multiparticle properties (including bosonic clouding). The implementation of a Fourier matrix circuit in a fully reconfigurable 6-mode device has been reported, and experimental observations of the suppression law have been shown for 2 photons in 4- and 8-mode Fourier matrices.\n\nApart from the photonic realization of the boson sampling task, several other setups have been proposed. This includes, e.g., the encoding of bosons into the local transverse phonon modes of trapped ions. The scheme allows deterministic preparation and high-efficiency readout of the corresponding phonon Fock states and universal manipulation of the phonon modes through a combination of inherent Coulomb interaction and individual phase shifts. This scheme is scalable and relies on the recent advances in ion trapping techniques (several dozens of ions can be successfully trapped, for example, in linear Paul traps by making use of anharmonic axial potentials).\n\nAnother platform for implementing the boson sampling setup is a system of interacting spins: recent observation show that boson sampling with \"M\" particles in \"N\" modes is equivalent to the short-time evolution with \"M\" excitations in the \"XY\" model of 2\"N\" spins. One necessitates several additional assumptions here, including small boson bunching probability and efficient error postselection. This scalable scheme, however, is rather promising, in the light of considerable development in the construction and manipulation of coupled superconducting qubits and specifically the D-Wave machine.\n\nThe task of boson sampling shares peculiar similarities with the problem of determining molecular vibronic spectra: a feasible modification of the boson sampling scheme results in a setup that can be used for the reconstruction of a molecule's Franck–Condon profiles (for which no efficient classical algorithm is currently known). Specifically, the task now is to input specific squeezed coherent states into a linear interferometer that is determined by the properties of the molecule of interest. Therefore, this prominent observation makes the interest towards the implementation of the boson sampling task to get spread well beyond the fundamental basis.\n\nIt has also been suggested to use a superconducting resonator network Boson Sampling device as an interferometer. This application is assumed to be practical, as small changes in the couplings between the resonators will change the sampling results. Sensing of variation in the parameters capable of altering the couplings is thus achieved, when comparing the sampling results to an unaltered reference.\n\nVariants of the boson sampling model have been used to construct \"classical\" computational algorithms, aimed, e.g., at the estimation of certain matrix permanents (for instance, permanents of positive-semidefinite matrices related to the corresponding open problem in computer science) by combining tools proper to quantum optics and computational complexity.\n\nCoarse-grained boson sampling has been proposed as a resource of decision and function problems that are computationally hard, and may thus have cryptographic applications.\n\n\n"}
{"id": "4694026", "url": "https://en.wikipedia.org/wiki?curid=4694026", "title": "Bounded mean oscillation", "text": "Bounded mean oscillation\n\nIn harmonic analysis in mathematics, a function of bounded mean oscillation, also known as a BMO function, is a real-valued function whose mean oscillation is bounded (finite). The space of functions of bounded mean oscillation (BMO), is a function space that, in some precise sense, plays the same role in the theory of Hardy spaces \"H\" that the space \"L\" of essentially bounded functions plays in the theory of \"L\"-spaces: it is also called John–Nirenberg space, after Fritz John and Louis Nirenberg who introduced and studied it for the first time.\n\nAccording to , the space of functions of bounded mean oscillation was introduced by in connection with his studies of mappings from a bounded set belonging to R into R and the corresponding problems arising from elasticity theory, precisely from the concept of elastic strain: the basic notation was introduced in a closely following paper by , where several properties of this function spaces were proved. The next important step in the development of the theory was the proof by Charles Fefferman of the duality between BMO and the Hardy space \"H\", in the noted paper : a constructive proof of this result, introducing new methods and starting a further development of the theory, was given by Akihito Uchiyama.\n\n The mean oscillation of a locally integrable function \"u\" over a hypercube \"Q\" in R is defined as the value of the following integral:\n\nwhere\n\nNote 1. The supremum of the mean oscillation is called the BMO norm of \"u\". and is denoted by ||\"u\"|| (and in some instances it is also denoted ||\"u\"||).\n\nNote 2. The use of cubes \"Q\" in R as the integration domains on which the is calculated, is not mandatory: uses balls instead and, as remarked by , in doing so a perfectly equivalent definition of functions of bounded mean oscillation arises.\n\n\nBMO functions are locally \"L\" if 0 < \"p\" < ∞, but need not be locally bounded. In fact, using the John-Nirenberg Inequality, we can prove that\n\nConstant functions have zero mean oscillation, therefore functions differing for a constant \"c\" > 0 can share the same BMO norm value even if their difference is not zero almost everywhere. Therefore, the function ||\"u\"|| is properly a norm on quotient space of BMO functions modulo the space of constant functions on the domain considered.\n\nAs the name suggests, the mean or average of a function in BMO does not oscillate very much when computing it over cubes close to each other in position and scale. Precisely, if \"Q\" and \"R\" are dyadic cubes such that their boundaries touch and the side length of \"Q\" is no less than one-half the side length of \"R\" (and vice versa), then\n\nwhere \"C\" > 0 is some universal constant. This property is, in fact, equivalent to \"f\" being in BMO, that is, if \"f\" is a locally integrable function such that |\"f\"−\"f\"| ≤ \"C\" for all dyadic cubes \"Q\" and \"R\" adjacent in the sense described above and \"f\" is in dyadic BMO (where the supremum is only taken over dyadic cubes \"Q\"), then \"f\" is in BMO.\n\n showed that the BMO space is dual to \"H\", the Hardy space with \"p\" = 1. The pairing between \"f\" ∈ \"H\" and \"g\" ∈ BMO is given by\nthough some care is needed in defining this integral, as it does not in general converge absolutely.\n\nThe John–Nirenberg Inequality is an estimate that governs how far a function of bounded mean oscillation may deviate from its average by a certain amount.\n\n For each \"f\" ∈ BMO(R) there are constants \"c\", \"c\" > 0 , such that for any cube \"Q\" in R,\n\nConversely, if this inequality holds over all cubes with some constant \"C\" in place of ||\"f\"||, then \"f\" is in BMO with norm at most a constant times \"C\".\n\nThe John–Nirenberg inequality can actually give more information than just the BMO norm of a function. For a locally integrable function \"f\", let \"A\"(\"f\") be the infimal \"A\">0 for which\n\nThe John–Nirenberg inequality implies that \"A\"(\"f\") ≤ C||\"f\"|| for some universal constant \"C\". For an \"L\" function, however, the above inequality will hold for all \"A\" > 0. In other words, \"A\"(\"f\") = 0 if \"f\" is in L. Hence the constant \"A\"(\"f\") gives us a way of measuring how far a function in BMO is from the subspace \"L\". This statement can be made more precise: there is a constant \"C\", depending only on the dimension \"n\", such that for any function \"f\" ∈ BMO(R) the following two-sided inequality holds\n\nWhen the dimension of the ambient space is 1, the space BMO can be seen as a linear subspace of harmonic functions on the unit disk and plays a major role in the theory of Hardy spaces: by using , it is possible to define the BMO(\"T\") space on the unit circle as the space of functions \"f\" : \"T\" → R such that\n\ni.e. such that its over every arc I of the unit circle is bounded. Here as before \"f\" is the mean value of f over the arc I.\n\nequipped with the norm:\n\nThe subspace of analytic functions belonging BMOH is called the Analytic BMO space or the BMOA space.\n\nCharles Fefferman in his original work proved that the real BMO space is dual to the real valued harmonic Hardy space on the upper half-space R × (0, ∞]. In the theory of Complex and Harmonic analysis on the unit disk, his result is stated as follows. Let \"H\"(\"D\") be the Analytic Hardy space on the unit Disc. For \"p\" = 1 we identify (\"H\")* with BMOA by pairing \"f\" ∈ \"H\"(\"D\") and \"g\" ∈ BMOA using the \"anti-linear transformation\" \"T\"\n\nNotice that although the limit always exists for an \"H\" function f and \"T\" is an element of the dual space (\"H\")*, since the transformation is \"anti-linear\", we don't have an isometric isomorphism between (\"H\")* and BMOA. However one can obtain an isometry if they consider a kind of \"space of conjugate BMOA functions\".\n\nThe space VMO of functions of vanishing mean oscillation is the closure in BMO of the continuous functions that vanish at infinity. It can also be defined as the space of functions whose \"mean oscillations\" on cubes \"Q\" are not only bounded, but also tend to zero uniformly as the radius of the cube \"Q\" tends to 0 or ∞. The space VMO is a sort of Hardy space analogue of the space of continuous functions vanishing at infinity, and in particular the real valued harmonic Hardy space \"H\" is the dual of VMO.\n\nA locally integrable function \"f\" on R is BMO if and only if it can be written as\n\nwhere \"f\" ∈ \"L\", α is a constant and \"H\" is the Hilbert transform.\n\nThe BMO norm is then equivalent to the infimum of formula_14 over all such representations.\n\nSimilarly \"f\" is VMO if and only if it can be represented in the above form with \"f\" bounded uniformly continuous functions on R.\n\nLet \"Δ\" denote the set of dyadic cubes in R. The space dyadic BMO, written BMO is the space of functions satisfying the same inequality as for BMO functions, only that the supremum is over all dyadic cubes. This supremum is sometimes denoted \"||•||\".\n\nThis space properly contains BMO. In particular, the function \"log(x)χ\" is a function that is in dyadic BMO but not in BMO. However, if a function \"f\" is such that ||\"f\"(•−\"x\")|| ≤ \"C\" for all \"x\" in R for some \"C\" > 0, then by the one-third trick \"f\" is also in BMO.\n\nAlthough dyadic BMO is a much narrower class than BMO, many theorems that are true for BMO are much simpler to prove for dyadic BMO, and in some cases one can recover the original BMO theorems by proving them first in the special dyadic case.\n\nExamples of BMO functions include the following:\n\n\n"}
{"id": "7583", "url": "https://en.wikipedia.org/wiki?curid=7583", "title": "Cauchy–Riemann equations", "text": "Cauchy–Riemann equations\n\nIn the field of complex analysis in mathematics, the Cauchy–Riemann equations, named after Augustin Cauchy and Bernhard Riemann, consist of a system of two partial differential equations which, together with certain continuity and differentiability criteria, form a necessary and sufficient condition for a complex function to be complex differentiable, that is, holomorphic. This system of equations first appeared in the work of Jean le Rond d'Alembert . Later, Leonhard Euler connected this system to the analytic functions . then used these equations to construct his theory of functions. Riemann's dissertation on the theory of functions appeared in 1851.\n\nThe Cauchy–Riemann equations on a pair of real-valued functions of two real variables \"u\"(\"x\",\"y\") and \"v\"(\"x\",\"y\") are the two equations:\n\nTypically \"u\" and \"v\" are taken to be the real and imaginary parts respectively of a complex-valued function of a single complex variable , . Suppose that \"u\" and \"v\" are real-differentiable at a point in an open subset of C (C is the set of complex numbers), which can be considered as functions from R to R. This implies that the partial derivatives of \"u\" and \"v\" exist (although they need not be continuous) and we can approximate small variations of \"f\" linearly. Then is complex-differentiable at that point if and only if the partial derivatives of \"u\" and \"v\" satisfy the Cauchy–Riemann equations (1a) and (1b) at that point. The sole existence of partial derivatives satisfying the Cauchy–Riemann equations is not enough to ensure complex differentiability at that point. It is necessary that u and v be real differentiable, which is a stronger condition than the existence of the partial derivatives, but in general, weaker than continuous differentiability.\n\nHolomorphy is the property of a complex function of being differentiable at every point of an open and connected subset of C (this is called a domain in C). Consequently, we can assert that a complex function \"f\", whose real and imaginary parts \"u\" and \"v\" are real-differentiable functions, is holomorphic if and only if, equations (1a) and (1b) are satisfied throughout the domain we are dealing with. Holomorphic functions are analytic and vice versa. This means that, in complex analysis, a function that is complex-differentiable in a whole domain (holomorphic) is the same as an analytic function. This is not true for real differentiable functions.\n\nSuppose that \"z\" = \"x\" + \"iy\". The complex-valued function \"f\"(\"z\") = \"z\" is differentiable at any point \"z\" in the complex plane. \nThe real part \"u\"(\"x\", \"y\") and the imaginary part \"v\"(\"x\", \"y\") of \"f\"(\"z\") are\nrespectively. The partial derivatives of these are \nThese partial derivatives have the following relationships: \nThus this complex-valued function \"f\"(\"z\") satisfies the Cauchy–Riemann equations.\n\nThe equations are one way of looking at the condition on a function to be differentiable in the sense of complex analysis: in other words they encapsulate the notion of function of a complex variable by means of conventional differential calculus. In the theory there are several other major ways of looking at this notion, and the translation of the condition into other language is often needed.\n\nFirst, the Cauchy–Riemann equations may be written in complex form\n\nIn this form, the equations correspond structurally to the condition that the Jacobian matrix is of the form\n\nwhere formula_14 and formula_15. A matrix of this form is the matrix representation of a complex number. Geometrically, such a matrix is always the composition of a rotation with a scaling, and in particular preserves angles. The Jacobian of a function \"f\"(\"z\") takes infinitesimal line segments at the intersection of two curves in z and rotates them to the corresponding segments in \"f\"(\"z\"). Consequently, a function satisfying the Cauchy–Riemann equations, with a nonzero derivative, preserves the angle between curves in the plane. That is, the Cauchy–Riemann equations are the conditions for a function to be conformal.\n\nMoreover, because the composition of a conformal transformation with another conformal transformation is also conformal, the composition of a solution of the Cauchy–Riemann equations with a conformal map must itself solve the Cauchy–Riemann equations. Thus the Cauchy–Riemann equations are conformally invariant.\n\nSuppose that\n\nis a function of a complex number \"z\". Then the complex derivative of \"f\" at a point \"z\" is defined by\n\nprovided this limit exists.\n\nIf this limit exists, then it may be computed by taking the limit as \"h\" → 0 along the real axis or imaginary axis; in either case it should give the same result. Approaching along the real axis, one finds\n\nOn the other hand, approaching along the imaginary axis,\n\nThe equality of the derivative of \"f\" taken along the two axis is\n\nwhich are the Cauchy–Riemann equations (2) at the point \"z\".\n\nConversely, if \"f\" : C → C is a function which is differentiable when regarded as a function on R, then \"f\" is complex differentiable if and only if the Cauchy–Riemann equations hold. In other words, if u and v are real-differentiable functions of two real variables, obviously \"u\" + \"iv\" is a (complex-valued) real-differentiable function, but \"u\" + \"iv\" is complex-differentiable if and only if the Cauchy–Riemann equations hold.\n\nIndeed, following , suppose \"f\" is a complex function defined in an open set Ω ⊂ C. Then, writing for every \"z\" ∈ Ω, one can also regard Ω as an open subset of R, and \"f\" as a function of two real variables \"x\" and \"y\", which maps Ω ⊂ R to C. We consider the Cauchy–Riemann equations at \"z\" = \"z\". So assume \"f\" is differentiable at \"z\", as a function of two real variables from Ω to C. This is equivalent to the existence of the following linear approximation\n\nwhere \"z\" = \"x\" + \"iy\" and \"η\"(Δ\"z\") → 0 as Δ\"z\" → 0. Since formula_22 and formula_23, the above can be re-written as\n\nDefining the two Wirtinger derivatives as\n\nin the limit formula_26 the above equality can be written as\n\nNow consider the potential values of formula_28 when the limit is taken the origin. For \"z\" along the real line, formula_29 so that formula_30. Similarly for purely imaginary \"z\" we have formula_31 so that the value of formula_32 is not well defined at the origin. It's easy to verify that formula_32 is not well defined at any complex \"z\", hence \"f\" is complex differentiable at \"z\" if and only if formula_34 at formula_35. But this is exactly the Cauchy–Riemann equations, thus \"f\" is differentiable at \"z\" if and only if the Cauchy–Riemann equations hold at \"z\".\n\nThe above proof suggests another interpretation of the Cauchy–Riemann equations. The complex conjugate of \"z\", denoted formula_36, is defined by\n\nfor real \"x\" and \"y\". The Cauchy–Riemann equations can then be written as a single equation\n\nby using the Wirtinger derivative with respect to the conjugate variable. In this form, the Cauchy–Riemann equations can be interpreted as the statement that \"f\" is independent of the variable formula_36. As such, we can view analytic functions as true functions of \"one\" complex variable as opposed to complex functions of \"two\" real variables.\n\nA standard physical interpretation of the Cauchy–Riemann equations going back to Riemann's work on function theory (see ) is that \"u\" represents a velocity potential of an incompressible steady fluid flow in the plane, and \"v\" is its stream function. Suppose that the pair of (twice continuously differentiable) functions formula_40 satisfies the Cauchy–Riemann equations. We will take \"u\" to be a velocity potential, meaning that we imagine a flow of fluid in the plane such that the velocity vector of the fluid at each point of the plane is equal to the gradient of \"u\", defined by\n\nBy differentiating the Cauchy–Riemann equations a second time, one shows that \"u\" solves Laplace's equation:\nThat is, \"u\" is a harmonic function. This means that the divergence of the gradient is zero, and so the fluid is incompressible.\n\nThe function \"v\" also satisfies the Laplace equation, by a similar analysis. Also, the Cauchy–Riemann equations imply that the dot product formula_43. This implies that the gradient of \"u\" must point along the formula_44 curves; so these are the streamlines of the flow. The formula_45 curves are the equipotential curves of the flow.\n\nA holomorphic function can therefore be visualized by plotting the two families of level curves formula_45 and formula_44. Near points where the gradient of \"u\" (or, equivalently, \"v\") is not zero, these families form an orthogonal family of curves. At the points where formula_48, the stationary points of the flow, the equipotential curves of formula_45 intersect. The streamlines also intersect at the same point, bisecting the angles formed by the equipotential curves.\n\nAnother interpretation of the Cauchy–Riemann equations can be found in . Suppose that \"u\" and \"v\" satisfy the Cauchy–Riemann equations in an open subset of R, and consider the vector field\n\nregarded as a (real) two-component vector. Then the second Cauchy–Riemann equation (1b) asserts that formula_51 is irrotational (its curl is 0):\n\nThe first Cauchy–Riemann equation (1a) asserts that the vector field is solenoidal (or divergence-free):\n\nOwing respectively to Green's theorem and the divergence theorem, such a field is necessarily a conservative one, and it is free from sources or sinks, having net flux equal to zero through any open domain without holes. (These two observations combine as real and imaginary parts in Cauchy's integral theorem.) In fluid dynamics, such a vector field is a potential flow . In magnetostatics, such vector fields model static magnetic fields on a region of the plane containing no current. In electrostatics, they model static electric fields in a region of the plane containing no electric charge.\n\nThis interpretation can equivalently be restated in the language of differential forms. The pair \"u\",\"v\" satisfy the Cauchy–Riemann equations if and only if the one-form formula_54 is both closed and coclosed (a harmonic differential form).\n\nAnother formulation of the Cauchy–Riemann equations involves the complex structure in the plane, given by\nThis is a complex structure in the sense that the square of \"J\" is the negative of the 2×2 identity matrix: formula_56. As above, if \"u\"(\"x\",\"y\"),\"v\"(\"x\",\"y\") are two functions in the plane, put\n\nThe Jacobian matrix of \"f\" is the matrix of partial derivatives\n\nThen the pair of functions \"u\", \"v\" satisfies the Cauchy–Riemann equations if and only if the 2×2 matrix \"Df\" commutes with \"J\" \n\nThis interpretation is useful in symplectic geometry, where it is the starting point for the study of pseudoholomorphic curves.\n\nOther representations of the Cauchy–Riemann equations occasionally arise in other coordinate systems. If (1a) and (1b) hold for a differentiable pair of functions \"u\" and \"v\", then so do\n\nfor any coordinate system such that the pair (∇\"n\", ∇\"s\") is orthonormal and positively oriented. As a consequence, in particular, in the system of coordinates given by the polar representation , the equations then take the form\n\nCombining these into one equation for \"f\" gives\n\nThe inhomogeneous Cauchy–Riemann equations consist of the two equations for a pair of unknown functions \"u\"(\"x\",\"y\") and \"v\"(\"x\",\"y\") of two real variables\n\nfor some given functions α(\"x\",\"y\") and β(\"x\",\"y\") defined in an open subset of R. These equations are usually combined into a single equation\n\nwhere \"f\" = \"u\" + i\"v\" and \"φ\" = (\"α\" + i\"β\")/2.\n\nIf \"φ\" is \"C\", then the inhomogeneous equation is explicitly solvable in any bounded domain \"D\", provided \"φ\" is continuous on the closure of \"D\". Indeed, by the Cauchy integral formula,\n\nfor all ζ ∈ \"D\".\n\nSuppose that is a complex-valued function which is differentiable as a function . Then Goursat's theorem asserts that \"f\" is analytic in an open complex domain Ω if and only if it satisfies the Cauchy–Riemann equation in the domain . In particular, continuous differentiability of \"f\" need not be assumed .\n\nThe hypotheses of Goursat's theorem can be weakened significantly. If is continuous in an open set Ω and the partial derivatives of \"f\" with respect to \"x\" and \"y\" exist in Ω, and satisfy the Cauchy–Riemann equations throughout Ω, then \"f\" is holomorphic (and thus analytic). This result is the Looman–Menchoff theorem.\n\nThe hypothesis that \"f\" obey the Cauchy–Riemann equations throughout the domain Ω is essential. It is possible to construct a continuous function satisfying the Cauchy–Riemann equations at a point, but which is not analytic at the point (e.g., \"f\"(\"z\") = . Similarly, some additional assumption is needed besides the Cauchy–Riemann equations (such as continuity), as the following example illustrates \n\nwhich satisfies the Cauchy–Riemann equations everywhere, but fails to be continuous at \"z\" = 0.\n\nNevertheless, if a function satisfies the Cauchy–Riemann equations in an open set in a weak sense, then the function is analytic. More precisely :\nThis is in fact a special case of a more general result on the regularity of solutions of hypoelliptic partial differential equations.\nThere are Cauchy–Riemann equations, appropriately generalized, in the theory of several complex variables. They form a significant overdetermined system of PDEs. This is done using a straightforward generalization of the Wirtinger derivative, where the function in question is required to have the (partial) Wirtinger derivative with respect to each complex variable vanish.\n\nAs often formulated, the \"d-bar operator\" \n\nannihilates holomorphic functions. This generalizes most directly the formulation\n\nwhere\n\nViewed as conjugate harmonic functions, the Cauchy–Riemann equations are a simple example of a Bäcklund transform. More complicated, generally non-linear Bäcklund transforms, such as in the sine-Gordon equation, are of great interest in the theory of solitons and integrable systems.\n\nIn Clifford algebra the complex number formula_69 is represented as formula_70 where formula_71. The fundamental derivative operator in Clifford algebra of Complex numbers is defined as formula_72. The function formula_73 is considered analytic if and only if formula_74, which can be calculated in following way:\n\nGrouping by formula_76 and formula_77:\n\nHenceforth in traditional notation:\n\nLet Ω be an open set in the Euclidean space R. The equation for an orientation-preserving mapping formula_80 to be a conformal mapping (that is, angle-preserving) is that\nwhere \"Df\" is the Jacobian matrix, with transpose formula_82, and \"I\" denotes the identity matrix . For , this system is equivalent to the standard Cauchy–Riemann equations of complex variables, and the solutions are holomorphic functions. In dimension , this is still sometimes called the Cauchy–Riemann system, and Liouville's theorem implies, under suitable smoothness assumptions, that any such mapping is a Möbius transformation.\n\n\n\n"}
{"id": "20901868", "url": "https://en.wikipedia.org/wiki?curid=20901868", "title": "Combinatorics and dynamical systems", "text": "Combinatorics and dynamical systems\n\nThe mathematical disciplines of combinatorics and dynamical systems interact in a number of ways. The ergodic theory of dynamical systems has recently been used to prove combinatorial theorems about number theory which has given rise to the field of arithmetic combinatorics. Also dynamical systems theory is heavily involved in the relatively recent field of combinatorics on words. Also combinatorial aspects of dynamical systems are studied. Dynamical systems can be defined on combinatorial objects; see for example graph dynamical system.\n\n\n\n"}
{"id": "6746775", "url": "https://en.wikipedia.org/wiki?curid=6746775", "title": "Compact embedding", "text": "Compact embedding\n\nIn mathematics, the notion of being compactly embedded expresses the idea that one set or space is \"well contained\" inside another. There are versions of this concept appropriate to general topology and functional analysis.\n\nLet (\"X\", \"T\") be a topological space, and let \"V\" and \"W\" be subsets of \"X\". We say that \"V\" is compactly embedded in \"W\", and write \"V\" ⊂⊂ \"W\", if\n\nLet \"X\" and \"Y\" be two normed vector spaces with norms ||•|| and ||•|| respectively, and suppose that \"X\" ⊆ \"Y\". We say that \"X\" is compactly embedded in \"Y\", and write \"X\" ⊂⊂ \"Y\", if\n\nIf \"Y\" is a Banach space, an equivalent definition is that the embedding operator (the identity) \"i\" : \"X\" → \"Y\" is a compact operator.\n\nWhen applied to functional analysis, this version of compact embedding is usually used with Banach spaces of functions. Several of the Sobolev embedding theorems are compact embedding theorems. When an embedding is not compact, it may possess a related, but weaker, property of cocompactness.\n\n"}
{"id": "4296490", "url": "https://en.wikipedia.org/wiki?curid=4296490", "title": "Completeness (cryptography)", "text": "Completeness (cryptography)\n\nIn cryptography, a boolean function is said to be complete if the value of each output bit depends on \"all\" input bits. \n\nThis is a desirable property to have in an encryption cipher, so that if one bit of the input (plaintext) is changed, every bit of the output (ciphertext) has an average of 50% probability of changing. The easiest way to show why this is good is the following: consider that if we changed our 8-byte plaintext's last byte, it would only have any effect on the 8th byte of the ciphertext. This would mean that if the attacker guessed 256 different plaintext-ciphertext pairs, he would always know the last byte of every 8byte sequence we send (effectively 12.5% of all our data). Finding out 256 plaintext-ciphertext pairs is not hard at all in the internet world, given that standard protocols are used, and standard protocols have standard headers and commands (e.g. \"get\", \"put\", \"mail from:\", etc.) which the attacker can safely guess. On the other hand, if our cipher has this property (and is generally secure in other ways, too), the attacker would need to collect 2 (~10) plaintext-ciphertext pairs to crack the cipher in this way.\n\n"}
{"id": "581124", "url": "https://en.wikipedia.org/wiki?curid=581124", "title": "Cramér–Rao bound", "text": "Cramér–Rao bound\n\nIn estimation theory and statistics, the Cramér–Rao bound (CRB), Cramér–Rao lower bound (CRLB), Cramér–Rao inequality, Frechet–Darmois–Cramér–Rao inequality, or information inequality expresses a lower bound on the variance of unbiased estimators of a deterministic (fixed, though unknown) parameter. This term is named in honor of Harald Cramér, Calyampudi Radhakrishna Rao, Maurice Fréchet and Georges Darmois all of whom independently derived this limit to statistical precision in the 1940s.\n\nIn its simplest form, the bound states that the variance of any unbiased estimator is at least as high as the inverse of the Fisher information. An unbiased estimator which achieves this lower bound is said to be (fully) efficient. Such a solution achieves the lowest possible mean squared error among all unbiased methods, and is therefore the minimum variance unbiased (MVU) estimator. However, in some cases, no unbiased technique exists which achieves the bound. This may occur even when an MVU estimator exists.\n\nThe Cramér–Rao bound can also be used to bound the variance of \"biased\" estimators of given bias. In some cases, a biased approach can result in both a variance and a mean squared error that are \"below\" the unbiased Cramér–Rao lower bound; see estimator bias.\n\nThe Cramér–Rao bound is stated in this section for several increasingly general cases, beginning with the case in which the parameter is a scalar and its estimator is unbiased. All versions of the bound require certain regularity conditions, which hold for most well-behaved distributions. These conditions are listed later in this section.\n\nSuppose formula_1 is an unknown deterministic parameter which is to be estimated from measurements formula_2, distributed according to some probability density function formula_3. The variance of any \"unbiased\" estimator formula_4 of formula_1 is then bounded by the reciprocal of the Fisher information formula_6:\n\nwhere the Fisher information formula_6 is defined by\nand formula_10 is the natural logarithm of the likelihood function and formula_11 denotes the expected value (over formula_2).\n\nThe efficiency of an unbiased estimator formula_4 measures how close this estimator's variance comes to this lower bound; estimator efficiency is defined as\n\nor the minimum possible variance for an unbiased estimator divided by its actual variance.\nThe Cramér–Rao lower bound thus gives\n\nA more general form of the bound can be obtained by considering a biased estimator formula_16 of a function of the parameter, formula_17. Here, biasedness is understood as stating that formula_18 is not generally equal to 0. In this case, the bound is given by\nwhere formula_20 is the derivative of formula_17 (by formula_1), and formula_6 is the Fisher information defined above.\n\nApart from being a bound on estimators of functions of the parameter, this approach can be used to derive a bound on the variance of biased estimators with a given bias, as follows. Consider an estimator formula_4 with bias formula_25, and let formula_26. By the result above, any unbiased estimator whose expectation is formula_17 has variance greater than or equal to formula_28. Thus, any estimator formula_4 whose bias is given by a function formula_30 satisfies\nThe unbiased version of the bound is a special case of this result, with formula_32.\n\nIt's trivial to have a small variance − an \"estimator\" that is constant has a variance of zero. But from the above equation we find that the mean squared error of a biased estimator is bounded by\n\nusing the standard decomposition of the MSE. Note, however, that if formula_34 this bound might be less than the unbiased Cramér–Rao bound formula_35. For instance, in the example of estimating variance below, formula_36.\n\nExtending the Cramér–Rao bound to multiple parameters, define a parameter column vector\nwith probability density function formula_38 which satisfies the two regularity conditions below.\n\nThe Fisher information matrix is a formula_39 matrix with element formula_40 defined as\n\nLet formula_42 be an estimator of any vector function of parameters, formula_43, and denote its expectation vector formula_44 by formula_45. The Cramér–Rao bound then states that the covariance matrix of formula_42 satisfies\nwhere\n\nIf formula_42 is an unbiased estimator of formula_54 (i.e., formula_55), then the Cramér–Rao bound reduces to\n\nIf it is inconvenient to compute the inverse of the Fisher information matrix,\nthen one can simply take the reciprocal of the corresponding diagonal element\nto find a (possibly loose) lower bound.\n\nThe bound relies on two weak regularity conditions on the probability density function, formula_58, and the estimator formula_16:\n\nSuppose, in addition, that the operations of integration and differentiation can be swapped for the second derivative of formula_3 as well, i.e.,\nIn this case, it can be shown that the Fisher information equals\nThe Cramèr–Rao bound can then be written as\nIn some cases, this formula gives a more convenient technique for evaluating the bound.\n\nThe following is a proof of the general scalar case of the Cramér–Rao bound described above. Assume that formula_76 is an unbiased estimator for the value formula_17 (based on the observations formula_78), and so formula_79. The goal is to prove that, for all formula_1,\n\nLet formula_78 be a random variable with probability density function formula_58.\nHere formula_84 is a statistic, which is used as an estimator for formula_85. Define formula_86 as the score:\n\nwhere the chain rule is used in the final equality above. Then the expectation of formula_86, written formula_89, is zero. This is because:\n\nwhere the integral and partial derivative have been interchanged (justified by the second regularity condition).\nIf we consider the covariance formula_91 of formula_86 and formula_65, we have formula_94, because formula_95. Expanding this expression we have\n\nagain because the integration and differentiation operations commute (second condition).\n\nThe Cauchy–Schwarz inequality shows that\n\ntherefore\n\nwhich proves the proposition.\n\nFor the case of a \"d\"-variate normal distribution\nthe Fisher information matrix has elements\nwhere \"tr\" is the trace.\n\nFor example, let formula_101 be a sample of formula_102 independent observations with unknown mean formula_1 and known variance formula_104 .\nThen the Fisher information is a scalar given by\n\nand so the Cramér–Rao bound is\n\nSuppose \"X\" is a normally distributed random variable with known mean formula_108 and unknown variance formula_104. Consider the following statistic:\n\nThen \"T\" is unbiased for formula_104, as formula_112. What is the variance of \"T\"?\n\n(the second equality follows directly from the definition of variance). The first term is the fourth moment about the mean and has value formula_114; the second is the square of the variance, or formula_115.\nThus\n\nNow, what is the Fisher information in the sample? Recall that the score \"V\" is defined as\n\nwhere formula_118 is the likelihood function. Thus in this case,\n\nwhere the second equality is from elementary calculus. Thus, the information in a single observation is just minus the expectation of the derivative of \"V\", or\n\nThus the information in a sample of formula_121 independent observations is just formula_121 times this, or formula_123\n\nThe Cramer–Rao bound states that\n\nIn this case, the inequality is saturated (equality is achieved), showing that the estimator is efficient.\n\nHowever, we can achieve a lower mean squared error using a biased estimator. The estimator\n\nobviously has a smaller variance, which is in fact\n\nIts bias is\n\nso its mean squared error is\n\nwhich is clearly less than the Cramér–Rao bound found above.\n\nWhen the mean is not known, the minimum mean squared error estimate of the variance of a sample from Gaussian distribution is achieved by dividing by \"n\" + 1, rather than \"n\" − 1 or \"n\" + 2.\n\n\n\n"}
{"id": "33888206", "url": "https://en.wikipedia.org/wiki?curid=33888206", "title": "Crisis (dynamical systems)", "text": "Crisis (dynamical systems)\n\nIn applied mathematics and Astrodynamics, in the theory of dynamical systems, a crisis is the sudden appearance or disappearance of a strange attractor as the parameters of a dynamical system are varied. This global bifurcation occurs when a chaotic attractor comes into contact with an unstable periodic orbit or its stable manifold. As the orbit approaches the unstable orbit it will diverge away from the previous attractor, leading to a qualitatively different behaviour. Crises can produce intermittent behaviour.\n\nGrebogi, Ott, Romeiras, and Yorke distinguished between three types of crises:\n\n\nNote that the reverse case (sudden appearance, shrinking or splitting of attractors) can also occur. The latter two crises are sometimes called explosive bifurcations.\n\nWhile crises are \"sudden\" as a parameter is varied, the dynamics of the system over time can show long transients before orbits leave the neighbourhood of the old attractor. Typically there is a time constant τ for the length of the transient that diverges as a power law (τ ≈ |\"p\" − \"p\"|) near the critical parameter value \"p\". The exponent \"γ\" is called the critical crisis exponent. There also exist systems where the divergence is stronger than a power law, so-called super-persistent chaotic transients.\n\n\n"}
{"id": "45087831", "url": "https://en.wikipedia.org/wiki?curid=45087831", "title": "Cubic mean", "text": "Cubic mean\n\nThe cubic mean formula_1 of formula_2 real numbers formula_3 is defined as:\n"}
{"id": "9386948", "url": "https://en.wikipedia.org/wiki?curid=9386948", "title": "Directed infinity", "text": "Directed infinity\n\nA directed infinity is a type of infinity in the complex plane that has a defined complex argument \"θ\" but an infinite absolute value \"r\". For example, the limit of 1/\"x\" where \"x\" is a positive real number approaching zero is a directed infinity with argument 0; however, 1/0 is not a directed infinity, but a complex infinity. Some rules for manipulation of directed infinities (with all variables finite) are:\n\n\nHere, sgn(\"z\") = is the complex signum function.\n\n"}
{"id": "9165", "url": "https://en.wikipedia.org/wiki?curid=9165", "title": "Directed set", "text": "Directed set\n\nIn mathematics, a directed set (or a directed preorder or a filtered set) is a nonempty set \"A\" together with a reflexive and transitive binary relation ≤ (that is, a preorder), with the additional property that every pair of elements has an upper bound. In other words, for any \"a\" and \"b\" in \"A\" there must exist \"c\" in \"A\" with \"a\" ≤ \"c\" and \"b\" ≤ \"c\".\n\nThe notion defined above is sometimes called an upward directed set. A downward directed set is defined analogously, meaning when every pair of elements is bounded below. Some authors (and this article) assume that a directed set is directed upward, unless otherwise stated. Beware that other authors call a set directed if and only if it is directed both upward and downward.\n\nDirected sets are a generalization of nonempty totally ordered sets. That is, all totally ordered sets are directed sets (contrast \"partially\" ordered sets, which need not be directed). Join semilattices (which are partially ordered sets) are directed sets as well, but not conversely. Likewise, lattices are directed sets both upward and downward.\n\nIn topology, directed sets are used to define nets, which generalize sequences and unite the various notions of limit used in analysis. Directed sets also give rise to direct limits in abstract algebra and (more generally) category theory.\n\nIn addition to the definition above, there is an equivalent definition. A directed set is a set \"A\" with a preorder such that every finite subset of \"A\" has an upper bound. In this definition, the existence of an upper bound of the empty subset implies that \"A\" is nonempty.\n\nExamples of directed sets include:\n\nDirected sets are a more general concept than (join) semilattices: every join semilattice is a directed set, as the join or least upper bound of two elements is the desired \"c\". The converse does not hold however, witness the directed set {1000,0001,1101,1011,1111} ordered bitwise (e.g. 1000 ≤ 1011 holds, but 0001 ≤ 1000 does not, since in the last bit 1 > 0), where {1000,0001} has three upper bounds but no \"least\" upper bound, cf. picture. (Also note that without 1111, the set is not directed.)\n\nThe order relation in a directed set is not required to be antisymmetric, and therefore directed sets are not always partial orders. However, the term \"directed set\" is also used frequently in the context of posets. In this setting, a subset \"A\" of a partially ordered set (\"P\",≤) is called a directed subset if it is a directed set according to the same partial order: in other words, it is not the empty set, and every pair of elements has an upper bound. Here the order relation on the elements of \"A\" is inherited from \"P\"; for this reason, reflexivity and transitivity need not be required explicitly.\n\nA directed subset of a poset is not required to be downward closed; a subset of a poset is directed if and only if its downward closure is an ideal. While the definition of a directed set is for an \"upward-directed\" set (every pair of elements has an upper bound), it is also possible to define a downward-directed set in which every pair of elements has a common lower bound. A subset of a poset is downward-directed if and only if its upper closure is a filter.\n\nDirected subsets are used in domain theory, which studies directed complete partial orders. These are posets in which every upward-directed set is required to have a least upper bound. In this context, directed subsets again provide a generalization of convergent sequences.\n\n\n"}
{"id": "188946", "url": "https://en.wikipedia.org/wiki?curid=188946", "title": "Disjoint union", "text": "Disjoint union\n\nIn set theory, the disjoint union (or discriminated union) of a family of sets is a modified union operation that indexes the elements according to which set they originated in. Or slightly different from this, the disjoint union of a family of subsets is the usual union of the subsets which are pairwise disjoint – disjoint sets means they have no element in common.\n\nNote that these two concepts are different but strongly related. Moreover, it seems that they are essentially identical to each other in category theory. That is, both are realizations of the coproduct of category of sets. \n\nConsider the sets formula_1 and formula_2. We can index the set elements according to set origin by forming the associated sets\nwhere the second element in each pair matches the subscript of the origin set (e.g., the formula_4 in formula_5 matches the subscript in formula_6, etc.). The disjoint union formula_7 can then be calculated as follows:\n\nFormally, let {\"A\" : \"i\" ∈ \"I\"} be a family of sets indexed by \"I\". The disjoint union of this family is the set\nThe elements of the disjoint union are ordered pairs (\"x\", \"i\"). Here \"i\" serves as an auxiliary index that indicates which \"A\" the element \"x\" came from. \n\nEach of the sets \"A\" is canonically isomorphic to the set\nThrough this isomorphism, one may consider that \"A\" is canonically embedded in the disjoint union.\nFor \"i\" ≠ \"j\", the sets \"A\"* and \"A\"* are disjoint even if the sets \"A\" and \"A\" are not. \n\nIn the extreme case where each of the \"A\" is equal to some fixed set \"A\" for each \"i\" ∈ \"I\", the disjoint union is the Cartesian product of \"A\" and \"I\":\n\nOne may occasionally see the notation\nfor the disjoint union of a family of sets, or the notation \"A\" + \"B\" for the disjoint union of two sets. This notation is meant to be suggestive of the fact that the cardinality of the disjoint union is the sum of the cardinalities of the terms in the family. Compare this to the notation for the Cartesian product of a family of sets. \n\nDisjoint unions are also sometimes written formula_13 or formula_14.\n\nIn the language of category theory, the disjoint union is the coproduct in the category of sets. It therefore satisfies the associated universal property. This also means that the disjoint union is the categorical dual of the Cartesian product construction. See coproduct for more details.\n\nFor many purposes, the particular choice of auxiliary index is unimportant, and in a simplifying abuse of notation, the indexed family can be treated simply as a collection of sets. In this case formula_15 is referred to as a \"copy\" of formula_16 and the notation formula_17 is sometimes used.\n\nIn category theory the disjoint union is defined as a coproduct in the category of sets. \n\nAs such, the disjoint union is defined up to an isomorphism, and the above definition is just one realization of the coproduct, among others. When the sets are pairwise disjoint, the usual union is another realization of the coproduct. This justifies the second definition in the lead.\n\nThis categorical aspect of the disjoint union explains why formula_18 is frequently used, instead of formula_19, to denote \"coproduct\".\n\n\n"}
{"id": "14615850", "url": "https://en.wikipedia.org/wiki?curid=14615850", "title": "Effective resolution bandwidth", "text": "Effective resolution bandwidth\n\nIn signal processing, effective resolution bandwidth is the frequency band of a signal converter where ENOB is still guaranteed. It is limited by dynamic quantization problems.\n"}
{"id": "9284", "url": "https://en.wikipedia.org/wiki?curid=9284", "title": "Equation", "text": "Equation\n\nIn mathematics, an equation is a statement of an equality containing one or more variables. \"Solving\" the equation consists of determining which values of the variables make the equality true. Variables are also called unknowns and the values of the unknowns that satisfy the equality are called solutions of the equation. There are two kinds of equations: identities and conditional equations. An identity is true for all values of the variable. A conditional equation is true for only particular values of the variables.\n\nAn equation is written as two expressions, connected by a equals sign (\"=\"). The expressions on the two sides of the equals sign are called the \"left-hand side\" and \"right-hand side\" of the equation.\n\nThe most common type of equation is an algebraic equation, in which the two sides are algebraic expressions.\nEach side of an algebraic equation will contain one or more terms. For example, the equation \nhas left-hand side formula_2, which has three terms, and right-hand side formula_3, consisting of just one term. The unknowns are \"x\" and \"y\" and the parameters are \"A\", \"B\", and \"C\".\n\nAn equation is analogous to a scale into which weights are placed. When equal weights of something (grain for example) are placed into the two pans, the two weights cause the scale to be in balance and are said to be equal. If a quantity of grain is removed from one pan of the balance, an equal amount of grain must be removed from the other pan to keep the scale in balance. Likewise, to keep an equation in balance, the same operations of addition, subtraction, multiplication and division must be performed on both sides of an equation for it to remain true.\n\nIn geometry, equations are used to describe geometric figures. As the equations that are considered, such as implicit equations or parametric equations, have infinitely many solutions, the objective is now different: instead of giving the solutions explicitly or counting them, which is impossible, one uses equations for studying properties of figures. This is the starting idea of algebraic geometry, an important area of mathematics.\n\nAlgebra studies two main families of equations: polynomial equations and, among them, the special case of linear equations. When there is only one variable, polynomial equations have the form \"P\"(\"x\") = 0, where \"P\" is a polynomial, and linear equations have the form \"ax\" + \"b\" = 0, where \"a\" and \"b\" are parameters. To solve equations from either family, one uses algorithmic or geometric techniques that originate from linear algebra or mathematical analysis. Algebra also studies Diophantine equations where the coefficients and solutions are integers. The techniques used are different and come from number theory. These equations are difficult in general; one often searches just to find the existence or absence of a solution, and, if they exist, to count the number of solutions.\n\nDifferential equations are equations that involve one or more functions and their derivatives. They are \"solved\" by finding an expression for the function that does not involve derivatives. Differential equations are used to model processes that involve the rates of change of the variable, and are used in areas such as physics, chemistry, biology, and economics.\n\nThe \"=\" symbol, which appears in every equation, was invented in 1557 by Robert Recorde, who considered that nothing could be more equal than parallel straight lines with the same length.\n\nAn equation is analogous to a weighing scale, balance, or seesaw.\n\nEach side of the equation corresponds to one side of the balance. Different quantities can be placed on each side: if the weights on the two sides are equal, the scale balances, and in analogy the equality that represents the balance is also balanced (if not, then the lack of balance corresponds to an inequality represented by an inequation).\n\nIn the illustration, \"x\", \"y\" and \"z\" are all different quantities (in this case real numbers) represented as circular weights, and each of \"x\", \"y\", and \"z\" has a different weight. Addition corresponds to adding weight, while subtraction corresponds to removing weight from what is already there. When equality holds, the total weight on each side is the same.\n\nEquations often contain terms other than the unknowns. These other terms, which are assumed to be \"known\", are usually called \"constants\", \"coefficients\" or \"parameters\".\n\nAn example of an equation involving \"x\" and \"y\" as unknowns and the parameter \"R\" is\n\nWhen \"R \"is chosen to have the value of 2 (\"R \"= 2), this equation would be recognized, when sketched in Cartesian coordinates, as the equation for a particular circle with a radius of 2. Hence, the equation with \"R\" unspecified is the general equation for the circle.\n\nUsually, the unknowns are denoted by letters at the end of the alphabet, \"x\", \"y\", \"z\", \"w\", …, while coefficients (parameters) are denoted by letters at the beginning, \"a\", \"b\", \"c\", \"d\", … . For example, the general quadratic equation is usually written \"ax\" + \"bx\" + \"c\" = 0. The process of finding the solutions, or, in case of parameters, expressing the unknowns in terms of the parameters is called solving the equation. Such expressions of the solutions in terms of the parameters are also called \"solutions\".\n\nA system of equations is a set of \"simultaneous equations\", usually in several unknowns, for which the common solutions are sought. Thus a \"solution to the system\" is a set of values for each of the unknowns, which together form a solution to each equation in the system. For example, the system\nhas the unique solution \"x\" = −1, \"y\" = 1.\n\nAn identity is an equation that is true for all possible values of the variable(s) it contains. Many identities are known in algebra and calculus. In the process of solving an equation, an identity is often used to simplify an equation making it more easily solvable.\n\nIn algebra, an example of an identity is the difference of two squares:\n\nwhich is true for all \"x\" and \"y\".\n\nTrigonometry is an area where many identities exist; these are useful in manipulating or solving trigonometric equations. Two of many that involve the sine and cosine functions are:\n\nand\n\nwhich are both true for all values of \"θ\".\n\nFor example, to solve for the value of \"θ\" that satisfies the equation:\n\nwhere \"θ\" is known to be limited to between 0 and 45 degrees, we may use the above identity for the product to give:\n\nyielding the solution for \"θ\"\n\nSince the sine function is a periodic function, there are infinitely many solutions if there are no restrictions on \"θ\". In this example, the restriction that \"θ\" be between 0 and 45 degrees implies there is only one solution.\n\nTwo equations or two systems of equations are \"equivalent\" if they have the same set of solutions. The following operations transform an equation or a system of equations into an equivalent one – provided that the operations are meaningful for the expressions they are applied to:\n\n\nIf some function is applied to both sides of an equation, the resulting equation has the solutions of the initial equation among its solutions, but may have further solutions called extraneous solutions. For example, the equation formula_12 has the solution formula_13 Raising both sides to the exponent of 2 (which means applying the function formula_14 to both sides of the equation) changes the equation to formula_15, which not only has the previous solution but also introduces the extraneous solution, formula_16 Moreover, if the function is not defined at some values (such as 1/\"x\", which is not defined for \"x\" = 0), solutions existing at those values may be lost. Thus, caution must be exercised when applying such a transformation to an equation.\n\nThe above transformations are the basis of most elementary methods for equation solving as well as some less elementary ones, like Gaussian elimination.\n\nIn general, an \"algebraic equation\" or polynomial equation is an equation of the form\n\nwhere \"P\" and \"Q\" are polynomials with coefficients in some field (real numbers, complex numbers, etc.), which is often the field of the rational numbers. An algebraic equation is \"univariate\" if it involves only one variable. On the other hand, a polynomial equation may involve several variables, in which case it is called \"multivariate\" (multiple variables, x, y, z, etc.). The term \"polynomial equation\" is usually preferred to \"algebraic equation\".\n\nFor example,\nis a univariate algebraic (polynomial) equation with integer coefficients and\nis a multivariate polynomial equation over the rational numbers.\n\nSome but not all polynomial equations with rational coefficients have a solution that is an algebraic expression with a finite number of operations involving just those coefficients (that is, it can be solved algebraically). This can be done for all such equations of degree one, two, three, or four; but for degree five or more it can be solved for some equations but, as the Abel–Ruffini theorem demonstrates, not for all. A large amount of research has been devoted to compute efficiently accurate approximations of the real or complex solutions of a univariate algebraic equation (see Root-finding algorithm) and of the common solutions of several multivariate polynomial equations (see System of polynomial equations).\n\nA system of linear equations (or \"linear system\") is a collection of linear equations involving the same set of variables. For example,\nis a system of three equations in the three variables . A solution to a linear system is an assignment of numbers to the variables such that all the equations are simultaneously satisfied. A solution to the system above is given by\nsince it makes all three equations valid. The word \"system\" indicates that the equations are to be considered collectively, rather than individually.\n\nIn mathematics, the theory of linear systems is the basis and a fundamental part of linear algebra, a subject which is used in most parts of modern mathematics. Computational algorithms for finding the solutions are an important part of numerical linear algebra, and play a prominent role in physics, engineering, chemistry, computer science, and economics. A system of non-linear equations can often be approximated by a linear system (see linearization), a helpful technique when making a mathematical model or computer simulation of a relatively complex system.\n\nIn Euclidean geometry, it is possible to associate a set of coordinates to each point in space, for example by an orthogonal grid. This method allows one to characterize geometric figures by equations. A plane in three-dimensional space can be expressed as the solution set of an equation of the form formula_23, where formula_24 and formula_25 are real numbers and formula_26 are the unknowns that correspond to the coordinates of a point in the system given by the orthogonal grid. The values formula_24 are the coordinates of a vector perpendicular to the plane defined by the equation. A line is expressed as the intersection of two planes, that is as the solution set of a single linear equation with values in formula_28 or as the solution set of two linear equations with values in formula_29\n\nA conic section is the intersection of a cone with equation formula_30 and a plane. In other words, in space, all conics are defined as the solution set of an equation of a plane and of the equation of a cone just given. This formalism allows one to determine the positions and the properties of the focuses of a conic.\n\nThe use of equations allows one to call on a large area of mathematics to solve geometric questions. The Cartesian coordinate system transforms a geometric problem into an analysis problem, once the figures are transformed into equations; thus the name analytic geometry. This point of view, outlined by Descartes, enriches and modifies the type of geometry conceived of by the ancient Greek mathematicians.\n\nCurrently, analytic geometry designates an active branch of mathematics. Although it still uses equations to characterize figures, it also uses other sophisticated techniques such as functional analysis and linear algebra.\n\nA Cartesian coordinate system is a coordinate system that specifies each point uniquely in a plane by a pair of numerical coordinates, which are the signed distances from the point to two fixed perpendicular directed lines, that are marked using the same unit of length.\n\nOne can use the same principle to specify the position of any point in three-dimensional space by the use of three Cartesian coordinates, which are the signed distances to three mutually perpendicular planes (or, equivalently, by its perpendicular projection onto three mutually perpendicular lines).\n\nThe invention of Cartesian coordinates in the 17th century by René Descartes (Latinized name: \"Cartesius\") revolutionized mathematics by providing the first systematic link between Euclidean geometry and algebra. Using the Cartesian coordinate system, geometric shapes (such as curves) can be described by Cartesian equations: algebraic equations involving the coordinates of the points lying on the shape. For example, a circle of radius 2 in a plane, centered on a particular point called the origin, may be described as the set of all points whose coordinates \"x\" and \"y\" satisfy the equation .\n\nA parametric equation for a curve expresses the coordinates of the points of the curve as functions of a variable, called a parameter. For example,\nare parametric equations for the unit circle, where \"t\" is the parameter. Together, these equations are called a parametric representation of the curve.\n\nThe notion of \"parametric equation\" has been generalized to surfaces, manifolds and algebraic varieties of higher dimension, with the number of parameters being equal to the dimension of the manifold or variety, and the number of equations being equal to the dimension of the space in which the manifold or variety is considered (for curves the dimension is \"one\" and \"one\" parameter is used, for surfaces dimension \"two\" and \"two\" parameters, etc.).\n\nA Diophantine equation is a polynomial equation in two or more unknowns for which only the integer solutions are sought (an integer solution is a solution such that all the unknowns take integer values). A linear Diophantine equation is an equation between two sums of monomials of degree zero or one. An example of linear Diophantine equation is where \"a\", \"b\", and \"c\" are constants. An exponential Diophantine equation is one for which exponents of the terms of the equation can be unknowns.\n\nDiophantine problems have fewer equations than unknown variables and involve finding integers that work correctly for all equations. In more technical language, they define an algebraic curve, algebraic surface, or more general object, and ask about the lattice points on it.\n\nThe word \"Diophantine\" refers to the Hellenistic mathematician of the 3rd century, Diophantus of Alexandria, who made a study of such equations and was one of the first mathematicians to introduce symbolism into algebra. The mathematical study of Diophantine problems that Diophantus initiated is now called Diophantine analysis.\n\nAn algebraic number is a number that is a solution of a non-zero polynomial equation in one variable with rational coefficients (or equivalently — by clearing denominators — with integer coefficients). Numbers such as that are not algebraic are said to be transcendental. Almost all real and complex numbers are transcendental.\n\nAlgebraic geometry is a branch of mathematics, classically studying solutions of polynomial equations. Modern algebraic geometry is based on more abstract techniques of abstract algebra, especially commutative algebra, with the language and the problems of geometry.\n\nThe fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves and quartic curves like lemniscates, and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.\n\nA differential equation is a mathematical equation that relates some function with its derivatives. In applications, the functions usually represent physical quantities, the derivatives represent their rates of change, and the equation defines a relationship between the two. Because such relations are extremely common, differential equations play a prominent role in many disciplines including physics, engineering, economics, and biology.\n\nIn pure mathematics, differential equations are studied from several different perspectives, mostly concerned with their solutions — the set of functions that satisfy the equation. Only the simplest differential equations are solvable by explicit formulas; however, some properties of solutions of a given differential equation may be determined without finding their exact form.\n\nIf a self-contained formula for the solution is not available, the solution may be numerically approximated using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.\n\nAn ordinary differential equation or ODE is an equation containing a function of one independent variable and its derivatives. The term \"ordinary\" is used in contrast with the term partial differential equation, which may be with respect to \"more than\" one independent variable.\n\nLinear differential equations, which have solutions that can be added and multiplied by coefficients, are well-defined and understood, and exact closed-form solutions are obtained. By contrast, ODEs that lack additive solutions are nonlinear, and solving them is far more intricate, as one can rarely represent them by elementary functions in closed form: Instead, exact and analytic solutions of ODEs are in series or integral form. Graphical and numerical methods, applied by hand or by computer, may approximate solutions of ODEs and perhaps yield useful information, often sufficing in the absence of exact, analytic solutions.\n\nA partial differential equation (PDE) is a differential equation that contains unknown multivariable functions and their partial derivatives. (This is in contrast to ordinary differential equations, which deal with functions of a single variable and their derivatives.) PDEs are used to formulate problems involving functions of several variables, and are either solved by hand, or used to create a relevant computer model.\n\nPDEs can be used to describe a wide variety of phenomena such as sound, heat, electrostatics, electrodynamics, fluid flow, elasticity, or quantum mechanics. These seemingly distinct physical phenomena can be formalised similarly in terms of PDEs. Just as ordinary differential equations often model one-dimensional dynamical systems, partial differential equations often model multidimensional systems. PDEs find their generalisation in stochastic partial differential equations.\n\nEquations can be classified according to the types of operations and quantities involved. Important types include:\n\n"}
{"id": "53465215", "url": "https://en.wikipedia.org/wiki?curid=53465215", "title": "Flatiron Institute", "text": "Flatiron Institute\n\nThe Flatiron Institute is an internal research division of the Simons Foundation, launched in 2016. It comprises three centers for computational science: the Center for Computational Astrophysics (CCA); the Center for Computational Biology (CCB); and the Center for Computational Quantum Physics (CCQ). It also has a group called the Scientific Computing Core (SCC). The institute takes its name from the Flatiron District in New York City where it's based.\n\nThe mission of the Flatiron Institute is to advance scientific research through computational methods, including data analysis, modeling, and simulation. The Flatiron Institute was dedicated with a ceremony on September 6, 2017.\n\n\n\n\n\n"}
{"id": "32267080", "url": "https://en.wikipedia.org/wiki?curid=32267080", "title": "Fraunhofer diffraction equation", "text": "Fraunhofer diffraction equation\n\nIn optics, the Fraunhofer diffraction equation is used to model the diffraction of waves when the diffraction pattern is viewed at a long distance from the diffracting object, and also when it is viewed at the focal plane of an imaging lens.\n\nThe equation was named in honour of Joseph von Fraunhofer although he was not actually involved in the development of the theory.\n\nThis article gives the equation in various mathematical forms, and provides detailed calculations of the Fraunhofer diffraction pattern for several different forms of diffracting apertures, specially for normally incident monochromatic plane wave. A qualitative discussion of Fraunhofer diffraction can be found elsewhere.\n\nWhen a beam of light is partly blocked by an obstacle, some of the light is scattered around the object, and light and dark bands are often seen at the edge of the shadow – this effect is known as diffraction. The Kirchhoff diffraction equation provides an expression, derived from the wave equation, which describes the wave diffracted by an aperture; analytical solutions to this equation are not available for most configurations.\n\nThe Fraunhofer diffraction equation is an approximation which can be applied when the diffracted wave is observed in the far field, and also when a lens is used to focus the diffracted light; in many instances, a simple analytical solution is available to the Fraunhofer equation – several of these are derived below.\n\n If the aperture is in plane, with the origin in the aperture and is illuminated by a monochromatic wave, of wavelength λ, wavenumber with complex amplitude , and the diffracted wave is observed in the plane where are the direction cosines of the point with respect to the origin, the complex amplitude of the diffracted wave is given by the Fraunhofer diffraction equation as:\n\nIt can be seen from this equation that the form of the diffraction pattern depends only on the direction of viewing, so the diffraction pattern changes in size but not in form with change of viewing distance.\n\nThe Fraunhofer diffraction equation can be expressed in a variety of mathematically equivalent forms. For example:\n\nIt can be seen that the integral in the above equations is the Fourier transform of the aperture function evaluated at frequencies\n\nThus, we can also write the equation in terms of a Fourier transform as:\n\nwhere is the Fourier transform of . The Fourier transform formulation can be very useful in solving diffraction problems.\n\nAnother form is:\n\nwhere represent the observation point and a point in the aperture respectively, and represent the wave vectors of the disturbance at the aperture and of the diffracted waves respectively, and represents the magnitude of the disturbance at the aperture.\n\nWhen the diffracting aperture has circular symmetry, it is useful to use polar rather than Cartesian co-ordinates.\n\nA point in the aperture has co-ordinates giving:\n\nand\n\nThe complex amplitude at is given by , and the area converts to \"ρ\"′ d\"ρ\"′ d\"ω\"′, giving\n\nUsing the integral representation of the Bessel function:\n\nwe have\n\nwhere the integration over gives since the equation is circularly symmetric, i.e. there is no dependence on .\n\nIn this case, we have equal to the Fourier–Bessel or Hankel transform of the aperture function, \n\nHere are given examples of Fraunhofer diffraction with a normally incident monochromatic plane wave.\n\nIn each case, the diffracting object is located in the \"z\" = 0 plane, and the complex amplitude of the incident plane wave is given by\n\nwhere\n\nand the phase is zero at time = 0.\n\nThe time dependent factor is omitted throughout the calculations, as it remains constant, and is averaged out when the intensity is calculated. The intensity at is proportional to the amplitude times its complex conjugate\n\nThese derivations can be found in most standard optics books, in slightly different forms using varying notations. A reference is given for each of the systems modelled here. The Fourier transforms used can be found here.\n\nThe aperture is a slit of width which is located along the -axis,\n\n\nAssuming the centre of the slit is located at , the first equation above, for all values of , is:\n\nUsing Euler's formula, this can be simplified to:\n\nwhere . It should be noted that the sinc function is sometimes defined as and this may cause confusion when looking at derivations in different texts.\n\nThis can also be written as:\n\nwhere is the angle between \"z\"-axis and the line joining x to the origin and when .\n\n\nThe slit can be represented by the rect function as:\n\nThe Fourier transform of this function is given by\n\nwhere is the Fourier transform frequency, and the function is here defined as sin(\"x\")/(\"x\")\n\nThe Fourier transform frequency here is , giving\n\nNote that the function is here defined as sin(\"x\")/(\"x\") to maintain consistency.\n\n\nThe intensity is proportional to the square of the amplitude, and is then\n\nWhen a slit of width \"W\" and height \"H\" is illuminated normally by a monochromatic plane wave of wavelength λ, the complex amplitude can be found using similar analyses to those in the previous section, applied over two independent dimensions as:\n\nThe intensity is given by\n\nwhere and are the angles between the and axes and the and axes, respectively.\n\nIn practice, all slits are of finite length and will therefore produce diffraction on both directions. If the length of the slit is much greater than its width, then the spacing of the horizontal diffraction fringes will be much less than the spacing of the vertical fringes. If the illuminating beam does not illuminate the whole length of the slit, the spacing of the horizontal fringes is determined by the dimensions of the laser beam. Close examination of the two-slit pattern below shows that there are very fine horizontal diffraction fringes above and below the main spot, as well as the more obvious vertical fringes.\n\nThe aperture has diameter . The complex amplitude in the observation plane is given by\n\n\nUsing the recurrence relationship\n\nto give\n\nIf we substitute\n\nand the limits of the integration become 0 and , we get\n\nPutting = sin , we get\n\n\nWe can write the aperture function as a step function\n\nThe Fourier–Bessel transform for this function is given by the relationship\n\nwhere is the transform frequency which is equal to and = .\n\nThus, we get \n\n\nThe intensity is given by:\n\nThis known as the Airy diffraction pattern\n\nThe diffracted pattern is symmetric about the normal axis.\n\nAn aperture with a Gaussian profile, for example, a photographic slide whose transmission has a Gaussian variation, so that the amplitude at a point in the aperture located at a distance \"r\"' from the origin is given by\n\ngiving\n\n\nThe Fourier–Bessel or Hankel transform is defined as\n\nwhere \"J\" is the Bessel function of the first kind of order ν with ν ≥ −1/2.\n\nThe Hankel transform is\n\ngiving\n\nand \n\n\nThe intensity is given by:\n\nThis function is plotted on the right, and it can be seen that, unlike the diffraction patterns produced by rectangular or circular apertures, it has no secondary rings. This can be used in a process called apodization - the aperture is covered by a filter whose transmission varies as a Gaussian function, giving a diffraction pattern with no secondary rings.:\n\nThe pattern which occurs when light diffracted from two slits overlaps is of considerable interest in physics, firstly for its importance in establishing the wave theory of light through Young's interference experiment, and secondly because of its role as a thought experiment in double-slit experiment in quantum mechanics.\n\nAssume we have two long slits illuminated by a plane wave of wavelength . The slits are in the plane, parallel to the axis, separated by a distance and are symmetrical about the origin. The width of the slits is small compared with the wavelength.\n\n\nThe incident light is diffracted by the slits into uniform spherical waves. The waves travelling in a given direction from the two slits have differing phases. The phase of the waves from the upper and lower slits relative to the origin is given by and \n\nThe complex amplitude of the summed waves is given by:\n\n\nThe aperture can be represented by the function:\n\nwhere is the delta function.\n\nWe have\n\nand\n\ngiving\n\nThis is the same expression as that derived above by integration.\n\n\nThis gives the intensity of the combined waves as:\n\n The width of the slits, is finite.\n\n\nThe diffracted pattern is given by:\n\n\nThe aperture function is given by:\n\nThe Fourier transform of this function is given by\n\nwhere is the Fourier transform frequency, and the function is here defined as sin(\"πx\")/(\"πx\")\n\nand\n\nWe have\n\nor\n\nThis is the same expression as was derived by integration.\n\n\nThe intensity is given by:\n\nIt can be seen that the form of the intensity pattern is the product of the individual slit diffraction pattern, and the interference pattern which would be obtained with slits of negligible width. This is illustrated in the image at the right which shows single slit diffraction by a laser beam, and also the diffraction/interference pattern given by two identical slits.\n\nA grating is defined in Born and Wolf as \"any arrangement which imposes on an incident wave a periodic variation of amplitude or phase, or both\".\n\nA simple grating consists of a screen with N slits whose width is significantly less than the wavelength of the incident light with slit separation of .\n\n\nThe complex amplitude of the diffracted wave at an angle is given by:\n\nformula_56\n\nsince this is the sum of a geometric series.\n\n\nThe aperture is given by\n\nThe Fourier transform of this function is:\n\n\nThe intensity is given by:\n\nformula_59\n\nThis function has a series of maxima and minima. There are regularly spaced \"principal maxima\", and a number of much smaller maxima in between the principal maxima. The principal maxima occur when\n\nand the main diffracted beams therefore occur at angles:\n\nThis is the grating equation for normally incident light.\n\nThe number of small intermediate maxima is equal to the number of slits,  − 1 and their size and shape is also determined by .\n\nThe form of the pattern for =50 is shown in the first figure .\n\nThe detailed structure for 20 and 50 slits gratings are illustrated in the second diagram.\n\nThe grating now has \"N\" slits of width and spacing \n\n\nThe amplitude is given by:\n\n\nThe aperture function can be written as:\n\nUsing the convolution theorem, which says that if we have two functions and , and we have\n\nwhere ∗ denotes the convolution operation, then we also have\n\nwe can write the aperture function as\n\nThe amplitude is then given by the Fourier transform of this expression as:\n\n\nThe intensity is given by:\n\nThe diagram shows the diffraction pattern for a grating with 20 slits, where the width of the slits is 1/5th of the slit separation. The size of the main diffracted peaks is modulated with the diffraction pattern of the individual slits.\n\nThe Fourier transform method above can be used to find the form of the diffraction for any periodic structure where the Fourier transform of the structure is known. Goodman uses this method to derive expressions for the diffraction pattern obtained with sinusoidal amplitude and phase modulation gratings. These are of particular interest in holography.\n\nIf the aperture is illuminated by a mono-chromatic plane wave incident in a direction , the first version of the Fraunhofer equation above becomes:\n\nThe equations used to model each of the systems above are altered only by changes in the constants multiplying and , so the diffracted light patterns will have the form, except that they will now be centred around the direction of the incident plane wave.\n\nThe grating equation becomes\n\nIn all of the above examples of Fraunhofer diffraction, the effect of increasing the wavelength of the illuminating light is to reduce the size of the diffraction structure, and conversely, when the wavelength is reduced, the size of the pattern increases. If the light is not mono-chromatic, i.e. it consists of a range of different wavelengths, each wavelength is diffracted into a pattern of a slightly different size to its neighbours. If the spread of wavelengths is significantly smaller than the mean wavelength, the individual patterns will vary very little in size, and so the basic diffraction will still appear with slightly reduced contrast. As the spread of wavelengths is increased, the number of \"fringes\" which can be observed is reduced.\n\n\n"}
{"id": "5723289", "url": "https://en.wikipedia.org/wiki?curid=5723289", "title": "Full employment theorem", "text": "Full employment theorem\n\nIn computer science and mathematics, a full employment theorem is a theorem which states that no algorithm can optimally perform a particular task done by some class of professionals. The name arises because such a theorem ensures that there is endless scope to keep discovering new techniques to improve the way at least some specific task is done.\n\nFor example, the \"full employment theorem for compiler writers\" states that there is no such thing as a provably perfect size-optimizing compiler, as such a proof for the compiler would have to detect non-terminating computations and reduce them to a one-instruction infinite loop. Thus, the existence of a provably perfect size-optimizing compiler would imply a solution to the halting problem, which cannot exist, making the proof itself an undecidable problem. This also implies that there may always be a better compiler since the proof that one has the best compiler cannot exist. Therefore, compiler writers will always be able to speculate that they have something to improve. Similarly, Gödel's incompleteness theorems have been called full employment theorems for mathematicians. In theoretical computer science this field of study is known as Kolmogorov complexity, or the smallest program which outputs a given string.\n\nTasks such as virus writing and detection, and spam filtering and filter-breaking are also subject to Rice's theorem.\n\n\n"}
{"id": "56600253", "url": "https://en.wikipedia.org/wiki?curid=56600253", "title": "Group-scheme action", "text": "Group-scheme action\n\nIn algebraic geometry, a action of a group scheme is a generalization of a group action to a group scheme. Precisely, given a group \"S\"-scheme \"G\", a left action of \"G\" on an \"S\"-scheme \"X\" is an \"S\"-morphism\nsuch that\n\nA right action of \"G\" on \"X is defined analogously. A scheme equipped with a left or right action of a group scheme \"G\" is called a G\"-scheme. An equivariant morphism between \"G\"-schemes is a morphism of schemes that intertwines the respective \"G\"-actions.\n\nMore generally, one can also consider (at least some special case of) an action of a group functor: viewing \"G\" as a functor, an action is given as a natural transformation satisfying the conditions analogous to the above. Alternatively, some authors study group action in the language of a groupoid; a group-scheme action is then an example of a groupoid scheme.\n\nThe usual constructs for a group action such as orbits generalize to a group-scheme action. Let formula_6 be a given group-scheme action as above.\n\n\nUnlike a set-theoretic group action, there is no straightforward way to construct a quotient for a group-scheme action. One exception is the case when the action is free, the case of a principal fiber bundle.\n\nThere are several approaches to overcome this difficulty:\n\nDepending on applications, another apppraoch would be to shift the focus away from a space then onto stuff on a space; e.g., topos. So the problem shifts from the classification of orbits to that of equivariant objects.\n\n"}
{"id": "41497810", "url": "https://en.wikipedia.org/wiki?curid=41497810", "title": "Hook length formula", "text": "Hook length formula\n\nIn combinatorial mathematics, the hook-length formula is a formula for the number of standard Young tableaux whose shape is a given Young diagram.\nIt has applications in diverse areas such as representation theory, probability, and algorithm analysis; for example, the problem of longest increasing subsequences.\n\nLet formula_1 be a partition of formula_2.\nIt is customary to interpret formula_3 graphically as a Young diagram, namely a left-justified array of square cells with formula_4 rows and formula_5 cells in the formula_6th row for each formula_7.\nA standard Young tableau of shape formula_3 is a Young diagram of shape formula_3 in which each of the formula_2 cells contains a distinct integer between 1 and formula_2 (i.e., no repetition), such that each row and each column form increasing sequences.\nFor each cell of the Young diagram in coordinates formula_12 (that is, the cell in the formula_6th row and formula_14th column), the hook formula_15 is the set of cells formula_16 such that formula_17 and formula_18 or formula_19 and formula_20.\nThe hook-length formula_21 is the number of cells in the hook formula_15.\n\nThen the hook-length formula expresses the number of standard Young tableaux of shape formula_3, sometimes denoted by formula_24, as\nwhere the product is over all cells formula_12 of formula_3.\n\nThe figure on the right shows hook-lengths for all cells in the Young diagram of the partition 9 = 4 + 3 + 1 + 1. Then the number of standard Young tableaux formula_24 for this Young diagram can be computed as\n\nThere are other formulas for formula_24, but the hook-length formula is particularly simple and elegant.\nThe hook-length formula was discovered in 1954 by J. S. Frame, G. de B. Robinson, and R. M. Thrall by improving a less convenient formula expressing formula_24 in terms of a determinant.\n\nThis earlier formula was deduced independently by G. Frobenius and A. Young in 1900 and 1902 respectively using algebraic methods.\nP. A. MacMahon found an alternate proof for the Young–Frobenius formula in 1916 using difference methods.\nDespite the simplicity of the hook-length formula, the Frame–Robinson–Thrall proof is uninsightful and does not provide an intuitive argument as to why hooks appear in the formula.\nThe search for a short, intuitive explanation befitting such a simple result gave rise to many alternate proofs for the hook-length formula.\n\nA. P. Hillman and R. M. Grassl gave the first proof that illuminates the role of hooks in 1976 by proving a special case of the Stanley hook-content formula, which is known to imply the hook-length formula.\n\nC. Greene, A. Nijenhuis, and H. S. Wilf found a probabilistic proof using the hook walk in which the hook lengths appear naturally in 1979.\n\nJ. B. Remmel adapted the original Frame–Robinson–Thrall proof into the first bijective proof for the hook-length formula in 1982.\n\nA direct bijective proof was first discovered by D. S. Franzblau and D. Zeilberger in 1982.\n\nD. Zeilberger also converted the Greene–Nijenhuis–Wilf hook walk proof into a bijective proof in 1984.\n\nA simpler direct bijective proof was announced by Igor Pak and Alexander V. Stoyanovskii in 1992, and its complete proof was presented by the pair and Jean-Christophe Novelli in 1997.\n\nMeanwhile, the hook-length formula has been generalized in several ways.\nR. M. Thrall found the analogue to the hook-length formula for shifted Young Tableaux in 1952.\n\nB. E. Sagan gave a shifted hook walk proof for the hook-length formula for shifted Young tableaux in 1980.\n\nB. E. Sagan and Y. N. Yeh proved the hook-length formula for binary trees using the hook walk in 1989.\n\nThe hook-length formula can be understood intuitively using the following heuristic, but incorrect, argument suggested by D. E. Knuth.\nGiven that each element of a tableau is the smallest in its hook and filling the tableau shape at random, the probability that cell formula_12 will contain the minimum element of the corresponding hook is the reciprocal of the hook length. Multiplying these probabilities over all formula_6 and formula_14 gives the formula. This argument is fallacious since the events are not independent.\n\nKnuth's argument is however correct for the enumeration of labellings on trees satisfying monotonicity properties analogous to those of a Young tableau. In this case, the 'hook' events in question are in fact independent events.\n\nThis is a probabilistic proof found by C. Greene, A. Nijenhuis, and H. S. Wilf in 1979. Here is a sketch of the proof. Define\n\nwe would like to show that formula_36. The first observation about formula_24 is\n\nwhere formula_39 denotes that formula_40 are Young tableau obtained from formula_41 by deleting one corner cell from formula_3. The sum is taken over all such formula_40. Here we are taking the convention that formula_44, where formula_45 denotes the empty diagram. The explanation for the above equation is that the maximal entry of the Young tableau of shape formula_3 occurs at one of its corner cells. By deleting that cell we will obtain a Young tableau of shape formula_40. Since the number of Young tableau of shape formula_40 is formula_49, taking the sum over all such formula_40 we get the equation.\n\nNotice that we also have formula_51. Therefore, it is enough to show that\n\nand the result formula_53 then follows by induction. The above sum can also be viewed as a sum of probabilities by rewriting the equation to be shown as\n\nWe therefore need to show that the numbers formula_55 define a probability measure on the set of Young diagrams formula_40 (where formula_39). This is done in a constructive way by defining a random walk, called the hook walk, on the cells of the Young diagram formula_3, which eventually selects one of the corner cells of formula_3 (which are in bijection with diagrams formula_40 for which formula_39). The hook walk is defined by the following rules.\n\n(1) Pick a cell uniformly at random from formula_62 cells. Start the random walk from there.\n\n(2) Successor of current cell formula_12 is chosen uniformly at random from the hook formula_64.\n\n(3) Continue until you reach at one of the corner cells, call it formula_65.\n\nProposition: For any corner cell formula_16 of formula_3 we have\n\nwhere formula_69.\n\nOnce we have the above proposition, taking the sum over all possible corner cells formula_70 we have formula_71, as claimed.\n\nThe hook-length formula is of great importance in the representation theory of the symmetric group formula_72, where the number formula_24 is known to be equal to the dimension of the complex irreducible representation formula_74 associated to formula_3, and is frequently denoted by formula_76, formula_77 or formula_78.\n\nThe complex irreducible representations formula_79 of the symmetric group are indexed by partitions formula_41 of formula_81 (for an explicit construction see Specht module) . Their characters are related to the theory of symmetric functions via the Hall inner product in the following formula\n\nwhere formula_83 is the Schur function associated to formula_41 and formula_85 is the power-sum symmetric function of the partition formula_86 associated to the cycle decomposition of formula_87. For example, if formula_88 then formula_89.\n\nSince the identity permutation formula_90 has the form formula_91 in cycle notation, formula_92. Then the formula says\n\nConsidering the expansion of Schur functions in terms of monomial symmetric functions using the Kostka numbers\n\nthe inner product with formula_95 is formula_96, because formula_97. Note that formula_98 is equal to formula_99. Hence\n\nAn immediate consequence of this is \nThe above equality is also a simple consequence of the Robinson–Schensted–Knuth correspondence.\n\nThe computation also shows that:\n\nWhich is the expansion of formula_103 in terms of Schur functions using the coefficients given by the inner product, because formula_104.\nThe above equality can be proven also checking the coefficients of each monomial at both sides and using the Robinson–Schensted–Knuth correspondence or, more conceptually, looking at the decomposition of formula_105 by irreducible formula_106 modules, and taking characters. See Schur–Weyl duality.\n\nBy the above considerations\nSo that \nwhere formula_109 is the Vandermonde determinant.\n\nFor a given partition formula_110 define formula_111 for formula_112. For the following we need at least as many variables as rows in the partition, so from now on we work with formula_113 variables formula_114.\n\nEach term formula_115 is equal to\n\nSee Schur function. Since the vector formula_117 is different for each partition, this means that the coefficient of formula_118 in formula_119, denoted formula_120, is equal to formula_121. This is known as the Frobenius Character Formula, which gives one of the earliest proofs.\nAll that remains is tracking that coefficient with a mixture of cleverness and brute force: Multiplying\n\nand\n\nwe conclude that the coefficient that we are looking for is\n\nwhich can be written as\n\nThe latter sum is equal to the following determinant\n\nwhich column reduces to the Vandermonde determinant, and we obtain the formula\n\nNote that formula_128 is the hook length of the first box in each row of the Young Diagram. Transforming this expression into the form formula_129 claimed by the hook-length formula is a fairly simple exercise in combinatorics: For any given formula_130, one has to argue that formula_131, where the latter product ranges over all cells formula_132 in the formula_6-row of the Young diagram of formula_3.\n\nThe hook length formula also has important applications to the analysis of longest increasing subsequences in random permutations. If formula_135 denotes a uniformly random permutation of order formula_2, formula_137 denotes the maximal length of an increasing subsequence of formula_135, and formula_139 denotes the expected (average) value of formula_137, Anatoly Vershik and Sergei Kerov and independently Benjamin F. Logan and Lawrence A. Shepp showed that when formula_2 is large, formula_139 is approximately equal to formula_143. This answers a question originally posed by Stanislaw Ulam. The proof is based on translating the question via the Robinson–Schensted correspondence to a problem about the limiting shape of a random Young tableau chosen according to Plancherel measure. Since the definition of Plancherel measure involves the quantity formula_24, the hook length formula can then be used to perform an asymptotic analysis of the limit shape and thereby also answer the original question.\n\nThe ideas of Vershik–Kerov and Logan–Shepp were later refined by Jinho Baik, Percy Deift and Kurt Johansson, who were able to achieve a much more precise analysis of the limiting behavior of the maximal increasing subsequence length, proving an important result now known as the Baik–Deift–Johansson theorem. Their analysis again makes crucial use of the fact that formula_24 has a number of good formulas, although instead of the hook length formula it made use of one of the determinantal expressions.\n\nThe formula for the number of Young tableau of a given shape was originally derived from the Frobenius determinant formula in connection to representation theory. If the shape of a Young diagram is given by the row lengths formula_146, then the number of tableau with that shape is given by\n\nHook lengths can also be used to give a product representation to the generating function for the number of reverse plane partitions of a given shape. If is a partition of some integer , a reverse plane partition of with shape is obtained by filling in the boxes in the Young diagram with non-negative integers such that the entries add to and are non-decreasing along each row and down each column. The hook lengths formula_148 can be defined as with Young tableau. If denotes the number of reverse plane partitions of with shape , then the generating function can be written as\n\nStanley discovered another formula for the same generating function. In general, if formula_150 is any poset with formula_81 elements, the generating function for reverse formula_150-partitions is \n\nwhere formula_154 is a polynomial such that formula_155 is the number of natural labelings of formula_150.\n\nIn the case of a partition formula_41, we are considering the poset in its cells given by the relation \nSo a natural labeling is simply a standard Young tableau, i.e. formula_159\n\nCombining the two formulas for the generating functions we have\n\nBoth sides converge inside the disk of radius one and the following expression makes sense for formula_161\n\nIt would be violent to plug in 1, but the right hand side is a continuous function inside the unit disk and a polynomial is continuous everywhere so at least we can say\n\nFinally, applying L'Hopital's rule formula_81 times yields the hook length formula\n\nSpecializing the schur functions to the variables formula_166 there is the formula\n\nThe number formula_168 is defined as \n\nwhere formula_170 is the conjugate partition\n\nThere is a generalization of this formula for skew shapes,\n\nwhere the sum is taken over \"excited diagrams\" of shape formula_3 and boxes distributed according to formula_40.\n\nThe Catalan numbers are ubiquitous in enumerative combinatorics. Not surprisingly, they are also part of this story:\n\nLets briefly mention why. When doing a Dyck path we may go up (U) or down (D). So for any Dyck path of length formula_81 consider the tableaux of shape formula_176 such that the first row is given by the numbers formula_177 such that the formula_177-th step was up and in the second row given by the positions in which it goes down. For example, UUDDUD correspond to the tableaux with rows 125 and 346.\n\nThe hook formula gives another way of getting a closed formula for the Catalan numbers\n\n\n"}
{"id": "14627", "url": "https://en.wikipedia.org/wiki?curid=14627", "title": "Isaac Newton", "text": "Isaac Newton\n\nSir Isaac Newton (25 December 1642 – 20 March 1726/27) was an English mathematician, astronomer, theologian, author and physicist (described in his own day as a \"natural philosopher\") who is widely recognised as one of the most influential scientists of all time, and a key figure in the scientific revolution. His book \"Philosophiæ Naturalis Principia Mathematica\" (\"Mathematical Principles of Natural Philosophy\"), first published in 1687, laid the foundations of classical mechanics. Newton also made seminal contributions to optics, and shares credit with Gottfried Wilhelm Leibniz for developing the infinitesimal calculus.\n\nIn \"Principia\", Newton formulated the laws of motion and universal gravitation that formed the dominant scientific viewpoint until it was superseded by the theory of relativity. Newton used his mathematical description of gravity to prove Kepler's laws of planetary motion, account for tides, the trajectories of comets, the precession of the equinoxes and other phenomena, eradicating doubt about the Solar System's heliocentricity. He demonstrated that the motion of objects on Earth and celestial bodies could be accounted for by the same principles. Newton's inference that the Earth is an oblate spheroid was later confirmed by the geodetic measurements of Maupertuis, La Condamine, and others, convincing most European scientists of the superiority of Newtonian mechanics over earlier systems.\n\nNewton built the first practical reflecting telescope and developed a sophisticated theory of colour based on the observation that a prism separates white light into the colours of the visible spectrum. His work on light was collected in his highly influential book \"Opticks\", published in 1704. He also formulated an empirical law of cooling, made the first theoretical calculation of the speed of sound, and introduced the notion of a Newtonian fluid. In addition to his work on calculus, as a mathematician Newton contributed to the study of power series, generalised the binomial theorem to non-integer exponents, developed a method for approximating the roots of a function, and classified most of the cubic plane curves.\n\nNewton was a fellow of Trinity College and the second Lucasian Professor of Mathematics at the University of Cambridge. He was a devout, but unorthodox, Christian who privately rejected the doctrine of the Trinity. Unusually for a member of the Cambridge faculty of the day, he refused to take holy orders in the Church of England. Beyond his work on the mathematical sciences, Newton dedicated much of his time to the study of alchemy and biblical chronology, but most of his work in those areas remained unpublished until long after his death. Politically and personally tied to the Whig party, Newton served two brief terms as Member of Parliament for the University of Cambridge, in 1689–90 and 1701–02. He was knighted by Queen Anne in 1705 and spent the last three decades of his life in London, serving as Warden (1696–1700) and Master (1700–1727) of the Royal Mint, as well as president of the Royal Society (1703–1727).\n\nIsaac Newton was born (according to the Julian calendar, in use in England at the time) on Christmas Day, 25 December 1642 (NS 4 January 1643) \"an hour or two after midnight\", at Woolsthorpe Manor in Woolsthorpe-by-Colsterworth, a hamlet in the county of Lincolnshire. His father, also named Isaac Newton, had died three months before. Born prematurely, Newton was a small child; his mother Hannah Ayscough reportedly said that he could have fit inside a quart mug. When Newton was three, his mother remarried and went to live with her new husband, the Reverend Barnabas Smith, leaving her son in the care of his maternal grandmother, Margery Ayscough. Newton disliked his stepfather and maintained some enmity towards his mother for marrying him, as revealed by this entry in a list of sins committed up to the age of 19: \"Threatening my father and mother Smith to burn them and the house over them.\" Newton's mother had three children from her second marriage.\n\nFrom the age of about twelve until he was seventeen, Newton was educated at The King's School, Grantham, which taught Latin and Greek and probably imparted a significant foundation of mathematics. He was removed from school, and returned to Woolsthorpe-by-Colsterworth by October 1659. His mother, widowed for the second time, attempted to make him a farmer, an occupation he hated. Henry Stokes, master at The King's School, persuaded his mother to send him back to school. Motivated partly by a desire for revenge against a schoolyard bully, he became the top-ranked student, distinguishing himself mainly by building sundials and models of windmills.\n\nIn June 1661, he was admitted to Trinity College, Cambridge, on the recommendation of his uncle Rev William Ayscough, who had studied there. He started as a subsizar—paying his way by performing valet's duties—until he was awarded a scholarship in 1664, guaranteeing him four more years until he could get his MA. At that time, the college's teachings were based on those of Aristotle, whom Newton supplemented with modern philosophers such as Descartes, and astronomers such as Galileo and Thomas Street, through whom he learned of Kepler's work. He set down in his notebook a series of \"\"Quaestiones\"\" about mechanical philosophy as he found it. In 1665, he discovered the generalised binomial theorem and began to develop a mathematical theory that later became calculus. Soon after Newton had obtained his BA degree in August 1665, the university temporarily closed as a precaution against the Great Plague. Although he had been undistinguished as a Cambridge student, Newton's private studies at his home in Woolsthorpe over the subsequent two years saw the development of his theories on calculus, optics, and the law of gravitation.\n\nIn April 1667, he returned to Cambridge and in October was elected as a fellow of Trinity. Fellows were required to become ordained priests, although this was not enforced in the restoration years and an assertion of conformity to the Church of England was sufficient. However, by 1675 the issue could not be avoided and by then his unconventional views stood in the way. Nevertheless, Newton managed to avoid it by means of a special permission from Charles II (see \"Middle years\" section below).\n\nHis studies had impressed the Lucasian professor Isaac Barrow, who was more anxious to develop his own religious and administrative potential (he became master of Trinity two years later); in 1669 Newton succeeded him, only one year after receiving his MA. He was elected a Fellow of the Royal Society (FRS) in 1672.\n\nNewton's work has been said \"to distinctly advance every branch of mathematics then studied\". His work on the subject usually referred to as fluxions or calculus, seen in a manuscript of October 1666, is now published among Newton's mathematical papers. The author of the manuscript \"De analysi per aequationes numero terminorum infinitas\", sent by Isaac Barrow to John Collins in June 1669, was identified by Barrow in a letter sent to Collins in August of that year as: \"Mr Newton, a fellow of our College, and very young ... but of an extraordinary genius and proficiency in these things.\"\n\nNewton later became involved in a dispute with Leibniz over priority in the development of calculus (the Leibniz–Newton calculus controversy). Most modern historians believe that Newton and Leibniz developed calculus independently, although with very different notations. Occasionally it has been suggested that Newton published almost nothing about it until 1693, and did not give a full account until 1704, while Leibniz began publishing a full account of his methods in 1684. (Leibniz's notation and \"differential Method\", nowadays recognised as much more convenient notations, were adopted by continental European mathematicians, and after 1820 or so, also by British mathematicians.) But such a suggestion fails to account for the calculus in Book 1 of Newton's \"Principia\" itself (published 1687) and in its forerunner manuscripts, such as \"De motu corporum in gyrum\" (\"On the motion of bodies in orbit\") of 1684; this content has been pointed out by critics of both Newton's time and modern times. The \"Principia\" is not written in the language of calculus either as we know it or as Newton's (later) 'dot' notation would write it. His work extensively uses calculus in geometric form based on limiting values of the ratios of vanishingly small quantities: in the \"Principia\" itself, Newton gave demonstration of this under the name of \"the method of first and last ratios\" and explained why he put his expositions in this form, remarking also that \"hereby the same thing is performed as by the method of indivisibles\".\n\nBecause of this, the \"Principia\" has been called \"a book dense with the theory and application of the infinitesimal calculus\" in modern times and \"lequel est presque tout de ce calcul\" ('nearly all of it is of this calculus') in Newton's time. His use of methods involving \"one or more orders of the infinitesimally small\" is present in his \"De motu corporum in gyrum\" of 1684 and in his papers on motion \"during the two decades preceding 1684\".\n\nNewton had been reluctant to publish his calculus because he feared controversy and criticism. He was close to the Swiss mathematician Nicolas Fatio de Duillier. In 1691, Duillier started to write a new version of Newton's \"Principia\", and corresponded with Leibniz. In 1693, the relationship between Duillier and Newton deteriorated and the book was never completed.\n\nStarting in 1699, other members of the Royal Society accused Leibniz of plagiarism. The dispute then broke out in full force in 1711 when the Royal Society proclaimed in a study that it was Newton who was the true discoverer and labelled Leibniz a fraud; it was later found that Newton wrote the study's concluding remarks on Leibniz. Thus began the bitter controversy which marred the lives of both Newton and Leibniz until the latter's death in 1716.\n\nNewton is generally credited with the generalised binomial theorem, valid for any exponent. He discovered Newton's identities, Newton's method, classified cubic plane curves (polynomials of degree three in two variables), made substantial contributions to the theory of finite differences, and was the first to use fractional indices and to employ coordinate geometry to derive solutions to Diophantine equations. He approximated partial sums of the harmonic series by logarithms (a precursor to Euler's summation formula) and was the first to use power series with confidence and to revert power series. Newton's work on infinite series was inspired by Simon Stevin's decimals.\n\nWhen Newton received his MA and became a Fellow of the \"College of the Holy and Undivided Trinity\" in 1667, he made the commitment that \"I will either set Theology as the object of my studies and will take holy orders when the time prescribed by these statutes [7 years] arrives, or I will resign from the college.\" Up until this point he had not thought much about religion and had twice signed his agreement to the thirty-nine articles, the basis of Church of England doctrine.\n\nHe was appointed Lucasian Professor of Mathematics in 1669 on Barrow's recommendation. During that time, any Fellow of a college at Cambridge or Oxford was required to take holy orders and become an ordained Anglican priest. However, the terms of the Lucasian professorship required that the holder \"not\" be active in the church (presumably so as to have more time for science). Newton argued that this should exempt him from the ordination requirement, and Charles II, whose permission was needed, accepted this argument. Thus a conflict between Newton's religious views and Anglican orthodoxy was averted.\n\nIn 1666, Newton observed that the spectrum of colours exiting a prism in the position of minimum deviation is oblong, even when the light ray entering the prism is circular, which is to say, the prism refracts different colours by different angles. This led him to conclude that colour is a property intrinsic to light—a point which had been debated in prior years.\n\nFrom 1670 to 1672, Newton lectured on optics. During this period he investigated the refraction of light, demonstrating that the multicoloured spectrum produced by a prism could be recomposed into white light by a lens and a second prism. Modern scholarship has revealed that Newton's analysis and resynthesis of white light owes a debt to corpuscular alchemy.\n\nHe showed that coloured light does not change its properties by separating out a coloured beam and shining it on various objects, and that regardless of whether reflected, scattered, or transmitted, the light remains the same colour. Thus, he observed that colour is the result of objects interacting with already-coloured light rather than objects generating the colour themselves. This is known as Newton's theory of colour.\n\nFrom this work, he concluded that the lens of any refracting telescope would suffer from the dispersion of light into colours (chromatic aberration). As a proof of the concept, he constructed a telescope using reflective mirrors instead of lenses as the objective to bypass that problem. Building the design, the first known functional reflecting telescope, today known as a Newtonian telescope, involved solving the problem of a suitable mirror material and shaping technique. Newton ground his own mirrors out of a custom composition of highly reflective speculum metal, using Newton's rings to judge the quality of the optics for his telescopes. In late 1668, he was able to produce this first reflecting telescope. It was about eight inches long and it gave a clearer and larger image. In 1671, the Royal Society asked for a demonstration of his reflecting telescope. Their interest encouraged him to publish his notes, \"Of Colours\", which he later expanded into the work \"Opticks\". When Robert Hooke criticised some of Newton's ideas, Newton was so offended that he withdrew from public debate. Newton and Hooke had brief exchanges in 1679–80, when Hooke, appointed to manage the Royal Society's correspondence, opened up a correspondence intended to elicit contributions from Newton to Royal Society transactions, which had the effect of stimulating Newton to work out a proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector (see Newton's law of universal gravitation – History and \"De motu corporum in gyrum\"). But the two men remained generally on poor terms until Hooke's death.\n\nNewton argued that light is composed of particles or corpuscles, which were refracted by accelerating into a denser medium. He verged on soundlike waves to explain the repeated pattern of reflection and transmission by thin films (Opticks Bk.II, Props. 12), but still retained his theory of 'fits' that disposed corpuscles to be reflected or transmitted (Props.13). However, later physicists favoured a purely wavelike explanation of light to account for the interference patterns and the general phenomenon of diffraction. Today's quantum mechanics, photons, and the idea of wave–particle duality bear only a minor resemblance to Newton's understanding of light.\n\nIn his \"Hypothesis of Light\" of 1675, Newton posited the existence of the ether to transmit forces between particles. The contact with the Cambridge Platonist philosopher Henry More revived his interest in alchemy. He replaced the ether with occult forces based on Hermetic ideas of attraction and repulsion between particles. John Maynard Keynes, who acquired many of Newton's writings on alchemy, stated that \"Newton was not the first of the age of reason: He was the last of the magicians.\" Newton's interest in alchemy cannot be isolated from his contributions to science. This was at a time when there was no clear distinction between alchemy and science. Had he not relied on the occult idea of action at a distance, across a vacuum, he might not have developed his theory of gravity. (See also Isaac Newton's occult studies.)\n\nIn 1704, Newton published \"Opticks\", in which he expounded his corpuscular theory of light. He considered light to be made up of extremely subtle corpuscles, that ordinary matter was made of grosser corpuscles and speculated that through a kind of alchemical transmutation \"Are not gross Bodies and Light convertible into one another, ... and may not Bodies receive much of their Activity from the Particles of Light which enter their Composition?\" Newton also constructed a primitive form of a frictional electrostatic generator, using a glass globe.\n\nIn an article entitled \"Newton, prisms, and the 'opticks' of tunable lasers\" it is indicated that Newton in his book \"Opticks\" was the first to show a diagram using a prism as a beam expander. In the same book he describes, via diagrams, the use of multiple-prism arrays. Some 278 years after Newton's discussion, multiple-prism beam expanders became central to the development of narrow-linewidth tunable lasers. Also, the use of these prismatic beam expanders led to the multiple-prism dispersion theory.\n\nSubsequent to Newton, much has been amended. Young and Fresnel combined Newton's particle theory with Huygens' wave theory to show that colour is the visible manifestation of light's wavelength. Science also slowly came to realise the difference between perception of colour and mathematisable optics. The German poet and scientist, Goethe, could not shake the Newtonian foundation but \"one hole Goethe did find in Newton's armour, ... Newton had committed himself to the doctrine that refraction without colour was impossible. He therefore thought that the object-glasses of telescopes must for ever remain imperfect, achromatism and refraction being incompatible. This inference was proved by Dollond to be wrong.\"\n\nIn 1679, Newton returned to his work on celestial mechanics by considering gravitation and its effect on the orbits of planets with reference to Kepler's laws of planetary motion. This followed stimulation by a brief exchange of letters in 1679–80 with Hooke, who had been appointed to manage the Royal Society's correspondence, and who opened a correspondence intended to elicit contributions from Newton to Royal Society transactions. Newton's reawakening interest in astronomical matters received further stimulus by the appearance of a comet in the winter of 1680–1681, on which he corresponded with John Flamsteed. After the exchanges with Hooke, Newton worked out proof that the elliptical form of planetary orbits would result from a centripetal force inversely proportional to the square of the radius vector (see Newton's law of universal gravitation – History and \"De motu corporum in gyrum\"). Newton communicated his results to Edmond Halley and to the Royal Society in \"De motu corporum in gyrum\", a tract written on about nine sheets which was copied into the Royal Society's Register Book in December 1684. This tract contained the nucleus that Newton developed and expanded to form the \"Principia\".\n\nThe \"Principia\" was published on 5 July 1687 with encouragement and financial help from Edmond Halley. In this work, Newton stated the three universal laws of motion. Together, these laws describe the relationship between any object, the forces acting upon it and the resulting motion, laying the foundation for classical mechanics. They contributed to many advances during the Industrial Revolution which soon followed and were not improved upon for more than 200 years. Many of these advancements continue to be the underpinnings of non-relativistic technologies in the modern world. He used the Latin word \"gravitas\" (weight) for the effect that would become known as gravity, and defined the law of universal gravitation.\n\nIn the same work, Newton presented a calculus-like method of geometrical analysis using 'first and last ratios', gave the first analytical determination (based on Boyle's law) of the speed of sound in air, inferred the oblateness of Earth's spheroidal figure, accounted for the precession of the equinoxes as a result of the Moon's gravitational attraction on the Earth's oblateness, initiated the gravitational study of the irregularities in the motion of the moon, provided a theory for the determination of the orbits of comets, and much more.\n\nNewton made clear his heliocentric view of the Solar System—developed in a somewhat modern way, because already in the mid-1680s he recognised the \"deviation of the Sun\" from the centre of gravity of the Solar System. For Newton, it was not precisely the centre of the Sun or any other body that could be considered at rest, but rather \"the common centre of gravity of the Earth, the Sun and all the Planets is to be esteem'd the Centre of the World\", and this centre of gravity \"either is at rest or moves uniformly forward in a right line\" (Newton adopted the \"at rest\" alternative in view of common consent that the centre, wherever it was, was at rest).\n\nNewton's postulate of an invisible force able to act over vast distances led to him being criticised for introducing \"occult agencies\" into science. Later, in the second edition of the \"Principia\" (1713), Newton firmly rejected such criticisms in a concluding General Scholium, writing that it was enough that the phenomena implied a gravitational attraction, as they did; but they did not so far indicate its cause, and it was both unnecessary and improper to frame hypotheses of things that were not implied by the phenomena. (Here Newton used what became his famous expression \"hypotheses non-fingo\").\n\nWith the \"Principia\", Newton became internationally recognised. He acquired a circle of admirers, including the Swiss-born mathematician Nicolas Fatio de Duillier.\n\nNewton found 72 of the 78 \"species\" of cubic curves and categorized them into four types. In 1717, and probably with Newton's help, James Stirling proved that every cubic was one of these four types. Newton also claimed that the four types could be obtained by plane projection from one of them, and this was proved in 1731, four years after his death.\n\nIn the 1690s, Newton wrote a number of religious tracts dealing with the literal and symbolic interpretation of the Bible. A manuscript Newton sent to John Locke in which he disputed the fidelity of and its fidelity to the original manuscripts of the New Testament, remained unpublished until 1785.\n\nScholars long debated whether Newton disputed the doctrine of the Trinity. His first biographer, Sir David Brewster, who compiled his manuscripts for over 20 years, interpreted Newton to be questioning the veracity of passages referring to this, but never denying the doctrine of the Trinity as such. In the twentieth century, encrypted manuscripts written by Newton and bought by John Maynard Keynes (among others) were deciphered and it became known that Newton did indeed reject Trinitarianism.\n\nLater works—\"The Chronology of Ancient Kingdoms Amended\" (1728) and \"Observations Upon the Prophecies of Daniel and the Apocalypse of St. John\" (1733)—were published after his death. He also devoted a great deal of time to alchemy (see above).\n\nNewton was also a member of the Parliament of England for Cambridge University in 1689–90 and 1701–2, but according to some accounts his only comments were to complain about a cold draught in the chamber and request that the window be closed. He was however noted by Cambridge diarist Abraham de la Pryme as having rebuked students who were frightening local residents by claiming that a house was haunted.\n\nNewton moved to London to take up the post of warden of the Royal Mint in 1696, a position that he had obtained through the patronage of Charles Montagu, 1st Earl of Halifax, then Chancellor of the Exchequer. He took charge of England's great recoining, somewhat treading on the toes of Lord Lucas, Governor of the Tower (and securing the job of deputy comptroller of the temporary Chester branch for Edmond Halley). Newton became perhaps the best-known Master of the Mint upon the death of Thomas Neale in 1699, a position Newton held for the last 30 years of his life. These appointments were intended as sinecures, but Newton took them seriously, retiring from his Cambridge duties in 1701, and exercising his power to reform the currency and punish clippers and counterfeiters.\n\nAs Warden, and afterwards Master, of the Royal Mint, Newton estimated that 20 percent of the coins taken in during the Great Recoinage of 1696 were counterfeit. Counterfeiting was high treason, punishable by the felon being hanged, drawn and quartered. Despite this, convicting even the most flagrant criminals could be extremely difficult. However, Newton proved equal to the task.\n\nDisguised as a habitué of bars and taverns, he gathered much of that evidence himself. For all the barriers placed to prosecution, and separating the branches of government, English law still had ancient and formidable customs of authority. Newton had himself made a justice of the peace in all the home counties—there is a draft of a letter regarding this matter stuck into Newton's personal first edition of his \"Philosophiæ Naturalis Principia Mathematica\" which he must have been amending at the time. Then he conducted more than 100 cross-examinations of witnesses, informers, and suspects between June 1698 and Christmas 1699. Newton successfully prosecuted 28 coiners.\n\nAs a result of a report written by Newton on 21 September 1717 to the Lords Commissioners of His Majesty's Treasury the bimetallic relationship between gold coins and silver coins was changed by Royal proclamation on 22 December 1717, forbidding the exchange of gold guineas for more than 21 silver shillings. This inadvertently resulted in a silver shortage as silver coins were used to pay for imports, while exports were paid for in gold, effectively moving Britain from the silver standard to its first gold standard. It is a matter of debate as whether he intended to do this or not. It has been argued that Newton conceived of his work at the Mint as a continuation of his alchemical work.\n\nNewton was made President of the Royal Society in 1703 and an associate of the French Académie des Sciences. In his position at the Royal Society, Newton made an enemy of John Flamsteed, the Astronomer Royal, by prematurely publishing Flamsteed's \"Historia Coelestis Britannica\", which Newton had used in his studies.\n\nIn April 1705, Queen Anne knighted Newton during a royal visit to Trinity College, Cambridge. The knighthood is likely to have been motivated by political considerations connected with the Parliamentary election in May 1705, rather than any recognition of Newton's scientific work or services as Master of the Mint. Newton was the second scientist to be knighted, after Sir Francis Bacon.\n\nNewton was one of many people who lost heavily when the South Sea Company collapsed. Their most significant trade was slaves, and according to his niece, he lost around £20,000.\n\nTowards the end of his life, Newton took up residence at Cranbury Park, near Winchester with his niece and her husband, until his death in 1727. His half-niece, Catherine Barton Conduitt, served as his hostess in social affairs at his house on Jermyn Street in London; he was her \"very loving Uncle\", according to his letter to her when she was recovering from smallpox.\n\nNewton died in his sleep in London on 20 March 1727 (OS 20 March 1726; NS 31 March 1727). His body was buried in Westminster Abbey. Voltaire may have been present at his funeral. A bachelor, he had divested much of his estate to relatives during his last years, and died intestate. His papers went to John Conduitt and Catherine Barton. After his death, Newton's hair was examined and found to contain mercury, probably resulting from his alchemical pursuits. Mercury poisoning could explain Newton's eccentricity in late life.\n\nAlthough it was claimed that he was once engaged, Newton never married. The French writer and philosopher Voltaire, who was in London at the time of Newton's funeral, said that he \"was never sensible to any passion, was not subject to the common frailties of mankind, nor had any commerce with women—a circumstance which was assured me by the physician and surgeon who attended him in his last moments\". The widespread belief that he died a virgin has been commented on by writers such as mathematician Charles Hutton, economist John Maynard Keynes, and physicist Carl Sagan.\n\nNewton had a close friendship with the Swiss mathematician Nicolas Fatio de Duillier, whom he met in London around 1689—some of their correspondence has survived. Their relationship came to an abrupt and unexplained end in 1693, and at the same time Newton suffered a nervous breakdown which included sending wild accusatory letters to his friends Samuel Pepys and John Locke—his note to the latter included the charge that Locke \"endeavoured to embroil me with woemen\".\n\nThe mathematician Joseph-Louis Lagrange said that Newton was the greatest genius who ever lived, and once added that Newton was also \"the most fortunate, for we cannot find more than once a system of the world to establish.\" English poet Alexander Pope wrote the famous epitaph:\n\nNature and nature's laws lay hid in night;<br>\nGod said \"Let Newton be\" and all was light.\n\nNewton was relatively modest about his achievements, writing in a letter to Robert Hooke in February 1676:\nIf I have seen further it is by standing on the shoulders of giants.\n\nTwo writers think that the above quotation, written at a time when Newton and Hooke were in dispute over optical discoveries, was an oblique attack on Hooke (said to have been short and hunchbacked), rather than—or in addition to—a statement of modesty. On the other hand, the widely known proverb about standing on the shoulders of giants, published among others by seventeenth-century poet George Herbert (a former orator of the University of Cambridge and fellow of Trinity College) in his \"Jacula Prudentum\" (1651), had as its main point that \"a dwarf on a giant's shoulders sees farther of the two\", and so its effect as an analogy would place Newton himself rather than Hooke as the 'dwarf'.\n\nIn a later memoir, Newton wrote:\nI do not know what I may appear to the world, but to myself I seem to have been only like a boy playing on the sea-shore, and diverting myself in now and then finding a smoother pebble or a prettier shell than ordinary, whilst the great ocean of truth lay all undiscovered before me.\n\nIn 1816, a tooth said to have belonged to Newton was sold for £730 ($3,633) in London to an aristocrat who had it set in a ring. The \"Guinness World Records 2002\" classified it as the most valuable tooth, which would value approximately £25,000 ($35,700) in late 2001. Who bought it and who currently has it has not been disclosed.\n\nAlbert Einstein kept a picture of Newton on his study wall alongside ones of Michael Faraday and James Clerk Maxwell. Newton remains influential to today's scientists, as demonstrated by a 2005 survey of members of Britain's Royal Society (formerly headed by Newton) asking who had the greater effect on the history of science, Newton or Einstein. Royal Society scientists deemed Newton to have made the greater overall contribution. In 1999, an opinion poll of 100 of today's leading physicists voted Einstein the \"greatest physicist ever;\" with Newton the runner-up, while a parallel survey of rank-and-file physicists by the site PhysicsWeb gave the top spot to Newton.\n\nNewton's monument (1731) can be seen in Westminster Abbey, at the north of the entrance to the choir against the choir screen, near his tomb. It was executed by the sculptor Michael Rysbrack (1694–1770) in white and grey marble with design by the architect William Kent. The monument features a figure of Newton reclining on top of a sarcophagus, his right elbow resting on several of his great books and his left hand pointing to a scroll with a mathematical design. Above him is a pyramid and a celestial globe showing the signs of the Zodiac and the path of the comet of 1680. A relief panel depicts putti using instruments such as a telescope and prism. The Latin inscription on the base translates as:Here is buried Isaac Newton, Knight, who by a strength of mind almost divine, and mathematical principles peculiarly his own, explored the course and figures of the planets, the paths of comets, the tides of the sea, the dissimilarities in rays of light, and, what no other scholar has previously imagined, the properties of the colours thus produced. Diligent, sagacious and faithful, in his expositions of nature, antiquity and the holy Scriptures, he vindicated by his philosophy the majesty of God mighty and good, and expressed the simplicity of the Gospel in his manners. Mortals rejoice that there has existed such and so great an ornament of the human race! He was born on 25 December 1642, and died on 20 March 1726/7.—Translation from G.L. Smyth, \"The Monuments and Genii of St. Paul's Cathedral, and of Westminster Abbey\" (1826), ii, 703–4.\n\nFrom 1978 until 1988, an image of Newton designed by Harry Ecclestone appeared on Series D £1 banknotes issued by the Bank of England (the last £1 notes to be issued by the Bank of England). Newton was shown on the reverse of the notes holding a book and accompanied by a telescope, a prism and a map of the Solar System.\n\nA statue of Isaac Newton, looking at an apple at his feet, can be seen at the Oxford University Museum of Natural History. A large bronze statue, \"Newton, after William Blake\", by Eduardo Paolozzi, dated 1995 and inspired by Blake's etching, dominates the piazza of the British Library in London.\n\nAlthough born into an Anglican family, by his thirties Newton held a Christian faith that, had it been made public, would not have been considered orthodox by mainstream Christianity; in recent times he has been described as a heretic.\n\nBy 1672 he had started to record his theological researches in notebooks which he showed to no one and which have only recently been examined. They demonstrate an extensive knowledge of early church writings and show that in the conflict between Athanasius and Arius which defined the Creed, he took the side of Arius, the loser, who rejected the conventional view of the Trinity. Newton \"recognized Christ as a divine mediator between God and man, who was subordinate to the Father who created him.\" He was especially interested in prophecy, but for him, \"the great apostasy was trinitarianism.\"\n\nNewton tried unsuccessfully to obtain one of the two fellowships that exempted the holder from the ordination requirement. At the last moment in 1675 he received a dispensation from the government that excused him and all future holders of the Lucasian chair.\n\nIn Newton's eyes, worshipping Christ as God was idolatry, to him the fundamental sin. Historian Stephen D. Snobelen says, \"Isaac Newton was a heretic. But ... he never made a public declaration of his private faith—which the orthodox would have deemed extremely radical. He hid his faith so well that scholars are still unravelling his personal beliefs.\" Snobelen concludes that Newton was at least a Socinian sympathiser (he owned and had thoroughly read at least eight Socinian books), possibly an Arian and almost certainly an anti-trinitarian.\n\nIn a minority view, T.C. Pfizenmaier argues that Newton held the Eastern Orthodox view on the Trinity. However, this type of view 'has lost support of late with the availability of Newton's theological papers', and now most scholars identify Newton as an Antitrinitarian monotheist.\n\nAlthough the laws of motion and universal gravitation became Newton's best-known discoveries, he warned against using them to view the Universe as a mere machine, as if akin to a great clock. He said, \"Gravity explains the motions of the planets, but it cannot explain who set the planets in motion. God governs all things and knows all that is or can be done.\"\n\nAlong with his scientific fame, Newton's studies of the Bible and of the early Church Fathers were also noteworthy. Newton wrote works on textual criticism, most notably \"An Historical Account of Two Notable Corruptions of Scripture\" and \"Observations upon the Prophecies of Daniel, and the Apocalypse of St. John\". He placed the crucifixion of Jesus Christ at 3 April, AD 33, which agrees with one traditionally accepted date.\n\nHe believed in a rationally immanent world, but he rejected the hylozoism implicit in Leibniz and Baruch Spinoza. The ordered and dynamically informed Universe could be understood, and must be understood, by an active reason. In his correspondence, Newton claimed that in writing the \"Principia\" \"I had an eye upon such Principles as might work with considering men for the belief of a Deity\". He saw evidence of design in the system of the world: \"Such a wonderful uniformity in the planetary system must be allowed the effect of choice\". But Newton insisted that divine intervention would eventually be required to reform the system, due to the slow growth of instabilities. For this, Leibniz lampooned him: \"God Almighty wants to wind up his watch from time to time: otherwise it would cease to move. He had not, it seems, sufficient foresight to make it a perpetual motion.\"\n\nNewton's position was vigorously defended by his follower Samuel Clarke in a famous correspondence. A century later, Pierre-Simon Laplace's work \"Celestial Mechanics\" had a natural explanation for why the planet orbits do not require periodic divine intervention.\n\nNewton and Robert Boyle's approach to the mechanical philosophy was promoted by rationalist pamphleteers as a viable alternative to the pantheists and enthusiasts, and was accepted hesitantly by orthodox preachers as well as dissident preachers like the latitudinarians. The clarity and simplicity of science was seen as a way to combat the emotional and metaphysical superlatives of both superstitious enthusiasm and the threat of atheism, and at the same time, the second wave of English deists used Newton's discoveries to demonstrate the possibility of a \"Natural Religion\".\n\nThe attacks made against pre-Enlightenment \"magical thinking\", and the mystical elements of Christianity, were given their foundation with Boyle's mechanical conception of the Universe. Newton gave Boyle's ideas their completion through mathematical proofs and, perhaps more importantly, was very successful in popularising them.\n\nIn a manuscript he wrote in 1704 (never intended to be published) he mentions the date of 2060, but it is not given as a date for the end of days. It has been falsely reported as a prediction. The passage is clear, when the date is read in context. He was against date setting for the end of days, concerned that this would put Christianity into disrepute.\n\n\"So then the time times & half a time are 42 months or 1260 days or three years & an half, recconing twelve months to a year & 30 days to a month as was done in the Calender of the primitive year. And the days of short lived Beasts being put for the years of [long-]lived kingdoms the period of 1260 days, if dated from the complete conquest of the three kings A.C. 800, will end 2060. It may end later, but I see no reason for its ending sooner.\"\n\n\"This I mention not to assert when the time of the end shall be, but to put a stop to the rash conjectures of fanciful men who are frequently predicting the time of the end, and by doing so bring the sacred prophesies into discredit as often as their predictions fail. Christ comes as a thief in the night, and it is not for us to know the times and seasons which God hath put into his own breast.\"\n\nHe later revised this date to 2016.\n\nIn the character of Morton Opperly in \"Poor Superman\" (1951), speculative fiction author Fritz Leiber says of Newton, \"Everyone knows Newton as the great scientist. Few remember that he spent half his life muddling with alchemy, looking for the philosopher's stone. That was the pebble by the seashore he really wanted to find.\"\n\nOf an estimated ten million words of writing in Newton's papers, about one million deal with alchemy. Many of Newton's writings on alchemy are copies of other manuscripts, with his own annotations. Alchemical texts mix artisanal knowledge with philosophical speculation, often hidden behind layers of wordplay, allegory, and imagery to protect craft secrets. Some of the content contained in Newton's papers could have been considered heretical by the church.\n\nIn 1888, after spending sixteen years cataloging Newton's papers, Cambridge University kept a small number and returned the rest to the Earl of Portsmouth. In 1936, a descendant offered the papers for sale at Sotheby's. The collection was broken up and sold for a total of about £9,000. John Maynard Keynes was one of about three dozen bidders who obtained part of the collection at auction. Keynes went on to reassemble an estimated half of Newton's collection of papers on alchemy before donating his collection to Cambridge University in 1946.\n\nAll of Newton's known writings on alchemy are currently being put online in a project undertaken by Indiana University: \"The Chymistry of Isaac Newton\".\n\nEnlightenment philosophers chose a short history of scientific predecessors – Galileo, Boyle, and Newton principally – as the guides and guarantors of their applications of the singular concept of nature and natural law to every physical and social field of the day. In this respect, the lessons of history and the social structures built upon it could be discarded.\n\nIt was Newton's conception of the universe based upon natural and rationally understandable laws that became one of the seeds for Enlightenment ideology. Locke and Voltaire applied concepts of natural law to political systems advocating intrinsic rights; the physiocrats and Adam Smith applied natural conceptions of psychology and self-interest to economic systems; and sociologists criticised the current social order for trying to fit history into natural models of progress. Monboddo and Samuel Clarke resisted elements of Newton's work, but eventually rationalised it to conform with their strong religious views of nature.\n\nNewton himself often told the story that he was inspired to formulate his theory of gravitation by watching the fall of an apple from a tree. Although it has been said that the apple story is a myth and that he did not arrive at his theory of gravity in any single moment, acquaintances of Newton (such as William Stukeley, whose manuscript account of 1752 has been made available by the Royal Society) do in fact confirm the incident, though not the apocryphal version that the apple actually hit Newton's head. Stukeley recorded in his \"Memoirs of Sir Isaac Newton's Life\" a conversation with Newton in Kensington on 15 April 1726:\n\nJohn Conduitt, Newton's assistant at the Royal Mint and husband of Newton's niece, also described the event when he wrote about Newton's life:\n\nIn similar terms, Voltaire wrote in his \"Essay on Epic Poetry\" (1727), \"Sir Isaac Newton walking in his gardens, had the first thought of his system of gravitation, upon seeing an apple falling from a tree.\"\n\nIt is known from his notebooks that Newton was grappling in the late 1660s with the idea that terrestrial gravity extends, in an inverse-square proportion, to the Moon; however it took him two decades to develop the full-fledged theory. The question was not whether gravity existed, but whether it extended so far from Earth that it could also be the force holding the Moon to its orbit. Newton showed that if the force decreased as the inverse square of the distance, one could indeed calculate the Moon's orbital period, and get good agreement. He guessed the same force was responsible for other orbital motions, and hence named it \"universal gravitation\".\n\nVarious trees are claimed to be \"the\" apple tree which Newton describes. The King's School, Grantham, claims that the tree was purchased by the school, uprooted and transported to the headmaster's garden some years later. The staff of the (now) National Trust-owned Woolsthorpe Manor dispute this, and claim that a tree present in their gardens is the one described by Newton. A descendant of the original tree can be seen growing outside the main gate of Trinity College, Cambridge, below the room Newton lived in when he studied there. The National Fruit Collection at Brogdale can supply grafts from their tree, which appears identical to Flower of Kent, a coarse-fleshed cooking variety.\n\n\n\n\n\n\nReligion\n\n\nWritings by Newton\n"}
{"id": "326182", "url": "https://en.wikipedia.org/wiki?curid=326182", "title": "Isoperimetric inequality", "text": "Isoperimetric inequality\n\nIn mathematics, the isoperimetric inequality is a geometric inequality involving the surface area of a set and its volume. In formula_1-dimensional space formula_2 the inequality lower bounds the surface area formula_3 of a set formula_4 by its volume formula_5,\n\nwhere formula_7 is a unit ball. The equality holds when formula_8 is a ball in formula_2.\n\nOn a plane, i.e. when formula_10, the isoperimetric inequality relates square of the circumference of a closed curve and the area of a plane region it encloses. \"Isoperimetric\" literally means \"having the same perimeter\". Specifically in formula_11, the isoperimetric inequality states, for the length \"L\" of a closed curve and the area \"A\" of the planar region that it encloses, that\n\nand that equality holds if and only if the curve is a circle.\n\nThe isoperimetric problem is to determine a plane figure of the largest possible area whose boundary has a specified length. The closely related \"Dido's problem\" asks for a region of the maximal area bounded by a straight line and a curvilinear arc whose endpoints belong to that line. It is named after Dido, the legendary founder and first queen of Carthage. The solution to the isoperimetric problem is given by a circle and was known already in Ancient Greece. However, the first mathematically rigorous proof of this fact was obtained only in the 19th century. Since then, many other proofs have been found.\n\nThe isoperimetric problem has been extended in multiple ways, for example, to curves on surfaces and to regions in higher-dimensional spaces. Perhaps the most familiar physical manifestation of the 3-dimensional isoperimetric inequality is the shape of a drop of water. Namely, a drop will typically assume a symmetric round shape. Since the amount of water in a drop is fixed, surface tension forces the drop into a shape which minimizes the surface area of the drop, namely a round sphere.\n\nThe classical \"isoperimetric problem\" dates back to antiquity. The problem can be stated as follows: Among all closed curves in the plane of fixed perimeter, which curve (if any) maximizes the area of its enclosed region? This question can be shown to be equivalent to the following problem: Among all closed curves in the plane enclosing a fixed area, which curve (if any) minimizes the perimeter?\n\nThis problem is conceptually related to the principle of least action in physics, in that it can be restated: what is the principle of action which encloses the greatest area, with the greatest economy of effort? The 15th-century philosopher and scientist, Cardinal Nicholas of Cusa, considered rotational action, the process by which a circle is generated, to be the most direct reflection, in the realm of sensory impressions, of the process by which the universe is created. German astronomer and astrologer Johannes Kepler invoked the isoperimetric principle in discussing the morphology of the solar system, in \"Mysterium Cosmographicum\" (\"The Sacred Mystery of the Cosmos\", 1596).\n\nAlthough the circle appears to be an obvious solution to the problem, proving this fact is rather difficult. The first progress toward the solution was made by Swiss geometer Jakob Steiner in 1838, using a geometric method later named \"Steiner symmetrisation\". Steiner showed that if a solution existed, then it must be the circle. Steiner's proof was completed later by several other mathematicians.\n\nSteiner begins with some geometric constructions which are easily understood; for example, it can be shown that any closed curve enclosing a region that is not fully convex can be modified to enclose more area, by \"flipping\" the concave areas so that they become convex. It can further be shown that any closed curve which is not fully symmetrical can be \"tilted\" so that it encloses more area. The one shape that is perfectly convex and symmetrical is the circle, although this, in itself, does not represent a rigorous proof of the isoperimetric theorem (see external links).\n\nThe solution to the isoperimetric problem is usually expressed in the form of an inequality that relates the length \"L\" of a closed curve and the area \"A\" of the planar region that it encloses. The isoperimetric inequality states that\n\nand that the equality holds if and only if the curve is a circle. The area of a disk of radius \"R\" is \"πR\" and the circumference of the circle is 2\"πR\", so both sides of the inequality are equal to 4\"π\"\"R\" in this case.\n\nDozens of proofs of the isoperimetric inequality have been found. In 1902, Hurwitz published a short proof using the Fourier series that applies to arbitrary rectifiable curves (not assumed to be smooth). An elegant direct proof based on comparison of a smooth simple closed curve with an appropriate circle was given by E. Schmidt in 1938. It uses only the arc length formula, expression for the area of a plane region from Green's theorem, and the Cauchy–Schwarz inequality.\n\nFor a given closed curve, the isoperimetric quotient is defined as the ratio of its area and that of the circle having the same perimeter. This is equal to\n\nand the isoperimetric inequality says that \"Q\" ≤ 1. Equivalently, the isoperimetric ratio is at least 4 for every curve.\n\nThe isoperimetric quotient of a regular \"n\"-gon is\n\nLet formula_16 be a smooth regular convex closed curve. Then the improved isoperimetric inequality states the following\n\nwhere formula_18 denote the length of formula_16, the area of the region bounded by formula_16 and the oriented area of the Wigner caustic of formula_16, respectively, and the equality holds if and only if formula_16 is a curve of constant width.\n\nLet \"C\" be a simple closed curve on a sphere of radius 1. Denote by \"L\" the length of \"C\" and by \"A\" the area enclosed by \"C\". The spherical isoperimetric inequality states that\n\nand that the equality holds if and only if the curve is a circle. There are, in fact, two ways to measure the spherical area enclosed by a simple closed curve, but the inequality is symmetric with the respect to taking the complement.\n\nThis inequality was discovered by Paul Lévy (1919) who also extended it to higher dimensions and general surfaces.\n\nIn the more general case of arbitrary radius \"R\", it is known that\n\nThe isoperimetric inequality states that a sphere has the smallest surface area per given volume. Given a set formula_26 with surface area formula_3 and volume formula_5, the isoperimetric inequality states\n\nwhere formula_30 is a unit ball. The equality holds when formula_8 is a ball in formula_32.\n\nThe proof on the inequality follows directly from Brunn–Minkowski inequality between a set formula_8 and a ball with radius formula_34, i.e. formula_35. By taking Brunn–Minkowski inequality to the power formula_1, subtracting formula_5 from both sides, dividing them by formula_34, and taking the limit as formula_39 (; ).\n\nIn full generality , the isoperimetric inequality states that for any set formula_4 whose closure has finite Lebesgue measure\n\nwhere formula_42 is the (\"n\"-1)-dimensional Minkowski content, \"L\" is the \"n\"-dimensional Lebesgue measure, and \"ω\" is the volume of the unit ball in formula_2. If the boundary of \"S\" is rectifiable, then the Minkowski content is the (\"n\"-1)-dimensional Hausdorff measure.\n\nThe \"n\"-dimensional isoperimetric inequality is equivalent (for sufficiently smooth domains) to the Sobolev inequality on formula_2 with optimal constant:\n\nfor all formula_46.\n\nMost of the work on isoperimetric problem has been done in the context of smooth regions in Euclidean spaces, or more generally, in Riemannian manifolds. However, the isoperimetric problem can be formulated in much greater generality, using the notion of \"Minkowski content\". Let formula_47 be a \"metric measure space\": \"X\" is a metric space with metric \"d\", and \"μ\" is a Borel measure on \"X\". The \"boundary measure\", or Minkowski content, of a measurable subset \"A\" of \"X\" is defined as the lim inf\n\nwhere\n\nis the ε-\"extension\" of \"A\".\n\nThe isoperimetric problem in \"X\" asks how small can formula_50 be for a given \"μ\"(\"A\"). If \"X\" is the Euclidean plane with the usual distance and the Lebesgue measure then this question generalizes the classical isoperimetric problem to planar regions whose boundary is not necessarily smooth, although the answer turns out to be the same.\n\nThe function\n\nis called the \"isoperimetric profile\" of the metric measure space formula_47. Isoperimetric profiles have been studied for Cayley graphs of discrete groups and for special classes of Riemannian manifolds (where usually only regions \"A\" with regular boundary are considered).\n\nIn graph theory, isoperimetric inequalities are at the heart of the study of expander graphs, which are sparse graphs that have strong connectivity properties. Expander constructions have spawned research in pure and applied mathematics, with several applications to complexity theory, design of robust computer networks, and the theory of error-correcting codes.\n\nIsoperimetric inequalities for graphs relate the size of vertex subsets to the size of their boundary, which is usually measured by the number of edges leaving the subset (edge expansion) or by the number of neighbouring vertices (vertex expansion). For a graph formula_53 and a number formula_54, the following are two standard isoperimetric parameters for graphs.\n\n\nHere formula_57 denotes the set of edges leaving formula_8 and formula_59 denotes the set of vertices that have a neighbour in formula_8. The isoperimetric problem consists of understanding how the parameters formula_61 and formula_62 behave for natural families of graphs.\n\nThe formula_63-dimensional hypercube formula_64 is the graph whose vertices are all Boolean vectors of length formula_63, that is, the set formula_66. Two such vectors are connected by an edge in formula_64 if they are equal up to a single bit flip, that is, their Hamming distance is exactly one.\nThe following are the isoperimetric inequalities for the Boolean hypercube.\n\nThe edge isoperimetric inequality of the hypercube is formula_68. This bound is tight, as is witnessed by each set formula_8 that is the set of vertices of any subcube of formula_64.\n\nHarper's theorem says that \"Hamming balls\" have the smallest vertex boundary among all sets of a given size. Hamming balls are sets that contain all points of Hamming weight at most formula_71 and no points of Hamming weight larger than formula_72 for some integer formula_71. This theorem implies that any set formula_74 with \n\nsatisfies \n\nAs a special case, consider set sizes formula_77 of the form \n\nfor some integer formula_71. Then the above implies that the exact vertex isoperimetric parameter is \n\nThe isoperimetric inequality for triangles in terms of perimeter \"p\" and area \"T\" states that\n\nwith equality for the equilateral triangle. This is implied, via the AM-GM inequality, by a stronger inequality which has also been called the isoperimetric inequality for triangles:\n\n\n"}
{"id": "31336256", "url": "https://en.wikipedia.org/wiki?curid=31336256", "title": "James reduced product", "text": "James reduced product\n\nIn topology, a branch of mathematics, the James reduced product or James construction \"J\"(\"X\") of a topological space \"X\" with given basepoint \"e\" is the quotient of the disjoint union of all powers \"X\", \"X\", \"X\", ... obtained by identifying points (\"x\"...,\"x\",\"e\",\"x\"...,\"x\") with (\"x\"...,\"x\", \"x\"...,\"x\"). In other words its underlying set is the free monoid generated by \"X\" (with unit \"e\"). It was introduced by .\n\nFor a connected CW complex \"X\", the James reduced product \"J\"(\"X\") has the same homotopy type as ΩΣ\"X\", the loop space of the suspension of \"X\".\n\nThe commutative analogue of the James reduced product is called the infinite symmetric product.\n"}
{"id": "14511776", "url": "https://en.wikipedia.org/wiki?curid=14511776", "title": "Kawasaki's theorem", "text": "Kawasaki's theorem\n\nKawasaki's theorem is a theorem in the mathematics of paper folding that describes the crease patterns with a single vertex that may be folded to form a flat figure. It states that the pattern is flat-foldable if and only if alternatingly adding and subtracting the angles of consecutive folds around the vertex gives an alternating sum of zero.\nCrease patterns with more than one vertex do not obey such a simple criterion, and are NP-hard to fold.\n\nThe theorem is named after one of its discoverers, Toshikazu Kawasaki. However, several others also contributed to its discovery, and it is sometimes called the Kawasaki–Justin theorem or Husimi's theorem after other contributors, Jacques Justin and Kôdi Husimi.\n\nA one-vertex crease pattern consists of a set of rays or creases drawn on a flat sheet of paper, all emanating from the same point interior to the sheet. (This point is called the vertex of the pattern.) Each crease must be folded, but the pattern does not specify whether the folds should be mountain folds or valley folds. The goal is to determine whether it is possible to fold the paper so that every crease is folded, no folds occur elsewhere, and the whole folded sheet of paper lies flat.\n\nTo fold flat, the number of creases must be even. This follows, for instance, from Maekawa's theorem, which states that the number of mountain folds at a flat-folded vertex differs from the number of valley folds by exactly two folds. Therefore, suppose that a crease pattern consists of an even number of creases, and let be the consecutive angles between the creases around the vertex, in clockwise order, starting at any one of the angles. Then Kawasaki's theorem states that the crease pattern may be folded flat if and only if the alternating sum and difference of the angles adds to zero:\n\nAn equivalent way of stating the same condition is that, if the angles are partitioned into two alternating subsets, then the sum of the angles in either of the two subsets is exactly 180 degrees. However, this equivalent form applies only to a crease pattern on a flat piece of paper, whereas the alternating sum form of the condition remains valid for crease patterns on conical sheets of paper with nonzero defect at the vertex.\n\nKawasaki's theorem, applied to each of the vertices of an arbitrary crease pattern, determines whether the crease pattern is locally flat-foldable, meaning that the part of the crease pattern near the vertex can be flat-folded. However, there exist crease patterns that are locally flat-foldable but that have no global flat folding that works for the whole crease pattern at once. conjectured that global flat-foldability could be tested by checking Kawasaki's theorem at each vertex of a crease pattern, and then also testing bipartiteness of an undirected graph associated with the crease pattern. However, this conjecture was disproven by , who showed that Hull's conditions are not sufficient. More strongly, Bern and Hayes showed that the problem of testing global flat-foldability is NP-complete.\n\nTo show that Kawasaki's condition necessarily holds for any flat-folded figure, it suffices to observe that, at each fold, the orientation of the paper is reversed. Thus, if the first crease in the flat-folded figure is placed in the plane parallel to the -axis, the next crease must be rotated from it by an angle of , the crease after that by an angle of (because the second angle has the reverse orientation from the first), etc. In order for the paper to meet back up with itself at the final angle, Kawasaki's condition must be met.\n\nShowing that the condition is also a sufficient condition is a matter of describing how to fold a given crease pattern so that it folds flat. That is, one must show how to choose whether to make mountain or valley folds, and in what order the flaps of paper should be arranged on top of each other. One way to do this is to choose a number such that the partial alternating sum\nis as small as possible. Either and the partial sum is an empty sum that is also zero, or for some nonzero choice of the partial sum is negative. Then, accordion fold the pattern, starting with angle and alternating between mountain and valley folds, placing each angular wedge of the paper below the previous folds. At each step until the final fold, an accordion fold of this type will never self-intersect. The choice of ensures that the first wedge sticks out to the left of all the other folded pieces of paper, allowing the final wedge to connect back up to it.\n\nAn alternative proof of sufficiency can be used to show that there are many different flat foldings. Consider the smallest angle and the two creases on either side of it. Mountain-fold one of these two creases and valley-fold the other, choosing arbitrarily which fold to use for which crease. Then, glue the resulting flap of paper onto the remaining part of the crease pattern. The result of this gluing will be a crease pattern with two fewer creases, on a conical sheet of paper, that still satisfies Kawasaki's condition. Therefore, by mathematical induction, repeating this process will eventually lead to a flat folding. The base case of the induction is a cone with only two creases and two equal-angle wedges, which can obviously be flat-folded by using a mountain fold for both creases. There are two ways to choose which folds to use in each step of this method, and each step eliminates two creases. Therefore, any crease pattern with creases that satisfies Kawasaki's condition has at least different choices of mountain and valley folds that all lead to valid flat foldings.\n\nIn the late 1970s, Kôdi Husimi and David A. Huffman independently observed that flat-folded figures with four creases have opposite angles adding to , a special case of Kawasaki's theorem. Huffman included the result in a 1976 paper on curved creases, and\nHusimi published the four-crease theorem in a book on origami geometry with his wife Mitsue Husimi.\nThe same result was published even earlier, in a pair of papers from 1966 by S. Murata that also included the six-crease case and the general case of Maekawa's theorem.\n\nThe fact that crease patterns with arbitrarily many creases necessarily have alternating sums of angles adding to was discovered by Kawasaki, by Stuart Robertson, and by Jacques Justin (again, independently of each other) in the late 1970s and early 1980s.\nBecause of Justin's contribution to the problem, Kawasaki's theorem has also been called the Kawasaki–Justin theorem.\nThe fact that this condition is sufficient—that is, that crease patterns with evenly many angles, alternatingly summing to can always be flat-folded—may have been first stated by .\n\nKawasaki himself has called the result Husimi's theorem, after Kôdi Husimi, and some other authors have followed this terminology as well. The name \"Kawasaki's theorem\" was first given to this result in \"Origami for the Connoisseur\" by Kunihiko Kasahara and Toshie Takahama (Japan Publications, 1987).\n\nAlthough Kawasaki's theorem completely describes the folding patterns that have flat-folded states, it does not describe the folding process needed to reach that state. For some folding patterns, it may be necessary to curve or bend the paper while transforming it from a flat sheet to its flat-folded state, rather than keeping the rest of the paper flat and only changing the dihedral angles at each fold. For rigid origami (a type of folding that keeps the surface flat except at its folds, suitable for hinged panels of rigid material rather than flexible paper), additional conditions are needed on a folding pattern to allow it to move from an unfolded state to a flat-folded state.\n"}
{"id": "5787012", "url": "https://en.wikipedia.org/wiki?curid=5787012", "title": "Lerche–Newberger sum rule", "text": "Lerche–Newberger sum rule\n\nThe Lerche–Newberger, or Newberger, sum rule, discovered by B. S. Newberger in 1982, finds the sum of certain infinite series involving Bessel functions \"J\" of the first kind. \nIt states that if \"μ\" is any non-integer complex number, formula_1, and Re(\"α\" + \"β\") > −1, then\n\nNewberger's formula generalizes a formula of this type proven by Lerche in 1966; Newberger discovered it independently. Lerche's formula has γ =1; both extend a standard rule for the summation of Bessel functions, and are useful in plasma physics.\n"}
{"id": "5971823", "url": "https://en.wikipedia.org/wiki?curid=5971823", "title": "List of mathematicians (Q)", "text": "List of mathematicians (Q)\n\n"}
{"id": "169324", "url": "https://en.wikipedia.org/wiki?curid=169324", "title": "Logical equivalence", "text": "Logical equivalence\n\nIn logic, statements formula_1 and formula_2 are logically equivalent if they have the same logical content. That is, if they have the same truth value in every model (Mendelson 1979:56). The logical equivalence of formula_1 and formula_2 is sometimes expressed as formula_5, formula_6, or formula_7.\nHowever, these symbols are also used for material equivalence. Proper interpretation depends on the context. Logical equivalence is different from material equivalence, although the two concepts are closely related.\n\nformula_7\n\nLogical equivalences involving conditional statements：\n\nLogical equivalences involving biconditionals：\n\nThe following statements are logically equivalent:\n\n\nSyntactically, (1) and (2) are derivable from each other via the rules of contraposition and double negation. Semantically, (1) and (2) are true in exactly the same models (interpretations, valuations); namely, those in which either \"Lisa is in France\" is false or \"Lisa is in Europe\" is true.\n\nLogical equivalence is different from material equivalence. Formulas formula_1 and formula_2 are logically equivalent if and only if the statement of their material equivalence (formula_7) is a tautology (Copi et at. 2014:348). \n\nThe material equivalence of formula_1 and formula_2 (often written formula_7) is itself another statement in the same object language as formula_1 and formula_2. This statement expresses the idea \"'formula_1 if and only if formula_2'\". In particular, the truth value of formula_7 can change from one model to another.\n\nThe claim that two formulas are logically equivalent is a statement in the metalanguage, expressing a relationship between two statements formula_1 and formula_2. The statements are logically equivalent if, in every model, they have the same truth value.\n\n\n"}
{"id": "28359471", "url": "https://en.wikipedia.org/wiki?curid=28359471", "title": "Ludwig Wittgenstein's philosophy of mathematics", "text": "Ludwig Wittgenstein's philosophy of mathematics\n\nLudwig Wittgenstein considered his chief contribution to philosophy to be in the philosophy of mathematics, a topic to which he devoted much of his work between 1929 and 1944. As with his philosophy of language, Wittgenstein's views on mathematics evolved from the period of the Tractatus Logico-Philosophicus, changing from logicism which was endorsed by his mentor Bertrand Russell, to a general anti-foundationalism and constructivism that was not readily accepted by the mathematical community. The main source in Wittgenstein's thinking on mathematics is the text compiled as \"Remarks on the Foundations of Mathematics\", which contains the late Wittgenstein's views, notably a controversial repudiation of Gödel's incompleteness theorems. The success of Wittgenstein's general philosophy has tended to displace the real debates on more technical issues.\n\nWittgenstein's initial conception of mathematics was logicist and even formalist. The \"Tractatus\" described the propositions of logic as a series of tautologies derived from syntactic manipulation, and without the pictorial force of elementary propositions depicting states of affairs obtaining in the world.\n\nWittgenstein asserted that “[t]he logic of the world, which is shown in tautologies by the propositions of logic, is shown in equations by mathematics” (6.22) and further that “Mathematics is a method of logic” (6.234).\n\nDuring the 1920s Wittgenstein turned away from philosophical matters but his interest in mathematics was rekindled when he attended in Vienna a lecture by the intuitionist L. E. J. Brouwer. After 1929, his primary mathematical preoccupation entailed resolving the account of logical necessity he had articulated in the Tractatus Logico-Philosophicus—an issue which had been fiercely pressed by Frank P. Ramsey. Wittgenstein's initial response, Some Remarks on Logical Form, was the only academic paper he published during his lifetime, and marked the beginnings of a departure from the ideal language philosophy and correspondence theory of truth of the Tractatus.\n\nDuring the two terms of 1938/9 Wittgenstein lectured without any notes before students for two hours twice a week. From four sets of notes made during the lectures a text has been created, presenting Wittgenstein's views at that time.\n\nAn editorial team prepared the edition of Wittgenstein's Remarks on the Foundations of mathematics from the manuscript notes he made during the years 1937-44. The material has been arranged in chronological order, allowing to observe some changes of emphasis or interest in Wittgenstein's views over the years.\n\n\n"}
{"id": "18313757", "url": "https://en.wikipedia.org/wiki?curid=18313757", "title": "Master/Session", "text": "Master/Session\n\nIn cryptography, Master/Session is a key management scheme in which a pre-shared Key Encrypting Key (called the \"Master\" key) is used to encrypt a randomly generated and insecurely communicated Working Key (called the \"Session\" key). The Working Key is then used for encrypting the data to be exchanged. Its advantage is simplicity, but it suffers the disadvantage of having to communicate the pre-shared Key Exchange Key, which can be difficult to update in the event of compromise.\n\nThe Master/Session technique was created in the days before asymmetric techniques, such as Diffie-Hellman, were invented. This technique still finds widespread use in the financial industry, and is routinely used between corporate parties such as issuers, acquirers, switches. Its use in device communications (such as PIN pads), however, is in decline given the advantages of techniques such as DUKPT.\n"}
{"id": "11465576", "url": "https://en.wikipedia.org/wiki?curid=11465576", "title": "Minimax eversion", "text": "Minimax eversion\n\nIn geometry, minimax eversions are a class of sphere eversions, constructed by using half-way models.\n\nIt is a variational method, and consists of special homotopies (they are shortest paths with respect to Willmore energy); contrast with Thurston's corrugations, which are generic.\n\nThe original method of half-way models was not optimal: the regular homotopies passed through the midway models, but the path from the round sphere to the midway model was constructed by hand, and was not gradient ascent/descent.\n\nEversions via half-way models are called \"tobacco-pouch eversions\" by Francis and Morin.\n\nA half-way model is an immersion of the sphere formula_1 in formula_2, which is so-called because it is the half-way point of a sphere eversion. This class of eversions has time symmetry: the first half of the regular homotopy goes from the standard round sphere to the half-way model, and the second half (which goes from the half-way model to the inside-out sphere) is the same process in reverse.\n\nRob Kusner proposed optimal eversions using the Willmore energy on the space of all immersions of the sphere formula_1 in formula_4.\nThe round sphere and the inside-out round sphere are the unique global minima for Willmore energy, and a minimax eversion is a path connecting these by passing over a saddle point (like traveling between two valleys via a mountain pass).\n\nKusner's half-way models are saddle points for Willmore energy, arising (according to a theorem of Bryant) from certain complete minimal surfaces in 3-space; the minimax eversions consist of gradient ascent from the round sphere to the half-way model, then gradient descent down (gradient descent for Willmore energy is called Willmore flow). More symmetrically, start at the half-way model; push in one direction and follow Willmore flow down to a round sphere; push in the opposite direction and follow Willmore flow down to the inside-out round sphere. \n\nThere are two families of half-way models (this observation is due to Francis and Morin):\n\nThe first explicit sphere eversion was by Shapiro and Phillips in the early 1960s, using Boy's surface as a half-way model. Later Morin discovered the Morin surface and used it to construct other sphere eversions. Kusner conceived the minimax eversions in the early 1980s: historical details.\n\n"}
{"id": "5608629", "url": "https://en.wikipedia.org/wiki?curid=5608629", "title": "Open book decomposition", "text": "Open book decomposition\n\nIn mathematics, an open book decomposition (or simply an open book) is a decomposition of a closed oriented 3-manifold \"M\" into a union of surfaces (necessarily with boundary) and solid tori. Open books have relevance to contact geometry, with a famous theorem of Emmanuel Giroux (given below) that shows that contact geometry can be studied from an entirely topological viewpoint.\n\nDefinition. An \"open book decomposition\" of a 3-dimensional manifold \"M\" is a pair (\"B\", π) where\nThis is the special case \"m\" = 3 of an open book decomposition of an \"m\"-dimensional manifold, for any \"m\".\n\nWhen Σ is an oriented compact surface with \"n\" boundary components and φ: Σ → Σ is a homeomorphism which is the identity near the boundary, we can construct an open book by first forming the mapping torus Σ. Since φ is the identity on ∂Σ, ∂Σ is the trivial circle bundle over a union of circles, that is, a union of tori; one torus for each boundary component. To complete the construction, solid tori are glued to fill in the boundary tori so that each circle \"S\" × {\"p\"} ⊂ \"S\"×∂\"D\" is identified with the boundary of a page. In this case, the binding is the collection of \"n\" cores \"S\"×{q} of the \"n\" solid tori glued into the mapping torus, for arbitrarily chosen \"q\" ∈ \"D\". It is known that any open book can be constructed this way. As the only information used in the construction is the surface and the homeomorphism, an alternate definition of open book is simply the pair (Σ, φ) with the construction understood. In short, an open book is a mapping torus with solid tori glued in so that the core circle of each torus runs parallel to the boundary of the fiber.\n\nEach torus in ∂Σ is fibered by circles parallel to the binding, each circle a boundary component of a page. One envisions a rolodex-looking structure for a neighborhood of the binding (that is, the solid torus glued to ∂Σ)—the pages of the rolodex connect to pages of the open book and the center of the rolodex is the binding. Thus the term \"open book\".\n\nIt is a 1972 theorem of Elmar Winkelnkemper that for \"m\" > 6, a simply-connected \"m\"-dimensional manifold has an open book decomposition if and only if it has signature 0. In 1977 Terry Lawson proved that for odd \"m\" > 6, every \"m\"-dimensional manifold has an open book decomposition. For even \"m\" > 6, an \"m\"-dimensional manifold has an open book decomposition if and only if an asymmetric Witt group obstruction is 0, by a 1979 theorem of Frank Quinn.\n\nIn 2002, Emmanuel Giroux published the following result:\n\nTheorem. Let \"M\" be a compact oriented 3-manifold. Then there is a bijection between the set of oriented contact structures on \"M\" up to isotopy and the set of open book decompositions of \"M\" up to positive stabilization.\n\n\"Positive stabilization\" consists of modifying the page by adding a 2-dimensional 1-handle and modifying the monodromy by adding a positive Dehn twist along a curve that runs over that handle exactly once. Implicit in this theorem is that the new open book defines the same contact 3-manifold. Giroux's result has led to some breakthroughs in what is becoming more commonly called contact topology, such as the classification of contact structures on certain classes of 3-manifolds. Roughly speaking, a contact structure corresponds to an open book if, away from the binding, the contact distribution is isotopic to the tangent spaces of the pages through confoliations. One imagines smoothing the contact planes (preserving the contact condition almost everywhere) to lie tangent to the pages.\n\n"}
{"id": "15613287", "url": "https://en.wikipedia.org/wiki?curid=15613287", "title": "Parker vector", "text": "Parker vector\n\nIn mathematics, especially the field of group theory, the Parker vector is an integer vector that describes a permutation group in terms of the cycle structure of its elements.\n\nThe Parker vector \"P\" of a permutation group \"G\" acting on a set of size \"n\", is the vector whose \"k\"th component for \"k\" = 1, …, \"n\" is given by:\n\nThe Parker vector can assist in the recognition of Galois groups.\n\n"}
{"id": "24868", "url": "https://en.wikipedia.org/wiki?curid=24868", "title": "Pauli matrices", "text": "Pauli matrices\n\nIn mathematical physics and mathematics, the Pauli matrices are a set of three complex matrices which are Hermitian and unitary. Usually indicated by the Greek letter sigma (), they are occasionally denoted by tau () when used in connection with isospin symmetries. They are\n\nThese matrices are named after the physicist Wolfgang Pauli. In quantum mechanics, they occur in the Pauli equation which takes into account the interaction of the spin of a particle with an external electromagnetic field.\n\nEach Pauli matrix is Hermitian, and together with the identity matrix (sometimes considered as the zeroth Pauli matrix ), the Pauli matrices (multiplied by \"real\" coefficients) form a basis for the vector space of Hermitian matrices.\n\nHermitian operators represent observables, so the Pauli matrices span the space of observables of the -dimensional complex Hilbert space. In the context of Pauli's work, represents the observable corresponding to spin along the th coordinate axis in three-dimensional Euclidean space .\n\nThe Pauli matrices (after multiplication by to make them anti-Hermitian), also generate transformations in the sense of Lie algebras: the matrices form a basis for formula_2, which exponentiates to the special unitary group SU(2). The algebra generated by the three matrices is isomorphic to the Clifford algebra of .\n\nAll three of the Pauli matrices can be compacted into a single expression:\n\nwhere is the imaginary unit, and is the Kronecker delta, which equals +1 if and 0 otherwise. This expression is useful for \"selecting\" any one of the matrices numerically by substituting values of , in turn useful when any of the matrices (but no particular one) is to be used in algebraic manipulations.\n\nThe matrices are involutory:\n\nwhere is the identity matrix.\n\n\nFrom above we can deduce that the eigenvalues of each are .\n\n\nEach of the (Hermitian) Pauli matrices has two eigenvalues, and . The corresponding normalized eigenvectors are:\n\nThe Pauli vector is defined by\n\nand provides a mapping mechanism from a vector basis to a Pauli matrix basis as follows,\n\nusing the summation convention. Further,\nits eigenvalues being formula_10, and moreover (see completeness, below) \n\nIts (unnormalized) eigenvectors are \nformula_12\n\nThe Pauli matrices obey the following commutation relations:\n\nand anticommutation relations:\n\nwhere the structure constant is the Levi-Civita symbol, Einstein summation notation is used, is the Kronecker delta, and is the identity matrix.\n\nFor example,\n\nPauli vectors elegantly map these commutation and anticommutation relations to corresponding vector products. Adding the commutator to the anticommutator gives\nso that, \nContracting each side of the equation with components of two -vectors and (which commute with the Pauli matrices, i.e., for each matrix and vector component (and likewise with ), and relabeling indices , to prevent notational conflicts, yields\n\nFinally, translating the index notation for the dot product and cross product results in \nIf formula_18 is identified with the pseudoscalar formula_19 then the right hand side becomes formula_20 which is also the definition for the product of two vectors in geometric algebra.\n\nFollowing traces can be derived using the commutation and anticommutation relations.\n\nFor \none has, for even powers, formula_26\nwhich can be shown first for the formula_28 case using the anticommutation relations. For convenience, the case formula_29 is taken to be formula_30 by convention.\n\nFor odd powers, formula_31\n\nMatrix exponentiating, and using the Taylor series for sine and cosine,\n\nIn the last line, the first sum is the cosine, while the second sum is the sine; so, finally,\nwhich is analogous to Euler's formula, extended to quaternions.\n\nNote that\nwhile the determinant of the exponential itself is just , which makes it the generic group element of SU(2).\n\nA more abstract version of formula for a general matrix can be found in the article on matrix exponentials. A general version of for an analytic (at \"a\" and −\"a\") function is provided by application of Sylvester's formula,\nA straightforward application of formula provides a parameterization of the composition law of the group . One may directly solve for in \n\nwhich specifies the generic group multiplication, where, manifestly, \n\nthe spherical law of cosines. Given , then, \n\nConsequently, the composite rotation parameters in this group element (a closed form of the respective BCH expansion in this case) simply amount to \nIt is also straightforward to likewise work out the adjoint action on the Pauli vector, namely rotation effectively by double the angle ,\n\nAn alternative notation that is commonly used for the Pauli matrices is to write the vector index in the superscript, and the matrix indices as subscripts, so that the element in row and column of the -th Pauli matrix is .\n\nIn this notation, the completeness relation for the Pauli matrices can be written\nAs noted above, it is common to denote the 2 × 2 unit matrix by \"σ\", so \"σ\" = \"δ\". \nThe completeness relation can alternatively be expressed as\n\nThe fact that any complex Hermitian matrices can be expressed in terms of the identity matrix and the Pauli matrices also leads to the Bloch sphere representation of mixed states' density matrix, ( positive semidefinite matrices with trace ). This can be seen by simply first writing an arbitrary Hermitian matrix as a real linear combination of as above, and then imposing the positive-semidefinite and trace conditions.\n\nLet be the transposition (also known as a permutation) between two spins and living in the tensor product space ,\n\nThis operator can also be written more explicitly as Dirac's spin exchange operator,\n\nIts eigenvalues are therefore 1 or −1. It may thus be utilized as an interaction term in a Hamiltonian, splitting the energy eigenvalues of its symmetric versus antisymmetric eigenstates.\n\nThe group SU(2) is the Lie group of unitary 2×2 matrices with unit determinant; its Lie algebra is the set of all 2×2 anti-Hermitian matrices with trace 0. Direct calculation, as above, shows that the Lie algebra formula_51 is the 3-dimensional real algebra spanned by the set }. In compact notation,\n\nAs a result, each can be seen as an infinitesimal generator of SU(2). The elements of SU(2) are exponentials of linear combinations of these three generators, and multiply as indicated above in discussing the Pauli vector. Although this suffices to generate SU(2), it is not a proper representation of, as the Pauli eigenvalues are scaled unconventionally. The conventional normalization is  , so that\n\nAs SU(2) is a compact group, its Cartan decomposition is trivial.\n\nThe Lie algebra is isomorphic to the Lie algebra , which corresponds to the Lie group SO(3), the group of rotations in three-dimensional space. In other words, one can say that the are a realization (and, in fact, the lowest-dimensional realization) of \"infinitesimal\" rotations in three-dimensional space. However, even though and are isomorphic as Lie algebras, and are not isomorphic as Lie groups. is actually a double cover of , meaning that there is a two-to-one group homomorphism from to , see relationship between SO(3) and SU(2).\n\nThe real linear span of is isomorphic to the real algebra of quaternions . The isomorphism from to this set is given by the following map (notice the reversed signs for the Pauli matrices):\n\nAlternatively, the isomorphism can be achieved by a map using the Pauli matrices in reversed order,\n\nAs the set of versors \"U\" ⊂ ℍ forms a group isomorphic to , \"U\" gives yet another way of describing . The two-to-one homomorphism from to may be given in terms of the Pauli matrices in this formulation.\n\nQuaternions form a division algebra—every non-zero element has an inverse—whereas Pauli matrices do not. As Pauli matrices lie in M(2,ℂ), isomorphic to the algebra of biquaternions, linear spaces of eight real dimensions, they go beyond the four-dimensional quaternions. \n\nMultiplying any two Pauli matrices always yield the matrix of a versor.\n\nIn classical mechanics, Pauli matrices are useful in the context of the Cayley-Klein parameters. The matrix \"P\" corresponding to the position formula_56 of a point in space is defined in terms of the above Pauli vector matrix, \n\nConsequently, the transformation matrix formula_58 for rotations about the \"x\"-axis through an angle \"θ\" may be written in terms of Pauli matrices and the unit matrix as \nSimilar expressions follow for general Pauli vector rotations as detailed above.\n\nIn quantum mechanics, each Pauli matrix is related to an angular momentum operator that corresponds to an observable describing the spin of a spin ½ particle, in each of the three spatial directions. As an immediate consequence of the Cartan decomposition mentioned above, are the generators of a projective representation (spin representation) of the rotation group SO(3) acting on non-relativistic particles with spin ½. The states of the particles are represented as two-component spinors. In the same way, the Pauli matrices are related to the isospin operator.\n\nAn interesting property of spin ½ particles is that they must be rotated by an angle of 4 in order to return to their original configuration. This is due to the two-to-one correspondence between SU(2) and SO(3) mentioned above, and the fact that, although one visualizes spin up/down as the north/south pole on the 2-sphere , they are actually represented by orthogonal vectors in the two dimensional complex Hilbert space.\n\nFor a spin ½ particle, the spin operator is given by , the fundamental representation of \"SU(2)\". By taking Kronecker products of this representation with itself repeatedly, one may construct all higher irreducible representations. That is, the resulting spin operators for higher spin systems in three spatial dimensions, for arbitrarily large \"j\", can be calculated using this spin operator and ladder operators. They can be found in Rotation group SO(3)#A note on Lie algebra. The analog formula to the above generalization of Euler's formula for Pauli matrices, the group element in terms of spin matrices, is tractable, but less simple.\n\nAlso useful in the quantum mechanics of multiparticle systems, the general Pauli group is defined to consist of all -fold tensor products of Pauli matrices.\n\nIn relativistic quantum mechanics, the spinors in four dimensions are 4 × 1 (or 1 × 4) matrices. Hence the Pauli matrices or the Sigma matrices operating on these spinors have to be 4 × 4 matrices. They are defined in terms of 2 × 2 Pauli matrices as\nIt follows from this definition that formula_61 matrices have the same algebraic properties as formula_62 matrices.\n\nHowever, relativistic angular momentum is not a three-vector, but a second order four-tensor. Hence formula_61 needs to be replaced by formula_64, the generator of Lorentz transformations on spinors. By the antisymmetry of angular momentum, the formula_64 are also antisymmetric. Hence there are only six independent matrices.\n\nThe first three are the formula_66 The remaining three, formula_67, are the Dirac formula_68 matrices defined as\n\nThe relativistic spin matrices formula_64 are written in compact form in terms of commutator of gamma matrices as\n\n\n\n"}
{"id": "29262224", "url": "https://en.wikipedia.org/wiki?curid=29262224", "title": "Periodic summation", "text": "Periodic summation\n\nIn signal processing, any periodic functionformula_1  with period P, can be represented by a summation of an infinite number of instances of an aperiodic functionformula_2 that are offset by integer multiples of P.  This representation is called periodic summation: \n\nWhen  formula_4  is alternatively represented as a complex Fourier series, the Fourier coefficients are proportional to the values (or \"samples\") of the continuous Fourier transform, formula_5  at intervals of 1/P.  That identity is a form of the Poisson summation formula. Similarly, a Fourier series whose coefficients are samples of  formula_6  at constant intervals (T) is equivalent to a periodic summation of  formula_7  which is known as a discrete-time Fourier transform.\n\nThe periodic summation of a Dirac delta function is the Dirac comb. Likewise, the periodic summation of an integrable function is its convolution with the Dirac comb.\n\nIf a periodic function is represented using the quotient space domain\nformula_8 then one can write\n\ninstead. The arguments of formula_11 are equivalence classes of real numbers that share the same fractional part when divided by formula_12.\n\n "}
{"id": "1992983", "url": "https://en.wikipedia.org/wiki?curid=1992983", "title": "Polylogism", "text": "Polylogism\n\nPolylogism is the belief that different groups of people reason in fundamentally different ways (coined from Greek poly=many + logos=logic). The term is attributed to Ludwig von Mises, who claimed that it described Marxism and other social philosophies. In the Misesian sense of the term, a polylogist ascribes different forms of \"logic\" to different groups, which may include groups based on race, gender, class, or time period.\n\nA polylogist would claim that different groups reason in fundamentally different ways: they use different \"logics\" for deductive inference. Normative polylogism is the claim that these different logics are equally valid. Descriptive polylogism is an empirical claim about different groups, but a descriptive polylogism need not claim equal validity for different \"logics\". That is, a descriptive polylogist may insist on a universally valid deductive logic while claiming as an empirical matter that some groups use other (incorrect) reasoning strategies.\n\nAn adherent of polylogism in the Misesian sense would be a normative polylogist. A normative polylogist might approach an argument by demonstrating how it was correct within a particular logical construct, even if it were incorrect within the logic of the analyst. As Mises noted \"this never has been and never can be attempted by anybody.\"\n\nThe term 'proletarian logic' is sometimes taken as evidence of polylogism. This term is usually traced back to Joseph Dietzgen in his 11th letter on logic. Dietzgen is the now obscure philosophical monist of the 19th century who coined the term 'dialectical materialism' and was praised by communist figures such as Karl Marx and V. I. Lenin. His work has received modern attention primarily from the philosopher Bertell Ollman. As a monist, Dietzgen insists on a unified treatment of mind and matter. As Simon Boxley puts it, for Dietzgen \"thought is as material an event as any other\". This means that logic too has \"material\" underpinnings. (But note that Dietzgen's \"materialism\" was explicitly not a physicalism.)\n\nRacialist polylogism is often identified with the Nazi era. It has been proposed that the ferment around Einstein's theory of relativity is an example of racialist polylogism. Some of the criticisms of relativity theory were mixed with racialist resistance that characterized the physics as an embodiment of Jewish ideology. (For example, Nobel Prize winner Philipp Lenard claimed scientific thought was conditioned by \"blood and race\", and he accused Werner Heisenberg of teaching \"Jewish physics\".) However this appears to be an argument ad hominem, not polylogism. Modern examples of supposed racialist polylogism are generally misleading. For example, US Supreme Court Justice Sotomayor has been accused of racialist polylogism for suggesting that a \"wise Latina\" might come to different legal conclusions than a white male. Although generally given the interpretation that life experience can influence one's ability to understand the practical implications of a legal argument, some commentators suggested that Sotomayor supported the idea that Latinas have a unique \"logic\".\n\n\n"}
{"id": "33478101", "url": "https://en.wikipedia.org/wiki?curid=33478101", "title": "Rep-tile", "text": "Rep-tile\n\nIn the geometry of tessellations, a rep-tile or reptile is a shape that can be dissected into smaller copies of the same shape. The term was coined as a pun on animal reptiles by the American mathematician Solomon W. Golomb, who used it to describe self-replicating tilings. In 2012 a generalization of rep-tiles called self-tiling tile sets was introduced by Lee Sallows in \"Mathematics Magazine\".\n\nA rep-tile is labelled rep-\"n\" if the dissection uses \"n\" copies. Such a shape necessarily forms the prototile for a tiling of the plane, in many cases an aperiodic tiling. \nA rep-tile dissection using different sizes of the original shape is called an irregular rep-tile or irreptile. If the dissection uses \"n\" copies, the shape is said to be irrep-\"n\". If all these sub-tiles are of different sizes then the tiling is additionally described as perfect. A shape that is rep-\"n\" or irrep-\"n\" is trivially also irrep-(\"kn\" − \"k\" + \"n\") for any \"k\" > 1, by replacing the smallest tile in the rep-\"n\" dissection by \"n\" even smaller tiles. The order of a shape, whether using rep-tiles or irrep-tiles is the smallest possible number of tiles which will suffice.\n\nEvery square, rectangle, parallelogram, rhombus, or triangle is rep-4. The sphinx hexiamond (illustrated above) is rep-4 and rep-9, and is one of few known self-replicating pentagons. The Gosper island is rep-7. The Koch snowflake is irrep-7: six small snowflakes of the same size, together with another snowflake with three times the area of the smaller ones, can combine to form a single larger snowflake.\n\nA right triangle with side lengths in the ratio 1:2 is rep-5, and its rep-5 dissection forms the basis of the aperiodic pinwheel tiling. By Pythagoras' theorem, the hypotenuse, or sloping side of the rep-5 triangle, has a length of .\n\nThe international standard ISO 216 defines sizes of paper sheets using the , in which the long side of a rectangular sheet of paper is the square root of two times the short side of the paper. Rectangles in this shape are rep-2. A rectangle (or parallelogram) is rep-\"n\" if its aspect ratio is :1. An isosceles right triangle is also rep-2.\n\nSome rep-tiles, like the square and equilateral triangle, are symmetrical and remain identical when reflected in a mirror. Others, like the sphinx, are asymmetrical and exist in two distinct forms related by mirror-reflection. Dissection of the sphinx and some other asymmetric rep-tiles requires use of both the original shape and its mirror-image.\n\nSome rep-tiles are based on polyforms like polyiamonds and polyominoes, or shapes created by laying equilateral triangles and squares edge-to-edge.\n\nIf a polyomino is rectifiable, that is, able to tile a rectangle, then it will also be a rep-tile, because the rectangle will then tile a square. This can be seen clearly in the octominoes, which are created from eight squares. Two copies of some octominoes will tile a square; therefore these octominoes are also rep-16 rep-tiles.\n\nFour copies of some nonominoes and nonakings will tile a square, therefore these polyforms are also rep-36 rep-tiles.\n\nSimilarly, if a polyiamond tiles an equilateral triangle, it will also be a rep-tile.\n\nA right triangle is a triangle containing one right angle of 90°. Two particular forms of right triangle have attracted the attention of rep-tile researchers, the 45°-90°-45° triangle and the 30°-60°-90° triangle.\n\nPolyforms based on isosceles right triangles, with sides in the ratio 1 : 1 : , are known as polyabolos. An infinite number of them are rep-tiles. Indeed, the simplest of all rep-tiles is a single isosceles right triangle. It is rep-2 when divided by a single line bisecting the right angle to the hypotenuse. Rep-2 rep-tiles are also rep-2 and the rep-4,8,16+ triangles yield further rep-tiles. These are found by discarding half of the sub-copies and permutating the remainder until they are mirror-symmetrical within a right triangle. In other words, two copies will tile a right triangle. One of these new rep-tiles is reminiscent of the fish formed from three equilateral triangles.\n\nPolyforms based on 30°-60°-90° right triangles, with sides in the ratio 1 :  : 2, are known as polydrafters. Some are identical to polyminoes and polyiamonds, others are distinct.\n\nMany of the common rep-tiles are rep- for all positive integer values of . In particular this is true for three trapezoids including the one formed from three equilateral triangles, for three axis-parallel hexagons (the L-tromino, L-tetromino, and P-pentomino), and the sphinx hexiamond. In addition, many rep-tiles, particularly those with higher rep-\"n\", can be self-tiled in different ways. For example, the rep-9 L-tetramino has at least fourteen different rep-tilings. The rep-9 sphinx hexiamond can also be tiled in different ways. \n\nThe most familiar rep-tiles are polygons with a finite number of sides, but some shapes with an infinite number of sides can also be rep-tiles. For example, the teragonic triangle, or horned triangle, is rep-4. It is also an example of a fractal rep-tile.\n\nTriangular and quadrilateral (four-sided) rep-tiles are common, but pentagonal rep-tiles are rare. For a long time, the sphinx was widely believed to be the only example known, but the German/New-Zealand mathematician Karl Scherer and the American mathematician George Sicherman have found more examples, including a double-pyramid and an elongated version of the sphinx. These pentagonal rep-tiles are illustrated on the Math Magic pages overseen by the American mathematician Erich Friedman. However, the sphinx and its extended versions are the only known pentagons that can be rep-tiled with equal copies.\nSee Clarke's Reptile pages.\n\nRep-tiles can be used to create fractals, or shapes that are self-similar at smaller and smaller scales. A rep-tile fractal is formed by subdividing the rep-tile, removing one or more copies of the subdivided shape, and then continuing recursively. For instance, the Sierpinski carpet is formed in this way from a rep-tiling of a square into 27 smaller squares, and the Sierpinski triangle is formed from a rep-tiling of an equilateral triangle into four smaller triangles. When one sub-copy is discarded, a rep-4 L-triomino can be used to create four fractals, two of which are identical except for orientation.\n\nBecause fractals are self-similar on smaller and smaller scales, many are also self-tiling and are therefore rep-tiles. For example, the Sierpinski triangle is rep-3, tiled with three copies of itself, and the Sierpinski carpet is rep-8, tiled with eight copies of itself.\n\nBy construction, any fractal defined by an iterated function system of n contracting maps of the same ratio is rep-n.\n\nAmong regular polygons, only the triangle and square can be dissected into smaller equally sized copies of themselves. However, a regular hexagon can be dissected into six equilateral triangles, each of which can be dissected into a regular hexagon and three more equilateral triangles. This is the basis for an infinite tiling of the hexagon with hexagons. The hexagon is therefore an irrep-∞ or irrep-infinity irreptile.\n\n\n\n\n"}
{"id": "44734124", "url": "https://en.wikipedia.org/wiki?curid=44734124", "title": "Replacement product", "text": "Replacement product\n\nIn graph theory, the replacement product of two graphs is a graph product that can be used to reduce the degree of a graph while maintaining its connectivity.\n\nSuppose \"G\" is a \"d\"-regular graph and \"H\" is an \"e\"-regular graph with vertex set {0, …, \"d\" − 1}. Let \"R\" denote the replacement product of \"G\" and \"H\". The vertex set of \"R\" is the Cartesian product \"V\"(\"G\") × \"V\"(\"H\"). For each vertex \"u\" in \"V\"(\"G\") and for each edge (\"i\", \"j\") in \"E\"(\"H\"), the vertex (\"u\", \"i\") is adjacent to (\"u\", \"j\") in \"R\". Furthermore, for each edge (\"u\", \"v\") in \"E\"(\"G\"), if \"v\" is the \"i\"th neighbor of \"u\" and \"u\" is the \"j\"th neighbor of \"v\", the vertex (\"u\", \"i\") is adjacent to (\"v\", \"j\") in \"R\".\n\nIf \"H\" is an \"e\"-regular graph, then \"R\" is an (\"e\" + 1)-regular graph.\n"}
{"id": "33742232", "url": "https://en.wikipedia.org/wiki?curid=33742232", "title": "Restricted Boltzmann machine", "text": "Restricted Boltzmann machine\n\nA restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs. \n\nRBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,\nand rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction,\nclassification,\ncollaborative filtering, feature learning\nand topic modelling.\nThey can be trained in either supervised or unsupervised ways, depending on the task.\n\nAs their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: \na pair of nodes from each of the two groups of units (commonly referred to as the \"visible\" and \"hidden\" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, \"unrestricted\" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.\n\nRestricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by \"stacking\" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.\n\nThe standard type of RBM has binary-valued (Boolean/Bernoulli) hidden and visible units, and consists of a matrix of weights formula_1 (size \"m\"×\"n\") associated with the connection between hidden unit formula_2 and visible unit formula_3, as well as bias weights (offsets) formula_4 for the visible units and formula_5 for the hidden units. Given these, the \"energy\" of a configuration (pair of boolean vectors) is defined as\n\nor, in matrix notation,\n\nThis energy function is analogous to that of a Hopfield network. As in general Boltzmann machines, probability distributions over hidden and/or visible vectors are defined in terms of the energy function:\n\nwhere formula_9 is a partition function defined as the sum of formula_10 over all possible configurations (in other words, just a normalizing constant to ensure the probability distribution sums to 1). Similarly, the (marginal) probability of a visible (input) vector of booleans is the sum over all possible hidden layer configurations:\n\nSince the RBM has the shape of a bipartite graph, with no intra-layer connections, the hidden unit activations are mutually independent given the visible unit activations and conversely, the visible unit activations are mutually independent given the hidden unit activations. That is, for formula_12 visible units and formula_13 hidden units, the conditional probability of a configuration of the visible units , given a configuration of the hidden units , is\n\nConversely, the conditional probability of given is\n\nThe individual activation probabilities are given by\n\nwhere formula_18 denotes the logistic sigmoid.\n\nThe visible units of RBM can be multinomial, although the hidden units are Bernoulli. In this case, the logistic function for visible units is replaced by the softmax function\n\nwhere \"K\" is the number of discrete values that the visible values have. They are applied in topic modeling, and recommender systems.\n\nRestricted Boltzmann machines are a special case of Boltzmann machines and Markov random fields.\nTheir graphical model corresponds to that of factor analysis.\n\nRestricted Boltzmann machines are trained to maximize the product of probabilities assigned to some training set formula_20 (a matrix, each row of which is treated as a visible vector formula_21),\n\nor equivalently, to maximize the expected log probability of a training sample formula_21 selected randomly from formula_20:\n\nThe algorithm most often used to train RBMs, that is, to optimize the weight vector formula_26, is the contrastive divergence (CD) algorithm due to Hinton, originally developed to train PoE (product of experts) models.\nThe algorithm performs Gibbs sampling and is used inside a gradient descent procedure (similar to the way backpropagation is used inside such a procedure when training feedforward neural nets) to compute weight update.\n\nThe basic, single-step contrastive divergence (CD-1) procedure for a single sample can be summarized as follows:\n\n\nA Practical Guide to Training RBMs written by Hinton can be found on his homepage.\n\n\n"}
{"id": "2554240", "url": "https://en.wikipedia.org/wiki?curid=2554240", "title": "Semi-symmetric graph", "text": "Semi-symmetric graph\n\nIn the mathematical field of graph theory, a semi-symmetric graph is an undirected graph that is edge-transitive and regular, but not vertex-transitive. In other words, a graph is semi-symmetric if each vertex has the same number of incident edges, and there is a symmetry taking any of the graph's edges to any other of its edges, but there is some pair of vertices such that no symmetry maps the first into the second.\n\nA semi-symmetric graph must be bipartite, and its automorphism group must act transitively on each of the two vertex sets of the bipartition. For instance, in the diagram of the Folkman graph shown here, green vertices can not be mapped to red ones by any automorphism, but every two vertices of the same color are symmetric with each other.\n\nSemi-symmetric graphs were first studied E. Dauber, a student of F. Harary, in a paper, no longer available, titled \"On line- but not point-symmetric graphs\". This was seen by Jon Folkman, whose paper, published in 1967, includes the smallest semi-symmetric graph, now known as the Folkman graph, on 20 vertices.\nThe term \"semi-symmetric\" was first used by Klin \"et al.\" in a paper they published in 1978.\n\nThe smallest cubic semi-symmetric graph (that is, one in which each vertex is incident to exactly three edges) is the Gray graph on 54 vertices. It was first observed to be semi-symmetric by . It was proven to be the smallest cubic semi-symmetric graph by Dragan Marušič and Aleksander Malnič.\n\nAll the cubic semi-symmetric graphs on up to 768 vertices are known. According to Conder, Malnič, Marušič and Potočnik, the four smallest possible cubic semi-symmetric graphs after the Gray graph are the Iofinova–Ivanov graph on 110 vertices, the Ljubljana graph on 112 vertices, a graph on 120 vertices with girth 8 and the Tutte 12-cage.\n"}
{"id": "46521179", "url": "https://en.wikipedia.org/wiki?curid=46521179", "title": "Spoofing (finance)", "text": "Spoofing (finance)\n\nSpoofing is a disruptive algorithmic trading entity employed by traders to outpace other market participants and to manipulate commodity markets. Spoofers feign interest in trading futures, stocks and other products in financial markets creating an illusion of exchange pessimism in the futures market when many offers are being cancelled or withdrawn, or false optimism or demand when many offers are being placed in bad faith. Spoofers bid or offer with intent to cancel before the orders are filled. The flurry of activity around the buy or sell orders is intended to attract other high-frequency traders (HFT) to induce a particular market reaction such as manipulating the market price of a security. Spoofing can be a factor in the rise and fall of the price of shares and can be very profitable to the spoofer who can time buying and selling based on this manipulation. Under the 2010 Dodd-Frank Act spoofing is defined as \"the illegal practice of bidding or offering with intent to cancel before execution.\" Spoofing can be used with layering algorithms and front-running, activities which are also illegal. High-frequency trading, the primary form of algorithmic trading used in financial markets is very profitable as it deals in high volumes of transactions. The five-year delay in arresting the lone spoofer, Navinder Singh Sarao, accused of exacerbating the 2010 Flash Crash—one of the most turbulent periods in the history of financial markets— has placed the self-regulatory bodies such as the Commodity Futures Trading Commission (CFTC) and Chicago Mercantile Exchange & Chicago Board of Trade under scrutiny. The CME was described as being in a \"massively conflicted\" position as they make huge profits from the HFT and algorithmic trading.\n\nIn Australia layering and spoofing in 2014 referred to the act of \"submitting a genuine order on one side of the book and multiple orders at different prices on the other side of the book to give the impression of substantial supply/demand, with a view to sucking in other orders to hit the genuine order. After the genuine order trades, the multiple orders on the other side are rapidly withdrawn.\"\n\nIn a 2012 report Finansinspektionen (FI), the Swedish Financial Supervisory Authority defined spoofing/layering as \"a strategy of placing orders that is intended to manipulate the price of an instrument, for example through a combination of buy and sell orders.\"\n\nIn the U.S. Department of Justice April 21, 2015 complaint of market manipulation and fraud laid against Navinder Singh Sarao, — dubbed the Hounslow day-trader — appeared \"to have used this 188-and-289-lot spoofing technique in certain instances to intensify the manipulative effects of his dynamic layering technique...The purpose of these bogus orders is to trick other market participants and manipulate the product's market price.\" He employed the technique of dynamic layering, a form of market manipulation in which traders \"place large sell orders for contracts\" tied to the Standard & Poor's 500 Index. Sarao used his customized computer-trading program from 2009 onwards.\n\nIn July 2013 the US Commodity Futures Trading Commission (CFTC) and Britain's Financial Conduct Authority (FCA) brought a milestone case against spoofing which represents the first Dodd-Frank Act application. A federal grand jury in Chicago indicted Panther Energy Trading and Michael Coscia, a high-frequency trader. In 2011 Coscia placed spoofed orders through CME Group Inc. and European futures markets with profits of almost $1.6 million. Coscia was charged with six counts of spoofing with each count carrying a maximum sentence of ten years in prison and a maximum fine of one million dollars. The illegal activity undertaken by Coscia and his firm took place in a six-week period from \"August 8, 2011 through October 18, 2011 on CME Group’s Globex trading platform.\" They used a \"computer algorithm that was designed to unlawfully place and quickly cancel orders in exchange-traded futures contracts.\" They placed a \"relatively small order to sell futures that they did want to execute, which they quickly followed with several large buy orders at successively higher prices that they intended to cancel. By placing the large buy orders, Mr. Coscia and Panther sought to give the market the impression that there was significant buying interest, which suggested that prices would soon rise, raising the likelihood that other market participants would buy from the small order Coscia and Panther were then offering to sell.\"\n\nBritain's FCA is also fining Coscia and his firm approximately $900,000 for \"taking advantage of the price movements generated by his layering strategy\" relating to his market abuse activities on the ICE Futures Europe exchange. They earned US$279,920 in profits over the six weeks period \"at the expense of other market participants – primarily other High Frequency Traders or traders using algorithmic and/or automated systems.\"\n\nOn 18 April 2014 Robbins Geller Rudman & Dowd LLP filed a class-action lawsuit on behalf of the city of Providence, Rhode Island in Federal Court in the Southern District of New York. The complaint in the high frequency matter named \"every major stock exchange in the U.S.\" This includes the New York Stock Exchange, Nasdaq, Better Alternative Trading System (Bats) — an electronic communication network (ECN)\nand Direct Edge among others. The suit also names major Wall Street firms including but not limited to, Goldman Sachs, Citigroup, JPMorgan and the Bank of America. High-frequency trading firms and hedge funds are also named in the lawsuit. The lawsuit claimed that, \"For at least the last five years, the Defendants routinely engaged in at least the following manipulative, self-dealing and deceptive conduct,\" which included \"spoofing – where the HFT Defendants send out orders with corresponding cancellations, often at the opening or closing of the stock market, in order to manipulate the market price of a security and/or induce a particular market reaction.\"\n\nCFTC’s Enforcement Director, David Meister, explained the difference between legal and illegal use of algorithmic trading,\n\nIt is \"against the law to spoof, or post requests to buy or sell futures, stocks and other products in financial markets without intending to actually follow through on those orders.\" Anti-spoofing statute is part of the 2010 Dodd-Frank Wall Street Reform and Consumer Protection Act passed in July 21, 2010. The Dodd-Frank brought significant changes to financial regulation in the United States. It made changes in the American financial regulatory environment that affect all federal financial regulatory agencies and almost every part of the nation's financial services industry.\n\nEric Moncada, another trader is accused of spoofing in wheat futures markets and faces CFTC fines of $1.56 million.\n\nOn April 21, 2015, five years after the incident, the U.S. Department of Justice laid \"22 criminal counts, including fraud and market manipulation\" against Navinder Singh Sarao, who became known as the Hounslow day-trader. Among the charges included was the use of spoofing algorithms, in which first, just prior to the Flash Crash, he placed thousands of E-mini S&P 500 stock index futures contract orders. These orders, amounting to about \"$200 million worth of bets that the market would fall\" were \"replaced or modified 19,000 times\" before they were cancelled that afternoon. Spoofing, layering and front-running are now banned. The CTFC concluded that Sarao \"was at least significantly responsible for the order imbalances\" in the derivatives market which affected stock markets and exacerbated the flash crash. Sarao began his alleged market manipulation in 2009 with commercially available trading software whose code he modified \"so he could rapidly place and cancel orders automatically.\" Sarao is a 36-year-old small-time trader who worked from his parents’ modest semi-attached stucco house in Hounslow in suburban west London. \"Traders Magazine\" correspondent John Bates argues that by April 2015, traders can still manipulate and impact markets in spite of regulators and banks' new, improved monitoring of automated trade systems. For years, Sarao denounced high-frequency traders, some of them billion-dollar organisations, who mass manipulate the market by generating and retract numerous buy and sell orders every millisecond (\"quote stuffing\") — which he witnessed when placing trades at the Chicago Mercantile Exchange (CME). Sarao claimed that he made his choices to buy and sell based on opportunity and intuition and did not consider himself to be one of the HFTs.\n\nThe 2010 Flash Crash was a United States trillion-dollar stock market crash, in which the \"S&P 500, the Nasdaq 100, and the Russell 2000 collapsed and rebounded with extraordinary velocity.\" Dow Jones Industrial Average \"experienced the biggest intraday point decline in its entire history,\" plunging 998.5 points (about 9%), most within minutes, only to recover a large part of the loss. A CFTC 2014 report described it as one of the most turbulent periods in the history of financial markets.\n\nIn 2011 the chief economist of the Bank of England — Andrew Haldane — delivered a famous speech entitled the \"Race to Zero\" at the International Economic Association Sixteenth World Congress in which he described how \"equity prices of some of the world’s biggest companies were in freefall. They appeared to be in a race to zero. Peak to trough.\" At the time of the speech Haldane acknowledged that there were many theories about the cause of the Flash Crash but that academics, governments and financial experts remained \"agog.\"\n\n"}
{"id": "330994", "url": "https://en.wikipedia.org/wiki?curid=330994", "title": "Squeeze theorem", "text": "Squeeze theorem\n\nIn calculus, the squeeze theorem, also known as the pinching theorem, the sandwich theorem, the sandwich rule, and sometimes the squeeze lemma, is a theorem regarding the limit of a function. \n\nThe squeeze theorem is used in calculus and mathematical analysis. It is typically used to confirm the limit of a function via comparison with two other functions whose limits are known or easily computed. It was first used geometrically by the mathematicians Archimedes and Eudoxus in an effort to compute , and was formulated in modern terms by Carl Friedrich Gauss.\n\nIn many languages (e.g. French, German, Italian and Russian), the squeeze theorem is also known as the two policemen (and a drunk) theorem, or some variation thereof. The story is that if two policemen are escorting a drunk prisoner between them, and both officers go to a cell, then (regardless of the path taken, and the fact that the prisoner may be wobbling about between the policemen) the prisoner must also end up in the cell.\n\nThe squeeze theorem is formally stated as follows.\n\nLet \"I\" be an interval having the point \"a\" as a limit point. Let \"g\", \"f\", and \"h\" be functions defined on \"I\", except possibly at \"a\" itself. Suppose that for every \"x\" in \"I\" not equal to \"a\", we have\n\nand also suppose that\n\nThen formula_3\nThis theorem is also valid for sequences. Let formula_13 be two sequences converging to formula_14, and formula_15 a sequence. If formula_16 we have formula_17, then formula_15 also converges to formula_14.\n\nFrom the above hypotheses we have, taking the limit inferior and superior:\nso all the inequalities are indeed equalities, and the thesis immediately follows.\n\nA direct proof, using the formula_21-definition of limit, would be to prove that for all real formula_22 there exists a real formula_23 such that for all formula_24 with formula_25, we have formula_26. Symbolically,\n\nAs\n\nmeans that\n\nand \n\nmeans that\n\nthen we have\n\nWe can choose formula_34. Then, if formula_35, combining (1) and (2), we have\n\nwhich completes the proof. formula_38\n\nThe proof for sequences is very similar, using the formula_39-definition of a limit of a sequence.\n\nThere is also the squeeze theorem for series, which can be stated as follows:\nLet formula_40 be two convergent series. If formula_41 such that formula_42then formula_43 also converges.\n\nLet formula_40 be two convergent series. Hence, the sequences formula_45 are Cauchy. That is, for fixed formula_46,\n\nformula_47 such that formula_48 (1)\n\nand similarly formula_49 such that formula_50 (2).\n\nWe know that formula_51 such that formula_52. Hence, formula_53, we have combining (1) and (2):\n\nformula_54.\n\nTherefore formula_55is a Cauchy sequence. So formula_43 converges. formula_38\n\nThe limit\n\ncannot be determined through the limit law\n\nbecause\n\ndoes not exist.\n\nHowever, by the definition of the sine function,\n\nIt follows that\n\nSince formula_63, by the squeeze theorem, formula_64 must also be 0.\n\nProbably the best-known examples of finding a limit by squeezing are the proofs of the equalities\n\nThe first limit follows by means of the squeeze theorem from the fact that\n\nfor \"x\" close enough to 0. The correctness of which for positive x can be seen by simple geometric reasoning (see drawing) that can be extended to negative x as well. The second limit follows from the squeeze theorem and the fact that\n\nfor \"x\" close enough to 0. This can be derived by replacing formula_68 in the earlier fact by formula_69 and squaring the resulting inequality.\n\nThese two limits are used in proofs of the fact that the derivative of the sine function is the cosine function. That fact is relied on in other proofs of derivatives of trigonometric functions.\n\nIt is possible to show that\n\nby squeezing, as follows.\n\nIn the illustration at right, the area of the smaller of the two shaded sectors of the circle is\n\nsince the radius is sec \"θ\" and the arc on the unit circle has length Δ\"θ\". Similarly, the area of the larger of the two shaded sectors is\n\nWhat is squeezed between them is the triangle whose base is the vertical segment whose endpoints are the two dots. The length of the base of the triangle is tan(\"θ\" + Δ\"θ\") − tan(\"θ\"), and the height is 1. The area of the triangle is therefore\n\nFrom the inequalities\n\nwe deduce that\n\nprovided Δ\"θ\" > 0, and the inequalities are reversed if Δ\"θ\" < 0. Since the first and third expressions approach sec\"θ\" as Δ\"θ\" → 0, and the middle expression approaches (\"d\"/\"dθ\") tan \"θ\", the desired result follows.\n\nThe squeeze theorem can still be used in multivariable calculus but the lower (and upper functions) must be below (and above) the target function not just along a path but around the entire neighborhood of the point of interest and it only works if the function really does have a limit there. It can, therefore, be used to prove that a function has a limit at a point, but it can never be used to prove that a function does not have a limit at a point.\n\ncannot be found by taking any number of limits along paths that pass through the point, but since\n\ntherefore, by the squeeze theorem,\n\n"}
{"id": "4074422", "url": "https://en.wikipedia.org/wiki?curid=4074422", "title": "Stochastic modelling (insurance)", "text": "Stochastic modelling (insurance)\n\n\"Stochastic\" means being or having a random variable. A stochastic model is a tool for estimating probability distributions of potential outcomes by allowing for random variation in one or more inputs over time. The random variation is usually based on fluctuations observed in historical data for a selected period using standard time-series techniques. Distributions of potential outcomes are derived from a large number of simulations (stochastic projections) which reflect the random variation in the input(s).\n\nIts application initially started in physics. It is now being applied in engineering, life sciences, social sciences, and finance. See also Economic capital.\n\nLike any other company, an insurer has to show that its assets exceeds its liabilities to be solvent. In the insurance industry, however, assets and liabilities are not known entities. They depend on how many policies result in claims, inflation from now until the claim, investment returns during that period, and so on.\n\nSo the valuation of an insurer involves a set of projections, looking at what is expected to happen, and thus coming up with the best estimate for assets and liabilities, and therefore for the company's level of solvency.\n\nThe simplest way of doing this, and indeed the primary method used, is to look at best estimates.\n\nThe projections in financial analysis usually use the most likely rate of claim, the most likely investment return, the most likely rate of inflation, and so on. The projections in engineering analysis usually use both the most likely rate and the most critical rate. The result provides a point estimate - the best single estimate of what the company's current solvency position is, or multiple points of estimate - depends on the problem definition. Selection and identification of parameter values are frequently a challenge to less experienced analysts.\n\nThe downside of this approach is it does not fully cover the fact that there is a whole range of possible outcomes and some are more probable and some are less.\n\nA stochastic model would be to set up a projection model which looks at a single policy, an entire portfolio or an entire company. But rather than setting investment returns according to their most likely estimate, for example, the model uses random variations to look at what investment conditions might be like.\n\nBased on a set of random variables, the experience of the policy/portfolio/company is projected, and the outcome is noted. Then this is done again with a new set of random variables. In fact, this process is repeated thousands of times.\n\nAt the end, a distribution of outcomes is available which shows not only the most likely estimate but what ranges are reasonable too. The most likely estimate is given by the distribution curve's (formally known as the Probability density function) center of mass which is typically also the peak(mode) of the curve, but may be different e.g. for asymmetric distributions.\n\nThis is useful when a policy or fund provides a guarantee, e.g. a minimum investment return of 5% per annum. A deterministic simulation, with varying scenarios for future investment return, does not provide a good way of estimating the cost of providing this guarantee. This is because it does not allow for the volatility of investment returns in each future time period or the chance that an extreme event in a particular time period leads to an investment return less than the guarantee. Stochastic modelling builds volatility and variability (randomness) into the simulation and therefore provides a better representation of real life from more angles.\n\nStochastic models help to assess the interactions between variables, and are useful tools to numerically evaluate quantities, as they are usually implemented using Monte Carlo simulation techniques (see Monte Carlo method). While there is an advantage here, in estimating quantities that would otherwise be difficult to obtain using analytical methods, a disadvantage is that such methods are limited by computing resources as well as simulation error. Below are some examples:\n\nUsing statistical notation, it is a well-known result that the mean of a function, f, of a random variable X is not necessarily the function of the mean of X.\n\nFor example, in application, applying the best estimate (defined as the mean) of investment returns to discount a set of cash flows will not necessarily give the same result as assessing the best estimate to the discounted cash flows.\n\nA stochastic model would be able to assess this latter quantity with simulations.\n\nThis idea is seen again when one considers percentiles (see percentile). When assessing risks at specific percentiles, the factors that contribute to these levels are rarely at these percentiles themselves. Stochastic models can be simulated to assess the percentiles of the aggregated distributions.\n\nTruncating and censoring of data can also be estimated using stochastic models. For instance, applying a non-proportional reinsurance layer to the best estimate losses will not necessarily give us the best estimate of the losses after the reinsurance layer. In a simulated stochastic model, the simulated losses can be made to \"pass through\" the layer and the resulting losses assessed appropriately.\n\nAlthough the text above referred to \"random variations\", the stochastic model does not just use any arbitrary set of values. The asset model is based on detailed studies of how markets behave, looking at averages, variations, correlations, and more.\n\nThe models and underlying parameters are chosen so that they fit historical economic data, and are expected to produce meaningful future projections.\n\nThere are many such models, including the Wilkie Model, the Thompson Model and the Falcon Model.\n\nThe claims arising from policies or portfolios that the company has written can also be modelled using stochastic methods. This is especially important in the general insurance sector, where the claim severities can have high uncertainties.\n\nDepending on the portfolios under investigation, a model can simulate all or some of the following factors stochastically:\n\n\nClaims inflations can be applied, based on the inflation simulations that are consistent with the outputs of the asset model, as are dependencies between the losses of different portfolios.\n\nThe relative uniqueness of the policy portfolios written by a company in the general insurance sector means that claims models are typically tailor-made.\n\nEstimating future claims liabilities might also involve estimating the uncertainty around the estimates of claim reserves.\n\nSee J Li's article \"Comparison of Stochastic Reserving Models\" (published in the \"Australian Actuarial Journal\", volume 12 issue 4) for a recent article on this topic.\n\n"}
{"id": "3350566", "url": "https://en.wikipedia.org/wiki?curid=3350566", "title": "Supersingular variety", "text": "Supersingular variety\n\nIn mathematics, a supersingular variety is (usually) a smooth projective variety in nonzero characteristic such that for all \"n\" the slopes of the Newton polygon of the \"n\"th crystalline cohomology are all \"n\"/2 . For special classes of varieties such as elliptic curves it is common to use various ad hoc definitions of \"supersingular\", which are (usually) equivalent to the one given above.\n\nThe term \"singular elliptic curve\" (or \"singular \"j\"-invariant\") was at one times used to refer to complex elliptic curves whose ring of endomorphisms has rank 2, the maximum possible. Helmut Hasse discovered that, in finite characteristic, elliptic curves can have larger rings of endomorphisms of rank 4, and these were called \"supersingular elliptic curves\". Supersingular elliptic curves can also be characterized by the slopes of their crystalline cohomology, and the term \"supersingular\" was later extended to other varieties whose cohomology has similar properties. The terms \"supersingular\" or \"singular\" do not mean that the variety has singularites.\n\nExamples include:\n"}
{"id": "5028195", "url": "https://en.wikipedia.org/wiki?curid=5028195", "title": "Suslin tree", "text": "Suslin tree\n\nIn mathematics, a Suslin tree is a tree of height ω such that\nevery branch and every antichain is at most countable. They are named after Mikhail Yakovlevich Suslin.\n\nEvery Suslin tree is an Aronszajn tree.\n\nThe existence of a Suslin tree is independent of ZFC, and is equivalent to the existence of a Suslin line (shown by ) or a Suslin algebra. The diamond principle, a consequence of V=L, implies that there is a Suslin tree, and Martin's axiom MA(ℵ) implies that there are no Suslin trees.\n\nMore generally, for any infinite cardinal κ, a κ-Suslin tree is a tree of height κ such that every branch and antichain has cardinality less than κ. In particular a Suslin tree is the same as a ω-Suslin tree. showed that if V=L then there is a κ-Suslin tree for every infinite successor cardinal κ. Whether the Generalized Continuum Hypothesis implies the existence of an ℵ-Suslin tree, is a longstanding open problem.\n\n\n"}
{"id": "7625671", "url": "https://en.wikipedia.org/wiki?curid=7625671", "title": "Tetralemma", "text": "Tetralemma\n\nThe tetralemma is a figure that features prominently in the logic of India. It states that with reference to any a logical proposition X, there are four possibilities:\n\nsee De Morgan's laws.\n\nThe history of fourfold negation, the Catuskoti (Sanskrit), is evident in the logico-epistemological tradition of India, given the categorical nomenclature Indian logic in Western discourse. Subsumed within the auspice of Indian logic, 'Buddhist logic' has been particularly focused in its employment of the fourfold negation, as evidenced by the traditions of Nagarjuna and the Madhyamaka, particularly the school of Madhyamaka given the retroactive nomenclature of Prasangika by the Tibetan Buddhist logico-epistemological tradition.\n\nA variant of the tetralemma is used in the Ancient Greek philosophical schools of Democritus and Pyrrhonism. Pyrrho includes it in his summary of his teachings, and Sextus Empiricus includes it among the Pyrrhonist Maxims.\n\n\n"}
{"id": "165259", "url": "https://en.wikipedia.org/wiki?curid=165259", "title": "Time value of money", "text": "Time value of money\n\nThe time value of money is the greater benefit of receiving money now rather than later. It is founded on time preference.\n\nThe time value of money explains why interest is paid or earned: Interest, whether it is on a bank deposit or debt, compensates the depositor or lender for the time value of money.\n\nIt also underlies investment. Investors are willing to forgo spending their money now only if they expect a favorable return on their investment in the future, such that the increased value to be available later is sufficiently high to offset the preference to have money now.\n\nThe Talmud (~500 CE) recognizes the time value of money. In Tractate Makkos page 3a the Talmud discusses a case where witnesses falsely claimed that the term of a loan was 30 days when it was actually 10 years. The false witnesses must pay the difference of the value of the loan \"in a situation where he would be required to give the money back (within) thirty days..., and that same sum in a situation where he would be required to give the money back (within) 10 years...The difference is the sum that the testimony of the (false) witnesses sought to have the borrower lose; therefore, it is the sum that they must pay.\" \n\nThe notion was later described by Martín de Azpilcueta (1491–1586) of the School of Salamanca.\n\nTime value of money problems involve the net value of cash flows at different points in time.\n\nIn a typical case, the variables might be: a balance (the real or nominal value of a debt or a financial asset in terms of monetary units), a periodic rate of interest, the number of periods, and a series of cash flows. (In the case of a debt, cash flows are payments against principal and interest; in the case of a financial asset, these are contributions to or withdrawals from the balance.) More generally, the cash flows may not be periodic but may be specified individually. Any of these variables may be the independent variable (the sought-for answer) in a given problem. For example, one may know that: the interest is 0.5% per period (per month, say); the number of periods is 60 (months); the initial balance (of the debt, in this case) is 25,000 units; and the final balance is 0 units. The unknown variable may be the monthly payment that the borrower must pay.\n\nFor example, £100 invested for one year, earning 5% interest, will be worth £105 after one year; therefore, £100 paid now \"and\" £105 paid exactly one year later \"both\" have the same value to a recipient who expects 5% interest assuming that inflation would be zero percent. That is, £100 invested for one year at 5% interest has a \"future value\" of £105 under the assumption that inflation would be zero percent.\n\nThis principle allows for the valuation of a likely stream of income in the future, in such a way that annual incomes are discounted and then added together, thus providing a lump-sum \"present value\" of the entire income stream; all of the standard calculations for time value of money derive from the most basic algebraic expression for the present value of a future sum, \"discounted\" to the present by an amount equal to the time value of money. For example, the future value sum formula_1 to be received in one year is discounted at the rate of interest formula_2 to give the present value sum formula_3:\n\nSome standard calculations based on the time value of money are:\n\n\n\nThere are several basic equations that represent the equalities listed above. The solutions may be found using (in most cases) the formulas, a financial calculator or a spreadsheet. The formulas are programmed into most financial calculators and several spreadsheet functions (such as PV, FV, RATE, NPER, and PMT).\n\nFor any of the equations below, the formula may also be rearranged to determine one of the other unknowns. In the case of the standard annuity formula, there is no closed-form algebraic solution for the interest rate (although financial calculators and spreadsheet programs can readily determine solutions through rapid trial and error algorithms).\n\nThese equations are frequently combined for particular uses. For example, bonds can be readily priced using these equations. A typical coupon bond is composed of two types of payments: a stream of coupon payments similar to an annuity, and a lump-sum return of capital at the end of the bond's maturity—that is, a future payment. The two formulas can be combined to determine the present value of the bond.\n\nAn important note is that the interest rate \"i\" is the interest rate for the relevant period. For an annuity that makes one payment per year, \"i\" will be the annual interest rate. For an income or payment stream with a different payment schedule, the interest rate must be converted into the relevant periodic interest rate. For example, a monthly rate for a mortgage with monthly payments requires that the interest rate be divided by 12 (see the example below). See compound interest for details on converting between different periodic interest rates.\n\nThe rate of return in the calculations can be either the variable solved for, or a predefined variable that measures a discount rate, interest, inflation, rate of return, cost of equity, cost of debt or any number of other analogous concepts. The choice of the appropriate rate is critical to the exercise, and the use of an incorrect discount rate will make the results meaningless.\n\nFor calculations involving annuities, you must decide whether the payments are made at the end of each period (known as an ordinary annuity), or at the beginning of each period (known as an annuity due). If you are using a financial calculator or a spreadsheet, you can usually set it for either calculation. The following formulas are for an ordinary annuity. If you want the answer for the present value of an annuity due, you can simply multiply the PV of an ordinary annuity by (1 + \"i\").\n\nThe following formula use these common variables:\n\nThe future value (\"FV\") formula is similar and uses the same variables.\n\nThe present value formula is the core formula for the time value of money; each of the other formulae is derived from this formula. For example, the annuity formula is the sum of a series of present value calculations.\n\nThe present value (\"PV\") formula has four variables, each of which can be solved for by numerical methods:\n\nThe cumulative present value of future cash flows can be calculated by summing the contributions of \"FV\", the value of cash flow at time \"t\":\n\nNote that this series can be summed for a given value of \"n\", or when \"n\" is ∞. This is a very general formula, which leads to several important special cases given below.\n\nIn this case the cash flow values remain the same throughout the \"n\" periods. The present value of an annuity (PVA) formula has four variables, each of which can be solved for by numerical methods:\n\nTo get the PV of an annuity due, multiply the above equation by (1 + \"i\").\n\nIn this case each cash flow grows by a factor of (1+\"g\"). Similar to the formula for an annuity, the present value of a growing annuity (PVGA) uses the same variables with the addition of \"g\" as the rate of growth of the annuity (A is the annuity payment in the first period). This is a calculation that is rarely provided for on financial calculators.\n\nWhere i ≠ g :\nWhere i = g :\n\nTo get the PV of a growing annuity due, multiply the above equation by (1 + \"i\").\n\nA perpetuity is payments of a set amount of money that occur on a routine basis and continue forever. When \"n\" → ∞, the \"PV\" of a perpetuity (a perpetual annuity) formula becomes a simple division.\n\nWhen the perpetual annuity payment grows at a fixed rate (\"g\", with \"g\" < \"i\") the value is determined according to the following formula, obtained by setting \"n\" to infinity in the earlier formula for a growing perpetuity:\n\nIn practice, there are few securities with precise characteristics, and the application of this valuation approach is subject to various qualifications and modifications. Most importantly, it is rare to find a growing perpetual annuity with fixed rates of growth and true perpetual cash flow generation. Despite these qualifications, the general approach may be used in valuations of real estate, equities, and other assets.\n\nThis is the well known Gordon Growth model used for stock valuation.\n\nThe future value (after \"n\" periods) of an annuity (FVA) formula has four variables, each of which can be solved for by numerical methods:\n\nTo get the FV of an annuity due, multiply the above equation by (1 + i).\n\nThe future value (after \"n\" periods) of a growing annuity (FVA) formula has five variables, each of which can be solved for by numerical methods:\n\nWhere i ≠ g :\nWhere i = g :\n\nThe following table summarizes the different formulas commonly used in calculating the time value of money. These values are often displayed in tables where the interest rate and time are specified.\nNotes:\n\nThe formula for the present value of a regular stream of future payments (an annuity) is derived from a sum of the formula for future value of a single future payment, as below, where \"C\" is the payment amount and \"n\" the period.\n\nA single payment C at future time \"m\" has the following future value at future time \"n\":\n\nSumming over all payments from time 1 to time n, then reversing t\n\nNote that this is a geometric series, with the initial value being \"a\" = \"C\", the multiplicative factor being 1 + \"i\", with \"n\" terms. Applying the formula for geometric series, we get\n\nThe present value of the annuity (PVA) is obtained by simply dividing by formula_19:\n\nAnother simple and intuitive way to derive the future value of an annuity is to consider an endowment, whose interest is paid as the annuity, and whose principal remains constant. The principal of this hypothetical endowment can be computed as that whose interest equals the annuity payment amount:\n\nNote that no money enters or leaves the combined system of endowment principal + accumulated annuity payments, and thus the future value of this system can be computed simply via the future value formula:\n\nInitially, before any payments, the present value of the system is just the endowment principal (formula_24). At the end, the future value is the endowment principal (which is the same) plus the future value of the total annuity payments (formula_25). Plugging this back into the equation:\n\nWithout showing the formal derivation here, the perpetuity formula is derived from the annuity formula. Specifically, the term:\ncan be seen to approach the value of 1 as \"n\" grows larger. At infinity, it is equal to 1, leaving formula_29 as the only term remaining.\n\nRates are sometimes converted into the continuous compound interest rate equivalent because the continuous equivalent is more convenient (for example, more easily differentiated). Each of the formulæ above may be restated in their continuous equivalents. For example, the present value at time 0 of a future payment at time t can be restated in the following way, where e is the base of the natural logarithm and r is the continuously compounded rate:\nThis can be generalized to discount rates that vary over time: instead of a constant discount rate \"r,\" one uses a function of time \"r\"(\"t\"). In that case the discount factor, and thus the present value, of a cash flow at time \"T\" is given by the integral of the continuously compounded rate \"r\"(\"t\"):\nIndeed, a key reason for using continuous compounding is to simplify the analysis of varying discount rates and to allow one to use the tools of calculus. Further, for interest accrued and capitalized overnight (hence compounded daily), continuous compounding is a close approximation for the actual daily compounding. More sophisticated analysis includes the use of differential equations, as detailed below.\n\nUsing continuous compounding yields the following formulas for various instruments:\nThese formulas assume that payment A is made in the first payment period and annuity ends at time t.\n\nOrdinary and partial differential equations (ODEs and PDEs) – equations involving derivatives and one (respectively, multiple) variables are ubiquitous in more advanced treatments of financial mathematics. While time value of money can be understood without using the framework of differential equations, the added sophistication sheds additional light on time value, and provides a simple introduction before considering more complicated and less familiar situations. This exposition follows .\n\nThe fundamental change that the differential equation perspective brings is that, rather than computing a \"number\" (the present value \"now\"), one computes a \"function\" (the present value now or at any point in \"future\"). This function may then be analyzed—how does its value change over time—or compared with other functions.\n\nFormally, the statement that \"value decreases over time\" is given by defining the linear differential operator formula_37 as:\nThis states that values decreases (−) over time (∂) at the discount rate (\"r\"(\"t\")). Applied to a function it yields:\nFor an instrument whose payment stream is described by \"f\"(\"t\"), the value \"V\"(\"t\") satisfies the inhomogeneous first-order ODE formula_40 (\"inhomogeneous\" is because one has \"f\" rather than 0, and \"first-order\" is because one has first derivatives but no higher derivatives) – this encodes the fact that when any cash flow occurs, the value of the instrument changes by the value of the cash flow (if you receive a £10 coupon, the remaining value decreases by exactly £10).\n\nThe standard technique tool in the analysis of ODEs is Green's functions, from which other solutions can be built. In terms of time value of money, the Green's function (for the time value ODE) is the value of a bond paying £1 at a single point in time \"u\" – the value of any other stream of cash flows can then be obtained by taking combinations of this basic cash flow. In mathematical terms, this instantaneous cash flow is modeled as a Dirac delta function formula_41\n\nThe Green's function for the value at time \"t\" of a £1 cash flow at time \"u\" is\nwhere \"H\" is the Heaviside step function – the notation \"formula_43\" is to emphasize that \"u\" is a \"parameter\" (fixed in any instance—the time when the cash flow will occur), while \"t\" is a \"variable\" (time). In other words, future cash flows are exponentially discounted (exp) by the sum (integral, formula_44) of the future discount rates (formula_45 for future, \"r\"(\"v\") for discount rates), while past cash flows are worth 0 (formula_46), because they have already occurred. Note that the value \"at\" the moment of a cash flow is not well-defined – there is a discontinuity at that point, and one can use a convention (assume cash flows have already occurred, or not already occurred), or simply not define the value at that point.\n\nIn case the discount rate is constant, formula_47 this simplifies to\nwhere formula_49 is \"time remaining until cash flow\".\n\nThus for a stream of cash flows \"f\"(\"u\") ending by time \"T\" (which can be set to formula_50 for no time horizon) the value at time \"t,\" formula_51 is given by combining the values of these individual cash flows:\nThis formalizes time value of money to future values of cash flows with varying discount rates, and is the basis of many formulas in financial mathematics, such as the Black–Scholes formula with varying interest rates.\n\n\n"}
{"id": "11904309", "url": "https://en.wikipedia.org/wiki?curid=11904309", "title": "Vis5D", "text": "Vis5D\n\nVis5D is a 3D visualization system used primarily for animated 3D visualization of weather simulations. It was the first system to produce fully interactive animated 3D displays of time-dynamic volumetric data sets and the first open source 3D visualization system. It is GNU GPL licensed.\n\nVis5D was created in response to two circumstances:\n\nVis5D takes its name from its 5D array containing time sequences of 3D spatial grids for a set of physical parameters of the atmosphere or ocean. Its graphical user interface enables users to select from various ways of visualizing each parameter (e.g., iso-surfaces, plane slices, volume renderings), and to select a combination of parameters for view. A key innovation of Vis5D is that it computes and stores the geometries and colors for such graphics over the simulated time sequence, allowing them to be animated quickly so users can watch movies of their simulations. Furthermore, users can interactively rotate the animations in 3D.\n\nVis5D provides other visualization techniques. Users can drag a 3D cursor to a selected time and location, then trigger the calculation of a forward and backward wind trajectory from that point. Users can drag a vertical bar cursor and see, in another window, a thermodynamic diagram for the selected vertical column of atmosphere. And users can drag a 3D cursor to a selected time and location and read out individual values for parameters at that point. These examples all involve direct manipulation interfaces, as does the placement of plane slices through 3D grids.\n\nVis5D provides options for memory management, so that very large data sets can be visualized at individual time steps without the need to compute graphics over the simulation's entire time sequence, while smaller data sets can be visualized with full animation. Vis5D also provides an API enabling developers of other systems to incorporate Vis5D's functionality. This API is the basis of a TCL scripting capability so users can write automated scripts for producing animations.\n\nVis5D was first demonstrated, via videotape, at the December 1988 Workshop on Graphics in Meteorology at the ECMWF. The first live demos were at the January 1989 annual meeting of the American Meteorological Society.\n\nVis5D running on the GS 1000 was the first visualization system to provide smooth animation of 3D gridded time-sequence data sets with interactive rotation.\n\nVis5D was the first open-source 3D visualization system.\n\nVis5D is a natural for immersive virtual reality and was adapted to the CAVE for the VROOM at the 1994 Siggraph conference. This became Cave5D.\n\n\n"}
