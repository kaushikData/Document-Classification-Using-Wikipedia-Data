{"id": "834364", "url": "https://en.wikipedia.org/wiki?curid=834364", "title": "213 (number)", "text": "213 (number)\n\n213 (two hundred [and] thirteen) is the number following 212 and preceding 214.\n\nIt is the smallest of a triple consecutive numbers that are products of two distinct prime numbers: 213 = 3 × 71, 214 = 2 × 107, and 215 = 5 × 43. Its square is a sum of distinct factorials: 213 = 45369 = 1! + 2! + 3! + 7! + 8!.\n\n"}
{"id": "403326", "url": "https://en.wikipedia.org/wiki?curid=403326", "title": "66 (number)", "text": "66 (number)\n\n66 (sixty-six) is the natural number following 65 and preceding 67.\n\nUsages of this number include:\n\n66 is:\n\n\n\n\n66 (more specifically 66.667) megahertz (MHz) is a common divisor for the front side bus (FSB) speed, overall central processing unit (CPU) speed, and base bus speed. On a Core 2 CPU, and a Core 2 motherboard, the FSB is 1066 MHz (~16 × 66 MHz), the memory speed is usually 666.67 MHz (~10 × 66 MHz), and the processor speed ranges from 1.86 gigahertz (GHz) (~66 MHz × 28) to 2.93 GHz (~66 MHz × 44), in 266 MHz (~66 MHz × 4) increments.\n\n\n\nThe number of the laps of the Spanish Grand Prix.\n\n\n\n\n"}
{"id": "30904877", "url": "https://en.wikipedia.org/wiki?curid=30904877", "title": "Age of the captain", "text": "Age of the captain\n\nThe age of the captain is a mathematical word problem which cannot be answered even though there seems to be plenty of information supplied. It was given for the first time by Gustave Flaubert in a letter to his sister Caroline in 1841:\n\nPuisque tu fais de la géométrie et de la trigonométrie, je vais te donner un problème : Un navire est en mer, il est parti de Boston chargé de coton, il jauge 200 tonneaux. Il fait voile vers le Havre, le grand mât est cassé, il y a un mousse sur le gaillard d’avant, les passagers sont au nombre de douze, le vent souffle N.-E.-E., l’horloge marque 3 heures un quart d’après-midi, on est au mois de mai…. On demande l’âge du capitaine?\nSince you are now studying geometry and trigonometry, I will give you a problem. A ship sails the ocean. It left Boston with a cargo of wool. It grosses 200 tons. It is bound for Le Havre. The mainmast is broken, the cabin boy is on deck, there are 12 passengers aboard, the wind is blowing East-North-East, the clock points to a quarter past three in the afternoon. It is the month of May. How old is the captain?\nMore recently, a simpler version has been used to study how students react to word problems:\n\nA captain owns 26 sheep and 10 goats. How old is the captain?\nMany children in elementary school, from different parts of the world, attempt to \"solve\" this nonsensical problem by giving the answer 36, obtained by adding the numbers 26 and 10. It has been suggested that this indicates schooling and education fail to teach children critical thinking, and that a question may be unsolvable. But, others have countered that in education students are taught that all questions have a solution and that giving any answer is better than leaving it blank, hence the attempt to \"solve\" it.\n"}
{"id": "4287", "url": "https://en.wikipedia.org/wiki?curid=4287", "title": "Biconditional elimination", "text": "Biconditional elimination\n\nBiconditional elimination is the name of two valid rules of inference of propositional logic. It allows for one to infer a conditional from a biconditional. If formula_1 is true, then one may infer that formula_2 is true, and also that formula_3 is true. For example, if it's true that I'm breathing if and only if I'm alive, then it's true that if I'm breathing, I'm alive; likewise, it's true that if I'm alive, I'm breathing. The rules can be stated formally as:\n\nand\n\nwhere the rule is that wherever an instance of \"formula_1\" appears on a line of a proof, either \"formula_2\" or \"formula_3\" can be placed on a subsequent line;\n\nThe \"biconditional elimination\" rule may be written in sequent notation:\nand\n\nwhere formula_11 is a metalogical symbol meaning that formula_2, in the first case, and formula_3 in the other are syntactic consequences of formula_1 in some logical system;\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_17, and formula_18 are propositions expressed in some formal system.\n\n"}
{"id": "43402512", "url": "https://en.wikipedia.org/wiki?curid=43402512", "title": "Bondage number", "text": "Bondage number\n\nIn mathematics, the bondage number of a nonempty graph is the cardinality of the smallest set \"E\" of edges such that the domination number of the graph with the edges \"E\" removed is strictly greater than the domination number of the original graph.\nThe concept was introduced by Fink et. al.\n"}
{"id": "52671280", "url": "https://en.wikipedia.org/wiki?curid=52671280", "title": "Bondi–Metzner–Sachs group", "text": "Bondi–Metzner–Sachs group\n\nThe ordinary Bondi–Metzner–Sachs (BMS) group B is the common asymptotic symmetry group of all radiating, asymptotically flat, Lorentzian spacetimes. This means that B is currently the best candidate for the universal symmetry group of General Relativity. It was originally proposed in 1962 by H. Bondi, M. G. van der Burg, and A. W. Metzner.\n"}
{"id": "40900167", "url": "https://en.wikipedia.org/wiki?curid=40900167", "title": "Cauchy–Euler operator", "text": "Cauchy–Euler operator\n\nIn mathematics a Cauchy–Euler operator is a differential operator of the form formula_1 for a polynomial \"p\". It is named after Augustin-Louis Cauchy and Leonhard Euler. The simplest example is that in which \"p\"(\"x\") = \"x\", which has eigenvalues \"n\" = 0, 1, 2, 3, ... and corresponding eigenfunctions \"x\".\n\n"}
{"id": "84130", "url": "https://en.wikipedia.org/wiki?curid=84130", "title": "Centrifugal governor", "text": "Centrifugal governor\n\nA centrifugal governor is a specific type of governor with a feedback system that controls the speed of an engine by regulating the amount of fuel (or working fluid) admitted, so as to maintain a near-constant speed, irrespective of the load or fuel-supply conditions. It uses the principle of proportional control.\n\nCentrifugal governors were invented by Christiaan Huygens and used to regulate the distance and pressure between millstones in windmills in the 17th century. In 1788, James Watt adapted one to control his steam engine where it regulates the admission of steam into the cylinder(s), a development that proved so important he is sometimes called the inventor. Centrifugal governors' widest use was on steam engines during the Steam Age in the 19th century. They are also found on stationary internal combustion engines and variously fueled turbines, and in some modern striking clocks.\n\nA simple governor does not maintain an exact speed but a speed range, since under increasing load the governor opens the throttle as the speed (RPM) decreases.\n\nThe devices shown are on steam engines. Power is supplied to the governor from the engine's output shaft by a belt or chain connected to the lower belt wheel. The governor is connected to a throttle valve that regulates the flow of working fluid (steam) supplying the prime mover. As the speed of the prime mover increases, the central spindle of the governor rotates at a faster rate and the kinetic energy of the balls increases. This allows the two masses on lever arms to move outwards and upwards against gravity. If the motion goes far enough, this motion causes the lever arms to pull down on a thrust bearing, which moves a beam linkage, which reduces the aperture of a throttle valve. The rate of working-fluid entering the cylinder is thus reduced and the speed of the prime mover is controlled, preventing over-speeding.\n\nMechanical stops may be used to limit the range of throttle motion, as seen near the masses in the image at right.\n\nA limitation of the two-arm, two-ball governor is its reliance on gravity, and that the governor must stay upright relative to the surface of the Earth for gravity to retract the balls when the governor slows down.\n\nGovernors can be built that do not use gravitational force, by using a single straight arm with weights on both ends, a center pivot attached to a spinning axle, and a spring that tries to force the weights towards the center of the spinning axle. The two weights on opposite ends of the pivot arm counterbalance any gravitational effects, but both weights use centrifugal force to work against the spring and attempt to rotate the pivot arm towards a perpendicular axis relative to the spinning axle.\n\nSpring-retracted non-gravitational governors are commonly used in single-phase alternating current (AC) induction motors to turn off the starting field coil when the motor's rotational speed is high enough.\n\nThey are also commonly used in snowmobile and all-terrain vehicle (ATV) continuously variable transmissions (CVT), both to engage/disengage vehicle motion and to vary the transmission's pulley diameter ratio in relation to the engine revolutions per minute.\n\nJames Watt designed his first governor in 1788 following a suggestion from his business partner Matthew Boulton. It was a conical pendulum governor and one of the final series of innovations Watt had employed for steam engines. James Watt never claimed the centrifugal governor to be an invention of his own. A giant statue of Watt's governor stands at Smethwick in the English West Midlands. It is known as the flyball governor.\n\nCentrifugal governors are also used in many modern repeating watches to limit the speed of the striking train, so the repeater doesn't run too quickly.\n\nAnother kind of centrifugal governor consists of a pair of masses on a spindle inside a cylinder, the masses or the cylinder being coated with pads, somewhat like a drum brake. This is used in a spring-loaded record player and a spring-loaded telephone dial to limit the speed.\n\nThe centrifugal governor is often used in the cognitive sciences as an example of a dynamic system, in which the representation of information cannot be clearly separated from the operations being applied to the representation. And, because the governor is a servomechanism, its analysis in a dynamic system is not trivial. In 1868, James Clerk Maxwell wrote a famous paper \"On governors\" that is widely considered a classic in feedback control theory. Maxwell distinguishes moderators (a centrifugal brake) and governors which control motive power input. He considers devices by James Watt, Professor James Thomson, Fleeming Jenkin, William Thomson, Léon Foucault and Carl Wilhelm Siemens (a liquid governor).\n\nIn his famous 1858 paper to the Linnean Society, which led Darwin to publish On the Origin of Species, Alfred Russel Wallace used governors as a metaphor for the evolutionary principle:\n\nThe action of this principle is exactly like that of the centrifugal governor of the steam engine, which checks and corrects any irregularities almost before they become evident; and in like manner no unbalanced deficiency in the animal kingdom can ever reach any conspicuous magnitude, because it would make itself felt at the very first step, by rendering existence difficult and extinction almost sure soon to follow.\n\nBateson revisited the topic in his 1979 book \"Mind and Nature: A Necessary Unity\", and other scholars have continued to explore the connection between natural selection and systems theory.\n\nA centrifugal governor is part of the city seal of Manchester, New Hampshire in the U.S. and is also used on the city flag. A 2017 effort to change the design was rejected by voters. \n\n"}
{"id": "2310842", "url": "https://en.wikipedia.org/wiki?curid=2310842", "title": "Chuvash numerals", "text": "Chuvash numerals\n\nChuvash numerals is an ancient numeral system from the Old Turkic script the Chuvash people used. (Modern Chuvash use Hindu-Arabic numerals.)\n\nThose numerals originate from \"finger numeration\". They look like Roman numerals, but larger numerals stay at the right side. It was possible to carve those numerals on wood. In some cases numerals were preserved until the beginning of the 20th century.\n"}
{"id": "2817175", "url": "https://en.wikipedia.org/wiki?curid=2817175", "title": "Compression theorem", "text": "Compression theorem\n\nIn computational complexity theory the compression theorem is an important theorem about the complexity of computable functions. \n\nThe theorem states that there exists no largest complexity class, with computable boundary, which contains all computable functions.\n\nGiven a Gödel numbering formula_1 of the computable functions and a Blum complexity measure formula_2 where a complexity class for a boundary function formula_3 is defined as\n\nThen there exists a total computable function formula_3 so that for all formula_6\nand\n\n"}
{"id": "725557", "url": "https://en.wikipedia.org/wiki?curid=725557", "title": "Cryptography newsgroups", "text": "Cryptography newsgroups\n\nThere are several newsgroups relevant for discussions about cryptography and related issues.\n\n\nIn 1995, Bruce Schneier commented, \"It is read by an estimated 100,000 people worldwide. Most of the posts are nonsense, bickering, or both; some are political, and most of the rest are requests for information or basic questions. Occasionally nuggets of new and useful information are posted to this newsgroup.\" (\"Applied Cryptography\", 2nd ed, pages 608-609).\n\nLeaked descriptions of secret algorithms have been posted to the Internet via sci.crypt, for example RC2, RC4 and Khufu and Khafre. Others have been hoaxes: Iraqi block cipher and S-1, the latter an alleged description of the (then-secret) Skipjack cipher. The group is also the origin of the term, \"Rubber-hose cryptanalysis\".\n\n"}
{"id": "180835", "url": "https://en.wikipedia.org/wiki?curid=180835", "title": "De Finetti's theorem", "text": "De Finetti's theorem\n\nIn probability theory, de Finetti's theorem states that exchangeable observations are conditionally independent relative to some latent variable. An epistemic probability distribution could then be assigned to this variable. It is named in honor of Bruno de Finetti.\n\nFor the special case of an exchangeable sequence of Bernoulli random variables it states that such a sequence is a \"mixture\" of sequences of independent and identically distributed (i.i.d.) Bernoulli random variables. \n\nWhile the variables of the exchangeable sequence are not \"themselves\" independent, only exchangeable, there is an \"underlying\" family of i.i.d. random variables. That is, there are underlying, generally unobservable, quantities that are i.i.d. – exchangeable sequences are mixtures of i.i.d. sequences.\n\nA Bayesian statistician often seeks the conditional probability distribution of a random quantity given the data. The concept of exchangeability was introduced by de Finetti. De Finetti's theorem explains a mathematical relationship between independence and exchangeability.\n\nAn infinite sequence\n\nof random variables is said to be exchangeable if for any finite cardinal number \"n\" and any two finite sequences \"i\", ..., \"i\" and \"j\", ..., \"j\" (with each of the \"i\"s distinct, and each of the \"j\"s distinct), the two sequences\n\nboth have the same joint probability distribution.\n\nIf an identically distributed sequence is independent, then the sequence is exchangeable; however, the converse is false—there exist exchangeable random variables that are not statistically independent, for example the Polya urn model.\n\nA random variable \"X\" has a Bernoulli distribution if Pr(\"X\" = 1) = \"p\" and Pr(\"X\" = 0) = 1 − \"p\" for some \"p\" ∈ (0, 1).\n\nDe Finetti's theorem states that the probability distribution of any infinite exchangeable sequence of Bernoulli random variables is a \"mixture\" of the probability distributions of independent and identically distributed sequences of Bernoulli random variables. \"Mixture\", in this sense, means a weighted average, but this need not mean a finite or countably infinite (i.e., discrete) weighted average: it can be an integral rather than a sum.\n\nMore precisely, suppose \"X\", \"X\", \"X\", ... is an infinite exchangeable sequence of Bernoulli-distributed random variables. Then there is some probability distribution \"m\" on the interval [0, 1] and some random variable \"Y\" such that\n\nSuppose formula_3 is an infinite exchangeable sequence of Bernoulli random variables. Then formula_3 are conditionally independent and identically distributed given the exchangeable sigma-algebra (i.e., the sigma-algebra of events measurable with respect to formula_5 and invariant under finite permutations of the indices).\n\nHere is a concrete example. We construct a sequence\n\nof random variables, by \"mixing\" two i.i.d. sequences as follows.\n\nWe assume \"p\" = 2/3 with probability 1/2 and \"p\" = 9/10 with probability 1/2. Given the event \"p\" = 2/3, the conditional distribution of the sequence is that the \"X\" are independent and identically distributed and \"X\" = 1 with probability 2/3 and \"X\" = 0 with probability 1 − 2/3. Given the event \"p\" = 9/10, the conditional distribution of the sequence is that the \"X\" are independent and identically distributed and \"X\" = 1 with probability 9/10 and \"X\" = 0 with probability 1 − 9/10. \n\nThe independence asserted here is \"conditional\" independence, i.e. the Bernoulli random variables in the sequence are conditionally independent given the event that \"p\" = 2/3, and are conditionally independent given the event that \"p\" = 9/10. But they are not unconditionally independent; they are positively correlated. \n\nIn view of the strong law of large numbers, we can say that\n\nRather than concentrating probability 1/2 at each of two points between 0 and 1, the \"mixing distribution\" can be any probability distribution supported on the interval from 0 to 1; which one it is depends on the joint distribution of the infinite sequence of Bernoulli random variables.\n\nThe definition of exchangeability, and the statement of the theorem, also makes sense for finite length sequences\n\nbut the theorem is not generally true in that case. It is true if the sequence can be extended to an exchangeable sequence that is infinitely long. The simplest example of an exchangeable sequence of Bernoulli random variables that cannot be so extended is the one in which \"X\" = 1 − \"X\" and \"X\" is either 0 or 1, each with probability 1/2. This sequence is exchangeable, but cannot be extended to an exchangeable sequence of length 3, let alone an infinitely long one.\n\nVersions of de Finetti's theorem for \"finite\" exchangeable sequences, \nTwo notions of partial exchangeability of arrays, known as \"separate\" and \"joint exchangeability\" lead to extensions of de Finetti's theorem for arrays by Aldous and Hoover.\n\nThe computable de Finetti theorem shows that if an exchangeable sequence of real random variables is given by a computer program, then a program which samples from the mixing measure can be automatically recovered.\n\nIn the setting of free probability, there is a noncommutative extension of de Finetti's theorem which characterizes noncommutative sequences invariant under quantum permutations.\n\nExtensions of de Finetti's theorem to quantum states have been found to be useful in quantum information, in topics like quantum key distribution and entanglement detection.\n\n\n"}
{"id": "13798040", "url": "https://en.wikipedia.org/wiki?curid=13798040", "title": "Field with one element", "text": "Field with one element\n\nIn mathematics, the field with one element is a suggestive name for an object that should behave similarly to a finite field with a single element, if such a field could exist. This object is denoted F, or, in a French–English pun, F. The name \"field with one element\" and the notation F are only suggestive, as there is no field with one element in classical abstract algebra. Instead, F refers to the idea that there should be a way to replace sets and operations, the traditional building blocks for abstract algebra, with other, more flexible objects. While there is still no field with a single element in these theories, there is a field-like object whose characteristic is one.\n\nF cannot be a field because all fields must contain two distinct elements, the additive identity zero and the multiplicative identity one. Even if this restriction is dropped, a ring with one element must be the zero ring, which does not behave like a finite field. Instead, most proposed theories of F replace abstract algebra entirely. Mathematical objects such as vector spaces and polynomial rings can be carried over into these new theories by mimicking their abstract properties. This allows the development of commutative algebra and algebraic geometry on new foundations. One of the defining features of theories of F is that these new foundations allow more objects than classical abstract algebra, one of which behaves like a field of characteristic one.\n\nThe possibility of studying the mathematics of F was originally suggested in 1956 by Jacques Tits, published in , on the basis of an analogy between symmetries in projective geometry and the combinatorics of simplicial complexes. F has been connected to noncommutative geometry and to a possible proof of the Riemann hypothesis. Many theories of F have been proposed, but it is not clear which, if any, of them give F all the desired properties.\n\nIn 1957, Jacques Tits introduced the theory of buildings, which relate algebraic groups to abstract simplicial complexes. One of the assumptions is a non-triviality condition: If the building is an \"n\"-dimensional abstract simplicial complex, and if , then every \"k\"-simplex of the building must be contained in at least three \"n\"-simplices. This is analogous to the condition in classical projective geometry that a line must contain at least three points. However, there are degenerate geometries which satisfy all the conditions to be a projective geometry except that the lines admit only two points. The analogous objects in the theory of buildings are called apartments. Apartments play such a constituent role in the theory of buildings that Tits conjectured the existence of a theory of projective geometry in which the degenerate geometries would have equal standing with the classical ones. This geometry would take place, he said, over a \"field of characteristic one\". Using this analogy it was possible to describe some of the elementary properties of F, but it was not possible to construct it.\n\nA separate inspiration for F came from algebraic number theory. Weil's proof of the Riemann hypothesis for curves over finite fields started with a curve \"C\" over a finite field \"k\", took its product , and then examined its diagonal. If the integers were a curve over a field, the same proof would prove the Riemann hypothesis. The integers Z are one-dimensional, which suggests that they may be a curve, but they are not an algebra over any field. One of the conjectured properties of F is that Z should be an F-algebra. This would make it possible to construct the product , and it is hoped that the Riemann hypothesis for Z can be proved in the same way as the Riemann hypothesis for a curve over a finite field.\n\nAnother angle comes from Arakelov geometry, where Diophantine equations are studied using tools from complex geometry. The theory involves complicated comparisons between finite fields and the complex numbers. Here the existence of F is useful for technical reasons.\n\nBy 1991, Alexander Smirnov had taken some steps towards algebraic geometry over F. He introduced extensions of F and used them to handle the projective line P over F. Algebraic numbers were treated as maps to this P, and conjectural approximations to the Riemann–Hurwitz formula for these maps were suggested. These approximations imply very profound assertions like the abc conjecture. The extensions of F later on were denoted as F with \"q\" = 1.\n\nIn 1993, Yuri Manin gave a series of lectures on zeta functions where he proposed developing a theory of algebraic geometry over F. He suggested that zeta functions of varieties over F would have very simple descriptions, and he proposed a relation between the K-theory of F and the homotopy groups of spheres. This inspired several people to attempt to construct F. In 2000, Zhu proposed that F was the same as F except that the sum of one and one was one, not zero. Deitmar suggested that F should be found by forgetting the additive structure of a ring and focusing on the multiplication. Toën and Vaquié built on Hakim's theory of relative schemes and defined F using symmetric monoidal categories. Nikolai Durov constructed F as a commutative algebraic monad. Soulé constructed it using algebras over the complex numbers and functors from categories of certain rings. Borger used descent to construct it from the finite fields and the integers.\n\nRecently, Alain Connes, Caterina Consani and Matilde Marcolli have connected F with noncommutative geometry.\nIt has also been suggested to have connections to the unique games conjecture in computational complexity theory.\n\nLorscheid, along with others, has recently achieved Tit's original aim of describing Chevalley groups over F by introducing objects called blueprints, which are a simultaneous generalisation of both semirings and monoids. These are used to define so-called \"blue schemes\", one of which is Spec F.\n\nF is expected to have the following properties.\n\nVarious structures on a set are analogous to structures on a projective space, and can be computed in the same way:\n\nThe number of elements of P(F) = P(F), the -dimensional projective space over the finite field F, is the \"q\"-integer\nTaking yields .\n\nThe expansion of the \"q\"-integer into a sum of powers of \"q\" corresponds to the Schubert cell decomposition of projective space.\n\nThere are \"n\"! permutations of a set with \"n\" elements, and [\"n\"]! maximal flags in F, where\nis the \"q\"-factorial. Indeed, a permutation of a set can be considered a filtered set, as a flag is a filtered vector space: for instance, the ordering of the set {0,1,2} corresponds to the filtration {0} ⊂ {0,1} ⊂ {0,1,2}.\n\nThe binomial coefficient \ngives the number of \"m\"-element subsets of an \"n\"-element set, and the \"q\"-binomial coefficient \ngives the number of \"m\"-dimensional subspaces of an \"n\"-dimensional vector space over F.\n\nThe expansion of the \"q\"-binomial coefficient into a sum of powers of \"q\" corresponds to the Schubert cell decomposition of the Grassmannian.\n\nOne may define field extensions of the field with one element as the group of roots of unity, or more finely (with a geometric structure) as the group scheme of roots of unity. This is non-naturally isomorphic to the cyclic group of order \"n\", the isomorphism depending on choice of a primitive root of unity:\nThus a vector space of dimension \"d\" over F is a finite set of order \"dn\" on which the roots of unity act freely, together with a base point.\n\nFrom this point of view the finite field F is an algebra over F, of dimension for any \"n\" that is a factor of (for example or ). This corresponds to the fact that the group of units of a finite field F (which are the non-zero elements) is a cyclic group of order , on which any cyclic group of order dividing acts freely (by raising to a power), and the zero element of the field is the base point.\n\nSimilarly, the real numbers R are an algebra over F, of infinite dimension, as the real numbers contain ±1, but no other roots of unity, and the complex numbers C are an algebra over F for all \"n\", again of infinite dimension, as the complex numbers have all roots of unity.\n\nFrom this point of view, any phenomenon that only depends on a field having roots of unity can be seen as coming from F – for example, the discrete Fourier transform (complex-valued) and the related number-theoretic transform (Z/\"n\"Z-valued).\n\n\n\n"}
{"id": "8871519", "url": "https://en.wikipedia.org/wiki?curid=8871519", "title": "Graduate Studies in Mathematics", "text": "Graduate Studies in Mathematics\n\nGraduate Studies in Mathematics (GSM) is a series of graduate-level textbooks in mathematics published by the American Mathematical Society (AMS). These books elaborate on several theories from notable personas, such as Martin Schechter and Terence Tao, in the mathematical industry. The books in this series are published only in hardcover.\n\n\n\n"}
{"id": "293606", "url": "https://en.wikipedia.org/wiki?curid=293606", "title": "Graph paper", "text": "Graph paper\n\nGraphing Paper\n\nGraph paper, coordinate paper, grid paper, or squared paper is writing paper that is printed with fine lines making up a regular grid. The lines are often used as guides for for mathematical functions or experimental data and drawing two-dimensional graphs. It is commonly found in mathematics and engineering education settings and in laboratory notebooks. Graph paper is available either as loose leaf paper or bound in notebooks.\n\nThe first commercially published \"coordinate paper\" is usually attributed to Dr. Buxton of England, who patented paper, printed with a rectangular coordinate grid, in 1794. A century later, E. H. Moore, a distinguished mathematician at the University of Chicago, advocated usage of paper with \"squared lines\" by students of high schools and universities. The 1906 edition of \"Algebra for Beginners\" by H. S. Hall and S. R. Knight included a strong statement that \"the squared paper should be of good quality and accurately ruled to inches and tenths of an inch. Experience shows that anything on a smaller scale (such as 'millimeter' paper) is practically worthless in the hands of beginners.\"\n\nThe term \"graph paper\" did not catch on quickly in American usage. \"A School Arithmetic\" (1919) by H. S. Hall and F. H. Stevens had a chapter on graphing with \"squared paper\". \"Analytic Geometry\" (1937) by W. A. Wilson and J. A. Tracey used the phrase \"coordinate paper\". The term \"squared paper\" remained in British usage for longer; for example it was used in \"Public School Arithmetic\" (1961) by W. M. Baker and A. A. Bourne published in London. minecraft.apk\n\n\nIn general, graphs showing grids are sometimes called Cartesian graphs because the square can be used to map measurements onto a Cartesian (x vs. y) coordinate system. It is also available without lines but with dots at the positions where the lines would intersect.\n\n\n"}
{"id": "32089853", "url": "https://en.wikipedia.org/wiki?curid=32089853", "title": "Greninger chart", "text": "Greninger chart\n\nIn crystallography, a Greninger chart is a chart that allows angular relations between zones and planes in a crystal to be directly read from an x-ray diffraction photograph.\n\nThe Greninger chart is a simple trigonometric tool to determine \"g\" and \"d\" for a fixed sample-to-film distance. (If one uses a 2-d detector the problem of determining \"g\" and \"d\" could be solved mathematically using the equations which generate the Greninger chart) A new chart must be generated for different sample to detector distances. (2\"s\" is 2\"q\" for the diffraction peak and tan \"m\" is \"x\"/\"y\" for the Cartesian coordinates of the diffraction peak.) The Greninger chart gives directly the two angles needed to plot poles on the Wulff net. It is critical to keep track of the relative arrangement of the sample to the film, if photographic film is used then this is achieved by cutting the corner of the film. For Polaroid film one must make a note of the arrangement of the face of the film in the camera.\n\n\n\n"}
{"id": "54580441", "url": "https://en.wikipedia.org/wiki?curid=54580441", "title": "HAIFA construction", "text": "HAIFA construction\n\nThe HAIFA construction (\"hash iterative framework\") is a cryptographic structure used in the design of hash functions. It is one of the modern alternatives to the Merkle–Damgård construction, avoiding its weaknesses like length extension attacks. The construction was designed by Eli Biham and Orr Dunkelman in 2007.\n\nThree of the 14 second round candidates in the NIST hash function competition were based on HAIFA constructions (BLAKE, SHAvite-3, ECHO). Other hash functions based on it are LAKE, Sarmal, SWIFFTX and HNF-256. The construction of Skein (Unique Block Iteration) is similar to HAIFA. Another alternative construction is the sponge construction.\n"}
{"id": "34060358", "url": "https://en.wikipedia.org/wiki?curid=34060358", "title": "Heat kernel signature", "text": "Heat kernel signature\n\nA heat kernel signature (HKS) is a feature descriptor for use in deformable shape analysis and belongs to the group of spectral shape analysis methods. For each point in the shape, HKS defines its feature vector representing the point's local and global geometric properties. Applications include segmentation, classification, structure discovery, shape matching and shape retrieval.\n\nHKS was introduced in 2009 by Jian Sun, Maks Ovsjanikov and Leonidas Guibas. It is based on heat kernel, which is a fundamental solution to the heat equation. HKS is one of the many recently introduced shape descriptors which are based on the Laplace–Beltrami operator associated with the shape.\n\nShape analysis is the field of automatic digital analysis of shapes, e.g., 3D objects. For many shape analysis tasks (such as shape matching/retrieval), feature vectors for certain key points are used instead of using the complete 3D model of the shape. An important requirement of such feature descriptors is for them to be invariant under certain transformations. For rigid transformations, commonly used feature descriptors include shape context, spin images, integral volume descriptors and multiscale local features, among others. HKS allows isometric transformations which generalizes rigid transformations.\n\nHKS is based on the concept of heat diffusion over a surface. Given an initial heat distribution formula_1 over the surface, the heat kernel formula_2 relates the amount of heat transferred from formula_3 to formula_4 after time formula_5. The heat kernel is invariant under isometric transformations and stable under small perturbations to the isometry. In addition, the heat kernel fully characterizes shapes up to an isometry and represents increasingly global properties of the shape with increasing time. Since formula_6 is defined for a pair of points over a temporal domain, using heat kernels directly as features would lead to a high complexity. HKS instead restricts itself to just the temporal domain by considering only formula_7. HKS inherits most of the properties of heat kernels under certain conditions.\n\nThe heat diffusion equation over a compact Riemannian manifold formula_8 (possibly with a boundary) is given by,\nwhere formula_10 is the Laplace–Beltrami operator and formula_11 is the heat distribution at a point formula_3 at time formula_5. The solution to this equation can be expressed as,\nThe eigen decomposition of the heat kernel is expressed as,\nwhere formula_16 and formula_17 are the formula_18 eigenvalue and eigenfunction of formula_10. The heat kernel fully characterizes a surface up to an isometry: For any surjective map formula_20 between two Riemannian manifolds formula_8 and formula_22, if formula_23 then formula_24 is an isometry, and vice versa. For a concise feature descriptor, HKS restricts the heat kernel only to the temporal domain,\nHKS, similar to the heat kernel, characterizes surfaces under the condition that the eigenvalues of formula_10 for formula_8 and formula_22 are non-repeating. The terms formula_29 can be intuited as a bank of low-pass filters, with formula_16 determining the cutoff frequencies.\n\nSince formula_7 is, in general, a non-parametric continuous function, HKS is in practice represented as a discrete sequence of formula_32 values sampled at times formula_33.\n\nIn most applications, the underlying manifold for an object is not known. The HKS can be computed if a mesh representation of the manifold is available, by using a discrete approximation to formula_10 and using the discrete analogue of the heat equation. In the discrete case, the Laplace–Beltrami operator is a sparse matrix and can be written as,\nwhere formula_36 is a positive diagonal matrix with entries formula_37 corresponding to the area of the triangles in the mesh sharing the vertex formula_38, and formula_39 is a symmetric semi-definite weighting matrix. formula_40 can be decomposed into formula_41, where formula_42 is a diagonal matrix of the eigenvalues of formula_40 arranged in the ascending order, and formula_44 is the matrix with the corresponding orthonormal eigenvectors. The discrete heat kernel is the matrix given by,\nThe elements formula_46 represents the heat diffusion between vertices formula_38 and formula_48 after time formula_5. The HKS is then given by the diagonal entries of this matrix, sampled at discrete time intervals. Similar to the continuous case, the discrete HKS is robust to noise.\n\nThe main property that characterizes surfaces using HKS up to an isometry holds only when the eigenvalues of the surfaces are non-repeating. There are certain surfaces (especially those with symmetry) where this condition is violated. A sphere is a simple example of such a surface.\n\nThe time parameter in the HKS is closely related to the scale of global information. However, there is no direct way to choose the time discretization. The existing method chooses time samples logarithmically which is a heuristic with no guarantees\n\nThe discrete heat kernel requires eigendecomposition of a matrix of size formula_50, where formula_51 is the number of vertices in the mesh representation of the manifold. Computing the eigendecomposition is an expensive operation, especially as formula_51 increases.\nNote, however, that because of the inverse exponential dependence on the eigenvalue, typically only a small (less than 100) eigenvectors are sufficient to obtain a good approximation of the HKS.\n\nThe performance guarantees for HKS only hold for truly isometric transformations. However, deformations for real shapes are often not isometric. A simple example of such transformation is closing of the fist by a person, where the geodesic distances between two fingers changes.\n\nThe (continuous) HKS at a point formula_3, formula_7 on the Riemannian manifold is related to the scalar curvature formula_55 by,\nHence, HKS can as be interpreted as the curvature of formula_3 at scale formula_5.\n\nThe WKS follows a similar idea to the HKS, replacing the heat equation with the Schrödinger wave equation,\nwhere formula_60 is the complex wave function. The average probability of measuring the particle at a point formula_3 is given by,\nwhere formula_63 is the initial energy distribution. By fixing a family of these energy distributions formula_64, the WKS can be obtained as a discrete sequence formula_65. Unlike HKS, the WKS can be intuited as a set of band-pass filters leading to better feature localization. However, the WKS does not represent large-scale features well (as they are \"filtered\" out) yielding poor performance at shape matching applications.\n\nSimilar to the HKS, the GPS is based on the Laplace-Beltrami operator. GPS at a point formula_3 is a vector of scaled eigenfunctions of the Laplace–Beltrami operator computed at formula_3. The GPS is a global feature whereas the scale of the HKS can be varied by varying the time parameter for heat diffusion. Hence, the HKS can be used in partial shape matching applications whereas the GPS cannot.\n\nSGWS provides a general form for spectral descriptors, where one can obtain HKS by specifying the filter function. SGWS is a multiresolution local descriptor that is not only isometric invariant, but also compact, easy to compute and combines the advantages of both band-pass and low-pass filters.\n\nEven though the HKS represents the shape at multiple scales, it is not inherently scale invariant. For example, the HKS for a shape and its scaled version are not the same without pre-normalization. A simple way to ensure scale invariance is by pre-scaling each shape to have the same surface area (e.g. 1). Using the notation above, this means:\n\nformula_68\n\nAlternatively, scale-invariant version of the HKS can also be constructed by generating a Scale space representation. In the scale-space, the HKS of a scaled shape corresponds to a translation up to a multiplicative factor. The Fourier transform of this HKS changes the time-translation into the complex plane, and the dependency on translation can be eliminated by considering the modulus of the transform.\nAn alternative scale invariant HKS can be established by working out its construction through a scale invariant metric, as defined in .\n\nThe HKS is defined for a boundary surface of a 3D shape, represented as a 2D Riemannian manifold. Instead of considering only the boundary, the entire volume of the 3D shape can be considered to define the volumetric version of the HKS. The Volumetric HKS is defined analogous to the normal HKS by considering the heat equation over the entire volume (as a 3-submanifold) and defining a Neumann boundary condition over the 2-manifold boundary of the shape. Volumetric HKS characterizes transformations up to a volume isometry, which represent the transformation for real 3D objects more faithfully than boundary isometry.\n\nThe scale-invariant HKS features can be used in the bag-of-features model for shape retrieval applications. The features are used to construct geometric words by taking into account their spatial relations, from which shapes can be constructed (analogous to using features as words and shapes as sentences). Shapes themselves are represented using compact binary codes to form an indexed collection. Given a query shape, similar shapes in the index with possibly isometric transformations can be retrieved by using the Hamming distance of the code as the nearness-measure.\n"}
{"id": "15189", "url": "https://en.wikipedia.org/wiki?curid=15189", "title": "IEEE 754-1985", "text": "IEEE 754-1985\n\nIEEE 754-1985 was an industry standard for representing floating-point numbers in computers, officially adopted in 1985 and superseded in 2008 by IEEE 754-2008. During its 23 years, it was the most widely used format for floating-point computation. It was implemented in software, in the form of floating-point libraries, and in hardware, in the instructions of many CPUs and FPUs. The first integrated circuit to implement the draft of what was to become IEEE 754-1985 was the Intel 8087.\n\nIEEE 754-1985 represents numbers in binary, providing definitions for four levels of precision, of which the two most commonly used are:\n\nThe standard also defines representations for positive and negative infinity, a \"negative zero\", five exceptions to handle invalid results like division by zero, special values called NaNs for representing those exceptions, denormal numbers to represent numbers smaller than shown above, and four rounding modes.\n\nFloating-point numbers in IEEE 754 format consist of three fields: a sign bit, a biased exponent, and a fraction. The following example illustrates the meaning of each.\n\nThe decimal number 0.15625 represented in binary is 0.00101 (that is, 1/8 + 1/32). (Subscripts indicate the number base.) Analogous to scientific notation, where numbers are written to have a single non-zero digit to the left of the decimal point, we rewrite this number so it has a single 1 bit to the left of the \"binary point\". We simply multiply by the appropriate power of 2 to compensate for shifting the bits left by three positions:\n\nNow we can read off the fraction and the exponent: the fraction is .01 and the exponent is −3.\n\nAs illustrated in the pictures, the three fields in the IEEE 754 representation of this number are:\n\nIEEE 754 adds a bias to the exponent so that numbers can in many cases be compared conveniently by the same hardware that compares signed 2's-complement integers. Using a biased exponent, the lesser of two positive floating-point numbers will come out \"less than\" the greater following the same ordering as for sign and magnitude integers. If two floating-point numbers have different signs, the sign-and-magnitude comparison also works with biased exponents. However, if both biased-exponent floating-point numbers are negative, then the ordering must be reversed. If the exponent were represented as, say, a 2's-complement number, comparison to see which of two numbers is greater would not be as convenient.\n\nThe leading 1 bit is omitted since all numbers except zero start with a leading 1; the leading 1 is implicit and doesn't actually need to be stored which gives an extra bit of precision for \"free.\"\n\nThe number zero is represented specially:\n\nThe number representations described above are called \"normalized,\" meaning that the implicit leading binary digit is a 1. To reduce the loss of precision when an underflow occurs, IEEE 754 includes the ability to represent fractions smaller than are possible in the normalized representation, by making the implicit leading digit a 0. Such numbers are called denormal. They don't include as many significant digits as a normalized number, but they enable a gradual loss of precision when the result of an arithmetic operation is not exactly zero but is too close to zero to be represented by a normalized number.\n\nA denormal number is represented with a biased exponent of all 0 bits, which represents an exponent of −126 in single precision (not −127), or −1022 in double precision (not −1023). In contrast, the smallest biased exponent representing a normal number is 1 (see examples below).\n\nThe biased-exponent field is filled with all 1 bits to indicate either infinity or an invalid result of a computation.\n\nPositive and negative infinity are represented thus:\n\nSome operations of floating-point arithmetic are invalid, such as taking the square root of a negative number. The act of reaching an invalid result is called a floating-point \"exception.\" An exceptional result is represented by a special code called a NaN, for \"Not a Number\". All NaNs in IEEE 754-1985 have this format:\n\nPrecision is defined as the minimum difference between two successive mantissa representations; thus it is a function only in the mantissa; while the gap is defined as the difference between two successive numbers.\n\nSingle-precision numbers occupy 32 bits. In single precision:\n\nSome example range and gap values for given exponents in single precision:\n\nAs an example, 16,777,217 can not be encoded as a 32-bit float as it will be rounded to 16,777,216. This shows why floating point arithmetic is unsuitable for accounting software. However, all integers within the representable range that are a power of 2 can be stored in a 32-bit float without rounding.\n\nDouble-precision numbers occupy 64 bits. In double precision:\n\nSome example range and gap values for given exponents in double precision:\n\nThe standard also recommends extended format(s) to be used to perform internal computations at a higher precision than that required for the final result, to minimise round-off errors: the standard only specifies minimum precision and exponent requirements for such formats. The x87 80-bit extended format is the most commonly implemented extended format that meets these requirements.\n\nHere are some examples of single-precision IEEE 754 representations:\n\nEvery possible bit combination is either a NaN or a number with a unique value in the affinely extended real number system with its associated order, except for the two bit combinations negative zero and positive zero, which sometimes require special attention (see below). The binary representation has the special property that, excluding NaNs, any two numbers can be compared as sign and magnitude integers (endianness issues apply). When comparing as 2's-complement integers: If the sign bits differ, the negative number precedes the positive number, so 2's complement gives the correct result (except that negative zero and positive zero should be considered equal). If both values are positive, the 2's complement comparison again gives the correct result. Otherwise (two negative numbers), the correct FP ordering is the opposite of the 2's complement ordering.\n\nRounding errors inherent in floating point calculations often make comparison of results for exact equality not useful. Choosing an acceptable range is a complex topic.\n\nAlthough negative zero and positive zero are generally considered equal for comparison purposes, some programming language relational operators and similar constructs might or do treat them as distinct. According to the Java Language Specification, comparison and equality operators treat them as equal, but codice_1 and codice_2 distinguish them (officially starting with Java version 1.1 but actually with 1.1.1), as do the comparison methods codice_3, codice_4 and even codice_5 of classes codice_6 and codice_7.\n\nThe IEEE standard has four different rounding modes; the first is the default; the others are called \"directed roundings\".\n\n\nThe IEEE standard employs (and extends) the affinely extended real number system, with separate positive and negative infinities. During drafting, there was a proposal for the standard to incorporate the projectively extended real number system, with a single unsigned infinity, by providing programmers with a mode selection option. In the interest of reducing the complexity of the final standard, the projective mode was dropped, however. The Intel 8087 and Intel 80287 floating point co-processors both support this projective mode.\n\nThe following functions must be provided:\n\n\nIn 1976 Intel began planning to produce a floating point coprocessor. John Palmer, the manager of the effort, persuaded them that they should try to develop a standard for all their floating point operations. William Kahan was hired as a consultant; he had helped improve the accuracy of Hewlett-Packard's calculators. Kahan initially recommended that the floating point base be decimal but the hardware design of the coprocessor was too far along to make that change.\n\nThe work within Intel worried other vendors, who set up a standardization effort to ensure a 'level playing field'. Kahan attended the second IEEE 754 standards working group meeting, held in November 1977. Here, he received permission from Intel to put forward a draft proposal based on the standard arithmetic part of their design for a coprocessor. The arguments over gradual underflow lasted until 1981 when an expert hired by DEC to assess it sided against the dissenters.\n\nEven before it was approved, the draft standard had been implemented by a number of manufacturers. The Intel 8087, which was announced in 1980, was the first chip to implement the draft standard.\n\n\n\n"}
{"id": "27454991", "url": "https://en.wikipedia.org/wiki?curid=27454991", "title": "Information seeking behavior", "text": "Information seeking behavior\n\nInformation seeking behavior refers to the way people search for and utilize information. The term was coined by Thomas D. Wilson in his 1981 paper, on the grounds that the then current 'information needs' was unhelpful as a basis for a research agenda, since 'need' could not be directly observed, while how people behaved in seeking information could be observed and investigated. However, there is increasing work in the information searching field that is relating behaviors to underlying needs.\n\nIn 2000, Wilson described information behavior as the totality of human behavior in relation to sources and channels of information, including both active and passive information-seeking, and information use. He described information seeking behavior as purposive seeking of information as a consequence of a need to satisfy some goal. Information seeking behavior is the micro-level of behavior employed by the searcher in interacting with information systems of all kinds, be it between the seeker and the system, or the pure method of creating and following up on a search.\n\nA variety of theories of information behavior – e.g. Zipf's principle of least effort, Brenda Dervin's sensemaking, Elfreda Chatman's life in the round – seek to understand the processes that surround information seeking. The analysis of the most cited publications on information behavior during the first years of this century shows its theoretical nature. Together with some works that have a constructivist focus, using references to Dewey, Kelly, Bruner and Vygotsky, others mention sociological concepts, such as Bourdieu's habitus. Several adopt a constructionist-discursive focus, whereas some, such as Chatman, who can in general be described as using an ethnographic perspective, stand out for the quantity and diversity of references to social research. The term 'information behaviour' was also coined by Wilson and occasioned some controversy on its introduction, but now seems to have been adopted, not only by researchers in information science but also in other disciplines.\n\nThe digital world is changing human information behavior and process. Focused almost exclusively on information seeking and using, information receiving, a central modality of the process is generally overlooked. As information seeking continues to migrate to the Internet, and artificial intelligence continues to advance the analysis of user behavior on the Internet across a range of user interactions, information receiving moves to the heart of the process, as systems \"learn\" what users like, want and need, as well as their search habits.\n\nISP was proposed and developed by Carol Kuhlthau.\n\nAn holistic framework based initially on research into high school students, but extended over time to include a diverse range of people, including those in the workplace. It examined the role of emotions, specifically uncertainty, in the information seeking process, concluding that many searches are abandoned due to an overwhelmingly high level of uncertainty.\n\nISP is a 6-stage process, with each stage each encompassing 4 aspects;\n\nInvestigated the behavior of researchers in the physical and social sciences and engineers and research scientists through semi-structured interviews using a grounded theory approach, with a focus on describing the activities rather than a process.\n\nThese initial investigations produced six key activities within the information seeking process:\n\n\nLater studies by Ellis (focusing on academic researchers in other disciplines) resulted in the addition of two more activities;\n\n\nThe episodic model was developed by Nicholas J. Belkin.\n\nThe episodic model is based largely on intuition and insight and concentrates on interactions with information. There are 4 dimensions which characterize search behavior. These dimensions can be combined in 16 different ways.\n\n\nASK was also developed by Nicholas J. Belkin.\n\nAn anomalous state of knowledge is one in which the searcher recognises a gap in the state of knowledge. This, his further hypothesis, is influential in studying why people start to search.\n\nThomas Wilson proposed that information behavior covers all aspects of human information behavior, whether active or passive.\n\nInformation \"Seeking\" behavior is the act of actively seeking information in order to answer a specific query.\n\nInformation \"Searching\" behavior is the behavior which stems from the searcher interacting with the system in question. This system could be a technological one, such as the searcher interacting with a search engine, or a manual one, such as the searcher selecting which book is most pertinent to their query.\n\nInformation \"Use\" behavior pertains to the searcher adopting the knowledge they sought.\n\nDeveloped by Stuart Card, Ed H. Chi and Peter Pirolli.\n\nThis model is derived from anthropological theories and is comparable to foraging for food. Information seekers use clues (or information scents) such as links, summaries and images to estimate how close they are to target information. A scent must be obvious as users often browse aimlessly or look for specific information. Information foraging is descriptive of why and not how people search in particular ways.\n\nDeveloped by Elfreda Chatman.\n\nShe defines life in the round as a world of tolerated approximation. It acknowledges reality at its most routine, predictable enough that unless an initial problem should arise, there is no point in seeking information.\n\nChatman examined this principle within a small world: a world which imposes on its participants similar concerns and awareness of who is important; which ideas are relevant and whom to trust. Participants in this world are considered insiders.\n\nChatman focused her study on women at a maximum security prison. She learned that over time, prisoner's private views were assimilated to a communal acceptance of life in the round: a small world perceived in accordance with agreed upon standards and communal perspective. Members who live in the round will not cross the boundaries of their world to seek information unless it is critical; there is a collective expectation that information is relevant; or life lived in the round no longer functions. The world outside prison has secondary importance to inmates who are absent from this reality which is changing with time.\n\nBrenda Dervin developed the concept of sensemaking. Sensemaking considers how we (attempt to) make sense of uncertain situations. Her description of Sensemaking consisted of the definition of how we interpret information to use for our own information related decisions.\n\nBrenda Dervin described sensemaking as a method through which people make sense of their worlds in their own language.\n\nThis principle explains that information seekers prioritise the most convenient path to acceptable information.\nThis compares the internet search methods of experienced information seekers (navigators) and inexperienced information seekers (explorers). Navigators revisit domains; follow sequential searches and have few deviations or regressions within their search patterns and interactions. Explorers visit many domains; submit many questions and their search trails branch frequently.\n\nRobinson's (2010) research suggests that when seeking information at work, people rely on both other people and information repositories (e.g., documents and databases), and spend similar amounts of time consulting each (7.8% and 6.4% of work time, respectively; 14.2% in total). However, of theoretical interest, the distribution of time among the constituent information seeking stages differs depending on the source. When consulting other people, people spend less time locating the information source and information within that source, similar time understanding the information, and more time problem solving and decision making, than when consulting information repositories. Furthermore, the research found that people spend substantially more time receiving information passively (i.e., information that they have not requested) than actively (i.e., information that they have requested), and this pattern is also reflected when they provide others with information.\n\nA review of the literature on information seeking behavior shows that information seeking has generally been accepted as dynamic and non-linear (Foster, 2005; Kuhlthau 2006). People experience the information search process as an interplay of thoughts, feelings and actions (Kuhlthau, 2006).\n\nInformation seeking has been found to be linked to a variety of interpersonal communication behaviors beyond question-asking, to include strategies such as candidate answers.\n\nA search for information may be linked to decision making. The decision involved may vary from a trivial personal matter to a decision which affects billions or may have cumulative economic or political effects as individual buying or voting decisions may.\n\nNicolaisen described four distinct types of information seeking behavior: visceral, conscious, formalized and compromised. The visceral need is expressed as the actual information need before it has been expressed. The conscious need is the need once it has been recognized by the seeker. The formalized need is the statement of the need and the compromised need is the query when related to the information system.\n\nJISC's study of the Google Generation detailed six different characteristics of online information seeking behavior;\n\n\nHorizontal information seeking is the method sometimes referred to as \"skimming\". An information seeker who skims views a couple of pages, then subsequently follows other links without necessarily returning to the initial sites. Navigators, as might be expected, spend their time finding their way around. Wilson found that users of e-book or e-journal sites were most likely spend, on average, a mere four to eight minutes viewing said sites. Squirreling behavior relates to users who download lots of documents but might not necessarily end up reading them. Checking information seekers assess the host in order to ascertain trustworthiness. The bracket of users named diverse information seekers are users whose behavior differs from the above sectors.\n\n"}
{"id": "2605397", "url": "https://en.wikipedia.org/wiki?curid=2605397", "title": "Insurance score", "text": "Insurance score\n\nAn insurance score - also called an insurance credit score - is a numerical point system based on select credit report characteristics. There is no direct relationship to financial credit scores used in lending decisions, as insurance scores are not intended to measure creditworthiness, but rather to predict risk. Insurance companies use insurance scores for underwriting decisions, and to partially determine charges for premiums. Insurance scores are applied in personal product lines, namely homeowners and private passenger automobile insurance, and typically not elsewhere.\n\nInsurance scoring models are built from selections of credit report factors, combined with insurance claim and profitability data, to produce numerical formulae or algorithms. A scoring model may be unique to an insurance company and to each line of business (e.g. homeowners or automobile), in terms of the factors selected for consideration and the weighting of the point assignments. As insurance credit scores are not intended to measure creditworthiness, they commonly focus on financial habits and choices (i.e., age of oldest account, number of inquiries in 24 months, ratio of total balance to total limits, number of open retail credit cards, number of revolving accounts with balances greater than 75% of limits, etc.) Therefore it is possible for a consumer with a high financial credit score, and excellent payment history, to receive a poor insurance score. Insurers consider credit report information in their underwriting and pricing decisions as a predictor of profitability and risk of loss. \n\nVarious studies have found a strong relationship between credit-based insurance scores and profitability or risk of loss. The scores are generally most predictive when little or no other information exists, such as in the case of clean driving records, or claims-free policies; in instances where past claims, points, or other similar information exist on record, the personal histories will typically be more predictive than the scores. Insurers consider credit report information, along with other factors, such as driving experience, previous claims and vehicle age, to develop a picture of a consumer's risk profile and to establish premium rates. The correlation, between credit-based insurance scores and overall insurance profitability and loss, has not been disputed.\n\nThe use of credit information in insurance pricing and underwriting is heavily disputed. Proponents of insurance credit scoring include insurance companies, the American Academy of Actuaries (AAA), the Insurance Information Institute (III), and credit bureaus such as Fair Isaac and TransUnion. Active opponents include many state insurance departments and regulators, and consumer protection organizations such as the Center for Economic Justice, the Consumer Federation of America, the National Consumer Law Center and Texas Watch. As a result of successful lobbying by the insurance industry, credit scoring is legal in nearly all states. The state of Hawaii has banned all use of credit information in personal automobile underwriting and rating, and other states have established restrictions. A number of states have also made unsuccessful attempts to ban or restrict the practice. The National Association of Insurance Commissioners has acknowledged that a correlation does exist between insurance scores and losses, but asserts that the benefit of credit reports to \"consumers\" has not yet been established.\n\nInsurance credit-scoring models are considered proprietary, and a trade secret, in most cases. The designers wish to protect their models from view for a number of reasons: they may provide competitive advantage in the insurance marketplace, or they anticipate consumers might attempt to alter results, by changing the information they provide, if the computations were common knowledge. Thus there is little public information available about the details of insurance credit-scoring models. \n\n\"Credit-Based Insurance Scores: Impacts on Consumers of Automobile Insurance, A Report to Congress by the Federal Trade Commission\". This study found that insurance credit scores are effective predictors of risk. It also showed that African-Americans and Hispanics are substantially overrepresented in the lowest credit scores, and substantially underrepresented in the highest, while Caucasians and Asians are more evenly spread across the scores. The credit scores were also found to predict risk within each of the ethnic groups, leading the Federal Trade Commission (FTC) to conclude that the scoring models are not solely proxies for redlining. The FTC stated that little data was available to evaluate benefit of insurance scores to consumers. The report was disputed by representatives of the Consumer Federation of America, the National Fair Housing Alliance, the National Consumer Law Center, and the Center for Economic Justice, for relying on data provided by the insurance industry, which was not open to examination. \n\n\"The Impact of Personal Credit History on Loss Performance in Personal Lines\", by James Monaghan ACAS MAAA. This actuarial study matched 170,000 policy records with credit report information to show the correlation between historical loss ratios and various credit report elements. \n\"The Use of Credit History for Personal Lines of Insurance: Report to the National Association of Insurance Commissioners\", American Academy of Actuaries Risk Classification Subcommittee of the Property/Casualty Products, Pricing and Market Committee.\n\n\"Insurers' Use of Credit Scoring for Homeowners Insurance in Ohio: A Report to the Ohio Civil Rights Commission\", from Birny Birnbaum, Center for Economic Justice. Birny Birnbaum, Consulting Economist, argues that insurance credit scoring is inherently unfair to consumers and violates basic risk classification principles.\n\n\"Insurance Credit Scoring: An Unfair Practice\", Center for Economic Justice. This report argues that insurance scoring: is inherently unfair; has a disproportionate impact on consumers in poor and minority communities; penalizes consumers for rational behavior and sound financial management practices; penalizes consumers for lenders’ business decisions unrelated to payment history; is an arbitrary practice; and undermines the basic insurance mechanism and public policy goals for insurance. \n\n\"The Use of Credit Scoring in Automobile and Homeowners Insurance, A Report to the Governor, the Legislature and the People of Michigan\", by Frank M. Fitzgerald, Commissioner, Office of Financial and Insurance Services. \nThis report reviewed the viewpoints of the industry, agents, consumers, and other interested parties. In conclusion, insurance credit scoring was found to be within the scope of Michigan law. \n\n\"Use of Credit Information by Insurers in Texas, Report to the 79th Legislature\", Texas Department of Insurance. This study found a consistent pattern of differences in credit scores among different racial/ethnic groups. Whites and Asians were found to have better scores than Blacks and Hispanics. Differences in income levels were not as pronounced as for racial/ethnic groups, but average credit scores at upper income levels were better than those at lower and moderate income levels. The study found a strong relationship between credit scores and claims experience on an aggregate basis. In 2002, the Texas Department of Insurance received a peak of 600 complaints related to credit scoring, which declined and leveled to 300 per year. \n\"Insurance Credit Scoring in Alaska\", State of Alaska, Department of Community and Economic Development, Division of Insurance. The study suggested unequal effects on consumers of varying income and ethnic backgrounds. Specifically, the higher income neighborhoods and those with a higher proportion of Caucasians were the least impacted by credit scoring. Although data available for the study was limited, the state of Alaska determined that some restrictions on credit scoring would be appropriate to protect the public.\n\n\n"}
{"id": "39034538", "url": "https://en.wikipedia.org/wiki?curid=39034538", "title": "Interval (graph theory)", "text": "Interval (graph theory)\n\nIn graph theory, an interval \"I\"(\"h\") in a directed graph is a maximal, single entry subgraph in which \"h\" is the only entry to \"I\"(\"h\") and all closed paths in \"I\"(\"h\") contain \"h\". Intervals were described in 1970 by F. E. Allen and J. Cocke. Interval graphs are integral to some algorithms used in compilers, specifically data flow analyses.\n\nThe following algorithm finds all the intervals in a graph consisting of vertices \"N\" and the entry vertex \"n\", and with the functions codice_1 and codice_2 which return the list of predecessors and successors of a given node \"n\", respectively.\n\nThe algorithm effectively partitions the graph into its intervals. \n\nEach interval can in turn be replaced with a single node, while all edges between nodes in different intervals in the original graph become edges between their corresponding nodes in the new graph. This new graph is called an interval derived graph. The process of creating derived graphs can be repeated until the resulting graph can't be reduced further. If the final graph consists of a single node, then the original graph is said to be reducible.\n\n"}
{"id": "239143", "url": "https://en.wikipedia.org/wiki?curid=239143", "title": "Isaac Watts", "text": "Isaac Watts\n\nIsaac Watts (17 July 1674 – 25 November 1748) was an English Christian minister (Congregational), hymn writer, theologian, and logician. He was a prolific and popular hymn writer and is credited with some 750 hymns. He is recognised as the \"Godfather of English Hymnody\"; many of his hymns remain in use today and have been translated into numerous languages.\n\nWatts was born in Southampton, England in 1674 and was brought up in the home of a committed religious nonconformist; his father, also Isaac Watts, had been incarcerated twice for his views. Watts had a classical education at King Edward VI School, learning Latin, Greek, and Hebrew.\n\nWatts displayed a propensity for rhyme from an early age. He was once asked why he had his eyes open during prayers, to which he responded:\nHe received corporal punishment for this, to which he cried:\nWatts could not attend Oxford or Cambridge because he was a noncomformist and these universities were restricted to Anglicans—as were government positions at the time. He went to the Dissenting Academy at Stoke Newington in 1690. Much of the remainder of his life centred on that village, which is now part of Inner London.\n\nFollowing his education, Watts was called as pastor of a large independent chapel in London, Mark Lane Congregational Chapel, where he helped train preachers, despite his poor health. He held religious opinions that were more nondenominational or ecumenical than was common for a nonconformist Congregationalist. He had a greater interest in promoting education and scholarship than preaching for any particular sect.\n\nWatts took work as a private tutor and lived with the nonconformist Hartopp family at Fleetwood House on Church Street in Stoke Newington. Through them, he became acquainted with their immediate neighbours Sir Thomas Abney and Lady Mary. He eventually lived for a total of 36 years in the Abney household, most of the time at Abney House, their second residence. (Lady Mary had inherited the manor of Stoke Newington in 1701 from her late brother Thomas Gunston.)\n\nOn the death of Sir Thomas Abney in 1722, his widow Lady Mary and her unmarried daughter Elizabeth moved all her household to Abney House from Hertfordshire, and she invited Watts to continue with them. He particularly enjoyed the grounds at Abney Park, which Lady Mary planted with two elm walks leading down to an island heronry in the Hackney Brook, and he often sought inspiration there for the many books and hymns that he wrote.\n\nWatts lived at Abney Hall in Stoke Newington until his death in 1748; he was buried in Bunhill Fields. He left an extensive legacy of hymns, treatises, educational works, and essays. His work was influential amongst nonconformist independents and religious revivalists of the 18th century, such as Philip Doddridge, who dedicated his best-known work to Watts.\n\nSacred music scholars Stephen Marini, Denny Prutow and Michael LeFebvre describe the ways in which Watts contributed to English hymnody and the previous tradition of the Church. Watts led the change in practice by including new poetry for \"original songs of Christian experience\" to be used in worship, according to Marini. The older tradition was based on the poetry of the Bible: the Psalms. According to LeFebvre, Psalms had been sung by God's people from the time of King David, who with a large staff over many years assembled the complete book of Psalms in a form appropriate for singing (by the Levites, during Temple sacrifices at the time). The practice of singing Psalms in worship was continued by Biblical command in the New Testament Church from its beginnings in Acts through the time of Watts, as documented by Prutow. The teachings of 16th-century Reformation leaders such as John Calvin, who translated the Psalms in the vernacular for congregational singing, followed this historic worship practice. Watts was not the first Protestant to promote the singing of hymns; however, his prolific hymnwriting helped usher in a new era of English worship as many other poets followed in his path.\n\nWatts also introduced a new way of rendering the Psalms in verse for church services, proposing that they be adapted for hymns with a specifically Christian perspective. As Watts put it in the title of his 1719 metrical Psalter, the Psalms should be \"imitated in the language of the New Testament.\" Besides writing hymns, Isaac Watts was also a theologian and logician, writing books and essays on these subjects.\n\nWatts wrote a textbook on logic which was particularly popular; its full title was, \"Logic, or The Right Use of Reason in the Enquiry After Truth With a Variety of Rules to Guard Against Error in the Affairs of Religion and Human Life, as well as in the Sciences\". This was first published in 1724, and it was printed in twenty editions.\n\nWatts wrote this work for beginners of logic, and arranged the book methodically. He divided the content of his elementary treatment of logic into four parts: perception, judgement, reasoning, and method, which he treated in this order. Each of these parts is divided into chapters, and some of these chapters are divided into sections. The content of the chapters and sections is subdivided by the following devices: divisions, distributions, notes, observations, directions, rules, illustrations, and remarks. Every contentum of the book comes under one or more of these headings, and this methodical arrangement serves to make the exposition clear.\n\nIn Watts' \"Logic,\" there are notable departures from other works of the time, and some notable innovations. The influence of British empiricism may be seen, especially that of contemporary philosopher and empiricist John Locke. \"Logic\" includes several references to Locke and his \"Essay Concerning Human Understanding\", in which he espoused his empiricist views. Watts was careful to distinguish between judgements and propositions, unlike some other logic authors. According to Watts, judgement is \"to compare... ideas together, and to join them by affirmation, or disjoin then by negation, according as we find them to agree or disagree\". He continues, \"when mere ideas are joined in the mind without words, it is rather called a judgement; but when clothed with words it is called a proposition\". Watts' \"Logic\" follows the scholastic tradition and divides propositions into universal affirmative, universal negative, particular affirmative, and particular negative.\n\nIn the third part, Watts discusses reasoning and argumentation, with particular emphasis on the theory of syllogism. This was considered a centrally important part of classical logic. According to Watts, and in keeping with logicians of his day, Watts defined logic as an art (see liberal arts), as opposed to a science. Throughout \"Logic,\" Watts revealed his high conception of logic by stressing the practical side of logic, rather than the speculative side. According to Watts, as a practical art, logic can be really useful in any inquiry, whether it is an inquiry in the arts, or inquiry in the sciences, or inquiry of an ethical kind. Watts' emphasis on logic as a practical art distinguishes his book from others.\n\nBy stressing a practical and non-formal part of logic, Watts gave rules and directions for any kind of inquiry, including the inquiries of science and the inquiries of philosophy. These rules of inquiry were given in addition to the formal content of classical logic common to textbooks on logic from that time. Watts' conception of logic as being divided into its practical part and its speculative part marks a departure from the conception of logic of most other authors. His conception of logic is more akin to that of the later, nineteenth-century logician, C. S. Peirce.\n\nIsaac Watts' \"Logic\" became the standard text on logic at Oxford, Cambridge, Harvard and Yale, being used at Oxford for well over 100 years. C. S. Peirce, the great nineteenth-century logician, wrote favourably of Watts' \"Logic\". When preparing his own textbook, titled \"A Critick of Arguments: How to Reason\" (also known as the \"Grand Logic\"), Peirce wrote, 'I shall suppose the reader to be acquainted with what is contained in Dr Watts' \"Logick\", a book... far superior to the treatises now used in colleges, being the production of a man distinguished for good sense.'\n\nWatts followed the \"Logic\" in 1741 by a supplement, \"The Improvement of the Mind.\" This also went through numerous editions and later inspired Michael Faraday. It was also widely used as a moral textbook in schools.\n\nOn his death, Isaac Watts' papers were given to Yale University in the Colony of Connecticut, which nonconformists (Puritans/Congregationalists) had established. King Edward VI School, Southampton, which he attended, named one of its houses \"Watts\" in his honour.\n\nThe Church of England and Lutheran Church remember Watts (and his ministerial service) annually in the Calendar of Saints on 25 November, and the Episcopal Church on the following day.\n\nThe earliest surviving built memorial to Isaac Watts is at Westminster Abbey; this was completed shortly after his death. His much-visited chest tomb at Bunhill Fields dates from 1808, replacing the original that had been paid for and erected by Lady Mary Abney and the Hartopp family. A stone bust of Watts is installed at the nonconformist Dr Williams's Library, in central London. The earliest public statue, erected in 1845, stands at Abney Park, where Watts had lived for more than 30 years at the manor house, where he also died. The park was later devoted to uses as a cemetery and public arboretum. A later, rather similar statue was funded by public subscription and erected in a new Victorian public park named for Watts in Southampton, the city of his birth. In the mid-nineteenth century, the Congregational Dr Watts Memorial Hall was built in Southampton and named for him. After World War II, it was lost to redevelopment. The Isaac Watts Memorial United Reformed Church was built on the site and named for him. One of the earliest built memorials may also now be lost: a bust to Watts that was commissioned on his death for the London chapel with which he was associated. The chapel was demolished in the late 18th century; remaining parts of the memorial were rescued at the last minute by a wealthy landowner for installation in his chapel near Liverpool. It is unclear whether the bust survives. The stone statue in front of the Abney Park Chapel at Dr Watts' Walk, Abney Park Cemetery, was erected in 1845 by public subscription. It was designed by the leading British sculptor, Edward Hodges Baily RA FRS. A scheme for a commemorative statue on this spot had first been promoted in the late 1830s by George Collison, who in 1840 published an engraving as the frontispiece of his book about cemetery design in Europe and America; and at Abney Park Cemetery in particular. This first cenotaph proposal was never commissioned, and Baily's later design was adopted in 1845. In 1974, the City of Southampton (Watts' home city) commemorated the 300 year anniversary of his birth by commissioning the biography \"Isaac Watts Remembered\", written by David G. Fountain, who like Watts, was also a nonconformist minister from Southampton.\n\nOne of Watts' best-known poems was an exhortation \"\" in \"Divine Songs for Children.\" This was parodied by Lewis Carroll in the poem \"How Doth the Little Crocodile\", included in his book \"Alice's Adventures in Wonderland.\" His parody is better known than Watts' original poem. The poem was also featured in the segment on the cartoon programme \"Rocky and His Friends\" called \"Bullwinkle's Corner\", in which Bullwinkle Moose recites poetry. In this case, the poem was titled \"The Bee\", with no author credit.\n\nIn his novel \"David Copperfield\" (1850), Charles Dickens has school master Dr. Strong quote from Watts' \"Against Idleness and Mischief\".\n\nThe 1884 comic opera \"Princess Ida\" includes a punning reference to Watts in Act I. At Princess Ida's women's university, no males are allowed. Her father King Gama says that \"She'll scarcely suffer Dr. Watts' 'hymns'\".\n\nA poem often referred to as \"False Greatness\" by Joseph Merrick (\"The Elephant Man\"), which was used in writing or \"signature block\" by Merrick, starting \"Tis true, my form is something odd\nbut blaming me, is blaming God...\" is often (incorrectly) quoted or cited as a work by Isaac Watts. In fact only the last few sentences were penned by Watts (\"False Greatness\", book II-Horae lyricae 1743) starting \"Mylo, forbear to call him bless'd That only boasts a large estate...\"\n\n\nWatts' hymns include:\n\n\nMany of Watts' hymns are included in the Anglican Hymns Ancient and Modern, the Oxford Book of Common Praise, the Christadelphian hymnal, the Episcopal Church's \"Hymnal 1982\", \"Evangelical Lutheran Worship\", the Baptist Hymnal, the Presbyterian Trinity Hymnal, and the Methodist Hymns and Psalms. Many of his texts are also used in the American hymnal, \"The Sacred Harp,\" using what is known as the shape note notation used for teaching non-musicians. Several of his hymns are used in the hymnals of the Church of Christ, Scientist and The Church of Jesus Christ of Latter-day Saints.\n\n\n\n"}
{"id": "47375779", "url": "https://en.wikipedia.org/wiki?curid=47375779", "title": "Joaquim Gomes de Souza", "text": "Joaquim Gomes de Souza\n\nJoaquim Gomes de Souza \"Souzinha\" (15 February 1829, in Itapecuru Mirim – 1 June 1864, in London) was a Brazilian mathematician who worked on numerical analysis and differential equations. He was a pioneer on the study of mathematics in Brazil, and was described by José Leite Lopes as \"the first great mathematician from Brazil\".\n\nIn 1844, Gomes de Souza enrolled at the Faculdade de Medicina do Rio de Janeiro (now a part of the Federal University of Rio de Janeiro) to study medicine. He had a deep love for the natural sciences, which led him to also be interested in mathematics, and so he started to learn mathematics as a self-taught in parallel with his studies of medicine.\n\nIn 1848, he obtained his doctorate in mathematics from the Escola Real Militar, with the thesis \"Dissertação Sobre o Modo de Indagar novos Astros sem o Auxílio das Observações Directas\" (Dissertation about the means of investigating new celestial objects without the aid of direct observations).\n\nHe later went to the Sorbonne, in France, where he continued his mathematical studies. He was a personal friend of Cauchy, of whose classes he attended (in one of them, Souza spotted a mathematical mistake by Cauchy, he then asked his license and corrected it on the blackboard). In 1856, he obtained a doctorate in medicine from Paris Faculty of Medicine. In the same year, he presented his mathematical works at the \"Académie des sciences\".\n\nSouza held a paid public post in Brazil, and after much time in Europe, he was noticed he should return immediately to Brazil because he had been elected a member of the parliament. Souza had already married Rosa Edith in England and then had to return to Brazil without her.\n\nIn his book \"Mélanges de calcul intégral\" (1882), Souza aimed to obtain a general method to solve PDEs, according to Manfredo do Carmo: \"[in his book] He [Souza] employed methods not entirely rigorous and it is not clear exactly how much of his work would remain if submitted to a careful scrutiny; as far as I know, it was never put to such a test.\"\n\nHe died at the age of 35, in London. The cause of death was a disease of the lung. C. S. Fernandez and C. M. Souza described his endeavorer in Europe: \"He was audacious and fought with insistence for his scientific recognition in Europe. His effort was fruitless, though.\"\n\n\n"}
{"id": "15571662", "url": "https://en.wikipedia.org/wiki?curid=15571662", "title": "Joint Statistical Meetings", "text": "Joint Statistical Meetings\n\nThe Joint Statistical Meetings (JSM) is a professional conference/academic conference for statisticians held annually every year since 1840 (usually in August). Billed as \"the largest gathering of statisticians held in North America\", JSM has attracted over 5000 participants in recent years. The following statistical societies are designated as official JSM partners:\nThe founding members of JSM were the ASA, IMS, IBS, and SSC.\n\nIn addition to committee meetings, JSM activities include\n\nSince 1978 when attendance figures were first reported.\n"}
{"id": "3884735", "url": "https://en.wikipedia.org/wiki?curid=3884735", "title": "KY-68", "text": "KY-68\n\nTSEC/KY-68 DSVT, commonly known as Digital Subscriber Voice Terminal, is a US military ruggedized, full- or half-duplex tactical telephone system with a built-in encryption/decryption module for secure traffic. \n\nIt transmits voice and data at 16 or 32 kbit/s, converting voice to a digital signal. The KY-68 can operate via civilian and military switches in either encrypted or un-encrypted mode, or point-to-point (encrypted mode only).\n\nAlthough used primarily for secure communications, the KY-68 can also transmit to a Digital Non-secure Voice Terminal (DNVT). A local switch warns the KY-68 user with a tone signal when initiating communication with a non-secure terminal.\nThe KY-68 is keyed using an Electronic Transfer Device, typically either a KYK-13 or AN/CYZ-10.\n\nAn almost identical office version (KY-78) features the same electronics as the KY-68, but has an exterior casing composed of lighter materials.\n\nThe KY-68 and KY-78 are approved for use with SECRET-classified information, and despite the KY-78 being compromised in the early 1990s, both versions remain in use.\n\n\n"}
{"id": "2855255", "url": "https://en.wikipedia.org/wiki?curid=2855255", "title": "Limits of computation", "text": "Limits of computation\n\nThere are several physical and practical limits to the amount of computation or data storage that can be performed with a given amount of mass, volume, or energy:\n\n\nSeveral methods have been proposed for producing computing devices or data storage devices that approach physical and practical limits:\n\nIn the field of theoretical computer science the computability and complexity of computational problems are often sought-after. Computability theory describes the degree to which problems are computable; whereas complexity theory describes the asymptotic degree of resource consumption. Computational problems are therefore confined into complexity classes. The arithmetical hierarchy and polynomial hierarchy classify the degree to which problems are respectively computable and computable in polynomial time. For instance, the level formula_5 of the arithmetical hierarchy classifies computable, partial functions. Moreover, this hierarchy is strict such that at any other class in the arithmetic hierarchy classifies strictly uncomputable functions.\n\nMany limits derived in terms of physical constants and abstract models of computation in Computer Science are loose. Very few known limits directly obstruct leading-edge technologies, but many engineering obstacles currently cannot be explained by closed-form limits.\n\n"}
{"id": "39291986", "url": "https://en.wikipedia.org/wiki?curid=39291986", "title": "Linear seismic inversion", "text": "Linear seismic inversion\n\nInverse modeling is a mathematical technique where the objective is to determine the physical properties of the subsurface of an earth region that has produced a given seismogram. Cooke and Schneider (1983) defined it as calculation of the earth’s structure and physical parameters from some set of observed seismic data. The underlying assumption in this method is that the collected seismic data are from an earth structure that matches the cross-section computed from the inversion algorithm. Some common earth properties that are inverted for include acoustic velocity, formation and fluid densities, acoustic impedance, Poisson's ratio, formation compressibility, shear rigidity, porosity, and fluid saturation.\n\nThe method has long been useful for geophysicists and can be categorized into two broad types: Deterministic and stochastic inversion. Deterministic inversion methods are based on comparison of the output from an earth model with the observed field data and continuously updating the earth model parameters to minimize a function, which is usually some form of difference between model output and field observation. As such, this method of inversion to which linear inversion falls under is posed as an minimization problem and the accepted earth model is the set of model parameters that minimizes the objective function in producing a numerical seismogram which best compares with collected field seismic data.\n\nOn the other hand, stochastic inversion methods are used to generate constrained models as used in reservoir flow simulation, using geostatistical tools like kriging. As opposed to deterministic inversion methods which produce a single set of model parameters, stochastic methods generate a suite of alternate earth model parameters which all obey the model constraint. However, the two methods are related as the results of deterministic models is the average of all the possible non-unique solutions of stochastic methods. Since seismic linear inversion is a deterministic inversion method, the stochastic method will not be discussed beyond this point.\n\nThe deterministic nature of linear inversion requires a functional relationship which models, in terms of the earth model parameters, the seismic variable to be inverted. This functional relationship is some mathematical model derived from the fundamental laws of physics and is more often called a forward model. The aim of the technique is to minimize a function which is dependent on the difference between the convolution of the forward model with a source wavelet and the field collected seismic trace. As in the field of optimization, this function to be minimized is called the objective function and in convectional inverse modeling, is simply the difference between the convolved forward model and the seismic trace. As earlier mentioned, different types of variables can be inverted for but for clarity, these variables will be referred to as the impedance series of the earth model. In the following subsections we will describe in more detail, in the context of linear inversion as a minimization problem, the different components that are necessary to invert seismic data.\n\nThe centerpiece of seismic linear inversion is the forward model which models the generation of the experimental data collected. According to Wiggins (1972), it provides a functional (computational) relationship between the model parameters and calculated values for the observed traces. Depending on the seismic data collected, this model may vary from the classical wave equations for predicting particle displacement or fluid pressure for sound wave propagation through rock or fluids, to some variants of these classical equations. For example, the forward model in Tarantola (1984) is the wave equation for pressure variation in a liquid media during seismic wave propagation while by assuming constant velocity layers with plane interfaces, Kanasewich and Chiu (1985) used the brachistotrone model of John Bernoulli for travel time of a ray along a path. In Cooke and Schneider (1983), the model is a synthetic trace generation algorithm expressed as in Eqn. 3, where R(t) is generated in the Z-domain by recursive formula. In whatever form the forward model appears, it is important that it not only predicts the collected field data, but also models how the data is generated. Thus, the forward model by Cooke and Schneider (1983) can only be used to invert CMP data since the model invariably assumes no spreading loss by mimicking the response of a laterally homogeneous earth to a plane-wave source\n\nwhere \"s\"(\"t\") = synthetic trace, \"w\"(\"t\") = source wavelet, and \"R\"(\"t\") = reflectivity function.\n\nAn important numerical process in inverse modeling is to minimize the objective function, which is a function defined in terms of the difference between the collected field seismic data and the numerically computed seismic data. Classical objective functions include the sum of squared deviations between experimental and numerical data, as in the least squares methods, the sum of the magnitude of the difference between field and numerical data, or some variant of these definitions. Irrespective of the definition used, numerical solution of the inverse problem is obtained as earth model that minimize the objective function.\n\nIn addition to the objective function, other constraints like known model parameters and known layer interfaces in some regions of the earth are also incorporated in the inverse modeling procedure. These constraints, according to Francis 2006, help to reduce non-uniqueness of the inversion solution by providing a priori information that is not contained in the inverted data while Cooke and Schneider (1983) reports their useful in controlling noise and when working in a geophysically well-known area.\n\nThe objective of mathematical analysis of inverse modeling is to cast the generalized linear inverse problem into a simple matrix algebra by considering all the components described in previous sections. viz; forward model, objective function etc. Generally, the numerically generated seismic data are non-linear functions of the earth model parameters. To remove the non-linearity and create a platform for application of linear algebra concepts, the forward model is linearized by expansion using a Taylor series as carried out below. For more details see Wiggins (1972), Cooke and Schneider (1983).\n\nConsider a set of formula_1 seismic field observations formula_2, for formula_3 and a set of formula_4 earth model parameters formula_5 to be inverted for, for formula_6. The field observations can be represented in either formula_7 or formula_8, where formula_9 and formula_7 are vectorial representations of model parameters and the field observations as a function of earth parameters. Similarly, for formula_11 representing guesses of model parameters, formula_12 is the vector of numerical computed seismic data using the forward model of Sec. 1.3. Taylor's series expansion of formula_7 about formula_14 is given below.\n\nformula_15 is called the difference vector in Cooke and Schneider (1983). It has a size of formula_16 and its components are the difference between the observed trace and the numerically computed seismic data. formula_17 is the corrector vector of size formula_18, while formula_19 is called the sensitivity matrix. It has a size of formula_20 and its comments are such that each column is the partial derivative of a component of the forward function with respect to one of the unknown earth model parameters. Similarly, each row is the partial derivative of a component of the numerically computed seismic trace with respect to all unknown model parameters.\n\nformula_12 is computed from the forward model, while formula_7 is the experimental data. Thus,formula_15 is a known quality. On the other hand, formula_17 is unknown and is obtained by solution of Eqn. 10. This equation is theoretically solvable only when formula_19 is invertible, that is, if it is a square matrix so that the number of observations formula_1 is equal to the number formula_4 of unknown earth parameters. If this is the case, the unknown corrector vector formula_17, is solved for as shown below, using any of the classical direct or iterative solvers for solution of a set of linear equations.\n\nIn most seismic inversion applications, there are more observations than the number of earth parameters to be inverted for, i.e. formula_29, leading to a system of equations that is mathematically over-determined. As a result, Eqn. 10 is not theoretically solvable and an exact solution is not obtainable. An estimate of the corrector vector is obtained using the least squares procedure to find the corrector vector formula_30 that minimizes formula_31, which is the sum of the squares of the error, formula_32.\n\nThe errorformula_32 is given by\n\nIn the least squares procedure, the corrector vector that minimizes formula_34 is obtained as below.\n\nThus,\n\nFrom the above discussions, the objective function is defined as either the formula_35 or formula_36 norm of formula_30 given by\nformula_38 or formula_39 or of formula_15 given by formula_41 or formula_42.\n\nThe generalized procedure for inverting any experimental seismic data for formula_43 or formula_44, using the mathematical theory for inverse modeling, as described above, is shown in Fig. 1 and described as follows.\n\nAn initial guess of the model impedance is provided to initiate the inversion process. The forward model uses this initial guess to compute a synthetic seismic data which is subtracted from the observed seismic data to calculate the difference vector.\n\n\nIrrespective of the variable to be inverted for, the earth’s impedance is a continuous function of depth (or time in seismic data) and for numerical linear inversion technique to be applicable for this continuous physical model, the continuous properties have to be discretized and/or sampled at discrete intervals along the depth of the earth model. Thus, the total depth over which model properties are to be determined is a necessary starting point for the discretization. Commonly, as shown in Fig. 3, this properties are sampled at close discrete intervals over this depth to ensure high resolution of impedance variation along the earth’s depth. The impedance values inverted from the algorithm represents the average value in the discrete interval.\n\nConsidering that inverse modeling problem is only theoretically solvable when the number of discrete intervals for sampling the properties is equal to the number of observation in the trace to be inverted, a high-resolution sampling will lead to a large matrix which will be very expensive to invert. Furthermore, the matrix may be singular for dependent equations, the inversion can be unstable in the presence of noise and the system may be under-constrained if parameters other than the primary variables inverted for, are desired. In relation to parameters desired, other than impedance, Cooke and Schneider (1983) gives them to include source wavelet and scale factor.\n\nFinally, by treating constraints as known impedance values in some layers or discrete intervals, the number of unknown impedance values to be solved for are reduced, leading to greater accuracy in the results of the inversion algorithm.\n\nWe start with an example to invert for earth parameter values from temperature depth distribution in a given earth region. Although this example does not directly relate to seismic inversion since no traveling acoustic waves are involved, it nonetheless introduces practical application of the inversion technique in a manner easy to comprehend, before moving on to seismic applications. In this example, the temperature of the earth is measured at discrete locations in a well bore by placing temperature sensors in the target depths. By assuming a forward model of linear distribution of temperature with depth, two parameters are inverted for from the temperature depth measurements.\n\nThe forward model is given by\nwhere formula_54. Thus, the dimension of formula_14 is 2 i.e. the number of parameters inverted for is 2.\n\nThe objective of this inversion algorithm is to find formula_9, which is the value of formula_57 that minimizes the difference between the observed temperature distribution and those obtained using the forward model of Eqn. 15. Considering the dimension of the forward model or the number of temperature observations to be formula_4, the components of the forward model is written as\n\nWe present results from Marescot (2010) for the case of formula_59 for which the observed temperature values at depths were formula_60 at formula_61 and formula_62 at formula_63. These experimental data were inverted to obtain earth parameter values of formula_64 and formula_65. For a more general case with large number of temperature observations, Fig. 4 shows the final linear forward model obtained from using the inverted values of formula_66 and formula_67. The figure shows a good match between experimental and numerical data.\n\nThis examples inverts for earth layer velocity from recorded seismic wave travel times. Fig. 5 shows the initial velocity guesses and the travel times recorded from the field, while Fig. 6a shows the inverted heterogeneous velocity model, which is the solution of the inversion algorithm obtained after 30 iterations. As seen in Fig. 6b, there is good comparison between the final travel times obtained from the forward model using the inverted velocity and the field record travel times. Using these solutions, the ray path was reconstructed and is shown to be highly tortuous through the earth model as shown in Fig. 7.\n\nThis example, taken from Cooke and Schneider (1983), shows inversion of a CMP seismic trace for earth model impedance (product of density and velocity) profile. The seismic trace inverted is shown in Fig. 8 while Fig. 9a shows the inverted impedance profile with the input initial impedance used for the inversion algorithm. Also recorded alongside the seismic trace is an impedance log of the earth region as shown in Fig. 9b. The figures show good comparison between the recorded impedance log and the numerical inverted impedance from the seismic trace.\n\n"}
{"id": "49655161", "url": "https://en.wikipedia.org/wiki?curid=49655161", "title": "List of things named after Friedrich Bessel", "text": "List of things named after Friedrich Bessel\n\nThis is a (partial) list of things named for Friedrich Wilhelm Bessel, a 19th-century German scholar who worked in astronomy, geodesy and mathematical sciences:\n\n\n\n"}
{"id": "26256312", "url": "https://en.wikipedia.org/wiki?curid=26256312", "title": "Literal movement grammar", "text": "Literal movement grammar\n\nLiteral movement grammars (LMGs) are a grammar formalism introduced by Groenink in 1995 intended to characterize certain extraposition phenomena of natural language such as topicalization and cross-serial dependencies. LMGs extend the class of CFGs by adding introducing pattern-matched function-like rewrite semantics, as well as the operations of variable binding and slash deletion.\n\nThe basic rewrite operation of an LMG is very similar to that of a CFG, with the addition of \"arguments\" to the non-terminal symbols. Where a context-free rewrite rule obeys the general schema formula_1 for some non-terminal formula_2 and some string of terminals and/or non-terminals formula_3, an LMG rewrite rule obeys the general schema formula_4, where X is a non-terminal with arity n (called a predicate in LMG terminology), and formula_3 is a string of \"items\", as defined below. The arguments formula_6 are strings of terminal symbols and/or variable symbols defining an argument pattern. In the case where an argument pattern has multiple adjacent variable symbols, the argument pattern will match any and all partitions of the actual value that unify. Thus, if the predicate is formula_7 and the actual pattern is formula_8, there are three valid matches: formula_9. In this way, a single rule is actually a family of alternatives.\n\nAn \"item\" in a literal movement grammar is one of\n\n\nIn a rule like formula_16, the variable y is bound to whatever terminal string the g predicate produces, and in formula_3 and formula_18, all occurrences of y are replaced by that string, and formula_3 and formula_18 are produced as if terminal string had always been there.\n\nAn item formula_21, where x is something that produces a terminal string (either a terminal string itself or some predicate), and y is a string of terminals and/or variables, is rewritten as the empty string (formula_22) if and only if formula_23, and otherwise cannot be rewritten at all.\n\nLMGs can characterize the non-CF language formula_24 as follows:\n\nThe derivation for \"aabbcc\", using parentheses also for grouping, is therefore\n\nformula_30\n\nLanguages generated by LMGs contain the context-free languages as a proper subset, as every CFG is an LMG where all predicates have arity 0 and no production rule contains variable bindings or slash deletions.\n"}
{"id": "30109665", "url": "https://en.wikipedia.org/wiki?curid=30109665", "title": "Logico-linguistic modeling", "text": "Logico-linguistic modeling\n\nLogico-linguistic modeling is a method for building knowledge-based systems with a learning capability using conceptual models from soft systems methodology, modal predicate logic, and the Prolog artificial intelligence language.\n\nLogico-linguistic modeling is a six-stage method developed primarily for building knowledge-based systems (KBS), but it also has application in manual decision support systems and information source analysis. Logico-linguistic models have a superficial similarity to John F. Sowa's conceptual graphs; both use bubble style diagrams, both are concerned with concepts, both can be expressed in logic and both can be used in artificial intelligence. However, logico-linguistic models are very different in both logical form and in their method of construction.\nLogico-linguistic modeling was developed in order to solve theoretical problems found in the soft systems method for information system design. The main thrust of the research into has been to show how soft systems methodology (SSM), a method of systems analysis, can be extended into artificial intelligence.\n\nSSM employs three modeling devices i.e. rich pictures, root definitions, and conceptual models of human activity systems. The root definitions and conceptual models are built by stakeholders themselves in an iterative debate organized by a facilitator. The strengths of this method lie, firstly, in its flexibility, the fact that it can address any problem situation, and, secondly, in the fact that the solution belongs to the people in the organization and is not imposed by an outside analyst.\n\nInformation requirements analysis (IRA) took the basic SSM method a stage further and showed how the conceptual models could be developed into a detailed information system design. IRA calls for the addition of two modeling devices: \"Information Categories\", which show the required information inputs and outputs from the activities identified in an expanded conceptual model; and the \"Maltese Cross\", a matrix which shows the inputs and outputs from the information categories and shows where new information processing procedures are required. A completed Maltese Cross is sufficient for the detailed design of a transaction processing system.\n\nThe initial impetus to the development of logico-linguistic modeling was a concern with the theoretical problem of how an information system can have a connection to the physical world. This is a problem in both IRA and more established methods (such as SSADM) because none base their information system design on models of the physical world. IRA designs are based on a notional conceptual model and SSADM is based on models of the movement of documents.\n\nThe solution to these problems provided a formula that was not limited to the design of transaction processing systems but could be used for the design of KBS with learning capability.\n\nThe logico-linguistic modeling method comprises six stages.\n\nIn the first stage logico-linguistic modeling uses SSM for systems analysis. This stage seeks to structure the problem in the client organization by identifying stakeholders, modelling organizational objectives and discussing possible solutions. At this stage it not assumed that a KBS will be a solution and logico-linguistic modeling often produces solutions that do not require a computerized KBS.\n\nExpert systems tend to capture the expertise, of individuals in different organizations, on the same topic. By contrast a KBS, produced by logico-linguistic modeling, seeks to capture the expertise of individuals in the same organization on different topics. The emphasis is on the elicitation of organizational or group knowledge rather than individual experts. In logico-linguistic modeling the stakeholders become the experts.\n\nThe end point of this stage is an SSM style conceptual models such as figure 1.\n\nAccording to the theory behind logico-linguistic modeling the SSM conceptual model building process is a Wittgensteinian language-game in which the stakeholders build a language to describe the problem situation. The logico-linguistic model expresses this language as a set of definitions, see figure 2.\n\nAfter the model of the language has been built putative knowledge about the real world can be added by the stakeholders. Traditional SSM conceptual models contain only one logical connective (a necessary condition). In order to represent causal sequences, \"sufficient conditions\" and \"necessary and sufficient conditions\" are also required. In logico-linguistic modeling this deficiency is remedied by two addition types of connective. The outcome of stage three is an empirical model, see figure 3.\n\nModal predicate logic (a combination of modal logic and predicate logic) is used as the formal method of knowledge representation. The connectives from the language model are logically true (indicated by the \"\"L\" modal operator) and connective added at the knowledge elicitation stage are possibility true (indicated by the \"M\"\" modal operator). Before proceeding to stage 5, the models are expressed in logical formulae.\n\nFormulae in predicate logic translate easily into the Prolog artificial intelligence language. The modality is expressed by two different types of Prolog rules. Rules taken from the language creation stage of model building process are treated as incorrigible. While rules from the knowledge elicitation stage are marked as hypothetical rules. The system is not confined to decision support but has a built in learning capability.\n\nA knowledge based system built using this method verifies itself. Verification takes place when the KBS is used by the clients. It is an ongoing process that continues throughout the life of the system. If the stakeholder beliefs about the real world are mistaken this will be brought out by the addition of Prolog facts that conflict with the hypothetical rules. It operates in accordance to the classic principle of falsifiability found in the philosophy of science\n\nLogico-linguistic modeling has been used to produce fully operational computerized knowledge based systems, such as one for the management of diabetes patients in a hospital out-patients department.\n\nIn other projects the need to move into Prolog was considered unnecessary because the printed logico-linguistic models provided an easy to use guide to decision making. For example, a system for mortgage loan approval\n\nIn some cases a KBS could not be built because the organization did not have all the knowledge needed to support all their activities. In these cases logico-linguistic modeling showed shortcomings in the supply of information and where more was needed. For example, a planning department in a telecoms company\n\nWhile logico-linguistic modeling overcomes the problems found in SSM's transition from conceptual model to computer code, it does so at the expense of increased stakeholder constructed model complexity. The benefits of this complexity are questionable\nand this modeling method may be much harder to use than other methods.\n\nThis contention has been exemplified by subsequent research. An attempt by researchers to model buying decisions across twelve companies using logico-linguistic modeling required simplification of the models and removal of the modal elements.\n\n\n"}
{"id": "233488", "url": "https://en.wikipedia.org/wiki?curid=233488", "title": "Machine learning", "text": "Machine learning\n\nMachine learning (ML) is the study of algorithms and mathematical models that computer systems use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to perform the task. Machine learning algorithms are used in the applications of email filtering, detection of network intruders, and computer vision, where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to computational statistics, which focuses on making predictions using computers. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a field of study within machine learning, and focuses on exploratory data analysis through unsupervised learning. In its application across business problems, machine learning is also referred to as predictive analytics.\n\nThe name \"machine learning\" was coined in 1959 by Arthur Samuel. Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: \"A computer program is said to learn from experience \"E\" with respect to some class of tasks \"T\" and performance measure \"P\" if its performance at tasks in \"T\", as measured by \"P\", improves with experience \"E\".\" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper \"Computing Machinery and Intelligence\", in which the question \"Can machines think?\" is replaced with the question \"Can machines do what we (as thinking entities) can do?\". In Turing's proposal the various characteristics that could be possessed by a \"thinking machine\" and the various implications in constructing one are exposed.\n\nMachine learning tasks are classified into several broad categories. In supervised learning, the algorithm builds a mathematical model of a set of data that contains both the inputs and the desired outputs. For example, if the task were determining whether an image contained a certain object, the training data for a supervised learning algorithm would include images with and without that object (the input), and each image would have a label (the output) designating whether it contained the object. In special cases, the input may be only partially available, or restricted to special feedback. Semi-supervised learning algorithms develop mathematical models from incomplete training data, where a portion of the sample inputs are missing the desired output.\n\nClassification algorithms and regression algorithms are types of supervised learning. Classification algorithms are used when the outputs are restricted to a limited set of values. For a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. For an algorithm that identifies spam emails, the output would be the prediction of either \"spam\" or \"not spam\", represented by the Boolean values one and zero. Regression algorithms are named for their continuous outputs, meaning they may have any value within a range. Examples of a continuous value are the temperature, length, or price of an object.\n\nIn unsupervised learning, the algorithm builds a mathematical model of a set of data which contains only inputs and no desired outputs. Unsupervised learning algorithms are used to find structure in the data, like grouping or clustering of data points. Unsupervised learning can discover patterns in the data, and can group the inputs into categories, as in feature learning. Dimensionality reduction is the process of reducing the number of \"features\", or inputs, in a set of data. \n\nActive learning algorithms access the desired outputs (training labels) for a limited set of inputs based on a budget, and optimize the choice of inputs for which it will acquire training labels. When used interactively, these can be presented to a human user for labeling. Reinforcement learning algorithms are given feedback in the form of positive or negative reinforcement in a dynamic environment, and are used in autonomous vehicles or in learning to play a game against a human opponent. Other specialized algorithms in machine learning include topic modeling, where the computer program is given a set of natural language documents and finds other documents that cover similar topics. Machine learning algorithms can be used to find the unobservable probability density function in density estimation problems. Meta learning algorithms learn their own inductive bias based on previous experience. In developmental robotics, robot learning algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies, and imitation.\n\nArthur Samuel, an American pioneer in the field of computer gaming and artificial intelligence, coined the term \"Machine Learning\" in 1959 while at IBM. \nAs a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed \"neural networks\"; these were mostly perceptrons and other models that were later found to be reinventions of the generalized linear models of statistics. Probabilistic reasoning was also employed, especially in automated medical diagnosis.\n\nHowever, an increasing emphasis on the logical, knowledge-based approach caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation. By 1980, expert systems had come to dominate AI, and statistics was out of favor. Work on symbolic/knowledge-based learning did continue within AI, leading to inductive logic programming, but the more statistical line of research was now outside the field of AI proper, in pattern recognition and information retrieval. Neural networks research had been abandoned by AI and computer science around the same time. This line, too, was continued outside the AI/CS field, as \"connectionism\", by researchers from other disciplines including Hopfield, Rumelhart and Hinton. Their main success came in the mid-1980s with the reinvention of backpropagation.\n\nMachine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and probability theory. It also benefited from the increasing availability of digitized information, and the ability to distribute it via the Internet.\n\nMachine learning and data mining often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on \"known\" properties learned from the training data, data mining focuses on the discovery of (previously) \"unknown\" properties in the data (this is the analysis step of knowledge discovery in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as \"unsupervised learning\" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, ECML PKDD being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to \"reproduce known\" knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously \"unknown\" knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.\n\nMachine learning also has intimate ties to optimization: many learning problems are formulated as minimization of some loss function on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.\n\nMachine learning and statistics are closely related fields. According to Michael I. Jordan, the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics. He also suggested the term data science as a placeholder to call the overall field.\n\nLeo Breiman distinguished two statistical modelling paradigms: data model and algorithmic model, wherein \"algorithmic model\" means more or less the machine learning algorithms like Random forest.\n\nSome statisticians have adopted methods from machine learning, leading to a combined field that they call \"statistical learning\".\n\nA core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.\n\nThe computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.\n\nFor the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.\n\nIn addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.\n\nThe types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.\n\nSupervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs. The data is known as training data, and consists of a set of training examples. Each training example has one or more inputs and a desired output, also known as a supervisory signal. In the case of semi-supervised learning algorithms, some of the training examples are missing the desired output. In the mathematical model, each training example is represented by an array or vector, and the training data by a matrix. Through the process of iteration, supervised learning algorithms develop and optimize a function that can be used to predict the output associated with new inputs. An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.\n\nSupervised learning algorithms include classification and regression. Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. Similarity learning is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in ranking, recommendation systems, visual identity tracking, face verification, and speaker verification.\n\nUnsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms therefore learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses other domains involving summarizing and explaining data features.\n\nCluster analysis is the assignment of a set of observations into subsets (called \"clusters\") so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some \"similarity metric\" and evaluated, for example, by \"internal compactness\", or the similarity between members of the same cluster, and \"separation\", the difference between clusters. Other methods are based on \"estimated density\" and \"graph connectivity\".\n\nReinforcement learning is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, statistics and genetic algorithms. In machine learning, the environment is typically represented as a Markov Decision Process (MDP). Many reinforcement learning algorithms use dynamic programming techniques. Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible. Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.\n\nVarious processes, techniques and methods can be applied to one or more types of machine learning algorithms to enhance their performance.\n\nSeveral learning algorithms aim at discovering better representations of the inputs provided during training. Classic examples include principal components analysis and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual feature engineering, and allows a machine to both learn the features and use them to perform a specific task.\n\nFeature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include artificial neural networks, multilayer perceptrons, and supervised dictionary learning. In unsupervised feature learning, features are learned with unlabeled input data. Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.\n\nManifold learning algorithms attempt to do so under the constraint that the learned representation is low-dimensional. Sparse coding algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. Multilinear subspace learning algorithms aim to learn low-dimensional representations directly from tensor representations for multidimensional data, without reshaping them into higher-dimensional vectors. Deep learning algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.\n\nFeature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.\n\nSparse dictionary learning is a feature learning method where a training example is represented as a linear combination of basis functions, and is assumed to be a sparse matrix. The method is strongly NP-hard and difficult to solve approximately. A popular heuristic method for sparse dictionary learning is the K-SVD algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine to which classes a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in image de-noising. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.\n\nIn data mining, anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data. Typically, the anomalous items represent an issue such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are referred to as outliers, novelties, noise, deviations and exceptions.\n\nIn particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.\n\nThree broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as \"normal\" and \"abnormal\" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model.\n\nDecision tree learning uses a decision tree as a predictive model to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.\n\nAssociation rule learning is a rule-based machine learning method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of \"interestingness\". This rule-based approach generates new rules as it analyzes more data. The ultimate goal, assuming the set of data is large enough, is to help a machine mimic the human brain’s feature extraction and abstract association capabilities for data that has not been categorized.\n\nRule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves \"rules\" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction. Rule-based machine learning approaches include learning classifier systems, association rule learning, and artificial immune systems.\n\nBased on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule formula_1 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional pricing or product placements. In addition to market basket analysis, association rules are employed today in application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.\n\nLearning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a genetic algorithm, with a learning component, performing either supervised learning, reinforcement learning, or unsupervised learning. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions.\n\nInductive logic programming (ILP) is an approach to rule-learning using logic programming as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that entails all positive and no negative examples. Inductive programming is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as functional programs.\n\nInductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term \"inductive\" here refers to philosophical induction, suggesting a theory to explain observed facts, rather than mathematical induction, proving a property for all members of a well-ordered set.\n\nArtificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired by the biological neural networks that constitute animal brains. The neural network itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs. Such systems \"learn\" to perform tasks by considering examples, generally without being programmed with any task-specific rules.\n\nAn ANN is a model based on a collection of connected units or nodes called \"artificial neurons\", which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit information, a \"signal\", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a real number, and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called \"edges\". Artificial neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.\n\nThe original goal of the ANN approach was to solve problems in the same way that a human brain would. However, over time, attention moved to performing specific tasks, leading to deviations from biology. Artificial neural networks have been used on a variety of tasks, including computer vision, speech recognition, machine translation, social network filtering, playing board and video games and medical diagnosis.\n\nDeep learning consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are computer vision and speech recognition.\n\nSupport vector machines (SVMs), also known as support vector networks, are a set of related supervised learning methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other. An SVM training algorithm is a non-probabilistic, binary, linear classifier, although methods such as Platt scaling exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.\n\nA Bayesian network, belief network or directed acyclic graphical model is a probabilistic graphical model that represents a set of random variables and their conditional independence with a directed acyclic graph (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform inference and learning. Bayesian networks that model sequences of variables, like speech signals or protein sequences, are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.\n\nA genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of natural selection, using methods such as mutation and crossover to generate new genotypes in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s. Conversely, machine learning techniques have been used to improve the performance of genetic and evolutionary algorithms.\n\nApplications for machine learning include:\n\nIn 2006, the online movie company Netflix held the first \"Netflix Prize\" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (\"everything is a recommendation\") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.\n\nAlthough machine learning has been transformative in some fields, effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, many machine-learning programs often fail to deliver the expected value. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems. Machine learning is also dependent on the availability of large, high-quality datasets.\n\nIn 2018, a self-driving car from Uber failed to detect a pedestrian, who got killed in the accident. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of investment. \n\nMachine learning approaches in particular can suffer from different data biases. In healthcare data, measurement errors can often result in bias of machine learning applications. A machine learning system trained on your current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society. Language models learned from data have been shown to contain human-like biases. Machine learning systems used for criminal risk assessment have been found to be biased against black people. In 2015, Google photos would often tag black people as gorillas, and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorilla from the training data, and thus was not able to recognize real gorillas at all. Similar issues with recognizing non-white people have been found in many other systems. In 2016, Microsoft tested a chatbot that learned from Twitter, and it quickly picked up racist and sexist language. Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.\n\nClassification machine learning models can be validated by accuracy estimation techniques like the Holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-cross-validation method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.\n\nIn addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the False Positive Rate (FPR) as well as the False Negative Rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The Total Operating Characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used Receiver Operating Characteristic (ROC) and ROC's associated Area Under the Curve (AUC).\n\nMachine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.\n\nBecause language contains biases, machines trained on language \"corpora\" will necessarily also learn bias.\n\nOther forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest, but as income generating machines. This is especially true in the United States where there is a perpetual ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes in. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these \"greed\" biases are addressed.\n\nSoftware suites containing a variety of machine learning algorithms include the following :\n\n\n\n"}
{"id": "1575127", "url": "https://en.wikipedia.org/wiki?curid=1575127", "title": "Mimesis (mathematics)", "text": "Mimesis (mathematics)\n\nIn mathematics, mimesis is the quality of a numerical method which imitates some properties of the continuum problem. The goal of numerical analysis is to approximate the continuum, so instead of solving a partial differential equation one aims to solve a discrete version of the continuum problem. Properties of the continuum problem commonly imitated by numerical methods are conservation laws, solution symmetries, and fundamental identities and theorems of vector and tensor calculus like the divergence theorem.\nBoth finite difference or finite element method can be mimetic; it depends on the properties that the method has.\n\nFor example, a mixed finite element method applied to Darcy flows strictly conserves the mass of the flowing fluid. \n\nThe term \"geometric integration\" denotes the same philosophy.\n"}
{"id": "43510390", "url": "https://en.wikipedia.org/wiki?curid=43510390", "title": "Murnaghan–Nakayama rule", "text": "Murnaghan–Nakayama rule\n\nIn group theory, a branch of mathematics, the Murnaghan–Nakayama rule is a combinatorial method to compute irreducible character values of a symmetric group.\nThere are several generalizations of this rule beyond the representation theory of symmetric groups, but they are not covered here.\n\nThe irreducible characters of a group are of interest to mathematicians because they concisely summarize important information about the group, such as the dimensions of the vector spaces in which the elements of the group can be represented by linear transformations that “mix” all the dimensions. For many groups, calculating irreducible character values is very difficult; the existence of simple formulas is the exception rather than the rule.\n\nThe Murnaghan–Nakayama rule is a combinatorial rule for computing symmetric group character values χ using a particular kind of Young tableaux.\nHere λ and ρ are both integer partitions of some integer \"n\", the order of the symmetric group under consideration. The partition λ specifies the irreducible character, while the partition ρ specifies the conjugacy class on whose group elements the character is evaluated to produce the character value. The partitions are represented as weakly decreasing tuples; for example, two of the partitions of 8 are (5,2,1) and (3,3,1,1).\n\nThere are two versions of the Murnaghan-Nakayama rule, one non-recursive and one recursive.\n\nTheorem: \nwhere the sum is taken over the set BST(λ,ρ) of all \"border-strip\" tableaux of shape λ and type ρ.\nThat is, each tableau \"T\" is a tableau such that\n\nThe \"height\", \"ht\"(T), is the sum of the heights of the border strips in \"T\". The height of a border strip is one less than the number of \nrows it touches.\n\nIt follows from this theorem that the character values of a symmetric group are integers.\n\nFor some combinations of λ and ρ, there are no border-strip tableaux. In this case, there are no terms in the sum and therefore the character value is zero.\n\nConsider the calculation of one of the character values for the symmetric group of order 8, when λ is the partition (5,2,1) and ρ is the partition (3,3,1,1). The shape partition λ specifies that the tableau must have three rows, the first having 5 boxes, the second having 2 boxes, and the third having 1 box. The type partition ρ specifies that the tableau must be filled with three 1's, three 2's, one 3, and one 4. There are six such border-strip tableaux:\n\nIf we call these formula_2, formula_3, formula_4, formula_5, formula_6, and formula_7, then their heights are\n\nformula_8\n\nand the character value is therefore\n\nformula_9\n\nTheorem: \nwhere the sum is taken over the set BS(λ,ρ) of border strips within the Young diagram of shape λ that have ρ boxes and whose removal leaves a valid Young diagram. The notation formula_11 represents the partition that results from removing the border strip ξ from λ. The notation formula_12 represents the partition that results from removing the first element ρ from ρ.\n\nNote that the right-hand side is a sum of characters for symmetric groups that have smaller order than that of the symmetric group we started with on the left-hand side. In other words, this version of the Murnaghan-Nakayama rule expresses a character of the symmetric group S in terms of the characters of smaller symmetric groups S with \"k\"<\"n\".\n\nApplying this rule recursively will result in a tree of character value evaluations for smaller and smaller partitions. Each branch stops for one of two reasons: Either there are no border strips of the required length within the reduced shape, so the sum on the right is zero, or a border strip occupying the entire reduced shape is removed, leaving a Young diagram with no boxes. At this point we are evaluating χ when both λ and ρ are the empty partition (), and the rule requires that this terminal case be defined as having character formula_13.\n\nThis recursive version of the Murnaghan-Nakayama rule is especially efficient for computer calculation when one computes character tables for S for increasing values of \"k\" and stores all of the previously computed character tables.\n\nWe will again compute the character value with λ=(5,2,1) and ρ=(3,3,1,1).\n\nTo begin, consider the Young diagram with shape λ. Since the first part of ρ is 3, look for border strips that consist of 3 boxes. There are two possibilities:\n\nIn the first diagram, the border strip has height 0, and removing it produces the reduced shape (2,2,1). In the second diagram, the border strip has height 1, and removing it produces the reduced shape (5). Therefore, one has\n\nformula_14,\n\nexpressing a character value of S in terms of two character values of S.\n\nApplying the rule again to both terms, one finds\n\nformula_15\n\nand\n\nformula_16,\n\nreducing to a character value of S.\n\nApplying again, one finds\n\nformula_17,\n\nreducing to the only character value of S.\n\nA final application produces the terminal character formula_13:\n\nformula_19\n\nWorking backwards from this known character, the result is formula_20, as before.\n"}
{"id": "2777780", "url": "https://en.wikipedia.org/wiki?curid=2777780", "title": "Optimal maintenance", "text": "Optimal maintenance\n\nOptimal maintenance is the discipline within operations research concerned with maintaining a system in a manner that maximizes profit or minimizes cost. Cost functions depending on the reliability, availability and maintainability characteristics of the system of interest determine the parameters to minimize. Parameters often considered are the cost of failure, the cost per time unit of \"downtime\" (for example: revenue losses), the cost (per time unit) of corrective maintenance, the cost per time unit of preventive maintenance and the cost of repairable system replacement [Cassady and Pohl]. The foundation of any maintenance model relies on the correct description of the underlying deterioration process and failure behavior of the component, and on the relationships between maintained components in the product breakdown (system / sub-system / assembly / sub-assembly...).\n\nOptimal Maintenance strategies are often constructed using stochastic models and focus on finding an optimal inspection time or the optimal acceptable degree of system degradation before maintenance and/or replacement. Cost considerations on an Asset scale may also lead to select a \"run-to-failure\" approach for specific components.\n\nThere are four main survey papers available accomplished to cover the spectrum of optimal maintenance:\n\n"}
{"id": "30505908", "url": "https://en.wikipedia.org/wiki?curid=30505908", "title": "Orthogonal symmetric Lie algebra", "text": "Orthogonal symmetric Lie algebra\n\nIn mathematics, an orthogonal symmetric Lie algebra is a pair formula_1 consisting of a real Lie algebra formula_2 and an automorphism formula_3 of formula_2 of order formula_5 such that the eigenspace formula_6 of \"s\" corrsponding to 1 (i.e., the set formula_6 of fixed points) is a compact subalgebra. If \"compactness\" is omitted, it is called a symmetric Lie algebra. An orthogonal symmetric Lie algebra is said to be \"effective\" if formula_6 intersects the center of formula_2 trivially. In practice, effectiveness is often assumed; we do this in this article as well.\n\nThe canonical example is the Lie algebra of a symmetric space, formula_3 being the differential of a symmetry.\n\nEvery orthogonal symmetric Lie algebra decomposes into a direct sum of ideals \"of compact type\", \"of noncompact type\" and \"of Euclidean type\".\n\n"}
{"id": "56281176", "url": "https://en.wikipedia.org/wiki?curid=56281176", "title": "Pfaffian orientation", "text": "Pfaffian orientation\n\nIn graph theory, a Pfaffian orientation of an undirected graph\nformula_1 is an orientation (an assignment of a direction to each edge of the graph) in which every even central cycle is oddly oriented. In this definition, a cycle formula_2 is even if it contains an even number of edges. formula_2 is central if the subgraph of formula_1 formed by removing all the vertices of formula_2 has a perfect matching; central cycles are also sometimes called alternating circuits. And formula_2 is oddly oriented if each of the two orientations of formula_2 is consistent with an odd number of edges in the orientation.\n\nPfaffian orientations have been studied in connection with the FKT algorithm for counting the number of perfect matchings in a given graph. In this algorithm, the orientations of the edges are used to assign the values formula_8 to the variables in the Tutte matrix of the graph. Then, the Pfaffian of this matrix (the square root of its determinant) gives the number of perfect matchings. Each perfect matching contributes formula_8 to the Pfaffian regardless of which orientation is used; the choice of a Pfaffian orientation ensures that these contributions all have the same sign as each other, so that none of them cancel.\nThis result stands in contrast to the much higher computational complexity of counting matchings in arbitrary graphs.\n\nA graph is said to be Pfaffian if it has a Pfaffian orientation.\nEvery planar graph is Pfaffian.\nAn orientation in which each face of a planar graph has an odd number of clockwise-oriented edges is automatically Pfaffian. Such an orientation can be found by starting with an arbitrary orientation of a spanning tree of the graph.\nThe remaining edges, not in this tree, form a spanning tree of the dual graph, and their orientations can be chosen according to a bottom-up traversal of the dual spanning tree in order to ensure that each face of the original graph has an odd number of clockwise edges. More generally, every formula_10-minor-free graph has a Pfaffian orientation. These are the graphs that do not have the utility graph formula_10 (which is not Pfaffian) as a graph minor. By Wagner's theorem, the formula_10-minor-free graphs are formed by gluing together copies of planar graphs and the complete graph formula_13 along shared edges. The same gluing structure can be used to obtain a Pfaffian orientation for these graphs.\n\nAlong with formula_10, there are infinitely many minimal non-Pfaffian graphs. For bipartite graphs, it is possible to determine whether a Pfaffian orientation exists, and if so find one, in polynomial time.\n"}
{"id": "503378", "url": "https://en.wikipedia.org/wiki?curid=503378", "title": "Radon transform", "text": "Radon transform\n\nIn mathematics, the Radon transform is the integral transform which takes a function \"f\" defined on the plane to a function \"Rf\" defined on the (two-dimensional) space of lines in the plane, whose value at a particular line is equal to the line integral of the function over that line. The transform was introduced in 1917 by Johann Radon, who also provided a formula for the inverse transform. Radon further included formulas for the transform in three dimensions, in which the integral is taken over planes (integrating over lines is known as the X-ray transform). It was later generalized to higher-dimensional Euclidean spaces, and more broadly in the context of integral geometry. The complex analog of the Radon transform is known as the Penrose transform. The Radon transform is widely applicable to tomography, the creation of an image from the projection data associated with cross-sectional scans of an object.\nIf a function formula_1 represents an unknown density, then the Radon transform represents the projection data obtained as the output of a tomographic scan. Hence the inverse of the Radon transform can be used to reconstruct the original density from the projection data, and thus it forms the mathematical underpinning for tomographic reconstruction, also known as iterative reconstruction.\n\nThe Radon transform data is often called a sinogram because the Radon transform of an off-center point source is a sinusoid. Consequently, the Radon transform of a number of small objects appears graphically as a number of blurred sine waves with different amplitudes and phases.\n\nThe Radon transform is useful in computed axial tomography (CAT scan), barcode scanners, electron microscopy of macromolecular assemblies like viruses and protein complexes, reflection seismology and in the solution of hyperbolic partial differential equations.\n\nLet \"ƒ\"(x) = \"ƒ\"(\"x\",\"y\") be a compactly supported continuous function on R. The Radon transform, \"Rƒ\", is a function defined on the space of straight lines \"L\" in R by the line integral along each such line:\nConcretely, the parametrization of any straight line \"L\" with respect to arc length \"z\" can always be written\nwhere \"s\" is the distance of \"L\" from the origin and formula_4 is the angle the normal vector to \"L\" makes with the \"x\" axis. It follows that the quantities (α,\"s\") can be considered as coordinates on the space of all lines in R, and the Radon transform can be expressed in these coordinates by\n\nMore generally, in the \"n\"-dimensional Euclidean space R, the Radon transform of a compactly supported continuous function \"ƒ\" is a function \"Rƒ\" on the space Σ of all hyperplanes in R. It is defined by\nfor ξ ∈Σ, where the integral is taken with respect to the natural hypersurface measure, \"d\"σ (generalizing the |\"d\"x| term from the 2-dimensional case). Observe that any element of Σ is characterized as the solution locus of an equation\nwhere α ∈ \"S\" is a unit vector and \"s\" ∈ R. Thus the \"n\"-dimensional Radon transform may be rewritten as a function on \"S\"×R via\n\nIt is also possible to generalize the Radon transform still further by integrating instead over \"k\"-dimensional affine subspaces of R. The X-ray transform is the most widely used special case of this construction, and is obtained by integrating over straight lines.\n\nThe Radon transform is closely related to the Fourier transform. We define the one variable Fourier transform here as\n\nand for a function of a 2-vector formula_10,\n\nFor convenience, denote formula_12. The Fourier slice theorem then states\n\nwhere\n\nThus the two-dimensional Fourier transform of the initial function along a line at the inclination angle formula_4 is the one variable Fourier transform of the Radon transform (acquired at angle formula_4) of that function. This fact can be used to compute both the Radon transform and its inverse.\n\nThe result can be generalized into \"n\" dimensions \n\nThe dual Radon transform is a kind of adjoint to the Radon transform. Beginning with a function \"g\" on the space Σ, the dual Radon transform is the function formula_18 on R defined by\nThe integral here is taken over the set of all hyperplanes incident with the point \"x\" ∈ R, and the measure \"d\"μ is the unique probability measure on the set formula_20 invariant under rotations about the point \"x\".\n\nConcretely, for the two-dimensional Radon transform, the dual transform is given by\nIn the context of image processing, the dual transform is commonly called \"backprojection\" as it takes a function defined on each line in the plane and 'smears' or projects it back over the line to produce an image.\n\nLet Δ denote the Laplacian on R: \nThis is a natural rotationally invariant second-order differential operator. On Σ, the \"radial\" second derivative\nis also rotationally invariant. The Radon transform and its dual are intertwining operators for these two differential operators in the sense that\n\nThe process of \"reconstruction\" produces the image (or function formula_1 in the previous section) from its projection data. \"Reconstruction\" is an inverse problem.\n\nIn the 2D case, the most commonly used analytical formula to recover formula_1 from its Radon transform is the \"Filtered Backprojection Formula\" or \"Radon Inversion Formula\":\n\nwhere formula_28 is such that formula_29.\n\nThe convolution kernel formula_28 is referred to as Ramp filter in some literature.\n\nIntuitively, in the \"filtered backprojection\" formula, by analogy with differentiation, for which formula_31, we see that the filter performs an operation similar to a derivative. Roughly speaking, then, the filter makes objects \"more\" singular.\n\nA quantitive statement of the ill-posedness of Radon Inversion goes as follows:\n\nWe have formula_32\n\nwhere formula_33 is the previously defined adjoint to the Radon Transform.\n\nThus for formula_34, \n\nThe complex exponential formula_36 is thus an eigenfunction of formula_37 with eigenvalue formula_38. Thus the singular values of formula_39 are formula_40. Since these singular values tend to 0, formula_41 is unbounded.\n\nCompared with the \"Filtered Backprojection\" method, iterative reconstruction costs large computation time, limiting its practical use. However, due to the ill-posedness of Radon Inversion, the \"Filtered Backprojection\" method may be infeasible in the presence of discontinuity or noise. Iterative reconstruction methods (\"e.g.\", iterative Sparse Asymptotic Minimum Variance) could provide metal artifact reduction, noise and dose reduction for the reconstructed result that attract much research interest around the world.\n\nExplicit and computationally efficient inversion formulas for the Radon transform and its dual are available. The Radon transform in \"n\" dimensions can be inverted by the formula\nwhere \nand the power of the Laplacian (−Δ) is defined as a pseudodifferential operator if necessary by the Fourier transform\n\nFor computational purposes, the power of the Laplacian is commuted with the dual transform \"R\" to give\nwhere \"H\" is the Hilbert transform with respect to the \"s\" variable. In two dimensions, the operator \"H\"\"d\"/\"ds\" appears in image processing as a ramp filter.\nOne can prove directly from the Fourier slice theorem and change of variables for integration that for a compactly supported continuous function ƒ of two variables\nThus in an image processing context the original image ƒ can be recovered from the 'sinogram' data \"R\"ƒ by applying a ramp filter (in the formula_47 variable) and then back-projecting. As the filtering step can be performed efficiently (for example using digital signal processing techniques) and the back projection step is simply an accumulation of values in the pixels of the image, this results in a highly efficient, and hence widely used, algorithm.\n\nExplicitly, the inversion formula obtained by the latter method is\nif \"n\" is odd, and\nif \"n\" is even.\n\nThe dual transform can also be inverted by an analogous formula:\n\n\n\n"}
{"id": "18154982", "url": "https://en.wikipedia.org/wiki?curid=18154982", "title": "Ronald C. Read", "text": "Ronald C. Read\n\nRonald Cedric Read (Ron Read, born December 19, 1924) is a professor emeritus of mathematics at the University of Waterloo, Canada. He has published many books and papers, primarily on enumeration of graphs, graph isomorphism, chromatic polynomials, and particularly, the use of computers in graph-theoretical research. A majority of his later work was done in Waterloo.\nRead received his Ph.D. (1959) in graph theory from the University of London.\n\nRonald Read served in the Royal Navy during World War II, then completed a degree in mathematics at the University of Cambridge before joining The University of the West Indies in Jamaica as the second founding member of the Mathematics Department there. In 1970 he moved his family to Canada to take up a post as Professor of Mathematics at the University of Waterloo, Ontario, Canada.\n\nWhile in Jamaica he became interested in cave exploration, and in 1957 he founded the Jamaica Caving Club.\n\nHe has had a lifelong interest in the making of string figures and is the inventor of the .\n\nHe is an accomplished musician and plays many instruments including violin, viola, cello, double bass, piano, guitar, lute, and many early music instruments, some of which he has also built. He has diplomas in Theory and in Composition from the Royal Conservatory of Music in Toronto, Canada, and has composed four works for orchestra and several pieces for smaller groups.\n\n\n"}
{"id": "33101260", "url": "https://en.wikipedia.org/wiki?curid=33101260", "title": "Scott Flansburg", "text": "Scott Flansburg\n\nScott Flansburg (born December 28, 1963) is an American man who is often called a mental calculator. Dubbed multiple times as \"The Human Calculator\", he was entered into the \"Guinness Book of World Records\" for speed of mental calculation. He is the annual host and ambassador for World Maths Day, and is a math educator and media personality. Flansburg has also published the books \"Math Magic\" and \"Math Magic for Your Kids\".\n\nScott Flansburg was born on December 28, 1963, in Herkimer, New York. Scott has stated that he was nine years old when he first discovered his mental calculator abilities. He stated that he wasn't paying attention in math class and his teacher picked him to solve a math equation on the board. He said instead of going right to left in that addition sum, he went left to right and he was able to solve the question. Afterwards he would keep a running tally of his family's groceries at the store, so his father could give the cashier an exact check before the bill had been rung up. In his youth he also began noticing that the shape and number of angles in numbers are clues to their value, and began counting from 0 to 9 on his fingers instead of 1 to 10.\n\nFlansburg can add, subtract, multiply, divide, and find square and cube roots in his head almost instantly with calculator accuracy. Around 1990 he began using his ability in an entertainment and educational context. \n\nIn 1991, Flansburg was involved in the creation of a product called \"The Human Calculator System\" designed for direct-marketing sales channels which consisted of a study guide and four cassettes teaching his method. He appeared in the informercial series \"Amazing Discoveries\" hosting by Mike Levey in the Spring of that year to help sell his product. He is introduced as \"The Human Calculator\" at the beginning of the program, perhaps for the first time for national media markets.\n\nHe was dubbed \"The Human Calculator\" by Regis Philbin after appearing on \"Live with Regis and Kathy Lee\".\n\nThe \"Guinness Book of World Records\" listed him as \"Fastest Human Calculator\" in 2001 and 2003, after he broke the record for adding the same number to itself more times in 15 seconds than someone could do with a calculator. In 1999 Flansburg invented a 13-month calendar that uses zero as a day, month, and year alternative to the Gregorian calendar that he called \"The Human Calculator Calendar.\"\n\nIn 1998 he published the book \"Math Magic for Your Kids: Hundreds of Games and Exercises from the Human Calculator to Make Math Fun and Easy\" on Harper Paperbacks. A revised edition of his book \"Math Magic: How to Master Everyday Math Problems\" was published in 2004.\n\nSince about 1990 Flansburg has regularly given lectures and presentations at schools. He has appeared as a presenter at institutions such as NASA, IBM, The Smithsonian Institution, the National Council of Teachers of Mathematics, and the Mental Calculation World Cup. The latter has described Flansburg as \"more an auditory than a visual [mental] calculator.\"\n\nAccording to Flansburg, one of his personal missions is to use education to elevate mathematical confidence and self-esteem in adults and children, stating \"Why has it become so socially acceptable to be bad at math? If you were illiterate you wouldn’t say that on TV, but you can say that you are bad at math. We have to change the attitude.\" He is a proponent of students becoming comfortable with calculation methods instead of relying on table memorization. Flansburg is the annual host and ambassador for World Maths Day. He is also an official promoter of the American Math Challenge, a competition for students preparing for World Math Day.\n\nFlansburg has appeared on television shows such as \"The Oprah Winfrey Show\", \"The Ellen DeGeneres Show\", \"The Tonight Show with Jay Leno\", \"Larry King Live\". On April 26, 2009, while on the Japanese primetime show \"Asahi's Otona no Sonata\", he broke his own world record with 37 answers in 15 seconds. He was featured as The Human Calculator in the first episode of \"Stan Lee's Superhumans\", which aired on The History Channel on August 5, 2010. Part of the episode analyzed his brain activity. An fMRI scan while he was doing complex calculations revealed that his brain activity in the Brodmann area 44 region of the frontal cortex was absent. Instead there was activity somewhat higher from area 44 and closer to the motor cortex.\n\nIn January 2016, the TV show titled \"The Human Calculator\", hosted by Flansburg, premiered on H2.\n\nFlansburg resides in San Diego, California.\n\n"}
{"id": "1237823", "url": "https://en.wikipedia.org/wiki?curid=1237823", "title": "Significance arithmetic", "text": "Significance arithmetic\n\nSignificance arithmetic is a set of rules (sometimes called significant figure rules) for approximating the propagation of uncertainty in scientific or statistical calculations. These rules can be used to find the appropriate number of significant figures to use to represent the result of a calculation. If a calculation is done without analysis of the uncertainty involved, a result that is written with too many significant figures can be taken to imply a higher precision than is known, and a result that is written with too few significant figures results in an avoidable loss of precision. Understanding these rules requires a good understanding of the concept of significant and insignificant figures.\n\nThe rules of significance arithmetic are an approximation based on statistical rules for dealing with probability distributions. See the article on propagation of uncertainty for these more advanced and precise rules. Significance arithmetic rules rely on the assumption that the number of significant figures in the operands gives accurate information about the uncertainty of the operands and hence the uncertainty of the result. For an alternatives see interval arithmetic and floating point error mitigation.\n\nAn important caveat is that significant figures apply only to \"measured\" values. Values known to be exact should be ignored for determining the number of significant figures that belong in the result. Examples of such values include:\nPhysical constants such as Avogadro's number, however, have a limited number of significant digits, because these constants are known to us only by measurement. On the other hand, c (speed of light) is exactly 299,792,458 m/s by definition.\n\nWhen multiplying or dividing numbers, the result is rounded to the \"number\" of significant figures in the factor with the least significant figures. Here, the \"quantity\" of significant figures in each of the factors is important—not the \"position\" of the significant figures. For instance, using significance arithmetic rules:\n\n\nIf, in the above, the numbers are assumed to be measurements (and therefore probably inexact) then \"8\" above represents an inexact measurement with only one significant digit. Therefore, the result of \"8 × 8\" is rounded to a result with only one significant digit, i.e., \"6 × 10\" instead of the unrounded \"64\" that one might expect. In many cases, the rounded result is less accurate than the non-rounded result; a measurement of \"8\" has an actual underlying quantity between 7.5 and 8.5. The true square would be in the range between 56.25 and 72.25. So 6 × 10 is the best one can give, as other possible answers give a false sense of accuracy. Further, the 6 × 10 is itself confusing (as it might be considered to imply 60 ±5, which is over-optimistic; more accurate would be 64 ±8).\n\nWhen adding or subtracting using significant figures rules, results are rounded to the \"position\" of the least significant digit in the most uncertain of the numbers being summed (or subtracted). That is, the result is rounded to the last digit that is significant in \"each\" of the numbers being summed. Here the \"position\" of the significant figures is important, but the \"quantity\" of significant figures is irrelevant. Some examples using these rules:\n\n100 + 110 ≈ 200\n100. + 110. = 210.\n 1×10 + 1.1×10 ≈ 2×10\n 1.0×10 + 111 = 2.1×10\n 123.25 + 46.0 + 86.26 ≈ 255.5\n 100 - 1 ≈ 100\n\nTranscendental functions have a complicated method for determining the significance of the result. These include the logarithm function, the exponential function and the trigonometric functions. The significance of the result depends on the condition number. In general, the number of significant figures for the result is equal to the number of significant figures for the input minus the order of magnitude of the condition number.\n\nThe condition number of a differentiable function \"f\" at a point \"x\" is formula_1 see Condition number: One variable for details. Note that if a function has a zero at a point, its condition number at the point is infinite, as infinitesimal changes in the input can change the output from zero to non-zero, yielding a ratio with zero in the denominator, hence an infinite relative change. The condition number of the mostly used functions are as follows; these can be used to compute significant figures for all elementary functions:\n\n\nThe fact that the number of significant figures for the result is equal to the number of significant figures for the input minus the logarithm of the condition number can be easily derived from first principles. Let formula_18 and formula_19 be the true values and let formula_20 and formula_21 be approximate values with errors formula_22 and formula_23 respectively. Then\nwe have formula_24, formula_25, and \nformula_26\n\nThe significant figures of a number is related to the uncertain error of the number by formula_27. Substituting this into the above equation gives:\nformula_28\n\nformula_29\n\nformula_30\n\nBecause significance arithmetic involves rounding, it is useful to understand a specific rounding rule that is often used when doing scientific calculations: the round-to-even rule (also called \"banker's rounding\"). It is especially useful when dealing with large data sets.\n\nThis rule helps to eliminate the upwards skewing of data when using traditional rounding rules. Whereas traditional rounding always rounds up when the following digit is 5, bankers sometimes round down to eliminate this upwards bias.\n\n\"See the article on rounding for more information on rounding rules and a detailed explanation of the round-to-even rule.\"\n\nSignificant figures are used extensively in high school and undergraduate courses as a shorthand for the precision with which a measurement is known. However, significant figures are \"not\" a perfect representation of uncertainty, and are not meant to be. Instead, they are a useful tool for avoiding expressing more information than the experimenter actually knows, and for avoiding rounding numbers in such a way as to lose precision.\n\nFor example, here are some important differences between significant figure rules and uncertainty:\n\nIn order to explicitly express the uncertainty in any uncertain result, the uncertainty should be given separately, with an uncertainty interval, and a confidence interval. The expression 1.23 U95 = 0.06 implies that the true (unknowable) value of the variable is expected to lie in the interval from 1.17 to 1.29 with at least 95% confidence. If the confidence interval is not specified it has traditionally been assumed to be 95% corresponding to two standard deviations from the mean. Confidence intervals at one standard deviation (68%) and three standard deviations (99%) are also commonly used.\n\n\n\n"}
{"id": "48675877", "url": "https://en.wikipedia.org/wiki?curid=48675877", "title": "Social genome", "text": "Social genome\n\nThe social genome is the collection of data about members of a society that is captured in ever-larger and ever-more complex databases (e.g., government administrative data, operational data, social media data etc.). Some have used the term digital footprint to refer to individual traces.\n\nThere have been two distinct uses of the term. First, the word Social Genome was used in a letter to the editor submission to Science in response to a seminal article about using big data for social science by King. The letter was published, but the word social genome was edited out of the letter. The original submission states, “A well-integrated federated data system of administrative databases updated on an ongoing basis could hold a collective representation of our society, our social genome.” Kum and others continue to use the word since 2011, with it being defined in a peer reviewed article in 2013. It states “Today there is a constant flow of data into, out of, and between ever-larger and ever-more complex databases about people. Together, these digital traces collectively capture our social genome, the footprints of our society.” In 2014, a vision paper on population informatics was published which further elaborated on the term.\n\nSecond, separately at about the same time, a group of researchers led by the Brookings Institution started the Social Genome Project which built a data-rich model to map the pathway to the Middle class by tracing the life course from birth until middle age. The first paper was published in 2012.\n\n\n"}
{"id": "45235652", "url": "https://en.wikipedia.org/wiki?curid=45235652", "title": "Straight-line program", "text": "Straight-line program\n\nIn mathematics, more specifically in computational algebra, a straight-line program (SLP) for a finite group \"G\" = 〈\"S\"〉 is a finite sequence \"L\" of elements of \"G\" such that every element of \"L\" either belongs to \"S\", is the inverse of a preceding element, or the product of two preceding elements. An SLP \"L\" is said to \"compute\" a group element \"g\" ∈ \"G\" if \"g\" ∈ \"L\", where \"g\" is encoded by a word in \"S\" and its inverses.\n\nIntuitively, an SLP computing some \"g\" ∈ \"G\" is an \"efficient\" way of storing \"g\" as a group word over \"S\"; observe that if \"g\" is constructed in \"i\" steps, the word length of \"g\" may be exponential in \"i\", but the length of the corresponding SLP is linear in \"i\". This has important applications in computational group theory, by using SLPs to efficiently encode group elements as words over a given generating set.\n\nStraight-line programs were introduced by Babai and Szemerédi in 1984 as a tool for studying the computational complexity of certain matrix group properties. Babai and Szemerédi prove that every element of a finite group \"G\" has an SLP of length \"O\"(log|\"G\"|) in every generating set.\n\nAn efficient solution to the \"constructive membership problem\" is crucial to many group-theoretic algorithms. It can be stated in terms of SLPs as follows. Given a finite group \"G\" = 〈\"S\"〉 and \"g\" ∈ \"G\", find a straight-line program computing \"g\" over \"S\". The constructive membership problem is often studied in the setting of black box groups. The elements are encoded by bit strings of a fixed length. Three \"oracles\" are provided for the group-theoretic functions of multiplication, inversion, and checking for equality with the identity. A \"black box algorithm\" is one which uses only these oracles. Hence, straight-line programs for black box groups are black box algorithms.\n\nExplicit straight-line programs are given for a wealth of finite simple groups in the online ATLAS of Finite Groups.\n\nLet \"G\" be a finite group and let \"S\" be a subset of \"G\". A sequence \"L\" = (\"g\",…,\"g\") of elements of \"G\" is a \"straight-line program\" over \"S\" if each \"g\" can be obtained by one of the following three rules: \nThe straight-line \"cost\" \"c\"(\"g\"|\"S\") of an element \"g\" ∈ \"G\" is the length of a shortest straight-line program over \"S\" computing \"g\". The cost is infinite if \"g\" is not in the subgroup generated by \"S\".\n\nA straight-line program is similar to a derivation in predicate logic. The elements of \"S\" correspond to axioms and the group operations correspond to the rules of inference.\n\nLet \"G\" be a finite group and let \"S\" be a subset of \"G\". A \"straight-line program\" of length \"m\" over \"S\" computing some \"g\" ∈ \"G\" is a sequence of expressions (\"w\",…,\"w\") such that for each \"i\", \"w\" is a symbol for some element of \"S\", or \"w\" = (\"w\",-1) for some \"j\" < \"i\", or \"w\" = (\"w\",\"w\") for some \"j\",\"k\" < \"i\", such that \"w\" takes upon the value \"g\" when evaluated in \"G\" in the obvious manner.\n\nThe original definition appearing in requires that \"G\" =〈\"S\"〉. The definition presented above is a common generalisation of this.\n\nFrom a computational perspective, the formal definition of a straight-line program has some advantages. Firstly, a sequence of abstract expressions requires less memory than terms over the generating set. Secondly, it allows straight-line programs to be constructed in one representation of \"G\" and evaluated in another. This is an important feature of some algorithms.\n\nThe dihedral group D is the group of symmetries of a hexagon. It can be generated by a 60 degree rotation ρ and one reflection λ. The leftmost column of the following is a straight-line program for λρ:\n\n\nIn S, the group of permutations on six letters, we can take α=(1 2 3 4 5 6) and β=(1 2) as generators. The leftmost column here is an example of a straight-line program to compute (1 2 3)(4 5 6):\n\n\n\n\"Short descriptions of finite groups\". Straight-line programs can be used to study compression of finite groups via first-order logic. They provide a tool to construct \"short\" sentences describing \"G\" (i.e. much shorter than |\"G\"|). In more detail, SLPs are used to prove that every finite simple group has a first-order description of length \"O\"(log|\"G\"|), and every finite group \"G\" has a first-order description of length \"O\"(log|\"G\"|).\n\n\"Straight-line programs computing generating sets for maximal subgroups of finite simple groups\". The online ATLAS of Finite Group Representations provides abstract straight-line programs for computing generating sets of maximal subgroups for many finite simple groups.\n\nExample: The group Sz(32), belonging to the infinite family of Suzuki groups, has rank 2 via generators \"a\" and \"b\", where \"a\" has order 2, \"b\" has order 4, \"ab\" has order 5, \"ab\" has order 25 and \"abab\"\"ab\" has order 25. The following is a straight-line program that computes a generating set for a maximal subgroup Eformula_1Eformula_3C. This straight-line program can be found in the online ATLAS of Finite Group Representations.\n\n\nThe reachability theorem states that, given a finite group \"G\" generated by \"S\", each \"g\" ∈ \"G\" has a maximum cost of . This can be understood as a bound on how hard it is to generate a group element from the generators.\n\nHere the function lg(\"x\") is an integer-valued version of the logarithm function: for \"k\"≥1 let lg(\"k\") = max{\"r\" : 2 ≤ \"k\"}.\n\nThe idea of the proof is to construct a set \"Z\" = {\"z\",…,\"z\"} that will work as a new generating set (\"s\" will be defined during the process). It is usually larger than \"S\", but any element of \"G\" can be expressed as a word of length at most over \"Z\". The set \"Z\" is constructed by inductively defining an increasing sequence of sets \"K\"(\"i\").\n\nLet \"K\"(\"i\") = {\"z\"·\"z\"·…·\"z\" : \"α\" ∈ {0,1}}, where \"z\" is the group element added to \"Z\" at the \"i\"-th step. Let \"c\"(\"i\") denote the length of a shortest straight-line program that contains \"Z\"(\"i\") = {\"z\",…,\"z\"}. Let \"K\"(0) = {1} and \"c\"(0)=0. We define the set \"Z\" recursively:\n\nBy this process, \"Z\" is defined in a way so that any \"g\" ∈ \"G\" can be written as an element of \"K\"(\"i\")\"K\"(\"i\"), effectively making it easier to generate from \"Z\".\n\nWe now need to verify the following claim to ensure that the process terminates within lg(|\"G\"|) many steps:\n\nThe next claim is used to show that the cost of every group element is within the required bound.\n\nIt takes at most 2\"i\" steps to generate \"g\" ∈ \"K\"(\"i\")\"K\"(\"i\"). There is no point in generating the element of maximum length, since it is the identity. Hence steps suffice. To generate \"g\"·\"g\" ∈ \"G\"\\\"K\"(\"i\")\"K\"(\"i\"), 2\"i\" steps are sufficient.\n\nWe now finish the theorem. Since \"K\"(\"s\")\"K\"(\"s\") = \"G\", any \"g\" ∈ \"G\" can be written in the form \"k\"·\"k\" with \"k\",\"k\" ∈ \"K\"(\"s\"). By Corollary 2, we need at most steps to generate \"Z\"(\"s\") = \"Z\", and no more than steps to generate \"g\" from \"Z\"(\"s\").\n\nTherefore .\n"}
{"id": "3969942", "url": "https://en.wikipedia.org/wiki?curid=3969942", "title": "Tatyana Pavlovna Ehrenfest", "text": "Tatyana Pavlovna Ehrenfest\n\nTatyana Pavlovna Ehrenfest, later van Aardenne-Ehrenfest, (Vienna, October 28, 1905 – Dordrecht, November 29, 1984) was a Dutch mathematician. She was the daughter of Paul Ehrenfest (1880–1933) and Tatyana Alexeyevna Afanasyeva (1876–1964).\n\nTatyana Ehrenfest was born in Vienna, and spent her childhood in St Petersburg. In 1912 the Ehrenfests moved to Leiden where her father succeeded H.A. Lorentz as professor at the University of Leiden. Until 1917 she was home schooled, after that she attended the Gymnasium in Leiden and passed the final exams in 1922.\nShe studied mathematics and physics at the University of Leiden. In 1928 she went to Göttingen where she took courses from Harald Bohr and Max Born. On December 8, 1931 she obtained her Ph.D. in Leiden. After that, she was never employed and, in particular, never held any academic position.\n\nUnder her married name, Tanja van Aardenne-Ehrenfest, she is known for her contributions to De Bruijn sequences, the discrepancy theorem and the BEST theorem.\n\n"}
{"id": "1544998", "url": "https://en.wikipedia.org/wiki?curid=1544998", "title": "Van Wijngaarden grammar", "text": "Van Wijngaarden grammar\n\nIn computer science, a Van Wijngaarden grammar (also vW-grammar or W-grammar) is a two-level grammar which provides a technique to define potentially infinite context-free grammars in a finite number of rules. The formalism was invented by Adriaan van Wijngaarden to define rigorously some syntactic restrictions which previously had to be formulated in natural language, despite their essentially syntactical content. Typical applications are the treatment of gender and number in natural language syntax and the well-definedness of identifiers in programming languages.\n\nThe technique was used and developed in the definition of the programming language ALGOL 68. It is an example of the larger class of affix grammars.\n\nA W-grammar consists of a finite set of meta-rules, which are used to derive (possibly infinitely many) production rules from a finite set of hyper-rules. Meta-rules are restricted to those defined by a context-free grammar. Hyper-rules restrict the admissible contexts at the upper level. Essentially, the \"consistent substitution\" used in the derivation process is equivalent to unification, as in Prolog, as was noted by Alain Colmerauer.\n\nFor example, the assignment codice_1 is only valid if the variable x can contain an integer. Therefore, the context-free syntax codice_2 is incomplete. In a two-level grammar, this might be specified in a context-sensitive manner as codice_3. Then codice_4 could be a production rule but codice_5 is not a possible production rule. This also means that assigning with incompatible types becomes a syntax error which can be caught at compile-time. Similarly, \n\nallows codice_6 and codice_7 but not codice_8.\n\nPrior to ALGOL 68 the language ALGOL 60 was formalised using the context-free Backus–Naur form. The appearance of new context-sensitive two-level grammar presented a challenge to some readers of the 1968 ALGOL 68 \"Final Report\". Subsequently, the final report was revised by Wijngaarden and his colleagues and published as the 1973 ALGOL 68 \"Revised Report\".\n\nThe grammar for ALGOL 68 is officially in the two-level Van Wijngaarden grammar, but a subset has been done in the one-level Backus–Naur form, compare:\n\n a) program : open symbol, standard prelude,\n\n program : strong void new closed clause\n\n\"yo-yo\" parser for van Wijngaarden grammars with example grammars for \"expressions\", \"eva\", \"sal\" and Pascal (the actual ISO 7185 standard for Pascal uses extended Backus–Naur form).\n\nW-grammars are based on the idea of providing the nonterminal symbols of context-free grammars with \"attributes\" (or \"affixes\") that pass information between the nodes of the parse tree, used to constrain the syntax and to specify the semantics. This idea was well known at the time; e.g. Donald Knuth visited the ALGOL 68 design committee while developing his own version of it, attribute grammars. Quite peculiar to W-grammars was their strict treatment of attributes as strings, defined by a context-free grammar, on which concatenation is the only possible operation; in attribute grammars, attributes can be of any data type, and any kind of operation can be applied to them.\n\nAfter their introduction in the Algol 68 report, W-grammars were widely considered as too powerful and unconstrained to be practical. This was partly a consequence of the way in which they had been applied; the revised Algol 68 report contained a much more readable grammar, without modifying the W-grammar formalism itself.\n\nMeanwhile, it became clear that W-grammars are indeed too powerful.\nThey describe precisely all recursively enumerable languages, which makes parsing impossible in general: it is an undecidable problem to decide whether a given string can be generated by a given W-grammar. Their use must be seriously constrained when used for automatic parsing or translation. Restricted and modified variants of W-grammars were developed to address this, e.g.\n\nAnthony Fisher has written a parser for a large class of W-grammars.\n\nDick Grune created a C program that would generate all possible productions of a 2-level grammar.\n\nThe applications of EAGs mentioned above can effectively be regarded as applications of W-grammars, since EAGs are so close to W-grammars.\n\nW-grammars have also been proposed for the description of complex human actions in ergonomics.\n\n\n"}
{"id": "3114930", "url": "https://en.wikipedia.org/wiki?curid=3114930", "title": "Vector calculus identities", "text": "Vector calculus identities\n\nThe following identities are important in vector calculus:\n\nIn the three-dimensional Cartesian coordinate system, the gradient of some function formula_1 is given by:\n\nwhere i, j, k are the standard unit vectors.\n\nThe gradient of a tensor field, formula_3, of order \"n\", is generally written as\n\nand is a tensor field of order . In particular, if the tensor field has order 0 (i.e. a scalar), formula_5, the resulting gradient,\n\nis a vector field.\n\nIn three-dimensional Cartesian coordinates, the divergence of a continuously differentiable vector field formula_7 is defined as the scalar-valued function:\n\nThe divergence of a tensor field, formula_3, of non-zero order \"n\", is generally written as\n\nand is a contraction to a tensor field of order . Specifically, the divergence of a vector is a scalar. The divergence of a higher order tensor field may be found by decomposing the tensor field into a sum of outer products, thereby allowing the use of the identity,\n\nwhere formula_12 is the directional derivative in the direction of formula_13 multiplied by its magnitude. Specifically, for the outer product of two vectors,\n\nIn Cartesian coordinates, for formula_7:\n\nformula_16\n\nformula_17\n\nwhere i, j, and k are the unit vectors for the \"x\"-, \"y\"-, and \"z\"-axes, respectively.\nFor a 3-dimensional vector field formula_18, curl is also a 3-dimensional vector field, generally written as:\n\nor in Einstein notation as:\n\nwhere ε is the Levi-Civita symbol.\n\nIn Cartesian coordinates, the Laplacian of a function formula_1 is\n\nFor a tensor field, formula_23, the laplacian is generally written as:\n\nand is a tensor field of the same order.\n\nIn \"Feynman subscript notation\",\n\nwhere the notation ∇ means the subscripted gradient operates on only the factor B.\n\nA less general but similar idea is used in \"geometric algebra\" where the so-called Hestenes \"overdot notation\" is employed. The above identity is then expressed as:\n\nwhere overdots define the scope of the vector derivative. The dotted vector, in this case B, is differentiated, while the (undotted) A is held constant.\n\nFor the remainder of this article, Feynman subscript notation will be used where appropriate.\n\nFor scalar fields formula_5 and formula_28, vector fields formula_3 and formula_30, and cartesian functions formula_31 and formula_32:\n\nThe gradient of the product of two scalar fields formula_5 and formula_28 follows the same form as the product rule in single variable calculus.\n\nwhere denotes the Jacobian of . For more details, refer to these notes \n\nAs a special case, when ,\n\nThe curl of the gradient of \"any\" continuously twice-differentiable scalar field formula_54 is always the zero vector:\n\nThe divergence of the curl of \"any\" vector field A is always zero:\n\nThe Laplacian of a scalar field is the divergence of its gradient:\nThe result is a scalar quantity.\n\nHere,∇ is the vector Laplacian operating on the vector field A.\n\n\n\n\n\n\n\nBelow, the curly symbol ∂ means \"boundary of\".\n\nIn the following surface–volume integral theorems, \"V\" denotes a three-dimensional volume with a corresponding two-dimensional boundary \"S\" = ∂\"V\" (a closed surface):\n\n\nIn the following curve–surface integral theorems, \"S\" denotes a 2d open surface with a corresponding 1d boundary \"C\" = ∂\"S\" (a closed curve):\n\n\nIntegration around a closed curve in the clockwise sense is the negative of the same line integral in the counterclockwise sense (analogous to interchanging the limits in a definite integral):\n\n\n"}
{"id": "12529188", "url": "https://en.wikipedia.org/wiki?curid=12529188", "title": "Weak value", "text": "Weak value\n\nIn quantum mechanics (and computation), a weak value is a quantity related to a shift of a measuring device's pointer when usually there is pre- and postselection. It should not be confused with a weak measurement, which is often defined in conjunction. The weak value was first defined by Yakir Aharonov, David Albert and Lev Vaidman, published in Physical Review Letters 1988, and is related to the two-state vector formalism. There is also to way to obtain weak values without postselection.\n\nThere are many excellent review articles on weak values (see e.g. ) here we briefly cover the basics.\n\nWe will denote the initial state of a system as formula_1, while the final state of the system is denoted as formula_2. We will refer to the initial and final states of the system as the pre- and post-selected quantum mechanical states. With respect to these state the \"weak value\" of the observable formula_3 is defined as:\n\nformula_4\n\nNotice that if formula_5 then the weak value is equal to the usual expected value in the initial state formula_6 or the final state formula_7. In general the weak value quantity is a complex number. The weak value of the observable becomes large when the post-selected state, formula_2, approaches being orthogonal to the pre-selected state, formula_1, i.e. formula_10. If formula_11 is larger than the largest eigenvalue of formula_3 or smaller than the smallest eigenvalue of formula_3 the weak value is said to be anomalous.\n\nAs an example consider a spin 1/2 particle. Take formula_3 to be the Pauli Z operator formula_15 with eigenvalues formula_16. Using the initial state\n\nformula_17\n\nand the final state\n\nformula_18\n\nwe can calculate the weak value to be\n\nformula_19.\n\nFor formula_20 the weak value is anomalous.\n\nHere we follow the presentation given by Duck, Stevenson, and Sudarshan, (with some notational updates from Kofman et al. )which makes explicit when the approximations used to derive the weak value are valid.\n\nConsider a quantum system that you want to measure by coupling an ancillary (also quantum) measuring device. The observable to be measured on the system is formula_21. The system and ancilla are coupled via the Hamiltonian\nformula_22 where the coupling constant is integrated over an interaction time\nformula_23 and formula_24 is the canonical commutator. The Hamiltonian generates the unitary\n\nformula_25\n\nTake the initial state of the ancilla to have a Gaussian distribution\n\nformula_26\n\nthe position wavefunction of this state is\n\nformula_27\n\nThe initial state of the system is given by formula_28 above; the state formula_29, jointly describing the initial state of the system and ancilla, is given then by:\n\nformula_30\n\nNext the system and ancilla interact via the unitary formula_31. After this one performs a projective measurement of the projectors formula_32 on the system. If we postselect (or condition) on getting the outcome formula_33, then the (unnormalized) final state of the meter is\n\nformula_34\n\nTo arrive at this conclusion, we use the first order series expansion of formula_35 on line (I), and we require that\n\nformula_36\n\nOn line (II) we use the approximation that formula_37 for small formula_38. This final approximation is only valid when\n\nformula_39\n\nAs formula_40 is the generator of translations, the ancilla's wavefunction is now given by\n\nformula_41\n\nThis is the original wavefunction, shifted by an amount formula_42. By Busch's theorem the system and meter wavefunctions are necessarily disturbed by the measurement. There is a certain sense in which the protocol that allows one to measure the weak value is minimally disturbing, but there is still disturbance.\n\nAt the end of the original weak value paper the authors suggested weak values could be used in quantum metrology:\nThis suggestion was followed by Hosten and Kwiat and later by Dixon et al. It appears to be an interesting line of research that could result in improved quantum sensing technology.\n\nAdditionally in 2011, weak measurements of many photons prepared in the same pure state, followed by strong measurements of a complementary variable, were used to perform quantum tomography (i.e. reconstruct the state in which the photons were prepared).\n\nWeak values have been used to examine some of the paradoxes in the foundations of quantum theory. For example, the research group of Aephraim Steinberg at the University of Toronto confirmed Hardy's paradox experimentally using joint weak measurement of the locations of entangled pairs of photons. (also see)\n\nBuilding on weak measurements, Howard M. Wiseman proposed a weak value measurement of the velocity of a quantum particle at a precise position, which he termed its \"naïvely observable velocity\". In 2010, a first experimental observation of trajectories of a photon in a double-slit interferometer was reported, which displayed the qualitative features predicted in 2001 by Partha Ghose for photons in the de Broglie-Bohm interpretation.\n\nCriticisms of weak values include philosophical and practical criticisms. Some noted researchers such as Asher Peres, Tony Leggett, David Mermin, and Charles H. Bennett are critical of weak values also:\n\n\n"}
{"id": "5397705", "url": "https://en.wikipedia.org/wiki?curid=5397705", "title": "Wolfgang Heinrich Johannes Fuchs", "text": "Wolfgang Heinrich Johannes Fuchs\n\nWolfgang Heinrich Johannes Fuchs (May 19, 1915, Munich – February 24, 1997) was a mathematician specializing in complex analysis. His main area of research was Nevanlinna theory.\n\nFuchs received his Ph.D. in 1941 from the University of Cambridge, under the direction of Albert Ingham. He joined the faculty of Cornell University in 1950 and spent the rest of his career there.\n\n"}
{"id": "19382017", "url": "https://en.wikipedia.org/wiki?curid=19382017", "title": "Zig-zag lemma", "text": "Zig-zag lemma\n\nIn mathematics, particularly homological algebra, the zig-zag lemma asserts the existence of a particular long exact sequence in the homology groups of certain chain complexes. The result is valid in every abelian category.\n\nIn an abelian category (such as the category of abelian groups or the category of vector spaces over a given field), let formula_1 and formula_2 be chain complexes that fit into the following short exact sequence:\n\nSuch a sequence is shorthand for the following commutative diagram:\n\nwhere the rows are exact sequences and each column is a chain complex. \n\nThe zig-zag lemma asserts that there is a collection of boundary maps\n\nthat makes the following sequence exact:\n\nThe maps formula_5 and formula_6 are the usual maps induced by homology. The boundary maps formula_7 are explained below. The name of the lemma arises from the \"zig-zag\" behavior of the maps in the sequence. In an unfortunate overlap in terminology, this theorem is also commonly known as the \"snake lemma,\" although there is another result in homological algebra with that name. The other snake lemma can be used to prove the zig-zag lemma, in a manner different from what is described below.\n\nThe maps formula_7 are defined using a standard diagram chasing argument. Let formula_9 represent a class in formula_10, so formula_11. Exactness of the row implies that formula_12 is surjective, so there must be some formula_13 with formula_14. By commutativity of the diagram, \n\nBy exactness, \n\nThus, since formula_17 is injective, there is a unique element formula_18 such that formula_19. This is a cycle, since formula_20 is injective and\n\nsince formula_22. That is, formula_23. This means formula_24 is a cycle, so it represents a class in formula_25. We can now define\n\nWith the boundary maps defined, one can show that they are well-defined (that is, independent of the choices of \"c\" and \"b\"). The proof uses diagram chasing arguments similar to that above. Such arguments are also used to show that the sequence in homology is exact at each group.\n\n\n"}
