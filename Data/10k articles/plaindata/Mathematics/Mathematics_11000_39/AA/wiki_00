{"id": "308628", "url": "https://en.wikipedia.org/wiki?curid=308628", "title": "50 (number)", "text": "50 (number)\n\n50 (fifty) is the natural number following 49 and preceding 51.\n\nFifty is the smallest number that is the sum of two non-zero square numbers in two distinct ways: 50 = 1 + 7 = 5 + 5. It is also the sum of three squares, 50 = 3 + 4 + 5, and the sum of four squares, 50 = 6 + 3 + 2 + 1. It is a Harshad number.\n\nThere is no solution to the equation φ(\"x\") = 50, making 50 a nontotient. Nor is there a solution to the equation \"x\" − φ(\"x\") = 50, making 50 a noncototient.\n\n\n\n\nFifty is:\n"}
{"id": "58701508", "url": "https://en.wikipedia.org/wiki?curid=58701508", "title": "Anne M. Leggett", "text": "Anne M. Leggett\n\nAnne Marie Leggett is an American mathematical logician. She is an associate professor emerita of mathematics at Loyola University Chicago.\n\nLeggett is the editor-in-chief of the newsletter of the Association for Women in Mathematics. With Bettye Anne Case, she is the editor of the book \"Complexities: Women in Mathematics\" (with Anne M. Leggett, Princeton University Press, 2005).\n\nLeggett did her undergraduate studies at Ohio State University, and completed her Ph.D. in 1973 at Yale University. Her dissertation, \"Maximal formula_1-r.e. sets and their complements\", was supervised by Manuel Lerman.\n\nShe became a C. L. E. Moore instructor at the Massachusetts Institute of Technology in 1973, and was also on the faculties of Western Illinois University and the University of Texas at Austin. In 1982, she married another mathematician, Gerard McDonald (1946–2012), and in 1983, they both joined the Loyola Chicago faculty.\n\nLeggett was chosen to be part of the 2019 class of fellows of the Association for Women in Mathematics, \"for extraordinary contributions in promoting opportunities for women in the mathematical sciences through AWM and as a teacher and scholar; for her amazing and steady work as editor of the AWM Newsletter since 1977; and for her invaluable leadership and guidance.\"\n"}
{"id": "6965651", "url": "https://en.wikipedia.org/wiki?curid=6965651", "title": "Arthur F. Griffith", "text": "Arthur F. Griffith\n\nArthur Frederick Griffith (30 July 1880 – 25 December 1911) was a calculating prodigy born July 30, 1880 in Milford, Kosciusko County, Indiana. He could count to 40,000 by age five. An illness at age seven resulted in epilepsy and prevented him from attending school until age 10. At age 12, he began to develop calculating short cuts. He dropped out of school at age 17, but continued to study numbers and practice mental calculation on his own.\n\nAt age 19, he met Dr. Ernest Hiram Lindley, who invited Arthur to Indiana University to be studied in the psychology laboratory which had been established in 1892 by Dr. William Lowe Bryan. Bryan and Lindley took Arthur to the American Psychological Association meeting at Yale in December 1899, where he exhibited his skills and the I.U. professors presented a paper about their research of Arthur's case. William Lowe Bryan, in 1900, presented a similar paper at the meeting of the International Congress of Psychology in Paris.\n\nArthur Griffith left Indiana University after five months, wrote a book of his methods, entitled \"The Easy and Speedy Reckoner\", and toured the vaudeville circuit until his death, allegedly of a stroke, in a Springfield, Massachusetts hotel room at age 31 on Christmas Day, 1911.\n"}
{"id": "4286", "url": "https://en.wikipedia.org/wiki?curid=4286", "title": "Biconditional introduction", "text": "Biconditional introduction\n\nIn propositional logic, biconditional introduction is a valid rule of inference. It allows for one to infer a biconditional from two conditional statements. The rule makes it possible to introduce a biconditional statement into a logical proof. If formula_1 is true, and if formula_2 is true, then one may infer that formula_3 is true. For example, from the statements \"if I'm breathing, then I'm alive\" and \"if I'm alive, then I'm breathing\", it can be inferred that \"I'm breathing if and only if I'm alive\". Biconditional introduction is the converse of biconditional elimination. The rule can be stated formally as:\n\nwhere the rule is that wherever instances of \"formula_1\" and \"formula_2\" appear on lines of a proof, \"formula_3\" can validly be placed on a subsequent line.\n\nThe \"biconditional introduction\" rule may be written in sequent notation:\n\nwhere formula_9 is a metalogical symbol meaning that formula_3 is a syntactic consequence when formula_1 and formula_2 are both in a proof;\n\nor as the statement of a truth-functional tautology or theorem of propositional logic:\n\nwhere formula_14, and formula_15 are propositions expressed in some formal system.\n"}
{"id": "3535789", "url": "https://en.wikipedia.org/wiki?curid=3535789", "title": "Binet–Cauchy identity", "text": "Binet–Cauchy identity\n\nIn algebra, the Binet–Cauchy identity, named after Jacques Philippe Marie Binet and Augustin-Louis Cauchy, states that\n\nfor every choice of real or complex numbers (or more generally, elements of a commutative ring).\nSetting \"a\" = \"c\" and \"b\" = \"d\", it gives the Lagrange's identity, which is a stronger version of the Cauchy–Schwarz inequality for the Euclidean space formula_2.\n\nWhen , the first and second terms on the right hand side become the squared magnitudes of dot and cross products respectively; in dimensions these become the magnitudes of the dot and wedge products. We may write it\n\nwhere , , , and are vectors. It may also be written as a formula giving the dot product of two wedge products, as \n\nwhich can be written as \nin the case.\n\nIn the special case and , the formula yields \n\nWhen both and are unit vectors, we obtain the usual relation\nwhere is the angle between the vectors.\n\nA relationship between the Levi–Cevita symbols and the generalized Kronecker delta is\n\nThe formula_9 form of the Binet–Cauchy identity can be written as \n\nExpanding the last term,\n\nwhere the second and fourth terms are the same and artificially added to complete the sums as follows:\n\nThis completes the proof after factoring out the terms indexed by \"i\".\n\nA general form, also known as the Cauchy–Binet formula, states the following:\nSuppose \"A\" is an \"m\"×\"n\" matrix and \"B\" is an \"n\"×\"m\" matrix. If \"S\" is a subset of {1, ..., \"n\"} with \"m\" elements, we write \"A\" for the \"m\"×\"m\" matrix whose columns are those columns of \"A\" that have indices from \"S\". Similarly, we write \"B\" for the \"m\"×\"m\" matrix whose \"rows\" are those rows of \"B\" that have indices from \"S\". \nThen the determinant of the matrix product of \"A\" and \"B\" satisfies the identity\nwhere the sum extends over all possible subsets \"S\" of {1, ..., \"n\"} with \"m\" elements.\n\nWe get the original identity as special case by setting\n"}
{"id": "981855", "url": "https://en.wikipedia.org/wiki?curid=981855", "title": "Blaschke product", "text": "Blaschke product\n\nIn complex analysis, the Blaschke product is a bounded analytic function in the open unit disc constructed to have zeros at a (finite or infinite) sequence of prescribed complex numbers\n\ninside the unit disc.\nBlaschke products were introduced by . They are related to Hardy spaces.\n\nA sequence of points formula_1 inside the unit disk is said to satisfy the Blaschke condition when\n\nGiven a sequence obeying the Blaschke condition, the Blaschke product is defined as\n\nwith factors\n\nprovided \"a\" ≠ 0. Here formula_5 is the complex conjugate of \"a\". When \"a\" = 0 take \"B\"(\"0\",\"z\") = \"z\".\n\nThe Blaschke product \"B\"(\"z\") defines a function analytic in the open unit disc, and zero exactly at the \"a\" (with multiplicity counted): furthermore it is in the Hardy class formula_6.\n\nThe sequence of \"a\" satisfying the convergence criterion above is sometimes called a Blaschke sequence.\n\nA theorem of Gábor Szegő states that if \"f\" is in formula_7, the Hardy space with integrable norm, and if \"f\" is not identically zero, then the zeroes of \"f\" (certainly countable in number) satisfy the Blaschke condition.\n\nFinite Blaschke products can be characterized (as analytic functions on the unit disc) in the following way: Assume that \"f\" is an analytic function on the open unit disc such that \n\"f\" can be extended to a continuous function on the closed unit disc\n\nwhich maps the unit circle to itself. Then ƒ is equal to a finite Blaschke product\n\nwhere \"ζ\" lies on the unit circle and \"m\" is the multiplicity of the zero \"a\", |\"a\"| < 1. In particular, if \"ƒ\" satisfies the condition above and has no zeros inside the unit circle then \"ƒ\" is constant (this fact is also a consequence of the maximum principle for harmonic functions, applied to the harmonic function log(|\"ƒ\"(\"z\")|)).\n\n\n"}
{"id": "7593", "url": "https://en.wikipedia.org/wiki?curid=7593", "title": "Calculator", "text": "Calculator\n\nAn electronic calculator is typically a portable electronic device used to perform calculations, ranging from basic arithmetic to complex mathematics.\n\nThe first solid-state electronic calculator was created in the early 1960s. Pocket-sized devices became available in the 1970s, especially after the Intel 4004, the first microprocessor, was developed by Intel for the Japanese calculator company Busicom. They later became used commonly within the petroleum industry (oil and gas).\n\nModern electronic calculators vary from cheap, give-away, credit-card-sized models to sturdy desktop models with built-in printers. They became popular in the mid-1970s as the incorporation of integrated circuits reduced their size and cost. By the end of that decade, prices had dropped to the point where a basic calculator was affordable to most and they became common in schools.\n\nComputer operating systems as far back as early Unix have included interactive calculator programs such as dc and hoc, and calculator functions are included in almost all personal digital assistant (PDA) type devices, the exceptions being a few dedicated address book and dictionary devices.\n\nIn addition to general purpose calculators, there are those designed for specific markets. For example, there are scientific calculators which include trigonometric and statistical calculations. Some calculators even have the ability to do computer algebra. Graphing calculators can be used to graph functions defined on the real line, or higher-dimensional Euclidean space. , basic calculators cost little, but scientific and graphing models tend to cost more.\n\nIn 1986, calculators still represented an estimated 41% of the world's general-purpose hardware capacity to compute information. By 2007, this diminished to less than 0.05%.\n\nElectronic calculators contain a keyboard with buttons for digits and arithmetical operations; some even contain \"00\" and \"000\" buttons to make larger or smaller numbers easier to enter. Most basic calculators assign only one digit or operation on each button; however, in more specific calculators, a button can perform multi-function working with key combinations.\n\nCalculators usually have liquid-crystal displays (LCD) as output in place of historical light-emitting diode (LED) displays and vacuum fluorescent displays (VFD); details are provided in the section \"Technical improvements\".\n\nLarge-sized figures are often used to improve readability; while using decimal separator (usually a point rather than a comma) instead of or in addition to vulgar fractions. Various symbols for function commands may also be shown on the display. Fractions such as are displayed as decimal approximations, for example rounded to . Also, some fractions (such as , which is ; to 14 significant figures) can be difficult to recognize in decimal form; as a result, many scientific calculators are able to work in vulgar fractions or mixed numbers.\n\nCalculators also have the ability to store numbers into computer memory. Basic calculators usually store only one number at a time; more specific types are able to store many numbers represented in variables. The variables can also be used for constructing formulas. Some models have the ability to extend memory capacity to store more numbers; the extended memory address is termed an array index.\n\nPower sources of calculators are: batteries, solar cells or mains electricity (for old models), turning on with a switch or button. Some models even have no turn-off button but they provide some way to put off (for example, leaving no operation for a moment, covering solar cell exposure, or closing their lid). Crank-powered calculators were also common in the early computer era.\n\nThe following keys are common to most pocket calculators. While the arrangement of the digits is standard, the positions of other keys vary from model to model; the illustration is an example.\n\nIn general, a basic electronic calculator consists of the following components:\n\n\nClock rate of a processor chip refers to the frequency at which the central processing unit (CPU) is running. It is used as an indicator of the processor's speed, and is measured in \"clock cycles per second\" or the SI unit hertz (Hz). For basic calculators, the speed can vary from a few hundred hertz to the kilohertz range.\nA basic explanation as to how calculations are performed in a simple four-function calculator:\n\nTo perform the calculation , one presses keys in the following sequence on most calculators:     .\n\nOther functions are usually performed using repeated additions or subtractions.\n\nMost pocket calculators do all their calculations in BCD rather than a floating-point representation. BCD is common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to a simpler overall system than converting to and from binary.\n\nThe same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.\n\nWhere calculators have added functions (such as square root, or trigonometric functions), software algorithms are required to produce high precision results. Sometimes significant design effort is needed to fit all the desired functions in the limited memory space available in the calculator chip, with acceptable calculation time.\n\nThe fundamental difference between a calculator and computer is that a computer can be programmed in a way that allows the program to take different branches according to intermediate results, while calculators are pre-designed with specific functions (such as addition, multiplication, and logarithms) built in. The distinction is not clear-cut: some devices classed as programmable calculators have programming functions, sometimes with support for programming languages (such as RPL or TI-BASIC).\n\nFor instance, instead of a hardware multiplier, a calculator might implement floating point mathematics with code in read-only memory (ROM), and compute trigonometric functions with the CORDIC algorithm because CORDIC does not require much multiplication. Bit serial logic designs are more common in calculators whereas bit parallel designs dominate general-purpose computers, because a bit serial design minimizes chip complexity, but takes many more clock cycles. This distinction blurs with high-end calculators, which use processor chips associated with computer and embedded systems design, more so the Z80, MC68000, and ARM architectures, and some custom designs specialized for the calculator market.\n\nThe first known tools used to aid arithmetic calculations were: bones (used to tally items), pebbles, and counting boards, and the abacus, known to have been used by Sumerians and Egyptians before 2000 BC. Except for the Antikythera mechanism (an \"out of the time\" astronomical device), development of computing tools arrived near the start of the 17th century: the geometric-military compass (by Galileo), logarithms and Napier bones (by Napier), and the slide rule (by Edmund Gunter).\nIn 1642, the Renaissance saw the invention of the mechanical calculator (by Wilhelm Schickard and several decades later Blaise Pascal), a device that was at times somewhat over-promoted as being able to perform all four arithmetic operations with minimal human intervention. Pascal's calculator could add and subtract two numbers directly and thus, if the tedium could be borne, multiply and divide by repetition. Schickard's machine, constructed several decades earlier, used a clever set of mechanised multiplication tables to ease the process of multiplication and division with the adding machine as a means of completing this operation. (Because they were different inventions with different aims a debate about whether Pascal or Schickard should be credited as the \"inventor\" of the adding machine (or calculating machine) is probably pointless.) Schickard and Pascal were followed by Gottfried Leibniz who spent forty years designing a four-operation mechanical calculator, the stepped reckoner, inventing in the process his leibniz wheel, but who couldn't design a fully operational machine. There were also five unsuccessful attempts to design a calculating clock in the 17th century.\nThe 18th century saw the arrival of some notable improvements, first by Poleni with the first fully functional calculating clock and four-operation machine, but these machines were almost always \"one of the kind\". Luigi Torchi invented the first direct multiplication machine in 1834: this was also the second key-driven machine in the world, following that of James White (1822). It was not until the 19th century and the Industrial Revolution that real developments began to occur. Although machines capable of performing all four arithmetic functions existed prior to the 19th century, the refinement of manufacturing and fabrication processes during the eve of the industrial revolution made large scale production of more compact and modern units possible. The Arithmometer, invented in 1820 as a four-operation mechanical calculator, was released to production in 1851 as an adding machine and became the first commercially successful unit; forty years later, by 1890, about 2,500 arithmometers had been sold plus a few hundreds more from two arithmometer clone makers (Burkhardt, Germany, 1878 and Layton, UK, 1883) and Felt and Tarrant, the only other competitor in true commercial production, had sold 100 comptometers.\n\nIt wasn't until 1902 that the familiar push-button user interface was developed, with the introduction of the Dalton Adding Machine, developed by James L. Dalton in the United States.\n\nIn 1921, Edith Clarke invented the \"Clarke calculator\", a simple graph-based calculator for solving line equations involving hyperbolic functions. This allowed electrical engineers to simplify calculations for inductance and capacitance in power transmission lines.\n\nThe Curta calculator was developed in 1948 and, although costly, became popular for its portability. This purely mechanical hand-held device could do addition, subtraction, multiplication and division. By the early 1970s electronic pocket calculators ended manufacture of mechanical calculators, although the Curta remains a popular collectable item.\n\nThe first mainframe computers, using firstly vacuum tubes and later transistors in the logic circuits, appeared in the 1940s and 1950s. This technology was to provide a stepping stone to the development of electronic calculators.\n\nThe Casio Computer Company, in Japan, released the Model \"14-A\" calculator in 1957, which was the world's first all-electric (relatively) compact calculator. It did not use electronic logic but was based on relay technology, and was built into a desk.\nIn October 1961, the world's first \"all-electronic desktop\" calculator, the British Bell Punch/Sumlock Comptometer ANITA (A New Inspiration To Arithmetic/Accounting) was announced. This machine used vacuum tubes, cold-cathode tubes and Dekatrons in its circuits, with 12 cold-cathode \"Nixie\" tubes for its display. Two models were displayed, the Mk VII for continental Europe and the Mk VIII for Britain and the rest of the world, both for delivery from early 1962. The Mk VII was a slightly earlier design with a more complicated mode of multiplication, and was soon dropped in favour of the simpler Mark VIII. The ANITA had a full keyboard, similar to mechanical comptometers of the time, a feature that was unique to it and the later Sharp CS-10A among electronic calculators. The ANITA weighed roughly due to its large tube system. Bell Punch had been producing key-driven mechanical calculators of the comptometer type under the names \"Plus\" and \"Sumlock\", and had realised in the mid-1950s that the future of calculators lay in electronics. They employed the young graduate Norbert Kitz, who had worked on the early British Pilot ACE computer project, to lead the development. The ANITA sold well since it was the only electronic desktop calculator available, and was silent and quick.\n\nThe tube technology of the ANITA was superseded in June 1963 by the U.S. manufactured Friden EC-130, which had an all-transistor design, a stack of four 13-digit numbers displayed on a cathode ray tube (CRT), and introduced Reverse Polish Notation (RPN) to the calculator market for a price of $2200, which was about three times the cost of an electromechanical calculator of the time. Like Bell Punch, Friden was a manufacturer of mechanical calculators that had decided that the future lay in electronics. In 1964 more all-transistor electronic calculators were introduced: Sharp introduced the CS-10A, which weighed and cost 500,000 yen ($), and Industria Macchine Elettroniche of Italy introduced the IME 84, to which several extra keyboard and display units could be connected so that several people could make use of it (but apparently not at the same time).\n\nThere followed a series of electronic calculator models from these and other manufacturers, including Canon, Mathatronics, Olivetti, SCM (Smith-Corona-Marchant), Sony, Toshiba, and Wang. The early calculators used hundreds of germanium transistors, which were cheaper than silicon transistors, on multiple circuit boards. Display types used were CRT, cold-cathode Nixie tubes, and filament lamps. Memory technology was usually based on the delay line memory or the magnetic core memory, though the Toshiba \"Toscal\" BC-1411 appears to have used an early form of dynamic RAM built from discrete components. Already there was a desire for smaller and less power-hungry machines.\n\nThe Olivetti Programma 101 was introduced in late 1965; it was a stored program machine which could read and write magnetic cards and displayed results on its built-in printer. Memory, implemented by an acoustic delay line, could be partitioned between program steps, constants, and data registers. Programming allowed conditional testing and programs could also be overlaid by reading from magnetic cards. It is regarded as the first personal computer produced by a company (that is, a desktop electronic calculating machine programmable by non-specialists for personal use). The Olivetti Programma 101 won many industrial design awards.\nAnother calculator introduced in 1965 was Bulgaria's ELKA 6521, developed by the Central Institute for Calculation Technologies and built at the Elektronika factory in Sofia. The name derives from \"ELektronen KAlkulator\", and it weighed around . It is the first calculator in the world which includes the square root function. Later that same year were released the ELKA 22 (with a luminescent display) and the ELKA 25, with an in-built printer. Several other models were developed until the first pocket model, the ELKA 101, was released in 1974. The writing on it was in Roman script, and it was exported to western countries.\n\nThe \"Monroe Epic\" programmable calculator came on the market in 1967. A large, printing, desk-top unit, with an attached floor-standing logic tower, it could be programmed to perform many computer-like functions. However, the only \"branch\" instruction was an implied unconditional branch (GOTO) at the end of the operation stack, returning the program to its starting instruction. Thus, it was not possible to include any conditional branch (IF-THEN-ELSE) logic. During this era, the absence of the conditional branch was sometimes used to distinguish a programmable calculator from a computer.\n\nThe first handheld calculator was a prototype called \"Cal Tech\", whose development was led by Jack Kilby at Texas Instruments in 1967. It could add, multiply, subtract, and divide, and its output device was a paper tape.\n\nThe electronic calculators of the mid-1960s were large and heavy desktop machines due to their use of hundreds of transistors on several circuit boards with a large power consumption that required an AC power supply. There were great efforts to put the logic required for a calculator into fewer and fewer integrated circuits (chips) and calculator electronics was one of the leading edges of semiconductor development. U.S. semiconductor manufacturers led the world in large scale integration (LSI) semiconductor development, squeezing more and more functions into individual integrated circuits. This led to alliances between Japanese calculator manufacturers and U.S. semiconductor companies: Canon Inc. with Texas Instruments, Hayakawa Electric (later renamed Sharp Corporation) with North-American Rockwell Microelectronics (later renamed Rockwell International), Busicom with Mostek and Intel, and General Instrument with Sanyo.\n\nBy 1970, a calculator could be made using just a few chips of low power consumption, allowing portable models powered from rechargeable batteries. The first portable calculators appeared in Japan in 1970, and were soon marketed around the world. These included the Sanyo ICC-0081 \"Mini Calculator\", the Canon Pocketronic, and the Sharp QT-8B \"micro Compet\". The Canon Pocketronic was a development of the \"Cal-Tech\" project which had been started at Texas Instruments in 1965 as a research project to produce a portable calculator. The Pocketronic has no traditional display; numerical output is on thermal paper tape. As a result of the \"Cal-Tech\" project, Texas Instruments was granted master patents on portable calculators.\n\nSharp put in great efforts in size and power reduction and introduced in January 1971 the Sharp EL-8, also marketed as the Facit 1111, which was close to being a pocket calculator. It weighed 1.59 pounds (721 grams), had a vacuum fluorescent display, rechargeable NiCad batteries, and initially sold for US $395.\n\nHowever, the efforts in integrated circuit development culminated in the introduction in early 1971 of the first \"calculator on a chip\", the MK6010 by Mostek, followed by Texas Instruments later in the year. Although these early hand-held calculators were very costly, these advances in electronics, together with developments in display technology (such as the vacuum fluorescent display, LED, and LCD), led within a few years to the cheap pocket calculator available to all.\n\nIn 1971 Pico Electronics. and General Instrument also introduced their first collaboration in ICs, a full single chip calculator IC for the Monroe Royal Digital III calculator. Pico was a spinout by five GI design engineers whose vision was to create single chip calculator ICs. Pico and GI went on to have significant success in the burgeoning handheld calculator market.\n\nThe first truly pocket-sized electronic calculator was the Busicom LE-120A \"HANDY\", which was marketed early in 1971. Made in Japan, this was also the first calculator to use an LED display, the first hand-held calculator to use a single integrated circuit (then proclaimed as a \"calculator on a chip\"), the Mostek MK6010, and the first electronic calculator to run off replaceable batteries. Using four AA-size cells the LE-120A measures .\n\nThe first European-made pocket-sized calculator, DB 800 is made in May 1971 by Digitron in Buje, Croatia (former Yugoslavia) with four functions and an eight-digit display and special characters for a negative number and a warning that the calculation has too many digits to display.\n\nThe first American-made pocket-sized calculator, the Bowmar 901B (popularly termed \"The Bowmar Brain\"), measuring , came out in the Autumn of 1971, with four functions and an eight-digit red LED display, for $240, while in August 1972 the four-function Sinclair Executive became the first slimline pocket calculator measuring and weighing . It retailed for around £79 ($). By the end of the decade, similar calculators were priced less than £5 ($).\n\nThe first Soviet Union made pocket-sized calculator, the \"Elektronika B3-04\" was developed by the end of 1973 and sold at the start of 1974.\n\nOne of the first low-cost calculators was the Sinclair Cambridge, launched in August 1973. It retailed for £29.95 ($), or £5 ($) less in kit form. The Sinclair calculators were successful because they were far cheaper than the competition; however, their design led to slow and inaccurate computations of transcendental functions.\n\nMeanwhile, Hewlett-Packard (HP) had been developing a pocket calculator. Launched in early 1972, it was unlike the other basic four-function pocket calculators then available in that it was the first pocket calculator with \"scientific\" functions that could replace a slide rule. The $395 HP-35, along with nearly all later HP engineering calculators, used reverse Polish notation (RPN), also called postfix notation. A calculation like \"8 plus 5\" is, using RPN, performed by pressing , , , and ; instead of the algebraic infix notation: , , , . It had 35 buttons and was based on Mostek Mk6020 chip.\n\nThe first Soviet \"scientific\" pocket-sized calculator the \"B3-18\" was completed by the end of 1975.\n\nIn 1973, Texas Instruments (TI) introduced the SR-10, (\"SR\" signifying slide rule) an \"algebraic entry\" pocket calculator using scientific notation for $150. Shortly after the SR-11 featured an added key for entering Pi (π). It was followed the next year by the SR-50 which added log and trig functions to compete with the HP-35, and in 1977 the mass-marketed TI-30 line which is still produced.\n\nIn 1978 a new company, Calculated Industries arose which focused on specialized markets. Their first calculator, the Loan Arranger (1978) was a pocket calculator marketed to the Real Estate industry with preprogrammed functions to simplify the process of calculating payments and future values. In 1985, CI launched a calculator for the construction industry called the Construction Master which came preprogrammed with common construction calculations (such as angles, stairs, roofing math, pitch, rise, run, and feet-inch fraction conversions). This would be the first in a line of construction related calculators.\n\nThe first desktop \"programmable calculators\" were produced in the mid-1960s by Mathatronics and Casio (AL-1000). These machines were very heavy and costly. The first programmable pocket calculator was the HP-65, in 1974; it had a capacity of 100 instructions, and could store and retrieve programs with a built-in magnetic card reader. Two years later the HP-25C introduced \"continuous memory\", i.e., programs and data were retained in CMOS memory during power-off. In 1979, HP released the first \"alphanumeric\", programmable, \"expandable\" calculator, the HP-41C. It could be expanded with random access memory (RAM, for memory) and read-only memory (ROM, for software) modules, and peripherals like bar code readers, microcassette and floppy disk drives, paper-roll thermal printers, and miscellaneous communication interfaces (RS-232, HP-IL, HP-IB).\n\nThe first Soviet programmable desktop calculator ISKRA 123, powered by the power grid, was released at the start of the 1970s. The first Soviet pocket battery-powered programmable calculator, Elektronika \"B3-21\", was developed by the end of 1976 and released at the start of 1977. The successor of B3-21, the Elektronika B3-34 wasn't backward compatible with B3-21, even if it kept the reverse Polish notation (RPN). Thus B3-34 defined a new command set, which later was used in a series of later programmable Soviet calculators. Despite very limited abilities (98 bytes of instruction memory and about 19 stack and addressable registers), people managed to write all kinds of programs for them, including adventure games and libraries of calculus-related functions for engineers. Hundreds, perhaps thousands, of programs were written for these machines, from practical scientific and business software, which were used in real-life offices and labs, to fun games for children. The Elektronika MK-52 calculator (using the extended B3-34 command set, and featuring internal EEPROM memory for storing programs and external interface for EEPROM cards and other periphery) was used in Soviet spacecraft program (for Soyuz TM-7 flight) as a backup of the board computer.\n\nThis series of calculators was also noted for a large number of highly counter-intuitive mysterious undocumented features, somewhat similar to \"synthetic programming\" of the American HP-41, which were exploited by applying normal arithmetic operations to error messages, jumping to nonexistent addresses and other methods. A number of respected monthly publications, including the popular science magazine \"Nauka i Zhizn\" (\"Наука и жизнь\", \"Science and Life\"), featured special columns, dedicated to optimization methods for calculator programmers and updates on undocumented features for hackers, which grew into a whole esoteric science with many branches, named \"yeggogology\" (\"еггогология\"). The error messages on those calculators appear as a Russian word \"YEGGOG\" (\"ЕГГОГ\") which, unsurprisingly, is translated to \"Error\".\n\nA similar hacker culture in the USA revolved around the HP-41, which was also noted for a large number of undocumented features and was much more powerful than B3-34.\n\nThrough the 1970s the hand-held electronic calculator underwent rapid development. The red LED and blue/green vacuum fluorescent displays consumed a lot of power and the calculators either had a short battery life (often measured in hours, so rechargeable nickel-cadmium batteries were common) or were large so that they could take larger, higher capacity batteries. In the early 1970s liquid-crystal displays (LCDs) were in their infancy and there was a great deal of concern that they only had a short operating lifetime. Busicom introduced the Busicom \"LE-120A \"HANDY\"\" calculator, the first pocket-sized calculator and the first with an LED display, and announced the Busicom \"LC\" with LCD. However, there were problems with this display and the calculator never went on sale. The first successful calculators with LCDs were manufactured by Rockwell International and sold from 1972 by other companies under such names as: Dataking \"LC-800\", Harden \"DT/12\", Ibico \"086\", Lloyds \"40\", Lloyds \"100\", Prismatic \"500\" (a.k.a. \"P500\"), Rapid Data \"Rapidman 1208LC\". The LCDs were an early form using the \"Dynamic Scattering Mode DSM\" with the numbers appearing as bright against a dark background. To present a high-contrast display these models illuminated the LCD using a filament lamp and solid plastic light guide, which negated the low power consumption of the display. These models appear to have been sold only for a year or two.\n\nA more successful series of calculators using a reflective DSM-LCD was launched in 1972 by Sharp Inc with the Sharp \"EL-805\", which was a slim pocket calculator. This, and another few similar models, used Sharp's \"Calculator On Substrate\" (COS) technology. An extension of one glass plate needed for the liquid crystal display was used as a substrate to mount the needed chips based on a new hybrid technology. The COS technology may have been too costly since it was only used in a few models before Sharp reverted to conventional circuit boards.\nIn the mid-1970s the first calculators appeared with field-effect, \"twisted nematic\" (TN) LCDs with dark numerals against a grey background, though the early ones often had a yellow filter over them to cut out damaging ultraviolet rays. The advantage of LCDs is that they are passive light modulators reflecting light, which require much less power than light-emitting displays such as LEDs or VFDs. This led the way to the first credit-card-sized calculators, such as the Casio \"Mini Card LC-78\" of 1978, which could run for months of normal use on button cells.\n\nThere were also improvements to the electronics inside the calculators. All of the logic functions of a calculator had been squeezed into the first \"calculator on a chip\" integrated circuits (ICs) in 1971, but this was leading edge technology of the time and yields were low and costs were high. Many calculators continued to use two or more ICs, especially the scientific and the programmable ones, into the late 1970s.\n\nThe power consumption of the integrated circuits was also reduced, especially with the introduction of CMOS technology. Appearing in the Sharp \"EL-801\" in 1972, the transistors in the logic cells of CMOS ICs only used any appreciable power when they changed state. The LED and VFD displays often required added driver transistors or ICs, whereas the LCDs were more amenable to being driven directly by the calculator IC itself.\n\nWith this low power consumption came the possibility of using solar cells as the power source, realised around 1978 by calculators such as the Royal \"Solar 1\", Sharp \"EL-8026\", and Teal \"Photon\".\n\nAt the start of the 1970s, hand-held electronic calculators were very costly, at two or three weeks' wages, and so were a luxury item. The high price was due to their construction requiring many mechanical and electronic components which were costly to produce, and production runs that were too small to exploit economies of scale. Many firms saw that there were good profits to be made in the calculator business with the margin on such high prices. However, the cost of calculators fell as components and their production methods improved, and the effect of economies of scale was felt.\n\nBy 1976, the cost of the cheapest four-function pocket calculator had dropped to a few dollars, about 1/20th of the cost five years before. The results of this were that the pocket calculator was affordable, and that it was now difficult for the manufacturers to make a profit from calculators, leading to many firms dropping out of the business or closing down. The firms that survived making calculators tended to be those with high outputs of higher quality calculators, or producing high-specification scientific and programmable calculators.\n\nThe first calculator capable of symbolic computing was the HP-28C, released in 1987. It could, for example, solve quadratic equations symbolically. The first graphing calculator was the Casio fx-7000G released in 1985.\n\nThe two leading manufacturers, HP and TI, released increasingly feature-laden calculators during the 1980s and 1990s. At the turn of the millennium, the line between a graphing calculator and a handheld computer was not always clear, as some very advanced calculators such as the TI-89, the Voyage 200 and HP-49G could differentiate and integrate functions, solve differential equations, run word processing and PIM software, and connect by wire or IR to other calculators/computers.\n\nThe HP 12c financial calculator is still produced. It was introduced in 1981 and is still being made with few changes. The HP 12c featured the reverse Polish notation mode of data entry. In 2003 several new models were released, including an improved version of the HP 12c, the \"HP 12c platinum edition\" which added more memory, more built-in functions, and the addition of the algebraic mode of data entry.\n\nCalculated Industries competed with the HP 12c in the mortgage and real estate markets by differentiating the key labeling; changing the “I”, “PV”, “FV” to easier labeling terms such as \"Int\", \"Term\", \"Pmt\", and not using the reverse Polish notation. However, CI's more successful calculators involved a line of construction calculators, which evolved and expanded in the 1990s to present. According to Mark Bollman, a mathematics and calculator historian and associate professor of mathematics at Albion College, the \"Construction Master is the first in a long and profitable line of CI construction calculators\" which carried them through the 1980s, 1990s, and to the present.\n\nPersonal computers often come with a calculator utility program that emulates the appearance and functions of a calculator, using the graphical user interface to portray a calculator. One such example is Windows Calculator. Most personal data assistants (PDAs) and smartphones also have such a feature.\n\nIn most countries, students use calculators for schoolwork. There was some initial resistance to the idea out of fear that basic or elementary arithmetic skills would suffer. There remains disagreement about the importance of the ability to perform calculations \"in the head\", with some curricula restricting calculator use until a certain level of proficiency has been obtained, while others concentrate more on teaching estimation methods and problem-solving. Research suggests that inadequate guidance in the use of calculating tools can restrict the kind of mathematical thinking that students engage in. Others have argued that calculator use can even cause core mathematical skills to atrophy, or that such use can prevent understanding of advanced algebraic concepts. In December 2011 the UK's Minister of State for Schools, Nick Gibb, voiced concern that children can become \"too dependent\" on the use of calculators. As a result, the use of calculators is to be included as part of a review of the Curriculum. In the United States, many math educators and boards of education enthusiastically endorsed the National Council of Teachers of Mathematics (NCTM) standards and actively promoted the use of classroom calculators from kindergarten through high school.\n\n\n\n\n"}
{"id": "2754781", "url": "https://en.wikipedia.org/wiki?curid=2754781", "title": "Chandler Davis", "text": "Chandler Davis\n\nHorace Chandler Davis (born August 12, 1926 in Ithaca, New York) is an American-Canadian mathematician, writer, and educator.\n\nHe was born in Ithaca, New York, to parents Horace B. Davis and Marian R. Davis. In 1948 he married Natalie Zemon Davis; they have three children. He was a member of the CPUSA and he was dismissed and jailed for his beliefs.\n\nHe moved to Canada in 1962 and began teaching at the University of Toronto. A lecture in honour of his stand for his beliefs is now held at the university that dismissed him.\n\nIn 1950 he received a doctorate in mathematics from Harvard University.\n\nHis principal research investigations involve linear algebra and operator theory in Hilbert space. Furthermore, he has made contributions to numerical analysis, geometry, and algebraic logic. He is one of the eponyms of the Davis–Kahan theorem and Bhatia–Davis inequality (along with Rajendra Bhatia). The Davis–Kahan–Weinberger dilation theorem is one of the landmark results in the dilation theory of Hilbert space operators and has found applications in many different areas. A PhD thesis titled \"Backward Perturbation and Sensitivity Analysis of Structured Polynomial Eigenvalue Problem\" is dedicated to this theorem. Davis has written around eighty research papers in mathematics.\nDavis was a professor in the mathematics department of University of Michigan, working alongside Wilfred Kaplan.\n\nHe is currently one of the co-Editors-in-Chief of the \"Mathematical Intelligencer\". In 2012 he became a fellow of the American Mathematical Society.\nHe is part of the 2019 class of fellows of the Association for Women in Mathematics.\n\nHe began his writing career in \"Astounding Science Fiction\" in 1946. From 1946 through 1962 he produced a spate of science fiction stories, mostly published there. One of the earliest, published May 1946, was \"The Nightmare\", later the lead story in \"A Treasury of Science Fiction\", edited by Groff Conklin; it argued for a national policy of decentralizing industry to evade nuclear attacks by terrorists. He also issued the fanzine \"Blitherings\" in the 1940s.\n\nHe attended Torcon I, the 6th World Science Fiction Convention in 1948, appeared at the 2010 SFContario science fiction convention, and was Science Guest of Honor at the 2013 SFContario science fiction convention.\n\nDavis came from a radical family and has identified himself as a socialist and former member of the Communist Party of America.\n\nDavis—along with two other professors, Mark Nickerson and Clement Markert—refused to cooperate with the House Unamerican Activities Committee and was subsequently dismissed from the University of Michigan. Davis was then sentenced to a six-month prison term where he was able to do some research. A paper from this era has the following acknowledgement:\n\nResearch supported in part by the Federal Prison System. Opinions expressed in this paper are the author's and are not necessarily those of the Bureau of Prisons. \n\nThe Federal government released Davis from prison in 1960. After his release, Davis moved to Canada, where he currently resides.\n\nIn 1991, the University of Michigan Senate initiated the annual Davis, Markert, Nickerson Lecture on Academic and Intellectual Freedom. Recent speakers have included: Cass Sunstein (2008), Nadine Strossen (2007), Bill Keller (2006), Floyd Abrams (2005), and Noam Chomsky (2004).\n\n"}
{"id": "15756448", "url": "https://en.wikipedia.org/wiki?curid=15756448", "title": "Church's thesis (constructive mathematics)", "text": "Church's thesis (constructive mathematics)\n\nIn constructive mathematics, Church's thesis (CT) is an axiom stating that all total functions are computable. The axiom takes its name from the Church–Turing thesis, which states that every effectively calculable function is a computable function, but the constructivist version is much stronger, claiming that every function is computable.\n\nThe axiom CT is incompatible with classical logic in sufficiently strong systems. For example, Heyting arithmetic (HA) with CT as an addition axiom is able to disprove some instances of the law of the excluded middle. However, Heyting arithmetic is equiconsistent with Peano arithmetic (PA) as well as with Heyting arithmetic plus Church's thesis. That is, adding either the law of the excluded middle or Church's thesis does not make Heyting arithmetic inconsistent, but adding both does.\n\nIn first-order theories such as HA, which cannot quantify over functions directly, CT is stated as an axiom schema saying that any definable function is computable, using Kleene's T predicate to define computability. For each formula φ(\"x\",\"y\") of two variables, the schema includes the axiom\nThis axiom asserts that, if for every \"x\" there is a \"y\" satisfying φ then there is in fact an \"e\" that is the Gödel number of a general recursive function that will, for every \"x\", produce such a \"y\" satisfying the formula, with some \"u\" being a Gödel number encoding a verifiable computation bearing witness to the fact that \"y\" is in fact the value of that function at \"x\".\n\nIn higher-order systems that can quantify over functions directly, CT can be stated as a single axiom saying that every function from the natural numbers to the natural numbers is computable.\n\nThe schema form of CT shown above, when added to constructive systems such as HA, implies the negation of the law of the excluded middle. As an example, it is a classical tautology that every Turing machine either halts or does not halt on a given input. Assuming this tautology, in sufficiently strong systems such as HA it is possible to form a function \"h\" that takes a code for a Turing machine and returns 1 if the machine halts and 0 if it does not halt. Then, from Church's Thesis one would conclude that this function is itself computable, but this is known to be false, because the Halting problem is not computably solvable. Thus HA and CT disproves some consequence of the law of the excluded middle.\n\nThe \"single axiom\" form of CT mentioned above, \nquantifies over functions and says that every function \"f\" is computable (with an index \"e\"). This axiom is consistent with some weak classical systems that do not have the strength to form functions such as the function \"f\" of the previous paragraph. For example, the weak classical system formula_3 is consistent with this single axiom, because formula_3 has a model in which every function is computable. However, the single-axiom form becomes inconsistent with the law of the excluded middle in any system that has sufficient axioms to construct functions such as the function \"h\" in the previous paragraph.\n\nExtended Church's thesis (ECT) extends the claim to functions which are totally defined over a certain type of domain. It is used by the school of constructive mathematics founded by Andrey Markov Jr. It can be formally stated by the schema:\n\nIn the above, formula_6 is restricted to be \"almost-negative\". For first-order arithmetic (where the schema is designated formula_7), this means formula_6 cannot contain any disjunction, and existential quantifiers can only appear in front of formula_9 (decidable) formulas.\n\nThis thesis can be characterised as saying that a sentence is true if and only if it is computably realisable. In fact this is captured by the following meta-theoretic equivalences:\nHere, formula_12 stands for \"formula_13\". So, it is provable in formula_14 with formula_7 that a sentence is true iff it is realisable. But also, formula_16 is true in formula_14 with formula_7 iff formula_16 is realisable in formula_14 without formula_7.\n\nThe second equivalence can be extended with Markov's principle (M) as follows:\nSo, formula_16 is true in formula_14 with formula_7 and formula_26 iff there is a number \"n\" which realises formula_16 in formula_28. The existential quantifier has to be outside formula_28 in this case, because PA is non-constructive and lacks the existence property.\n\n"}
{"id": "3034280", "url": "https://en.wikipedia.org/wiki?curid=3034280", "title": "Ciprian Manolescu", "text": "Ciprian Manolescu\n\nCiprian Manolescu (born December 24, 1978) is a Romanian-American mathematician, working in gauge theory, symplectic geometry, and low-dimensional topology. He is currently a Professor of Mathematics at the University of California, Los Angeles.\n\nManolescu completed his first eight classes at School no. 11 Mihai Eminescu and his secondary education at Ion Brătianu High School in Piteşti. He did his undergrad and Ph.D. at Harvard University under the direction of Peter B. Kronheimer, and became a teaching fellow in the Math 55 undergraduate course. He was the winner of the Morgan Prize, awarded jointly by AMS-MAA-SIAM, in 2002. His undergraduate thesis was on \"Finite dimensional approximation in Seiberg–Witten theory\", and his Ph.D. thesis topic was \"A spectrum valued TQFT from the Seiberg–Witten equations\".\n\nIn early 2013 he released a paper detailing a disproof of the Triangulation Conjecture for manifolds of dimension 5 and higher.\n\nHe was among the handful of recipients of the Clay Research Fellowship (2004–2008).\n\nIn 2012 he was awarded one of the ten prizes of the European Mathematical Society for his work on low-dimensional topology, and particularly for his role in the development of combinatorial Heegaard Floer homology.\n\nHe was elected as a member of the 2017 class of Fellows of the American Mathematical Society \"for contributions to Floer homology and the topology of manifolds\".\n\nHe has one of the best records ever in mathematical competitions:\n\n\n"}
{"id": "54347", "url": "https://en.wikipedia.org/wiki?curid=54347", "title": "Complement (set theory)", "text": "Complement (set theory)\n\nIn set theory, the complement of a set refers to elements not in .\n\nWhen all sets under consideration are considered to be subsets of a given set , the absolute complement of is the set of elements in but not in .\n\nThe relative complement of with respect to a set , also termed the difference of sets and , written , is the set of elements in but not in .\n\nIf is a set, then the absolute complement of (or simply the complement of ) is the set of elements not in . In other words, if is the universe that contains all the elements under study, and there is no need to mention it because it is obvious and unique, then the absolute complement of is the relative complement of in :\n\nFormally:\n\nThe absolute complement of is usually denoted by formula_3. Other notations include formula_4, formula_5, formula_6, and formula_7.\n\n\nLet and be two sets in a universe . The following identities capture important properties of absolute complements:\n\nThe first two complement laws above show that if is a non-empty, proper subset of , then is a partition of .\n\nIf and are sets, then the relative complement of in , also termed the set difference of and , is the set of elements in but not in .\n\nThe relative complement of in is denoted according to the ISO 31-11 standard. It is sometimes written , but this notation is ambiguous, as in some contexts it can be interpreted as the set of all elements , where is taken from and from .\n\nFormally:\n\n\nLet , , and be three sets. The following identities capture notable properties of relative complements:\n\nIn the LaTeX typesetting language, the command codice_1 is usually used for rendering a set difference symbol, which is similar to a backslash symbol. When rendered, the codice_1 command looks identical to codice_3 except that it has a little more space in front and behind the slash, akin to the LaTeX sequence codice_4. A variant codice_5 is available in the amssymb package.\n\nSome programming languages allow for manipulation of sets as data structures, using these operators or functions to construct the difference of sets codice_6 and codice_7:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "2075246", "url": "https://en.wikipedia.org/wiki?curid=2075246", "title": "Correlation dimension", "text": "Correlation dimension\n\nIn chaos theory, the correlation dimension (denoted by \"ν\") is a measure of the dimensionality of the space occupied by a set of random points, often referred to as a type of fractal dimension.\n\nFor example, if we have a set of random points on the real number line between 0 and 1, the correlation dimension will be \"ν\" = 1, while if they are distributed on say, a triangle embedded in three-dimensional space (or \"m\"-dimensional space), the correlation dimension will be \"ν\" = 2. This is what we would intuitively expect from a measure of dimension. The real utility of the correlation dimension is in determining the (possibly fractional) dimensions of fractal objects. There are other methods of measuring dimension (e.g. the Hausdorff dimension, the box-counting dimension, and the\ninformation dimension) but the correlation dimension has the advantage of being straightforwardly and quickly calculated, of being less noisy when only a small number of points is available, and is often in agreement with other calculations of dimension.\n\nFor any set of \"N\" points in an \"m\"-dimensional space\n\nthen the correlation integral \"C\"(\"ε\") is calculated by:\n\nwhere \"g\" is the total number of pairs of points which have a distance between them that is less than distance \"ε\" (a graphical representation of such close pairs is the recurrence plot). As the number of points tends to infinity, and the distance between them tends to zero, the correlation integral, for small values of \"ε\", will take the form:\n\nIf the number of points is sufficiently large, and evenly distributed, a log-log graph of the correlation integral versus \"ε\" will yield an estimate of \"ν\". This idea can be qualitatively understood by realizing that for higher-dimensional objects, there will be more ways for points to be close to each other, and so the number of pairs close to each other will rise more rapidly for higher dimensions.\n\nGrassberger and Procaccia introduced the technique in 1983; the article gives the results of such estimates for a number of fractal objects, as well as comparing the values to other measures of fractal dimension. The technique can be used to distinguish between (deterministic) chaotic and truly random behavior, although it may not be good at detecting deterministic behavior if the deterministic generating mechanism is very complex.\n\nAs an example, in the \"Sun in Time\" article, the method was used to show that the number of sunspots on the sun, after accounting for the known cycles such as the daily and 11-year cycles, is very likely not random noise, but rather chaotic noise, with a low-dimensional fractal attractor.\n\n"}
{"id": "2606907", "url": "https://en.wikipedia.org/wiki?curid=2606907", "title": "Cylindric numbering", "text": "Cylindric numbering\n\nIn computability theory a cylindric numbering is a special kind of numbering first introduced by Yuri L. Ershov in 1973. \n\nIf a numberings formula_1 is reducible to formula_2 then there exists a computable function formula_3 with formula_4. Usually formula_3 is not injective but if formula_2 is a cylindric numbering we can always find an injective formula_3. \n\nA numbering formula_1 is called cylindric if \nThat is if it is one-equivalent to its cylindrification\n\nA set formula_10 is called cylindric if its indicator function \nis a cylindric numbering.\n\n\n\n"}
{"id": "37270550", "url": "https://en.wikipedia.org/wiki?curid=37270550", "title": "Dynamic aperture (accelerator physics)", "text": "Dynamic aperture (accelerator physics)\n\nThe dynamic aperture is the stability region of phase space in a circular accelerator.\n\nIn the case of protons or heavy ion accelerators, (or synchrotrons, or storage rings), there is minimal radiation, and hence the dynamics is symplectic.\nFor long term stability, tiny dynamical diffusion (or Arnold diffusion) can lead an initially stable orbit slowly into an unstable region. This makes the dynamic aperture problem particularly challenging. One may be considering stability over billions of turns.\nA scaling law for Dynamic aperture vs. number of turns has been proposed by Giovannozzi.\n\nFor the case of electrons, the electrons will radiate which causes a damping effect. This means that one typically only cares about stability over thousands of turns.\n\nThe basic method for computing dynamic aperture involves the use of a tracking code. A model of the ring is built within the code that includes an integration routine for each magnetic element. The particle is tracked many turns and stability is determined.\n\nIn addition, there are other quantities that may be computed to characterize the dynamics, and can be related to the dynamic aperture. One example is the tune shift with amplitude.\n\nThere have also been other proposals for approaches to enlarge dynamic aperture, such as \n"}
{"id": "43263116", "url": "https://en.wikipedia.org/wiki?curid=43263116", "title": "Ernst Kötter", "text": "Ernst Kötter\n\nErnst Kötter was a German mathematician who graduated in 1884 from Berlin University.\nHis treatise \"Fundamentals of a purely geometrical theory of algebraic plane curves\" gained the 1886 prize of the Berlin Royal Academy.\nIn 1901, he published his report on \"The development of synthetic geometry from Monge to Staudt (1847)\";\nit had been sent to the press as early as 1897, but completion was deferred by Kötter's appointment to Aachen University and a subsequent persisting illness.\nHe constructed a mobile wood model to illustrate the theorems of Dandelin spheres.\nIn a discussion with Schoenflies and Kötter, Hilbert reportedly uttered his famous quotation according to which points, lines, and planes in geometry could be named as well \"tables, chairs, and beer mugs\".\n\n"}
{"id": "15115411", "url": "https://en.wikipedia.org/wiki?curid=15115411", "title": "Formation rule", "text": "Formation rule\n\nIn mathematical logic, formation rules are rules for describing which strings of symbols formed from the alphabet of a formal language are syntactically valid within the language. These rules only address the location and manipulation of the strings of the language. It does not describe anything else about a language, such as its semantics (i.e. what the strings mean). (See also formal grammar).\n\nA \"formal language\" is an organized set of symbols the essential feature being that it can be precisely defined in terms of just the shapes and locations of those symbols. Such a language can be defined, then, without any reference to any meanings of any of its expressions; it can exist before any interpretation is assigned to it—that is, before it has any meaning. A formal grammar determines which symbols and sets of symbols are formulas in a formal language.\n\nA \"formal system\" (also called a \"logical calculus\", or a \"logical system\") consists of a formal language together with a deductive apparatus (also called a \"deductive system\"). The deductive apparatus may consist of a set of transformation rules (also called \"inference rules\") or a set of axioms, or have both. A formal system is used to derive one expression from one or more other expressions. Propositional and predicate calculi are examples of formal systems.\n\nThe formation rules of a propositional calculus may, for instance, take a form such that;\n\n\nA predicate calculus will usually include all the same rules as a propositional calculus, with the addition of quantifiers such that if we take Φ to be a formula of propositional logic and α as a variable then we can take (α)Φ and (α)Φ each to be formulas of our predicate calculus.\n\n"}
{"id": "47977109", "url": "https://en.wikipedia.org/wiki?curid=47977109", "title": "Haeckelites", "text": "Haeckelites\n\nHaeckelites are three-fold coordinated networks of carbon atoms generated by a periodic arrangement of pentagons, hexagons and heptagons. They were first proposed by Humberto and Mauricio Terrones and their colleagues in 2000. They were named in honour of Ernst Haeckel, whose diagrams of radiolaria contained similar structural features. They have not yet been synthesised in the laboratory, but have been the subject of a considerable amount of theoretical work.\n"}
{"id": "3781180", "url": "https://en.wikipedia.org/wiki?curid=3781180", "title": "Hartley function", "text": "Hartley function\n\nThe Hartley function is a measure of uncertainty, introduced by Ralph Hartley in 1928. If a sample from a finite set \"A\" uniformly at random is picked, the information revealed after the outcome is known is given by the Hartley function\nwhere denotes the cardinality of \"A\".\n\nIf the base of the logarithm is 2, then the unit of uncertainty is the shannon. If it is the natural logarithm, then the unit is the nat. Hartley used a base-ten logarithm, and with this base, the unit of information is called the hartley in his honor. It is also known as the Hartley entropy.\n\nThe Hartley function coincides with the Shannon entropy (as well as with the Rényi entropies of all orders) in the case of a uniform probability distribution. It is a special case of the Rényi entropy since:\n\nBut it can also be viewed as a primitive construction, since, as emphasized by Kolmogorov and Rényi, the Hartley function can be defined without introducing any notions of probability (see \"Uncertainty and information\" by George J. Klir, p. 423).\n\nThe Hartley function only depends on the number of elements in a set, and hence can be viewed as a function on natural numbers. Rényi showed that the Hartley function in base 2 is the only function mapping natural numbers to real numbers that satisfies\n\n\nCondition 1 says that the uncertainty of the Cartesian product of two finite sets \"A\" and \"B\" is the sum of uncertainties of \"A\" and \"B\". Condition 2 says that larger set has larger uncertainty.\n\nWe want to show that the Hartley function, log(\"n\"), is the only function mapping natural numbers to real numbers that satisfies\n\n\nLet \"ƒ\" be a function on positive integers that satisfies the above three properties. From the additive property, we can show that for any integer \"n\" and \"k\",\n\nLet \"a\", \"b\", and \"t\" be any positive integers. There is a unique integer \"s\" determined by\n\nTherefore,\n\nand\n\nOn the other hand, by monotonicity,\n\nUsing equation (1), one gets\n\nand\n\nHence,\n\nSince \"t\" can be arbitrarily large, the difference on the left hand side of the above inequality must be zero,\n\nSo,\n\nfor some constant \"μ\", which must be equal to 1 by the normalization property.\n\n\n"}
{"id": "37822732", "url": "https://en.wikipedia.org/wiki?curid=37822732", "title": "History of network traffic models", "text": "History of network traffic models\n\nDesign of robust and reliable networks and network services relies on an understanding of the traffic characteristics of the network. Throughout history, different models of network traffic have been developed and used for evaluating existing and proposed networks and services.\n\nDemands on computer networks are not entirely predictable. Performance modeling is necessary for deciding the quality of service (QoS) level. Performance models in turn, require accurate traffic models that have the ability to capture the statistical characteristics of the actual traffic on the network. Many traffic models have been developed based on traffic measurement data. If the underlying traffic models do not efficiently capture the characteristics of the actual traffic, the result may be the under-estimation or over-estimation of the performance of the network. This impairs the design of the network. Traffic models are hence, a core component of any performance evaluation of networks and they need to be very accurate.\n\n“Teletraffic theory is the application of mathematics to the measurement, modeling, and control of traffic in telecommunications networks. The aim of traffic modeling is to find stochastic processes to represent the behavior of traffic. Working at the Copenhagen Telephone Company in the 1910s, A. K. Erlang famously characterized telephone traffic at the call level by certain probability distributions for arrivals of new calls and their holding times. Erlang applied the traffic models to estimate the telephone switch capacity needed to achieve a given call blocking probability. The Erlang blocking formulas had tremendous practical interest for public carriers because telephone facilities (switching and transmission) involved considerable investments. Over several decades, Erlang’s work stimulated the use of queuing theory, and applied probability in general, to engineer the public switched telephone network. Teletraffic theory for packet networks has seen considerable progress in recent decades. Significant advances have been made in long-range dependence, wavelet, and multifractal approaches. At the same time, traffic modeling continues to be challenged by evolving network technologies and new multimedia applications. For example, wireless technologies allow greater mobility of users. Mobility must be an additional consideration for modeling traffic in wireless networks. Traffic modeling is an ongoing process without a real end. Traffic models represent our best current understanding of traffic behavior, but our understanding will change and grow over time.”\n\nMeasurements are useful and necessary for verifying the actual network performance. However, measurements do not have the level of abstraction that makes traffic models useful. Traffic models can be used for hypothetical problem solving whereas traffic measurements only reflect current reality. In probabilistic terms, a traffic trace is a realization of a random process, whereas a traffic model is a random process. Thus, traffic models have universality. A traffic trace gives insight about a particular traffic source, but a traffic model gives insight about all traffic sources of that type. Traffic models have three major uses. One important use of traffic models is to properly dimension network resources for a target level of QoS. It was mentioned earlier that Erlang developed models of voice calls to estimate telephone switch capacity to achieve a target call blocking probability. Similarly, models of packet traffic are needed to estimate the bandwidth and buffer resources to provide acceptable packet delays and packet loss probability. Knowledge of the average traffic rate is not sufficient. It is known from queuing theory that queue lengths increase with the variability of traffic. Hence, an understanding of traffic burstiness or variability is needed to determine sufficient buffer sizes at nodes and link capacities. A second important use of traffic models is to verify network performance under specific traffic controls. For example, given a packet scheduling algorithm, it would be possible to evaluate the network performance resulting from different traffic scenarios. For another example, a popular area of research is new improvements to the TCP congestion avoidance algorithm. It is critical that any algorithm is stable and allows multiple hosts to share bandwidth fairly, while sustaining a high throughput. Effective evaluation of the stability, fairness, and throughput of new algorithms would not be possible without realistic source models. A third important use of traffic models is admission control. In particular, connection oriented networks such as ATM depends on admission control to block new connections to maintain QOS guarantees. A simple admission strategy could be based on the peak rate of a new connection; a new connection is admitted if the available bandwidth is greater than the peak rate. However, that strategy would be overly conservative because a variable bit-rate connection may need significantly less bandwidth than its peak rate. A more sophisticated admission strategy is based on effective bandwidths. The source traffic behavior is translated into an effective bandwidth between the peak rate and average rate, which is the specific amount of bandwidth required to meet a given QoS constraint. The effective bandwidth depends on the variability of the source.\n\nTraffic modeling consists of three steps:\nParameter estimation is based on a set of statistics (e.g. mean, variance, density function or auto covariance function, multifractal characteristics) that are measured or calculated from observed data. The set of statistics used in the inference process depends on the impact they may have in the main performance metrics of interest.\n\nIn recent years several types of traffic behavior, that can have significant impact on network performance, were discovered: long-range dependence, self-similarity and, more recently, multifractality.\nThere are two major parameters generated by network traffic models: packet length distributions and packet inter-arrival distributions. Other parameters, such as routes, distribution of destinations, etc., are of less importance. Simulations that use traces generated by network traffic models usually examine a single node in the network, such as a router or switch; factors that depend on specific network topologies or routing information are specific to those topologies and simulations. The problem of packet size distribution is fairly well-understood today. Existing models of packet sizes have proven to be valid and simple. Most packet size models do not consider the problem of order in packet sizes. For example, a TCP datagram in one direction is likely to be followed by a tiny ACK in the other direction about half of one Round-Trip Time (RTT) later. The problem of packet inter-arrival distribution is much more difficult. Understanding of network traffic has evolved significantly over the years, leading to a series of evolutions in network traffic models.\n\nOne of the earliest objections to self-similar traffic models was the difficulty in mathematical analysis. Existing self-similar models could not be used in conventional queuing models. This limitation was rapidly overturned and workable models were constructed. Once basic self-similar models became feasible, the traffic modeling community settled into the “detail” concerns. TCP’s congestion control algorithm complicated the matter of modeling traffic, so solutions needed to be created. Parameter estimation of self-similar models was always difficult, and recent research addresses ways to model network traffic without fully understanding it.\n\nWhen self-similar traffic models were first introduced, there were no efficient, analytically tractable processes to generate the models. Ilkka Norros devised a stochastic process for a storage model with self-similar input and constant bit-rate output. While this initial model was continuous rather than discrete, the model was effective, simple, and attractive.\nAll self-similar traffic models suffer from one significant drawback: estimating the self-similarity parameters from real network traffic requires huge amounts of data and takes extended computation. The most modern method, wavelet multi-resolution analysis, is more efficient, but still very costly. This is undesirable in a traffic model. SWING uses a surprisingly simple model for the network traffic analysis and generation. The model examines characteristics of users, Request-Response Exchanges (RREs), connections, individual packets, and the overall network. No attempt is made to analyze self-similarity characteristics; any self-similarity in the generated traffic comes naturally from the aggregation of many ON/OFF sources.\nThe Pareto distribution process produces independent and identically distributed (IID) inter-arrival times. In general if X is a random variable with a Pareto distribution, then the probability that X is greater than some number x is given by P(X > x) = (x/x_m)-k for all x ≥ x_m where k is a positive parameter and x_m is the minimum possible value of Xi The probability distribution and the density functions are represented as:\nF(t) = 1 – (α/t)β where α,β ≥ 0 & t ≥ α\nf(t) = βαβ t-β-1\nThe parameters β and α are the shape and location parameters, respectively. The Pareto distribution is applied to model self-similar arrival in packet traffic. It is also referred to as double exponential, power law distribution. Other important characteristics of the model are that the Pareto distribution has infinite variance, when β ≥ 2 and achieves infinite mean, when β ≤ 1.\nThe Weibull distributed process is heavy-tailed and can model the fixed rate in ON period and ON/OFF period lengths, when producing self-similar traffic by multiplexing ON/OFF sources. The distribution function in this case is given by:\nF(t) = 1 – e-(t/β)α t > 0\nand the density function of the weibull distribution is given as:\nf(t) = αβ-α tα-1 e -(t/β)α t > 0\nwhere parameters β ≥ 0 and α > 0 are the scale and location parameters respectively.\nThe Weibull distribution is close to a normal distribution. For β ≤ 1 the density function of the distribution is L shaped and for values of β > 1, it is bell shaped. This distribution gives a failure rate increasing with time. For β > 1, the failure rate decreases with time. At, β = 1, the failure rate is constant and the lifetimes are exponentially distributed.\nThe Autoregressive model is one of a group of linear prediction formulas that attempt to predict an output y_n of a system based on previous set of outputs {y_k} where k < n and inputs x_n and {x_k} where k < n. There exist minor changes in the way the predictions are computed based on which, several variations of the model are developed. Basically, when the model depends only on the previous outputs of the system, it is referred to as an auto-regressive model. It is referred to as a Moving Average Model (MAM), if it depends on only the inputs to the system. Finally, Autoregressive-Moving Average models are those that depend both on the inputs and the outputs, for prediction of current output. Autoregressive model of order p, denoted as AR(p), has the following form:\nXt = R1 Xt-1 + R2 Xt-2 + ... + Rp Xt-p + Wt\nwhere Wt is the white noise, Ri are real numbers and Xt are prescribed correlated random numbers. The auto-correlation function of the AR(p) process consists of damped sine waves depending on whether the roots (solutions) of the model are real or imaginary. Discrete Autoregressive Model of order p, denoted as DAR(p), generates a stationary sequence of discrete random variables with a probability distribution and with an auto-correlation structure similar to that of the Autoregressive model of order p.[3]\nRegression models define explicitly the next random variable in the sequence by previous ones within a specified time window and a moving average of a white noise.[5]\nTransform-expand-sample (TES) models are non-linear regression models with modulo-1 arithmetic. They aim to capture both auto-correlation and marginal distribution of empirical data. TES models consist of two major TES processes: TES+ and TES–. TES+ produces a sequence which has positive correlation at lag 1, while TES– produces a negative correlation at lag 1.\n\nEarly traffic models were derived from telecommunications models and focused on simplicity of analysis. They generally operated under the assumption that aggregating traffic from a large number of sources tended to smooth out bursts; that burstiness decreased as the number of traffic sources increased.\nOne of the most widely used and oldest traffic models is the Poisson Model. The memoryless Poisson distribution is the predominant model used for analyzing traffic in traditional telephony networks. The Poisson process is characterized as a renewal process. In a Poisson process the inter-arrival times are exponentially distributed with a rate parameter λ: P{An ≤ t} = 1 – exp(-λt). The Poisson distribution is appropriate if the arrivals are from a large number of independent sources, referred to as Poisson sources. The distribution has a mean and variance equal to the parameter λ.\nThe Poisson distribution can be visualized as a limiting form of the binomial distribution, and is also used widely in queuing models. There are a number of interesting mathematical properties exhibited by Poisson processes. Primarily, superposition of independent Poisson processes results in a new Poisson process whose rate is the sum of the rates of the independent Poisson processes. Further, the independent increment property renders a Poisson process memoryless. Poisson processes are common in traffic applications scenarios that consist of a large number of independent traffic streams. The reason behind the usage stems from Palm's Theorem which states that under suitable conditions, such large number of independent multiplexed streams approach a Poisson process as the number of processes grows, but the individual rates decrease in order to keep the aggregate rate constant. Nevertheless, it is to be noted that traffic aggregation need not always result in a Poisson process. The two primary assumptions that the Poisson model makes are:\n1. The number of sources is infinite\n2. The traffic arrival pattern is random.\nIn the compound Poisson model, the base Poisson model is extended to deliver batches of packets at once. The inter-batch arrival times are exponentially distributed, while the batch size is geometric Mathematically, this model has two parameters, λ, the arrival rate, and ρ in (0,1), the batch parameter. Thus, the mean number of packets in a batch is 1/ ρ, while the mean inter-batch arrival time is 1/ λ. Mean packet arrivals over time period t are tλ/ ρ.\nThe compound Poisson model shares some of the analytical benefits of the pure Poisson model: the model is still memoryless, aggregation of streams is still (compound) Poisson, and the steady-state equation is still reasonably simple to calculate, although varying batch parameters for differing flows would complicate the derivation.\nMarkov models attempt to model the activities of a traffic source on a network, by a finite number of states. The accuracy of the model increases linearly with the number of states used in the model. However, the complexity of the model also increases proportionally with increasing number of states. An important aspect of the Markov model - the Markov Property, states that the next (future) state depends only on the current state. In other words, the probability of the next state, denoted by some random variable Xn+1, depends only on the current state, indicated by Xn, and not on any other state Xi, where i<n. The set of random variables referring to different states {Xn} is referred to as a Discrete Markov Chain.\nAnother attempt at providing a bursty traffic model is found in Jain and Routhier’s Packet Trains model. This model was principally designed to recognize that address locality applies to routing decisions; that is, packets that arrive near each other in time are frequently going to the same destination. In generating a traffic model that allows for easier analysis of locality, the authors created the notion of packet trains, a sequence of packets from the same source, traveling to the same destination (with replies in the opposite direction). Packet trains are optionally sub-divided into tandem trailers. Traffic between a source and a destination usually consists of a series of messages back and forth. Thus, a series of packets go one direction, followed by one or more reply packets, followed by a new series in the initial direction. Traffic quantity is then a superposition of packet trains, which generates substantial bursty behavior. This refines the general conception of the compound Poisson model, which recognized that packets arrived in groups, by analyzing why they arrive in groups, and better characterizing the attributes of the group. Finally, the authors demonstrate that packet arrival times are not Poisson distributed, which led to a model that departs from variations on the Poisson theme. The packet train model is characterized by the following parameters and their associated probability distributions:\nThe train model is designed for analyzing and categorizing real traffic, not for generating synthetic loads for simulation. Thus, little claim has been made about the feasibility of packet trains for generating synthetic traffic. Given accurate parameters and distributions, generation should be straightforward, but derivation of these parameters is not addressed.\n\nNS-2 is a popular network simulator; PackMimeHTTP is a web traffic generator for NS-2, published in 2004. It does take long-range dependencies into account, and uses the Weibull distribution. Thus, it relies on heavy tails to emulate true self-similarity. Over most time scales, the effort is a success; only a long-running simulation would allow a distinction to be drawn. This follows suggestions from where it is suggested that self-similar processes can be represented as a superposition of many sources each individually modeled with a heavy-tailed distribution. It is clear that self-similar traffic models are in the mainstream.\n\n\n"}
{"id": "9282128", "url": "https://en.wikipedia.org/wiki?curid=9282128", "title": "Hypoelliptic operator", "text": "Hypoelliptic operator\n\nIn the theory of partial differential equations, a partial differential operator formula_1 defined on an open subset \n\nis called hypoelliptic if for every distribution formula_3 defined on an open subset formula_4 such that formula_5 is formula_6 (smooth), formula_3 must also be formula_6.\n\nIf this assertion holds with formula_6 replaced by real analytic, then formula_1 is said to be \"analytically hypoelliptic\". \n\nEvery elliptic operator with formula_6 coefficients is hypoelliptic. In particular, the Laplacian is an example of a hypoelliptic operator (the Laplacian is also analytically hypoelliptic). The heat equation operator \n\n(where formula_13) is hypoelliptic but not elliptic. The wave equation operator \n\n(where formula_15) is not hypoelliptic.\n\n"}
{"id": "8088912", "url": "https://en.wikipedia.org/wiki?curid=8088912", "title": "Imaginary line (mathematics)", "text": "Imaginary line (mathematics)\n\nIn complex geometry, an imaginary line is a straight line that only contains one real point. It can be proven that this point is the intersection point with the conjugated line.\n\nIt is a special case of an imaginary curve.\n\nAn imaginary line is found in the complex projective plane P(C) where points are represented by three homogeneous coordinates formula_1\n\nBoyd Patterson described the lines in this plane:\n\nFelix Klein described imaginary geometrical structures: \"We will characterize a geometric structure as imaginary if its coordinates are not all real.:\n\nAccording to Hatton:\nHatton continues,\n\n\n"}
{"id": "52232960", "url": "https://en.wikipedia.org/wiki?curid=52232960", "title": "Individual pieces set", "text": "Individual pieces set\n\nIn the theory of fair cake-cutting, the individual-pieces set (IPS)\nis a geometric object that represents all possible utility vectors in cake partitions.\n\nSuppose we have a cake made of four parts. There are two people, Alice and George, with different tastes: each person values the different parts of the cake differently. The table below describes the parts and their values.\n\nThe cake can be divided in various ways. Each division (Alice's-piece, George's-piece) yields a different utility vector (Alice's utility, George's utility). The IPS is the set of utility vectors of all possible partitions.\n\nThe IPS for the example cake is shown on the right.\n\nThe IPS is a convex set and a compact set. This follows from the Dubins–Spanier theorems.\n\nWith two agents, the IPS is symmetric across the middle point (in this case it is the point (15,15)). Take some int formula_1 on the IPS. This point comes from some partition. Swap the pieces between Alice and George. Then, Alice's new utility is 30 minus her previous utility, and George's new utility is 30 minus his previous utility, so the symmetric point formula_2 is also on the IPS.\n\nThe top-right boundary of the IPS is the Pareto frontier – it is the set of all Pareto efficient partitions. With two agents, this frontier can be constructed in the following way:\n\nThe IPS was introduced as part of the Dubins–Spanier theorems and used in the proof of Weller's theorem. The term \"Individual Pieces set\" was coined by Julius Barbanel.\n\n"}
{"id": "11969092", "url": "https://en.wikipedia.org/wiki?curid=11969092", "title": "Italian Mathematical Union", "text": "Italian Mathematical Union\n\nThe Italian Mathematical Union () is a mathematics society based in Italy. It was founded on December 7, 1922 by Luigi Bianchi, Vito Volterra, and most notably, Salvatore Pincherle, who became the Union's first President. The Union's journal is the \"Bollettino dell'Unione Matematica Italiana\", which contains two sections: one for research papers, and one for expository articles.\n\nThe Italian mathematical union awards the Bartolozzi Prize, the Caccioppoli Prize and the Stampacchia Medal.\n\n"}
{"id": "22256304", "url": "https://en.wikipedia.org/wiki?curid=22256304", "title": "K-frame", "text": "K-frame\n\nIn linear algebra, a branch of mathematics, a k\"-frame is an ordered set of \"k\" linearly independent vectors in a space; thus \"k\" ≤ \"n\", where \"n\" is the dimension of the vector space, and if \"k\" = \"n\" an n\"-frame is precisely an ordered basis.\n\nIf the vectors are orthogonal, or orthonormal, the frame is called an orthogonal frame, or orthonormal frame, respectively.\n\n\n\n"}
{"id": "1367040", "url": "https://en.wikipedia.org/wiki?curid=1367040", "title": "Lambert series", "text": "Lambert series\n\nIn mathematics, a Lambert series, named for Johann Heinrich Lambert, is a series taking the form\n\nIt can be resummed formally by expanding the denominator:\n\nwhere the coefficients of the new series are given by the Dirichlet convolution of \"a\" with the constant function 1(\"n\") = 1:\n\nThis series may be inverted by means of the Möbius inversion formula, and is an example of a Möbius transform.\n\nSince this last sum is a typical number-theoretic sum, almost any natural multiplicative function will be exactly summable when used in a Lambert series. Thus, for example, one has\n\nwhere formula_5 is the number of positive divisors of the number \"n\".\n\nFor the higher order sigma functions, one has\n\nwhere formula_7 is any complex number and\n\nis the divisor function.\n\nAdditional Lambert series related to the previous identity include those for the variants of the \nMöbius function given below formula_9 \n\nRelated Lambert series over the Moebius function include the following identities for any \nprime formula_11:\nThe proof of the first identity above follows from a multi-section (or bisection) identity of these \nLambert series generating functions in the following form where we denote \nformula_13 to be the Lambert series generating function of the arithmetic function \"f\": \nThe second identity in the previous equations follows from the fact that the coefficients of the left-hand-side sum are given by \nwhere the function formula_16 is the multiplicative inverse with respect to the operation of Dirichlet convolution of arithmetic functions. \n\nFor Euler's totient function formula_17:\n\nFor Liouville's function formula_19:\n\nwith the sum on the right similar to the Ramanujan theta function, or Jacobi theta function formula_21. Note that Lambert series in which the \"a\" are trigonometric functions, for example, \"a\" = sin(2\"n\" \"x\"), can be evaluated by various combinations of the logarithmic derivatives of Jacobi theta functions.\n\nGenerally speaking, we can extend the previous generating function expansion by letting formula_22 denote the characteristic function of the formula_23 powers, formula_24, for positive natural numbers formula_25 and defining the generalized \"m\"-Liouville lambda function to be the arithmetic function satisfying formula_26. This definition of formula_27 clearly implies that formula_28, which in turn shows that \n\nWe also have a slightly more generalized Lambert series expansion generating the sum of squares function formula_30 in the form of \n\nIn general, if we write the Lambert series over formula_32 which generates the arithmetic functions formula_33, the next pairs of functions correspond to other well-known convolutions expressed by their Lambert series generating functions in the forms of \nwhere formula_16 is the multiplicative identity for Dirichlet convolutions, formula_36 is the identity function for formula_37 powers, formula_38 denotes the characteristic function for the squares, formula_39 which counts the number of distinct prime factors of formula_40 (see prime omega function), formula_41 is Jordan's totient function, and formula_42 is the divisor function (see Dirichlet convolutions).\n\nSubstituting formula_43 one obtains another common form for the series, as\n\nwhere\n\nas before. Examples of Lambert series in this form, with formula_46, occur in expressions for the Riemann zeta function for odd integer values; see Zeta constants for details.\n\nIn the literature we find \"Lambert series\" applied to a wide variety of sums. For example, since formula_47 is a polylogarithm function, we may refer to any sum of the form\n\nas a Lambert series, assuming that the parameters are suitably restricted. Thus\n\nwhich holds for all complex \"q\" not on the unit circle, would be considered a Lambert series identity. This identity follows in a straightforward fashion from some identities published by the Indian mathematician S. Ramanujan. A very thorough exploration of Ramanujan's works can be found in the works by Bruce Berndt.\n\nA somewhat newer construction recently published over 2017–2018 relates to so-termed \"Lambert series factorization theorems\" of the form \n\nwhere formula_51 is the respective sum or difference of the \nrestricted partition functions formula_52 which denote the number of formula_53's in all partitions of formula_40 into an \"even\" (respectively, \"odd\") number of distinct parts. Let formula_55 denote the invertible lower triangular sequence whose first few values are shown in the table below.\n\nAnother characteristic form of the Lambert series factorization theorem expansions is given by \n\nwhere formula_57 is the (infinite) q-Pochhammer symbol. The invertible matrix products on the right-hand-side of the previous equation correspond to inverse matrix products whose lower triangular entries are given in terms of the partition function and the Möbius function by the divisor sums\n\nThe next table lists the first several rows of these corresponding inverse matrices.\n\nWe let formula_59 denote the sequence of interleaved pentagonal numbers, i.e., so that the pentagonal number theorem is expanded in the form of\n\nThen for any Lambert series formula_61 generating the sequence of formula_62, we have the corresponding inversion relation of the factorization theorem expanded above given by \n\nThis work on Lambert series factorization theorems is extended in to more general expansions of the form\n\nwhere formula_65 is any (partition-related) reciprocal generating function, formula_66 is any arithmetic function, and where the \nmodified coefficients are expanded by\n\nThe corresponding inverse matrices in the above expansion satisfy\n\nso that as in the first variant of the Lambert factorization theorem above we obtain an inversion relation for the right-hand-side coefficients of the form\n\nWithin this section we define the following functions for natural numbers formula_70: \n\nWe also adopt the notation from the previous section that\n\nwhere formula_57 is the infinite q-Pochhammer symbol. Then we have the following recurrence relations for involving these functions and the pentagonal numbers proved in:\n\nDerivatives of a Lambert series can be obtained by differentiation of the series termwise with respect to formula_77. We have the following identities for the termwise formula_78 derivatives of a Lambert series for any formula_79 \n\nwhere the bracketed triangular coefficients in the previous equations denote the Stirling numbers of the first and second kinds. \nWe also have the next identity for extracting the individual coefficients of the terms implicit to the previous expansions given in the form of\n\nNow if we define the functions formula_83 for any formula_84 by\n\nwhere formula_86 denotes Iverson's convention, then we have the coefficients for the formula_87 derivatives of a Lambert series \ngiven by\n\nOf course, by a typical argument purely by operations on formal power series we also have that\n\n\n"}
{"id": "4730929", "url": "https://en.wikipedia.org/wiki?curid=4730929", "title": "Levenshtein coding", "text": "Levenshtein coding\n\nLevenstein coding, or Levenshtein coding, is a universal code encoding the non-negative integers developed by Vladimir Levenshtein.\n\nThe code of zero is \"0\"; to code a positive number:\n\nThe code begins:\n\nTo decode a Levenstein-coded integer:\n\nThe Levenstein code of a positive integer is always one bit longer than the Elias omega code of that integer. However, there is a Levenstein code for zero, whereas Elias omega coding would require the numbers to be shifted so that a zero is represented by the code for one instead.\n\n"}
{"id": "353748", "url": "https://en.wikipedia.org/wiki?curid=353748", "title": "List of computability and complexity topics", "text": "List of computability and complexity topics\n\nThis is a list of computability and complexity topics, by Wikipedia page.\n\nComputability theory is the part of the theory of computation that deals with what can be computed, in principle. Computational complexity theory deals with how hard computations are, in quantitative terms, both with upper bounds (algorithms whose complexity in the worst cases, as use of computing resources, can be estimated), and from below (proofs that no procedure to carry out some task can be very fast).\n\nFor more abstract foundational matters, see the list of mathematical logic topics. See also list of algorithms, list of algorithm general topics.\n\n\n\n\n\n\n\"See the list of complexity classes\"\n\n\n\n"}
{"id": "24526791", "url": "https://en.wikipedia.org/wiki?curid=24526791", "title": "Localization theorem", "text": "Localization theorem\n\nIn mathematics, particularly in integral calculus, the localization theorem allows, under certain conditions, to infer the nullity of a function given only information about its continuity and the value of its integral.\n\nLet be a real-valued function defined on some open interval Ω of the real line that is continuous in Ω. Let D be an arbitrary subinterval contained in Ω. The theorem states the following implication:\n\nA simple proof is as follows: if there were a point x within Ω for which , then the continuity of would require the existence of a neighborhood of x in which the value of was nonzero, and in particular of the same sign than in x. Since such a neighborhood N, which can be taken to be arbitrarily small, must however be of a nonzero width on the real line, the integral of over N would evaluate to a nonzero value. However, since x is part of the \"open\" set Ω, all neighborhoods of x smaller than the distance of x to the frontier of Ω are included within it, and so the integral of over them must evaluate to zero. Having reached the contradiction that must be both zero and nonzero, the initial hypothesis must be wrong, and thus there is no x in Ω for which .\n\nThe theorem is easily generalized to multivariate functions, replacing intervals with the more general concept of connected open sets, that is, domains, and the original function with some , with the constraints of continuity and nullity of its integral over any subdomain . The proof is completely analogous to the single variable case, and concludes with the impossibility of finding a point such that .\n\nAn example of the use of this theorem in physics is the law of conservation of mass for fluids, which states that the mass of any fluid volume must not change:\n\nApplying the Reynolds transport theorem, one can change the reference to an arbitrary (non-fluid) control volume V. Further assuming that the density function is continuous (i.e. that our fluid is monophasic and thermodynamically metastable) and that V is not moving relative to the chosen system of reference, the equation becomes:\n\nAs the equation holds for \"any\" such control volume, the localization theorem applies, rendering the common partial differential equation for the conservation of mass in monophase fluids:\n"}
{"id": "1809181", "url": "https://en.wikipedia.org/wiki?curid=1809181", "title": "Mathematical structure", "text": "Mathematical structure\n\nIn mathematics, a structure on a set is an additional mathematical object that, in some manner, attaches (or relates) to that set to endow it with some additional meaning or significance.\n\nA partial list of possible structures are measures, algebraic structures (groups, fields, etc.), topologies, metric structures (geometries), orders, events, equivalence relations, differential structures, and categories.\n\nSometimes, a set is endowed with more than one structure simultaneously; this enables mathematicians to study it more richly. For example, an ordering imposes a rigid form, shape, or topology on the set. As another example, if a set has both a topology and is a group, and these two structures are related in a certain way, the set becomes a topological group.\n\nMappings between sets which preserve structures (so that structures in the source or domain are mapped to equivalent structures in the destination or codomain) are of special interest in many fields of mathematics. Examples are homomorphisms, which preserve algebraic structures; homeomorphisms, which preserve topological structures; and diffeomorphisms, which preserve differential structures.\n\nIn 1939, the French group with the pseudonym Nicolas Bourbaki saw structures as the root of mathematics. They first mentioned them in their \"Fascicule\" of \"Theory of Sets\" and expanded it into Chapter IV of the 1957 edition. They identified three \"mother structures\": algebraic, topological, and order.\n\nThe set of real numbers has several standard structures:\nThere are interfaces among these:\n\n\n\n"}
{"id": "13799863", "url": "https://en.wikipedia.org/wiki?curid=13799863", "title": "Membrane computing", "text": "Membrane computing\n\nMembrane computing (or MC) is an area within computer science that seeks to discover new computational models from the study of biological cells, particularly of the cellular membranes. It is a sub-task of creating a cellular model.\n\nMembrane computing deals with distributed and parallel computing models, processing multisets of symbol objects in a localized manner. Thus, evolution rules allow for evolving objects to be encapsulated into compartments defined by membranes. The communications between compartments and with the environment play an essential role in the processes. The various types of membrane systems are known as P systems after Gheorghe Păun who first conceived the model in 1998. \n\nAn essential ingredient of a P system is its membrane structure, which can be a hierarchical arrangement of membranes, as in a cell, or a net of membranes (placed in the nodes of a graph), as in a tissue or a neural net. P systems are often depicted graphically with drawings.\n\nThe intuition behind the notion of a membrane is a three-dimensional vesicle from biology. However the concept itself is more general, and a membrane is seen as a separator of two regions. The membrane provides for selective communication between the two regions. As per Gheorghe Păun, the separation is of the Euclidean space into a finite “inside” and an infinite “outside”. The selective communication is where the computing comes in. \n\nGraphical representations may have numerous elements, according to the variation of the model that is being studied. For example, a rule may produce the special symbol δ, in which case the membrane that contains it is dissolved and all its contents move up in the region hierarchy.\n\nThe variety of suggestions from biology and the range of possibilities to define the architecture and the functioning of a membrane-based multiset processing device are practically endless. Indeed the membrane computing literature contains a very large number of models. Thus, MC is not merely a theory related to a specific model, it is a framework for devising compartmentalized models.\n\nChemicals are modeled by symbols, or alternatively by strings of symbols. The region, which is defined by a membrane, can contain other symbols or strings (collectively referred to as objects) or other membranes, so that a P system has exactly one outer membrane, called the skin membrane, and a hierarchical relationship governing all its membranes under the skin membrane. \n\nIf objects are symbols, then their multiplicity within a region matters; however multi-sets are also used in some string models. Regions have associated rules that define how objects are produced, consumed, passed to other regions and otherwise interact with one another. The nondeterministic maximally parallel application of rules throughout the system is a transition between system states, and a sequence of transitions is called a computation. Particular goals can be defined to signify a halting state, at which point the result of the computation would be the objects contained in a particular region. Alternatively the result may be made up of objects sent out of the skin membrane to the environment.\n\nMany variant models have been studied, and interest has focused on proving computational universality for systems with a small number of membranes, for the purpose of solving NP-complete problems such as Boolean satisfiability (SAT) problems and the traveling salesman problem (TSP). The P systems may trade space and time complexities and less often use models to explain natural processes in living cells. The studies devise models that may at least theoretically be implemented on hardware. To date, the P systems are nearly all theoretical models that have never been reduced to practice, although a practical system is given in.\n\n"}
{"id": "57481237", "url": "https://en.wikipedia.org/wiki?curid=57481237", "title": "Modeshape", "text": "Modeshape\n\nIn applied mathematics, mode shapes are a manifestation of eigenvectors which describe the relative displacement of two or more elements in a mechanical system or wave front .\n\n"}
{"id": "47491138", "url": "https://en.wikipedia.org/wiki?curid=47491138", "title": "Mohammad Hajiaghayi", "text": "Mohammad Hajiaghayi\n\nMohammad Taghi Hajiaghayi (; born in 1979) is a computer scientist known for his work in algorithms, game theory, social networks, network design, graph theory, and big data. More specifically he has designed numerous algorithms and taught classes in the areas of approximation algorithms, fixed-parameter algorithms, algorithmic game theory, algorithmic graph theory, online algorithms, and streaming algorithms. He has over 200 publications with over 185 collaborators and 10 issued patents.\n\nHe is currently the Jack and Rita G. Minker (full) Professor at the University of Maryland Department of Computer Science.\n\nMohammad Hajiaghayi was born in 1979 in Qazvin, Iran. His parents were both K-12 teachers. He went to high school at Shahid Babaee High School (Qazvin Sampad), National Organization for Development of Exceptional Talents (NODET).\n\nIn 1997, Hajiaghayi won a silver medal in the International Olympiad in Informatics.\n\nHajiaghayi received his BSc with highest distinction in Computer Engineering from Sharif University of Technology in 2000 (in three years),\nhis MSc in Computer Science from the University of Waterloo in 2001, and his PhD in applied mathematics and computer science from Massachusetts Institute of Technology in 2005 advised by Erik Demaine and F. Thomson Leighton.\n\nHis thesis was titled \"The Bidimensionality Theory and Its Algorithmic Applications\" founded the theory of bidimensionality which later received the Nerode Prize and was subject of studies in a few workshops.\n\nHe was a post-doc at the Carnegie Mellon School of Computer Science and MIT Computer Science and Artificial Intelligence Laboratory as well as a researcher at AT&T Labs—Research.\n\nHajiaghayi was hired as an assistant professor at the University of Maryland in the Fall of 2010. He was promoted to an associate professor with tenure in 2012 and then to a full professor in 2016. He has served on the program committees of numerous conferences as well as editorial boards of several journals.\n\nHajiaghayi has been the coach of the \nUniversity of Maryland ACM International Collegiate Programming team in The World Finals.\n\nIn 2014, Hajiaghayi along with Seddighin introduced a new CS Theory Ranking based on publications in theory conferences.\n\nHajiaghayi's has received National Science Foundation CAREER Award (2010), Office of Naval Research Young Investigator Award (2011), University of Maryland Graduate Faculty Mentor of the Year Award (2015), as well as Google Faculty Research Awards (2010 & 2014). So far Hajiaghayi has raised more than \"3.4\" million dollars in terms of grant award money from government and industry since joining the University of Maryland.\n\nWith his co-authors Erik Demaine, Fedor Fomin, and Dimitrios Thilikos, he received the 2015 European Association for Theoretical Computer Science Nerode Prize for his work (also the topic of his Ph.D. thesis) on bidimensionality, a general technique for developing both fixed-parameter tractable exact algorithms and approximation algorithms for a wide class of algorithmic problems on graphs.\n\n"}
{"id": "48260", "url": "https://en.wikipedia.org/wiki?curid=48260", "title": "Monotonic function", "text": "Monotonic function\n\nIn mathematics, a monotonic function (or monotone function) is a function between ordered sets that preserves or reverses the given order. This concept first arose in calculus, and was later generalized to the more abstract setting of order theory.\n\nIn calculus, a function formula_1 defined on a subset of the real numbers with real values is called monotonic if and only if it is either entirely non-increasing, or entirely non-decreasing. That is, as per Fig. 1, a function that increases monotonically does not exclusively have to increase, it simply must not decrease.\n\nA function is called monotonically increasing (also increasing or non-decreasing), if for all formula_2 and formula_3 such that formula_4 one has formula_5, so formula_1 preserves the order (see Figure 1). Likewise, a function is called monotonically decreasing (also decreasing or non-increasing) if, whenever formula_4, then formula_8, so it \"reverses\" the order (see Figure 2).\n\nIf the order formula_9 in the definition of monotonicity is replaced by the strict order formula_10, then one obtains a stronger requirement. A function with this property is called strictly increasing. Again, by inverting the order symbol, one finds a corresponding concept called strictly decreasing. Functions that are strictly increasing or decreasing are one-to-one (because for formula_2 not equal to formula_3, either formula_13 or formula_14 and so, by monotonicity, either formula_15 or formula_16, thus formula_17 is not equal to formula_18.)\n\nIf it is not clear that \"increasing\" and \"decreasing\" are taken to include the possibility of repeating the same value at successive arguments, one may use the terms weakly increasing and weakly decreasing to stress this possibility.\n\nThe terms \"non-decreasing\" and \"non-increasing\" should not be confused with the (much weaker) negative qualifications \"not decreasing\" and \"not increasing\". For example, the function of figure 3 first falls, then rises, then falls again. It is therefore not decreasing and not increasing, but it is neither non-decreasing nor non-increasing.\n\nA function formula_17 is said to be absolutely monotonic over an interval formula_20 if the derivatives of all orders of formula_1 are nonnegative or all nonpositive at all points on the interval.\n\nThe term monotonic transformation (or monotone transformation) can also possibly cause some confusion because it refers to a transformation by a strictly increasing function. This is the case in economics with respect to the ordinal properties of a utility function being preserved across a monotonic transform (see also monotone preferences). In this context, what we are calling a \"monotonic transformation\" is, more accurately, called a \"positive monotonic transformation\", in order to distinguish it from a “negative monotonic transformation,” which reverses the order of the numbers.\n\nThe following properties are true for a monotonic function formula_22:\n\nThese properties are the reason why monotonic functions are useful in technical work in analysis. Two facts about these functions are:\n\nAn important application of monotonic functions is in probability theory. If formula_42 is a random variable, its cumulative distribution function formula_43 is a monotonically increasing function.\n\nA function is \"unimodal\" if it is monotonically increasing up to some point (the \"mode\") and then monotonically decreasing.\n\nWhen formula_1 is a \"strictly monotonic\" function, then formula_1 is injective on its domain, and if formula_46 is the range of formula_1, then there is an inverse function on formula_46 for formula_1.\n\nA map \"formula_50\" is said to be monotone if each of its fibers is connected i.e. for each element \"formula_3\" in formula_52 the (possibly empty) set formula_53 is connected.\n\nIn functional analysis on a topological vector space formula_42, a (possibly non-linear) operator formula_55 is said to be a monotone operator if\n\nKachurovskii's theorem shows that convex functions on Banach spaces have monotonic operators as their derivatives.\n\nA subset formula_57 of formula_58 is said to be a monotone set if for every pair formula_59 and formula_60 in formula_57,\n\nformula_57 is said to be maximal monotone if it is maximal among all monotone sets in the sense of set inclusion. The graph of a monotone operator formula_64 is a monotone set. A monotone operator is said to be maximal monotone if its graph is a maximal monotone set.\n\nOrder theory deals with arbitrary partially ordered sets and preordered sets in addition to real numbers. The above definition of monotonicity is relevant in these cases as well. However, the terms \"increasing\" and \"decreasing\" are avoided, since their conventional pictorial representation does not apply to orders that are not total. Furthermore, the strict relations < and > are of little use in many non-total orders and hence no additional terminology is introduced for them.\n\nA monotone function is also called isotone, or . The dual notion is often called antitone, anti-monotone, or order-reversing. Hence, an antitone function \"f\" satisfies the property\n\nfor all \"x\" and \"y\" in its domain. The composite of two monotone mappings is also monotone.\n\nA constant function is both monotone and antitone; conversely, if \"f\" is both monotone and antitone, and if the domain of \"f\" is a lattice, then \"f\" must be constant.\n\nMonotone functions are central in order theory. They appear in most articles on the subject and examples from special applications are found in these places. Some notable special monotone functions are order embeddings (functions for which \"x\" ≤ \"y\" if and only if \"f\"(\"x\") ≤ \"f\"(\"y\")) and order isomorphisms (surjective order embeddings).\n\nIn the context of search algorithms monotonicity (also called consistency) is a condition applied to heuristic functions. A heuristic \"h(n)\" is monotonic if, for every node \"n\" and every successor \"n\"' of \"n\" generated by any action \"a\", the estimated cost of reaching the goal from \"n\" is no greater than the step cost of getting to \" n' \" plus the estimated cost of reaching the goal from \" n' \",\n\nThis is a form of triangle inequality, with \"n\", \"n\"', and the goal \"G\" closest to \"n\". Because every monotonic heuristic is also admissible, monotonicity is a stricter requirement than admissibility. Some heuristic algorithms such as A* can be proven optimal provided that the heuristic they use is monotonic.\n\nIn Boolean algebra, a monotonic function is one such that for all \"a\" and \"b\" in {0,1}, if \"a\" ≤ \"b\", \"a\" ≤ \"b\", ..., \"a\" ≤ \"b\" (i.e. the Cartesian product {0, 1} is ordered coordinatewise), then f(\"a\", ..., \"a\") ≤ f(\"b\", ..., \"b\"). In other words, a Boolean function is monotonic if, for every combination of inputs, switching one of the inputs from false to true can only cause the output to switch from false to true and not from true to false. Graphically, this means that a Boolean function is monotonic when in its Hasse diagram (dual of its Venn diagram), there is no 1 connected to a higher 0.\n\nThe monotonic Boolean functions are precisely those that can be defined by an expression combining the inputs (which may appear more than once) using only the operators \"and\" and \"or\" (in particular \"not\" is forbidden). For instance \"at least two of \"a\",\"b\",\"c\" hold\" is a monotonic function of \"a\",\"b\",\"c\", since it can be written for instance as ((\"a\" and \"b\") or (\"a\" and \"c\") or (\"b\" and \"c\")).\n\nThe number of such functions on \"n\" variables is known as the Dedekind number of \"n\".\n\n\n\n"}
{"id": "5719783", "url": "https://en.wikipedia.org/wiki?curid=5719783", "title": "Multiple edges", "text": "Multiple edges\n\nIn graph theory, multiple edges (also called parallel edges or a multi-edge), are two or more edges that are incident to the same two vertices. A simple graph has no multiple edges.\n\nDepending on the context, a graph may be defined so as to either allow or disallow the presence of multiple edges (often in concert with allowing or disallowing loops):\n\n\nMultiple edges are, for example, useful in the consideration of electrical networks, from a graph theoretical point of view. Additionally, they constitute the core differentiating feature of multidimensional networks.\n\nA planar graph remains planar if an edge is added between two vertices already joined by an edge; thus, adding multiple edges preserves planarity.\n\nA dipole graph is a graph with two vertices, in which all edges are parallel to each other.\n\n"}
{"id": "8099221", "url": "https://en.wikipedia.org/wiki?curid=8099221", "title": "Ordered vector space", "text": "Ordered vector space\n\nIn mathematics, an ordered vector space or partially ordered vector space is a vector space equipped with a partial order that is compatible with the vector space operations.\n\nGiven a vector space \"V\" over the real numbers R and a preorder ≤ on the set \"V\", the pair is called a preordered vector space if for all \"x\", \"y\", \"z\" in \"V\" and in R the following two axioms are satisfied\n\n\nIf ≤ is a partial order, is called an ordered vector space. The two axioms imply that translations and positive homotheties are automorphisms of the order structure and the mapping is an isomorphism to the dual order structure. Ordered vector spaces are ordered groups under their addition operation.\n\nGiven a preordered vector space \"V\", the subset \"V\" of all elements \"x\" in \"V\" satisfying \"x\" ≥ 0 is a convex cone, called the positive cone of \"V\". If \"V\" is an ordered vector space, then \"V\" ∩ (−\"V\") = {0}, and hence \"V\" is a proper cone. \n\nIf \"V\" is a real vector space and \"C\" is a proper convex cone in \"V\", there exists a unique partial order on \"V\" that makes \"V\" into an ordered vector space such \"V\" = \"C\". This partial order is given by \nTherefore, there exists a one-to-one correspondence between the partial orders on a vector space \"V\" that are compatible with the vector space structure and the proper convex cones of \"V\".\n\n\n\n\n"}
{"id": "36366249", "url": "https://en.wikipedia.org/wiki?curid=36366249", "title": "Penrose square root law", "text": "Penrose square root law\n\nIn the mathematical theory of games, the Penrose square root law, originally formulated by Lionel Penrose, concerns the distribution of the voting power in a voting body consisting of \"N\" members. It states that the \"a priori\" voting power of any voter, measured by the Penrose–Banzhaf index formula_1 scales like formula_2.\n\nThis result was used to design the Penrose method of for allocating the voting weights of representatives in a decision-making bodies proportional to the square root of the population represented.\n\nTo estimate the voting index of any player one needs to estimate the number of the possible winning coalitions in which his vote is decisive. Assume for simplicity that the number of voters is odd, \"N\" = 2\"j\" + 1, and the body votes according to the standard majority rule. Following Penrose one concludes that a given voter will be able to effectively influence the outcome of the voting only if the votes split half and half: if \"j\" players say 'Yes' and the remaining \"j\" players vote 'No', the last vote is decisive.\n\nAssuming that all members of the body vote independently (the votes are uncorrelated) \nand that the probability of each vote 'Yes' is equal to \"p\" = 1/2 one can estimate likelihood of such an event using the Bernoulli trial. The probability to obtain \"j\" votes 'Yes' out of 2\"j\" votes reads\n\nFor large \"N\" we may use the Stirling's approximation for the factorial \"j\" <nowiki>!</nowiki> and obtain the probability formula_1 that the vote of a given voter is decisive\n\nThe same approximation is obtained for an even number \"N\".\n\nA mathematical investigation of the influence of possible correlations between the voters for the Penrose square root law was presented by Kirsch.\n\nPenrose law is applied to construct Penrose-like systems of two-tier voting, including the Jagiellonian Compromise designed for the Council of the European Union.\n\n"}
{"id": "29696524", "url": "https://en.wikipedia.org/wiki?curid=29696524", "title": "Propositional proof system", "text": "Propositional proof system\n\nIn propositional calculus and proof complexity a propositional proof system (pps), also called a Cook–Reckhow propositional proof system, is system for proving classical propositional tautologies.\n\nFormally a pps is a polynomial-time function \"P\" whose range is the set of all propositional tautologies (denoted TAUT). If \"A\" is a formula, then any \"x\" such that \"P\"(\"x\") = \"A\" is called a \"P\"-proof of \"A\". The condition defining pps can be broken up as follows:\n\n\nIn general, a proof system for a language \"L\" is a polynomial-time function whose range is \"L\". Thus, a propositional proof system is a proof system for TAUT.\n\nSometimes the following alternative definition is considered: a pps is given as a proof-verification algorithm \"P\"(\"A\",\"x\") with two inputs. If \"P\" accepts the pair (\"A\",\"x\") we say that \"x\" is a \"P\"-proof of \"A\". \"P\" is required to run in polynomial time, and moreover, it must hold that \"A\" has a \"P\"-proof if and only if it is a tautology.\n\nIf \"P\" is a pps according to the first definition, then \"P\" defined by \"P\"(\"A\",\"x\") if and only if \"P\"(\"x\") = \"A\" is a pps according to the second definition. Conversely, if \"P\" is a pps according to the second definition, then \"P\" defined by\n(\"P\" takes pairs as input) is a pps according to the first definition, where formula_2 is a fixed tautology.\n\nOne can view the second definition as a non-deterministic algorithm for solving membership in TAUT. This means that proving a superpolynomial proof size lower-bound for pps would rule out existence of a certain class of polynomial-time algorithms based on that pps.\n\nAs an example, exponential proof size lower-bounds in resolution for the pigeon hole principle imply that any algorithm based on resolution cannot decide TAUT or SAT efficiently and will fail on pigeon hole principle tautologies. This is significant because the class of algorithms based on resolution includes most of current propositional proof search algorithms and modern industrial SAT solvers.\n\nHistorically, Frege's propositional calculus was the first propositional proof system. The general definition of a propositional proof system is due to Stephen Cook and Robert A. Reckhow (1979).\n\nPropositional proof system can be compared using the notion of p-simulation. A propositional proof system \"P\" \"p-simulates\" \"Q\" (written as \"P\" ≤\"Q\") when there is a polynomial-time function \"F\" such that \"P\"(\"F\"(\"x\")) = \"Q\"(\"x\") for every \"x\". That is, given a \"Q\"-proof \"x\", we can find in polynomial time a \"P\"-proof of the same tautology. If \"P\" ≤\"Q\" and \"Q\" ≤\"P\", the proof systems \"P\" and \"Q\" are \"p-equivalent\". There is also a weaker notion of simulation: a pps \"P\" \"simulates\" or \"weakly p-simulates\" a pps \"Q\" if there is a polynomial \"p\" such that for every \"Q\"-proof \"x\" of a tautology \"A\", there is a \"P\"-proof \"y\" of \"A\" such that the length of \"y\", |\"y\"| is at most \"p\"(|\"x\"|). (Some authors use the words p-simulation and simulation interchangeably for either of these two concepts, usually the latter.)\n\nA propositional proof system is called \"p-optimal\" if it \"p\"-simulates all other propositional proof systems, and it is \"optimal\" if it simulates all other pps. A propositional proof system \"P\" is \"polynomially bounded\" (also called super) if every tautology has a short (i.e., polynomial-size) \"P\"-proof.\n\nIf \"P\" is polynomially bounded and \"Q\" simulates \"P\", then \"Q\" is also polynomially bounded.\n\nThe set of propositional tautologies, TAUT, is a coNP-complete set. A propositional proof system is a certificate-verifier for membership in TAUT. Existence of a polynomially bounded propositional proof system means that there is a verifier with polynomial-size certificates, i.e., TAUT is in NP. In fact these two statements are equivalent, i.e., there is a polynomially bounded propositional proof system if and only if the complexity classes NP and coNP are equal.\n\nSome equivalence classes of proof systems under simulation or \"p\"-simulation are closely related to theories of bounded arithmetic; they are essentially \"non-uniform\" versions of the bounded arithmetic, in the same way that circuit classes are non-uniform versions of resource-based complexity classes. \"Extended Frege\" systems (allowing the introduction of new variables by definition) correspond in this way to polynomially-bounded systems, for example. Where the bounded arithmetic in turn corresponds to a circuit-based complexity class, there are often similarities between the theory of proof systems and the theory of the circuit families, such as matching lower bound results and separations. For example, just as counting cannot be done by an formula_3 circuit family of subexponential size, many tautologies relating to the pigeonhole principle cannot have subexponential proofs in a proof system based on bounded-depth formulas (and in particular, not by resolution-based systems, since they rely solely on depth 1 formulas).\n\nSome examples of propositional proof systems studied are:\n\n\n"}
{"id": "50578", "url": "https://en.wikipedia.org/wiki?curid=50578", "title": "Queueing theory", "text": "Queueing theory\n\nQueueing theory is the mathematical study of waiting lines, or queues. A queueing model is constructed so that queue lengths and waiting time can be predicted. Queueing theory is generally considered a branch of operations research because the results are often used when making business decisions about the resources needed to provide a service.\n\nQueueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the Copenhagen telephone exchange. The ideas have since seen applications including telecommunication, traffic engineering, computing\nand, particularly in industrial engineering, in the design of factories, shops, offices and hospitals, as well as in project management.\n\nThe spelling \"queueing\" over \"queuing\" is typically encountered in the academic research field. In fact, one of the flagship journals of the profession is named \"Queueing Systems\".\n\nSingle queueing nodes are usually described using Kendall's notation in the form \"A\"/\"S\"/\"C\" where \"A\" describes the time between arrivals to the queue, \"S\" the size of jobs and \"C\" the number of servers at the node. Many theorems in queueing theory can be proved by reducing queues to mathematical systems known as Markov chains, first described by Andrey Markov in his 1906 paper.\n\nIn 1909, Agner Krarup Erlang, a Danish engineer who worked for the Copenhagen Telephone Exchange, published the first paper on what would now be called queueing theory. He modeled the number of telephone calls arriving at an exchange by a Poisson process and solved the M/D/1 queue in 1917 and M/D/k queueing model in 1920. In Kendall's notation:\n\n\nThe M/M/1 queue is a simple model where a single server serves jobs that arrive according to a Poisson process and have exponentially distributed service requirements. In an M/G/1 queue, the G stands for general and indicates an arbitrary probability distribution. The M/G/1 model was solved by Felix Pollaczek in 1930, a solution later recast in probabilistic terms by Aleksandr Khinchin and now known as the Pollaczek–Khinchine formula.\n\nAfter the 1940s queueing theory became an area of research interest to mathematicians. In 1953 David George Kendall solved the GI/M/k queue and introduced the modern notation for queues, now known as Kendall's notation. In 1957 Pollaczek studied the GI/G/1 using an integral equation. John Kingman gave a formula for the mean waiting time in a G/G/1 queue: Kingman's formula.\n\nThe matrix geometric method and matrix analytic methods have allowed queues with phase-type distributed inter-arrival and service time distributions to be considered.\n\nProblems such as performance metrics for the M/G/k queue remain an open problem.\n\nVarious scheduling policies can be used at queuing nodes:\n\n\n\nA common basic queuing system is attributed to Erlang, though its exact origin remains unclear. The queue refers to a single server with one demand rate (formula_1), one dropout rate (formula_2), and one rate of service (formula_3), for which the length of the queue \"L\" is defined as:\n\nformula_4\n\nAssuming an exponential distribution for the rates, the waiting time \"W\" can be defined as the proportion of demand that is served. This is equal to the exponential survival rate of those who do not drop out over the waiting period, giving:\n\nformula_5\n\nThe second equation is commonly rewritten as:\n\nformula_6\n\nThe two-stage one-box model is common in epidemiology.\n\nNetworks of queues are systems in which a number of queues are connected by what's known as customer routing. When a customer is serviced at one node it can join another node and queue for service, or leave the network.\n\nFor networks of \"m\" nodes, the state of the system can be described by an \"m\"–dimensional vector (\"x\",\"x\"...,\"x\") where \"x\" represents the number of customers at each node.\n\nThe simplest non-trivial network of queues is called tandem queues. The first significant results in this area were Jackson networks, for which an efficient product-form stationary distribution exists and the mean value analysis which allows average metrics such as throughput and sojourn times to be computed. If the total number of customers in the network remains constant the network is called a closed network and has also been shown to have a product–form stationary distribution in the Gordon–Newell theorem. This result was extended to the BCMP network where a network with very general service time, regimes and customer routing is shown to also exhibit a product-form stationary distribution. The normalizing constant can be calculated with the Buzen's algorithm, proposed in 1973.\n\nNetworks of customers have also been investigated, Kelly networks where customers of different classes experience different priority levels at different service nodes. Another type of network are G-networks first proposed by Erol Gelenbe in 1993: these networks do not assume exponential time distributions like the classic Jackson Network.\n\n\n\nIn discrete time networks where there is a constraint on which service nodes can be active at any time, the max-weight scheduling algorithm chooses a service policy to give optimal throughput in the case that each job visits only a single person service node. In the more general case where jobs can visit more than one node, backpressure routing gives optimal throughput. A network scheduler must choose a queuing algorithm, which affects the characteristics of the larger network. See also Stochastic scheduling for more about scheduling of queueing systems.\n\nMean field models consider the limiting behaviour of the empirical measure (proportion of queues in different states) as the number of queues (\"m\" above) goes to infinity. The impact of other queues on any given queue in the network is approximated by a differential equation. The deterministic model converges to the same stationary distribution as the original model.\n\nFluid models are continuous deterministic analogs of queueing networks obtained by taking the limit when the process is scaled in time and space, allowing heterogeneous objects. This scaled trajectory converges to a deterministic equation which allows the stability of the system to be proven. It is known that a queueing network can be stable, but have an unstable fluid limit.\n\nIn a system with high occupancy rates (utilisation near 1) a heavy traffic approximation can be used to approximate the queueing length process by a reflected Brownian motion, Ornstein–Uhlenbeck process or more general diffusion process. The number of dimensions of the RBM is equal to the number of queueing nodes and the diffusion is restricted to the non-negative orthant.\n\n\n"}
{"id": "27168414", "url": "https://en.wikipedia.org/wiki?curid=27168414", "title": "Raikov's theorem", "text": "Raikov's theorem\n\nRaikov’s theorem is a result in probability theory. It is well known that if each of two independent random variables ξ and ξ has a Poisson distribution, then their sum ξ=ξ+ξ has a Poisson distribution as well. It turns out that the converse is also valid .\n\nSuppose that a random variable ξ has a Poisson distribution and admits a decomposition as a sum ξ=ξ+ξ of two independent random variables. Then the distribution of each summand is a shifted Poisson's distribution.\n\nRaikov's theorem is similar to Cramér’s decomposition theorem. The latter result claims that if a sum of two independent random variables has normal distribution, then each summand is normally distributed as well. It was also proved by Yu.V.Linnik that a convolution of normal distribution and Poisson's distribution possesses a similar property (Linnik's theorem).\n\nLet formula_1 be a locally compact Abelian group. Denote by formula_2the convolution semigroup of probability distributions on formula_1, and by formula_4the degenerate distribution concentrated at formula_5. Let formula_6.\n\nPoisson's distribution generated by the measure formula_7 is defined as a shifted distribution of the form\n\nformula_8\n\nOne has the following\n\nLet formula_9 be the Poisson's distribution generated by the measure formula_7. Suppose that formula_11, with formula_12. If formula_13 is either an infinite order element, or has order 2, then formula_14 is also a Poisson distribution. In the case of formula_13 being an element of finite order formula_16 with formula_17, formula_14 can fail to be a Poisson distribution.\n"}
{"id": "2324377", "url": "https://en.wikipedia.org/wiki?curid=2324377", "title": "Ramanujan theta function", "text": "Ramanujan theta function\n\nIn mathematics, particularly q-analog theory, the Ramanujan theta function generalizes the form of the Jacobi theta functions, while capturing their general properties. In particular, the Jacobi triple product takes on a particularly elegant form when written in terms of the Ramanujan theta. The function is named after Srinivasa Ramanujan.\n\nThe Ramanujan theta function is defined as\n\nfor |\"ab\"| < 1. The Jacobi triple product identity then takes the form\n\nHere, the expression formula_3 denotes the q-Pochhammer symbol. Identities that follow from this include\n\nand\n\nand\n\nthis last being the Euler function, which is closely related to the Dedekind eta function. The Jacobi theta function may be written in terms of the Ramanujan theta function as:\n\nWe have the following integral representation for the full two-parameter form of Ramanujan's theta function:\n\nThe special cases of Ramanujan's theta functions given by formula_9 and formula_10 also have the following integral representations:\n\nThis leads to several special case integrals for constants defined by these functions when formula_12 (cf. theta function explicit values). In particular, we have that \n\nand that\n\nThe Ramanujan theta function is used to determine the critical dimensions in Bosonic string theory, superstring theory and M-theory.\n\n"}
{"id": "32922845", "url": "https://en.wikipedia.org/wiki?curid=32922845", "title": "RaptorX", "text": "RaptorX\n\nRaptorX for protein structure modeling and function prediction\n\nRaptorX is a software and web server for protein structure and function prediction that is free for non-commercial use. RaptorX is among the most popular methods for protein structure prediction. Like other remote homology recognition/protein threading techniques, RaptorX is able to regularly generate reliable protein models when the widely used PSI-BLAST cannot. However, RaptorX is also significantly different from those profile-based methods (e.g., HHpred / HHsearch and Phyre / Phyre2) in that RaptorX excels at modeling of protein sequences without a large number of sequence homologs by exploiting structure information. RaptorX Server has been designed to ensure a user-friendly interface for users inexpert in protein structure prediction methods.\n\nThe RaptorX project was started in 2008 and RaptorX Server was released to the public in 2011.\n\nAfter pasting a protein sequence into the RaptorX submission form, a user will typically wait a couple of hours (depending on sequence length) for a prediction to complete. An email is sent to the user together with a link to a web page of results. RaptorX Server currently generates the following results: 3-state and 8-state secondary structure prediction, sequence-template alignment, 3D structure prediction, solvent accessibility prediction, disorder prediction and binding site prediction. The predicted results are displayed to support visual examination. The result files are also available for download.\n\nRaptorX Server also produces some confidence scores indicating the quality of the predicted 3D models (in the absence of their corresponding native structures). For example, it produces P-value for relative global quality of a 3D model, GDT (Global Distance Test) and uGDT (unnormalized-GDT) for absolute global quality of a 3D model and per-position RMSD for absolute local quality at each residue of a 3D model.\n\nBelow are some screenshots of RaptorX Server.\n\nApplications of RaptorX include protein structure prediction, function prediction, protein sequence-structure alignment, evolutionary classification of proteins, guiding site-directed mutagenesis and solving protein crystal structures by molecular replacement. In the CASP9 blind protein structure prediction experiment, RaptorX was ranked 2nd out of about 80 automatic structure prediction servers. RaptorX also generated the best alignments for the 50 hardest CASP9 TBM(template-based modeling) targets. in CASP10, RaptorX is the only server group among the top 10 human/server groups for the 15 most difficult CASP10 TBM targets.\n\nRaptorX is the successor to the RAPTOR protein structure prediction system. RAPTOR was designed and developed by Dr. Jinbo Xu and Dr. Ming Li at the University of Waterloo. RaptorX was designed and developed by a research group led by Prof. Jinbo Xu at the Toyota Technological Institute at Chicago.\n\n\n"}
{"id": "2009207", "url": "https://en.wikipedia.org/wiki?curid=2009207", "title": "Small-angle approximation", "text": "Small-angle approximation\n\nThe small-angle approximation is a useful simplification of the basic trigonometric functions which is approximately true in the limit where the angle approaches zero. They are truncations of the Taylor series for the basic trigonometric functions to a second-order approximation. This truncation gives:\n\nwhere is the angle in radians.\n\nThe small angle approximation is useful in many areas of engineering and physics, including mechanics, electromagnetics, optics (where it forms the basis of the paraxial approximation), cartography, astronomy, computer science, and so on.\n\nThe accuracy of the approximations can be seen below in Figure 1 and Figure 2. As the angle approaches zero, it is clear that the gap between the approximation and the original function quickly vanishes.\nThe red section on the right, , is the difference between the lengths of the hypotenuse, , and the adjacent side, . As is shown, and are almost the same length, meaning is close to 1 and helps trim the red away.\n\nThe opposite leg, , is approximately equal to the length of the blue arc, . Gathering facts from geometry, , from trigonometry, and , and from the picture, and leads to:\n\nSimplifying leaves,\n\nformula_5 , formula_6 , \n\ncan also be expressed this way, considering the fact that θ tends to zero\n\nIn fact, an approximation can also be synonymous with the word \"limit\" and θ in radians, being too small, tends to zero. It can be proved using Squeeze Theorem that in terms of limit calculus,\n\nThe Maclaurin expansion (the Taylor expansion about 0) of the relevant trigonometric function is \n\nwhere is the angle in radians. In clearer terms, \n\nIt is readily seen that the second most significant (third-order) term falls off as the cube of the first term; thus, even for a not-so-small argument such as 0.01, the value of the second most significant term is on the order of , or the first term. One can thus safely approximate: \n\nBy extension, since the cosine of a small angle is very nearly 1, and the tangent is given by the sine divided by the cosine, \n\nFigure 3 shows the relative errors of the small angle approximations. The angles at which the relative error exceeds 1% are as follows:\n\nIn astronomy, the angle subtended by the image of a distant object is often only a few arcseconds, so it is well suited to the small angle approximation. The linear size () is related to the angular size () and the distance from the observer () by the simple formula\n\nwhere is measured in arcseconds.\n\nThe number is approximately equal to the number of arcseconds in a circle (), divided by .\n\nThe exact formula is\n\nand the above approximation follows when is replaced by .\n\nThe second-order cosine approximation is especially useful in calculating the potential energy of a pendulum, which can then be applied with a Lagrangian to find the indirect (energy) equation of motion.\n\nWhen calculating the period of a simple pendulum, the small-angle approximation for sine is used to allow the resulting differential equation to be solved easily by comparison with the differential equation describing simple harmonic motion.\n\nThe small-angle approximation also appears in structural mechanics, especially in stability and bifurcation analyses (mainly of axially-loaded columns ready to undergo buckling). This leads to significant simplifications, though at a cost in accuracy and insight into the true behavior.\nThe 1 in 60 rule used in air navigation has its basis in the small-angle approximation, plus the fact that one radian is approximately 60 degrees.\n\nSmall-angle formulas may be used for interpolating between trigonometric table values. When β ≈ 0, then sin(β) ≈ β, cos(β) ≈ 1, and the angle sum and difference identities reduce to: \n\ncos(α + β) ≈ cos(α) - βsin(α), \n\ncos(α - β) ≈ cos(α) + βsin(α),\n\nsin(α + β) ≈ sin(α) + βcos(α),\n\nsin(α - β) ≈ sin(α) - βcos(α).\n\nExample: sin(0.755)\n"}
{"id": "5499671", "url": "https://en.wikipedia.org/wiki?curid=5499671", "title": "Soil production function", "text": "Soil production function\n\nSoil production function refers to the rate of bedrock weathering into soil as a function of soil thickness. \n\nA general model suggested that the rate of physical weathering of bedrock (de/dt) can be represented as an exponential decline with soil thickness:\n\nformula_1\n\nwhere \"h\" is soil thickness [m], \"P\" [mm/year] is the potential (or maximum) weathering rate of bedrock and \"k\" [m] is an empirical constant. \n\nThe reduction of weathering rate with thickening of soil is related to the exponential decrease of temperature amplitude with increasing depth below the soil surface, and also the exponential decrease in average water penetration (for freely-drained soils). Parameters \"P\" and \"k\" are related to the climate and type of parent materials. found the value of \"P\" ranges from 0.08 to 2.0 mm/yr for sites in Northern California, and 0.05–0.14 mm/yr for sites in Southeastern Australia. Meanwhile values of \"k\" do not vary significantly, ranging from 2 to 4 m.\n\nA number of landscape evolution models have adopted the so-called \"humped\" model. This model dates back to G.K. Gilbert's \"Report on the Geology of the Henry Mountains\" (1877). Gilbert reasoned that the weathering of bedrock was fastest under an intermediate thickness of soil and slower under exposed bedrock or under thick mantled soil. This is because chemical weathering requires the presence of water. Under thin soil or exposed bedrock water tends to run off, reducing the chance of the decomposition of bedrock.\n\n\n"}
{"id": "25274967", "url": "https://en.wikipedia.org/wiki?curid=25274967", "title": "Solovay model", "text": "Solovay model\n\nIn the mathematical field of set theory, the Solovay model is a model constructed by in which all of the axioms of Zermelo–Fraenkel set theory (ZF) hold, exclusive of the axiom of choice, but in which all sets of real numbers are Lebesgue measurable. The construction relies on the existence of an inaccessible cardinal.\n\nIn this way Solovay showed that the axiom of choice is essential to the proof of the existence of a non-measurable set, at least granted that the existence of an inaccessible cardinal is consistent with ZFC, the axioms of Zermelo–Fraenkel set theory including the axiom of choice.\n\nZF stands for Zermelo–Fraenkel set theory, and DC for the axiom of dependent choice.\n\nSolovay's theorem is as follows. \nAssuming the existence of an inaccessible cardinal, there is an inner model of ZF + DC of a suitable forcing extension \"V\"[\"G\"] such that every set of reals is Lebesgue measurable, has the perfect set property, and has the Baire property.\n\nSolovay constructed his model in two steps, starting with a model \"M\" of ZFC containing an inaccessible cardinal κ.\n\nThe first step is to take a Levy collapse \"M\"[\"G\"] of \"M\" by adding a generic set \"G\" for the notion of forcing that collapses all cardinals less than κ to ω. Then \"M\"[\"G\"] is a model of ZFC with the property that every set of reals that is definable over a countable sequence of ordinals is Lebesgue measurable, and has the Baire and perfect set properties. (This includes all definable and projective sets of reals; however for reasons related to Tarski's undefinability theorem the notion of a definable set of reals cannot be defined in the language of set theory, while the notion of a set of reals definable over a countable sequence of ordinals can be.)\n\nThe second step is to construct Solovay's model \"N\" as the class of all sets in \"M\"[\"G\"] that are hereditarily definable over a countable sequence of ordinals. The model \"N\" is an inner model of \"M\"[\"G\"] satisfying ZF + DC such that every set of reals is Lebesgue measurable, has the perfect set property, and has the Baire property. The proof of this uses the fact that every real in \"M\"[\"G\"] is definable over a countable sequence of ordinals, and hence \"N\" and \"M\"[\"G\"] have the same reals.\n\nInstead of using Solovay's model \"N\", one can also use the smaller inner model \"L\"(R) of \"M\"[\"G\"], consisting of the constructible closure of the real numbers, which has similar properties.\n\nSolovay suggested in his paper that the use of an inaccessible cardinal might not be necessary. Several authors proved weaker versions of Solovay's result without assuming the existence of an inaccessible cardinal. In particular showed there was a model of ZFC in which every ordinal-definable set of reals is measurable, Solovay showed there is a model of ZF + DC in which there is some translation-invariant extension of Lebesgue measure to all subsets of the reals, and showed that there is a model in which all sets of reals have the Baire property (so that the inaccessible cardinal is indeed unnecessary in this case).\n\nThe case of the perfect set property was solved by , who showed (in ZF) that if every set of reals has the perfect set property and the first uncountable cardinal ℵ is regular then ℵ is inaccessible in the constructible universe. Combined with Solovay's result, this shows that the statements \"There is an inaccessible cardinal\" and \"Every set of reals has the perfect set property\" are equiconsistent over ZF.\n\nFinally, showed that consistency of an inaccessible cardinal is also necessary for constructing a model in which all sets of reals are Lebesgue measurable. More precisely he showed that if every Σ set of reals is measurable then the first uncountable cardinal ℵ is inaccessible in the constructible universe, so that the condition about an inaccessible cardinal cannot be dropped from Solovay's theorem. Shelah also showed that the Σ condition is close to the best possible by constructing a model (without using an inaccessible cardinal) in which all Δ sets of reals are measurable. See and and for expositions of Shelah's result.\n\n"}
{"id": "21102203", "url": "https://en.wikipedia.org/wiki?curid=21102203", "title": "Stable module category", "text": "Stable module category\n\nIn representation theory, the stable module category is a category in which projectives are \"factored out.\"\n\nLet \"R\" be a ring. For two modules \"M\" and \"N\", define formula_1 to be the set of \"R\"-linear maps from \"M\" to \"N\" modulo the relation that \"f\" ~ \"g\" if \"f\" − \"g\" factors through a projective module. The stable module category is defined by setting the objects to be the \"R\"-modules, and the morphisms are the equivalence classes formula_1.\n\nGiven a module \"M\", let \"P\" be a projective module with a surjection formula_3. Then set formula_4 to be the kernel of \"p\". Suppose we are given a morphism formula_5 and a surjection formula_6 where \"Q\" is projective. Then one can lift \"f\" to a map formula_7 which maps formula_4 into formula_9. This gives a well-defined functor formula_10 from the stable module category to itself.\n\nFor certain rings, such as Frobenius algebras, formula_10 is an equivalence of categories. In this case, the inverse formula_12 can be defined as follows. Given \"M\", find an injective module \"I\" with an inclusion formula_13. Then formula_14 is defined to be the cokernel of \"i\". A case of particular interest is when the ring \"R\" is a group algebra.\n\nThe functor Ω can even be defined on the module category of a general ring (without factoring out projectives), as the cokernel of the injective envelope. It need not be true in this case that the functor Ω is actually an inverse to Ω. One important property of the stable module category is it allows defining the Ω functor for general rings. When \"R\" is perfect (or \"M\" is finitely generated and \"R\" is semiperfect), then Ω(\"M\") can be defined as the kernel of the projective cover, giving a functor on the module category. However, in general projective covers need not exist, and so passing to the stable module category is necessary.\n\nNow we suppose that \"R = kG\" is a group algebra for some field \"k\" and some group \"G\". One can show that there exist isomorphisms\nfor every positive integer \"n\". The group cohomology of a representation \"M\" is given by formula_16 where \"k\" has a trivial \"G\"-action, so in this way the stable module category gives a natural setting in which group cohomology lives. \n\nFurthermore, the above isomorphism suggests defining cohomology groups for negative values of \"n\", and in this way, one recovers Tate cohomology.\n\nAn exact sequence \nin the usual module category defines an element of formula_18, and hence an element of formula_19, so that we get a sequence\nTaking formula_12 to be the translation functor and such sequences as above to be exact triangles, the stable module category becomes a triangulated category.\n\n\n"}
{"id": "571280", "url": "https://en.wikipedia.org/wiki?curid=571280", "title": "Stratification (mathematics)", "text": "Stratification (mathematics)\n\nStratification has several usages in mathematics.\n\nIn mathematical logic, stratification is any consistent assignment of numbers to predicate symbols guaranteeing that a unique formal interpretation of a logical theory exists. Specifically, we say that a set of clauses of the form formula_1 is stratified if and only if\nthere is a stratification assignment S that fulfills the following conditions:\n\n\nThe notion of stratified negation leads to a very effective operational semantics for stratified programs in terms of the stratified least fixpoint, that is obtained by iteratively applying the fixpoint operator to each \"stratum\" of the program, from the lowest one up.\nStratification is not only useful for guaranteeing unique interpretation of Horn clause\ntheories. It has also been used by W.V. Quine (1937) to address Russell's paradox, which undermined Frege's central work \"Grundgesetze der Arithmetik\" (1902).\n\nIn New Foundations (NF) and related set theories, a formula formula_4 in the language of first-order logic with equality and membership is said to be\nstratified if and only if there is a function\nformula_5 which sends each variable appearing in formula_4 (considered as an item of syntax) to\na natural number (this works equally well if all integers are used) in such a way that\nany atomic formula formula_7 appearing in formula_4 satisfies formula_9 and any atomic formula formula_10 appearing in formula_4 satisfies formula_12.\n\nIt turns out that it is sufficient to require that these conditions be satisfied only when\nboth variables in an atomic formula are bound in the set abstract formula_13\nunder consideration. A set abstract satisfying this weaker condition is said to be\nweakly stratified.\n\nThe stratification of New Foundations generalizes readily to languages with more\npredicates and with term constructions. Each primitive predicate needs to have specified\nrequired displacements between values of formula_5 at its (bound) arguments\nin a (weakly) stratified formula. In a language with term constructions, terms themselves\nneed to be assigned values under formula_5, with fixed displacements from the\nvalues of each of their (bound) arguments in a (weakly) stratified formula. Defined term\nconstructions are neatly handled by (possibly merely implicitly) using the theory\nof descriptions: a term formula_16 (the x such that formula_4) must\nbe assigned the same value under formula_5 as the variable x.\n\nA formula is stratified if and only if it is possible to assign types to all variables appearing\nin the formula in such a way that it will make sense in a version TST of the theory of\ntypes described in the New Foundations article, and this is probably the best way\nto understand the stratification of New Foundations in practice.\n\nThe notion of stratification can be extended to the lambda calculus; this is found\nin papers of Randall Holmes.\n\nIn singularity theory, there is a different meaning, of a decomposition of a topological space \"X\" into disjoint subsets each of which is a topological manifold (so that in particular a \"stratification\" defines a partition of the topological space). This is not a useful notion when unrestricted; but when the various strata are defined by some recognisable set of conditions (for example being locally closed), and fit together manageably, this idea is often applied in geometry. Hassler Whitney and René Thom first defined formal conditions for stratification. See Whitney stratification and topologically stratified space.\n\nSee stratified sampling.\n"}
{"id": "25587550", "url": "https://en.wikipedia.org/wiki?curid=25587550", "title": "Suresh Venapally", "text": "Suresh Venapally\n\nSuresh Venapally (; born 1966) is an Indian mathematician known for his research work in algebra. He is a professor at Emory University.\n\nSuresh was born in Vangoor, Telangana, India and studied in ZPHS at Vangoor up to 9th standard. He did his M.Sc at University of Hyderabad.\nHe joined Tata Institute of Fundamental Research (TIFR) in 1989 and got his PhD in under the guidance of Raman Parimala (1994). He later joined the faculty at University of Hyderabad.\n\n\n\n"}
{"id": "23652040", "url": "https://en.wikipedia.org/wiki?curid=23652040", "title": "WaveLab (mathematics software)", "text": "WaveLab (mathematics software)\n\nWaveLab is a collection of MATLAB functions for wavelet analysis. Following the success of WaveLab package, there is now the availability of CurveLab and ShearLab.\n"}
{"id": "33192618", "url": "https://en.wikipedia.org/wiki?curid=33192618", "title": "William P. Byers", "text": "William P. Byers\n\nWilliam P. Byers (born 1943) is a Canadian mathematician and philosopher; professor emeritus in mathematics and statistics at Concordia University in Montreal, Quebec, Canada.\n\nHe completed a BSc ('64), and an MSc ('65) from McGill University, and obtained his PhD ('69) from the University of California, Berkeley.\n\nHis area of interests include Dynamical Systems, and the Philosophy of Mathematics.\n\nByers has published in various mathematical journals and published some books. \"How Mathematicians Think\" and \"The Blind Spot\", were both published by Princeton University Press.\nByers' latest book, \"Deep Thinking\" was published by World Scientific Publishing.\n\n"}
{"id": "5767229", "url": "https://en.wikipedia.org/wiki?curid=5767229", "title": "Zsigmondy's theorem", "text": "Zsigmondy's theorem\n\nIn number theory, Zsigmondy's theorem, named after Karl Zsigmondy, states that if are coprime integers, then for any integer \"n\" ≥ 1, there is a prime number \"p\" (called a \"primitive prime divisor\") that divides and does not divide for any positive integer , with the following exceptions:\n\n\nThis generalizes Bang's theorem, which states that if and \"n\" is not equal to 6, then has a prime divisor not dividing any with .\n\nSimilarly, has at least one primitive prime divisor with the exception .\n\nZsigmondy's theorem is often useful, especially in group theory, where it is used to prove that various groups have distinct orders except when they are known to be the same.\n\nThe theorem was discovered by Zsigmondy working in Vienna from 1894 until 1925.\n\nLet formula_1 be a sequence of nonzero integers.\nThe Zsigmondy set associated to the sequence is the set\n\nformula_2\n\ni.e., the set of indices formula_3 such that every prime dividing formula_4\nalso divides some formula_5 for some formula_6. Thus Zsigmondy's theorem implies that formula_7, and Carmichael's theorem says that the\nZsigmondy set of the Fibonacci sequence is formula_8, and that of the Pell sequence is formula_9. In 2001 Bilu, Hanrot, and \nVoutier\nproved that in general, if formula_1 is a Lucas sequence or a Lehmer sequence, then formula_11 (see , there are only 13 such formula_3s, namely 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, 13, 18, 30).\nLucas and Lehmer sequences are examples of divisibility sequences.\n\nIt is also known that\nif formula_13 is an elliptic divisibility sequence, then its Zsigmondy\nset formula_14 is finite. However, the result is ineffective in the sense\nthat the proof does give an explicit upper bound for the largest element in formula_14,\nalthough it is possible to give an effective upper bound for the number of elements\nin formula_14.\n\n\n"}
