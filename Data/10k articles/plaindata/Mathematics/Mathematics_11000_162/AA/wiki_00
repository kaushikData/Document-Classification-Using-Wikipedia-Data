{"id": "1438635", "url": "https://en.wikipedia.org/wiki?curid=1438635", "title": "134 (number)", "text": "134 (number)\n\n134 (one hundred [and] thirty-four) is the natural number following 133 and preceding 135.\n\n134 is a nontotient since there is no integer with exactly 134 coprimes below it. And it is a noncototient since there is no integer with 134 integers with common factors below it.\n\n134 is formula_1.\n\nIn Roman numerals, 134 is a Friedman number since CXXXIV = XV * (XC/X) - I.\n\n\n\n\n134 is also:\n\n"}
{"id": "39792", "url": "https://en.wikipedia.org/wiki?curid=39792", "title": "3-sphere", "text": "3-sphere\n\nIn mathematics, a 3-sphere, or glome, is a higher-dimensional analogue of a sphere. It may be embedded in 4-dimensional Euclidean space as the set of points equidistant from a fixed central point. Analogously to how the boundary of a ball in three dimensions is an ordinary sphere (or 2-sphere, a two-dimensional surface), the boundary of a ball in four dimensions is a 3-sphere (an object with three dimensions). A 3-sphere is an example of a 3-manifold.\n\nIn coordinates, a 3-sphere with center and radius is the set of all points in real, 4-dimensional space () such that\nThe 3-sphere centered at the origin with radius 1 is called the unit 3-sphere and is usually denoted :\n\nIt is often convenient to regard as the space with 2 complex dimensions () or the quaternions (). The unit 3-sphere is then given by\n\nor\n\nThis description as the quaternions of norm one identifies the 3-sphere with the versors in the quaternion division ring. Just as the unit circle is important for planar polar coordinates, so the 3-sphere is important in the polar view of 4-space involved in quaternion multiplication. See polar decomposition of a quaternion for details of this development of the three-sphere.\nThis view of the 3-sphere is the basis for the study of elliptic space as developed by Georges Lemaître.\n\nThe 3-dimensional cubic hyperarea of a 3-sphere of radius is\nwhile the 4-dimensional quartic hypervolume (the volume of the 4-dimensional region bounded by the 3-sphere) is\n\nEvery non-empty intersection of a 3-sphere with a three-dimensional hyperplane is a 2-sphere (unless the hyperplane is tangent to the 3-sphere, in which case the intersection is a single point). As a 3-sphere moves through a given three-dimensional hyperplane, the intersection starts out as a point, then becomes a growing 2-sphere that reaches its maximal size when the hyperplane cuts right through the \"equator\" of the 3-sphere. Then the 2-sphere shrinks again down to a single point as the 3-sphere leaves the hyperplane.\n\nA 3-sphere is a compact, connected, 3-dimensional manifold without boundary. It is also simply connected. What this means, in the broad sense, is that any loop, or circular path, on the 3-sphere can be continuously shrunk to a point without leaving the 3-sphere. The Poincaré conjecture, proved in 2003 by Grigori Perelman, provides that the 3-sphere is the only three-dimensional manifold (up to homeomorphism) with these properties.\n\nThe 3-sphere is homeomorphic to the one-point compactification of . In general, any topological space that is homeomorphic to the 3-sphere is called a topological 3-sphere.\n\nThe homology groups of the 3-sphere are as follows: and are both infinite cyclic, while for all other indices . Any topological space with these homology groups is known as a homology 3-sphere. Initially Poincaré conjectured that all homology 3-spheres are homeomorphic to , but then he himself constructed a non-homeomorphic one, now known as the Poincaré homology sphere. Infinitely many homology spheres are now known to exist. For example, a Dehn filling with slope on any knot in the 3-sphere gives a homology sphere; typically these are not homeomorphic to the 3-sphere.\n\nAs to the homotopy groups, we have and is infinite cyclic. The higher-homotopy groups () are all finite abelian but otherwise follow no discernible pattern. For more discussion see homotopy groups of spheres.\nThe 3-sphere is naturally a smooth manifold, in fact, a closed embedded submanifold of . The Euclidean metric on induces a metric on the 3-sphere giving it the structure of a Riemannian manifold. As with all spheres, the 3-sphere has constant positive sectional curvature equal to where is the radius.\n\nMuch of the interesting geometry of the 3-sphere stems from the fact that the 3-sphere has a natural Lie group structure given by quaternion multiplication (see the section below on group structure). The only other spheres with such a structure are the 0-sphere and the 1-sphere (see circle group).\n\nUnlike the 2-sphere, the 3-sphere admits nonvanishing vector fields (sections of its tangent bundle). One can even find three linearly independent and nonvanishing vector fields. These may be taken to be any left-invariant vector fields forming a basis for the Lie algebra of the 3-sphere. This implies that the 3-sphere is parallelizable. It follows that the tangent bundle of the 3-sphere is trivial. For a general discussion of the number of linear independent vector fields on a -sphere, see the article vector fields on spheres.\n\nThere is an interesting action of the circle group on giving the 3-sphere the structure of a principal circle bundle known as the Hopf bundle. If one thinks of as a subset of , the action is given by\nThe orbit space of this action is homeomorphic to the two-sphere . Since is not homeomorphic to , the Hopf bundle is nontrivial.\n\nThere are several well-known constructions of the three-sphere. Here we describe gluing a pair of three-balls and then the one-point compactification.\n\nA 3-sphere can be constructed topologically by \"gluing\" together the boundaries of a pair of 3-balls. The boundary of a 3-ball is a 2-sphere, and these two 2-spheres are to be identified. That is, imagine a pair of 3-balls of the same size, then superpose them so that their 2-spherical boundaries match, and let matching pairs of points on the pair of 2-spheres be identically equivalent to each other. In analogy with the case of the 2-sphere (see below), the gluing surface is called an equatorial sphere.\n\nNote that the interiors of the 3-balls are not glued to each other. One way to think of the fourth dimension is as a continuous real-valued function of the 3-dimensional coordinates of the 3-ball, perhaps considered to be \"temperature\". We take the \"temperature\" to be zero along the gluing 2-sphere and let one of the 3-balls be \"hot\" and let the other 3-ball be \"cold\". The \"hot\" 3-ball could be thought of as the \"upper hemisphere\" and the \"cold\" 3-ball could be thought of as the \"lower hemisphere\". The temperature is highest/lowest at the centers of the two 3-balls.\n\nThis construction is analogous to a construction of a 2-sphere, performed by gluing the boundaries of a pair of disks. A disk is a 2-ball, and the boundary of a disk is a circle (a 1-sphere). Let a pair of disks be of the same diameter. Superpose them and glue corresponding points on their boundaries. Again one may think of the third dimension as temperature. Likewise, we may inflate the 2-sphere, moving the pair of disks to become the northern and southern hemispheres.\n\nAfter removing a single point from the 2-sphere, what remains is homeomorphic to the Euclidean plane. In the same way, removing a single point from the 3-sphere yields three-dimensional space. \nAn extremely useful way to see this is via stereographic projection. We first describe the lower-dimensional version.\n\nRest the south pole of a unit 2-sphere on the -plane in three-space. We map a point of the sphere (minus the north pole ) to the plane by sending to the intersection of the line with the plane. Stereographic projection of a 3-sphere (again removing the north pole) maps to three-space in the same manner. (Notice that, since stereographic projection is conformal, round spheres are sent to round spheres or to planes.)\n\nA somewhat different way to think of the one-point compactification is via the exponential map. Returning to our picture of the unit two-sphere sitting on the Euclidean plane: Consider a geodesic in the plane, based at the origin, and map this to a geodesic in the two-sphere of the same length, based at the south pole. Under this map all points of the circle of radius are sent to the north pole. Since the open unit disk is homeomorphic to the Euclidean plane, this is again a one-point compactification.\n\nThe exponential map for 3-sphere is similarly constructed; it may also be discussed using the fact that the 3-sphere is the Lie group of unit quaternions.\n\nThe four Euclidean coordinates for are redundant since they are subject to the condition that . As a 3-dimensional manifold one should be able to parameterize by three coordinates, just as one can parameterize the 2-sphere using two coordinates (such as latitude and longitude). Due to the nontrivial topology of it is impossible to find a single set of coordinates that cover the entire space. Just as on the 2-sphere, one must use \"at least\" two coordinate charts. Some different choices of coordinates are given below.\n\nIt is convenient to have some sort of hyperspherical coordinates on in analogy to the usual spherical coordinates on . One such choice — by no means unique — is to use , where\nwhere and run over the range 0 to , and runs over 0 to 2. Note that, for any fixed value of , and parameterize a 2-sphere of radius , except for the degenerate cases, when equals 0 or , in which case they describe a point.\n\nThe round metric on the 3-sphere in these coordinates is given by\nand the volume form by\n\nThese coordinates have an elegant description in terms of quaternions. Any unit quaternion can be written as a versor:\nwhere is a unit imaginary quaternion; that is, a quaternion that satisfies . This is the quaternionic analogue of Euler's formula. Now the unit imaginary quaternions all lie on the unit 2-sphere in so any such can be written:\nWith in this form, the unit quaternion is given by\nwhere are as above.\n\nWhen is used to describe spatial rotations (cf. quaternions and spatial rotations), it describes a rotation about through an angle of .\n\nFor unit radius another choice of hyperspherical coordinates, , makes use of the embedding of in . In complex coordinates we write\n\nThis could also be expressed in as\nHere runs over the range 0 to , and and can take any values between 0 and 2. These coordinates are useful in the description of the 3-sphere as the Hopf bundle\nFor any fixed value of between 0 and , the coordinates parameterize a 2-dimensional torus. Rings of constant and above form simple orthogonal grids on the tori. See image to right. In the degenerate cases, when equals 0 or , these coordinates describe a circle.\n\nThe round metric on the 3-sphere in these coordinates is given by\nand the volume form by\n\nTo get the interlocking circles of the Hopf fibration, make a simple substitution in the equations above\n\nIn this case , and specify which circle, and specifies the position along each circle. One round trip (0 to 2) of or equates to a round trip of the torus in the 2 respective directions.\n\nAnother convenient set of coordinates can be obtained via stereographic projection of from a pole onto the corresponding equatorial hyperplane. For example, if we project from the point we can write a point in as\nwhere is a vector in and . In the second equality above, we have identified with a unit quaternion and with a pure quaternion. (Note that the numerator and denominator commute here even though quaternionic multiplication is generally noncommutative). The inverse of this map takes in to\n\nWe could just as well have projected from the point , in which case the point is given by\nwhere is another vector in . The inverse of this map takes to\n\nNote that the coordinates are defined everywhere but and the coordinates everywhere but . This defines an atlas on consisting of two coordinate charts or \"patches\", which together cover all of . Note that the transition function between these two charts on their overlap is given by\nand vice versa.\n\nWhen considered as the set of unit quaternions, inherits an important structure, namely that of quaternionic multiplication. Because the set of unit quaternions is closed under multiplication, takes on the structure of a group. Moreover, since quaternionic multiplication is smooth, can be regarded as a real Lie group. It is a nonabelian, compact Lie group of dimension 3. When thought of as a Lie group is often denoted or .\n\nIt turns out that the only spheres that admit a Lie group structure are , thought of as the set of unit complex numbers, and , the set of unit quaternions. One might think that , the set of unit octonions, would form a Lie group, but this fails since octonion multiplication is nonassociative. The octonionic structure does give one important property: \"parallelizability\". It turns out that the only spheres that are parallelizable are , , and .\n\nBy using a matrix representation of the quaternions, , one obtains a matrix representation of . One convenient choice is given by the Pauli matrices:\nThis map gives an injective algebra homomorphism from to the set of 2 × 2 complex matrices. It has the property that the absolute value of a quaternion is equal to the square root of the determinant of the matrix image of .\n\nThe set of unit quaternions is then given by matrices of the above form with unit determinant. This matrix subgroup is precisely the special unitary group . Thus, as a Lie group is isomorphic to .\n\nUsing our Hopf coordinates we can then write any element of in the form\n\nAnother way to state this result is if we express the matrix representation of an element of as a linear combination of the Pauli matrices. It is seen that an arbitrary element can be written as\nThe condition that the determinant of is +1 implies that the coefficients are constrained to lie on a 3-sphere.\n\nIn Edwin Abbott Abbott's \"Flatland\", published in 1884, and in \"Sphereland\", a 1965 sequel to Flatland by Dionys Burger, the 3-sphere is referred to as an oversphere, and a 4-sphere is referred to as a hypersphere.\n\nWriting in the American Journal of Physics, Mark A. Peterson describes three different ways of visualizing 3-spheres and points out language in \"The Divine Comedy\" that suggests Dante viewed the Universe in the same way.\n\n\n"}
{"id": "713688", "url": "https://en.wikipedia.org/wiki?curid=713688", "title": "Alain Badiou", "text": "Alain Badiou\n\nAlain Badiou (; ; born 17 January 1937) is a French philosopher, formerly chair of Philosophy at the École normale supérieure (ENS) and founder of the faculty of Philosophy of the Université de Paris VIII with Gilles Deleuze, Michel Foucault and Jean-François Lyotard. Badiou has written about the concepts of being, truth, event and the subject in a way that, he claims, is neither postmodern nor simply a repetition of modernity. Badiou has been involved in a number of political organisations, and regularly comments on political events. Badiou argues for resurrecting the practice of communism.\n\nBadiou is the son of a mathematician who was a working member of the Resistance in France during World War II (1905–1996). He was a student at the Lycée Louis-Le-Grand and then the École Normale Supérieure (1955–1960). In 1960, he wrote his \"\" (roughly equivalent to an MA thesis) on Spinoza for Georges Canguilhem (the topic was \"Demonstrative Structures in the First Two Books of Spinoza's Ethics\", \"Structures démonstratives dans les deux premiers livres de l'Éthique de Spinoza\"). He taught at the lycée in Reims from 1963 where he became a close friend of fellow playwright (and philosopher) François Regnault, and published a couple of novels before moving first to the faculty of letters of the University of Reims (the \"collège littéraire universitaire\") and then to the University of Paris VIII (Vincennes-Saint Denis) in 1969. Badiou was politically active very early on, and was one of the founding members of the Unified Socialist Party (PSU). The PSU was particularly active in the struggle for the decolonization of Algeria. He wrote his first novel, \"Almagestes\", in 1964. In 1967 he joined a study group organized by Louis Althusser, became increasingly influenced by Jacques Lacan and became a member of the editorial board of \"Cahiers pour l'Analyse\". By then he \"already had a solid grounding in mathematics and logic (along with Lacanian theory)\", and his own two contributions to the pages of \"Cahiers\" \"anticipate many of the distinctive concerns of his later philosophy\".\n\nThe student uprisings of May 1968 reinforced Badiou's commitment to the far Left, and he participated in increasingly militant groups, such as the (UCFml). To quote Badiou himself, the UCFml is \"the Maoist organization established in late 1969 by Natacha Michel, Sylvain Lazarus, myself and a fair number of young people\". During this time, Badiou joined the faculty of the newly founded University of Paris VIII/Vincennes-Saint Denis which was a bastion of counter-cultural thought. There he engaged in fierce intellectual debates with fellow professors Gilles Deleuze and Jean-François Lyotard, whose philosophical works he considered unhealthy deviations from the Althusserian program of a scientific Marxism.\n\nIn the 1980s, as both Althusserian structural Marxism and Lacanian psychoanalysis went into decline (after Lacan died and Althusser was committed to a psychiatric hospital), Badiou published more technical and abstract philosophical works, such as \"Théorie du sujet\" (1982), and his magnum opus, \"Being and Event\" (1988). Nonetheless, Badiou has never renounced Althusser or Lacan, and sympathetic references to Marxism and psychoanalysis are not uncommon in his more recent works (most notably \"Petit panthéon portatif\" / \"Pocket Pantheon\").\n\nHe took up his current position at the ENS in 1999. He is also associated with a number of other institutions, such as the Collège International de Philosophie. He was a member of which, as mentioned above, he founded in 1985 with some comrades from the Maoist UCFml. This organization disbanded in 2007, according to the French Wikipedia article (linked to in the previous sentence). In 2002, he was a co-founder of the Centre International d'Etude de la Philosophie Française Contemporaine, alongside Yves Duroux and his former student Quentin Meillassoux. Badiou has also enjoyed success as a dramatist with plays such as \"Ahmed le Subtil\".\n\nIn the last decade, an increasing number of Badiou's works have been translated into English, such as \"Ethics\", \"Deleuze\", \"Manifesto for Philosophy\", \"Metapolitics\", and \"Being and Event\". Short pieces by Badiou have likewise appeared in American and English periodicals, such as \"Lacanian Ink\", \"New Left Review\", \"Radical Philosophy\", \"Cosmos and History\" and \"Parrhesia\". Unusually for a contemporary European philosopher his work is increasingly being taken up by militants in countries like India, the Democratic Republic of Congo and South Africa.\n\nIn 2005–6 Badiou got into a fierce controversy within the confines of Parisian intellectual life. It started in 2005 with the publication of his \"Circonstances 3: Portées du mot 'juif' – The Uses of the Word 'Jew'\". This book generated a strong response with Badiou being labelled Anti-Semitic. The wrangling became a \"cause célèbre\" with articles going back and forth in the French newspaper \"Le Monde\" and in the cultural journal \"Les Temps modernes\". Linguist and Lacanian philosopher Jean-Claude Milner, a past president of Collège international de philosophie, has accused Badiou of Anti-Semitism.\n\nIn 2014–15, Badiou had the role of Honorary President at The Global Center for Advanced Studies.\n\nBadiou makes repeated use of several concepts throughout his philosophy. One of the aims of his thought is to show that his categories of truth are useful for any type of philosophical critique. Therefore, he uses them to interrogate art and history as well as ontology and scientific discovery. Johannes Thumfart argues that Badiou's philosophy can be regarded as a contemporary reinterpretation of Platonism.\n\nAccording to Badiou, philosophy is suspended from four conditions (art, love, politics, and science), each of them fully independent \"truth procedures.\" (For Badiou's notion of truth procedures, see below.) Badiou consistently maintains throughout his work (but most systematically in \"Manifesto for Philosophy\") that philosophy must avoid the temptation to suture itself ('sew itself', that is, to hand over its entire intellectual effort) to any of these independent truth procedures. When philosophy does suture itself to one of its conditions (and Badiou argues that the history of philosophy during the nineteenth and twentieth centuries is primarily a history of sutures), what results is a philosophical \"disaster.\" Consequently, philosophy is, according to Badiou, a thinking of the \"compossibility\" of the several truth procedures, whether this is undertaken through the investigation of the intersections between distinct truth procedures (the intersection of art and love in the novel, for instance), or whether this is undertaken through the more traditionally philosophical work of addressing categories like truth or the subject (concepts that are, as concepts, external to the individual truth procedures, though they are functionally operative in the truth procedures themselves). For Badiou, when philosophy addresses the four truth procedures in a genuinely philosophical manner, rather than through a suturing abandonment of philosophy as such, it speaks of them with a theoretical terminology that marks its philosophical character: \"inaesthetics\" rather than art; metapolitics rather than politics; ontology rather than science; etc.\n\nTruth, for Badiou, is a specifically philosophical category. While philosophy's several conditions are, on their own terms, \"truth procedures\" (i.e., they produce truths as they are pursued), it is only philosophy that can speak of the several truth procedures \"as\" truth procedures. (The lover, for instance, does not think of her love as a question of truth, but simply and rightly as a question of love. Only the philosopher sees in the true lover's love the unfolding of a truth.) Badiou has a very rigorous notion of truth, one that is strongly against the grain of much of contemporary European thought. Badiou at once embraces the traditional modernist notion that truths are genuinely invariant (always and everywhere the case, eternal and unchanging) and the incisively postmodernist notion that truths are constructed through processes. Badiou's theory of truth, exposited throughout his work, accomplishes this strange mixture by uncoupling invariance from self-evidence (such that invariance does not imply self-evidence), as well as by uncoupling constructedness from relativity (such that constructedness does not lead to relativism).\n\nThe idea, here, is that a truth's invariance makes it genuinely indiscernible: because a truth is everywhere and always the case, it passes unnoticed unless there is a rupture in the laws of being and appearance, during which the truth in question becomes, but only for a passing moment, discernible. Such a rupture is what Badiou calls an event, according to a theory originally worked out in \"Being and Event\" and fleshed out in important ways in \"Logics of Worlds\". The individual who chances to witness such an event, if he is faithful to what he has glimpsed, can then introduce the truth by naming it into worldly situations. For Badiou, it is by positioning oneself to the truth of an event that a human animal becomes a subject; subjectivity is not an inherent human trait. According to a process or procedure that subsequently unfolds only if those who subject themselves to the glimpsed truth continue to be faithful in the work of announcing the truth in question, genuine knowledge is produced (knowledge often appears in Badiou's work under the title of the \"veridical\"). While such knowledge is produced in the process of being faithful to a truth event, it should be noted that, for Badiou, knowledge, in the figure of the encyclopedia, always remains fragile, subject to what may yet be produced as faithful subjects of the event produce further knowledge. According to Badiou, truth procedures proceed to infinity, such that faith (fidelity) outstrips knowledge. (Badiou, following both Lacan and Heidegger, distances truth from knowledge.) The dominating ideology of the day, which Badiou terms \"democratic materialism,\" denies the existence of truth and only recognizes \"bodies\" and \"languages.\" Badiou proposes a turn towards the \"materialist dialectic,\" which recognizes that there are only bodies and languages, \"except\" there are also truths.\n\nIn \"Handbook of Inaesthetics\" Badiou both draws on the original Greek meaning and the later Kantian concept of \"aesthesis\" as \"material perception\" and coins the phrase \"inaesthetic\" to refer to a concept of artistic creation that denies \"the reflection/object relation\" yet, at the same time, in reaction against the idea of mimesis, or poetic reflection of \"nature\", he affirms that art is \"immanent\" and \"singular\". Art is immanent in the sense that its truth is given in its immediacy in a given work of art, and singular in that its truth is found in art and art alone—hence reviving the ancient materialist concept of \"aesthesis\". His view of the link between philosophy and art is tied into the motif of pedagogy, which he claims functions so as to \"arrange the forms of knowledge in a way that some truth may come to pierce a hole in them\". He develops these ideas with examples from the prose of Samuel Beckett and the poetry of Stéphane Mallarmé and Fernando Pessoa (who he argues has developed a body of work that philosophy is currently incapable of incorporating), among others.\n\nThe major propositions of Badiou's philosophy all find their basis in \"Being and Event\", in which he continues his attempt (which he began in \"Théorie du sujet\") to reconcile a notion of the subject with ontology, and in particular post-structuralist and constructivist ontologies. A frequent criticism of post-structuralist work is that it prohibits, through its fixation on semiotics and language, any notion of a subject. Badiou's work is, by his own admission, an attempt to break out of contemporary philosophy's fixation upon language, which he sees almost as a straitjacket. This effort leads him, in \"Being and Event\", to combine rigorous mathematical formulae with his readings of poets such as Mallarmé and Hölderlin and religious thinkers such as Pascal. His philosophy draws upon both 'analytical' and 'continental' traditions. In Badiou's own opinion, this combination places him awkwardly relative to his contemporaries, meaning that his work had been only slowly taken up. \"Being and Event\" offers an example of this slow uptake, in fact: it was translated into English only in 2005, a full seventeen years after its French publication.\n\nAs is implied in the title of the book, two elements mark the thesis of \"Being and Event\": the place of ontology, or 'the science of being qua being' (being in itself), and the place of the event – which is seen as a rupture in being – through which the subject finds realization and reconciliation with truth. This situation of being and the rupture which characterizes the event are thought in terms of set theory, and specifically Zermelo–Fraenkel set theory (with the axiom of choice), to which Badiou accords a fundamental role in a manner quite distinct from the majority of either mathematicians or philosophers.\n\nFor Badiou the problem which the Greek tradition of philosophy has faced and never satisfactorily dealt with is that while beings themselves are plural, and thought in terms of multiplicity, being itself is thought to be singular; that is, \"it\" is thought in terms of the one. He proposes as the solution to this impasse the following declaration: that the One is not (\"l'Un n'est pas\"). This is why Badiou accords set theory (the axioms of which he refers to as the \"ideas of the multiple\") such stature, and refers to mathematics as the very place of ontology: Only set theory allows one to conceive a 'pure doctrine of the multiple'. Set theory does not operate in terms of definite individual elements in groupings but only functions insofar as what belongs to a set is of the same relation as that set (that is, another set too). What individuates a set, therefore, is not an existential positive proposition, but other multiples whose properties (i.e., \"structural\" relations) validate its presentation. The \"structure\" of being thus secures the regime of the count-as-one. So if one is to think of a set – for instance, the set of people, or humanity – as counting as one, the multiple elements which belong to that set are secured as one consistent concept (humanity), but only in terms of what does \"not\" belong to that set. What is crucial for Badiou is that the structural form of the count-as-one, which makes multiplicities thinkable, implies (somehow or other) that the proper name of \"being\" does not belong to an \"element\" as such (an original 'one'), but rather the void set (written Ø), the set to which nothing (not even the void set itself) belongs. It may help to understand the concept 'count-as-one' if it is associated with the concept of 'terming': a multiple is \"not\" one, but it is referred to with 'multiple': one word. To count a set as one is to mention that set. How the being of terms such as 'multiple' does not contradict the non-being of the one can be understood by considering the multiple nature of terminology: for there to be a term without there also being a system of terminology, within which the difference between terms gives context and meaning to any one term, is impossible. 'Terminology' implies precisely difference between terms (thus multiplicity) as the condition for meaning. The idea of a term without meaning is incoherent, the count-as-one is a \"structural effect\" or a \"situational operation\"; it is not an event of 'truth'. Multiples which are 'composed' or 'consistent' are count-effects. 'Inconsistent multiplicity' [\"meaning?\"] is [somehow or other] 'the presentation of presentation.'\n\nBadiou's use of set theory in this manner is not just illustrative or heuristic. Badiou uses the axioms of Zermelo–Fraenkel set theory to identify the relationship of being to history, Nature, the State, and God. Most significantly this use means that (as with set theory) there is a strict prohibition on self-belonging; a set cannot contain or belong to itself. This results from the axiom of foundation – or the axiom of regularity – which enacts such a prohibition (cf. p. 190 in \"Being and Event\"). (This axiom states that every non-empty set A contains an element y that is disjoint from A.) Badiou's philosophy draws two major implications from this prohibition. Firstly, it secures the inexistence of the 'one': there cannot be a grand overarching set, and thus it is fallacious to conceive of a grand cosmos, a whole Nature, or a Being of God. Badiou is therefore – against Georg Cantor, from whom he draws heavily – staunchly atheist. However, secondly, this prohibition prompts him to introduce the event. Because, according to Badiou, the axiom of foundation 'founds' all sets in the void, it ties all being to the historico-social situation of the multiplicities of de-centred sets – thereby effacing the positivity of subjective action, or an entirely 'new' occurrence. And whilst this is acceptable ontologically, it is unacceptable, Badiou holds, philosophically. Set theory mathematics has consequently 'pragmatically abandoned' an area which philosophy cannot. And so, Badiou argues, there is therefore only one possibility remaining: that ontology can say nothing about the event.\n\nSeveral critics have questioned Badiou's use of mathematics. Mathematician Alan Sokal and physicist Jean Bricmont write that Badiou proposes, with seemingly \"utter seriousness,\" a blending of psychoanalysis, politics and set theory that they contend is preposterous. Similarly, philosopher Roger Scruton has questioned Badiou's grasp of the foundation of mathematics, writing in 2012:\n\nAn example of a critique from a mathematician's point of view is the essay 'Badiou's Number: A Critique of Mathematics as Ontology' by Ricardo L. Nirenberg and David Nirenberg, which takes issue in particular with Badiou's matheme of the Event in \"Being and Event\", which has already been alluded to in respect of the 'axiom of foundation' above. Nirenberg and Nirenberg write:\n\nBadiou again turns here to mathematics and set theory – Badiou's language of ontology – to study the possibility of an indiscernible element existing extrinsically to the situation of ontology. He employs the strategy of the mathematician Paul J. Cohen, using what are called the \"conditions\" of sets. These conditions are thought of in terms of domination, a domination being that which defines a set. (If one takes, in binary language, the set with the condition 'items marked only with ones', any item marked with zero negates the property of the set. The condition which has only ones is thus dominated by any condition which has zeros in it [cf. p. 367-71 in \"Being and Event\"].) Badiou reasons using these conditions that every discernible (nameable or constructible) set is dominated by the conditions which don't possess the property that makes it discernible as a set. (The property 'one' is always dominated by 'not one'.) These sets are, in line with constructible ontology, relative to one's being-in-the-world and one's being in language (where sets and concepts, such as the concept 'humanity', get their names). However, he continues, the dominations themselves are, whilst being relative concepts, not necessarily intrinsic to language and constructible thought; rather one can axiomatically define a domination – in the terms of mathematical ontology – as a set of conditions such that any condition outside the domination is dominated by at least one term inside the domination. One does not necessarily need to refer to constructible language to conceive of a 'set of dominations', which he refers to as the indiscernible set, or the generic set. It is therefore, he continues, possible to think beyond the strictures of the relativistic constructible universe of language, by a process Cohen calls forcing. And he concludes in following that while ontology can mark out a space for an inhabitant of the constructible situation to decide upon the indiscernible, it falls to the subject – about which the ontological situation cannot comment – to nominate this indiscernible, this generic point; and thus nominate, and give name to, the undecidable event. Badiou thereby marks out a philosophy by which to refute the apparent relativism or apoliticism in post-structuralist thought.\n\nBadiou's ultimate ethical maxim is therefore one of: 'decide upon the undecidable'. It is to name the indiscernible, the generic set, and thus name the event that re-casts ontology in a new light. He identifies four domains in which a subject (who, it is important to note, \"becomes\" a subject through this process) can potentially witness an event: love, science, politics and art. By enacting fidelity to the event within these four domains one performs a 'generic procedure', which in its undecidability is necessarily experimental, and one potentially recasts the situation in which being takes place. Through this maintenance of fidelity, truth has the potentiality to emerge.\n\nIn line with his concept of the event, Badiou maintains, politics is not about politicians, but activism based on the present situation and the (his translators' neologism) rupture. So too does love have this characteristic of becoming \"anew\". Even in science the guesswork that marks the event is prominent. He vigorously rejects the tag of 'decisionist' (the idea that once something is decided it 'becomes true'), but rather argues that the recasting of a truth comes prior to its veracity or verifiability. As he says of Galileo (p. 401):\n\nWhile Badiou is keen to reject an equivalence between politics and philosophy, he correlates nonetheless his political activism and skepticism toward the parliamentary-democratic process with his philosophy, based around singular, situated truths, and potential revolutions.\n\nAlain Badiou is a founding member (along with Natacha Michel and Sylvain Lazarus) of the militant French political organisation \"L'Organisation Politique\", which was active from 1985 until it disbanded in 2007. It called itself a post-party organization concerned with direct popular intervention in a wide range of issues (including immigration, labor, and housing). In addition to numerous writings and interventions, \"L'Organisation Politique\" highlighted the importance of developing political prescriptions concerning undocumented migrants (les sans papiers), stressing that they must be conceived primarily as workers and not immigrants.\n\nAlain Badiou gained great notoriety in 2007 with his pamphlet \"The Meaning of Sarkozy\" (\"De quoi Sarkozy est-il le nom?\"), which quickly sold 60,000 copies, whereas for 40 years the sales of his books had oscillated between 2,000 and 6,000 copies.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "17856380", "url": "https://en.wikipedia.org/wiki?curid=17856380", "title": "American Society for Cybernetics", "text": "American Society for Cybernetics\n\nThe American Society for Cybernetics (ASC) is an American non-profit scholastic organization for the advancement of cybernetics as a science and the interdisciplinary collaboration and synthesis of cybernetics. The society contributes to the cooperation around the research and development of cybernetics methods and techniques to manage complex systems.\n\nIn order to do so it holds conferences and seminars, and maintains contacts with cyberneticians and organizations for cybernetics in other countries. Further activities of the ASC are:\n\nThe American Society for Cybernetics was founded in 1964 in Washington, DC to encourage new developments in cybernetics as an interdisciplinary field with Warren McCulloch as first elected president of the ASC. In the 1980s ASC became a member of the International Federation for Systems Research, and begin 1990s supported the Principia Cybernetica Project. Since 1995 the home office for the ASC is located at the George Washington University.\n\nThe ASC has been maintaining an editorial column in the interdisciplinary \"Cybernetics and Human Knowing Journal\" since its first issue in 1992. Rodney Donaldson was the first ASC president to write for the column.\n\nThe \"Wiener Medal in Cybernetics\" is an annual award by the American Society for Cybernetics in recognition of outstanding achievements or contributions in the field of cybernetics. Since 2005 the award has been redefined to recognize achievements and contributions from younger scholars and researchers working in cybernetics or with applications of cybernetics. Recipients of the Wiener and McCulloch awards:\n"}
{"id": "34938611", "url": "https://en.wikipedia.org/wiki?curid=34938611", "title": "Andreotti–Vesentini theorem", "text": "Andreotti–Vesentini theorem\n\nIn mathematics, the Andreotti–Vesentini separation theorem, introduced by states that certain cohomology groups of coherent sheaves are separated.\n\n"}
{"id": "32262980", "url": "https://en.wikipedia.org/wiki?curid=32262980", "title": "André–Oort conjecture", "text": "André–Oort conjecture\n\nIn mathematics, the André–Oort conjecture is an open problem in number theory that generalises the Manin–Mumford conjecture. A prototypical version of the conjecture was stated by Yves André in 1989 and a more general version was conjectured by Frans Oort in 1995. The modern version is a natural generalisation of these two conjectures.\n\nThe conjecture in its modern form is as follows. Let \"S\" be a Shimura variety and let \"V\" be a set of special points in \"S\". Then the irreducible components of the Zariski closure of \"V\" are special subvarieties.\n\nAndré's first version of the conjecture was just for one dimensional subvarieties of Shimura varieties, while Oort proposed that it should work with subvarieties of the moduli space of principally polarised Abelian varieties of dimension \"g\".\n\nVarious results have been established towards the full conjecture by Ben Moonen, Yves André, Andrei Yafaev, Bas Edixhoven, Laurent Clozel, and Emmanuel Ullmo, among others. Most of these results were conditional upon the generalized Riemann hypothesis being true. The biggest unconditional results came in 2009 when Jonathan Pila used techniques from o-minimal geometry and transcendental number theory to prove the conjecture for arbitrary products of modular curves, a result which earned him the 2011 Clay Research Award.\n\nJust as the André–Oort conjecture can be seen as a generalisation of the Manin–Mumford conjecture, so too the André–Oort conjecture can be generalised. The usual generalisation considered is the Zilber–Pink conjecture, an open problem which combines a generalisation of the André–Oort conjecture proposed by Richard Pink and conjectures put forth by Boris Zilber.\n"}
{"id": "5063146", "url": "https://en.wikipedia.org/wiki?curid=5063146", "title": "Applied general equilibrium", "text": "Applied general equilibrium\n\nIn mathematical economics, applied general equilibrium (AGE) models were pioneered by Herbert Scarf at Yale University in 1967, in two papers, and a follow-up book with Terje Hansen in 1973, with the aim of empirically estimating the Arrow–Debreu model of general equilibrium theory with empirical data, to provide \"“a general method for the explicit numerical solution of the neoclassical model”\n\nScarf's method iterated a sequence of simplicial subdivisions which would generate a decreasing sequence of simplices around any solution of the general equilibrium problem. With sufficiently many steps, the sequence would produce a price vector that clears the market.\n\nBrouwer's Fixed Point theorem states that a continuous mapping of a simplex into itself has at least one fixed point. This paper describes a numerical algorithm for approximating, in a sense to be explained below, a fixed point of such a mapping (Scarf 1967a: 1326).\n\nScarf never built an AGE model, but hinted that “these novel numerical techniques might be useful in assessing consequences for the economy of a change in the economic environment” (Kehoe et al. 2005, citing Scarf 1967b). His students elaborated the Scarf algorithm into a tool box, where the price vector could be solved for any changes in policies (or exogenous shocks), giving the equilibrium ‘adjustments’ needed for the prices. This method was first used by Shoven and Whalley (1972 and 1973), and then was developed through the 1970s by Scarf’s students and others.\n\nMost contemporary applied general equilibrium models are numerical\nanalogs of traditional two-sector general equilibrium models popularized\nby James Meade, Harry Johnson, Arnold Harberger, and others in the\n1950s and 1960s. Earlier analytic work with these models has examined\nthe distortionary effects of taxes, tariffs, and other policies, along with\nfunctional incidence questions. More recent applied models, including\nthose discussed here, provide numerical estimates of efficiency and distributional\neffects within the same framework.\n\nScarf's fixed-point method was a break-through in the mathematics of computation generally, and specifically in optimization and computational economics. Later researchers continued to develop iterative methods for computing fixed-points, both for topological models like Scarf's and for models described by functions with continuous second derivatives or convexity or both. Of course, \"global Newton methods\" for essentially convex and smooth functions and path-following methods for diffeomorphisms converged faster than did robust algorithms for continuous functions, when the smooth methods are applicable.\n\nAGE models, being based on Arrow–Debreu general equilibrium theory, work in a different manner than CGE models. The model first establishes the existence of equilibrium through the standard Arrow–Debreu exposition, then inputs data into all the various sectors, and then applies Scarf’s algorithm (Scarf 1967a, 1967b and Scarf with Hansen 1973) to solve for a price vector that would clear all markets. This algorithm would narrow down the possible relative prices through a simplex method, which kept reducing the size of the ‘net’ within which possible solutions were found. AGE modelers then consciously choose a cutoff, and set an approximate solution as the net never closed on a unique point through the iteration process.\n\nCGE models are based on macro balancing equations, and use an equal number of equations (based on the standard macro balancing equations) and unknowns solvable as simultaneous equations, where exogenous variables are changed outside the model, to give the endogenous results.\n\n"}
{"id": "35764367", "url": "https://en.wikipedia.org/wiki?curid=35764367", "title": "Bounded growth", "text": "Bounded growth\n\nBounded growth occurs when the growth rate of a mathematical function is constantly increasing at a decreasing rate. Asymptotically, bounded growth approaches a fixed value.\nThis contrasts with exponential growth, which is constantly increasing at an accelerating rate, and therefore approaches infinity in the limit.\n\nAn example of bounded growth is the logistic function.\n\n"}
{"id": "3676801", "url": "https://en.wikipedia.org/wiki?curid=3676801", "title": "Centre for Computational Geography", "text": "Centre for Computational Geography\n\nThe Centre for Computational Geography (CCG) is an inter-disciplinary research centre based at the University of Leeds. The CCG was founded in 1993 by Stan Openshaw and Phil Rees, and builds on over 40 years experience in spatial analysis and modelling within the School of Geography. CCG research is concerned with the development and application of tools for analysis, visualisation and modelling geographical systems.\n\n"}
{"id": "3655598", "url": "https://en.wikipedia.org/wiki?curid=3655598", "title": "Choquet theory", "text": "Choquet theory\n\nIn mathematics, Choquet theory, named after Gustave Choquet, is an area of functional analysis and convex analysis concerned with measures which have support on the extreme points of a convex set \"C\". Roughly speaking, every vector of \"C\" should appear as a weighted average of extreme points, a concept made more precise by generalizing the notion of weighted average from a convex combination to an integral taken over the set \"E\" of extreme points. Here \"C\" is a subset of a real vector space \"V\", and the main thrust of the theory is to treat the cases where \"V\" is an infinite-dimensional (locally convex Hausdorff) topological vector space along lines similar to the finite-dimensional case. The main concerns of Gustave Choquet were in potential theory. Choquet theory has become a general paradigm, particularly for treating convex cones as determined by their extreme rays, and so for many different notions of \"positivity\" in mathematics.\n\nThe two ends of a line segment determine the points in between: in vector terms the segment from \"v\" to \"w\" consists of the λ\"v\" + (1 − λ)\"w\" with 0 ≤ λ ≤ 1. The classical result of Hermann Minkowski says that in Euclidean space, a bounded, closed convex set \"C\" is the convex hull of its extreme point set \"E\", so that any \"c\" in \"C\" is a (finite) convex combination of points \"e\" of \"E\". Here \"E\" may be a finite or an infinite set. In vector terms, by assigning non-negative weights \"w\"(\"e\") to the \"e\" in \"E\", almost all 0, we can represent any \"c\" in \"C\" as\n\nwith \n\nIn any case the \"w\"(\"e\") give a probability measure supported on a finite subset of \"E\". For any affine function \"f\" on \"C\", its value at the point \"c\" is \n\nIn the infinite dimensional setting, one would like to make a similar statement.\n\nChoquet's theorem states that for a compact convex subset \"C\" of a normed space \"V\", given \"c\" in \"C\" there exists a probability measure \"w\" supported on the set \"E\" of extreme points of \"C\" such that, for any affine function \"f\" on \"C,\" \n\nIn practice \"V\" will be a Banach space. The original Krein–Milman theorem follows from Choquet's result. Another corollary is the Riesz representation theorem for states on the continuous functions on a metrizable compact Hausdorff space.\n\nMore generally, for \"V\" a locally convex topological vector space, the Choquet–Bishop–de Leeuw theorem gives the same formal statement.\n\nIn addition to the existence of a probability measure supported on the extreme boundary that represents a given point \"c\", one might also consider the uniqueness of such measures. It is easy to see that uniqueness does not hold even in the finite dimensional setting. One can take, for counterexamples, the convex set to be a cube or a ball in R. Uniqueness does hold, however, when the convex set is a finite dimensional simplex. So that the weights \"w\"(\"e\") are unique. A finite dimensional simplex is a special case of a Choquet simplex. Any point in a Choquet simplex is represented by a unique probability measure on the extreme points.\n\n\n"}
{"id": "19448392", "url": "https://en.wikipedia.org/wiki?curid=19448392", "title": "Circular coloring", "text": "Circular coloring\n\nIn graph theory, circular coloring may be viewed as a refinement of usual graph coloring. The \"circular chromatic number\" of a graph formula_1, denoted formula_2 can be given by any of the following definitions, all of which are equivalent (for finite graphs).\n\n\nIt is relatively easy to see that formula_16 (especially using 1. or 2.), but in fact formula_17. It is in this sense that we view circular chromatic number as a refinement of the usual chromatic number. \n\nCircular coloring was originally defined by , who called it \"star coloring\".\n\nColoring is dual to the subject of nowhere-zero flows and indeed, circular coloring has a natural dual notion: circular flows.\n\nFor integers formula_18 such that formula_19, the circular complete graph (also known as a circular clique) is the graph with vertex set formula_10 and edges between elements at distance formula_11 apart.\nThat is, the vertices are numbers {0, 1, ..., \"n\"-1} and vertex \"i\" is adjacent to:\nFor example, is just the complete graph , while is isomorphic to the cycle graph .\n\nA circular coloring is then, according to the second definition above, a homomorphism into a circular complete graph.\nThe crucial fact about these graphs is that admits a homomorphism into if and only if \"a/b\" ≤ \"c/d\". This justifies the notation, since if the rational numbers \"a/b\" and \"c/d\" are equal, then and are homomorphically equivalent. \nMoreover, the homomorphism order among them refines the order given by complete graphs into a dense order, corresponding to rational numbers formula_22. For example\nor equivalently\nThe example on the figure can be interpreted as a homomorphism from the flower snark into , which comes earlier than , corresponding to the fact that formula_23.\n\n\n"}
{"id": "435399", "url": "https://en.wikipedia.org/wiki?curid=435399", "title": "Common cause and special cause (statistics)", "text": "Common cause and special cause (statistics)\n\nCommon and special causes are the two distinct origins of variation in a process, as defined in the statistical thinking and methods of Walter A. Shewhart and W. Edwards Deming. Briefly, \"common causes\", also called natural patterns, are the usual, historical, quantifiable variation in a system, while \"special causes\" are unusual, not previously observed, non-quantifiable variation.\n\nThe distinction is fundamental in philosophy of statistics and philosophy of probability, with different treatment of these issues being a classic issue of probability interpretations, being recognised and discussed as early as 1703 by Gottfried Leibniz; various alternative names have been used over the years.\n\nThe distinction has been particularly important in the thinking of economists Frank Knight, John Maynard Keynes and G. L. S. Shackle.\n\nIn 1703, Jacob Bernoulli wrote to Gottfried Leibniz to discuss their shared interest in applying mathematics and probability to games of chance. Bernoulli speculated whether it would be possible to gather mortality data from gravestones and thereby calculate, by their existing practice, the probability of a man currently aged 20 years outliving a man aged 60 years. Leibniz replied that he doubted this was possible:\n\n\"Nature has established patterns originating in the return of events but only for the most part. New illnesses flood the human race, so that no matter how many experiments you have done on corpses, you have not thereby imposed a limit on the nature of events so that in the future they could not vary.\"\n\nThis captures the central idea that some variation is predictable, at least approximately in frequency. This \"common-cause variation\" is evident from the experience base. However, new, unanticipated, emergent or previously neglected phenomena (e.g. \"new diseases\") result in variation outside the historical experience base. Shewhart and Deming argued that such \"special-cause variation\" is fundamentally unpredictable in frequency of occurrence or in severity.\n\nJohn Maynard Keynes emphasised the importance of special-cause variation when he wrote:\n\n\"By \"uncertain\" knowledge ... I do not mean merely to distinguish what is known for certain from what is only probable. The game of roulette is not subject, in this sense, to uncertainty ... The sense in which I am using the term is that in which the prospect of a European war is uncertain, or the price of copper and the rate of interest twenty years hence, or the obsolescence of a new invention ... About these matters there is no scientific basis on which to form any calculable probability whatever. We simply do not know!\"\n\nCommon-cause variation is characterised by:\n\n\nThe outcomes of a perfectly balanced roulette wheel are a good example of common-cause variation. Common-cause variation is the \"noise\" within the system.\n\nWalter A. Shewhart originally used the term \"chance cause\". The term \"common cause\" was coined by Harry Alpert in 1947. The Western Electric Company used the term \"natural pattern\". Shewhart called a process that features only common-cause variation as being \"in statistical control\". This term is deprecated by some modern statisticians who prefer the phrase \"stable and predictable\".\n\nSpecial-cause variation is characterised by:\n\n\nSpecial-cause variation always arrives as a surprise. It is the \"signal\" within a system.\n\nWalter A. Shewhart originally used the term \"assignable cause\". The term \"special-cause\" was coined by W. Edwards Deming. The Western Electric Company used the term \"unnatural pattern\".\n\n\n\nIn economics, this circle of ideas is analysed under the rubric of \"Knightian uncertainty\". John Maynard Keynes and Frank Knight both discussed the inherent unpredictability of economic systems in their work and used it to criticise the mathematical approach to economics, in terms of expected utility, developed by Ludwig von Mises and others. Keynes in particular argued that economic systems did not automatically tend to the equilibrium of full employment owing to their agents' inability to predict the future. As he remarked in \"The General Theory of Employment, Interest and Money\":\n\n\"... as living and moving beings, we are forced to act ... [even when] our existing knowledge does not provide a sufficient basis for a calculated mathematical expectation.\"\n\nKeynes' thinking was at odds with the classical liberalism of the Austrian School of economists, but G. L. S. Shackle recognised the importance of Keynes's insight and sought to formalise it within a free-market philosophy.\n\nIn financial economics, the black swan theory of Nassim Nicholas Taleb is based on the significance and unpredictability of special causes.\n\nA special-cause failure is a failure that can be corrected by changing a component or process, whereas a common-cause failure is equivalent to noise in the system and specific actions cannot be made to prevent the failure.\n\nHarry Alpert observed:\n\nThe quote recognises that there is a temptation to react to an extreme outcome and to see it as significant, even where its causes are common to many situations and the distinctive circumstances surrounding its occurrence, the results of mere chance. Such behaviour has many implications within management, often leading to ad hoc interventions that merely increase the level of variation and frequency of undesirable outcomes.\n\nDeming and Shewhart both advocated the control chart as a means of managing a business process in an economically efficient manner.\n\nWithin the frequency probability framework, there is no process whereby a probability can be attached to the future occurrence of special cause. One might naively ask whether the Bayesian approach does allow such a probability to be specified. The existence of special-cause variation led Keynes and Deming to an interest in Bayesian probability, but no formal synthesis emerged from their work. Most statisticians of the Shewhart-Deming school take the view that special causes are not embedded in either experience or in current thinking (that's why they come as a surprise; their prior probability has been neglected—in effect, assigned the value zero) so that any subjective probability is doomed to be hopelessly badly calibrated in practice.\n\nIt is immediately apparent from the Leibniz quote above that there are implications for sampling. Deming observed that in any forecasting activity, the population is that of future events while the sampling frame is, inevitably, some subset of historical events. Deming held that the disjoint nature of population and sampling frame was inherently problematic once the existence of special-cause variation was admitted, rejecting the general use of probability and conventional statistics in such\nsituations. He articulated the difficulty as the distinction between analytic and enumerative statistical studies.\n\nShewhart argued that, as processes subject to special-cause variation were inherently unpredictable, the usual techniques of probability could not be used to separate special-cause from common-cause variation. He developed the control chart as a statistical heuristic to distinguish the two types of variation. Both Deming and Shewhart advocated the control chart as a means of assessing a process's state of statistical control and as a foundation for forecasting.\n\nKeynes identified three domains of probability:\n\n\nand sought to base a probability theory thereon.\n\nCommon mode failure has a more specific meaning in engineering. It refers to events which are not statistically independent. Failures in multiple parts of a system may be caused by a single fault, particularly random failures due to environmental conditions or aging. An example is when all of the pumps for a fire sprinkler system are located in one room. If the room becomes too hot for the pumps to operate, they will all fail at essentially the same time, from one cause (the heat in the room). Another example is an electronic system wherein a fault in a power supply injects noise onto a supply line, causing failures in multiple subsystems.\n\nThis is particularly important in safety-critical systems using multiple redundant channels. If the probability of failure in one subsystem is \"p\", then it would be expected that an \"N\" channel system would have a probability of failure of \"p\" However, in practice, the probability of failure is much higher because they are not statistically independent; for example ionizing radiation or electromagnetic interference (EMI) may affect all the channels.\n\nThe \"principle of redundancy\" states that, when events of failure of a component are statistically independent, the probabilities of their joint occurrence multiply. Thus, for instance, if the probability of failure of a component of a system is one in one thousand per year, the probability of the joint failure of two of them is one in one million per year, provided that the two events are statistically independent. This principle favors the strategy of the redundancy of components. One place this strategy is implemented is in RAID 1, where two hard disks store a computer's data redundantly.\n\nBut even so there can be many common modes: consider a RAID1 where two disks are purchased online and are installed in a computer, there can be many common modes:\n\n\nAlso, if the events of failure of two components are maximally statistically dependent, the probability of the joint failure of both is identical to the probability of failure of them individually. In such a case, the advantages of redundancy are negated. Strategies for the avoidance of common mode failures include keeping redundant components physically isolated.\n\nA prime example of redundancy with isolation is a nuclear power plant. The new ABWR has three divisions of Emergency Core Cooling Systems, each with its own generators and pumps and each isolated from the others. The new European Pressurized Reactor has two containment buildings, one inside the other. However, even here it is possible for a common mode failure to occur (for example, in the Fukushima Daiichi Nuclear Power Plant, mains power was severed by the Tōhoku earthquake, then the thirteen backup diesel generators were all simultaneously disabled by the subsequent tsunami that flooded the basements of the turbine halls).\n\n\n"}
{"id": "7472170", "url": "https://en.wikipedia.org/wiki?curid=7472170", "title": "Composition of relations", "text": "Composition of relations\n\nIn the mathematics of binary relations, the composition relations is a concept of forming a new relation from two given relations \"R\" and \"S\". The composition of relations is called relative multiplication in the calculus of relations. The composition is then the relative product of the factor relations. Composition of functions is a special case of composition of relations.\n\nThe words uncle and aunt indicate a compound relation: for a person to be an uncle, he must be a brother of a parent (or a sister for an aunt). In algebraic logic it is said that the relation of Uncle ( \"xUz\" ) is the composition of relations \"is a brother of\" ( \"xBy\" ) and \"is a parent of\" ( \"yPz\" ).\n\nBeginning with Augustus De Morgan, the traditional form of reasoning with by syllogism has been subsumed by relational logical expressions and their composition.\n\nIf formula_2 and formula_3 are two binary relations, then\ntheir composition formula_4 is the relation\n\nIn other words, formula_6 is defined by the rule that says formula_7 if and only if there is an element formula_8 such that formula_9 (i.e. formula_10 and formula_11).\n\nIn particular fields, authors might denote by what is defined here to be .\nThe convention chosen here is such that function composition (with the usual notation) is obtained as a special case, when \"R\" and \"S\" are functional relations. Some authors prefer to write formula_12 and formula_13 explicitly when necessary, depending whether the left or the right relation is the first one applied.\n\nA further variation encountered in computer science is the Z notation: formula_14 is used to denote the traditional (right) composition, but ⨾ ; (a fat open semicolon with Unicode code point U+2A3E) denotes left composition. This use of semicolon coincides with the notation for function composition used (mostly by computer scientists) in category theory, as well as the notation for dynamic conjunction within linguistic dynamic semantics. The semicolon notation (with this semantic) was introduced by Ernst Schröder in 1895.\n\nThe binary relations formula_2 are sometimes regarded as the morphisms formula_16 in a category Rel which has the sets as objects. In Rel, composition of morphisms is exactly composition of relations as defined above. The category Set of sets is a subcategory of Rel that has the same objects but fewer morphisms. A generalization of this is found in the theory of allegories.\n\n\n\nFinite homogeneous binary relations are represented by logical matrices. The entries of these matrices are either zero or one, depending on whether the relation represented is false or true for the row and column corresponding to compared objects. Working with such matrices involves the Boolean arithmetic with 1 + 1 = 1 and 1 × 1 = 1. An entry in the matrix product of two logical matrices will be 1, then, only if the row and column multiplied have a corresponding 1. Thus the logical matrix of a composition of relations can be found by computing the matrix product of the matrices representing the factors of the composition. \"Matrices constitute a method for \"computing\" the conclusions traditionally drawn by means of hypothetical syllogisms and sorites.\"\n\nConsider a heterogeneous relation \"R\" ⊆ \"A\" × \"B\". Then using composition of relation \"R\" with its converse \"R\", there are homogeneous relations \"R R\" (on \"A\") and \"R\" \"R\" (on \"B\").\n\nIf ∀\"x\" ∈ \"A\" ∃\"b\" ∈ B \"aRb\" (\"R\" is a total relation), then ∀\"x\" \"xRR\"\"x\" so that \"R R\" is a reflexive relation or I ⊆ \"R R\" where I is the identity relation {\"x\"I\"x\" : \"x\" ∈ \"A\"}. Similarly, if \"R\" is a surjective relation then \n\nThe composition formula_18 is used to distinguish relations of Ferrer's type, which satisfy formula_19\n\nLet \"A\" = { France, Germany, Italy, Switzerland } and \"B\" = { French, German, Italian } with the relation \"R\" given by \"aRb\" when \"b\" is a national language of \"a\". The logical matrix for \"R\" is given by\n\nFor a given set \"V\", the collection of all binary relations on \"V\" forms a Boolean lattice ordered by inclusion (⊆). Recall that complementation reverses inclusion:\nformula_23 In the calculus of relations it is common to represent the complement of a set by an overbar: formula_24\n\nIf \"S\" is a binary relation, let formula_25 represent the converse relation, also called the \"transpose\". Then the Schröder rules are\nVerbally, one equivalence can be obtained from another: select the first or second factor and transpose it; then complement the other two relations and permute them.\n\nThough this transformation of an inclusion of a composition of relations was detailed by Ernst Schröder, in fact Augustus De Morgan first articulated the transformation as Theorem K in 1860. He wrote \n\nWith Schröder rules and complementation one can solve for an unknown relation \"X\" in relation inclusions such as \nFor instance, by Schröder rule formula_29 and complementation gives formula_30 which is called the right residual of S by R .\n\nJust as composition of relations is a type of multiplication resulting in a product, so some compositions compare to division and produce quotients. Three quotients are exhibited here: left residual, right residual, and symmetric quotient. The left residual of two relations is defined presuming that they have the same domain (source), and the right residual presumes the same codomain (range, target). The symmetric quotient presumes two relations share a domain.\n\nDefinitions:\n\nUsing Schröder's rules, \"AX\" ⊆ \"B\" is equivalent to \"X\" ⊆ \"A\"\\\"B\". Thus the left residual is the greatest relation satisfying \"AX\" ⊆ \"B\". Similarly, the inclusion \"YC\" ⊆ \"D\" is equivalent to \"Y\" ⊆ \"D\"/\"C\", and the right residual is the greatest relation satisfying \"YC\" ⊆ \"D\".\n\nOther forms of composition of relations, which apply to general \"n\"-place relations instead of binary relations, are found in the \"join\" operation of relational algebra. The usual composition of two binary relations as defined here can be obtained by taking their join, leading to a ternary relation, followed by a projection that removes the middle component. For example, in the query language SQL there is the operation Join (SQL).\n\n\n"}
{"id": "28311949", "url": "https://en.wikipedia.org/wiki?curid=28311949", "title": "Continuum structure function", "text": "Continuum structure function\n\nIn mathematics, a continuum structure function (CSF) is defined by Laurence Baxter as a nondecreasing mapping from the unit hypercube to the unit interval. It is used by Baxter to help in the Mathematical modelling of the level of performance of a system in terms of the performance levels of its components.\n\n"}
{"id": "34796730", "url": "https://en.wikipedia.org/wiki?curid=34796730", "title": "Danny Dolev", "text": "Danny Dolev\n\nDaniel (Danny) Dolev is an Israeli computer scientist known for his research in cryptography and distributed computing. He holds the Berthold Badler Chair in Computer Science at the Hebrew University of Jerusalem and is a member of the scientific council of the European Research Council.\n\nDolev did his undergraduate studies at the Hebrew University, earning a bachelor's degree in 1971. He then moved to the Weizmann Institute of Science, earning a master's degree in 1973 and a doctorate in 1979 under the supervision of Eli Shamir. After postdoctoral research at Stanford University and IBM Research, he joined the Hebrew University faculty in 1982. He took a second position at the IBM Almaden Research Center from 1987 to 1993, but retained his appointment at the Hebrew University. From 1998 to 2002, he was chair of the Institute of Computer Science and then Director of the School of Engineering and Computer Science at the Hebrew University. In 2011, he became the first Israeli on the Scientific Council of the European Research Council.\n\nDolev has published many highly cited papers, including works on public-key cryptography, non-malleable cryptography, consensus in asynchronous distributed systems, atomic broadcasting, high availability and high-availability clusters, and Byzantine fault tolerance.\n\nDolev was elected as an ACM Fellow in 2007 for his \"contributions to fault-tolerant distributed computing\". In 2011, Dolev and his co-authors Hagit Attiya and Amotz Bar-Noy were honored with the Edsger W. Dijkstra Prize in Distributed Computing for their work on implementing shared memory using message passing.\n"}
{"id": "6356291", "url": "https://en.wikipedia.org/wiki?curid=6356291", "title": "David Pingree", "text": "David Pingree\n\nDavid Edwin Pingree (January 2, 1933, New Haven, Connecticut – November 11, 2005, Providence, Rhode Island) was a University Professor, and Professor of History of Mathematics and Classics at Brown University, and one of America's leading historians of the exact sciences in antiquity (primarily mathematics).\n\nHe graduated from Phillips Academy in Andover, Massachusetts in 1950 and thereafter attended Harvard University, where he earned his doctorate in 1960 with a dissertation on the supposed transmission of Hellenistic astrology to India under the joint supervision of Daniel Henry Holmes Ingalls, Sr. and Otto Eduard Neugebauer. After completing his PhD, Pingree remained at Harvard three more years as a member of its Society of Fellows before moving to the University of Chicago to accept the position of Research Associate at the Oriental Institute.\n\nHe joined the History of Mathematics Department at Brown University in 1971, eventually holding the chair until his death.\n\nAs successor to Otto Neugebauer (1899–1990) in Brown's History of Mathematics Department (which Neugebauer established in 1947), Pingree numbered among his colleagues men of extraordinary learning, especially Abraham Sachs and Gerald Toomer.\n\nD. Pingree is known for propounding his theory of \"unoriginality\" of the Indian science of astronomy (jyotiṣa), the majority of this science rests, according to his preference, solely on foreign concepts. This theory of unoriginality is highly debated amongst scholars and some of his arguments have been dismissed. K. S. Shukla for example, points out Pingree’s free and incorrect amendations to the manuscript of the Yavanajātaka, which Pingree believed to be highly corrupted.\n\nJon McGinnis of the University of Missouri, St. Louis, describes Pingree's life-work thus:\n\n... Pingree devoted himself to the study of the exact sciences, such as mathematics, mathematical astronomy and astral omens. He was also acutely interested in the transmission of those sciences across cultural and linguistic boundaries. His interest in the transmission of the exact sciences came from two fronts or, perhaps more correctly, his interest represents two sides of the same coin. On the one hand, he was concerned with how one culture might appropriate, and so alter, the science of another (earlier) culture in order to make that earlier scientific knowledge more accessible to the recipient culture. On the other hand, Pingree was also interested in how scientific texts surviving from a later culture might be used to reconstruct or cast light on our fragmentary records of earlier sciences. In this quest, Pingree would, with equal facility use ancient Greek works to clarify Babylonian texts on divination, turn to Arabic treatises to illuminate early Greek astronomical and astrological texts, seek Sanskrit texts to explain Arabic astronomy, or track the appearance of Indian astronomy in medieval Europe.\n\nIn June, 2007 the Brown University Library acquired Pingree's personal collection of scholarly materials. The collection focuses on the study of mathematics and exact sciences in the ancient world, especially India, and the relationship of Eastern mathematics to the development of mathematics and related disciplines in the West. The collection contains some 22,000 volumes, 700 fascicles, and a number of manuscripts. The holdings consist of both antiquarian and recent materials published in Sanskrit, Arabic, Hindi, Persian and Western languages.\n\nRecipient of a Guggenheim Fellowship in 1975 and a MacArthur Fellowship in 1981, he was a member of the Society of Fellows at Harvard, the American Philosophical Society, and the Institute for Advanced Study; he was also A.D. White Professor-at-Large at Cornell University from 1995.\n\n\n"}
{"id": "1908142", "url": "https://en.wikipedia.org/wiki?curid=1908142", "title": "Finite-difference time-domain method", "text": "Finite-difference time-domain method\n\nFinite-difference time-domain or Yee's method (named after the Chinese American applied mathematician Kane S. Yee, born 1934) is a numerical analysis technique used for modeling computational electrodynamics (finding approximate solutions to the associated system of differential equations). Since it is a time-domain method, FDTD solutions can cover a wide frequency range with a single simulation run, and treat nonlinear material properties in a natural way.\n\nThe FDTD method belongs in the general class of grid-based differential numerical modeling methods (finite difference methods). The time-dependent Maxwell's equations (in partial differential form) are discretized using central-difference approximations to the space and time partial derivatives. The resulting finite-difference equations are solved in either software or hardware in a leapfrog manner: the electric field vector components in a volume of space are solved at a given instant in time; then the magnetic field vector components in the same spatial volume are solved at the next instant in time; and the process is repeated over and over again until the desired transient or steady-state electromagnetic field behavior is fully evolved.\n\nFinite difference schemes for time-dependent partial differential equations (PDEs) have been employed for many years in computational fluid dynamics problems, including the idea of using centered finite difference operators on staggered grids in space and time to achieve second-order accuracy.\nThe novelty of Kane Yee's FDTD scheme, presented in his seminal 1966 paper, was to apply centered finite difference operators on staggered grids in space and time for each electric and magnetic vector field component in Maxwell's curl equations.\nThe descriptor \"Finite-difference time-domain\" and its corresponding \"FDTD\" acronym were originated by Allen Taflove in 1980.\nSince about 1990, FDTD techniques have emerged as primary means to computationally model many scientific and engineering problems dealing with electromagnetic wave interactions with material structures. Current FDTD modeling applications range from near-DC (ultralow-frequency geophysics involving the entire Earth-ionosphere waveguide) through microwaves (radar signature technology, antennas, wireless communications devices, digital interconnects, biomedical imaging/treatment) to visible light (photonic crystals, nanoplasmonics, solitons, and biophotonics). In 2006, an estimated 2,000 FDTD-related publications appeared in the science and engineering literature (see Popularity). As of 2013, there are at least 25 commercial/proprietary FDTD software vendors; 13 free-software/open-source-software FDTD projects; and 2 freeware/closed-source FDTD projects, some not for commercial use (see External links).\n\nAn appreciation of the basis, technical development, and possible future of FDTD numerical techniques for Maxwell’s equations can be developed by first considering their history. The following lists some of the key publications in this area.\n\nWhen Maxwell's differential equations are examined, it can be seen that the change in the E-field in time (the time derivative) is dependent on the change in the H-field across space (the curl). This results in the basic FDTD time-stepping relation that, at any point in space, the updated value of the E-field in time is dependent on the stored value of the E-field and the numerical curl of the local distribution of the H-field in space.\n\nThe H-field is time-stepped in a similar manner. At any point in space, the updated value of the H-field in time is dependent on the stored value of the H-field and the numerical curl of the local distribution of the E-field in space. Iterating the E-field and H-field updates results in a marching-in-time process wherein sampled-data analogs of the continuous electromagnetic waves under consideration propagate in a numerical grid stored in the computer memory.\nThis description holds true for 1-D, 2-D, and 3-D FDTD techniques. When multiple dimensions are considered, calculating the numerical curl can become complicated. Kane Yee's seminal 1966 paper proposed spatially staggering the vector components of the E-field and H-field about rectangular unit cells of a Cartesian computational grid so that each E-field vector component is located midway between a pair of H-field vector components, and conversely. This scheme, now known as a Yee lattice, has proven to be very robust, and remains at the core of many current FDTD software constructs.\n\nFurthermore, Yee proposed a leapfrog scheme for marching in time wherein the E-field and H-field updates are staggered so that E-field updates are conducted midway during each time-step between successive H-field updates, and conversely. On the plus side, this explicit time-stepping scheme avoids the need to solve simultaneous equations, and furthermore yields dissipation-free numerical wave propagation. On the minus side, this scheme mandates an upper bound on the time-step to ensure numerical stability. As a result, certain classes of simulations can require many thousands of time-steps for completion.\n\nTo implement an FDTD solution of Maxwell's equations, a computational domain must first be established. The computational domain is simply the physical region over which the simulation will be performed. The E and H fields are determined at every point in space within that computational domain. The material of each cell within the computational domain must be specified. Typically, the material is either free-space (air), metal, or dielectric. Any material can be used as long as the permeability, permittivity, and conductivity are specified.\n\nThe permittivity of dispersive materials in tabular form cannot be directly substituted into the FDTD scheme.\nInstead, it can be approximated using multiple Debye, Drude, Lorentz or critical point terms.\nThis approximation can be obtained using open fitting programs and does not necessarily have physical meaning.\n\nOnce the computational domain and the grid materials are established, a source is specified. The source can be current on a wire, applied electric field or impinging plane wave.\nIn the last case FDTD can be used to simulate light scattering from arbitrary shaped objects, planar periodic structures at various incident angles, and photonic band structure of infinite periodic structures.\n\nSince the E and H fields are determined directly, the output of the simulation is usually the E or H field at a point or a series of points within the computational domain. The simulation evolves the E and H fields forward in time.\n\nProcessing may be done on the E and H fields returned by the simulation. Data processing may also occur while the simulation is ongoing.\n\nWhile the FDTD technique computes electromagnetic fields within a compact spatial region, scattered and/or radiated far fields can be obtained via near-to-far-field transformations.\n\nEvery modeling technique has strengths and weaknesses, and the FDTD method is no different.\n\n\n(PSSD), which instead propagates the fields forward in space.\n\nThe most commonly used grid truncation techniques for open-region FDTD modeling problems are the Mur absorbing boundary condition (ABC), the Liao ABC, and various perfectly matched layer (PML) formulations. The Mur and Liao techniques are simpler than PML. However, PML (which is technically an absorbing region rather than a boundary condition \"per se\") can provide orders-of-magnitude lower reflections. The PML concept was introduced by J.-P. Berenger in a seminal 1994 paper in the Journal of Computational Physics. Since 1994, Berenger's original split-field implementation has been modified and extended to the uniaxial PML (UPML), the convolutional PML (CPML), and the higher-order PML. The latter two PML formulations have increased ability to absorb evanescent waves, and therefore can in principle be placed closer to a simulated scattering or radiating structure than Berenger's original formulation.\n\nTo reduce undesired numerical reflection from the PML additional back absorbing layers technique can be used.\n\nNotwithstanding both the general increase in academic publication\nthroughput during the same period and the overall expansion of interest\nin all Computational electromagnetics (CEM) techniques, there are\nseven primary reasons for the tremendous expansion of interest in FDTD\ncomputational solution approaches for Maxwell’s equations:\n\nTaflove has argued that these factors combine to suggest that FDTD will remain one of\nthe dominant computational electrodynamics techniques (as well as potentially other multiphysics problems).\n\nThere are hundreds of simulation tools that implement FDTD algorithms, many optimized to run on parallel-processing clusters.\n\nFrederick Moxley suggests further applications with computational quantum mechanics and simulations.\n\n\nThe following article in \"Nature Milestones: Photons\" illustrates the historical significance of the FDTD method as related to Maxwell's equations:\n\nAllen Taflove's interview, \"Numerical Solution,\" in the January 2015 focus issue of \"Nature Photonics\" honoring the 150th anniversary of the publication of Maxwell's equations. This interview touches on how the development of FDTD ties into the century and one-half history of Maxwell's theory of electrodynamics:\n\nThe following university-level textbooks provide a good general introduction to the FDTD method:\n\n\nFree software/Open-source software FDTD projects:\nFreeware/Closed source FDTD projects (some not for commercial use):\n"}
{"id": "58527", "url": "https://en.wikipedia.org/wiki?curid=58527", "title": "Finitely generated abelian group", "text": "Finitely generated abelian group\n\nIn abstract algebra, an abelian group is called finitely generated if there exist finitely many elements \"x\", ..., \"x\" in \"G\" such that every \"x\" in \"G\" can be written in the form\nwith integers \"n\", ..., \"n\". In this case, we say that the set is a \"generating set\" of \"G\" or that \"x\", ..., \"x\" \"generate\" \"G\".\n\nEvery finite abelian group is finitely generated. The finitely generated abelian groups can be completely classified.\n\n\nThere are no other examples (up to isomorphism). In particular, the group formula_4 of rational numbers is not finitely generated: if formula_5 are rational numbers, pick a natural number formula_6 coprime to all the denominators; then formula_7 cannot be generated by formula_5. The group formula_9 of non-zero rational numbers is also not finitely generated. The groups of real numbers under addition formula_10 and non-zero real numbers under multiplication formula_11 are also not finitely generated.\n\nThe fundamental theorem of finitely generated abelian groups can be stated two ways, generalizing the two forms of the fundamental theorem of \"finite\" abelian groups. The theorem, in both forms, in turn generalizes to the structure theorem for finitely generated modules over a principal ideal domain, which in turn admits further generalizations.\n\nThe primary decomposition formulation states that every finitely generated abelian group \"G\" is isomorphic to a direct sum of primary cyclic groups and infinite cyclic groups. A primary cyclic group is one whose order is a power of a prime. That is, every finitely generated abelian group is isomorphic to a group of the form\nwhere \"n\" ≥ 0 is the \"rank\", and the numbers \"q\", ..., \"q\" are powers of (not necessarily distinct) prime numbers. In particular, \"G\" is finite if and only if \"n\" = 0. The values of \"n\", \"q\", ..., \"q\" are (up to rearranging the indices) uniquely determined by \"G\".\n\nWe can also write any finitely generated abelian group \"G\" as a direct sum of the form\nwhere \"k\" divides \"k\", which divides \"k\" and so on up to \"k\". Again, the rank \"n\" and the \"invariant factors\" \"k\", ..., \"k\" are uniquely determined by \"G\" (here with a unique order). The rank and the sequence of invariant factors determine the group up to isomorphism.\n\nThese statements are equivalent as a result of the Chinese remainder theorem, which implies that formula_14 if and only if \"j\" and \"k\" are coprime.\n\nThe history and credit for the fundamental theorem is complicated by the fact that it was proven when group theory was not well-established, and thus early forms, while essentially the modern result and proof, are often stated for a specific case. Briefly, an early form of the finite case was proven in , the finite case was proven in , and stated in group-theoretic terms in . The finitely \"presented\" case is solved by Smith normal form, and hence frequently credited to , though the finitely \"generated\" case is sometimes instead credited to ; details follow.\n\nGroup theorist László Fuchs states:\nThe fundamental theorem for \"finite\" abelian groups was proven by Leopold Kronecker in , using a group-theoretic proof, though without stating it in group-theoretic terms; a modern presentation of Kronecker's proof is given in , 5.2.2 Kronecker's Theorem, 176–177. This generalized an earlier result of Carl Friedrich Gauss from \"Disquisitiones Arithmeticae\" (1801), which classified quadratic forms; Kronecker cited this result of Gauss's. The theorem was stated and proved in the language of groups by Ferdinand Georg Frobenius and Ludwig Stickelberger in 1878. Another group-theoretic formulation was given by Kronecker's student Eugen Netto in 1882.\n\nThe fundamental theorem for \"finitely presented\" abelian groups was proven by Henry John Stephen Smith in , as integer matrices correspond to finite presentations of abelian groups (this generalizes to finitely presented modules over a principal ideal domain), and Smith normal form corresponds to classifying finitely presented abelian groups. There is the additional technicality of showing that a finitely \"presented\" abelian group is in fact finitely \"generated\", so Smith's classification is not a complete proof for finitely generated abelian groups.\n\nThe fundamental theorem for \"finitely generated\" abelian groups was proven by Henri Poincaré in , using a matrix proof (which generalizes to principal ideal domains). This was done in the context of computing the\nhomology of a complex, specifically the Betti number and torsion coefficients of a dimension of the complex, where the Betti number corresponds to the rank of the free part, and the torsion coefficients correspond to the torsion part.\n\nKronecker's proof was generalized to \"finitely generated\" abelian groups by Emmy Noether in .\n\nStated differently the fundamental theorem says that a finitely generated abelian group is the direct sum of a free abelian group of finite rank and a finite abelian group, each of those being unique up to isomorphism. The finite abelian group is just the torsion subgroup of \"G\". The rank of \"G\" is defined as the rank of the torsion-free part of \"G\"; this is just the number \"n\" in the above formulas.\n\nA corollary to the fundamental theorem is that every finitely generated torsion-free abelian group is free abelian. The finitely generated condition is essential here: formula_15 is torsion-free but not free abelian.\n\nEvery subgroup and factor group of a finitely generated abelian group is again finitely generated abelian. The finitely generated abelian groups, together with the group homomorphisms, form an abelian category which is a Serre subcategory of the category of abelian groups.\n\nNote that not every abelian group of finite rank is finitely generated; the rank 1 group formula_15 is one counterexample, and the rank-0 group given by a direct sum of countably infinitely many copies of formula_17 is another one.\n\n\n"}
{"id": "42515", "url": "https://en.wikipedia.org/wiki?curid=42515", "title": "Infinite monkey theorem", "text": "Infinite monkey theorem\n\nThe infinite monkey theorem states that a monkey hitting keys at random on a typewriter keyboard for an infinite amount of time will almost surely type a given text, such as the complete works of William Shakespeare. In fact, the monkey would almost surely type every possible finite text an infinite number of times. However, the probability that monkeys filling the observable universe would type a complete work such as Shakespeare's Hamlet is so tiny that the chance of it occurring during a period of time hundreds of thousands of orders of magnitude longer than the age of the universe is \"extremely\" low (but technically not zero).\n\nIn this context, \"almost surely\" is a mathematical term with a precise meaning, and the \"monkey\" is not an actual monkey, but a metaphor for an abstract device that produces an endless random sequence of letters and symbols. One of the earliest instances of the use of the \"monkey metaphor\" is that of French mathematician Émile Borel in 1913, but the first instance may have been even earlier.\n\nVariants of the theorem include multiple and even infinitely many typists, and the target text varies between an entire library and a single sentence. Jorge Luis Borges traced the history of this idea from Aristotle's \"On Generation and Corruption\" and Cicero's \"De natura deorum\" (On the Nature of the Gods), through Blaise Pascal and Jonathan Swift, up to modern statements with their iconic simians and typewriters. In the early 20th century, Borel and Arthur Eddington used the theorem to illustrate the timescales implicit in the foundations of statistical mechanics.\n\nThere is a straightforward proof of this theorem. As an introduction, recall that if two events are statistically independent, then the probability of both happening equals the product of the probabilities of each one happening independently. For example, if the chance of rain in Moscow on a particular day in the future is 0.4 and the chance of an earthquake in San Francisco on any particular day is 0.00003, then the chance of both happening on the same day is , assuming that they are indeed independent.\n\nSuppose the typewriter has 50 keys, and the word to be typed is \"banana\". If the keys are pressed randomly and independently, it means that each key has an equal chance of being pressed. Then, the chance that the first letter typed is 'b' is 1/50, and the chance that the second letter typed is \"a\" is also 1/50, and so on. Therefore, the chance of the first six letters spelling \"banana\" is\nless than one in 15 billion, but not zero, hence a possible outcome.\n\nFrom the above, the chance of \"not\" typing \"banana\" in a given block of 6 letters is 1 − (1/50). Because each block is typed independently, the chance \"X\" of not typing \"banana\" in any of the first \"n\" blocks of 6 letters is\n\nAs \"n\" grows, \"X\" gets smaller. For an \"n\" of a million, \"X\" is roughly 0.9999, but for an \"n\" of 10 billion \"X\" is roughly 0.53 and for an \"n\" of 100 billion it is roughly 0.0017. As \"n\" approaches infinity, the probability \"X\" approaches zero; that is, by making \"n\" large enough, \"X\" can be made as small as is desired, and the chance of typing \"banana\" approaches 100%.\n\nThe same argument shows why at least one of infinitely many monkeys will produce a text as quickly as it would be produced by a perfectly accurate human typist copying it from the original. In this case \"X\" = (1 − (1/50)) where \"X\" represents the probability that none of the first \"n\" monkeys types \"banana\" correctly on their first try. When we consider 100 billion monkeys, the probability falls to 0.17%, and as the number of monkeys \"n\" increases, the value of \"X\" – the probability of the monkeys failing to reproduce the given text – approaches zero arbitrarily closely. The limit, for \"n\" going to infinity, is zero. So the probability of the word \"banana\" appearing at some point in an infinite sequence of keystrokes is equal to one.\n\nThis can be stated more generally and compactly in terms of strings, which are sequences of characters chosen from some finite alphabet:\n\nBoth follow easily from the second Borel–Cantelli lemma. For the second theorem, let \"E\" be the event that the \"k\"th string begins with the given text. Because this has some fixed nonzero probability \"p\" of occurring, the \"E\" are independent, and the below sum diverges,\nthe probability that infinitely many of the \"E\" occur is 1. The first theorem is shown similarly; one can divide the random string into nonoverlapping blocks matching the size of the desired text, and make \"E\" the event where the \"k\"th block equals the desired string.\n\nHowever, for physically meaningful numbers of monkeys typing for physically meaningful lengths of time the results are reversed. If there were as many monkeys as there are atoms in the observable universe typing extremely fast for trillions of times the life of the universe, the probability of the monkeys replicating even a \"single page\" of Shakespeare is unfathomably small.\n\nIgnoring punctuation, spacing, and capitalization, a monkey typing letters uniformly at random has a chance of one in 26 of correctly typing the first letter of \"Hamlet.\" It has a chance of one in 676 (26 × 26) of typing the first two letters. Because the probability shrinks exponentially, at 20 letters it already has only a chance of one in 26 = 19,928,148,895,209,409,152,340,197,376 (almost 2 × 10). In the case of the entire text of \"Hamlet\", the probabilities are so vanishingly small as to be inconceivable. The text of Hamlet contains approximately 130,000 letters. Thus there is a probability of one in 3.4 × 10 to get the text right at the first trial. The average number of letters that needs to be typed until the text appears is also 3.4 × 10, or including punctuation, 4.4 × 10.\n\nEven if every proton in the observable universe were a monkey with a typewriter, typing from the Big Bang until the end of the universe (when protons might no longer exist), they would still need a still far greater amount of time – more than three hundred and sixty thousand \"orders of magnitude\" longer – to have even a 1 in 10 chance of success. To put it another way, for a one in a trillion chance of success, there would need to be 10 universes made of atomic monkeys. As Kittel and Kroemer put it in their textbook on thermodynamics, the field whose statistical foundations motivated the first known expositions of typing monkeys, \"The probability of \"Hamlet\" is therefore zero in any operational sense of an event...\", and the statement that the monkeys must eventually succeed \"gives a misleading conclusion about very, very large numbers.\"\n\nIn fact there is less than a one in a trillion chance of success that such a universe made of monkeys could type any particular document a mere 79 characters long.\n\nThe probability that an infinite randomly generated string of text will contain a particular finite substring is 1. However, this does not mean the substring's absence is \"impossible\", despite the absence having a prior probability of 0. For example, the immortal monkey \"could\" randomly type G as its first letter, G as its second, and G as every single letter thereafter, producing an infinite string of Gs; at no point must the monkey be \"compelled\" to type anything else. (To assume otherwise implies the gambler's fallacy.) However long a randomly generated finite string is, there is a small but nonzero chance that it will turn out to consist of the same character repeated throughout; this chance approaches zero as the string's length approaches infinity. There is nothing special about such a monotonous sequence except that it is easy to describe; the same fact applies to any nameable specific sequence, such as \"RGRGRG\" repeated forever, or \"a-b-aa-bb-aaa-bbb-...\", or \"Three, Six, Nine, Twelve…\".\n\nIf the hypothetical monkey has a typewriter with 90 equally likely keys that include numerals and punctuation, then the first typed keys might be \"3.14\" (the first three digits of pi) with a probability of (1/90), which is 1/65,610,000. Equally probable is any other string of four characters allowed by the typewriter, such as \"GGGG\", \"mATh\", or \"q%8e\". The probability that 100 randomly typed keys will consist of the first 99 digits of pi (including the separator key), or any other \"particular\" sequence of that length, is much lower: (1/90). If the monkey's allotted length of text is infinite, the chance of typing only the digits of pi is 0, which is just as \"possible\" (mathematically probable) as typing nothing but Gs (also probability 0).\n\nThe same applies to the event of typing a particular version of \"Hamlet\" followed by endless copies of itself; or \"Hamlet\" immediately followed by all the digits of pi; these specific strings are equally infinite in length, they are not prohibited by the terms of the thought problem, and they each have a prior probability of 0. In fact, \"any\" particular infinite sequence the immortal monkey types will have \"had\" a prior probability of 0, even though the monkey must type something.\n\nThis is an extension of the principle that a finite string of random text has a lower and lower probability of \"being\" a particular string the longer it is (though all specific strings are equally unlikely). This probability approaches 0 as the string approaches infinity. Thus, the probability of the monkey typing an endlessly long string, such as all of the digits of pi in order, on a 90-key keyboard is (1/90) which equals (1/∞) which is essentially 0. At the same time, the probability that the sequence \"contains\" a particular subsequence (such as the word MONKEY, or the 12th through 999th digits of pi, or a version of the King James Bible) increases as the total string increases. This probability approaches 1 as the total string approaches infinity, and thus the original theorem is correct.\n\nIn a simplification of the thought experiment, the monkey could have a typewriter with just two keys: 1 and 0. The infinitely long string thusly produced would correspond to the binary digits of a particular real number between 0 and 1. A countably infinite set of possible strings end in infinite repetitions, which means the corresponding real number is rational. Examples include the strings corresponding to one-third (010101…), five-sixths (11010101…) and five-eighths (1010000…). Only a subset of such real number strings (albeit a countably infinite subset) contains the entirety of \"Hamlet\" (assuming that the text is subjected to a numerical encoding, such as ASCII).\n\nMeanwhile, there is an \"uncountably\" infinite set of strings which do not end in such repetition; these correspond to the irrational numbers. These can be sorted into two uncountably infinite subsets: those which contain \"Hamlet\" and those which do not. However, the \"largest\" subset of all the real numbers are those which not only contain \"Hamlet\", but which contain every other possible string of any length, and with equal distribution of such strings. These irrational numbers are called normal. Because almost all numbers are normal, almost all possible strings contain all possible finite substrings. Hence, the probability of the monkey typing a normal number is 1. The same principles apply regardless of the number of keys from which the monkey can choose; a 90-key keyboard can be seen as a generator of numbers written in base 90.\n\nIn one of the forms in which probabilists now know this theorem, with its \"dactylographic\" [i.e., typewriting] monkeys (; the French word \"singe\" covers both the monkeys and the apes), appeared in Émile Borel's 1913 article \"Mécanique Statistique et Irréversibilité\" (\"Statistical mechanics and irreversibility\"), and in his book \"Le Hasard\" in 1914. His \"monkeys\" are not actual monkeys; rather, they are a metaphor for an imaginary way to produce a large, random sequence of letters. Borel said that if a million monkeys typed ten hours a day, it was extremely unlikely that their output would exactly equal all the books of the richest libraries of the world; and yet, in comparison, it was even more unlikely that the laws of statistical mechanics would ever be violated, even briefly.\n\nThe physicist Arthur Eddington drew on Borel's image further in \"The Nature of the Physical World\" (1928), writing:\n\nThese images invite the reader to consider the incredible improbability of a large but finite number of monkeys working for a large but finite amount of time producing a significant work, and compare this with the even greater improbability of certain physical events. Any physical process that is even less likely than such monkeys' success is effectively impossible, and it may safely be said that such a process will never happen. It is clear from the context that Eddington is not suggesting that the probability of this happening is worthy of serious consideration. On the contrary, it was a rhetorical illustration of the fact that below certain levels of probability, the term \"improbable\" is functionally equivalent to \"impossible\".\n\nIn a 1939 essay entitled \"The Total Library\", Argentine writer Jorge Luis Borges traced the infinite-monkey concept back to Aristotle's \"Metaphysics.\" Explaining the views of Leucippus, who held that the world arose through the random combination of atoms, Aristotle notes that the atoms themselves are homogeneous and their possible arrangements only differ in shape, position and ordering. In \"On Generation and Corruption\", the Greek philosopher compares this to the way that a tragedy and a comedy consist of the same \"atoms\", \"i.e.\", alphabetic characters. Three centuries later, Cicero's \"De natura deorum\" (\"On the Nature of the Gods\") argued against the atomist worldview:\n\nBorges follows the history of this argument through Blaise Pascal and Jonathan Swift, then observes that in his own time, the vocabulary had changed. By 1939, the idiom was \"that a half-dozen monkeys provided with typewriters would, in a few eternities, produce all the books in the British Museum.\" (To which Borges adds, \"Strictly speaking, one immortal monkey would suffice.\") Borges then imagines the contents of the Total Library which this enterprise would produce if carried to its fullest extreme:\n\nBorges' total library concept was the main theme of his widely read 1941 short story \"The Library of Babel\", which describes an unimaginably vast library consisting of interlocking hexagonal chambers, together containing every possible volume that could be composed from the letters of the alphabet and some punctuation characters.\n\nIn 2003, lecturers and students from the University of Plymouth MediaLab Arts course used a £2,000 grant from the Arts Council to study the literary output of real monkeys. They left a computer keyboard in the enclosure of six Celebes crested macaques in Paignton Zoo in Devon in England for a month, with a radio link to broadcast the results on a website.\n\nNot only did the monkeys produce nothing but five total pages largely consisting of the letter S, the lead male began bashing the keyboard with a stone, and the monkeys followed by soiling it. Mike Phillips, director of the university's Institute of Digital Arts and Technology (i-DAT), said that the artist-funded project was primarily performance art, and they had learned \"an awful lot\" from it. He concluded that monkeys \"are not random generators. They're more complex than that. ... They were quite interested in the screen, and they saw that when they typed a letter, something happened. There was a level of intention there.\"\n\nIn his 1931 book \"The Mysterious Universe\", Eddington's rival James Jeans attributed the monkey parable to a \"Huxley\", presumably meaning Thomas Henry Huxley. This attribution is incorrect. Today, it is sometimes further reported that Huxley applied the example in a now-legendary debate over Charles Darwin's \"On the Origin of Species\" with the Anglican Bishop of Oxford, Samuel Wilberforce, held at a meeting of the British Association for the Advancement of Science at Oxford on 30 June 1860. This story suffers not only from a lack of evidence, but the fact that in 1860 the typewriter itself had yet to emerge.\n\nDespite the original mix-up, monkey-and-typewriter arguments are now common in arguments over evolution. For example, Doug Powell argues as a Christian apologist that even if a monkey accidentally types the letters of \"Hamlet\", it has failed to produce \"Hamlet\" because it lacked the intention to communicate. His parallel implication is that natural laws could not produce the information content in DNA. A more common argument is represented by Reverend John F. MacArthur, who claims that the genetic mutations necessary to produce a tapeworm from an amoeba are as unlikely as a monkey typing Hamlet's soliloquy, and hence the odds against the evolution of all life are impossible to overcome.\n\nEvolutionary biologist Richard Dawkins employs the typing monkey concept in his book \"The Blind Watchmaker\" to demonstrate the ability of natural selection to produce biological complexity out of random mutations. In a simulation experiment Dawkins has his weasel program produce the Hamlet phrase \"METHINKS IT IS LIKE A WEASEL\", starting from a randomly typed parent, by \"breeding\" subsequent generations and always choosing the closest match from progeny that are copies of the parent, with random mutations. The chance of the target phrase appearing in a single step is extremely small, yet Dawkins showed that it could be produced rapidly (in about 40 generations) using cumulative selection of phrases. The random choices furnish raw material, while cumulative selection imparts information. As Dawkins acknowledges, however, the weasel program is an imperfect analogy for evolution, as \"offspring\" phrases were selected \"according to the criterion of resemblance to a \"distant ideal\" target.\" In contrast, Dawkins affirms, evolution has no long-term plans and does not progress toward some distant goal (such as humans). The weasel program is instead meant to illustrate the difference between non-random cumulative selection, and random single-step selection. In terms of the typing monkey analogy, this means that \"Romeo and Juliet\" could be produced relatively quickly if placed under the constraints of a nonrandom, Darwinian-type selection because the fitness function will tend to preserve in place any letters that happen to match the target text, improving each successive generation of typing monkeys.\n\nA different avenue for exploring the analogy between evolution and an unconstrained monkey lies in the problem that the monkey types only one letter at a time, independently of the other letters. Hugh Petrie argues that a more sophisticated setup is required, in his case not for biological evolution but the evolution of ideas:\n\nJames W. Valentine, while admitting that the classic monkey's task is impossible, finds that there is a worthwhile analogy between written English and the metazoan genome in this other sense: both have \"combinatorial, hierarchical structures\" that greatly constrain the immense number of combinations at the alphabet level.\n\nR. G. Collingwood argued in 1938 that art cannot be produced by accident, and wrote as a sarcastic aside to his critics,\n\nNelson Goodman took the contrary position, illustrating his point along with Catherine Elgin by the example of Borges' \"Pierre Menard, Author of the Quixote\",\n\nIn another writing, Goodman elaborates, \"That the monkey may be supposed to have produced his copy randomly makes no difference. It is the same text, and it is open to all the same interpretations...\" Gérard Genette dismisses Goodman's argument as begging the question.\n\nFor Jorge J. E. Gracia, the question of the identity of texts leads to a different question, that of author. If a monkey is capable of typing \"Hamlet\", despite having no intention of meaning and therefore disqualifying itself as an author, then it appears that texts do not require authors. Possible solutions include saying that whoever finds the text and identifies it as \"Hamlet\" is the author; or that Shakespeare is the author, the monkey his agent, and the finder merely a user of the text. These solutions have their own difficulties, in that the text appears to have a meaning separate from the other agents: what if the monkey operates before Shakespeare is born, or if Shakespeare is never born, or if no one ever finds the monkey's typescript?\n\nThe theorem concerns a thought experiment which cannot be fully carried out in practice, since it is predicted to require prohibitive amounts of time and resources. Nonetheless, it has inspired efforts in finite random text generation.\n\nOne computer program run by Dan Oliver of Scottsdale, Arizona, according to an article in \"The New Yorker\", came up with a result on August 4, 2004: After the group had worked for 42,162,500,000 billion billion monkey-years, one of the \"monkeys\" typed, \"VALENTINE. Cease toIdor:eFLP0FRjWK78aXzVOwm)-‘;8.t\" The first 19 letters of this sequence can be found in \"The Two Gentlemen of Verona\". Other teams have reproduced 18 characters from \"Timon of Athens\", 17 from \"Troilus and Cressida\", and 16 from \"Richard II\".\n\nA website entitled \"The Monkey Shakespeare Simulator\", launched on July 1, 2003, contained a Java applet that simulated a large population of monkeys typing randomly, with the stated intention of seeing how long it takes the virtual monkeys to produce a complete Shakespearean play from beginning to end. For example, it produced this partial line from \"Henry IV, Part 2\", reporting that it took \"2,737,850 million billion billion billion monkey-years\" to reach 24 matching characters:\n\nDue to processing power limitations, the program used a probabilistic model (by using a random number generator or RNG) instead of actually generating random text and comparing it to Shakespeare. When the simulator \"detected a match\" (that is, the RNG generated a certain value or a value within a certain range), the simulator simulated the match by generating matched text.\n\nMore sophisticated methods are used in practice for natural language generation. If instead of simply generating random characters one restricts the generator to a meaningful vocabulary and conservatively following grammar rules, like using a context-free grammar, then a random document generated this way can even fool some humans (at least on a cursory reading) as shown in the experiments with SCIgen, snarXiv, and the Postmodernism Generator.\n\nQuestions about the statistics describing how often an ideal monkey is expected to type certain strings translate into practical tests for random-number generators; these range from the simple to the \"quite sophisticated\". Computer-science professors George Marsaglia and Arif Zaman report that they used to call one such category of tests \"overlapping m-tuple tests\" in lectures, since they concern overlapping m-tuples of successive elements in a random sequence. But they found that calling them \"monkey tests\" helped to motivate the idea with students. They published a report on the class of tests and their results for various RNGs in 1993.\n\nThe infinite monkey theorem and its associated imagery is considered a popular and proverbial illustration of the mathematics of probability, widely known to the general public because of its transmission through popular culture rather than through formal education. This is helped by the innate humor stemming from the image of literal monkeys rattling away on a set of typewriters, and is a popular visual gag.\n\nIn \"The Simpsons\" episode \"Last Exit to Springfield\", Mr. Burns shows Homer \"a room with a thousand monkeys on a thousand typewriters. Soon they will have written the greatest novel known to man!\" Burns then chastises one monkey for typing, \"It was the best of times, it was the \"blurst\" of times!\".\n\nIn his 1978 radio play, \"The Hitchhiker's Guide to the Galaxy\", Douglas Adams invoked the theorem to illustrate the power of the 'Infinite Improbability Drive' that powered a spaceship. From Episode 2: \"Ford, there's an infinite number of monkeys outside who want to talk to us about this script for Hamlet they've worked out.\"\n\nA quotation attributed to a 1996 speech by Robert Wilensky stated, \"We've heard that a million monkeys at a million keyboards could produce the complete works of Shakespeare; now, thanks to the Internet, we know that is not true.\"\n\nThe enduring, widespread popularity of the theorem was noted in the introduction to a 2001 paper, \"Monkeys, Typewriters and Networks: The Internet in the Light of the Theory of Accidental Excellence\". In 2002, an article in \"The Washington Post\" said, \"Plenty of people have had fun with the famous notion that an infinite number of monkeys with an infinite number of typewriters and an infinite amount of time could eventually write the works of Shakespeare\". In 2003, the previously mentioned Arts Council funded experiment involving real monkeys and a computer keyboard received widespread press coverage. In 2007, the theorem was listed by \"Wired\" magazine in a list of eight classic thought experiments.\n\n\n"}
{"id": "4470203", "url": "https://en.wikipedia.org/wiki?curid=4470203", "title": "Inscribed figure", "text": "Inscribed figure\n\nIn geometry, an inscribed planar shape or solid is one that is enclosed by and \"fits snugly\" inside another geometric shape or solid. To say that \"figure F is inscribed in figure G\" means precisely the same thing as \"figure G is circumscribed about figure F\". A circle or ellipse inscribed in a convex polygon (or a sphere or ellipsoid inscribed in a convex polyhedron) is tangent to every side or face of the outer figure (but see Inscribed sphere for semantic variants). A polygon inscribed in a circle, ellipse, or polygon (or a polyhedron inscribed in a sphere, ellipsoid, or polyhedron) has each vertex on the outer figure; if the outer figure is a polygon or polyhedron, there must be a vertex of the inscribed polygon or polyhedron on each side of the outer figure. An inscribed figure is not necessarily unique in orientation; this can easily be seen, for example, when the given outer figure is a circle, in which case a rotation of an inscribed figure gives another inscribed figure that is congruent to the original one.\n\nFamiliar examples of inscribed figures include circles inscribed in triangles or regular polygons, and triangles or regular polygons inscribed in circles. A circle inscribed in any polygon is called its incircle, in which case the polygon is said to be a tangential polygon. A polygon inscribed in a circle is said to be a cyclic polygon, and the circle is said to be its circumscribed circle or circumcircle.\n\nThe inradius or filling radius of a given outer figure is the radius of the inscribed circle or sphere, if it exists.\n\nThe definition given above assumes that the objects concerned are embedded in two- or three-dimensional Euclidean space, but can easily be generalized to higher dimensions and other metric spaces.\n\nFor an alternative usage of the term \"inscribed\", see the inscribed square problem, in which a square is considered to be inscribed in another figure (even a non-convex one) if all four of its vertices are on that figure.\n\n\n\n"}
{"id": "9158134", "url": "https://en.wikipedia.org/wiki?curid=9158134", "title": "Jackknife resampling", "text": "Jackknife resampling\n\nIn statistics, the jackknife is a resampling technique especially useful for variance and bias estimation. The jackknife predates other common resampling methods such as the bootstrap. The jackknife estimator of a parameter is found by systematically leaving out each observation from a dataset and calculating the estimate and then finding the average of these calculations. Given a sample of size formula_1, the jackknife estimate is found by aggregating the estimates of each formula_2-sized sub-sample.\n\nThe jackknife technique was developed by Maurice Quenouille (1924-1973) from 1949, and refined in 1956. John Tukey expanded on the technique in 1958 and proposed the name \"jackknife\" since, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.\n\nThe jackknife is a linear approximation of the bootstrap.\n\nThe jackknife estimate of a parameter can be found by estimating the parameter for each subsample omitting the \"i\"-th observation. For example, if the parameter to be estimated is the population mean of \"x\", we compute the mean formula_3 for each subsample consisting of all but the \"i\"-th data point:\n\nThese \"n\" estimates form an estimate of the distribution of the sample statistic if it were computed over a large number of samples. In particular, the mean of this sampling distribution is the average of these \"n\" estimates:\n\nA jackknife estimate of the variance of the estimator can be calculated from the variance of this distribution of formula_6\n\nThe jackknife technique can be used to estimate the bias of an estimator calculated over the entire sample. Say formula_8 is the calculated estimator of the parameter of interest based on all formula_9 observations. Let\nwhere formula_11 is the estimate of interest based on the sample with the \"i\"-th observation removed, and formula_12 is the average of these \"leave-one-out\" estimates.\nThe jackknife estimate of the bias of formula_8 is given by:\n\nand the resulting bias-corrected jackknife estimate of formula_15 is given by:\n\nThis removes the bias in the special case that the bias is formula_17 and to formula_18 in other cases.\n\n"}
{"id": "33413362", "url": "https://en.wikipedia.org/wiki?curid=33413362", "title": "Jacob's ladder surface", "text": "Jacob's ladder surface\n\nIn mathematics, Jacob's ladder is a surface with infinite genus and two ends. It was named after Jacob's ladder by Étienne , because the surface can be constructed as the boundary of a ladder that is infinitely long in both directions.\n\n\n"}
{"id": "55284288", "url": "https://en.wikipedia.org/wiki?curid=55284288", "title": "Kolmogorov's normability criterion", "text": "Kolmogorov's normability criterion\n\nIn mathematics, Kolmogorov's normability criterion is a theorem that provides a necessary and sufficient condition for a topological vector space to be normable, i.e. for the existence of a norm on the space that generates the given topology. The normability criterion can be seen as a result in same vein as the Nagata–Smirnov metrization theorem, which gives a necessary and sufficient condition for a topological space to be metrizable. The result was proved by the Russian mathematician Andrey Nikolayevich Kolmogorov in 1934.\n\nIt may be helpful to first recall the following terms:\n\nExpressed in these terms, Kolmogorov's normability criterion is as follows:\n\nTheorem. A topological vector space formula_3 is normable if and only if it is a T space and admits a bounded convex neighbourhood of the origin.\n"}
{"id": "22038671", "url": "https://en.wikipedia.org/wiki?curid=22038671", "title": "Least-upper-bound property", "text": "Least-upper-bound property\n\nIn mathematics, the least-upper-bound property (sometimes the completeness or supremum property) is a fundamental property of the real numbers and certain other ordered sets. A set has the least-upper-bound property if and only if every non-empty subset of with an upper bound has a \"least\" upper bound (supremum) in .\n\nThe least-upper-bound property is one form of the completeness axiom for the real numbers, and is sometimes referred to as Dedekind completeness. It can be used to prove many of the fundamental results of real analysis, such as the intermediate value theorem, the Bolzano–Weierstrass theorem, the extreme value theorem, and the Heine–Borel theorem. It is usually taken as an axiom in synthetic constructions of the real numbers (see least upper bound axiom), and it is also intimately related to the construction of the real numbers using Dedekind cuts.\n\nIn order theory, this property can be generalized to a notion of completeness for any partially ordered set. A linearly ordered set that is dense and has the least upper bound property is called a linear continuum.\n\nLet be a non-empty set of real numbers.\nThe least-upper-bound property states that any non-empty set of real numbers that has an upper bound must have a least upper bound in \"real numbers\".\n\nMore generally, one may define upper bound and least upper bound for any subset of a partially ordered set , with “real number” replaced by “element of ”. In this case, we say that has the least-upper-bound property if every non-empty subset of with an upper bound has a least upper bound.\n\nFor example, the set of rational numbers does not have the least-upper-bound property under the usual order. For instance, the set\n\nhas an upper bound in , but does not have a least upper bound in (since the square root of two is irrational). The construction of the real numbers using Dedekind cuts takes advantage of this failure by defining the irrational numbers as the least upper bounds of certain subsets of the rationals.\n\nThe least-upper-bound property is equivalent to other forms of the completeness axiom, such as the convergence of Cauchy sequences or the nested intervals theorem. The logical status of the property depends on the construction of the real numbers used: in the synthetic approach, the property is usually taken as an axiom for the real numbers (see least upper bound axiom); in a constructive approach, the property must be proved as a theorem, either directly from the construction or as a consequence of some other form of completeness.\n\nIt is possible to prove the least-upper-bound property using the assumption that every Cauchy sequence of real numbers converges. Let be a nonempty set of real numbers, and suppose that has an upper bound . Since is nonempty, there exists a real number that is not an upper bound for . Define sequences and recursively as follows:\nThen and as . It follows that both sequences are Cauchy and have the same limit , which must be the least upper bound for .\n\nThe least-upper-bound property of can be used to prove many of the main foundational theorems in real analysis.\n\nLet be a continuous function, and suppose that and . In this case, the intermediate value theorem states that must have a root in the interval . This theorem can proved by considering the set\nThat is, is the initial segment of that takes negative values under . Then is an upper bound for , and the least upper bound must be a root of .\n\nThe Bolzano–Weierstrass theorem for states that every sequence of real numbers in a closed interval must have a convergent subsequence. This theorem can be proved by considering the set\nClearly is an upper bound for , so has a least upper bound . Then must be a limit point of the sequence , and it follows that has a subsequence that converges to .\n\nLet be a continuous function and let , where if has no upper bound. The extreme value theorem states that is finite and for some . This can be proved by considering the set\nIf is the least upper bound of this set, then it follows from continuity that .\n\nLet be a closed interval in , and let be a collection of open sets that covers . Then the Heine–Borel theorem states that some finite subcollection of covers as well. This statement can be proved by considering the set\nThis set must have a least upper bound . But is itself an element of some open set , and it follows that can be covered by finitely many for some sufficiently small . This proves that , and it also yields a contradiction unless .\n\nThe importance of the least-upper-bound property was first recognized by Bernard Bolzano in his 1817 paper \"Rein analytischer Beweis des Lehrsatzes dass zwischen je zwey Werthen, die ein entgegengesetztes Resultat gewäahren, wenigstens eine reelle Wurzel der Gleichung liege\".\n\n\n"}
{"id": "621447", "url": "https://en.wikipedia.org/wiki?curid=621447", "title": "List of computer graphics and descriptive geometry topics", "text": "List of computer graphics and descriptive geometry topics\n\nThis is a list of computer graphics and descriptive geometry topics, by article name.\n\n"}
{"id": "34032940", "url": "https://en.wikipedia.org/wiki?curid=34032940", "title": "Logan plot", "text": "Logan plot\n\nA Logan plot (or Logan graphical analysis) is a graphical analysis technique based on the compartment model that uses linear regression to analyze pharmacokinetics of tracers involving reversible uptake. It is mainly used for the evaluation of nuclear medicine imaging data after the injection of a labeled ligand that binds reversibly to specific receptor or enzyme.\n\nIn conventional compartmental analysis, an iterative method is used to fit the individual model parameters in the solution of a compartmental model of specific configuration to the measurements with a measured plasma time-activity curve that serves as an forcing (input) function, and the binding of the tracer can then be described. Graphical analysis is a simplified method that transforms the model equations into a linear equation evaluated at multiple time points and provides fewer parameters (i.e., slope and intercept). Although the slope and the intercept can be interpreted in terms of a combination of model parameters if a compartmental model configuration is assumed, the graphical methods are independent of any specific model configuration. In case of irreversible tracers, certain fraction of the radioactivity is trapped in the tissue or the binding site during the course of the experiment, whereas reversible tracers show uptake and loss from all compartments throughout the study. The theoretical foundation of graphical analysis for irreversible tracers (also called Patlak graphical analysis or Patlak plot) was laid by Clifford Patlak and his colleagues at NIH. Based on the original work of Patlak, Jean Logan and her colleagues from Brookhaven National Laboratory extended the method to tracers with reversible kinetics.\n\nThe kinetics of radiolabeled compounds in a compartmental system can be described in terms of a set of first-order, constant-coefficient, ordinary differential equations. The time course of the activity in the multicompartmental system driven by a metabolite-corrected plasma input function formula_1 can be described by:\n\nwhere formula_3 is a column vector of activity concentration for each compartment at time formula_4, formula_5 is the matrix of the transfer constants between compartments, and formula_6 is the vector of plasma-to-tissue transfer constants. Patlak and Blasberg showed that the above equation can be written as:\n\nwhere formula_8 represents a row vector of 1s and formula_9. The total activity in the region of interest, formula_10, is a combination of radioactivities from all compartments plus a plasma volume fraction (formula_11) and thus:\n\nBy dividing both sides by formula_10, one obtains the following linear equation:\n"}
{"id": "9165727", "url": "https://en.wikipedia.org/wiki?curid=9165727", "title": "Lung compliance", "text": "Lung compliance\n\nLung compliance, or pulmonary compliance, is a measure of the lung's ability to stretch and expand (distensibility of elastic tissue). In clinical practice it is separated into two different measurements, static compliance and dynamic compliance. Static lung compliance is the change in volume for any given applied pressure. Dynamic lung compliance is the compliance of the lung at any given time during actual movement of air.\n\nLow compliance indicates a stiff lung (one with high elastic recoil) and can be thought of as a thick balloon – this is the case often seen in fibrosis. High compliance indicates a pliable lung (one with low elastic recoil) and can be thought of as a grocery bag – this is the case often seen in emphysema. Compliance is highest at moderate lung volumes, and much lower at volumes which are very low or very high. The compliance of the lungs demonstrate lung hysteresis; that is, the compliance is different on inspiration and expiration for identical volumes.\n\nPulmonary compliance is calculated using the following equation, where Δ\"V\" is the change in volume, and Δ\"P\" is the change in pleural pressure:\n\nFor example, if a patient inhales 500 mL of air from a spirometer with an intrapleural pressure before inspiration of −5 cm HO and −10 cm HO at the end of inspiration. Then: \n\nStatic compliance represents pulmonary compliance during periods without gas flow, such as during an inspiratory pause. It can be calculated with the formula:\nwhere \n\"P\" is measured at the end of inhalation and prior to exhalation using an inspiratory hold maneuver. During this maneuver, airflow is transiently (~0.5 sec) discontinued, which eliminates the effects of airway resistance. \"P\" is never bigger than PIP and is typically <10 cm HO lower than PIP when airway resistance is not elevated. .\n\nDynamic compliance represents pulmonary compliance during periods of gas flow, such as during active inspiration. Dynamic compliance is always lower than or equal to static lung compliance because PIP − PEEP is always greater than \"P\" − PEEP. It can be calculated using the following equation, where \nAlterations in airway resistance, lung compliance and chest wall compliance influence \"C\".\n\nLung compliance is an important measurement in respiratory physiology.\n\n\nPulmonary surfactant increases compliance by decreasing the surface tension of water. The internal surface of the alveolus is covered with a thin coat of fluid. The water in this fluid has a high surface tension, and provides a force that could collapse the alveolus. The presence of surfactant in this fluid breaks up the surface tension of water, making it less likely that the alveolus can collapse inward. If the alveolus were to collapse, a great force would be required to open it, meaning that compliance would decrease drastically.\n\nLow compliance indicates a stiff lung and means extra work is required to bring in a normal volume of air. This occurs as the lungs in this case become fibrotic, lose their distensibility and become stiffer.\n\nIn a highly compliant lung, as in emphysema, the elastic tissue is damaged by enzymes. These enzymes are secreted by leukocytes (white blood cells) in response to a variety of inhaled irritants, such as cigarette smoke. Patients with emphysema have a very high lung compliance due to the poor elastic recoil. They have extreme difficulty exhaling air. In this condition extra work is required to get air out of the lungs. In addition, patients often have difficulties inhaling air as well. This is due to the fact that a high compliant lung results in many collapsed alveoli which makes inflation difficult. Compliance also increases with increasing age.\n\nBoth peak inspiratory and plateau pressure increase when elastic resistance increases or when pulmonary compliance decreases (e.g. during abdominal insufflation, ascites, intrinsic lung disease, obesity, pulmonary edema, tension pneumothorax). On the other hand, only peak inspiratory pressure increases (plateau pressure unchanged) when airway resistance increases (e.g. airway compression, bronchospasm, mucous plug, kinked tube, secretions, foreign body). \n\nCompliance decreases in the following cases:\n\n"}
{"id": "56349373", "url": "https://en.wikipedia.org/wiki?curid=56349373", "title": "Markov theorem", "text": "Markov theorem\n\nIn mathematics the Markov theorem gives necessary and sufficient conditions for two braids to have closures which are equivalent links. In algebraic topology, Alexander's theorem states that every knot or link in three-dimensional Euclidean space is the closure of a braid. The Markov theorem, proved by Russian mathematician Andrei Andreevich Markov Jr. states that three conditions are necessary and sufficient for two braids to have equivalent closures:\n\n\n"}
{"id": "12087300", "url": "https://en.wikipedia.org/wiki?curid=12087300", "title": "Minimum bounding box", "text": "Minimum bounding box\n\nIn geometry, the minimum or smallest bounding or enclosing box for a point set (\"S\") in \"N\" dimensions is the box with the smallest measure (area, volume, or hypervolume in higher dimensions) within which all the points lie. When other kinds of measure are used, the minimum box is usually called accordingly, e.g., \"minimum-perimeter bounding box\".\n\nThe minimum bounding box of a point set is the same as the minimum bounding box of its convex hull, a fact which may be used heuristically to speed up computation.\n\nThe term \"box\"/\"hyperrectangle\" comes from its usage in the Cartesian coordinate system, where it is indeed visualized as a rectangle (two-dimensional case), rectangular parallelepiped (three-dimensional case), etc.\n\nIn the two-dimensional case it is called the minimum bounding rectangle.\n\nThe axis-aligned minimum bounding box (or AABB) for a given point set is its minimum bounding box subject to the constraint that the edges of the box are parallel to the (Cartesian) coordinate axes. It is simply the Cartesian product of \"N\" intervals each of which is defined by the minimal and maximal value of the corresponding coordinate for the points in \"S\".\n\nAxis-aligned minimal bounding boxes are used to an approximate location of an object in question and as a very simple descriptor of its shape. For example, in computational geometry and its applications when it is required to find intersections in the set of objects, the initial check is the intersections between their MBBs. Since it is usually a much less expensive operation than the check of the actual intersection (because it only requires comparisons of coordinates), it allows quickly excluding checks of the pairs that are far apart.\n\nThe arbitrarily oriented minimum bounding box is the minimum bounding box, calculated subject to no constraints as to the orientation of the result. Minimum bounding box algorithms based on the rotating calipers method can be used to find the minimum-area or minimum-perimeter bounding box of a two-dimensional convex polygon in linear time, and of a two-dimensional point set in the time it takes to construct its convex hull followed by a linear-time computation. A three-dimensional rotating calipers algorithm can find the minimum-volume arbitrarily-oriented bounding box of a three-dimensional point set in cubic time. Matlab implementations of the latter as well as the optimal compromise between accuracy and CPU time are available. \n\nIn the case where an object has its own local coordinate system, it can be useful to store a bounding box relative to these axes, which requires no transformation as the object's own transformation changes.\n\nIn digital image processing, the \"bounding box\" is merely the coordinates of the rectangular border that fully encloses a digital image when it is placed over a page, a canvas, a screen or other similar bidimensional background.\n\n"}
{"id": "2415128", "url": "https://en.wikipedia.org/wiki?curid=2415128", "title": "Monopole (mathematics)", "text": "Monopole (mathematics)\n\nIn mathematics, a monopole is a connection over a principal bundle \"G\" with a section of the associated adjoint bundle.\n\nPhysically, the section can be interpreted as a \"Higgs field\", where the connection and Higgs field should satisfy the Bogomolny equations and be of finite action.\n\n\n"}
{"id": "7097405", "url": "https://en.wikipedia.org/wiki?curid=7097405", "title": "Nina Bari", "text": "Nina Bari\n\nNina Karlovna Bari (, November 19, 1901, Moscow – July 15, 1961, Moscow) was a Soviet mathematician known for her work on trigonometric series.\n\nNina Bari was born in Russia on 19 November 1901, the daughter of Olga and Karl Adolfovich Bari, a physician. In 1918, she became one of the first women to be accepted to the Department of Physics and Mathematics at the prestigious Moscow State University. She graduated in 1921—just three years after entering the university. After graduation, Bari began her teaching career. She lectured at the Moscow Forestry Institute, the Moscow Polytechnic Institute, and the Sverdlov Communist Institute. Bari applied for and received the only paid research fellowship awarded by the newly created Research Institute of Mathematics and Mechanics. As a student, Bari was drawn to an elite group nicknamed the Luzitania—an informal academic and social organization. She studied trigonometric series and functions under the tutelage of Nikolai Luzin, becoming one of his star students. She presented the main result of her research to the Moscow Mathematical Society in 1922—the first women to address the society.\n\nIn 1926, Bari completed her doctoral work on the topic of trigonometric expansions, winning the Glavnauk Prize for her thesis work. In 1927, Bari took advantage of an opportunity to study in Paris at the Sorbonne and the College de France. She then attended the Polish Mathematical Congress in Lwów, Poland; a Rockefeller grant enabled her to return to Paris to continue her studies. Bari's decision to travel may have been influenced by the disintegration of the Luzitanians. Luzin's irascible, demanding personality had alienated many of the mathematicians who had gathered around him. By 1930, all traces of the Luzitania movement had vanished, and Luzin left Moscow State for the Academy of Science's Steklov Institute of Mathematics. In 1932, she became a professor at Moscow State University and in 1935 was awarded the title of Doctor of Physical and Mathematical Sciences, a more prestigious research degree than traditional Ph.D. By this time, she had completed foundational work on trigonometric series. \n\nShe was a close collaborator with Dmitrii Menshov on a number of research projects. She and Menshov took charge of function theory work at Moscow State during the 1940s. In 1952, she published an important piece on primitive functions, and trigonometric series and their almost everywhere convergence. Bari also posted works at the 1956 Third All—Union Congress in Moscow and the 1958 International Congress of Mathematicians in Edinburgh.\n\nMathematics was the center of Bari's intellectual life, but she enjoyed literature and the arts. She was also a mountain hiking enthusiast and tackled the Caucasus, Altai, Pamir and Tian Shan mountain ranges in Russia. Bari's interest in mountain hiking was inspired by her husband, Viktor Vladimirovich Nemytskii, a Soviet mathematician, Moscow State professor and an avid mountain explorer. There is no documentation of their marriage available, but contemporaries believe the two married later in life. Bari's last work—her 55th publication—was a 900-page monograph on the state of the art of trigonometric series theory, which is recognized as a standard reference work for those specializing in function and trigonometric series theory.\n\nOn 12 July 1961, Bari was killed when she fell in front of an oncoming metro train in Moscow.\n"}
{"id": "1403847", "url": "https://en.wikipedia.org/wiki?curid=1403847", "title": "Pandiagonal magic cube", "text": "Pandiagonal magic cube\n\nIn recreational mathematics, a pandiagonal magic cube is a magic cube with the additional property that all broken diagonals (parallel to exactly two of the three coordinate axes) have the same sum as each other. Pandiagonal magic cubes are extensions of diagonal magic cubes (in which only the unbroken diagonals need to have the same sum as the rows of the cube) and generalize pandiagonal magic squares to three dimensions.\n\nIn a pandiagonal magic cube, all 3m planar arrays must be panmagic squares. The 6 oblique squares are always magic. Several of them may be panmagic squares.\nA proper pandiagonal magic cube has exactly 9m lines plus the 4 main triagonals summing correctly. (No broken triagonals have the correct sum.)\n\nOrder 7 is the smallest possible pandiagonal magic cube.\n\n\n"}
{"id": "44424907", "url": "https://en.wikipedia.org/wiki?curid=44424907", "title": "Process qualification", "text": "Process qualification\n\nProcess qualification is the qualification of manufacturing and production processes to confirm they are able to operate at a certain standard during sustained commercial manufacturing. Data covering critical process parameters must be recorded and analyzed to ensure critical quality attributes can be guaranteed throughout production. This may include testing equipment at maximum operating capacity to show quantity demands can be met. Once all processes have been qualified the manufacturer should have a complete understanding of the process design and have a framework in place to routinely monitor operations. Only after process qualification has been completed can the manufacturing process begin production for commercial use. Equally important as qualifying processes and equipment is qualifying software and personnel. A well trained staff and accurate, thorough records helps ensure ongoing protection from process faults and quick recovery from otherwise costly process malfunctions. In many countries qualification measures are also required, especially in the pharmaceutical manufacturing field.\n\nProcess qualification should cover the following aspects of manufacturing:\n\nProcess qualification is the second stage of Process Validation.\n\nA vital component of process qualification is Process Performance Qualification Protocol. PPQ Protocol is essential in defining and maintaining production standards within an organization. \n\n\n"}
{"id": "28345524", "url": "https://en.wikipedia.org/wiki?curid=28345524", "title": "Realized variance", "text": "Realized variance\n\nRealized variance or realised variance (RV, see spelling differences) is the sum of squared returns. For instance the RV can be the sum of squared daily returns for a particular month, which would yield a measure of price variation over this month. More commonly, the realized variance is computed as the sum of squared intraday returns for a particular day.\n\nThe realized variance is useful because it provides a relatively accurate measure of volatility\nwhich is useful for many purposes, including volatility forecasting and forecast evaluation.\n\nUnlike the variance the realized variance is a random quantity.\n\nThe realized volatility is the square root of the realized variance, or the square root of the RV multiplied by a suitable constant to bring the measure of volatility to an annualized scale.\nFor instance, if the RV is computed as the sum of squared daily returns for some month, then an annualized realized volatility is given by formula_1.\n\nUnder ideal circumstances the RV consistently estimates the quadratic variation of the price process that the returns are computed from.\nOle E. Barndorff-Nielsen and Neil Shephard (2002), Journal of the Royal Statistical Society, Series B, 63, 2002, 253–280.\n\nFor instance suppose that the price process formula_2 is given by the stochastic integral\n\nwhere formula_4 is a standard Brownian motion, and formula_5 is some (possibly random) process for which the integrated variance,\n\nis well defined.\n\nThe realized variance based on formula_7 intraday returns is given by formula_8 where the intraday returns may be defined by\n\nThen it has been shown that, as formula_10 the realized variance converges to IV in probability. Moreover, the RV also converges in distribution in the sense that\n\nis approximately distributed as a standard normal random variables when formula_7 is large.\n\nWhen prices are measured with noise the RV may not estimate the desired quantity.\nThis problem motivated the development of a wide range of robust realized measures of volatility, such as the realized kernel estimator.\n\n"}
{"id": "7898351", "url": "https://en.wikipedia.org/wiki?curid=7898351", "title": "Romanesco broccoli", "text": "Romanesco broccoli\n\nRomanesco broccoli (also known as Roman cauliflower, Broccolo Romanesco, Romanesque cauliflower or simply Romanesco) is an edible flower bud of the species \"Brassica oleracea\". First documented in Italy, it is chartreuse in color. Romanesco has a striking appearance because its form is a natural approximation of a fractal. When compared to a traditional cauliflower, its texture as a vegetable is far more crunchy, and its flavor is not as assertive, being delicate and nutty.\n\nThe Romanesco has been grown in Italy since the 16th century.\n\nRomanesco superficially resembles a cauliflower, but it is chartreuse in color, and its form is strikingly fractal in nature. The inflorescence (the bud) is self-similar in character, with the branched meristems making up a logarithmic spiral. In this sense the bud's form approximates a natural fractal; each bud is composed of a series of smaller buds, all arranged in yet another logarithmic spiral. This self-similar pattern continues at several smaller levels. The pattern is only an approximate fractal since the pattern eventually terminates when the feature size becomes sufficiently small. The number of spirals on the head of Romanesco broccoli is a Fibonacci number.\n\nNutritionally, romanesco is rich in vitamin C, vitamin K, dietary fiber, and carotenoids.\n\nThe causes of its differences in appearance from the normal cauliflower and broccoli have been modeled as an extension of the preinfloresence stage of bud growth, but the genetic basis of this is not known.\n\n"}
{"id": "50398478", "url": "https://en.wikipedia.org/wiki?curid=50398478", "title": "Selection principle", "text": "Selection principle\n\nIn mathematics, a selection principle is a rule asserting\nthe possibility of obtaining mathematically significant objects by \nselecting elements from given sequences of sets. The theory of selection principles\nstudies these principles and their relations to other mathematical properties.\nSelection principles mainly describe covering properties, \nmeasure- and category-theoretic properties, and local properties in \ntopological spaces, especially function spaces. Often, the \ncharacterization of a mathematical property using a selection \nprinciple is a nontrivial task leading to new insights on the \ncharacterized property.\n\nIn 1924, Karl Menger\nintroduced the following basis property for metric spaces: \nEvery basis of the topology contains a sequence of sets with vanishing \ndiameters that covers the space. Soon thereafter, \nWitold Hurewicz \nobserved that Menger's basis property is equivalent to the \nfollowing selective property: for every sequence of open covers of the space, \none can select finitely many open sets from each cover in the sequence, such that the selected sets cover the space.\nTopological spaces having this covering property are called Menger spaces.\n\nHurewicz's reformulation of Menger's property was the first important \ntopological property described by a selection principle. \nLet formula_1 and formula_2 be classes of mathematical objects.\nIn 1996, Marion Scheepers \nintroduced the following selection hypotheses,\ncapturing a large number of classic mathematical properties:\n\n\nIn the case where the classes formula_1 and formula_2 consist of covers of some ambient space, Scheepers also introduced the following selection principle.\n\n\nLater, Boaz Tsaban identified the prevalence of the following related principle:\n\nThe notions thus defined are \"selection principles\". An instantiation of a selection principle, by considering specific classes formula_1 and formula_2, gives a \"selection (or: selective) property\". However, these terminologies are used interchangeably in the literature.\n\nFor a set formula_25 and a family formula_26 of subsets of formula_27, the star of formula_28 in formula_26 is the set formula_30.\n\nIn 1999, Ljubisa D.R. Kocinac introduced the following \"star selection principles\":\n\n\nCovering properties form the kernel of the theory of selection principles. Selection properties that are not covering properties are often studied by using implications to and from selective covering properties of related spaces.\n\nLet formula_27 be a topological space. An \"open cover\" of formula_27 is a family of open sets whose union is the entire space formula_43 For technical reasons, we also request that the entire space formula_27 is not a member of the cover. The class of open covers of the space formula_27 is denoted by formula_46. (Formally, formula_47, but usually the space formula_27 is fixed in the background.) The above-mentioned property of Menger is, thus, formula_49. In 1942, Fritz Rothberger considered Borel's strong measure zero sets, and introduced a topological variation later called Rothberger space (also known as \"Cformula_50 space\"). In the notation of selections, Rothberger's property is the property formula_51.\n\nAn open cover formula_52 of formula_27 is point-cofinite if it has infinitely many elements, and every point formula_54 belongs to all but finitely many sets formula_55. (This type of cover was considered by Gerlits and Nagy, in the third item of a certain list in their paper. The list was enumerated by Greek letters, and thus these covers are often called formula_56-covers.) The class of point-cofinite open covers of formula_27 is denoted by formula_58. A topological space is a Hurewicz space if it satisfies formula_59.\n\nAn open cover formula_52 of formula_27 is an formula_62-cover if every finite subset of formula_27 is contained in some member of formula_52. The class of formula_62-covers of formula_27 is denoted by formula_67. A topological space is a γ-space if it satisfies formula_68.\n\nBy using star selection hypotheses one obtains properties such as star-Menger (formula_69), star-Rothberger (formula_70) and star-Hurewicz (formula_71).\n\nThere are 36 selection properties of the form formula_72, for formula_73 and formula_74. Some of them are trivial (hold for all spaces, or fail for all spaces). Restricting attention to Lindelöf spaces, the diagram below, known as the \"Scheepers Diagram\", presents nontrivial selection properties of the above form, and every nontrivial selection property is equivalent to one in the diagram. Arrows denote implications.\n\nSelection principles also capture important non-covering properties.\n\nLet formula_75 be a topological space, and formula_76. The class of sets formula_28 in the space formula_75 that have the point formula_79 in their closure is denoted by formula_80. The class formula_81 consists of the \"countable\" elements of the class formula_80. The class of sequences in formula_75 that converge to formula_79 is denoted by formula_85.\n\n\nThere are close connections between selection principles and Topological Games.\n\nLet formula_27 be a topological space. The Menger game formula_102 played on formula_27 is a game for two players, Alice and Bob. It has an inning per each natural number formula_104. At the formula_105 inning, Alice chooses an open cover formula_106 of formula_27,\nand Bob chooses a finite subset formula_108 of formula_52. \nIf the family formula_110 is a cover of the space formula_27, then Bob wins the game. Otherwise, Alice wins.\n\nA strategy for a player is a function determining the move of the player, given the earlier moves of both players. A strategy for a player is a winning strategy if each play where this player sticks to this strategy is won by this player.\n\nIn a similar way, we define games for other selection principles from the given Scheepers Diagram. In all these cases a topological space has a property from the Scheepers Diagram if and only if Alice has no winning strategy in the corresponding game.\n\n\nSubsets of the real line formula_125 (with the induced subspace topology) holding selection principle properties, most notably Menger and Hurewicz spaces, can be characterized by their continuous images in the Baire space formula_126. For functions formula_127, write formula_128 if formula_129 for all but finitely many natural numbers formula_130. Let formula_28 be a subset of formula_126. The set formula_133 is bounded if there is a function formula_134 such that formula_135 for all functions formula_136. The set formula_133 is dominating if for each function formula_138 there is a function formula_139 such that formula_135.\n\n\nLet P be a property of spaces. A space formula_27 is productively P if, for each space formula_75 with property P, the product space formula_146 has property P.\n\n\n\nLet formula_27 be a Tychonoff space, and formula_155 be the space of continuous functions formula_156 with pointwise convergence topology. \n\n"}
{"id": "27431403", "url": "https://en.wikipedia.org/wiki?curid=27431403", "title": "Shadowing lemma", "text": "Shadowing lemma\n\nIn the theory of dynamical systems, the shadowing lemma is a lemma describing the behaviour of pseudo-orbits near a hyperbolic invariant set. Informally, the theory states that every pseudo-orbit (which one can think of as a numerically computed trajectory with rounding errors on every step) stays uniformly close to some true trajectory (with slightly altered initial position)—in other words, a pseudo-trajectory is \"shadowed\" by a true one. \n\nGiven a map \"f\" : \"X\" → \"X\" of a metric space (\"X\", \"d\") to itself, define a ε-pseudo-orbit (or ε-orbit) as a sequence formula_1 of points such that formula_2 belongs to a ε-neighborhood of formula_3.\n\nThen, near a hyperbolic invariant set, the following statement holds: \nLet Λ be a hyperbolic invariant set of a diffeomorphism f. There exists a neighborhood U of Λ with the following property: for any \"δ\" > 0 there exists \"ε\" > 0, such that any (finite or infinite) ε-pseudo-orbit that stays in U also stays in a δ-neighborhood of some true orbit.\n\n"}
{"id": "30922018", "url": "https://en.wikipedia.org/wiki?curid=30922018", "title": "Single-entry single-exit", "text": "Single-entry single-exit\n\nIn graph theory, a single-entry single-exit (SESE) region in a given graph is an ordered edge pair (\"a\", \"b\") of distinct control flow edges \"a\" and \"b\" where:\n\n\nwhere a node \"x\" is said to dominate node \"y\" in a directed graph if every path from start to \"y\" includes \"x\". A node \"x\" is said to postdominate a node \"y\" if every path from \"y\" to end includes \"x\". \n\nSo, \"a\" and \"b\" refer to the entry and exit edge, respectively. The first condition ensures that every path from start into the region passes through the region’s entry edge, \"a\". The second condition ensures that every path from inside the region to end passes through the region’s exit edge, \"b\". The first two conditions are necessary but not enough to characterize SESE regions: since backedges do not alter the dominance or postdominance relationships, the first two conditions alone do not prohibit backedges entering or exiting the region. The third condition encodes two constraints: every path from inside the region to a point 'above' \"a\" passed through \"b\", and every path from a point 'below' \"b\" to a point inside the region passes through \"a\".\n"}
{"id": "52535241", "url": "https://en.wikipedia.org/wiki?curid=52535241", "title": "Singularity (system theory)", "text": "Singularity (system theory)\n\nThe term singularity for an explanation of unstable systems was first, and in a most general meaning used in 1873 by James Clerk Maxwell. Maxwell does not differentiate between dynamical systems and social systems. Therefore, a singularity refers to a context in which a small change can cause a large effect. The existence of singularities is primarily an argument against determinism and absolute causality for Maxwell. Indeed, following the same initial conditions will always achieve the same results, but such a statement is of little value in a world in which the same initial conditions are never repeated.\n\nIn summary, singularities are determined by the following characteristics which can vary in strength:\n\nA further development of Maxwell's thoughts in relation to dynamic systems was carried out first by the French mathematician Henri Poincaré. Poincaré distinguished four different simple singularities (points singuliers) of differential equations. These are the node (les noeuds), the saddle (les cols), the focus (les foyers) and the center (les centers).\nIn recent times, the chaos theory found special attention. However, deterministic chaos is just a special case of a singularity, in which a small cause produces a large observable effect due to a nonlinear dynamic behavior. In contrast the singularities raised by Maxwell, such as a loose rock at a singular point on a slope, show a linear dynamic behavior as it was demonstrated by Poincaré.\nSingularities are the common staple of the chaos theory, catastrophe theory and bifurcation theory.\n\nIn social systems, a deterministic chaos is unlikely, because the elements of the system are some individuals that engage with awareness, will and foresight purposefully into the dynamic behavior of the system. However, this does not exclude that approaches deterministic chaos in social systems are available. Rather, there is also an increase in the social development of nonlinear dynamics and instabilities .\nChaos in the colloquial sense of complete disorder or confusion, however, is to be found. It is often the basis for singularities, where cause-and-effect relationships are not clear. There are already numerous examples of singularities in social systems with Maxwell and Poincaré. Maxwell states that a word can start a war and all the great discoveries of man based on singular states. Poincaré gives the example of a roofer who drops a brick and randomly kills a passing man.\n\nThe development of systems provides the science currently so before that by a singular Big Bang uniformly dispersed plasma spread after the creation of the universe in space, which is cooled with increasing expansion, so that formed atoms and finally for very small (singular) fluctuations in the uniform density inhomogeneities created self-reinforcing. They subsequently led to the formation of galaxies, stars and other systems in the universe, from which humans emerged at the end. Even if the singularity of the Big Bang can be avoided in the mathematical models, singularities remain an essential element of history.\nThe evolutionary history shows that not only successful mutations can be perceived as positive singularities, but the humanization and the human becoming, the singular most important event in the evolution and represents a jump from the continuum of past evolutionary development of the planet Earth.\nRecently, Ward and Kirschvink show that the history of life has been more influenced by disasters than by continuous evolution. Disasters are here first destructive singularities that create space for new developments in the sense of innovations as productive singularities.\n\nClosely related is the notion of singularity with the concept of complexity. J.C. Maxwell has already pointed out that a system has all the more singular points, the more complex it is. Complexity is also the basis of perceived chaos and singularities. \nSuppose a seemingly insignificant event that produces a great effect, even in a simple context, how difficult would it be to detect the reason in a complex situation with tremendously many elements and relationships.\nComplexity that is kind of a breeding ground for singularities, shows the downfall of ancient cultures. Causes such as intruders, internal conflicts or natural disasters are not sufficient alone to justify the destruction of a culture. Rather requirement is an increasing complexity and associated declining marginal returns.\nThe financial crisis of 2007-2008 shows how difficult decisions are in a very complex environment. Thus, the complexity of financial systems and financial products is a major challenge of the financial markets and institutions to look at. One solution is to reduce complexity and increase the potential for adaptation and robustness. In a complex world with increasing singularities, it is therefore necessary to abandon optimization potential to gain adaptability to external shocks and disasters.\n\n\n"}
{"id": "9730285", "url": "https://en.wikipedia.org/wiki?curid=9730285", "title": "Spherical design", "text": "Spherical design\n\nA spherical design, part of combinatorial design theory in mathematics, is a finite set of \"N\" points on the \"d\"-dimensional unit \"d\"-sphere \"S\" such that the average value of any polynomial \"f\" of degree \"t\" or less on the set equals the average value of \"f\" on the whole sphere (that is, the integral of \"f\" over \"S\" divided by the area or measure of \"S\"). Such a set is often called a spherical \"t\"-design to indicate the value of \"t\", which is a fundamental parameter.\n\nSpherical designs can be of value in approximation theory, in statistics for experimental design (being usable to construct rotatable designs), in combinatorics, and in geometry. The main problem is to find examples, given \"d\" and \"t\", that are not too large; however, such examples may be hard to come by.\nSpherical t-designs have also recently been appropriated in quantum mechanics in the form of quantum t-designs with various applications to quantum information theory, quantum computing and POVMs.\n\nThe concept of a spherical design is due to Delsarte, Goethals, and Seidel (1977). \n\nThe existence and structure of spherical designs with \"d\" = 1 (that is, in a circle) were studied in depth by Hong (1982).\n\nShortly thereafter, Seymour and Zaslavsky (1984) proved that such designs exist of all sufficiently large sizes; that is, given positive integers \"n\" and \"t\", there is a number \"N\"(\"d\",\"t\") such that for every \"N\" ≥ \"N\"(\"d\",\"t\") there exists a spherical \"t\"-design of \"N\" points in dimension \"d\". However, their proof gave no idea of how big \"N\"(\"d\",\"t\") is. Good estimates for that were found later on. Besides these \"large\" sizes, there are many sporadic small spherical designs; many of them are related to finite group actions on the sphere and are of great interest in themselves.\n\nRecently, Bondarenko, Radchenko, and Viazovska obtained the asymptotic upper bound\nformula_1 for all positive integers \"d\" and \"t\". This is optimal except that the value of \"C\" is unknown.\n\nSpherical designs with \"d\" = 2 (that is, on the surface of a sphere) ...\n\nOne application of spherical designs is for whole-sphere data collection.\nSpherical t-designs meet the \"accurately approximate integrals by sums\" criteria for \"good\" pixelizations of the sphere.\n\n\n\n"}
{"id": "39432", "url": "https://en.wikipedia.org/wiki?curid=39432", "title": "Stephen Cook", "text": "Stephen Cook\n\nStephen Arthur Cook, (born December 14, 1939) is an American-Canadian computer scientist and mathematician who has made major contributions to the fields of complexity theory and proof complexity. He is a university professor at the University of Toronto, Department of Computer Science and Department of Mathematics.\n\nCook received his Bachelor's degree in 1961 from the University of Michigan, and his Master's degree and Ph.D. from Harvard University, respectively in 1962 and 1966, from the Mathematics Department. He joined the University of California, Berkeley, mathematics department in 1966 as an assistant professor, and stayed there until 1970 when he was denied reappointment. In a speech celebrating the 30th anniversary of the Berkeley EECS department, fellow Turing Award winner and Berkeley professor Richard Karp said that, \"It is to our everlasting shame that we were unable to persuade the math department to give him tenure.\" Cook joined the faculty of University of Toronto, Computer Science and Mathematics Departments in 1970 as an associate professor, where he was promoted to professor in 1975 and Distinguished Professor in 1985.\n\nStephen Cook is considered one of the forefathers of computational complexity theory.\n\nDuring his PhD, Cook worked on complexity of functions, mainly on multiplication. In his seminal 1971 paper \"The Complexity of Theorem Proving Procedures\", Cook formalized the notions of polynomial-time reduction (a.k.a. Cook reduction) and NP-completeness, and proved the existence of an NP-complete problem by showing that the Boolean satisfiability problem (usually known as SAT) is NP-complete. This theorem was proven independently by Leonid Levin in the Soviet Union, and has thus been given the name the Cook-Levin theorem. The paper also formulated the most famous problem in computer science, the P vs. NP problem. Informally, the \"P vs. NP\" question asks whether every optimization problem whose answers can be efficiently verified for correctness/optimality can be solved optimally with an efficient algorithm. Given the abundance of such optimization problems in everyday life, a positive answer to the \"P vs. NP\" question would likely have profound practical and philosophical consequences.\n\nCook conjectures that there are optimization problems (with easily checkable solutions) which cannot be solved by efficient algorithms, i.e., P is not equal to NP. This conjecture has generated a great deal of research in computational complexity theory, which has considerably improved our understanding of the inherent difficulty of computational problems and what can be computed efficiently. Yet, the conjecture remains open and is among the seven famous Millennium Prize Problems.\n\nIn 1982, Cook received the Turing award for his contributions to complexity theory. His citation reads:\n\nFor his advancement of our understanding of the complexity of computation in a significant and profound way. His seminal paper, \"The Complexity of Theorem Proving Procedures,\" presented at the 1971 ACM SIGACT Symposium on the Theory of Computing, laid the foundations for the theory of NP-Completeness. The ensuing exploration of the boundaries and nature of NP-complete class of problems has been one of the most active and important research activities in computer science for the last decade.\n\nIn his \"Feasibly Constructive Proofs and the Propositional Calculus\" paper published in 1975, he introduced the equational theory PV (standing for Polynomial-time Verifiable) to formalize the notion of proofs using only polynomial-time concepts. He made another major contribution to the field in his 1979 paper, joint with his student Robert A. Reckhow, \"The Relative Efficiency of Propositional Proof Systems\", in which they formalized the notions of p-simulation and efficient propositional proof system, which started an area now called propositional proof complexity. They proved that the existence of a proof system in which every true formula has a short proof is equivalent to NP = coNP. Cook co-authored a book with his student Phuong The Nguyen in this area titled \"Logical Foundations of Proof Complexity\".\n\nHis main research areas are complexity theory and proof complexity, with excursions into programming language semantics, parallel computation, and artificial intelligence. Other areas which he has contributed to include bounded arithmetic, bounded reverse mathematics, complexity of higher type functions, complexity of analysis, and lower bounds in propositional proof systems.\n\nHe named the complexity class NC after Nick Pippenger. The complexity class SC is named after him. The definition of the complexity class AC0 and its hierarchy AC are also introduced by him.\n\nAccording to Don Knuth the KMP algorithm was inspired by Cook's automata for recognizing concatenated palindromes in linear time.\n\nCook was awarded a Steacie Fellowship in 1977, a Killam Research Fellowship in 1982, and received the CRM-Fields-PIMS prize in 1999. He has won John L. Synge Award and Bernard Bolzano Medal, and is a fellow of the Royal Society of London and Royal Society of Canada. Cook was elected to membership in the National Academy of Sciences (United States) and the American Academy of Arts and Sciences.\n\nCook won the ACM Turing Award in 1982. \nAssociation for Computing Machinery honored him as a Fellow of ACM in 2008 for his\n\"fundamental contributions to the theory of computational complexity\".\nThe Government of Ontario appointed him to the Order of Ontario in 2013, the highest honor in Ontario. He has won the 2012 Gerhard Herzberg Canada Gold Medal for Science and Engineering, the highest honor for scientist and engineers in Canada. The Herzberg Medal is awarded by NSERC for \"both the sustained excellence and overall influence of research work conducted in Canada in the natural sciences or engineering\". He was named an Officer of the Order of Canada in 2015.\n\nCook was granted the BBVA Foundation Frontiers of Knowledge Award 2015 in the Information and Communication Technologies category \"for his important role in identifying what computers can and cannot solve efficiently,\" in the words of the jury's citation. His work, it continues, \"has had a dramatic impact in all fields where complex computations are crucial.\"\n\nCook has supervised numerous MSc students, and 34 PhD students have completed their degrees under his supervision.\n\nCook lives with his wife in Toronto. They have two sons, Gordon and James. He plays the violin and enjoys sailing. He is often called by his short name Steve Cook.\n\n\n"}
{"id": "35892437", "url": "https://en.wikipedia.org/wiki?curid=35892437", "title": "Strassburg tablet", "text": "Strassburg tablet\n\nThe Strassburg tablet, dating to c. 1800 BCE, is the oldest instance of algebra that has been found. The Strassburg tablet is often claimed to be the historical beginning point of algebra. The Strassburg tablet is a Babylonian tablet that was recently discovered.\n\nThe Strassburg tablet among other things asks the question: \"An area A consisting of the sum of two squares is 1000. The side of one square is 10 less than two-thirds of the side of the other square. What are the sides of the square?\"\n\n\n"}
{"id": "412909", "url": "https://en.wikipedia.org/wiki?curid=412909", "title": "Substructural logic", "text": "Substructural logic\n\nIn logic, a substructural logic is a logic lacking one of the usual structural rules (e.g. of classical and intuitionistic logic), such as weakening, contraction, exchange or associativity. Two of the more significant substructural logics are relevance logic and linear logic.\n\nIn a sequent calculus, one writes each line of a proof as\n\nHere the structural rules are rules for rewriting the LHS of the sequent, denoted Γ, initially conceived of as a string (sequence) of propositions. The standard interpretation of this string is as conjunction: we expect to read\n\nas the sequent notation for\n\nHere we are taking the RHS Σ to be a single proposition \"C\" (which is the intuitionistic style of sequent); but everything applies equally to the general case, since all the manipulations are taking place to the left of the turnstile symbol formula_3.\n\nSince conjunction is a commutative and associative operation, the formal setting-up of sequent theory normally includes structural rules for rewriting the sequent Γ accordingly—for example for deducing\n\nfrom\n\nThere are further structural rules corresponding to the \"idempotent\" and \"monotonic\" properties of conjunction: from\n\nwe can deduce\n\nAlso from\n\none can deduce, for any \"B\",\n\nLinear logic, in which duplicated hypotheses 'count' differently from single occurrences, leaves out both of these rules, while relevant (or relevance) logics merely leaves out the latter rule, on the ground that \"B\" is clearly irrelevant to the conclusion.\n\nThe above are basic examples of structural rules. It is not that these rules are contentious, when applied in conventional propositional calculus. They occur naturally in proof theory, and were first noticed there (before receiving a name).\n\nThere are numerous ways to compose premises (and in the multiple-conclusion case, conclusions as well). One way is to collect them into a set. But since e.g. {a,a} = {a} we have contraction for free if premises are sets. We also have associativity and permutation (or commutativity) for free as well, among other properties. In substructural logics, typically premises are not composed into sets, but rather they are composed into more fine-grained structures, such as trees or multisets (sets that distinguish multiple occurrences of elements) or sequences of formulae. For example, in linear logic, since contraction fails, the premises must be composed in something at least as fine-grained as multisets.\n\nIt is a relatively young field. The first conference on the topic was held in October 1990 in Tübingen, as \"Logics with Restricted Structural Rules\". During the conference Kosta Došen proposed the term \"substructural logics\", which is now in use today.\n\n\n\n\n"}
{"id": "45661050", "url": "https://en.wikipedia.org/wiki?curid=45661050", "title": "Swirl function", "text": "Swirl function\n\nIn mathematics, swirl functions are special functions defined as follows：\n\nwhere \"k\", \"n\" are integers.\n\n\"n\" is the number of blades, \"k\" is related to the shape of each blade.\n\nThe function \"S\"(\"k\",\"n\",\"r\",\"θ\") satisfies the following relations:\n\n\n\n"}
{"id": "7385565", "url": "https://en.wikipedia.org/wiki?curid=7385565", "title": "Thue number", "text": "Thue number\n\nIn the mathematical area of graph theory, the Thue number of a graph is a variation of the chromatic index, defined by Alon et al. (2002) and named after mathematician Axel Thue, who studied the squarefree words used to define this number.\n\nAlon et al. define a \"nonrepetitive coloring\" of a graph to be an assignment of colors to the edges of the graph, such that there does not exist any even-length simple path in the graph in which the colors of the edges in the first half of the path form the same sequence as the colors of the edges in the second half of the path. The Thue number of a graph is the minimum number of colors needed in any nonrepetitive coloring.\n\nVariations on this concept involving vertex colorings or more general walks on a graph have been studied by several authors including Barát and Varjú, Barát and Wood (2005), Brešar and Klavžar (2004), and Kündgen and Pelsmajer.\n\nConsider a pentagon, that is, a cycle of five vertices. If we color the edges with two colors, some two adjacent edges will have the same color x; the path formed by those two edges will have the repetitive color sequence xx. If we color the edges with three colors, one of the three colors will be used only once; the path of four edges formed by the other two colors will either have two consecutive edges or will form the repetitive color sequence xyxy. However, with four colors it is not difficult to avoid all repetitions. Therefore, the Thue number of \"C\" is four.\n\nAlon et al. use the Lovász local lemma to prove that the Thue number of any graph is at most quadratic in its maximum degree; they provide an example showing that for some graphs this quadratic dependence is necessary. In addition they show that the Thue number of a path of four or more vertices is exactly three, and that the Thue number of any cycle is at most four, and that the Thue number of the Petersen graph is exactly five.\n\nThe known cycles with Thue number four are \"C\", \"C\", \"C\", \"C\", \"C\", and \"C\". Alon et al. conjecture that the Thue number of any larger cycle is three; they verified computationally that the cycles listed above are the only ones of length ≤ 2001 with Thue number four. Currie resolved this in a 2002 paper, showing that all cycles with 18 or more vertices have Thue number 3.\n\nTesting whether a coloring has a repetitive path is in NP, so testing whether a coloring is nonrepetitive is in co-NP, and Manin showed that it is co-NP-complete. The problem of finding such a coloring belongs to formula_1 in the polynomial hierarchy, and again Manin showed that it is complete for this level.\n\n"}
{"id": "81560", "url": "https://en.wikipedia.org/wiki?curid=81560", "title": "Zeros and poles", "text": "Zeros and poles\n\nIn mathematics, a zero of a function is a value such that .\n\nIn complex analysis, zeros of holomorphic functions and meromorphic functions play a particularly important role because of the duality between zeros and poles.\n\nA function of a complex variable is \"meromorphic\" in the neighbourhood of a point formula_1 if either or its reciprocal function is \"holomorphic\" in some neighbourhood of formula_2 (that is, if or is differentiable in a neighbourhood of formula_2). If formula_2 is zero of , then it is a pole of .\n\nThus a pole is a certain type of singularity of a function, nearby which the function behaves relatively regularly, in contrast to essential singularities, such as 0 for the logarithm function, and branch points, such as 0 for the complex square root function.\n\nA function of a complex variable is holomorphic in an open domain if it is differentiable with respect to at every point of . Equivalently, it is holomorphic if it is analytic, that is, if its Taylor series exists at every point of , and converges to the function in some neighbourhood of the point. A function is meromorphic in if every point of has a neighbourhood such that either or is holomorphic in it.\n\nA zero of a meromorphic function is a complex number such that . A pole of is a zero of .\n\nIf is a function that is meromorphic in a neighbourhood of a point formula_2 of the complex plane, then there exists an integer such that \nis holomorphic and nonzero in a neighbourhood of formula_2 (this is a consequence of the analytic property).\nIf , then formula_2 is a \"pole\" of order (or multiplicity) of . If , then formula_2 is a \"zero\" of order formula_10 of . \"Simple zero\" and \"simple pole\" are terms used for zeroes and poles of order formula_11. \"Degree\" is sometimes used synonymously to order.\n\nThis characterization of zeros and poles implies that zeros and poles are isolated, that is, every zero or pole has a neighbourhood that does not contain any other zero and pole.\n\nBecause of the \"order\" of zeros and poles being defined as a non-negative number and the symmetry between them, it is often useful to consider a pole of order as a zero of order and a zero of order as a pole of order . In this case a point that is neither a pole nor a zero is viewed as a pole (or zero) of order 0.\n\nA meromorphic function may have infinitely many zeros and poles. This is the case for the Gamma function (see the image in the info box), which is meromorphic in the whole complex plane, and has a simple pole at every non-positive integer. The Riemann zeta function is also meromorphic in the whole complex plane, with a single pole of order 1 at . It has no zeros in the left halfplane besides at all even integers along the negative real line, and the Riemann hypothesis is the conjecture that all other zeros are along .\n\nIn a neighbourhood of a point formula_1 a nonzero meromorphic function is the sum of a Laurent series with at most finite \"principal part\" (the terms with negative index values):\nwhere is an integer, and formula_14 Again, if (the sum starts with formula_15, the principal part has terms), one has a pole of order , and if (the sum starts with formula_16, there is no principal part), one has a zero of order formula_10.\n\nA function formula_18 is \"meromorphic at infinity\" if it is meromorphic in some neighbourhood of infinity (that is outside some disk), and there is an integer such that \nexists and is a nonzero complex number.\n\nIn this case, the point at infinity is a pole of order if , and a zero of order if .\n\nFor example, a polynomial of degree has a pole of degree at infinity.\n\nThe complex plane extended by a point at infinity is called the Riemann sphere.\n\nIf is a function that is meromorphic on the whole Riemann sphere, then it has a finite number of zeros and poles, and the sum of the orders of its poles equals the sum of the orders of its zeros. \n\nEvery rational function is meromorphic on the whole Riemann sphere, and, in this case, the sum of orders of the zeros or of the poles is the maximum of the degrees of the numerator and the denominator.\n\n\n\n\n\nAll above examples except for the third are rational functions. For a general discussion of zeros and poles of such functions, see .\n\nThe concept of zeros and poles extends naturally to functions on a \"complex curve\", that is complex analytic manifold of dimension one (over the complex numbers). The simplest examples of such curves are the complex plane and the Riemann surface. This extension is done by transferring structures and properties through charts, which are analytic isomorphisms.\n\nMore precisely, let be a function from a complex curve to the complex numbers. This function is holomorphic (resp. meromorphic) in a neighbourhood of a point of if there is a chart formula_30 such that formula_31 is holomorphic (resp. meromorphic) in a neighbourhood of formula_32 Then, is a pole or a zero of order if the same is true for formula_32\n\nIf the curve is compact, and the function is meromorphic on the whole curve, then the number of zeros and poles is finite, and the sum of the orders of the poles equals the sum of the orders of the zeros. This is one of the basic facts that are involved in Riemann–Roch theorem.\n\n\n"}
{"id": "22483813", "url": "https://en.wikipedia.org/wiki?curid=22483813", "title": "Łojasiewicz inequality", "text": "Łojasiewicz inequality\n\nIn real algebraic geometry, the Łojasiewicz inequality, named after Stanisław Łojasiewicz, gives an upper bound for the distance of a point to the nearest zero of a given real analytic function. Specifically, let ƒ : \"U\" → R be a real analytic function on an open set \"U\" in R, and let \"Z\" be the zero locus of ƒ. Assume that \"Z\" is not empty. Then for any compact set \"K\" in \"U\", there exist positive constants α and \"C\" such that, for all \"x\" in \"K\" \n\nHere α can be large.\n\nThe following form of this inequality is often seen in more analytic contexts: with the same assumptions on ƒ, for every \"p\" ∈ \"U\" there is a possibly smaller open neighborhood \"W\" of \"p\" and constants θ ∈ (0,1) and \"c\" > 0 such that\n\n"}
