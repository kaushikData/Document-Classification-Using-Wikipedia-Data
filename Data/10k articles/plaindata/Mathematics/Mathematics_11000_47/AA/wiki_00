{"id": "4360474", "url": "https://en.wikipedia.org/wiki?curid=4360474", "title": "178 (number)", "text": "178 (number)\n\n178 (one hundred [and] seventy-eight) is the natural number following 177 and preceding 179.\n\n\n\n\n\n178 is also:\n\n\n"}
{"id": "2966204", "url": "https://en.wikipedia.org/wiki?curid=2966204", "title": "241 (number)", "text": "241 (number)\n\n241 (two hundred [and] forty-one) is the natural number between 240 and 242. It is also a prime number.\n\n241 is the larger of the twin primes (239, 241). Twin primes are pairs of primes separated by 2. \n\n241 is a regular prime and a lucky prime. \n\nSince 241 = 15 × 2 + 1, it is a Proth prime.\n\n241 is a repdigit in base 15 (111).\n\n241 is the only known Lucas–Wieferich prime to (\"U\", \"V\") = (3, −1).\n\nAmericium-241 is the most common isotope of Americium, used in smoke detectors.\n\n\n"}
{"id": "21009545", "url": "https://en.wikipedia.org/wiki?curid=21009545", "title": "Algebra Universalis", "text": "Algebra Universalis\n\nAlgebra Universalis is an international scientific journal focused on universal algebra and lattice theory. The journal, founded in 1971, is currently published by Springer-Verlag. Honorary editors in chief of the journal included Alfred Tarski and (currently) Bjarni Jónsson.\n\n"}
{"id": "58883860", "url": "https://en.wikipedia.org/wiki?curid=58883860", "title": "An Introduction to the Philosophy of Mathematics", "text": "An Introduction to the Philosophy of Mathematics\n\nAn Introduction to the Philosophy of Mathematics is a 2012 book by Mark Colyvan in which he offers an introduction to the philosophy of mathematics with a focus on contemporary debates including realism/anti-realism, mathematical explanation, the limits of mathematics, the significance of mathematical notation, inconsistent mathematics and the applications of mathematics.\n\n"}
{"id": "301670", "url": "https://en.wikipedia.org/wiki?curid=301670", "title": "Anti-de Sitter space", "text": "Anti-de Sitter space\n\nIn mathematics and physics, \"n\"-dimensional anti-de Sitter space (AdS) is a maximally symmetric Lorentzian manifold with constant negative scalar curvature. Anti-de Sitter space and de Sitter space are named after Willem de Sitter (1872–1934), professor of astronomy at Leiden University and director of the Leiden Observatory. Willem de Sitter and Albert Einstein worked together closely in Leiden in the 1920s on the spacetime structure of the universe.\n\nManifolds of constant curvature are most familiar in the case of two dimensions, where the surface of a sphere is a surface of constant positive curvature, a flat (Euclidean) plane is a surface of constant zero curvature, and a hyperbolic plane is a surface of constant negative curvature.\n\nEinstein's general theory of relativity places space and time on equal footing, so that one considers the geometry of a unified spacetime instead of considering space and time separately. The cases of spacetime of constant curvature are de Sitter space (positive), Minkowski space (zero), and anti-de Sitter space (negative). As such, they are exact solutions of Einstein's field equations for an empty universe with a positive, zero, or negative cosmological constant, respectively.\n\nAnti-de Sitter space generalises to any number of space dimensions. In higher dimensions, it is best known for its role in the AdS/CFT correspondence, which suggests that it is possible to describe a force in quantum mechanics (like electromagnetism, the weak force or the strong force) in a certain number of dimensions (for example four) with a string theory where the strings exist in an anti-de Sitter space, with one additional dimension.\n\nThis non-technical explanation first defines the terms used in the introductory material of this entry. Then, it briefly sets forth the underlying idea of a general relativity-like spacetime. Then it discusses how de Sitter space describes a distinct variant of the ordinary spacetime of general relativity (called Minkowski space) related to the cosmological constant, and how anti-de Sitter space differs from de Sitter space. It also explains that Minkowski space, de Sitter space and anti-de Sitter space, as applied to general relativity, can all be thought of as being embedded in a flat five-dimensional spacetime. Finally, it offers some caveats that describe in general terms how this non-technical explanation fails to capture the full detail of the mathematical concept.\n\nA maximally symmetric Lorentzian manifold is a spacetime in which no point in space and time can be distinguished in any way from another, and (being Lorentzian) the only way in which a direction (or tangent to a path at a spacetime point) can be distinguished is whether it is spacelike, lightlike or timelike. The space of special relativity (Minkowski space) is an example.\n\nA constant scalar curvature means a general relativity gravity-like bending of spacetime that has a curvature described by a single number that is the same everywhere in spacetime in the absence of matter or energy.\n\nNegative curvature means curved hyperbolically, like a saddle surface or the Gabriel's Horn surface, similar to that of a trumpet bell. It might be described as being the \"opposite\" of the surface of a sphere, which has a positive curvature.\n\nGeneral relativity is a theory of the nature of time, space and gravity in which gravity is a curvature of space and time that results from the presence of matter or energy. Energy and mass are equivalent (as expressed in the equation \"E\" = \"mc\"), and space and time can be translated into equivalent units based on the speed of light (\"c\" in the \"E\" = \"mc\" equation).\n\nA common analogy involves the way that a dip in a flat sheet of rubber, caused by a heavy object sitting on it, influences the path taken by small objects rolling nearby, causing them to deviate inward from the path they would have followed had the heavy object been absent. Of course, in general relativity, both the small and large objects mutually influence the curvature of spacetime.\n\nThe attractive force of gravity created by matter is due to a negative curvature of spacetime, represented in the rubber sheet analogy by the negatively curved (trumpet-bell-like) dip in the sheet.\n\nA key feature of general relativity is that it describes gravity not as a conventional force like electromagnetism, but as a change in the geometry of spacetime that results from the presence of matter or energy.\n\nThe analogy used above describes the curvature of a two-dimensional space caused by gravity in general relativity in a three-dimensional superspace in which the third dimension corresponds to the effect of gravity. A geometrical way of thinking about general relativity describes the effects of the gravity in the real world four-dimensional space geometrically by projecting that space into a five-dimensional superspace with the fifth dimension corresponding to the curvature in spacetime that is produced by gravity and gravity-like effects in general relativity.\n\nAs a result, in general relativity, the familiar Newtonian equation of gravity formula_1 (i.e. gravitation pull between two objects equals the gravitational constant times the product of their masses divided by the square of the distance between them) is merely an approximation of the gravity-like effects seen in general relativity. However this approximation becomes inaccurate in extreme physical situations. For example, in general relativity, objects in motion have a slightly different gravitation effect than objects at rest.\n\nSome of the differences between the familiar Newtonian equation of gravity and the predictions of general relativity flow from the fact that gravity in general relativity bends both time and space, not just space. In normal circumstances, gravity bends time so slightly that the differences between Newtonian gravity and general relativity are detectable only with precise instruments.\n\nde Sitter space involves a variation of general relativity in which spacetime is slightly curved in the absence of matter or energy. This is analogous to the relationship between Euclidean geometry and non-Euclidean geometry.\n\nAn intrinsic curvature of spacetime in the absence of matter or energy is modeled by the cosmological constant in general relativity. This corresponds to the vacuum having an energy density and pressure. This spacetime geometry results in initially parallel timelike geodesics diverging, with spacelike sections having positive curvature.\n\nAn anti-de Sitter space in general relativity is similar to a de Sitter space, except with the sign of the curvature changed. In anti-de Sitter space, in the absence of matter or energy, the curvature of spacelike sections is negative, corresponding to a hyperbolic geometry, and initially parallel timelike geodesics eventually intersect. This corresponds to a negative cosmological constant (which does not match cosmological observations). Here, empty space itself has negative energy density but positive pressure.\n\nIn an anti-de Sitter space, as in a de Sitter space, the inherent spacetime curvature corresponds to the cosmological constant.\n\nAs noted above, the analogy used above describes curvature of a two-dimensional space caused by gravity in general relativity in a three-dimensional embedding space that is flat, like the Minkowski space of special relativity. Embedding de Sitter and anti-de Sitter spaces of five flat dimensions allows the properties of the embedded spaces to be determined. Distances and angles within the embedded space may be directly determined from the simpler properties of the five-dimensional flat space.\n\nWhile anti-de Sitter space does not correspond to gravity in general relativity with the observed cosmological constant, an anti-de Sitter space is believed to correspond to other forces in quantum mechanics (like electromagnetism, the weak nuclear force and the strong nuclear force). This is called the AdS/CFT correspondence.\n\nThe remainder of this article explains the details of these concepts with a much more rigorous and precise mathematical and physical description. People are ill-suited to visualizing things in five or more dimensions, but mathematical equations are not similarly challenged and can represent five-dimensional concepts in a way just as appropriate as the methods that mathematical equations use to describe easier to visualize three and four-dimensional concepts.\n\nThere is a particularly important implication of the more precise mathematical description that differs from the analogy-based heuristic description of de Sitter space and anti-de Sitter space above. The mathematical description of anti-de Sitter space generalizes the idea of curvature. In the mathematical description, curvature is a property of a particular point and can be divorced from some invisible surface to which curved points in spacetime meld themselves. So for example, concepts like singularities (the most widely known of which in general relativity is the black hole) which cannot be expressed completely in a real world geometry, can correspond to particular states of a mathematical equation.\n\nThe full mathematical description also captures some subtle distinctions made in general relativity between space-like dimensions and time-like dimensions.\n\nMuch as spherical and hyperbolic spaces can be visualized by an isometric embedding in a flat space of one higher dimension (as the sphere and pseudosphere respectively), anti-de Sitter space can be visualized as the Lorentzian analogue of a sphere in a space of one additional dimension. To a physicist the extra dimension is timelike. In this article we adopt the convention that the metric in a timelike direction is negative.\nThe anti-de Sitter space of signature can then be isometrically embedded in the space formula_2 with coordinates and the metric\nas the quasi-sphere\nwhere formula_5 is a nonzero constant with dimensions of length (the radius of curvature). This is a (generalized) sphere in the sense that it is a collection of points for which the \"distance\" (determined by the quadratic form) from the origin is constant, but visually it is a hyperboloid, as in the image shown.\n\nThe metric on anti-de Sitter space is that induced from the ambient metric. It is nondegenerate and, in the case of has Lorentzian signature.\n\nWhen , this construction gives a standard hyperbolic space. The remainder of the discussion applies when .\n\nWhen , the embedding above has closed timelike curves; for example, the path parameterized by formula_6 and all other coordinates zero, is such a curve. When these curves are inherent to the geometry (unsurprisingly, as any space with more than one temporal dimension contains closed timelike curves), but when , they can be eliminated by passing to the universal covering space, effectively \"unrolling\" the embedding. A similar situation occurs with the pseudosphere, which curls around on itself although the hyperbolic plane does not; as a result it contains self-intersecting straight lines (geodesics) while the hyperbolic plane does not. Some authors define anti-de Sitter space as equivalent to the embedded quasi-sphere itself, while others define it as equivalent to the universal cover of the embedding.\n\nIf the universal cover is not taken, anti-de Sitter space has as its isometry group. If the universal cover is taken the isometry group is a cover of . This is most easily understood by defining anti-de Sitter space as a symmetric space, using the quotient space construction, given below.\n\nA coordinate patch covering part of the space gives the half-space coordinatization of anti-de Sitter space. The metric tensor for this patch is\n\nwith formula_8 giving the half-space. We easily see that this metric is conformally equivalent to a flat half-space Minkowski spacetime.\n\nThe constant time slices of this coordinate patch are hyperbolic spaces in the Poincaré half-space metric. In the limit as formula_9, this half-space metric is conformally equivalent to the Minkowski metric formula_10. Thus, the anti-de Sitter space contains a conformal Minkowski space at infinity (\"infinity\" having y-coordinate zero in this patch).\n\nIn AdS space time is periodic, and the universal cover has non-periodic time. The coordinate patch above covers half of a single period of the spacetime.\n\nBecause the conformal infinity of AdS is timelike, specifying the initial data on a spacelike hypersurface would not determine the future evolution uniquely (\"i.e.\" deterministically) unless there are boundary conditions associated with the conformal infinity.\n\nAnother commonly used coordinate system which covers the entire space is given by the coordinates t, formula_11 and the hyper-polar coordinates α, θ and φ.\n\nThe adjacent image represents the \"half-space\" region of anti-de Sitter space and its boundary. The interior of the cylinder corresponds to anti-de Sitter spacetime, while its cylindrical boundary corresponds to its conformal boundary. The green shaded region in the interior corresponds to the region of AdS covered by the half-space coordinates and it is bounded by two null, \"aka\" lightlike, geodesic hyperplanes; the green shaded area on the surface corresponds to the region of conformal space covered by Minkowski space.\n\nThe green shaded region covers half of the AdS space and half of the conformal spacetime; the left ends of the green discs will touch in the same fashion as the right ends.\n\nIn the same way that the 2-sphere\n\nis a quotient of two orthogonal groups, anti-de Sitter with parity (reflectional symmetry) and time reversal symmetry can be seen as a quotient of two generalized orthogonal groups\n\nwhereas AdS without P or C can be seen as the quotient\n\nof spin groups.\n\nThis quotient formulation gives formula_16 the structure of a homogeneous space. The Lie algebra of the generalized orthogonal group formula_17 is given by matrices\n\nwhere formula_19 is a skew-symmetric matrix. A complementary generator in the Lie algebra of formula_20 is\n\nThese two fulfill formula_22. Explicit matrix computation shows that \nformula_23 and formula_24. Thus, anti-de Sitter is a reductive homogeneous space, and a non-Riemannian symmetric space.\n\nformula_16 is an \"n\"-dimensional solution for the theory of gravitation with Einstein–Hilbert action with negative cosmological constant formula_26, (formula_27), i.e. the theory described by the following Lagrangian density:\nwhere is the gravitational constant in -dimensional spacetime.\nTherefore, it is a solution of the Einstein field equations:\nwhere formula_30 is Einstein tensor and formula_31 is the metric of the spacetime. Introducing the radius formula_32 as formula_33 this solution can be immersed in a formula_34 dimensional spacetime with signature formula_35 by the following constraint:\n\nformula_16is parametrized in global coordinates by the parameters formula_38 as:\nwhere formula_40 parametrize a formula_41 sphere. i.e. we have \nformula_42, \nformula_43,\nformula_44 etc. The formula_16 metric in these coordinates is:\nwhere formula_47 and formula_48. Considering the periodicity of time formula_49 and in order to avoid closed timelike curves (CTC), one should take the universal cover formula_50. In the limit formula_51 one can approach to the boundary of this spacetime usually called formula_16 conformal boundary.\n\nWith the transformations formula_53 and formula_54 we can have the usual formula_16 metric in global coordinates:\nwhere formula_57\n\nBy the following parametrization:\nthe formula_59 metric in the Poincaré coordinates is:\nin which formula_61. The codimension 2 surface formula_62 is Poincaré Killing horizon and formula_63 approaches to the boundary of formula_16 spacetime, so unlike the global coordinates, the Poincaré coordinates do not cover all formula_16 manifold. Using formula_66 this metric can be written in the following way:\nwhere formula_68. By the transformation formula_69 also it can be written as:\n\nformula_16 metric with radius formula_5 is one of the maximal symmetric \"n\"-dimensional spacetimes. It has the following geometric properties:\n\nRiemann curvature tensor:\n\nRicci curvature:\n\nScalar curvature:\n\n\n"}
{"id": "25648858", "url": "https://en.wikipedia.org/wiki?curid=25648858", "title": "Anton Kotzig", "text": "Anton Kotzig\n\nAnton Kotzig (22 October 1919 – 20 April 1991) was a Slovak–Canadian mathematician, expert in statistics, combinatorics and graph theory.\n\nThe Ringel-Kotzig conjecture on graceful labeling of trees is named after him and Gerhard Ringel.\n\nKotzig was born in Kočovce, a village in Western Slovakia, in 1919. He studied at the secondary grammar school in Nové Mesto nad Váhom, and began his undergraduate studies at Charles University in Prague. After the closure of Czech universities in 1939, he moved to Bratislava, where after the war he earned a doctoral degree (RNDr.) in mathematical statistics from Comenius University in Bratislava. He remained in Bratislava working at the Central Bureau of Social Insurance for Slovakia, as the head of department of mathematical statistics.\n\nLater he published a book on economy planning. From 1951 to 1959, he lectured at Vysoká škola Ekonomická (today University of Economics in Bratislava), where he served as rector from 1952 to 1958. Thus he spent 20 years in close contact with applications of mathematics.\n\nIn 1959, he left the University of Economics to become the head of the newly created Mathematical Institute of the Slovak Academy of Sciences, where he remained until 1964. From 1965 to 1969, he was head of the department of Applied Mathematics on Faculty of Natural Sciences of Comenius University, where he was also dean for one year. He also earned a habilitation degree (DrSc.) from Charles University in 1961 for a thesis in graph theory (relation and regular relation of finite graphs).\n\nBy 1969, the list of his publications already included over 60 articles and 4 books. Many of his results have become classical, including results about graph relations, 1-factors and cubic graphs. As they were published only in Slovak, many of them remained unknown and some of the results were independently rediscovered much later by other mathematicians. \nOne of his arguments is generally known as Kotzig's Theorem. Kotzig established the now well-known Slovak School of Graph Theory. One of his first students was RNDR. Juraj Bosák, DrSc., who was awarded the Czechoslovak State Prize in 1969.\n\nIn 1969, Kotzig moved to Canada, and spent a year at the University of Calgary. He became a researcher at the Centre de recherches mathematiques (CRM) and the University of Montreal in 1970, where he remained until his death. Because of the political situation, he could not travel back to Czechoslovakia, and remained in his adopted country without his books and notes. Although he was separated from his Slovak students, he continued doing mathematics. The result was more than 75 articles, supervision of many masters and doctoral theses, as well as university lecture courses.\n\nHis publications covers a wide range of topics in graph theory and combinatorics;; convex polyhedra, quasigroups, special decompositions Hamiltonian paths , Latin squares, decompositions of complete graphs, perfect systems of difference sets, additive sequences of permutations, tournaments and combinatorial games theory.\n\nIn honor of his 60th birthday, the publication \"Theory and Practice of Combinatorics\" was published, to which various experts from around the world contributed. Kotzig published many open problems. One of them is the still unresolved Ringel-Kotzig hypothesis. He died on April 20. 1991 in Montreal, leaving his wife Edita, and a son Ľuboš.\n\nIn 1999, a commemorative plaque was erected on his birth house in Kočovce on the 80th anniversary of his birth.\n\n\n"}
{"id": "28767637", "url": "https://en.wikipedia.org/wiki?curid=28767637", "title": "Arnaud Beauville", "text": "Arnaud Beauville\n\nArnaud Beauville (born 10 May 1947) is a French mathematician, whose research interest is algebraic geometry.\n\nBeauville earned his doctorate from Paris Diderot University in 1977, with a thesis regarding Prym varieties and the Schottky problem, under supervision of Jean-Louis Verdier.\n\nHe has been a Professor at the Université Paris-Sud, then Director of the Mathematics Department at the École Normale Supérieure. He is currently Professor emeritus at the Université de Nice Sophia-Antipolis.\n\nBeauville was a visiting scholar at the Institute for Advanced Study in the summer of 1982. He was an invited speaker at the International Congress of Mathematicians in 1986 at Berkeley. He was a member of Bourbaki.\nHe has had 25 Ph.D. students, among them Claire Voisin, Olivier Debarre, Yves Laszlo.\n\nIn 2012 he became a fellow of the American Mathematical Society.\n\n\n"}
{"id": "1884241", "url": "https://en.wikipedia.org/wiki?curid=1884241", "title": "Brahmagupta theorem", "text": "Brahmagupta theorem\n\nIn geometry, Brahmagupta's theorem states that if a cyclic quadrilateral is orthodiagonal (that is, has perpendicular diagonals), then the perpendicular to a side from the point of intersection of the diagonals always bisects the opposite side. It is named after the Indian mathematician Brahmagupta.\n\nMore specifically, let \"A\", \"B\", \"C\" and \"D\" be four points on a circle such that the lines \"AC\" and \"BD\" are perpendicular. Denote the intersection of \"AC\" and \"BD\" by \"M\". Drop the perpendicular from \"M\" to the line \"BC\", calling the intersection \"E\". Let \"F\" be the intersection of the line \"EM\" and the edge \"AD\". Then, the theorem states that \"F\" is the midpoint \"AD\".\n\nWe need to prove that \"AF\" = \"FD\". We will prove that both \"AF\" and \"FD\" are in fact equal to \"FM\".\n\nTo prove that \"AF\" = \"FM\", first note that the angles \"FAM\" and \"CBM\" are equal, because they are inscribed angles that intercept the same arc of the circle. Furthermore, the angles \"CBM\" and \"CME\" are both complementary to angle \"BCM\" (i.e., they add up to 90°), and are therefore equal. Finally, the angles \"CME\" and \"FMA\" are the same. Hence, \"AFM\" is an isosceles triangle, and thus the sides \"AF\" and \"FM\" are equal.\n\nThe proof that \"FD\" = \"FM\" goes similarly: the angles \"FDM\", \"BCM\", \"BME\" and \"DMF\" are all equal, so \"DFM\" is an isosceles triangle, so \"FD\" = \"FM\". It follows that \"AF\" = \"FD\", as the theorem claims.\n\n\n"}
{"id": "7762273", "url": "https://en.wikipedia.org/wiki?curid=7762273", "title": "Cheeger constant (graph theory)", "text": "Cheeger constant (graph theory)\n\nIn mathematics, the Cheeger constant (also Cheeger number or isoperimetric number) of a graph is a numerical measure of whether or not a graph has a \"bottleneck\". The Cheeger constant as a measure of \"bottleneckedness\" is of great interest in many areas: for example, constructing well-connected networks of computers, card shuffling. The graph theoretical notion originated after the Cheeger isoperimetric constant of a compact Riemannian manifold.\n\nThe Cheeger constant is named after the mathematician Jeff Cheeger.\n\nLet be an undirected finite graph with vertex set and edge set . For a collection of vertices , let denote the collection of all edges going from a vertex in to a vertex outside of (sometimes called the \"edge boundary\" of ):\n\n(Remember that edges are unordered, so the edge is the same as the edge .) Then the Cheeger constant of , denoted , is defined by\n\nThe Cheeger constant is strictly positive if and only if is a connected graph. Intuitively, if the Cheeger constant is small but positive, then there exists a \"bottleneck\", in the sense that there are two \"large\" sets of vertices with \"few\" links (edges) between them. The Cheeger constant is \"large\" if any possible division of the vertex set into two subsets has \"many\" links between those two subsets.\n\nIn applications to theoretical computer science, one wishes to devise network configurations for which the Cheeger constant is high (at least, bounded away from zero) even when (the number of computers in the network) is large.\n\nFor example, consider a ring network of computers, thought of as a graph . Number the computers clockwise around the ring. Mathematically, the vertex set and the edge set are given by:\n\nTake to be a collection of formula_4 of these computers in a connected chain:\n\nSo,\n\nand\n\nThis example provides an upper bound for the Cheeger constant , which also tends to zero as . Consequently, we would regard a ring network as highly \"bottlenecked\" for large , and this is highly undesirable in practical terms. We would only need one of the computers on the ring to fail, and network performance would be greatly reduced. If two non-adjacent computers were to fail, the network would split into two disconnected components.\n\nThe Cheeger constant is especially important in the context of expander graphs as it is a way to measure the edge expansion of a graph. The so-called Cheeger inequalities relate the Eigenvalue gap of a graph with its Cheeger constant.\n\n\n"}
{"id": "44695495", "url": "https://en.wikipedia.org/wiki?curid=44695495", "title": "Chomsky–Schützenberger representation theorem", "text": "Chomsky–Schützenberger representation theorem\n\nIn formal language theory, the Chomsky–Schützenberger representation theorem is a theorem derived by Noam Chomsky and Marcel-Paul Schützenberger about representing a given context-free language in terms of two simpler languages. These two simpler languages, namely a regular language and a Dyck language, are combined by means of an intersection and a homomorphism.\n\nA few notions from formal language theory are in order. A context-free language is \"regular\", if can be described by a regular expression, or, equivalently, if it is accepted by a finite automaton. A homomorphism is based on a function formula_1 which maps symbols from an alphabet formula_2 to words over another alphabet formula_3; If the domain of this function is extended to words over formula_2 in the natural way, by letting formula_5 for all words formula_6 and formula_7, this yields a \"homomorphism\" formula_8. A \"matched alphabet\" formula_9 is an alphabet with two equal-sized sets; it is convenient to think of it as a set of parentheses types, where formula_10 contains the opening parenthesis symbols, whereas the symbols in formula_11 contains the closing parenthesis symbols. For a matched alphabet formula_9, the \"Dyck language\" formula_13 is given by\nwords that are well-nested parentheses over formula_9.\n\nProofs of this theorem are found in several textbooks, e.g. or .\n\n"}
{"id": "19903176", "url": "https://en.wikipedia.org/wiki?curid=19903176", "title": "Closed geodesic", "text": "Closed geodesic\n\nIn differential geometry and dynamical systems, a closed geodesic on a Riemannian manifold is a geodesic that returns to its starting point with the same tangent direction. It may be formalized as the projection of a closed orbit of the geodesic flow on the tangent space of the manifold.\n\nIn a Riemannian manifold (\"M\",\"g\"), a closed geodesic is a curve formula_1 that is a geodesic for the metric \"g\" and is periodic.\n\nClosed geodesics can be characterized by means of a variational principle. Denoting by formula_2 the space of smooth 1-periodic curves on \"M\", closed geodesics of period 1 are precisely the critical points of the energy function formula_3, defined by\n\nformula_4\n\nIf formula_5 is a closed geodesic of period \"p\", the reparametrized curve formula_6 is a closed geodesic of period 1, and therefore it is a critical point of \"E\". If formula_5 is a critical point of \"E\", so are the reparametrized curves formula_8, for each formula_9, defined by formula_10. Thus every closed geodesic on \"M\" gives rise to an infinite sequence of critical points of the energy \"E\".\n\nOn the unit sphere formula_11 with the standard round Riemannian metric, every great circle is an example of a closed geodesic. Thus, on the sphere, all geodesics are closed. On a smooth surface topologically equivalent to the sphere, this may not be true, but there are always at least three simple closed geodesics; this is the theorem of the three geodesics. Manifolds all of whose geodesics are closed have been thoroughly investigated in the mathematical literature. On a compact hyperbolic surface, whose fundamental group has no torsion, closed geodesics are in one-to-one correspondence with non-trivial conjugacy classes of elements in the Fuchsian group of the surface.\n\n\n"}
{"id": "46179999", "url": "https://en.wikipedia.org/wiki?curid=46179999", "title": "Compound of tetrahedra", "text": "Compound of tetrahedra\n\nA compound of tetrahedra might be:\n"}
{"id": "7147956", "url": "https://en.wikipedia.org/wiki?curid=7147956", "title": "Differential calculus over commutative algebras", "text": "Differential calculus over commutative algebras\n\nIn mathematics the differential calculus over commutative algebras is a part of commutative algebra based on the observation that most concepts known from classical differential calculus can be formulated in purely algebraic terms. Instances of this are:\n\nformula_14\n\nwhere the bracket formula_15 is defined as the commutator\n\nformula_16\n\nDenoting the set of \"k\"th order linear differential operators from an formula_5-module formula_18 to an formula_5-module formula_20 with formula_21 we obtain a bi-functor with values in the category of formula_5-modules. Other natural concepts of calculus such as jet spaces, differential forms are then obtained as representing objects of the functors formula_23 and related functors.\n\nSeen from this point of view calculus may in fact be understood as the theory of these functors and their representing objects.\n\nReplacing the real numbers formula_2 with any commutative ring, and the algebra formula_25 with any commutative algebra the above said remains meaningful, hence differential calculus can be developed for arbitrary commutative algebras. Many of these concepts are widely used in algebraic geometry, differential geometry and secondary calculus. Moreover, the theory generalizes naturally to the setting of graded commutative algebra, allowing for a natural foundation of calculus on supermanifolds, graded manifolds and associated concepts like the Berezin integral.\n\n\n"}
{"id": "1138912", "url": "https://en.wikipedia.org/wiki?curid=1138912", "title": "Differential of the first kind", "text": "Differential of the first kind\n\nIn mathematics, differential of the first kind is a traditional term used in the theories of Riemann surfaces (more generally, complex manifolds) and algebraic curves (more generally, algebraic varieties), for everywhere-regular differential 1-forms. Given a complex manifold \"M\", a differential of the first kind ω is therefore the same thing as a 1-form that is everywhere holomorphic; on an algebraic variety \"V\" that is non-singular it would be a global section of the coherent sheaf Ω of Kähler differentials. In either case the definition has its origins in the theory of abelian integrals.\n\nThe dimension of the space of differentials of the first kind, by means of this identification, is the Hodge number\n\nThe differentials of the first kind, when integrated along paths, give rise to integrals that generalise the elliptic integrals to all curves over the complex numbers. They include for example the hyperelliptic integrals of type\n\nwhere \"Q\" is a square-free polynomial of any given degree > 4. The allowable power \"k\" has to be determined by analysis of the possible pole at the point at infinity on the corresponding hyperelliptic curve. When this is done, one finds that the condition is\n\nor in other words, \"k\" at most 1 for degree of \"Q\" 5 or 6, at most 2 for degree 7 or 8, and so on (as \"g\" = [(1+ deg \"Q\")/2]).\n\nQuite generally, as this example illustrates, for a compact Riemann surface or algebraic curve, the Hodge number is the genus \"g\". For the case of algebraic surfaces, this is the quantity known classically as the irregularity \"q\". It is also, in general, the dimension of the Albanese variety, which takes the place of the Jacobian variety.\n\nThe traditional terminology also included differentials of the second kind and of the third kind. The idea behind this has been supported by modern theories of algebraic differential forms, both from the side of more Hodge theory, and through the use of morphisms to commutative algebraic groups.\n\nThe Weierstrass zeta function was called an \"integral of the second kind\" in elliptic function theory; it is a logarithmic derivative of a theta function, and therefore has simple poles, with integer residues. The decomposition of a (meromorphic) elliptic function into pieces of 'three kinds' parallels the representation as (i) a constant, plus (ii) a linear combination of translates of the Weierstrass zeta function, plus (iii) a function with arbitrary poles but no residues at them.\n\nThe same type of decomposition exists in general, \"mutatis mutandis\", though the terminology is not completely consistent. In the algebraic group (generalized Jacobian) theory the three kinds are abelian varieties, algebraic tori, and affine spaces, and the decomposition is in terms of a composition series.\n\nOn the other hand, a meromorphic abelian differential of the \"second kind\" has traditionally been one with residues at all poles being zero. One of the third kind is one where all poles are simple. There is a higher-dimensional analogue available, using the Poincaré residue\n\n"}
{"id": "985087", "url": "https://en.wikipedia.org/wiki?curid=985087", "title": "Exterior (topology)", "text": "Exterior (topology)\n\nIn topology, the exterior of a subset \"S\" of a topological space \"X\" is the union of all open sets of \"X\" which are disjoint from \"S\". It is itself an open set and is disjoint from \"S\". The exterior of \"S\" is denoted by\n\nor\n\nThe exterior is equal to \"X\" \\ \"S̅\", the complement of the topological closure of \"S\" and to the interior of the complement of \"S\" in \"X\".\n\nMany properties follow in a straightforward way from those of the interior operator, such as the following.\n\n\nUnlike the interior operator, ext is not idempotent, but the following holds:\n\n\n"}
{"id": "864595", "url": "https://en.wikipedia.org/wiki?curid=864595", "title": "First Draft of a Report on the EDVAC", "text": "First Draft of a Report on the EDVAC\n\nThe First Draft of a Report on the EDVAC (commonly shortened to First Draft) is an incomplete 101-page document written by John von Neumann and distributed on June 30, 1945 by Herman Goldstine, security officer on the classified ENIAC project. It contains the first published description of the logical design of a computer using the stored-program concept, which has controversially come to be known as the von Neumann architecture.\n\nVon Neumann wrote the report by hand while commuting by train to Los Alamos, New Mexico and mailed the handwritten notes back to Philadelphia. Goldstine had the report typed and duplicated. While the date on the typed report is June 30, 24 copies of the \"First Draft\" were distributed to persons closely connected with the EDVAC project five days earlier on June 25. Interest in the report caused it to be sent all over the world; Maurice Wilkes of Cambridge University cited his excitement over the report's content as the impetus for his decision to travel to the United States for the Moore School Lectures in Summer 1946.\n\nVon Neumann describes a detailed design of a \"very high speed automatic digital computing system.\" He divides it into six major subdivisions: a central arithmetic part, CA, a central control part, CC, memory, M, input, I, output, O, and (slow) external memory, R, such as punched cards, Teletype tape, or magnetic wire or steel tape.\n\nThe CA will perform addition, subtraction, multiplication, division and square root. Other mathematical operations, such as logarithms and trigonometric functions are to be done with table look up and interpolation, possibly biquadratic. He notes that multiplication and division could be done with logarithm tables, but to keep the tables small enough, interpolation would be needed and this in turn requires multiplication, though perhaps with less precision. \n\nNumbers are to be represented in binary notation. He estimates 27 binary digits (he did not use the term \"bit,\" which was coined by Claude Shannon in 1948) would be sufficient (yielding 8 decimal place accuracy) but rounds up to 30 bit numbers with a sign bit and a bit to distinguish numbers from orders, resulting in 32-bit word he calls a \"minor cycle.\" Two’s complement arithmetic is to be used, simplifying subtraction. For multiplication and division, he proposes placing the binary point after sign bit, which means all numbers are treated as being between -1 and 1 and therefore computation problems must be scaled accordingly.\n\nVacuum tubes are to be used rather than relays due to tubes’ ability operate in one microsecond vs 10 milliseconds for relays.\n\nVon Neumann suggests (Sec. 5.6) keeping the computer as simple as possible, avoiding any attempt at improving performance by overlapping operations. Arithmetic operations are to be performed one binary digit at a time. He estimates addition of two binary digits as taking one microsecond and that therefore a 30-bit multiplication should take about 30 microseconds or about one millisecond, much faster than any computing device available at the time. \n\nVon Neumann's design is built up using what he call \"E elements,\" which are based on the biological neuron as model, but are digital devices which he says can be constructed using one or two vacuum tubes. In modern terms his simplest E element is a two input ‘\"and\" gate with one input inverted (the inhibit input). E elements with more inputs have an associated threshold and produce an output when the number of positive input signals meets or exceed the threshold, so long as the (only) inhibit line is not pulsed. He states that E elements with more inputs can be constructed from the simplest version, but suggests they be built directly as vacuum tube circuits as fewer tubes will be needed.\n\nMore complex function blocks are to be built from these E elements. He shows how to use these E elements to build circuits for addition, subtraction, multiplication, division and square root, as well as two state memory blocks and control circuits. He does not use Boolean logic terminology. \n\nCircuits are to be synchronous with a master system clock derived from a vacuum tube oscillator, possibly crystal controlled. His logic diagrams include an arrowhead symbol to denote a unit time delay, as time delays must be accounted for in a synchronous design. He points out that in one microsecond an electric pulse moves 300 meters so that until much higher clock speeds, e.g. 10 cycles per second (100 MHz), wire length would not be an issue.\n\nThe need for error detection and correction is mentioned but not elaborated.\n\nA key design concept enunciated, and later named the Von Neumann architecture, is a uniform memory containing both numbers (data) and orders (instructions).\n\"The device requires a considerable memory. While it appeared that various parts of this memory have to perform functions which differ somewhat in their nature and considerably in their purpose, it is nevertheless tempting to treat the entire memory as one organ, and to have its parts even as interchangeable as possible for the various functions enumerated above.\" (Sec. 2.5)\n\n\"The orders which are received by CC come from M, i.e. from the same place where the numerical material is stored.\" (Sec. 14.0)\nVon Neumann estimates the amount of memory required based on several classes of mathematical problems, including ordinary and partial differential equations, sorting and probability experiments. Of these, partial differential equations in two dimensions plus time will require the most memory, with three dimensions plus time being beyond what can be done using technology that was then available. He concludes that memory will be the largest subdivision of the system and he proposes 8,192 minor cycles (words) of 32-bits as a design goal, with 2,048 minor cycles still being useful. He estimates a few hundred minor cycles will suffice for storing the program.\n\nHe proposes two kinds of fast memory, delay line and Iconoscope tube. Each minor cycle is to be addressed as a unit (word addressing, Sec. 12.8).\nInstructions are to be executed sequentially, with a special instruction to switch to a different point in memory (i.e. a jump instruction). \n\nBinary digits in a delay line memory pass through the line and are fed back to the beginning. Accessing data in a delay line imposes a time penalty while waiting for the desired data to come around again. After analyzing these timing issues, he proposes organizing the delay line memory into 256 delay line \"organs\" (DLAs) each storing 1024 bits, or 32 minor cycles, called a \"major cycle.\" A memory access first selects the DLA (8 bits) and then the minor cycle within the DLA (5 bits), for a total of 13 address bits. \n\nFor the Iconoscope memory, he recognizes that each scan point on the tube face is a capacitor and that a capacitor can store one bit. Very high precision scanning will be needed and the memory will only last a short time, perhaps as little as a second, and therefore will need to be periodically recopied (refreshed).\n\nIn Sec 14.1 von Neumann proposes the format for orders, which he calls a code. Order types include the basic arithmetic operations, moving minor cycles between CA and M (word load and store in modern terms), an order (\"s\") that selects one of two numbers based on the sign of the previous operation, input and output and transferring CC to a memory location elsewhere (a jump). He determines the number of bits needed for the different order types, suggests \"immediate orders\" where the following word is the operand and discusses the desirability of leaving spare bits in the order format to allow for more addressable memory in the future, as well as other unspecified purposes. The possibility of storing more than one order in a minor cycle is discussed, with little enthusiasm for that approach. A table of orders is provided, but no discussion of input and output instructions was included in the First Draft.\n\nThe treatment of the preliminary report as a publication (in the legal sense) was the source of bitter acrimony between factions of the EDVAC design team for two reasons. First, publication amounted to a public disclosure that prevented the EDVAC from being patented; second, some on the EDVAC design team contended that the stored-program concept had evolved out of meetings at the University of Pennsylvania's Moore School of Electrical Engineering predating von Neumann's activity as a consultant there, and that much of the work represented in the \"First Draft\" was no more than a translation of the discussed concepts into the language of formal logic in which von Neumann was fluent. Hence, failure of von Neumann and Goldstine to list others as authors on the \"First Draft\" led credit to be attributed to von Neumann alone. (See Matthew effect and Stigler's law.)\n\n\n\n"}
{"id": "3655074", "url": "https://en.wikipedia.org/wiki?curid=3655074", "title": "First variation", "text": "First variation\n\nIn applied mathematics and the calculus of variations, the first variation of a functional \"J\"(\"y\") is defined as the linear functional formula_1 mapping the function \"h\" to\n\nwhere \"y\" and \"h\" are functions, and \"ε\" is a scalar. This is recognizable as the Gâteaux derivative of the functional.\n\nCompute the first variation of\n\nFrom the definition above,\n\n"}
{"id": "35880830", "url": "https://en.wikipedia.org/wiki?curid=35880830", "title": "Fractional wavelet transform", "text": "Fractional wavelet transform\n\nFractional wavelet transform (FRWT) is a generalization of the classical wavelet transform (WT). This transform is proposed in order to rectify the limitations of the WT and the fractional Fourier transform (FRFT). The FRWT inherits the advantages of multiresolution analysis of the WT and has the capability of signal representations in the fractional domain which is similar to the FRFT.\n\nThe fractional Fourier transform (FRFT)，a generalization of the Fourier transform (FT),\nserves a useful and powerful analyzing tool in optics, communications, signal and image processing, etc.\nThis transform, however, has one major drawback due to using global kernel, i.e., the fractional Fourier representation only provides such FRFT spectral content with no indication about the time localization of the FRFT spectral components.\nTherefore, the analysis of non-stationary signals whose FRFT spectral characteristics change with time requires joint signal representations in both time and FRFT domains, rather than just a FRFT-domain representation.\n\nThe first modification to the FRFT to allow analysis of aforementioned non-stationary signals came as the short-time FRFT (STFRFT). The idea behind the STFRFT was segmenting the signal by using a time-localized window, and performing FRFT spectral analysis for each segment.\nSince the FRFT was computed for every windowed segment of the signal, the STFRFT was able to provide a true joint signal representation in both time and FRFT domains.\nHowever, the drawback is that the STFRFT has the limitation of a fixed window width which needs to be fixed a priori; this effectively means that it does not provide the requisite good resolution in both time and FRFT domains. In other words, the efficiency of the STFRFT techniques is limited by the fundamental uncertainty principle, which implies that narrow windows produce good time resolution but poor spectral resolution, whereas wide windows provide good spectral resolution but poor time resolution. Most of the signals of practical interest are such that they have high spectral components for short durations and low spectral components for long durations.\n\nAs a generalization of the wavelet transform, Mendlovic and David first introduced the fractional wavelet transform (FRWT) as a way to deal with optical signals, which was defined as a cascade of the FRFT and the ordinary wavelet transform (WT), i.e., \nwhere the transform kernel formula_2 is given by\nwhere formula_4, and formula_5 denotes the FRFT of formula_6. But it couldn't be regarded as a kind of joint time-FRFT-domain representation since time information is lost in this transform. Moreover, Prasad and Mahato expressed the ordinary WT of a signal in terms of the FRFTs of the signal and mother wavelet, and also called the expression the FRWT. That is,\nwhere formula_8 and formula_9 denote the FTs (with their arguments scaled by formula_10) formula_6 and formula_12, respectively. Clearly, this so-called FRWT is identical to the ordinary WT.\n\nRecently, Shi et al. proposed a new definition of the FRWT by introducing a new structure of the fractional convolution associated with the FRFT. \nSpecifically, the FRWT of any function formula_13 is defined as [8]\nwhere formula_15 is a continuous affine transformation and chirp modulation of the mother wavelet formula_12, i.e.,\nin which formula_18 and formula_19 are scaling and translation parameters, respectively.\nConversely, the inverse FRWT is given by\nwhere formula_21 is a constant that depends on the wavelet used. The success of the reconstruction depends on this constant called, the admissibility constant, to satisfy the following admissibility condition:\nwhere formula_23 denotes the FT of formula_12. The admissibility condition implies that formula_25, which is formula_26. Consequently, continuous fractional wavelets must oscillate and behave as bandpass filters in the fractional Fourier domain. From this viewpoint, the FRWT of formula_6 can be expressed in terms of the FRFT-domain representation as\nwhere formula_5 indicates the FRFT of formula_6, and formula_31 denotes the FT (with its argument scaled by formula_32) of formula_12. \nNote that when formula_34, the FRWT reduces to the classical WT. For more details of this type of the FRWT, see [8] and.\n\nA comprehensive overview of MRA and orthogonal fractional wavelets associated with the FRWT can be found in the paper.\n"}
{"id": "6766624", "url": "https://en.wikipedia.org/wiki?curid=6766624", "title": "Generalized algebraic data type", "text": "Generalized algebraic data type\n\nIn functional programming, a generalized algebraic data type (GADT, also first-class phantom type, guarded recursive datatype, or equality-qualified type) is a generalization of parametric algebraic data types.\n\nIn a GADT, the product constructors (called data constructors in Haskell) can provide an explicit instantiation of the ADT as the type instantiation of their return value. This allows one to define functions with a more advanced type behaviour. For a data constructor of Haskell 98, the return value has the type instantiation implied by the instantiation of the ADT parameters at the constructor's application.\n-- A parametric ADT that is not a GADT\ndata List a = Nil | Cons a (List a)\n\nintegers = Cons 12 (Cons 107 Nil) -- the type of integers is List Int\nstrings = Cons \"boat\" (Cons \"dock\" Nil) -- the type of strings is List String\n\n-- A GADT\ndata Expr a where\n\neval :: Expr a -> a\n\neval e = case e of\n\nexpr1 = EEqual (EInt 2) (EInt 3) -- the type of expr1 is Expr Bool\nret = eval expr1 -- ret is False\nThey are currently implemented in the GHC compiler as a non-standard extension, used by, among others, Pugs and Darcs. OCaml supports GADT natively since version 4.00.\n\nThe GHC implementation provides support for existentially quantified type parameters and for local constraints.\n\nAn early version of generalized algebraic data types were described by and based on pattern matching in ALF.\n\nGeneralized algebraic data types were introduced independently by and prior by as extensions to ML's and Haskell's algebraic data types. Both are essentially equivalent to each other. They are similar to the \"inductive families of data types\" (or \"inductive datatypes\") found in Coq's Calculus of Inductive Constructions and other dependently typed languages, modulo the dependent types and except that the latter have an additional positivity restriction which is not enforced in GADTs.\n\nType inference in the absence of any programmer supplied type annotations is undecidable and functions defined over GADTs do not admit principal types in general. Type reconstruction requires several design trade-offs and is an area of active research (; ; ; ; ; ; ; ; ; ).\n\nApplications of GADTs include generic programming, modelling programming languages (higher-order abstract syntax), maintaining invariants in data structures, expressing constraints in embedded domain-specific languages, and modelling objects.\n\nAn important application of GADTs is to embed higher-order abstract syntax in a type safe fashion. Here is an embedding of the simply typed lambda calculus with an arbitrary collection of base types, tuples and a fixed point combinator:\nAnd a type safe evaluation function:\n\nThe factorial function can now be written as:\n\nWe would have run into problems using regular algebraic data types. Dropping the type parameter would have made the lifted base types existentially quantified, making it impossible to write the evaluator. With a type parameter we would still be restricted to a single base type. Furthermore, ill-formed expressions such as codice_1 would have been possible to construct, while they are type incorrect using the GADT. A well-formed analogue is codice_2. This is because the type of codice_3 is codice_4, inferred from the type of the codice_5 data constructor.\n\n\n"}
{"id": "1127884", "url": "https://en.wikipedia.org/wiki?curid=1127884", "title": "Hamming weight", "text": "Hamming weight\n\nThe Hamming weight of a string is the number of symbols that are different from the zero-symbol of the alphabet used. It is thus equivalent to the Hamming distance from the all-zero string of the same length. For the most typical case, a string of bits, this is the number of 1's in the string, or the digit sum of the binary representation of a given number and the \"ℓ\"₁ norm of a bit vector. In this binary case, it is also called the population count, popcount, sideways sum, or bit summation.\n\nThe Hamming weight is named after Richard Hamming although he did not originate the notion. The Hamming weight of binary numbers was already used in 1899 by James W. L. Glaisher to give a formula for the number of odd binomial coefficients in a single row of Pascal's triangle. Irving S. Reed introduced a concept, equivalent to Hamming weight in the binary case, in 1954.\n\nHamming weight is used in several disciplines including information theory, coding theory, and cryptography. Examples of applications of the Hamming weight include:\n\nThe population count of a bitstring is often needed in cryptography and other applications. The Hamming distance of two words \"A\" and \"B\" can be calculated as the Hamming weight of \"A\" xor \"B\".\n\nThe problem of how to implement it efficiently has been widely studied. Some processors have a single command to calculate it (see below), and some have parallel operations on bit vectors. For processors lacking those features, the best solutions known are based on adding counts in a tree pattern. For example, to count the number of 1 bits in the 16-bit binary number a = 0110 1100 1011 1010, these operations can be done:\n\nHere, the operations are as in C programming language, so means to shift X right by Y bits, X & Y means the bitwise AND of X and Y, and + is ordinary addition. The best algorithms known for this problem are based on the concept illustrated above and are given here:\n\nThe above implementations have the best worst-case behavior of any known algorithm. However, when a value is expected to have few nonzero bits, it may instead be more efficient to use algorithms that count these bits one at a time. As described, the bitwise and of \"x\" with \"x\" − 1 differs from \"x\" only in zeroing out the least significant nonzero bit: subtracting 1 changes the rightmost string of 0s to 1s, and changes the rightmost 1 to a 0. If \"x\" originally had \"n\" bits that were 1, then after only \"n\" iterations of this operation, \"x\" will be reduced to zero. The following implementation is based on this principle.\n\nIf we are allowed greater memory usage, we can calculate the Hamming weight faster than the above methods. With unlimited memory, we could simply create a large lookup table of the Hamming weight of every 64 bit integer. If we can store a lookup table of the hamming function of every 16 bit integer, we can do the following to compute the Hamming weight of every 32 bit integer.\n\nMuła et al. have shown that a vectorized version of popcount64b can run faster than dedicated instructions (e.g., popcnt on x64 processors).\n\nSome C compilers provide intrinsic functions that provide bit counting facilities. For example, GCC (since version 3.4 in April 2004) includes a builtin function codice_1 that will use a processor instruction if available or an efficient library implementation otherwise. LLVM-GCC has included this function since version 1.5 in June 2005.\n\nIn C++ STL, the bit-array data structure codice_2 has a codice_3 method that counts the number of bits that are set.\n\nIn Java, the growable bit-array data structure has a method that counts the number of bits that are set. In addition, there are and functions to count bits in primitive 32-bit and 64-bit integers, respectively. Also, the arbitrary-precision integer class also has a method that counts bits.\n\nIn Common Lisp, the function logcount, given a non-negative integer, returns the number of 1 bits. (For negative integers it returns the number of 0 bits in 2's complement notation.) In either case the integer can be a BIGNUM.\n\nStarting in GHC 7.4, the Haskell base package has a codice_4 function available on all types that are instances of the codice_5 class (available from the codice_6 module).\n\nMySQL version of SQL language provides codice_7 as a standard function.\n\nFortran 2008 has the standard, intrinsic, elemental function codice_8 returning the number of nonzero bits within an integer (or integer array).\n\nSome programmable scientific pocket calculators feature special commands to calculate the number of set bits, e.g. codice_9 on the HP-16C, codice_10, codice_11, or codice_12 on the WP 34S.\n\n\n\n\n"}
{"id": "1013950", "url": "https://en.wikipedia.org/wiki?curid=1013950", "title": "Heilbronn triangle problem", "text": "Heilbronn triangle problem\n\nIn discrete geometry and discrepancy theory, the Heilbronn triangle problem is a problem of placing points within a region in the plane, in order to avoid triangles of small area. It is named after Hans Heilbronn, who conjectured prior to 1950 that this smallest triangle area is necessarily at most inversely proportional to the square of the number of points. Heilbronn's conjecture was proven false, but the asymptotic growth rate of the minimum triangle area remains unknown.\n\nThe problem may be defined in terms of any compact set \"D\" in the plane with nonzero area such as the unit square or the unit disk. If \"S\" is a set of \"n\" points of \"D\", then every three points of \"S\" determine a triangle (possibly a degenerate one, with zero area). Let Δ(\"S\") denote the minimum of the areas of these triangles, and let Δ(\"n\") (for an integer \"n\" ≥ 3) denote the supremum of the values of Δ(\"S\").\n\nThe question posed by Heilbronn was to give an expression, or matching asymptotic upper and lower bounds, for Δ(\"n\"). That is, the goal is to find a function \"f\", described by a closed-form expression, and constants \"c\" and \"c\", such that for all \"n\",\nIn terms of big O notation, the left inequality may be written as Δ(\"n\") = Ω(\"f\"(\"n\")), the right inequality may be written as Δ(\"n\") = \"O\"(\"f\"(\"n\")), and both of them together may be written as Δ(\"n\") = Θ(\"f\"(\"n\")). The shape and area of \"D\" may affect the exact values of Δ(\"n\"), but only by a constant factor, so they are unimportant for its asymptotic growth rate.\n\nHeilbronn conjectured that\nAs Paul Erdős showed, no smaller bound is possible: when \"n\" is a prime number, the set of \"n\" points (\"i\", \"i\" mod \"n\") on an \"n\" × \"n\" integer grid have no three collinear points, and therefore by Pick's formula each of the triangles they form has area at least 1/2. When this set of grid points is scaled to a unit square, they form a set of points whose smallest triangle area is at least proportional to 1/\"n\", matching Heilbronn's conjectured upper bound.\nIf \"n\" is not prime, then a similar construction using the next prime number larger than \"n\" achieves the same asymptotic lower bound.\n\nTrivially, either by triangulating the convex hull of the given point set \"S\" or by choosing consecutive triples of points in the sorted order of their \"x\"-coordinates, it is possible to show that every point set contains a small triangle, whose area is at most inversely proportional to \"n\". was the first to prove a nontrivial upper bound on Δ(\"n\"), of the form\nThe best bound known to date is of the form\nfor some constant \"c\", proven by .\n\n has investigated the optimal arrangements of \"n\" points in a square, for \"n\" up to 16. Goldberg's constructions for up to six points lie on the boundary of the square, and are placed to form an affine transformation of the vertices of a regular polygon. For larger values of \"n\", improved Goldberg's bounds, and for these values the solutions include points interior to the square. These constructions have been proven optimal for up to seven points.\n\nThere have been many variations of this problem \nincluding the case of a uniformly random set of points, for which an argument based on Kolmogorov complexity shows that the expected value of the minimum area is inversely proportional to the cube of the number of points. Variations involving the volume of higher-dimensional simplices have also been studied.\n\n"}
{"id": "27689116", "url": "https://en.wikipedia.org/wiki?curid=27689116", "title": "Huzihiro Araki", "text": "Huzihiro Araki\n\nAraki is the son of the University of Kyoto physics professor Gentarō Araki, with whom he studied and with whom in 1954 he published his first physics paper. He earned his diploma under Hideki Yukawa and in 1960 he attained his doctorate at Princeton University with thesis advisors Rudolf Haag and Arthur Strong Wightman (\"Hamiltonian formalism and canonical commutation relations in quantum field theory\"). He was since 1966 professor at the University of Kyoto, at the Research Institute for Mathematical Sciences (RIMS), of which he was also the director.\n\nAraki works on axiomatic quantum field theory and statistical mechanics in particular on application of operator algebras (von Neumann algebras, C*-algebras). He already at the beginning of the 1960s at Princeton made important contributions to the \"local quantum physics\" of Haag and Kastler and also to the scattering theories of Haag and David Ruelle. He also supplied important contributions in the mathematical theory of operator algebras, classifying type-III factors of von Neumann algebras. Araki originated the concept of relative entropy of states of Von Neumann algebras. In the 1970s he showed the equivalence in quantum thermodynamics of, on the one hand, the KMS (Kubo-Martin-Schwinger) condition for the characterization of quantum mechanical states in thermodynamic equilibrium with, on the other hand, the variational principle for quantum mechanical spin systems on lattices. With Yanase he worked on the foundations of quantum mechanics (Wigner-Araki-Yanase Theorem, which describes restrictions that conservation laws impose upon the physical measuring process). Stated in more precise terms, they proved that an exact measurement of an operator, which additively replaces the operator with a conserved size, is impossible. However, Yanase did prove that the uncertainty of the measurement can be made arbitrarily small, provided that the measuring apparatus is sufficiently large.\n\nHe was the first president of the International Association of Mathematical Physics. In 2003 he received with Oded Schramm and Elliott Lieb the Henri Poincaré Prize. In 1990 he was the chief organizer of the ICM in Kyoto. He is one of the editors of the \"Communications in Mathematical Physics\" and founder of \"Reviews in Mathematical Physics\". In 2012 he became a fellow of the American Mathematical Society.\n\n\n\n"}
{"id": "3172817", "url": "https://en.wikipedia.org/wiki?curid=3172817", "title": "Hyperbolic group", "text": "Hyperbolic group\n\nIn group theory, more precisely in geometric group theory, a hyperbolic group, also known as a \"word hyperbolic group\" or \"Gromov hyperbolic group\", is a finitely generated group equipped with a word metric satisfying certain properties abstracted from classical hyperbolic geometry. The notion of a hyperbolic group was introduced and developed by . The inspiration came from various existing mathematical theories: hyperbolic geometry but also low-dimensional topology (in particular the results of Max Dehn concerning the fundamental group of a hyperbolic Riemann surface, and more complex phenomena in three-dimensional topology), and combinatorial group theory. In a very influential (over 1000 citations ) paper from 1987, Gromov proposed a wide-ranging research program. Ideas and foundational material in the theory of hyperbolic groups also stem from the work of George Mostow, William Thurston, James W. Cannon, Eliyahu Rips, and many others.\n\nLet formula_1 be a finitely generated group, and formula_2 be its Cayley graph with respect to some finite set formula_3 of generators. The set formula_2 is endowed with its graph metric (in which edges are of length one and the distance between two vertices is the minimal number of edges in a path connecting them) which turns it into a length space. The group formula_1 is then said to be \"hyperbolic\" if formula_2 is a hyperbolic space in the sense of Gromov. Shortly, this means that there exists a formula_7 such that any triangle in formula_2 is formula_9-thin, as illustrated in the figure on the right (the space is then said to be formula_9-hyperbolic).\n\nA priori this definition depends on the choice of a finite generating set formula_3. That this is not the case follows from the two following facts:\n\n\nThus we can legitimately speak of a finitely generated group formula_1 being hyperbolic without referring to a generating set. On the other hand, a space which is quasi-isometric to a formula_9-hyperbolic space is itself formula_14-hyperbolic for some formula_15 but the latter depends on both the original formula_9 and on the quasi-isometry, thus it does not make sense to speak of formula_1 being formula_9-hyperbolic.\n\nThe Svarc--Milnor lemma states that if a group formula_1 acting properly discontinuously and with compact quotient (such an action is often called \"geometric\") on a proper length space formula_20 then it is finitely generated, and any Cayley graph for formula_1 is quasi-isometric to formula_20. Thus a group is (finitely generated and) hyperbolic if and only if it has a geometric action on a proper hyperbolic space.\n\nIf formula_23 is a subgroup with finite index (i.e. the set formula_24 is finite) then the inclusion induces a quasi-isometry on the vertices of any (locally finite) Cayley graph of formula_25 into any (ditto) Cayley graph of formula_1. Thus formula_25 is hyperbolic if and only if formula_1 itself is. More generally if two groups are commensurable then one is hyperbolic if and only if the other is.\n\nThe simplest examples of hyperbolic groups are finite groups (whose Cayley graphs are of finite diameter hence formula_9-hyperbolic with formula_9 equal to this diameter).\n\nAnother very simple example is given by infinite cyclic group formula_31 (with the generating set formula_32 triangles are line segments, hence the Cayley graph is formula_33-hyperbolic). It follows also that any group which is virtually cyclic (contains a copy of formula_31 of finite index) is also hyperbolic, for example the infinite dihedral group.\n\nMembers in this class of groups are often called \"elementary hyperbolic groups\" (the terminology is adapted from that of actions on the hyperbolic plane).\n\nLet formula_35 be a finite set and formula_36 be the free group with generating set formula_3. Then the Cayley graph of formula_36 with respect to formula_3 is a locally finite tree and hence a 0-hyperbolic space. Thus formula_36 is an hyperbolic group.\n\nMore generally we see that any group formula_1 which acts properly discontinuously on a locally finite tree (in this context this means exactly that the stabilizers in formula_1 of the vertices are finite) is hyperbolic. Indeed, this follows from the fact that formula_1 has an invariant subtree on which it acts with compact quotient, and the Svarc—Milnor lemma. Such groups are in fact virtually free (i.e. contain a finitely generated free subgroup of finite index), which gives another proof of their hyperbolicity.\n\nAn interesting example is the modular group formula_44: it acts on the tree given by the 1-skeleton of the associated tessellation of the hyperbolic plane and it has a finite index free subgroup (on two generators) of index 6 (for example the set of matrices in formula_1 which reduce to the identity modulo 2 is such a group). Note an interesting feature of this example: it acts properly discontinuously on a hyperbolic space (the hyperbolic plane) but the action is not cocompact (and indeed formula_1 is \"not\" quasi-isometric to the hyperbolic plane).\n\nGeneralising the example of the modular group a Fuchsian group is a group admitting a properly discontinuous action on the hyperbolic plane (equivalently, a discrete subgroup of formula_47). The hyperbolic plane is a formula_9-hyperbolic space and hence the Svarc—Milnor lemma tells us that cocompact Fuchsian groups are hyperbolic.\n\nExamples of such are the fundamental groups of closed surfaces of negative Euler characteristic. Indeed, these surfaces can be obtained as quotients of the hyperbolic plane, as implied by the Poincaré—Koebe Uniformisation theorem.\n\nAnother family of examples of cocompact Fuchsian groups is given by triangle groups: all but finitely many are hyperbolic.\n\nGeneralising the example of closed surfaces, the fundamental groups of compact Riemannian manifolds with strictly negative sectional curvature are hyperbolic. For example, cocompact lattices in the orthogonal or unitary group of a form of signature formula_49 are hyperbolic.\n\nA further generalisation is given by groups admitting a geometric action on a CAT(k) space. There exists examples which are not commensurable to any of the previous constructions (for instance groups acting geometrically on hyperbolic buildings).\n\nGroups having presentations which satisfy small cancellation conditions are hyperbolic. This gives a source of examples which do not have a geometric origin as the ones given above. In fact one of the motivations for the initial development of hyperbolic groups was to give a more geometric interpretation of small cancellation.\n\nIn some sense, \"most\" finitely presented groups with large defining relations are hyperbolic. For a quantitative statement of what this means see Random group.\n\n\n\n\n\n\nRelatively hyperbolic groups are a class generalising hyperbolic groups. \"Very\" roughly formula_1 is hyperbolic relative to a collection formula_60 of subgroups if it admits a (\"not necessarily cocompact\") properly discontinuous action on a proper hyperbolic space formula_2 which is \"nice\" on the boundary of formula_2 and such that the stabilisers in formula_1 of points on the boundary are subgroups in formula_60. This is interesting when both formula_2 and the action of formula_1 on formula_2 are not elementary (in particular formula_2 is infinite: for example every group is hyperbolic relatively to itself via its action on a single point!).\n\nInteresting examples in this class include in particular non-uniform lattices in rank 1 semisimple Lie groups, for example fundamental groups of non-compact hyperbolic manifolds of finite volume. Non-examples are lattices in higher-rank Lie groups and mapping class groups.\n\nAn even more general notion is that of an acylindically hyperbolic group. Acylindricity of an action of a group formula_1 on a metric space formula_2 is a weakening of proper discontinuity of the action.\n\nA group is said to be acylindrically hyperbolic if it admits a non-elementary acylindrical action on a (\"not necessarily proper\") Gromov-hyperbolic space. This notion includes mapping class groups via their actions on curve complexes. Lattices in higher-rank Lie groups are (still!) not acylindrically hyperbolic.\n\nIn another direction one can weaken the assumption about curvature in the examples above: a \"CAT(0) group\" is a group admitting a geometric action on a CAT(0) space. This includes Euclidean crystallographic groups and uniform lattices in higher-rank Lie groups.\n\nIt is not known whether there exists a hyperbolic group which is not CAT(0).\n\n\n"}
{"id": "4774316", "url": "https://en.wikipedia.org/wiki?curid=4774316", "title": "Industrial-grade prime", "text": "Industrial-grade prime\n\nIndustrial-grade primes (the term is apparently due to Henri Cohen) are integers for which primality has not been certified (i.e. rigorously proven), but they have undergone probable prime tests such as the Miller-Rabin primality test, which has a positive, but negligible, failure rate, or the Baillie-PSW primality test, which no composites are known to pass.\n\nIndustrial-grade primes are sometimes used instead of certified primes in algorithms such as RSA encryption, which require the user to generate large prime numbers. Certifying the primality of large numbers (over 100 digits for instance) is significantly harder than showing they are industrial-grade primes. The latter can be done almost instantly with a failure rate so low that it is highly unlikely to ever fail in practice. In other words, the number is believed to be prime with very high, but not absolute, confidence.\n"}
{"id": "10335099", "url": "https://en.wikipedia.org/wiki?curid=10335099", "title": "Jackson integral", "text": "Jackson integral\n\nIn q-analog theory, the Jackson integral series in the theory of special functions that expresses the operation inverse to q-differentiation.\n\nThe Jackson integral was introduced by Frank Hilton Jackson. For methods of numerical evaluation, see .\n\nLet \"f\"(\"x\") be a function of a real variable \"x\". The Jackson integral of \"f\" is defined by the following series expansion:\n\nMore generally, if \"g\"(\"x\") is another function and \"D\"\"g\" denotes its \"q\"-derivative, we can formally write\n\ngiving a \"q\"-analogue of the Riemann–Stieltjes integral.\n\nJust as the ordinary antiderivative of a continuous function can be represented by its Riemann integral, it is possible to show that the Jackson integral gives a unique \"q\"-antiderivative \nwithin a certain class of functions, see,\n\nSuppose that formula_4 If formula_5 is bounded on the interval formula_6 for some formula_7 then the Jackson integral converges to a function formula_8 on formula_6 which is a \"q\"-antiderivative of formula_10 Moreover, formula_8 is continuous at formula_12 with formula_13 and is a unique antiderivative of formula_14 in this class of functions.\n\n"}
{"id": "768802", "url": "https://en.wikipedia.org/wiki?curid=768802", "title": "James Whitbread Lee Glaisher", "text": "James Whitbread Lee Glaisher\n\nJames Whitbread Lee Glaisher FRS FRSE FRAS (5 November 1848, Lewisham – 7 December 1928, Cambridge), son of James Glaisher the meteorologist and Cecilia Glaisher the photographer, was a prolific English mathematician and astronomer.\n\nHe was born in Lewisham in Kent on 5 November 1848 the son of the eminent astronomer James Glaisher and his wife, Cecilia Louisa Belville. His mother was a noted photographer.\n\nHe was educated at St Paul's School from 1858. He became somewhat of a school celebrity in 1861 when he made two hot-air balloon ascents with his father to study the stratosphere.\n\nHe won a Campden Exhibition Scholarship allowing him to study at Trinity College, Cambridge, where he was second wrangler in 1871 and was made a Fellow of the College. Influential in his time on teaching at the University of Cambridge, he is now remembered mostly for work in number theory that anticipated later interest in the detailed properties of modular forms. He published widely over other fields of mathematics.\n\nGlaisher was elected FRS in 1875. He was the editor-in-chief of \"Messenger of Mathematics\". He was also the 'tutor' of the philosopher Ludwig Wittgenstein (tutor being a non-academic role in Cambridge University). He was president of the Royal Astronomical Society 1886–1888 and 1901–1903. When George Biddell Airy retired as Astronomer Royal in 1881 it is said that Glaisher was offered the post but declined.\n\nHe did not marry and lived in a set of rooms at Trinity College. He died there on 7 December 1928.\n\nHe was a keen cyclist but preferred his penny-farthing to the newer \"safety\" bicycles. He was President of Cambridge University Cycling Club 1882 to 1885. He was a keen collector of Delftware and the university indulged him by allowing him a room of the Fitzwilliam Museum to house his personal collection. He also amassed a collection of some 1,600 valentines, which he bequeathed to the museum.\n\n\nGlaisher published many papers and was editor and contributor to both the \"Messenger of Mathematics\" and the \"Quarterly Journal of Mathematics\".\n\n\n"}
{"id": "38543091", "url": "https://en.wikipedia.org/wiki?curid=38543091", "title": "Joachim Cuntz", "text": "Joachim Cuntz\n\nJoachim Cuntz (born 28 September 1948 in Mannheim) is a German mathematician, currently a professor at the University of Münster.\n\nJoachim Cuntz has made fundamental contributions to the area of C*-algebras and to the field of noncommutative geometry in the sense of Alain Connes. He initiated the analysis of the structure of simple C*-algebras and introduced new methods and examples, including the Cuntz algebras and the Cuntz semigroup. He was one of the first to apply K-theory to noncommutative operator algebras and contributed to the development of that theory. In collaboration with Daniel Quillen, he developed a new approach to cyclic cohomology and proved the excision property of periodic cyclic theory. In recent years, he has been working mainly on C*-algebras that are related to structures from number theory.\n\nHis doctoral students include Wilhelm Winter.\n\n"}
{"id": "13956553", "url": "https://en.wikipedia.org/wiki?curid=13956553", "title": "John Kaye (bishop)", "text": "John Kaye (bishop)\n\nJohn Kaye (27 December 1783, Hammersmith – 18 February 1853, Riseholme, Lincolnshire) was an English churchman.\n\nHe was born the only son of Abraham Kaye in Hammersmith, London and educated at the school of Sir Charles Burney in Hammersmith and then Greenwich. He entered Christ's College, Cambridge and graduated Senior wrangler in 1804. He was the 21st Master of Christ's College from 1814 to 1830, Vice-Chancellor of Cambridge University in 1814, Bishop of Bristol from 1820 to 1827 and Bishop of Lincoln from 1827 until his death. He reformed the educational requirements for the Anglican clergy and attacked the Tractarians for betraying the English Reformation.\n\nHe was elected Fellow of the Royal Society in 1811.\n\nHe is buried in Lincoln Cathedral beneath a recumbent effigy by Richard Westmacott.\n\n"}
{"id": "18677709", "url": "https://en.wikipedia.org/wiki?curid=18677709", "title": "Juggler sequence", "text": "Juggler sequence\n\nIn recreational mathematics a juggler sequence is an integer sequence that starts with a positive integer \"a\", with each subsequent term in the sequence defined by the recurrence relation:\n\nJuggler sequences were publicised by American mathematician and author Clifford A. Pickover. The name is derived from the rising and falling nature of the sequences, like balls in the hands of a juggler.\n\nFor example, the juggler sequence starting with \"a\" = 3 is\n\nIf a juggler sequence reaches 1, then all subsequent terms are equal to 1. It is conjectured that all juggler sequences eventually reach 1. This conjecture has been verified for initial terms up to 10, but has not been proved. Juggler sequences therefore present a problem that is similar to the Collatz conjecture, about which Paul Erdős stated that \"mathematics is not yet ready for such problems\".\n\nFor a given initial term \"n\", one defines \"l\"(\"n\") to be the number of steps which the juggler sequence starting at \"n\" takes to first reach 1, and \"h\"(\"n\") to be the maximum value in the juggler sequence starting at \"n\". For small values of \"n\" we have:\n\nJuggler sequences can reach very large values before descending to 1. For example, the juggler sequence starting at \"a\" = 37 reaches a maximum value of 24906114455136. Harry J. Smith has determined that the juggler sequence starting at \"a\" = 48443 reaches a maximum value at \"a\" with 972,463 digits, before reaching 1 at \"a\".\n\n"}
{"id": "33297462", "url": "https://en.wikipedia.org/wiki?curid=33297462", "title": "Keller's conjecture", "text": "Keller's conjecture\n\nIn geometry, Keller's conjecture is the conjecture that in any tiling of Euclidean space by identical hypercubes there are two cubes that meet face to face. For instance, as shown in the illustration, in any tiling of the plane by identical squares, some two squares must meet edge to edge.\n\nThis conjecture was introduced by , after whom it is named. It was shown to be true in dimensions at most 6 by . However, for higher dimensions it is false, as was shown in dimensions at least 10 by and in dimensions at least 8 by . These disproofs used a reformulation of the problem in terms of the clique number of certain graphs now known as Keller graphs. Although this graph-theoretic version of the conjecture is now resolved for all dimensions, Keller's original cube-tiling conjecture remains open in dimension 7.\n\nThe related Minkowski lattice cube-tiling conjecture states that, whenever a tiling of space by identical cubes has the additional property that the cube centers form a lattice, some cubes must meet face to face. It was proved by György Hajós in 1942.\n\n, , and give surveys of work on Keller's conjecture and related problems.\n\nA family of closed sets called \"tiles\" forms a tessellation or tiling of a Euclidean space if their union is the whole space and every two distinct sets in the family have disjoint interiors. A tiling is said to be \"monohedral\" if all of the tiles are congruent to each other. Keller's conjecture concerns monohedral tilings in which all of the tiles are hypercubes of the same dimension as the space. As formulates the problem, a \"cube tiling\" is a tiling by congruent hypercubes in which the tiles are additionally required to all be translations of each other, without any rotation, or equivalently to have all of their sides parallel to the coordinate axes of the space. Not every tiling by congruent cubes has this property: for instance, three-dimensional space may be tiled by two-dimensional sheets of cubes that are twisted at arbitrary angles with respect to each other. instead defines a cube tiling to be any tiling of space by congruent hypercubes, and states without proof that the assumption that cubes are axis-parallel can be added without loss of generality.\n\nAn \"n\"-dimensional hypercube has 2\"n\" faces of dimension \"n\" − 1, that are themselves hypercubes; for instance, a square has four edges, and a three-dimensional cube has six square faces. Two tiles in a cube tiling (defined in either of the above ways) meet \"face-to-face\" if there is an (\"n\" − 1)-dimensional hypercube that is a face of both of them. Keller's conjecture is the statement that every cube tiling has at least one pair of tiles that meet face-to-face in this way.\n\nThe original version of the conjecture stated by Keller was for a stronger statement, that every cube tiling has a column of cubes all meeting face to face. As with the weaker statement more commonly studied in subsequent research, this is true for dimensions up to six, false for dimensions eight or greater, and remains open for seven dimensions \nIt is a necessary part of the conjecture that the cubes in the tiling all be congruent to each other, for if similar but not congruent cubes are allowed then the Pythagorean tiling would form a trivial counterexample in two dimensions.\n\nThe disproof of Keller's conjecture, for sufficiently high dimensions, has progressed through a sequence of reductions that transform it from a problem in the geometry of tilings into a problem in group theory, and from there into a problem in graph theory.\n\n reformulated Szabó's result as a condition about the existence of a large clique in a certain family of graphs, which subsequently became known as the Keller graphs. More precisely, the vertices of the Keller graph of dimension \"n\" are the 4 elements (\"m\"...,\"m\") where each \"m\" is 0, 1, 2, or 3. Two vertices are joined by an edge if they differ in at least two coordinates and differ by exactly two in at least one coordinate. Corrádi and Szabó showed that the maximum clique in this graph has size at most 2, and that if there is a clique of this size then Keller's conjecture is false. Given such a clique, one can form a covering of space by cubes of side two whose centers have coordinates that, when taken modulo four, are vertices of the clique. The condition that any two vertices of the clique have a coordinate that differs by two implies that cubes corresponding to these vertices do not overlap. The condition that the clique has size 2 implies that the cubes within any period of the tiling have the same total volume as the period itself. Together with the fact that they don't overlap, this implies that the cubes placed in this way tile space. However, the condition that any two clique vertices differ in at least two coordinates implies that no two cubes have a face in common.\n\nFinally, showed that the Keller graph of dimension seven has a maximum clique of size 124 <  2. Because this is less than 2, the graph-theoretic version of Keller's conjecture is true in seven dimensions. However, the translation from cube tilings to graph theory can change the dimension of the problem, so this result doesn't settle the geometric version of the conjecture in seven dimensions.\n\nThe sizes of the maximum cliques in the smaller Keller graphs of dimensions 2, 3, 4, 5, and 6 are, respectively, 2, 5, 12, 28, and 60. The Keller graphs of dimensions 4, 5, and 6 have been included in the set of \"DIMACS challenge graphs\" frequently used as a benchmark for clique-finding algorithms .\n\nAs describes, Hermann Minkowski was led to a special case of the cube-tiling conjecture from a problem in diophantine approximation. One consequence of Minkowski's theorem is that any lattice (normalized to have determinant one) must contain a nonzero point whose Chebyshev distance to the origin is at most one. The lattices that do not contain a nonzero point with Chebyshev distance strictly less than one are called critical, and the points of a critical lattice form the centers of the cubes in a cube tiling. Minkowski conjectured in 1900 that, whenever a cube tiling has its cubes centered at lattice points in this way, it must contain two cubes that meet face to face. If this is true, then (because of the symmetries of the lattice) each cube in the tiling must be part of a column of cubes, and the cross-sections of these columns form a cube tiling of one smaller dimension. Reasoning in this way, Minkowski showed that (assuming the truth of his conjecture) every critical lattice has a basis that can be expressed as a triangular matrix, with ones on its main diagonal and numbers less than one away from the diagonal. György Hajós proved Minkowski's conjecture in 1942 using Hajós's theorem on factorizations of abelian groups, a similar group-theoretic method to the one that he would later apply to Keller's more general conjecture.\n\nKeller's conjecture is a variant of Minkowski's conjecture in which the condition that the cube centers form a lattice is relaxed. A second related conjecture, made by Furtwängler in 1936, instead relaxes the condition that the cubes form a tiling. Furtwängler asked whether a system of cubes centered on lattice points, forming a \"k\"-fold covering of space (that is, all but a measure-zero subset of the points in the space must be interior to exactly \"k\" cubes) must necessarily have two cubes meeting face to face. Furtwängler's conjecture is true for two- and three-dimensional space, but Hajós found a four-dimensional counterexample in 1938. characterized the combinations of \"k\" and the dimension \"n\" that permit a counterexample. Additionally, combining both Furtwängler's and Keller's conjectures, Robinson showed that \"k\"-fold square coverings of the Euclidean plane must include two squares that meet edge to edge. However, for every \"k\" > 1 and every \"n\" > 2 there is a \"k\"-fold tiling of \"n\"-dimensional space by cubes with no shared faces .\n\nOnce counterexamples to Keller's conjecture became known, it became of interest to ask for the maximum dimension of a shared face that can be guaranteed to exist in a cube tiling. When the dimension \"n\" is at most six, this maximum dimension is just \"n\" − 1, by Perron's proof of Keller's conjecture for small dimensions, and when \"n\" is at least eight, then this maximum dimension is at most \"n\" − 2. showed more strongly that it is at most \"n\" − /3.\n\nIn 1975, Ludwig Danzer and independently Branko Grünbaum and G. C. Shephard found a tiling of three-dimensional space by parallelepipeds with 60° and 120° face angles in which no two parallelepipeds share a face; see .\n\n"}
{"id": "55704468", "url": "https://en.wikipedia.org/wiki?curid=55704468", "title": "LTPP Data Analysis Contest", "text": "LTPP Data Analysis Contest\n\nThe LTPP International Data Analysis Contest or the LTPP Data Analysis Contest is an annual international data analysis contest held by the American Society of Civil Engineers and Federal Highway Administration. As the name suggests, the participants are supposed to use the LTPP data in their analysis. The winners of this data analysis contest are announced in the early January during the Transportation Research Board annual meeting.\n\nThe LTPP database contains the data of more than 2500 road sections across the US and Canada. The FHWA and ASCE launched a joint effort to encourage researchers around the world to use the LTPP data. The contest was first introduced in 1998 by the Transportation and Development Institute (T&DI) of the American Society of Civil Engineers and the LTPP of FHWA. The goal of the contest is to encourage consultants, academics and data scientists around the world to use the LTPP database for generating knowledge about the behaviour of pavements and roads.\n\nThe LTPP data analysis contest has four different categories:\nThe first two categories are limited to students. The participants of all categories are required to summarize their work within an article.\n"}
{"id": "1135068", "url": "https://en.wikipedia.org/wiki?curid=1135068", "title": "Mythical number", "text": "Mythical number\n\n\"Not to be confused with an imaginary number.\n\nA mythical number is a number used and accepted as deriving from scientific investigation and/or careful selection, but whose origin is unknown and whose basis is unsubstantiated. An example is the number 48 billion, which has often been accepted as the number of dollars per year of identity theft. This number \"has appeared in hundreds of news stories, including a New York Times piece\" despite the fact that it has been shown repeatedly to be highly inaccurate. The term was coined in 1971 by Max Singer, one of the founders of the Hudson Institute.\n\nThe origins of such numbers are akin to those of urban legends and may include (among others):\n\n\n\nThe (Ongoing) Vitality of Mythical Numbers by Jack Shafer\n"}
{"id": "20963", "url": "https://en.wikipedia.org/wiki?curid=20963", "title": "Möbius inversion formula", "text": "Möbius inversion formula\n\nIn mathematics, the classic Möbius inversion formula was introduced into number theory on 1832 by August Ferdinand Möbius.\n\nA large generalization of this formula applies to summation over an arbitrary locally finite partially ordered set, with Möbius' classical formula applying to the set of the natural numbers ordered by divisibility: see incidence algebra.\n\nThe classic version states that if and are arithmetic functions satisfying\n\nthen\n\nwhere is the Möbius function and the sums extend over all positive divisors of . In effect, the original can be determined given by using the inversion formula. The two sequences are said to be Möbius transforms of each other.\n\nThe formula is also correct if and are functions from the positive integers into some abelian group (viewed as a -module).\n\nIn the language of Dirichlet convolutions, the first formula may be written as\n\nwhere denotes the Dirichlet convolution, and is the constant function . The second formula is then written as\n\nMany specific examples are given in the article on multiplicative functions.\n\nThe theorem follows because is (commutative and) associative, and , where is the identity function for the Dirichlet convolution, taking values , for all . Thus\n\nLet\n\nso that\n\nis its transform. The transforms are related by means of series: the Lambert series\n\nand the Dirichlet series:\n\nwhere is the Riemann zeta function.\n\nGiven an arithmetic function, one can generate a bi-infinite sequence of other arithmetic functions by repeatedly applying the first summation. \n\nFor example, if one starts with Euler's totient function , and repeatedly applies the transformation process, one obtains:\n\n\nIf the starting function is the Möbius function itself, the list of functions is:\n\nBoth of these lists of functions extend infinitely in both directions. The Möbius inversion formula enables these lists to be traversed backwards.\n\nAs an example the sequence starting with is:\n\nThe generated sequences can perhaps be more easily understood by considering the corresponding Dirichlet series: each repeated application of the transform corresponds to multiplication by the Riemann zeta function.\n\nA related inversion formula more useful in combinatorics is as follows: suppose and are complex-valued functions defined on the interval such that\n\nthen\n\nHere the sums extend over all positive integers which are less than or equal to .\n\nThis in turn is a special case of a more general form. If is an arithmetic function possessing a Dirichlet inverse , then if one defines\n\nthen\n\nThe previous formula arises in the special case of the constant function , whose Dirichlet inverse is .\n\nA particular application of the first of these extensions arises if we have (complex-valued) functions and defined on the positive integers, with\n\nBy defining and , we deduce that\n\nA simple example of the use of this formula is counting the number of reduced fractions , where and are coprime and . If we let be this number, then is the total number of fractions with , where and are not necessarily coprime. (This is because every fraction with and can be reduced to the fraction with , and vice versa.) Here it is straightforward to determine , but is harder to compute.\n\nAnother inversion formula is (where we assume that the series involved are absolutely convergent):\n\nAs above, this generalises to the case where is an arithmetic function possessing a Dirichlet inverse :\n\nAs Möbius inversion applies to any abelian group, it makes no difference whether the group operation is written as addition or as multiplication. This gives rise to the following notational variant of the inversion formula:\n\nThe first generalization can be proved as follows. We use Iverson's convention that [condition] is the indicator function of the condition, being 1 if the condition is true and 0 if false. We use the result that\nthat is, .\n\nWe have the following:\n\nThe proof in the more general case where replaces 1 is essentially identical, as is the second generalisation.\n\nFor a poset , a set endowed with a partial order relation formula_23, define the Möbius function formula_24 of recursively by \n\n(Here one assumes the summations are finite.) Then for formula_26, where is a field, we have \n\nif and only if \n\n"}
{"id": "1684758", "url": "https://en.wikipedia.org/wiki?curid=1684758", "title": "Nielsen–Thurston classification", "text": "Nielsen–Thurston classification\n\nIn mathematics, Thurston's classification theorem characterizes homeomorphisms of a compact orientable surface. William Thurston's theorem completes the work initiated by .\n\nGiven a homeomorphism \"f\" : \"S\" → \"S\", there is a map \"g\" isotopic to \"f\" such that at least one of the following holds:\n\nThe case where \"S\" is a torus (i.e., a surface whose genus is one) is handled separately (see torus bundle) and was known before Thurston's work. If the genus of \"S\" is two or greater, then \"S\" is naturally hyperbolic, and the tools of Teichmüller theory become useful. In what follows, we assume \"S\" has genus at least two, as this is the case Thurston considered. (Note, however, that the cases where \"S\" has boundary or is not orientable are definitely still of interest.)\n\nThe three types in this classification are not mutually exclusive, though a \"pseudo-Anosov\" homeomorphism is never \"periodic\" or \"reducible\". A \"reducible\" homeomorphism \"g\" can be further analyzed by cutting the surface along the preserved union of simple closed curves \"Γ\". Each of the resulting compact surfaces \"with boundary\" is acted upon by some power (i.e. iterated composition) of \"g\", and the classification can again be applied to this homeomorphism.\n\nThurston's classification applies to homeomorphisms of orientable surfaces of genus ≥ 2, but the type of a homeomorphism only depends on its associated element of the mapping class group \"Mod(S)\". In fact, the proof of the classification theorem leads to a canonical representative of each mapping class with good geometric properties. For example:\n\nThurston's original motivation for developing this classification was to find geometric structures on \"mapping tori\" of the type predicted by the Geometrization conjecture. The mapping torus \"M\" of a homeomorphism \"g\" of a surface \"S\" is the 3-manifold obtained from \"S\" × [0,1] by gluing \"S\" × {0} to \"S\" × {1} using \"g\". The geometric structure of \"M\" is related to the type of \"g\" in the classification as follows:\n\nThe first two cases are comparatively easy, while the existence of a hyperbolic structure on the mapping torus of a pseudo-Anosov homeomorphism is a deep and difficult theorem (also due to Thurston). The hyperbolic 3-manifolds that arise in this way are called \"fibered\" because they are surface bundles over the circle, and these manifolds are treated separately in the proof of Thurston's geometrization theorem for Haken manifolds. Fibered hyperbolic 3-manifolds have a number of interesting and pathological properties; for example, Cannon and Thurston showed that the surface subgroup of the arising Kleinian group has limit set which is a sphere-filling curve.\n\nThe three types of surface homeomorphisms are also related to the dynamics of the mapping class group Mod(\"S\") on the Teichmüller space \"T\"(\"S\"). Thurston introduced a compactification of \"T\"(\"S\") that is homeomorphic to a closed ball, and to which the action of Mod(\"S\") extends naturally. The type of an element \"g\" of the mapping class group in the Thurston classification is related to its fixed points when acting on the compactification of \"T\"(\"S\"):\n\nThis is reminiscent of the classification of hyperbolic isometries into \"elliptic\", \"parabolic\", and \"hyperbolic\" types (which have fixed point structures similar to the \"periodic\", \"reducible\", and \"pseudo-Anosov\" types listed above).\n\n\n"}
{"id": "49872639", "url": "https://en.wikipedia.org/wiki?curid=49872639", "title": "Olive Jean Dunn", "text": "Olive Jean Dunn\n\nOlive Jean Dunn (1 September 1915 – 12 January 2008) was an American mathematician and statistician, and professor of biostatistics at the University of California Los Angeles (UCLA). She described methods for computing confidence intervals and also codified the Bonferroni correction's application to confidence intervals. She authored the textbook, \"Basic Statistics: A Primer for the Biomedical Sciences\" in 1977.\n\nDunn studied mathematics at the UCLA, earning a BA in 1936 and an MA in 1951. She was awarded a PhD in mathematics in 1956 at UCLA, supervised by Paul G. Hoel.\n\nThe title of Dunn's doctoral dissertation was \"Estimation problems for dependent regression\".\n\nIn 1956, she was appointed assistant professor of statistics at Iowa State College. Dunn returned to UCLA in 1959 as assistant professor of biostatistics and assistant professor of preventive medicine and health, later becoming full professor and serving in that role until her retirement. Dunn served on the editorial boards of several journals.\n\nSome of Dunn's 1958 and 1959 work led to the conjecture of the Gaussian correlation inequality, which was only proved by German mathematician \nThomas Royen in 2014 and was only widely recognized as proved in 2017.\n\nDunn's doctoral dissertation work formed the basis for her continuing development of methods for confidence intervals in biostatistics, and the development of a method for correcting for multiple testing. From the notes to her 1959 publication on confidence intervals:\n\n\"Most of the research for this paper was part of my doctoral dissertation. The idea of writing an article for the research worker who uses statistical methods was suggested to me by one of the non-statisticians on my doctoral committee at the time of my final examination. In working on the various confidence intervals for \"k\" means, I thought of the Bonferroni inequality ones quite early, but since they were so simple I thought they couldn't possibly be of any use. I spent a long time trying to prove that the confidence intervals which would be used in the case of independent variables could also be used or dependent variables. After failing to find a general proof for this, I finally noticed that the simple Bonferroni intervals were nearly as short\".\n\nThe first edition of her textbook, \"Basic statistics: a primer for the biomedical sciences\", was published in 1977. Later editions were co-authored with another professor from UCLA, Virginia A. Clark. Dunn and Clark also co-authored \"Applied statistics: an analysis of variance and regression\", which has also had several editions, with Ruth M. Mickey joining the authors.\n\nDunn became a Fellow of the American Statistical Association in 1968. She was also a fellow of the American Association for the Advancement of Science (AAAS) and the American Public Health Association.\n\nIn October 1974, Dunn was honored as the annual UCLA annual Woman of Science, awarded to \"an outstanding woman who has made significant contributions in the field of science\". She was quoting as saying \"I am thrilled to death for myself and for biostatistics...There are few medical centers in the country where a woman in biostatistics would be selected for this honor\".\n"}
{"id": "18931249", "url": "https://en.wikipedia.org/wiki?curid=18931249", "title": "Principal geodesic analysis", "text": "Principal geodesic analysis\n\nIn geometric data analysis and statistical shape analysis, principal geodesic analysis is a generalization of principal component analysis to a non-Euclidean, non-linear setting of manifolds suitable for use with shape descriptors such as medial representations.\n\n"}
{"id": "1183041", "url": "https://en.wikipedia.org/wiki?curid=1183041", "title": "Quadratic residuosity problem", "text": "Quadratic residuosity problem\n\nThe quadratic residuosity problem in computational number theory is to decide, given integers formula_1 and formula_2, whether formula_1 is a quadratic residue modulo formula_2 or not.\nHere formula_5 for two unknown primes formula_6 and formula_7, and formula_1 is among the numbers which are not obviously quadratic non-residues (see below).\n\nThe problem was first described by Gauss in his \"Disquisitiones Arithmeticae\" in 1801. \nThis problem is believed to be computationally difficult.\nSeveral cryptographic methods rely on its hardness, see Applications.\n\nAn efficient algorithm for the quadratic residuosity problem immediately implies efficient algorithms for other number theoretic problems, such as deciding whether a composite formula_2 of unknown factorization is the product of 2 or 3 primes.\n\nGiven integers formula_1 and formula_11, formula_1 is said to be a \"quadratic residue modulo formula_11\" if there exists an integer formula_14 such that\n\nOtherwise we say it is a quadratic non-residue.\nWhen formula_16 is a prime, it is customary to use the Legendre symbol:\n\nThis is a multiplicative character which means formula_18 for exactly formula_19 of the values formula_20, and it is formula_21 for the remaining.\n\nIt is easy to compute using the law of quadratic reciprocity in a manner akin to the Euclidean algorithm, see Legendre symbol.\n\nConsider now some given formula_5 where formula_6 and formula_7 are two, different unknown primes.\nA given formula_1 is a quadratic residue modulo formula_2 if and only if formula_1 is a quadratic residue modulo both formula_6 and formula_7.\n\nSince we don't know formula_6 or formula_7, we cannot compute formula_32 and formula_33. However, it is easy to compute their product.\nThis is known as the Jacobi symbol:\n\nThis can also be efficiently computed using the law of quadratic reciprocity for Jacobi symbols.\n\nHowever, formula_35 can not in all cases tell us whether formula_1 is a quadratic residue modulo formula_2 or not!\nMore precisely, if formula_38 then formula_1 is necessarily a quadratic non-residue modulo either formula_6 or formula_7, in which case we are done.\nBut if formula_42 then it is either the case that formula_1 is a quadratic residue modulo both formula_6 and formula_7, or a quadratic non-residue modulo both formula_6 and formula_7.\nWe cannot distinguish these cases from knowing just that formula_42.\n\nThis leads to the precise formulation of the quadratic residue problem:\n\nProblem:\nGiven integers formula_1 and formula_5, where formula_6 and formula_7 are unknown, different primes, and where formula_42, determine whether formula_1 is a quadratic residue modulo formula_2 or not.\n\nIf formula_1 is drawn uniformly at random from integers formula_57 such that formula_42, is formula_1 more often a quadratic residue or a quadratic non-residue modulo formula_2?\n\nAs mentioned earlier, for exactly half of the choices of formula_61, then formula_62, and for the rest we have formula_63.\nBy extension, this also holds for half the choices of formula_64.\nSimilarly for formula_7.\nFrom basic algebra, it follows that this partitions formula_66 into 4 parts of equal size, depending on the sign of formula_32 and formula_33.\n\nThe allowed formula_1 in the quadratic residue problem given as above constitute exactly those two parts corresponding to the cases formula_70 and formula_71.\nConsequently, exactly half of the possible formula_1 are quadratic residues and the remaining are not.\n\nThe intractability of the quadratic residuosity problem is the basis for the security of the Blum Blum Shub pseudorandom number generator and the Goldwasser–Micali cryptosystem.\n\n"}
{"id": "371914", "url": "https://en.wikipedia.org/wiki?curid=371914", "title": "Real tree", "text": "Real tree\n\nIn mathematics, real trees (also called formula_1-trees) are a class of metric spaces generalising simplicial trees. They arise naturally in many mathematical contexts, in particular geometric group theory and probability theory. They are also the simplest examples of Gromov hyperbolic spaces.\n\nA metric space formula_2 is a real tree if it is a geodesic space where every triangle is a tripod. That is, for every three points formula_3 there exists a point formula_4 such that the geodesic segments formula_5 intersect in the segment formula_6 and also formula_7. This definition is equivalent to formula_2 being a \"zero-hyperbolic space\" in the sense of Gromov (all triangles are \"zero-thin\"). \nReal trees can also be characterised by a topological property. A metric space formula_2 is a real tree if for any pair of points formula_10 all (topological) embeddings formula_11 of the segment formula_12 into formula_2 such that formula_14 have the same image (which is then a geodesic segment from formula_15 to formula_16).\n\n\nReal trees often appear, in various situations, as limits of more classical metric spaces.\n\nA brownian tree is a (non-simplicial) real tree almost surely. Brownian trees arise as limits of various random processes on finite trees.\n\nAny ultralimit of a sequence of hyperbolic spaces is a real tree. In particular, the asymptotic cone of any hyperbolic space is a real tree.\n\nLet formula_25 be a group. For a sequence of based formula_25-spaces formula_27 there is a notion of convergence to a based formula_25-space formula_29 due to M. Bestvina and F. Paulin. When the spaces are hyperbolic and the actions are unbounded the limit (if it exists) is a real tree.\n\nA simple example is obtained by taking formula_30 where formula_31 is a compact surface, and formula_32 the universal cover of formula_31 with the metric formula_34 (where formula_35 is a fixed hyperbolic metric on formula_31).\n\nThis is useful to produce actions of hyperbolic groups on real trees. Such actions are analyzed using the so-called Rips machine. A case of particular interest is the study of degeneration of groups acting properly discuntinuously on a real hyperbolic space (this predates Rips', Bestvina's and Paulin's work and is due to J. Morgan and P. Shalen).\n\nIf formula_37 is a field with an ultrametric valuation then the Bruhat–Tits building of formula_38 is a real tree. It is simplicial if and only if the valuations is discrete.\n\nIf formula_39 is a totally ordered abelian group there is a natural notion of a distance with values in formula_39 (classical metric spaces correspond to formula_42). There is a notion of formula_39-tree which recovers simplicial trees (for formula_44) and real trees (for formula_42). The structure of finitely presented groups acting freely on formula_39 -trees was described. In particular, such a group acts freely on some formula_47 -tree.\n\nThe axioms for a building can be generalized to give a definition of a real building. These arise for example as asymptotic cones of higher-rank symmetric spaces or as Bruhat—Tits buildings of higher-rank groups over valued fields.\n\n"}
{"id": "32210654", "url": "https://en.wikipedia.org/wiki?curid=32210654", "title": "Reference dimension", "text": "Reference dimension\n\nA reference dimension is a dimension on an engineering drawing provided for information only. Reference dimensions are provided for a variety of reasons and are often an accumulation of other dimensions that are defined elsewhere (e.g. on the drawing or other related documentation). These dimensions may also be used for convenience to identify a single dimension that is specified elsewhere (e.g. on a different drawing sheet).\n\nReference dimensions are not intended to be used directly to define the geometry of an object. Reference dimensions do not normally govern manufacturing operations (such as machining) in any way and, therefore, do not typically include a dimensional tolerance (though a tolerance may be provided if such information is deemed helpful). Consequently, reference dimensions are also not subject to dimensional inspection under normal circumstances.\n\nPrior to use of modern Computer-Aided Design (CAD) software, reference dimensions were traditionally indicated on a drawing by the abbreviation \"REF\" written adjacent to the dimension (typically to the right or underneath the dimension). However, with the assistance of modern CAD software, the abbreviation \"REF\" has often been replaced with the use of parentheses around the dimension. As an example, a distance of 1500 millimeters might be denoted by \"\"(1500 mm)\" instead of \"1500 mm REF\".\" Although both of these methods of denoting a reference dimension are accepted and still in use today, it is increasingly common to see the use of parentheses instead of the abbreviation \"REF\" primarily due to widespread use of modern CAD software that makes use of parentheses as the default denotation method whenever reference dimensions are \"automatically\" created by the software.\n\n\n"}
{"id": "2786727", "url": "https://en.wikipedia.org/wiki?curid=2786727", "title": "Stream processing", "text": "Stream processing\n\nStream processing is a computer programming paradigm, equivalent to dataflow programming, event stream processing, and reactive programming, that allows some applications to more easily exploit a limited form of parallel processing. Such applications can use multiple computational units, such as the floating point unit on a graphics processing unit or field-programmable gate arrays (FPGAs), without explicitly managing allocation, synchronization, or communication among those units.\n\nThe stream processing paradigm simplifies parallel software and hardware by restricting the parallel computation that can be performed. Given a sequence of data (a \"stream\"), a series of operations (\"kernel functions\") is applied to each element in the stream. Kernel functions are usually pipelined, and optimal local on-chip memory reuse is attempted, in order to minimize the loss in bandwidth, accredited to external memory interaction. \"Uniform streaming\", where one kernel function is applied to all elements in the stream, is typical. Since the kernel and stream abstractions expose data dependencies, compiler tools can fully automate and optimize on-chip management tasks. Stream processing hardware can use scoreboarding, for example, to initiate a direct memory access (DMA) when dependencies become known. The elimination of manual DMA management reduces software complexity, and an associated elimination for hardware cached I/O, reduces the data area expanse that has to be involved with service by specialized computational units such as arithmetic logic units.\n\nDuring the 1980s stream processing was explored within dataflow programming. An example is the language SISAL (Streams and Iteration in a Single Assignment Language).\n\nStream processing is essentially a compromise, driven by a data-centric model that works very well for traditional DSP or GPU-type applications (such as image, video and digital signal processing) but less so for general purpose processing with more randomized data access (such as databases). By sacrificing some flexibility in the model, the implications allow easier, faster and more efficient execution. Depending on the context, processor design may be tuned for maximum efficiency or a trade-off for flexibility.\n\nStream processing is especially suitable for applications that exhibit three application characteristics:\n\nExamples of records within streams include:\n\nFor each record we can only read from the input, perform operations on it, and write to the output. It is permissible to have multiple inputs and multiple outputs, but never a piece of memory that is both readable and writable. \n\nBasic computers started from a sequential execution paradigm. Traditional CPUs are SISD based, which means they conceptually perform only one operation at a time.\nAs the computing needs of the world evolved, the amount of data to be managed increased very quickly. It was obvious that the sequential programming model could not cope with the increased need for processing power. Various efforts have been spent on finding alternative ways to perform massive amounts of computations but the only solution was to exploit some level of parallel execution.\nThe result of those efforts was SIMD, a programming paradigm which allowed applying one instruction to multiple instances of (different) data. Most of the time, SIMD was being used in a SWAR environment. By using more complicated structures, one could also have MIMD parallelism.\n\nAlthough those two paradigms were efficient, real-world implementations were plagued with limitations from memory alignment problems to synchronization issues and limited parallelism. Only few SIMD processors survived as stand-alone components; most were embedded in standard CPUs.\n\nConsider a simple program adding up two arrays containing 100 4-component vectors (i.e. 400 numbers in total).\n\nThis is the sequential paradigm that is most familiar. Variations do exist (such as inner loops, structures and such), but they ultimately boil down to that construct.\n\nThis is actually oversimplified. It assumes the instruction codice_1 works. Although this is what happens with instruction intrinsics, much information is actually not taken into account here such as the number of vector components and their data format. This is done for clarity.\n\nYou can see however, this method reduces the number of decoded instructions from \"numElements * componentsPerElement\" to \"numElements\". The number of jump instructions is also decreased, as the loop is run fewer times. These gains result from the parallel execution of the four mathematical operations.\n\nWhat happened however is that the packed SIMD register holds a certain amount of data so it's not possible to get more parallelism. The speed up is somewhat limited by the assumption we made of performing four parallel operations (please note this is common for both AltiVec and SSE).\n\nIn this paradigm, the whole dataset is defined, rather than each component block being defined separately. Describing the set of data is assumed to be in the first two rows. After that, the result is inferred from the sources and kernel. For simplicity, there's a 1:1 mapping between input and output data but this does not need to be. Applied kernels can also be much more complex.\n\nAn implementation of this paradigm can \"unroll\" a loop internally. This allows throughput to scale with chip complexity, easily utilizing hundreds of ALUs. The elimination of complex data patterns makes much of this extra power available.\n\nWhile stream processing is a branch of SIMD/MIMD processing, they must not be confused. Although SIMD implementations can often work in a \"streaming\" manner, their performance is not comparable: the model envisions a very different usage pattern which allows far greater performance by itself.\n\nIt has been noted that when applied on generic processors such as standard CPU, only a 1.5x speedup can be reached. By contrast, ad-hoc stream processors easily reach over 10x performance, mainly attributed to the more efficient memory access and higher levels of parallel processing.\n\nAlthough there are various degrees of flexibility allowed by the model, stream processors usually impose some limitations on the kernel or stream size. For example, consumer hardware often lacks the ability to perform high-precision math, lacks complex indirection chains or presents lower limits on the number of instructions which can be executed.\n\nStanford University stream processing projects included the Stanford Real-Time Programmable Shading Project started in 1999.\nA prototype called Imagine was developed in 2002.\nA project called Merrimac ran until about 2004.\nAT&T also researched stream-enhanced processors as graphics processing units rapidly evolved in both speed and functionality. Since these early days, dozens of stream processing languages have been developed, as well as specialized hardware.\n\nThe most immediate challenge in the realm of parallel processing does not lie as much in the type of hardware architecture used, but in how easy it will be to program the system in question in a real-world environment with acceptable performance. Machines like Imagine use a straightforward single-threaded model with automated dependencies, memory allocation and DMA scheduling. This in itself is a result of the research at MIT and Stanford in finding an optimal \"layering of tasks\" between programmer, tools and hardware. Programmers beat tools in mapping algorithms to parallel hardware, and tools beat programmers in figuring out smartest memory allocation schemes, etc. Of particular concern are MIMD designs such as Cell, for which the programmer needs to deal with application partitioning across multiple cores and deal with process synchronization and load balancing. Efficient multi-core programming tools are severely lacking today.\n\nA drawback of SIMD programming was the issue of Array-of-Structures (AoS) and Structure-of-Arrays (SoA). Programmers often wanted to build data structures with a 'real' meaning, for example:\n\nWhat happened is that those structures were then assembled in arrays to keep things nicely organized. This is \"array of structures\" (AoS).\nWhen the structure is laid out in memory, the compiler will produce interleaved data, in the sense that all the structures will be contiguous but there will be a constant offset between, say, the \"size\" attribute of a structure instance and the same element of the following instance. The offset depends on the structure definition (and possibly other things not considered here such as compiler's policies).\nThere are also other problems. For example, the three position variables cannot be SIMD-ized that way, because it's not sure they will be allocated in continuous memory space. To make sure SIMD operations can work on them, they shall be grouped in a 'packed memory location' or at least in an array.\nAnother problem lies in both \"color\" and \"xyz\" to be defined in three-component vector quantities. SIMD processors usually have support for 4-component operations only (with some exceptions however).\n\nThese kinds of problems and limitations made SIMD acceleration on standard CPUs quite nasty.\nThe proposed solution, \"structure of arrays\" (SoA) follows as:\n\nFor readers not experienced with C, the '*' before each identifier means a pointer. In this case, they will be used to point to the first element of an array, which is to be allocated later. For Java programmers, this is roughly equivalent to \"[]\".\nThe drawback here is that the various attributes could be spread in memory. To make sure this does not cause cache misses, we'll have to update all the various \"reds\", then all the \"greens\" and \"blues\".\n\nFor stream processors, the usage of structures is encouraged. From an application point of view, all the attributes can be defined with some flexibility.\nTaking GPUs as reference, there is a set of attributes (at least 16) available. For each attribute, the application can state the number of components and the format of the components (but only primitive data types are supported for now). The various attributes are then attached to a memory block, possibly defining a \"stride\" between 'consecutive' elements of the same attributes, effectively allowing interleaved data.\nWhen the GPU begins the stream processing, it will \"gather\" all the various attributes in a single set of parameters (usually this looks like a structure or a \"magic global variable\"), performs the operations and \"scatters\" the results to some memory area for later processing (or retrieving).\n\nMore modern stream processing frameworks provide a FIFO like interface to structure data as a literal stream. This abstraction \nprovides a means to specify data dependencies implicitly while enabling the runtime/hardware to take full advantage of that \nknowledge for efficient computation. One of the simplest and most efficient stream processing modalities to date for C++, \nis RaftLib. RaftLib enables linking independent compute kernels together as a data flow graph using C++ stream operators. \nAs an example:\nApart from specifying streaming applications in high-level language. Models of computation (MoCs) also have been widely used such as dataflow models and process-based models.\n\nHistorically, CPUs began implementing various tiers of memory access optimizations because of the ever-increasing performance when compared to relatively slow growing external memory bandwidth. As this gap widened, big amounts of die area were dedicated to hiding memory latencies. Since fetching information and opcodes to those few ALUs is expensive, very little die area is dedicated to actual mathematical machinery (as a rough estimation, consider it to be less than 10%).\n\nA similar architecture exists on stream processors but thanks to the new programming model, the amount of transistors dedicated to management is actually very little.\n\nBeginning from a whole system point of view, stream processors usually exist in a controlled environment. GPUs do exist on an add-in board (this seems to also apply to Imagine). CPUs do the dirty job of managing system resources, running applications and such.\n\nThe stream processor is usually equipped with a fast, efficient, proprietary memory bus (crossbar switches are now common, multi-buses have been employed in the past). The exact amount of memory lanes is dependent on the market range. As this is written, there are still 64-bit wide interconnections around (entry-level). Most mid-range models use a fast 128-bit crossbar switch matrix (4 or 2 segments), while high-end models deploy huge amounts of memory (actually up to 512 MB) with a slightly slower crossbar that is 256 bits wide. By contrast, standard processors from Intel Pentium to some Athlon 64 have only a single 64-bit wide data bus.\n\nMemory access patterns are much more predictable. While arrays do exist, their dimension is fixed at kernel invocation. The thing which most closely matches a multiple pointer indirection is an \"indirection chain\", which is however guaranteed to finally read or write from a specific memory area (inside a stream).\n\nBecause of the SIMD nature of the stream processor's execution units (ALUs clusters), read/write operations are expected to happen in bulk, so memories are optimized for high bandwidth rather than low latency (this is a difference from Rambus and DDR SDRAM, for example). This also allows for efficient memory bus negotiations.\n\nMost (90%) of a stream processor's work is done on-chip, requiring only 1% of the global data to be stored to memory. This is where knowing the kernel temporaries and dependencies pays.\n\nInternally, a stream processor features some clever communication and management circuits but what's interesting is the \"Stream Register File\" (SRF). This is conceptually a large cache in which stream data is stored to be transferred to external memory in bulks. As a cache-like software-controlled structure to the various ALUs, the SRF is shared between all the various ALU clusters. The key concept and innovation here done with Stanford's Imagine chip is that the compiler is able to automate and allocate memory in an optimal way, fully transparent to the programmer. The dependencies between kernel functions and data is known through the programming model which enables the compiler to perform flow analysis and optimally pack the SRFs. Commonly, this cache and DMA management can take up the majority of a project's schedule, something the stream processor (or at least Imagine) totally automates. Tests done at Stanford showed that the compiler did an as well or better job at scheduling memory than if you hand tuned the thing with much effort.\n\nThere is proof; there can be a lot of clusters because inter-cluster communication is assumed to be rare. Internally however, each cluster can efficiently exploit a much lower amount of ALUs because intra-cluster communication is common and thus needs to be highly efficient.\n\nTo keep those ALUs fetched with data, each ALU is equipped with local register files (LRFs), which are basically its usable registers.\n\nThis three-tiered data access pattern, makes it easy to keep temporary data away from slow memories, thus making the silicon implementation highly efficient and power-saving.\n\nAlthough an order of magnitude speedup can be reasonably expected (even from mainstream GPUs when computing in a streaming manner), not all applications benefit from this.\nCommunication latencies are actually the biggest problem. Although PCI Express improved this with full-duplex communications, getting a GPU (and possibly a generic stream processor) to work will possibly take long amounts of time. This means it's usually counter-productive to use them for small datasets. Because changing the kernel is a rather expensive operation the stream architecture also incurs penalties for small streams, a behaviour referred to as the \"short stream effect\".\n\nPipelining is a very widespread and heavily used practice on stream processors, with GPUs featuring pipelines exceeding 200 stages. The cost for switching settings is dependent on the setting being modified but it is now considered to always be expensive. To avoid those problems at various levels of the pipeline, many techniques have been deployed such as \"über shaders\" and \"texture atlases\". Those techniques are game-oriented because of the nature of GPUs, but the concepts are interesting for generic stream processing as well.\n\n\nMost programming languages for stream processors start with Java, C or C++ and add extensions which provide specific instructions to allow application developers to tag kernels and/or streams. This also applies to most shading languages, which can be considered stream programming languages to a certain degree.\n\nNon-commercial examples of stream programming languages include:\nCommercial implementations are either general purpose or tied to specific hardware by a vendor. Examples of general purpose languages include:\n\nVendor-specific languages include:\n\nEvent-Based Processing\n\nBatch File-Based Processing (emulates some of actual stream processing, but much lower performance in general)\n\nContinuous Operator Stream Processing\n\nStream Processing Services:\n\n\n"}
{"id": "43236771", "url": "https://en.wikipedia.org/wiki?curid=43236771", "title": "Suslin algebra", "text": "Suslin algebra\n\nIn mathematics, a Suslin algebra is a Boolean algebra that is complete, atomless, countably distributive, and satisfies the countable chain condition. They are named after Mikhail Yakovlevich Suslin.\n\nThe existence of Suslin algebras is independent of the axioms of ZFC, and is equivalent to the existence of Suslin trees or Suslin lines.\n"}
{"id": "16658636", "url": "https://en.wikipedia.org/wiki?curid=16658636", "title": "Taleb distribution", "text": "Taleb distribution\n\nIn economics and finance, a Taleb distribution is the statistical profile of an investment which normally provides a payoff of small positive returns, while carrying a small but significant risk of catastrophic losses. The term was coined by journalist Martin Wolf and economist John Kay to describe investments with a \"high probability of a modest gain and a low probability of huge losses in any period.\"\n\nThe concept is named after Nassim Nicholas Taleb, based on ideas outlined in his book \"Fooled by Randomness\".\n\nAccording to Taleb in \"Silent Risk\", the term should be called \"payoff\" to reflect the importance of the payoff function of the underlying probability distribution, rather than the distribution itself. The term is meant to refer to an investment returns profile in which there is a high probability of a small gain, and a small probability of a very large loss, which more than outweighs the gains. In these situations the expected value is very much less than zero, but this fact is camouflaged by the appearance of low risk and steady returns. It is a combination of kurtosis risk and skewness risk: overall returns are dominated by extreme events (kurtosis), which are to the downside (skew).\n\nMore detailed and formal discussion of the bets on small probability events is in the academic essay by Taleb, called \"Why Did the Crisis of 2008 Happen?\" and in the 2004 paper in the \"Journal of Behavioral Finance\" called \"Why Do We Prefer Asymmetric Payoffs?\" in which he writes \"agents risking other people’s capital would have the incentive to camouflage the properties by showing a steady income. Intuitively, hedge funds are paid on an annual basis while disasters happen every four or five years, for example. The fund manager does not repay his incentive fee.\"\n\nPursuing a trading strategy with a Taleb distribution yields a high probability of steady returns for a time, but with a risk of ruin that approaches eventual certainty over time. This is done consciously by some as a risky trading strategy, while some critics argue that it is done either unconsciously by some, unaware of the hazards (\"innocent fraud\"), or consciously by others, particularly in hedge funds.\n\nIf done consciously, with one's own capital or openly disclosed to investors, this is a risky strategy, but appeals to some: one will want to exit the trade before the rare event happens. This occurs for instance in a speculative bubble, where one purchases an asset in the expectation that it will likely go up, but may plummet, and hopes to sell the asset before the bubble bursts.\n\nThis has also been referred to as \"picking up pennies in front of a steamroller\".\n\nJohn Kay has likened securities trading to bad driving, as both are characterized by Taleb distributions. Drivers can make many small gains in time by taking risks such as overtaking on the inside and tailgating, however, they are then at risk of experiencing a very large loss in the form of a serious traffic accident. Kay has described Taleb Distributions as the basis of the carry trade and has claimed that along with mark-to-market accounting and other practices, constitute part of what John Kenneth Galbraith has called \"innocent fraud\".\n\nSome critics of the hedge fund industry claim that the compensation structure generates high fees for investment strategies that follow a Taleb distribution, creating moral hazard. In such a scenario, the fund can claim high asset management and performance fees until they suddenly \"blow up\", losing the investor significant sums of money and wiping out all the gains to the investor generated in previous periods; however, the fund manager keeps all fees earned prior to the losses being incurred – and ends up enriching himself in the long run because he does not pay for his losses.\n\nTaleb distributions pose several fundamental problems, all possibly leading to risk being overlooked:\nMore formally, while the risks for a \"known\" distribution can be calculated, in practice one does not know the distribution: one is operating under uncertainty, in economics called Knightian uncertainty.\n\nA number of mitigants have been proposed, by Taleb and others. These include:\n\n"}
{"id": "1222595", "url": "https://en.wikipedia.org/wiki?curid=1222595", "title": "Tame group", "text": "Tame group\n\nIn mathematical group theory, a tame group is a certain kind of group defined in model theory.\n\nFormally, we define a bad field as a structure of the form (\"K\", \"T\"), where \"K\" is an algebraically closed field and \"T\" is an infinite, proper, distinguished subgroup of \"K\", such that (\"K\", \"T\") is of finite Morley rank in its full language. A group \"G\" is then called a tame group if no bad field is interpretable in \"G\".\n\n"}
{"id": "21922970", "url": "https://en.wikipedia.org/wiki?curid=21922970", "title": "Teichmüller–Tukey lemma", "text": "Teichmüller–Tukey lemma\n\nIn mathematics, the Teichmüller–Tukey lemma (sometimes named just Tukey's lemma), named after John Tukey and Oswald Teichmüller, is a lemma that states that every nonempty collection of finite character has a maximal element with respect to inclusion. Over Zermelo–Fraenkel set theory, the Teichmüller–Tukey lemma is equivalent to the axiom of choice, and therefore to the well-ordering theorem, Zorn's lemma, and the Hausdorff maximal principle.\n\nA family of sets formula_1 is of finite character provided it has the following properties:\n\nLet formula_9 be a set and let formula_10. If formula_1 is of finite character and formula_12, then there is a maximal formula_13 (according to the inclusion relation) such that formula_14.\n\nIn linear algebra, the lemma may be used to show the existence of a basis. Let \"V\" be a vector space. Consider the collection formula_1 of linearly independent sets of vectors. This is a collection of finite character Thus, a maximal set exists, which must then span \"V\" and be a basis for \"V\".\n\n"}
{"id": "36265360", "url": "https://en.wikipedia.org/wiki?curid=36265360", "title": "The Universal Book of Mathematics", "text": "The Universal Book of Mathematics\n\nThe Universal Book of Mathematics: From Abracadabra to Zeno's Paradoxes (2004) is a bestselling book by British author David Darling.\n\nThe book is presented in a dictionary format. The book is divided into headwords, which, as the title suggests, run from Abracadabra to Zeno's paradoxes.\n\nThe book also provides relevant diagrams and illustrations.\n\nThe first edition of the book had several errors which were fixed in later editions. Several famous scientists have sent in corrections to the author of the book. These include Warren Johnson and Freeman Dyson.\n\nThe book has been praised by BoingBoing and British newspaper \"The Independent\".\n\nProblems and Puzzles mentioned in the book have been discussed and debated several times by several major mathematicians.\n\n\n"}
{"id": "251883", "url": "https://en.wikipedia.org/wiki?curid=251883", "title": "William Burnside", "text": "William Burnside\n\nWilliam Burnside (2 July 1852 – 21 August 1927) was an English mathematician. He is known mostly as an early researcher in the theory of finite groups.\n\nBurnside was born in London, and attended St. John's and Pembroke Colleges at the University of Cambridge, where he was the Second Wrangler in 1875. He lectured at Cambridge for the following ten years, before being appointed professor of mathematics at the Royal Naval College in Greenwich. While this was a little outside the main centres of British mathematical research, Burnside remained a very active researcher, publishing more than 150 papers in his career.\n\nBurnside's early research was in applied mathematics. This work was of sufficient distinction to merit his election as a fellow of the Royal Society in 1893, though it is little remembered today. Around the same time as his election his interests turned to the study of finite groups. This was not a widely studied subject in Britain in the late 19th century, and it took some years for his research in this area to gain widespread recognition.\n\nThe central part of Burnside's group theory work was in the area of group representations, where he helped to develop some of the foundational theory, complementing, and sometimes competing with, the work of Ferdinand Georg Frobenius, who began his research in the subject during the 1890s. One of Burnside's best known contributions to group theory is his \"pq\" theorem, which shows that every finite group whose order is divisible by fewer than three distinct primes is solvable.\n\nIn 1897 Burnside's classic work \"Theory of Groups of Finite Order\" was published. The second edition (pub. 1911) was for many decades the standard work in the field. A major difference between the editions was the inclusion of character theory in the second.\n\nBurnside is also remembered for the formulation of Burnside's problem that concerns the question of bounding the size of a group if there are fixed bounds both on the order of all of its elements and the number of elements needed to generate it, and also for Burnside's lemma (a formula relating the number of orbits of a permutation group acting on a set with the number of fixed points of each of its elements) though the latter had been discovered earlier and independently by Frobenius and Augustin Cauchy.\n\nHe received an honorary doctorate (D.Sc.) from the University of Dublin in June 1901.\n\nIn addition to his mathematical work, Burnside was a noted rower. While he was a lecturer at Cambridge, he also coached the rowing crew team. In fact, his obituary in \"The Times\" took more interest in his athletic career, calling him \"one of the best known Cambridge athletes of his day\".\n\nHe is buried at the West Wickham Parish Church in South London.\n\n\n\n\n"}
{"id": "1643733", "url": "https://en.wikipedia.org/wiki?curid=1643733", "title": "Øystein Ore", "text": "Øystein Ore\n\nØystein Ore (7 October 1899 – 13 August 1968) was a Norwegian mathematician known for his work in ring theory, Galois connections, graph theory, and the history of mathematics.\n\nOre graduated from the University of Oslo in 1922, with a Cand.Scient. degree in mathematics. In 1924, the University of Oslo awarded him the Ph.D. for a thesis titled \"Zur Theorie der algebraischen Körper\", supervised by Thoralf Skolem. Ore also studied at Göttingen University, where he learned Emmy Noether's new approach to abstract algebra. He was also a fellow at the Mittag-Leffler Institute in Sweden, and spent some time at the University of Paris. In 1925, he was appointed research assistant at the University of Oslo.\n\nYale University’s James Pierpont went to Europe in 1926 to recruit research mathematicians. In 1927, Yale hired Ore as an assistant professor of mathematics, promoted him to associate professor in 1928, then to full professor in 1929. In 1931, he became a Sterling Professor (Yale's highest academic rank), a position he held until he retired in 1968.\n\nOre gave an American Mathematical Society Colloquium lecture in 1941 and was a plenary speaker at the International Congress of Mathematicians in 1936 in Oslo. He was also elected to the American Academy of Arts and Sciences and the Oslo Academy of Science. He was a founder of the Econometric Society.\n\nOre visited Norway nearly every summer. During World War II, he was active in the \"American Relief for Norway\" and \"Free Norway\" movements. In gratitude for the services rendered to his native country during the war, he was decorated in 1947 with the Order of St. Olav.\n\nIn 1930, Ore married Gudrun Lundevall. They had two children. Ore had a passion for painting and sculpture, collected ancient maps, and spoke several languages.\n\nOre is known for his work in ring theory, Galois connections, and most of all, graph theory.\n\nHis early work was on algebraic number fields, how to decompose the ideal generated by a prime number into prime ideals. He then worked on noncommutative rings, proving his celebrated theorem on embedding a domain into a division ring. He then examined polynomial rings over skew fields, and attempted to extend his work on factorisation to non-commutative rings. The Ore condition, which (if true) allows a ring of fractions to be defined, and the Ore extension, a non-commutative analogue of rings of polynomials, are part of this work. In more elementary number theory, Ore's harmonic numbers are the numbers whose divisors have an integer harmonic mean. \n\nAs a teacher, Ore is notable for teaching mathematics to two doctoral students who would make contributions to science and mathematics, Grace Hopper, who would eventually become a United States rear admiral and computer scientist, who was a pioneer in the development of the first computers, and Marshall Hall, Jr., an American mathematician who did important research in group theory and combinatorics.\n\nIn 1930 the \"Collected Works of Richard Dedekind\" were published in three volumes, jointly edited by Ore and Emmy Noether. He then turned his attention to lattice theory becoming, together with Garrett Birkhoff, one of the two founders of American expertise in the subject. Ore's early work on lattice theory led him to the study of equivalence and closure relations, Galois connections, and finally to graph theory, which occupied him to the end of his life. He wrote two books on the subject, one on the theory of graphs and another on their applications. Within graph theory, Ore's theorem is one of several results proving that sufficiently dense graphs contain Hamiltonian cycles.\n\nOre had a lively interest in the history of mathematics, and was an unusually able author of books for laypeople, such as his biographies of Cardano and Niels Henrik Abel.\n\n\n"}
