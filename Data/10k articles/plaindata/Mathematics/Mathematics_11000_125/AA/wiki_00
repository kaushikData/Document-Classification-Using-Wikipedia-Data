{"id": "34513", "url": "https://en.wikipedia.org/wiki?curid=34513", "title": "0", "text": "0\n\n0 (zero) is both a number and the numerical digit used to represent that number in numerals. The number 0 fulfills a central role in mathematics as the additive identity of the integers, real numbers, and many other algebraic structures. As a digit, 0 is used as a placeholder in place value systems. Names for the number 0 in English include zero, nought (UK), naught (US) (), nil, or—in contexts where at least one adjacent digit distinguishes it from the letter \"O\"—oh or o (). Informal or slang terms for zero include zilch and zip.\n\"Ought\" and \"aught\" (), as well as \"cipher\", have also been used historically.\n\nThe word \"zero\" came into the English language via French \"zéro\" from Italian \"zero\", Italian contraction of Venetian \"zevero\" form of 'Italian \"zefiro\" via \"ṣafira\" or \"ṣifr\". In pre-Islamic time the word \"ṣifr\" (Arabic صفر) had the meaning \"empty\". \"Sifr\" evolved to mean zero when it was used to translate \"śūnya\" (Sanskrit: शून्य) from India. The first known English use of \"zero\" was in 1598.\n\nThe Italian mathematician Fibonacci (c. 1170–1250), who grew up in North Africa and is credited with introducing the decimal system to Europe, used the term \"zephyrum\". This became \"zefiro\" in Italian, and was then contracted to \"zero\" in Venetian. The Italian word \"\" was already in existence (meaning \"west wind\" from Latin and Greek \"zephyrus\") and may have influenced the spelling when transcribing Arabic \"ṣifr\".\n\nThere are different words used for the number or concept of zero depending on the context. For the simple notion of lacking, the words \"nothing\" and \"none\" are often used. Sometimes the words \"nought\", \"naught\" and \"aught\" are used. Several sports have specific words for zero, such as \"nil\" in association football (soccer), \"love\" in tennis and \"a duck\" in cricket. It is often called \"oh\" in the context of telephone numbers. Slang words for zero include \"zip\", \"zilch\", \"nada\", and \"scratch.\" \"Duck egg\" and \"goose egg\" are also slang for zero.\n\nAncient Egyptian numerals were base 10. They used hieroglyphs for the digits and were not positional. By 1770 BC, the Egyptians had a symbol for zero in accounting texts. The symbol nfr, meaning beautiful, was also used to indicate the base level in drawings of tombs and pyramids and distances were measured relative to the base line as being above or below this line.\n\nBy the middle of the 2nd millennium BC, the Babylonian mathematics had a sophisticated sexagesimal positional numeral system. The lack of a positional value (or zero) was indicated by a \"space\" between sexagesimal numerals. By 300 BC, a punctuation symbol (two slanted wedges) was co-opted as a placeholder in the same Babylonian system. In a tablet unearthed at Kish (dating from about 700 BC), the scribe Bêl-bân-aplu wrote his zeros with three hooks, rather than two slanted wedges.\n\nThe Babylonian placeholder was not a true zero because it was not used alone. Nor was it used at the end of a number. Thus numbers like 2 and 120 (2×60), 3 and 180 (3×60), 4 and 240 (4×60), looked the same because the larger numbers lacked a final sexagesimal placeholder. Only context could differentiate them.\n\nThe Mesoamerican Long Count calendar developed in south-central Mexico and Central America required the use of zero as a place-holder within its vigesimal (base-20) positional numeral system. Many different glyphs, including this partial quatrefoil——were used as a zero symbol for these Long Count dates, the earliest of which (on Stela 2 at Chiapa de Corzo, Chiapas) has a date of 36 BC.\n\nSince the eight earliest Long Count dates appear outside the Maya homeland, it is generally believed that the use of zero in the Americas predated the Maya and was possibly the invention of the Olmecs. Many of the earliest Long Count dates were found within the Olmec heartland, although the Olmec civilization ended by the , several centuries before the earliest known Long Count dates.\n\nAlthough zero became an integral part of Maya numerals, with a different, empty tortoise-like \"shell shape\" used for many depictions of the \"zero\" numeral, it is assumed to have not influenced Old World numeral systems.\n\nQuipu, a knotted cord device, used in the Inca Empire and its predecessor societies in the Andean region to record accounting and other digital data, is encoded in a base ten positional system. Zero is represented by the absence of a knot in the appropriate position.\n\nThe ancient Greeks had no symbol for zero (μηδέν), and did not use a digit placeholder for it. They seemed unsure about the status of zero as a number. They asked themselves, \"How can nothing \"be\" something?\", leading to philosophical and, by the medieval period, religious arguments about the nature and existence of zero and the vacuum. The paradoxes of Zeno of Elea depend in large part on the uncertain interpretation of zero.\nBy 130 AD, Ptolemy, influenced by Hipparchus and the Babylonians, was using a symbol for zero (a small circle with a long overbar) in his work on mathematical astronomy called the \"Syntaxis Mathematica\", also known as the \"Almagest\". The way in which it is used can be seen in his table of chords in that book. Ptolemy's zero was used within a sexagesimal numeral system otherwise using alphabetic Greek numerals. Because it was used alone, not just as a placeholder, this Hellenistic zero was perhaps the earliest documented use of a numeral representing zero in the Old World. However, the positions were usually limited to the fractional part of a number (called minutes, seconds, thirds, fourths, etc.)—they were not used for the integral part of a number, indicating a concept perhaps better expressed as \"none\", rather than \"zero\" in the modern sense. In later Byzantine manuscripts of Ptolemy's \"Almagest\", the Hellenistic zero had morphed into the Greek letter omicron (otherwise meaning 70).\n\nAnother zero was used in tables alongside Roman numerals by 525 (first known use by Dionysius Exiguus), but as a word, \"nulla\" meaning \"nothing\", not as a symbol. When division produced zero as a remainder, \"nihil\", also meaning \"nothing\", was used. These medieval zeros were used by all future medieval calculators of Easter. The initial \"N\" was used as a zero symbol in a table of Roman numerals by Bede or his colleagues around 725.\n\nThe \"Sūnzĭ Suànjīng\", of unknown date but estimated to be dated from the 1st to , and Japanese records dated from the 18th century, describe how the Chinese counting rods system enables one to perform decimal calculations. According to \"A History of Mathematics\", the rods \"gave the decimal representation of a number, with an empty space denoting zero.\" The counting rod system is considered a positional notation system.\n\nIn AD 690, Empress Wǔ promulgated Zetian characters, one of which was \"〇\". The symbol 0 for denoting zero is a variation of this character.\n\nZero was not treated as a number at that time, but as a \"vacant position\". Qín Jiǔsháo's 1247 \"Mathematical Treatise in Nine Sections\" is the oldest surviving Chinese mathematical text using a round symbol for zero. Chinese authors had been familiar with the idea of negative numbers by the Han Dynasty , as seen in \"The Nine Chapters on the Mathematical Art\".\n\nPingala (c. 3rd/2nd century BC), a Sanskrit prosody scholar, used binary numbers in the form of short and long syllables (the latter equal in length to two short syllables), a notation similar to Morse code. Pingala used the Sanskrit word \"śūnya\" explicitly to refer to zero.\n\nIt was considered that the earliest text to use a decimal place-value system, including a zero, is the \"Lokavibhāga\", a Jain text on cosmology surviving in a medieval Sanskrit translation of the Prakrit original, which is internally dated to AD 458 (Saka era 380). In this text, \"śūnya\" (\"void, empty\") is also used to refer to zero.\n\nA symbol for zero, a large dot likely to be the precursor of the still-current hollow symbol, is used throughout the Bakhshali manuscript, a practical manual on arithmetic for merchants. In 2017 three samples from the manuscript were shown by radiocarbon dating to come from three different centuries: from 224-383 AD, 680-779 AD, and 885-993 AD, making it the world's oldest recorded use of the zero symbol. It is not known how the birch bark fragments from different centuries that form the manuscript came to be packaged together.\n\nThe origin of the modern decimal-based place value notation can be traced to the \"Aryabhatiya\" (c. 500), which states \"sthānāt sthānaṁ daśaguṇaṁ syāt\" \"from place to place each is ten times the preceding.\" The concept of zero as a digit in the decimal place value notation was developed in India, presumably as early as during the Gupta period , with the oldest unambiguous evidence dating to the 7th century.\n\nThe rules governing the use of zero appeared for the first time in Brahmagupta's \"Brahmasputha Siddhanta\" (7th century). This work considers not only zero, but also negative numbers and the algebraic rules for the elementary operations of arithmetic with such numbers. In some instances, his rules differ from the modern standard, specifically the definition of the value of zero divided by zero as zero.\n\nThere are numerous copper plate inscriptions, with the same small \"o\" in them, some of them possibly dated to the 6th century, but their date or authenticity may be open to doubt.\n\nA stone tablet found in the ruins of a temple near Sambor on the Mekong, Kratié Province, Cambodia, includes the inscription of \"605\" in Khmer numerals (a set of numeral glyphs for the Hindu–Arabic numeral system). The number is the year of the inscription in the Saka era, corresponding to a date of AD 683.\n\nThe first known use of special glyphs for the decimal digits that includes the indubitable appearance of a symbol for the digit zero, a small circle, appears on a stone inscription found at the Chaturbhuja Temple at Gwalior in India, dated 876. Zero is also used as a placeholder in the Bakhshali manuscript, portions of which date from AD 224–383.\n\nThe Arabic-language inheritance of science was largely Greek, followed by Hindu influences. In 773, at Al-Mansur's behest, translations were made of many ancient treatises including Greek, Roman, Indian, and others.\n\nIn AD 813, astronomical tables were prepared by a Persian mathematician, Muḥammad ibn Mūsā al-Khwārizmī, using Hindu numerals; and about 825, he published a book synthesizing Greek and Hindu knowledge and also contained his own contribution to mathematics including an explanation of the use of zero. This book was later translated into Latin in the 12th century under the title \"Algoritmi de numero Indorum\". This title means \"al-Khwarizmi on the Numerals of the Indians\". The word \"Algoritmi\" was the translator's Latinization of Al-Khwarizmi's name, and the word \"Algorithm\" or \"Algorism\" started meaning any arithmetic based on decimals.\n\nMuhammad ibn Ahmad al-Khwarizmi, in 976, stated that if no number appears in the place of tens in a calculation, a little circle should be used \"to keep the rows\". This circle was called \"ṣifr\".\n\nThe Hindu–Arabic numeral system (base 10) reached Europe in the 11th century, via the Iberian Peninsula through Spanish Muslims, the Moors, together with knowledge of astronomy and instruments like the astrolabe, first imported by Gerbert of Aurillac. For this reason, the numerals came to be known in Europe as \"Arabic numerals\". The Italian mathematician Fibonacci or Leonardo of Pisa was instrumental in bringing the system into European mathematics in 1202, stating:\nAfter my father's appointment by his homeland as state official in the customs house of Bugia for the Pisan merchants who thronged to it, he took charge; and in view of its future usefulness and convenience, had me in my boyhood come to him and there wanted me to devote myself to and be instructed in the study of calculation for some days. There, following my introduction, as a consequence of marvelous instruction in the art, to the nine digits of the Hindus, the knowledge of the art very much appealed to me before all others, and for it I realized that all its aspects were studied in Egypt, Syria, Greece, Sicily, and Provence, with their varying methods; and at these places thereafter, while on business. I pursued my study in depth and learned the give-and-take of disputation. But all this even, and the algorism, as well as the art of Pythagoras, I considered as almost a mistake in respect to the method of the Hindus (Modus Indorum). Therefore, embracing more stringently that method of the Hindus, and taking stricter pains in its study, while adding certain things from my own understanding and inserting also certain things from the niceties of Euclid's geometric art. I have striven to compose this book in its entirety as understandably as I could, dividing it into fifteen chapters. Almost everything which I have introduced I have displayed with exact proof, in order that those further seeking this knowledge, with its pre-eminent method, might be instructed, and further, in order that the Latin people might not be discovered to be without it, as they have been up to now. If I have perchance omitted anything more or less proper or necessary, I beg indulgence, since there is no one who is blameless and utterly provident in all things. The nine Indian figures are: 9 8 7 6 5 4 3 2 1. With these nine figures, and with the sign 0  ... any number may be written.\nHere Leonardo of Pisa uses the phrase \"sign 0\", indicating it is like a sign to do operations like addition or multiplication. From the 13th century, manuals on calculation (adding, multiplying, extracting roots, etc.) became common in Europe where they were called \"algorismus\" after the Persian mathematician al-Khwārizmī. The most popular was written by Johannes de Sacrobosco, about 1235 and was one of the earliest scientific books to be \"printed\" in 1488. Until the late 15th century, Hindu–Arabic numerals seem to have predominated among mathematicians, while merchants preferred to use the Roman numerals. In the 16th century, they became commonly used in Europe.\n\n0 is the integer immediately preceding 1. Zero is an even number because it is divisible by 2 with no remainder. 0 is neither positive nor negative. By most definitions 0 is a natural number, and then the only natural number not to be positive. Zero is a number which quantifies a count or an amount of null size. In most cultures, 0 was identified before the idea of negative things, or quantities less than zero, was accepted.\n\nThe value, or \"number\", zero is not the same as the \"digit\" zero, used in numeral systems using positional notation. Successive positions of digits have higher weights, so inside a numeral the digit zero is used to skip a position and give appropriate weights to the preceding and following digits. A zero digit is not always necessary in a positional number system, for example, in the number 02. In some instances, a leading zero may be used to distinguish a number.\n\nThe number 0 is the smallest non-negative integer. The natural number following 0 is 1 and no natural number precedes 0. The number 0 may or may not be considered a natural number, but it is an integer, and hence a rational number and a real number (as well as an algebraic number and a complex number).\n\nThe number 0 is neither positive nor negative and is usually displayed as the central number in a number line. It is neither a prime number nor a composite number. It cannot be prime because it has an infinite number of factors, and cannot be composite because it cannot be expressed as a product of prime numbers (0 must always be one of the factors). Zero is, however, even (as well as being a multiple of any other integer, rational, or real number).\n\nThe following are some basic (elementary) rules for dealing with the number 0. These rules apply for any real or complex number \"x\", unless otherwise stated.\n\nThe expression , which may be obtained in an attempt to determine the limit of an expression of the form as a result of applying the lim operator independently to both operands of the fraction, is a so-called \"indeterminate form\". That does not simply mean that the limit sought is necessarily undefined; rather, it means that the limit of , if it exists, must be found by another method, such as l'Hôpital's rule.\n\nThe sum of 0 numbers (the \"empty sum\") is 0, and the product of 0 numbers (the \"empty product\") is 1. The factorial 0! evaluates to 1, as a special case of the empty product.\n\n\n\nThe value zero plays a special role for many physical quantities. For some quantities, the zero level is naturally distinguished from all other levels, whereas for others it is more or less arbitrarily chosen. For example, for an absolute temperature (as measured in kelvins) zero is the lowest possible value (negative temperatures are defined, but negative-temperature systems are not actually colder). This is in contrast to for example temperatures on the Celsius scale, where zero is arbitrarily defined to be at the freezing point of water. Measuring sound intensity in decibels or phons, the zero level is arbitrarily set at a reference value—for example, at a value for the threshold of hearing. In physics, the zero-point energy is the lowest possible energy that a quantum mechanical physical system may possess and is the energy of the ground state of the system.\n\nZero has been proposed as the atomic number of the theoretical element tetraneutron. It has been shown that a cluster of four neutrons may be stable enough to be considered an atom in its own right. This would create an element with no protons and no charge on its nucleus.\n\nAs early as 1926, Andreas von Antropoff coined the term neutronium for a conjectured form of matter made up of neutrons with no protons, which he placed as the chemical element of atomic number zero at the head of his new version of the periodic table. It was subsequently placed as a noble gas in the middle of several spiral representations of the periodic system for classifying the chemical elements.\n\nThe most common practice throughout human history has been to start counting at one, and this is the practice in early classic computer science programming languages such as Fortran and COBOL. However, in the late 1950s LISP introduced zero-based numbering for arrays while Algol 58 introduced completely flexible basing for array subscripts (allowing any positive, negative, or zero integer as base for array subscripts), and most subsequent programming languages adopted one or other of these positions. For example, the elements of an array are numbered starting from 0 in C, so that for an array of \"n\" items the sequence of array indices runs from 0 to . This permits an array element's location to be calculated by adding the index directly to address of the array, whereas 1-based languages precalculate the array's base address to be the position one element before the first.\n\nThere can be confusion between 0- and 1-based indexing, for example Java's JDBC indexes parameters from 1 although Java itself uses 0-based indexing.\n\nIn databases, it is possible for a field not to have a value. It is then said to have a null value. For numeric fields it is not the value zero. For text fields this is not blank nor the empty string. The presence of null values leads to three-valued logic. No longer is a condition either \"true\" or \"false\", but it can be \"undetermined\". Any computation including a null value delivers a null result.\n\nA null pointer is a pointer in a computer program that does not point to any object or function. In C, the integer constant 0 is converted into the null pointer at compile time when it appears in a pointer context, and so 0 is a standard way to refer to the null pointer in code. However, the internal representation of the null pointer may be any bit pattern (possibly different values for different data types).\n\nIn mathematics both −0 and +0 represent exactly the same number, i.e., there is no \"positive zero\" or \"negative zero\" distinct from zero. However, in some computer hardware signed number representations, zero has two distinct representations, a positive one grouped with the positive numbers and a negative one grouped with the negatives; this kind of dual representation is known as signed zero, with the latter form sometimes called negative zero. These representations include the signed magnitude and one's complement binary integer representations (but not the two's complement binary form used in most modern computers), and most floating point number representations (such as IEEE 754 and IBM S/390 floating point formats).\n\nIn binary, 0 represents the value for \"off\", which means no electricity flow.\n\nZero is the value of false in many programming languages.\n\nThe Unix epoch (the date and time associated with a zero timestamp) begins the midnight before the first of January 1970.\n\nThe MacOS epoch and Palm OS epoch (the date and time associated with a zero timestamp) begins the midnight before the first of January 1904.\n\nMany APIs and operating systems that require applications to return an integer value as an exit status typically use zero to indicate success and non-zero values to indicate specific error or warning conditions.\n\n\nThe modern numerical digit 0 is usually written as a circle or ellipse. Traditionally, many print typefaces made the capital letter O more rounded than the narrower, elliptical digit 0. Typewriters originally made no distinction in shape between O and 0; some models did not even have a separate key for the digit 0. The distinction came into prominence on modern character displays.\n\nA slashed zero can be used to distinguish the number from the letter. The digit 0 with a dot in the center seems to have originated as an option on IBM 3270 displays and has continued with some modern computer typefaces such as Andalé Mono, and in some airline reservation systems. One variation uses a short vertical bar instead of the dot. Some fonts designed for use with computers made one of the capital-O–digit-0 pair more rounded and the other more angular (closer to a rectangle). A further distinction is made in falsification-hindering typeface as used on German car number plates by slitting open the digit 0 on the upper right side. Sometimes the digit 0 is used either exclusively, or not at all, to avoid confusion altogether.\n\nIn the BC calendar era, the year 1 BC is the first year before AD 1; there is not a year zero. By contrast, in astronomical year numbering, the year 1 BC is numbered 0, the year 2 BC is numbered −1, and so on.\n\n\n"}
{"id": "1042859", "url": "https://en.wikipedia.org/wiki?curid=1042859", "title": "131 (number)", "text": "131 (number)\n\n131 (one hundred [and] thirty-one) is the natural number following 130 and preceding 132.\n\n131 is a Sophie Germain prime, an irregular prime, the second 3-digit palindromic prime, and also a permutable prime with 113 and 311. It can be expressed as the sum of three consecutive primes, 131 = 41 + 43 + 47. 131 is an Eisenstein prime with no imaginary part and real part of the form formula_1. Because the next odd number, 133, is a semiprime, 131 is a Chen prime. 131 is an Ulam number.\n\n131 is a full reptend prime in base 10 (and also in base 2). The decimal expansion of 1/131 repeats the digits 007633587786259541984732824427480916030534351145038167938931 297709923664122137404580152671755725190839694656488549618320 6106870229 indefinitely.\n\n\n\n131 is also:\n\n"}
{"id": "36545176", "url": "https://en.wikipedia.org/wiki?curid=36545176", "title": "Alcuin's sequence", "text": "Alcuin's sequence\n\nIn mathematics, Alcuin's sequence, named after Alcuin of York, is the sequence of coefficients of the power-series expansion of:\n\nThe sequence begins with these integers:\n\nThe \"n\"th term is the number of triangles with integer sides and perimeter \"n\". It is also the number of triangles with \"distinct\" integer sides and perimeter \"n\" + 6, i.e. number of triples (\"a\", \"b\", \"c\") such that 1 ≤ \"a\" < \"b\" < \"c\" < \"a\" + \"b\", \"a\" + \"b\" + \"c\" = \"n\" + 6.\n\nIf one deletes the three leading zeros, then it is the number of ways in which \"n\" empty casks, \"n\" casks half-full of wine and \"n\" full casks can be distributed to three persons in such a way that each one gets the same number of casks and the same amount of wine. This is the generalization of problem 12 appearing in Propositiones ad Acuendos Juvenes (\"Problems to Sharpen the Young\") usually attributed to Alcuin. That problem is given as,\n\nThe term \"Alcuin's sequence\" may be traced back to D. Olivastro's 1993 book on mathematical games, \"Ancient Puzzle: Classical Brainteasers and Other Timeless Mathematical Games of the Last 10 Centuries\" (Bantam, New York).\n\nThe sequence with the three leading zeros deleted is obtained as the sequence of coefficients of the power-series expansion of\nThis sequence has also been called Alcuin's sequence by some authors.\n"}
{"id": "1751", "url": "https://en.wikipedia.org/wiki?curid=1751", "title": "Alexander Anderson (mathematician)", "text": "Alexander Anderson (mathematician)\n\nAlexander Anderson ( in Aberdeen – in Paris) was a Scottish mathematician.\n\nHe was born in Aberdeen, possibly in 1582, according to a print which suggests he was aged 35 in 1617. It is unknown where he was educated, but it is likely that he initially studied writing and philosophy (the \"belles lettres\") in his home city of Aberdeen.\n\nHe then went to the continent, and was a professor of mathematics in Paris by the start of the seventeenth century. There he published or edited, between the years 1612 and 1619, various geometric and algebraic tracts. He described himself as having \"more wisdom than riches\" in the dedication of \"Vindiciae Archimedis\" (1616).\n\nHe was first cousin of David Anderson of Finshaugh, a celebrated mathematician, and David Anderson's daughter was the mother of mathematician James Gregory.\n\nHe was selected by the executors of François Viète to revise and edit Viète's manuscript works. Viète died in 1603, and it is unclear if Anderson knew him, but his eminence was sufficient to attract the attention of the dead man's executors. Anderson corrected and expanded upon Viète's manuscripts, which extended known geometry to the new algebra, which used general symbols to represent quantities.\n\nThe known works of Anderson amount to six thin quarto volumes, and as the last of them was published in 1619, it is probable that the author died soon after that year, but the precise date is unknown. He wrote other works that have since been lost. From his last work it appears he wrote another piece, \"A Treatise on the Mensuration of Solids,\" and copies of two other works, \"Ex. Math.\" and \"Stereometria Triangulorum Sphæricorum\", were in the possession of Sir Alexander Hume until the after the middle of the seventeenth century.\n\n\n\nAttribution:\n"}
{"id": "22474664", "url": "https://en.wikipedia.org/wiki?curid=22474664", "title": "Algorithmic Lovász local lemma", "text": "Algorithmic Lovász local lemma\n\nIn theoretical computer science, the algorithmic Lovász local lemma gives an algorithmic way of constructing objects that obey a system of constraints with limited dependence.\n\nGiven a finite set of \"bad\" events {\"A\", ..., \"A\"} in a probability space with limited dependence amongst the \"A\"s and with specific bounds on their respective probabilities, the Lovász local lemma proves that with non-zero probability all of these events can be avoided. However, the lemma is non-constructive in that it does not provide any insight on \"how\" to avoid the bad events.\n\nIf the events {\"A\", ..., \"A\"} are determined by a finite collection of mutually independent random variables, a simple Las Vegas algorithm with expected polynomial runtime proposed by Robin Moser and Gábor Tardos can compute an assignment to the random variables such that all events are avoided.\n\nThe Lovász Local Lemma is a powerful tool commonly used in the probabilistic method to prove the existence of certain complex mathematical objects with a set of prescribed features. A typical proof proceeds by operating on the complex object in a random manner and uses the Lovász Local Lemma to bound the probability that any of the features is missing. The absence of a feature is considered a \"bad event\" and if it can be shown that all such bad events can be avoided simultaneously with non-zero probability, the existence follows. The lemma itself reads as follows:\n\nLet formula_1 be a finite set of events in the probability space Ω. For formula_2 let formula_3 denote a subset of formula_4 such that formula_5 is independent from the collection of events formula_6. If there exists an assignment of reals formula_7 to the events such that\n\nthen the probability of avoiding all events in formula_9 is positive, in particular\n\nThe Lovász Local Lemma is non-constructive because it only allows us to conclude the existence of structural properties or complex objects but does not indicate how these can be found or constructed efficiently in practice. Note that random sampling from the probability space Ω is likely to be inefficient, since the probability of the event of interest\n\nis only bounded by a product of small numbers\n\nand therefore likely to be very small.\n\nUnder the assumption that all of the events in formula_9 are determined by a finite collection of mutually independent random variables formula_14 in Ω, Robin Moser and Gábor Tardos proposed an efficient randomized algorithm that computes an assignment to the random variables in formula_14 such that all events in formula_9 are avoided.\n\nHence, this algorithm can be used to efficiently construct witnesses of complex objects with prescribed features for most problems to which the Lovász Local Lemma applies.\n\nPrior to the recent work of Moser and Tardos, earlier work had also made progress in developing algorithmic versions of the Lovász Local Lemma. József Beck in 1991 first gave proof that an algorithmic version was possible. In this breakthrough result, a stricter requirement was imposed upon the problem formulation than in the original non-constructive definition. Beck's approach required that for each formula_17, the number of dependencies of \"A\" was bounded above with formula_18 (approximately). The existential version of the Local Lemma permits a larger upper bound on dependencies:\n\nThis bound is known to be tight. Since the initial algorithm, work has been done to push algorithmic versions of the Local Lemma closer to this tight value. Moser and Tardos's recent work are the most recent in this chain, and provide an algorithm that achieves this tight bound.\n\nLet us first introduce some concepts that are used in the algorithm.\n\nFor any random variable formula_20 denotes the current assignment (evaluation) of \"P\". An assignment (evaluation) to all random variables is denoted formula_21.\n\nThe unique minimal subset of random variables in formula_14 that determine the event \"A\" is denoted by vbl(\"A\").\n\nIf the event \"A\" is true under an evaluation formula_21, we say that formula_21 satisfies \"A\", otherwise it avoids \"A\".\n\nGiven a set of bad events formula_9 we wish to avoid that is determined by a collection of mutually independent random variables formula_14, the algorithm proceeds as follows:\n\n\nIn the first step, the algorithm randomly initializes the current assignment \"v\" for each random variable formula_35. This means that an assignment \"v\" is sampled randomly and independently according to the distribution of the random variable \"P\".\n\nThe algorithm then enters the main loop which is executed until all events in formula_9 are avoided and which point the algorithm returns the current assignment. At each iteration of the main loop, the algorithm picks an arbitrary satisfied event \"A\" (either randomly or deterministically) and resamples all the random variables that determine \"A\".\n\nLet formula_14 be a finite set of mutually independent random variables in the probability space Ω. Let formula_9 be a finite set of events determined by these variables. If there exists an assignment of reals formula_39 to the events such that\n\nthen there exists an assignment of values to the variables formula_41 avoiding all of the events in formula_9.\n\nMoreover, the randomized algorithm described above resamples an event formula_2 at most an expected\n\ntimes before it finds such an evaluation. Thus the expected total number of resampling steps and therefore the expected runtime of the algorithm is at most\n\nThe proof of this theorem using the method of entropy compression can be found in the paper by Moser and Tardos \n\nThe requirement of an assignment function \"x\" satisfying a set of inequalities in the theorem above is complex and not intuitive. But this requirement can be replaced by three simple conditions:\n\nThe version of the Lovász Local Lemma with these three conditions instead of the assignment function \"x\" is called the \"Symmetric Lovász Local Lemma\". We can also state the \"Symmetric Algorithmic Lovász Local Lemma\":\n\nLet formula_41 be a finite set of mutually independent random variables and formula_4 be a finite set of events determined by these variables as before. If the above three conditions hold then there exists an assignment of values to the variables formula_41 avoiding all of the events in formula_52.\n\nMoreover, the randomized algorithm described above resamples an event formula_2 at most an expected formula_54 times before it finds such an evaluation. Thus the expected total number of resampling steps and therefore the expected runtime of the algorithm is at most formula_55.\n\nThe following example illustrates how the algorithmic version of the Lovász Local Lemma can be applied to a simple problem.\n\nLet Φ be a CNF formula over variables \"X\", ..., \"X\", containing \"n\" clauses, and with at least \"k\" literals in each clause, and with each variable \"X\" appearing in at most formula_56 clauses. Then, Φ is satisfiable.\n\nThis statement can be proven easily using the symmetric version of the Algorithmic Lovász Local Lemma. Let \"X\", ..., \"X\" be the set of mutually independent random variables formula_14 which are sampled uniformly at random.\n\nFirstly, we truncate each clause in Φ to contain exactly \"k\" literals. Since each clause is a disjunction, this does not harm satisfiability, for if we can find a satisfying assignment for the truncated formula, it can easily be extended to a satisfying assignment for the original formula by reinserting the truncated literals.\n\nNow, define a bad event \"A\" for each clause in Φ, where \"A\" is the event that clause \"j\" in Φ is unsatisfied by the current assignment. Since each clause contains \"k\" literals (and therefore \"k\" variables) and since all variables are sampled uniformly at random, we can bound the probability of each bad event by\n\nSince each variable can appear in at most formula_59 clauses and there are \"k\" variables in each clause, each bad event \"A\" can depend on at most\n\nother events. Therefore:\n\nmultiplying both sides by \"ep\" we get:\n\nit follows by the symmetric Lovász Local Lemma that the probability of a random assignment to \"X\", ..., \"X\" satisfying all clauses in Φ is non-zero and hence such an assignment must exist.\n\nNow, the Algorithmic Lovász Local Lemma actually allows us to efficiently compute such an assignment by applying the algorithm described above. The algorithm proceeds as follows:\n\nIt starts with a random truth value assignment to the variables \"X\", ..., \"X\" sampled uniformly at random. While there exists a clause in Φ that is unsatisfied, it randomly picks an unsatisfied clause \"C\" in Φ and assigns a new truth value to all variables that appear in \"C\" chosen uniformly at random. Once all clauses in Φ are satisfied, the algorithm returns the current assignment. Hence, the Algorithmic Lovász Local Lemma proves that this algorithm has an expected runtime of at most\n\nsteps on CNF formulas that satisfy the two conditions above. A stronger version of the above statement is proven by Moser, see also Berman, Karpinski and Scott.\n\nThe algorithm is similar to WalkSAT which is used to solve general boolean satisfiability problems. The main difference is that in WalkSAT, after the unsatisfied clause \"C\" is selected, a \"single\" variable in \"C\" is selected at random and has its value flipped (which can be viewed as selecting uniformly among only formula_64 rather than all formula_65 value assignments to \"C\").\n\nAs mentioned before, the Algorithmic Version of the Lovász Local Lemma applies to most problems for which the general Lovász Local Lemma is used as a proof technique. Some of these problems are discussed in the following articles:\n\n\nThe algorithm described above lends itself well to parallelization, since resampling two independent events formula_66, i.e. formula_67, in parallel is equivalent to resampling \"A\", \"B\" sequentially. Hence, at each iteration of the main loop one can determine the maximal set of independent and satisfied events \"S\" and resample all events in \"S\" in parallel.\n\nUnder the assumption that the assignment function \"x\" satisfies the slightly stronger conditions:\n\nfor some ε > 0 Moser and Tardos proved that the parallel algorithm achieves a better runtime complexity. In this case, the parallel version of the algorithm takes an expected\n\nsteps before it terminates. The parallel version of the algorithm can be seen as a special case of the sequential algorithm shown above, and so this result also holds for the sequential case.\n"}
{"id": "56315819", "url": "https://en.wikipedia.org/wiki?curid=56315819", "title": "Arithmetic billiards", "text": "Arithmetic billiards\n\nIn recreational mathematics, arithmetic billiards provide a geometrical method to determine the least common multiple and the greatest common divisor of two natural numbers by making use of reflections inside a rectangle whose sides are the two given numbers. This is an easy example of trajectory analysis of dynamical billiards. \nArithmetic billiards have been discussed as mathematical puzzles by Hugo Steinhaus. and Martin Gardner, and are known to mathematics teachers under the name 'Paper Pool'.\nThey have been used as a source of questions in mathematical circles. \n\nConsider a rectangle with integer sides, and construct a path inside this rectangle as follows: \n\n\nIf one side length divides the other, the path is a zigzag consisting of one or more segments.\nElse, the path has self-intersections and consists of segments of various lengths in two orthogonal directions.\nIn general, the path is the intersection of the rectangle with a grid of squares (oriented at 45° w.r.t. the rectangle sides).\n\nCall formula_1 and formula_2 the side lengths of the rectangle, and divide this into formula_3 unit squares. The \"least common multiple\" formula_4 is the number of unit squares crossed by the arithmetic billiard path or, equivalently, the length of the path divided by formula_5. In particular, the path goes through each unit square if and only if formula_1 and formula_2 are coprime.\n\nSuppose that none of the two side lengths divides the other. Then the first segment of the arithmetic billiard path contains the point of self-intersection which is closest to the starting point. The \"greatest common divisor\" formula_8 is the number of unit squares crossed by the first segment of the path up to that point of self-intersection.\n\nThe \"number of bouncing points\" for the arithmetic billiard path on the two sides of length formula_1 equals formula_10, and similarly formula_11 for the two sides of length formula_2. In particular, if formula_1 and formula_2 are coprime, then the total number of contact points between the path and the perimeter of the rectangle (i.e. the bouncing points plus starting and ending corner) equals formula_15.\n\nThe \"ending corner\" of the path is opposite to the starting corner if and only if formula_1 and formula_2 are exactly divisible by the same power of two (for example, if they are both odd), else it is one of the two adjacent corners, according to whether formula_1 or formula_2 has more factors formula_20 in its prime factorisation.\n\nThe path is \"symmetric\": if the starting and the ending corner are opposite, then the path is pointsymmetric w.r.t. the center of the rectangle, else it is symmetric with respect to the bisector of the side connecting the starting and the ending corner.\n\nThe contact points between the arithmetic billiard path and the rectangle perimeter are evenly distributed: the distance along the perimeter (i.e. possibly going around the corner) between two such neighbouring points equals formula_21. \n\nSet coordinates in the rectangle such that the starting point is formula_22 and the opposite corner is formula_23. Then any point on the arithmetic billiard path which has integer coordinates has the property that the sum of the coordinates is even (the parity cannot change by moving along diagonals of unit squares). The points of self-intersection of the path, the bouncing points, and the starting and ending corner are exactly the points in the rectangle whose coordinates are multiples of formula_8 and such that the sum of the coordinates is an even multiple of formula_8.\n\nReflecting the billiard: Consider a square with side formula_4. By displaying multiple copies of the original rectangle (with mirror symmetry) we can visualise the arithmetic billiard path as a diagonal of that square. In other words, we can think of reflecting the rectangle rather than the path segments.\n\nReducing to the coprime case: It is convenient to rescale the rectangle dividing formula_1 and formula_2 by their greatest common divisor, operation which does not alter the geometry of the path (e.g. the number of bouncing points).\n\nReversing the time: The motion of the path is “time reversible”, meaning that if the path is currently traversing one particular unit square\n(in a particular direction), then there is absolutely no doubt from which unit square and from which direction it just came\n\nIf we allow the starting point of the path to be any point in the rectangle with integer coordinates, then there are also periodic paths unless the rectangle sides are coprime. The length of any periodic path equals formula_29.\n"}
{"id": "21442530", "url": "https://en.wikipedia.org/wiki?curid=21442530", "title": "Barlow's formula", "text": "Barlow's formula\n\nBarlow's formula relates the internal pressure that a pipe can withstand to its dimensions and the strength of its material.\n\nwhere\n\nThis formula figures prominently in the design of autoclaves and other pressure vessels.\n\nThe formula is named after Peter Barlow, an English mathematician.\n\nThe design of a complex pressure containment system involves much more than the application of Barlow's formula. For pressure vessels, design codes such as the ASME code stipulate the requirements for design and testing.\nThe formula is also common in the pipeline industry to verify that pipe used for gathering, transmission, and distribution lines can safely withstand operating pressures. The design factor is multiplied by the resulting pressure which gives the maximum operating pressure (MAOP) for the pipeline. This design factor is dependent on class locations which are defined in DOT Part 192. There are four class locations corresponding to four design factors:\n\nhttp://www.ecfr.gov/cgi-bin/text-idx?c=ecfr&tpl=/ecfrbrowse/Title49/49cfr192_main_02.tpl\n"}
{"id": "33868", "url": "https://en.wikipedia.org/wiki?curid=33868", "title": "Casorati–Weierstrass theorem", "text": "Casorati–Weierstrass theorem\n\nIn complex analysis, a branch of mathematics, the Casorati–Weierstrass theorem describes the behaviour of holomorphic functions near their essential singularities. It is named for Karl Theodor Wilhelm Weierstrass and Felice Casorati. In Russian literature it is called Sokhotski's theorem.\n\nStart with some open subset formula_1 in the complex plane containing the number formula_2, and a function formula_3 that is holomorphic on formula_4, but has an essential singularity at formula_2 . The \"Casorati–Weierstrass theorem\" then states that \n\nThis can also be stated as follows: \n\nOr in still more descriptive terms:\n\nThe theorem is considerably strengthened by Picard's great theorem, which states, in the notation above, that formula_3 assumes \"every\" complex value, with one possible exception, infinitely often on formula_6.\n\nIn the case that formula_3 is an entire function and formula_22, the theorem says that the values formula_23\napproach every complex number and formula_24, as formula_13 tends to infinity.\nIt is remarkable that this does not hold for holomorphic maps in higher dimensions,\nas the famous example of Pierre Fatou shows.\n\nThe function \"f\"(\"z\") = exp(1/\"z\") has an essential singularity at 0, but the function \"g\"(\"z\") = 1/\"z\" does not (it has a pole at 0).\n\nConsider the function\n\nThis function has the following Taylor series about the essential singular point at 0:\n\nBecause formula_28 exists for all points \"z\" ≠ 0 we know that \"ƒ\"(\"z\") is analytic in a punctured neighborhood of \"z\" = 0. Hence it is an isolated singularity, as well as being an essential singularity. \n\nUsing a change of variable to polar coordinates formula_29 our function, \"ƒ\"(\"z\") = \"e\" becomes:\n\nTaking the absolute value of both sides:\n\nThus, for values of \"θ\" such that cos \"θ\" > 0, we have formula_32 as formula_33, and for formula_34, formula_35 as formula_33.\n\nConsider what happens, for example when \"z\" takes values on a circle of diameter 1/\"R\" tangent to the imaginary axis. This circle is given by \"r\" = (1/\"R\") cos \"θ\". Then,\n\nand\n\nThus,formula_39 may take any positive value other than zero by the appropriate choice of \"R\". As formula_40 on the circle, formula_41 with \"R\" fixed. So this part of the equation:\n\ntakes on all values on the unit circle infinitely often. Hence \"f\"(\"z\") takes on the value of every number in the complex plane except for zero infinitely often.\n\nA short proof of the theorem is as follows:\n\nTake as given that function \"f\" is meromorphic on some punctured neighborhood \"V\" \\ {\"z\"}, and that \"z\" is an essential singularity. Assume by way of contradiction that some value \"b\" exists that the function can never get close to; that is: assume that there is some complex value \"b\" and some ε > 0 such that |\"f\"(\"z\") − \"b\"| ≥ ε for all \"z\" in \"V\" at which \"f\" is defined.\n\nThen the new function:\n\nmust be holomorphic on \"V\" \\ {\"z\"}, with zeroes at the poles of \"f\", and bounded by 1/ε. It can therefore be analytically continued (or continuously extended, or holomorphically extended) to \"all\" of \"V\" by Riemann's analytic continuation theorem. So the original function can be expressed in terms of \"g\":\n\nfor all arguments \"z\" in \"V\" \\ {\"z\"}. Consider the two possible cases for\n\nIf the limit is 0, then \"f\" has a pole at \"z\" . If the limit is not 0, then \"z\" is a removable singularity of \"f\" . Both possibilities contradict the assumption that the point \"z\" is an essential singularity of the function \"f\" . Hence the assumption is false and the theorem holds.\n\nThe history of this important theorem is described by\nCollingwood and Lohwater.\nIt was published by Weierstrass in 1876 (in German) and by Sokhotski in 1868 in his Master thesis (in Russian).\nSo it was called Sokhotski's theorem in the Russian literature and Weierstrass's theorem in\nthe Western literature. \nThe same theorem was published by Casorati in 1868, and\nby Briot and Bouquet in the \"first edition\" of their book (1859).\nHowever, Briot and Bouquet \"removed\" this theorem from the second edition (1875).\n\n"}
{"id": "22586586", "url": "https://en.wikipedia.org/wiki?curid=22586586", "title": "Continuous-repayment mortgage", "text": "Continuous-repayment mortgage\n\nAnalogous to continuous compounding, a continuous annuity is an ordinary annuity in which the payment interval is narrowed indefinitely. A (theoretical) continuous repayment mortgage is a mortgage loan paid by means of a continuous annuity.\n\nMortgages (i.e., mortgage loans) are generally settled over a period of years by a series of fixed regular payments commonly referred to as an annuity. Each payment accumulates compound interest from time of deposit to the end of the mortgage timespan at which point the sum of the payments with their accumulated interest equals the value of the loan with interest compounded over the entire timespan. Given loan \"P\", per period interest rate i, number of periods \"n\" and fixed per period payment \"x\", the end of term balancing equation is:\n\nSummation can be computed using the standard formula for summation of a geometric sequence.\n\nIn a (theoretical) continuous-repayment mortgage the payment interval is narrowed indefinitely until the discrete interval process becomes continuous and the fixed interval payments become—in effect—a literal cash \"flow\" at a fixed annual rate. In this case, given loan \"P\", annual interest rate \"r\", loan timespan \"T\" (years) and annual rate \"M\", the infinitesimal cash flow elements \"M\"\"δt\" accumulate continuously compounded interest from time t to the end of the loan timespan at which point the balancing equation is:\n\nSummation of the cash flow elements and accumulated interest is effected by integration as shown. It is assumed that compounding interval and payment interval are equal—i.e., compounding of interest always occurs at the same time as payment is deducted.\n\nWithin the timespan of the loan the time continuous mortgage balance function obeys a first order linear differential equation (LDE) and an alternative derivation thereof may be obtained by solving the LDE using the method of Laplace transforms.\n\nApplication of the equation yields a number of results relevant to the financial process which it describes. Although this article focuses primarily on mortgages, the methods employed are relevant to any situation in which payment or saving is effected by a regular stream of fixed interval payments (annuity).\n\nThe classical formula for the present value of a series of \"n\" fixed monthly payments amount \"x\" invested at a monthly interest rate \"i\"% is:\n\nThe formula may be re-arranged to determine the monthly payment \"x\" on a loan of amount \"P\" taken out for a period of \"n\" months at a monthly interest rate of \"i\"%:\n\nWe begin with a small adjustment of the formula: replace \"i\" with \"r\"/\"N\" where \"r\" is the annual interest rate and \"N\" is the annual frequency of compounding periods (\"N\" = 12 for monthly payments). Also replace \"n\" with \"NT\" where \"T\" is the total loan period in years. In this more general form of the equation we are calculating \"x\"(\"N\") as the fixed payment corresponding to frequency \"N\". For example if \"N\" = 365, \"x\" corresponds to a daily fixed payment. As \"N\" increases, \"x\"(\"N\") decreases but the product \"N\"·\"x\"(\"N\") approaches a limiting value as will be shown:\n\nNote that \"N\"·\"x\"(\"N\") is simply the amount paid per year – in effect an annual repayment rate \"M\".\n\nIt is well established that:\n\nApplying the same principle to the formula for annual repayment, we can determine a limiting value:\n\nAt this point in the orthodox formula for present value, the latter is more properly represented as a function of annual compounding frequency \"N\" and time \"t\":\n\nApplying the limiting expression developed above we may write present value as a purely time dependent function:\n\nNoting that the balance due \"P\"(\"t\") on a loan \"t\" years after its inception is simply the present value of the contributions for the remaining period (i.e. \"T\" − \"t\"), we determine:\n\nThe graph(s) in the diagram are a comparison of balance due on a mortgage (1 million for 20 years @ \"r\" = 10%) calculated firstly according to the above time continuous model and secondly using the Excel PV function. As may be seen the curves are virtually indistinguishable – calculations effected using the model differ from those effected using the Excel PV function by a mere 0.3% (max). The data from which the graph(s) were derived can be viewed \n\nDefine the \"reverse time\" variable \"z\" = \"T\" − \"t\". (\"t\" = 0, \"z\" = \"T\" and \"t\" = \"T\", \"z\" = 0). Then:\n\nThis may be recognized as a solution to the \"reverse time\" differential equation:\n\nElectrical/electronic engineers and physicists will be familiar with an equation of this nature: it is an exact analogue of the type of differential equation which governs (for example) the charging of a capacitor in an RC circuit.\n\nThe key characteristics of such equations are explained in detail at RC circuits. For home owners with mortgages the important parameter to keep in mind is the time constant of the equation which is simply the reciprocal of the annual interest rate \"r\". So (for example) the time constant when the interest rate is 10% is 10 years and the period of a home loan should be determined – within the bounds of affordability – as a minimum multiple of this if the objective is to minimise interest paid on the loan.\n\nThe conventional difference equation for a mortgage loan is relatively straightforward to derive - balance due in each successive period is the previous balance plus per period interest less the per period fixed payment.\n\nGiven an annual interest rate \"r\" and a borrower with an annual payment capability \"M\" (divided into N equal payments made at time intervals Δ\"t\" where Δ\"t\" = 1/\"N\" years), we may write:\n\nIf \"N\" is increased indefinitely so that Δ\"t\" → 0, we obtain the continuous time differential equation:\n\nNote that for there to be a continually diminishing mortgage balance, the following inequality must hold:\n\n\"P\" is the same as \"P\"(0) – the original loan amount or loan balance at time \"t\" = 0.\n\nWe begin by re-writing the difference equation in recursive form:\n\nUsing the notation \"P\" to indicate the mortgage balance after \"n\" periods, we may apply the recursion relation iteratively to determine \"P\" and \"P\":\n\nIt can already be seen that the terms containing \"M\" form a geometric series with common ratio 1 + \"r\"Δ \"t\". This enables us to write a general expression for \"P\":\n\nFinally noting that \"r\" Δ \"t\" = \"i\" the per-period interest rate and formula_22 the per period payment, the expression may be written in conventional form:\n\nIf the loan timespan is m periods, then \"P\" = 0 and we obtain the standard present value formula:\n\nOne method of solving the equation is to obtain the Laplace transform \"P\"(\"s\"):\n\nUsing a table of Laplace transforms and their time domain equivalents, \"P\"(\"t\") may be determined:\n\nIn order to fit this solution to the particular start and end points of the mortgage function we need to introduce a time shift of \"T\" years (\"T\" = loan period) to ensure the function reaches zero at the end of the loan period:\n\nNote that both the original solution and \"time-shifted\" version satisfy the original differential equation whence both are derived. \n\nSimilar to the expression derived above for \"P\" in the difference equation, the expression for \"P\"(\"t\") may be written in the following algebraically equivalent form:\n\nRe-arranging the original differential equation we obtain:\n\nIntegrating both sides of the equation yields:\n\nThe first integral on the right hand side determines the accumulated interest payments from time of inception to time t whilst the second determines the accumulated principal payments over the same period. The sum of these interest and principal payments must equal the cumulative fixed payments at time \"t\" i.e. \"M\"\"t\". Evaluating the first integral on the right we obtain an expression for \"I\"(\"t\"), the interest paid:\n\nUnsurprisingly the second integral evaluates to \"P\" − \"P\"(\"t\") and therefore:\n\nThe reader may easily verify that this expression is algebraically identical to the one above.\n\nThe cost of a loan is simply the annual rate multiplied by loan period:\n\nLet \"s\" = \"rT\". Then we may define loan cost factor \"C\"(\"s\") such that \"C\" = \"P\"\"C\"(s) i.e.: \"C\"(\"s\") is the cost per unit of currency loaned.\n\nThe function \"C\"(\"s\") is characterised by having a limiting value of 1 when \"s\" is close to zero since for small values of \"s\", exp(−\"s\") ≈ 1 − \"s\" and the denominator simplifies to \"s\". Also when \"s\" is very large, exp(−\"s\") is small so \"C\"(\"s\") ≈ \"s\" and thus loan cost \"C\" ≈ \"P\"\"rT\" (\"rT\" » 0).\n\nBy way of example, consider a loan of 1000000 at 10% repaid over 20 years. Then \"s\" = 0.1 × 20 = 2.\n\nThe product rT is an easily obtained but important parameter in determining loan cost according to the equation C=PxC(s). This is best illustrated by plotting the cost factor function for s values in domain [0;5]. The linear behaviour of the function for higher values of \"s\" is clear.\n\nFor a fixed term loan of t years, we may compare the above loan cost factor against an equivalent simple interest cost factor \"1+s\" where \"s=rt\" and \"r\" is the equivalent simple interest rate:\n\nIt is straightforward to determine \"s\" in terms of s. Dividing by loan time period t will then give the equivalent simple interest rate. More challenging is the reverse determination of s given \"s\".\n\nIn his book \"Problem Solving with True Basic\", Dr B.D. Hahn has a short section on certain 'hire purchase' schemes in which \"interest is calculated in advance in one lump sum, which is added to the capital amount, the sum being equally divided over the repayment period. The buyer, however, is often under the impression that the interest is calculated on a reducing balance.\"\n\nThe above example is adapted from the one given in Dr Hahn's book in which he employs the Newton-Raphson algorithm to solve the same problem albeit for a discrete interval (i.e. monthly) repayment loan over the same time period (3 years). As with many similar examples the discrete interval problem and its solution is closely approximated by calculations based on the continuous repayment model - Dr Hahn's solution for interest rate is 40.8% as compared to the 41.6% calculated above.\n\nIf a borrower can afford an annual repayment rate \"M\", then we can re-arrange the formula for calculating \"M\" to obtain an expression for the time period \"T\" of a given loan \"P\":\n\nThe minimum payment ratio of a loan is the ratio of minimum possible payment rate to actual payment rate. The minimum possible payment rate is that which just covers the loan interest – a borrower would in theory pay this amount forever because there is never any decrease in loan capital. We will use the letter \"k\" to denote minimum payment ratio:\n\nNow we may consider a small re-arrangement of the equation for loan period \"T\":\n\nPlotting \"s\"(\"k\") against \"k\" gives a very graphic demonstration of why it is a good idea to keep the \"k\" value well below the asymptote at \"k\" = 1 since in the vicinity thereof, \"s\"(\"k\") increases sharply and therefore so does loan cost which is in turn an increasing function of parameter \"s\" (\"rT\" product).\n\nA useful parameter of the mortgage model is the \"half-life\" of the loan which is the time it takes for the balance on the loan to reach half its original value. To determine the \"half-life\" we may write:\n\nSolving for \"t\" we obtain:\n\nFor example applying the formula to some test data (loan of 1 million at 10% for 20 years) we obtain the half-life as 14.34 years. If in practice the loan is being repaid via monthly instalments, the decimal portion can be converted to months and rounded so this answer would equate to 172 months.\n\nIn the discrete time interval model, calculation of a mortgage based interest rate given the remaining parameters has not been possible using analytic methods. Implementations such as the Excel \"rate\" function employ a numerical \"trial and improvement\" method to determine interest rate. At first glance this would also seem to be the case for the continuous repayment model. Given:\n\nwe may write:\n\nIn order to visualise the above as a function of \"r\" (for which we wish to determine zeroes), it will be helpful to select numerical values of \"P\", \"M\" and \"T\" as 10000, 6000 and 3 respectively and plot as shown at right. It will be noted that the function has a minimum value which can be determined by differentiation:\n\nSince the function is approximately parabolic between the roots at \"r\" = 0 and the sought value, we may estimate the required root as:\n\nUsing this as a starting point, increasingly accurate values for the root may be determined by repeated iterations of the Newton–Raphson algorithm:\n\nSome experimentation on Wolfram Alpha reveals that an exact analytical solution employing the Lambert-W or \"product log\" function can be obtained. Setting \"s\" = \"M\"\"T\"/\"P\" we obtain:\n\nIn the region of interest \"W\"(−\"se\") is a bi-valued function. The first value is just −\"s\" which yields the trivial solution \"r\" = 0. The second value evaluated within the context of the above formula will provide the required interest rate.\n\nThe following table shows calculation of an initial estimate of interest rate followed by a few iterations of the Newton–Raphson algorithm. There is rapid convergence to a solution accurate to several decimal places as may be corroborated against the using the Lambert \"W\" or \"productlog\" function on Wolfram Alpha.\n\nNewton–Raphson iterations\n\nCorresponding to the standard formula for the present value of a series of fixed monthly payments, we have already established a time continuous analogue:\n\nIn similar fashion, a future value formula can be determined:\n\nIn this case the annual rate \"M\" is determined from a specified (future) savings or sinking fund target \"P\" as follows.\n\nIt will be noted that as might be expected:\n\nAnother way to calculate balance due \"P\"(\"t\") on a continuous-repayment loan is to subtract the future value (at time \"t\") of the payment stream from the future value of the loan (also at time \"t\"):\n\nThe following example from a school text book will illustrate the conceptual difference between a savings annuity based on discrete time intervals (per month in this case) and one based on continuous payment employing the above future value formula:\n\n\"On his 30th birthday, an investor decides he wants to accumulate R500000 by his 40th birthday. Starting in one month's time he decides to make equal monthly payments into an account that pays interest at 12% per annum compounded monthly. What monthly payments will he have to make?\"\n\nFor the sake of brevity, we will solve the \"discrete interval\" problem using the Excel PMT function:\n\nThe amount paid annually would therefore be 26082.57.\n\nFor a theoretical continuous payment savings annuity we can only calculate an annual \"rate\" of payment:\n\nAt this point there is a temptation to simply divide by 12 to obtain a monthly payment. However this would contradict the primary assumption upon which the \"continuous payment\" model is based: namely that the annual payment \"rate\" is defined as:\n\nSince it is of course impossible for an investor to make an infinitely small payment infinite times per annum, a bank or other lending institution wishing to offer \"continuous payment\" annuities or mortgages would in practice have to choose a large but finite value of \"N\" (annual frequency of payments) such that the continuous time formula will always be correct to within some minimal pre-specified error margin. For example hourly fixed payments (calculated using the conventional formula) in this example would accumulate to an annual payment of 25861.07 and the error would be < 0.02%. If the error margin is acceptable, the hourly payment rate can be more simply determined by dividing \"M\" by 365×24. The (hypothetical) lending institution would then need to ensure its computational resources are sufficient to implement (when required) hourly deductions from customer accounts. In short cash \"flow\" for continuous payment annuities is to be understood in the very literal sense of the word.\n\nThe following table shows how as \"N\" (annual compounding frequency) increases, the \"annual\" payment approaches the limiting value of \"M\", the annual payment \"rate\". The difference (error) between annual payment and the limiting value is calculated and expressed as a percentage of the limiting value.\n\nIt will be apparent from the above that the concept of a \"continuous repayment\" mortgage is a somewhat theoretical construct. Whether it has practical value or not is a question that would need to be carefully considered by economists and actuaries. In particular the meaning of an annual repayment \"rate\" must be clearly understood as illustrated in the above example.\n\nHowever the \"continuous payment\" model does provide some meaningful insights into the behaviour of the discrete mortgage balance function – in particular that it is largely governed by a time constant equal to the reciprocal of r the nominal annual interest rate. And if a mortgage were to be paid off via fixed daily amounts, then balance due calculations effected using the model would – in general – be accurate to within a small fraction of a percent. Finally the model demonstrates that it is to the modest advantage of the mortgage holder to increase frequency of payment where practically possible.\n\nAnnual payment rate (mortgage loan):       formula_61\n\nAnnual payment rate (sinking fund):       formula_62\n\nFuture value:       formula_63\n\nPresent value:       formula_64\n\nLoan balance:       formula_65\n\nLoan period:              formula_66\n\nHalf-life of loan:       formula_43\n\nInterest rate:           formula_50              formula_69\n\nUniversal mortgage calculator. Given any three of four variables, this calculates the fourth (unknown) value.\n\nMortgage graph. This illustrates the characteristic curve of mortgage balance vs time over a given loan timespan. Loan amount and loan interest rate (\"p\"/\"a\") may also be specified. A discrete interval loan will have a very similar characteristic.\n\n\n"}
{"id": "239238", "url": "https://en.wikipedia.org/wiki?curid=239238", "title": "D. R. Fulkerson", "text": "D. R. Fulkerson\n\nDelbert Ray Fulkerson (; August 14, 1924 – January 10, 1976) was an American mathematician who co-developed the FordFulkerson algorithm, one of the most well-known algorithms to solve the maximum flow problem in networks.\n\nD. R. Fulkerson was born in Tamms, Illinois, the third of six children of Elbert and Emma Fulkerson. Fulkerson became an undergraduate at Southern Illinois University. His academic career was interrupted by military service during World War II. Having returned to complete his degree after the war, he went on to do a Ph.D. in mathematics at the University of Wisconsin–Madison under the supervision of Cyrus MacDuffee, who was a student of L. E. Dickson. Fulkerson received his Ph.D. in 1951.\n\nHe was then with the mathematics department at the RAND Corporation until 1971 when he moved to Cornell University as the Maxwell Upson Professor of Engineering. He remained at Cornell until he committed suicide in 1976.\n\nFulkerson was the supervisor of Jon Folkman at RAND and Tatsuo Oyama at GRIPS .\n\nIn 1956, he published his noted paper on the Ford–Fulkerson algorithm\ntogether with L.R. Ford, Jr.. In 1979, the renowned Fulkerson Prize was established which is now awarded every three years for outstanding papers in discrete mathematics jointly by the Mathematical Programming Society and the American Mathematical Society.\n\n\n"}
{"id": "4078411", "url": "https://en.wikipedia.org/wiki?curid=4078411", "title": "Decimal sequences for cryptography", "text": "Decimal sequences for cryptography\n\nDecimal sequences for cryptography. A recurring decimal such as 1/7=.142857142857... can be considered a periodic series with the digits 142857 in the base 10. In general, such sequences may be written to any base. The maximum period of such a d-sequence (when the base is not necessarily 10) written for 1/q, q prime, is q-1.\n\nA binary d-sequence may be written as:\n\nAs example, the d-sequence for 1/19 is 000011010111100101.\n\nS. Kak has proposed their use for error correction coding, cryptography and as random sequences. These sequences have fairly good autocorrelation properties.\n\n\n"}
{"id": "8327127", "url": "https://en.wikipedia.org/wiki?curid=8327127", "title": "Dense-in-itself", "text": "Dense-in-itself\n\nIn mathematics, a subset formula_1 of a topological space is said to be dense-in-itself if formula_1 contains no isolated points.\n\nEvery dense-in-itself closed set is perfect. Conversely, every perfect set is dense-in-itself.\n\nA simple example of a set which is dense-in-itself but not closed (and hence not a perfect set) is the subset of irrational numbers (considered as a subset of the real numbers). This set is dense-in-itself because every neighborhood of an irrational number formula_3 contains at least one other irrational number formula_4. On the other hand, this set of irrationals is not closed because every rational number lies in its closure. For similar reasons, the set of rational numbers (also considered as a subset of the real numbers) is also dense-in-itself but not closed.\n\nThe above examples, the irrationals and the rationals, are also dense sets in their topological space, namely formula_5. As an example that is dense-in-itself but not dense in its topological space, consider formula_6. This set is not dense in formula_5 but is dense-in-itself.\n\n"}
{"id": "591931", "url": "https://en.wikipedia.org/wiki?curid=591931", "title": "Dependency ratio", "text": "Dependency ratio\n\nIn economics, geography, demography and sociology, the dependency ratio is an age-population ratio of those typically not in the labor force (the \"dependent\" part ages 0 to 14 and 65+) and those typically in the labor force (the \"productive\" part ages 15 to 64). It is used to measure the pressure on productive population.\n\nConsideration of the dependency ratio is essential for governments, economists, bankers, business, industry, universities and all other major economic segments which can benefit from understanding the impacts of changes in the dependency ratio. In the article, \"Minimizing the Dependency Ratio in a Population with Below-Replacement Fertility Through Immigration\", having an intermediate dependency ratio informs us that there are sufficient people in the working class who can support the dependent population. A lower ratio could allow for example for better pensions and better health care for residents. A higher ratio would indicate more financial stress between working people and dependents. While the strategies of increasing fertility and of allowing immigration especially of younger working age people have been formulas for lowering dependency ratios, future job reductions through automation may impact the effectivness of those strategies.\n\nIn published international statistics, the dependent part usually includes those under the age of 15 and over the age of 64.The productive part makes up the population in between, ages 15 – 64. It is normally expressed as a percentage:\n\nAs the ratio increases there may be an increased burden on the productive part of the population to maintain the upbringing and pensions of the economically dependent. This results in direct impacts on financial expenditures on things like social security, as well as many indirect consequences.\n\nThe (total) dependency ratio can be decomposed into the child dependency ratio and the aged dependency ratio:\n\nBelow is a table constructed from data provided by the UN Population Division. It shows a historical ratio for the regions shown for the period 1950 - 2010. Columns to the right show projections of the ratio. Each number in the table shows the total number of dependents (people aged 0-14 plus people aged over 65) per hundred people in the workforce (number of people aged 15-64). The number can also be expressed as a percent. So, the total dependency ratio for the world in 1950 was 64.8% of the workforce.\n\nAs of 2010, Japan and Europe had high aged dependency ratios (that is over 65 as % of workforce) compared to other parts of the world. In Europe 2010, for every adult aged 65 and older there are approximately four working age adults (15-64); This ratio (one:four, or 25%) is expected to decrease to one:two, or 50%, by 2050. An aging population is caused by a decline in fertility and longer life expectancy. The average life expectancy of males and females are expected to increase from 79 years in 1990 to 82 years in 2025. The dependency amongst Japan residents aged 65 and older is expected to increase which will have a major impact on Japan's economy.\n\nThe inverse of the dependency ratio, the inverse dependency ratio can be interpreted as how many independent workers have to provide for one dependent person (pension & expenditure on children).\n\nA high dependency ratio can cause serious problems for a country if a large proportion of a government's expenditure is on health, social security & education, which are most used by the youngest and the oldest in a population. The fewer people of working age, the fewer the people who can support schools, retirement pensions, disability pensions and other assistances to the youngest and oldest members of a population, often considered the most vulnerable members of society.\n\nNevertheless, the dependency ratio ignores the fact that the 65+ are not necessarily dependent (an increasing proportion of them are working) and that many of those of 'working age' are actually not working. Alternatives have been developed', such as the 'economic dependency ratio', but they still ignore factors such as increases in productivity and in working hours. Worries about increasing (demographic) dependency ratio should thus be taken with caution.\n\nHigh dependency ratios can also lead to long-term economic changes within the population such as saving rates, investment rates, the housing markets, and the consumption patterns. Typically, workers will start to increase their savings as they grow closer to retirement age, but this will eventually affect their long-term interest rates due to the retirement population increasing and the fertility rates decreasing. If the demographic population continues to follow this trend, their savings will decrease while their long-term interest rates increase. Due to the saving rates decreasing, the investment rate will prevent economic growth because there will be less funding for investment projects. There is a correlation between labor force and housing markets, so when there is a high age-dependency ratio in a country, the investments in housing markets will decrease since the labor force is decreasing due to a high dependency population.\n\nMigrant labor dependency ratio (MLDR) is used to describe the extent to which the domestic population is dependent upon migrant labor.\n\nLow dependency ratios promote economic growth while high dependency ratios decrease economic growth due to the large amounts of dependents that pay little to no taxes. A solution to decreasing the dependency ratio within a country is to promote immigration for younger people. This will stimulate a higher economic growth because the working-age population will grow in numbers if more young adults migrate into their country. This method has shown great results in Asia's economic growth.\n\nThe increase in the involvement of women in the work force has contributed to the working-age population which compliments the dependency ratio for a country. Encouraging women to work will help decrease the dependency ratio. Because more women are getting a higher education, it is less likely for them to have children causing the fertility rates to decrease as well. \n\nThe age-dependency ratio can determine which stage in the Demographic Transition Model a certain country is in. The dependency ratio acts like a rollercoaster when going through the stages of the Demographic Transition Model. During stages 1 and 2, the dependency ratio is high due to significantly high crude birth rates putting pressure onto the smaller working-age population to take care of all of them. In stage 3, the dependency ratio starts to decrease because fertility and mortality rates start to decrease which shows that the proportion of adults to the young and elderly are much larger in this stage. In stages 4 and 5, the dependency ratio starts to increase once again as the working-age population retires. Because fertility rates caused the younger population to decrease, once they grow up and start working, there will be more pressure for them to take care of the previous working-age population that just retired since there will be more young and elderly people than working-age adults during that time period. The population structure of a country is an important factor for determining the economic status of their country. Japan is a great example of an aging population. They have a 1:4 ratio of people 65 years and older. This causes trouble for them because there is not enough people in the working-age population to support all of the elders. Rwanda is another example of a population that struggles with a younger population (also known as the \"youth bulge\"). Both of these countries are struggling with high dependency ratios even though both countries are on opposite stages of the Demographic Transition Model.\n\n\nCase studies:\n\n"}
{"id": "17538757", "url": "https://en.wikipedia.org/wiki?curid=17538757", "title": "Equal incircles theorem", "text": "Equal incircles theorem\n\nIn geometry, the equal incircles theorem derives from a Japanese Sangaku, and pertains to the following construction: a series of rays are drawn from a given point to a given line such that the inscribed circles of the triangles formed by adjacent rays and the base line are equal. In the illustration the equal blue circles define the spacing between the rays, as described.\n\nThe theorem states that the incircles of the triangles formed (starting from any given ray) by every other ray, every third ray, etc. and the base line are also equal. The case of every other ray is illustrated above by the green circles, which are all equal.\n\nFrom the fact that the theorem doesn't depend on the angle of the initial ray, it can be seen that the theorem properly belongs to analysis, rather than geometry, and must relate to a continuous scaling function which defines the spacing of the rays. In fact, this function is the hyperbolic sine.\n\nThe theorem is a direct corollary of the following lemma:\n\nSuppose that the nth ray makes an angle formula_1 with the normal to the baseline. If formula_1 is parameterized according to the equation, formula_3, then values of formula_4, where formula_5 and formula_6 are real constants, define a sequence of rays that satisfy the condition of equal incircles, and furthermore any sequence of rays satisfying the condition can be produced by suitable choice of the constants formula_5 and formula_6. \n\nIn the diagram, lines PS and PT are adjacent rays making angles formula_1 and formula_10 with line PR, which is perpendicular to the baseline, RST.\n\nLine QXOY is parallel to the baseline and passes through O, the center of the incircle of formula_11 PST, which is tangent to the rays at W and Z. Also, line PQ has length formula_12, and line QR has length formula_13, the radius of the incircle.\n\nThen formula_11 OWX is similar to formula_11 PQX and formula_11 OZY is similar to formula_11 PQY, and from XY = XO + OY we get\n\nThis relation on a set of angles, formula_19, expresses the condition of equal incircles.\n\nTo prove the lemma, we set formula_20, which gives formula_21.\n\nUsing formula_22, we apply the addition rules for formula_23 and formula_24, and verify that the equal incircles relation is satisfied by setting\n\nThis gives an expression for the parameter formula_6 in terms of the geometric measures, formula_27 and formula_13. With this definition of formula_6 we then obtain an expression for the radii, formula_30, of the incircles formed by taking every \"N\"th ray as the sides of the triangles\n\n\n"}
{"id": "1030420", "url": "https://en.wikipedia.org/wiki?curid=1030420", "title": "Equidistributed sequence", "text": "Equidistributed sequence\n\nIn mathematics, a sequence {\"s\", \"s\", \"s\", ...} of real numbers is said to be equidistributed, or uniformly distributed, if the proportion of terms falling in a subinterval is proportional to the length of that interval. Such sequences are studied in Diophantine approximation theory and have applications to Monte Carlo integration.\n\nA sequence {\"s\", \"s\", \"s\", ...} of real numbers is said to be \"equidistributed\" on a non-degenerate interval [\"a\", \"b\"] if for any subinterval [\"c\", \"d\"] of [\"a\", \"b\"] we have\n\nFor example, if a sequence is equidistributed in [0, 2], since the interval [0.5, 0.9] occupies 1/5 of the length of the interval [0, 2], as \"n\" becomes large, the proportion of the first \"n\" members of the sequence which fall between 0.5 and 0.9 must approach 1/5. Loosely speaking, one could say that each member of the sequence is equally likely to fall anywhere in its range. However, this is not to say that {\"s\"} is a sequence of random variables; rather, it is a determinate sequence of real numbers.\n\nWe define the discrepancy \"D\" for a sequence {\"s\", \"s\", \"s\", ...} with respect to the interval [\"a\", \"b\"] as\n\nA sequence is thus equidistributed if the discrepancy \"D\" tends to zero as \"N\" tends to infinity.\n\nEquidistribution is a rather weak criterion to express the fact that a sequence fills the segment leaving no gaps. For example, the drawings of a random variable uniform over a segment will be equidistributed in the segment, but there will be large gaps compared to a sequence which first enumerates multiples of ε in the segment, for some small ε, in an appropriately chosen way, and then continues to do this for smaller and smaller values of ε. For stronger criteria and for constructions of sequences that are more evenly distributed, see low-discrepancy sequence.\n\nRecall that if \"f\" is a function having a Riemann integral in the interval [\"a\", \"b\"], then its integral is the limit of Riemann sums taken by sampling the function \"f\" in a set of points chosen from a fine partition of the interval. Therefore, if some sequence is equidistributed in [\"a\", \"b\"], it is expected that this sequence can be used to calculate the integral of a Riemann-integrable function. This leads to the following criterion for an equidistributed sequence:\n\nSuppose {\"s\", \"s\", \"s\", ...} is a sequence contained in the interval [\"a\", \"b\"]. Then the following conditions are equivalent:\n\nThis criterion leads to the idea of Monte-Carlo integration, where integrals are computed by sampling the function over a sequence of random variables equidistributed in the interval.\n\nIt is not possible to generalize the integral criterion to a class of functions bigger than just the Riemann-integrable ones. For example, if the Lebesgue integral is considered and \"f\" is taken to be in \"L\", then this criterion fails. As a counterexample, take \"f\" to be the indicator function of some equidistributed sequence. Then in the criterion, the left hand side is always 1, whereas the right hand side is zero, because the sequence is countable, so \"f\" is zero almost everywhere.\n\nIn fact, the de Bruijn–Post Theorem states the converse of the above criterion: If \"f\" is a function such that the criterion above holds for any equidistributed sequence in [\"a\", \"b\"], then \"f\" is Riemann-integrable in [\"a\", \"b\"].\n\nA sequence {\"a\", \"a\", \"a\", ...} of real numbers is said to be equidistributed modulo 1 or uniformly distributed modulo 1 if the sequence of the fractional parts of \"a\", denoted by {\"a\"} or by \"a\" − ⌊\"a\"⌋, is equidistributed in the interval [0, 1].\n\n\nThis was proven by Weyl and is an application of van der Corput's difference theorem.\n\n\nWeyl's criterion states that the sequence \"a\" is equidistributed modulo 1 if and only if for all non-zero integers ℓ,\nThe criterion is named after, and was first formulated by, Hermann Weyl. It allows equidistribution questions to be reduced to bounds on exponential sums, a fundamental and general method.\n\n\nThe sequence \"v\" of vectors in R is equidistributed modulo 1 if and only if for any non-zero vector ℓ ∈ Z,\n\nWeyl's criterion can be used to easily prove the equidistribution theorem, stating that the sequence of multiples 0, \"α\", 2\"α\", 3\"α\", ... of some real number \"α\" is equidistributed modulo 1 if and only if \"α\" is irrational.\n\nSuppose \"α\" is irrational and denote our sequence by \"a\" = \"jα\" (where \"j\" starts from 0, to simplify the formula later). Let \"ℓ\" ≠ 0 be an integer. Since \"α\" is irrational, \"ℓα\" can never be an integer, so formula_6 can never be 1. Using the formula for the sum of a finite geometric series,\na finite bound that does not depend on \"n\". Therefore after dividing by \"n\" and letting \"n\" tend to infinity, the left hand side tends to zero, and Weyl's criterion is satisfied.\n\nConversely, notice that if \"α\" is rational then this sequence is not equidistributed modulo 1, because there are only a finite number of options for the fractional part of \"a\" = \"jα\".\n\nA theorem of Johannes van der Corput states that if for each \"h\" the sequence \"s\" − \"s\" is uniformly distributed modulo 1, then so is \"s\".\n\nA van der Corput set is a set \"H\" of integers such that if for each \"h\" in \"H\" the sequence \"s\" − \"s\" is uniformly distributed modulo 1, then so is s.\n\nMetric theorems describe the behaviour of a parametrised sequence for almost all values of some parameter \"α\": that is, for values of \"α\" not lying in some exceptional set of Lebesgue measure zero.\n\nIt is not known whether the sequences or are equidistributed mod 1. However it is known that the sequence is \"not\" equidistributed mod 1 if \"α\" is a PV number.\n\nA sequence {\"s\", \"s\", \"s\", ...} of real numbers is said to be well-distributed on [\"a\", \"b\"] if for any subinterval [\"c\", \"d\"] of [\"a\", \"b\"] we have\n\"uniformly\" in \"k\". Clearly every well-distributed sequence is uniformly distributed, but the converse does not hold. The definition of well-distributed modulo 1 is analogous.\n\nFor an arbitrary probability measure space formula_9, a sequence of points formula_10 is said to be equidistributed with respect to formula_11 if the mean of point measures converges weakly to formula_11:\nIt is true, for example, that for any Borel probability measure on a separable, metrizable space, there exists an equidistributed sequence (with respect to the measure).\n\n\n\n\n"}
{"id": "1482085", "url": "https://en.wikipedia.org/wiki?curid=1482085", "title": "Euclidean topology", "text": "Euclidean topology\n\nIn mathematics, and especially general topology, the Euclidean topology is the natural topology induced on Euclidean n-space formula_1 by the Euclidean metric.\n\nIn any metric space, the open balls form a base for a topology on that space. The Euclidean topology on formula_1 is then simply the topology \"generated\" by these balls. In other words, the open sets of the Euclidean topology on formula_1 are given by (arbitrary) unions of the open balls formula_4 defined as formula_5, for all formula_6 and all formula_7, where formula_8 is the Euclidean metric.\n\n"}
{"id": "2545319", "url": "https://en.wikipedia.org/wiki?curid=2545319", "title": "Evolutionarily stable state", "text": "Evolutionarily stable state\n\n\"A population is said to be in an evolutionarily stable state if its genetic composition is restored by selection after a disturbance, provided the disturbance is not too large. Such a population can be genetically monomorphic or polymorphic.\" —Maynard Smith (1982).\n\nContrast this with the definition of an evolutionarily stable strategy, which is a game theory concept.\n\n\"An ESS or evolutionarily stable strategy is a strategy such that, if all the members of a population adopt it, no mutant strategy can invade.\" —Maynard Smith (1982).\n\nEvolutionarily stable states are frequently used to identify solutions to the replicator equation, given in its linear payoff form by\nThe state formula_2 is said to be evolutionarily stable if for all formula_3 in some neighborhood of formula_2\n\nThis is similar but distinct from the concept of an evolutionarily stable strategy.\n\nIt can be shown that for the replicator equation if a state is evolutionarily stable then it is an asymptotically stable rest point, so the evolutionarily stable states are often taken as solutions of the replicator equation. The converse is true if the game matrix is symmetric.\n\nThe concept of evolutionary stability is equivalent to the concept of strong stability for normal form games, due to Cressman.\n\n"}
{"id": "849738", "url": "https://en.wikipedia.org/wiki?curid=849738", "title": "Exterior covariant derivative", "text": "Exterior covariant derivative\n\nIn mathematics, the exterior covariant derivative is an analog of an exterior derivative that takes into account the presence of a connection.\n\nLet \"G\" be a Lie group and be a principal \"G\"-bundle on a smooth manifold \"M\". Suppose there is a connection on \"P\"; this yields a natural direct sum decomposition formula_1 of each tangent space into the horizontal and vertical subspaces. Let formula_2 be the projection to the horizontal subspace.\n\nIf \"ϕ\" is a \"k\"-form on \"P\" with values in a vector space \"V\", then its exterior covariant derivative \"Dϕ\" is a form defined by\nwhere \"v\" are tangent vectors to \"P\" at \"u\".\n\nSuppose that is a representation of \"G\" on a vector space \"V\". If \"ϕ\" is equivariant in the sense that\n\nwhere formula_5, then \"Dϕ\" is a tensorial -form on \"P\" of the type \"ρ\": it is equivariant and horizontal (a form \"ψ\" is horizontal if .)\n\nBy abuse of notation, the differential of ρ at the identity element may again be denoted by ρ:\n\nLet formula_7 be the connection one-form and formula_8 the representation of the connection in formula_9 That is, formula_8 is a formula_11-valued form, vanishing on the horizontal subspace. If \"ϕ\" is a tensorial \"k\"-form of type \"ρ\", then\n\nwhere, following the notation in , we wrote\n\nUnlike the usual exterior derivative, which squares to 0, the exterior covariant derivative does not. In general, one has, for a tensorial zero-form \"ϕ\",\n\nwhere is the representation in formula_11 of the curvature two-form Ω. The form F is sometimes referred to as the field strength tensor, in analogy to the role it plays in electromagnetism. Note that \"D\" vanishes for a flat connection (i.e. when ).\n\nIf , then one can write\nwhere formula_17 is the matrix with 1 at the -th entry and zero on the other entries. The matrix formula_18 whose entries are 2-forms on \"P\" is called the curvature matrix.\n\nWhen is a representation, one can form the associated bundle . Then the exterior covariant derivative \"D\" given by a connection on \"P\" induces an exterior covariant derivative (sometimes called the exterior connection) on the associated bundle, this time using the nabla symbol: \nHere, Γ denotes the sheaf of local sections of the vector bundle. The extension is made through the correspondence between \"E\"-valued forms and tensorial forms of type \"ρ\" (see tensorial forms on principal bundles.)\n\nRequiring ∇ to satisfy Leibniz's rule, ∇ also acts on any \"E\"-valued form; thus, it is given on decomposable elements of the space formula_20 of formula_21-valued \"k\"-forms by \n\nFor a section \"s\" of \"E\", we also set\nwhere formula_24 is the contraction by \"X\".\n\nConversely, given a vector bundle \"E\", one can take its frame bundle, which is a principal bundle, and so obtain an exterior covariant differentiation on \"E\" (depending on a connection). Identifying tensorial forms and \"E\"-valued forms, one may show that\n\nwhich can be easily recognized as the definition of the Riemann curvature tensor on Riemannian manifolds.\n\n"}
{"id": "34329151", "url": "https://en.wikipedia.org/wiki?curid=34329151", "title": "Fundamental ephemeris", "text": "Fundamental ephemeris\n\nA fundamental ephemeris of the Solar System is a model of the objects of the system in space, with all of their positions and motions accurately represented. It is intended to be a high-precision primary reference for prediction and observation of those positions and motions, and which provides a basis for further refinement of the model. It is generally not intended to cover the entire life of the Solar System; usually a short-duration time span, perhaps a few centuries, is represented to high accuracy. Some long ephemerides cover several millennia to medium accuracy.\n\nThey are published by the Jet Propulsion Laboratory as Development Ephemeris. The latest releases include DE430 which covers planetary and lunar ephemeris from Dec 21, 1549 to Jan 25, 2650 with high precision and is intended for general use for modern time periods . DE431 was created to cover a longer time period Aug 15, -13200 to March 15, 17191 with slightly less precision for use with historic observations and far reaching forecasted positions. DE432 was released as a minor update to DE430 with improvements to the Pluto barycenter in support of the New Horizons mission.\n\nThe set of physical laws and numerical constants used in the calculation of the ephemeris must be self-consistent and precisely specified. The ephemeris must be calculated strictly in accordance with this set, which represents the most current knowledge of all relevant physical forces and effects. Current fundamental ephemerides are typically released with exact descriptions of all mathematical models, methods of computation, observational data, and adjustment to the observations at the time of their announcement. This may not have been the case in the past, as fundamental ephemerides were then computed from a collection of methods derived over a span of decades by many researchers.\n\nThe independent variable of the ephemeris is always time. In the case of the most current ephemerides, it is a relativistic coordinate time scale equivalent to the IAU definition of TCB. In the past, mean solar time (before the discovery of the non-uniform rotation of the Earth) and ephemeris time (before the implementation of relativistic gravitational equations) were used. The remainder of the ephemeris can consist of either the mathematical equations and initial conditions which describe the motions of the bodies of the Solar System, of tabulated data calculated from those equations and conditions, or of condensed mathematical representations of the tabulated data.\n\nA fundamental ephemeris is the basis from which apparent ephemerides, phenomena, and orbital elements are computed for astronomical, nautical, and surveyors' almanacs. Apparent ephemerides give positions and motions of Solar System bodies as seen by observers from the surface of Earth, and are useful for astronomers, navigators, and surveyors in planning observations and in reducing the data acquired, although much of the work of latter two has been supplanted by GPS technology. Phenomena are events related to the configurations of Solar System bodies, for instance rise and set times, phases, eclipses and occultations, and have many civil and scientific applications. Orbital elements are descriptions of the motion of a body at a particular instant, used for further short-time-span calculation of the body's position when high accuracy is not required.\n\nAstronomers have been tasked with computing accurate ephemerides, originally for purposes of sea navigation, from at least the 18th century. In England, Charles II founded the Royal Observatory in 1675, which began publishing \"The Nautical Almanac\" in 1766. In France, the \"Bureau des Longitudes\" was founded in 1795 to publish the \"Connaissance des Temps\". The early fundamental ephemerides of these publications came from many different sources and authors as the science of celestial mechanics matured.\n\nAt the end of the 19th century, the analytical methods of general perturbations reached the probable limits of what could be accomplished by hand calculation. The planetary \"theories\" of Newcomb and Hill formed the fundamental ephemerides of the \"Nautical Almanac\" at that time. For the Sun, Mercury, Venus, and Mars, the tabulations of the \"Astronomical Almanac\" continued to be derived from the work of Newcomb and Ross through 1983. In France, the works of LeVerrier and Gaillot formed the fundamental ephemeris of the \"Connaissance des Temps\".\n\nFrom the mid 20th century, work began on numerical integration of the equations of motion on early computing machines for purposes of producing fundamental ephemerides for the \"Astronomical Almanac\". Jupiter, Saturn, Uranus, Neptune, and Pluto were based on the work of Eckert, \"et al\". and Clemence through 1983. The fundamental ephemeris of the Moon, always a difficult problem in celestial mechanics, remained a work-in-progress through the early 1980s. It was based originally on the work of Brown, with updates and corrections by Clemence, \"et al\". and Eckert, \"et al\".\n\nStarting in 1984, a revolution in the methods of producing fundamental ephemerides began. From 1984 through 2002, the fundamental ephemeris of the \"Astronomical Almanac\" was the Jet Propulsion Laboratory's DE200/LE200, a fully numerically-integrated ephemeris fitted to modern position and velocity observations of the Sun, Moon, and planets. From 2003 onward (as of Feb 2012), JPL's DE405/LE405, an integrated ephemeris referred to the International Celestial Reference Frame, has been used. In France, the \"Bureau des Longitudes\" began using their machine-generated semi-analytical theory VSOP82 in 1984, and their work continued with the founding of the \"Institut de mécanique céleste et de calcul des éphémérides\" in 1998 and the INPOP series of numerical ephemerides. DE405/LE405 were superseded by DE421/LE421 in 2008.\n\n\n"}
{"id": "26263536", "url": "https://en.wikipedia.org/wiki?curid=26263536", "title": "Generalized context-free grammar", "text": "Generalized context-free grammar\n\nGeneralized Context-free Grammar (GCFG) is a grammar formalism that expands on context-free grammars by adding potentially non-context free composition functions to rewrite rules. Head grammar (and its weak equivalents) is an instance of such a GCFG which is known to be especially adept at handling a wide variety of non-CF properties of natural language.\n\nA GCFG consists of two components: a set of composition functions that combine string tuples, and a set of rewrite rules. The composition functions all have the form formula_1, where formula_2 is either a single string tuple, or some use of a (potentially different) composition function which reduces to a string tuple. Rewrite rules look like formula_3, where formula_4, formula_5, ... are string tuples or non-terminal symbols.\n\nThe rewrite semantics of GCFGs is fairly straightforward. An occurrence of a non-terminal symbol is rewritten using rewrite rules as in a context-free grammar, eventually yielding just compositions (composition functions applied to string tuples or other compositions). The composition functions are then applied, successively reducing the tuples to a single tuple.\n\nA simple translation of a context-free grammar into a GCFG can be performed in the following fashion. Given the grammar in (), which generates the palindrome language formula_6, where formula_7 is the string reverse of formula_8, we can define the composition function \"conc\" as in () and the rewrite rules as in ().\n\nThe CF production of \"abbbba\" is\n\nand the corresponding GCFG production is\n\nWeir (1988) describes two properties of composition functions, linearity and regularity. A function defined as formula_17 is linear if and only if each variable appears at most once on either side of the \"=\", making formula_18 linear but not formula_19. A function defined as formula_17 is regular if the left hand side and right hand side have exactly the same variables, making formula_21 regular but not formula_18 or formula_23.\n\nA grammar in which all composition functions are both linear and regular is called a Linear Context-free Rewriting System (LCFRS). LCFRS is a proper subclass of the GCFGs, i.e. it has strictly less computational power than the GCFGs as a whole.\n\nOn the other hand, LCFRSs are strictly more expressive than linear-indexed grammars and their weakly equivalent variant tree adjoining grammars (TAGs). Head grammar is another example of an LCFRS that is strictly less powerful than the class of LCFRSs as a whole. \n\nLCFRS are weakly equivalent to (set-local) \"multicomponent\" TAGs (MCTAGs) and also with multiple context-free grammar (MCFGs ). and minimalist grammars (MGs). The languages generated by LCFRS (and their weakly equivalents) can be parsed in polynomial time.\n\n"}
{"id": "20898181", "url": "https://en.wikipedia.org/wiki?curid=20898181", "title": "Geometric combinatorics", "text": "Geometric combinatorics\n\nGeometric combinatorics is a branch of mathematics in general and combinatorics in particular. It includes a number of subareas such as polyhedral combinatorics (the study of faces of convex polyhedra), convex geometry (the study of convex sets, in particular combinatorics of their intersections), and discrete geometry, which in turn has many applications to computational geometry. Other important areas include metric geometry of polyhedra, such as the Cauchy theorem on rigidity of convex polytopes. The study of regular polytopes, Archimedean solids, and kissing numbers is also a part of geometric combinatorics. Special polytopes are also considered, such as the permutohedron, associahedron and Birkhoff polytope. \n\n\n"}
{"id": "314780", "url": "https://en.wikipedia.org/wiki?curid=314780", "title": "Hardy space", "text": "Hardy space\n\nIn complex analysis, the Hardy spaces (or Hardy classes) \"H\" are certain spaces of holomorphic functions on the unit disk or upper half plane. They were introduced by Frigyes Riesz , who named them after G. H. Hardy, because of the paper . In real analysis Hardy spaces are certain spaces of distributions on the real line, which are (in the sense of distributions) boundary values of the holomorphic functions of the complex Hardy spaces, and are related to the \"L\" spaces of functional analysis. For 1 ≤ \"p\" ≤ ∞ these real Hardy spaces \"H\" are certain subsets of \"L\", while for \"p\" < 1 the \"L\" spaces have some undesirable properties, and the Hardy spaces are much better behaved.\n\nThere are also higher-dimensional generalizations, consisting of certain holomorphic functions on tube domains in the complex case, or certain spaces of distributions on R in the real case.\n\nHardy spaces have a number of applications in mathematical analysis itself, as well as in control theory (such as \"H\" methods) and in scattering theory.\n\nFor spaces of holomorphic functions on the open unit disk, the Hardy space \"H\" consists of the functions \"f\" whose mean square value on the circle of radius \"r\" remains bounded as \"r\" → 1 from below.\n\nMore generally, the Hardy space \"H\" for 0 < \"p\" < ∞ is the class of holomorphic functions \"f\" on the open unit disk satisfying\n\nThis class \"H\" is a vector space. The number on the left side of the above inequality is the Hardy space \"p\"-norm for \"f\", denoted by formula_2 It is a norm when \"p\" ≥ 1, but not when 0 < \"p\" < 1.\n\nThe space \"H\" is defined as the vector space of bounded holomorphic functions on the disk, with the norm\n\nFor 0 < p ≤ q ≤ ∞, the class \"H\" is a subset of \"H\", and the \"H\"-norm is increasing with \"p\" (it is a consequence of Hölder's inequality that the \"L\"-norm is increasing for probability measures, i.e. measures with total mass 1).\n\nThe Hardy spaces defined in the preceding section can also be viewed as certain closed vector subspaces of the complex \"L\" spaces on the unit circle. This connection is provided by the following theorem : Given \"f\" ∈ \"H\", with \"p\" ≥ 0, the radial limit\n\nexists for almost every θ. The function formula_5 belongs to the \"L\" space for the unit circle, and one has that\n\nDenoting the unit circle by T, and by \"H\"(T) the vector subspace of \"L\"(T) consisting of all limit functions formula_5, when \"f\" varies in \"H\", one then has that for \"p\" ≥ 1,\n\nwhere the \"ĝ\"(\"n\") are the Fourier coefficients of a function \"g\" integrable on the unit circle,\n\nThe space \"H\"(T) is a closed subspace of \"L\"(T). Since \"L\"(T) is a Banach space (for 1 ≤ \"p\" ≤ ∞), so is \"H\"(T).\n\nThe above can be turned around. Given a function formula_5 ∈ \"L\"(T), with \"p\" ≥ 1, one can regain a (harmonic) function \"f\" on the unit disk by means of the Poisson kernel \"P\":\n\nand \"f\" belongs to \"H\" exactly when formula_5 is in \"H\"(T). Supposing that formula_5 is in \"H\"(T). \"i.e.\" that formula_5 has Fourier coefficients (\"a\") with \"a\" = 0 for every \"n\" < 0,then the element \"f\" of the Hardy space \"H\" associated to formula_5 is the holomorphic function\n\nIn applications, those functions with vanishing negative Fourier coefficients are commonly interpreted as the causal solutions. Thus, the space \"H\" is seen to sit naturally inside \"L\" space, and is represented by infinite sequences indexed by N; whereas \"L\" consists of bi-infinite sequences indexed by Z.\n\nWhen 1 ≤ \"p\" < ∞, the \"real Hardy spaces\" \"H\" discussed further down in this article are easy to describe in the present context. A real function \"f\" on the unit circle belongs to the real Hardy space \"H\"(T) if it is the real part of a function in \"H\"(T), and a complex function \"f\" belongs to the real Hardy space iff Re(\"f\") and Im(\"f\") belong to the space (see the section on real Hardy spaces below). Thus for 1 ≤ \"p\" < ∞, the real Hardy space contains the Hardy space, but is much bigger, since no relationship is imposed between the real and imaginary part of the function.\n\nFor 0 < \"p\" < 1, such tools as Fourier coefficients, Poisson integral, conjugate function, are no longer valid. For example, consider the function\n\nThen \"F\" is in \"H\" for every 0 < \"p\" < 1, and the radial limit\n\nexists for a.e. \"θ\" and is in \"H\"(T), but Re(\"f\") is 0 almost everywhere, so it is no longer possible to recover \"F\" from Re(\"f\"). As a consequence of this example, one sees that for 0 < \"p\" < 1, one cannot characterize the real-\"H\"(T) (defined below) in the simple way given above, but must use the actual definition using maximal functions, which is given further along somewhere below.\n\nFor the same function \"F\", let \"f\"(e) = \"F\"(\"re\"). The limit when \"r\" → 1 of Re(\"f\"), \"in the sense of\" \"distributions\" on the circle, is a non-zero multiple of the Dirac distribution at \"z\" = 1. The Dirac distribution at a point of the unit circle belongs to real-\"H\"(T) for every \"p\" < 1 (see below).\n\nFor 0 < \"p\" ≤ ∞, every non-zero function \"f\" in \"H\" can be written as the product \"f\" = \"Gh\" where \"G\" is an \"outer function\" and \"h\" is an \"inner function\", as defined below . This \"Beurling factorization\" allows the Hardy space to be completely characterized by the spaces of inner and outer functions.\n\nOne says that \"G\"(\"z\") is an outer (exterior) function if it takes the form\n\nfor some complex number \"c\" with |\"c\"| = 1, and some positive measurable function formula_20 on the unit circle such that formula_21 is integrable on the circle. In particular, when formula_20 is integrable on the circle, \"G\" is in \"H\" because the above takes the form of the Poisson kernel . This implies that\n\nfor almost every θ.\n\nOne says that \"h\" is an inner (interior) function if and only if |\"h\"| ≤ 1 on the unit disk and the limit\n\nexists for almost all θ and its modulus is equal to 1 a.e. In particular, \"h\" is in \"H\". The inner function can be further factored into a form involving a Blaschke product.\n\nThe function \"f\", decomposed as \"f\" = \"Gh\", is in \"H\" if and only if φ belongs to \"L\"(T), where φ is the positive function in the representation of the outer function \"G\".\n\nLet \"G\" be an outer function represented as above from a function φ on the circle. Replacing φ by φ, α > 0, a family (\"G\") of outer functions is obtained, with the properties:\n\nIt follows that whenever 0 < \"p\", \"q\", \"r\" < ∞ and 1/\"r\" = 1/\"p\" + 1/\"q\", every function \"f\" in \"H\" can be expressed as the product of a function in \"H\" and a function in \"H\". For example: every function in \"H\" is the product of two functions in \"H\"; every function in \"H\", \"p\" < 1, can be expressed as product of several functions in some \"H\", \"q\" > 1.\n\nReal-variable techniques, mainly associated to the study of \"real Hardy spaces\" defined on R (see below), are also used in the simpler framework of the circle. It is a common practice to allow for complex functions (or distributions) in these \"real\" spaces. The definition that follows does not distinguish between real or complex case.\n\nLet \"P\" denote the Poisson kernel on the unit circle T. For a distribution \"f\" on the unit circle, set\n\nwhere the \"star\" indicates convolution between the distribution \"f\" and the function e → \"P\"(θ) on the circle. Namely, (\"f\" ∗ \"P\")(e) is the result of the action of \"f\" on the \"C\"-function defined on the unit circle by\n\nFor 0 < \"p\" < ∞, the \"real Hardy space\" \"H\"(T) consists of distributions \"f\" such that \"M f\"  is in \"L\"(T).\n\nThe function \"F\" defined on the unit disk by \"F\"(\"re\") = (\"f\" ∗ \"P\")(e) is harmonic, and \"M f\"  is the \"radial maximal function\" of \"F\". When \"M f\"  belongs to \"L\"(T) and \"p\" ≥ 1, the distribution \"f\"  \"\"is\" a function in \"L\"(T), namely the boundary value of \"F\". For \"p\" ≥ 1, the \"real Hardy space\" \"H\"(T) is a subset of \"L\"(T).\n\nTo every real trigonometric polynomial \"u\" on the unit circle, one associates the real \"conjugate polynomial\" \"v\" such that \"u\" + i\"v\" extends to a holomorphic function in the unit disk,\n\nThis mapping \"u\" → \"v\" extends to a bounded linear operator \"H\" on \"L\"(T), when 1 < \"p\" < ∞ (up to a scalar multiple, it is the Hilbert transform on the unit circle), and \"H\" also maps \"L\"(T) to weak-\"L\"(T). When 1 ≤ \"p\" < ∞, the following are equivalent for a \"real valued\" integrable function \"f\" on the unit circle:\n\nWhen 1 < \"p\" < ∞, \"H(f)\" belongs to \"L\"(T) when \"f\" ∈ \"L\"(T), hence the real Hardy space \"H\"(T) coincides with \"L\"(T) in this case. For \"p\" = 1, the real Hardy space \"H\"(T) is a proper subspace of \"L\"(T).\n\nThe case of \"p\" = ∞ was excluded from the definition of real Hardy spaces, because the maximal function \"M f\"  of an \"L\" function is always bounded, and because it is not desirable that real-\"H\" be equal to \"L\". However, the two following properties are equivalent for a real valued function \"f\"\n\nWhen 0 < \"p\" < 1, a function \"F\" in \"H\" cannot be reconstructed from the real part of its boundary limit \"function\" on the circle, because of the lack of convexity of \"L\" in this case. Convexity fails but a kind of \"complex convexity\"\" remains, namely the fact that \"z\" → |\"z\"| is subharmonic for every \"q\" > 0. As a consequence, if\n\nis in \"H\", it can be shown that \"c\" = O(\"n\"). It follows that the Fourier series\n\nconverges in the sense of distributions to a distribution \"f\" on the unit circle, and \"F\"(\"re\") =(\"f\" ∗ \"P\")(θ). The function \"F\" ∈ \"H\" can be reconstructed from the real distribution Re(\"f\") on the circle, because the Taylor coefficients \"c\" of \"F\" can be computed from the Fourier coefficients of Re(\"f\").\n\nDistributions on the circle are general enough for handling Hardy spaces when \"p\" < 1. Distributions that are not functions do occur, as is seen with functions \"F\"(\"z\") = (1−\"z\") (for |\"z\"| < 1), that belong to \"H\" when 0 < \"N\" \"p\" < 1 (and \"N\" an integer ≥ 1).\n\nA real distribution on the circle belongs to real-\"H\"(T) iff it is the boundary value of the real part of some \"F\" ∈ \"H\". A Dirac distribution δ, at any point \"x\" of the unit circle, belongs to real-\"H\"(T) for every \"p\" < 1; derivatives δ′ belong when \"p\" < 1/2, second derivatives δ′′ when \"p\" < 1/3, and so on.\n\nIt is possible to define Hardy spaces on other domains than the disc, and in many applications Hardy spaces on a complex half-plane (usually the right half-plane or upper half-plane) are used.\n\nThe Hardy space \"H\"(H) on the upper half-plane H is defined to be the space of holomorphic functions \"f\" on H with bounded (quasi-)norm, the norm being given by\n\nThe corresponding \"H\"(H) is defined as functions of bounded norm, with the norm given by\n\nAlthough the unit disk D and the upper half-plane H can be mapped to one another by means of Möbius transformations, they are not interchangeable as domains for Hardy spaces. Contributing to this difference is the fact that the unit circle has finite (one-dimensional) Lebesgue measure while the real line does not. However, for \"H\", one has the following theorem: if \"m\" : D → H denotes the Möbius transformation\n\nThen the linear operator \"M\" : \"H\"(H) → \"H\"(D) defined by\n\nis an isometric isomorphism of Hilbert spaces.\n\nIn analysis on the real vector space R, the Hardy space \"H\" (for 0 < \"p\" ≤ ∞) consists of tempered distributions \"f\" such that for some Schwartz function Φ with ∫Φ = 1, the maximal function\n\nis in \"L\"(R), where ∗ is convolution and . The \"H\"-quasinorm ||\"f\" || of a distribution \"f\" of \"H\" is defined to be the \"L\" norm of \"M\"\"f\" (this depends on the choice of Φ, but different choices of Schwartz functions Φ give equivalent norms). The \"H\"-quasinorm is a norm when \"p\" ≥ 1, but not when \"p\" < 1.\n\nIf 1 < \"p\" < ∞, the Hardy space \"H\" is the same vector space as \"L\", with equivalent norm. When \"p\" = 1, the Hardy space \"H\" is a proper subspace of \"L\". One can find sequences in \"H\" that are bounded in \"L\" but unbounded in \"H\", for example on the line\n\nThe \"L\" and \"H\" norms are not equivalent on \"H\", and \"H\" is not closed in \"L\". The dual of \"H\" is the space \"BMO\" of functions of bounded mean oscillation. The space \"BMO\" contains unbounded functions (proving again that \"H\" is not closed in \"L\").\n\nIf \"p\" < 1 then the Hardy space \"H\" has elements that are not functions, and its dual is the homogeneous Lipschitz space of order \"n\"(1/\"p\" − 1). When \"p\" < 1, the \"H\"-quasinorm is not a norm, as it is not subadditive. The \"p\"th power ||\"f\" || is subadditive for \"p\" < 1 and so defines a metric on the Hardy space \"H\", which defines the topology and makes \"H\" into a complete metric space.\n\nWhen 0 < \"p\" ≤ 1, a bounded measurable function \"f\" of compact support is in the Hardy space \"H\" if and only if all its moments\n\nwhose order \"i\"+ ... +\"i\" is at most \"n\"(1/\"p\" − 1), vanish. For example, the integral of \"f\" must vanish in order that \"f\" ∈ \"H\", 0 < \"p\" ≤ 1, and as long as \"p\" > this is also sufficient.\n\nIf in addition \"f\" has support in some ball \"B\" and is bounded by |\"B\"| then \"f\" is called an \"H\"-atom (here |\"B\"| denotes the Euclidean volume of \"B\" in R). The \"H\"-quasinorm of an arbitrary \"H\"-atom is bounded by a constant depending only on \"p\" and on the Schwartz function Φ.\n\nWhen 0 < \"p\" ≤ 1, any element \"f\" of \"H\" has an atomic decomposition as a convergent infinite combination of \"H\"-atoms,\n\nwhere the \"a\" are \"H\"-atoms and the \"c\" are scalars.\n\nOn the line for example, the difference of Dirac distributions \"f\" = δ−δ can be represented as a series of Haar functions, convergent in \"H\"-quasinorm when 1/2 < \"p\" < 1 (on the circle, the corresponding representation is valid for 0 < \"p\" < 1, but on the line, Haar functions do not belong to \"H\" when \"p\" ≤ 1/2 because their maximal function is equivalent at infinity to \"a\" \"x\" for some \"a\" ≠ 0).\n\nLet (\"M\") be a martingale on some probability space (Ω, Σ, \"P\"), with respect to an increasing sequence of σ-fields (Σ). Assume for simplicity that Σ is equal to the σ-field generated by the sequence (Σ). The \"maximal function\" of the martingale is defined by\n\nLet 1 ≤ \"p\" < ∞. The martingale (\"M\") belongs to \"martingale\"-\"H\" when \"M*\" ∈ \"L\".\n\nIf \"M*\" ∈ \"L\", the martingale (\"M\") is bounded in \"L\"; hence it converges almost surely to some function \"f\" by the martingale convergence theorem. Moreover, \"M\" converges to \"f\" in \"L\"-norm by the dominated convergence theorem; hence \"M\" can be expressed as conditional expectation of \"f\" on Σ. It is thus possible to identify martingale-\"H\" with the subspace of \"L\"(Ω, Σ, \"P\") consisting of those \"f\" such that the martingale\n\nbelongs to martingale-\"H\".\n\nDoob's maximal inequality implies that martingale-\"H\" coincides with \"L\"(Ω, Σ, \"P\") when 1 < \"p\" < ∞. The interesting space is martingale-\"H\", whose dual is martingale-BMO .\n\nThe Burkholder–Gundy inequalities (when \"p\" > 1) and the Burgess Davis inequality (when \"p\" = 1) relate the \"L\"-norm of the maximal function to that of the \"square function\" of the martingale\n\nMartingale-\"H\" can be defined by saying that \"S\"(\"f\")∈ \"L\" .\n\nMartingales with continuous time parameter can also be considered. A direct link with the classical theory is obtained via the complex Brownian motion (\"B\") in the complex plane, starting from the point \"z\" = 0 at time \"t\" = 0. Let τ denote the hitting time of the unit circle. For every holomorphic function \"F\" in the unit disk,\n\nis a martingale, that belongs to martingale-\"H\" iff \"F\" ∈ \"H\" .\n\nIn this example, Ω = [0, 1] and Σ is the finite field generated by the dyadic partition of [0, 1] into 2 intervals of length 2, for every \"n\" ≥ 0. If a function \"f\" on [0, 1] is represented by its expansion on the Haar system (\"h\")\n\nthen the martingale-\"H\" norm of \"f\" can be defined by the \"L\" norm of the square function\n\nThis space, sometimes denoted by \"H\"(δ), is isomorphic to the classical real \"H\" space on the circle . The Haar system is an unconditional basis for \"H\"(δ).\n\n"}
{"id": "26102914", "url": "https://en.wikipedia.org/wiki?curid=26102914", "title": "Head grammar", "text": "Head grammar\n\nHead grammar (HG) is a grammar formalism introduced in Carl Pollard (1984) as an extension of the context-free grammar class of grammars. Head grammar is therefore a type of phrase structure grammar, as opposed to a dependency grammar. The class of head grammars is a subset of the linear context-free rewriting systems.\n\nOne typical way of defining head grammars is to replace the terminal strings of CFGs with indexed terminal strings, where the index denotes the \"head\" word of the string. Thus, for example, a CF rule such as formula_1 might instead be formula_2, where the 0th terminal, the \"a\", is the head of the resulting terminal string. For convenience of notation, such a rule could be written as just the terminal string, with the head terminal denoted by some sort of mark, as in formula_3.\n\nTwo fundamental operations are then added to all rewrite rules: wrapping and concatenation.\n\nWrapping is an operation on two headed strings defined as follows:\n\nLet formula_4 and formula_5 be terminal strings headed by \"x\" and \"y\", respectively.\n\nformula_6\n\nConcatenation is a family of operations on n > 0 headed strings, defined for n = 1, 2, 3 as follows:\n\nLet formula_4, formula_5, and formula_9 be terminal strings headed by \"x\", \"y\", and \"z\", respectively.\n\nformula_10\n\nformula_11\n\nformula_12\n\nformula_13\n\nformula_14\n\nformula_15\n\nAnd so on for formula_16. One can sum up the pattern here simply as \"concatenate some number of terminal strings \"m\", with the head of string \"n\" designated as the head of the resulting string\".\n\nHead grammar rules are defined in terms of these two operations, with rules taking either of the forms\n\nformula_17\n\nformula_18\n\nwhere formula_19, formula_20, ... are each either a terminal string or a non-terminal symbol.\n\nHead grammars are capable of generating the language formula_21. We can define the grammar as follows:\n\nformula_22\n\nformula_23\n\nformula_24\n\nThe derivation for \"abcd\" is thus:\n\nformula_25\n\nformula_26\n\nformula_27\n\nformula_28\n\nformula_29\n\nformula_30\n\nformula_31\n\nAnd for \"\":\n\nformula_25\n\nformula_26\n\nformula_27\n\nformula_35\n\nformula_36\n\nformula_37\n\nformula_38\n\nformula_39\n\nformula_40\n\nformula_41\n\nformula_42\n\nVijay-Shanker and Weir (1994) demonstrates that linear indexed grammars, combinatory categorial grammar, tree-adjoining grammars, and head grammars are weakly equivalent formalisms, in that they all define the same string languages.\n"}
{"id": "4127357", "url": "https://en.wikipedia.org/wiki?curid=4127357", "title": "Hermitian symmetric space", "text": "Hermitian symmetric space\n\nIn mathematics, a Hermitian symmetric space is a Hermitian manifold which at every point has as an inversion symmetry preserving the Hermitian structure. First studied by Élie Cartan, they form a natural generalization of the notion of Riemannian symmetric space from real manifolds to complex manifolds.\n\nEvery Hermitian symmetric space is a homogeneous space for its isometry group and has a unique decomposition as a product of irreducible spaces and a Euclidean space. The irreducible spaces arise in pairs as a non-compact space that, as Borel showed, can be embedded as an open subspace of its compact dual space. Harish Chandra showed that each non-compact space can be realized as a bounded symmetric domain in a complex vector space. The simplest case involves the groups SU(2), SU(1,1) and their common complexification SL(2,C). In this case the non-compact space is the unit disk, a homogeneous space for SU(1,1). It is a bounded domain in the complex plane C. The one-point compactification of C, the Riemann sphere, is the dual space, a homogeneous space for SU(2) and SL(2,C).\n\nIrreducible compact Hermitian symmetric spaces are exactly the homogeneous spaces of simple compact Lie groups by maximal closed connected subgroups which contain a maximal torus and have center isomorphic to T. There is a complete classification of irreducible spaces, with four classical series, studied by Cartan, and two exceptional cases; the classification can be deduced from\nBorel–de Siebenthal theory, which classifies closed connected subgroups containing a maximal torus. Hermitian symmetric spaces appear in the theory of Jordan triple systems, several complex variables, complex geometry, automorphic forms and group representations, in particular permitting the construction of the holomorphic discrete series representations of semisimple Lie groups.\n\nLet \"H\" be a connected compact semisimple Lie group, σ an automorphism of \"H\" of order 2 and \"H\" the fixed point subgroup of σ. Let \"K\" be a closed subgroup of \"H\" lying between \"H\" and its identity component. The compact homogeneous space \"H\" / \"K\" is called a symmetric space of compact type. The Lie algebra formula_1 admits a decomposition\n\nwhere formula_3, the Lie algebra of \"K\", is the +1 eigenspace of σ and formula_4 the –1 eigenspace.\nIf formula_3 contains no simple summand of formula_1, the pair (formula_1, σ) is called an orthogonal symmetric Lie algebra of \"compact type\".\n\nAny inner product on formula_1, invariant under the adjoint representation and σ, induces a Riemannian structure on \"H\" / \"K\", with \"H\" acting by isometries. A canonical example is given by minus the Killing form. Under such an inner product, formula_3 and formula_4 are orthogonal. \"H\" / \"K\" is then a Riemannian symmetric space of compact type.\n\nThe symmetric space \"H\" / \"K\" is called a Hermitian symmetric space if it has an almost complex structure preserving the Riemannian metric. This is equivalent to the existence of a linear map \"J\" with \"J\" = −\"I\" on formula_4 which preserves the inner product and commutes with the action of \"K\".\n\nIf (formula_1,σ) is Hermitian, \"K\" has non-trivial center and the symmetry σ is inner, implemented by an element of the center of \"K\".\n\nIn fact \"J\" lies in formula_3 and exp \"tJ\" forms a one-parameter group in the center of \"K\". This follows because if \"A\", \"B\", \"C\", \"D\" lie in formula_4, then by the invariance of the inner product on formula_1\n\nReplacing \"A\" and \"B\" by \"JA\" and \"JB\", it follows that\n\nDefine a linear map δ on formula_1 by extending \"J\" to be 0 on formula_3. The last relation shows that δ is a derivation of formula_1. Since formula_1 is semisimple, δ must be an inner derivation, so that\n\nwith \"T\" in formula_3 and \"A\" in formula_4. Taking \"X\" in formula_3, it follows that \"A\" = 0 and \"T\" lies in the center of formula_3 and hence that \"K\" is non-semisimple. The symmetry σ is implemented by \"z\" = exp π\"T\" and the almost complex structure by exp π/2 \"T\".\n\nThe innerness of σ implies that \"K\" contains a maximal torus of \"H\", so has maximal rank. On the other hand, the centralizer of the subgroup generated by the torus \"S\" of elements exp \"tT\" is connected, since if \"x\" is any element in \"K\" there is a maximal torus containing \"x\" and \"S\", which lies in the centralizer. On the other hand, it contains \"K\" since \"S\" is central in \"K\" and is contained in \"K\" since \"z\" lies in \"S\". So \"K\" is the centralizer of \"S\" and hence connected. In particular \"K\" contains the center of \"H\".\n\nThe symmetric space or the pair (formula_1, σ) is said to be \"irreducible\" if the adjoint action of formula_3 (or equivalently the identity component of \"H\" or \"K\") is irreducible on formula_4. This is equivalent to the maximality of formula_3 as a subalgebra.\n\nIn fact there is a one-one correspondence between intermediate subalgebras formula_31 and \"K\"-invariant subspaces \nformula_32 of formula_4 given by\n\nAny orthogonal symmetric algebra (formula_35, σ) of Hermitian type can be decomposed as an (orthogonal) direct sum of irreducible orthogonal symmetric algebras of Hermitian type.\n\nIn fact formula_1 can be written as a direct sum of simple algebras\n\neach of which is left invariant by the automorphism σ and the complex structure \"J\", since they are both inner. The eigenspace decomposition of formula_38 coincides with its intersections with formula_3 and formula_4. So the restriction of σ to formula_38 is irreducible.\n\nThis decomposition of the orthogonal symmetric Lie algebra yields a direct product decomposition of the corresponding compact symmetric space \"H\" / \"K\" when \"H\" is simply connected. In this case the fixed point subgroup \"H\" is automatically connected. For simply connected \"H\", the symmetric space \"H\" / \"K\" is the direct product of \"H\" / \"K\" with \"H\" simply connected and simple. In the irreducible case, \"K\" is a maximal connected subgroup of \"H\". Since \"K\" acts irreducibly on formula_4 (regarded as a complex space for the complex structure defined by \"J\"), the center of \"K\" is a one-dimensional torus T, given by the operators exp \"tT\". Since each \"H\" is simply connected and \"K\" connected, the quotient \"H\"/\"K\" is simply connected.\n\nif \"H\" / \"K\" is irreducible with \"K\" non-semisimple, the compact group \"H\" must be simple and \"K\" of maximal rank. From [[Borel-de Siebenthal theory]], the involution σ is inner and \"K\" is the centralizer of its center, which is isomorphic to T. In particular \"K\" is connected. It follows that \"H\" / \"K\" is simply connected and there is a [[parabolic subgroup]] \"P\" in the [[complexification (Lie group)|complexification]] \"G\" of \"H\" such that \"H\" / \"K\" = \"G\" / \"P\". In particular there is a complex structure on \"H\" / \"K\" and the action of \"H\" is holomorphic. Since any Hermitian symmetric space is a product of irreducible spaces, the same is true in general.\n\nAt the [[Lie algebra]] level, there is a symmetric decomposition\nwhere formula_44 is a real vector space with a complex structure \"J\", whose complex dimension is given in the table. Correspondingly, there is a [[graded Lie algebra]] decomposition\nwhere formula_46 is the decomposition into +\"i\" and −\"i\" eigenspaces of \"J\" and formula_47. The Lie algebra of \"P\" is the semidirect product formula_48. The complex Lie algebras formula_49 are Abelian. Indeed, if \"U\" and \"V\" lie in formula_49, [\"U\",\"V\"] = \"J\"[\"U\",\"V\"] = [\"JU\",\"JV\"] = [±\"iU\",±\"iV\"] = –[\"U\",\"V\"], so the Lie bracket must vanish.\n\nThe complex subspaces formula_49 of formula_52 are irreducible for the action of \"K\", since \"J\" commutes with \"K\" so that each is isomorphic to formula_4 with complex structure ±\"J\". Equivalently the centre T of \"K\" acts on formula_54 by the identity representation and on formula_55 by its conjugate.\n\nThe realization of \"H\"/\"K\" as a generalized flag variety \"G\"/\"P\" is obtained by taking \"G\" as in the table (the [[complexification (Lie group)|complexification]] of \"H\") and \"P\" to be the [[parabolic subgroup]] equal to the semidirect product of \"L\", the complexification of \"K\", with the complex Abelian subgroup exp formula_54. (In the language of [[algebraic groups]], \"L\" is the [[Levi factor]] of \"P\".)\n\nAny Hermitian symmetric space of compact type is simply connected and can be written as a direct product of irreducible hermitian symmetric spaces \"H\" / \"K\" with \"H\" simple, \"K\" connected of maximal rank with center T. The irreducible ones are therefore exactly the non-semisimple cases classified by [[Borel–de Siebenthal theory]].\nAccordingly, the irreducible compact Hermitian symmetric spaces \"H\"/\"K\" are classified as follows.\n\nIn terms of the classification of compact Riemannian symmetric spaces, the Hermitian symmetric spaces are the four infinite series AIII, DIII, CI and BDI with \"p\" = 2 or \"q\" = 2, and two exceptional spaces, namely EIII and EVII.\n\nThe irreducible Hermitian symmetric spaces of compact type are all simply connected. The corresponding symmetry σ of the simply connected simple compact Lie group is inner, given by conjugation by the unique element \"S\" in \"Z\"(\"K\") / \"Z\"(\"H\") of period 2. For the classical groups, as in the table above, these symmetries are as follows:\n\n\nThe maximal parabolic subgroup \"P\" can be described explicitly in these classical cases. For AIII\n\nin SL(\"p\"+\"q\",C). \"P\"(\"p\",\"q\") is the stabilizer of a subspace of dimension \"p\" in C.\n\nThe other groups arise as fixed points of involutions. Let \"J\" be the \"n\" × \"n\" matrix with 1's on the antidiagonal and 0's elsewhere and set\n\nThen Sp(\"n\",C) is the fixed point subgroup of the involution θ(\"g\") = \"A\" (\"g\") \"A\" of SL(2\"n\",C). SO(\"n\",C) can be realised as the fixed points of ψ(\"g\") = \"B\" (\"g\") \"B\" in SL(\"n\",C) where \"B\" = \"J\". These involutions leave invariant \"P\"(\"n\",\"n\") in the cases DIII and CI and \"P\"(\"p\",2) in the case BDI. The corresponding parabolic subgroups \"P\" are obtained by taking the fixed points. The compact group \"H\" acts transitively on \"G\" / \"P\", so that \"G\" / \"P\" = \"H\" / \"K\".\n\nAs with symmetric spaces in general, each compact Hermitian symmetric space \"H\"/\"K\" has a noncompact dual \"H\"/\"K\" obtained by replacing \"H\" with the closed real Lie subgroup \"H\" of the complex Lie group \"G\" with Lie algebra\n\nWhereas the natural map from \"H\"/\"K\" to \"G\"/\"P\" is an isomorphism, the natural map from \"H\"/\"K\" to \"G\"/\"P\" is only an inclusion onto an open subset. This inclusion is called the Borel embedding after [[Armand Borel]]. In fact \"P\" ∩ \"H\" = \"K\" = \"P\" ∩ \"H\"*. The images of \"H\" and \"H\"* have the same dimension so are open. Since the image of \"H\" is compact, so closed, it follows that \"H\"/\"K\" = \"G\"/\"P\".\n\nThe polar decomposition in the complex linear group \"G\" implies the Cartan decomposition \"H\"* = \"K\" ⋅ exp formula_63 in \"H\"*.\n\nMoreover, given a maximal Abelian subalgebra formula_64 in t, \"A\" = exp formula_64 is a toral subgroup such that σ(\"a\") = \"a\" on \"A\"; and any two such formula_64's are conjugate by an element of \"K\". A similar statement holds for formula_67. Morevoer if \"A\"* = exp formula_68, then\n\nThese results are special cases of the Cartan decomposition in any Riemannian symmetric space and its dual. The geodesics emanating from the origin in the homogeneous spaces can be identified with one parameter groups with generators in formula_63 or formula_4. Similar results hold for in the compact case: \"H\"= \"K\" ⋅ exp formula_63 and \"H\" = \"KAK\".\n\nThe properties of the [[totally geodesic]] subspace \"A\" can be shown directly. \"A\" is closed because the closure of \"A\" is a toral subgroup satisfying σ(\"a\") = \"a\", so its Lie algebra lies in formula_4 and hence equals formula_64 by maximality. \"A\" can be generated topologically by a single element exp \"X\", so formula_64 is the centralizer of \"X\" in formula_4. In the \"K\"-orbit of any element of formula_4 there is an element \"Y\" such that (X,Ad \"k\" Y) is minimized at \"k\" = 1. Setting \"k\" = exp \"tT\" with \"T\" in formula_3, it follows that (\"X\",[\"T\",\"Y\"]) = 0 and hence [\"X\",\"Y\"] = 0, so that \"Y\" must lie in formula_64. Thus formula_4 is the union of the conjugates of formula_64. In particular some conjugate of \"X\" lies in any other choice of formula_64, which centralizes that conjugate; so by maximality the only possibilities are conjugates of formula_64.\nThe decompositions\n\ncan be proved directly by applying the [[slice theorem (differential geometry)|slice theorem]] for [[transformation group|compact transformation groups]] to the action of \"K\" on \"H\" / \"K\". In fact the space \"H\" / \"K\" can be identified with\n\na closed submanifold of \"H\", and the Cartan decomposition follows by showing that \"M\" is the union of the \"kAk\" for \"k\" in \"K\". Since this union is the continuous image of \"K\" × \"A\", it is compact and connected. So it suffices to show that the union is open in \"M\" and for this it is enough to show each \"a\" in \"A\" has an open neighbourhood in this union. Now by computing derivatives at 0, the union contains an open neighbourhood of 1. If \"a\" is central the union is invariant under multiplication by \"a\", so contains an open neighbourhood of \"a\". If \"a\" is not central, write \"a\" = \"b\" with \"b\" in \"A\". Then τ = Ad \"b\" − Ad \"b\" is a skew-adjoint operator on formula_1 anticommuting with σ, which can be regarded as a Z-grading operator σ on formula_1. By an [[Euler–Poincaré characteristic]] argument it follows that the superdimension of formula_1 coincides with the superdimension of the kernel of τ. In other words,\n\nwhere formula_90 and formula_91 are the subspaces fixed by Ad \"a\". Let the orthogonal complement of formula_90 in formula_3 be formula_94. Computing derivstives it follows that Ad \"e\" (\"a\" \"e\"), where \"X\" lies in formula_94 and \"Y\" in formula_91, is an open neighbourhood of \"a\" in the union. Here the terms \"a\" \"e\" lie in the union by the argument for central \"a\": indeed \"a\" is in the center of the identity component of the centralizer of \"a\" which is invariant under σ and contains \"A\".\n\nThe dimension of formula_64 is called the rank of the Hermitian symmetric space.\n\nIn the case of Hermitian symmetric spaces, Harish-Chandra gave a canonical choice for formula_64. \nThis choice of formula_64 is determined by taking a maximal torus \"T\" of \"H\" in \"K\" with Lie algebra formula_100. Since the symmetry σ is implemented by an element of \"T\" lying in the centre of \"H\", the root spaces formula_101 in formula_35 are left invariant by σ. It acts as the identity on those contained in formula_103 and minus the identity on those in formula_104.\n\nThe roots with root spaces in formula_103 \nare called compact roots and those with root spaces in formula_104 are called noncompact roots. (This terminology originates from the symmetric space of noncompact type.) If \"H\" is simple, the generator \"Z\" of the centre of \"K\" can be used to define a set of positive roots, according to the sign of α(\"Z\"). With this choice of roots formula_54 and formula_55 are the direct sum of the root spaces formula_101 over positive and negative noncompact roots α. Root vectors \"E\" can be chosen so that\n\nlie in formula_1. The simple roots α, ..., α are the indecomposable positive roots. These can be numbered so that α vanishes on the center of formula_1 for \"i\", whereas α does not. Thus α is the unique noncompact simple root and the other simple roots are compact. Any positive noncompact root then has the form β = α + \"c\" α + ⋅⋅⋅ + \"c\" α with non-negative coefficients \"c\". These corfficients lead to a [[lexicographic order]] on positive roots. The coefficient of α is always one because formula_55 is irreducible for \"K\" so is spanned by vectors obtained by successively applying the lowering operators \"E\" for simple compact roots α.\n\nTwo roots α and β are said to be strongly orthogonal if ±α ±β are not roots or zero, written α ≐ β. The highest positive root ψ is noncompact. Take ψ to be the highest noncompact positive root strongly orthogonal to ψ (for the lexicographic order). Then continue in this way taking ψ to be the highest noncompact positive root strongly orthogonal to ψ, ..., ψ until the process terminates. The corresponding vectors\n\nlie in formula_4 and commute by strong orthogonality. Their span formula_64 is Harish-Chandra's canonical maximal Abelian subalgebra. (As Sugiura later showed, having fixed \"T\", the set of strongly orthogonal roots is uniquely determined up to applying an element in the Weyl group of \"K\".)\n\nMaximality can be checked by showing that if\n\nfor all \"i\", then \"c\" = 0 for all positive noncompact roots α different from the ψ's. This follows by showing inductively that if \"c\" ≠ 0, then α is strongly orthogonal to ψ, ψ, ... a contradiction. Indeed, the above relation shows ψ + α cannot be a root; and that if ψ – α is a root, then it would necessarily have the form β – ψ. If ψ – α were negative, then α would be a higher positive root than ψ, strongly orthogonal to the ψ with \"j\" < \"i\", which is not possible; similarly if β – ψ were positive.\n\nHarish-Chandra's canonical choice of formula_64 leads to a polydisk and polysphere theorem in \"H\"*/\"K\" and \"H\"/\"K\". This result reduces the geometry to products of the prototypic example involving SL(2,C), SU(1,1) and SU(2), namely the unit disk inside the Riemann sphere.\n\nIn the case of \"H\" = SU(2) the symmetry σ is given by conjugation by the diagonal matrix with entries ±\"i\" so that\n\nThe fixed point subgroup is the maximal torus \"T\", the diagonal matrices with entries \"e\". SU(2) acts on the Riemann sphere formula_120 transitively by Möbius transformations and \"T\" is the stabilizer of 0. SL(2,C), the complexification of SU(2), also acts by Möbius transformations and the stabiliser of 0 is the subgroup \"B\" of lower triangular matrices. The noncompact subgroup SU(1,1) acts with precisely three orbits: the open unit disk |\"z\"| < 1; the unit circle \"z\" = 1; and its exterior |\"z\"| > 1. Thus\n\nwhere \"B\" and \"T\" denote the subgroups of upper triangular and diagonal matrices in SL(2,C). The middle term is the orbit of 0 under the upper unitriangular matrices\n\nNow for each root ψ there is a homomorphism of π of SU(2) into \"H\" which is compatible with the symmetries. It extends uniquely to a homomorphism of SL(2,C) into \"G\". The images of the Lie algebras for different ψ's since they are strongly orthogonal. Thus there is a homomorphism π of the direct product SU(2) into \"H\" compatible with the symmetries. It extends to a homomorphism of SL(2,C) into \"G\". The kernel of π is contained in the center (±1) of SU(2) which is fixed pointwise by the symmetry. So the image of the center under π lies in \"K\". Thus there is an embedding of the polysphere (SU(2)/T) into \"H\" / \"K\" = \"G\" / \"P\" and the polysphere contains the polydisk (SU(1,1)/T). The polysphere and polydisk are the direct product of \"r\" copies of the Riemann sphere and the unit disk. By the Cartan decompositions in SU(2) and SU(1,1), \nthe polysphere is the orbit of \"T\"\"A\" in \"H\" / \"K\" and the polydisk is the orbit of \"T\"\"A\"*, where \"T\" = π(T) ⊆ \"K\". On the other hand, \"H\" = \"KAK\" and \"H\"* = \"K\" \"A\"* \"K\".\n\nHence every element in the compact Hermitian symmetric space \"H\" / \"K\" is in the \"K\"-orbit of a point in the polysphere; and every element in the image under the Borel embedding of the noncompact Hermitian symmetric space \"H\"* / \"K\" is in the \"K\"-orbit of a point in the polydisk.\n\n\"H\"* / \"K\", the Hermitian symmetric space of noncompact type, lies in the image of formula_123, a dense open subset of \"H\" / \"K\" biholomorphic to formula_124. The corresponding domain in formula_124 is bounded. This is the Harish-Chandra embedding named after [[Harish-Chandra]]. \nIn fact Harish-Chandra showed the following properties of the space formula_126:\n\n\nIn fact formula_128 are complex Abelian groups normalised by \"K\". Moreover, formula_129 since formula_130.\n\nThis implies \"P\" ∩ \"M\" = {1}. For if \"x\" = \"e\" with \"X\" in \nformula_54 lies in \"P\", it must normalize \"M\" and hence formula_55. But if \"Y\" lies in formula_55, then\n\nso that \"X\" commutes with formula_55. But if \"X\" commutes with every noncompact root space, it must be 0, so \"x\" = 1. It follows that the multiplication map μ on \"M\" × \"P\" is injective so (1) follows. Similarly the derivative of μ at (\"x\",\"p\") is\n\nwhich is injective, so (2) follows. For the special case \"H\" = SU(2), \"H\"* = SU(1,1) and \"G\" = SL(2,C) the remaining assertions are consequences of the identification with the Riemann sphere, C and unit disk. They can be applied to the groups defined for each root ψ. By the polysphere and polydisk theorem \"H\"*/\"K\", X/\"P\" and \"H\"/\"K\" are the union of the \"K\"-translates of the polydisk, C and the polysphere. So \"H\"* lies in X, the closure of \"H\"*/\"K\" is compact in X/\"P\", which is in turn dense in \"H\"/\"K\".\n\nNote that (2) and (3) are also consequences of the fact that the image of \"X\" in \"G\"/\"P\" is that of the big cell \"B\"\"B\" in the [[Complexification (Lie group)#Gauss decomposition|Gauss decomposition]] of \"G\".\n\nUsing results on the [[restricted root system]] of the symmetric spaces \"H\"/\"K\" and \"H\"*/\"K\", \n[[Robert Hermann (mathematician)|Hermann]] showed that the image of \"H\"*/\"K\" in formula_54 is a generalized unit disk. In fact it is the [[convex set]] of \"X\" for which the [[operator norm]] of ad Im \"X\" is less than one.\n\nA bounded domain \"Ω\" in a complex vector space is said to be a bounded symmetric domain if for every \"x\" in \"Ω\", there is an involutive biholomorphism \"σ\" of \"Ω\" for which \"x\" is an isolated fixed point. The Harish-Chandra embedding exhibits every Hermitian symmetric space of noncompact type \"H\"* / \"K\" as a bounded symmetric domain. The biholomorphism group of \"H\" / \"K\" is equal to its isometry group \"H\".\n\nConversely every bounded symmetric domain arises in this way. Indeed, given a bounded symmetric domain \"Ω\", the [[Bergman kernel]] defines a [[Riemannian metric|metric]] on \"Ω\", the [[Bergman metric]], for which every biholomorphism is an isometry. This realizes \"Ω\" as a Hermitian symmetric space of noncompact type.\n\nThe irreducible bounded symmetric domains are called Cartan domains and are classified as follows.\n\nIn the classical cases (I–IV), the noncompact group can be realized by 2 × 2 block matrices\n\nacting by generalized [[Möbius transformation]]s\n\nThe polydisk theorem takes the following concrete form in the classical cases:\n\nThe noncompact group \"H\"* acts on the complex Hermitian symmetric space \"H\"/\"K\" = \"G\"/\"P\" with only finitely many orbits. The orbit structure is described in detail in . In particular the closure of the bounded domain \"H\"*/\"K\" has a unique closed orbit, which is the [[Shilov boundary]] of the domain. In general the orbits are unions of Hermitian symmetric spaces of lower dimension. The complex function theory of the domains, in particular the analogue of the [[Cauchy integral formula]]s, are described for the Cartan domains in . The closure of the bounded domain is the [[Baily–Borel compactification]] of \"H\"*/\"K\".\n\nThe boundary structure can be described using [[Cayley transform]]s. For each copy of SU(2) defined by one of the noncompact roots ψ, there is a Cayley transform \"c\" which as a Möbius transformation maps the unit disk onto the upper half plane. Given a subset \"I\" of indices of the strongly orthogonal family ψ, ..., ψ, the \"partial Cayley transform\" \"c\" is defined as the product of the \"c\"'s with \"i\" in \"I\" in the product of the groups π. Let \"G\"(\"I\") be the centralizer of this product in \"G\" and \"H\"*(\"I\") = \"H\"* ∩ \"G\"(\"I\"). Since σ leaves \"H\"*(\"I\") invariant, there is a corresponding Hermitian symmetric space \"M\" \"H\"*(\"I\")/\"H\"*(\"I\")∩\"K\" ⊂ \"H\"*/\"K\" = \"M\" . The boundary component for the subset \"I\" is the union of the \"K\"-translates of \"c\" \"M\". When \"I\" is the set of all indices, \"M\" is a single point and the boundary component is the Shilov boundary. Moreover, \"M\" is in the closure of \"M\" if and only if \"I\" ⊇ \"J\".\n\nEvery Hermitian symmetric space is a [[Kähler manifold]]. They can be defined equivalently as Riemannian symmetric spaces with a parallel complex structure with respect to which the Riemannian metric is [[Hermitian metric|Hermitian]]. The complex structure is automatically preserved by the isometry group \"H\" of the metric, and so any Hermitian symmetric space \"M\" is a homogeneous complex manifold. Some examples are [[complex vector space]]s and [[complex projective space]]s, with their usual Hermitian metrics and [[Fubini–Study metric]]s, and the complex [[unit ball]]s with suitable metrics so that they become [[Complete metric space|complete]] and Riemannian symmetric. The [[compact space|compact]] Hermitian symmetric spaces are [[projective variety|projective varieties]], and admit a strictly larger [[Lie group]] \"G\" of [[biholomorphism]]s with respect to which they are homogeneous: in fact, they are [[generalized flag manifold]]s, i.e., \"G\" is [[semisimple Lie group|semisimple]] and the stabilizer of a point is a [[parabolic subgroup]] \"P\" of \"G\". Among (complex) generalized flag manifolds \"G\"/\"P\", they are characterized as those for which the [[Nilradical of a Lie algebra|nilradical]] of the Lie algebra of \"P\" is abelian. The non-compact Hermitian symmetric spaces can be realized as bounded [[domain (mathematics)|domains]] in complex vector spaces.\n\nAlthough the classical Hermitian symmetric spaces can be constructed by ad hoc methods, [[Jordan triple system]]s, or equivalently Jordan pairs, provide a uniform algebraic means of describing all the basic properties connected with a Hermitian symmetric space of compact type and its non-compact dual. This theory is described in detail in and and summarized in . The development is in the reverse order from that using the structure theory of compact Lie groups. It starting point is the Hermitian symmetric space of noncompact type realized as a bounded symmetric domain. It can be described in terms of a [[Jordan pair]] or hermitian [[Jordan triple system]]. This Jordan algebra structure can be used to reconstruct the dual Hermitian symmetric space of compact type, including in particular all the associated Lie algebras and Lie groups.\n\nThe theory is easiest to describe when the irreducible compact Hermitian symmetric space is of tube type. In that case the space is determined by a simple real Lie algebra formula_35\nwith negative definite Killing form. It must admit an action of SU(2) which only acts via the trivial and adjoint representation, both types occurring. Since formula_35 is simple, this action is inner, so implemented by an inclusion of the Lie algebra of SU(2) in formula_35. The complexification of formula_35 decomposes as a direct sum of three eigenspaces for the diagonal matrices in SU(2). It is a three-graded complex Lie algebra, with the Weyl group element of SU(2) providing the involution. Each of the ±1 eigenspaces has the structure of a unital complex Jordan algebra explicitly arising as the complexification of a Euclidean Jordan algebra. It can be identified with the multiplicity space of the adjoint representation of SU(2) in formula_35.\n\nThe description of irreducible Hermitian symmetric spaces of tube type starts from a simple Euclidean Jordan algebra \"E\". It admits [[Jordan frame (Jordan algebra)|Jordan frames]], i.e. sets of orthogonal minimal idempotents \"e\", ..., \"e\". Any two are related by an automorphism of \"E\", so that the integer \"m\" is an invariant called the rank of \"E\". Moreover, if \"A\" is the complexification of \"E\", it has a unitary [[structure group (Jordan algebra)|structure group]]. It is a subgroup of GL(\"A\") preserving the natural complex inner product on \"A\". Any element \"a\" in \"A\" has a polar decomposition with . The spectral norm is defined by ||a|| = sup α. The associated [[bounded symmetric domain]] is just the open unit ball \"D\" in \"A\". There is a biholomorphism between \"D\" and the tube domain \"T\" = \"E\" + \"iC\" where \"C\" is the open self-dual convex cone of elements in \"E\" of the form with \"u\" an automorphism of \"E\" and α > 0. This gives two descriptions of the Hermitian symmetric space of noncompact type. There is a natural way of using [[mutation (Jordan algebra)|mutations]] of the Jordan algebra \"A\" to compactify the space \"A\". The compactification \"X\" is a complex manifold and the finite-dimensional Lie algebra formula_35 of holomorphic vector fields on \"X\" can be determined explicitly. One parameter groups of biholomorphisms can be defined such that the corresponding holomorphic vector fields span formula_35. This includes the group of all complex Möbius transformations corresponding to matrices in SL(2,C). The subgroup SU(1,1) leaves invariant the unit ball and its closure. The subgroup SL(2,R) leaves invariant the tube domain and its closure. The usual Cayley transform and its inverse, mapping the unit disk in C to the upper half plane, establishes analogous maps between \"D\" and \"T\". The polydisk corresponds to the real and complex Jordan subalgebras generated by a fixed Jordan frame. It admits a transitive action of SU(2) and this action extends to \"X\". The group \"G\" generated by the one-parameter groups of biholomorphisms acts faithfully on formula_35. The subgroup generated by the identity component \"K\" of the unitary structure group and the operators in SU(2). It defines a compact Lie group \"H\" which acts transitively on \"X\". Thus \"H\" / \"K\" is the corresponding Hermitian symmetric space of compact type. The group \"G\" can be identified with the [[complexification (Lie group)|complexification]] of \"H\". The subgroup \"H\"* leaving \"D\" invariant is a noncompact real form of \"G\". It acts transitively on \"D\" so that \"H\"* / \"K\" is the dual Hermitian symmetric space of noncompact type. The inclusions \"D\" ⊂ \"A\" ⊂ \"X\" reproduce the Borel and Harish-Chandra embeddings. The classification of Hermitian symmetric spaces of tube type reduces to that of simple Euclidean Jordan algebras. These were classified by in terms of [[Euclidean Hurwitz algebra]]s, a special type of [[composition algebra]].\n\nIn general a Hermitian symmetric space gives rise to a 3-graded Lie algebra with a period 2 conjugate linear automorphism switching the parts of degree ±1 and preserving the degree 0 part. This gives rise to the structure of a [[Jordan pair]] or hermitian [[Jordan triple system]], to which extended the theory of Jordan algebras. All irreducible Hermitian symmetric spaces can be constructed uniformly within this framework. constructed the irreducible Hermitian symmetric space of non-tube type from a simple Euclidean Jordan algebra together with a period 2 automorphism. The −1 eigenspace of the automorphism has the structure of a Jordan pair, which can be deduced from that of the larger Jordan algebra. In the non-tube type case corresponding to a [[Siegel domain]] of type II, there is no distinguished subgroup of real or complex Möbius transformations. For irreducible Hermitian symmetric spaces, tube type is characterized by the real dimension of the Shilov boundary being equal to the complex dimension of .\n\n\n\n[[Category:Differential geometry]]\n[[Category:Complex manifolds]]\n[[Category:Riemannian geometry]]\n[[Category:Lie groups]]\n[[Category:Homogeneous spaces]]"}
{"id": "320319", "url": "https://en.wikipedia.org/wiki?curid=320319", "title": "How to Solve It", "text": "How to Solve It\n\nHow to Solve It (1945) is a small volume by mathematician George Pólya describing methods of problem solving.\n\n\"How to Solve It\" suggests the following steps when solving a mathematical problem:\n\nIf this technique fails, Pólya advises: \"If you can't solve a problem, then there is an easier problem you can solve: find it.\" Or: \"If you cannot solve the proposed problem, try to solve first some related problem. Could you imagine a more accessible related problem?\"\n\n\"Understand the problem\" is often neglected as being obvious and is not even mentioned in many mathematics classes. Yet students are often stymied in their efforts to solve it, simply because they don't understand it fully, or even in part. In order to remedy this oversight, Pólya taught teachers how to prompt each student with appropriate questions, depending on the situation, such as:\n\n\nThe teacher is to select the question with the appropriate level of difficulty for each student to ascertain if each student understands at their own level, moving up or down the list to prompt each student, until each one can respond with something constructive.\n\nPólya mentions that there are many reasonable ways to solve problems. The skill at choosing an appropriate strategy is best learned by solving many problems. You will find choosing a strategy increasingly easy. A partial list of strategies is included:\n\n\nAlso suggested:\n\n\nThis step is usually easier than devising the plan. In general, all you need is care and patience, given that you have the necessary skills. Persist with the plan that you have chosen. If it continues not to work, discard it and choose another. Don't be misled; this is how mathematics is done, even by professionals.\n\nPólya mentions that much can be gained by taking the time to reflect and look back at what you have done, what worked and what didn't. Doing this will enable you to predict what strategy to use to solve future problems, if these relate to the original problem.\n\nThe book contains a dictionary-style set of heuristics, many of which have to do with generating a more accessible problem. For example:\n\n\n\n"}
{"id": "46473082", "url": "https://en.wikipedia.org/wiki?curid=46473082", "title": "Intertemporal budget constraint", "text": "Intertemporal budget constraint\n\nIn economics and finance, an intertemporal budget constraint is a constraint faced by a decision maker who is making choices for both the present and the future. In its general form it says that the present value of current and future cash outflows cannot exceed the present value of currently available funds and future cash inflows. Typically this is expressed as\n\nwhere formula_2 is expenditure at time \"t\", formula_3 is the cash that becomes available at time \"t\", \"T\" is the most distant relevant time period, 0 is the current time period, and formula_4 is the discount factor computed from the interest rate \"r\".\n\nComplications are possible in various circumstances. For example, the interest rate for discounting cash receipts might be greater than the interest rate for discounting expenditures, because future inflows may be borrowed against while currently available funds may be invested temporarily pending use for future expenditures, and borrowing rates may exceed investment returns.\n\nIn most applications, the entire budget would be used up, because any unspent funds would represent unobtained potential utility. In these situations, the intertemporal budget constraint is effectively an equality constraint.\n\nIn an intertemporal consumption model, the sum of utilities from expenditures made at various times in the future, these utilities discounted back to the present at the consumer's rate of time preference, would be maximized with respect to the amounts \"x\" consumed in each period, subject to an intertemporal budget constraint.\n\nIn a model of intertemporal portfolio choice, the objective would be to maximize the expected value or expected utility of final period wealth. Since investment returns in each period generally would not be known in advance, the constraint effectively imposes a limit on the amount that can be invested in the final period—namely, whatever the wealth accumulated as of the end of the next-to-last period is.\n\n"}
{"id": "1057467", "url": "https://en.wikipedia.org/wiki?curid=1057467", "title": "James Rumbaugh", "text": "James Rumbaugh\n\nJames E. Rumbaugh (born August 22, 1947) is an American computer scientist and object-oriented methodologist who is best known for his work in creating the Object Modeling Technique (OMT) and the Unified Modeling Language (UML).\n\nBorn in Bethlehem, Pennsylvania, Rumbaugh received a B.S. in physics from the Massachusetts Institute of Technology (MIT), an M.S. in astronomy from the California Institute of Technology (Caltech), and received a Ph.D. in computer science from MIT under Professor Jack Dennis.\n\nRumbaugh started his career in the 1960s at Digital Equipment Corporation (DEC) as a lead research scientist. From 1968 to 1994 he worked at the General Electric Research and Development Center developing technology, teaching, and consulting. At General Electric he also led the development of Object-modeling technique (OMT), an object modeling language for software modeling and designing.\n\nIn 1994, he joined Rational Software, where he worked with Ivar Jacobson and Grady Booch (\"the Three Amigos\") to develop Unified Modeling Language (UML). Later they merged their software development methologies, OMT, OOSE and Booch into the Rational Unified Process (RUP). In 2003 he moved to IBM, after its acquisition of Rational Software. He retired in 2006.\n\nHe has two grown up sons and (in 2009) lived in Saratoga, California with his wife.\n\nRumbaugh's main research interests are formal description languages, \"semantics of computation, tools for programming productivity, and applications using complex algorithms and data structures\".\n\nIn his graduate work at MIT, Rumbaugh contributed to the development of data flow computer architecture. His thesis described parallel programming language, parallel processor computer and a basis for a network architecture, which orients itself at data flow. Rumbaugh made further contributions to Object Modeling Technique, IDEF4, the Rational Unified Process and Unified Modeling Language.\n\nRumbaugh has written a number of books about UML and RUP together with Ivar Jacobson and Grady Booch. A selection includes:\n\n\n"}
{"id": "29899232", "url": "https://en.wikipedia.org/wiki?curid=29899232", "title": "KR advantage", "text": "KR advantage\n\nIn cryptography, the key-recovery advantage (KR advantage) of a particular algorithm is a measure of how effective an algorithm can mount a key-recovery attack. Consequently, the maximum key-recovery advantage attainable by any algorithm with a fixed amount of computational resources is a measure of how difficult it is to recover a cipher's key. It is defined as the probability that the adversary algorithm can guess a cipher's randomly selected key, given a fixed amount of computational resources. An extremely low KR advantage is essential for an encryption scheme's security.\n\n"}
{"id": "32744656", "url": "https://en.wikipedia.org/wiki?curid=32744656", "title": "L'association femmes et mathématiques", "text": "L'association femmes et mathématiques\n\nL'association femmes et mathématiques (in English: Association of Women and Mathematics), created in 1987, is a voluntary association promoting women in scientific studies and research in general, and mathematics in particular. This organization currently has about 200 members, including university professors of math, math teachers, sociologists, philosophers and historians that are interested in the \"woman question\" in scientific domains.\n\nAccording to its mandate, its principle objectives are:\n\n\nIt specifically organizes a forum of young women mathematicians, as well as conferences on different topics related to its objectives. They hold regularly a general assembly, either in Paris or outside of the city, based on various themes. It also publishes an academic journal.\n\nThe association has its headquarters at the Maison des mathématiciens at l'Institut Henri Poincaré in Paris. It participates in different initiatives with other scholarly and professional societies, in particular the Société Mathématique de France(Mathematical Society of France), la Société de mathématiques appliquées et industrielles (Society of Applied and Industrial Maths), l'Association des professeurs de mathématiques de l'enseignement public (Association of Math Professors and Public Teachers) and l'Union des professeurs de spéciales.\n\nBoard of the Association for 2013:\n\n\n"}
{"id": "25745566", "url": "https://en.wikipedia.org/wiki?curid=25745566", "title": "Ludwig Burmester", "text": "Ludwig Burmester\n\nLudwig Ernst Hans Burmester (5 May 1840 – 20 April 1927) was a German kinematician and geometer.\n\nHis doctoral thesis \"Über die Elemente einer Theorie der Isophoten\" (About the elements of a theory of isophotes) concerned lines on a surface defined by light direction. After a period as a teacher in Łódź he became professor of synthetic geometry at Dresden where his growing interest in kinematics culminated in his \"Lehrbuch der Kinematik, Erster Band, Die ebene Bewegung\" (Textbook of Kinematics, First Volume, Planar Motion) of 1888, developing the approach to the theory of linkages introduced by Franz Reuleaux, whereby a planar mechanism was understood as a collection of Euclidean planes in relative motion with one degree of freedom. Burmester considered both the theory of planar kinematics and practically all actual mechanisms known in his time. In doing so, Burmester developed Burmester theory which applies projective geometry to the loci of points on planes moving in straight lines and in circles, where any motion may be understood in relation to four Burmester points.\n\nThe Burmester linkage of 1888 is a four bar linkage part of whose coupler curve is an approximately straight line (see also Watt's linkage).\n\nA French curve may also be known as a Burmester curve.\n\n"}
{"id": "22419757", "url": "https://en.wikipedia.org/wiki?curid=22419757", "title": "Name collision", "text": "Name collision\n\nThe term \"name collision\" refers to the nomenclature problem that occurs in computer programs when the same variable name is used for different things in two separate areas that are joined, merged, or otherwise go from occupying separate namespaces to sharing one. As with the collision of other identifiers, it must be resolved in some way for the new software (such as a mashup) to work right.\nProblems of name collision, and methods to avoid them, are a common issue in an introductory level analysis of computer languages, such as for C++.\nThe term \"name collision\" has been used in computer science for more than three decades, when referring to names in various classification systems.\n\nThere are several techniques for avoiding name collisions, including the use of:\nThose are some of the tactics used to resolve the name collisions.\n\n"}
{"id": "20688523", "url": "https://en.wikipedia.org/wiki?curid=20688523", "title": "Napkin folding problem", "text": "Napkin folding problem\n\nThe napkin folding problem is a problem in geometry and the mathematics of paper folding that explores whether folding a square or a rectangular napkin can increase its perimeter. The problem is known under several names, including the Margulis napkin problem, suggesting it is due to Grigory Margulis, and the Arnold's rouble problem referring to Vladimir Arnold and the folding of a Russian ruble bank note. Some versions of the problem were solved by Robert J. Lang, Svetlana Krat, Alexey S. Tarasov, and Ivan Yaschenko. One form of the problem remains open.\n\nThere are several way to define the notion of folding, giving different interpretations. By convention, the napkin is always a unit square.\n\nConsidering the folding as a reflection along a line that reflects all the layers of the napkin, the perimeter is always non-increasing, thus never exceeding 4.\n\nBy considering more general foldings that possibly reflects only a single layer of the napkin (in this case, each folding is a reflection of a connected component of folded napkin on one side of a straight line), it still open if a sequence of these foldings can increase the perimeter. In other words, it still unknown if exists a solution that can be folded using some combination of mountain folds, valley folds, reverse folds, and/or sink folds (with all folds in the latter two cases being formed along a single line). Also unknown, of course, is whether such a fold would be possible using the more-restrictive pureland origami.\n\nOne can ask for a realizable construction within the constraints of rigid origami where the napkin is never stretched whilst being folded. In 2004 A. Tarasov showed that such constructions can indeed be obtained. This can be considered a complete solution to the original problem.\n\nOne can ask whether there exists a folded planar napkin (without regard as to how it was folded into that shape). \n\nRobert J. Lang showed in 1997 that several classical origami constructions give rise to an easy solution. In fact, Lang showed that the perimeter can be made as large as desired by making the construction more complicated, while still resulting in a flat folded solution. However his constructions are not necessarily rigid origami because of their use of sink folds and related forms. Although no stretching is needed in sink and unsink folds, it is often (though not always) necessary to curve facets and/or sweep one or more creases continuously through the paper in intermediate steps before obtaining a flat result. Whether a general rigidly foldable solution exists based on sink folds is an open problem.\n\nIn 1998, I. Yaschenko constructed a 3D folding with projection onto a plane which has a bigger perimeter. This indicated to mathematicians that there was probably a flat folded solution to the problem.\n\nThe same conclusion was made by Svetlana Krat. Her approach is different, she gives very simple construction of a \"rumpling\" which increase perimeter and then proves that any \"rumpling\" can be arbitrarily well approximated by a \"folding\". In essence she shows that the precise details of the how to do the folds don't matter much if stretching is allowed in intermediate steps.\n\nLang devised two different solutions. Both involved sinking flaps and so were not necessarily rigidly foldable. The simplest was based on the origami bird base and gave a solution with a perimeter of about 4.12 compared to the original perimeter of 4.\n\nThe second solution can be used to make a figure with a perimeter as large as desired. He divides the square into a large number of smaller squares and employs the 'sea urchin' type origami construction described in his 1990 book, \"Origami Sea Life\". The crease pattern shown is the \"n\" = 5 case and can be used to produce a flat figure with 25 flaps, one for each of the large circles, and sinking is used to thin them. When very thin the 25 arms will give a 25 pointed star with a small center and a perimeter approaching \"N\"/(\"N\" − 1). In the case of \"N\" = 5 this is about 6.25, and the total length goes up approximately as \"N\".\n\nArnold states in his book that he formulated the problem in 1956, but the formulation was left intentionally vague. He called it 'the rumpled rouble problem', and it was the first of many interesting problems he set at seminars in Moscow over 40 years. In the West, it became known as Margulis napkin problem after Jim Propp's newsgroup posting in 1996. Despite attention, it received folklore status and its origin is often referred as \"unknown\".\n\n"}
{"id": "42245723", "url": "https://en.wikipedia.org/wiki?curid=42245723", "title": "Number: The Language of Science", "text": "Number: The Language of Science\n\nNumber: The Language of Science: A Critical Survey Written for the Cultured Non-Mathematician is a popular mathematics book written by Russian-American mathematician Tobias Dantzig. The original U.S. publication was by Macmillan in 1930. A second edition (third impression) was published in 1947 in Prague, Czechoslovakia by Melantrich Company. It recounts the history of mathematical ideas, and how they have evolved.\n\nThe book is divided into 12 chapters. There is an appendix of illustrations. The third edition of the book contains a separate section for essays, at the book's end.\n\n\n"}
{"id": "6108552", "url": "https://en.wikipedia.org/wiki?curid=6108552", "title": "Ordinal notation", "text": "Ordinal notation\n\nIn mathematical logic and set theory, an ordinal notation is a partial function from the set of all finite sequences of symbols from a finite alphabet to a countable set of ordinals, and a Gödel numbering is a function from the set of well-formed formulae (a well-formed formula is a finite sequence of symbols on which the ordinal notation function is defined) of some formal language to the natural numbers. This associates each wff with a unique natural number, called its Gödel number. If a Gödel numbering is fixed, then the subset relation on the ordinals induces an ordering on well-formed formulae which in turn induces a well-ordering on the subset of natural numbers. A recursive ordinal notation must satisfy the following two additional properties:\n\n\nThere are many such schemes of ordinal notations, including schemes by Wilhelm Ackermann, Heinz Bachmann, Wilfried Buchholz, Georg Cantor, Solomon Feferman, Gerhard Jäger, Isles, Pfeiffer, Wolfram Pohlers, Kurt Schütte, Gaisi Takeuti (called ordinal diagrams), Oswald Veblen. Stephen Cole Kleene has a system of notations, called Kleene's O, which includes ordinal notations but it is not as well behaved as the other systems described here.\n\nUsually one proceeds by defining several functions from ordinals to ordinals and representing each such function by a symbol. In many systems, such as Veblen's well known system, the functions are normal functions, that is, they are strictly increasing and continuous in at least one of their arguments, and increasing in other arguments. Another desirable property for such functions is that the value of the function is greater than each of its arguments, so that an ordinal is always being described in terms of smaller ordinals. There are several such desirable properties. Unfortunately, no one system can have all of them since they contradict each other.\n\nAs usual, we must start off with a constant symbol for zero, \"0\", which we may consider to be a function of arity zero. This is necessary because there are no smaller ordinals in terms of which zero can be described. The most obvious next step would be to define a unary function, \"S\", which takes an ordinal to the smallest ordinal greater than it; in other words, S is the successor function. In combination with zero, successor allows one to name any natural number.\n\nThe third function might be defined as one that maps each ordinal to the smallest ordinal that cannot yet be described with the above two functions and previous values of this function. This would map β to ω·β except when β is a fixed point of that function plus a finite number in which case one uses ω·(β+1).\n\nThe fourth function would map α to ω·α except when α is a fixed point of that plus a finite number in which case one uses ω·(α+1).\n\nOne could continue in this way, but it would give us an infinite number of functions. So instead let us merge the unary functions together into a binary function. By transfinite recursion on α, we can use transfinite recursion on β to define ξ(α,β) = the smallest ordinal γ such that α < γ and β < γ and γ is not the value of ξ for any smaller α or for the same α with a smaller β.\n\nThus, define ξ-notations as follows:\n\nξ is defined for all pairs of ordinals and is one-to-one. It always gives values larger than its arguments and its range is all ordinals other than 0 and the epsilon numbers (ε=ω).\n\nξ(α,β)<ξ(γ,δ) if and only if either (α=γ and β<δ) or (α<γ and β<ξ(γ,δ)) or (α>γ and ξ(α,β)≤δ).\n\nWith this definition, the first few ξ-notations are:\n\nIn general, ξ(0,β) = β+1. While ξ(1+α,β) = ω·(β+k) for k = 0 or 1 or 2 depending on special situations:<br>\nk = 2 if α is an epsilon number and β is finite.<br>\nOtherwise, k = 1 if β is a multiple of ω plus a finite number.<br>\nOtherwise, k = 0.\n\nThe ξ-notations can be used to name any ordinal less than ε with an alphabet of only two symbols (\"0\" and \"ξ\"). If these notations are extended by adding functions that enumerate epsilon numbers, then they will be able to name any ordinal less than the first epsilon number that cannot be named by the added functions. This last property, adding symbols within an initial segment of the ordinals gives names within that segment, is called repleteness (after Solomon Feferman).\n\nThere are many different systems for ordinal notation introduced by various authors. It is often quite hard to convert between the different systems.\n\n\"Exponential polynomials\" in 0 and ω gives a system of ordinal notation for ordinals less than epsilon zero. There are many equivalent ways to write these; instead of exponential polynomials, one can use rooted trees, or nested parentheses, or the system described above.\n\nThe 2-variable Veblen functions can be used to give a system of ordinal notation for ordinals less than the Feferman-Schutte ordinal. The Veblen functions in a finite or transfinite number of variables give systems of ordinal notations for ordinals less than the small and large Veblen ordinals.\n\n described a system of ordinal notation rather weaker than the system described earlier by Veblen. The limit of his system is sometimes called the Ackermann ordinal.\n\n introduced the key idea of using uncountable ordinals to produce new countable ordinals. His original system was rather cumbersome to use as it required choosing a special sequence converging to each ordinal. Later systems of notation introduced by Feferman and others avoided this complication.\n\n described a very powerful system of ordinal notation called \"ordinal diagrams\", which is hard to understand but was later simplified by Feferman.\n\nFeferman introduced theta functions, described in as follows. \nThe function for an ordinal α, θ is a function from ordinals to ordinals. \nOften θ(β) is written as θαβ. The set \"C\"(α,β) is defined by induction on α to be the set of ordinals that can be generated from 0, ω, ω, ..., ω, together with the ordinals less than β by the operations of ordinal addition and the functions θ for ξ<α. And the function θ is defined to be the function enumerating the ordinals δ with δ∉\"C\"(γ,δ).\n\n described the following system of ordinal notation as a simplification of Feferman's theta functions. Define:\nThe functions ψ(α) for α an ordinal, \"v\" an ordinal at most ω, are defined by induction on α as follows: \nwhere \"C\"(α) is the smallest set such that \n\nThis system has about the same strength as Fefermans system, as formula_2 for \"v\" ≤ ω.\n\n described a system of notation for all recursive ordinals (those less than the Church–Kleene ordinal). It uses a subset of the natural numbers instead of finite strings of symbols. Unfortunately, unlike the other systems described above there is in general no effective way to tell whether some natural number represents an ordinal, or whether two numbers represent the same ordinal. However, one can effectively find notations that represent the ordinal sum, product, and power (see ordinal arithmetic) of any two given notations in Kleene's formula_3; and given any notation for an ordinal, there is a recursively enumerable set of notations that contains one element for each smaller ordinal and is effectively ordered. Kleene's formula_3 denotes a canonical (and very non-computable) set of notations.\n\n\n"}
{"id": "21051205", "url": "https://en.wikipedia.org/wiki?curid=21051205", "title": "Perfectly orderable graph", "text": "Perfectly orderable graph\n\nIn graph theory, a perfectly orderable graph is a graph whose vertices can be ordered in such a way that a greedy coloring algorithm with that ordering optimally colors every induced subgraph of the given graph. Perfectly orderable graphs form a special case of the perfect graphs, and they include the chordal graphs, comparability graphs, and distance-hereditary graphs. However, testing whether a graph is perfectly orderable is NP-complete.\n\nThe greedy coloring algorithm, when applied to a given ordering of the vertices of a graph \"G\", considers the vertices of the graph in sequence and assigns each vertex its first available color, the minimum excluded value for the set of colors used by its neighbors. Different vertex orderings may lead this algorithm to use different numbers of colors. There is always an ordering that leads to an optimal coloring – this is true, for instance, of the ordering determined from an optimal coloring by sorting the vertices by their color – but it may be difficult to find.\nThe perfectly orderable graphs are defined to be the graphs for which there is an ordering that is optimal for the greedy algorithm not just for the graph itself, but for all of its induced subgraphs.\n\nMore formally, a graph \"G\" is said to be \"perfectly orderable\" if there exists an ordering π of the vertices of \"G\", such that every induced subgraph of \"G\" is optimally colored by the greedy algorithm using the subsequence of π induced by the vertices of the subgraph. An ordering π has this property exactly when there do not exist four vertices \"a\", \"b\", \"c\", and \"d\" for which \"abcd\" is an induced path, \"a\" appears before \"b\" in the ordering, and \"c\" appears after \"d\" in the ordering.\n\nPerfectly orderable graphs are NP-complete to recognize. However, it is easy to test whether a particular ordering is a perfect ordering of a graph. Consequently, it is also NP-hard to find a perfect ordering of a graph, even if the graph is already known to be perfectly orderable.\n\nEvery perfectly orderable graph is a perfect graph.\n\nChordal graphs are perfectly orderable; a perfect ordering of a chordal graph may be found by reversing a perfect elimination ordering for the graph. Thus, applying greedy coloring to a perfect ordering provides an efficient algorithm for optimally coloring chordal graphs. Comparability graphs are also perfectly orderable, with a perfect ordering being given by a topological ordering of a transitive orientation of the graph.\n\nAnother class of perfectly orderable graphs is given by the graphs \"G\" such that, in every subset of five vertices from \"G\", at least one of the five has a closed neighborhood that is a subset of (or equal to) the closed neighborhood of another of the five vertices. Equivalently, these are the graphs in which the partial order of closed neighborhoods, ordered by set inclusion, has width at most four. The 5-vertex cycle graph has a neighborhood partial order of width five, so four is the maximum width that ensures perfect orderability. As with the chordal graphs (and unlike the perfectly orderable graphs more generally) the graphs with width four are recognizable in polynomial time.\n\nA concept intermediate between the perfect elimination ordering of a chordal graph and a perfect ordering is a \"semiperfect elimination ordering\": in an elimination ordering, there is no three-vertex induced path in which the middle vertex is the first of the three to be eliminated, and in a semiperfect elimination ordering, there is no four-vertex induced path in which one of the two middle vertices is the first to be eliminated. The reverse of this ordering therefore satisfies the requirements of a perfect ordering, so graphs with semiperfect elimination orderings are perfectly orderable. In particular, the same lexicographic breadth-first search algorithm used to find perfect elimination orders of chordal graphs can be used to find semiperfect elimination orders of distance-hereditary graphs, which are therefore also perfectly orderable.\n\nThe graphs for which every vertex ordering is a perfect ordering are the cographs. Because cographs are the graphs with no four-vertex induced path, they cannot violate the path-ordering requirement on a perfect ordering.\n\nSeveral additional classes of perfectly orderable graphs are known.\n\n"}
{"id": "4141563", "url": "https://en.wikipedia.org/wiki?curid=4141563", "title": "Predictive analytics", "text": "Predictive analytics\n\nPredictive analytics encompasses a variety of statistical techniques from data mining, predictive modelling, and machine learning, that analyze current and historical facts to make predictions about future or otherwise unknown events.\n\nIn business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision making for candidate transactions.\n\nThe defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.\n\nPredictive analytics is used in actuarial science, marketing, financial services, insurance, telecommunications, retail, travel, mobility, healthcare, child protection, pharmaceuticals, capacity planning, social networking and other fields.\n\nOne of the best-known applications is credit scoring, which is used throughout financial services. Scoring models process a customer's credit history, loan application, customer data, etc., in order to rank-order individuals by their likelihood of making future credit payments on time.\n\nPredictive analytics is an area of statistics that deals with extracting information from data and using it to predict trends and behavior patterns. The enhancement of predictive web analytics calculates statistical probabilities of future events online. Predictive analytics statistical techniques include data modeling, machine learning, AI, deep learning algorithms and data mining. Often the unknown event of interest is in the future, but predictive analytics can be applied to any type of unknown whether it be in the past, present or future. For example, identifying suspects after a crime has been committed, or credit card fraud as it occurs. The core of predictive analytics relies on capturing relationships between explanatory variables and the predicted variables from past occurrences, and exploiting them to predict the unknown outcome. It is important to note, however, that the accuracy and usability of results will depend greatly on the level of data analysis and the quality of assumptions.\n\nPredictive analytics is often defined as predicting at a more detailed level of granularity, i.e., generating predictive scores (probabilities) for each individual organizational element. This distinguishes it from forecasting. For example, \"Predictive analytics—Technology that learns from experience (data) to predict the future behavior of individuals in order to drive better decisions.\" In future industrial systems, the value of predictive analytics will be to predict and prevent potential issues to achieve near-zero break-down and further be integrated into prescriptive analytics for decision optimization. Furthermore, the converted data can be used for closed-loop product life cycle improvement which is the vision of the Industrial Internet Consortium.\n\nGenerally, the term predictive analytics is used to mean predictive modeling, \"scoring\" data with predictive models, and forecasting. However, people are increasingly using the term to refer to related analytical disciplines, such as descriptive modeling and decision modeling or optimization. These disciplines also involve rigorous data analysis, and are widely used in business for segmentation and decision making, but have different purposes and the statistical techniques underlying them vary.\n\nPredictive models are models of the relation between the specific performance of a unit in a sample and one or more known attributes or features of the unit. The objective of the model is to assess the likelihood that a similar unit in a different sample will exhibit the specific performance. This category encompasses models in many areas, such as marketing, where they seek out subtle data patterns to answer questions about customer performance, or fraud detection models. Predictive models often perform calculations during live transactions, for example, to evaluate the risk or opportunity of a given customer or transaction, in order to guide a decision. With advancements in computing speed, individual agent modeling systems have become capable of simulating human behaviour or reactions to given stimuli or scenarios.\n\nThe available sample units with known attributes and known performances is referred to as the \"training sample\". The units in other samples, with known attributes but unknown performances, are referred to as \"out of [training] sample\" units. The out of sample units do not necessarily bear a chronological relation to the training sample units. For example, the training sample may consist of literary attributes of writings by Victorian authors, with known attribution, and the out-of sample unit may be newly found writing with unknown authorship; a predictive model may aid in attributing a work to a known author. Another example is given by analysis of blood splatter in simulated crime scenes in which the out of sample unit is the actual blood splatter pattern from a crime scene. The out of sample unit may be from the same time as the training units, from a previous time, or from a future time.\n\nDescriptive models quantify relationships in data in a way that is often used to classify customers or prospects into groups. Unlike predictive models that focus on predicting a single customer behavior (such as credit risk), descriptive models identify many different relationships between customers or products. Descriptive models do not rank-order customers by their likelihood of taking a particular action the way predictive models do. Instead, descriptive models can be used, for example, to categorize customers by their product preferences and life stage. Descriptive modeling tools can be utilized to develop further models that can simulate large number of individualized agents and make predictions.\n\nDecision models describe the relationship between all the elements of a decision—the known data (including results of predictive models), the decision, and the forecast results of the decision—in order to predict the results of decisions involving many variables. These models can be used in optimization, maximizing certain outcomes while minimizing others. Decision models are generally used to develop decision logic or a set of business rules that will produce the desired action for every customer or circumstance.\n\nAlthough predictive analytics can be put to use in many applications, we outline a few examples where predictive analytics has shown positive impact in recent years.\n\nAnalytical customer relationship management (CRM) is a frequent commercial application of predictive analysis. Methods of predictive analysis are applied to customer data to pursue CRM objectives, which involve constructing a holistic view of the customer no matter where their information resides in the company or the department involved. CRM uses predictive analysis in applications for marketing campaigns, sales, and customer services to name a few. These tools are required in order for a company to posture and focus their efforts effectively across the breadth of their customer base. They must analyze and understand the products in demand or have the potential for high demand, predict customers' buying habits in order to promote relevant products at multiple touch points, and proactively identify and mitigate issues that have the potential to lose customers or reduce their ability to gain new ones. Analytical customer relationship management can be applied throughout the customers' lifecycle (acquisition, relationship growth, retention, and win-back). Several of the application areas described below (direct marketing, cross-sell, customer retention) are part of customer relationship management.\n\nOver the last 5 years, some child welfare agencies have started using predictive analytics to flag high risk cases. The approach has been called \"innovative\" by the Commission to Eliminate Child Abuse and Neglect Fatalities (CECANF), and in Hillsborough County, Florida, where the lead child welfare agency uses a predictive modeling tool, there have been no abuse-related child deaths in the target population as of this writing.\n\nExperts use predictive analysis in health care primarily to determine which patients are at risk of developing certain conditions, like diabetes, asthma, heart disease, and other lifetime illnesses. Additionally, sophisticated clinical decision support systems incorporate predictive analytics to support medical decision making at the point of care. A working definition has been proposed by Jerome A. Osheroff and colleagues: \"Clinical decision support (CDS) provides clinicians, staff, patients, or other individuals with knowledge and person-specific information, intelligently filtered or presented at appropriate times, to enhance health and health care. It encompasses a variety of tools and interventions such as computerized alerts and reminders, clinical guidelines, order sets, patient data reports and dashboards, documentation templates, diagnostic support, and clinical workflow tools\".\n\nA 2016 study of neurodegenerative disorders provides a powerful example of a CDS platform to diagnose, track, predict and monitor the progression of Parkinson's disease. Using large and multi-source imaging, genetics, clinical and demographic data, these investigators developed a decision support system that can predict the state of the disease with high accuracy, consistency and precision. They employed classical model-based and machine learning model-free methods to discriminate between different patient and control groups. Similar approaches may be used for predictive diagnosis and disease progression forecasting in many neurodegenerative disorders like Alzheimer’s, Huntington’s, amyotrophic lateral sclerosis, and for other clinical and biomedical applications where Big Data is available.\n\nMany portfolios have a set of delinquent customers who do not make their payments on time. The financial institution has to undertake collection activities on these customers to recover the amounts due. A lot of collection resources are wasted on customers who are difficult or impossible to recover. Predictive analytics can help optimize the allocation of collection resources by identifying the most effective collection agencies, contact strategies, legal actions and other strategies to each customer, thus significantly increasing recovery at the same time reducing collection costs.\n\nOften corporate organizations collect and maintain abundant data (e.g. customer records, sale transactions) as exploiting hidden relationships in the data can provide a competitive advantage. For an organization that offers multiple products, predictive analytics can help analyze customers' spending, usage and other behavior, leading to efficient cross sales, or selling additional products to current customers. This directly leads to higher profitability per customer and stronger customer relationships.\n\nWith the number of competing services available, businesses need to focus efforts on maintaining continuous customer satisfaction, rewarding consumer loyalty and minimizing customer attrition. In addition, small increases in customer retention have been shown to increase profits disproportionately. One study concluded that a 5% increase in customer retention rates will increase profits by 25% to 95%. Businesses tend to respond to customer attrition on a reactive basis, acting only after the customer has initiated the process to terminate service. At this stage, the chance of changing the customer's decision is almost zero. Proper application of predictive analytics can lead to a more proactive retention strategy. By a frequent examination of a customer's past service usage, service performance, spending and other behavior patterns, predictive models can determine the likelihood of a customer terminating service sometime soon. An intervention with lucrative offers can increase the chance of retaining the customer. Silent attrition, the behavior of a customer to slowly but steadily reduce usage, is another problem that many companies face. Predictive analytics can also predict this behavior, so that the company can take proper actions to increase customer activity.\n\nWhen marketing consumer products and services, there is the challenge of keeping up with competing products and consumer behavior. Apart from identifying prospects, predictive analytics can also help to identify the most effective combination of product versions, marketing material, communication channels and timing that should be used to target a given consumer. The goal of predictive analytics is typically to lower the cost per order or cost per action.\n\nFraud is a big problem for many businesses and can be of various types: inaccurate credit applications, fraudulent transactions (both offline and online), identity thefts and false insurance claims. Some examples of likely victims are credit card issuers, insurance companies, retail merchants, manufacturers, business-to-business suppliers and even services providers. A predictive model can help weed out the \"bads\" and reduce a business's exposure to fraud.\n\nPredictive modeling can also be used to identify high-risk fraud candidates in business or the public sector. Mark Nigrini developed a risk-scoring method to identify audit targets. He describes the use of this approach to detect fraud in the franchisee sales reports of an international fast-food chain. Each location is scored using 10 predictors. The 10 scores are then weighted to give one final overall risk score for each location. The same scoring approach was also used to identify high-risk check kiting accounts, potentially fraudulent travel agents, and questionable vendors. A reasonably complex model was used to identify fraudulent monthly reports submitted by divisional controllers.\n\nThe Internal Revenue Service (IRS) of the United States also uses predictive analytics to mine tax returns and identify tax fraud.\n\nRecent advancements in technology have also introduced predictive behavior analysis for web fraud detection. This type of solution utilizes heuristics in order to study normal web user behavior and detect anomalies indicating fraud attempts.\n\nOften the focus of analysis is not the consumer but the product, portfolio, firm, industry or even the economy. For example, a retailer might be interested in predicting store-level demand for inventory management purposes. Or the Federal Reserve Board might be interested in predicting the unemployment rate for the next year. These types of problems can be addressed by predictive analytics using time series techniques (see below). They can also be addressed via machine learning approaches which transform the original time series into a feature vector space, where the learning algorithm finds patterns that have predictive power.\n\nWhen employing risk management techniques, the results are always to predict and benefit from a future scenario. The capital asset pricing model (CAP-M) \"predicts\" the best portfolio to maximize return. Probabilistic risk assessment (PRA) when combined with mini-Delphi techniques and statistical approaches yields accurate forecasts. These are examples of approaches that can extend from project to market, and from near to long term. Underwriting (see below) and other business approaches identify risk management as a predictive method.\n\nMany businesses have to account for risk exposure due to their different services and determine the cost needed to cover the risk. For example, auto insurance providers need to accurately determine the amount of premium to charge to cover each automobile and driver. A financial company needs to assess a borrower's potential and ability to pay before granting a loan. For a health insurance provider, predictive analytics can analyze a few years of past medical claims data, as well as lab, pharmacy and other records where available, to predict how expensive an enrollee is likely to be in the future. Predictive analytics can help underwrite these quantities by predicting the chances of illness, default, bankruptcy, etc. Predictive analytics can streamline the process of customer acquisition by predicting the future risk behavior of a customer using application level data. Predictive analytics in the form of credit scores have reduced the amount of time it takes for loan approvals, especially in the mortgage market where lending decisions are now made in a matter of hours rather than days or even weeks. Proper predictive analytics can lead to proper pricing decisions, which can help mitigate future risk of default.\n\nBig data is a collection of data sets that are so large and complex that they become awkward to work with using traditional database management tools. The volume, variety and velocity of big data have introduced challenges across the board for capture, storage, search, sharing, analysis, and visualization. Examples of big data sources include web logs, RFID, sensor data, social networks, Internet search indexing, call detail records, military surveillance, and complex data in astronomic, biogeochemical, genomics, and atmospheric sciences. Big Data is the core of most predictive analytic services offered by IT organizations. \nThanks to technological advances in computer hardware—faster CPUs, cheaper memory, and MPP architectures—and new technologies such as Hadoop, MapReduce, and in-database and text analytics for processing big data, it is now feasible to collect, analyze, and mine massive amounts of structured and unstructured data for new insights. It is also possible to run predictive algorithms on streaming data. Today, exploring big data and using predictive analytics is within reach of more organizations than ever before and new methods that are capable for handling such datasets are proposed.\n\nThe approaches and techniques used to conduct predictive analytics can broadly be grouped into regression techniques and machine learning techniques.\n\nRegression models are the mainstay of predictive analytics. The focus lies on establishing a mathematical equation as a model to represent the interactions between the different variables in consideration. Depending on the situation, there are a wide variety of models that can be applied while performing predictive analytics. Some of them are briefly discussed below.\n\nThe linear regression model analyzes the relationship between the response or dependent variable and a set of independent or predictor variables. This relationship is expressed as an equation that predicts the response variable as a linear function of the parameters. These parameters are adjusted so that a measure of fit is optimized. Much of the effort in model fitting is focused on minimizing the size of the residual, as well as ensuring that it is randomly distributed with respect to the model predictions.\n\nThe goal of regression is to select the parameters of the model so as to minimize the sum of the squared residuals. This is referred to as ordinary least squares (OLS) estimation and results in best linear unbiased estimates (BLUE) of the parameters if and only if the Gauss-Markov assumptions are satisfied.\n\nOnce the model has been estimated we would be interested to know if the predictor variables belong in the model—i.e. is the estimate of each variable's contribution reliable? To do this we can check the statistical significance of the model's coefficients which can be measured using the t-statistic. This amounts to testing whether the coefficient is significantly different from zero. How well the model predicts the dependent variable based on the value of the independent variables can be assessed by using the R² statistic. It measures predictive power of the model i.e. the proportion of the total variation in the dependent variable that is \"explained\" (accounted for) by variation in the independent variables.\n\nMultiple regression (above) is generally used when the response variable is continuous and has an unbounded range. Often the response variable may not be continuous but rather discrete. While mathematically it is feasible to apply multiple regression to discrete ordered dependent variables, some of the assumptions behind the theory of multiple linear regression no longer hold, and there are other techniques such as discrete choice models which are better suited for this type of analysis. If the dependent variable is discrete, some of those superior methods are logistic regression, multinomial logit and probit models. Logistic regression and probit models are used when the dependent variable is binary.\n\nIn a classification setting, assigning outcome probabilities to observations can be achieved through the use of a logistic model, which is basically a method which transforms information about the binary dependent variable into an unbounded continuous variable and estimates a regular multivariate model (See Allison's \"Logistic Regression\" for more information on the theory of logistic regression).\n\nThe Wald and likelihood-ratio test are used to test the statistical significance of each coefficient \"b\" in the model (analogous to the t tests used in OLS regression; see above). A test assessing the goodness-of-fit of a classification model is the \"percentage correctly predicted\".\n\nAn extension of the binary logit model to cases where the dependent variable has more than 2 categories is the multinomial logit model. In such cases collapsing the data into two categories might not make good sense or may lead to loss in the richness of the data. The multinomial logit model is the appropriate technique in these cases, especially when the dependent variable categories are not ordered (for examples colors like red, blue, green). Some authors have extended multinomial regression to include feature selection/importance methods such as random multinomial logit.\n\nProbit models offer an alternative to logistic regression for modeling categorical dependent variables. Even though the outcomes tend to be similar, the underlying distributions are different. Probit models are popular in social sciences like economics.\n\nA good way to understand the key difference between probit and logit models is to assume that the dependent variable is driven by a latent variable z, which is a sum of a linear combination of explanatory variables and a random noise term.\n\nWe do not observe z but instead observe y which takes the value 0 (when z < 0) or 1 (otherwise). In the logit model we assume that the random noise term follows a logistic distribution with mean zero. In the probit model we assume that it follows a normal distribution with mean zero. Note that in social sciences (e.g. economics), probit is often used to model situations where the observed variable y is continuous but takes values between 0 and 1.\n\nThe probit model has been around longer than the logit model. They behave similarly, except that the logistic distribution tends to be slightly flatter tailed. One of the reasons the logit model was formulated was that the probit model was computationally difficult due to the requirement of numerically calculating integrals. Modern computing however has made this computation fairly simple. The coefficients obtained from the logit and probit model are fairly close. However, the odds ratio is easier to interpret in the logit model.\n\nPractical reasons for choosing the probit model over the logistic model would be:\n\nTime series models are used for predicting or forecasting the future behavior of variables. These models account for the fact that data points taken over time may have an internal structure (such as autocorrelation, trend or seasonal variation) that should be accounted for. As a result, standard regression techniques cannot be applied to time series data and methodology has been developed to decompose the trend, seasonal and cyclical component of the series. Modeling the dynamic path of a variable can improve forecasts since the predictable component of the series can be projected into the future.\n\nTime series models estimate difference equations containing stochastic components. Two commonly used forms of these models are autoregressive models (AR) and moving-average (MA) models. The Box–Jenkins methodology (1976) developed by George Box and G.M. Jenkins combines the AR and MA models to produce the ARMA (autoregressive moving average) model, which is the cornerstone of stationary time series analysis. ARIMA (autoregressive integrated moving average models), on the other hand, are used to describe non-stationary time series. Box and Jenkins suggest differencing a non-stationary time series to obtain a stationary series to which an ARMA model can be applied. Non-stationary time series have a pronounced trend and do not have a constant long-run mean or variance.\n\nBox and Jenkins proposed a three-stage methodology involving model identification, estimation and validation. The identification stage involves identifying if the series is stationary or not and the presence of seasonality by examining plots of the series, autocorrelation and partial autocorrelation functions. In the estimation stage, models are estimated using non-linear time series or maximum likelihood estimation procedures. Finally the validation stage involves diagnostic checking such as plotting the residuals to detect outliers and evidence of model fit.\n\nIn recent years time series models have become more sophisticated and attempt to model conditional heteroskedasticity with models such as ARCH (autoregressive conditional heteroskedasticity) and GARCH (generalized autoregressive conditional heteroskedasticity) models frequently used for financial time series. In addition time series models are also used to understand inter-relationships among economic variables represented by systems of equations using VAR (vector autoregression) and structural VAR models.\n\nSurvival analysis is another name for time-to-event analysis. These techniques were primarily developed in the medical and biological sciences, but they are also widely used in the social sciences like economics, as well as in engineering (reliability and failure time analysis).\n\nCensoring and non-normality, which are characteristic of survival data, generate difficulty when trying to analyze the data using conventional statistical models such as multiple linear regression. The normal distribution, being a symmetric distribution, takes positive as well as negative values, but duration by its very nature cannot be negative and therefore normality cannot be assumed when dealing with duration/survival data. Hence the normality assumption of regression models is violated.\n\nThe assumption is that if the data were not censored it would be representative of the population of interest. In survival analysis, censored observations arise whenever the dependent variable of interest represents the time to a terminal event, and the duration of the study is limited in time.\n\nAn important concept in survival analysis is the hazard rate, defined as the probability that the event will occur at time t conditional on surviving until time t. Another concept related to the hazard rate is the survival function which can be defined as the probability of surviving to time t.\n\nMost models try to model the hazard rate by choosing the underlying distribution depending on the shape of the hazard function. A distribution whose hazard function slopes upward is said to have positive duration dependence, a decreasing hazard shows negative duration dependence whereas constant hazard is a process with no memory usually characterized by the exponential distribution. Some of the distributional choices in survival models are: F, gamma, Weibull, log normal, inverse normal, exponential etc. All these distributions are for a non-negative random variable.\n\nDuration models can be parametric, non-parametric or semi-parametric. Some of the models commonly used are Kaplan-Meier and Cox proportional hazard model (non parametric).\n\nGlobally-optimal classification tree analysis (GO-CTA) (also called hierarchical optimal discriminant analysis) is a generalization of optimal discriminant analysis that may be used to identify the statistical model that has maximum accuracy for predicting the value of a categorical dependent variable for a dataset consisting of categorical and continuous variables. The output of HODA is a non-orthogonal tree that combines categorical variables and cut points for continuous variables that yields maximum predictive accuracy, an assessment of the exact Type I error rate, and an evaluation of potential cross-generalizability of the statistical model. Hierarchical optimal discriminant analysis may be thought of as a generalization of Fisher's linear discriminant analysis. Optimal discriminant analysis is an alternative to ANOVA (analysis of variance) and regression analysis, which attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA and regression analysis give a dependent variable that is a numerical variable, while hierarchical optimal discriminant analysis gives a dependent variable that is a class variable.\n\nClassification and regression trees (CART) are a non-parametric decision tree learning technique that produces either classification or regression trees, depending on whether the dependent variable is categorical or numeric, respectively.\n\nDecision trees are formed by a collection of rules based on variables in the modeling data set:\n\nEach branch of the tree ends in a terminal node. Each observation falls into one and exactly one terminal node, and each terminal node is uniquely defined by a set of rules.\n\nA very popular method for predictive analytics is Leo Breiman's random forests.\n\nMultivariate adaptive regression splines (MARS) is a non-parametric technique that builds flexible models by fitting piecewise linear regressions.\n\nAn important concept associated with regression splines is that of a knot. Knot is where one local regression model gives way to another and thus is the point of intersection between two splines.\n\nIn multivariate and adaptive regression splines, basis functions are the tool used for generalizing the search for knots. Basis functions are a set of functions used to represent the information contained in one or more variables.\nMultivariate and Adaptive Regression Splines model almost always creates the basis functions in pairs.\n\nMultivariate and adaptive regression spline approach deliberately overfits the model and then prunes to get to the optimal model. The algorithm is computationally very intensive and in practice we are required to specify an upper limit on the number of basis functions.\n\nMachine learning, a branch of artificial intelligence, was originally employed to develop techniques to enable computers to learn. Today, since it includes a number of advanced statistical methods for regression and classification, it finds application in a wide variety of fields including medical diagnostics, credit card fraud detection, face and speech recognition and analysis of the stock market. In certain applications it is sufficient to directly predict the dependent variable without focusing on the underlying relationships between variables. In other cases, the underlying relationships can be very complex and the mathematical form of the dependencies unknown. For such cases, machine learning techniques emulate human cognition and learn from training examples to predict future events.\n\nA brief discussion of some of these methods used commonly for predictive analytics is provided below. A detailed study of machine learning can be found in Mitchell (1997).\n\nNeural networks are nonlinear sophisticated modeling techniques that are able to model complex functions. They can be applied to problems of prediction, classification or control in a wide spectrum of fields such as finance, cognitive psychology/neuroscience, medicine, engineering, and physics.\n\nNeural networks are used when the exact nature of the relationship between inputs and output is not known. A key feature of neural networks is that they learn the relationship between inputs and output through training. There are three types of training used by different neural networks: supervised and unsupervised training and reinforcement learning, with supervised being the most common one.\n\nSome examples of neural network training techniques are backpropagation, quick propagation, conjugate gradient descent, projection operator, Delta-Bar-Delta etc. Some unsupervised network architectures are multilayer perceptrons, Kohonen networks, Hopfield networks, etc.\n\nThe multilayer perceptron (MLP) consists of an input and an output layer with one or more hidden layers of nonlinearly-activating nodes or sigmoid nodes. This is determined by the weight vector and it is necessary to adjust the weights of the network. The backpropagation employs gradient fall to minimize the squared error between the network output values and desired values for those outputs. The weights adjusted by an iterative process of repetitive present of attributes. Small changes in the weight to get the desired values are done by the process called training the net and is done by the training set (learning rule).\n\nA radial basis function (RBF) is a function which has built into it a distance criterion with respect to a center. Such functions can be used very efficiently for interpolation and for smoothing of data. Radial basis functions have been applied in the area of neural networks where they are used as a replacement for the sigmoidal transfer function. Such networks have 3 layers, the input layer, the hidden layer with the RBF non-linearity and a linear output layer. The most popular choice for the non-linearity is the Gaussian. RBF networks have the advantage of not being locked into local minima as do the feed-forward networks such as the multilayer perceptron.\n\nSupport vector machines (SVM) are used to detect and exploit complex patterns in data by clustering, classifying and ranking the data. They are learning machines that are used to perform binary classifications and regression estimations. They commonly use kernel based methods to apply linear classification techniques to non-linear classification problems. There are a number of types of SVM such as linear, polynomial, sigmoid etc.\n\nNaïve Bayes based on Bayes conditional probability rule is used for performing classification tasks. Naïve Bayes assumes the predictors are statistically independent which makes it an effective classification tool that is easy to interpret. It is best employed when faced with the \"curse of dimensionality\" problem, i.e. when the number of predictors is very high.\n\nThe nearest neighbour algorithm (KNN) belongs to the class of pattern recognition statistical methods. The method does not impose a priori any assumptions about the distribution from which the modeling sample is drawn. It involves a training set with both positive and negative values. A new sample is classified by calculating the distance to the nearest neighbouring training case. The sign of that point will determine the classification of the sample. In the k-nearest neighbour classifier, the k nearest points are considered and the sign of the majority is used to classify the sample. The performance of the kNN algorithm is influenced by three main factors: (1) the distance measure used to locate the nearest neighbours, (2) the decision rule used to derive a classification from the k-nearest neighbours, and (3) the number of neighbours used to classify the new sample. It can be proved that, unlike other methods, this method is universally asymptotically convergent, i.e. as the size of the training set increases, if the observations are independent and identically distributed (i.i.d.), regardless of the distribution from which the sample is drawn, the predicted class will converge to the class assignment that minimizes misclassification error. See Devroy et al.\n\nConceptually, geospatial predictive modeling is rooted in the principle that the occurrences of events being modeled are limited in distribution. Occurrences of events are neither uniform nor random in distribution—there are spatial environment factors (infrastructure, sociocultural, topographic, etc.) that constrain and influence where the locations of events occur. Geospatial predictive modeling attempts to describe those constraints and influences by spatially correlating occurrences of historical geospatial locations with environmental factors that represent those constraints and influences. Geospatial predictive modeling is a process for analyzing events through a geographic filter in order to make statements of likelihood for event occurrence or emergence.\n\nHistorically, using predictive analytics tools—as well as understanding the results they delivered—required advanced skills. However, modern predictive analytics tools are no longer restricted to IT specialists. As more organizations adopt predictive analytics into decision-making processes and integrate it into their operations, they are creating a shift in the market toward business users as the primary consumers of the information. Business users want tools they can use on their own. Vendors are responding by creating new software that removes the mathematical complexity, provides user-friendly graphic interfaces and/or builds in short cuts that can, for example, recognize the kind of data available and suggest an appropriate predictive model. Predictive analytics tools have become sophisticated enough to adequately present and dissect data problems, so that any data-savvy information worker can utilize them to analyze data and retrieve meaningful, useful results. For example, modern tools present findings using simple charts, graphs, and scores that indicate the likelihood of possible outcomes.\n\nThere are numerous tools available in the marketplace that help with the execution of predictive analytics. These range from those that need very little user sophistication to those that are designed for the expert practitioner. The difference between these tools is often in the level of customization and heavy data lifting allowed.\n\nSome open-source software predictive analytic tools include:\nCommercial predictive analytic tools include:\nBeside these software packages, specific tools have also been developed for industrial applications. For example, Watchdog Agent Toolbox has been developed and optimized for predictive analysis in prognostics and health management applications and is available for MATLAB and LabVIEW.\n\nThe most popular commercial predictive analytics software packages according to the Rexer Analytics Survey for 2013 are IBM SPSS Modeler, SAS Enterprise Miner, and Dell Statistica.\n\nThe Predictive Model Markup Language (PMML) was proposed for standard language for expressing predictive models. Such an XML-based language provides a way for the different tools to define predictive models and to share them. PMML 4.0 was released in June, 2009.\n\nThere are plenty of skeptics when it comes to computers' and algorithms' abilities to predict the future, including Gary King, a professor from Harvard University and the director of the Institute for Quantitative Social Science. People are influenced by their environment in innumerable ways. Predicting perfectly what people will do next requires that all the influential variables be known and measured accurately. \"People's environments change even more quickly than they themselves do. Everything from the weather to their relationship with their mother can change the way people think and act. All of those variables are unpredictable. How they will impact a person is even less predictable. If put in the exact same situation tomorrow, they may make a completely different decision. This means that a statistical prediction is only valid in sterile laboratory conditions, which suddenly isn't as useful as it seemed before.\"\n\nIn a study of 1072 papers published in Information Systems Research and MIS Quarterly between 1990 and 2006, only 52 empirical papers attempted predictive claims, of which only 7 carried out proper predictive modeling or testing.\n\n\n"}
{"id": "614962", "url": "https://en.wikipedia.org/wiki?curid=614962", "title": "Ralph Faudree", "text": "Ralph Faudree\n\nRalph Jasper Faudree (August 23, 1939 – January 13, 2015) was a mathematician, a professor of mathematics and the former provost of the University of Memphis.\n\nFaudree was born in Durant, Oklahoma. He did his undergraduate studies at Oklahoma Baptist University, graduating in 1961, and received his Ph.D. in 1964 from Purdue University under the supervision of Eugene Schenkman (1922–1977). Faudree was an instructor at the University of California, Berkeley and an assistant professor at the University of Illinois before joining the Memphis State University faculty as an associate professor in 1971. Memphis State became renamed as the University of Memphis in 1994, and Faudree was appointed as provost in 2001.\n\nFaudree specialized in combinatorics, and specifically in graph theory and Ramsey theory. He published more than 200 mathematical papers on these topics together with such notable mathematicians as Béla Bollobás, Stefan Burr, Paul Erdős, Ron Gould, András Gyárfás, Brendan McKay, Cecil Rousseau, Richard Schelp, Miklós Simonovits, Joel Spencer, and Vera Sós. He was the 2005 recipient of the Euler Medal for his contributions to combinatorics. His Erdős number was 1: he cowrote 50 joint papers with Paul Erdős beginning in 1976 and was among the three mathematicians who most frequently co-authored with Erdős.\n\n\n"}
{"id": "35889132", "url": "https://en.wikipedia.org/wiki?curid=35889132", "title": "Random walk closeness centrality", "text": "Random walk closeness centrality\n\nRandom walk closeness centrality is a measure of centrality in a network, which describes the average speed with which randomly walking processes reach a node from other nodes of the network. It is similar to the closeness centrality except that the farness is measured by the expected length of a random walk rather than by the shortest path.\n\nThe concept was first proposed by White and Smyth (2003) under the name \"Markov centrality\".\n\nConsider a network with a finite number of nodes and a random walk process that starts in a certain node and proceeds from node to node along the edges. From each node, it chooses randomly the edge to be followed. In an unweighted network, the probability of choosing a certain edge is equal across all available edges, while in a weighted network it is proportional to the edge weights.\nA node is considered to be close to other nodes, if the random walk process initiated from any node of the network arrives to this particular node in relatively few steps on average.\n\nConsider a weighted network – either directed or undirected – with n nodes denoted by j=1, …, n; and a random walk process on this network with a transition matrix M. The formula_1 element of M describes the probability of the random walker that has reached node i, proceeds directly to node j. These probabilities are defined in the following way.\n\nwhere formula_3 is the (i,j)th element of the weighting matrix A of the network. When there is no edge between two nodes, the corresponding element of the A matrix is zero.\n\nThe random walk closeness centrality of a node i is the inverse of the average mean first passage time to that node:\n\nThe mean first passage time from node i to node j is the expected number of steps it takes for the process to reach node j from node i for the first time:\n\nwhere P(i,j,r) denotes the probability that it takes exactly r steps to reach j from i for the first time.\nTo calculate these probabilities of reaching a node for the first time in r steps, it is useful to regard the target node as an absorbing one, and introduce a transformation of M by deleting its j-th row and column and denoting it by formula_6. As the probability of a process starting at i and being in k after r-1 steps is simply given by the (i,k)th element of formula_7, P(i,j,r) can be expressed as\n\nSubstituting this into the expression for mean first passage time yields\n\nUsing the formula for the summation of geometric series for matrices yields\n\nwhere I is the n-1 dimensional identity matrix.\n\nFor computational convenience, this expression can be vectorized as\n\nwhere formula_12 is the vector for first passage times for a walk ending at node j, and e is an n-1 dimensional vector of ones.\n\nMean first passage time is not symmetric, even for undirected graphs.\n\nAccording to simulations performed by Noh and Rieger (2004), the distribution of random walk closeness centrality in a Barabási-Albert model is mainly determined by the degree distribution. In such a network, the random walk closeness centrality of a node is roughly proportional to, but does not increase monotonically with its degree.\n\nRandom walk closeness centrality is more relevant measure than the simple closeness centrality in case of applications where the concept of shortest paths is not meaningful or is very restrictive for a reasonable assessment of the nature of the system.\nThis is the case for example when the analyzed process evolves in the network without any specific intention to reach a certain point, or without the ability of finding the shortest path to reach its target. One example for a random walk in a network is the way a certain coin circulates in an economy: it is passed from one person to another through transactions, without any intention of reaching a specific individual. \nAnother example where the concept of shortest paths is not very useful is a densely connected network. Furthermore, as shortest paths are not influenced by self-loops, random walk closeness centrality is a more adequate measure than closeness centrality when analyzing networks where self-loops are important.\n\nAn important application on the field of economics is the analysis of the input-output model of an economy, which is represented by a densely connected weighted network with important self-loops.\n\nThe concept is widely used in natural sciences as well. One biological application is the analysis of protein-protein interactions.\n\nA related concept, proposed by Newman, is random walk betweenness centrality. Just as random walk closeness centrality is a random walk counterpart of closeness centrality, random walk betweenness centrality is, similarly, the random walk counterpart of betweenness centrality. Unlike the usual betweenness centrality measure, it does not only count shortest paths passing through the given node, but all possible paths crossing it.\n\nFormally, the random walk betweenness centrality of a node is\n\nwhere the formula_14 element of matrix R contains the probability of a random walk starting at node j with absorbing node k, passing through node i.\n\nCalculating random walk betweenness in large networks is computationally very intensive.\n\n"}
{"id": "20228317", "url": "https://en.wikipedia.org/wiki?curid=20228317", "title": "Rocket City Math League", "text": "Rocket City Math League\n\nRocket City Math League (RCML) is a student-run mathematics competition in the United States. Run by students at Virgil I. Grissom High School in Huntsville, Alabama, RCML gets its name from Huntsville's nickname as the \"Rocket City\". RCML was started in 2001 and has been annually sponsored by the Mu Alpha Theta Math Honor Society. The competition consists of 3 individual rounds and a team round that was added in 2008. It is divided into 5 divisions named for NASA programs: Explorer (pre-algebra), Mercury (algebra I), Gemini (geometry), Apollo (algebra II), and Discovery (comprehensive).\n\nEach of the 3 individual rounds consists of a 10 question test with a 45-minute time limit. Out of the 10 questions, there are four 1-point questions, three 2-point questions, two 3-point questions, and one 4-point question, with the more difficult questions having larger point values. The maximum score on an individual test is 20, and individual tests often contain many interesting space-themed questions.\n\nThe team round is divided into a senior division and a junior division that take separate tests for the team round. It consists of a 15 question test with a 30-minute time limit, in which team members work together to get as many correct answers as possible. Out of the 15 questions, there are five 1-point questions, four 2-point questions, three 3-point questions, two 4-point questions, and one 5-point question, making the maximum score on the team test a 35.\n\n\n"}
{"id": "37554140", "url": "https://en.wikipedia.org/wiki?curid=37554140", "title": "Ruth Lyttle Satter Prize in Mathematics", "text": "Ruth Lyttle Satter Prize in Mathematics\n\nThe Ruth Lyttle Satter Prize in Mathematics, also called the Satter Prize, is one of twenty-one prizes given out by the American Mathematical Society (AMS). It is presented biennially in recognition of an outstanding contribution to mathematics research by a woman in the previous six years. The award was established in 1990 using a donation from Joan Birman, in memory of her sister, Ruth Lyttle Satter, who worked primarily in biological sciences, and was a proponent for equal opportunities for women in science. First awarded in 1991, the award is intended to \"honor [Satter's] commitment to research and to encourage women in science\". The winner is selected by the council of the AMS, based on the recommendation of a selection committee. The prize is awarded at the Joint Mathematics Meetings during odd numbered years, and has always carried a modest cash reward. Since 2003, the prize has been $5,000, while from 1997 to 2001, the prize came with $1,200, and prior to that it was $4,000. If a joint award is made, the prize money is split between the recipients.\n\n, the award has been given 14 times, to 15 different individuals. Dusa McDuff was the first recipient of the award, for her work on symplectic geometry. A joint award was made for the only time in 2001, when Karen E. Smith and Sijue Wu shared the award. The 2013 prize winner was Maryam Mirzakhani, who, in 2014, was the first woman to be awarded the Fields Medal. This is considered to be the highest honor a mathematician can receive. She won both awards for her work on \"the geometry of Riemann surfaces and their moduli spaces\". The most recent winner is Laura DeMarco, who was awarded the prize in 2017 for her \"fundamental contributions to complex dynamics, potential theory, and the emerging field of arithmetic dynamics\".\n\nThe Association for Women in Science have a similarly titled award, the Ruth Satter Memorial Award, which is a cash prize of $1,000 for \"an outstanding graduate student who interrupted her education for at least 3 years to raise a family\".\n"}
{"id": "17432950", "url": "https://en.wikipedia.org/wiki?curid=17432950", "title": "Sharp map", "text": "Sharp map\n\nIn differential geometry, the sharp map is the mapping that converts 1-forms into corresponding vectors, given a non-degenerate (0,2)-tensor.\n\nLet formula_1 be a manifold and formula_2 denote the space of all sections of its tangent bundle. Fix a nondegenerate (0,2)-tensor field formula_3 , for example a metric tensor or a symplectic form. The definition\nyields a linear map sometimes called the flat map\nwhich is an isomorphism, since formula_6 is non-degenerate. Its inverse\nis called the sharp map.\n"}
{"id": "18203815", "url": "https://en.wikipedia.org/wiki?curid=18203815", "title": "Speedtest.net", "text": "Speedtest.net\n\nSpeedtest.net is a web service that provides free analysis of Internet access performance metrics, such as connection data rate and latency. It was founded by Ookla in 2006, and is based in Seattle, Washington.\n\nThe service measures the bandwidth (\"speed\") and latency of a visitor's Internet connection against one of 4,759 geographically dispersed servers (as of August 2016) located around the world. Each test measures the data rate for the download direction, i.e. from the server to the user computer, and the upload data rate, i.e. from the user's computer to the server. The tests are performed within the user's web browser or within apps. , over 21 billion speed tests have been completed.\n\nTests were previously performed using the HTTP protocol at Layer 7 of the OSI model. To further improve accuracy, Speedtest.net now performs tests via direct TCP sockets and uses a custom protocol for communication between servers and clients.\n\nThe site also offers detailed statistics based on test results. This data has been used by numerous publications in the analysis of Internet access data rates around the world.\n\nThe owner and operator of Speedtest.net, Ookla, was established in 2006 by a small team of internet and technology veterans. Ookla was acquired by Ziff Davis in 2014.\n\nThe technology of Speedtest.net is similar to that of Ookla NetGauge, which is provided to a wide variety of companies and organizations on a licensed basis. Speedtest.net Mini was a free, stripped-down Flash version of the speed test technology used on Speedtest.net that users could run on their own web server, however it was replaced in 2016 with Speedtest Custom, an HTML5-based tool. Together, over 20 million speed tests are generated each month using Ookla's software.\n\nIn 2016, Speedtest began releasing market reports for different countries and cities, providing raw statistics regarding download and upload speeds for the past year for ISPs and mobile carriers. It also includes analysis of the current ISP and mobile markets of the respective country and breakdowns by region and city. ISPs and mobile carriers are ranked by their geographic performance.\n\nThe site offers a service by which groups of friends may compare results against each other and as a group average. Badges are also awarded for achievements such as \"Highest Download Speed\" and \"Lowest Latency\".\nBadges are awarded when either \"Highest Download\" or Lowest Ping\" will be.\n\nThe Ookla Speed Test also has a tool called “My Results” which lets you graphically compare your upload and download speeds as well as the different servers you have tested. Users who have been through many internet service providers, or that have more than one, may find value in this tool and could use it to choose the most efficient.\n\n"}
{"id": "39830945", "url": "https://en.wikipedia.org/wiki?curid=39830945", "title": "Stochastic portfolio theory", "text": "Stochastic portfolio theory\n\nStochastic portfolio theory (SPT) is a mathematical theory for analyzing stock market structure and portfolio behavior introduced by E. Robert Fernholz in 2002. It is descriptive as opposed to normative, and is consistent with the observed behavior of actual markets. Normative assumptions, which serve as a basis for earlier theories like modern portfolio theory (MPT) and the capital asset pricing model (CAPM), are absent from SPT.\n\nSPT uses continuous-time random processes (in particular, continuous semi-martingales) to represent the prices of individual securities. Processes with discontinuities, such as jumps, have also been incorporated into the theory. \n\nSPT considers stocks and stock markets, but its methods can be applied to other classes of assets as well. A stock is represented by its price process, usually in the logarithmic representation. In the case the market is a collection of stock-price processes formula_1 for formula_2 each defined by a continuous semimartingale\nwhere formula_4 is an formula_5-dimensional Brownian motion (Wiener) process with formula_6, and the processes formula_7 and formula_8 are progressively measurable with respect to the Brownian filtration\nformula_9. In this representation formula_10 is called the (compound) growth rate of formula_1 and the covariance between formula_12 and formula_13 is formula_14 It is frequently assumed that, for all formula_15 the process formula_16 is positive, locally square-integrable, and does not grow too rapidly as formula_17\n\nThe logarithmic representation is equivalent to the classical arithmetic representation which uses the rate of return formula_18 however the growth rate can be a meaningful indicator of long-term performance of a financial asset, whereas the rate of return has an upward bias. The relation between the rate of return and the growth rate is\nThe usual convention in SPT is to assume that each stock has a single share outstanding, so formula_20\nrepresents the total capitalization of the formula_21-th stock at time formula_22 and \nformula_23 is the total capitalization of the market. \nDividends can be included in this representation, but are omitted here for simplicity.\n\nAn investment strategy formula_24 is a vector of bounded, progressively measurable\nprocesses; the quantity formula_25 represents the proportion of total wealth invested in the formula_21-th stock at\ntime formula_27, and formula_28 is the proportion hoarded (invested in a money market with zero interest rate). Negative weights correspond to short positions. The cash strategy formula_29 keeps all wealth in the money market. A strategy formula_30 is called portfolio, if it is fully invested in the stock market, that is formula_31 holds, at all times.\n\nThe value process formula_32 of a strategy formula_30 is always positive and satisfies\n\nwhere the process formula_35 is called the excess growth rate process and is given by\n\nThis expression is non-negative for a portfolio with non-negative weights formula_25 and has been used\nin quadratic optimization of stock portfolios, a special case of which is optimization with respect to the logarithmic utility function.\n\nThe market weight processes,\nwhere formula_39 define the market portfolio formula_40. With the initial condition formula_41 the associated value process will satisfy formula_42 for all formula_43 \n\nA number of conditions can be imposed on a market, sometimes to model actual markets and sometimes to emphasize certain types of hypothetical market behavior. Some commonly invoked conditions are:\nformula_53\n\nDiversity and weak diversity are rather weak conditions, and markets are generally far more diverse than would be tested by these extremes. A measure of market diversity is market entropy, defined by \nWe consider the vector process formula_55 with formula_56 of ranked market weights\nwhere ties are resolved “lexicographically”, always in favor of the lowest index. The log-gaps\nwhere formula_59 and formula_60 are continuous, non-negative semimartingales; we denote by formula_61 their local times at the origin. These quantities measure the amount of turnover between ranks formula_62 and formula_63 during the time-interval formula_64.\n\nA market is called stochastically stable, if formula_65 converges in distribution as formula_66 to a random vector formula_67 with values in the Weyl chamber \nformula_68 \nof the unit simplex, and if the strong law of large numbers\nholds for suitable real constants formula_70\n\nGiven any two investment strategies formula_71 and a real number formula_72, we say that formula_30 is arbitrage relative to formula_74 over the time-horizon formula_75, if formula_76 and formula_77 both hold; this relative arbitrage is called “strong” if formula_78 When formula_74 is formula_80 we recover the usual definition of arbitrage relative to cash.\nWe say that a given strategy formula_81 has the numeraire property, if for any strategy formula_30 the ratio formula_83 is a formula_84−supermartingale. In such a case, the process formula_85 is called a “deflator” for the market.\n\nNo arbitrage is possible, over any given time horizon, relative to a strategy formula_81 that has the numeraire property (either with respect to the underlying probability measure formula_84, or with respect to any other probability measure which is equivalent to formula_84). A strategy formula_81 with the numeraire property maximizes the asymptotic growth rate from investment, in the sense that\nholds for any strategy formula_30; it also maximizes the expected log-utility from investment, in the sense that for any strategy formula_30 and real number formula_72 we have\nIf the vector formula_95 of instantaneous rates of return, and the matrix formula_96 of instantaneous covariances, are known, then the strategy\nhas the numeraire property whenever the indicated maximum is attained.\n\nThe study of the numeraire portfolio links SPT to the so-called Benchmark approach to Mathematical Finance, which takes such a numeraire portfolio as given and provides a way to price contingent claims, without any further assumptions.\n\nA probability measure formula_98 is called equivalent martingale measure (EMM) on a given time-horizon formula_75, if it has the same null sets as formula_84 on formula_101, and if the processes formula_102 with formula_103 are all formula_98−martingales. Assuming that such an EMM exists, arbitrage is not possible on formula_75 relative to either cash formula_106 or to the market portfolio formula_40 (or more generally, relative to any\nstrategy formula_74 whose wealth process formula_109 is a martingale under some EMM). Conversely, if formula_110 are portfolios and one of them is arbitrage relative to the other on formula_75 then no EMM can exist on this horizon.\n\nSuppose we are given a smooth function formula_112 on some neighborhood \nformula_113 of the unit simplex in formula_114 . We call\nthe portfolio generated by the function formula_116. It can be shown that all the weights of this portfolio are non-negative, if its generating function formula_116 is concave. Under mild conditions, the relative performance of this functionally-generated portfolio formula_118 with respect to the market portfolio formula_40, is given by the F-G decomposition\nwhich involves no stochastic integrals. Here the expression \nis called the drift process of the portfolio (and it is a non-negative quantity if the generating function formula_116 is concave); and the quantities \n\nwith formula_124 are called the relative covariances between formula_125 and formula_126 with respect to the market.\n\n\nThe excess growth rate of the market portfolio admits\nthe representation formula_138 as a capitalization-weighted average relative stock\nvariance. This quantity is nonnegative; if it happens to be bounded away from zero, namely\nfor all formula_56 for some real constant formula_141, then it can be shown using the F-G decomposition that, \nfor every formula_142 there exists a constant formula_133 for which the modified entropic portfolio formula_144 is strict arbitrage relative to the market formula_40 over formula_47; see Fernholz and Karatzas (2005) for details. It is an\nopen question, whether such arbitrage exists over arbitrary time horizons (for two special cases, in\nwhich the answer to this question turns out to be affirmative, please see the paragraph below and\nthe next section).\n\nIf the eigenvalues of the covariance matrix formula_147 are bounded away from both zero and infinity, the condition formula_148 can be shown to be equivalent to diversity, namely formula_149 for a suitable formula_150 Then the diversity-weighted portfolio formula_151 leads to strict arbitrage\nrelative to the market portfolio over sufficiently long time horizons; whereas, suitable modifications\nof this diversity-weighted portfolio realize such strict arbitrage over arbitrary time horizons.\n\nWe consider the example of a system of stochastic differential equations\nwith formula_153 given real constants formula_154 and an formula_5-dimensional Brownian motion \nformula_156 It follows from the work of Bass and Perkins (2002) that this system has a weak solution, which is unique in distribution. Fernholz and Karatzas (2005) show how to construct this solution in terms of scaled and time-changed squared Bessel processes, and prove that the resulting system is coherent.\n\nThe total market capitalization formula_157 behaves here as geometric Brownian motion with drift, and has the same constant growth rate as the largest stock; whereas the excess growth rate of the market\nportfolio is a positive constant. On the other hand, the relative market weights formula_158\nwith formula_159 have the dynamics of multi-allele Wright-Fisher processes. \nThis model is an example of a non-diverse market with unbounded variances, in which strong arbitrage opportunities with respect to the market portfolio formula_40 exist over \"arbitrary time horizons\", as was shown by Banner and Fernholz (2008). Moreover, Pal (2012) derived the joint density of market weights at fixed times and at certain stopping times.\n\nWe fix an integer formula_161 and construct two capitalization-weighted portfolios: one consisting of the top formula_162 stocks, denoted formula_163, and one consisting of the bottom formula_164 stocks, denoted formula_165. More specifically,\nfor formula_167 Fernholz (1999), (2002) showed that the relative performance of the large-stock portfolio with respect to the market is given as\nIndeed, if there is no turnover at the mth rank during the interval formula_47, the fortunes of formula_163 relative\nto the market are determined solely on the basis of how the total capitalization of this sub-universe\nof the formula_162 largest stocks fares, at time formula_172 versus time 0; whenever there is turnover at the formula_162-th rank,\nthough, formula_163 has to sell at a loss a stock that gets “relegated” to the lower league, and buy a stock\nthat has risen in value and been promoted. This accounts for the “leakage” that is evident in the\nlast term, an integral with respect to the cumulative turnover process formula_175 of the relative weight in the large-cap portfolio formula_163 of the stock that occupies the mth rank.\n\nThe reverse situation prevails with the portfolio formula_165 of small stocks, which gets to sell at a profit stocks that are being promoted to the “upper capitalization” league, and buy relatively cheaply stocks that are being relegated:\nIt is clear from these two expressions that, in a \"coherent\" and \"stochastically stable\" market, the small-\nstock cap-weighted portfolio formula_163 will tend to outperform its large-stock counterpart formula_165, at least over\nlarge time horizons and; in particular, we have under those conditions\nThis quantifies the so-called size effect. In Fernholz (1999, 2002), constructions such as these are generalized to include functionally generated portfolios based on ranked market weights.\n\nFirst- and second-order models are hybrid Atlas models that reproduce some of the structure of real stock markets. First-order models have only rank-based parameters, and second-order models have both rank-based and name-based parameters.\n\nSuppose that formula_182 is a coherent market, and that the limits\n\nand\n\nexist for formula_185, where formula_186 is the rank of formula_187. Then the Atlas model formula_188 defined by\n\nwhere formula_190 is the rank of formula_191 and formula_192 is an formula_5-dimensional Brownian motion process, is the first-order model for the original market, formula_182.\n\nUnder reasonable conditions, the capital distribution curve for a first-order model will be close to that of the original market. However, a first-order model is ergodic in the sense that each stock asymptotically spends formula_195-th of its time at each rank, a property that is not present in actual markets. In order to vary the proportion of time that a stock spends at each rank, it is necessary to use some form of hybrid Atlas model with parameters that depend on both rank and name. An effort in this direction was made by Fernholz, Ichiba, and Karatzas (2013), who introduced a second-order model for the market with rank- and name-based growth parameters, and variance parameters that depended on rank alone.\n\n"}
{"id": "8798339", "url": "https://en.wikipedia.org/wiki?curid=8798339", "title": "Supervaluationism", "text": "Supervaluationism\n\nIn philosophical logic, supervaluationism is a semantics for dealing with irreferential singular terms and vagueness. It allows one to apply the tautologies of propositional logic in cases where truth values are undefined.\n\nAccording to supervaluationism, a proposition can have a definite truth value even when its components do not. The proposition \"Pegasus likes licorice\", for example, is often interpreted as having no truth-value given the assumption that the name \"Pegasus\" fails to refer. If indeed reference fails for \"Pegasus\", then it seems as though there is nothing that can justify an assignment of a truth-value to any apparent assertion in which the term \"Pegasus\" occurs. The statement \"Pegasus likes licorice or Pegasus doesn't like licorice\", however, is an instance of the valid schema formula_1 (\"formula_2 or not-formula_2\"), so, according to supervaluationism, it should be true regardless of whether or not its disjuncts have a truth value; that is, it should be true in all interpretations. If, in general, something is true in all precisifications, supervaluationism describes it as \"supertrue\", while something false in all precisifications is described as \"superfalse\".\n\nSupervaluations were first formalized by Bas van Fraassen.\n\nLet \"v\" be a classical valuation defined on every atomic sentence of the language \"L\" and let \"At(x)\" be the number of distinct atomic sentences in \"x\". There are then at most 2^\"At(x)\" classical valuations defined on every sentence \"x\". A supervaluation \"V\" is a function from sentences to truth values such that \"x\" is supertrue (i.e. \"V(x)\"=True) if and only if \"v(x)\"=True for every \"v\". Likewise for superfalse.\n\n\"V(x)\" is undefined when there are exactly two valuations \"v\" and \"v\"* such that \"v(x)\"=True and \"v\"*\"(x)\"=False. For example, let \"Lp\" be the formal translation of \"Pegasus likes licorice\". There are then exactly two classical valuations \"v\" and \"v\"* on \"Lp\", namely \"v(Lp)\"=True and \"v\"*\"(Lp)\"=False. So \"Lp\" is neither supertrue nor superfalse.\n\n\n"}
{"id": "29466441", "url": "https://en.wikipedia.org/wiki?curid=29466441", "title": "System on TPTP", "text": "System on TPTP\n\nSystem on TPTP is an online interface to several automated theorem proving systems and other automated reasoning tools.\nIt allows users to run the systems either on problems from the latest releases from the TPTP problem library or on user-supplied problems in the TPTP syntax.\n\nThe system is maintained by Geoff Sutcliffe at the University of Miami. In November 2010, it featured more than 50 systems, including both theorem provers and model finders. System on TPTP can either run user-selected systems, or pick systems automatically based on problem features, and run them in parallel.\n"}
{"id": "35682641", "url": "https://en.wikipedia.org/wiki?curid=35682641", "title": "Veronese bellringing art", "text": "Veronese bellringing art\n\nVeronese bellringing art is a style of ringing church bells that developed around Verona, Italy from the eighteenth century. The bells are rung full circle (mouth uppermost to mouth uppermost), being held up by a rope and wheel until a note is required.\n\nIn \"The History of Verona\" Ludovico Moscardo records that on the 21 November 622 the bell towers of the city rang to announce the death of Bishop Mauro. It is not known how many towers and bells, but clearly by that date Verona had a tradition of ringing. In the following century the bell \"the storm\" (\"dei temporali\") was cast. It is of octagonal shape and thought to be one of the oldest such castings in Europe. It is now preserved in the San Zeno Museum in Verona.\n\nThe earliest technical information on the casting of Veronese bells is by the master Gislimerio in 1149. He described the casting of the bells for San Zeno Maggiore. Gislimerio was the first of fifty bell founders which worked in Verona over the centuries. Initially the shape of the bells were empirical, however from around 1200 studies were done to determine the best shape. The results were then shared across the continent. Other castings were made in 1065 for San Fermo, 1081 for San Massimo and 1172 for San Salvar.\n\nUp until the 14th century the best bell founders were from Venice, due to its more advanced industry, but there were skilled veronese founders too. Master Jacopo considered by some to be one of the best of the time. In 1370 he cast the bell for the Gardello tower. It is wide and it weighs about 18 quintals or (35-2-24) The tower is a clock tower unattached to a church. The 1370 clock was one of the first striking clocks in the world. The bell is now in the Castelvecchio Museum.\n\nIn the 15th century when Verona was given to Venice there were few native Veronese foundry workers and therefore itinerant workmen worked for the Veronese masters. Bells for San Zeno were made by German workmen and that for the castle of Malcesine by Spaniards. A Frenchman, Mr. Michel, started a company in Verona that continued until the 19th century. He studied the sound of the bells, designing a shape which created a pleasant and tuneful sound. His successors, such as Checcherle and Bonaventurini, continued the development and started to apply decorations making the bells into works of art. Particularly skilled founder was Gasparino which made in 1444 one bell for Santa Maria della Scala, still rung actually. In this century, wonderful belltowers were built up such as Santa Anastasia.\n\nIn the 16th century the number of bells in some towers increased. The first rings of five or six bells were in four churches: San Zeno, Santa Maria in Organo, Santa Maria della Scala and Santa Anastasia. All four were monasteries where the monks rang the bells themselves. In 1557 the Bonaventurini foundry cast the (82-3-24) \"Rengo\" civic bell which is still in good condition today. Reinassence style belltowers are San Nazaro and Santa Maria in Organo.\n\nDuring the 17th century the Da Levo family and their students (one of whom was Pesenti) were casting diatonically tuned bells. Common arrangements were the major chord (doh-me-soh-doh', for example C E G C'), the Gloria (doh-ray-fah, for example C D F) or the subdominant (doh-far-doh', for example C F C'). Significant churches equipped installations were Madonna di Campagna, San Bernardino, San Nicolò and the Cathedral. At the latter the bell ringers were lay musicians living in the city western suburbs who were paid with a reduction in the rent of arable land. The first evidence of wheels and counterbalances dates from this period; from which it is supposed that the bells were starting to be rung in sequence, rather than swung randomly. The Da Levo family specialised in making small and medium size fully decorated bells. In 1653 Pesenti cast the (116-0-8) civic bell of Bergamo, which is still in use today. Pupils of Pesenti were De Rossi, Poni, Larducci and Micheletti (dead 1804).\n\nIn the mid-18th century another three interesting rings were installed. One was made by Crespi foundry for the Monastery of San Fermo, another by Antonio Larducci for Santa Lucia which was the last one made in the French-Veronese Renaissance style. In 1776 Professor Giuseppe Ruffini cast a ring for San Giorgio in Braida. Crespi and Ruffini introduced the \"Manieristica\" shape to Verona. This new shape had originated in the 16th century in the Alps and it became the basis for shapes still used today. The San Giorgio in Braida bells are notable works of art both for decoration and musical precision.\n\nIn San Giorgio in Braida the new bells were hung in such a way that the new style of ringing could develop. The bells are hung for full circle ringing where each bell is swung from balanced mouth uppermost through 360 degrees to again balanced mouth uppermost. This method of ringing permits a precise control of the time that the bell sounds and hence allows music. It is not clear whether hanging the bells in this way was independently developed at San Giorgio or whether the method was imported from England where bells are also hung for full circle ringing. Local farmers who attended church services and had an aptitude for music were chosen to ring and look after maintenance of the bells of San Giorgio. They developed the art of ringing \"concerti\". The new style of music was not appreciated by Holy Roman Emperor Joseph II, but was loved by Pope Pius VI who listened to it in February 1782.\n\nThe ringers were paid for their efforts with food given to them at the start of autumn (fall): \"polenta, salame e vino rosso\" (polenta, salami and red wine). The ringing was quite onerous, on holy days they had to get up very early for the Ave Maria, then play again for the principal mass, the afternoon and evening service and finally at night for the vespers. They eventually used to sell the food, receiving an amount of about current 130 euro per year.\n\nAt around the same time the bells of Santa Maria in Organo were modified, probably to allow them to play concerti. Many of the churches of Verona started to follow this example and the original ringers of San Giorgio in Braida were sent to organize the bands and train the new players.\nThe next tower to be converted was Santissima Trinità in 1803. Next Chievo (1808) and then later the Cathedral, Santi Apostoli, Santo Stefano, San Salvatore Corte Regia and Santa Anastasia. The ringers at Santa Anastasia were a group who worked on the floating mills on the river Adige, behind the apse of the basilica. This demand for new bells led to four bell foundries, managed by the students of Ruffini: Partilora-Selegari, Chiappani and son, and the two Cavadini companies.\nThe only significant adverse effect was the extinction of the older technique of bell ringing as a Carillon. This tradition came from the masters like Vincenzi and Gardoni, some of whom decided to change technique by starting to ring the bells with their new method. An example is Giacomo Milossi (a student of Gardoni) whose skill was praised in a sonnet commemorating the bells of Santa Anastasia. In 1820 the church group from Tomba arrived to give a peal, followed by another wonderful peal in the bell tower of San Tomaso Becket, Quinzano e Parona.\n\nThe priest at the Stimate brought together a band of ringers under the tutelage of Modesto Cainer. Writing in his memoirs he describes the precise methods of playing a concert in rounds with the sacred bronze bell. The Partilora-Selegari foundry equipped the bell towers of San Lorenzo and San Massimo which were the first in the region to have eight bells, each of which required yet another ringing team.\n\nIn 1846 the Cavadini company installed a new ring in San Giovanni in Valle and three years later in San Nazaro. New groups of players were created which tried to compete with the ringers of San Giorgio. At the same time the towers predisposed to the technique of playing in rounds had increased, for example San Michele, Santa Maria del Paradiso, San Paolo, Poiano e Avesa, each tower having new ringers' societies. In this period bellringers in the city centre were about 150, most of whom resided in the suburbs.\n\nThe only remaining supplier of bells after 1850 was Luigi Cavadini, whose company continued until 1974. With the reconstruction of the biggest bell of S. Trinità, some young people from the area created a group of concert players which was then assorbed by San Giorgio, at the time directed by the Peroni brothers and Giacomo Tomasini.\n\nThe same thing happened to Molinari's group. In 1882 the bells of Scalzi and Santa Eufemia were increased in size and number.\n\nIn the church of Santa Maria della Scala a new group was started directed at first by Pietro Sancassani (1881–1972) and then by future maestros Alberti, Oliboni and Signorato. In 1902 in S. Rocco another peal was cast and another ringing society was born.\n\nIn 1903 a new bell tower installation was created in Cà di David, where for the inauguration there was the first competition of bell ringing. This event was organized in a way so that all the teams of the city and also from the suburbs like Chievo and Santa Lucia, had to adhere to the oldest and most prestigious: San Giorgio in Braida. The team of San Giorgio won the competition despite the allegation that the other teams cheated, this event started the rivalry between the city players and those of the province.\n\nThe San Bernardino Church acquired their bells in 1907.\nIn the 1914 some of the young ringers of San Giorgio decided to form their own group directed by Sancassani in Santo Stefano and San Tommaso churches, giving themselves the name \"Audace\" (which means the \"Brave\"). Ten years later the players of San Giorgio and those of San Paolo united and renamed the group \"Società Campanaria Santa Anastasia in Verona\", as the tower increased the number of bells from 6 to 9. The new president was Mario Carregari (1911–1997). During this time, the Audace group was able to transfer to the Cathedral assuming the name of the church. At the same time as the first world war, they started a thirty-year rivalry between the two groups. The only un-warlike group was the Santi Apostoli who didn't last very long.\n\nThis rivalry had the positive effect of leading to further installations: San Leonardo, the imposing San Nicolò all'Arena, Filippini, San Luca, Misericordia, and the Cathedral. The cathedral has nine bells in the major scale the biggest of which now weighs . The bell is the largest bell hung for full circle ringing in the world and with its headstock the rotating mass is over . The tower of San Tommaso all'Isolo was the first in the region to have ten bells.\n\nThe rivalry was all the more intense because in each team there were the most important men, directors, composers, players, maintenance workers, and experts in bells. The battle was also fought with sheet music composition, creating new sounds, pauses, chords and triplets. The company of the teacher Sancassani, more or less, was the winner of the long conflict, but the turbulent spirit of the Audace team didn't leave the members, even if there weren't any more young people. The arguments changed by the day: an unsuccessful competition, disagreement about the management of the money, a proposal made to the other team, a social office not renewed. Every little thing created disagreement because the team was made up of players from San Michele, Tomba, Cà di David, Montorio and San Massimo. From this society, managed by Accordini and Biondani, other groups were created: the Santo Stefano, the Santa Maria in Organo (called the \"Rebel\") and the team of Sabaini-S.Eufemia which stopped spontaneously.\n\nDuring the second world war some of the Veronese bells were destroyed. When the players returned home they all decided to join together with the old San Giorgio team (now Santa Anastasia). Fortunately they were able to give demonstrations in documentaries and exhibitions all over Italy and enjoy the popularity. After the war they gave a bell ringing concert in Borgo Nuovo, Santa Toscana, Tombetta, Palazzina, Golosine, San Giuseppe fuori Le Mura and Borgo Trieste.\n\nSince the fifties, modernity has taken control of Verona lifestyle and the society started to lose interest in church and in bellringing art. It was fashionable for priests to ring the bells electrically which prevented the players from continuing their tradition. In 1983, the creation of a regional bell ringing association managed to gradually turn around the decline, but not in the city of Verona itself, where the downwards trend continued for another twenty years.\n\nIn 1998 the ringing school of St. George was re-opened and in 2010 an event, that had always been dreamed of, took place. The ringers of the city and the suburbs wanted to unite to create a single group (which was the project of M° Sancassani ninety years before), who apart from ringing decided to look together for new members and for a major publicization of the art. A lot of energy was invested in historical, technical, scientific research and the restoring of disused bell towers.\n\nThe bell ringing community of Verona was completely revolutionised and things quickly got better. It was like the old time: churches were inspired to install other bells, because of this the smallest Veronese ring was created in San Carlo. Even though imminent extinction had only just been avoided they had never seen such enthusiasm.\n\nThe old San Giorgio society which had been renamed Santa Anastasia after the first world war now incorporated other teams and changed the name to Scuola Campanaria Verona in S.Anastasia (Bellringing school of Verona based in S.Anastasia church). \nIn the Veneto region there are about 2,500 bell ringers of the Veronese method, united in local teams, more than half of which are members of the \"Associazone Suonatori di Campane a Systema Veronese\" (ASCSV) or association of bell ringers of the Veronese method.\n\nToday new technologically advanced systems, allow the possibility of playing the bells both electrically and manually.\nHowever many bell towers have not yet installed the new system and are obliged to continue to use the old electric only system. The electric system needs very expensive maintenance and can damage the towers.\n\nThe bells are cast from 75% copper and 25% tin bell metal using traditional methods of a loam covered core and outer cope. Complex decoration of a religious nature is applied to the mould.\n\nThe towers are tall with the bells installed at the top. The bells may swing through the tower openings and the sound is much louder than enclosed bell chambers. The clappers are wired on with a safety rope, a broken or detached clapper must not become a missile in the streets surrounding the tower. A heavy counterbalanced headstock reduces the forces on the tower and leads to a slower turning bell. The headstock carries a steel wheel with the rope attached at 3 o'clock rather than the higher 2 o'clock attachment in the English style. No stays or sliders are supplied, the bell must be held at balance when not ringing. The rope is steel which removes the problem of stretch when the bells are rung from ground floor ringing chambers. Being less flexible than natural fibres the steel rope is attached to the wheel with a toggle mechanism rather than being led through a garter hole. The rope is terminated in a natural hemp rope where it is handled. The ropes are plain without the sally associated with English style hanging.\n\nSince the bells are called according to music, the ringers do not need to see each other as a circle. The ropes fall therefore wherever is most convenient for the bell hangers.\n\nSince there is no provision for the bells to be retained in the up position, each piece or \"concerto\" starts with the bells being raised. The bells are either raised sequentially with each bell joining in the rounds in turn, or just pulled up all at once.\n\nThe \"Maestro\", or conductor, calls out each bell or bells to ring. The Maestro does not handle a bell and will read from music. They play slowly moving tunes, not the continuous change ringing of the English tradition. The \"Maestro\" calls out the bells, including two and three bell chords. Each bell uses whichever stroke is available, the ringers holding the bells on balance when not required. The concerto ends with a flourish of chords. Once the concerto is complete the bells are rung down all at the same time, sometimes with the ringers leaving the tower whilst the bells just sort themselves out.\n\nFootnotes\n\nCitations\n\nBibliography\n\n"}
{"id": "4918427", "url": "https://en.wikipedia.org/wiki?curid=4918427", "title": "Wright (ADL)", "text": "Wright (ADL)\n\nIn software architecture, Wright is an architecture description language developed at Carnegie Mellon University. Wright formalizes a software architecture in terms of concepts such as \"components\", \"connectors\", \"roles\", and \"ports\". The dynamic behavior of different ports of an individual component is described using the Communicating Sequential Processes (CSP) process algebra. The roles that different components interacting through a connector can take are also described using CSP. Due to the formal nature of the behavior descriptions, automatic checks of port/role compatibility, and overall system consistency can be performed.\n\nWright was principally developed by Robert Allen and David Garlan.\n\n"}
{"id": "31443389", "url": "https://en.wikipedia.org/wiki?curid=31443389", "title": "Zoghman Mebkhout", "text": "Zoghman Mebkhout\n\nZoghman Mebkhout (born 1949 ) (مبخوت زغمان) is a French-Algerian mathematician known for his work in algebraic analysis, geometry, and representation theory, more precisely on the theory of D-modules.\n\nZoghman is one of the first modern international-caliber North-African mathematicians, a symposium in Spain having been held on his sixtieth birthday, he was also invited to the Institute for Advanced Study on two occasions.\n\nAlexander Grothendieck writes on page 106 of \"Récoltes et Sémailles\":\n\nGrothendieck says that Mebkhout's name was hidden and his role neglected for a theory Zoghman was the first to develop.\n\nZoghman Mebkhout is currently a research director at the French National Centre for Scientific Research. In 2002 Zoghman received the Servant Medal from the CNRS.\n\nZoghman Mebkhout proved in September 1979 the Riemann–Hilbert correspondence, which is a generalization of Hilbert's twenty-first problem to higher dimensions. The original setting was for Riemann surfaces, where it was about the existence of regular differential equations with prescribed monodromy groups. In higher dimensions, Riemann surfaces are replaced by complex manifolds of dimension > 1, and there is a correspondence between certain systems of partial differential equations (linear and having very special properties for their solutions) and possible monodromies of their solutions.\nSee http://adsabs.harvard.edu/abs/1980LNP...126...90M\nThe result was also proved independently by Masaki Kashiwara 8 months later in April 1980. See \"\"Faisceaux constructibles et systemes holonomes d'équations aux derivées partielles linéaires à points singuliers réguliers Se. Goulaouic-Schwartz, 1979–80, Exp. 19.\" \n\nZoghman is now largely known as a specialist in D-modules theory.\n"}
