{"id": "10073693", "url": "https://en.wikipedia.org/wiki?curid=10073693", "title": "1/2 + 1/4 + 1/8 + 1/16 + ⋯", "text": "1/2 + 1/4 + 1/8 + 1/16 + ⋯\n\nIn mathematics, the infinite series is an elementary example of a geometric series that converges absolutely.\n\nThere are many different expressions that can be shown to be equivalent to the problem, such as the form: 2 + 2 + 2 + ...\n\nThe sum of this series can be denoted in summation notation as:\n\nAs with any infinite series, the infinite sum\nis defined to mean the limit of the sum of the first terms \nas approaches infinity.\n\nMultiplying by 2 reveals a useful relationship:\nSubtracting from both sides, \nAs approaches infinity, tends to 1.\n\nThis series was used as a representation of many of Zeno's paradoxes, one of which, Achilles and the Tortoise, is shown here. In the paradox, The warrior Achilles was to race against a tortoise. Achilles could run at 10 m/s, while the tortoise only 5. The tortoise, with a 10 meter advantage, Zeno argued, would win. The Achilles would have to move 10 meters to catch up to the tortoise, but by then, the tortoise would already have moved another five meters. Achilles would then have to move 5 meters, where the tortoise would move 2.5 meters, and so on Zeno argued that the tortoise would always remain ahead of Achilles.\n\nThe parts of the Eye of Horus were once thought to represent the first six summands of the series.\n\n"}
{"id": "208151", "url": "https://en.wikipedia.org/wiki?curid=208151", "title": "10", "text": "10\n\n10 (ten) is an even natural number following 9 and preceding 11. Ten is the base of the decimal numeral system, by far the most common system of denoting numbers in both spoken and written language. The reason for the choice of ten is assumed to be that humans have ten fingers (digits). \n\n\n\n\n\n\n\n\n\n\n\n\n\nAs is the case for any base in its system, ten is the first two-digit number in decimal and thus the lowest number where the position of a numeral affects its value. Any integer written in the decimal system can be multiplied by ten by adding a zero to the end (e.g. 855 × 10 = 8550).\n\nThe Roman numeral for ten is X (which looks like two Vs [the Roman numeral for 5] put together); it is thought that the V for five is derived from an open hand (five digits displayed), and X for ten from both hands. Incidentally, the Chinese word numeral for ten, is also a cross: .\n\nThe digit '1' followed by '0' is how the value of \"p\" is written in base \"p\". (E.g. 16 in hexadecimal is 10.)\n\nThe SI prefix for 10 is \"deca-\".\n\nThe meaning \"10\" is part of the following terms:\n\nAlso, the number 10 plays a role in the following: \n\nThe metric system is based on the number 10, so converting units is done by adding or removing zeros (e.g. 1 centimeter = 10 millimeters, 1 decimeter = 10 centimeters, 1 meter = 100 centimeters, 1 dekameter = 10 meters, 1 kilometer = 1,000 meters).\n\n\n\nMost countries issue coins and bills with a denomination of 10 (See e.g. 10 dollar note). Of these, the U.S. dime, with the value of ten cents, or one tenth of a dollar, derives its name from the meaning \"one-tenth\" − see Dime (United States coin)#Denomination history and etymology.\n\n\n\n\n"}
{"id": "6317418", "url": "https://en.wikipedia.org/wiki?curid=6317418", "title": "243 (number)", "text": "243 (number)\n\n243 (two hundred [and] forty-three) is the natural number following 242 and preceding 244.\n\n243 is:\n\n243 is:\n"}
{"id": "394782", "url": "https://en.wikipedia.org/wiki?curid=394782", "title": "37 (number)", "text": "37 (number)\n\n37 (thirty-seven) is the natural number following 36 and preceding 38.\n\n\n\n\n\n\n\n\n\nJosé María López used this number during his successful years in the World Touring Car Championship from 2014 until 2016. He still uses this number in Formula E since joining in 2016-17 season with DS Virgin Racing.\n\nThirty-seven is:\n\n"}
{"id": "2349", "url": "https://en.wikipedia.org/wiki?curid=2349", "title": "Abstract data type", "text": "Abstract data type\n\nIn computer science, an abstract data type (ADT) is a mathematical model for data types, where a data type is defined by its behavior (semantics) from the point of view of a \"user\" of the data, specifically in terms of possible values, possible operations on data of this type, and the behavior of these operations. This contrasts with data structures, which are concrete representations of data, and are the point of view of an implementer, not a user.\n\nFormally, an ADT may be defined as a \"class of objects whose logical behavior is defined by a set of values and a set of operations\"; this is analogous to an algebraic structure in mathematics. What is meant by \"behavior\" varies by author, with the two main types of formal specifications for behavior being \"axiomatic (algebraic) specification\" and an \"abstract model;\" these correspond to axiomatic semantics and operational semantics of an abstract machine, respectively. Some authors also include the computational complexity (\"cost\"), both in terms of time (for computing operations) and space (for representing values). In practice many common data types are not ADTs, as the abstraction is not perfect, and users must be aware of issues like arithmetic overflow that are due to the representation. For example, integers are often stored as fixed width values (32-bit or 64-bit binary numbers), and thus experience integer overflow if the maximum value is exceeded.\n\nADTs are a theoretical concept in computer science, used in the design and analysis of algorithms, data structures, and software systems, and do not correspond to specific features of computer languages—mainstream computer languages do not directly support formally specified ADTs. However, various language features correspond to certain aspects of ADTs, and are easily confused with ADTs proper; these include abstract types, opaque data types, protocols, and design by contract. ADTs were first proposed by Barbara Liskov and Stephen N. Zilles in 1974, as part of the development of the CLU language.\n\nFor example, integers are an ADT, defined as the values …, −2, −1, 0, 1, 2, …, and by the operations of addition, subtraction, multiplication, and division, together with greater than, less than, etc., which behave according to familiar mathematics (with care for integer division), independently of how the integers are represented by the computer. Explicitly, \"behavior\" includes obeying various axioms (associativity and commutativity of addition etc.), and preconditions on operations (cannot divide by zero). Typically integers are represented in a data structure as binary numbers, most often as two's complement, but might be binary-coded decimal or in ones' complement, but the user is abstracted from the concrete choice of representation, and can simply use the data as data types.\n\nAn ADT consists not only of operations, but also of values of the underlying data and of constraints on the operations. An \"interface\" typically refers only to the operations, and perhaps some of the constraints on the operations, notably pre-conditions and post-conditions, but not other constraints, such as relations between the operations.\n\nFor example, an abstract stack, which is a last-in-first-out structure, could be defined by three operations: push, that inserts a data item onto the stack; pop, that removes a data item from it; and peek or top, that accesses a data item on top of the stack without removal. An abstract queue, which is a first-in-first-out structure, would also have three operations: enqueue, that inserts a data item into the queue; dequeue, that removes the first data item from it; and front, that accesses and serves the first data item in the queue. There would be no way of differentiating these two data types, unless a mathematical constraint is introduced that for a stack specifies that each pop always returns the most recently pushed item that has not been popped yet. When analyzing the efficiency of algorithms that use stacks, one may also specify that all operations take the same time no matter how many data items have been pushed into the stack, and that the stack uses a constant amount of storage for each element.\n\nAbstract data types are purely theoretical entities, used (among other things) to simplify the description of abstract algorithms, to classify and evaluate data structures, and to formally describe the type systems of programming languages. However, an ADT may be implemented by specific data types or data structures, in many ways and in many programming languages; or described in a formal specification language. ADTs are often implemented as modules: the module's interface declares procedures that correspond to the ADT operations, sometimes with comments that describe the constraints. This information hiding strategy allows the implementation of the module to be changed without disturbing the client programs.\n\nThe term abstract data type can also be regarded as a generalized approach of a number of algebraic structures, such as lattices, groups, and rings. The notion of abstract data types is related to the concept of data abstraction, important in object-oriented programming and design by contract methodologies for software development.\n\nAn abstract data type is defined as a mathematical model of the data objects that make up a data type as well as the functions that operate on these objects.\nThere are no standard conventions for defining them. A broad division may be drawn between \"imperative\" and \"functional\" definition styles.\n\nIn the philosophy of imperative programming languages, an abstract data structure is conceived as an entity that is \"mutable\"—meaning that it may be in different \"states\" at different times. Some operations may change the state of the ADT; therefore, the order in which operations are evaluated is important, and the same operation on the same entities may have different effects if executed at different times—just like the instructions of a computer, or the commands and procedures of an imperative language. To underscore this view, it is customary to say that the operations are \"executed\" or \"applied\", rather than \"evaluated\". The imperative style is often used when describing abstract algorithms. (See The Art of Computer Programming by Donald Knuth for more details)\n\nImperative-style definitions of ADT often depend on the concept of an \"abstract variable\", which may be regarded as the simplest non-trivial ADT. An abstract variable \"V\" is a mutable entity that admits two operations:\nwith the constraint that\n\nAs in so many programming languages, the operation store(\"V\", \"x\") is often written \"V\" ← \"x\" (or some similar notation), and fetch(\"V\") is implied whenever a variable \"V\" is used in a context where a value is required. Thus, for example, \"V\" ← \"V\" + 1 is commonly understood to be a shorthand for store(\"V\",fetch(\"V\") + 1).\n\nIn this definition, it is implicitly assumed that storing a value into a variable \"U\" has no effect on the state of a distinct variable \"V\". To make this assumption explicit, one could add the constraint that\n\nMore generally, ADT definitions often assume that any operation that changes the state of one ADT instance has no effect on the state of any other instance (including other instances of the same ADT) — unless the ADT axioms imply that the two instances are connected (aliased) in that sense. For example, when extending the definition of abstract variable to include abstract records, the operation that selects a field from a record variable \"R\" must yield a variable \"V\" that is aliased to that part of \"R\".\n\nThe definition of an abstract variable \"V\" may also restrict the stored values \"x\" to members of a specific set \"X\", called the \"range\" or \"type\" of \"V\". As in programming languages, such restrictions may simplify the description and analysis of algorithms, and improve their readability.\n\nNote that this definition does not imply anything about the result of evaluating fetch(\"V\") when \"V\" is \"un-initialized\", that is, before performing any store operation on \"V\". An algorithm that does so is usually considered invalid, because its effect is not defined. (However, there are some important algorithms whose efficiency strongly depends on the assumption that such a fetch is legal, and returns some arbitrary value in the variable's range.)\n\nSome algorithms need to create new instances of some ADT (such as new variables, or new stacks). To describe such algorithms, one usually includes in the ADT definition a create() operation that yields an instance of the ADT, usually with axioms equivalent to\nThis axiom may be strengthened to exclude also partial aliasing with other instances. On the other hand, this axiom still allows implementations of create() to yield a previously created instance that has become inaccessible to the program.\n\nAs another example, an imperative-style definition of an abstract stack could specify that the state of a stack \"S\" can be modified only by the operations\nwith the constraint that\n\nSince the assignment \"V\" ← \"x\", by definition, cannot change the state of \"S\", this condition implies that \"V\" ← pop(\"S\") restores \"S\" to the state it had before the push(\"S\", \"x\"). From this condition and from the properties of abstract variables, it follows, for example, that the sequence\nwhere \"x\", \"y\", and \"z\" are any values, and \"U\", \"V\", \"W\" are pairwise distinct variables, is equivalent to\n\nHere it is implicitly assumed that operations on a stack instance do not modify the state of any other ADT instance, including other stacks; that is,\n\nAn abstract stack definition usually includes also a Boolean-valued function empty(\"S\") and a create() operation that returns a stack instance, with axioms equivalent to\n\nSometimes an ADT is defined as if only one instance of it existed during the execution of the algorithm, and all operations were applied to that instance, which is not explicitly notated. For example, the abstract stack above could have been defined with operations push(\"x\") and pop(), that operate on \"the\" only existing stack. ADT definitions in this style can be easily rewritten to admit multiple coexisting instances of the ADT, by adding an explicit instance parameter (like \"S\" in the previous example) to every operation that uses or modifies the implicit instance.\n\nOn the other hand, some ADTs cannot be meaningfully defined without assuming multiple instances. This is the case when a single operation takes two distinct instances of the ADT as parameters. For an example, consider augmenting the definition of the abstract stack with an operation compare(\"S\", \"T\") that checks whether the stacks \"S\" and \"T\" contain the same items in the same order.\n\nAnother way to define an ADT, closer to the spirit of functional programming, is to consider each state of the structure as a separate entity. In this view, any operation that modifies the ADT is modeled as a mathematical function that takes the old state as an argument, and returns the new state as part of the result. Unlike the imperative operations, these functions have no side effects. Therefore, the order in which they are evaluated is immaterial, and the same operation applied to the same arguments (including the same input states) will always return the same results (and output states).\n\nIn the functional view, in particular, there is no way (or need) to define an \"abstract variable\" with the semantics of imperative variables (namely, with fetch and store operations). Instead of storing values into variables, one passes them as arguments to functions.\n\nFor example, a complete functional-style definition of an abstract stack could use the three operations:\n\nIn a functional-style definition there is no need for a create operation. Indeed, there is no notion of \"stack instance\". The stack states can be thought of as being potential states of a single stack structure, and two stack states that contain the same values in the same order are considered to be identical states. This view actually mirrors the behavior of some concrete implementations, such as linked lists with hash cons.\n\nInstead of create(), a functional-style definition of an abstract stack may assume the existence of a special stack state, the \"empty stack\", designated by a special symbol like Λ or \"()\"; or define a bottom() operation that takes no arguments and returns this special stack state. Note that the axioms imply that\nIn a functional-style definition of a stack one does not need an empty predicate: instead, one can test whether a stack is empty by testing whether it is equal to Λ.\n\nNote that these axioms do not define the effect of top(\"s\") or pop(\"s\"), unless \"s\" is a stack state returned by a push. Since push leaves the stack non-empty, those two operations are undefined (hence invalid) when \"s\" = Λ. On the other hand, the axioms (and the lack of side effects) imply that push(\"s\", \"x\") = push(\"t\", \"y\") if and only if \"x\" = \"y\" and \"s\" = \"t\".\n\nAs in some other branches of mathematics, it is customary to assume also that the stack states are only those whose existence can be proved from the axioms in a finite number of steps. In the abstract stack example above, this rule means that every stack is a \"finite\" sequence of values, that becomes the empty stack (Λ) after a finite number of pops. By themselves, the axioms above do not exclude the existence of infinite stacks (that can be poped forever, each time yielding a different state) or circular stacks (that return to the same state after a finite number of pops). In particular, they do not exclude states \"s\" such that pop(\"s\") = \"s\" or push(\"s\", \"x\") = \"s\" for some \"x\". However, since one cannot obtain such stack states with the given operations, they are assumed \"not to exist\".\n\nAside from the behavior in terms of axioms, it is also possible to include, in the definition of an ADT operation, their algorithmic complexity. Alexander Stepanov, designer of the C++ Standard Template Library, included complexity guarantees in the STL specification, arguing:\n\nAbstraction provides a promise that any implementation of the ADT has certain properties and abilities; knowing these is all that is required to make use of an ADT object. The user does not need any technical knowledge of how the implementation works to use the ADT. In this way, the implementation may be complex but will be encapsulated in a simple interface when it is actually used.\n\nCode that uses an ADT object will not need to be edited if the implementation of the ADT is changed. Since any changes to the implementation must still comply with the interface, and since code using an ADT object may only refer to properties and abilities specified in the interface, changes may be made to the implementation without requiring any changes in code where the ADT is used.\n\nDifferent implementations of the ADT, having all the same properties and abilities, are equivalent and may be used somewhat interchangeably in code that uses the ADT. This gives a great deal of flexibility when using ADT objects in different situations. For example, different implementations of the ADT may be more efficient in different situations; it is possible to use each in the situation where they are preferable, thus increasing overall efficiency.\n\nSome operations that are often specified for ADTs (possibly under other names) are\n\nIn imperative-style ADT definitions, one often finds also\n\nThe free operation is not normally relevant or meaningful, since ADTs are theoretical entities that do not \"use memory\". However, it may be necessary when one needs to analyze the storage used by an algorithm that uses the ADT. In that case one needs additional axioms that specify how much memory each ADT instance uses, as a function of its state, and how much of it is returned to the pool by free.\n\nSome common ADTs, which have proved useful in a great variety of applications, are\n\nEach of these ADTs may be defined in many ways and variants, not necessarily equivalent. For example, an abstract stack may or may not have a count operation that tells how many items have been pushed and not yet popped. This choice makes a difference not only for its clients but also for the implementation.\n\nAn extension of ADT for computer graphics was proposed in 1979: an abstract graphical data type (AGDT). It was introduced by Nadia Magnenat Thalmann, and Daniel Thalmann. AGDTs provide the advantages of ADTs with facilities to build graphical objects in a structured way.\n\nImplementing an ADT means providing one procedure or function for each abstract operation. The ADT instances are represented by some concrete data structure that is manipulated by those procedures, according to the ADT's specifications.\n\nUsually there are many ways to implement the same ADT, using several different concrete data structures. Thus, for example, an abstract stack can be implemented by a linked list or by an array.\n\nIn order to prevent clients from depending on the implementation, an ADT is often packaged as an \"opaque data type\" in one or more modules, whose interface contains only the signature (number and types of the parameters and results) of the operations. The implementation of the module—namely, the bodies of the procedures and the concrete data structure used—can then be hidden from most clients of the module. This makes it possible to change the implementation without affecting the clients. If the implementation is exposed, it is known instead as a \"transparent data type.\"\n\nWhen implementing an ADT, each instance (in imperative-style definitions) or each state (in functional-style definitions) is usually represented by a handle of some sort.\n\nModern object-oriented languages, such as C++ and Java, support a form of abstract data types. When a class is used as a type, it is an abstract type that refers to a hidden representation. In this model an ADT is typically implemented as a class, and each instance of the ADT is usually an object of that class. The module's interface typically declares the constructors as ordinary procedures, and most of the other ADT operations as methods of that class. However, such an approach does not easily encapsulate multiple representational variants found in an ADT. It also can undermine the extensibility of object-oriented programs.\nIn a pure object-oriented program that uses interfaces as types, types refer to behaviors not representations.\n\nAs an example, here is an implementation of the abstract stack above in the C programming language.\nAn imperative-style interface might be:\nThis interface could be used in the following manner:\nThis interface can be implemented in many ways. The implementation may be arbitrarily inefficient, since the formal definition of the ADT, above, does not specify how much space the stack may use, nor how long each operation should take. It also does not specify whether the stack state \"s\" continues to exist after a call \"x\" ← pop(\"s\").\n\nIn practice the formal definition should specify that the space is proportional to the number of items pushed and not yet popped; and that every one of the operations above must finish in a constant amount of time, independently of that number. To comply with these additional specifications, the implementation could use a linked list, or an array (with dynamic resizing) together with two integers (an item count and the array size).\n\nFunctional-style ADT definitions are more appropriate for functional programming languages, and vice versa. However, one can provide a functional-style interface even in an imperative language like C. For example:\nMany modern programming languages, such as C++ and Java, come with standard libraries that implement several common ADTs, such as those listed above.\n\nThe specification of some programming languages is intentionally vague about the representation of certain built-in data types, defining only the operations that can be done on them. Therefore, those types can be viewed as \"built-in ADTs\". Examples are the arrays in many scripting languages, such as Awk, Lua, and Perl, which can be regarded as an implementation of the abstract list.\n\n\n"}
{"id": "14457637", "url": "https://en.wikipedia.org/wiki?curid=14457637", "title": "Andrey Markov Jr.", "text": "Andrey Markov Jr.\n\nAndrey Andreyevich Markov Jr. (; St. Petersburg, September 22, 1903 – Moscow, October 11, 1979) was a Soviet mathematician, the son of the Russian mathematician Andrey Andreyevich Markov Sr, and one of the key founders of the Russian school of constructive mathematics and logic. He made outstanding contributions to various areas of mathematics, including differential equations, topology, mathematical logic and the foundations of mathematics.\n\nHis name is in particular associated with Markov's principle, Markov's rule, and Markov algorithm. An important result which he proved in 1947 was that the word problem for semigroups was unsolvable; Emil Post obtained the same result independently at about the same time. In 1953 he became a member of the Communist Party.\n\nIn 1960, Markov obtained fundamental results showing that the classification of four-dimensional manifolds is undecidable: no general algorithm exists for distinguishing two arbitrary manifolds with four or more dimensions. This is because four dimensional manifolds have sufficient flexibility to allow us to embed any algorithm within their structure, so that classification of all four-manifolds would imply a solution to Turing's halting problem. This result has profound implications for the limitations of mathematical analysis.\n"}
{"id": "348004", "url": "https://en.wikipedia.org/wiki?curid=348004", "title": "Arg max", "text": "Arg max\n\nIn mathematics, the arguments of the maxima (abbreviated arg max or argmax) are the points of the domain of some function at which the function values are maximized. In contrast to global maxima, referring to the largest \"outputs\" of a function, arg max refers to the \"inputs\", or arguments, at which the function outputs are as large as possible.\n\nGiven an arbitrary set \"X\", a totally ordered set \"Y\", and a function, formula_1, the arg max over some subset, \"S\", of \"X\" is defined by\n\nIf \"S\" = \"X\" or \"S\" is clear from the context, then \"S\" is often left out, as in formula_3 In other words, arg max is the set of points, \"x,\" for which \"f\"(\"x\") attains the function's largest value (if it exists). Arg max may be the empty set, a singleton, or contain multiple elements. For example, if \"f\"(\"x\") is 1−|\"x\"|, then \"f\" attains its maximum value of 1 if and only if \"x\" = 0, implying\n\nThe arg max operator is to be distinguished from the max operator which, given the same function, returns the maximum value instead of the point or points that reach that value; in other words\n\nLike arg max, max may be the empty set (in which case the maximum is undefined) or a singleton, but unlike arg max, max may not contain multiple elements: for example, if \"f\"(\"x\") is , then formula_7, but formula_8 because the function attains the same value at every element of arg max.\n\nEquivalently, if \"M\" is the maximum of \"f\", then the arg max is the level set of the maximum:\n\nWe can rearrange to give the simple identity\n\nIf the maximum is reached at a single point then this point is often referred to as \"the\" arg max, and arg max is considered a point, not a set of points. So, for example,\n\n(rather than the singleton set {5}), since the maximum value of is 25, which occurs for \"x\" = 5. However, in case the maximum is reached at many points, arg max needs to be considered a \"set\" of points.\n\nFor example\n\nsince the maximum value of cos(\"x\") is 1, which occurs on this interval for \"x\" = 0, 2π or 4π. On the whole real line, the arg max is the infinite set formula_13\n\nFunctions need not in general attain a maximum value, and hence the arg max is sometimes the empty set; for example, formula_14, since formula_15 is unbounded on the real line. As another example, formula_16, although \"arc tan\" is bounded by ±π/2. However, by the extreme value theorem, a continuous real-valued function on a closed interval has a maximum, and thus a nonempty arg max.\n\narg min (or argmin) stands for argument of the minimum, and is defined analogously. For instance,\n\nare points \"x\" for which \"f\"(\"x\") attains its smallest value. The complementary operator is argmax.\n\n"}
{"id": "39067533", "url": "https://en.wikipedia.org/wiki?curid=39067533", "title": "Autowave reverberator", "text": "Autowave reverberator\n\nIn the theory of autowave phenomena an autowave reverberator is an autowave vortex in a two-dimensional active medium.\n\nA reverberator appears a result of a rupture in the front of a plane autowave. Such a rupture may occur, for example, via collision of the front with a nonexcitable obstacle. In this case, depending on the conditions, either of two phenomena may arise: a \"spiral wave\", which rotates around the obstacle, or an \"autowave reverberator\" which rotates with its tip free.\n\nThe \"reverberator\" was one of the first autowave solutions, researchers found, and, because of this historical context, it remains by nowadays the most studied autowave object.\n\nUp until the late 20th century, the term \"auto-wave reverberator\" was used very active and widely in the scientific literature, written by soviet authors, because of active developing these investigations in USSR (for more details, see \"A brief history of autowave researches\" in Autowave). And, inasmuch as the soviet scientific literature was very often republished in English translation (see e.g.), the term \"autowave reverberator\" became known also in English-speaking countries.\n\nThe \"reverberator\" is often confused with another state of the active medium, which is similar to it, - with the \"spiral wave\". Indeed, at a superficial glance, these two autowave solutions look almost identical. Moreover, the situation is further complicated by the fact that the spiral wave may under certain circumstances become the reverberator, and the reverberator may, on the contrary, become the spiral wave!\n\nHowever, it must be remembered that many features of \"rotating autowaves\" were quite thoroughly studied as long ago as the 1970s, and already at that time some significant differences in properties of a spiral wave and a reverberator were revealed. Unfortunately, all the detailed knowledge from those years remains now scattered in different publications of the 1970-1990s, which became little-known now even for the new generations of researchers, not to mention the people that are far from this research topic. Perhaps, the only book in that it were more or less completely brought together in the form of abstracts basic information about autowaves, known at the time of its publication, remains still the Proceedings „Autowave processes in systems with diffusion“, which was published in 1981 and became already a rare bibliographic edition in nowadays; its content was partially reiterated in another book in 2009.\n\nThe differences between a reverberator and a spiral wave are considered below in detail. But for the beginning it is useful to demonstrate these differences with a simple analogy. Everyone knows well the seasons of a year... Under some conditions, winter can turn into summer, and summer, on the contrary, into winter; and, moreover, these miraculous transformations occur quite regularly! However, though a winter and a summer are similar, for example, in regular alternation of day and night, you cannot think of saying that winter and summer are the same thing, can you? Nearly the same things are with reverberator and spiral waves; and therefore they should not be confused.\n\nIt is useful also to keep in mind that it is known now, in addition to the rotating-wave, quite a number of other autowave solutions, and every year the number grows continuously with increasing speed. Because of these causes (or as a result of these events), it was found during the 21st century that many of the conclusions about the properties of autowaves, - which were widely known among readers of the early papers on the subject as well as widely discussed in the press of that time, - unfortunately, proved to be a sort of erroneous hasty generalizations.\n\nVarious autowave regimes, such as \"plane waves\" or \"spiral waves\" can exist in an active media, but only under certain conditions on the medium properties. Using the FitzhHugh-Nagumo model for a generic active medium, Winfree constructed a diagram depicting the regions of parameter space in which the principle phenomena may be observed. Such diagrams are a common way of presenting the different dynamical regimes observed in both experimental and theoretical settings. They are sometimes called \"flower gardens\" since the paths traced by autowave tips may often resemble the petals of a flower. A flower garden for the FitzHugh-Nagumo model is shown to the right. It contains: the line \"∂P\", which confines the range of the model parameters under which impulses can propagate through one-dimensional medium, and \"plane autowaves\" can spread in the two-dimensional medium; the \"rotor boundary\" \"∂R\", which confines the range of the parameters under which there can be the reverberators rotating around fixed cores (i.e. performing uniform circular rotation); the \"meander\" boundary \"∂M\" and the \"hyper-meander\" boundary \"∂C\", which confine the areas where two-period and more complex (possibly chaotic) regimes can exist. Rotating autowaves with large cores exist only in the areas with parameters close to the boundary \"∂R\".\n\nSimilar autowave regimes were also obtained for the other models — Beeler-Reuter model, Barkley model, Aliev-Panfilov model, Fenton-Karma model etc.\n\nIt was also shown that these simple autowave regimes should be common to all active media because a system of differential equations of any complexity, which describes this or that active medium, can be always simplified to two equations.\n\nIn the simplest case without drift (i.e., the regime of \"uniform circular rotation\"), the tip of a reverberator rotates around a fixed point along the circumference of a certain radius (the circular motion of the \"tip of the reverberator\"). The autowave cannot penetrate into the circle bounded by this circumference. As far as it approaches the centre of the reverberator rotation, the amplitude of the excitation pulse is reduced, and, at a relatively low excitability of the medium there is a region of finite size in the centre of reverberator, where the amplitude of the excitation pulse is zero (recall that we speak now about a homogeneous medium, for each point of which its properties are the same). This area of low amplitude in the centre of the reverberator is usually called \"the core of the reverberator\". The existence of such a region in the center of reverberator seems, at first glance, quite incomprehensible, as it borders all the time with the excited sites. A detailed investigation of this phenomenon showed that resting area in the centre of reverberator remains of its normal excitability, and the existence of a quiescent region in the centre of the reverberator is related to the phenomenon of the critical curvature. In the case of \"infinite\" homogeneous medium, the core radius and the speed of the rotor rotation are determined only by the properties of the medium itself, rather than the initial conditions. The shape of the front of the rotating spiral wave in the distance from the centre of rotation is close to the evolvent of the circumference - the boundaries of its core. The certain size of the core of the reverberator is conditioned by that the excitation wave, which circulates in a closed path, should completely fit in this path without bumping into its own refractory tail.\n\nAs the \"critical size\" of the reverberator, it is understood as the minimum size of the homogeneous medium in which the reverberator can exist indefinitely. For assessing the critical size of the reverberator one uses sometimes the size of its core, assuming that adjacent to the core region of the medium should be sufficient for the existence of sustainable re-entry. However, the quantitative study of the dependence of the reverberator behaviour on conductivity of rapid transmembrane current (that characterize the excitability of the medium), it was found that the critical size of the reverberator and the size its core are its different characteristics, and the critical size of the reverberator is much greater, in many cases, than the size of its core (i.e. reverberator dies, even the case, if its core fits easily in the boundaries of the medium and its drift is absent)\n\nAt meander and hyper-meander, the displacement of the center of autowave rotation (i.e. its drift) is influenced by the forces generated by the very same rotating autowave.\n\nHowever, in result of the scientific study of rotating autowaves was also identified a number of external conditions that force reverberator drift. It can be, for example, the heterogeneity of the active medium by any parameter. Perhaps, it is the works Biktasheva, where different types of the reverberator drift are currently represented the most completely (although there are other authors who are also involved in the study of drift of the autowave reverberator).\n\nIn particular, Biktashev offers to distinguish the following types of reverberator drift in the active medium:\n\nNote that even for such a simple question, what should be called a drift of autowaves, and what should not be called, there is still no agreement among researchers. Some researchers (mostly mathematicians) tends to consider as reverberator drift only those of its displacement, which occur under the influence of external events (and this view is determined exactly by the peculiarity of the mathematical approach to the study of autowaves). The other part of the researchers did not find significant differences between the spontaneous displacement of reverberator in result of the events generated by it itself, and its displacement as a result of external influences; and therefore these researchers tend to believe that meander and hyper-meander are also variants of drift, namely \"the spontaneous drift of the reverberator\". There was not debate on this question of terminology in the scientific literature, but it can be found easily these features of describing the same phenomena by the different authors.\n\nIn the numerical study of reverberator using the Aliev-Panfilov model, the phenomenon of bifurcation memory was revealed, when the reverberator changes spontaneously its behaviour from \"meander\" to \"uniform circular rotation\"; this new regime was named \"autowave lacet\".\n\nBriefly, spontaneous deceleration of the reverberator drift by the forces generated by the reverberator itself occurs during the autowave lacet, with the velocity of its drift decreasing gradually down to zero in the result. The regime meander thus degenerates into a simple uniform circular rotation. As already mentioned, this unusual process is related to phenomenon of bifurcation memory.\n\nWhen autowave lacet was discovered, the first question arose: Does the \"meander\" exist ever or the halt of the reverberator drift can be observed every time in all the cases, which are called meander, if the observation will be sufficiently long? The comparative quantitative analysis of the drift velocity of reverberator in the regimes of \"meander\" and \"lacet\" revealed a clear difference between these two types of evolution of the reverberator: while the drift velocity quickly goes to a stationary value during meander, a steady decrease in the drift velocity of the vortex can be observed during the lacet, in which can be clearly identified the phase of slow deceleration and phase of rapid deceleration of the drift velocity.\n\nThe revealing of autowave lacet may be important for cardiology. It is known that reverberators show remarkable stability of their properties, they behave \"at their discretion\", and their behaviour can significantly affect only the events that occur near the tip of reverberator. The fact that the behaviour of the reverberator can significantly affected only by the events that occur near its core, results, for example, in the fact that, at a meeting with reverberator nonexcitability heterogeneity (e.g. small myocardial scar), the tip of the rotating wave \"sticks\" to this heterogeneity, and reverberator begins to rotate around the stationary nonexcitability obstacles. The transition from polymorphic to monomorphic tachycardia is observed on the ECG in such cases. This phenomenon is called the \"anchoring\" of spiral wave.\nHowever, it was found in the simulations that spontaneous transition of polymorphic tachycardia in monomorphic one can be observed also on the ECG during the autowave lacet; in other words, the \"lacet\" may be another mechanism of transformation of polymorphic ventricular tachycardia in a monomorphic. Thus, the autowave theory predicts the existence of special type of ventricular arrhythmias, conditionally called \"lacetic\", which cardiologists do not still distinguish in diagnostics.\n\nRecall that from 1970th to the present time it is customary to distinguish three variants rotating autowaves: \nDimensions of the core of reverberator is usually less than the minimal critical size of the circular path of circulation, which is associated with the phenomenon of \"critical curvature\". In addition, the refractory period appears to be longer for the waves with non-zero curvature (reverberator and spiral wave) and begins to increase with decreasing the excitability of the medium before the refractory period for the plane waves (in the case of circular rotation). These and other significant differences between the reverberator and the circular rotation of excitation wave make us distinguish these two regimes of re-entry.\n\nThe figure shows the differences found in the behavior of the plane autowave circulating in the ring and reverberator. You can see that, in the same local characteristics of the excitable medium (excitability, refractoriness, etc., given by the nonlinear member), there are significant quantitative differences between dependencies of the reverberator characteristics and characteristics of the regime of one-dimensional rotation of impulse, although respective dependencies match qualitatively.\n\n"}
{"id": "30340342", "url": "https://en.wikipedia.org/wiki?curid=30340342", "title": "Bounded type (mathematics)", "text": "Bounded type (mathematics)\n\nIn mathematics, a function defined on a region of the complex plane is said to be of bounded type if it is equal to the ratio of two analytic functions bounded in that region. But more generally, a function is of bounded type in a region formula_1 if and only if formula_2 is analytic on formula_1 and formula_4 has a harmonic majorant on formula_5 where formula_6. Being the ratio of two bounded analytic functions is a sufficient condition for a function to be of bounded type (defined in terms of a harmonic majorant), and if formula_1 is simply connected the condition is also necessary.\n\nThe class of all such formula_2 on formula_1 is commonly denoted formula_10 and is sometimes called the \"Nevanlinna class\" for formula_1. The Nevanlinna class includes all the Hardy classes.\n\nFunctions of bounded type are not necessarily bounded, nor do they have a property called \"type\" which is bounded. The reason for the name is probably that when defined on a disc, the Nevanlinna characteristic (a function of distance from the centre of the disc) is bounded.\n\nClearly, if a function is the ratio of two bounded functions, then it can be expressed as the ratio of two functions which are bounded by 1:\nThe logarithms of formula_13 and of formula_14 are non-negative in the region, so\n\nThe latter is the real part of an analytic function and is therefore harmonic, showing that formula_4 has a harmonic majorant on Ω.\n\nFor a given region, sums, differences, and products of functions of bounded type are of bounded type, as is the quotient of two such functions as long as the denominator is not identically zero.\n\nPolynomials are of bounded type in any bounded region. They are also of bounded type in the upper half-plane (UHP), because a polynomial formula_18 of degree \"n\" can be expressed as a ratio of two analytic functions bounded in the UHP:\n\nwith\n\nThe inverse of a polynomial is also of bounded type in a region, as is any rational function.\n\nThe function formula_22 is of bounded type in the UHP if and only if \"a\" is real. If \"a\" is positive the function itself is bounded in the UHP (so we can use formula_23), and if \"a\" is negative then the function equals 1/Q(z) with formula_24.\n\nSine and cosine are of bounded type in the UHP. Indeed,\n\nwith\n\nboth of which are bounded in the UHP.\n\nAll of the above examples are of bounded type in the lower half-plane as well, using different \"P\" and \"Q\" functions. But the region mentioned in the definition of the term \"bounded type\" cannot be the whole complex plane unless the function is constant because one must use the same \"P\" and \"Q\" over the whole region, and the only entire functions (that is, analytic in the whole complex plane) which are bounded are constants, by Liouville's theorem.\n\nAnother example in the upper half-plane is a \"Nevanlinna function\", that is, an analytic function that maps the UHP to the closed UHP. If \"f\"(\"z\") is of this type, then\n\nwhere \"P\" and \"Q\" are the bounded functions:\n\nFor a given region, the sum, product, or quotient of two (non-null) functions of bounded type is also of bounded type. The set of functions of bounded type is an algebra over the complex numbers and is in fact a field.\n\nAny function of bounded type in the upper half-plane (with a finite number of roots in some neighborhood of 0) can be expressed as a Blaschke product (an analytic function, bounded in the region, which factors out the zeros) multiplying the quotient formula_32 where formula_33 and formula_34 are bounded by 1 and have no zeros in the UHP. One can then express this quotient as\n\nwhere formula_36 and formula_37 are analytic functions having non-negative real part in the UHP. Each of these in turn can be expressed by a Poisson representation (see Nevanlinna functions):\n\nwhere \"c\" and \"d\" are imaginary constants, \"p\" and \"q\" are non-negative real constants, and μ and ν are non-decreasing functions of a real variable (well behaved so the integrals converge). The difference \"q−p\" has been given the name \"mean type\" by Louis de Branges and describes the growth or decay of the function along the imaginary axis:\n\nThe mean type in the upper half-plane is the limit of a weighted average of the logarithm of the function's absolute value divided by distance from zero, normalized in such a way that the value for formula_41 is 1:\n\nIf an entire function is of bounded type in both the upper and the lower half-plane then it is of exponential type equal to the higher of the two respective \"mean types\" (and the higher one will be non-negative). An entire function of order greater than 1 (which means that in some direction it grows faster than a function of exponential type) cannot be of bounded type in any half-plane.\n\nWe may thus produce a function of bounded type by using an appropriate exponential of \"z\" and exponentials of arbitrary Nevanlinna functions multiplied by \"i\", for example:\n\nConcerning the examples given above, the mean type of polynomials or their inverses is zero. The mean type of formula_22 in the upper half-plane is −\"a\", while in the lower half-plane it is \"a\". The mean type of formula_45 in both half-planes is 1.\n\nFunctions of bounded type in the upper half-plane with non-positive mean type and having a continuous, square-integrable extension to the real axis have the interesting property (useful in applications) that the integral (along the real axis)\n\nequals formula_18 if \"z\" is in the upper half-plane and zero if \"z\" is in the lower half-plane. This may be termed the Cauchy formula for the upper half-plane.\n\n\n"}
{"id": "5748450", "url": "https://en.wikipedia.org/wiki?curid=5748450", "title": "Bus encryption", "text": "Bus encryption\n\nBus encryption is the use of encrypted program instructions on a data bus in a computer that includes a secure cryptoprocessor for executing the encrypted instructions. Bus encryption is used primarily in electronic systems that require high security, such as automated teller machines, TV set-top boxes, and secure data communication devices such as two-way digital radios.\n\nBus encryption can also mean encrypted data transmission on a data bus from one processor to another processor. For example, from the CPU to a GPU which does not require input of encrypted instructions. Such bus encryption is used by Windows Vista and newer Microsoft operating systems to protect certificates, BIOS, passwords, and program authenticity. PVP-UAB (Protected Video Path) provides bus encryption of premium video content in PCs as it passes over the PCIe bus to graphics cards to enforce Digital rights management.\n\nThe need for bus encryption arises when multiple people have access to the internal circuitry of an electronic system, either because they service and repair such systems, stock spare components for the systems, own the system, steal the system, or find a lost or abandoned system. Bus encryption is necessary not only to prevent tampering of encrypted instructions that may be easily discovered on a data bus or during data transmission, but also to prevent discovery of decrypted instructions that may reveal security weaknesses that an intruder can exploit.\n\nIn TV set-top boxes, it is necessary to download program instructions periodically to customer's units to provide new features and to fix bugs. These new instructions are encrypted before transmission, but must also remain secure on data buses and during execution to prevent the manufacture of unauthorized cable TV boxes. This can be accomplished by secure crypto-processors that read encrypted instructions on the data bus from external data memory, decrypt the instructions in the cryptoprocessor, and execute the instructions in the same cryptoprocessor.\n\n\n"}
{"id": "3668370", "url": "https://en.wikipedia.org/wiki?curid=3668370", "title": "Carathéodory–Jacobi–Lie theorem", "text": "Carathéodory–Jacobi–Lie theorem\n\nThe Carathéodory–Jacobi–Lie theorem is a theorem in symplectic geometry which generalizes Darboux's theorem.\n\nLet \"M\" be a 2\"n\"-dimensional symplectic manifold with symplectic form ω. For \"p\" ∈ \"M\" and \"r\" ≤ \"n\", let \"f\", \"f\", ..., \"f\" be smooth functions defined on an open neighborhood \"V\" of \"p\" whose differentials are linearly independent at each point, or equivalently\n\nwhere {f, f} = 0. (In other words, they are pairwise in involution.) Here {–,–} is the Poisson bracket. Then there are functions \"f\", ..., \"f\", \"g\", \"g\", ..., \"g\" defined on an open neighborhood \"U\" ⊂ \"V\" of \"p\" such that (f, g) is a symplectic chart of \"M\", i.e., ω is expressed on \"U\" as\n\nAs a direct application we have the following. Given a Hamiltonian system as formula_3 where \"M\" is a symplectic manifold with symplectic form formula_4 and \"H\" is the Hamiltonian function, around every point where formula_5 there is a symplectic chart such that one of its coordinates is \"H\".\n\n"}
{"id": "1034358", "url": "https://en.wikipedia.org/wiki?curid=1034358", "title": "Chirplet transform", "text": "Chirplet transform\n\nIn signal processing, the chirplet transform is an inner product of an input signal with a family of analysis primitives called chirplets.\n\nSimilar to the wavelet transform, chirplets are usually generated from (or can be expressed as being from) a single \"mother chirplet\" (analogous to the so-called \"mother wavelet\" of wavelet theory).\n\nThe term \"chirplet transform\" was coined by Steve Mann, as the title of the first published paper on chirplets. The term \"chirplet\" itself (apart from chirplet transform) was also used by Steve Mann, Domingo Mihovilovic, and Ronald Bracewell to describe a windowed portion of a chirp function. In Mann's words:\n\nThe chirplet transform thus represents a rotated, sheared, or otherwise transformed tiling of the time–frequency plane. Although chirp signals have been known for many years in radar, pulse compression, and the like, the first published reference to the \"chirplet transform\" described specific signal representations based on families of functions related to one another by time–varying frequency modulation or frequency varying time modulation, in addition to time and frequency shifting, and scale changes. In that paper, the Gaussian chirplet transform was presented as one such example, together with a successful application to ice fragment detection in radar (improving target detection results over previous approaches). The term \"chirplet\" (but not the term \"chirplet transform\") was also proposed for a similar transform, apparently independently, by Mihovilovic and Bracewell later that same year.\n\nThe chirplet transform is a useful signal analysis and representation framework that has been used to excise\nchirp-like interference in spread spectrum communications, \nin EEG processing, \nand Chirplet Time Domain Reflectometry.\n\nThe warblet transform is a particular example of the chirplet transform introduced by Mann and Haykin in 1992 and now widely used. It provides a signal representation based on cyclically varying frequency modulated signals (warbling signals).\n\n\n\nFlorian Bossmann, Jianwei Ma, Asymmetric chirplet transform--Part 2: phase, frequency, and chirp rate, Geophysics, 2016, 81 (6), V425-V439.\n\nFlorian Bossmann, Jianwei Ma, Asymmetric chirplet transform for sparse representation of seismic data, Geophysics, 2015, 80 (6), WD89-WD100.\n\n"}
{"id": "459453", "url": "https://en.wikipedia.org/wiki?curid=459453", "title": "Common operator notation", "text": "Common operator notation\n\nIn programming languages, scientific calculators and similar common operator notation or operator grammar is a way to define and analyse mathematical and other formal expressions. In this model a linear sequence of tokens are divided into two classes: operators and operands.\n\nOperands are objects upon which the operators operate. These include literal numbers and other constants as well as identifiers (names) which may represent anything from simple scalar variables to complex aggregated structures and objects, depending on the complexity and capability of the language at hand as well as usage context. One special type of operand is the parenthesis group. An expression enclosed in parentheses is typically recursively evaluated to be treated as a single operand on the next evaluation level.\n\nEach operator is given a position, precedence, and an associativity. The operator precedence is a number (from high to low or vice versa) that defines which operator that takes an operand surrounded by two operators of different precedence (or priority). Multiplication normally has higher precedence than addition, for example, so 3+4×5 = 3+(4×5) ≠ (3+4)×5.\n\nIn terms of operator position, an operator may be prefix, postfix, or infix. A prefix operator immediately precedes its operand, as in −x. A postfix operator immediately succeeds its operand, as in x! for instance. An infix operator is positioned in between a left and a right operand, as in x+y. Some languages, most notably the C-syntax family, stretches this conventional terminology and speaks also of \"ternary\" infix operators (a?b:c). Theoretically it would even be possible (but not necessarily practical) to define parenthesization as a unary bifix operation.\n\nOperator associativity determines what happens when an operand is surrounded by operators of the same precedence, as in 1-2-3: An operator can be left-associative, right-associative, or non-associative. Left-associative operators are applied to operands in left-to-right order while right-associative operators are the other way round. The basic arithmetic operators are normally all left-associative, which means that 1-2-3 = (1-2)-3 ≠ 1-(2-3), for instance. This does not hold true for higher operators. For example, exponentiation is normally right-associative in mathematics, but is implemented as left-associative in some computer applications like Excel. In programming languages where assignment is implemented as an operator, that operator is often right-associative. If so, a statement like would be equivalent to , which means that the value of c is copied to b which is then copied to a. An operator which is non-associative cannot compete for operands with operators of equal precedence. In Prolog for example, the infix operator is non-associative, so constructs such as are syntax errors. \nUnary prefix operators such as − (negation) or sin (trigonometric function) are typically associative prefix operators. When more than one associative prefix or postfix operator of equal precedence precedes or succeeds an operand, the operators closest to the operand goes first. So −sin x = −(sin x), and sin -x = sin(-x).\n\nMathematically oriented languages (such as on scientific calculators) sometimes allow implicit multiplication with higher priority than prefix operators (such as sin), so that sin 2x+1 = (sin(2x))+1, for instance.\n\nHowever, prefix (and postfix) operators do not \"necessarily\" have higher precedence than all infix operators. Some (hypothetical) programming language may well have an operator called sin with a precedence lower than × but higher than + for instance. In such a language, sin 2·x+1 = sin(2·x)+1 would be true, instead of (sin 2)·x+1, as would normally be the case.\n\nThe rules for expression evaluation are usually three-fold:\n\nSome more examples:\n\nThe use of operator precedence classes and associativities is just one way. However, it is not the most general way: this model cannot give an operator more precedence when competing with '−' than it can when competing with '+', while still giving '+' and '−' equivalent precedences and associativities. A generalized version of this model (in which each operator can be given independent left and right precedences) can be found at .\n\n"}
{"id": "680261", "url": "https://en.wikipedia.org/wiki?curid=680261", "title": "Compact element", "text": "Compact element\n\nIn the mathematical area of order theory, the compact or finite elements of a partially ordered set are those elements that cannot be subsumed by a supremum of any non-empty directed set that does not already contain members above the compact element.\n\nNote that there are other notions of compactness in mathematics; also, the term \"finite\" in its normal set theoretic meaning does not coincide with the order-theoretic notion of a \"finite element\".\n\nIn a partially ordered set (\"P\",≤) an element \"c\" is called \"compact\" (or \"finite\") if it satisfies one of the following equivalent conditions:\n\nIf the poset \"P\" additionally is a join-semilattice (i.e., if it has binary suprema) then these conditions are equivalent to the following statement:\nIn particular, if \"c\" = sup \"S\", then \"c\" is the supremum of a finite subset of \"S\".\n\nThese equivalences are easily verified from the definitions of the concepts involved. For the case of a join-semilattice note that any set can be turned into a directed set with the same supremum by closing under finite (non-empty) suprema.\n\nWhen considering directed complete partial orders or complete lattices the additional requirements that the specified suprema exist can of course be dropped. Note also that a join-semilattice which is directed complete is almost a complete lattice (possibly lacking a least element) -- see completeness (order theory) for details.\n\nIf it exists, the least element of a poset is always compact. It may be that this is the only compact element, as the example of the real unit interval [0,1] shows.\n\n\nA poset in which every element is the supremum of the compact elements below it is called an \"algebraic poset\". Such posets which are dcpos are much used in domain theory.\n\nAs an important special case, an \"algebraic lattice\" is a complete lattice \"L\", such that every element \"x\" of \"L\" is the supremum of the compact elements below \"x\".\n\nA typical example (which served as the motivation for the name \"algebraic\") is the following:\n\nFor any algebra \"A\" (for example, a group, a ring, a field, a lattice, etc.; or even a mere set without any operations), let Sub(\"A\") be the set of all substructures of \"A\", i.e., of all subsets of \"A\" which are closed under all operations of \"A\" (group addition, ring addition and multiplication, etc.) Here the notion of substructure includes the empty substructure in case the algebra \"A\" has no nullary operations.\n\nThen:\n\nAlso, a kind of converse holds: Every algebraic lattice is isomorphic to Sub(\"A\") for some algebra \"A\".\n\nThere is another algebraic lattice which plays an important role in universal algebra: For every algebra \"A\" \nwe let Con(\"A\") be the set of all congruence relations on \"A\". Each congruence on \"A\" is a subalgebra of the product algebra \"A\"x\"A\", so Con(\"A\") ⊆ Sub(\"A\"x\"A\"). Again we have\n\nAgain there is a converse: By a theorem of George Grätzer and E. T. Schmidt, every algebraic lattice is isomorphic to Con(\"A\") for some algebra \"A\".\n\nCompact elements are important in computer science in the semantic approach called domain theory, where they are considered as a kind of primitive element: the information represented by compact elements cannot be obtained by any approximation that does not already contain this knowledge. Compact elements cannot be approximated by elements strictly below them. On the other hand, it may happen that all non-compact elements can be obtained as directed suprema of compact elements. This is a desirable situation, since the set of compact elements is often smaller than the original poset – the examples above illustrate this.\n\nSee the literature given for order theory and domain theory.\n"}
{"id": "18104802", "url": "https://en.wikipedia.org/wiki?curid=18104802", "title": "Complementarity theory", "text": "Complementarity theory\n\nA complementarity problem is a type of mathematical optimization problem. It is the problem of optimizing (minimizing or maximizing) a function of two vector variables subject to certain requirements (constraints) which include: that the inner product of the two vectors must equal zero, i.e. they are orthogonal. In particular for finite-dimensional real vector spaces this means that, if one has vectors \"X\" and \"Y\" with all \"nonnegative\" components (\"x\" ≥ 0 and \"y\" ≥ 0 for all formula_1: in the first quadrant if 2-dimensional, in the first octant if 3-dimensional), then for each pair of components \"x\" and \"y\" one of the pair must be zero, hence the name \"complementarity\". e.g. \"X\" = (1, 0) and \"Y\" = (0, 2) are complementary, but \"X\" = (1, 1) and \"Y\" = (2, 0) are not. A complementarity problem is a special case of a variational inequality.\n\nComplementarity problems were originally studied because the Karush–Kuhn–Tucker conditions in linear programming and quadratic programming constitute a linear complementarity problem (LCP) or a mixed complementarity problem (MCP). In 1963 Lemke and Howson showed that, for two person games, computing a Nash equilibrium point is equivalent to an LCP. In 1968 Cottle and Dantzig unified linear and quadratic programming and bimatrix games. Since then the study of complementarity problems and variational inequalities has expanded enormously.\n\nAreas of mathematics and science that contributed to the development of complementarity theory\ninclude: optimization, equilibrium problems, variational inequality theory, fixed point theory, topological degree theory and nonlinear analysis.\n\n\n\n\n"}
{"id": "681190", "url": "https://en.wikipedia.org/wiki?curid=681190", "title": "Compression (functional analysis)", "text": "Compression (functional analysis)\n\nIn functional analysis, the compression of a linear operator \"T\" on a Hilbert space to a subspace \"K\" is the operator\n\nwhere formula_2 is the orthogonal projection onto \"K\". This is a natural way to obtain an operator on \"K\" from an operator on the whole Hilbert space. If \"K\" is an invariant subspace for \"T\", then the compression of \"T\" to \"K\" is the restricted operator \"K→K\" sending \"k\" to \"Tk\".\n\nMore generally, for a linear operator \"T\" on a Hilbert space formula_3 and an isometry \"V\" on a subspace formula_4 of formula_3, define the compression of \"T\" to formula_4 by\n\nwhere formula_8 is the adjoint of \"V\". If \"T\" is a self-adjoint operator, then the compression formula_9 is also self-adjoint.\nWhen \"V\" is replaced by the inclusion map formula_10, formula_11, and we acquire the special definition above.\n\n\n"}
{"id": "5926", "url": "https://en.wikipedia.org/wiki?curid=5926", "title": "Computation", "text": "Computation\n\nComputation is any type of calculation that includes both arithmetical and non-arithmetical steps and follows a well-defined model, for example an algorithm.\n\nThe study of computation is paramount to the discipline of computer science.\n\nA computation can be seen as a purely physical phenomenon occurring inside a closed physical system called a computer.\nExamples of such physical systems include digital computers, mechanical computers, quantum computers, DNA computers, molecular computers, microfluidics-based computers, analog computers, or wetware computers.\nThis point of view has been adopted by the physics of computation, a branch of theoretical physics, as well as the field of natural computing.\n\nAn even more radical point of view, pancomputationalism, is the postulate of digital physics that argues that the evolution of the universe is itself a computation.\n\nThe classic account of computation is found throughout the works of Hilary Putnam and others. Peter Godfrey-Smith has dubbed this the \"simple mapping account.\" Gualtiero Piccinini's summary of this account states that a physical system can be said to perform a specific computation when there is a mapping between the state of that system to the computation such that the “microphysical states [of the system] mirror the state transitions between the computational states.”\n\nPhilosophers such as Jerry Fodor have suggested various accounts of computation with the restriction that semantic content be a necessary condition for computation (that is, what differentiates an arbitrary physical system from a computing system is that the operands of the computation represent something). This notion attempts to prevent the logical abstraction of the mapping account of pancomputationalism, the idea that everything can be said to be computing everything.\n\nGualtiero Piccinini proposes an account of computation based in mechanical philosophy. It states that physical computing systems are types of mechanisms that, by design, perform physical computation, or “the manipulation (by a functional mechanism) of a medium-independent vehicle according to a rule.” Medium-independence requires that the property is able to be instantiated by multiple realizers and multiple mechanisms and that the inputs and outputs of the mechanism also be multiply realizable. In short, medium-independence allows for the use of physical variables with traits other than voltage (as in typical digital computers); this is imperative in considering other types of computation, such as that occurs in the brain or in a quantum computer. A rule, in this sense, provides a mapping among inputs, outputs, and internal states of the physical computing system. \n\nIn the theory of computation, a diversity of mathematical models of computers has been developed.\nTypical mathematical models of computers are the following:\nGiunti calls the models studied by computation theory \"computational systems,\" and he argues that all of them are mathematical dynamical systems with discrete time and discrete state space. He maintains that a computational system is a complex object which consists of three parts. First, a mathematical dynamical systems formula_1 with discrete time and discrete state space; second, a computational setup formula_2, which is made up of a theoretical part formula_3, and a real part formula_4; third, an interpretation formula_5, which links the dynamical system formula_1 with the setup formula_7.\n\n"}
{"id": "680672", "url": "https://en.wikipedia.org/wiki?curid=680672", "title": "Critical graph", "text": "Critical graph\n\nIn graph theory, a critical graph is a graph \"G\" in which every vertex or edge is a critical element, that is, if its deletion decreases the chromatic number of \"G\". Such a decrement can be not more than 1 in a graph.\n\nA k\"-critical graph is a critical graph with chromatic number \"k\"; a graph \"G\" with chromatic number \"k\" is k\"-vertex-critical if each of its vertices is a critical element. Critical graphs are the \"minimal\" members in terms of chromatic number, which is a very important measure in graph theory.\n\nSome properties of a \"k\"-critical graph \"G\" with \"n\" vertices and \"m\" edges:\n\nGraph G is vertex-critical if and only if for every vertex \"v\", there is an optimal proper coloring in which \"v\" is a singleton color class.\n\nAs showed, every \"k\"-critical graph may be formed from a complete graph \"K\" by combining the Hajós construction with an operation that identifies two non-adjacent vertices. The graphs formed in this way always require \"k\" colors in any proper coloring.\n\nA double-critical graph is a connected graph in which the deletion of any pair of adjacent vertices decreases the chromatic number by two. One open problem is to determine whether \"K\" is the only double-critical \"k\"-chromatic graph\n\n"}
{"id": "1147822", "url": "https://en.wikipedia.org/wiki?curid=1147822", "title": "Current yield", "text": "Current yield\n\nThe current yield, interest yield, income yield, flat yield, market yield, mark to market yield or running yield is a financial term used in reference to bonds and other fixed-interest securities such as gilts. It is the ratio of the annual interest payment and the bond's current clean price:\n\nformula_1\n\nThe current yield only therefore refers to the yield of the bond at the current moment. It does not reflect the total return over the life of the bond. In particular, it takes no account of reinvestment risk (the uncertainty about the rate at which future cashflows can be reinvested) or the fact that bonds usually mature at par value, which can be an important component of a bond's return.\n\nThe concept of current yield is closely related to other bond concepts, including yield to maturity, and coupon yield. When a coupon-bearing bond sells at;\n\n\nFor zero-coupon bonds selling at a discount, the coupon yield and current yield are zero, and the YTM is positive.\n\nTo calculate the current yield of a bond with a face value of $100 and a coupon rate of 5.00% that is selling at $95.00 (clean; not including accrued interest), use:\n\nformula_2\n\n\n"}
{"id": "27777043", "url": "https://en.wikipedia.org/wiki?curid=27777043", "title": "Daniel Bonevac", "text": "Daniel Bonevac\n\nDaniel Bonevac is an American philosopher born in Pittsburgh. He is professor of philosophy at the University of Texas at Austin. He has degrees in philosophy from Haverford College, and the University of Pittsburgh. His areas of interest are metaphysics, philosophical logic, ethics, and Eastern philosophy.\n\n\nIn autumn 2016, Bonevac joined 145 other scholars and writers in declaring support for Donald Trump for president.\n\n"}
{"id": "51742", "url": "https://en.wikipedia.org/wiki?curid=51742", "title": "Drawing board", "text": "Drawing board\n\nA drawing board (also drawing table, drafting table or architect's table) is, in its antique form, a kind of multipurpose desk which can be used for any kind of drawing, writing or impromptu sketching on a large sheet of paper or for reading a large format book or other oversized document or for drafting precise technical illustrations (such as engineering drawings or architectural drawings). The drawing table used to be a frequent companion to a pedestal desk in a gentleman's study or private library, during the pre-industrial and early industrial era.\n\nDuring the Industrial Revolution draftsmanship gradually became a specialized trade and drawing tables slowly moved out of the libraries and offices of most gentlemen. They became more utilitarian and were built of steel and plastic instead of fine woods and brass.\n\nMore recently, engineers and draftsmen use the drawing board for making and modifying drawings on paper with ink or pencil. Different drawing instruments (set square, protractor, etc.) are used on it to draw parallel, perpendicular or oblique lines. There are instruments for drawing circles, arcs, other curves and symbols too (compass, French curve, stencil, etc.). However, with the gradual introduction of computer aided drafting and design (CADD or CAD) in the last decades of the 20th century and the first of the 21st century, the drawing board is becoming less common.\n\nA drawing table is also sometimes called a mechanical desk because, for several centuries, most mechanical desks were drawing tables. Unlike the gadgety mechanical desks of the second part of the 18th century, however, the mechanical parts of drawing tables were usually limited to notches, ratchets, and perhaps a few simple gears, or levers or cogs to elevate and incline the working surface.\n\nVery often a drawing table could look like a writing table or even a pedestal desk when the working surface was set at the horizontal and the height adjusted to 29 inches, in order to use it as a \"normal\" desk. The only giveaway was usually a lip on one of the sides of the desktop. This lip or edge stopped paper or books from sliding when the surface was given an angle. It was also sometimes used to hold writing implements. When the working surface was extended at its full height, a drawing table could be used as a standing desk.\n\nMany reproductions have been made and are still being produced of drawing tables, copying the period styles they were originally made in during the 18th and 19th centuries.\n\nThe expression \"back to the drawing board\" is used when a plan or course of action needs to be changed, often drastically; usually due to a very unsuccessful result; e.g., \"The battle plan, the result of months of conferences, failed because the enemy retreated too far back. It was back to the drawing board for the army captains.\"\n\nThe phrase was coined in the caption to a Peter Arno cartoon of The New Yorker of March 1, 1941 (cartoon)\n\nDespite the prevalence of computer aided drafting, many older architects and even some structural designers still rely on paper and pencil graphics produced on a drafting table. \n\nModern drafting tables typically rely on a steel frame. Steel provides as much strength as the old oak drafting table frames and much easier portability. Typically the drafting board surface is a thick sheet of compressed fibreboard with sheets of Formica laminated to all its surfaces. The drafting board surface is usually secured to the frame by screws which can easily be removed for drafting table transportation.\n\nThe steel frame allows mechanical linkages to be installed that control both the height and angle of the drafting board surface. Typically, a single foot pedal is used to control a clutch which clamps the board in the desired position. A heavy counterweight full of lead shot is installed in the steel linkage so that if the pedal is accidentally released, the drafting board will not spring into the upright position and injure the user. Drafting table linkages and clutches have to be maintained to ensure that this safety mechanism counterbalances the weight of the table surface.\n\nThe drafting table surface is usually covered with a thin vinyl sheet called a board cover. This provides an optimum surface for pen and pencil drafting. It allows compasses and dividers to be used without damaging the wooden surface of the board. A board cover must be frequently cleaned to prevent graphite buildup from making new drawings dirty. At the bottom edge of the table, a single strip of aluminum or steel may serve as a place to rest drafting pencils. More purpose-built trays are also used which hold pencils even while the board is being adjusted.\n\nVarious types of drafting machine may be attached to the board surface to assist the draftsperson or artist. Parallel rules often span the entire width of the board and are so named because they remain parallel to the top edge of the board as they are moved up and down. Drafting machines use pre-calibrated scales and built in protractors to allow accurate drawing measurement.\n\nSome drafting tables incorporate electric motors to provide the up and down and angle adjustment of the drafting table surface. These tables are at least as heavy as the original oak and brass drafting tables and so sacrifice portability for the convenience of push button table adjustment.\n\n\n"}
{"id": "27753472", "url": "https://en.wikipedia.org/wiki?curid=27753472", "title": "Durfee square", "text": "Durfee square\n\nIn number theory, a Durfee square is an attribute of an integer partition. A partition of \"n\" has a Durfee square of side \"s\" if \"s\" is the largest number such that the partition contains at least \"s\" parts with values ≥ \"s\". An equivalent, but more visual, definition is that the Durfee square is the largest square that is contained within a partition's Ferrers diagram. The side-length of the Durfee square is known as the \"rank\" of the partition.\n\nThe Durfee symbol consists of the two partitions represented by the points to the right or below the Durfee square. \n\nThe partition 4 + 3 + 3 + 2 + 1 + 1:\n\nhas a Durfee square of side 3 (in red) because it contains 3 parts that are ≥ 3, but does not contain 4 parts that are ≥ 4. Its Durfee symbol consists of the 2 partitions 1 and 3+1.\n\nDurfee squares are named after William Pitt Durfee, a student of English mathematician James Joseph Sylvester. In a letter to Arthur Cayley in 1883, Sylvester wrote:\n\nIt is clear from the visual definition that the Durfee square of a partition and its conjugate partition have the same size. The partitions of an integer \"n\" contain Durfee squares with sides up to and including formula_1.\n\n"}
{"id": "7778192", "url": "https://en.wikipedia.org/wiki?curid=7778192", "title": "Encyclopedia of Statistical Sciences", "text": "Encyclopedia of Statistical Sciences\n\nThe Encyclopedia of Statistical Sciences is an encyclopaedia of statistics published by John Wiley & Sons.\n\nThe first edition, in nine volumes, was edited by Norman Lloyd Johnson and Samuel Kotz and appeared in 1982. The second edition, in 16 volumes, was published in 2006. Samuel Kotz was the senior editor.\n\n\n"}
{"id": "1934129", "url": "https://en.wikipedia.org/wiki?curid=1934129", "title": "Explicit symmetry breaking", "text": "Explicit symmetry breaking\n\nIn theoretical physics, explicit symmetry breaking is the breaking of a symmetry of a theory by terms in its defining equations of motion (most typically, to the Lagrangian or the Hamiltonian) that do not respect the symmetry. Usually this term is used in situations where these symmetry-breaking terms are small, so that the symmetry is approximately respected by the theory. An example is the spectral line splitting in the Zeeman effect, due to a magnetic interaction perturbation in the Hamiltonian of the atoms involved.\n\nExplicit symmetry breaking differs from spontaneous symmetry breaking. In the latter, the defining equations respect the symmetry but the ground state (vacuum) of the theory breaks it.\n\nExplicit symmetry breaking is also associated with electromagnetic radiation. A system of accelerated charges results in electromagnetic radiation when the geometric symmetry of the electric field in free space is explicitly broken by the associated electrodynamic structure under time varying excitation of the given system. This is quite evident in an antenna where the electric lines of field curl around or have rotational geometry around the radiating terminals in contrast to linear geometric orientation within a pair of transmission lines which does not radiate even under time varying excitation.\n\n"}
{"id": "728596", "url": "https://en.wikipedia.org/wiki?curid=728596", "title": "Griess algebra", "text": "Griess algebra\n\nIn mathematics, the Griess algebra is a commutative non-associative algebra on a real vector space of dimension 196884 that has the Monster group \"M\" as its automorphism group. It is named after mathematician R. L. Griess, who constructed it in 1980 and subsequently used it in 1982 to construct \"M\". The Monster fixes (vectorwise) a 1-space in this algebra and acts absolutely irreducibly on the 196883-dimensional orthogonal complement of this 1-space.\n\nGriess's construction was later simplified by Jacques Tits and John H. Conway. \n\nThe Griess algebra is the same as the degree 2 piece of the monster vertex algebra, and the Griess product is one of the vertex algebra products.\n\n"}
{"id": "33849734", "url": "https://en.wikipedia.org/wiki?curid=33849734", "title": "Identity channel", "text": "Identity channel\n\nIn quantum information theory, the identity channel is a noise-free quantum channel. That is, the channel outputs exactly what was put in.\n\nThe identity channel is commonly denoted as formula_1, formula_2 or formula_3.\n"}
{"id": "11743104", "url": "https://en.wikipedia.org/wiki?curid=11743104", "title": "Infinite alleles model", "text": "Infinite alleles model\n\nThe infinite alleles model is a mathematical model for calculating genetic mutations. The Japanese geneticist Motoo Kimura and American geneticist James F. Crow (1964) introduced the \"infinite alleles model\", an attempt to determine for a finite diploid population what proportion of loci would be homozygous. This was, in part, motivated by assertions by other geneticists that more than 50 percent of \"Drosophila\" loci were heterozygous, a claim they initially doubted. In order to answer this question they assumed first, that there were a large enough number of alleles so that any mutation would lead to a different allele (that is the probability of back mutation to the original allele would be low enough to be negligible); and second, that the mutations would result in a number of different outcomes from neutral to .\n\nThey determined that in the neutral case, the probability that an individual would be homozygous, \"F\", was:\n\nwhere \"u\" is the mutation rate, and \"N\" is the effective population size. The effective number of alleles \"n\" maintained in a population is defined as the inverse of the homozygosity, that is\n\nwhich is a lower bound for the actual number of alleles in the population.\n\nIf the effective population is large, then a large number of alleles can be maintained. However, this result only holds for the \"neutral\" case, and is not necessarily true for the case when some alleles are subject to selection, i.e. more or less fit than others, for example when the fittest genotype is a heterozygote (a situation often referred to as overdominance or heterosis). \n\nIn the case of overdominance, because Mendel's second law (the law of segregation) necessarily results in the production of homozygotes (which are by definition in this case, less fit), this means that population will always harbor a number of less fit individuals, which leads to a decrease in the average fitness of the population. This is sometimes referred to as \"genetic load\", in this case it is a special kind of load known as \"segregational load\". Crow and Kimura showed that at equilibrium conditions, for a given strength of selection (\"s\"), that there would be an upper limit to the number of fitter alleles (polymorphisms) that a population could harbor for a particular locus. Beyond this number of alleles, the selective advantage of presence of those alleles in heterozygous genotypes would be cancelled out by continual generation of less fit homozygous genotypes.\n\nThese results became important in the formation of the neutral theory, because neutral (or nearly neutral) alleles create no such segregational load, and allow for the accumulation of a great deal of polymorphism. When Richard Lewontin and J. Hubby published their groundbreaking results in 1966 which showed high levels of genetic variation in Drosophila via protein electrophoresis, the theoretical results from the infinite alleles model were used by Kimura and others to support the idea that this variation would have to be neutral (or result in excess segregational load).\n\n\n"}
{"id": "31075378", "url": "https://en.wikipedia.org/wiki?curid=31075378", "title": "Joseph Turner (priest)", "text": "Joseph Turner (priest)\n\nJoseph Turner (1746 or 1747 – 3 August 1828) was a British academic and clergyman.\n\nHe entered Pembroke College, Cambridge University in 1763 at age 17 and was senior wrangler in 1767. He received an M.A. in 1770.\n\nHe was Master of Pembroke College from 1784 to 1828, and Dean of Norwich from 1790 to 1828.\n\nHe was a Vice-Chancellor of Cambridge University in 1785–6 and 1805–6.\n\nHis only son was William Hamilton Turner, who became vicar of Banwell, Somerset.\n\n"}
{"id": "41172240", "url": "https://en.wikipedia.org/wiki?curid=41172240", "title": "Jürgen Jost", "text": "Jürgen Jost\n\nJürgen Jost (born June 9, 1956 in Münster) is a German mathematician specializing in geometry. He has been a director of the Max Planck Institute for Mathematics in the Sciences in Leipzig since 1996.\n\nIn 1975, he began studying mathematics, physics, economics and philosophy. In 1980 he received a Ph.D. from the University of Bonn under the supervision of Stefan Hildebrandt. In 1984 he was at the University of Bonn for the habilitation. After his habilitation, he was at the Ruhr University Bochum, the chair of Mathematics X, Analysis. During this time he was the coordinator of the project \"Stochastic Analysis and systems with infinitely many degrees of freedom\" July 1987 to December 1996.\n\nFor this work he received the 1993 Gottfried Wilhelm Leibniz Prize of the Deutsche Forschungsgemeinschaft.\n\nSince 1996, he has been director and scientific member at the Max Planck Institute for Mathematics in the Sciences in Leipzig. After more than 10 years of work in Bochum, this he followed: \"tackle new research problems in the border area between mathematics and the natural sciences and simultaneously encourage mathematical research in Germany, particularly in the fields of geometry and analysis.\"\n\nIn 1998 he was an honorary professor at the University of Leipzig. In 2002, there, he initiated with two other scientists from the Max Planck Institute, the Interdisciplinary Center for Bioinformatics (IZBI).\n\nIn 1986 he was invited speaker at the International Congress of Mathematicians in Berkeley (\"Two dimensional geometric variational problems\"). He is a fellow of the American Mathematical Society.\n\nHis research focuses are:\n\n\n"}
{"id": "56553444", "url": "https://en.wikipedia.org/wiki?curid=56553444", "title": "Katalin Vesztergombi", "text": "Katalin Vesztergombi\n\nKatalin L. Vesztergombi (born 1948) is a Hungarian mathematician known for her contributions to graph theory and discrete geometry. A student of Vera T. Sós and a co-author of Paul Erdős, she is an emeritus associate professor at Eötvös Loránd University and a member of the Hungarian Academy of Sciences.\n\nAs a high-school student in the 1960s, Vesztergombi became part of a special class for gifted mathematics students at Fazekas Mihály Gimnázium with her future collaborators László Lovász, József Pelikán, and others.\nShe completed her Ph.D. in 1987 at Eötvös Loránd University. Her dissertation, \"Distribution of Distances in Finite Point Sets\", is connected to the Erdős distinct distances problem and was supervised by Vera Sós.\n\nVesztergombi's research contributions include works on permutations, graph coloring and graph products,\ncombinatorial discrepancy theory, distance problems in discrete geometry, geometric graph theory,\nthe rectilinear crossing number of the complete graph, and graphons.\n\nWith László Lovász and József Pelikán, she is the author of the textbook \"Discrete Mathematics: Elementary and Beyond\".\n\nVesztergombi is married to László Lovász, with whom she is also a frequent research collaborator.\n"}
{"id": "45188167", "url": "https://en.wikipedia.org/wiki?curid=45188167", "title": "List of convex regular-faced polyhedra", "text": "List of convex regular-faced polyhedra\n\nThis is a list of convex, regular-faced polyhedra. Since there are an infinite number of prisms and antiprisms, only a few have been listed.\n"}
{"id": "5971830", "url": "https://en.wikipedia.org/wiki?curid=5971830", "title": "List of mathematicians (S)", "text": "List of mathematicians (S)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"}
{"id": "949880", "url": "https://en.wikipedia.org/wiki?curid=949880", "title": "Long and short scales", "text": "Long and short scales\n\nThe long and short scales are two of several large-number naming systems for integer powers of ten that use the same words with different meanings. The long scale is based on powers of one million, whereas the short scale is based on powers of one thousand. The short scale has an offset, unlike the long scale.\n\nFor whole numbers less than a thousand million (< 10) the two scales are identical. From a thousand million up (≥ 10) the two scales diverge, using the same words for different numbers; this can cause misunderstanding.\n\nEvery new term greater than \"million\" is one thousand times as large as the previous term. Thus, \"billion\" means \"a thousand millions\" (10), \"trillion\" means \"a thousand billions\" (10), and so on. Thus, an \"n\"-illion equals 10.\n\nEvery new term greater than \"million\" is one million times as large as the previous term. Thus, \"billion\" means \"a million millions\" (10), \"trillion\" means \"a million billions\" (10), and so on. Thus, an \"n\"-illion equals 10.\n\nCountries where the long scale is currently used include most countries in continental Europe and most that are French-speaking, Spanish-speaking (except Spanish-speakers born into an English-speaking culture, e.g. [[Puerto Rico]], because of its influence from English-speaking United States) and [[Lusophone|Portuguese-speaking countries]], except [[Brazil]].\n\nThe short scale is now used in most [[English-speaking world|English-speaking]] and [[List of countries where Arabic is an official language|Arabic-speaking countries]], in [[Brazil]], in the former [[Soviet Union]] and several other countries.\n\nNumber names are rendered in the language of the country, but are similar everywhere due to shared [[etymology]]. Some languages, particularly in [[East Asia]] and [[South Asia]], have large number naming systems that are different from both the long and short scales, for example the [[Indian numbering system]].\n\nFor most of the 19th and 20th centuries, the [[United Kingdom]] largely used the long scale, whereas the [[United States]] used the short scale, so that the two systems were often referred to as \"British\" and \"American\" in the [[English language]]. After several decades of increasing informal British usage of the short scale, in 1974 the government of the UK adopted it, and it is used for all official purposes. With very few exceptions, the British usage and American usage are now identical.\n\nThe first recorded use of the terms \"short scale\" () and \"long scale\" () was by the French mathematician [[Geneviève Guitel]] in 1975.\n\nTo avoid confusion resulting from the coexistence of short and long term in any language, the SI recommends using the [[Metric prefix]], which keeps the same meaning regardless of the country and the language. Long and short scales remain in de facto use for counting money.\n\nThe relationship between the numeric values and the corresponding names in the two scales can be described as:\n\nThe relationship between the names and the corresponding numeric values in the two scales can be described as:\n\nThe root \"mil\" in \"million\" does not refer to the numeral, \"1\". The word, \"million\", derives from the Old French, \"milion\", from the earlier Old Italian, \"milione\", an intensification of the Latin word, \"mille\", a thousand. That is, a \"million\" is a \"big thousand\", much as a \"[[great gross]]\" is a dozen gross or 12×144 = [[1728 (number)|1728]].\n\nThe word \"milliard\", or its translation, is found in many European languages and is used in those languages for 10. However, it is unknown in American English, which uses \"billion\", and not used in British English, which preferred to use \"thousand million\" before the current usage of \"billion\". The financial term, \"yard\", which derives from \"milliard\", is used on financial markets, as, unlike the term, \"billion\", it is internationally unambiguous and phonetically distinct from \"million\". Likewise, many long scale countries use the word \"billiard\" (or similar) for one thousand long scale billions (i.e., 10), and the word \"trilliard\" (or similar) for one thousand long scale trillions (i.e., 10), etc.\n\nThe existence of the different scales means that care must be taken when comparing large numbers between languages or countries, or when interpreting old documents in countries where the dominant scale has changed over time. For example, British English, French, and Italian historical documents can refer to either the short or long scale, depending on the date of the document, since each of the three countries has used both systems at various times in its history. Today, the United Kingdom officially uses the short scale, but [[France]] and [[Italy]] use the long scale.\n\nThe pre-1974 former British English word \"billion\", post-1961 current French word \"billion\", post-1994 current Italian word \"bilione\", German \"Billion\"; Dutch \"biljoen\"; Swedish \"biljon\"; Finnish \"biljoona\"; Danish \"billion\"; Polish \"bilion\", Spanish \"billón\"; Slovenian \"bilijon\" and the European Portuguese word \"bilião\" (with a different spelling to the Brazilian Portuguese variant, but in Brazil referring to short scale) all refer to 10, being long-scale terms. Therefore, each of these words translates to the American English or post-1974 British English word: \"trillion\" (10 in the short scale), and not \"billion\" (10 in the short scale).\n\nOn the other hand, the pre-1961 former French word \"billion\", pre-1994 former Italian word \"bilione\", Brazilian Portuguese word \"bilhão\" and the Welsh word \"biliwn\" all refer to 10, being short scale terms. Each of these words translates to the American English or post-1974 British English word \"billion\" (10 in the short scale).\n\nThe term \"billion\" originally meant 10 when introduced.\n\nAs large numbers in natural sciences are usually represented by metric prefixes, scientific notation or otherwise, the most commonplace occurrence of large numbers represented by long or short scale terms is in finance. The following table includes some historic examples related to hyper-inflation and other financial incidents.\n\n[[File:EScalas corta y larga.svg|thumb|center|650px|Short and long scale usage throughout the world\n\nMost English-language countries and regions use the short scale with 10 being \"billion\". For example:\n\nMost [[Arabic language|Arabic-language]] countries and regions use the short scale with 10 being   \"milyar\". For example:\n\nOther countries also use a word similar to \"trillion\" to mean 10, etc. Whilst a few of these countries like English use a word similar to \"billion\" to mean 10, most like Arabic have kept a traditional long scale word similar to \"milliard\" for 10. Some examples of short scale use, and the words used for 10 and 10, are\nThe traditional long scale is used by most [[Continental Europe]]an countries and by most other countries whose languages derive from [[Continental Europe]] (with the notable exceptions of Albania, Greece, Romania, and Brazil). These countries use a word similar to \"billion\" to mean 10. Some use a word similar to \"milliard\" to mean 10, while others use a word or phrase equivalent to \"thousand millions\".\n\nMost [[Spanish language|Spanish-language]] countries and regions use the long scale, for example:\n\nMost [[French language|French-language]] countries and regions use the long scale with 10 = milliard, for example:\nWith the notable exception of Brazil, a short scale country, most [[Portuguese language|Portuguese-language]] countries and regions use the long scale with 10 = mil de milhões \"or\" milhar de milhões, for example:\nMost [[Dutch language|Dutch-language]] countries and regions use the long scale with 10 = miljard, for example:\n\nGerman-language countries and regions use the long scale with 10 = Milliarde, for example:\n\nSome examples of long scale use, and the words used for 10 and 10, are\n\nSome countries use either the short or long scales, depending on the internal language being used or the context.\n\nThe following countries use naming systems for large numbers that are not etymologically related to the short and long scales:\n\nThe long and short scales are both present on most continents, with usage dependent on the language used. Examples include:\n\nUnambiguous ways of identifying large numbers include:\n\n\n[[Category:Numerals]]\n[[Category:Numeral systems]]\n[[Category:Articles with images not understandable by color blind users]]"}
{"id": "46900811", "url": "https://en.wikipedia.org/wiki?curid=46900811", "title": "Low-degree saturation", "text": "Low-degree saturation\n\nIn a scale-free network the degree distribution follows a power law function. In some empirical examples this power-law fits the degree distribution well only in the high degree region, however for small degree nodes the empirical degree-distribution deviates from it. See for example the network of scientific citations. This deviation of the observed degree-distribution from the theoretical prediction at the low-degree region is often referred as low-degree saturation.\n\nTypically the empirical degree-distribution deviates downwards from the power-law function fitted on higher order nodes, which means low-degree nodes are less frequent in real data than what is predicted by the Barabási–Albert model.\n\nOne of the key assumptions of the BA model is preferential attachment. It states, the probability of acquiring a new link from a new entrant node is proportional to the degree of each node. In other words, every new entrant favors to connect to higher-degree nodes. Formally:\n\nformula_1\n\nWhere formula_2 is the probability of acquiring a link by a node with degree formula_3.\n\nWith a slight modification of this rule low-degree saturation can be predicted easily, by adding a term called initial attractiveness (formula_4). This was first introduced by Dorogovtsev, Mendes and Samukhin in 2000.\n\nformula_5\n\nWith this modified attachment rule a low-degree node (with low formula_3) has a higher probability to acquire new links compared to the original set-up. Thus it is more \"attractive\". Therefore, this handicap makes less likely the existence of small degree-nodes as it is observed in real data.\nMore formally this modifies the degree distribution as:\n\nformula_7\n\nAs a side effect it also increases the exponent relative to the original BA model.\n\nIt is called \"initial\" attractiveness because in the BA framework every node grows in degree by time. And as formula_3 goes large the significance of this fixed additive term formula_9 diminishes.\n\nAll the distinctive features of scale-free networks are due to the existence of extremely high degree nodes, often referred as \"hubs\". The existence of these hubs are predicted by the power-law distribution of the degrees. However low-degree saturation is a deviation from this theoretical degree distribution, since it characterize the low end of the degree distribution, it does not deny the existence of hubs. Therefore, a scale-free network with low-degree saturation can produce all the following characteristics: small-world characteristic, robustness, low attack tolerance, spreading behavior.\n\nIt should be noted however that if it is modeled via the BA model augmented by the initial attractiveness, then this solution reduces the size of hubs because it affects the exponent of the degree distribution positively relative to the original BA model.\n\nInitial attractiveness\n"}
{"id": "1988240", "url": "https://en.wikipedia.org/wiki?curid=1988240", "title": "László Kalmár", "text": "László Kalmár\n\nLászló Kalmár (27 March 1905, Edde – 2 August 1976, Mátraháza) was a Hungarian mathematician and Professor at the University of Szeged. Kalmár is considered the founder of mathematical logic and theoretical computer science in Hungary.\n\nKalmár was of Jewish ancestry. His early life mixed promise and tragedy. His father died when he was young, and his mother died when he was 17, the year he entered the University of Budapest, making him essentially an orphan.\n\nKalmár's brilliance manifested itself while in Budapest schools. At the University of Budapest, his teachers included Kürschák and Fejér. His fellow students included the future logician Rózsa Péter. Kalmár graduated in 1927. He discovered mathematical logic, his chosen field, while visiting Göttingen in 1929.\n\nUpon completing his doctorate at Budapest, he took up a position at the University of Szeged. That university was mostly made up of staff from the former University of Kolozsvár, a major Hungarian university before World War I that found itself after the War in Romania. Kolozsvár was renamed Cluj. The Hungarian university moved to Szeged in 1920, where there had previously been no university. The appointment of Haar and Riesz turned Szeged into a major research center for mathematics. Kalmár began his career as a research assistant to Haar and Riesz. Kalmár was appointed a full professor at Szeged in 1947. He was the inaugural holder of Szeged's chair for the Foundations of Mathematics and Computer Science. He also founded Szeged's Cybernetic Laboratory and the Research Group for Mathematical Logic and Automata Theory.\n\nIn mathematical logic, Kalmár proved that certain classes of formulas of the first order predicate calculus were decidable. In 1936, he proved that the predicate calculus could be formulated using a single binary predicate, if the recursive definition of a term was sufficiently rich. (This result is commonly attributed to a 1954 paper of Quine's.) He discovered an alternative form of primitive recursive arithmetic, known as elementary recursive arithmetic, based on primitive functions that differ from the usual kind. He did his utmost to promote computers and computer science in Hungary. He wrote on theoretical computer science, including programming languages, automatic error correction, non-numerical applications of computers, and the connection between computer science and mathematical logic.\n\nKalmar is one of the very few logicians who has raised doubts about Church's Thesis that all intuitively mechanistic, algorithmic functions are representable by recursive functions. \nKalmar was elected to the Hungarian Academy of Sciences in 1949, and was awarded the Kossuth Prize in 1950 and the Hungarian State Prize in 1975.\n\nIn 1933 Kalmár married Erzsébet Arvay; they had four children.\nKalmar defined what are known as elementary functions, number-theoretic functions (i.e. those based on the natural numbers) built up from the notions of composition and variables, the constants 0 and 1, repeated addition + of the constants, proper subtraction ∸, bounded summation and bounded product (Kleene 1952:526). Elimination of the bounded product from this list yields the \"subelementary\" or \"lower elementary\" functions. By use of the abstract computational model called a register machine Schwichtenberg provides a demonstration that \"all elementary functions are computable and totally defined\" (Schwichtenberg 58).\n\n\n"}
{"id": "35200984", "url": "https://en.wikipedia.org/wiki?curid=35200984", "title": "Minimal K-type", "text": "Minimal K-type\n\nIn mathematics, a minimal K-type is a representation of a maximal compact subgroup \"K\" of a semisimple Lie group \"G\" that is in some sense the smallest representation of \"K\" occurring in a Harish-Chandra module of \"G\". Minimal K-types were introduced by as part of an algebraic description of the Langlands classification.\n"}
{"id": "4408934", "url": "https://en.wikipedia.org/wiki?curid=4408934", "title": "Palais–Smale compactness condition", "text": "Palais–Smale compactness condition\n\nThe Palais–Smale compactness condition, named after Richard Palais and Stephen Smale, is a hypothesis for some theorems of the calculus of variations. It is useful for guaranteeing the existence of certain kinds of critical points, in particular saddle points. The Palais-Smale condition is a condition on the functional that one is trying to extremize. \n\nIn finite-dimensional spaces, the Palais–Smale condition for a continuously differentiable real-valued function is satisfied automatically for proper maps: functions which do not take unbounded sets into bounded sets. In the calculus of variations, where one is typically interested in infinite-dimensional function spaces, the condition is necessary because some extra notion of compactness beyond simple boundedness is needed. See, for example, the proof of the mountain pass theorem in section 8.5 of Evans.\n\nA continuously Fréchet differentiable functional formula_1 from a Hilbert space \"H\" to the reals satisfies the Palais-Smale condition if every sequence formula_2 such that:\nhas a convergent subsequence in \"H\".\n\nLet \"X\" be a Banach space and formula_5 be a Gâteaux differentiable functional. The functional formula_6 is said to satisfy the weak Palais-Smale condition if for each sequence formula_7 such that\n\nthere exists a critical point formula_13 of formula_6 with\n"}
{"id": "2706525", "url": "https://en.wikipedia.org/wiki?curid=2706525", "title": "Perfect Developer", "text": "Perfect Developer\n\nPerfect Developer (PD) is a tool for developing computer programs in a rigorous manner. It is used to develop applications in areas including IT systems and airborne critical systems. The principle is to develop a formal specification and refine the specification to code. Even though the tool is founded on formal methods, the suppliers claim that advanced mathematical knowledge is not a prerequisite.\n\nPD supports the Verified Design by Contract paradigm, which is an extension of Design by contract. In Verified Design by Contract, the contracts are verified by static analysis and automated theorem proving, so that it is certain that they will not fail at runtime.\n\nThe Perfect specification language used has an object-oriented style, producing code in programming languages including Java, C# and C++. It has been developed by the UK company \"Escher Technologies Ltd\". They note on their website that their claim is not that the language itself is perfect, but that it can be used to produce code which perfectly implements a precise specification.\n\n\n"}
{"id": "21437222", "url": "https://en.wikipedia.org/wiki?curid=21437222", "title": "Poincaré plot", "text": "Poincaré plot\n\nA Poincaré plot, named after Henri Poincaré, is a species of recurrence plot used to quantify self-similarity in processes, usually periodic functions. It is also known as a return map. Poincaré plots can be used to distinguish chaos from randomness by embedding a data set into a higher-dimensional state space.\n\nGiven a time series of the form\n\na return map in its simplest form first plots (\"x\", \"x\"), then plots (\"x\", \"x\"), then (\"x\", \"x\"), and so on.\n\nAn electrocardiogram (ECG) is a tracing of the voltage changes in the chest generated by the heart, whose contraction in the normal person is triggered by an electrical impulse that originates the sinoatrial node. The ECG normally consists of a series of waves, labeled the P, Q, R, S and T waves. The P wave represents depolarization of the atria, Q-R-S series of waves the depolarization of the ventricles, and T wave the repolarization of the ventricles. The interval between two successive R waves (the RR interval) is a measure of the heart rate.\n\nThe heart rate normally varies slightly: during a deep breath, it speeds up and during a deep exhalation, it slows down. (The RR interval will shorten when the heart speeds up, and lengthen when it slows.) An RR tachograph is a graph of the numerical value of the RR-interval versus time. \n\nIn the context of RR tachography, a Poincaré plot is a graph of RR(\"n\") on the \"x\"-axis versus RR(\"n\" + 1) (the succeeding RR interval) on the \"y\"-axis, i.e. one takes a sequence of intervals and plots each interval against the following interval.\nThe recurrence plot is used as a standard visualizing technique to detect the presence of oscillations in non-linear dynamic systems. In the context of electrocardiography, the rate of the healthy heart is normally tightly controlled by the body's regulatory mechanisms (specifically, by the autonomic nervous system). Several research papers demonstrate the potential of ECG signal-based Poincaré plots in detecting heart-related diseases or abnormalities.\n\n"}
{"id": "2069923", "url": "https://en.wikipedia.org/wiki?curid=2069923", "title": "Popular mathematics", "text": "Popular mathematics\n\nPopular mathematics is mathematical presentation aimed at a general audience.\nSometimes this is in the form of books which require no mathematical background and in other cases it is in the form of expository articles written by professional mathematicians to reach out to others working in different areas.\n\nSome of the most prolific popularisers of mathematics include Keith Devlin, Martin Gardner and Ian Stewart. Titles by these three authors can be found on their respective pages.\n\n\n\n\n\n\n\n\n\n\nThe journals listed below can be found in many university libraries.\n\n\n\nSeveral museums aim at enhancing public understanding of mathematics:\n\nIn the United States:\n\nIn Austria:\n\nIn Germany:\n\nIn Italy\n"}
{"id": "47399711", "url": "https://en.wikipedia.org/wiki?curid=47399711", "title": "Pushmeet Kohli", "text": "Pushmeet Kohli\n\nPushmeet Kohli is a computer scientist at Google DeepMind. Before joining DeepMind, he was partner scientist and director of research at Microsoft Research. He also is a member of the psychometrics center in University of Cambridge and is an ACM Distinguished speaker.\n\nThe majority of his research is in the field of machine learning and computer vision. However, he has also made contributions in game theory, discrete algorithms and psychometrics. He is the recipient of the BMVA Sullivan Prize. His papers have received awards at CVPR 2015, WWW 2014, ISMAR 2011 and ECCV 2010.\n\n"}
{"id": "24958775", "url": "https://en.wikipedia.org/wiki?curid=24958775", "title": "Pöschl–Teller potential", "text": "Pöschl–Teller potential\n\nIn mathematical physics, a Pöschl–Teller potential, named after the physicists Herta Pöschl (credited as G. Pöschl) and Edward Teller, is a special class of potentials for which the one-dimensional Schrödinger equation can be solved in terms of Special functions.\n\nIn its symmetric form is explicitly given by \n\nand the solutions of the time-independent Schrödinger equation\nwith this potential can be found by virtue of the substitution formula_3, which yields\nThus the solutions formula_5 are just the Legendre functions formula_6 with formula_7, and formula_8, formula_9. Moreover, eigenvalues and scattering data can be explicitly computed. In the special case of integer formula_10, the potential is reflectionless and such potentials also arise as the N-soliton solutions of the Korteweg-de Vries equation.\n\nThe more general form of the potential is given by\n\nA related potential is given by an additional term.\n\nformula_12\n\n\n"}
{"id": "38878183", "url": "https://en.wikipedia.org/wiki?curid=38878183", "title": "Reachability problem", "text": "Reachability problem\n\nReachability is a fundamental problem that appears in several different contexts: finite- and infinite-state concurrent systems, computational models like cellular automata and Petri nets, program analysis, discrete and continuous systems, time critical systems, hybrid systems, rewriting systems, probabilistic and parametric systems, and open systems modelled as games.\n\nIn general the reachability problem can be formulated as follows:\n\nVariants of the reachability problem may result from additional constraints on the initial or final states, specific requirement for reachability paths as well as for iterative reachability or changing the questions into analysis of winning strategies in infinite games or unavoidability of some dynamics.\n\nTypically, for a fixed system description given in some form (reduction rules, systems of equations, logical formulas, etc.) a reachability problem consists of checking whether a given set of target states can be reached starting from a fixed set of initial states. The set of target states can be represented explicitly or via some implicit representation (e.g., a system of equations, a set of minimal elements with respect to some ordering on the states). Sophisticated quantitative and qualitative properties can often be reduced to basic reachability questions. Decidability and complexity boundaries, algorithmic solutions, and efficient heuristics are all important aspects to be considered in this context. Algorithmic solutions are often based on different combinations of exploration strategies, symbolic manipulations of sets of states, decomposition properties, or reduction to linear programming problems, and they often benefit from approximations, abstractions, accelerations and extrapolation heurisitics. Ad hoc solutions as well as solutions based on general purpose constraint solvers and deduction engines are often combined in order to balance efficiency and flexibility.\n\nThe Workshop on Reachability Problems series \nis an annual academic conference which\ngathers together researchers from\ndiverse disciplines and backgrounds interested in reachability problems that appear in algebraic\nstructures, computational models, hybrid systems, infinite games, logic and verification.\nThe workshop tries to fill the gap between results obtained in\ndifferent fields but sharing common mathematical\nstructure or conceptual difficulties.\n"}
{"id": "6013248", "url": "https://en.wikipedia.org/wiki?curid=6013248", "title": "Regular homotopy", "text": "Regular homotopy\n\nIn the mathematical field of topology, a regular homotopy refers to a special kind of homotopy between immersions of one manifold in another. The homotopy must be a 1-parameter family of immersions.\n\nSimilar to homotopy classes, one defines two immersions to be in the same regular homotopy class if there exists a regular homotopy between them. Regular homotopy for immersions is similar to isotopy of embeddings: they are both restricted types of homotopies. Stated another way, two continuous functions formula_1 are homotopic if they represent points in the same path-components of the mapping space formula_2, given the compact-open topology. The space of immersions is the subspace of formula_2 consisting of immersions, denote it by formula_4. Two immersions formula_5 are regularly homotopic if they represent points in the same path-component of formula_4.\n\nThe Whitney–Graustein theorem classifies the regular homotopy classes of a circle into the plane; two immersions are regularly homotopic if and only if they have the same turning number – equivalently, total curvature; equivalently, if and only if their Gauss maps have the same degree/winding number.\nStephen Smale classified the regular homotopy classes of a \"k\"-sphere immersed in formula_7 – they are classified by homotopy groups of Stiefel manifolds, which is a generalization of the Gauss map, with here \"k\" partial derivatives not vanishing. A corollary of his work is that there is only one regular homotopy class of a \"2\"-sphere immersed in formula_8. In particular, this means that sphere eversions exist, i.e. one can turn the 2-sphere \"inside-out\".\n\nBoth of these examples consist of reducing regular homotopy to homotopy; this has subsequently been substantially generalized in the homotopy principle (or \"h\"-principle) approach.\n\n"}
{"id": "13268241", "url": "https://en.wikipedia.org/wiki?curid=13268241", "title": "Simple magic cube", "text": "Simple magic cube\n\nA simple magic cube is the lowest of six basic classes of magic cube. These classes are based on extra features required.\n\nThe simple magic cube requires only the basic features a cube requires to be magic. Namely; all lines parallel to the faces, and all 4 triagonals sum correctly. i.e. all 1-agonals and all 3-agonals sum to\n\nNo planar diagonals (2-agonals) are required to sum correctly, so there are probably no magic squares in the cube.\n\n\n"}
{"id": "212838", "url": "https://en.wikipedia.org/wiki?curid=212838", "title": "Sir George Stokes, 1st Baronet", "text": "Sir George Stokes, 1st Baronet\n\nSir George Gabriel Stokes, 1st Baronet, (; 13 August 1819 – 1 February 1903), was an Anglo-Irish physicist and mathematician. Born in County Sligo, Ireland, Stokes spent all of his career at the University of Cambridge, where he was the Lucasian Professor of Mathematics from 1849 until his death in 1903. As a physicist, Stokes made seminal contributions to fluid dynamics, including the Navier-Stokes equation, and to physical optics, with notable works on polarization and fluorescence. As a mathematician, he formulated \"Stokes's theorem\" in vector calculus and contributed to the theory of asymptotic expansions.\n\nStokes was made a baronet (hereditary knight) by the British monarch in 1889. In 1893 he received the Royal Society's Copley Medal, then the most prestigious scientific prize in the world, \"for his researches and discoveries in physical science\". He represented Cambridge University in the British House of Commons from 1887 to 1892, sitting as a Tory. Stokes also served as president of the Royal Society from 1885 to 1890 and was briefly the Master of Pembroke College, Cambridge.\n\nGeorge Stokes was the youngest son of the Reverend Gabriel Stokes, a clergyman in the Church of Ireland who served as rector of Skreen, in County Sligo. Stokes home life was strongly influenced by his father's evangelical Protestantism. After attending schools in Skreen, Dublin, and Bristol, in 1837 Stokes matriculated at Pembroke College, Cambridge. Four years later he graduated as senior wrangler and first Smith's prizeman, achievements that earned him election of a fellow of the college. In accordance with the college statutes, Stokes had to resign the fellowship when he married in 1857. Twelve years later, under new statutes, he was re-elected to the fellowship and he retained that place until 1902, when on the day before his 83rd birthday, he was elected as the college's Master. Stokes did not hold that position for long, for he died at Cambridge on 1 February the following year, and was buried in the Mill Road cemetery.\n\nIn 1849, Stokes was appointed to the Lucasian professorship of mathematics at Cambridge, a position he held until his death in 1903. On 1 June 1899, the jubilee of this appointment was celebrated there in a ceremony, which was attended by numerous delegates from European and American universities. A commemorative gold medal was presented to Stokes by the chancellor of the university and marble busts of Stokes by Hamo Thornycroft were formally offered to Pembroke College and to the university by Lord Kelvin. Stokes, who was made a baronet in 1889, further served his university by representing it in parliament from 1887 to 1892 as one of the two members for the Cambridge University constituency. During a portion of this period (1885–1890) he also was president of the Royal Society, of which he had been one of the secretaries since 1854. Since he was also Lucasian Professor at this time, Stokes was the first person to hold all three positions simultaneously; Newton held the same three, although not at the same time.\n\nStokes was the oldest of the trio of natural philosophers, James Clerk Maxwell and Lord Kelvin being the other two, who especially contributed to the fame of the Cambridge school of mathematical physics in the middle of the 19th century. Stokes's original work began about 1840, and from that date onwards the great extent of his output was only less remarkable than the brilliance of its quality. The Royal Society's catalogue of scientific papers gives the titles of over a hundred memoirs by him published down to 1883. Some of these are only brief notes, others are short controversial or corrective statements, but many are long and elaborate treatises.\n\nIn scope, his work covered a wide range of physical inquiry but, as Marie Alfred Cornu remarked in his Rede lecture of 1899, the greater part of it was concerned with waves and the transformations imposed on them during their passage through various media.\n\nHis first published papers, which appeared in 1842 and 1843, were on the steady motion of incompressible fluids and some cases of fluid motion. These were followed in 1845 by one on the friction of fluids in motion and the equilibrium and motion of elastic solids, and in 1850 by another on the effects of the internal friction of fluids on the motion of pendulums. To the theory of sound he made several contributions, including a discussion of the effect of wind on the intensity of sound and an explanation of how the intensity is influenced by the nature of the gas in which the sound is produced. These inquiries together put the science of fluid dynamics on a new footing, and provided a key not only to the explanation of many natural phenomena, such as the suspension of clouds in air, and the subsidence of ripples and waves in water, but also to the solution of practical problems, such as the flow of water in rivers and channels, and the skin resistance of ships.\n\nHis work on fluid motion and viscosity led to his calculating the terminal velocity for a sphere falling in a viscous medium. This became known as Stokes's law. He derived an expression for the frictional force (also called drag force) exerted on spherical objects with very small Reynolds numbers.\n\nHis work is the basis of the falling sphere viscometer, in which the fluid is stationary in a vertical glass tube. A sphere of known size and density is allowed to descend through the liquid. If correctly selected, it reaches terminal velocity, which can be measured by the time it takes to pass two marks on the tube. Electronic sensing can be used for opaque fluids. Knowing the terminal velocity, the size and density of the sphere, and the density of the liquid, Stokes's law can be used to calculate the viscosity of the fluid. A series of steel ball bearings of different diameter is normally used in the classic experiment to improve the accuracy of the calculation. The school experiment uses glycerine as the fluid, and the technique is used industrially to check the viscosity of fluids used in processes.\n\nThe same theory explains why small water droplets (or ice crystals) can remain suspended in air (as clouds) until they grow to a critical size and start falling as rain (or snow and hail). Similar use of the equation can be made in the settlement of fine particles in water or other fluids.\n\nThe CGS unit of kinematic viscosity was named \"stokes\" in recognition of his work.\n\nPerhaps his best-known researches are those which deal with the wave theory of light. His optical work began at an early period in his scientific career. His first papers on the aberration of light appeared in 1845 and 1846, and were followed in 1848 by one on the theory of certain bands seen in the spectrum.\n\nIn 1849 he published a long paper on the dynamical theory of diffraction, in which he showed that the plane of polarisation must be perpendicular to the direction of propagation. Two years later he discussed the colours of thick plates.\n\nStokes also investigated George Airy's mathematical description of rainbows. Airy's findings involved an integral that was awkward to evaluate. Stokes expressed the integral as a divergent series, which were little understood. However, by cleverly truncating the series (i.e., ignoring all except the first few terms of the series), Stokes obtained an accurate approximation to the integral that was far easier to evaluate than the integral itself. Stokes's research on asymptotic series led to fundamental insights about such series.\n\nIn 1852, in his famous paper on the change of wavelength of light, he described the phenomenon of fluorescence, as exhibited by fluorspar and uranium glass, materials which he viewed as having the power to convert invisible ultra-violet radiation into radiation of longer wavelengths that are visible. The Stokes shift, which describes this conversion, is named in Stokes's honour. A mechanical model, illustrating the dynamical principle of Stokes's explanation was shown. The offshoot of this, Stokes line, is the basis of Raman scattering. In 1883, during a lecture at the Royal Institution, Lord Kelvin said he had heard an account of it from Stokes many years before, and had repeatedly but vainly begged him to publish it.\n\nIn the same year, 1852, there appeared the paper on the composition and resolution of streams of polarised light from different sources, and in 1853 an investigation of the metallic reflection exhibited by certain non-metallic substances. The research was to highlight the phenomenon of light polarisation. About 1860 he was engaged in an inquiry on the intensity of light reflected from, or transmitted through, a pile of plates; and in 1862 he prepared for the British Association a valuable report on double refraction, a phenomenon where certain crystals show different refractive indices along different axes. Perhaps the best known crystal is Iceland spar, transparent calcite crystals.\n\nA paper on the long spectrum of the electric light bears the same date, and was followed by an inquiry into the absorption spectrum of blood.\n\nThe chemical identification of organic bodies by their optical properties was treated in 1864; and later, in conjunction with the Rev. William Vernon Harcourt, he investigated the relation between the chemical composition and the optical properties of various glasses, with reference to the conditions of transparency and the improvement of achromatic telescopes. A still later paper connected with the construction of optical instruments discussed the theoretical limits to the aperture of microscope objectives.\n\nIn other departments of physics may be mentioned his paper on the conduction of heat in crystals (1851) and his inquiries in connection with Crookes radiometer; his explanation of the light border frequently noticed in photographs just outside the outline of a dark body seen against the sky (1883); and, still later, his theory of the x-rays, which he suggested might be transverse waves travelling as innumerable solitary waves, not in regular trains. Two long papers published in 1849 – one on attractions and Clairaut's theorem, and the other on the variation of gravity at the surface of the earth (1849)—also demand notice, as do his mathematical memoirs on the critical values of sums of periodic series (1847) and on the numerical calculation of a class of definite integrals and infinite series (1850) and his discussion of a differential equation relating to the breaking of railway bridges (1849), research related to his evidence given to the \"Royal Commission on the Use of Iron in Railway structures\" after the Dee bridge disaster of 1847.\n\nMany of Stokes' discoveries were not published, or were only touched upon in the course of his oral lectures. One such example is his work in the theory of spectroscopy.\n\nIn his presidential address to the British Association in 1871, Lord Kelvin stated his belief that the application of the prismatic analysis of light to solar and stellar chemistry had never been suggested directly or indirectly by anyone else when Stokes taught it to him at Cambridge University some time prior to the summer of 1852, and he set forth the conclusions, theoretical and practical, which he learnt from Stokes at that time, and which he afterwards gave regularly in his public lectures at Glasgow. These statements, containing as they do the physical basis on which spectroscopy rests, and the way in which it is applicable to the identification of substances existing in the sun and stars, make it appear that Stokes anticipated Kirchhoff by at least seven or eight years. Stokes, however, in a letter published some years after the delivery of this address, stated that he had failed to take one essential step in the argument—not perceiving that emission of light of definite wavelength not merely permitted, but necessitated, absorption of light of the same wavelength. He modestly disclaimed \"any part of Kirchhoff's admirable discovery,\" adding that he felt some of his friends had been over-zealous in his cause. It must be said, however, that English men of science have not accepted this disclaimer in all its fullness, and still attribute to Stokes the credit of having first enunciated the fundamental principles of spectroscopy.\n\nIn another way, too, Stokes did much for the progress of mathematical physics. Soon after he was elected to the Lucasian chair he announced that he regarded it as part of his professional duties to help any member of the university in difficulties he might encounter in his mathematical studies, and the assistance rendered was so real that pupils were glad to consult him, even after they had become colleagues, on mathematical and physical problems in which they found themselves at a loss. Then during the thirty years he acted as secretary of the Royal Society he exercised an enormous if inconspicuous influence on the advancement of mathematical and physical science, not only directly by his own investigations, but indirectly by suggesting problems for inquiry and inciting men to attack them, and by his readiness to give encouragement and help.\n\nStokes was involved in several investigations into railway accidents, especially the Dee bridge disaster in May 1847, and he served as a member of the subsequent Royal Commission into the use of cast iron in railway structures. He contributed to the calculation of the forces exerted by moving engines on bridges. The bridge failed because a cast iron beam was used to support the loads of passing trains. Cast iron is brittle in tension or bending, and many other similar bridges had to be demolished or reinforced.\n\nHe appeared as an expert witness at the Tay Bridge disaster, where he gave evidence about the effects of wind loads on the bridge. The centre section of the bridge (known as the High Girders) was completely destroyed during a storm on 28 December 1879, while an express train was in the section, and everyone aboard died (more than 75 victims). The Board of Inquiry listened to many expert witnesses, and concluded that the bridge was \"badly designed, badly built and badly maintained\".\n\nAs a result of his evidence, he was appointed a member of the subsequent Royal Commission into the effect of wind pressure on structures. The effects of high winds on large structures had been neglected at that time, and the commission conducted a series of measurements across Britain to gain an appreciation of wind speeds during storms, and the pressures they exerted on exposed surfaces.\n\nStokes held conservative religious values and beliefs. In 1886, he became president of the Victoria Institute, which had been founded to defend evangelical Christian principles against challenges from the new sciences, especially the Darwinian theory of biological evolution. He gave the 1891 Gifford lecture on natural theology. He was also the vice-president of the British and Foreign Bible Society and was actively involved in doctrinal debates concerning missionary work.\n\nAs President of the Victoria Institute, Stokes wrote: \"\"We all admit that the book of Nature and the book of Revelation come alike from God, and that consequently there can be no real discrepancy between the two if rightly interpreted. The provisions of Science and Revelation are, for the most part, so distinct that there is little chance of collision. But if an apparent discrepancy should arise, we have no right on principle, to exclude either in favour of the other. For however firmly convinced we may be of the truth of revelation, we must admit our liability to err as to the extent or interpretation of what is revealed; and however strong the scientific evidence in favour of a theory may be, we must remember that we are dealing with evidence which, in its nature, is probable only, and it is conceivable that wider scientific knowledge might lead us to alter our opinion\".\"\n\nHe married, on 4 July 1857 at St Patrick's Cathedral, Armagh, Mary Susanna Robinson, daughter of the Rev Thomas Romney Robinson. They had five children: Arthur Romney, who inherited the baronetcy; Susanna Elizabeth, who died in infancy; Isabella Lucy (Mrs Laurence Humphry) who contributed the personal memoir of her father in \"Memoir and Scientific Correspondence of the Late George Gabriel Stokes, Bart\"; Dr William George Gabriel, physician, a troubled man who committed suicide aged 30 whilst temporarily insane; and Dora Susanna, who died in infancy. He is survived by his great great granddaughter Briana Stokes.\n\n\nStokes's mathematical and physical papers (see external links) were published in a collected form in five volumes; the first three (Cambridge, 1880, 1883, and 1901) under his own editorship, and the two last (Cambridge, 1904 and 1905) under that of Sir Joseph Larmor, who also selected and arranged the \"Memoir and Scientific Correspondence of Stokes\" published at Cambridge in 1907.\n\n\n"}
{"id": "2216678", "url": "https://en.wikipedia.org/wiki?curid=2216678", "title": "Term algebra", "text": "Term algebra\n\nIn universal algebra and mathematical logic, a term algebra is a freely generated algebraic structure over a given signature. For example, in a signature consisting of a single binary operation, the term algebra over a set \"X\" of variables is exactly the free magma generated by \"X\". Other synonyms for the notion include absolutely free algebra and anarchic algebra.\n\nFrom a category theory perspective, a term algebra is the initial object for the category of all algebras of the same signature, and this object, unique up to isomorphism, is called an initial algebra; it generates by homomorphic projection all algebras in the category.\n\nA similar notion is that of a Herbrand universe in logic, usually used under this name in logic programming, which is (absolutely freely) defined starting from the set of constants and function symbols in a set of clauses. That is, the Herbrand universe consists of all ground terms: terms that have no variables in them.\n\nAn atomic formula or atom is commonly defined as a predicate applied to a tuple of terms; a ground atom is then a predicate in which only ground terms appear. The Herbrand base is the set of all ground atoms that can be formed from predicate symbols in the original set of clauses and terms in its Herbrand universe. These two concepts are named after Jacques Herbrand.\n\nTerm algebras also play a role in the semantics of abstract data types, where an abstract data type declaration provides the signature of a multi-sorted algebraic structure and the term algebra is a concrete model of the abstract declaration.\n\nTerm algebras can be shown decidable using quantifier elimination. The complexity of the decision problem is in NONELEMENTARY.\n\nThe signature σ of a language is a triple <O,F,P> consisting of the alphabet of constants \"O\", the function symbols \"F\", and the predicates \"P\". The Herbrand base of a signature σ consists of all ground atoms of σ: of all formulas of the form \"R\"(\"t\", …, \"t\"), where \"t\", …, \"t\" are terms containing no variables (i.e. elements of the Herbrand universe) and \"R\" is an \"n\"-ary relation symbol (\"i.e.\" predicate). In the case of logic with equality, it also contains all equations of the form \"t\"=\"t\", where \"t\" and \"t\" contain no variables.\n\n\n"}
{"id": "3728109", "url": "https://en.wikipedia.org/wiki?curid=3728109", "title": "Variable (mathematics)", "text": "Variable (mathematics)\n\nIn elementary mathematics, a variable is a symbol, commonly an alphabetic character, that represents a number, called the \"value\" of the variable, which is either arbitrary, not fully specified, or unknown. Making algebraic computations with variables as if they were explicit numbers allows one to solve a range of problems in a single computation. A typical example is the quadratic formula, which allows one to solve every quadratic equation by simply substituting the numeric values of the coefficients of the given equation to the variables that represent them.\n\nThe concept of a \"variable\" is also fundamental in calculus.\nTypically, a function involves two variables, and , representing respectively the value and the argument of the function. The term \"variable\" comes from the fact that, when the argument (also called the \"variable of the function\") \"varies\", then the value \"varies\" accordingly.\n\nIn more advanced mathematics, a \"variable\" is a symbol that denotes a mathematical object, which could be a number, a vector, a matrix, or even a function. In this case, the original property of \"variability\" of a variable is not kept (except, sometimes, for informal explanations).\n\nSimilarly, in computer science, a \"variable\" is a name (commonly an alphabetic character or a word) representing some value stored in computer memory. In mathematical logic, a \"variable\" is either a symbol representing an unspecified term of the theory, or a basic object of the theory, which is manipulated without referring to its possible intuitive interpretation.\n\n\"Variable\" comes from a Latin word, \"variābilis\", with \"vari(us)\"' meaning \"various\" and \"-ābilis\"' meaning \"-able\", meaning \"capable of changing\".\n\nIn the 7th century Brahmagupta used different colours to represent the unknowns in algebraic equations in the \"Brāhmasphuṭasiddhānta\". One section of this book is called \"Equations of Several Colours\".\n\nAt the end of the 16th century François Viète introduced the idea of representing known and unknown numbers by letters, nowadays called variables, and of computing with them as if they were numbers, in order to obtain the result by a simple replacement. Viète's convention was to use consonants for known values and vowels for unknowns.\n\nIn 1637, René Descartes \"invented the convention of representing unknowns in equations by \"x\", \"y\", and \"z\", and knowns by \"a\", \"b\", and \"c\"\". Contrarily to Viète's convention, Descartes' is still commonly in use.\n\nStarting in the 1660s, Isaac Newton and Gottfried Wilhelm Leibniz independently developed the infinitesimal calculus, which essentially consists of studying how an infinitesimal variation of a \"variable quantity\" induces a corresponding variation of another quantity which is a \"function\" of the first variable (quantity). Almost a century later Leonhard Euler fixed the terminology of infinitesimal calculus and introduced the notation for a function , its variable and its value . Until the end of the 19th century, the word \"variable\" referred almost exclusively to the arguments and the values of functions.\n\nIn the second half of the 19th century, it appeared that the foundation of infinitesimal calculus was not formalized enough to deal with apparent paradoxes such as a continuous function which is nowhere differentiable. To solve this problem, Karl Weierstrass introduced a new formalism consisting of replacing the intuitive notion of limit by a formal definition. The older notion of limit was \"when the \"variable\" varies and tends toward , then tends toward \", without any accurate definition of \"tends\". Weierstrass replaced this sentence by the formula\nin which none of the five variables is considered as varying.\n\nThis static formulation led to the modern notion of variable which is simply a symbol representing a mathematical object which either is unknown or may be replaced by any element of a given set; for example, the set of real numbers.\n\nIt is common for variables to play different roles in the same mathematical formula and names or qualifiers have been introduced to distinguish them. \nFor example, the general cubic equation\nis interpreted as having five variables: four, , which are taken to be given numbers and the fifth variable, is understood to be an \"unknown\" number. To distinguish them, the variable is called \"an unknown\", and the other variables are called \"parameters\" or \"coefficients\", or sometimes \"constants\", although this last terminology is incorrect for an equation and should be reserved for the function defined by the left-hand side of this equation.\n\nIn the context of functions, the term \"variable\" refers commonly to the arguments of the functions. This is typically the case in sentences like \"function of a real variable\", \" is the variable of the function \", \" is a function of the variable \" (meaning that the argument of the function is referred to by the variable ).\n\nIn the same context, variables that are independent of define constant functions and are therefore called \"constant\". For example, a \"constant of integration\" is an arbitrary constant function that is added to a particular antiderivative to obtain the other antiderivatives. Because the strong relationship between polynomials and polynomial function, the term \"constant\" is often used to denote the coefficients of a polynomial, which are constant functions of the indeterminates.\n\nThis use of \"constant\" as an abbreviation of \"constant function\" must be distinguished from the normal meaning of the word in mathematics. A constant, or mathematical constant is a well and unambiguously defined number or other mathematical object, as, for example, the numbers 0, 1, and the identity element of a group.\n\nOther specific names for variables are:\n\n\nIt should be emphasized that all these denominations of variables are of semantic nature and that the way of computing with them (syntax) is the same for all.\n\nIn calculus and its application to physics and other sciences, it is rather common to consider a variable, say , whose possible values depend on the value of another variable, say . In mathematical terms, the \"dependent\" variable represents the value of a function of . To simplify formulas, it is often useful to use the same symbol for the dependent variable and the function mapping onto . For example, the state of a physical system depends on measurable quantities such as the pressure, the temperature, the spatial position, ..., and all these quantities vary when the system evolves, that is, they are function of the time. In the formulas describing the system, these quantities are represented by variables which are dependent on the time, and thus considered implicitly as functions of the time.\n\nTherefore, in a formula, a dependent variable is a variable that is implicitly a function of another (or several other) variables. An independent variable is a variable that is not dependent.\n\nThe property of a variable to be dependent or independent depends often of the point of view and is not intrinsic. For example, in the notation , the three variables may be all independent and the notation represents a function of three variables. On the other hand, if and depend on (are \"dependent variables\") then the notation represents a function of the single \"independent variable\" .\n\nIf one defines a function \"f\" from the real numbers to the real numbers by\n\nthen \"x\" is a variable standing for the argument of the function being defined, which can be any real number. In the identity\n\nthe variable \"i\" is a summation variable which designates in turn each of the integers 1, 2, ..., \"n\" (it is also called index because its variation is over a discrete set of values) while \"n\" is a parameter (it does not vary within the formula).\n\nIn the theory of polynomials, a polynomial of degree 2 is generally denoted as \"ax\" + \"bx\" + \"c\", where \"a\", \"b\" and \"c\" are called coefficients (they are assumed to be fixed, i.e., parameters of the problem considered) while \"x\" is called a variable. When studying this polynomial for its polynomial function this \"x\" stands for the function argument. When studying the polynomial as an object in itself, \"x\" is taken to be an indeterminate, and would often be written with a capital letter instead to indicate this status.\n\nIn mathematics, the variables are generally denoted by a single letter. However, this letter is frequently followed by a subscript, as in , and this subscript may be a number, another variable (), a word or the abbreviation of a word ( and ), and even a mathematical expression. Under the influence of computer science, one may encounter in pure mathematics some variable names consisting in several letters and digits.\n\nFollowing the 17th century French philosopher and mathematician, René Descartes, letters at the beginning of the alphabet, e.g. \"a\", \"b\", \"c\" are commonly used for known values and parameters, and letters at the end of the alphabet, e.g. \"x\", \"y\", \"z\", and \"t\" are commonly used for unknowns and variables of functions. In printed mathematics, the norm is to set variables and constants in an italic typeface.\n\nFor example, a general quadratic function is conventionally written as:\nwhere \"a\", \"b\" and \"c\" are parameters (also called constants, because they are constant functions), while \"x\" is the variable of the function. A more explicit way to denote this function is\nwhich makes the function-argument status of \"x\" clear, and thereby implicitly the constant status of \"a\", \"b\" and \"c\". Since \"c\" occurs in a term that is a constant function of \"x\", it is called the constant term.\n\nSpecific branches and applications of mathematics usually have specific naming conventions for variables. Variables with similar roles or meanings are often assigned consecutive letters. For example, the three axes in 3D coordinate space are conventionally called \"x\", \"y\", and \"z\". In physics, the names of variables are largely determined by the physical quantity they describe, but various naming conventions exist.\nA convention often followed in probability and statistics is to use \"X\", \"Y\", \"Z\" for the names of random variables, keeping \"x\", \"y\", \"z\" for variables representing corresponding actual values.\n\nThere are many other notational usages. Usually, variables that play a similar role are represented by consecutive letters or by the same letter with different subscript. Below are some of the most common usages.\n\n\n"}
{"id": "52722448", "url": "https://en.wikipedia.org/wiki?curid=52722448", "title": "XVA", "text": "XVA\n\nAn X-Value Adjustment (XVA, xVA) is a generic term referring collectively to a number of different “Valuation Adjustments” in relation to derivative instruments held by banks. The purpose of these is twofold: primarily to hedge for possible losses due to counterparty default; but also, to determine (and hedge) the amount of capital required under Basel III. For a discussion as to the impact of xVA on the bank's overall balance sheet, return on equity, and dividend policy, see: XVA has, in many institutions, led to the creation of specialized desks. Note that the various XVA require careful and correct aggregation without double counting.\n\nHistorically, (OTC) derivative pricing has relied on the Black-Scholes' risk neutral pricing framework, under the assumptions of funding at the risk free rate and the ability to perfectly replicate derivatives so as to fully hedge; see ; . \nThis, in turn, is built on the assumption of a credit-risk-free environment. Post financial crisis of 2008, therefore, counterparty credit risk must also be considered in the valuation, and the risk neutral value is then adjusted correspondingly. See for further discussion.\n\nThese calculations in overview: When the deal is collateralized then the \"fair-value\" is computed as before, but using the overnight index swap (OIS) curve for discounting. (The OIS is chosen here as it reflects the rate for overnight unsecured lending between banks, and is thus considered a good indicator of the interbank credit markets.) When the deal is not collateralized then a credit valuation adjustment, or CVA, is added to this value; essentially, the discounted risk-neutral expectation of any loss due to the counterparty not performing - typically calculated under a simulation framework. \n\nNote that when transactions are governed by a master agreement that includes netting, then the expected loss from a default depends on the whole portfolio, and cannot be calculated on a transaction-by-transaction basis. The CVA (xVA) applied to a new transaction should be the incremental effect of the new transaction on portfolio CVA.\n\nWhile the CVA reflects the market value of counterparty credit risk, additional \"Valuation Adjustments\" for Debit, Funding, regulatory capital and margin may similarly be added. As for CVA, these results are modeled via simulation as a function of the risk-neutral expectation of (a) values of the underlying instrument, and the relevant market values, and (b) creditworthyness of the counterparty. As above, the various XVA require careful and correct aggregation without double counting.\n\nThese adjustments:\n\nOther adjustments are also sometimes made including TVA, for tax and RVA, for replacement of the derivative on downgrade. FVA may be decomposed into FCA for receivables and FBA for payables - where FCA is due to self funded borrowing spread over Libor, and FBA due to self funded lending. Relatedly, LVA represents the specific liquidity adjustment, while CollVA is the value of the optionality embedded in a CSA to post collateral in different currencies. CRA , the collateral rate adjustment, reflects the present value of the expected excess of net interest paid on cash collateral over the net interest that would be paid if the interest rate\nequaled the risk-free rate.\n\n"}
