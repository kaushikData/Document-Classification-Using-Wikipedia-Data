{"id": "2290298", "url": "https://en.wikipedia.org/wiki?curid=2290298", "title": "179 (number)", "text": "179 (number)\n\n179 (one hundred [and] seventy-nine) is the natural number following 178 and preceding 180.\n\n179 is an odd number.\n\n179 is a prime number; that is, it is not divisible by integer (except for 1 and itself). It is an Eisenstein prime, as it is indivisible even by complex Gaussian integers. It is a Chen prime, being two less than another prime, 181. It is a full reptend prime, meaning 1/179 has a decimal expansion of a repeated sequence of 178 digits.\n\n179 is a safe prime, as it is one more than two times the prime 89. It is also a Sophie Germain prime, as the prime 359 is one more than two times 179. It is only the fifth number with both of these properties (after 5, 11, 23, and 83).\n\n179 is a strictly non-palindromic number. It is not a palindromic number in any base.\n\n179 (of 365) days of the year are even numbered.\n\n\n"}
{"id": "695082", "url": "https://en.wikipedia.org/wiki?curid=695082", "title": "Antifundamental representation", "text": "Antifundamental representation\n\nIn mathematics, an antifundamental representation of a Lie group is the complex conjugate of the fundamental representation, although the distinction between the fundamental and the antifundamental representation is a matter of convention. However, these two are often non-equivalent, because each of them is a complex representation.\n"}
{"id": "56369", "url": "https://en.wikipedia.org/wiki?curid=56369", "title": "Bell's theorem", "text": "Bell's theorem\n\nBell's theorem is a \"no-go theorem\" that draws an important distinction between quantum mechanics and the world as described by classical mechanics, particularly concerning \"quantum entanglement\" where two or more particles in a \"quantum state\" continue to be mutually dependent, even at large physical separations. This theorem is named after John Stewart Bell.\n\nA series of experiments has verified the theorem and showed that quantum entanglement occurs over large distances. \nQuantum entanglement has profound implications for the outcomes of measurements of quantum systems; for example, in \"quantum computing\".\n\nIn its simplest form, Bell's theorem states:\n\nCornell solid-state physicist David Mermin has described the appraisals of the importance of Bell's theorem in the physics community as ranging from \"indifference\" to \"wild extravagance\". Lawrence Berkeley particle physicist Henry Stapp declared: \"Bell's theorem is the most profound discovery of science.\"\n\nBell's theorem rules out local hidden variables as a viable explanation of quantum mechanics (though it still leaves the door open for non-local hidden variables, such as De Broglie–Bohm theory, etc). Bell concluded:\n\nBell summarized one of the least popular ways to address the theorem, superdeterminism, in a 1985 BBC Radio interview:\n\nIn the early 1930s, the philosophical implications of the current interpretations of quantum theory troubled many prominent physicists of the day, including Albert Einstein. In a well-known 1935 paper, Boris Podolsky and co-authors Einstein and Nathan Rosen (collectively \"EPR\") sought to demonstrate by the EPR paradox that quantum mechanics was incomplete. This provided hope that a more complete (and less troubling) theory might one day be discovered. But that conclusion rested on the seemingly reasonable assumptions of \"locality\" and \"realism\" (together called \"local realism\" or \"local hidden variables\", often interchangeably). In the vernacular of Einstein: locality meant no instantaneous (\"spooky\") action at a distance; realism meant the moon is there even when not being observed. These assumptions were hotly debated in the physics community, notably between Einstein and Niels Bohr.\n\nIn his groundbreaking 1964 paper, \"On the Einstein Podolsky Rosen paradox\", physicist John Stewart Bell presented an analogy (based on spin measurements on pairs of entangled electrons) to EPR's hypothetical paradox. Using their reasoning, he said, a choice of measurement setting here should not affect the outcome of a measurement there (and vice versa). After providing a mathematical formulation of locality and realism based on this, he showed specific cases where this would be inconsistent with the predictions of quantum mechanics theory.\n\nIn experimental tests following Bell's example, now using quantum entanglement of photons instead of electrons, John Clauser and Stuart Freedman (1972) and Alain Aspect \"et al\". (1981) demonstrated that the predictions of quantum mechanics are correct in this regard, although relying on additional unverifiable assumptions that open loopholes for local realism.\n\nIn October 2015, Hensen and co-workers reported that they performed a loophole-free Bell test which might force one to reject at least one of the principles of locality, realism, or freedom-of-choice (the last \"could\" lead to alternative superdeterministic theories). Two of these logical possibilities, non-locality and non-realism, correspond to well-developed interpretations of quantum mechanics, and have many supporters; this is not the case for the third logical possibility, non-freedom. Conclusive experimental evidence of the violation of Bell's inequality would drastically reduce the class of acceptable deterministic theories but would not falsify absolute determinism, which was described by Bell himself as \"not just inanimate nature running on behind-the-scenes clockwork, but with our behaviour, including our belief that we are free to choose to do one experiment rather than another, absolutely predetermined\". However, Bell himself considered absolute determinism an implausible solution.\n\nBell's theorem states that any physical theory that incorporates local realism cannot reproduce all the predictions of quantum mechanical theory. Because numerous experiments agree with the predictions of quantum mechanical theory, and show differences between correlations that could not be explained by local hidden variables, the experimental results have been taken by many as refuting the concept of local realism as an explanation of the physical phenomena under test. For a hidden variable theory, if Bell's conditions are correct, the results that agree with quantum mechanical theory appear to indicate superluminal (faster-than-light) effects, in contradiction to the principle of locality.\n\nThese three key concepts – locality, realism, freedom – are highly technical and much debated. In particular, the concept of \"realism\" is now somewhat different from what it was in discussions in the 1930s. It is more precisely called \"counterfactual definiteness\"; it means that we may think of outcomes of measurements that were not actually performed as being just as much part of reality as those that were made. \"Locality\" is short for \"local relativistic causality\". (Currently accepted quantum field theories \"are\" local in the terminology of the Lagrangian formalism and axiomatic approach.) \"Freedom\" refers to the physical possibility of determining settings on measurement devices independently of the internal state of the physical system being measured.\n\nThe theorem is usually proved by consideration of a quantum system of two entangled qubits. The most common examples concern systems of particles that are entangled in spin or polarization. Quantum mechanics allows predictions of correlations that would be observed if these two particles have their spin or polarization measured in different directions. Bell showed that if a local hidden variable theory holds, then these correlations would have to satisfy certain constraints, called Bell inequalities. However, for the quantum correlations arising in the specific example considered, those constraints are not satisfied, hence the phenomenon being studied cannot be explained by a local hidden variables theory.\n\nFollowing the argument in the Einstein–Podolsky–Rosen (EPR) paradox paper (but using the example of spin, as in David Bohm's version of the EPR argument), Bell considered an experiment in which there are \"a pair of spin one-half particles formed somehow in the singlet spin state and moving freely in opposite directions.\" The two particles travel away from each other to two distant locations, at which measurements of spin are performed, along axes that are independently chosen. Each measurement yields a result of either spin-up (+) or spin-down (−); it means, spin in the positive or negative direction of the chosen axis.\n\nThe probability of the same result being obtained at the two locations depends on the relative angles at which the two spin measurements are made, and is strictly between zero and one for all relative angles other than perfectly parallel or antiparallel alignments (0° or 180°). Since total angular momentum is conserved, and since the total spin is zero in the singlet state, the probability of the same result with parallel (antiparallel) alignment is 0 (1). This last prediction is true classically as well as quantum mechanically.\n\nBell's theorem is concerned with correlations defined in terms of averages taken over very many trials of the experiment. The correlation of two binary variables is usually defined in quantum physics as the average of the products of the pairs of measurements. Note that this is different from the usual definition of correlation in statistics. The quantum physicist's \"correlation\" is the statistician's \"raw (uncentered, unnormalized) product moment\". They are similar in that, with either definition, if the pairs of outcomes are always the same, the correlation is +1; if the pairs of outcomes are always opposite, the correlation is -1; and if the pairs of outcomes agree 50% of the time, then the correlation is 0. The correlation is related in a simple way to the probability of equal outcomes, namely it is equal to twice the probability of equal outcomes, minus one.\n\nMeasuring the spin of these entangled particles along anti-parallel directions—i.e., along the same axis but in opposite directions, the set of all results is perfectly correlated. On the other hand, if measurements are performed along parallel directions they always yield opposite results, and the set of measurements shows perfect anti-correlation. This is in accord with the above stated probabilities of measuring the same result in these two cases. Finally, measurement at perpendicular directions has a 50% chance of matching, and the total set of measurements is uncorrelated. These basic cases are illustrated in the table below. Columns should be read as \"examples\" of pairs of values that could be recorded by Alice and Bob with time increasing going to the right.\n\nWith the measurements oriented at intermediate angles between these basic cases, the existence of local hidden variables could agree with/would be consistent with a linear dependence of the correlation in the angle but, according to Bell's inequality (see below), could not agree with the dependence predicted by quantum mechanical theory, namely, that the correlation is the negative cosine of the angle. Experimental results match the curve predicted by quantum mechanics.\n\nOver the years, Bell's theorem has undergone a wide variety of experimental tests. However, various common deficiencies in the testing of the theorem have been identified, including the \"detection loophole\" and the \"communication loophole\". Over the years experiments have been gradually improved to better address these loopholes. In 2015, the first experiment to simultaneously address all of the loopholes was performed.\n\nTo date, Bell's theorem is generally regarded as supported by a substantial body of evidence and there are few supporters of local hidden variables, though the theorem is continually the subject of study, criticism, and refinement.\n\nBell's theorem, derived in his seminal 1964 paper titled \"On the Einstein Podolsky Rosen paradox\", has been called, on the assumption that the theory is correct, \"the most profound in science\". Perhaps of equal importance is Bell's deliberate effort to encourage and bring legitimacy to work on the completeness issues, which had fallen into disrepute. Later in his life, Bell expressed his hope that such work would \"continue to inspire those who suspect that what is proved by the impossibility proofs is lack of imagination.\"\n\nThe title of Bell's seminal article refers to the 1935 paper by Einstein, Podolsky and Rosen that challenged the completeness of quantum mechanics. In his paper, Bell started from the same two assumptions as did EPR, namely (i) \"reality\" (that microscopic objects have real properties determining the outcomes of quantum mechanical measurements), and (ii) \"locality\" (that reality in one location is not influenced by measurements performed simultaneously at a distant location). Bell was able to derive from those two assumptions an important result, namely Bell's inequality. The theoretical (and later experimental) violation of this inequality implies that at least one of the two assumptions must be false.\n\nIn two respects Bell's 1964 paper was a step forward compared to the EPR paper: firstly, it considered more hidden variables than merely the element of physical reality in the EPR paper; and Bell's inequality was, in part, experimentally testable, thus raising the possibility of testing the local realism hypothesis. Limitations on such tests to date are noted below. Whereas Bell's paper deals only with deterministic hidden variable theories, Bell's theorem was later generalized to stochastic theories as well, and it was also realised that the theorem is not so much about hidden variables, as about the outcomes of measurements that could have been taken instead of the one actually taken. Existence of these variables is called the assumption of realism, or the assumption of counterfactual definiteness.\n\nAfter the EPR paper, quantum mechanics was in an unsatisfactory position: either it was incomplete, in the sense that it failed to account for some elements of physical reality, or it violated the principle of a finite propagation speed of physical effects. In a modified version of the EPR thought experiment, two hypothetical observers, now commonly referred to as \"Alice\" and \"Bob\", perform independent measurements of spin on a pair of electrons, prepared at a source in a special state called a \"spin singlet state\". It is the conclusion of EPR that once Alice measures spin in one direction (e.g. on the \"x\" axis), Bob's measurement in that direction is determined with certainty, as being the opposite outcome to that of Alice, whereas immediately before Alice's measurement Bob's outcome was only statistically determined (i.e., was only a probability, not a certainty); thus, either the spin in each direction is an \"element of physical reality\", or the effects travel from Alice to Bob instantly.\n\nIn QM, predictions are formulated in terms of probabilities — for example, the probability that an electron will be detected in a particular place, or the probability that its spin is up or down. The idea persisted, however, that the electron in fact has a \"definite\" position and spin, and that QM's weakness is its inability to predict those values precisely. The possibility existed that some unknown theory, such as a \"hidden variables theory\", might be able to predict those quantities exactly, while at the same time also being in complete agreement with the probabilities predicted by QM. If such a hidden variables theory exists, then because the hidden variables are not described by QM the latter would be an incomplete theory.\n\nThe concept of local realism is formalized to state, and prove, Bell's theorem and generalizations. A common approach is the following:\n\nPerfect anti-correlation would require . Implicit in assumption 1) above, the hidden parameter space has a probability measure and the expectation of a random variable on with respect to is written\n\nwhere for accessibility of notation we assume that the probability measure has a probability density that therefore is nonnegative and integrates to . The hidden parameter is often thought of as being associated with the source but it can just as well also contain components associated with the two measurement devices.\n\nBell inequalities concern measurements made by observers on pairs of particles that have interacted and then separated. Assuming local realism, certain constraints must hold on the relationships between the correlations between subsequent measurements of the particles under various possible measurement settings. Let and be as above. Define for the present purposes three correlation functions:\nThe two-particle spin space is the tensor product of the two-dimensional spin Hilbert spaces of the individual particles. Each individual space is an irreducible representation space of the rotation group SO(3). The product space decomposes as a direct sum of irreducible representations with definite total spins and of dimensions and respectively. Full details may be found in Clebsch—Gordan decomposition. The total spin zero subspace is spanned by the singlet state in the product space, a vector explicitly given by\n\nwith adjoint in this representation\n\nThe way single particle operators act on the product space is exemplified below by the example at hand; one defines the tensor product of operators, where the factors are single particle operators, thus if are single particle operators,\n\nand\n\netc., where the superscript in parentheses indicates on which Hilbert space in the tensor product space the action is intended and the action is defined by the right hand side. The singlet state has total spin as may be verified by application of the operator of total spin by a calculation similar to that presented below.\n\nThe expectation value of the operator\n\nin the singlet state can be calculated straightforwardly. One has, by definition of the Pauli matrices,\n\nUpon left application of this on one obtains\n\nLikewise, application (to the left) of the operator corresponding to on yields\n\nThe inner products on the tensor product space is defined by\n\nGiven this, the expectation value reduces to\n\nWith this notation, a concise summary of what follows can be made.\n\nThe inequality that Bell derived can then be written as:\n\nwhere and refer to three arbitrary settings of the two analysers. This inequality is however restricted in its application to the rather special case in which the outcomes on both sides of the experiment are always exactly anticorrelated whenever the analysers are parallel. The advantage of restricting attention to this special case is the resulting simplicity of the derivation. In experimental work, the inequality is not very useful because it is hard, if not impossible, to create \"perfect\" anti-correlation.\n\nThis simple form has an intuitive explanation, however. It is equivalent to the following elementary result from probability theory. Consider three (highly correlated, and possibly biased) coin-flips , and \"Z\", with the property that:\n\nthen \"X\" and \"Z\" must also yield the same outcome at least 98% of the time. The number of mismatches between \"X\" and \"Y\" (1/100) plus the number of mismatches between \"Y\" and \"Z\" (1/100) are together the \"maximum possible\" number of mismatches between \"X\" and \"Z\" (a simple Boole–Fréchet inequality).\n\nImagine a pair of particles that can be measured at distant locations. Suppose that the measurement devices have settings, which are angles—e.g., the devices measure something called spin in some direction. The experimenter chooses the directions, one for each particle, separately. Suppose the measurement outcome is binary (e.g., spin up, spin down). Suppose the two particles are perfectly anti-correlated—in the sense that whenever both measured in the same direction, one gets identically opposite outcomes, when both measured in opposite directions they always give the same outcome. The only way to imagine how this works is that both particles leave their common source with, somehow, the outcomes they will deliver when measured in any possible direction. (How else could particle 1 know how to deliver the same answer as particle 2 when measured in the same direction? They don't know in advance how they are going to be measured...). The measurement on particle 2 (after switching its sign) can be thought of as telling us what the same measurement on particle 1 would have given.\n\nStart with one setting exactly opposite to the other. All the pairs of particles give the same outcome (each pair is either both spin up or both spin down). Now shift Alice's setting by one degree relative to Bob's. They are now one degree off being exactly opposite to one another. A small fraction of the pairs, say \"f\", now give different outcomes. If instead we had left Alice's setting unchanged but shifted Bob's by one degree (in the opposite direction), then again a fraction \"f\" of the pairs of particles turns out to give different outcomes. Finally consider what happens when both shifts are implemented at the same time: the two settings are now exactly two degrees away from being opposite to one another. By the mismatch argument, the chance of a mismatch at two degrees can't be more than twice the chance of a mismatch at one degree: it cannot be more than 2\"f\".\n\nCompare this with the predictions from quantum mechanics for the singlet state. For a small angle , measured in radians, the chance of a different outcome is approximately formula_20 as explained by small-angle approximation. At two times this small angle, the chance of a mismatch is therefore about 4 times larger, since formula_21. But we just argued that it cannot be more than 2 times as large.\n\nThis intuitive formulation is due to David Mermin. The small-angle limit is discussed in Bell's original article, and therefore goes right back to the origin of the Bell inequalities.\n\nGeneralizing Bell's original inequality, John Clauser, Michael Horne, Abner Shimony and R. A. Holt introduced the CHSH inequality, which puts classical limits on the set of four correlations in Alice and Bob's experiment, without any assumption of perfect correlations (or anti-correlations) at equal settings\n\nMaking the special choice formula_23, denoting formula_24, and assuming perfect anti-correlation at equal settings, perfect correlation at opposite settings, therefore formula_25 and formula_26, the CHSH inequality reduces to the original Bell inequality. Nowadays, (1) is also often simply called \"the Bell inequality\", but sometimes more completely \"the Bell-CHSH inequality\".\n\nWith abbreviated notation\n\nthe CHSH inequality can be derived as follows. Each of the four quantities is and each depends on . It follows that for any , one of and is zero, and the other is . From this it follows that\n\nand therefore\n\nAt the heart of this derivation is a simple algebraic inequality concerning four variables, , which take the values only:\n\nThe CHSH inequality is seen to depend only on the following three key features of a local hidden variables theory: (1) realism: alongside of the outcomes of actually performed measurements, the outcomes of potentially performed measurements also exist at the same time; (2) locality, the outcomes of measurements on Alice's particle don't depend on which measurement Bob chooses to perform on the other particle; (3) freedom: Alice and Bob can indeed choose freely which measurements to perform.\n\nThe \"realism\" assumption is actually somewhat idealistic, and Bell's theorem only proves non-locality with respect to variables that only \"exist\" for metaphysical reasons. However, before the discovery of quantum mechanics, both realism and locality were completely uncontroversial features of physical theories.\n\nThe measurements performed by Alice and Bob are spin measurements on electrons. Alice can choose between two detector settings labeled \"a\" and \"a\"′; these settings correspond to measurement of spin along the \"z\" or the \"x\" axis. Bob can choose between two detector settings labeled \"b\" and \"b\"′; these correspond to measurement of spin along the \"z\"′ or \"x\"′ axis, where the coordinate system is rotated 135° relative to the coordinate system. The spin observables are represented by the 2 × 2 self-adjoint matrices:\n\nThese are the Pauli spin matrices normalized so that the corresponding eigenvalues are . As is customary, we use the bra–ket notation to denote the eigenvectors of by\n\nLet formula_33 be the spin singlet state for a pair of electrons discussed in the EPR paradox. This is a specially constructed state described by the following vector in the tensor product\n\nNow let us apply the CHSH formalism to the measurements that can be performed by Alice and Bob.\n\nThe operators formula_36 correspond to Bob's spin measurements along \"x\"′ and \"z\"′. Note that the operators commute with the operators, so we can apply our calculation for the correlation. In this case, we can show that the CHSH inequality fails. In fact, a straightforward calculation shows that \n\nso that\n\nBell's Theorem: If the quantum mechanical formalism is correct, then the system consisting of a pair of entangled electrons cannot satisfy the principle of local realism. Note that is indeed the upper bound for quantum mechanics called Tsirelson's bound. The operators giving this maximal value are always isomorphic to the Pauli matrices.\n\nExperimental tests can determine whether the Bell inequalities required by local realism hold up to the empirical evidence.\n\nActually, most experiments have been performed using polarization of photons rather than spin of electrons (or other spin-half particles). The quantum state of the pair of entangled photons is not the singlet state, and the correspondence between angles and outcomes is different from that in the spin-half set-up. The polarization of a photon is measured in a pair of perpendicular directions. Relative to a given orientation, polarization is either vertical (denoted by V or by +) or horizontal (denoted by H or by -). The photon pairs are generated in the quantum state\n\nwhere formula_40 and formula_41 denotes the state of a single vertically or horizontally polarized photon, respectively (relative to a fixed and common reference direction for both particles).\n\nWhen the polarization of both photons is measured in the same direction, both give the same outcome: perfect correlation. When measured at directions making an angle 45° with one another, the outcomes are completely random (uncorrelated). Measuring at directions at 90° to one another, the two are perfectly anti-correlated. In general, when the polarizers are at an angle to one another, the correlation is . So relative to the correlation function for the singlet state of spin half particles, we have a positive rather than a negative cosine function, and angles are halved: the correlation is periodic with period instead of .\n\nBell's inequalities are tested by \"coincidence counts\" from a Bell test experiment such as the optical one shown in the diagram. Pairs of particles are emitted as a result of a quantum process, analysed with respect to some key property such as polarisation direction, then detected. The setting (orientations) of the analysers are selected by the experimenter.\n\nBell test experiments to date overwhelmingly violate Bell's inequality.\n\nThe \"fair sampling\" problem was faced openly in the 1970s. In early designs of their 1973 experiment, Freedman and Clauser used \"fair sampling\" in the form of the Clauser–Horne–Shimony–Holt (CHSH) hypothesis. However, shortly afterwards Clauser and Horne made the important distinction between inhomogeneous (IBI) and homogeneous (HBI) Bell inequalities. Testing an IBI requires that we compare certain coincidence rates in two separated detectors with the singles rates of the two detectors. Nobody needed to perform the experiment, because singles rates with all detectors in the 1970s were at least ten times all the coincidence rates. So, taking into account this low detector efficiency, the QM prediction actually satisfied the IBI. To arrive at an experimental design in which the QM prediction violates IBI we require detectors whose efficiency exceeds 82.8% for singlet states, but have very low dark rate and short dead and resolving times. This is now within reach.\n\nBecause, at that time, even the best detectors didn't detect a large fraction of all photons, Clauser and Horne recognized that testing Bell's inequality required some extra assumptions. They introduced the \"No Enhancement Hypothesis\" (NEH):\n\nGiven this assumption, there is a Bell inequality between the coincidence rates with polarizers and coincidence rates without polarizers.\n\nThe experiment was performed by Freedman and Clauser, who found that the Bell's inequality was violated. So the no-enhancement hypothesis cannot be true in a local hidden variables model.\n\nWhile early experiments used atomic cascades, later experiments have used parametric down-conversion, following a suggestion by Reid and Walls, giving improved generation and detection properties. As a result, the most recent experiments with photons no longer suffer from the detection loophole (see Bell test experiments). This makes the photon the first experimental system for which all main experimental loopholes have been surmounted, albeit presently only in separate experiments (Giustina et al. (2013), \"Bell violation using entangled photons without the fair-sampling assumption\", Nature 497, 227–230; B.G. Christensen et al. (2013), \"Detection-Loophole-Free Test of Quantum Nonlocality, and Applications\", arXiv:1306.5772).\n\nMost advocates of the hidden-variables idea believe that experiments have ruled out local hidden variables. They are ready to give up locality, explaining the violation of Bell's inequality by means of a non-local hidden variable theory, in which the particles exchange information about their states. This is the basis of the Bohm interpretation of quantum mechanics, which requires that all particles in the universe be able to instantaneously exchange information with all others. A 2007 experiment ruled out a large class of non-Bohmian non-local hidden variable theories.\n\nIf the hidden variables can communicate with each other faster than light, Bell's inequality can easily be violated. Once one particle is measured, it can communicate the necessary correlations to the other particle. Since in relativity the notion of simultaneity is not absolute, this is unattractive. One idea is to replace instantaneous communication with a process that travels backwards in time along the past light cone. This is the idea behind a transactional interpretation of quantum mechanics, which interprets the statistical emergence of a quantum history as a gradual coming to agreement between histories that go both forward and backward in time.\n\nA few advocates of deterministic models have not given up on local hidden variables. For example, Gerard 't Hooft has argued that the superdeterminism loophole cannot be dismissed.\n\nA possible (but not universally accepted) solution is offered by the many worlds theory of quantum mechanics. According to this, not only is collapse of the wave function illusory, but the apparent random branching of possible futures when quantum systems interact with the macroscopic world is also an illusion. Measurement does not lead to a random choice of possible outcome; rather, the only ingredient of quantum mechanics is the unitary evolution of the wave function. All possibilities co-exist forever and the only reality is the quantum mechanical wave function. According to this view, two distant observers both split into superpositions when measuring a spin. The Bell inequality violations are no longer counterintuitive, because it is not clear which copy of the observer B will be seen by observer A when they compare notes. If reality includes all the different outcomes, locality in physical space (not outcome space) places no restrictions on how the split observers can meet up.\n\nThis point underlines the fact that the argument that realism is incompatible with quantum mechanics and locality depends on a particular formalization of the concept of realism. In its weakest form, the assumption underpinning that particular formalization is called counterfactual definiteness. This is the assumption that outcomes of measurements that are not performed are just as real as those of measurements that were performed. Counterfactual definiteness is an uncontroversial property of all classical physical theories prior to quantum theory, due to their determinism. Many worlds interpretations are not only counterfactually indefinite, but are also factually indefinite. The results of all experiments, even ones that have been performed, are not uniquely determined.\n\nIf one chooses to reject counterfactual definiteness, reality has been made smaller, and there is no non-locality problem. On the other hand, one is thereby introducing irreducible or intrinsic randomness into our picture of the world: randomness that cannot be \"explained\" as merely the reflection of our ignorance of underlying, variable, physical quantities. Non-determinism becomes a fundamental property of nature.\n\nAssuming counterfactual definiteness, reality has been enlarged, and there is a non-locality problem. On the other hand, in the many-worlds interpretation of quantum mechanics, reality consists only of a deterministically evolving wave function and non-locality is a non-issue.\n\nThere have also been repeated claims that Bell's arguments are irrelevant because they depend on hidden assumptions that, in fact, are questionable. For example, E. T. Jaynes claimed in 1989 that there are two hidden assumptions in Bell's theorem that could limit its generality. According to him:\n\nHowever, Richard D. Gill has argued that Jaynes misunderstood Bell's analysis. Gill points out that in the same conference volume in which Jaynes argues against Bell, Jaynes confesses to being extremely impressed by a short proof by Steve Gull presented at the same conference, that the singlet correlations could not be reproduced by a computer simulation of a local hidden variables theory. According to Jaynes (writing nearly 30 years after Bell's landmark contributions), it would probably take us another 30 years to fully appreciate Gull's stunning result.\n\nIn 2006 a flurry of activity about implications for determinism arose with the paper: \"The Free Will Theorem\" which stated \"the response of a spin 1 particle to a triple experiment is free—that is to say, is not a function of properties of that part of the universe that is earlier than this response with respect to any given inertial frame.\" This theorem raised awareness of a tension between determinism fully governing an experiment (on the one hand) and Alice and Bob being free to choose any settings they like for their observations (on the other). The philosopher David Hodgson supports this theorem as showing that determinism is \"unscientific\", and that quantum mechanics allows observers (at least in some instances) the freedom to make observations of their choosing, thereby leaving the door open for free will.\n\nThe violations of Bell's inequalities, due to quantum entanglement, provide near definitive demonstrations of something that was already strongly suspected: that quantum physics cannot be represented by any version of the classical picture of physics. Some earlier elements that had seemed incompatible with classical pictures included complementarity and wavefunction collapse. The Bell violations show that no resolution of such issues can avoid the ultimate strangeness of quantum behavior.\n\nThe EPR paper \"pinpointed\" the unusual properties of the \"entangled states\", e.g. the above-mentioned singlet state, which is the foundation for present-day applications of quantum physics, such as quantum cryptography; one application involves the measurement of quantum entanglement as a physical source of bits for Rabin's oblivious transfer protocol. This non-locality was originally supposed to be illusory, because the standard interpretation could easily do away with action-at-a-distance by simply assigning to each particle definite spin-states for all possible spin directions. The EPR argument was: therefore these definite states exist, therefore quantum theory is incomplete, since they do not appear in the theory. Bell's theorem showed that the \"entangledness\" prediction of quantum mechanics has a degree of non-locality that cannot be explained away by any local theory.\n\nWhat is powerful about Bell's theorem is that it doesn't refer to any particular physical theory. It shows that nature violates the most general assumptions behind classical pictures, not just details of some particular models. No combination of local deterministic and local random variables can reproduce the phenomena predicted by quantum mechanics and repeatedly observed in experiments.\n\n\nThe following are intended for general audiences.\n\n\n"}
{"id": "1063799", "url": "https://en.wikipedia.org/wiki?curid=1063799", "title": "Categorical logic", "text": "Categorical logic\n\nCategorical logic is the branch of mathematics in which tools and concepts from category theory are applied to the study of mathematical logic. It is also notable for its connections to theoretical computer science. In broad terms, categorical logic represents both syntax and semantics by a category, and an interpretation by a functor. The categorical framework provides a rich conceptual background for logical and type-theoretic constructions. The subject has been recognisable in these terms since around 1970.\n\nThere are three important themes in the categorical approach to logic:\n\n\n\n\nSeminal papers\n\n\n\n"}
{"id": "11568881", "url": "https://en.wikipedia.org/wiki?curid=11568881", "title": "Complex geodesic", "text": "Complex geodesic\n\nIn mathematics, a complex geodesic is a generalization of the notion of geodesic to complex spaces.\n\nLet (\"X\", || ||) be a complex Banach space and let \"B\" be the open unit ball in \"X\". Let Δ denote the open unit disc in the complex plane C, thought of as the Poincaré disc model for 2-dimensional real/1-dimensional complex hyperbolic geometry. Let the Poincaré metric \"ρ\" on Δ be given by\n\nand denote the corresponding Carathéodory metric on \"B\" by \"d\". Then a holomorphic function \"f\" : Δ → \"B\" is said to be a complex geodesic if\n\nfor all points \"w\" and \"z\" in Δ.\n\n"}
{"id": "1059791", "url": "https://en.wikipedia.org/wiki?curid=1059791", "title": "Computational photography", "text": "Computational photography\n\nComputational photography refers to digital image capture and processing techniques that use digital computation instead of optical processes. Computational photography can improve the capabilities of a camera, or introduce features that were not possible at all with film based photography, or reduce the cost or size of camera elements. Examples of computational photography include in-camera computation of digital panoramas, high-dynamic-range images, and light field cameras. Light field cameras use novel optical elements to capture three dimensional scene information which can then be used to produce 3D images, enhanced depth-of-field, and selective de-focusing (or \"post focus\"). Enhanced depth-of-field reduces the need for mechanical focusing systems. All of these features use computational imaging techniques.\n\nThe definition of computational photography has evolved to cover a number of\nsubject areas in computer graphics, computer vision, and applied\noptics. These areas are given below, organized according to a taxonomy\nproposed by Shree K. Nayar. Within each area is a list of techniques, and for\neach technique one or two representative papers or books are cited.\nDeliberately omitted from the\ntaxonomy are image processing (see also digital image processing)\ntechniques applied to traditionally captured\nimages in order to produce better images. Examples of such techniques are\nimage scaling, dynamic range compression (i.e. tone mapping),\ncolor management, image completion (a.k.a. inpainting or hole filling),\nimage compression, digital watermarking, and artistic image effects.\nAlso omitted are techniques that produce range data,\nvolume data, 3D models, 4D light fields,\n4D, 6D, or 8D BRDFs, or other high-dimensional image-based representations. Epsilon Photography is a sub-field of computational photography.\n\nThis is controlling photographic illumination in a structured fashion, then processing the captured images,\nto create new images. The applications include image-based relighting, image enhancement, image deblurring, geometry/material recovery and so forth.\n\nHigh-dynamic-range imaging uses differently exposed pictures of the same scene to extend dynamic range. Other examples include processing and merging differently illuminated images of the same subject matter (\"lightspace\").\n\nThis is capture of optically coded images, followed by computational decoding to produce new images.\nCoded aperture imaging was mainly applied in astronomy or X-ray imaging to boost the image quality. Instead of a single pin-hole, a pinhole pattern is applied in imaging, and deconvolution is performed to recover the image. In coded exposure imaging, the on/off state of the shutter is coded to modify the kernel of motion blur. In this way motion deblurring becomes a well-conditioned problem. Similarly, in a lens based coded aperture, the aperture can be modified by inserting a broadband mask. Thus, out of focus deblurring becomes a well-conditioned problem. The coded aperture can also improve the quality in light field acquisition using Hadamard transform optics.\n\nCoded aperture patterns can also be designed using color filters, in order to apply different codes at different wavelengths. This allows to increase the amount of light that reaches the camera sensor, compared to binary masks.\n\nComputational imaging is a set of imaging techniques that combine data acquisition and data processing to create the image of an object through indirect means to yield enhanced resolution, additional information such as optical phase or 3D reconstruction. The information is often recorded without using a conventional optical microscope configuration or with limited datasets.\n\nComputational imaging allows to go beyond physical limitations of optical systems, such as numerical aperture\n, or even obliterates the need for optical elements\n\nFor parts of the optical spectrum where imaging elements such as objectives are difficult to manufacture or image sensors cannot be miniaturized, computational imaging provides useful alternatives, in fields such as X-Ray and THz radiations.\n\nAmong common computational imaging techniques are lensless imaging, computational speckle imaging, ptychography and Fourier ptychography.\n\nComputational imaging technique often draws on compressive sensing or phase retrieval techniques, where the angular spectrum of the object is being reconstructed. Other techniques are related to the field of computational imaging, such as digital holography, computer vision and inverse problems such as tomography.\n\nThis is processing of non-optically-coded images to produce new images.\n\nThese are detectors that combine sensing and processing, typically in hardware, like the oversampled binary image sensor. \n\nAlthough computational photography is a currently popular buzzword in computer graphics, many of its\ntechniques first appeared in the computer vision literature,\neither under other names or within papers aimed at 3D shape analysis.\n\nComputational photography, as an art form, has been practiced by capture of differently exposed pictures of the same subject matter, and combining them together. This was the insipiration for the development of the wearable computer in the 1970s and early 1980s. Computational photography was inspired by the work of Charles Wyckoff, and thus computational photography datasets (e.g. differently exposed pictures of the same subject matter that are taken in order to make a single composite image) are sometimes referred to as Wyckoff Sets, in his honor.\n\nEarly work in this area (joint estimation of image projection and exposure value) was undertaken by Mann and Candoccia.\n\nCharles Wyckoff devoted much of his life to creating special kinds of 3-layer photographic films that captured different exposures of the same subject matter. A picture of a nuclear explosion, taken on Wyckoff's film, appeared on the cover of Life Magazine and showed the dynamic range from dark outer areas to inner core.\n\n\n"}
{"id": "364774", "url": "https://en.wikipedia.org/wiki?curid=364774", "title": "Conformal field theory", "text": "Conformal field theory\n\nA conformal field theory (CFT) is a quantum field theory that is invariant under conformal transformations. In two dimensions, there is an infinite-dimensional algebra of local conformal transformations, and conformal field theories can sometimes be exactly solved or classified.\n\nConformal field theory has important applications to condensed matter physics, statistical mechanics, quantum statistical mechanics, and string theory. Statistical and condensed matter systems are indeed often conformally invariant at their thermodynamic or quantum critical points.\n\nWhile it is possible for a quantum field theory to be scale invariant but not conformally-invariant, examples are rare. For this reason, the terms are often used interchangeably in the context of quantum field theory, even though the scale symmetry group is smaller.\n\nIn some particular cases it is possible to prove that scale invariance implies conformal invariance in a quantum field theory, for example in unitary compact conformal field theories in two dimensions.\n\nThere are two versions of 2D CFT: 1) Euclidean, and 2) Lorentzian. The former applies to statistical mechanics, and the latter to quantum field theory. The two versions are related by a Wick rotation.\n\nTwo-dimensional CFTs are (in some way) invariant under an \"infinite-dimensional symmetry group\". For example, consider a CFT on the Riemann sphere. It has the Möbius transformations as the conformal group, which is isomorphic to (the finite-dimensional) PSL(2,C).\n\nHowever, the infinitesimal conformal transformations form an infinite-dimensional algebra, called the Witt algebra, but this infinity of conformal transformations do not have global inverses on ℂ. Only the primary fields (or chiral fields) are invariant with respect to this full infinitesimal conformal group. Its generators are indexed by integers ,\nwhere is the holomorphic part of the non-trace piece of the energy momentum tensor of the theory.\nE.g., for a free scalar field, \n\nIn most conformal field theories, a conformal anomaly, also known as a Weyl anomaly, arises in the quantum theory. This results in the appearance of a nontrivial central charge, and the Witt algebra is extended to the Virasoro algebra.\n\nIn Euclidean CFT, one has both a holomorphic and an antiholomorphic copy of the Virasoro algebra. In Lorentzian CFT, one has a left-moving and a right moving copy of the Virasoro algebra (spacetime is a cylinder, with space being a circle, and time a line).\n\nThis symmetry makes it possible to classify two-dimensional CFTs much more precisely than in higher dimensions. In particular, it is possible to relate the spectrum of primary operators in a theory to the value of the central charge, .\n\nThe Hilbert space of physical states is a unitary module of the Virasoro algebra corresponding to a fixed value of \"c\". Stability requires that the energy spectrum of the Hamiltonian be nonnegative. The modules of interest are the highest weight modules of the Virasoro algebra.\n\nA chiral field is a holomorphic field \"W\"(\"z\") which transforms as\nand\nAnalogously, mutatis mutandis, for an antichiral field. is called the \"conformal weight\" of the chiral field .\n\nFurthermore, it was shown by Alexander Zamolodchikov that there exists a function, C, which decreases monotonically under the renormalization group flow of a two-dimensional quantum field theory, and is equal to the central charge for a two-dimensional conformal field theory. This is known as the Zamolodchikov C-theorem, and tells us that renormalization group flow in two dimensions is irreversible.\n\nFrequently, we are not just interested in the operators, but we are also interested in the vacuum state, or in statistical mechanics, the thermal state. Unless \"c=0\", there can't possibly be any state which leaves the entire infinite dimensional conformal symmetry unbroken. The best we can come up with is a state which is invariant under L, L, L, L, formula_5. This contains the Möbius subgroup. The rest of the conformal group is spontaneously broken.\n\nTwo-dimensional conformal field theories play an important role in statistical mechanics, where they describe critical points of many lattice models.\n\nIn \"d\" > 2 dimensions, the conformal group is locally isomorphic to in Euclidean signature, or in Minkowski space.\n\nHigher-dimensional conformal field theories are prominent in the AdS/CFT correspondence, in which a gravitational theory in anti-de Sitter space (AdS) is equivalent to a conformal field theory on the AdS boundary. Notable examples are \"d\" = 4, N = 4 supersymmetric Yang–Mills theory, which is dual to Type IIB string theory on AdS × S, and \"d\" = 3, \"N\" = 6 super-Chern–Simons theory, which is dual to M-theory on AdS × S. (The prefix \"super\" denotes supersymmetry, \"N\" denotes the degree of extended supersymmetry possessed by the theory, and d the number of space-time dimensions on the boundary.)\n\nConformal symmetry is a symmetry under scale invariance and under the special conformal transformations having the following relations.\n\nwhere formula_11 generates translations, formula_12 generates scaling transformations as a scalar and formula_13 generates the special conformal transformations as a covariant vector under Lorentz transformation.\n\n\n"}
{"id": "51402829", "url": "https://en.wikipedia.org/wiki?curid=51402829", "title": "Dixon's elliptic functions", "text": "Dixon's elliptic functions\n\nIn mathematics, Dixon's elliptic functions, are two doubly periodic meromorphic functions on the complex plane that have regular hexagons as repeating units: the plane can be tiled by regular hexagons in such a way that the restriction of the function to such a hexagon is simply a shift of its restriction to any of the other hexagons. This in no way contradicts the fact that a doubly periodic meromorphic function has a fundamental region that is a parallelogram: the vertices of such a parallelogram (indeed, in this case a rectangle) may be taken to be the centers of four suitably located hexgaons.\n\nThese functions are named after Alfred Cardew Dixon, who introduced them in 1890.\n\nDixon's elliptic functions are denoted sm and cm, and they satisfy the following identities:\n\n"}
{"id": "608289", "url": "https://en.wikipedia.org/wiki?curid=608289", "title": "Eduard Heine", "text": "Eduard Heine\n\nHeinrich Eduard Heine (16 March 1821 – October 1881) was a German mathematician.\n\nHeine became known for results on special functions and in real analysis. In particular, he authored an important treatise on spherical harmonics and Legendre functions (\"Handbuch der Kugelfunctionen\"). He also investigated basic hypergeometric series. He introduced the Mehler–Heine formula.\n\nHeinrich Eduard Heine was born on 16 March 1821 in Berlin, as the eighth child of banker Karl Heine and his wife Henriette Märtens. Eduard was initially home schooled, then studied at the Friedrichswerdersche Gymnasium and Köllnische Gymnasium in Berlin. In 1838, after graduating from gymnasium, he enrolled at the University of Berlin, but transferred to the University of Göttingen to attend the mathematics lectures of Carl Friedrich Gauss and Moritz Stern. In 1840 Heine returned to Berlin, where he studied mathematics under Peter Gustav Lejeune Dirichlet, while also attending classes of Jakob Steiner and Johann Franz Encke. In 1842 he was awarded a Ph.D. by the University of Berlin for a thesis on differential equations submitted with Enno Dirksen and Martin Ohm as advisors. Heine dedicated the doctoral thesis to his professor Gustav Dirichlet. Next he went to the University of Königsberg to participate in the mathematical seminar of Carl Gustav Jacobi, while also following mathematical physics classes of Franz Ernst Neumann. In Königsberg Heine got in contact with fellow students Gustav Kirchhoff and Philipp Ludwig von Seidel.\n\nIn 1844 Heine went for a teaching position at the University of Bonn, passing his habilitation and starting as a privatdozent. He continued his research in mathematics in Bonn and, in 1848, was promoted to extraordinary professor. In 1850 he married Sophie Wolff, the daughter of a Berlin merchant; the couple had five children, four daughters and one son. In 1856 Heine moved as a full professor to the University of Halle, where he remained for the rest of his life. From 1864 to 1865, he served as a rector of the university. In 1875, the University of Göttingen offered Heine a mathematics chair but he decided to reject the offer and remain in Halle. In 1877, at the centenary of Gauss' birth, he was awarded the Gauss Medal for his research. Eduard Heine died on 21 October 1881 in Halle.\n\n\n\n"}
{"id": "7889445", "url": "https://en.wikipedia.org/wiki?curid=7889445", "title": "Epipolar geometry", "text": "Epipolar geometry\n\nEpipolar geometry is the geometry of stereo vision. When two cameras view a 3D scene from two distinct positions, there are a number of geometric relations between the 3D points and their projections onto the 2D images that lead to constraints between the image points. These relations are derived based on the assumption that the cameras can be approximated by the pinhole camera model.\n\nThe figure below depicts two pinhole cameras looking at point X. In real cameras, the image plane is actually behind the focal center, and produces an image that is the symmetry about the focal center of the lens. Here, however, the problem is simplified by placing a \"virtual image plane\" in front of the focal center i.e. optical center of each camera lens to produce an image not transformed by the symmetry. O and O represent the centers of symmetry of the two cameras lenses. X represents the point of interest in both cameras. Points x and x are the projections of point X onto the image planes.\n\nEach camera captures a 2D image of the 3D world. This conversion from 3D to 2D is referred to as a perspective projection and is described by the pinhole camera model. It is common to model this projection operation by rays that emanate from the camera, passing through its focal center. Note that each emanating ray corresponds to a single point in the image.\n\nSince the optical centers of the cameras lenses are distinct, each center projects onto a distinct point into the other camera's image plane. These two image points, denoted by e and e, are called \"epipoles\" or \"epipolar points\". Both epipoles e and e in their respective image planes and both optical centers O and O lie on a single 3D line.\n\nThe line O–X is seen by the left camera as a point because it is directly in line with that camera's lens optical center. However, the right camera sees this line as a line in its image plane. That line (e–x) in the right camera is called an \"epipolar line\". Symmetrically, the line O–X seen by the right camera as a point is seen as epipolar line e–xby the left camera.\n\nAn epipolar line is a function of the position of point X in the 3D space, i.e. as X varies a set of epipolar lines is generated in both images. Since the 3D line\nO–X passes through the optical center of the lens O, the corresponding epipolar line in the right image must pass through the epipole e (and correspondingly for epipolar lines in the left image). All epipolar lines in one image contain the epipolar point of that image. In fact, any line which contains the epipolar point is an epipolar line since it can be derived from some 3D point X.\n\nAs an alternative visualization, consider the points X, O & O that form a plane called the \"epipolar plane\". The epipolar plane intersects each camera's image plane where it forms lines—the epipolar lines. All epipolar planes and epipolar lines intersect the epipole regardless of where X is located.\n\nIf the relative position of the two cameras is known, this leads to two important observations:\n\n\nThe epipolar geometry is simplified if the two camera image planes coincide. In this case, the epipolar lines also coincide (E–P = E–P). Furthermore, the epipolar lines are parallel to the line O–O between the centers of projection, and can in practice be aligned with the horizontal axes of the two images. This means that for each point in one image, its corresponding point in the other image can be found by looking only along a horizontal line. If the cameras cannot be positioned in this way, the image coordinates from the cameras may be transformed to emulate having a common image plane. This process is called image rectification.\n\nIn contrast to the conventional frame camera which uses a two-dimensional CCD, pushbroom camera adopts an array of one-dimensional CCDs to produce long continuous image strip which is called \"image carpet\". Epipolar geometry of this sensor is quite different from that of pinhole projection cameras. First, the epipolar line of pushbroom sensor is not straight, but hyperbola-like curve. Second, epipolar 'curve' pair does not exist.\n\n"}
{"id": "23831652", "url": "https://en.wikipedia.org/wiki?curid=23831652", "title": "Erdős Prize", "text": "Erdős Prize\n\nThe Anna and Lajos Erdős Prize in Mathematics is a prize given by the Israel Mathematical Union to an Israeli mathematician (in any field of mathematics and computer science), \"with preference to candidates up to the age of 40.\" The prize was established by Paul Erdős in 1977 in honor of his parents, and is awarded annually or biannually. The name was changed from \"Erdős Prize\" in 1996, after Erdős's death, to reflect his original wishes.\n\nSource: Israel Mathematical Union\n"}
{"id": "26600432", "url": "https://en.wikipedia.org/wiki?curid=26600432", "title": "Esakia duality", "text": "Esakia duality\n\nIn mathematics, Esakia duality is the dual equivalence between the category of Heyting algebras and the category of Esakia spaces. Esakia duality provides an order-topological representation of Heyting algebras via Esakia spaces.\n\nLet Esa denote the category of Esakia spaces and Esakia morphisms.\n\nLet be a Heyting algebra, denote the set of prime filters of , and denote set-theoretic inclusion on the prime filters of . Also, for each , let }, and let denote the topology on generated by }.\n\nTheorem: is an Esakia space, called the \"Esakia dual\" of . Moreover, is a Heyting algebra isomorphism from onto the Heyting algebra of all clopen up-sets of . Furthermore, each Esakia space is isomorphic in Esa to the Esakia dual of some Heyting algebra.\n\nThis representation of Heyting algebras by means of Esakia spaces is functorial and yields a dual equivalence between the category HA of Heyting algebras and Heyting algebra homomorphisms and the category Esa of Esakia spaces and Esakia morphisms.\n\nTheorem: HA is dually equivalent to Esa.\n\n"}
{"id": "2050276", "url": "https://en.wikipedia.org/wiki?curid=2050276", "title": "Fetch-and-add", "text": "Fetch-and-add\n\nIn computer science, the fetch-and-add CPU instruction (FAA) atomically increments the contents of a memory location by a specified value. \n\nThat is, fetch-and-add performs the operation\nin such a way that if this operation is executed by one process in a concurrent system, no other process will ever see an intermediate result. \n\nFetch-and-add can be used to implement concurrency control structures such as mutex locks and semaphores.\n\nThe motivation for having an atomic fetch-and-add is that operations that appear in programming languages as\n\nare not safe in a concurrent system, where multiple processes or threads are running concurrently (either in a multi-processor system, or preemptively scheduled onto some single-core systems). The reason is that such an operation is actually implemented as multiple machine instructions:\n\n\nWhen one process is doing and another is doing concurrently, there is a race condition. They might both fetch and operate on that, then both store their results with the effect that one overwrites the other and the stored value becomes either or , not as might be expected.\n\nIn uniprocessor systems with no kernel preemption supported, it is sufficient to disable interrupts before accessing a critical section.\nHowever, in multiprocessor systems (even with interrupts disabled) two or more processors could be attempting to access the same memory at the same time. The fetch-and-add instruction allows any processor to atomically increment a value in memory, preventing such multiple processor collisions.\n\nMaurice Herlihy (1991) proved that fetch-and-add has a finite consensus number, in contrast to the compare-and-swap operation. The fetch-and-add operation can solve the wait-free consensus problem for no more than two concurrent processes.\n\nThe fetch-and-add instruction behaves like the following function. Crucially, the entire function is executed atomically: no process can interrupt the function mid-execution and hence see a state that only exists during the execution of the function. This code only serves to help explain the behaviour of fetch-and-add; atomicity requires explicit hardware support and hence can not be implemented as a simple high level function.\n\nTo implement a mutual exclusion lock, we define the operation FetchAndIncrement, which is equivalent to FetchAndAdd with inc=1.\nWith this operation, a mutual exclusion lock can be implemented using the ticket lock algorithm as:\n\nThese routines provide a mutual-exclusion lock when following conditions are met:\n\nAn atomic function appears in the C++11 standard. It is available as a proprietary extension to C in the Itanium ABI specification, and (with the same syntax) in GCC.\n\nIn the x86 architecture, the instruction ADD with the destination operand specifying a memory location is a fetch-and-add instruction that has been there since the 8086 (it just wasn't called that then), and with the LOCK prefix, is atomic across multiple processors. However, it could not return the original value of the memory location (though it returned some flags) until the 486 introduced the XADD instruction.\n\nThe following is a C implementation for the GCC compiler, for both 32 and 64 bit x86 Intel platforms, based on extended asm syntax:\nfetch-and-add was introduced by the Ultracomputer project, which also produced a multiprocessor supporting fetch-and-add and containing custom vlsi switches that were able to combine concurrent memory references (including fetch-and-adds) to prevent them from serializing at the memory module containing the destination operand.\n\n"}
{"id": "378881", "url": "https://en.wikipedia.org/wiki?curid=378881", "title": "Gauss map", "text": "Gauss map\n\nIn differential geometry, the Gauss map (named after Carl F. Gauss) maps a surface in Euclidean space R to the unit sphere \"S\". Namely, given a surface \"X\" lying in R, the Gauss map is a continuous map \"N\": \"X\" → \"S\" such that \"N\"(\"p\") is a unit vector orthogonal to \"X\" at \"p\", namely the normal vector to \"X\" at \"p\".\n\nThe Gauss map can be defined (globally) if and only if the surface is orientable, in which case its degree is half the Euler characteristic. The Gauss map can always be defined locally (i.e. on a small piece of the surface). The Jacobian determinant of the Gauss map is equal to Gaussian curvature, and the differential of the Gauss map is called the shape operator.\n\nGauss first wrote a draft on the topic in 1825 and published in 1827.\n\nThere is also a Gauss map for a link, which computes linking number.\n\nThe Gauss map can be defined for hypersurfaces in R as a map from a hypersurface to the unit sphere \"S\"  ⊆  R.\n\nFor a general oriented \"k\"-submanifold of R the Gauss map can also be defined, and its target space is the \"oriented\" Grassmannian \nformula_1, i.e. the set of all oriented \"k\"-planes in R. In this case a point on the submanifold is mapped to its oriented tangent subspace. One can also map to its oriented \"normal\" subspace; these are equivalent as formula_2 via orthogonal complement.\nIn Euclidean 3-space, this says that an oriented 2-plane is characterized by an oriented 1-line, equivalently a unit normal vector (as formula_3), hence this is consistent with the definition above.\n\nFinally, the notion of Gauss map can be generalized to an oriented submanifold \"X\" of dimension \"k\" in an oriented ambient Riemannian manifold \"M\" of dimension \"n\". In that case, the Gauss map then goes from \"X\" to the set of tangent \"k\"-planes in the tangent bundle \"TM\". The target space for the Gauss map \"N\" is a Grassmann bundle built on the tangent bundle \"TM\". In the case where formula_4, the tangent bundle is trivialized (so the Grassmann bundle becomes a map to the Grassmannian), and we recover the previous definition.\n\nThe area of the image of the Gauss map is called the total curvature and is equivalent to the surface integral of the Gaussian curvature. This is the original interpretation given by Gauss. The Gauss–Bonnet theorem links total curvature of a surface to its topological properties.\n\nThe Gauss map reflects many properties of the surface: when the surface has zero Gaussian curvature, (that is along a parabolic line) the Gauss map will have a fold catastrophe. This fold may contain cusps and these cusps were studied in depth by Thomas Banchoff, Terence Gaffney and Clint McCrory. Both parabolic lines and cusp are stable phenomena and will remain under slight deformations of the surface. Cusps occur when:\nThere are two types of cusp: \"elliptic cusp\" and \"hyperbolic cusps\".\n\n\n"}
{"id": "51246780", "url": "https://en.wikipedia.org/wiki?curid=51246780", "title": "Genetic method", "text": "Genetic method\n\nThe genetic method is a method of teaching mathematics coined by Otto Toeplitz in 1927. As an alternative to the axiomatic system, the method suggests using history of mathematics to deliver excitement and motivation and engage the class.\n\nOtto Toeplitz, a research mathematician in the area of functional analysis, introduced the method in his manuscript \"The problem of calculus courses at universities and their demarcation against calculus courses at high schools\" in 1927. A part of this manuscript was published in a book in 1949, after Toeplitz's death.\n\nToeplitz's method was not completely new at the time. In his 1895 talk given at the public meeting of the royal society of sciences in Goettingen, \"On the arithmetization of mathematics\", the famous German mathematician Felix Klein suggested the idea \"that on a small scale, a learner naturally and always has to repeat the same developments that the sciences went through on a large scale\".\n\nIn addition, the genetic method was occasionally applied in Gerhard Kowalewski's book from 1909, \"The classical problems of the analysis of the infinite\".\n\nIn 1962 the mathematics education in the US met a situation similar to that of Toeplitz in 1926 in Germany, in connection with the introduction of \"New Mathematics\". Shortly after the Sputnik crisis, a \"New Mathematics\" reform was introduced to improve the level of mathematics education in the US, so that the threat of Soviet engineers, assumed to be well educated in mathematics, could be met. To prepare students for advanced mathematics, the curriculum shifted to focus on abstraction and rigor. One of the more reasonable responses to \"New Mathematics\" was a collective statement by Lipman Bers, Morris Kline, George Pólya, and Max Schiffer, cosigned by 61 others, that was published in \"The Mathematics Teacher\" and \"The American Mathematical Monthly\" in 1962. In this letter, the undersigned called for the use of the genetic method:\n\nAlso, in the 1980s, departments of mathematics in the US were facing criticism from other departments, especially departments in engineering, that they were failing too many of their students, and that those students that were certified as knowing calculus in fact had no idea how to apply its concepts in other classes. This led to the \"Calculus Reform\" in the US.\n\nOtto Toeplitz had alleged that only 5% of the class can be reached by the traditional axiomatic approaches. To engage 45% of the students, he suggested to expose the students to the history of mathematics. The history of mathematics would give students an idea of the challenges and the elements of mathematics research process and applications. Furthermore, Toeplitz claimed that 50% of the students in universities were not 'reachable' and were 'unfit' for university education. The classification is illustrated in the picture.\n\nThere are two recognised variants of the genetic method.\n\nA direct genetic method displays the history of the development of mathematical concepts as a narrative. The history is taught step by step, exposing the class to each step that lead to the development of a mathematical concept. It is suggested to include confusions as a part of this method to demonstrate that mistakes and unsuccessful hypotheses are a part of the mathematics research process during the entire duration of mathematics history.\n\nThe indirect genetic method includes the same information as the direct one, but the confusions and problems throughout the development of each mathematical concept are analysed and the motivations for the correct resolution are discussed. More focus is given to the diagnosis of problems to allow students to diagnose problems in the current state of art in mathematics to form a part of their critical analysis skills in the field.\n\n\n\n"}
{"id": "19363014", "url": "https://en.wikipedia.org/wiki?curid=19363014", "title": "Geometric calculus", "text": "Geometric calculus\n\nIn mathematics, geometric calculus extends the geometric algebra to include differentiation and integration. The formalism is powerful and can be shown to encompass other mathematical theories including differential geometry and differential forms.\n\nWith a geometric algebra given, let \"a\" and \"b\" be vectors and let \"F(a)\" be a multivector-valued function. The directional derivative of \"F(a)\" along \"b\" is defined as\n\nprovided that the limit exists, where the limit is taken for scalar \"ε\". This is similar to the usual definition of a directional derivative but extends it to functions that are not necessarily scalar-valued.\n\nNext, choose a set of basis vectors formula_2 and consider the operators, noted formula_3, that perform directional derivatives in the directions of formula_4:\n\nThen, using the Einstein summation notation, consider the operator:\nwhich means\nor, more verbosely:\n\nIt can be shown that this operator is independent of the choice of frame, and can thus be used to define the \"geometric derivative\":\n\nThis is similar to the usual definition of the gradient, but it, too, extends to functions that are not necessarily scalar-valued.\n\nIt can be shown that the directional derivative is linear regarding its direction, that is:\nFrom this follows that the directional derivative is the inner product of its direction by the geometric derivative. All needs to be observed is that the direction formula_11 can be written formula_12, so that:\nFor this reason, formula_14 is often noted formula_15.\n\nThe standard order of operations for the geometric derivative is that it acts only on the function closest to its immediate right. Given two functions \"F\" and \"G\", then for example we have\n\nAlthough the partial derivative exhibits a product rule, the geometric derivative only partially inherits this property. Consider two functions \"F\" and \"G\":\n\nSince the geometric product is not commutative with formula_18 in general, we cannot proceed further without new notation. A solution is to adopt the \"overdot notation\", in which the scope of a geometric derivative with an overdot is the multivector-valued function sharing the same overdot. In this case, if we define\n\nthen the product rule for the geometric derivative is\n\nLet \"F\" be an \"r\"-grade multivector. Then we can define an additional pair of operators, the interior and exterior derivatives,\n\nIn particular, if \"F\" is grade 1 (vector-valued function), then we can write\n\nand identify the divergence and curl as\n\nNote, however, that these two operators are considerably weaker than the geometric derivative counterpart for several reasons. Neither the interior derivative operator nor the exterior derivative operator is invertible.\n\nLet formula_26 be a set of basis vectors that span an \"n\"-dimensional vector space. From geometric algebra, we interpret the pseudoscalar formula_27 to be the signed volume of the \"n\"-parallelotope subtended by these basis vectors. If the basis vectors are orthonormal, then this is the unit pseudoscalar.\n\nMore generally, we may restrict ourselves to a subset of \"k\" of the basis vectors, where formula_28, to treat the length, area, or other general \"k\"-volume of a subspace in the overall \"n\"-dimensional vector space. We denote these selected basis vectors by formula_29. A general \"k\"-volume of the \"k\"-parallelotope subtended by these basis vectors is the grade \"k\" multivector formula_30.\n\nEven more generally, we may consider a new set of vectors formula_31 proportional to the \"k\" basis vectors, where each of the formula_32 is a component that scales one of the basis vectors. We are free to choose components as infinitesimally small as we wish as long as they remain nonzero. Since the outer product of these terms can be interpreted as a \"k\"-volume, a natural way to define a measure is\n\nThe measure is therefore always proportional to the unit pseudoscalar of a \"k\"-dimensional subspace of the vector space. Compare the Riemannian volume form in the theory of differential forms. The integral is taken with respect to this measure:\n\nMore formally, consider some directed volume \"V\" of the subspace. We may divide this volume into a sum of simplices. Let formula_35 be the coordinates of the vertices. At each vertex we assign a measure formula_36 as the average measure of the simplices sharing the vertex. Then the integral of \"F(x)\" with respect to \"U(x)\" over this volume is obtained in the limit of finer partitioning of the volume into smaller simplices:\n\nThe reason for defining the geometric derivative and integral as above is that they allow a strong generalization of Stokes' theorem. Let formula_38 be a multivector-valued function of \"r\"-grade input \"A\" and general position \"x\", linear in its first argument. Then the fundamental theorem of geometric calculus relates the integral of a derivative over the volume \"V\" to the integral over its boundary:\n\nAs an example, let formula_39 for a vector-valued function \"F(x)\" and a (\"n\"−1)-grade multivector \"A\". We find that\n\nLikewise,\n\nThus we recover the divergence theorem,\n\nA sufficiently smooth \"k\"-surface in an \"n\"-dimensional space is deemed a manifold. To each point on the manifold, we may attach a \"k\"-blade \"B\" that is tangent to the manifold. Locally, \"B\" acts as a pseudoscalar of the \"k\"-dimensional space. This blade defines a projection of vectors onto the manifold:\n\nJust as the geometric derivative formula_44 is defined over the entire \"n\"-dimensional space, we may wish to define an \"intrinsic derivative\" formula_45, locally defined on the manifold:\n\nIf \"a\" is a vector tangent to the manifold, then indeed both the geometric derivative and intrinsic derivative give the same directional derivative:\n\nAlthough this operation is perfectly valid, it is not always useful because formula_49 itself is not necessarily on the manifold. Therefore, we define the \"covariant derivative\" to be the forced projection of the intrinsic derivative back onto the manifold:\n\nSince any general multivector can be expressed as a sum of a projection and a rejection, in this case\n\nwe introduce a new function, the shape tensor formula_52, which satisfies\n\nwhere formula_54 is the commutator product. In a local coordinate basis formula_2 spanning the tangent surface, the shape tensor is given by\n\nImportantly, on a general manifold, the covariant derivative does not commute. In particular, the commutator is related to the shape tensor by\n\nClearly the term formula_58 is of interest. However it, like the intrinsic derivative, is not necessarily on the manifold. Therefore, we can define the Riemann tensor to be the projection back onto the manifold:\n\nLastly, if \"F\" is of grade \"r\", then we can define interior and exterior covariant derivatives as\n\nand likewise for the intrinsic derivative.\n\nOn a manifold, locally we may assign a tangent surface spanned by a set of basis vectors formula_2. We can associate the components of a metric tensor, the Christoffel symbols, and the Riemann tensor as follows:\n\nThese relations embed the theory of differential geometry within geometric calculus.\n\nIn a local coordinate system (\"x\", ..., \"x\"), the coordinate differentials \"dx\", ..., \"dx\" form a basic set of one-forms within the coordinate chart. Given a multi-index with for , we can define a \"k\"-form \n\nWe can alternatively introduce a \"k\"-grade multivector \"A\" as\n\nand a measure\n\nApart from a subtle difference in meaning for the exterior product with respect to differential forms versus the exterior product with respect to vectors (in the former the \"increments\" are covectors, whereas in the latter they represent scalars), we see the correspondences of the differential form\n\nits derivative\n\nand its Hodge dual\n\nembed the theory of differential forms within geometric calculus.\n\nFollowing is a diagram summarizing the history of geometric calculus.\n"}
{"id": "20335837", "url": "https://en.wikipedia.org/wiki?curid=20335837", "title": "Graph dynamical system", "text": "Graph dynamical system\n\nIn mathematics, the concept of graph dynamical systems can be used to capture a wide range of processes taking place on graphs or networks. A major theme in the mathematical and computational analysis of GDSs is to relate their structural properties (e.g. the network connectivity) and the global dynamics that result.\n\nThe work on GDSs considers finite graphs and finite state spaces. As such, the research typically involves techniques from, e.g., graph theory, combinatorics, algebra, and dynamical systems rather than differential geometry. In principle, one could define and study GDSs over an infinite graph (e.g. cellular automata or probabilistic cellular automata over formula_1 or interacting particle systems when some randomness is included), as well as GDSs with infinite state space (e.g. formula_2 as in coupled map lattices); see, for example, Wu. In the following, everything is implicitly assumed to be finite unless stated otherwise.\n\nA graph dynamical system is constructed from the following components:\nThe \"phase space\" associated to a dynamical system with map \"F\": \"K → K\" is the finite directed graph with vertex set \"K\" and directed edges (\"x\", \"F\"(\"x\")). The structure of the phase space is governed by the properties of the graph \"Y\", the vertex functions (\"f\")\"\", and the update scheme. The research in this area seeks to infer phase space properties based on the structure of the system constituents. The analysis has a local-to-global character.\n\nIf, for example, the update scheme consists of applying the vertex functions synchronously one obtains the class of \"generalized cellular automata\" (CA). In this case, the global map \"F\": \"K → K\" is given by\n\nformula_3\n\nThis class is referred to as generalized cellular automata since the classical or standard cellular automata are typically defined and studied over regular graphs or grids, and the vertex functions are typically assumed to be identical.\n\nExample: Let \"Y\" be the circle graph on vertices {1,2,3,4} with edges {1,2}, {2,3}, {3,4} and {1,4}, denoted Circ. Let \"K\" = {0,1} be the state space for each vertex and use the function nor : \"K\" → \"K\" defined by nor(\"x,y,z\") = (1 + \"x\")(1 + \"y\")(1 + \"z\") with arithmetic modulo 2 for all vertex functions. Then for example the system state (0,1,0,0) is mapped to (0, 0, 0, 1) using a synchronous update. All the transitions are shown in the phase space below.\n\nIf the vertex functions are applied asynchronously in the sequence specified by a word \"w\" = (\"w\", \"w\", ... , \"w\") or permutation formula_4 = ( formula_5, formula_6) of \"v\"[\"Y\"] one obtains the class of \"Sequential dynamical systems\" (SDS). In this case it is convenient to introduce the \"Y\"-local maps \"F\" constructed from the vertex functions by\n\nThe SDS map \"F\" = [\"F\" , \"w\"] : \"K\" → \"K\" is the function composition\n\nIf the update sequence is a permutation one frequently speaks of a \"permutation SDS\" to emphasize this point.\n\nExample: Let \"Y\" be the circle graph on vertices {1,2,3,4} with edges {1,2}, {2,3}, {3,4} and {1,4}, denoted Circ. Let \"K\"={0,1} be the state space for each vertex and use the function nor : \"K\" → \"K\" defined by nor(\"x, y, z\") = (1 + \"x\")(1 + \"y\")(1 + \"z\") with arithmetic modulo 2 for all vertex functions. Using the update sequence (1,2,3,4) then the system state (0, 1, 0, 0) is mapped to (0, 0, 1, 0). All the system state transitions for this sequential dynamical system are shown in the phase space below.\n\nFrom, e.g., the point of view of applications it is interesting to consider the case where one or more of the components of a GDS contains stochastic elements. Motivating applications could include processes that are not fully understood (e.g. dynamics within a cell) and where certain aspects for all practical purposes seem to behave according to some probability distribution. There are also applications governed by deterministic principles whose description is so complex or unwieldy that it makes sense to consider probabilistic approximations.\n\nEvery element of a graph dynamical system can be made stochastic in several ways. For example, in a sequential dynamical system the update sequence can be made stochastic. At each iteration step one may choose the update sequence \"w\" at random from a given distribution of update sequences with corresponding probabilities. The matching probability space of update sequences induces a probability space of SDS maps. A natural object to study in this regard is the Markov chain on state space induced by this collection of SDS maps. This case is referred to as \"update sequence stochastic GDS\" and is motivated by, e.g., processes where \"events\" occur at random according to certain rates (e.g. chemical reactions), synchronization in parallel computation/discrete event simulations, and in computational paradigms described later.\n\nThis specific example with stochastic update sequence illustrates two general facts for such systems: when passing to a stochastic graph dynamical system one is generally led to (1) a study of Markov chains (with specific structure governed by the constituents of the GDS), and (2) the resulting Markov chains tend to be large having an exponential number of states. A central goal in the study of stochastic GDS is to be able to derive reduced models.\n\nOne may also consider the case where the vertex functions are stochastic, i.e., \"function stochastic GDS\". For example, Random Boolean networks are examples of function stochastic GDS using a synchronous update scheme and where the state space is \"K\" = {0, 1}. Finite probabilistic cellular automata (PCA) is another example of function stochastic GDS. In principle the class of Interacting particle systems (IPS) covers finite and infinite PCA, but in practice the work on IPS is largely concerned with the infinite case since this allows one to introduce more interesting topologies on state space.\n\nGraph dynamical systems constitute a natural framework for capturing distributed systems such as biological networks and epidemics over social networks, many of which are frequently referred to as complex systems.\n\n\n\n"}
{"id": "35543850", "url": "https://en.wikipedia.org/wiki?curid=35543850", "title": "Gray's conjecture", "text": "Gray's conjecture\n\nIn mathematics, Gray's conjecture is a conjecture made by Brayton Gray in 1984 about maps between loop spaces of spheres. It was later proved by John Harper and Stephen Theriault.\n"}
{"id": "632762", "url": "https://en.wikipedia.org/wiki?curid=632762", "title": "Haboush's theorem", "text": "Haboush's theorem\n\nIn mathematics Haboush's theorem, often still referred to as the Mumford conjecture, states that for any semisimple algebraic group \"G\" over a field \"K\", and for any linear representation ρ of \"G\" on a \"K\"-vector space \"V\", given \"v\" ≠ 0 in \"V\" that is fixed by the action of \"G\", there is a \"G\"-invariant polynomial \"F\" on \"V\", without constant term, such that\n\nThe polynomial can be taken to be homogeneous, in other words an element of a symmetric power of the dual of \"V\", and if the characteristic is \"p\">0 the degree of the polynomial can be taken to be a power of \"p\". \nWhen \"K\" has characteristic 0 this was well known; in fact Weyl's theorem on the complete reducibility of the representations of \"G\" implies that \"F\" can even be taken to be linear. Mumford's conjecture about the extension to prime characteristic \"p\" was proved by W. J. , about a decade after the problem had been posed by David Mumford, in the introduction to the first edition of his book \"Geometric Invariant Theory\".\n\nHaboush's theorem can be used to generalize results of geometric invariant theory from characteristic 0, where they were already known, to characteristic \"p\">0. In particular Nagata's earlier results together with Haboush's theorem show that if a reductive group (over an algebraically closed field) acts on a finitely generated algebra then the fixed subalgebra is also finitely generated.\n\nHaboush's theorem implies that if \"G\" is a reductive algebraic group acting regularly on an affine algebraic variety, then disjoint closed invariant sets \"X\" and \"Y\" can be separated by an invariant function \"f\" (this means that \"f\" is 0 on \"X\" and 1 on \"Y\").\n\nC.S. Seshadri (1977) extended Haboush's theorem to reductive groups over schemes.\n\nIt follows from the work of , Haboush, and Popov that the following conditions are equivalent for an affine algebraic group \"G\" over a field \"K\":\n\nThe theorem is proved in several steps as follows:\n\n"}
{"id": "1651213", "url": "https://en.wikipedia.org/wiki?curid=1651213", "title": "Henk Barendregt", "text": "Henk Barendregt\n\nHendrik Pieter (Henk) Barendregt (born 18 December 1947, Amsterdam) is a Dutch logician, known for his work in lambda calculus and type theory.\n\nBarendregt studied mathematical logic at Utrecht University, obtaining his master's degree in 1968 and his PhD in 1971, both \"cum laude\", under Dirk van Dalen and Georg Kreisel. After a postdoctoral position at Stanford University, he taught at Utrecht University.\n\nSince 1986, Barendregt has taught at Radboud University Nijmegen, where he now holds the Chair of Foundations of Mathematics and Computer Science. His research group works on Constructive Interactive Mathematics. He is also Adjunct Professor at Carnegie Mellon University, Pittsburgh, USA. He has been a visiting scholar at Darmstadt, ETH Zürich, Siena, and Kyoto.\n\nIn 1997 Barendregt was elected member of the Royal Netherlands Academy of Arts and Sciences. On 6 February 2003 Barendregt was awarded the Spinozapremie for 2002, the highest scientific award in the Netherlands. In 2002 he was knighted in the Orde van de Nederlandse Leeuw.\n\nBarendregt received an honorary doctorate from Heriot-Watt University in 2015.\n\n\n"}
{"id": "51478216", "url": "https://en.wikipedia.org/wiki?curid=51478216", "title": "Increased limit factor", "text": "Increased limit factor\n\nIncreased limit factors or ILFs are multiplicative factors that are applied to premiums for \"basic\" limits of coverage to determine premiums for higher limits of coverage. They are commonly used in casualty insurance pricing.\n\nOften, limited data is available to determine appropriate charges for high limits of insurance. In order to price policies with high limits of insurance adequately, actuaries may first determine a \"basic limit\" premium and then apply increased limits factors. The basic limit is a lower limit of liability under which there is a more credible amount of data.\n\nFor example, basic limit loss costs or rates may be calculated for many territories and classes of business. At a relatively low limit of liability, such as $100,000, there may be a high volume of data that can be used to derive those rates. For higher limits, there may be a credible volume of data at the countrywide level but not much data available for individual territories or classes. Increased limit factors can be derived at the countrywide level (or some other broad grouping) and then applied to the basic limit rates to arrive at rates for higher limits of liability.\n\nAn increased limit factor (ILF) at limit L relative to basic limit B can be defined as\n\nformula_1\n\nwhere ALAE is the allocated loss adjustment expense provision, ULAE is the unallocated loss adjustment expense provision, and RL is the risk load provision.\n\nAn indemnity-only ILF can be expressed as\n\nformula_2\n\nOften, frequency is assumed to be independent of the policy limit, in which case the formula can be simplified to\n\nformula_3\nThe expected severity at each limit is often referred to as \"limited average severity,\" or LAS.\n\nIn the United States, many insurers use ILFs published by the Insurance Services Office, a division of Verisk.\n\n\n"}
{"id": "31429542", "url": "https://en.wikipedia.org/wiki?curid=31429542", "title": "Induced character", "text": "Induced character\n\nIn mathematics, an induced character is the character of the representation \"V\" of a finite group \"G\" induced from a representation \"W\" of a subgroup \"H\" ≤ \"G\". More generally, there is also a notion of induction formula_1 of a class function \"f\" on \"H\" given by the formula\n\nIf \"f\" is a character of the representation \"W\" of \"H\", then this formula for formula_1 calculates the character of the induced representation \"V\" of \"G\".\n\nThe basic result on induced characters is Brauer's theorem on induced characters. It states that every irreducible character on \"G\" is a linear combination with integer coefficients of characters induced from elementary subgroups.\n"}
{"id": "39382", "url": "https://en.wikipedia.org/wiki?curid=39382", "title": "Infimum and supremum", "text": "Infimum and supremum\n\nIn mathematics, the infimum (abbreviated inf; plural infima) of a subset \"S\" of a partially ordered set \"T\" is the greatest element in \"T\" that is less than or equal to all elements of \"S\", if such an element exists. Consequently, the term \"greatest lower bound\" (abbreviated as \"GLB\") is also commonly used.\n\nThe supremum (abbreviated sup; plural suprema) of a subset \"S\" of a partially ordered set \"T\" is the least element in \"T\" that is greater than or equal to all elements of \"S\", if such an element exists. Consequently, the supremum is also referred to as the \"least upper bound\" (or \"LUB\").\n\nThe infimum is in a precise sense dual to the concept of a supremum. Infima and suprema of real numbers are common special cases that are important in analysis, and especially in Lebesgue integration. However, the general definitions remain valid in the more abstract setting of order theory where arbitrary partially ordered sets are considered.\n\nThe concepts of infimum and supremum are similar to minimum and maximum, but are more useful in analysis because they better characterize special sets which may have \"no minimum or maximum\". For instance, the positive real numbers ℝ (not including 0) does not have a minimum, because any given element of ℝ could simply be divided in half resulting in a smaller number that is still in ℝ. There is, however, exactly one infimum of the positive real numbers: 0, which is smaller than all the positive real numbers and greater than any other real number which could be used as a lower bound.\n\nA \"lower bound\" of a subset \"S\" of a partially ordered set (\"P\",≤) is an element \"a\" of \"P\" such that\nA lower bound \"a\" of \"S\" is called an \"infimum\" (or \"greatest lower bound\", or \"meet\") of \"S\" if\n\nSimilarly, an \"upper bound\" of a subset \"S\" of a partially ordered set (\"P\",≤) is an element \"b\" of \"P\" such that\nAn upper bound \"b\" of \"S\" is called a \"supremum\" (or \"least upper bound\", or \"join\") of \"S\" if\n\nInfima and suprema do not necessarily exist. Existence of an infimum of a subset \"S\" of \"P\" can fail if \"S\" has no lower bound at all, or if the set of lower bounds does not contain a maximal element. However, if an infimum or supremum does exist, it is unique.\n\nConsequently, partially ordered sets for which certain infima are known to exist become especially interesting. For instance, a lattice is a partially ordered set in which all subsets have both a supremum and an infimum, and a complete lattice is a partially ordered set in which subsets have both a supremum and an infimum. More information on the various classes of partially ordered sets that arise from such considerations are found in the article on completeness properties.\n\nIf the supremum of a subset \"S\" exists, it is unique. If \"S\" contains a greatest element, then that element is the supremum; otherwise, the supremum does not belong to \"S\" (or does not exist). Likewise, if the infimum exists, it is unique. If \"S\" contains a least element, then that element is the infimum; otherwise, the infimum does not belong to \"S\" (or does not exist).\n\nThe infimum of a subset \"S\" of a partially ordered set \"P\", assuming it exists, does not necessarily belong to \"S\". If it does, it is a minimum or least element of \"S\". Similarly, if the supremum of \"S\" belongs to \"S\", it is a maximum or greatest element of \"S\".\n\nFor example, consider the set of negative real numbers (excluding zero). This set has no greatest element, since for every element of the set, there is another, larger, element. For instance, for any negative real number \"x\", there is another negative real number formula_1, which is greater. On the other hand, every real number greater than or equal to zero is certainly an upper bound on this set. Hence, 0 is the least upper bound of the negative reals, so the supremum is 0. This set has a supremum but no greatest element.\n\nHowever, the definition of maximal and minimal elements is more general. In particular, a set can have many maximal and minimal elements, whereas infima and suprema are unique.\n\nFinally, a partially ordered set may have many minimal upper bounds without having a least upper bound. Minimal upper bounds are those upper bounds for which there is no strictly smaller element that also is an upper bound. This does not say that each minimal upper bound is smaller than all other upper bounds, it merely is not greater. The distinction between \"minimal\" and \"least\" is only possible when the given order is not a total one. In a totally ordered set, like the real numbers, the concepts are the same.\n\nAs an example, let \"S\" be the set of all finite subsets of natural numbers and consider the partially ordered set obtained by taking all sets from \"S\" together with the set of integers ℤ and the set of positive real numbers ℝ, ordered by subset inclusion as above. Then clearly both ℤ and ℝ are greater than all finite sets of natural numbers. Yet, neither is ℝ smaller than ℤ nor is the converse true: both sets are minimal upper bounds but none is a supremum.\n\nThe \"least-upper-bound property\" is an example of the aforementioned completeness properties which is typical for the set of real numbers. This property is sometimes called \"Dedekind completeness\".\n\nIf an ordered set \"S\" has the property that every nonempty subset of \"S\" having an upper bound also has a least upper bound, then \"S\" is said to have the least-upper-bound property. As noted above, the set ℝ of all real numbers has the least-upper-bound property. Similarly, the set ℤ of integers has the least-upper-bound property; if \"S\" is a nonempty subset of ℤ and there is some number \"n\" such that every element \"s\" of \"S\" is less than or equal to \"n\", then there is a least upper bound \"u\" for \"S\", an integer that is an upper bound for \"S\" and is less than or equal to every other upper bound for \"S\". A well-ordered set also has the least-upper-bound property, and the empty subset has also a least upper bound: the minimum of the whole set.\n\nAn example of a set that \"lacks\" the least-upper-bound property is ℚ, the set of rational numbers. Let \"S\" be the set of all rational numbers \"q\" such that \"q\" < 2. Then \"S\" has an upper bound (1000, for example, or 6) but no least upper bound in ℚ: If we suppose \"p\" ∈ ℚ is the least upper bound, a contradiction is immediately deduced because between any two reals \"x\" and \"y\" (including and \"p\") there exists some rational \"p\"′, which itself would have to be the least upper bound (if \"p\" > ) or a member of \"S\" greater than \"p\" (if \"p\" < ). Another example is the hyperreals; there is no least upper bound of the set of positive infinitesimals.\n\nThere is a corresponding 'greatest-lower-bound property'; an ordered set possesses the greatest-lower-bound property if and only if it also possesses the least-upper-bound property; the least-upper-bound of the set of lower bounds of a set is the greatest-lower-bound, and the greatest-lower-bound of the set of upper bounds of a set is the least-upper-bound of the set.\n\nIf in a partially ordered set \"P\" every bounded subset has a supremum, this applies also, for any set \"X\", in the function space containing all functions from \"X\" to \"P\", where \"f\" ≤ \"g\" if and only if \"f\"(\"x\") ≤ \"g\"(\"x\") for all \"x\" in \"X\". For example, it applies for real functions, and, since these can be considered special cases of functions, for real \"n\"-tuples and sequences of real numbers.\n\nThe least-upper-bound property is an indicator of the suprema.\n\nIn analysis, infima and suprema of subsets \"S\" of the real numbers are particularly important. For instance, the negative real numbers do not have a greatest element, and their supremum is 0 (which is not a negative real number).\nThe completeness of the real numbers implies (and is equivalent to) that any bounded nonempty subset \"S\" of the real numbers has an infimum and a supremum. If \"S\" is not bounded below, one often formally writes inf(\"S\") = −∞. If \"S\" is empty, one writes inf(\"S\") = +∞.\n\nLet \"A\", \"B\" ⊆ ℝ and suppose the infima and suprema of these sets exist. Define \"λA\" = { \"λx\" : \"x\" ∈ \"A\" }, \"A\" + \"B\" = { \"x\" + \"y\" : \"x\" ∈ \"A\", \"y\" ∈ \"B\" }, and \"AB\" = { \"xy\" : \"x\" ∈ \"A\", \"y\" ∈ B }.\n\nIf one denotes by \"P\" the partially-ordered set \"P\" with the opposite order relation, i.e.\nthen infimum of a subset \"S\" in \"P\" equals the supremum of \"S\" in \"P\" and vice versa.\n\nFor subsets of the real numbers, another kind of duality holds: inf \"S\" = −sup(−\"S\"), where −\"S\" = { −\"s\" | \"s\" ∈ \"S\" }.\n\n\nIn the last example, the supremum of a set of rationals is irrational, which means that the rationals are incomplete.\n\nOne basic property of the supremum is\n\nfor any functionals \"f\" and \"g\".\n\nThe supremum of a subset \"S\" of (ℕ,|) where | denotes \"divides\", is the lowest common multiple of the elements of \"S\".\n\nThe supremum of a subset \"S\" of (\"P\",⊆), where \"P\" is the power set of some set, is the supremum with respect to ⊆ (subset) of a subset \"S\" of \"P\" is the union of the elements of \"S\".\n\n\n"}
{"id": "1830232", "url": "https://en.wikipedia.org/wiki?curid=1830232", "title": "Iterative closest point", "text": "Iterative closest point\n\nIterative closest point (ICP) is an algorithm employed to minimize the difference between two clouds of points. ICP is often used to reconstruct 2D or 3D surfaces from different scans, to localize robots and achieve optimal path planning (especially when wheel odometry is unreliable due to slippery terrain), to co-register bone models, etc.\n\nIn the Iterative Closest Point or, in some sources, the Iterative Corresponding Point, one point cloud (vertex cloud), the \"reference\", or \"target\", is kept fixed, while the other one, the \"source\", is transformed to best match the reference. The algorithm iteratively revises the transformation (combination of translation and rotation) needed to minimize an error metric, usually a distance from the source to the reference point cloud, such as the sum of squared differences between the coordinates of the matched pairs. ICP is one of the widely used algorithms in aligning three dimensional models given an initial guess of the rigid body transformation required.\nThe ICP algorithm was first introduced by Chen and Medioni, and Besl and McKay.\n\nThe Iterative Closest Point algorithm contrasts with the Kabsch algorithm and other solutions to the orthogonal Procrustes problem in that the Kabsch algorithm requires correspondence between point sets as an input where-as Iterative Closest Point treats correspondence as a variable to be estimated.\n\nInputs: reference and source point clouds, initial estimation of the transformation to align the source to the reference (optional), criteria for stopping the iterations.\n\nOutput: refined transformation.\n\nEssentially, the algorithm steps are:\n\n\nZhang proposes a modified K-D tree algorithm for efficient closest point computation. In this work a statistical method based on the distance distribution is used to deal with outliers, occlusion, appearance, and disappearance, which enables subset-subset matching.\n\nThere exist many ICP variants, from which point-to-point and point-to-plane are the most popular. The latter usually performs better in structured environments.\n\n\n"}
{"id": "48785760", "url": "https://en.wikipedia.org/wiki?curid=48785760", "title": "K-connectivity certificate", "text": "K-connectivity certificate\n\nIn graph theory, for a k-connected graph G = (V, E), a subset of edges formula_1 is considered a certificate for the k-connectivity of the graph G if and only if the subgraph G' = (V, E') is k-connected.\n\nFor a k-connected graph with \"n\" vertices, there always exists a \"k\"-connectivity certificate with at most k(n-1) edges. K-connectivity certificates are considered sparse if they contain \"O\"(\"kn\") edges. In this figure, the graph on the right is also a sparse certificate for the graph \"G\" on the left.\n\nScan-First is an algorithm for the parallel construction of k-connectivity certificates for graphs. It was introduced in the paper Scan-First Search and Sparse Certificates: An Improved Parallel Algorithm for K-Vertex Connectivity by Joseph Cheriyan, Ming-Yang Kao, and Ramakrishna Thurimella. The Scan-First Search algorithm improves the running time of building a sparse certificate for k-connectivity using the parallel computation model.\n\nWe can find a sparse certificate for k-connectivity by iteratively running scan-first search k times on sub-graphs of our input graph. Our input is a graph G = (V, E) and a root vertex r. For each iteration of scan-first search, we first compute a spanning tree T of our input graph G, and assign a pre-order numbering to all the vertices, which we will use as our scanning order. From our root r, we first scan r, which involves marking all its neighboring vertices.\n\nGiven a connected undirected graph and a specified vertex, a scan-first search in the graph starting from the specified vertex is a systematic way of marking the vertices. The main marking step is called \"scan\": to scan a marked vertex means to mark all previously unmarked neighbors of that vertex. At the beginning of the search, only the specified starting vertex is marked. Then the search iteratively scans a marked and unscanned vertex until all vertices are scanned.\n\nA scan-first search in a connected undirected graph produces a spanning tree defined as follows. At the beginning of the search, the tree is empty. Then, for each vertex x in the graph, when x is scanned, all the edges between x and its previously unmarked neighbors are added to the tree; the edges between x and its previously marked neighbors are not added to the tree.\n\nAll previously unmarked vertices constitute the end-point of an edge from the currently scanned vertex, so if we start from some vertex v, and it has neighbors w and x, then if both w and x are unmarked, we create the edges (v, w) and (v, x) and add them to our output tree T'. If either w or x was previously marked, we do not add the edge that includes that vertex to T'. With these new edges in T', we move to the next vertex with the lowest preorder number to scan, which involves continuously marking previously unmarked vertices and adding the edges from the current vertex to these vertices to our output tree.\n\nWe use scan-first search to generate certificates for k-connectivity by running it for k iterations. An important note moving forward is that for each edge added to some output tree T' in each iteration, we remove the edges from the original graph G so they may not be included in some spanning forest for the next iteration. However, we can view the markings on the vertices as reset, so no vertices are marked on the next iteration.\n\nOnce we have exhausted all vertices, we have an edge set for the first iteration, E. We then remove E from G = G, and make that G, and move onto the second iteration using the graph G. At the end of each iteration we have:\nWe say that \"H\" is the sparse certificate for the graph G.\n\nGiven an undirected graph \"G\" = (\"V\", \"E\") with \"n\" vertices, let \"k\" be some positive integer. For all \"i\" = 1, 2, . . . , \"k\", let \"E\" be the set of edges generated by the \"i\"-th iteration of scan-first search, corresponding to a graph \"G\" = (\"V\", \"E\" − (\"E\" ∪ . . . ∪ \"E\")). So for each iteration of scan-first search, as stated above, we will remove edges from the graph \"G\" to create some new graph \"G\" that results at the end of the \"i\"th iteration. For every iteration \"i\", our scan-first search forest is built from the graph \"G\", where \"G\" = \"G\". The claim of the Main Certificate Theorem is that the union \"E\" ∪ . . . ∪ \"E\" is a certificate for the \"k\"-vertex connectivity of \"G\" and that it has at most edges.\n\nThe most important running-time is that of the algorithm running in parallel, using the CRCW PRAM model in this case. Our first spanning tree \"T\" can be found in time using \"C\"(\"n\",\"m\") processors. Our preorder numbers and neighbours can also be calculated in O(log n) time because parallel techniques with processors, our \"C\"(\"n\",\"m\") value. For this reason, we can generate a single \"T&prime\" corresponding to one iteration in \"O\"(log \"n\") time.\n\nUsing a distributed breadth-first search approach, we can find our spanning forest in time on a graph with diameter \"d\" using messages. The sequential approach is quite simply the running time for breadth-first search, .\n\n"}
{"id": "5496919", "url": "https://en.wikipedia.org/wiki?curid=5496919", "title": "Karl Menninger (mathematics)", "text": "Karl Menninger (mathematics)\n\nKarl Menninger (October 6, 1898 – October 2, 1963) was a German teacher of and writer about mathematics. His major work was \"Zahlwort und Ziffer\" (1934,; English trans., \"Number Words and Number Symbols\"), about non-academic mathematics in much of the world. (The omission of Africa was rectified by Claudia Zaslavsky in her book \"Africa Counts\".)\n\n"}
{"id": "3212091", "url": "https://en.wikipedia.org/wiki?curid=3212091", "title": "Local cohomology", "text": "Local cohomology\n\nIn algebraic geometry, local cohomology is an analog of relative cohomology. Alexander Grothendieck introduced it in seminars in Harvard in 1961 written up by , and in 1961-2 at IHES written up as SGA2 - , republished as .\n\nIn the geometric form of the theory, sections Γ are considered of a sheaf \"F\" of abelian groups, on a topological space \"X\", with support in a closed subset \"Y\". The derived functors of Γ form local cohomology groups\n\nFor applications in commutative algebra, the space \"X\" is the spectrum Spec(\"R\") of a commutative ring \"R\" (supposed to be Noetherian throughout this article) and the sheaf \"F\" is the quasicoherent sheaf associated to an \"R\"-module \"M\", denoted by formula_2. The closed subscheme \"Y\" is defined by an ideal \"I\". In this situation, the functor Γ(\"F\") corresponds to the annihilator\n\ni.e., the elements of \"M\" which are annihilated by some power of \"I\". Equivalently,\n\nwhich also shows that local cohomology of quasi-coherent sheaves agrees with\n\nThere is a long exact sequence of sheaf cohomology linking the ordinary sheaf cohomology of \"X\" and of the open set \"U\" = \"X\" \\\"Y\", with the local cohomology groups.\n\nIf \"Y\" is defined by an ideal \"I\"=(\"f\", ..., \"f\"), then local cohomology can be computed by means of Koszul complexes:\n\nwhere \"K\" denotes the Koszul complex, obtained as the tensor product of the Koszul complex for the individual formula_7, defined as \n\nIn particular, this leads to an exact sequence\n\nwhere \"U\" is the open complement of \"Y\" and the middle map is the restriction of sections. The target of this restriction map is also referred to as the ideal transform. For \"n\" ≥ 1, there are isomorphisms\n\nAn important special case is the one when \"R\" is graded, \"I\" consists of the elements of degree ≥ 1, and \"M\" is a graded module. In this case, the cohomology of \"U\" above can be identified with the cohomology groups\n\nof the projective scheme associated to \"R\" and (\"k\") denotes the Serre twist. This relates local cohomology with global cohomology on projective schemes. For example, Castelnuovo–Mumford regularity can be formulated using local cohomology.\n\nThe dimension dim(M) of a module (defined as the Krull dimension of its support) provides an upper bound for local cohomology groups:\n\nIf \"R\" is local and \"M\" finitely generated, then this bound is sharp, i.e., formula_13.\n\nThe depth (defined as the maximal length of a regular \"M\"-sequence; also referred to as the grade of \"M\") provides a sharp lower bound, i.e., it is the smallest integer \"n\" such that\n\nThese two bounds together yield a characterisation of Cohen–Macaulay modules over local rings: they are precisely those modules where formula_15 vanishes for all but one \"n\".\n\nThe local duality theorem is a local analogue of Serre duality. For a Gorenstein ring \"R\", it states that the natural pairing\n\nis a perfect pairing, where ω denotes a dualising module.\n\nThe initial applications were to analogues of the Lefschetz hyperplane theorems. In general such theorems state that homology or cohomology is supported on a hyperplane section of an algebraic variety, except for some 'loss' that can be controlled. These results applied to the algebraic fundamental group and to the Picard group.\n\nAnother type of application are connectedness theorems such as Grothendieck's connectedness theorem (a local analogue of the Bertini theorem) or the Fulton–Hansen connectedness theorem due to and . The latter asserts that for two projective varieties \"V\" and \"W\" in P over an algebraically closed field, the connectedness dimension of \"Z\" = \"V\" ∩ \"W\" (i.e., the minimal dimension of a closed subset \"T\" of \"Z\" that has to be removed from \"Z\" so that the complement \"Z\" \\ \"T\" is disconnected) is bound by\nFor example, \"Z\" is connected if dim \"V\" + dim \"W\" > \"r\".\n\n\n"}
{"id": "41981077", "url": "https://en.wikipedia.org/wiki?curid=41981077", "title": "Matrix-exponential distribution", "text": "Matrix-exponential distribution\n\nIn probability theory, the matrix-exponential distribution is an absolutely continuous distribution with rational Laplace–Stieltjes transform. They were first introduced by David Cox in 1955 as distributions with rational Laplace–Stieltjes transforms.\n\nThe probability density function is\n\n(and 0 when \"x\" < 0) where\n\nThere are no restrictions on the parameters α, T, s other than that they correspond to a probability distribution. There is no straightforward way to ascertain if a particular set of parameters form such a distribution. The dimension of the matrix T is the order of the matrix-exponential representation.\n\nThe distribution is a generalisation of the phase type distribution.\n\nIf \"X\" has a matrix-exponential distribution then the \"k\"th moment is given by\n\nMatrix exponential distributions can be fitted using maximum likelihood estimation.\n\n\n"}
{"id": "1561467", "url": "https://en.wikipedia.org/wiki?curid=1561467", "title": "Metric (mathematics)", "text": "Metric (mathematics)\n\nIn mathematics, a metric or distance function is a function that defines a distance between each pair of elements of a set. A set with a metric is called a metric space. A metric induces a topology on a set, but not all topologies can be generated by a metric. A topological space whose topology can be described by a metric is called metrizable.\n\nAn important source of metrics in differential geometry are metric tensors, bilinear forms that may be defined from the tangent vectors of a differentiable manifold onto a scalar. A metric tensor allows distances along curves to be determined through integration, and thus determines a metric. However, not every metric comes from a metric tensor in this way.\n\nA metric on a set is a function (called the \"distance function\" or simply distance)\nwhere formula_2 is the set of non-negative real numbers and for all formula_3, the following conditions are satisfied:\n\nConditions 1 and 2 together define a \"positive-definite function\". \nThe first condition is implied by the others.\n\nA metric is called an ultrametric if it satisfies the following stronger version of the \"triangle inequality\" where points can never fall 'between' other points:\nfor all formula_3\n\nA metric on is called intrinsic if any two points and in can be joined by a curve with length arbitrarily close to .\n\nFor sets on which an addition + : is defined,\nfor all , , and in .\n\nThese conditions express intuitive notions about the concept of distance. For example, that the distance between distinct points is positive and the distance from \"x\" to \"y\" is the same as the distance from \"y\" to \"x\". The triangle inequality means that the distance from \"x\" to \"z\" via \"y\" is at least as great as from \"x\" to \"z\" directly. Euclid in his work stated that the shortest distance between two points is a line; that was the triangle inequality for his geometry.\n\nIf a modification of the triangle inequality\nis used in the definition then property 1 follows straight from property 4*. Properties 2 and 4* give property 3 which in turn gives property 4.\n\n\nFor a given set \"X\", two metrics \"d\" and \"d\" are called topologically equivalent (uniformly equivalent) if the identity mapping\nis a homeomorphism (uniform isomorphism).\n\nFor example, if formula_11 is a metric, then formula_12 and formula_13 are metrics equivalent to formula_14\n\nSee also notions of metric space equivalence.\n\nNorms on vector spaces are equivalent to certain metrics, namely homogeneous, translation-invariant ones. In other words, every norm determines a metric, and some metrics determine a norm.\n\nGiven a normed vector space formula_15 we can define a metric on \"X\" by\nThe metric \"d\" is said to be induced by the norm formula_17.\n\nConversely if a metric \"d\" on a vector space \"X\" satisfies the properties\nthen we can define a norm on \"X\" by\n\nSimilarly, a seminorm induces a pseudometric (see below), and a homogeneous, translation invariant pseudometric induces a seminorm.\n\nWe can generalize the notion of a metric from a distance between two elements to a distance between two nonempty finite multisets of elements. A multiset is a generalization of the notion of a set such that an element can occur more than once. Define formula_21 if\nformula_22 is the multiset consisting of the elements of the multisets formula_23 and formula_24, that is, if formula_25 occurs once in formula_23 and once in formula_24 then it occurs twice in formula_22. \nA distance function\nformula_11 on the set of nonempty finite multisets is a metric if\nNote that the familiar metric between two elements results if the multiset formula_23 has two elements in 1 and 2 and the multisets formula_37 have one element each in 3. For instance if formula_23 consists of two occurrences of formula_25, then formula_30 according to 1.\n\nA simple example is the set of all nonempty finite multisets formula_23 of integers with formula_42. More complex examples are information distance in multisets; and normalized compression distance (NCD) in multisets.\n\nThere are numerous ways of relaxing the axioms of metrics, giving rise to various notions of generalized metric spaces. These generalizations can also be combined. The terminology used to describe them is not completely standardized. Most notably, in functional analysis pseudometrics often come from seminorms on vector spaces, and so it is natural to call them \"semimetrics\". This conflicts with the use of the term in topology.\n\nSome authors allow the distance function \"d\" to attain the value ∞, i.e. distances are non-negative numbers on the extended real number line. \nSuch a function is called an \"extended metric\" or \"∞-metric\". \nEvery extended metric can be transformed to a finite metric such that the metric spaces are equivalent as far as notions of topology (such as continuity or convergence) are concerned. This can be done using a subadditive monotonically increasing bounded function which is zero at zero, e.g. \"d\"′(\"x\", \"y\") = \"d\"(\"x\", \"y\") / (1 + \"d\"(\"x\", \"y\")) or \"d\"′′(\"x\", \"y\") = min(1, \"d\"(\"x\", \"y\")).\n\nThe requirement that the metric take values in <nowiki>[0,∞)</nowiki> can even be relaxed to consider metrics with values in other directed sets. The reformulation of the axioms in this case leads to the construction of uniform spaces: topological spaces with an abstract structure enabling one to compare the local topologies of different points.\n\nA pseudometric on \"X\" is a function \"d\" : \"X\" × \"X\" → R which satisfies the axioms for a metric, except that instead of the second (identity of indiscernibles) only \"d\"(\"x\",\"x\")=0 for all \"x\" is required. In other words, the axioms for a pseudometric are:\n\n\n\nIn some contexts, pseudometrics are referred to as \"semimetrics\" because of their relation to seminorms.\n\nOccasionally, a quasimetric is defined as a function that satisfies all axioms for a metric with the possible exception of symmetry:. The name of this generalisation is not entirely standardized.\n\n\nQuasimetrics are common in real life. For example, given a set \"X\" of mountain villages, the typical walking times between elements of \"X\" form a quasimetric because travel up hill takes longer than travel down hill. Another example is a taxicab geometry topology having one-way streets, where a path from point \"A\" to point \"B\" comprises a different set of streets than a path from \"B\" to \"A\". \n\nA quasimetric on the reals can be defined by setting\nThe topological space underlying this quasimetric space is the Sorgenfrey line. This space describes the process of filing down a metal stick: it is easy to reduce its size, but it is difficult or impossible to grow it.\n\nIf \"d\" is a quasimetric on \"X\", a metric \"d<nowiki>'</nowiki>\" on \"X\" can be formed by taking\n\nIn a \"metametric\", all the axioms of a metric are satisfied except that the distance between identical points is not necessarily zero. In other words, the axioms for a metametric are:\n\n\nMetametrics appear in the study of Gromov hyperbolic metric spaces and their boundaries. The \"visual metametric\" on such a space satisfies \"d\"(\"x\", \"x\") = 0 for points \"x\" on the boundary, but otherwise \"d\"(\"x\", \"x\") is approximately the distance from \"x\" to the boundary. Metametrics were first defined by Jussi Väisälä.\n\nA semimetric on \"X\" is a function \"d\" : \"X\" × \"X\" → R that satisfies the first three axioms, but not necessarily the triangle inequality:\n\n\nSome authors work with a weaker form of the triangle inequality, such as:\nThe ρ-inframetric inequality implies the ρ-relaxed triangle inequality (assuming the first axiom), and the ρ-relaxed triangle inequality implies the 2ρ-inframetric inequality. Semimetrics satisfying these equivalent conditions have sometimes been referred to as \"quasimetrics\", \"nearmetrics\" or inframetrics.\n\nThe ρ-inframetric inequalities were introduced to model round-trip delay times in the internet. The triangle inequality implies the 2-inframetric inequality, and the ultrametric inequality is exactly the 1-inframetric inequality.\n\nRelaxing the last three axioms leads to the notion of a premetric, i.e. a function satisfying the following conditions:\n\n\nThis is not a standard term. Sometimes it is used to refer to other generalizations of metrics such as pseudosemimetrics or pseudometrics; in translations of Russian books it sometimes appears as \"prametric\".\n\nAny premetric gives rise to a topology as follows. For a positive real \"r\", the \"r\"-ball centered at a point \"p\" is defined as\nA set is called \"open\" if for any point \"p\" in the set there is an \"r\"-ball centered at \"p\" which is contained in the set. Every premetric space is a topological space, and in fact a sequential space.\nIn general, the \"r\"-balls themselves need not be open sets with respect to this topology. \nAs for metrics, the distance between two sets \"A\" and \"B\", is defined as\nThis defines a premetric on the power set of a premetric space. If we start with a (pseudosemi-)metric space, we get a pseudosemimetric, i.e. a symmetric premetric.\nAny premetric gives rise to a preclosure operator \"cl\" as follows:\n\nThe prefixes \"pseudo-\", \"quasi-\" and \"semi-\" can also be combined, e.g., a pseudoquasimetric (sometimes called hemimetric) relaxes both the indiscernibility axiom and the symmetry axiom and is simply a premetric satisfying the triangle inequality. For pseudoquasimetric spaces the open \"r\"-balls form a basis of open sets. A very basic example of a pseudoquasimetric space is the set {0,1} with the premetric given by \"d\"(0,1) = 1 and \"d\"(1,0) = 0. The associated topological space is the Sierpiński space.\n\nSets equipped with an extended pseudoquasimetric were studied by William Lawvere as \"generalized metric spaces\". From a categorical point of view, the extended pseudometric spaces and the extended pseudoquasimetric spaces, along with their corresponding nonexpansive maps, are the best behaved of the metric space categories. One can take arbitrary products and coproducts and form quotient objects within the given category. If one drops \"extended\", one can only take finite products and coproducts. If one drops \"pseudo\", one cannot take quotients. Approach spaces are a generalization of metric spaces that maintains these good categorical properties.\n\nIn differential geometry, one considers a metric tensor, which can be thought of as an \"infinitesimal\" quadratic metric function. This is defined as a nondegenerate symmetric bilinear form on the tangent space of a manifold with an appropriate differentiability requirement. While these are not metric functions as defined in this article, they induce what is called a pseudo-semimetric function by integration of its square root along a path through the manifold. If one imposes the positive-definiteness requirement of an inner product on the metric tensor, this restricts to the case of a Riemannian manifold, and the path integration yields a metric.\n\nIn general relativity the related concept is a metric tensor (general relativity) which expresses the structure of a pseudo-Riemannian manifold. Though the term \"metric\" is used in cosmology, the fundamental idea is different because there are non-zero null vectors in the tangent space of these manifolds. This generalized view of \"metrics\", in which zero distance does \"not\" imply identity, has crept into some mathematical writing too:\n\n\n\n"}
{"id": "230488", "url": "https://en.wikipedia.org/wiki?curid=230488", "title": "Minkowski space", "text": "Minkowski space\n\nIn mathematical physics, Minkowski space (or Minkowski spacetime) is a combining of three-dimensional Euclidean space and time into a four-dimensional manifold where the spacetime interval between any two events is independent of the inertial frame of reference in which they are recorded. Although initially developed by mathematician Hermann Minkowski for Maxwell's equations of electromagnetism, the mathematical structure of Minkowski spacetime was shown to be an immediate consequence of the postulates of special relativity.\n\nMinkowski space is closely associated with Einstein's theory of special relativity, and is the most common mathematical structure on which special relativity is formulated. While the individual components in Euclidean space and time may differ due to length contraction and time dilation, in Minkowski spacetime, all frames of reference will agree on the total distance in spacetime between events. Because it treats time differently than it treats the 3 spatial dimensions, Minkowski space differs from four-dimensional Euclidean space.\n\nIn 3-dimensional Euclidean space (e.g. simply \"space\" in Galilean relativity), the isometry group (the maps preserving the regular Euclidean distance) is the Euclidean group. It is generated by rotations, reflections and translations. When time is amended as a fourth dimension, the further transformations of translations in time and Galilean boosts are added, and the group of all these transformations is called the Galilean group. All Galilean transformations preserve the \"3-dimensional\" Euclidean distance. This distance is purely spatial. Time differences are \"separately\" preserved as well. This changes in the spacetime of special relativity, where space and time are interwoven.\n\nSpacetime is equipped with an indefinite non-degenerate bilinear form, variously called the \"Minkowski metric\", the \"Minkowski norm squared\" or \"Minkowski inner product\" depending on the context. The Minkowski inner product is defined as to yield the spacetime interval between two events when given their coordinate difference vector as argument. Equipped with this inner product, the mathematical model of spacetime is called Minkowski space. The analogue of the Galilean group for Minkowski space, preserving the spacetime interval (as opposed to the spatial Euclidean distance) is the Poincaré group.\n\nIn summary, Galilean spacetime and Minkowski spacetime are, when viewed as manifolds, actually \"the same\". They differ in what further structures are defined \"on\" them. The former has the Euclidean distance function and time (separately) together with inertial frames whose coordinates are related by Galilean transformations, while the latter has the Minkowski metric together with inertial frames whose coordinates are related by Poincaré transformations.\n\nIn 1905–06 Henri Poincaré showed that by taking time to be an imaginary fourth spacetime coordinate , where is the speed of light and is the imaginary unit, a Lorentz transformation can formally be regarded as a rotation of coordinates in a four-dimensional space with three real coordinates representing space, and one imaginary coordinate representing time, as the fourth dimension. In physical spacetime special relativity stipulates that the quantity\n\nis invariant under coordinate changes from one inertial frame to another, i. e. under Lorentz transformations. Here the speed of light is, following Poincaré, set to unity. In the space suggested by him (Poincaré mentions this only in passing) where physical spacetime is \"coordinatized\" by , call it \"coordinate space\", Lorentz transformations appear as ordinary rotations preserving the quadratic form\n\non coordinate space. The naming and ordering of coordinates, with the same labels for space coordinates, but with the imaginary time coordinate as the \"fourth coordinate\", is conventional. The above expression, while making the former expression more familiar, may potentially be confusing because it is \"not\" the same that appears in the latter (\"time coordinate\") as in the former (\"time itself\" in some inertial system as measured by clocks stationary in that system).\n\nRotations in planes spanned by two space unit vectors appear in coordinate space as well as in physical spacetime appear as Euclidean rotations and are interpreted in the ordinary sense. The \"rotation\" in a plane spanned by a space unit vector and a time unit vector, while formally still a rotation in coordinate space, is a Lorentz boost in physical spacetime with \"real\" inertial coordinates (see also hyperbolic rotation). The analogy with Euclidean rotations is thus only partial.\n\nThis idea was elaborated by Hermann Minkowski, who used it to restate the Maxwell equations in four dimensions, showing directly their invariance under the Lorentz transformation. He further reformulated in four dimensions the then-recent theory of special relativity of Einstein. From this he concluded that time and space should be treated equally, and so arose his concept of events taking place in a unified four-dimensional spacetime continuum.\n\nIn a further development in his 1908 \"Space and Time\" lecture, Minkowski gave an alternative formulation of this idea that used a real time coordinate instead of an imaginary one, representing the four variables of space and time in coordinate form in a four dimensional real vector space. Points in this space correspond to events in spacetime. In this space, there is a defined light-cone associated with each point, and events not on the light-cone are classified by their relation to the apex as \"spacelike\" or \"timelike\". It is principally this view of spacetime that is current nowadays, although the older view involving imaginary time has also influenced special relativity. \n\nIn the English translation of Minkowski's paper, the Minkowski metric as defined below is referred to as the \"line element\". The Minkowski inner product of below appears unnamed when referring to orthogonality (which he calls \"normality\") of certain vectors, and the Minkowski norm squared is referred to (somewhat cryptically, perhaps this is translation dependent) as \"sum\".\n\nMinkowski's principal tool is the Minkowski diagram, and he uses it to define concepts and demonstrate properties of Lorentz transformations (e.g. proper time and length contraction) and to provide geometrical interpretation to the generalization of Newtonian mechanics to relativistic mechanics. For these special topics, see the referenced articles, as the presentation below will be principally confined to the mathematical structure (Minkowski metric and from it derived quantities and the Poincaré group as symmetry group of spacetime) \"following\" from the invariance of the spacetime interval on the spacetime manifold as consequences of the postulates of special relativity, not to specific application or \"derivation\" of the invariance of the spacetime interval. This structure provides the background setting of all present relativistic theories, barring general relativity for which flat Minkowski spacetime still provides a springboard as curved spacetime is locally Lorentzian.\n\nMinkowski, aware of the fundamental restatement of the theory which he had made, said\nThough Minkowski took an important step for physics, Einstein saw its limitation:\n\nFor further historical information see references , and .\n\nIt is assumed below that spacetime is endowed with a coordinate system corresponding to an inertial frame. This provides an \"origin\", which is necessary in order to be able to refer to spacetime as being modeled as a vector space. This is not really \"physically\" motivated in that a canonical origin (\"central\" event in spacetime) should exist. One can get away with less structure, that of an affine space, but this would needlessly complicate the discussion and would not reflect how flat spacetime is normally treated mathematically in modern introductory literature.\n\nFor an overview, Minkowski space is a -dimensional real vector space equipped with a nondegenerate, symmetric bilinear form on the tangent space at each point in spacetime, here simply called the \"Minkowski inner product\", with metric signature either or . The tangent space at each event is a vector space of the same dimension as spacetime, .\n\nIn practice, one need not be concerned with the tangent spaces. The vector space nature of Minkowski space allows for the canonical identification of vectors in tangent spaces at points (events) with vectors (points, events) in Minkowski space itself. See e.g. These identifications are routinely done in mathematics. They can be expressed formally in Cartesian coordinates as\n\nwith basis vectors in the tangent spaces defined by\n\nHere and are any two events and the last identification is referred to as parallel transport. The first identification is the canonical identification of vectors in the tangent space at any point with vectors in the space itself. The appearance of basis vectors in tangent spaces as first order differential operators is due to this identification. It is motivated by the observation that a geometrical tangent vector can be associated in a one-to-one manner with a directional derivative operator on the set of smooth functions. This is promoted to a \"definition\" of tangent vectors in manifolds \"not\" necessarily being embedded in . This definition of tangent vectors is not the only possible one as ordinary n-tuples can be used as well.\n\nA tangent vector at a point may be defined, here specialized to Cartesian coordinates in Lorentz frames, as column vectors associated to \"each\" Lorentz frame related by Lorentz transformation such that the vector in a frame related to some frame by transforms according to . This is the \"same\" way in which the coordinates transform. Explicitly,\nThis definition is equivalent to the definition given above under a canonical isomorphism.\nFor some purposes it is desirable to identify tangent vectors at a point with \"displacement vectors\" at , which is, of course, admissible by essentially the same canonical identification. The identifications of vectors referred to above in the mathematical setting can correspondingly be found in a more physical and explicitly geometrical setting in . They offer various degree of sophistication (and rigor) depending on which part of the material one chooses to read.\n\nThe metric signature refers to which sign the Minkowski inner product yields when given space (\"spacelike\" to be specific, defined further down) and time basis vectors (\"timelike\") as arguments. Further discussion about this theoretically inconsequential, but practically necessary choice for purposes of internal consistency and convenience is deferred to the hide box below.\n\nIn general, but with several exceptions, mathematicians and general relativists prefer spacelike vectors to yield a positive sign, , while particle physicists tend to prefer timelike vectors to yield a positive sign, . Authors covering several areas of physics, e.g. Steven Weinberg and Landau and Lifshitz ( and respectively) stick to one choice regardless of topic. Arguments for the former convention include \"continuity\" from the Euclidean case corresponding to the non-relativistic limit . Arguments for the latter include that minus signs, otherwise ubiquitous in particle physics, go away. Yet other authors, especially of introductory texts, e.g. , do \"not\" choose a signature at all, but instead opt to coordinatize spacetime such that the time \"coordinate\" (but not time itself!) is imaginary. This removes the need of the \"explicit\" introduction of a metric tensor (which may seem as an extra burden in an introductory course), and one needs \"not\" be concerned with covariant vectors and contravariant vectors (or raising and lowering indices) to be described below. The inner product is instead effected by a straightforward extension of the dot product in to . This works in the flat spacetime of special relativity, but not in the curved spacetime of general relativity, see (who, by the way use ). MTW also argues that it hides the true \"indefinite\" nature of the metric and the true nature of Lorentz boosts, which aren't rotations. It also needlessly complicates the use of tools of differential geometry that are otherwise immediately available and useful for geometrical description and calculation – even in the flat spacetime of special relativity, e.g. of the electromagnetic field.\nMathematically associated to the bilinear form is a tensor of type at each point in spacetime, called the \"Minkowski metric\". The Minkowski metric, the bilinear form, and the Minkowski inner product are actually all the very same object; it is a bilinear function that accepts two (contravariant) vectors and returns a real number. In coordinates, this is the matrix representing the bilinear form.\n\nFor comparison, in general relativity, a Lorentzian manifold is likewise equipped with a metric tensor , which is a nondegenerate symmetric bilinear form on the tangent space at each point of . In coordinates, it may be represented by a matrix \"depending on spacetime position\". Minkowski space is thus a comparatively simple special case of a Lorentzian manifold. Its metric tensor is in coordinates the same symmetric matrix at every point of , and its arguments can, per above, be taken as vectors in spacetime itself.\n\nIntroducing more terminology (but not more structure), Minkowski space is thus a pseudo-Euclidean space with total dimension and signature or . Elements of Minkowski space are called events. Minkowski space is often denoted or to emphasize the chosen signature, or just . It is perhaps the simplest example of a pseudo-Riemannian manifold.\n\nAn interesting example of non-inertial coordinates for (part of) Minkowski spacetime are the Born coordinates. Another useful set of coordinates are the lightcone coordinates.\n\nThe Minkowski metric is the metric tensor of Minkowski space. It is a pseudo-Euclidean metric, or more generally a \"constant\" pseudo-Riemannian metric in Cartesian coordinates. As such it is a nondegenerate symmetric bilinear form, a type tensor. It accepts two arguments , vectors in , the tangent space at in . Due to the above-mentioned canonical identification of with itself, it accepts arguments with both and in .\n\nAs a notational convention, vectors in , called 4-vectors, are denoted in sans-serif italics, and not, as is common in the Euclidean setting, with boldface . The latter is generally reserved for the -vector part (to be introduced below) of a -vector.\n\nThe definition\n\nyields an inner product-like structure on , previously and also henceforth, called the \"Minkowski inner product\", similar to the Euclidean inner product, but it describes a different geometry. It is also called the \"relativistic dot product\". If the two arguments are the same,\n\nthe resulting quantity will be called the \"Minkowski norm squared\". The Minkowski inner product satisfies the following properties.\n\n\nThe first two conditions imply bilinearity. The defining \"difference\" between a pseudo-inner product and an inner product proper is that the former is \"not\" required to be positive definite, that is, is allowed.\n\nThe most important feature of the inner product and norm squared is that \"these are quantities unaffected by Lorentz transformations\". In fact, it can be taken as the defining property of a Lorentz transformation that it preserves the inner product (i.e. the value of the corresponding bilinear form on two vectors). This approach is taken more generally for \"all\" classical groups definable this way in classical group. There, the matrix is identical in the case (the Lorentz group) to the matrix to be displayed below.\n\nTwo vectors and are said to be orthogonal if . For a geometric interpretation of orthogonality in the special case when and (or vice versa), see hyperbolic orthogonality.\n\nA vector is called a unit vector if . A basis for consisting of mutually orthogonal unit vectors is called an orthonormal basis.\n\nFor a given inertial frame, an orthonormal basis in space, combined by the unit time vector, forms an orthonormal basis in Minkowski space. The number of positive and negative unit vectors in any such basis is a fixed pair of numbers, equal to the signature of the bilinear form associated with the inner product. This is Sylvester's law of inertia.\n\nMore terminology (but not more structure): The Minkowski metric is a pseudo-Riemannian metric, more specifically, a Lorentzian metric, even more specifically, \"the\" Lorentz metric, reserved for -dimensional flat spacetime with the remaining ambiguity only being the signature convention.\n\nFrom the second postulate of special relativity, together with homogeneity of spacetime and isotropy of space, it follows that the spacetime interval between two arbitrary events called and is:\n\nThe interval is independent of the inertial frame chosen, as is shown here. The factor determines the choice of the metric signature as an arbitrary sign convention. The numerical values of , viewed as a matrix representing the Minkowski inner product, follow from the theory of bilinear forms.\n\nJust as the signature of the metric is differently defined in the literature, this quantity is not consistently named. The interval (as defined here) is sometimes referred to as the interval squared. Even the square root of the present interval occurs. When signature and interval are fixed, ambiguity still remains as which coordinate is the time coordinate. It may be the fourth, or it may be the zeroth. This is not an exhaustive list of notational inconsistencies. It is a fact of life that one has to check out the definitions first thing when one consults the relativity literature.\n\nThe invariance of the interval under coordinate transformations between inertial frames follows from the invariance of\n\n(with either sign preserved), provided the transformations are linear. This quadratic form can be used to define a bilinear form\n\nvia the polarization identity. This bilinear form can in turn be written as\n\nwhere is a matrix associated with . Possibly confusingly, denote with just as is common practice. The matrix is read off from the explicit bilinear form as\n\nand the bilinear form\n\nwith which this section started by assuming its existence, is now identified.\n\nFor definiteness and shorter presentation, the signature is adopted below. This choice (or the other possible choice) has no (known) physical implications. The symmetry group preserving the bilinear form with one choice of signature is isomorphic (under the map given here) with the symmetry group preserving the other choice of signature. This means that both choices are in accord with the two postulates of relativity. Switching between the two conventions is straightforward. If the metric tensor has been used in a derivation, go back to the earliest point where it was used, substitute for , and retrace forward to the desired formula with the desired metric signature.\n\nA standard basis for Minkowski space is a set of four mutually orthogonal vectors such that\nThese conditions can be written compactly in the form\n\nRelative to a standard basis, the components of a vector are written where the Einstein notation is used to write . The component is called the timelike component of while the other three components are called the spatial components. The spatial components of a -vector may be identified with a -vector .\n\nIn terms of components, the Minkowski inner product between two vectors and is given by\n\nand\n\nHere lowering of an index with the metric was used.\n\nTechnically, a non-degenerate bilinear form provides a map between a vector space and its dual, in this context, the map is between the tangent spaces of and the cotangent spaces of . At a point in , the tangent and cotangent spaces are dual vector spaces (so the dimension of the cotangent space at an event is also ). Just as an authentic inner product on a vector space with one argument fixed, by Riesz representation theorem, may be expressed as the action of a linear functional on the vector space, the same holds for the Minkowski inner product of Minkowski space.\n\nThus if are the components of a vector in a tangent space, then are the components of a vector in the cotangent space (a linear functional). Due to the identification of vectors in tangent spaces with vectors in itself, this is mostly ignored, and vectors with lower indices are referred to as covariant vectors. In this latter interpretation, the covariant vectors are (almost always implicitly) identified with vectors (linear functionals) in the dual of Minkowski space. The ones with upper indices are contravariant vectors. In the same fashion, the inverse of the map from tangent to cotangent spaces, explicitly given by the inverse of in matrix representation, can be used to define raising of an index. The components of this inverse are denoted . It happens that . These maps between a vector space and its dual can be denoted (eta-flat) and (eta-sharp) by the musical analogy.\n\nContravariant and covariant vectors are geometrically very different objects. The first can and should be thought of as arrows. A linear functional can be characterized by two objects: its kernel, which is a hyperplane passing through the origin, and its norm. Geometrically thus, covariant vectors should be viewed as a set of hyperplanes, with spacing depending on the norm (bigger = smaller spacing), with one of them (the kernel) passing through the origin. The mathematical term for a covariant vector is 1-covector or 1-form (though the latter is usually reserved for covector \"fields\").\n\nThe electromagnetic field tensor is a differential 2-form, which geometrical description can as well be found in MTW.\n\nOne may, of course, ignore geometrical views all together (as is the style in e.g. and ) and proceed algebraically in a purely formal fashion. The time-proven robustness of the formalism itself, sometimes referred to as index gymnastics, ensures that moving vectors around and changing from contravariant to covariant vectors and vice versa (as well as higher order tensors) is mathematically sound. Incorrect expressions tend to reveal themselves quickly.\n\nThe present purpose is to show semi-rigorously how \"formally\" one may apply the Minkowski metric to two vectors and obtain a real number, i.e. to display the role of the differentials, and how they disappear in a calculation. The setting is that of smooth manifold theory, and concepts such as convector fields and exterior derivatives are introduced.\n\nA full-blown version of the Minkowski metric in coordinates as a tensor field on spacetime has the appearance\n\nExplanation: The coordinate differentials are 1-form fields. They are defined as the exterior derivative of the coordinate functions . These quantities evaluated at a point provide a basis for the cotangent space at . The tensor product (denoted by the symbol ) yields a tensor field of type , i.e. the type that expects two contravariant vectors as arguments. On the right hand side, the symmetric product (denoted by the symbol or by juxtaposition) has been taken. The equality holds since, by definition, the Minkowski metric is symmetric. The notation on the far right is also sometimes used for the related, but different, line element. It is \"not\" a tensor. For elaboration on the differences and similarities, see \n\n\"Tangent\" vectors are, in this formalism, given in terms of a basis of differential operators of the first order,\n\nwhere is an event. This operator applied to a function gives the directional derivative of at in the direction of increasing with fixed. They provide a basis for the tangent space at .\n\nThe exterior derivative of a function is a covector field, i.e. an assignment of a cotangent vector to each point , by definition such that\n\nfor each vector field . A vector field is an assignment of a tangent vector to each point . In coordinates can be expanded at each point in the basis given by the . Applying this with , the coordinate function itself, and , called a \"coordinate vector field\", one obtains\n\nSince this relation holds at each point , the provide a basis for the cotangent space at each and the bases and are dual to each other,\n\nat each . Furthermore, one has\n\nfor general one-forms on a tangent space and general tangent vectors . (This can be taken as a definition, but may also be proved in a more general setting.)\n\nThus when the metric tensor is fed two vectors fields , both expanded in terms of the basis coordinate vector fields, the result is\n\nwhere , are the \"component functions\" of the vector fields. The above equation holds at each point , and the relation may as well be interpreted as the Minkowski metric at applied to two tangent vectors at .\n\nAs mentioned, in a vector space, such as that modelling the spacetime of special relativity, tangent vectors can be canonically identified with vectors in the space itself, and vice versa. This means that the tangent spaces at each point are canonically identified with each other and with the vector space itself. This explains how the right hand side of the above equation can be employed directly, without regard to spacetime point the metric is to be evaluated and from where (which tangent space) the vectors come from.\n\nThis situation changes in general relativity. There one has\n\nwhere now , i.e. is still a metric tensor but now depending on spacetime and is a solution of Einstein's field equations. Moreover, \"must\" be tangent vectors at spacetime point and can no longer be moved around freely.\n\nThe Poincaré group is the group of all transformations preserving the interval. The interval is quite easily seen to be preserved by the translation group in dimensions. The other transformations are those that preserve the interval and leave the origin fixed. Given the bilinear form associated with the Minkowski metric, the appropriate group follows directly from the theory (in particular the definition) of classical groups. In the linked article, one should identify (in its a matrix representation) with the matrix .\n\nThe appropriate group is , in this context called the Lorentz group. Its elements are called (homogeneous) Lorentz transformations. For other methods of derivation, with a more physical twist, see derivations of the Lorentz transformations.\n\nAmong the simplest Lorentz transformations is a Lorentz boost. For reference, a boost in the -direction is given by\n\nwhere\n\nis the Lorentz factor, and\n\nOther Lorentz transformations are pure rotations, and hence elements of the subgroup of . A general homogeneous Lorentz transformation is a product of a pure boost and a pure rotation. An \"inhomogeneous\" Lorentz transformation is a homogeneous transformation followed by a translation in space and time. Special transformations are those that invert the space coordinates () and time coordinate () respectively, or both .\n\nAll four-vectors in Minkowski space transform, by definition, according to the same formula under Lorentz transformations. Minkowski diagrams illustrate Lorentz transformations.\n\nWhere v is velocity, and x, y, and z are Cartesian coordinates in 3-dimensional space, and c is the constant representing the universal speed limit, and t is time, the four-dimensional vector is classified according to the sign of . A vector is timelike if , spacelike if , and null or lightlike if . This can be expressed in terms of the sign of as well, but depends on the signature. The classification of any vector will be the same in all frames of reference that are related by a Lorentz transformation (but not by a general Poincaré transformation because the origin may then be displaced) because of the invariance of the interval.\n\nThe set of all null vectors at an event of Minkowski space constitutes the light cone of that event. Given a timelike vector , there is a worldline of constant velocity associated with it, represented by a straight line in a Minkowski diagram.\n\nOnce a direction of time is chosen, timelike and null vectors can be further decomposed into various classes. For timelike vectors one has\nNull vectors fall into three classes:\nSpacelike vectors are in elsewhere. The terminology stems from the fact that spacelike separated events are connected by vectors requiring faster-than-light travel, and so cannot possibly influence each other. Together with spacelike and lightlike vectors there are 7 classes in all.\n\nAn orthonormal basis for Minkowski space necessarily consists of one timelike and three spacelike unit vectors. If one wishes to work with non-orthonormal bases it is possible to have other combinations of vectors. For example, one can easily construct a (non-orthonormal) basis consisting entirely of null vectors, called a null basis.\n\nVector fields are called timelike, spacelike or null if the associated vectors are timelike, spacelike or null at each point where the field is defined.\n\nLet . We say that\n\n\nSuppose \"x\" ∈ \"M\" is timelike. Then the simultaneous hyperplane for x is formula_32 Since this hyperplane varies as \"x\" varies, there is a relativity of simultaneity in Minkowski space.\n\nIf and are both future-directed timelike four-vectors, then in the sign convention for norm, \n\nA Lorentzian manifold is a generalization of Minkowski space in two ways. The total number of spacetime dimensions is not restricted to be ( or more) and a Lorentzian manifold need not be flat, i.e. it allows for curvature.\n\nMinkowski space refers to a mathematical formulation in four dimensions. However, the mathematics can easily be extended or simplified to create an analogous generalized Minkowski space in any number of dimensions. If , -dimensional Minkowski space is a vector space of real dimension on which there is a constant Minkowski metric of signature or . These generalizations are used in theories where spacetime is assumed to have more or less than dimensions. String theory and M-theory are two examples where . In string theory, there appears conformal field theories with spacetime dimensions.\n\nde Sitter space can be formulated as a submanifold of generalized Minkowski space as can the model spaces of hyperbolic geometry (see below).\n\nAs a \"flat spacetime\", the three spatial components of Minkowski spacetime always obey the Pythagorean Theorem. Minkowski space is a suitable basis for special relativity, a good description of physical systems over finite distances in systems without significant gravitation. However, in order to take gravity into account, physicists use the theory of general relativity, which is formulated in the mathematics of a non-Euclidean geometry. When this geometry is used as a model of physical space, it is known as curved space.\n\nEven in curved space, Minkowski space is still a good description in an infinitesimal region surrounding any point (barring gravitational singularities). More abstractly, we say that in the presence of gravity spacetime is described by a curved 4-dimensional manifold for which the tangent space to any point is a 4-dimensional Minkowski space. Thus, the structure of Minkowski space is still essential in the description of general relativity.\n\nThe meaning of the term \"geometry\" in the context of Minkowski space depends heavily on what is meant by the term. Minkowski space is not endowed with a Euclidean geometry, and not with any of the generalized Riemannian geometries with intrinsic curvature, those exposed by the \"model spaces\" in hyperbolic geometry (negative curvature) and the geometry modeled by the sphere (positive curvature). The reason is the indefiniteness of the Minkowski metric. Minkowski space is, in particular, not a metric space and not a Riemannian manifold with a Riemannian metric. However, Minkowski space contains submanifolds endowed with a Riemannian metric yielding hyperbolic geometry.\n\nModel spaces of hyperbolic geometry of low dimension, say or , \"cannot\" be isometrically embedded in Euclidean space with one more dimension, i.e. or respectively, with the Euclidean metric , disallowing easy visualization. By comparison, model spaces with positive curvature are just spheres in Euclidean space of one higher dimension. It turns out however that these hyperbolic spaces \"can\" be isometrically embedded in spaces of one more dimension when the embedding space is endowed with the Minkowski metric .\n\nDefine to be the upper sheet () of the hyperboloid\n\nin generalized Minkowski space of spacetime dimension . This is one of the surfaces of transitivity of the generalized Lorentz group. The induced metric on this submanifold,\n\nthe pullback of the Minkowski metric under inclusion, is a Riemannian metric. With this metric is a Riemannian manifold. It is one of the model spaces of Riemannian geometry, the hyperboloid model of hyperbolic space. It is a space of constant negative curvature . The in the upper index refers to an enumeration of the different model spaces of hyperbolic geometry, and the for its dimension. A corresponds to the Poincaré disk model, while corresponds to the Poincaré half-space model of dimension .\n\nIn the definition above is the inclusion map and the superscript star denotes the pullback. The present purpose is to describe this and similar operations as a preparation for the actual demonstration that actually is a hyperbolic space. \n\nIn order to exhibit the metric it is necessary to pull it back via a suitable \"parametrization\". A parametrization of a submanifold of is a map whose range is an open subset of . If has the same dimension as , a parametrization is just the inverse of a coordinate map . The parametrization to be used is the inverse of \"hyperbolic stereographic projection\". This is illustrated in the figure to the left for . It is instructive to compare to stereographic projection for spheres.\n\nStereographic projection and its inverse are given by\n\nwhere, for simplicity, . The are coordinates on and the are coordinates on .\n\n\n"}
{"id": "43774420", "url": "https://en.wikipedia.org/wiki?curid=43774420", "title": "Mostowski model", "text": "Mostowski model\n\nIn mathematical set theory, the Mostowski model is a model of set theory with atoms where the full axiom of choice fails, but every set can be linearly ordered. It was introduced by . The Mostowski model can be constructed as the permutation model corresponding to the group of all automorphisms of the ordered set of rational numbers and the ideal of finite subsets of the rational numbers.\n"}
{"id": "11396581", "url": "https://en.wikipedia.org/wiki?curid=11396581", "title": "Nodoid", "text": "Nodoid\n\nIn differential geometry, a nodoid is a surface of revolution with constant nonzero mean curvature obtained by rolling a hyperbola along a fixed line, tracing the focus, and revolving the resulting nodary curve around the line.\n\n"}
{"id": "758718", "url": "https://en.wikipedia.org/wiki?curid=758718", "title": "Noncototient", "text": "Noncototient\n\nIn mathematics, a noncototient is a positive integer \"n\" that cannot be expressed as the difference between a positive integer \"m\" and the number of coprime integers below it. That is, \"m\" − φ(\"m\") = \"n\", where φ stands for Euler's totient function, has no solution for \"m\". The \"cototient\" of \"n\" is defined as \"n\" − φ(\"n\"), so a noncototient is a number that is never a cototient.\n\nIt is conjectured that all noncototients are even. This follows from a modified form of the slightly stronger version of the Goldbach conjecture: if the even number \"n\" can be represented as a sum of two distinct primes \"p\" and \"q,\" then\n\nIt is expected that every even number larger than 6 is a sum of two distinct primes, so probably no odd number larger than 5 is a noncototient. The remaining odd numbers are covered by the observations formula_2 and formula_3.\n\nFor even numbers, it can be shown\n\nThus, all even numbers \"n\" such that \"n\"+2 can be written as (p+1)*(q+1) with \"p\", \"q\" primes are cototients.\n\nThe first few noncototients are\n\nThe cototient of \"n\" are\n\nLeast \"k\" such that the cototient of \"k\" is \"n\" are (start with \"n\" = 0, 0 if no such \"k\" exists)\n\nGreatest \"k\" such that the cototient of \"k\" is \"n\" are (start with \"n\" = 0, 0 if no such \"k\" exists)\n\nNumber of \"k\"s such that \"k\"-φ(\"k\") is \"n\" are (start with \"n\" = 0)\n\nErdős (1913-1996) and Sierpinski (1882-1969) asked whether there exist infinitely many noncototients. This was finally answered in the affirmative by Browkin and Schinzel (1995), who showed every member of the infinite family formula_5 is an example (See Riesel number). Since then other infinite families, of roughly the same form, have been given by Flammenkamp and Luca (2000).\n\n\n"}
{"id": "1254566", "url": "https://en.wikipedia.org/wiki?curid=1254566", "title": "Numerical differentiation", "text": "Numerical differentiation\n\nIn numerical analysis, numerical differentiation describes algorithms for estimating the derivative of a mathematical function or function subroutine using values of the function and perhaps other knowledge about the function.\n\nThe simplest method is to use finite difference approximations.\n\nA simple two-point estimation is to compute the slope of a nearby secant line through the points (\"x\",\"f(x)\") and (\"x+h\",\"f(x+h)\"). Choosing a small number \"h\", \"h\" represents a small change in \"x\", and it can be either positive or negative. The slope of this line is\nThis expression is Newton's difference quotient (also known as a first-order divided difference.)\n\nThe slope of this secant line differs from the slope of the tangent line by an amount that is approximately proportional to \"h\". As \"h\" approaches zero, the slope of the secant line approaches the slope of the tangent line. Therefore, the true derivative of f at x is the limit of the value of the difference quotient as the secant lines get closer and closer to being a tangent line:\n\nSince immediately substituting 0 for \"h\" results in division by zero, calculating the derivative directly can be unintuitive.\n\nEquivalently, the slope could be estimated by employing positions (x - h) and x.\n\nAnother two-point formula is to compute the slope of a nearby secant line through the points (\"x-h\",\"f(x-h)\") and (\"x+h\",\"f(x+h)\"). The slope of this line is\n\nThis formula is known as the symmetric difference quotient. In this case the first-order errors cancel, so the slope of these secant lines differ from the slope of the tangent line by an amount that is approximately proportional to formula_4. Hence for small values of \"h\" this is a more accurate approximation to the tangent line than the one-sided estimation. Note however that although the slope is being computed at x, the value of the function at x is not involved.\n\nThe estimation error is given by:\n\nwhere formula_6 is some point between formula_7 and formula_8.\nThis error does not include the rounding error due to numbers being represented and calculations being performed in limited precision.\n\nThe symmetric difference quotient is employed as the method of approximating the derivative in a number of calculators, including TI-82, TI-83, TI-84, TI-85 all of which use this method with \"h\"=0.001.\n\nAn important consideration in practice when the function is calculated using floating point arithmetic is how small a value of \"h\" to choose. If chosen too small, the subtraction will yield a large rounding error. In fact all the finite difference formulae are ill-conditioned and due to cancellation will produce a value of zero if \"h\" is small enough. If too large, the calculation of the slope of the secant line will be more accurately calculated, but the estimate of the slope of the tangent by using the secant could be worse.\n\nA choice for \"h\" which is small without producing a large rounding error is formula_9 (though not when \"x\" = 0) where the machine epsilon \"ε\" is typically of the order of 2.2×10.\n\nformula_10\n\n(though not when f\"(x) = 0) and to employ it will require knowledge of the function.\n\nThis epsilon is for double precision (64-bit) variables: such calculations in single precision are rarely useful. The resulting value is unlikely to be a \"round\" number in binary, so it is important to realise that although \"x\" is a machine-representable number, \"x\" + \"h\" almost certainly will not be. This means that \"x\" + \"h\" will be changed (via rounding or truncation) to a nearby machine-representable number, with the consequence that (\"x\" + \"h\") - \"x\" will \"not\" equal \"h\"; the two function evaluations will not be exactly \"h\" apart. In this regard, since most decimal fractions are recurring sequences in binary (just as 1/3 is in decimal) a seemingly round step such as \"h\" = 0.1 will not be a round number in binary; it is 0.000110011001100... A possible approach is as follows:\nHowever, with computers, compiler optimization facilities may fail to attend to the details of actual computer arithmetic, and instead apply the axioms of mathematics to deduce that \"dx\" and \"h\" are the same. With C and similar languages, a directive that \"xph\" is a volatile variable will prevent this.\n\nHigher-order methods for approximating the derivative, as well as methods for higher derivatives exist.\n\nGiven below is the five point method for the first derivative (five-point stencil in one dimension).\nwhere formula_12.\n\nFor other stencil configurations and derivative orders, the Finite Difference Coefficients Calculator is a tool which can be used to generate derivative approximation methods for any stencil with any derivative order (provided a solution exists).\n\nUsing Newton's difference quotient,\n\nformula_13\n\nthe following can be shown (for positive n):\n\nformula_14\n\nDifferential quadrature is the approximation of derivatives by using weighted sums of function values. The name is in analogy with \"quadrature\" meaning Numerical integration where weighted sums are used in methods such as Simpson's method or the Trapezium rule. There are various methods for determining the weight coefficients. Differential quadrature is used to solve partial differential equations.\n\nThe classical finite difference approximations for numerical differentiation are ill-conditioned. However, if formula_15 is a holomorphic function, real-valued on the real line, which can be evaluated at points in the complex plane near formula_16 then there are stable methods. For example, the first derivative can be calculated by the complex-step derivative formula:\n\nThe above formula is only valid for calculating a first-order derivative. A generalization of the above for calculating derivatives of any order derivatives employ multicomplex numbers, resulting in multicomplex derivatives.\n\nIn general, derivatives of any order can be calculated using Cauchy's integral formula:\nwhere the integration is done numerically.\n\nUsing complex variables for numerical differentiation was started by Lyness and Moler in 1967. A method based on numerical inversion of a complex Laplace transform was developed by Abate and Dubner. An algorithm which can be used without requiring knowledge about the method or the character of the function was developed by Fornberg.\n\n\n"}
{"id": "44481226", "url": "https://en.wikipedia.org/wiki?curid=44481226", "title": "Ostrogradsky instability", "text": "Ostrogradsky instability\n\nIn applied mathematics, the Ostrogradsky instability is a consequence of a theorem of Mikhail Ostrogradsky in classical mechanics according to which a non-degenerate Lagrangian dependent on time derivatives higher than the first corresponds to a linearly unstable Hamiltonian associated with the Lagrangian via a Legendre transform. The Ostrogradsky instability has been proposed as an explanation as to why no differential equations of higher order than two appear to describe physical phenomena.\n\nThe main points of the proof can be made clearer by considering a one-dimensional system with a Lagrangian formula_1. The Euler–Lagrange equation is\n\nNon-degeneracy of formula_3 means that the canonical coordinates can be expressed in terms of the derivatives of formula_4 and vice versa. Thus, formula_5 is a function of formula_6 (if it was not, the Jacobian formula_7 would vanish, which would mean that formula_3 is degenerate), meaning that we can write formula_9 or, inverting, formula_10. Since the evolution of formula_11 depends upon four initial parameters, this means that there are four canonical coordinates. We can write those as\n\nand by using the definition of the conjugate momentum,\n\nDue to non-degeneracy, we can write formula_16 as formula_17. Note that only \"three\" arguments are needed since the Lagrangian itself only has three free parameters. By Legendre transforming, we find the Hamiltonian to be\n\nWe now notice that the Hamiltonian is linear in formula_19. This is Ostrogradsky's instability, and it stems from the fact that the Lagrangian depends on fewer coordinates than there are canonical coordinates (which correspond to the initial parameters needed to specify the problem). The extension to higher dimensional systems is analogous, and the extension to higher derivatives simply means that the phase space is of even higher dimension than the configuration space, which exacerbates the instability (since the Hamiltonian is linear in even more canonical coordinates).\n"}
{"id": "26179254", "url": "https://en.wikipedia.org/wiki?curid=26179254", "title": "Pentagram map", "text": "Pentagram map\n\nIn mathematics, the pentagram map is a discrete dynamical system on the moduli space of polygons in the projective plane. The pentagram map takes a given polygon, finds the intersections of the shortest diagonals of the polygon, and constructs a new polygon from these intersections. Richard Schwartz introduced the pentagram map for a general polygon in a 1992 paper though it seems that the special case, in which the map is defined for pentagons only, goes back to an 1871 paper of Alfred Clebsch and a 1945 paper of Theodore Motzkin. The pentagram map is similar in spirit to the constructions underlying Desargues' theorem and Poncelet's porism. It echoes the rationale and construction underlying a conjecture of Branko Grünbaum concerning the diagonals of a polygon. \n\nSuppose that the vertices of the polygon P are given by formula_1 The image of \"P\" under the pentagram map is the polygon \"Q\" with vertices formula_2 as shown in the figure. Here formula_3 is the intersection of the diagonals formula_4 and formula_5, and so on.\nOn a basic level, one can think of the pentagram map as an operation defined on convex polygons in the plane. From a more sophisticated point of view, the pentagram map is defined for a polygon contained in the projective plane over a field provided that the vertices are in sufficiently general position. The pentagram map commutes with projective transformations and thereby induces a mapping on the moduli space of projective equivalence classes of polygons.\n\nThe map formula_6 is slightly problematic, in the sense that the indices of the \"P\"-vertices are naturally odd integers whereas the indices of \"Q\"-vertices are naturally even integers. A more conventional approach to the labeling would be to label the vertices of P and Q by integers of the same parity. One can arrange this either by adding or subtracting 1 from each of the indices of the \"Q\"-vertices. Either choice is equally canonical. An even more conventional choice would be to label the vertices of \"P\" and \"Q\" by consecutive integers, but again there are two natural choices for how to align these labellings: Either formula_7 is just clockwise from formula_8 or just counterclockwise. In most papers on the subject, some choice is made once and for all at the beginning of the paper and then the formulas are tuned to that choice.\n\nThere is a perfectly natural way to label the vertices of the second iterate of the pentagram map by consecutive integers. For this reason, the second iterate of the pentagram map is more naturally considered as an iteration defined on labeled polygons. See the figure.\n\nThe pentagram map is also defined on the larger space of twisted polygons.\n\nA twisted \"N\"-gon is a bi-infinite sequence of points in the projective plane that is \"N\"-periodic modulo a projective transformation That is, some projective transformation \"M\" carries formula_8 to formula_10 for all \"k\". The map \"M\" is called the monodromy of the twisted \"N\"-gon. When \"M\" is the identity, a twisted \"N\"-gon can be interpreted as an ordinary \"N\"-gon whose vertices have been listed out repeatedly. Thus, a twisted \"N\"-gon is a generalization of an ordinary \"N\"-gon.\n\nTwo twisted \"N\"-gons are equivalent if a projective transformation carries one to the other. The moduli space of twisted \"N\"-gons is the set of equivalence classes of twisted \"N\"-gons. The space of twisted \"N\"-gons contains the space of ordinary \"N\"-gons as a sub-variety of co-dimension 8.\n\nThe pentagram map is the identity on the moduli space of pentagons. This is to say that there is always a projective transformation carrying a pentagon to its image under the pentagram map.\n\nThe map formula_11 is the identity on the space of labeled hexagons. Here \"T\" is the second iterate of the pentagram map, which acts naturally on labeled hexagons, as described above. This is to say that the hexagons formula_12 and formula_13 are equivalent by a label-preserving projective transformation. More precisely, the hexagons formula_14 and formula_15 are projectively equivalent, where formula_14 is the labeled hexagon obtained from formula_12 by shifting the labels by 3. See the figure. It seems entirely possible that this fact was also known in the 19th century.\nThe action of the pentagram map on pentagons and hexagons is similar in spirit to classical configuration theorems in projective geometry such as Pascal's theorem, Desargues's theorem and others. \n\nThe iterates of the pentagram map shrink any convex polygon exponentially fast to a point. This is to say that the diameter of the nth iterate of a convex polygon is less than formula_18 for constants formula_19 and formula_20 such that formula_21 So, each point formula_22, satisfying the constraints just mentioned, indexes a triangle (up to scale). One might say that formula_22 are coordinates for the moduli space of scale equivalence classes of triangles. If you want to index all possible quadrilaterals, either up to scale or not, you would need some additional parameters. This would lead to a higher-dimensional moduli space. The moduli space relevant to the pentagram map is the moduli space of projective equivalence classes of polygons. Each point in this space corresponds to a polygon, except that two polygons which are different views of each other are considered the same. Since the pentagram map is adapted to projective geometry, as mentioned above, it induces a mapping on this particular moduli space. That is, given any point in the moduli space, you can apply the pentagram map to the corresponding polygon and see what new point you get.\n\nThe reason for considering what the pentagram map does to the moduli space is that it gives more salient features of the map. If you just watch, geometrically, what happens to an individual polygon, say a convex polygon, then repeated application shrinks the polygon to a point. To see things more clearly, you might dilate the shrinking family of polygons so that they all have, say, the same area. If you do this, then typically you will see that the family of polygons gets long and thin. Now you can change the aspect ratio so as to try to get yet a better view of these polygons. If you do this process as systematically as possible, you find that you are simply looking at what happens to points in the moduli space. The attempts to zoom in to the picture in the most perceptive possible way lead to the introduction of the moduli space.\n\nTo explain how the pentagram map acts on the moduli space, one must say a few words about the torus. One way to roughly define the torus is to say that it is the surface of an idealized donut. Another way is that it is the playing field for the Asteroids video game. Yet another way to describe the torus is to say that it is a computer screen with wrap, both left-to-right and up-to-down. The torus is a classical example of what is known in mathematics as a manifold. This is a space that looks somewhat like ordinary Euclidean space at each point, but somehow is hooked together differently. A sphere is another example of a manifold. This is why it took people so long to figure out that the Earth was not flat; on small scales one cannot easily distinguish a sphere from a plane. So, too, with manifolds like the torus. There are higher-dimensional tori as well. You could imagine playing Asteroids in your room, where you can freely go through the walls and ceiling/floor, popping out on the opposite side.\n\nOne can do experiments with the pentagram map, where one looks at how this mapping acts on the moduli space of polygons. One starts with a point and just traces what happens to it as the map is applied over and over again. One sees a surprising thing: These points seem to line up along multi-dimensional tori. These invisible tori fill up the moduli space somewhat like the way the layers of an onion fill up the onion itself, or how the individual cards in a deck fill up the deck. The technical statement is that the tori make a foliation of the moduli space. The tori have half the dimension of the moduli space. For instance, the moduli space of formula_24-gons is formula_25 dimensional and the tori in this case are formula_26 dimensional.\n\nThe tori are invisible subsets of the moduli space. They are only revealed when one does the pentagram map and watches a point move round and round, filling up one of the tori. Roughly speaking, when dynamical systems have these invariant tori, they are called integrable systems. Most of the results in this article have to do with establishing that the pentagram map is an integrable system, that these tori really exist. The monodromy invariants, discussed below, turn out to be the equations for the tori. The Poisson bracket, discussed below, is a more sophisticated math gadget that sort of encodes the local geometry of the tori. What is nice is that the various objects fit together exactly, and together add up to a proof that this torus motion really exists.\n\nWhen the field underlying all the constructions is \"F\", the affine line is just a copy of \"F\". The affine line is a subset of the projective line. Any finite list of points in the projective line can be moved into the affine line by a suitable projective transformation.\n\nGiven the four points formula_27 in the affine line one defines the (inverse) cross ratio\n\nMost authors consider 1/\"X\" to be the cross-ratio, and that is why \"X\" is called the inverse cross ratio. The inverse cross ratio is invariant under projective transformations and thus makes sense for points in the projective line. However, the formula above only makes sense for points in the affine line.\n\nIn the slightly more general set-up below, the cross ratio makes sense for any four collinear points in projective space One just identifies the line containing the points with the projective line by a suitable projective transformation and then uses the formula above. The result is independent of any choices made in the identification. The inverse cross ratio is used in order to define a coordinate system on the moduli space of polygons, both ordinary and twisted.\n\nThe corner invariants are basic coordinates on the space of twisted polygons. Suppose that P is a polygon. A flag of \"P\" is a pair (\"p\",\"L\"), where \"p\" is a vertex of \"P\" and \"L\" is an adjacent line of \"P\". Each vertex of \"P\" is involved in two flags, and likewise each edge of \"P\" is involved in two flags. The flags of \"P\" are ordered according to the orientation of \"P\", as shown in the figure. In this figure, a flag is represented by a thick arrow. Thus, there are 2\"N\" flags associated to an N-gon.\n\nLet \"P\" be an \"N\"-gon, with flags formula_29 To each flag F, we associate the inverse cross ratio of the points formula_30 shown in the figure at left. In this way, one associates numbers formula_31 to an n-gon. If two n-gons are related by a projective transformation, they get the same coordinates. Sometimes the variables formula_32 are used in place of formula_33\n\nThe corner invariants make sense on the moduli space of twisted polygons. When one defines the corner invariants of a twisted polygon, one obtains a 2\"N\"-periodic bi-infinite sequence of numbers. Taking one period of this sequence identifies a twisted \"N\"-gon with a point in formula_34 where \"F\" is the underlying field. Conversely, given almost any (in the sense of measure theory) point in formula_34 one can construct a twisted \"N\"-gon having this list of corner invariants. Such a list will not always give rise to an ordinary polygon; there are an additional 8 equations which the list must satisfy for it to give rise to an ordinary \"N\"-gon.\n\nThere is a second set of coordinates for the moduli space of twisted polygons, developed by Sergei Tabachnikov and Valentin Ovsienko. One describes a polygon in the projective plane by a sequence of vectors formula_36 in formula_37 so that each consecutive triple of vectors spans a parallelopiped having unit volume. This leads to the relation\n\nThe coordinates formula_39 serve as coordinates for the moduli space of twisted \"N\"-gons as long as \"N\" is not divisible by 3.\n\nThe (ab) coordinates bring out the close analogy between twisted polygons and solutions of 3rd order linear ordinary differential equations, normalized to have unit Wronskian.\n\nHere is a formula for the pentagram map, expressed in corner coordinates. The equations work more gracefully when one considers the second iterate of the pentagram map, thanks to the canonical labelling scheme discussed above. The second iterate of the pentagram map is the composition formula_40. The maps formula_41 and formula_42 are birational mappings of order 2, and have the following action.\nwhere\n\n(Note: the index 2\"k\" + 0 is just 2\"k\". The 0 is added to align the formulas.) In these coordinates, the pentagram map is a birational mapping of formula_34\n\nThe formula for the pentagram map has a convenient interpretation as a certain compatibility rule for labelings on the edges of triangular grid, as shown in the figure. In this interpretation, the corner invariants of a polygon P label the non-horizontal edges of a single row, and then the non-horizontal edges of subsequent rows are labeled by the corner invariants of formula_47, formula_48, formula_49, and so forth. the compatibility rules are\nThese rules are meant to hold for all configurations which are congruent to the ones shown in the figure. In other words, the figures involved in the relations can be in all possible positions and orientations. The labels on the horizontal edges are simply auxiliary variables introduced to make the formulas simpler. Once a single row of non-horizontal edges is provided, the remaining rows are uniquely determined by the compatibility rules. \n\nIt follows directly from the formula for the pentagram map, in terms of corner coordinates, that the two quantities\nare invariant under the pentagram map. This observation is closely related to the 1991 paper of Joseph Zaks concerning the diagonals of a polygon.\n\nWhen \"N\" = 2\"k\" is even, the functions\nare likewise seen, directly from the formula, to be invariant functions. All these products turn out to be Casimir invariants with respect to the invariant Poisson bracket discussed below. At the same time, the functions formula_56 and formula_57 are the simplest examples of the monodromy invariants defined below.\n\nThe level sets of the function formula_58 are compact, when f is restricted to the moduli space of real convex polygons. Hence, each orbit of the pentagram map acting on this space has a compact closure.\n\nThe pentagram map, when acting on the moduli space \"X\" of convex polygons, has an invariant volume form. At the same time, as was already mentioned, the function formula_59 has compact level sets on \"X\". These two properties combine with the Poincaré recurrence theorem to imply that the action of the pentagram map on \"X\" is recurrent: The orbit of almost any equivalence class of convex polygon \"P\" returns infinitely often to every neighborhood of \"P\". This is to say that, modulo projective transformations, one typically sees nearly the same shape, over and over again, as one iterates the pentagram map. (It is important to remember that one is considering the projective equivalence classes of convex polygons. The fact that the pentagram map visibly shrinks a convex polygon is irrelevant.)\n\nIt is worth mentioning that the recurrence result is subsumed by the complete integrability results discussed below.\n\nThe so-called monodromy invariants are a collection of functions on the moduli space that are invariant under the pentagram map. \n\nWith a view towards defining the monodromy invariants, say that a block is either a single integer or a triple of consecutive integers, for instance 1 and 567. Say that a block is odd if it starts with an odd integer. Say that two blocks are well-separated if they have at least 3 integers between them. For instance 123 and 567 are not well separated but 123 and 789 are well separated. Say that an odd admissible sequence is a finite sequence of integers that decomposes into well separated odd blocks. When we take these sequences from the set 1, ..., 2\"N\", the notion of well separation is meant in the cyclic sense. Thus, 1 and 2\"N\" − 1 are not well separated.\n\nEach odd admissible sequence gives rise to a monomial in the corner invariants. This is best illustrated by example\nThe sign is determined by the parity of the number of single-digit blocks in the sequence. The monodromy invariant formula_56 is defined as the sum of all monomials coming from odd admissible sequences composed of k blocks. The monodromy invariant formula_57 is defined the same way, with even replacing odd in the definition.\n\nWhen \"N\" is odd, the allowable values of \"k\" are 1, 2, ..., (\"n\" − 1)/2. When \"N\" is even, the allowable values of k are 1, 2, ..., \"n\"/2. When \"k\" = \"n\"/2, one recovers the product invariants discussed above. In both cases, the invariants formula_64 and formula_65 are counted as monodromy invariants, even though they are not produced by the above construction.\n\nThe monodromy invariants are defined on the space of twisted polygons, and restrict to give invariants on the space of closed polygons. They have the following geometric interpretation. The monodromy M of a twisted polygon is a certain rational function in the corner coordinates. The monodromy invariants are essentially the homogeneous parts of the trace of \"M\". There is also a description of the monodromy invariants in terms of the (ab) coordinates. In these coordinates, the invariants arise as certain determinants of 4-diagonal matrices. \n\nWhenever \"P\" has all its vertices on a conic section (such as a circle) one has formula_66 for all \"k\". \n\nA Poisson bracket is an anti-symmetric linear operator formula_67 on the space of functions which satisfies the Leibniz Identity and the Jacobi identity. In a 2010 paper, Valentin Ovsienko, Richard Schwartz and Sergei Tabachnikov produced a Poisson bracket on the space of twisted polygons which is invariant under the pentagram map. They also showed that monodromy invariants commute with respect to this bracket. This is to say that\n\nfor all indices.\n\nHere is a description of the invariant Poisson bracket in terms of the variables.\nThere is also a description in terms of the (ab) coordinates, but it is more complicated.\n\nHere is an alternate description of the invariant bracket. Given any function formula_76 on the moduli space, we have the so-called Hamiltonian vector field\n\nwhere a summation over the repeated indices is understood. Then\n\nThe first expression is the directional derivative of formula_79 in the direction of the vector field formula_80. In practical terms, the fact that the monodromy invariants Poisson-commute means that the corresponding Hamiltonian vector fields define commuting flows.\n\nThe monodromy invariants and the invariant bracket combine to establish Arnold–Liouville integrability of the pentagram map on the space of twisted \"N\"-gons. The situation is easier to describe for N odd. In this case, the two products\nare Casimir invariants for the bracket, meaning (in this context) that\nfor all functions f. A Casimir level set is the set of all points in the space having a specified value for both formula_84 and formula_85.\n\nEach Casimir level set has an iso-monodromy foliation, namely, a decomposition into the common level sets of the remaining monodromy functions. The Hamiltonian vector fields associated to the remaining monodromy invariants generically span the tangent distribution to the iso-monodromy foliation. The fact that the monodromy invariants Poisson-commute means that these vector fields define commuting flows. These flows in turn define local coordinate charts on each iso-monodromy level such that the transition maps are Euclidean translations. That is, the Hamiltonian vector fields impart a flat Euclidean structure on the iso-monodromy levels, forcing them to be flat tori when they are smooth and compact manifolds. This happens for almost every level set. Since everything in sight is pentagram-invariant, the pentagram map, restricted to an iso-monodromy leaf, must be a translation. This kind of motion is known as quasi-periodic motion. This explains the Arnold-Liouville integrability.\n\nFrom the point of view of symplectic geometry, the Poisson bracket gives rise to a symplectic form on each Casimir level set.\n\nIn a 2011 preprint, Fedor Soloviev showed that the pentagram map has a Lax representation with a spectral parameter, and proved its algebraic-geometric integrability. This means that the space of polygons (either twisted or ordinary) is parametrized in terms of a spectral curve with marked points and a divisor. The spectral curve is determined by the monodromy invariants, and the divisor corresponds to a point on a torus—the Jacobi variety of the spectral curve. The algebraic-geometric methods guarantee that the pentagram map exhibits quasi-periodic motion on a torus (both in the twisted and the ordinary case), and they allow one to construct explicit solutions formulas using Riemann theta functions (i.e., the variables that determine the polygon as explicit functions of time). Soloviev also obtains the invariant Poisson bracket from the Krichever–Phong universal formula.\n\nThe octahedral recurrence is a dynamical system defined on the vertices of the octahedral tiling of space. Each octahedron has 6 vertices, and these vertices are labelled in such a way that\nHere formula_87 and formula_88 are the labels of antipodal vertices. A common convention is that formula_89 always lie in a central horizontal plane and a_1,b_1 are the top and bottom vertices. The octahedral recurrence is closely related to C. L. Dodgson's method of condensation for computing determinants. Typically one labels two horizontal layers of the tiling and then uses the basic rule to let the labels propagate dynamically.\n\nMax Glick used the cluster algebra formalism to find formulas for the iterates of the pentagram map in terms of alternating sign matrices. These formulas are similar in spirit to the formulas found by David P. Robbins and Harold Rumsey for the iterates of the octahedral recurrence.\nAlternatively, the following construction relates the octahedral recurrence directly to the pentagram map. Let formula_90 be the octahedral tiling. Let formula_91 be the linear projection which maps each octahedron in formula_90 to the configuration of 6 points shown in the first figure. Say that an adapted labeling of formula_90 is a labeling so that all points in the (infinite) inverse image of any point in formula_94 get the same numerical label. The octahedral recurrence applied to an adapted labeling is the same as a recurrence on formula_95 in which the same rule as for the octahedral recurrence is applied to every configuration of points congruent to the configuration in the first figure. Call this the planar octahedral recurrence.\nGiven a labeling of formula_95 which obeys the planar octahedral recurrence, one can create a labeling of the edges of formula_95 by applying the rule\n\nto every edge. This rule refers to the figure at right and is meant to apply to every configuration that is congruent to the two shown. \nWhen this labeling is done, the edge-labeling of G satisfies the relations for the pentagram map.\n\nThe continuous limit of a convex polygon is a parametrized convex curve in the plane. When the time parameter is suitably chosen, the continuous limit of the pentagram map is the classical Boussinesq equation. This equation is a classical example of an integrable partial differential equation.\n\nHere is a description of the geometric action of the Boussinesq equation. Given a locally convex curve formula_99, and real numbers x and t, we consider the chord connecting formula_100 to formula_101. The envelop of all these chords is a new curve formula_102. When t is extremely small, the curve formula_102 is a good model for the time t evolution of the original curve formula_104 under the Boussinesq equation. This geometric description makes it fairly obvious that the B-equation is the continuous limit of the pentagram map. At the same time, the pentagram invariant bracket is a discretization of a well known invariant Poisson bracket associated to the Boussinesq equation. \n\nRecently, there has been some work on higher-dimensional generalizations of the pentagram map and its connections to Boussinesq-type partial differential equations \n\nThe pentagram map and the Boussinesq equation are examples of projectively natural geometric evolution equations. Such equations arise in diverse fields of mathematics, such as projective geometry and computer vision. \n\nIn a 2010 paper Max Glick identified the pentagram map as a special case of a cluster algebra.\n\n\n"}
{"id": "16869761", "url": "https://en.wikipedia.org/wiki?curid=16869761", "title": "Process-data diagram", "text": "Process-data diagram\n\nA process-data diagram (PDD), also known as process-deliverable diagram is a diagram that describes processes and data that act as output of these processes. On the left side the meta-process model can be viewed and on the right side the meta-data model can be viewed.\n\nA process-data diagram can be seen as combination of a business process model and data model.\n\nThe process-data diagram that is depicted at the right, gives an overview of all of these activities/processes and deliverables. The four gray boxes depict the four main implementation phases, which each contain several processes that are in this case all sequential. The boxes at the right show all the deliverables/concepts that result from the processes. Boxes without a shadow have no further sub-concepts. Boxes with a black shadow depict complex closed concepts, so concepts that have sub-concepts, which however will not be described in any more detail. Boxes with a white shadow (a box behind it) depict open closed concepts, where the sub-concepts are expanded in greater detail. The lines with diamonds show a has-a relationship between concepts.\n\nThe SAP Implementation process is made up out of four main phases, i.e. the project preparation where a vision of the future-state of the SAP solution is being created, a sizing and blueprinting phase where a software stack is acquired and training is being performuued, a functional development phase and finally a final preparation phase, when the last tests are being performed before the actual go live. For each phase, the vital activities are addressed and the deliverables/products are explained.\n\nSequential activities are activities that need to be carried out in a pre-defined order. The activities are connected with an arrow, implying that they have to be followed in that sequence. Both activities and sub-activities can be modeled in a sequential way. In Figure 1 an activity diagram is illustrated with one activity and two sequential sub-activities. A special kind of sequential activities are the start and stop states, which are also illustrated in Figure 1.\n\nIn Figure 2 an example from practice is illustrated. The example is taken from the requirements capturing workflow in UML-based Web Engineering. The main activity, user & domain modeling, consists of three activities that need to be carried out in a predefined order.\n\nUnordered activities are used when sub-activities of an activity do not have a pre-defined sequence in which they need to be carried out. Only sub-activities can be unordered. Unordered activities are represented as sub-activities without transitions within an activity, as is represented in Figure 3.\n\nSometimes an activity consists of both sequential and unordered sub-activities. The solution to this modeling issue is to divide the main activity in different parts. In Figure 4 an example is illustrated, which clarifies the necessity to be able to model unordered activities. The example is taken from the requirements analysis workflow of the Unified Process. The main activity, “describe candidate requirements”, is divided into two parts. The first part is a sequential activity. The second part consists of four activities that do not need any sequence in order to be carried out correctly.\n\nActivities can occur concurrently. This is handled with forking and joining. By drawing the activities parallel in the diagram, connected with a synchronization bar, one can fork several activities. Later on these concurrent activities can join again by using the same synchronization bar. Both activities and sub-activities can occur concurrently. In the example of Figure 5, Activity 2 and Activity 3 are concurrent activities.\n\nIn Figure 6, a fragment of a requirements capturing process is depicted. Two activities, defining the actors and defining the use cases, are carried out concurrently. The reason for carrying out these activities concurrently is that defining the actors influences the use cases greatly, and vice versa.\n\nConditional activities are activities that are only carried out if a pre-defined condition is met. This is graphically represented by using a branch. Branches are illustrated with a diamond and can have incoming and outgoing transitions. Every outgoing transition has a guard expression, the condition. This guard expression is actually a Boolean expression, used to make a choice which direction to go. Both activities and sub-activities can be modeled as conditional activities. In Figure 7 two conditional activities are illustrated.\n\nIn Figure 8 an example from practice is illustrated. A requirements analysis starts with studying the material. Based on this study, the decision is taken whether to do an extensive requirements elicitation session or not. The condition for not carrying out this requirements session is represented at the left of the branch, namely [requirements clear]. If this condition is not met, [else], the other arrow is followed.\n\nThe integration of both types of diagrams is quite straightforward. Each action or activity results in a concept. They are connected with a dotted arrow to the produced artifacts, as is demonstrated in Figure 9. The concepts and activities are abstract in this picture.\n\nIn Table 1 a generic table is presented with the description of activities, sub-activities and their relations to the concepts. In section 5 examples are given of both process-data diagram and activity table.\n\nIn Figure 10 an example of a process-data diagram is illustrated. It concerns an example from the orientation phase of complex project in a WebEngineering method.\n\nNotable is the use of open and closed concepts. Since project management is actually not within the scope of this research, the concept CONTROL MANAGEMENT has not been expanded. However, in a complex project is RISK MANAGEMENT of great importance. Therefore, the choice is made to expand the RISK MANAGEMENT concept.\n\nIn Table 2 the activities and sub-activities, and relation to the concepts are described.\n\n"}
{"id": "207387", "url": "https://en.wikipedia.org/wiki?curid=207387", "title": "Put–call parity", "text": "Put–call parity\n\nIn financial mathematics, put–call parity defines a relationship between the price of a European call option and European put option, both with the identical strike price and expiry, namely that a portfolio of a long call option and a short put option is equivalent to (and hence has the same value as) a single forward contract at this strike price and expiry. This is because if the price at expiry is above the strike price, the call will be exercised, while if it is below, the put will be exercised, and thus in either case one unit of the asset will be purchased for the strike price, exactly as in a forward contract.\n\nThe validity of this relationship requires that certain assumptions be satisfied; these are specified and the relationship is derived below. In practice transaction costs and financing costs (leverage) mean this relationship will not exactly hold, but in liquid markets the relationship is close to exact.\n\nPut–call parity is a static replication, and thus requires minimal assumptions, namely the existence of a forward contract. In the absence of traded forward contracts, the forward contract can be replaced (indeed, itself replicated) by the ability to buy the underlying asset and finance this by borrowing for fixed term (e.g., borrowing bonds), or conversely to borrow and sell (short) the underlying asset and loan the received money for term, in both cases yielding a self-financing portfolio.\n\nThese assumptions do not require any transactions between the initial date and expiry, and are thus significantly weaker than those of the Black–Scholes model, which requires dynamic replication and continual transaction in the underlying.\n\nReplication assumes one can enter into derivative transactions, which requires leverage (and capital costs to back this), and buying and selling entails transaction costs, notably the bid–ask spread. The relationship thus only holds exactly in an ideal frictionless market with unlimited liquidity. However, real world markets may be sufficiently liquid that the relationship is close to exact, most significantly FX markets in major currencies or major stock indices, in the absence of market turbulence.\n\nPut–call parity can be stated in a number of equivalent ways, most tersely as:\nwhere \"C\" is the (current) value of a call, \"P\" is the (current) value of a put, \"D\" is the discount factor, \"F\" is the forward price of the asset, and \"K\" is the strike price. Note that the spot price is given by formula_2 (spot price is present value, forward price is future value, discount factor relates these). The left side corresponds to a portfolio of long a call and short a put, while the right side corresponds to a forward contract. The assets \"C\" and \"P\" on the left side are given in current values, while the assets \"F\" and \"K\" are given in future values (forward price of asset, and strike price paid at expiry), which the discount factor \"D\" converts to present values.\n\nUsing spot price \"S\" instead of forward price \"F\" yields: \n\nRearranging the terms yields a different interpretation:\nIn this case the left-hand side is a fiduciary call, which is long a call and enough cash (or bonds) to pay the strike price if the call is exercised, while the right-hand side is a protective put, which is long a put and the asset, so the asset can be sold for the strike price if the spot is below strike at expiry. Both sides have payoff \"max\"(\"S\"(\"T\"), \"K\") at expiry (i.e., at least the strike price, or the value of the asset if more), which gives another way of proving or interpreting put–call parity.\n\nIn more detail, this original equation can be stated as:\nwhere\n\nNote that the right-hand side of the equation is also the price of buying a forward contract on the stock with delivery price \"K\". Thus one way to read the equation is that a portfolio that is long a call and short a put is the same as being long a forward. In particular, if the underlying is not tradeable but there exists forwards on it, we can replace the right-hand-side expression by the price of a forward.\n\nIf the bond interest rate, formula_13, is assumed to be constant then\n\nNote: formula_13 refers to the force of interest, which is approximately equal to the effective annual rate for small interest rates. However, one should take care with the approximation, especially with larger rates and larger time periods. To find formula_13 exactly, use formula_17, where formula_18 is the effective annual interest rate.\n\nWhen valuing European options written on stocks with known dividends that will be paid out during the life of the option, the formula becomes:\n\nwhere D(t) represents the total value of the dividends from one stock share to be paid out over the remaining life of the options, discounted to present value. \nWe can rewrite the equation as:\n\nand note that the right-hand side is the price of a forward contract on the stock with delivery price \"K\", as before.\n\nWe will suppose that the put and call options are on traded stocks, but the underlying can be any other tradeable asset. The ability to buy and sell the underlying is crucial to the \"no arbitrage\" argument below.\n\nFirst, note that under the assumption that there are no arbitrage opportunities (the prices are arbitrage-free), two portfolios that always have the same payoff at time T must have the same value at any prior time. To prove this suppose that, at some time \"t\" before \"T\", one portfolio were cheaper than the other. Then one could purchase (go long) the cheaper portfolio and sell (go short) the more expensive. At time \"T\", our overall portfolio would, for any value of the share price, have zero value (all the assets and liabilities have canceled out). The profit we made at time \"t\" is thus a riskless profit, but this violates our assumption of no arbitrage.\n\nWe will derive the put-call parity relation by creating two portfolios with the same payoffs (static replication) and invoking the above principle (rational pricing).\n\nConsider a call option and a put option with the same strike \"K\" for expiry at the same date \"T\" on some stock \"S\", which pays no dividend. We assume the existence of a bond that pays 1 dollar at maturity time \"T\". The bond price may be random (like the stock) but must equal 1 at maturity.\n\nLet the price of \"S\" be S(t) at time t. Now assemble a portfolio by buying a call option \"C\" and selling a put option \"P\" of the same maturity \"T\" and strike \"K\". The payoff for this portfolio is \"S(T) - K\". Now assemble a second portfolio by buying one share and borrowing \"K\" bonds. Note the payoff of the latter portfolio is also \"S(T) - K\" at time \"T\", since our share bought for \"S(t)\" will be worth \"S(T)\" and the borrowed bonds will be worth \"K\".\n\nBy our preliminary observation that identical payoffs imply that both portfolios must have the same price at a general time formula_7, the following relationship exists between the value of the various instruments:\n\nThus given no arbitrage opportunities, the above relationship, which is known as put-call parity, holds, and for any three prices of the call, put, bond and stock one can compute the implied price of the fourth.\n\nIn the case of dividends, the modified formula can be derived in similar manner to above, but with the modification that one portfolio consists of going long a call, going short a put, and \"D(T)\" bonds that each pay 1 dollar at maturity \"T\" (the bonds will be worth \"D(t)\" at time \"t\"); the other portfolio is the same as before - long one share of stock, short \"K\" bonds that each pay 1 dollar at \"T\". The difference is that at time \"T\", the stock is not only worth \"S(T)\" but has paid out \"D(T)\" in dividends.\n\nForms of put-call parity appeared in practice as early as medieval ages, and was formally described by a number of authors in the early 20th century.\n\nMichael Knoll, in \"The Ancient Roots of Modern Financial Innovation: The Early History of Regulatory Arbitrage\", describes the important role that put-call parity played in developing the equity of redemption, the defining characteristic of a modern mortgage, in Medieval England.\n\nIn the 19th century, financier Russell Sage used put-call parity to create synthetic loans, which had higher interest rates than the usury laws of the time would have normally allowed.\n\nNelson, an option arbitrage trader in New York, published a book: \"The A.B.C. of Options and Arbitrage\" in 1904 that describes the put-call parity in detail. His book was re-discovered by Espen Gaarder Haug in the early 2000s and many references from Nelson's book are given in Haug's book \"Derivatives Models on Models\".\n\nHenry Deutsch describes the put-call parity in 1910 in his book \"Arbitrage in Bullion, Coins, Bills, Stocks, Shares and Options, 2nd Edition\". London: Engham Wilson but in less detail than Nelson (1904).\n\nMathematics professor Vinzenz Bronzin also derives the put-call parity in 1908 and uses it as part of his arbitrage argument to develop a series of mathematical option models under a series of different distributions. The work of professor Bronzin was just recently rediscovered by professor Wolfgang Hafner and professor Heinz Zimmermann. The original work of Bronzin is a book written in German and is now translated and published in English in an edited work by Hafner and Zimmermann (\"Vinzenz Bronzin's option pricing models\", Springer Verlag).\n\nIts first description in the modern academic literature appears to be by Hans R. Stoll in the \"Journal of Finance\". \n\nPut–call parity implies:\n\n\n\n"}
{"id": "2657905", "url": "https://en.wikipedia.org/wiki?curid=2657905", "title": "Quasiperiodic function", "text": "Quasiperiodic function\n\nIn mathematics, a quasiperiodic function is a function that has a certain similarity to a periodic function. A function formula_1 is quasiperiodic with quasiperiod formula_2 if formula_3, where formula_4 is a \"\"simpler\" function than formula_1. What it means to be \"simpler\"\" is vague.\n\nA simple case (sometimes called arithmetic quasiperiodic) is if the function obeys the equation:\n\nAnother case (sometimes called geometric quasiperiodic) is if the function obeys the equation:\n\nAn example of this is the Jacobi theta function, where\n\nshows that for fixed \"τ\" it has quasiperiod \"τ\"; it also is periodic with period one. Another example is provided by the Weierstrass sigma function, which is quasiperiodic in two independent quasiperiods, the periods of the corresponding Weierstrass \"℘\" function.\n\nFunctions with an additive functional equation\n\nare also called quasiperiodic. An example of this is the Weierstrass zeta function, where\n\nfor a \"z\"-independent η when ω is a period of the corresponding Weierstrass ℘ function.\n\nIn the special case where formula_11 we say \"f\" is periodic with period ω in the period lattice formula_12.\n\nQuasiperiodic signals in the sense of audio processing are not quasiperiodic functions in the sense defined here; instead they have the nature of almost periodic functions and that article should be consulted. The more vague and general notion of quasiperiodicity has even less to do with quasiperiodic functions in the mathematical sense.\n\nA useful example is the function:\n\nIf the ratio \"A\"/\"B\" is rational, this will have a true period, but if \"A\"/\"B\" is irrational there is no true period, but a succession of increasingly accurate \"almost\" periods.\n\n\n"}
{"id": "49400436", "url": "https://en.wikipedia.org/wiki?curid=49400436", "title": "Riemannian metric and Lie bracket in computational anatomy", "text": "Riemannian metric and Lie bracket in computational anatomy\n\nComputational anatomy (CA) is the study of shape and form in medical imaging. The study of deformable shapes in computational anatomy rely on high-dimensional diffeomorphism groups formula_1 which generate orbits of the form formula_2. In CA, this orbit is in general considered a smooth Riemannian manifold\nsince at every point of the manifold formula_3 there is an inner product inducing the norm formula_4 on the tangent space\nthat varies smoothly from point to point in the manifold of shapes formula_3. This is generated by viewing the\ngroup of diffeomorphisms formula_1 as a Riemannian manifold with formula_7, associated to the tangent space at formula_8 . This induces the norm and metric on the orbit formula_3 under the action from the group of diffeomorphisms.\n\nThe diffeomorphisms in computational anatomy are generated to satisfy the Lagrangian and Eulerian specification of the flow fields, formula_10, generated via the ordinary differential equation\nwith the Eulerian vector fields formula_11 in formula_12 for formula_13, with the inverse for the flow given by\nand the formula_14 Jacobian matrix for flows in formula_15 given as formula_16\n\nTo ensure smooth flows of diffeomorphisms with inverse, the vector fields formula_12 must be at least 1-time continuously differentiable in space which are modelled as elements of the Hilbert space formula_18 using the Sobolev embedding theorems so that each element formula_19 has 3-square-integrable derivatives thusly implies formula_18 embeds smoothly in 1-time continuously differentiable functions. The diffeomorphism group are flows with vector fields absolutely integrable in Sobolev norm:\n\nShapes in Computational Anatomy (CA) are studied via the use of diffeomorphic mapping for establishing correspondences between anatomical coordinate systems. In this setting, 3-dimensional medical images are modelled as diffemorphic transformations of some exemplar, termed the template formula_21, resulting in the observed images to be elements of the random orbit model of CA. For images these are defined as formula_22, with for charts representing sub-manifolds denoted as formula_23.\n\nThe orbit of shapes and forms in Computational Anatomy are generated by the group actionformula_24. This is made into a Riemannian orbit by introducing a metric associated to each point and associated tangent space. For this a metric is defined on the group which induces the metric on the orbit. Take as the metric for Computational anatomy at each element of the tangent space formula_25 in the group of diffeomorphisms \nwith the vector fields modelled to be in a Hilbert space with the norm in the Hilbert space formula_18. We model formula_28 as a reproducing kernel Hilbert space (RKHS) defined by a 1-1, differential operatorformula_29. For formula_30 a distribution or generalized function, the linear form formula_31 determines the norm:and inner product for formula_32 according to \nwhere the integral is calculated by integration by parts for formula_34 a generalized function formula_35 the dual-space.\nThe differential operator is selected so that the Green's kernel associated to the inverse is sufficiently smooth so that the vector fields support 1-continuous derivative.\n\nThe metric on the group of diffeomorphisms is defined by the distance as defined on pairs of elements in the group of diffeomorphisms according to\n\nThis distance provides a right-invariant metric of diffeomorphometry, invariant to reparameterization of space since for all formula_1,\n\nThe Lie bracket gives the adjustment of the velocity term resulting from a perturbation of the motion in the setting of curved spaces. Using Hamilton's principle of least-action derives the optimizing flows as a critical point for the action integral of the integral of the kinetic energy. The Lie bracket for vector fields in Computational Anatomy was first introduced in Miller, Trouve and Younes. The derivation calculates the perturbation formula_38 on the vector fields\nformula_39 in terms of the derivative in time of the group perturbation adjusted by the correction of the Lie bracket of vector fields in this function setting involving the Jacobian matrix, unlike the matrix group case:\nProof:\nProving Lie bracket of vector fields take a first order perturbation of the flow at point formula_1.\n\nThe Lie bracket gives the first order variation of the vector field with respect to first order variation of the flow.\n\nThe Euler–Lagrange equation can be used to calculate geodesic flows through the group which form the basis for the metric. The action integral for the Lagrangian of the kinetic energy for Hamilton's principle becomes \n\nThe action integral in terms of the vector field corresponds to integrating the kinetic energy\nThe shortest paths geodesic connections in the orbit are defined via Hamilton's Principle of least action requires first order variations of the solutions in the orbits of Computational Anatomy which are based on computing critical points on the metric length or energy of the path.\nThe original derivation of the Euler equation associated to the geodesic flow of diffeomorphisms exploits the was a generalized function equation whenformula_43 is a distribution, or generalized function, take the first order variation of the action integral using the adjoint operator for the Lie bracket () gives for all smooth formula_44, \nUsing the bracket formula_46 and formula_47 gives\nmeaning for all smooth formula_48\nEquation () is the Euler-equation when diffeomorphic shape momentum is a generalized function.\n\nThis equation has been called EPDiff, Euler–Poincare equation for diffeomorphisms and has been studied in the context of fluid mechanics for incompressible fluids with formula_50 metric.\n\nIn the random orbit model of Computational anatomy, the entire flow is reduced to the initial condition which forms the coordinates encoding the diffeomorphism, as well as providing the means of positioning information in the orbit. This was first terms a geodesic positioning system in Miller, Trouve, and Younes. From the initial condition formula_51 then geodesic positioning with respect to the Riemannian metric of Computational anatomy solves for the flow of the Euler–Lagrange equation. Solving the geodesic from the initial condition formula_51 is termed the Riemannian-exponential, a mapping formula_53 at identity to the group.\n\nThe Riemannian exponential satisfies formula_54 for initial condition formula_55, vector field dynamics formula_56, \n\n\nIt is\nextended to the entire group,\nformula_62.\n\nMatching information across coordinate systems is central to computational anatomy. Adding a matching term formula_63 to the action integral of Equation ()\nwhich represents the target endpoint \nThe endpoint term adds a boundary condition for the Euler–Lagrange equation ()\nwhich gives the Euler equation with boundary term. Taking the variation gives\n\nProof: The Proof via variation calculus uses the perturbations from above and classic calculus of variation arguments.\nThe earliest large deformation diffeomorphic metric mapping (LDDMM) algorithms solved matching problems associated to images and registered landmarks. are in a vector spaces. The image matching geodesic equation satisfies the classical dynamical equation with endpoint condition. The necessary conditions for the geodesic for image matching takes the form of the classic Equation () of Euler–Lagrange with boundary condition:\n\n\nThe registered landmark matching problem satisfies the dynamical equation for generalized functions with endpoint condition:\nProof:\n\nThe variation formula_70 requires variation of the inverse formula_71 generalizes the matrix perturbation of the inverse via formula_72 giving \nformula_73\ngiving\n\n"}
{"id": "5984280", "url": "https://en.wikipedia.org/wiki?curid=5984280", "title": "Sign-value notation", "text": "Sign-value notation\n\nA sign-value notation represents numbers by a series of numeric signs that added together equal the number represented. In Roman numerals for example, X means ten and L means fifty. Hence LXXX means eighty (50 + 10 + 10 + 10). There is no need for zero in sign-value notation. Sign-value notation was the pre-historic way of writing numbers and only gradually evolved into place-value notation, also known as positional notation. \n\nWhen pre-historic people wanted to write \"two sheep\" in clay, they could inscribe in clay a picture of two sheep. But this would be impractical when they wanted to write \"twenty sheep\". In Mesopotamia they used small clay tokens to represent a number of a specific commodity, and strung the tokens like beads on a string, which were used for accounting. There was a token for one sheep and a token for ten sheep, and a different token for ten goats, etc. To ensure that nobody could alter the number and type of tokens, they invented a clay envelope shaped like a hollow ball into which the tokens on a string were placed and then baked. If anybody contested the number, they could break open the clay envelope and do a recount. To avoid unnecessary damage to the record, they pressed archaic number signs on the outside of the envelope before it was baked, each sign similar in shape to the tokens they represented. Since there was seldom any need to break open the envelope, the signs on the outside became the first written language for writing numbers in clay, using sign-value notation.\n\n\n\n"}
{"id": "36545943", "url": "https://en.wikipedia.org/wiki?curid=36545943", "title": "Slope number", "text": "Slope number\n\nIn graph drawing and geometric graph theory, the slope number of a graph is the minimum possible number of distinct slopes of edges in a drawing of the graph in which vertices are represented as points in the Euclidean plane and edges are represented as line segments that do not pass through any non-incident vertex.\n\nAlthough closely related problems in discrete geometry had been studied earlier, e.g. by and ,\nthe problem of determining the slope number of a graph was introduced by , who showed that the slope number of an -vertex complete graph is exactly . A drawing with this slope number may be formed by placing the vertices of the graph on a regular polygon.\n\nThe slope number of a graph of maximum degree is clearly at least formula_1, because at most two of the incident edges at a degree- vertex can share a slope. More precisely, the slope number is at least equal to the linear arboricity of the graph, since the edges of a single slope must form a linear forest, and the linear arboricity in turn is at least formula_1.\nThere exist graphs with maximum degree five that have arbitrarily large slope number. However, every graph of maximum degree three has slope number at most four; the result of for the complete graph shows that this is tight. Not every set of four slopes is suitable for drawing all degree-3 graphs: a set of slopes is suitable for this purpose if and only it forms the slopes of the sides and diagonals of a parallelogram. In particular, any degree 3 graph can be drawn so that its edges are either axis-parallel or parallel to the main diagonals of the integer lattice. It is not known whether graphs of maximum degree four have bounded or unbounded slope number.\n\nAs showed, every planar graph has a planar straight-line drawing in which the number of distinct slopes is a function of the degree of the graph. Their proof follows a construction of for bounding the angular resolution of planar graphs as a function of degree, by completing the graph to a maximal planar graph without increasing its degree by more than a constant factor, and applying the circle packing theorem to represent this augmented graph as a collection of tangent circles. If the degree of the initial graph is bounded, the ratio between the radii of adjacent circles in the packing will also be bounded, which in turn implies that using a quadtree to place each graph vertex on a point within its circle will produce slopes that are ratios of small integers. The number of distinct slopes produced by this construction is exponential in the degree of the graph.\n\nIt is NP-complete to determine whether a graph has slope number two. From this, it follows that it is NP-hard to determine the slope number of an arbitrary graph, or to approximate it with an approximation ratio better than 3/2.\n\nIt is also NP-complete to determine whether a planar graph has a planar drawing with slope number two,\nand hard for the existential theory of the reals to determine the minimum slope number of a planar drawing.\n\n"}
{"id": "24140640", "url": "https://en.wikipedia.org/wiki?curid=24140640", "title": "Smoothness (probability theory)", "text": "Smoothness (probability theory)\n\nIn probability theory and statistics, smoothness of a density function is a measure which determines how many times the density function can be differentiated, or equivalently the limiting behavior of distribution’s characteristic function.\n\nFormally, we call the distribution of a random variable \"X\" ordinary smooth of order \"β\" if its characteristic function satisfies\nfor some positive constants \"d\", \"d\", \"β\". The examples of such distributions are gamma, exponential, uniform, etc.\n\nThe distribution is called supersmooth of order \"β\" if its characteristic function satisfies\nfor some positive constants \"d\", \"d\", \"β\", \"γ\" and constants \"β\", \"β\". Such supersmooth distributions have derivatives of all orders. Examples: normal, Cauchy, mixture normal.\n\n"}
{"id": "56072400", "url": "https://en.wikipedia.org/wiki?curid=56072400", "title": "Spin squeezing", "text": "Spin squeezing\n\nSpin squeezing is a quantum process that decreases the variance of one of the angular momentum components in an ensemble of particles with a spin. The quantum states obtained are called spin squeezed states. Such states can be used for quantum metrology, as they can provide a better precision for estimating a rotation angle than classical interferometers.\n\nSpin squeezed states for an ensemble of spins have been defined analogously to squeezed states of a bosonic mode. A quantum state always obeys the Heisenberg uncertainty relation\n\nformula_1\n\nwhere formula_2 are the collective angular momentum components defined as formula_3 and formula_4 are the single particle angular momentum components. The state is spin-squeezed in the formula_5-direction, if the variance of the formula_5-component is smaller than the square root of the right-hand side of the inequality above\n\nformula_7\n\nIt is important that formula_8 is the direction of the mean spin. A different definition was based on using states with a reduced spin-variance for metrology.\n\nSpin squeezed states can be used to estimate a rotation angle with a precision better than the classical or shot-noise limit. In particular, if the almost maximal mean spin points to the formula_8-direction, and the state is spin-squeezed in the formula_5-direction, then it can be used to estimate the rotation angle around the formula_11-axis. For instance, this can be used for magnetometry.\n\nSpin squeezed states can be proven to be entangled based on measuring the spin length and the variance of the spin in an orthogonal direction. Let us define the spin squeezing parameter\n\nformula_12,\n\nwhere formula_13 is the number of the spin-formula_14 particles in the ensemble. Then, if formula_15 is smaller than formula_16 then the state is entangled. It has also been shown that a higher and higher level of multipartite entanglement is needed to achieve a larger and larger degree of spin squeezing.\n\nExperiments have been carried out with cold or even room temperature atomic ensembles. In this case, the atoms do not interact with each other. Hence, in order to entangle them, they make them interact with light which is then measured. A 20 dB (100 times) spin squeezing has been obtained in such a system. Simultaneous spin squeezing of two ensembles, which interact with the same light field, has been used to entangle the two ensembles. Spin squeezing can be enhanced by using cavities.\n\nCold gas experiments have also been carried out with Bose-Einstein Condensates (BEC). In this case, the spin squeezing is due to the interaction between the atoms.\n\nMost experiments have been carried out using only two internal states of the particles, hence, effectively with spin-formula_14 particles. There are also experiments aiming at spin squeezing with particles of a higher spin. Nuclear-electron spin squeezing within the atoms, rather than interatomic spin squeezing, has also been created in room temperature gases.\n\nExperiments with atomic ensembles usually are usually implemented in free-space with Gaussian laser beams. To enhance the spin squeezing effect towards generating non-Gaussian states, which are metrologically useful, the free-space apparatuses are not enough. Cavities and nanophotonic waveguides have been used to enhance the squeezing effect with less atoms. \nFor the waveguide systems, the atom-light coupling and the squeezing effect can be enhanced using the evanescent field near to the waveguides, and the type of atom-light interaction can be controlled by choosing a proper polarization state of the guided input light, the internal state subspace of the atoms and the geometry of the trapping shape. Spin squeezing protocols using nanophotonic waveguides based on the birefringence effect and the Faraday effect have been proposed. By optimizing the optical depth or cooperativity through controlling the geometric factors mentioned above, the Faraday protocol demonstrates that, to enhance the squeezing effect, one needs to find a geometry that generates weaker local electric field at the atom positions. This is counterintuitive, because usually to enhance atom-light coupling, a strong local field is required. But it opens the door to perform very precise measurement with little disruptions to the quantum system, which cannot be simultaneously satisfied with a strong field.\n\nIn entanglement theory, generalized spin squeezing also refers to any criterion that is given with the first and second moments of angular momentum coordinates, and detects entanglement in a quantum state. For a large ensemble of spin-1/2 particles a complete set of such relations have been found, which have been generalized to particles with an arbitrary spin. Apart from detecting entanglement in general, there are relations that detect multipartite entanglement. Some of the generalized spin-squeezing entanglement criteria have also a relation to quantum metrological tasks. For instance, planar squeezed states can be used to measure an unknown rotation angle optimally.\n"}
{"id": "6899692", "url": "https://en.wikipedia.org/wiki?curid=6899692", "title": "Tachytrope", "text": "Tachytrope\n\nA tachytrope is a curve in which the law of the velocity is given. It was first used by American mathematician Benjamin Peirce in \"A System of Analytic Mechanics\", first published in 1855.\n\n"}
{"id": "31646448", "url": "https://en.wikipedia.org/wiki?curid=31646448", "title": "Transcomputational problem", "text": "Transcomputational problem\n\nIn computational complexity theory, a transcomputational problem is a problem that requires processing of more than 10 bits of information. Any number greater than 10 is called a transcomputational number. The number 10, called Bremermann's limit, is, according to Hans-Joachim Bremermann, the total number of bits processed by a hypothetical computer the size of the Earth within a time period equal to the estimated age of the Earth. The term \"transcomputational\" was coined by Bremermann.\n\nExhaustively testing all combinations of an integrated circuit with 309 inputs and 1 output requires testing of a total of 2 combinations of inputs. Since the number 2 is a transcomputational number (that is, a number greater than 10), the problem of testing such a system of integrated circuits is a transcomputational problem. This means that there is no way one can verify the correctness of the circuit for all combinations of inputs through brute force alone.\n\nConsider a \"q\"×\"q\" array of the chessboard type, each square of which can have one of \"k\" colors. Altogether there are \"k\" color patterns, where \"n\" = \"q\". The problem of determining the best classification of the patterns, according to some chosen criterion, may be solved by a search through all possible color patterns. For two colors, such a search becomes transcomputational when the array is 18×18 or larger. For a 10×10 array, the problem becomes transcomputational when there are 9 or more colors.\n\nThis has some relevance in the physiological studies of the retina. The retina contains about a million light-sensitive cells. Even if there were only two possible states for each cell (say, an active state and an inactive state) the processing of the retina as a whole requires processing of more than 10 bits of information. This is far beyond Bremermann's limit.\n\nA system of \"n\" variables, each of which can take \"k\" different states, can have \n\"k\" possible system states. To analyze such a system, a minimum of \"k\" bits of information are to be processed. The problem becomes transcomputational when \"k\" > 10. This happens for the following values of \"k\" and \"n\":\nThe existence of real-world transcomputational problems implies the limitations of computers as data processing tools. This point is best summarized in Bremermann's own words:\n\nIn Douglas Adams's \"The Hitchhiker's Guide to the Galaxy\", Earth is a supercomputer, designed to calculate the question known as the \"Ultimate Question of Life, The Universe and Everything\" (the answer to which is known to be 42).\n\n"}
{"id": "12809718", "url": "https://en.wikipedia.org/wiki?curid=12809718", "title": "William Henry Maule", "text": "William Henry Maule\n\nSir William Henry Maule PC (25 April 1788 – 1858) was an English lawyer, member of parliament and judge.\n\nMaule was born in Edmonton, Middlesex. His father, Henry, was a physician and his mother, Hannah \"née\" Rawson, a Quaker. He was educated at a private school then at Trinity College, Cambridge, where he was senior wrangler and Smith's prize winner in 1810 and where he became a fellow in 1811.\n\nHe initially remained in Cambridge, where he was a close friend of Charles Babbage, and worked as a mathematics tutor, including Edward Ryan and Cresswell Cresswell among his students. He was offered the post of professor of mathematics at the East India College but, in 1810, Maule had already entered Lincoln's Inn with the intention of practising law. He was called to the bar in 1814 and began practice in commercial law, especially marine insurance, at 3 Essex Court. Maule was appointed King's Council in 1833 and in 1835 became counsel to the Bank of England, succeeding Sir James Scarlett.\n\nHis retention by the bank did not prevent him from acting for Nicholas Aylward Vigors who faced an election petition over his County Carlow by-election victory in February 1837. Maule's success established his reputation in the region and he himself was elected for Carlow Borough in the United Kingdom general election, 1837 of August.\n\nMaule was knighted and appointed a Baron of the Court of the Exchequer in 1839, transferring to the Court of Common Pleas later that year. He was a practical and knowledgeable judge with a fine judicial sense of humour (witness his pointed opinion in a case of wife selling). Maule was the only judge to dissent (in part) on the ruling in \"M'Naghten's Case\" (insanity).\nMaule retired from the bench because of poor health in 1855 but became a Privy Councillor. Maule never married, sharing a house with his widowed sister, Emma Maria Leathley, and unmarried niece, Emma Leathley. He died at home in London.\n\n"}
{"id": "449568", "url": "https://en.wikipedia.org/wiki?curid=449568", "title": "−1", "text": "−1\n\nIn mathematics, −1 is the additive inverse of 1, that is, the number that when added to 1 gives the additive identity element, 0. It is the negative integer greater than negative two (−2) and less than 0.\n\nNegative one bears relation to Euler's identity since \"e\" = −1.\n\nIn software development, −1 is a common initial value for integers and is also used to show that a variable contains no useful information.\n\nIn programming languages, −1 can be used to index the last (or 2nd last) item of an array, depending on whether 0 or 1 represents the first item.\n\nNegative one has some similar but slightly different properties to positive one.\n\nMultiplying a number by −1 is equivalent to changing the sign on the number. This can be proved using the distributive law and the axiom that 1 is the multiplicative identity: for \"x\" real, we have\n\nwhere we used the fact that any real \"x\" times 0 equals 0, implied by cancellation from the equation\n\nIn other words,\n\nso (−1) · \"x\", or −\"x\", is the arithmetic inverse of \"x\".\n\nThe square of −1, i.e. −1 multiplied by −1, equals 1. As a consequence, a product of two negative real numbers is positive.\n\nFor an algebraic proof of this result, start with the equation\n\nThe first equality follows from the above result. The second follows from the definition of −1 as additive inverse of 1: it is precisely that number that when added to 1 gives 0. Now, using the distributive law, we see that\n\nThe second equality follows from the fact that 1 is a multiplicative identity. But now adding 1 to both sides of this last equation implies\n\nThe above arguments hold in any ring, a concept of abstract algebra generalizing integers and real numbers.\n\nAlthough there are no real square roots of -1, the complex number \"i\" satisfies \"i\" = −1, and as such can be considered as a square root of −1. The only other complex number who's square is 1 is −\"i\". In the algebra of quaternions, which contain the complex plane, the equation \"x\" = −1 has infinite solutions.\n\nExponentiation of a non-zero real number can be extended to negative integers. We make the definition that \"x\" = , meaning that we define raising a number to the power −1 to have the same effect as taking its reciprocal. This definition is then extended to negative integers preserves the exponential law \"x\"\"x\" = \"x\" for real numbers \"a\" and \"b\".\n\nExponentiation to negative integers can be extended to invertible elements of a ring, by defining \"x\" as the multiplicative inverse of \"x\".\n\n−1 that appears next to functions or matrices does not mean raising them to the power −1 but their inverse functions or inverse matrices. For example, \"f\"(\"x\") is the inverse of \"f\"(\"x\"), or sin(\"x\") is a notation of arcsine function.\n\nMost computer systems represent negative integers using two's complement. In such systems, −1 is represented using a bit pattern of all ones. For example, an 8-bit signed integer using two's complement would represent −1 as the bitstring \"11111111\", or \"FF\" in hexadecimal (base 16). If interpreted as an unsigned integer, the same bitstring of \"n\" ones represents 2 − 1, the largest possible value that \"n\" bits can hold. For example, the 8-bit string \"11111111\" above represents 2 − 1 = 255.\n\nIn some programming languages, when used to index some data types (such as an array), then -1 can be used to identify the very last (or 2nd last) item, depending on whether 0 or 1 represents the first item.\nIf the first item is indexed by 0, then -1 identifies the last item.\nIf the first item is indexed by 1, then -1 identifies the second-to-last item.\n"}
