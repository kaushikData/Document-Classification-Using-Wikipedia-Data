{"id": "343334", "url": "https://en.wikipedia.org/wiki?curid=343334", "title": "70 (number)", "text": "70 (number)\n\n70 (seventy) is the natural number following 69 and preceding 71.\n\n70 is:\n\n\nThe sum of the first 24 squares starting from 1 is 70. This relates 70 to the Leech lattice and thus string theory.\n\n\n\n\n\n\n\nSeveral languages, especially ones with vigesimal number systems, do not have a specific word for 70: for example, French \"soixante-dix\" \"sixty-ten\"; Danish \"halvfjerds\", short for \"halvfjerdsindstyve\" \"three and a half score\". (For French, this is true only in France; other French-speaking regions such as Belgium, Switzerland, Aosta Valley and Jersey use \"septante\".)\n\n"}
{"id": "30876191", "url": "https://en.wikipedia.org/wiki?curid=30876191", "title": "99 (number)", "text": "99 (number)\n\n99 (ninety-nine) is the natural number following 98 and preceding 100.\n\n99 is:\n\n\n"}
{"id": "44959719", "url": "https://en.wikipedia.org/wiki?curid=44959719", "title": "Adhyavasaya", "text": "Adhyavasaya\n\nAdhyavasāya (Sanskrit: अध्यवसाय ) generally means – 'apprehension', 'clinging to', 'mental effort' and also 'perseverance', 'having decided'.\n\nNārāyana Tīrtha, in his \"Sāmkhya-Candrikā\", explains \"adhyavasāya\" as \" a modified condition of the intellect (\"buddhi\") the determination in such a form as – \"such an act is to be done by me\" \". Gaudapāda explains it as \" intellectual determination of the object of perception as belonging to a definite class \". Vācaspatimiśra explains it as \" ascertainment or determinate knowledge consequent upon the manifestation of the essence (\"sattva\") of the intellect, when the inertia of the intellect is overcome by the operation of the sense-organs in apprehending their objects, as the intention which follows self-appropriated knowledge or volition of the agent to react to the object of perception in a definite way \". Ascertainment also means definite awareness which according to the Samkhya school is associated in \"buddhi\" (अध्यवसायो बुद्धिः), in perception which is definite ascertainment involving the senses with respect to specific kinds of empirical objects, and which is an activity of \"buddhi\". It implies determination of objects (\"avidhāranā\") which by itself is decisive knowledge (\"niścaya\").\n\nThe sage of the Maitri Upanishad (VI.30) explains:-\n\nthe opposite of this is liberated. \"\n\nVindhyāvasin holds the view that \"sankalpa\", \"abhimāna\" and \"adhyavasāya\" are not different functions but the modifications of the mind or intellect. Samkhyas hold the view that perception is the reflective discerning (\"adhyavasāya\"), a kind of awareness, an operation of the intellect in its \"sattva\" modality; to be aware of perception as perception is to define and distinguish it from others which definition (\"adhyavasāya\") can come only through conception (\"vikalpa\"). Sadyojyoti defines discernment as the ascertaining cognitive activity (\"adhyavasāya\") which is understanding (\"jñāna\").\n\nAccording to the Yoga school and Ayurveda, \"adhyavasāya\" i.e. apprehension of clear and clean knowledge of the knowable, is one of the sixteen qualities of the empirical soul (\"jīvātmān\"). Lakshmi Tantra (XVI.4) explains that the intellect (\"buddhi\") is the incentive to mental effort (\"adhyavasāya\"); \"prana\" is the incentive to endeavour (\"prayatna\") and time (\"kāla\") is the incentive to transform in the form of impulse and effective development; whereas ego (\"ahamkara\") results from a modification of \"mahat\" in which cognition is indeterminate cognition (\"nirvikalpa adhyavasāya\").\n\nThe followers of Jainism consider \"adhyavasāya\" referring to good or bad sentiments, and of the nature of \"Bhava-Yoga\" which is made up of 'inauspicious combined meditation' and 'meditation'. Utpala in his following statement:-\n\nterms - \"adhyavasāya\" as mental apprehension as he proceeds to explain \"māyā-shakti\" to be the differentiating power of Brahman that affects consciousness resulting in the false perception of duality. And, Ratnakīrti holds the view that non-momentary existence is a figment of imaginative intuition projected as real by the process of intellection called \"adhyavasāya\" which is impulsive movement of the mind generated by the force of immediately preceding cognition. The Buddhist view is that the judgment of the object of cognition ('spatio-temporal objects') owing to special synthesis of moments gains sensible qualities etc., to become universal (\"sāmānaya lakshana\", \"ekatva adhyavasāya\"). Jñanasrimitra holds the view that only deluded persons interpret an image by conceptual thinking. (\"adhyavasāya\", \"vikalpa\").\n\nCommon characteristics (\"sādhāranadharma\") in poetry can either be in the form of existence (\"bhāva\") or negation (\"abhāva\"), and can involve supersession (\"adhyavasāya\"). In \"apahnuti\" (concealment of truth, denial), when a thing is concealed and something else is described in its place, the \"āropa\" (raising, elevating, superimposition) that is based on negation amounts to \"adhyavasāya\", which view is refuted by Jayaratha who agrees with Ruyyaka that \"adhyavasāya\" is the phenomenon where the \"visayah\" is concealed and its \"non-difference (abheda) \" with the \"vishayin\" is known, and that doubt (\"bhrānti\") is always based on \"adhyavasāya\" and not on \"āropa\". In Sanskrit literature, \"Alankāraśāstra\" deals with the beauty which poetry (\"kāvya\") alone can display. \"Adhyavasāya\" is considered as \"siddha\" (accomplished) when the object is not expressed in words but is lost in comparison, it is considered as \"sādhya\" (perfect) when the object in the process of being represented is identical with the object of comparison. It distinguishes \"utprekśa\" ('alliteration') and \"atiśayokti\" ('hyperbole') from \"rūpaka\" ('dramatic representation', 'form') and \"āropa\" ('superimposition').\n"}
{"id": "20491663", "url": "https://en.wikipedia.org/wiki?curid=20491663", "title": "Agmon's inequality", "text": "Agmon's inequality\n\nIn mathematical analysis, Agmon's inequalities, named after Shmuel Agmon, consist of two closely related interpolation inequalities between the Lebesgue space formula_1 and the Sobolev spaces formula_2. It is useful in the study of partial differential equations.\n\nLet formula_3 where formula_4. Then Agmon's inequalities in 3D state that there exists a constant formula_5 such that\n\nand\n\nIn 2D, the first inequality still holds, but not the second: let formula_3 where formula_9. Then Agmon's inequality in 2D states that there exists a constant formula_5 such that\n\nFor the formula_12-dimensional case, choose formula_13 and formula_14 such that formula_15. Then, if formula_16 and formula_17, the following inequality holds for any formula_18\n\n\n"}
{"id": "31803163", "url": "https://en.wikipedia.org/wiki?curid=31803163", "title": "BF-graph", "text": "BF-graph\n\nIn graph theory, a BF-graph is a type of directed hypergraph where each hyperedge is directed either to one particular vertex or away from one particular vertex.\n\nIn a directed hypergraph, each hyperedge may be directed away from some of its vertices (its \"tails\") and towards some others of its vertices (its \"heads\").\nA hyperedge that is directed to a single head vertex, and away from all its other vertices, is called a \"B\"-arch. Symmetrically, a hyperedge that is directed away from a single tail vertex, and towards all its other vertices, is called an \"F\"-arc.\n\nIf a hypergraph has only \"B\"-arcs, it is called a \"B\"-graph. If a hypergraph has only \"F\"-arcs, it is called an \"F\"-graph.yperarc with a tail of size 1. A hypergraph with only B-arcs is a B-graph and a hypergraph with only F-arcs is a F-graph.\n\n"}
{"id": "23762740", "url": "https://en.wikipedia.org/wiki?curid=23762740", "title": "Beltrami vector field", "text": "Beltrami vector field\n\nIn vector calculus, a Beltrami vector field, named after Eugenio Beltrami, is a vector field in three dimensions that is parallel to its own curl. That is, F is a Beltrami vector field provided that\n\nThus formula_2 and formula_3 are parallel vectors in other words, formula_4.\n\nIf formula_2 is solenoidal - that is, if formula_6 such as for an incompressible fluid or a magnetic field, the identity formula_7 becomes formula_8 and this leads to\n\nand if we further assume that formula_10 is a constant, we arrive at the simple form\n\nBeltrami vector fields with nonzero curl correspond to Euclidean contact forms in three dimensions.\n\nThe vector field\nis a multiple of the standard contact structure −\"z\" i + j, and furnishes an example of a Beltrami vector field.\n\n\n"}
{"id": "3821", "url": "https://en.wikipedia.org/wiki?curid=3821", "title": "Binary-coded decimal", "text": "Binary-coded decimal\n\nIn computing and electronic systems, binary-coded decimal (BCD) is a class of binary encodings of decimal numbers where each decimal digit is represented by a fixed number of bits, usually four or eight. Special bit patterns are sometimes used for a sign or for other indications (e.g., error or overflow).\n\nIn byte-oriented systems (i.e. most modern computers), the term \"unpacked\" BCD usually implies a full byte for each digit (often including a sign), whereas \"packed\" BCD typically encodes two decimal digits within a single byte by taking advantage of the fact that four bits are enough to represent the range 0 to 9. The precise 4-bit encoding may vary however, for technical reasons, see Excess-3 for instance. The ten states representing a BCD decimal digit are sometimes called \"tetrades\" (for the nibble typically needed to hold them also known as tetrade) with those don't care-states unused named or \"pseudo-decimal digit\").\n\nBCD's main virtue is its more accurate representation and rounding of decimal quantities as well as an ease of conversion into human-readable representations, in comparison to binary positional systems. BCD's principal drawbacks are a small increase in the complexity of the circuits needed to implement basic arithmetics and a slightly less dense storage.\n\nBCD was used in many early decimal computers, and is implemented in the instruction set of machines such as the IBM System/360 series and its descendants, Digital Equipment Corporation's VAX and the Motorola 68000-series processors. Although BCD \"per se\" is not as widely used as in the past and is no longer implemented in newer computers' instruction sets (such as ARM; x86 does not support its BCD instructions in long mode any more), decimal fixed-point and floating-point formats are still important and continue to be used in financial, commercial, and industrial computing, where subtle conversion and fractional rounding errors that are inherent in floating point binary representations cannot be tolerated.\n\nBCD takes advantage of the fact that any one decimal numeral can be represented by a four bit pattern. The most obvious way of encoding digits is \"natural BCD\" (NBCD), where each decimal digit is represented by its corresponding four-bit binary value, as shown in the following table. This is also called \"8421\" encoding.\nOther encodings are also used, including so-called \"4221\" and \"7421\"—named after the weighting used for the bits—and \"Excess-3\". For example, the BCD digit 6, '0110'b in 8421 notation, is '1100'b in 4221 (two encodings are possible), '0110'b in 7421, and '1001'b (6+3=9) in excess-3.\n\nAs most computers deal with data in 8-bit bytes, it is possible to use one of the following methods to encode a BCD number:\n\nAs an example, encoding the decimal number 91 using unpacked BCD results in the following binary pattern of two bytes:\n\nIn packed BCD, the same number would fit into a single byte:\n\nHence the numerical range for one unpacked BCD byte is zero through nine inclusive, whereas the range for one packed BCD is zero through ninety-nine inclusive.\n\nTo represent numbers larger than the range of a single byte any number of contiguous bytes may be used. For example, to represent the decimal number 12345 in packed BCD, using big-endian format, a program would encode as follows:\n\nNote that the most significant nibble of the most significant byte is zero, implying that the number is in actuality 012345. Also note how packed BCD is more efficient in storage usage as compared to unpacked BCD; encoding the same number (with the leading zero) in unpacked format would consume twice the storage.\n\nShifting and masking operations are used to pack or unpack a packed BCD digit. Other logical operations are used to convert a numeral to its equivalent bit pattern or reverse the process.\n\nBCD is very common in electronic systems where a numeric value is to be displayed, especially in systems consisting solely of digital logic, and not containing a microprocessor. By employing BCD, the manipulation of numerical data for display can be greatly simplified by treating each digit as a separate single sub-circuit. This matches much more closely the physical reality of display hardware—a designer might choose to use a series of separate identical seven-segment displays to build a metering circuit, for example. If the numeric quantity were stored and manipulated as pure binary, interfacing to such a display would require complex circuitry. Therefore, in cases where the calculations are relatively simple, working throughout with BCD can lead to a simpler overall system than converting to and from binary. Most pocket calculators do all their calculations in BCD.\n\nThe same argument applies when hardware of this type uses an embedded microcontroller or other small processor. Often, smaller code results when representing numbers internally in BCD format, since a conversion from or to binary representation can be expensive on such limited processors. For these applications, some small processors feature BCD arithmetic modes, which assist when writing routines that manipulate BCD quantities.\n\nIn packed BCD (or simply packed decimal), each of the two nibbles of each byte represent a decimal digit. Packed BCD has been in use since at least the 1960s and is implemented in all IBM mainframe hardware since then. Most implementations are big endian, i.e. with the more significant digit in the upper half of each byte, and with the leftmost byte (residing at the lowest memory address) containing the most significant digits of the packed decimal value. The lower nibble of the rightmost byte is usually used as the sign flag, although some unsigned representations lack a sign flag. As an example, a 4-byte value consists of 8 nibbles, wherein the upper 7 nibbles store the digits of a 7-digit decimal value and the lowest nibble indicates the sign of the decimal integer value.\n\nStandard sign values are 1100 (hex C) for positive (+) and 1101 (D) for negative (−). This convention comes from the zone field for EBCDIC characters and the signed overpunch representation. Other allowed signs are 1010 (A) and 1110 (E) for positive and 1011 (B) for negative. IBM System/360 processors will use the 1010 (A) and 1011 (B) signs if the A bit is set in the PSW, for the ASCII-8 standard that never passed. Most implementations also provide unsigned BCD values with a sign nibble of 1111 (F). ILE RPG uses 1111 (F) for positive and 1101 (D) for negative. These match the EBCDIC zone for digits without a sign overpunch. In packed BCD, the number 127 is represented by 0001 0010 0111 1100 (127C) and −127 is represented by 0001 0010 0111 1101 (127D). Burroughs systems used 1101 (D) for negative, and any other value is considered a positive sign value (the processors will normalize a positive sign to 1100 (C)).\nNo matter how many bytes wide a word is, there are always an even number of nibbles because each byte has two of them. Therefore, a word of \"n\" bytes can contain up to (2\"n\")−1 decimal digits, which is always an odd number of digits. A decimal number with \"d\" digits requires (\"d\"+1) bytes of storage space.\n\nFor example, a 4-byte (32-bit) word can hold seven decimal digits plus a sign, and can represent values ranging from ±9,999,999. Thus the number −1,234,567 is 7 digits wide and is encoded as:\n\nIn contrast, a 4-byte binary two's complement integer can represent values from −2,147,483,648 to +2,147,483,647.\n\nWhile packed BCD does not make optimal use of storage (about one-sixth of the memory used is wasted), conversion to ASCII, EBCDIC, or the various encodings of Unicode is still trivial, as no arithmetic operations are required. The extra storage requirements are usually offset by the need for the accuracy and compatibility with calculator or hand calculation that fixed-point decimal arithmetic provides. Denser packings of BCD exist which avoid the storage penalty and also need no arithmetic operations for common conversions.\n\nPacked BCD is supported in the COBOL programming language as the \"COMPUTATIONAL-3\" (an IBM extension adopted by many other compiler vendors) or \"PACKED-DECIMAL\" (part of the 1985 COBOL standard) data type. It is supported in PL/I as \"FIXED DECIMAL\". Besides the IBM System/360 and later compatible mainframes, packed BCD is implemented in the native instruction set of the original VAX processors from Digital Equipment Corporation and some models of the SDS Sigma series mainframes, and is the native format for the Burroughs Corporation Medium Systems line of mainframes (descended from the 1950s Electrodata 200 series).\n\nTen's complement representations for negative numbers offer an alternative approach to encoding the sign of packed (and other) BCD numbers. In this case, positive numbers always have a most significant digit between 0 and 4 (inclusive), while negative numbers are represented by the 10's complement of the corresponding positive number. As a result, this system allows for, a 32-bit packed BCD numbers to range from −50,000,000 to +49,999,999, and −1 is represented as 99999999. (As with two's complement binary numbers, the range is not symmetric about zero.)\n\nFixed-point decimal numbers are supported by some programming languages (such as COBOL, PL/I and Ada). These languages allow the programmer to specify an implicit decimal point in front of one of the digits. For example, a packed decimal value encoded with the bytes 12 34 56 7C represents the fixed-point value +1,234.567 when the implied decimal point is located between the 4th and 5th digits:\n\nThe decimal point is not actually stored in memory, as the packed BCD storage format does not provide for it. Its location is simply known to the compiler and the generated code acts accordingly for the various arithmetic operations.\n\nIf a decimal digit requires four bits, then three decimal digits require 12 bits. However, since 2 (1,024) is greater than 10 (1,000), if three decimal digits are encoded together, only 10 bits are needed. Two such encodings are \"Chen–Ho encoding\" and \"densely packed decimal\" (DPD). The latter has the advantage that subsets of the encoding encode two digits in the optimal seven bits and one digit in four bits, as in regular BCD.\n\nSome implementations, for example IBM mainframe systems, support zoned decimal numeric representations. Each decimal digit is stored in one byte, with the lower four bits encoding the digit in BCD form. The upper four bits, called the \"zone\" bits, are usually set to a fixed value so that the byte holds a character value corresponding to the digit. EBCDIC systems use a zone value of 1111 (hex F); this yields bytes in the range F0 to F9 (hex), which are the EBCDIC codes for the characters \"0\" through \"9\". Similarly, ASCII systems use a zone value of 0011 (hex 3), giving character codes 30 to 39 (hex).\n\nFor signed zoned decimal values, the rightmost (least significant) zone nibble holds the sign digit, which is the same set of values that are used for signed packed decimal numbers (see above). Thus a zoned decimal value encoded as the hex bytes F1 F2 D3 represents the signed decimal value −123:\n\n(*) \"Note: These characters vary depending on the local character code page setting.\"\n\nSome languages (such as COBOL and PL/I) directly support fixed-point zoned decimal values, assigning an implicit decimal point at some location between the decimal digits of a number. For example, given a six-byte signed zoned decimal value with an implied decimal point to the right of the fourth digit, the hex bytes F1 F2 F7 F9 F5 C0 represent the value +1,279.50:\n\nIBM used the terms \"Binary-Coded Decimal Interchange Code\" (BCDIC, sometimes just called BCD), for 6-bit \"alphanumeric\" codes that represented numbers, upper-case letters and special characters. Some variation of BCDIC \"alphamerics\" is used in most early IBM computers, including the IBM 1620, IBM 1400 series, and non-Decimal Architecture members of the IBM 700/7000 series.\n\nThe IBM 1400 series are character-addressable machines, each location being six bits labeled \"B, A, 8, 4, 2\" and \"1,\" plus an odd parity check bit (\"C\") and a word mark bit (\"M\"). For encoding digits \"1\" through \"9\", \"B\" and \"A\" are zero and the digit value represented by standard 4-bit BCD in bits \"8\" through \"1\". For most other characters bits \"B\" and \"A\" are derived simply from the \"12\", \"11\", and \"0\" \"zone punches\" in the punched card character code, and bits \"8\" through \"1\" from the \"1\" through \"9\" punches. A \"12 zone\" punch set both \"B\" and \"A\", an \"11 zone\" set \"B\", and a \"0 zone\" (a 0 punch combined with any others) set \"A\". Thus the letter A, which is \"(12,1)\" in the punched card format, is encoded \"(B,A,1)\". The currency symbol $, \"(11,8,3)\" in the punched card, was encoded in memory as \"(B,8,2,1)\". This allows the circuitry to convert between the punched card format and the internal storage format to be very simple with only a few special cases. One important special case is digit \"0\", represented by a lone \"0\" punch in the card, and \"(8,2)\" in core memory.\n\nThe memory of the IBM 1620 is organized into 6-bit addressable digits, the usual \"8, 4, 2, 1\" plus \"F\", used as a flag bit and \"C\", an odd parity check bit. BCD \"alphamerics\" are encoded using digit pairs, with the \"zone\" in the even-addressed digit and the \"digit\" in the odd-addressed digit, the \"zone\" being related to the \"12\", \"11\", and \"0\" \"zone punches\" as in the 1400 series. Input/Output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.\n\nIn the Decimal Architecture IBM 7070, IBM 7072, and IBM 7074 \"alphamerics\" are encoded using digit pairs (using two-out-of-five code in the digits, not BCD) of the 10-digit word, with the \"zone\" in the left digit and the \"digit\" in the right digit. Input/Output translation hardware converted between the internal digit pairs and the external standard 6-bit BCD codes.\n\nWith the introduction of System/360, IBM expanded 6-bit BCD \"alphamerics\" to 8-bit EBCDIC, allowing the addition of many more characters (e.g., lowercase letters). A variable length Packed BCD \"numeric\" data type is also implemented, providing machine instructions that perform arithmetic directly on packed decimal data.\n\nOn the IBM 1130 and 1800, packed BCD is supported in software by IBM's Commercial Subroutine Package.\n\nToday, BCD data is still heavily used in IBM processors and databases, such as IBM DB2, mainframes, and Power6. In these products, the BCD is usually zoned BCD (as in EBCDIC or ASCII), Packed BCD (two decimal digits per byte), or \"pure\" BCD encoding (one decimal digit stored as BCD in the low four bits of each byte). All of these are used within hardware registers and processing units, and in software. To convert packed decimals in EBCDIC table unloads to readable numbers, you can use the OUTREC FIELDS mask of the JCL utility DFSORT.\n\nThe Digital Equipment Corporation VAX-11 series includes instructions that can perform arithmetic directly on packed BCD data and convert between packed BCD data and other integer representations. The VAX's packed BCD format is compatible with that on IBM System/360 and IBM's later compatible processors. The MicroVAX and later VAX implementations dropped this ability from the CPU but retained code compatibility with earlier machines by implementing the missing instructions in an operating system-supplied software library. This is invoked automatically via exception handling when the no longer implemented instructions are encountered, so that programs using them can execute without modification on the newer machines.\n\nThe Intel x86 architecture supports a unique 18-digit (ten-byte) BCD format that can be loaded into and stored from the floating point registers, and computations can be performed there.\n\nThe Motorola 68000 series had BCD instructions.\n\nIn more recent computers such capabilities are almost always implemented in software rather than the CPU's instruction set, but BCD numeric data is still extremely common in commercial and financial applications. There are tricks for implementing packed BCD and zoned decimal add or subtract operations using short but difficult to understand sequences of word-parallel logic and binary arithmetic operations. For example, the following code (written in C) computes an unsigned 8-digit packed BCD add using 32-bit binary operations:\nIt is possible to perform addition in BCD by first adding in binary, and then converting to BCD afterwards. Conversion of the simple sum of two digits can be done by adding 6 (that is, 16 – 10) when the five-bit result of adding a pair of digits has a value greater than 9. For example:\n\nNote that 10001 is the binary, not decimal, representation of the desired result. Also note that it cannot fit in a 4-bit number. In BCD as in decimal, there cannot exist a value greater than 9 (1001) per digit. To correct this, 6 (0110) is added to that sum and then the result is treated as two nibbles:\n\nThe two nibbles of the result, 0001 and 0111, correspond to the digits \"1\" and \"7\". This yields \"17\" in BCD, which is the correct result.\n\nThis technique can be extended to adding multiple digits by adding in groups from right to left, propagating the second digit as a carry, always comparing the 5-bit result of each digit-pair sum to 9. Some CPUs provide a half-carry flag to facilitate BCD arithmetic adjustments following binary addition and subtraction operations.\n\nSubtraction is done by adding the ten's complement of the subtrahend. To represent the sign of a number in BCD, the number 0000 is used to represent a positive number, and 1001 is used to represent a negative number. The remaining 14 combinations are invalid signs. To illustrate signed BCD subtraction, consider the following problem: 357 − 432.\n\nIn signed BCD, 357 is 0000 0011 0101 0111. The ten's complement of 432 can be obtained by taking the nine's complement of 432, and then adding one. So, 999 − 432 = 567, and 567 + 1 = 568. By preceding 568 in BCD by the negative sign code, the number −432 can be represented. So, −432 in signed BCD is 1001 0101 0110 1000.\n\nNow that both numbers are represented in signed BCD, they can be added together:\n\nSince BCD is a form of decimal representation, several of the digit sums above are invalid. In the event that an invalid entry (any BCD digit greater than 1001) exists, 6 is added to generate a carry bit and cause the sum to become a valid entry. The reason for adding 6 is that there are 16 possible 4-bit BCD values (since 2 = 16), but only 10 values are valid (0000 through 1001). So adding 6 to the invalid entries results in the following:\n\nThus the result of the subtraction is 1001 1001 0010 0101 (−925). To check the answer, note that the first digit is 9, which means negative. This seems to be correct, since 357 − 432 should result in a negative number. To check the rest of the digits, represent them in decimal. 1001 0010 0101 is 925. The ten's complement of 925 is 1000 − 925 = 999 − 925 + 1 = 074 + 1 = 75, so the calculated answer is −75. To check, perform standard subtraction to verify that 357 − 432 is −75.\n\nNote that in the event that there are a different number of nibbles being added together (such as 1053 − 122), the number with the fewest digits must first be padded with zeros before taking the ten's complement or subtracting. So, with 1053 − 122, 122 would have to first be represented as 0122, and the ten's complement of 0122 would have to be calculated.\n\nThe binary-coded decimal scheme described in this article is the most common encoding, but there are many others. The method here can be referred to as \"Simple Binary-Coded Decimal\" (\"SBCD\") or \"BCD 8421\".\n\nIn the headers to the table, the \", indicates the weight of each bit shown; note that in the fifth column, \"BCD 8 4 −2 −1\", two of the weights are negative. Both ASCII and EBCDIC character codes for the digits are examples of zoned BCD, and are also shown in the table.\n\nThe following table represents decimal digits from 0 to 9 in various BCD systems:\nIn the 1972 case \"Gottschalk v. Benson\", the U.S. Supreme Court overturned a lower court decision which had allowed a patent for converting BCD encoded numbers to binary on a computer. This was an important case in determining the patentability of software and algorithms.\n\n\n\nThe BIOS in many personal computers stores the date and time in BCD because the MC6818 real-time clock chip used in the original IBM PC AT motherboard provided the time encoded in BCD. This form is easily converted into ASCII for display.\n\nThe Atari 8-bit family of computers used BCD to implement floating-point algorithms. The MOS 6502 processor has a BCD mode that affects the addition and subtraction instructions. The Psion Organiser 1 handheld computer’s manufacturer-supplied software also used entirely BCD to implement floating point; later Psion models used binary exclusively.\n\nEarly models of the PlayStation 3 store the date and time in BCD. This led to a worldwide outage of the console on 1 March 2010. The last two digits of the year stored as BCD were misinterpreted as 16 causing an error in the unit's date, rendering most functions inoperable. This has been referred to as the Year 2010 Problem.\n\nVarious BCD implementations exist that employ other representations for numbers. Programmable calculators manufactured by Texas Instruments, Hewlett-Packard, and others typically employ a floating-point BCD format, typically with two or three digits for the (decimal) exponent. The extra bits of the sign digit may be used to indicate special numeric values, such as infinity, underflow/overflow, and error (a blinking display).\n\nSigned decimal values may be represented in several ways. The COBOL programming language, for example, supports a total of five zoned decimal formats, each one encoding the numeric sign in a different way:\n3GPP developed TBCD, an expansion to BCD where the remaining (unused) bit combinations are used to add specific telephony characters, with digits similar to those found in telephone keypads original design.\nThe mentioned 3GPP document defines TBCD-STRING with swapped nibbles in each byte. Bits, octets and digits indexed from 1, bits from the right, digits and octets from the left.\n\nbits 8765 of octet n encoding digit 2n\n\nbits 4321 of octet n encoding digit 2(n-1) +1\nMeaning number codice_1, would become codice_2 in TBCD.\n\nIf errors in representation and computation are more important than the speed of conversion to and from display, a scaled binary representation may be used, which stores a decimal number as a binary-encoded integer and a binary-encoded signed decimal exponent. For example, 0.2 can be represented as 2.\n\nThis representation allows rapid multiplication and division, but may require shifting by a power of 10 during addition and subtraction to align the decimal points. It is appropriate for applications with a fixed number of decimal places that do not then require this adjustment—particularly financial applications where 2 or 4 digits after the decimal point are usually enough. Indeed, this is almost a form of fixed point arithmetic since the position of the radix point is implied.\n\nChen–Ho encoding provides a boolean transformation for converting groups of three BCD-encoded digits to and from 10-bit values that can be efficiently encoded in hardware with only 2 or 3 gate delays. Densely packed decimal (DPD) is a similar scheme that is used for most of the significand, except the lead digit, for one of the two alternative decimal encodings specified in the IEEE 754-2008 standard.\n\n\n\n"}
{"id": "2597784", "url": "https://en.wikipedia.org/wiki?curid=2597784", "title": "Book (graph theory)", "text": "Book (graph theory)\n\nIn graph theory, a book graph (often written formula_1 ) may be any of several kinds of graph.\n\nOne kind, which may be called a quadrilateral book, consists of \"p\" quadrilaterals sharing a common edge (known as the \"spine\" or \"base\" of the book). That is, it is a Cartesian product of a star and a single edge. The 7-page book graph of this type provides an example of a graph with no harmonious labeling.\n\nA second type, which might be called a triangular book, is the complete tripartite graph \"K\". It is a graph consisting of formula_2 triangles sharing a common edge. A book of this type is a split graph. \nThis graph has also been called a formula_3. Triangular books form one of the key building blocks of line perfect graphs.\n\nGiven a graph formula_4, one may write formula_5 for the largest book (of the kind being considered) contained within formula_4.\n\nThe term \"book-graph\" has been employed for other uses. Barioli used it to mean a graph composed of a number of arbitrary subgraphs having two vertices in common. (Barioli did not write formula_1 for his book-graph.)\n\nDenote the Ramsey number of two (triangular) books by formula_8\n\n"}
{"id": "6085", "url": "https://en.wikipedia.org/wiki?curid=6085", "title": "Cauchy sequence", "text": "Cauchy sequence\n\nIn mathematics, a Cauchy sequence (; ), named after Augustin-Louis Cauchy, is a sequence whose elements become arbitrarily close to each other as the sequence progresses. More precisely, given any small positive distance, all but a finite number of elements of the sequence are less than that given distance from each other.\n\nIt is not sufficient for each term to become arbitrarily close to the term. For instance, in the sequence of square roots of natural numbers:\nthe consecutive terms become arbitrarily close to each other:\nHowever, with growing values of the index , the terms become arbitrarily large, so for any index and distance , there exists an index big enough such that . (Actually, any suffices.) As a result, despite how far one goes, the remaining terms of the sequence never get close to , hence the sequence is not Cauchy.\n\nThe utility of Cauchy sequences lies in the fact that in a complete metric space (one where all such sequences are known to converge to a limit), the criterion for convergence depends only on the terms of the sequence itself, as opposed to the definition of convergence, which uses the limit value as well as the terms. This is often exploited in algorithms, both theoretical and applied, where an iterative process can be shown relatively easily to produce a Cauchy sequence, consisting of the iterates, thus fulfilling a logical condition, such as termination.\n\nThe notions above are not as unfamiliar as they might at first appear. The customary acceptance of the fact that any real number \"x\" has a decimal expansion is an implicit acknowledgment that a particular Cauchy sequence of rational numbers (whose terms are the successive truncations of the decimal expansion of \"x\") has the real limit \"x\". In some cases it may be difficult to describe \"x\" independently of such a limiting process involving rational numbers.\n\nGeneralizations of Cauchy sequences in more abstract uniform spaces exist in the form of Cauchy filters and Cauchy nets.\n\nA sequence\n\nof real numbers is called a Cauchy sequence, if for every positive real number \"ε\", there is a positive integer \"N\" such that for all natural numbers \"m\", \"n\" > \"N\"\n\nwhere the vertical bars denote the absolute value. In a similar way one can define Cauchy sequences of rational or complex numbers. Cauchy formulated such a condition by requiring formula_5 to be infinitesimal for every pair of infinite \"m\", \"n\".\n\nSince the definition of a Cauchy sequence only involves metric concepts, it is straightforward to generalize it to any metric space \"X\". To do so, the absolute value |\"x\" - \"x\"| is replaced by the distance \"d\"(\"x\", \"x\") (where \"d\" denotes a metric) between \"x\" and \"x\".\n\nFormally, given a metric space (\"X\", \"d\"), a sequence\n\nis Cauchy, if for every positive real number \"ε\" > 0 there is a positive integer \"N\" such that for all positive integers \"m\", \"n\" > \"N\", the distance\n\nRoughly speaking, the terms of the sequence are getting closer and closer together in a way that suggests that the sequence ought to have a limit in \"X\". Nonetheless, such a limit does not always exist within \"X\".\n\nA metric space (\"X\", \"d\") in which every Cauchy sequence converges to an element of \"X\" is called complete.\n\nThe real numbers are complete under the metric induced by the usual absolute value, and one of the standard constructions of the real numbers involves Cauchy sequences of rational numbers. In this construction, each equivalence class of Cauchy sequences of rational numbers with a certain tail behavior—that is, each class of sequences that get arbitrarily close to one another— is a real number.\n\nA rather different type of example is afforded by a metric space \"X\" which has the discrete metric (where any two distinct points are at distance 1 from each other). Any Cauchy sequence of elements of \"X\" must be constant beyond some fixed point, and converges to the eventually repeating term.\n\nThe rational numbers Q are not complete (for the usual distance):\nThere are sequences of rationals that converge (in R) to irrational numbers; these are Cauchy sequences having no limit in Q. In fact, if a real number \"x\" is irrational, then the sequence (\"x\"), whose \"n\"-th term is the truncation to \"n\" decimal places of the decimal expansion of \"x\", gives a Cauchy sequence of rational numbers with irrational limit \"x\". Irrational numbers certainly exist in R, for example:\n\n\nThe open interval formula_11 in the set of real numbers with an ordinary distance in R is not a complete space: there is a sequence formula_12 in it, which is Cauchy (for arbitrarily small distance bound formula_13 all terms formula_14 of formula_15 fit in the formula_16 interval), however does not converge in formula_17 — its 'limit', number formula_18, does not belong to the space formula_17.\n\n\nThese last two properties, together with the Bolzano–Weierstrass theorem, yield one standard proof of the completeness of the real numbers, closely related to both the Bolzano–Weierstrass theorem and the Heine–Borel theorem. Every Cauchy sequence of real numbers is bounded, hence by Bolzano-Weierstrass has a convergent subsequence, hence is itself convergent. It should be noted, though, that this proof of the completeness of the real numbers implicitly makes use of the least upper bound axiom. The alternative approach, mentioned above, of the real numbers as the completion of the rational numbers, makes the completeness of the real numbers tautological.\n\nOne of the standard illustrations of the advantage of being able to work with Cauchy sequences and make use of completeness is provided by consideration of the summation of an infinite series of real numbers\n(or, more generally, of elements of any complete normed linear space, or Banach space). Such a series \nformula_20 is considered to be convergent if and only if the sequence of partial sums formula_21 is convergent, where \nformula_22. It is a routine matter \nto determine whether the sequence of partial sums is Cauchy or not,\nsince for positive integers \"p\" > \"q\", \n\nIf formula_24 is a uniformly continuous map between the metric spaces \"M\" and \"N\" and (\"x\") is a Cauchy sequence in \"M\", then formula_25 is a Cauchy sequence in \"N\". If formula_26 and formula_27 are two Cauchy sequences in the rational, real or complex numbers, then the sum formula_28 and the product formula_29 are also Cauchy sequences.\n\nThere is also a concept of Cauchy sequence for a topological vector space formula_30: Pick a local base formula_31 for formula_30 about 0; then (formula_33) is a Cauchy sequence if for each member formula_34, there is some number formula_35 such that whenever \nformula_36 is an element of formula_37. If the topology of formula_30 is compatible with a translation-invariant metric formula_39, the two definitions agree.\n\nSince the topological vector space definition of Cauchy sequence requires only that there be a continuous \"subtraction\" operation, it can just as well be stated in the context of a topological group: A sequence formula_40 in a topological group formula_41 is a Cauchy sequence if for every open neighbourhood formula_42 of the identity in formula_41 there exists some number formula_35 such that whenever formula_45 it follows that formula_46. As above, it is sufficient to check this for the neighbourhoods in any local base of the identity in formula_41.\n\nAs in the construction of the completion of a metric space, one can furthermore define the binary relation on Cauchy sequences in formula_41 that formula_40 and formula_50 are equivalent if for every open neighbourhood formula_42 of the identity in formula_41 there exists some number formula_35 such that whenever formula_45 it follows that formula_55. This relation is an equivalence relation: It is reflexive since the sequences are Cauchy sequences. It is symmetric since formula_56 which by continuity of the inverse is another open neighbourhood of the identity. It is transitive since formula_57 where formula_58 and formula_59 are open neighbourhoods of the identity such that formula_60; such pairs exist by the continuity of the group operation.\n\nThere is also a concept of Cauchy sequence in a group formula_41:\nLet formula_62 be a decreasing sequence of normal subgroups of formula_41 of finite index.\nThen a sequence formula_26 in formula_41 is said to be Cauchy (w.r.t. formula_66) if and only if for any formula_67 there is formula_35 such that formula_69.\n\nTechnically, this is the same thing as a topological group Cauchy sequence for a particular choice of topology on formula_41, namely that for which formula_66 is a local base.\n\nThe set formula_72 of such Cauchy sequences forms a group (for the componentwise product), and the set formula_73 of null sequences (s.th. formula_74) is a normal subgroup of formula_72. The factor group formula_76 is called the completion of formula_41 with respect to formula_66.\n\nOne can then show that this completion is isomorphic to the inverse limit of the sequence formula_79.\n\nAn example of this construction, familiar in number theory and algebraic geometry is the construction of the \"p\"-adic completion of the integers with respect to a prime \"p\". In this case, \"G\" is the integers under addition, and \"H\" is the additive subgroup consisting of integer multiples of \"p\".\n\nIf formula_66 is a cofinal sequence (i.e., any normal subgroup of finite index contains some formula_81), then this completion is canonical in the sense that it is isomorphic to the inverse limit of formula_82, where formula_66 varies over normal subgroups of finite index. For further details, see ch. I.10 in Lang's \"Algebra\".\n\nIn constructive mathematics, Cauchy sequences often must be given with a \"modulus of Cauchy convergence\" to be useful. If formula_84 is a Cauchy sequence in the set formula_30, then a modulus of Cauchy convergence for the sequence is a function formula_86 from the set of natural numbers to itself, such that formula_87.\n\nClearly, any sequence with a modulus of Cauchy convergence is a Cauchy sequence. The converse (that every Cauchy sequence has a modulus) follows from the well-ordering property of the natural numbers (let formula_88 be the smallest possible formula_35 in the definition of Cauchy sequence, taking formula_67 to be formula_91). However, this well-ordering property does not hold in constructive mathematics (it is equivalent to the principle of excluded middle). On the other hand, this converse also follows (directly) from the principle of dependent choice (in fact, it will follow from the weaker AC), which is generally accepted by constructive mathematicians. Thus, moduli of Cauchy convergence are needed directly only by constructive mathematicians who (like Fred Richman) do not wish to use any form of choice.\n\nThat said, using a modulus of Cauchy convergence can simplify both definitions and theorems in constructive analysis. Perhaps even more useful are \"regular Cauchy sequences\", sequences with a given modulus of Cauchy convergence (usually formula_92 or formula_93). Any Cauchy sequence with a modulus of Cauchy convergence is equivalent (in the sense used to form the completion of a metric space) to a regular Cauchy sequence; this can be proved without using any form of the axiom of choice. Regular Cauchy sequences were used by Errett Bishop in his Foundations of Constructive Analysis, but they have also been used by Douglas Bridges in a non-constructive textbook (). However, Bridges also works on mathematical constructivism; the concept has not spread far outside of that milieu.\n\nA real sequence formula_94 has a natural hyperreal extension, defined for hypernatural values \"H\" of the index \"n\" in addition to the usual natural \"n\". The sequence is Cauchy if and only if for every infinite \"H\" and \"K\", the values formula_95 and formula_96 are infinitely close, or adequal, i.e. \nwhere \"st\" is the standard part function.\n\n introduced a notion of Cauchy completion of a category. Applied to Q (the category whose objects are rational numbers, and there is a morphism from \"x\" to \"y\" if and only if \"x\" ≤ \"y\"), this Cauchy completion yields R (again interpreted as a category using its natural ordering).\n\n\n"}
{"id": "1821365", "url": "https://en.wikipedia.org/wiki?curid=1821365", "title": "Charles Parsons (philosopher)", "text": "Charles Parsons (philosopher)\n\nCharles Dacre Parsons (born April 13, 1933) is an American philosopher best known for his work in the philosophy of mathematics and the study of the philosophy of Immanuel Kant. He is professor emeritus at Harvard University.\n\nParsons is a son of the famous Harvard sociologist Talcott Parsons. He earned his Ph.D. in philosophy at Harvard University in 1961, under the direction of Burton Dreben and Willard Van Orman Quine. He taught for many years at Columbia University before moving to Harvard University in 1989. He retired in 2005 as the Edgar Pierce professor of philosophy, a position formerly held by Quine.\n\nHe is an elected Fellow of the American Academy of Arts and Sciences and the Norwegian Academy of Science and Letters.\n\nAmong his former doctoral students are Michael Levin, James Higginbotham, Peter Ludlow, Gila Sher and Øystein Linnebo.\n\nIn addition to his work in logic and the philosophy of mathematics, Parsons was an editor, with Solomon Feferman and others, of the posthumous works of Kurt Gödel. He has also written on historical figures, especially Immanuel Kant, Gottlob Frege, Kurt Gödel, and Willard Van Orman Quine.\n\n\n"}
{"id": "53268", "url": "https://en.wikipedia.org/wiki?curid=53268", "title": "Convolution theorem", "text": "Convolution theorem\n\nIn mathematics, the convolution theorem states that under suitable conditions the Fourier transform of a convolution of two signals is the pointwise product of their Fourier transforms. In other words, convolution in one domain (e.g., time domain) equals point-wise multiplication in the other domain (e.g., frequency domain). Versions of the convolution theorem are true for various Fourier-related transforms. Let formula_1 and formula_2 be two functions with convolution formula_3. (Note that the asterisk denotes convolution in this context, not standard multiplication. The tensor product symbol formula_4 is sometimes used instead.) <br>\n\nIf formula_5 denotes the Fourier transform operator, then formula_6 and formula_7 are the Fourier transforms of formula_1 and formula_2, respectively. Then\n\nwhere formula_11 denotes point-wise multiplication. It also works the other way around:\n\nBy applying the inverse Fourier transform formula_13, we can write:\n\nand:\n\nThe relationships above are only valid for the form of the Fourier transform shown in the Proof section below. The transform may be normalized in other ways, in which case constant scaling factors (typically formula_16 or formula_17) will appear in the relationships above.\n\nThis theorem also holds for the Laplace transform, the two-sided Laplace transform and, when suitably modified, for the Mellin transform and Hartley transform (see Mellin inversion theorem). It can be extended to the Fourier transform of abstract harmonic analysis defined over locally compact abelian groups.\n\nThis formulation is especially useful for implementing a numerical convolution on a computer: The standard convolution algorithm has quadratic computational complexity. With the help of the convolution theorem and the fast Fourier transform, the complexity of the convolution can be reduced from formula_18 to formula_19, using Donald Knuth's big O notation. This can be exploited to construct fast multiplication algorithms, as in .\n\n\"The proof here is shown for a particular normalization of the Fourier transform. As mentioned above, if the transform is normalized differently, then constant scaling factors will appear in the derivation.\"\n\nLet formula_20 belong to the Lp space formula_21. Let formula_22 be the Fourier transform of formula_1 and formula_24 be the Fourier transform of formula_2:\nwhere the \"dot\" between formula_28 and formula_29 indicates the inner product of formula_30. Let formula_31 be the convolution of formula_1 and formula_2\n\nAlso\n\nHence by Fubini's theorem we have that formula_36 so its Fourier transform formula_37 is defined by the integral formula\n\nIt should be noted that formula_39 and hence by the argument above we may apply Fubini's theorem again (i.e. interchange the order of integration):\n\nSubstituting formula_41 yields formula_42. Therefore\n\nThese two integrals are the definitions of formula_46 and formula_47, so:\n\nQED.\n\nA similar argument, as the above proof, can be applied to the convolution theorem for the inverse Fourier transform;\n\nand:\n\nBy similar arguments, it can be shown that the discrete convolution of sequences formula_28 and formula_54 is given by:\n\nwhere DTFT represents the discrete-time Fourier transform.\n\nAn important special case is the circular convolution of formula_28 and formula_54 defined by formula_58 where formula_59 is a periodic summation:\n\nIt can then be shown that:\n\nwhere DFT represents the discrete Fourier transform.\n\nThe proof follows from DTFT#Periodic data, which indicates that formula_62 can be written as:\n\nThe product with formula_64 is thereby reduced to a discrete-frequency function:\n\nThe inverse DTFT is:\n\nQED.\n\nTwo convolution theorems exist for the Fourier series coefficients of a periodic function:\n\n\n\n\nFor a visual representation of the use of the convolution theorem in signal processing, see:\n\n"}
{"id": "49544464", "url": "https://en.wikipedia.org/wiki?curid=49544464", "title": "Danielle Macbeth", "text": "Danielle Macbeth\n\nDanielle Monique Macbeth (born 1954 in Edmonton, Canada) is a philosopher whose work focuses on the philosophy of mathematics, the philosophy of language, metaphysics, and the philosophy of logic. She is T. Wistar Brown Professor of Philosophy at Haverford College in Pennsylvania where she has taught since 1989. Macbeth also taught at the University of Hawaii from 1986-1989.\nMacbeth received a Bachelor of Science degree in Biochemistry at the University of Alberta in 1977 before beginning her philosophical studies. She then went on to receive a Bachelor of Arts degree in Philosophy and Religious Studies at McGill University in Montreal in 1980 and received her PhD from University of Pittsburgh in 1988. She wrote her dissertation under John Haugeland, and studied also with Wilfrid Sellars, John McDowell, and Robert Brandom. Macbeth has received numerous awards and fellowships including NEH Grants, and an ACLS Frederick Burkhardt Residential Fellowship. In 2002-2003, she was a Fellow at the Center for Advanced Study in Behavioral Sciences in Palo Alto, California.\n\nMacbeth is the author of two books, \"Frege’s Logic\" and \"Realizing Reason: A Narrative of Truth and Knowing\". In the first, Macbeth proposes a new reading of Frege’s notation and logical project. Rather than treating Begriffsschrift (Frege's logic) as a notational variant of quantificational logic, Macbeth proposes that reasoning in Begriffsschrift is more like the diagrammatic reasoning of the geometrician or algebraicist. She argues that philosophers and mathematicians alike have failed to recognize the revolutionary powers of Begriffsschrift in its expressive and demonstrative capacities.\n\nRealizing Reason, her most recent book, takes a Hegelian approach to the philosophy of mathematics and traces developments in philosophy, logic, mathematics, and physics beginning with Aristotle in order to illuminate how (pure) reason has come to be realized as a power of knowing. She focuses on three periods: Ancient Greece, early modern mathematics, physics, and philosophy (Descartes to Kant), and late nineteenth-century and early twentieth-century mathematics and physics. Macbeth argues that with her new reading of Frege, we can finally break out of the Kantian framework that remains in place even in twentieth-century analytic philosophy and thereby finally understand how contemporary mathematics enables real extensions of our knowledge on the basis of strictly deductive reasoning. Thus, she demonstrates how pure reason has finally been realized as a power of knowing. \nMacbeth has also published many articles on a wide range of topics in the history and philosophy of mathematics, the philosophy of language, the philosophy of mind, and pragmatism.\n\n"}
{"id": "2720954", "url": "https://en.wikipedia.org/wiki?curid=2720954", "title": "Data analysis", "text": "Data analysis\n\nData analysis is a process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, while being used in different business, science, and social science domains. In today's business, data analysis is playing a role in making decisions more scientific and helping the business achieve effective operation.\n\nData mining is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.\n\nData integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination. The term \"data analysis\" is sometimes used as a synonym for data modeling.\n\nAnalysis refers to breaking a whole into its separate components for individual examination. Data analysis is a process for obtaining raw data and converting it into information useful for decision-making by users. Data are collected and analyzed to answer questions, test hypotheses or disprove theories.\n\nStatistician John Tukey defined data analysis in 1961 as: \"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"\n\nThere are several phases that can be distinguished, described below. The phases are iterative, in that feedback from later phases may result in additional work in earlier phases.\n\nThe data are necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analysis or customers (who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained. Data may be numerical or categorical (i.e., a text label for numbers).\n\nData are collected from a variety of sources. The requirements may be communicated by analysts to custodians of the data, such as information technology personnel within an organization. The data may also be collected from sensors in the environment, such as traffic cameras, satellites, recording devices, etc. It may also be obtained through interviews, downloads from online sources, or reading documentation.\n\nData initially obtained must be processed or organised for analysis. For instance, these may involve placing data into rows and columns in a table format (i.e., structured data) for further analysis, such as within a spreadsheet or statistical software.\n\nOnce processed and organised, the data may be incomplete, contain duplicates, or contain errors. The need for data cleaning will arise from problems in the way that data are entered and stored. Data cleaning is the process of preventing and correcting these errors. Common tasks include record matching, identifying inaccuracy of data, overall quality of existing data, deduplication, and column segmentation. Such data problems can also be identified through a variety of analytical techniques. For example, with financial information, the totals for particular variables may be compared against separately published numbers believed to be reliable. Unusual amounts above or below pre-determined thresholds may also be reviewed. There are several types of data cleaning that depend on the type of data such as phone numbers, email addresses, employers etc. Quantitative data methods for outlier detection can be used to get rid of likely incorrectly entered data. Textual data spell checkers can be used to lessen the amount of mistyped words, but it is harder to tell if the words themselves are correct.\n\nOnce the data are cleaned, it can be analyzed. Analysts may apply a variety of techniques referred to as exploratory data analysis to begin understanding the messages contained in the data. The process of exploration may result in additional data cleaning or additional requests for data, so these activities may be iterative in nature. Descriptive statistics, such as the average or median, may be generated to help understand the data. Data visualization may also be used to examine the data in graphical format, to obtain additional insight regarding the messages within the data.\n\nMathematical formulas or models called algorithms may be applied to the data to identify relationships among the variables, such as correlation or causation. In general terms, models may be developed to evaluate a particular variable in the data based on other variable(s) in the data, with some residual error depending on model accuracy (i.e., Data = Model + Error).\n\nInferential statistics includes techniques to measure relationships between particular variables. For example, regression analysis may be used to model whether a change in advertising (independent variable X) explains the variation in sales (dependent variable Y). In mathematical terms, Y (sales) is a function of X (advertising). It may be described as Y = aX + b + error, where the model is designed such that a and b minimize the error when the model predicts Y for a given range of values of X. Analysts may attempt to build models that are descriptive of the data to simplify analysis and communicate results.\n\nA data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. An example is an application that analyzes data about customer purchasing history and recommends other purchases the customer might enjoy.\n\nOnce the data are analyzed, it may be reported in many formats to the users of the analysis to support their requirements. The users may have feedback, which results in additional analysis. As such, much of the analytical cycle is iterative.\n\nWhen determining how to communicate the results, the analyst may consider data visualization techniques to help clearly and efficiently communicate the message to the audience. Data visualization uses information displays (such as tables and charts) to help communicate key messages contained in the data. Tables are helpful to a user who might lookup specific numbers, while charts (e.g., bar charts or line charts) may help explain the quantitative messages contained in the data.\n\nStephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\n\nAuthor Jonathan Koomey has recommended a series of best practices for understanding quantitative data. These include:\n\nFor the variables under examination, analysts typically obtain descriptive statistics for them, such as the mean (average), median, and standard deviation. They may also analyze the distribution of the key variables to see how the individual values cluster around the mean.\n\nAnalysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.\n\nRegression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.\n\nNecessary condition analysis (NCA) may be used when the analyst is trying to determine the extent to which independent variable X allows variable Y (e.g., \"To what extent is a certain unemployment rate (X) necessary for a certain inflation rate (Y)?\"). Whereas (multiple) regression analysis uses additive logic where each X-variable can produce the outcome and the X's can compensate for each other (they are sufficient but not necessary), necessary condition analysis (NCA) uses necessity logic, where one or more X-variables allow the outcome to exist, but may not produce it (they are necessary but not sufficient). Each single necessary condition must be present and compensation is not possible.\n\nUsers may have particular data points of interest within a data set, as opposed to general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\n\nBarriers to effective analysis may exist among the analysts performing the data analysis or among the audience. Distinguishing fact from opinion, cognitive biases, and innumeracy are all challenges to sound data analysis.\n\nEffective analysis requires obtaining relevant facts to answer questions, support a conclusion or formal opinion, or test hypotheses. Facts by definition are irrefutable, meaning that any person involved in the analysis should be able to agree upon them. For example, in August 2010, the Congressional Budget Office (CBO) estimated that extending the Bush tax cuts of 2001 and 2003 for the 2011–2020 time period would add approximately $3.3 trillion to the national debt. Everyone should be able to agree that indeed this is what CBO reported; they can all examine the report. This makes it a fact. Whether persons agree or disagree with the CBO is their own opinion.\n\nAs another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects.\" This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\n\nThere are a variety of cognitive biases that can adversely affect analysis. For example, confirmation bias is the tendency to search for or interpret information in a way that confirms one's preconceptions. In addition, individuals may discredit information that does not support their views.\n\nAnalysts may be trained specifically to be aware of these biases and how to overcome them. In his book \"Psychology of Intelligence Analysis\", retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\n\nEffective analysts are generally adept with a variety of numerical techniques. However, audiences may not have such literacy with numbers or numeracy; they are said to be innumerate. Persons communicating the data may also be attempting to mislead or misinform, deliberately using bad numerical techniques.\n\nFor example, whether a number is rising or falling may not be the key factor. More important may be the number relative to another number, such as the size of government revenue or spending relative to the size of the economy (GDP) or the amount of cost relative to revenue in corporate financial statements. This numerical technique is referred to as normalization or common-sizing. There are many such techniques employed by analysts, whether adjusting for inflation (i.e., comparing real vs. nominal data) or considering population increases, demographics, etc. Analysts apply a variety of techniques to address the various quantitative messages described in the section above.\n\nAnalysts may also analyze data under different assumptions or scenarios. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock. Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.\n\nA data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\n\nAnalytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that use data to understand and analyze business performance.\n\nIn education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators’ data analyses.\n\nThis section contains rather technical explanations that may assist practitioners but are beyond the typical scope of a Wikipedia article.\n\nThe most important distinction between the initial data analysis phase and the main analysis phase, is that during initial data analysis one refrains from any analysis that is aimed at answering the original research question. The initial data analysis phase is guided by the following four questions:\n\nThe quality of the data should be checked as early as possible. Data quality can be assessed in several ways, using different types of analysis: frequency counts, descriptive statistics (mean, standard deviation, median), normality (skewness, kurtosis, frequency histograms, n: variables are compared with coding schemes of variables external to the data set, and possibly corrected if coding schemes are not comparable.\nThe choice of analyses to assess the data quality during the initial data analysis phase depends on the analyses that will be conducted in the main analysis phase.\n\nThe quality of the measurement instruments should only be checked during the initial data analysis phase when this is not the focus or research question of the study. One should check whether structure of measurement instruments corresponds to structure reported in the literature.\n\nThere are two ways to assess measurement: [NOTE: only one way seems to be listed]\n\nAfter assessing the quality of the data and of the measurements, one might decide to impute missing data, or to perform initial transformations of one or more variables, although this can also be done during the main analysis phase.\nPossible transformations of variables are:\n\nOne should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. \nIf the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.\nOther possible data distortions that should be checked are:\n\nIn any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.\nThe characteristics of the data sample can be assessed by looking at:\n\nDuring the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.\nAlso, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\n\nSeveral analyses can be used during the initial data analysis phase:\n\nIt is important to take the measurement levels of the variables into account for the analyses, as special statistical techniques are available for each level:\n\n\nNonlinear analysis will be necessary when the data is recorded from a nonlinear system. Nonlinear systems can exhibit complex dynamic effects including bifurcations, chaos, harmonics and subharmonics that cannot be analyzed using simple linear methods. Nonlinear data analysis is closely related to nonlinear system identification.\n\nIn the main analysis phase analyses aimed at answering the research question are performed as well as any other relevant analysis needed to write the first draft of the research report.\n\nIn the main analysis phase either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.\n\nExploratory data analysis should be interpreted carefully. When testing multiple models at once there is a high chance on finding at least one of them to be significant, but this can be due to a type 1 error. It is important to always adjust the significance level when testing multiple models with, for example, a Bonferroni correction. Also, one should not follow up an exploratory analysis with a confirmatory analysis in the same dataset. An exploratory analysis is used to find ideas for a theory, but not to test that theory as well. When a model is found exploratory in a dataset, then following up that analysis with a confirmatory analysis in the same dataset could simply mean that the results of the confirmatory analysis are due to the same type 1 error that resulted in the exploratory model in the first place. The confirmatory analysis therefore will not be more informative than the original exploratory analysis.\n\nIt is important to obtain some indication about how generalizable the results are. While this is hard to check, one can look at the stability of the results. Are the results reliable and reproducible? There are two main ways of doing this:\n\nMany statistical methods have been used for statistical analyses. A very brief list of four of the more popular methods is:\n\n\nDifferent companies or organizations hold a data analysis contests to encourage researchers utilize their data or to solve a particular question using data analysis. A few examples of well-known international data analysis contests are as follows. \n\n\n"}
{"id": "1182982", "url": "https://en.wikipedia.org/wiki?curid=1182982", "title": "Dual basis", "text": "Dual basis\n\nIn linear algebra, given a vector space \"V\" with a basis \"B\" of vectors indexed by an index set \"I\" (the cardinality of \"I\" is the dimensionality of \"V\"), its dual set is a set \"B\" of vectors in the dual space \"V\" with the same index set \"I\" such that \"B\" and \"B\" form a biorthogonal system. The dual set is always linearly independent but does not necessarily span \"V\". If it does span \"V\", then \"B\" is called the dual basis or reciprocal basis for the basis \"B\".\n\nDenoting the indexed vector sets as formula_1 and formula_2, being biorthogonal means that the elements pair to have an inner product equal to 1 if the indexes are equal, and equal to 0 otherwise. Symbolically, evaluating a dual vector in \"V\" on a vector in the original space \"V\":\nwhere formula_4 is the Kronecker delta symbol.\n\nAnother way to introduce the dual space of a vector space (module) is by introducing it in a categorical sense. To do this, let formula_5 be a module defined over the ring formula_6 (that is, formula_5 is an object in the category formula_8). Then we define the dual space of formula_5, denoted formula_10, to be formula_11, the module formed of all formula_6-linear module homomorphisms from formula_5 into formula_6. Note then that we may define a dual to the dual, referred to as the double dual of formula_5, written as formula_16, and defined as formula_17.\n\nTo formally construct a basis for the dual space, we shall now restrict our view to the case where formula_18 is a finite-dimensional free (left) formula_6-module, where formula_6 is a ring of unity. Then, we assume that the set formula_21 is a basis for formula_18. From here, we define the Kronecker Delta function formula_23 over the basis formula_21 by formula_25 if formula_26 and formula_27 if formula_28. Then the set formula_29 describes a linearly independent set with each formula_30. Since formula_18 is finite-dimensional, the basis formula_21 is of finite cardinality. Then, the set formula_33 is a basis to formula_34 and formula_34 is a free (right) formula_6-module.\n\nThe dual set always exists and gives an injection from \"V\" into \"V\", namely the mapping that sends \"v\" to \"v\". This says, in particular, that the dual space has dimension greater or equal to that of \"V\".\n\nHowever, the dual set of an infinite-dimensional \"V\" does not span its dual space \"V\". For example, consider the map \"w\" in \"V\" from \"V\" into the underlying scalars \"F\" given by for all \"i\". This map is clearly nonzero on all \"v\". If \"w\" were a finite linear combination of the dual basis vectors \"v\", say formula_37 for a finite subset \"K\" of \"I\", then for any \"j\" not in \"K\", formula_38, contradicting the definition of \"w\". So, this \"w\" does not lie in the span of the dual set.\n\nThe dual of an infinite-dimensional space has greater dimensionality (this being a greater infinite cardinality) than the original space has, and thus these cannot have a basis with the same indexing set. However, a dual set of vectors exists, which defines a subspace of the dual isomorphic to the original space. Further, for topological vector spaces, a continuous dual space can be defined, in which case a dual basis may exist.\n\nIn the case of finite-dimensional vector spaces, the dual set is always a dual basis and it is unique. These bases are denoted by and . If one denotes the evaluation of a covector on a vector as a pairing, the biorthogonality condition becomes:\n\nThe association of a dual basis with a basis gives a map from the space of bases of \"V\" to the space of bases of \"V\", and this is also an isomorphism. For topological fields such as the real numbers, the space of duals is a topological space, and this gives a homeomorphism between the Stiefel manifolds of bases of these spaces.\n\nTo perform operations with a vector, we must have a straightforward method of calculating its components. In a Cartesian frame the necessary operation is the dot product of the vector and the base vector. E.g.,\n\nwhere formula_41 is the bases in a Cartesian frame.The components of formula_42 can be found by\n\nIn a non-Cartesian frame, we do not necessarily have e · for all . However, it is always possible to find a vector e such that\n\nthe equality holds when e is the dual base of e\n\nIn a Cartesian frame, we have formula_45\n\nFor example, the standard basis vectors of R (the Cartesian plane) are\n\nand the standard basis vectors of its dual space R* are\n\nIn 3-dimensional Euclidean space, for a given basis {e, e, e}, you can find the biorthogonal (dual) basis {e, e, e} by formulas below:\n\nwhere denotes the transpose and\n\nis the volume of the parallelepiped formed by the basis vectors formula_50 and formula_51\n\n"}
{"id": "10523311", "url": "https://en.wikipedia.org/wiki?curid=10523311", "title": "Edward Bromhead", "text": "Edward Bromhead\n\nSir Edward Thomas Ffrench Bromhead, 2nd Baronet FRS FRSE (26 March 1789 – 14 March 1855) was a British landowner and mathematician best remembered as patron of the mathematician and physicist George Green.\n\nBorn the son of Gonville Bromhead, 1st Baronet Bromhead (grandfather of the British second in command of the same name at Rorke's Drift) and Lady Jane Ffrench, Baroness Ffrench, in Dublin. Bromhead was educated at the University of Glasgow and later at Caius College, Cambridge ( B.A. 1812, M.A. 1815) before taking up the study of law at the Inner Temple in London. He was elected a Fellow of the Royal Society in 1817. Returning to Lincolnshire, he became High Steward of Lincoln. He became the 2nd Bromhead baronet, of Thurlby Hall in 1822.\n\nWhile at Cambridge, Bromhead was a founder of the Analytical Society, a precursor of the Cambridge Philosophical Society, together with John Herschel, George Peacock and Charles Babbage, with whom he maintained a close and lifelong friendship. While he was, by all accounts, a gifted mathematician in his own right (although ill-health prevented him from pursuing his studies further), his greatest contribution to the subject is at second hand: having subscribed to the first publication of self-taught mathematician and physicist George Green, he encouraged Green to continue his research and to write further papers (which Bromhead sent on to be published in the Transactions of the Cambridge Philosophical Society and those of the Royal Society of Edinburgh).\n\nBromhead repeated his success by encouraging the young George Boole from Lincoln. Bromhead was President of the Lincoln Mechanics Institute in the Lincoln Greyfriars, where George Boole's father was the curator. Boole came to notice when he gave a lecture on the work of Sir Isaac Newton on 5 February 1835. The young Boole's development was fed by books that Bromhead supplied.\n\nBromhead lost his sight when he was old and he died unmarried at his home of Thurlby Hall in Thurlby, North Kesteven on 14 March 1855.\n\n\n"}
{"id": "1369166", "url": "https://en.wikipedia.org/wiki?curid=1369166", "title": "Elias omega coding", "text": "Elias omega coding\n\nElias ω coding or Elias omega coding is a universal code encoding the positive integers developed by Peter Elias. Like Elias gamma coding and Elias delta coding, it works by prefixing the integer with a representation of its order of magnitude in a universal code. Unlike those other two codes, however, Elias omega recursively encodes that prefix; thus, they are sometimes known as recursive Elias codes.\n\nOmega coding is used in applications where the largest encoded value is not known ahead of time, or to compress data in which small values are much more frequent than large values.\n\nTo code a number \"N\":\n\nTo decode an Elias omega-coded integer:\n\nOmega codes can be thought of as a number of \"groups\". A group is either a single 0 bit, which terminates the code, or two or more bits beginning with 1, which is followed by another group.\n\nThe first few codes are shown below. Included is the so-called \"implied distribution\", describing the distribution of values for which this coding yields a minimum-size code; see Relationship of universal codes to practical compression for details.\n\nThe encoding for 1 googol, 10, is 11 1000 101001100 (15 bits of length header) followed by the 333-bit binary representation of 1 googol, which is 10010 01001001 10101101 00100101 10010100 11000011 01111100 11101011 00001011 00100111 10000100 11000100 11001110 00001011 11110011 10001010 11001110 01000000 10001110 00100001 00011010 01111100 10101010 10110010 01000011 00001000 10101000 00101110 10001111 00010000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000 and a trailing 0, for a total of 349 bits.\n\nA googol to the hundredth power (10) is a 33220-bit binary number. Its omega encoding is 33,243 bits long: 11 1111 1000000111000100 (22 bits), followed by 33,220 bits of the value, and a trailing 0. Under Elias delta coding, the same number is 33,250 bits long: 000000000000000 1000000111000100 (31 bits) followed by 33,219 bits of the value. As log(10) = 33219.28, so in this instance, omega and delta coding are, respectively, only 0.07% and 0.09% longer than optimal.\n\nElias omega coding does not code zero or negative integers.\nOne way to code all non negative integers is to add 1 before coding and then subtract 1 after decoding.\nOne way to code all integers is to set up a bijection, mapping all integers (0, 1, -1, 2, -2, 3, -3, ...) to strictly positive integers (1, 2, 3, 4, 5, 6, 7, ...) before coding.\n\n\n\n"}
{"id": "1032170", "url": "https://en.wikipedia.org/wiki?curid=1032170", "title": "Equally spaced polynomial", "text": "Equally spaced polynomial\n\nAn equally spaced polynomial (ESP) is a polynomial used in finite fields, specifically GF(2) (binary).\n\nAn s-ESP of degree \"sm\" can be written as:\n\nor\n\nOver GF(2) the ESP has many interesting properties, including:\n\nA 1-ESP is known as an all one polynomial and has additional properties including the above.\n"}
{"id": "1166579", "url": "https://en.wikipedia.org/wiki?curid=1166579", "title": "Exponential formula", "text": "Exponential formula\n\nIn combinatorial mathematics, the exponential formula (called the polymer expansion in physics) states that the exponential generating function for structures on finite sets is the exponential of the exponential generating function for connected structures.\nThe exponential formula is a power-series version of a special case of Faà di Bruno's formula.\n\nFor any formal power series of the form\n\nwe have\n\nwhere\n\nand the index runs through the list of all partitions { \"S\", ..., \"S\" } of the set { 1, ..., \"n\" }. (When \"k\" = 0, the product is empty and by definition equals 1.)\n\nOne can write the formula in the following form:\nand thus\nwhere \"B\"(\"a\", ..., \"a\") is the \"n\"th complete Bell polynomial.\n\n\nIn applications, the numbers \"a\" often count the number of some sort of \"connected\" structure on an \"n\"-point set, and the numbers \"b\" count the number of (possibly disconnected) structures. The numbers \"b\"/\"n\"! count the number of isomorphism classes of structures on \"n\" points, with each structure being weighted by the reciprocal of its automorphism group, and the numbers \"a\"/\"n\"! count isomorphism classes of connected structures in the same way.\n\nIn quantum field theory and statistical mechanics, the partition functions \"Z\", or more generally correlation functions, are given by a formal sum over Feynman diagrams. The exponential formula shows that log(\"Z\") can be written as a sum over connected Feynman diagrams, in terms of connected correlation functions.\n\n"}
{"id": "16084455", "url": "https://en.wikipedia.org/wiki?curid=16084455", "title": "Feferman–Schütte ordinal", "text": "Feferman–Schütte ordinal\n\nIn mathematics, the Feferman–Schütte ordinal Γ is a large countable ordinal.\nIt is the proof theoretic ordinal of several mathematical theories, such as arithmetical transfinite recursion.\nIt is named after Solomon Feferman and Kurt Schütte.\n\nIt is sometimes said to be the first impredicative ordinal, though this is controversial, partly because there is no generally accepted precise definition of \"predicative\". Sometimes an ordinal is said to be predicative if it is less than Γ.\n\nThere is no standard notation for ordinals at and beyond the Feferman–Schütte ordinal, so there are several ways of representing it, some of which use ordinal collapsing functions: formula_1, formula_2 or formula_3\n\nThe Feferman–Schütte ordinal can be defined as the smallest ordinal that cannot be obtained by starting with 0 and using the operations of ordinal addition and the Veblen functions φ(β). That is, it is the smallest α such that φ(0) = α.\n\n"}
{"id": "36799559", "url": "https://en.wikipedia.org/wiki?curid=36799559", "title": "Finite subdivision rule", "text": "Finite subdivision rule\n\nIn mathematics, a finite subdivision rule is a recursive way of dividing a polygon or other two-dimensional shape into smaller and smaller pieces. Subdivision rules in a sense are generalizations of fractals. Instead of repeating exactly the same design over and over, they have slight variations in each stage, allowing a richer structure while maintaining the elegant style of fractals. Subdivision rules have been used in architecture, biology, and computer science, as well as in the study of hyperbolic manifolds. Substitution tilings are a well-studied type of subdivision rule.\n\nA subdivision rule takes a tiling of the plane by polygons and turns it into a new tiling by subdividing each polygon into smaller polygons. It is finite if there are only finitely many ways that every polygon can subdivide. Each way of subdividing a tile is called a tile type. Each tile type is represented by a label (usually a letter). Every tile type subdivides into smaller tile types. Each edge also gets subdivided according to finitely many edge types. Finite subdivision rules can only subdivide tilings that are made up of polygons labelled by tile types. Such tilings are called subdivision complexes for the subdivision rule. Given any subdivision complex for a subdivision rule, we can subdivide it over and over again to get a sequence of tilings.\n\nFor instance, binary subdivision has one tile type and one edge type:\nSince the only tile type is a quadrilateral, binary subdivision can only subdivide tilings made up of quadrilaterals. This means that the only subdivision complexes are tilings by quadrilaterals. The tiling can be regular, but doesn't have to be:\n\nHere we start with a complex made of four quadrilaterals and subdivide it twice. All quadrilaterals are type A tiles.\n\nBarycentric subdivision is an example of a subdivision rule with one edge type (that gets subdivided into two edges) and one tile type (a triangle that gets subdivided into 6 smaller triangles). Any triangulated surface is a barycentric subdivision complex.\n\nThe Penrose tiling can be generated by a subdivision rule on a set of four tile types (the curved lines in the table below only help to show how the tiles fit together):\n\nCertain rational maps give rise to finite subdivision rules. This includes most Lattès maps.\n\nEvery prime, non-split alternating knot or link complement has a subdivision rule, with some tiles that do not subdivide, corresponding to the boundary of the link complement. The subdivision rules show what the night sky would look like to someone living in a knot complement; because the universe wraps around itself (i.e. is not simply connected), an observer would see the visible universe repeat itself in an infinite pattern. The subdivision rule describes that pattern.\n\nThe subdivision rule looks different for different geometries. This is a subdivision rule for the trefoil knot, which is not a hyperbolic knot:\nAnd this is the subdivision rule for the Borromean rings, which is hyperbolic:\nIn each case, the subdivision rule would act on some tiling of a sphere (i.e. the night sky), but it is easier to just draw a small part of the night sky, corresponding to a single tile being repeatedly subdivided. This is what happens for the trefoil knot:\n\nAnd for the Borromean rings: \n\nSubdivision rules can easily be generalized to other dimensions. For instance, barycentric subdivision is used in all dimensions. Also, binary subdivision can be generalized to other dimensions (where hypercubes get divided by every midplane), as in the proof of the Heine–Borel theorem.\n\nA finite subdivision rule formula_1 consists of the following.\n\n1. A finite 2-dimensional CW complex formula_2, called the subdivision complex, with a fixed cell structure such that formula_2 is the union of its closed 2-cells. We assume that for each closed 2-cell formula_4 of formula_2 there is a CW structure formula_6 on a closed 2-disk such that formula_6 has at least two vertices, the vertices and edges of formula_6 are contained in formula_9, and the characteristic map formula_10 which maps onto formula_4 restricts to a homeomorphism onto each open cell.\n\n2. A finite two dimensional CW complex formula_12, which is a subdivision of formula_2.\n\n3.A continuous cellular map formula_14 called the subdivision map, whose restriction to every open cell is a homeomorphism onto an open cell.\n\nEach CW complex formula_6 in the definition above (with its given characteristic map formula_16) is called a tile type.\n\nAn formula_17-complex for a subdivision rule formula_17 is a 2-dimensional CW complex formula_19 which is the union of its closed 2-cells, together with a continuous cellular map formula_20 whose restriction to each open cell is a homeomorphism. We can subdivide formula_19 into a complex formula_22 by requiring that the induced map formula_23 restricts to a homeomorphism onto each open cell. formula_22 is again an formula_17-complex with map formula_26. By repeating this process, we obtain a sequence of subdivided formula_17-complexes formula_28 with maps formula_29.\n\nBinary subdivision is one example:\n\nThe subdivision complex can be created by gluing together the opposite edges of the square, making the subdivision complex formula_2 into a torus. The subdivision map formula_31 is the doubling map on the torus, wrapping the meridian around itself twice and the longitude around itself twice. This is a four-fold covering map. The plane, tiled by squares, is a subdivision complex for this subdivision rule, with the structure map formula_32 given by the standard covering map. Under subdivision, each square in the plane gets subdivided into squares of one-fourth the size.\n\nSubdivision rules can be used to study the quasi-isometry properties of certain spaces. Given a subdivision rule formula_17 and subdivision complex formula_19, we can construct a graph called the history graph that records the action of the subdivision rule. The graph consists of the dual graphs of every stage formula_28, together with edges connecting each tile in formula_28 with its subdivisions in formula_37.\n\nThe quasi-isometry properties of the history graph can be studied using subdivision rules. For instance, the history graph is quasi-isometric to hyperbolic space exactly when the subdivision rule is conformal, as described in the combinatorial Riemann mapping theorem.\n\nIslamic Girih tiles in Islamic architecture are self-similar tilings that can be modeled with finite subdivision rules. In 2007, Peter J. Lu of Harvard University and Professor Paul J. Steinhardt of Princeton University published a paper in the journal \"Science\" suggesting that girih tilings possessed properties consistent with self-similar fractal quasicrystalline tilings such as Penrose tilings (presentation 1974, predecessor works starting in about 1964) predating them by five centuries.\n\nSubdivision surfaces in computer graphics use subdivision rules to refine a surface to any given level of precision. These subdivision surfaces (such as the Catmull-Clark subdivision surface) take a polygon mesh (the kind used in 3D animated movies) and refines it to a mesh with more polygons by adding and shifting points according to different recursive formulas. Although many points get shifted in this process, each new mesh is combinatorially a subdivision of the old mesh (meaning that for every edge and vertex of the old mesh, you can identify a corresponding edge and vertex in the new one, plus several more edges and vertices).\n\nSubdivision rules were applied by Cannon, Floyd and Parry (2000) to the study of large-scale growth patterns of biological organisms. Cannon, Floyd and Parry produced a mathematical growth model which demonstrated that some systems determined by simple finite subdivision rules can results in objects (in their example, a tree trunk) whose large-scale form oscillates wildly over time even though the local subdivision laws remain the same. Cannon, Floyd and Parry also applied their model to the analysis of the growth patterns of rat tissue. They suggested that the \"negatively curved\" (or non-euclidean) nature of microscopic growth patterns of biological organisms is one of the key reasons why large-scale organisms do not look like crystals or polyhedral shapes but in fact in many cases resemble self-similar fractals. In particular they suggested that such \"negatively curved\" local structure is manifested in highly folded and highly connected nature of the brain and the lung tissue.\n\nCannon, Floyd, and Parry first studied finite subdivision rules in an attempt to prove the following conjecture:\n\nCannon's conjecture: Every Gromov hyperbolic group with a 2-sphere at infinity acts geometrically on hyperbolic 3-space.\n\nHere, a geometric action is a cocompact, properly discontinuous action by isometries. This conjecture was partially solved by Grigori Perelman in his proof of the Geometrization conjecture, which states (in part) than any Gromov hyperbolic group that is a 3-manifold group must act geometrically on hyperbolic 3-space. However, it still remains to show that a Gromov hyperbolic group with a 2-sphere at infinity is a 3-manifold group.\n\nCannon and Swenson showed that a hyperbolic group with a 2-sphere at infinity has an associated subdivision rule. If this subdivision rule is conformal in a certain sense, the group will be a 3-manifold group with the geometry of hyperbolic 3-space.\n\nSubdivision rules give a sequence of tilings of a surface, and tilings give an idea of distance, length, and area (by letting each tile have length and area 1). In the limit, the distances that come from these tilings may converge in some sense to an analytic structure on the surface. The Combinatorial Riemann Mapping Theorem gives necessary and sufficient conditions for this to occur.\n\nIts statement needs some background. A tiling formula_38 of a ring formula_17 (i.e., a closed annulus) gives two invariants, formula_40 and formula_41, called approximate moduli. These are similar to the classical modulus of a ring. They are defined by the use of weight functions. A weight function formula_42 assigns a non-negative number called a weight to each tile of formula_38. Every path in formula_17 can be given a length, defined to be the sum of the weights of all tiles in the path. Define the height formula_45 of formula_17 under formula_42 to be the infimum of the length of all possible paths connecting the inner boundary of formula_17 to the outer boundary. The circumference formula_49 of formula_17 under formula_42 is the infimum of the length of all possible paths circling the ring (i.e. not nullhomotopic in R). The areaformula_52 of formula_17 under formula_42 is defined to be the sum of the squares of all weights in formula_17. Then define\n\nNote that they are invariant under scaling of the metric.\n\nA sequence formula_58 of tilings is conformal (formula_59) if mesh approaches 0 and:\n\n\nIf a sequence formula_58 of tilings of a surface is conformal (formula_59) in the above sense, then there is a conformal structure on the surface and a constant formula_77 depending only on formula_59 in which the classical moduli and approximate moduli (from formula_79 for formula_63 sufficiently large) of any given annulus are formula_77-comparable, meaning that they lie in a single interval formula_82.\n\nThe Combinatorial Riemann Mapping Theorem implies that a group formula_83 acts geometrically on formula_84 if and only if it is Gromov hyperbolic, it has a sphere at infinity, and the natural subdivision rule on the sphere gives rise to a sequence of tilings that is conformal in the sense above. Thus, Cannon's conjecture would be true if all such subdivision rules were conformal.\n\n"}
{"id": "34824761", "url": "https://en.wikipedia.org/wiki?curid=34824761", "title": "Fraïssé's theorem", "text": "Fraïssé's theorem\n\nIn mathematics, Fraïssé's theorem, named after Roland Fraïssé, states that a class \"K\" of finite relational structures is the age of a countable homogeneous relational structure if and only if it satisfies the following four conditions:\n\n\nIf these conditions hold, then the countable homogeneous structure whose age is \"K\" is unique up to isomorphism.\n\nFraïssé proved the theorem in the 1950s. \n\nFor a proof and more details see Section 1.2 and Appendix A of this thesis. \n"}
{"id": "7819462", "url": "https://en.wikipedia.org/wiki?curid=7819462", "title": "Goodman–Nguyen–van Fraassen algebra", "text": "Goodman–Nguyen–van Fraassen algebra\n\nA Goodman–Nguyen–van Fraassen algebra is a type of conditional event algebra (CEA) that embeds the standard Boolean algebra of unconditional events in a larger algebra which is itself Boolean. The goal (as with all CEAs) is to equate the conditional probability \"P\"(\"A\" ∩ \"B\") / \"P\"(\"A\") with the probability of a conditional event, \"P\"(\"A\" → \"B\") for more than just trivial choices of \"A\", \"B\", and \"P\".\n\nGiven set Ω, which is the set of possible outcomes, and set \"F\" of subsets of Ω—so that \"F\" is the set of possible events—consider an infinite Cartesian product of the form \"E\" × \"E\" × … × \"E\" × Ω × Ω × Ω × …, where \"E\", \"E\", … \"E\" are members of \"F\". Such a product specifies the set of all infinite sequences whose first element is in \"E\", whose second element is in \"E\", …, and whose \"n\"th element is in \"E\", and all of whose elements are in Ω. Note that one such product is the one where \"E\" = \"E\" = … = \"E\" = Ω, i.e., the set Ω × Ω × Ω × Ω × …. Designate this set as formula_1; it is the set of all infinite sequences whose elements are in Ω.\n\nA new Boolean algebra is now formed, whose elements are subsets of formula_1. To begin with, any event which was formerly represented by subset \"A\" of Ω is now represented by formula_3 = \"A\" × Ω × Ω × Ω × ….\n\nAdditionally, however, for events \"A\" and \"B\", let the conditional event \"A\" → \"B\" be represented as the following infinite union of disjoint sets: \n\nThe motivation for this representation of conditional events will be explained shortly. Note that the construction can be iterated; \"A\" and \"B\" can themselves be conditional events.\n\nIntuitively, unconditional event \"A\" ought to be representable as conditional event Ω → \"A\". And indeed: because Ω ∩ \"A\" = \"A\" and Ω′ = ∅, the infinite union representing Ω → \"A\" reduces to \"A\" × Ω × Ω × Ω × ….\n\nLet formula_4 now be a set of subsets of formula_1, which contains representations of all events in \"F\" and is otherwise just large enough to be closed under construction of conditional events and under the familiar Boolean operations. formula_4 is a Boolean algebra of conditional events which contains a Boolean algebra corresponding to the algebra of ordinary events.\n\nCorresponding to the newly constructed logical objects, called conditional events, is a new definition of a probability function, formula_7, based on a standard probability function \"P\": \n\nIt follows from the definition of formula_7 that formula_7 (formula_3) = \"P\"(\"A\"). Thus formula_7 = \"P\" over the domain of \"P\".\n\nNow comes the insight that motivates all of the preceding work. For \"P\", the original probability function, \"P\"(\"A\"′) = 1 – \"P\"(\"A\"), and therefore \"P\"(\"B\"|\"A\") = \"P\"(\"A\" ∩ \"B\") / \"P\"(\"A\") can be rewritten as \"P\"(\"A\" ∩ \"B\") / [1 – \"P\"(\"A\"′)]. The factor 1 / [1 – \"P\"(\"A\"′)], however, can in turn be represented by its Maclaurin series expansion, 1 + \"P\"(\"A\"′) + \"P\"(\"A\"′) …. Therefore, \"P\"(\"B\"|\"A\") = \"P\"(\"A\" ∩ \"B\") + \"P\"(\"A\"′)\"P\"(\"A\" ∩ \"B\") + \"P\"(\"A\"′)\"P\"(\"A\" ∩ \"B\") + ….\n\nThe right side of the equation is exactly the expression for the probability formula_7 of \"A\" → \"B\", just defined as a union of carefully chosen disjoint sets. Thus that union can be taken to represent the conditional event \"A\"→ \"B\", such that formula_7(\"A\" → \"B\") = \"P\"(\"B\"|\"A\") for any choice of \"A\", \"B\", and \"P\". But since formula_7 = \"P\" over the domain of \"P\", the hat notation is optional. So long as the context is understood (i.e., conditional event algebra), one can write \"P\"(\"A\" → \"B\") = \"P\"(\"B\"|\"A\"), with \"P\" now being the extended probability function.\n\nBamber, Donald, I. R. Goodman, and H. T. Nguyen. 2004. \"Deduction from Conditional Knowledge\". \"Soft Computing\" 8: 247–255.\n\nGoodman, I. R., R. P. S. Mahler, and H. T. Nguyen. 1999. \"What is conditional event algebra and why should you care?\" \"SPIE Proceedings\", Vol 3720.\n"}
{"id": "55835082", "url": "https://en.wikipedia.org/wiki?curid=55835082", "title": "Gunning Victoria Jubilee Prize", "text": "Gunning Victoria Jubilee Prize\n\nThe Gunning Victoria Jubilee Prize Lectureship is a quadrennial award made by the Royal Society of Edinburgh to recognise original work done by scientists resident in or connected with Scotland.\n\nThe award was founded in 1887 by Dr Robert Halliday Gunning, a Scottish surgeon, entrepreneur and philanthropist who spent much of his life in Brazil.\n\nAwards by a similar name have also been awarded by the University of Edinburgh.\n\nSource: Royal Society of Edinburgh\n\n"}
{"id": "44939247", "url": "https://en.wikipedia.org/wiki?curid=44939247", "title": "Gunther Schmidt", "text": "Gunther Schmidt\n\nGunther Schmidt (born 1939, Rüdersdorf) is a German mathematician who works also in informatics.\n\nSchmidt began studying Mathematics in 1957 at Göttingen University, studying with Carl Ludwig Siegel who had returned to Göttingen after spending time in the United States during \nWWII. In 1960 he transferred to Ludwig-Maximilians-Universität München where he studied functions of several complex variables with Karl Stein. Schmidt wrote a thesis on analytic continuation of such functions.\n\nIn 1962 Schmidt began work at TU München with students of Robert Sauer, in the beginning in labs and tutorials, later in mentoring and administration. Schmidt's interests turned toward programming when he collaborated with Hans Langmaack on rewriting and the braid group in 1969. Friedrich L. Bauer and Klaus Samelson were establishing software engineering at the university and Schmidt joined their group in 1974. In 1977 he submitted his Habilitation \"Programs as partial graphs\".\n\nHe became a professor in 1980. Shortly after that, he was appointed to hold the chair of the late Klaus Samelson for one and a half years. From 1988 until his retirement in 2004, he held a professorship at the Faculty for Computer Science of the Universität der Bundeswehr München. He was a classroom instructor for beginners courses as well as special courses in mathematical logic, semantics of programming languages, construction of compilers, and algorithmic languages. Working with Thomas Strohlein, he authored a textbook on relations and graphs, published in German in 1989 and English in 1993.\n\nThe calculus of relations had a relatively low profile among mathematical topics in the twentieth century, but Schmidt and others have raised that profile. The partial order of binary relations can be organized by grouping through closure. In 2018 Schmidt and Michael Winter published \"Relational Topology\" to exhibit topologies of relations using algebraic logic.\n\nIn 2001 he became involved in a large project (17 nations) with the European Cooperation in Science and Technology: Schmidt was chairman of project COST 274 TARSKI (Theory and Application of Relational Structures as Knowledge Instruments).\n\nGunther Schmidt is mainly known for his work on Relational Mathematics; he was co-founder of the RAMiCS conference series.\n\n\n\n\n"}
{"id": "31272108", "url": "https://en.wikipedia.org/wiki?curid=31272108", "title": "Institute of Statistics (Albania)", "text": "Institute of Statistics (Albania)\n\nThe Institute of Statistics (INSTAT) (), is the statistical agency of Albania. It was established in 1940.\n\nThe first statistical office in Albania was opened in 1924. At the time it collected economic data on the Ministry of Public Affairs and Agriculture of Albania. The activity of this office was limited to agriculture inventories that consisted in registration of the number of farmers, type of land and the use of agriculture livestock plants, as well as in a few statistics on industry, trade, export-import and prices.\n\nThe statistical service was for the first time institutionalized by the decision no. 121, dated. 8 April 1940. The statistical service system was established by the decision no. 35, dated 13/01/1945 with the creation of Statistics Department under the authority of the Council of Ministers of Albania. This Department was put later under the authority of The State Plan Commission.\n\nToday the institute operates under the direct authority of the Council of Ministers.\n\n"}
{"id": "20647689", "url": "https://en.wikipedia.org/wiki?curid=20647689", "title": "Irrational number", "text": "Irrational number\n\nIn mathematics, the irrational numbers are all the real numbers which are not rational numbers, the latter being the numbers constructed from ratios (or fractions) of integers. When the ratio of lengths of two line segments is an irrational number, the line segments are also described as being \"incommensurable\", meaning that they share no \"measure\" in common, that is, there is no length (\"the measure\"), no matter how short, that could be used to express the lengths of both of the two given segments as integer multiples of itself.\n\nAmong irrational numbers are the ratio of a circle's circumference to its diameter, Euler's number \"e\", the golden ratio \"φ\", and the square root of two; in fact all square roots of natural numbers, other than of perfect squares, are irrational.\n\nIt can be shown that irrational numbers, when expressed in a positional numeral system (e.g. as decimal numbers, or with any other natural basis), do not terminate, nor do they repeat, i.e., do not contain a subsequence of digits, the repetition of which makes up the tail of the representation. For example, the decimal representation of the number starts with 3.14159, but no finite number of digits can represent exactly, nor does it repeat. The proof that the decimal expansion of a rational number must terminate or repeat is distinct from the proof that a decimal expansion that terminates or repeats must be a rational number, and although elementary and not lengthy, both proofs take some work. Mathematicians do not generally take \"terminating or repeating\" to be the definition of the concept of rational number.\n\nIrrational numbers may also be dealt with via non-terminating continued fractions.\n\nAs a consequence of Cantor's proof that the real numbers are uncountable and the rationals countable, it follows that almost all real numbers are irrational.\n\nThe first proof of the existence of irrational numbers is usually attributed to a Pythagorean (possibly Hippasus of Metapontum), who probably discovered them while identifying sides of the pentagram.\nThe then-current Pythagorean method would have claimed that there must be some sufficiently small, indivisible unit that could fit evenly into one of these lengths as well as the other. However, Hippasus, in the 5th century BC, was able to deduce that there was in fact no common unit of measure, and that the assertion of such an existence was in fact a contradiction. He did this by demonstrating that if the hypotenuse of an isosceles right triangle was indeed commensurable with a leg, then one of those lengths measured in that unit of measure must be both odd and even, which is impossible. His reasoning is as follows:\n\nGreek mathematicians termed this ratio of incommensurable magnitudes \"alogos\", or inexpressible. Hippasus, however, was not lauded for his efforts: according to one legend, he made his discovery while out at sea, and was subsequently thrown overboard by his fellow Pythagoreans “…for having produced an element in the universe which denied the…doctrine that all phenomena in the universe can be reduced to whole numbers and their ratios.” Another legend states that Hippasus was merely exiled for this revelation. Whatever the consequence to Hippasus himself, his discovery posed a very serious problem to Pythagorean mathematics, since it shattered the assumption that number and geometry were inseparable–a foundation of their theory.\n\nThe discovery of incommensurable ratios was indicative of another problem facing the Greeks: the relation of the discrete to the continuous. Brought into light by Zeno of Elea, who questioned the conception that quantities are discrete and composed of a finite number of units of a given size. Past Greek conceptions dictated that they necessarily must be, for “whole numbers represent discrete objects, and a commensurable ratio represents a relation between two collections of discrete objects.” However Zeno found that in fact “[quantities] in general are not discrete collections of units; this is why ratios of incommensurable [quantities] appear….[Q]uantities are, in other words, continuous.” What this means is that, contrary to the popular conception of the time, there cannot be an indivisible, smallest unit of measure for any quantity. That in fact, these divisions of quantity must necessarily be infinite. For example, consider a line segment: this segment can be split in half, that half split in half, the half of the half in half, and so on. This process can continue infinitely, for there is always another half to be split. The more times the segment is halved, the closer the unit of measure comes to zero, but it never reaches exactly zero. This is just what Zeno sought to prove. He sought to prove this by formulating four paradoxes, which demonstrated the contradictions inherent in the mathematical thought of the time. While Zeno’s paradoxes accurately demonstrated the deficiencies of current mathematical conceptions, they were not regarded as proof of the alternative. In the minds of the Greeks, disproving the validity of one view did not necessarily prove the validity of another, and therefore further investigation had to occur.\n\nThe next step was taken by Eudoxus of Cnidus, who formalized a new theory of proportion that took into account commensurable as well as incommensurable quantities. Central to his idea was the distinction between magnitude and number. A magnitude “...was not a number but stood for entities such as line segments, angles, areas, volumes, and time which could vary, as we would say, continuously. Magnitudes were opposed to numbers, which jumped from one value to another, as from 4 to 5.” Numbers are composed of some smallest, indivisible unit, whereas magnitudes are infinitely reducible. Because no quantitative values were assigned to magnitudes, Eudoxus was then able to account for both commensurable and incommensurable ratios by defining a ratio in terms of its magnitude, and proportion as an equality between two ratios. By taking quantitative values (numbers) out of the equation, he avoided the trap of having to express an irrational number as a number. “Eudoxus’ theory enabled the Greek mathematicians to make tremendous progress in geometry by supplying the necessary logical foundation for incommensurable ratios.” This incommensurability is dealt with in Euclid's Elements, Book X, Proposition 9.\n\nAs a result of the distinction between number and magnitude, geometry became the only method that could take into account incommensurable ratios. Because previous numerical foundations were still incompatible with the concept of incommensurability, Greek focus shifted away from those numerical conceptions such as algebra and focused almost exclusively on geometry. In fact, in many cases algebraic conceptions were reformulated into geometrical terms. This may account for why we still conceive of x or x as x squared and x cubed instead of x second power and x third power. Also crucial to Zeno’s work with incommensurable magnitudes was the fundamental focus on deductive reasoning that resulted from the foundational shattering of earlier Greek mathematics. The realization that some basic conception within the existing theory was at odds with reality necessitated a complete and thorough investigation of the axioms and assumptions that underlie that theory. Out of this necessity, Eudoxus developed his method of exhaustion, a kind of reductio ad absurdum that “…established the deductive organization on the basis of explicit axioms…” as well as “…reinforced the earlier decision to rely on deductive reasoning for proof.” This method of exhaustion is the first step in the creation of calculus.\n\nTheodorus of Cyrene proved the irrationality of the surds of whole numbers up to 17, but stopped there probably because the algebra he used could not be applied to the square root of 17.\n\nIt was not until Eudoxus developed a theory of proportion that took into account irrational as well as rational ratios that a strong mathematical foundation of irrational numbers was created.\n\nGeometrical and mathematical problems involving irrational numbers such as square roots were addressed very early during the Vedic period in India. There are references to such calculations in the \"Samhitas\", \"Brahmanas\", and the \"Shulba Sutras\" (800 BC or earlier). (See Bag, Indian Journal of History of Science, 25(1-4), 1990).\n\nIt is suggested that the concept of irrationality was implicitly accepted by Indian mathematicians since the 7th century BC, when Manava (c. 750 – 690 BC) believed that the square roots of numbers such as 2 and 61 could not be exactly determined. However, historian Carl Benjamin Boyer writes that \"such claims are not well substantiated and unlikely to be true\".\n\nIt is also suggested that Aryabhata (5th century AD), in calculating a value of pi to 5 significant figures, used the word āsanna (approaching), to mean that not only is this an approximation but that the value is incommensurable (or irrational).\n\nLater, in their treatises, Indian mathematicians wrote on the arithmetic of surds including addition, subtraction, multiplication, rationalization, as well as separation and extraction of square roots.\n\nMathematicians like Brahmagupta (in 628 AD) and Bhaskara I (in 629 AD) made contributions in this area as did other mathematicians who followed. In the 12th century Bhaskara II evaluated some of these formulas and critiqued them, identifying their limitations.\n\nDuring the 14th to 16th centuries, Madhava of Sangamagrama and the Kerala school of astronomy and mathematics discovered the infinite series for several irrational numbers such as \"π\" and certain irrational values of trigonometric functions. Jyeṣṭhadeva provided proofs for these infinite series in the \"Yuktibhāṣā\".\n\nIn the Middle ages, the development of algebra by Muslim mathematicians allowed irrational numbers to be treated as \"algebraic objects\". Middle Eastern mathematicians also merged the concepts of \"number\" and \"magnitude\" into a more general idea of real numbers, criticized Euclid's idea of ratios, developed the theory of composite ratios, and extended the concept of number to ratios of continuous magnitude. In his commentary on Book 10 of the \"Elements\", the Persian mathematician Al-Mahani (d. 874/884) examined and classified quadratic irrationals and cubic irrationals. He provided definitions for rational and irrational magnitudes, which he treated as irrational numbers. He dealt with them freely but explains them in geometric terms as follows:\n\nIn contrast to Euclid's concept of magnitudes as lines, Al-Mahani considered integers and fractions as rational magnitudes, and square roots and cube roots as irrational magnitudes. He also introduced an arithmetical approach to the concept of irrationality, as he attributes the following to irrational magnitudes:\n\nThe Egyptian mathematician Abū Kāmil Shujā ibn Aslam (c. 850 – 930) was the first to accept irrational numbers as solutions to quadratic equations or as coefficients in an equation, often in the form of square roots, cube roots and fourth roots. In the 10th century, the Iraqi mathematician Al-Hashimi provided general proofs (rather than geometric demonstrations) for irrational numbers, as he considered multiplication, division, and other arithmetical functions. Iranian mathematician, Abū Ja'far al-Khāzin (900–971) provides a definition of rational and irrational magnitudes, stating that if a definite quantity is:\n\nMany of these concepts were eventually accepted by European mathematicians sometime after the Latin translations of the 12th century. Al-Hassār, a Moroccan mathematician from Fez specializing in Islamic inheritance jurisprudence during the 12th century, first mentions the use of a fractional bar, where numerators and denominators are separated by a horizontal bar. In his discussion he writes, \"..., for example, if you are told to write three-fifths and a third of a fifth, write thus, formula_1.\" This same fractional notation appears soon after in the work of Leonardo Fibonacci in the 13th century.\n\nThe 17th century saw imaginary numbers become a powerful tool in the hands of Abraham de Moivre, and especially of Leonhard Euler. The completion of the theory of complex numbers in the 19th century entailed the differentiation of irrationals into algebraic and transcendental numbers, the proof of the existence of transcendental numbers, and the resurgence of the scientific study of the theory of irrationals, largely ignored since Euclid. The year 1872 saw the publication of the theories of Karl Weierstrass (by his pupil Ernst Kossak), Eduard Heine (\"Crelle's Journal\", 74), Georg Cantor (Annalen, 5), and Richard Dedekind. Méray had taken in 1869 the same point of departure as Heine, but the theory is generally referred to the year 1872. Weierstrass's method has been completely set forth by Salvatore Pincherle in 1880, and Dedekind's has received additional prominence through the author's later work (1888) and the endorsement by Paul Tannery (1894). Weierstrass, Cantor, and Heine base their theories on infinite series, while Dedekind founds his on the idea of a cut (Schnitt) in the system of all rational numbers, separating them into two groups having certain characteristic properties. The subject has received later contributions at the hands of Weierstrass, Leopold Kronecker (Crelle, 101), and Charles Méray.\n\nContinued fractions, closely related to irrational numbers (and due to Cataldi, 1613), received attention at the hands of Euler, and at the opening of the 19th century were brought into prominence through the writings of Joseph-Louis Lagrange. Dirichlet also added to the general theory, as have numerous contributors to the applications of the subject.\n\nJohann Heinrich Lambert proved (1761) that π cannot be rational, and that \"e\" is irrational if \"n\" is rational (unless \"n\" = 0). While Lambert's proof is often called incomplete, modern assessments support it as satisfactory, and in fact for its time it is unusually rigorous. Adrien-Marie Legendre (1794), after introducing the Bessel–Clifford function, provided a proof to show that π is irrational, whence it follows immediately that π is irrational also. The existence of transcendental numbers was first established by Liouville (1844, 1851). Later, Georg Cantor (1873) proved their existence by a different method, which showed that every interval in the reals contains transcendental numbers. Charles Hermite (1873) first proved \"e\" transcendental, and Ferdinand von Lindemann (1882), starting from Hermite's conclusions, showed the same for π. Lindemann's proof was much simplified by Weierstrass (1885), still further by David Hilbert (1893), and was finally made elementary by Adolf Hurwitz and Paul Gordan.\n\nThe square root of 2 was the first number proved irrational, and that article contains a number of proofs. The golden ratio is another famous quadratic irrational and there is a simple proof of its irrationality in its article. The square roots of all natural numbers which are not perfect squares are irrational and a proof may be found in quadratic irrationals.\n\nThe proof above for the square root of two can be generalized using the fundamental theorem of arithmetic. This asserts that every integer has a unique factorization into primes. Using it we can show that if a rational number is not an integer then no integral power of it can be an integer, as in lowest terms there must be a prime in the denominator that does not divide into the numerator whatever power each is raised to. Therefore, if an integer is not an exact \"k\" power of another integer, then that first integer's \"k\" root is irrational.\n\nPerhaps the numbers most easy to prove irrational are certain logarithms. Here is a proof by contradiction that log 3 is irrational. Notice that log 3 ≈ 1.58 > 0.\n\nAssume log 3 is rational. For some positive integers \"m\" and \"n\", we have\n\nIt follows that\n\nHowever, the number 2 raised to any positive integer power must be even (because it is divisible by 2) and the number 3 raised to any positive integer power must be odd (since none of its prime factors will be 2). Clearly, an integer cannot be both odd and even at the same time: we have a contradiction. The only assumption we made was that log 3 is rational (and so expressible as a quotient of integers \"m\"/\"n\" with \"n\" ≠ 0). The contradiction means that this assumption must be false, i.e. log 3 is irrational, and can never be expressed as a quotient of integers \"m\"/\"n\" with \"n\" ≠ 0.\n\nCases such as log 2 can be treated similarly.\n\nAlmost all irrational numbers are transcendental and all real transcendental numbers are irrational (there are also complex transcendental numbers): the article on transcendental numbers lists several examples. So \"e\" and π are irrational for all nonzero rational \"r\", and, e.g., \"e\" is irrational, too.\n\nIrrational numbers can also be found within the countable set of real algebraic numbers (essentially defined as the real roots of polynomials with integer coefficients), i.e., as real solutions of polynomial equations\n\nwhere the coefficients formula_7 are integers and formula_8. Any rational root of this polynomial equation must be of the form \"r\" /\"s\", where \"r\" is a divisor of \"a\" and \"s\" is a divisor of \"a\". If a real root formula_9 of a polynomial formula_10 is not among these finitely many possibilities, it must be an irrational algebraic number. An exemplary proof for the existence of such algebraic irrationals is by showing that \"x\"  = (2 + 1) is an irrational root of a polynomial with integer coefficients: it satisfies (\"x\" − 1) = 2 and hence \"x\" − 2\"x\" − 1 = 0, and this latter polynomial has no rational roots (the only candidates to check are ±1, and \"x\", being greater than 1, is neither of these), so \"x\" is an irrational algebraic number.\n\nBecause the algebraic numbers form a subfield of the real numbers, many irrational real numbers can be constructed by combining transcendental and algebraic numbers. For example, 3 + 2,  +  and \"e\" are irrational (and even transcendental).\n\nThe decimal expansion of an irrational number never repeats or terminates (the latter being equivalent to repeating zeroes), unlike any rational number. The same is true for binary, octal or hexadecimal expansions, and in general for expansions in every positional notation with natural bases.\n\nTo show this, suppose we divide integers \"n\" by \"m\" (where \"m\" is nonzero). When long division is applied to the division of \"n\" by \"m\", only \"m\" remainders are possible. If 0 appears as a remainder, the decimal expansion terminates. If 0 never occurs, then the algorithm can run at most \"m\" − 1 steps without using any remainder more than once. After that, a remainder must recur, and then the decimal expansion repeats.\n\nConversely, suppose we are faced with a repeating decimal, we can prove that it is a fraction of two integers. For example, consider:\n\nHere the repetend is 162 and the length of the repetend is 3. First, we multiply by an appropriate power of 10 to move the decimal point to the right so that it is just in front of a repetend. In this example we would multiply by 10 to obtain:\n\nNow we multiply this equation by 10 where \"r\" is the length of the repetend. This has the effect of moving the decimal point to be in front of the \"next\" repetend. In our example, multiply by 10:\n\nThe result of the two multiplications gives two different expressions with exactly the same \"decimal portion\", that is, the tail end of 10,000\"A\" matches the tail end of 10\"A\" exactly. Here, both 10,000\"A\" and 10\"A\" have .162162162 ... at the end.\n\nTherefore, when we subtract the 10\"A\" equation from the 10,000\"A\" equation, the tail end of 10\"A\" cancels out the tail end of 10,000\"A\" leaving us with:\n\nThen\n\nis a ratio of integers and therefore a rational number, as required for the proof.\n\nDov Jarden gave a simple non-constructive proof that there exist two irrational numbers \"a\" and \"b\", such that \"a\" is rational:\n\nConsider ; if this is rational, then take \"a\" = \"b\" = . Otherwise, take \"a\" to be the irrational number and \"b\" = . Then \"a\" = () = = = 2, which is rational.\n\nAlthough the above argument does not decide between the two cases, the Gelfond–Schneider theorem shows that is transcendental, hence irrational. This theorem states that if \"a\" and \"b\" are both algebraic numbers, and \"a\" is not equal to 0 or 1, and \"b\" is not a rational number, then any value of \"a\" is a transcendental number (there can be more than one value if complex number exponentiation is used).\n\nAn example that provides a simple constructive proof is\n\nThe base of the left side is irrational and the right side is rational, so one must prove that the exponent on the left side, formula_17, is irrational. This is so because, by the formula relating logarithms with different bases,\n\nwhich we can assume, for the sake of establishing a contradiction, equals a ratio \"m/n\" of positive integers. Then formula_19 hence formula_20 hence formula_21 hence formula_22, which is a contradictory pair of prime factorizations and hence violates the fundamental theorem of arithmetic (unique prime factorization).\n\nA stronger result is the following: Every rational number in the interval formula_23 can be written either as \"a\" for some irrational number \"a\" or as \"n\" for some natural number \"n\". Similarly, every positive rational number can be written either as formula_24 for some irrational number \"a\" or as formula_25 for some natural number \"n\".\n\nIt is not known whether formula_26 (or formula_27) is irrational. In fact, there is no pair of non-zero integers formula_28 for which it is known whether formula_29 is irrational. Moreover, it is not known whether or not the set formula_30 is algebraically independent over formula_31.\n\nIt is not known whether formula_32 Catalan's constant, or the Euler–Mascheroni gamma constant formula_33 are irrational.\nIt is not known if either of the tetrations formula_34 or formula_35 is rational for some integer formula_36.\n\nSince the reals form an uncountable\nset, of which the rationals are a countable subset, the complementary set of\nirrationals is uncountable.\n\nUnder the usual (Euclidean) distance function d(\"x\", \"y\") = |\"x\" − \"y\"|, the real numbers are a metric space and hence also a topological space. Restricting the Euclidean distance function gives the irrationals the structure of a metric space. Since the subspace of irrationals is not closed,\nthe induced metric is not complete. However, being a G-delta set—i.e., a countable intersection of open subsets—in a complete metric space, the space of irrationals is completely metrizable: that is, there is a metric on the irrationals inducing the same topology as the restriction of the Euclidean metric, but with respect to which the irrationals are complete. One can see this without knowing the aforementioned fact about G-delta sets: the continued fraction expansion of an irrational number defines a homeomorphism from the space of irrationals to the space of all sequences of positive integers, which is easily seen to be completely metrizable.\n\nFurthermore, the set of all irrationals is a disconnected metrizable space. In fact, the irrationals have a basis of clopen sets so the space is zero-dimensional.\n\n\n\n"}
{"id": "47567005", "url": "https://en.wikipedia.org/wiki?curid=47567005", "title": "Krackhardt kite graph", "text": "Krackhardt kite graph\n\nIn graph theory, the Krackhardt kite graph is a simple graph with ten nodes. The graph is named after David Krackhardt, a researcher of social network theory.\n\nKrackhardt introduced the graph in 1990 to distinguish different concepts of centrality. It has the property that the vertex with maximum degree (labeled 3 in the figure, with degree 6), the vertex with maximum betweenness centrality (labeled 7), and the two vertices with maximum closeness centrality (labeled 5 and 6) are all different from each other.\n"}
{"id": "352181", "url": "https://en.wikipedia.org/wiki?curid=352181", "title": "List of abstract algebra topics", "text": "List of abstract algebra topics\n\nAbstract algebra is the subject area of mathematics that studies algebraic structures, such as groups, rings, fields, modules, vector spaces, and algebras. The phrase abstract algebra was coined at the turn of the 20th century to distinguish this area from what was normally referred to as algebra, the study of the rules for manipulating formulae and algebraic expressions involving unknowns and real or complex numbers, often now called \"elementary algebra\". The distinction is rarely made in more recent writings.\n\nAlgebraic structures are defined primarily as sets with \"operations\". \n\nStructure preserving maps called \"homomorphisms\" are vital in the study of algebraic objects.\n\nThere are several basic ways to combine algebraic objects of the same type to produce a third object of the same type. These constructions are used throughout algebra.\n\nAdvanced concepts:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRepresentation theory\n\n\n\n\n\n"}
{"id": "3076863", "url": "https://en.wikipedia.org/wiki?curid=3076863", "title": "Machine epsilon", "text": "Machine epsilon\n\nMachine epsilon gives an upper bound on the relative error due to rounding in floating point arithmetic. This value characterizes computer arithmetic in the field of numerical analysis, and by extension in the subject of computational science. The quantity is also called macheps or unit roundoff, and it has the symbols Greek epsilon formula_1 or bold Roman u, respectively.\n\nThe following values of machine epsilon apply to standard floating point formats:\n\n\"Rounding\" is a procedure for choosing the representation of a real number in a floating point number system. For a number system and a rounding procedure, machine epsilon is the maximum relative error of the chosen rounding procedure.\n\nSome background is needed to determine a value from this definition. A floating point number system is characterized by a radix which is also called the base, formula_2, and by the precision formula_3, i.e. the number of radix formula_2 digits of the significand (including any leading implicit bit). All the numbers with the same exponent, formula_5, have the spacing, formula_6. The spacing changes at the numbers that are perfect powers of formula_2; the spacing on the side of larger magnitude is formula_2 times larger than the spacing on the side of smaller magnitude.\n\nSince machine epsilon is a bound for relative error, it suffices to consider numbers with exponent formula_9. It also suffices to consider positive numbers. For the usual round-to-nearest kind of rounding, the absolute rounding error is at most half the spacing, or formula_10. This value is the biggest possible numerator for the relative error. The denominator in the relative error is the number being rounded, which should be as small as possible to make the relative error large. The worst relative error therefore happens when rounding is applied to numbers of the form formula_11 where formula_12 is between formula_13 and formula_10. All these numbers round to formula_15 with relative error formula_16. The maximum occurs when formula_12 is at the upper end of its range. The formula_11 in the denominator is negligible compared to the numerator, so it is left off for expediency, and just formula_10 is taken as machine epsilon. As has been shown here, the relative error is worst for numbers that round to formula_15, so machine epsilon also is called \"unit roundoff\" meaning roughly \"the maximum error that can occur when rounding to the unit value\".\n\nThus, the maximum spacing between a normalised floating point number, formula_21, and an adjacent normalised number is formula_22 x formula_23.\n\nNumerical analysis uses machine epsilon to study the effects of rounding error. The actual errors of machine arithmetic are far too complicated to be studied directly, so instead, the following simple model is used. The IEEE arithmetic standard says all floating point operations are done as if it were possible to perform the infinite-precision operation, and then, the result is rounded to a floating point number. Suppose (1) formula_21, formula_25 are floating point numbers, (2) formula_26 is an arithmetic operation on floating point numbers such as addition or multiplication, and (3) formula_27 is the infinite precision operation. According to the standard, the computer calculates:\n\nBy the meaning of machine epsilon, the relative error of the rounding is at most machine epsilon in magnitude, so:\n\nwhere formula_30 in absolute magnitude is at most formula_1 or u. The books by Demmel and Higham in the references can be consulted to see how this model is used to analyze the errors of, say, Gaussian elimination.\n\nThe IEEE standard does not define the terms \"machine epsilon\" and \"unit roundoff\", so differing definitions of these terms are in use, which can cause some confusion.\n\nThe definition given here for \"machine epsilon\" is the one used by Prof. James Demmel in lecture scripts, the \"LAPACK\" linear algebra package, numerics research papers and some scientific computing software. Most numerical analysts use the words \"machine epsilon\" and \"unit roundoff\" interchangeably with this meaning.\n\nThe following different definition is much more widespread outside academia: \"Machine epsilon is defined as the difference between 1 and the next larger floating point number.\" By this definition, formula_1 equals the value of the unit in the last place relative to 1, i.e. formula_33, and the unit roundoff is uformula_34, assuming round-to-nearest mode. The prevalence of this definition is rooted in its use in the ISO C Standard for constants relating to floating-point types and corresponding constants in other programming languages. It is also widely used in scientific computing software, and in the numerics and computing literature.\n\nWhere standard libraries do not provide precomputed values (as <float.h> does with codice_1, codice_2 and codice_3 for C and <limits> does with codice_4 in C++), the best way to determine machine epsilon is to refer to the table, above, and use the appropriate power formula. Computing machine epsilon is often given as a textbook exercise. The following examples compute machine epsilon in the sense of the spacing of the floating point numbers at 1 rather than in the sense of the unit roundoff.\n\nNote that results depend on the particular floating-point format used, such as float, double, long double, or similar as supported by the programming language, the compiler, and the runtime library for the actual platform.\n\nSome formats supported by the processor might not be supported by the chosen compiler and operating system. Other formats might be emulated by the runtime library, including arbitrary-precision arithmetic available in some languages and libraries.\n\nIn a strict sense the term \"machine epsilon\" means the 1+eps accuracy directly supported by the processor (or coprocessor), not some 1+eps accuracy supported by a specific compiler for a specific operating system, unless it's known to use the best format.\n\nIEEE 754 floating-point formats have the property that, when reinterpreted as a two's complement integer of the same width, they monotonically increase over positive values and monotonically decrease over negative values (see ). They also have the property that 0 < |\"f\"(\"x\")| < ∞, and |\"f\"(\"x\"+1) − \"f\"(\"x\")| ≥ |\"f\"(\"x\") − \"f\"(\"x\"−1)| (where \"f\"(\"x\") is the aforementioned integer reinterpretation of \"x\"). In languages that allow type punning and always use IEEE 754-1985, we can exploit this to compute a machine epsilon in constant time. For example, in C:\n\nThis will give a result of the same sign as value. If a positive result is always desired, the return statement of machine_eps can be replaced with:\n64-bit doubles give 2.220446e-16, which is 2 as expected.\n\nThe following simple algorithm can be used to approximate the machine epsilon, to within a factor of two (one order of magnitude) of its true value, using a linear search.\n\n\n"}
{"id": "29379212", "url": "https://en.wikipedia.org/wiki?curid=29379212", "title": "Math-U-See", "text": "Math-U-See\n\nMath-U-See, developed by Steve Demme is a K–12 homeschool mathematics curriculum, emphasizing mastery of materials and encouraging instructors to let students learn at their own pace. Because of the tutor style model Math-U-See is being adopted by special education departments Response to intervention programs. Math-U-See curricula includes Primer, Alpha, Beta, Gamma, Delta, & Epsilon as well as other books.\n\n"}
{"id": "18416734", "url": "https://en.wikipedia.org/wiki?curid=18416734", "title": "Median algebra", "text": "Median algebra\n\nIn mathematics, a median algebra is a set with a ternary operation formula_1 satisfying a set of axioms which generalise the notion of median or majority function, as a Boolean function.\n\nThe axioms are \n\nThe second and third axioms imply commutativity: it is possible (but not easy) to show that in the presence of the other three, axiom (3) is redundant. The fourth axiom implies associativity.\nThere are other possible axiom systems: for example the two\nalso suffice.\n\nIn a Boolean algebra, or more generally a distributive lattice, the median function formula_8 satisfies these axioms, so that every Boolean algebra and every distributive lattice forms a median algebra.\n\nBirkhoff and Kiss showed that a median algebra with elements formula_9 and formula_10 satisfying formula_11 is a distributive lattice.\n\nA median graph is an undirected graph in which for every three vertices formula_12, formula_13, and formula_14 there is a unique vertex formula_1 that belongs to shortest paths between any two of formula_12, formula_13, and formula_14. If this is the case, then the operation formula_1 defines a median algebra having the vertices of the graph as its elements.\n\nConversely, in any median algebra, one may define an \"interval\" formula_20 to be the set of elements formula_13 such that formula_22. One may define a graph from a median algebra by creating a vertex for each algebra element and an edge for each pair formula_23 such that the interval formula_20 contains no other elements. If the algebra has the property that every interval is finite, then this graph is a median graph, and it accurately represents the algebra in that the median operation defined by shortest paths on the graph coincides with the algebra's original median operation.\n\n\n"}
{"id": "22130008", "url": "https://en.wikipedia.org/wiki?curid=22130008", "title": "Multiplication of vectors", "text": "Multiplication of vectors\n\nIn mathematics, Vector multiplication refers to one of several techniques for the multiplication of two (or more) vectors with themselves. It may concern any of the following articles:\n\n\n"}
{"id": "50027042", "url": "https://en.wikipedia.org/wiki?curid=50027042", "title": "Multiplicative independence", "text": "Multiplicative independence\n\nIn number theory, two positive integers \"a\" and \"b\" are said to be multiplicatively independent if their only common integer power is 1. That is, for all integer \"n\" and \"m\", formula_1 implies formula_2. Two integers which are not multiplicatively independent are said to be multiplicatively dependent.\n\nFor example, 36 and 216 are multiplicatively dependent since formula_3 and 6 and 12 are multiplicatively independent\n\nBeing multiplicatively independent admits some other characterizations. \"a\" and \"b\" are multiplicatively independent if and only if formula_4 is irrational. This property holds independently of the base of the logarithm. \n\nLet formula_5 and formula_6 be the canonical representations of \"a\" and \"b\". The integers \"a\" and \"b\" are multiplicatively dependent if and only if \"k\" = \"l\", formula_7 and formula_8 for all \"i\" and \"j\".\n\nBüchi arithmetic in base \"a\" and \"b\" define the same sets if and only if \"a\" and \"b\" are multiplicatively dependent. \n\nLet \"a\" and \"b\" be multiplicatively dependent integers, that is, there exists \"n,m>1\" such that formula_1. The integers \"c\" such that the length of its expansion in base \"a\" is at most \"m\" are exactly the integers such that the length of their expansion in base \"b\" is at most \"n\". It implies that computing the base \"b\" expansion of a number, given its base \"a\" expansion, can be done by transforming consecutive sequences of \"m\" base \"a\" digits into consecutive sequence of \"n\" base \"b\" digits.\n"}
{"id": "22947690", "url": "https://en.wikipedia.org/wiki?curid=22947690", "title": "Narsingh Deo", "text": "Narsingh Deo\n\nNarsingh Deo is a professor and Charles N. Millican Endowed Chair of the Department of Computer Science, University of Central Florida. He received his Ph.D. for his dissertation 'Topological Analysis of Active Networks and Generalization of Hamiltonian tree' from Northwestern University, IL., in 1965; S. L. Hakimi was his adviser. Deo was professor at the Indian Institute of Technology, Kanpur.\n\n\n"}
{"id": "24447471", "url": "https://en.wikipedia.org/wiki?curid=24447471", "title": "Natural population growth", "text": "Natural population growth\n\nNatural population increase (in contrast to total population increase) happens as people are born (in contrast to immigration) into a country, and decrease as people die (in contrast to emigrate). Rates of population growth, usually expressed as a percentage, vary greatly. \nThe immigrating population has increased by a huge margin in the past few years. Due to this rising trend of migration it is important that we look into how migration has affected the population, and the trends that follow with it. The countries that are to be looked into are Denmark, Finland, Iceland, Norway and Sweden. The Western European countries under discussion are England, Wales, France, the Netherlands, Scotland, Switzerland, and Italy from Southern Europe. During the whole conducted study it appears that six different countries had negative conclusions for net migration. Every Nordic country other than Denmark, Italy, and Scotland had values that are close to zero. The countries that had a positive migrating population are Denmark, England, Wales, Netherlands, France, and Switzerland. In the early 19th century it appeared all countries had an outflow of immigrants leaving the country but in the 20th century it became a positive inflow. Due to this push and pull these countries experienced during the 21st century there was a positive flow of immigration into the countries. \n\nDue to this increased migrating population soon countries will begin to start persuading individuals to move into a more rural area other countries because the big cities will begin to start see an overcrowding population. For example Australia is beginning to see this overcrowding that would soon follow for other countries. Because of this Australia has begun to place a cap on the amount of citizens allowed into the bigger cities which will force citizens to live in the less populated rural areas. This idea is called the Strategic Plan which sets a cap on the amount of population that is allowed in certain cities. This is a prime example of what will happen to other countries due to the exceeding immigrant population growth they will soon start to experience. Caps will be placed on big cities and force those who are already living in the city to be moved to the rural areas. \nReferences\n\nMurphy, M. (2016). The Impact of Migration on Long-Term European Population Trends, 1950 to Present. Population And Development Review, 42(2), 225-244.\n\nWilson, T. (2015). The Demographic Constraints on Future Population Growth in Regional Australia. Australian Geographer, 46(1), 91-111. doi:10.1080/00049182.2014.986786\n"}
{"id": "2157056", "url": "https://en.wikipedia.org/wiki?curid=2157056", "title": "Neeraj Kayal", "text": "Neeraj Kayal\n\nNeeraj Kayal () is an Indian computer scientist. Kayal was born and raised in Guwahati, India.\n\nKayal graduated with a B.Tech from the Computer Science Department of the Indian Institute of Technology, Kanpur (IITK), India in 2002. In that year, he, Manindra Agrawal and Nitin Saxena proposed the AKS Primality Test, which attracted worldwide attention, including an article in the \"New York Times\". \n\nKayal received his PhD in theoretical computer science from the Department of Computer Science and Engineering at the Indian Institute of Technology, Kanpur. He did postdoctoral research at the Institute for Advanced Study in Princeton and at Rutgers University. Since 2008, he has been working with the Microsoft Research Lab India as a researcher.\n\nNeeraj Kayal was given the Distinguished Alumnus Award of the IITK, for his work in computational complexity theory. He is also a recipient of the Gödel prize and the Fulkerson Prize for the same along with his co-authors. In 2012, he was awarded the Young Scientist Award from the Indian National Science Academy (INSA) for contributions to the development of arithmetic complexity theory including the development of a deterministic algorithm for primality testing, the resolution of the constant fan-in conjecture for depth three circuits, and a reconstruction algorithm for arithmetic formulas.\n\n"}
{"id": "30618217", "url": "https://en.wikipedia.org/wiki?curid=30618217", "title": "Non-convexity (economics)", "text": "Non-convexity (economics)\n\nIn economics, non-convexity refers to violations of the convexity assumptions of elementary economics. Basic economics textbooks concentrate on consumers with convex preferences (that do not prefer extremes to in-between values) and convex budget sets and on producers with convex production sets; for convex models, the predicted economic behavior is well understood. When convexity assumptions are violated, then many of the good properties of competitive markets need not hold: Thus, non-convexity is associated with market failures, where supply and demand differ or where market equilibria can be inefficient. Non-convex economies are studied with nonsmooth analysis, which is a generalization of convex analysis.\n\nIf a preference set is \"non-convex\", then some prices determine a budget-line that supports two \"separate\" optimal-baskets. For example, we can imagine that, for zoos, a lion costs as much as an eagle, and further that a zoo's budget suffices for one eagle or one lion. We can suppose also that a zoo-keeper views either animal as equally valuable. In this case, the zoo would purchase either one lion or one eagle. Of course, a contemporary zoo-keeper does not want to purchase half of an eagle and half of a lion. Thus, the zoo-keeper's preferences are non-convex: The zoo-keeper prefers having either animal to having any strictly convex combination of both.\nWhen the consumer's preference set is non-convex, then (for some prices) the consumer's demand is not connected; A disconnected demand implies some discontinuous behavior by the consumer, as discussed by Harold Hotelling:\n\nIf indifference curves for purchases be thought of as possessing a wavy character, convex to the origin in some regions and concave in others, we are forced to the conclusion that it is only the portions convex to the origin that can be regarded as possessing any importance, since the others are essentially unobservable. They can be detected only by the discontinuities that may occur in demand with variation in price-ratios, leading to an abrupt jumping of a point of tangency across a chasm when the straight line is rotated. But, while such discontinuities may reveal the existence of chasms, they can never measure their depth. The concave portions of the indifference curves and their many-dimensional generalizations, if they exist, must forever remain in\nunmeasurable obscurity.\n\nThe difficulties of studying non-convex preferences were emphasized by Herman Wold and again by Paul Samuelson, who wrote that non-convexities are \"shrouded in eternal according to Diewert.\n\nWhen convexity assumptions are violated, then many of the good properties of competitive markets need not hold: Thus, non-convexity is associated with market failures, where supply and demand differ or where market equilibria can be inefficient.\nNon-convex preferences were illuminated from 1959 to 1961 by a sequence of papers in \"The Journal of Political Economy\" (\"JPE\"). The main contributors were Farrell, Bator, Koopmans, and Rothenberg. In particular, Rothenberg's paper discussed the approximate convexity of sums of non-convex sets. These \"JPE\"-papers stimulated a paper by Lloyd Shapley and Martin Shubik, which considered convexified consumer-preferences and introduced the concept of an \"approximate equilibrium\". The \"JPE\"-papers and the Shapley–Shubik paper influenced another notion of \"quasi-equilibria\", due to Robert Aumann.\n\nNon-convex sets have been incorporated in the theories of general economic equilibria. These results are described in graduate-level textbooks in microeconomics, general equilibrium theory, game theory, mathematical economics,\nand applied mathematics (for economists). The Shapley–Folkman lemma establishes that non-convexities are compatible with approximate equilibria in markets with many consumers; these results also apply to production economies with many small firms.\n\nNon-convexity is important under oligopolies and especially monopolies. Concerns with large producers exploiting market power initiated the literature on non-convex sets, when Piero Sraffa wrote about on firms with increasing returns to scale in 1926, after which Harold Hotelling wrote about marginal cost pricing in 1938. Both Sraffa and Hotelling illuminated the market power of producers without competitors, clearly stimulating a literature on the supply-side of the economy.\n\nRecent research in economics has recognized non-convexity in new areas of economics. In these areas, non-convexity is associated with market failures, where equilibria need not be efficient or where no competitive equilibrium exists because supply and demand differ. Non-convex sets arise also with environmental goods (and other externalities), and with market failures, and public economics.\nNon-convexities occur also with information economics, and with stock markets (and other incomplete markets). Such applications continued to motivate economists to study non-convex sets. In some cases, non-linear pricing or bargaining may overcome the failures of markets with competitive pricing; in other cases, regulation may be justified.\n\nThe previously mentioned applications concern non-convexities in finite-dimensional vector spaces, where points represent commodity bundles. However, economists also consider dynamic problems of optimization over time, using the theories of differential equations, dynamic systems, stochastic processes, and functional analysis: Economists use the following optimization methods:\n\nIn these theories, regular problems involve convex functions defined on convex domains, and this convexity allows simplifications of techniques and economic meaningful interpretations of the results. In economics, dynamic programing was used by Martin Beckmann and Richard F. Muth for work on inventory theory and consumption theory. Robert C. Merton used dynamic programming in his 1973 article on the intertemporal capital asset pricing model. (See also Merton's portfolio problem). In Merton's model, investors chose between income today and future income or capital gains, and their solution is found via dynamic programming. Stokey, Lucas & Prescott use dynamic programming to solve problems in economic theory, problems involving stochastic processes. Dynamic programming has been used in optimal economic growth, resource extraction, principal–agent problems, public finance, business investment, asset pricing, factor supply, and industrial organization. Ljungqvist & Sargent apply dynamic programming to study a variety of theoretical questions in monetary policy, fiscal policy, taxation, economic growth, search theory, and labor economics. Dixit & Pindyck used dynamic programming for capital budgeting. For dynamic problems, non-convexities also are associated with market failures, just as they are for fixed-time problems.\n\nEconomists have increasingly studied non-convex sets with nonsmooth analysis, which generalizes convex analysis. Convex analysis centers on convex sets and convex functions, for which it provides powerful ideas and clear results, but it is not adequate for the analysis of non-convexities, such as increasing returns to scale. \"Non-convexities in [both] production and consumption ... required mathematical tools that went beyond convexity, and further development had to await the invention of non-smooth calculus\": For example, Clarke's differential calculus for Lipschitz continuous functions, which uses Rademacher's theorem and which is described by and , according to . wrote that the \"major methodological innovation in the general equilibrium analysis of firms with pricing rules\" was \"the introduction of the methods of non-smooth analysis, as a [synthesis] of global analysis (differential topology) and [of] convex analysis.\" According to , \"Non-smooth analysis extends the local approximation of manifolds by tangent planes [and extends] the analogous approximation of convex sets by tangent cones to sets\" that can be non-smooth or non-convex.\n\n\n"}
{"id": "4928530", "url": "https://en.wikipedia.org/wiki?curid=4928530", "title": "Perron's formula", "text": "Perron's formula\n\nIn mathematics, and more particularly in analytic number theory, Perron's formula is a formula due to Oskar Perron to calculate the sum of an arithmetical function, by means of an inverse Mellin transform.\n\nLet formula_1 be an arithmetic function, and let\n\nbe the corresponding Dirichlet series. Presume the Dirichlet series to be uniformly convergent for formula_3. Then Perron's formula is\n\nHere, the prime on the summation indicates that the last term of the sum must be multiplied by 1/2 when \"x\" is an integer. The integral is not a convergent Lebesgue integral, it is understood as the Cauchy principal value. The formula requires \"c\" > 0, \"c\" > σ, and \"x\" > 0 real, but otherwise arbitrary.\n\nAn easy sketch of the proof comes from taking Abel's sum formula\n\nThis is nothing but a Laplace transform under the variable change formula_6 Inverting it one gets Perron's formula.\n\nBecause of its general relationship to Dirichlet series, the formula is commonly applied to many number-theoretic sums. Thus, for example, one has the famous integral representation for the Riemann zeta function:\n\nand a similar formula for Dirichlet L-functions:\n\nwhere\n\nand formula_10 is a Dirichlet character. Other examples appear in the articles on the Mertens function and the von Mangoldt function.\n\nPerron's formula is just a special case of the Mellin discrete convolution\n\nformula_11\n\nwhere formula_12 and formula_13 the Mellin transform. The Perron formula is just the special case of the test function formula_14, for formula_15 the Heaviside step function.\n\n"}
{"id": "6704759", "url": "https://en.wikipedia.org/wiki?curid=6704759", "title": "Philosophy of Arithmetic", "text": "Philosophy of Arithmetic\n\nPhilosophy of Arithmetic (PA; ) is an 1891 book by Edmund Husserl. Husserl's first published book, it is a synthesis of his studies in mathematics, under Karl Weierstrass, with his studies in philosophy and psychology, under Franz Brentano, to whom is dedicated, and Carl Stumpf.\n\nThe \"Philosophy of Arithmetic\" constitutes the first volume of a work which Husserl intended to comprise two volumes, of which the second was never published. Comprehensively it would have encompassed four parts and an Appendix.\n\nThe first volume is divided in two parts, in the first of which Husserl purports to analyse the \"Proper concepts of multiplicity, unity and amount\" (\"Die eigentliche Begriffe von Vielheit, Einheit und Anzahl\") and in the second \"The symbolic amount-concepts and the logical sources of amount-arithmetic\" (\"Die symbolischen Anzahlbegrife und die logischen Quellen der Anzahlen-Arithmetik\").\n\nThe basic issue of the book is a philosophical analysis of the concept of number, which is the most basic concept on which the entire edifice of arithmetic and mathematics can be founded. In order to proceed with this analysis, Husserl, following Brentano and Stumpf, uses the tools of psychology to look for the \"origin and content\" of the concept of number. He begins with the classical definition, already given by Euclid, Hobbes and Leibniz, that \"number is a multiplicity of unities\" and then asks himself: what is multiplicity and what is unity? Anything that we can think of, anything we can present, can be considered at its most basic level to be \"something\". Multiplicity is then the \"collective connection\" of \"something and something and something etc.\" In order to get a number instead of a mere quantity, we can also think of these featureless, abstract \"somethings\" as \"ones\" and then get \"one and one and one etc.\" as basic definition of number \"in abstracto\". However, these are just the proper numbers, i.e. number which we can conceive of properly, without the help of instruments or symbols. Psychologically we are limited to just the very first few numbers if we want to conceive of them properly, with higher numbers our short term memory is not enough to think of them all together, but still as identical to themselves and different from all others. Hence, Husserl says, we have to move on to the analysis of symbolically conceived numbers, which are effectively those that are used in mathematics.\n\nThe book is a product of his years of study with Weierstrass (in Berlin) and his student Leo Königsberger (in Vienna) on the mathematical side and his studies with Brentano (in Vienna) and Stumpf (in Halle) on the psychological/philosophical side. The book is mostly based on his habilitationsschrift of 1887 \"On the Concept of Number\" (\"Über den Begriff der Zahl\"). Husserl also lectured on the concept of number between 1889 and 1891, much in the same vein. He continued working on the second volume up to at least 1894.\n\nGottlob Frege was critical of \"Philosophy of Arithmetic\", and accused Husserl of relying too much on the metaphysical and not enough on the logical aspects of mathematics. Frege’s criticisms influenced negatively the young mathematician’s career as a professor. Husserl’s \"Logical Investigations\" secured his reputation ten years later, but Frege and others never accepted Husserl as a practitioner of true logic.\n\nThe original edition:\n\n\"Husserliana\" edition:\n\nOfficial English translation of the \"Husserliana\" edition:\n\n"}
{"id": "504509", "url": "https://en.wikipedia.org/wiki?curid=504509", "title": "Probabilistically checkable proof", "text": "Probabilistically checkable proof\n\nIn computational complexity theory, a probabilistically checkable proof (PCP) is a type of proof that can be checked by a randomized algorithm using a bounded amount of randomness and reading a bounded number of bits of the proof. The algorithm is then required to accept correct proofs and reject incorrect proofs with very high probability. A standard proof (or certificate), as used in the verifier-based definition of the complexity class NP, also satisfies these requirements, since the checking procedure deterministically reads the whole proof, always accepts correct proofs and rejects incorrect proofs. However, what makes them interesting is the existence of probabilistically checkable proofs that can be checked by reading only a few bits of the proof using randomness in an essential way.\n\nProbabilistically checkable proofs give rise to many complexity classes depending on the number of queries required and the amount of randomness used. The class PCP[\"r\"(\"n\"),\"q\"(\"n\")] refers to the set of decision problems that have probabilistically checkable proofs that can be verified in polynomial time using at most \"r\"(\"n\") random bits and by reading at most \"q\"(\"n\") bits of the proof. Unless specified otherwise, correct proofs should always be accepted, and incorrect proofs should be rejected with probability greater than 1/2. The PCP theorem, a major result in computational complexity theory, states that PCP[O(log \"n\"),O(1)] = NP.\n\nGiven a decision problem \"L\"(or a language L with its alphabet set Σ), a probabilistically checkable proof system for \"L\" with completeness \"c\"(\"n\") and soundness \"s\"(\"n\"), where 0 ≤ \"s\"(\"n\") ≤ \"c\"(\"n\") ≤ 1, consists of a prover and a verifier. Given a claimed solution x with length n, which might be false, the prover produces a proof \"π\" which states \"x\" solves L (\"x\" ∈ \"L\", the proof is a string ∈ Σ). And the verifier is a randomized oracle Turing Machine \"V\" (the \"verifier\") that checks the proof \"π\" for the statement that \"x\" solves \"L\"(or \"x\" ∈ \"L\") and decides whether to accept the statement. The system has the following properties:\n\nFor the computational complexity of the verifier, we have the \"randomness complexity\" \"r\"(\"n\") to measure the maximum number of random bits that \"V\" uses over all \"x\" of length \"n\" and the \"query complexity\" \"q\"(\"n\") of the verifier is the maximum number of queries that \"V\" makes to π over all \"x\" of length \"n\".\n\nIn the above definition, the length of proof is not mentioned since usually it includes the alphabet set and all the witness. By the way, for the prover, we do not mention how it gets to know the solution to the problem. We only care how can it writes down a proof of it.\n\nThe verifier is said to be \"non-adaptive\" if it makes all its queries before it receives any of the answers to previous queries.\n\nThe complexity class PCP[\"r\"(\"n\"), \"q\"(\"n\")] is the class of all decision problems having probabilistically checkable proof systems over binary alphabet of completeness \"c\"(\"n\") and soundness \"s\"(\"n\"), where the verifier is nonadaptive, runs in polynomial time, and it has randomness complexity \"r\"(\"n\") and query complexity \"q\"(\"n\").\n\nThe shorthand notation PCP[\"r\"(\"n\"), \"q\"(\"n\")] is sometimes used for PCP[\"r\"(\"n\"), \"q\"(\"n\")]. The complexity class PCP is defined as PCP[O(log \"n\"), O(1)].\n\nThe theory of probabilistically checkable proofs studies the power of probabilistically checkable proof systems under various restrictions of the parameters (completeness, soundness, randomness complexity, query complexity, and alphabet size). It has applications to computational complexity (in particular hardness of approximation) and cryptography.\n\nThe definition of a probabilistically checkable proof was explicitly introduced by Arora and Safra in 1992, although their properties were studied earlier. In 1990 Babai, Fortnow, and Lund proved that PCP[poly(\"n\"), poly(\"n\")] = NEXP, providing the first nontrivial equivalence between standard proofs (NEXP) and probabilistically checkable proofs. The PCP theorem proved in 1992 states that PCP[O(log \"n\"),O(1)] = NP.\n\nThe theory of hardness of approximation requires a detailed understanding of the role of completeness, soundness, alphabet size, and query complexity in probabilistically checkable proofs.\n\nFrom computational complexity point of view, for extreme settings of the parameters, the definition of probabilistically checkable proofs is easily seen to be equivalent to standard complexity classes. For example, we have the following:\n\nThe PCP theorem and MIP = NEXP can be characterized as follows:\n\nIt is also known that PCP[\"r\"(\"n\"), \"q\"(\"n\")] ⊆ NTIME(poly(n,2\"q\"(\"n\"))). \nIn particular, PCP[log \"n\", poly(\"n\")] = NP. \nOn the other hand, if NP ⊆ PCP[o(log \"n\"),o(log \"n\")] then P = NP.\n\nLinear PCP is one such that given query q, the oracle does only linear operation on the proof formula_1. That's to say the response from the oracle to the query q is a linear function formula_2.\n\n\n"}
{"id": "39684957", "url": "https://en.wikipedia.org/wiki?curid=39684957", "title": "Quantifier variance", "text": "Quantifier variance\n\nThe term quantifier variance refers to claims there is no uniquely best ontological language with which to describe the world. According to Hirsch, it is an outgrowth of \"Urmson's dictum\":\nThe term \"quantifier variance\" rests upon the philosophical term 'quantifier', more precisely existential quantifier. A 'quantifier' is an expression like \"there exists at least one ‘such-and-such’\".\n\nThe word \"quantifier\" in the introduction refers to a variable used in a domain of discourse, a collection of objects under discussion. In daily life, the domain of discourse could be 'apples', or 'persons', or even everything. In a more technical arena, the domain of discourse could be 'integers', say. The quantifier variable \"x\", say, in the given domain of discourse can take on the 'value' or designate any object in the domain. The presence of a particular object, say a 'unicorn' is expressed in the manner of symbolic logic as:\n\nHere the 'turned \"E\" ' or ∃ is read as \"there exists...\" and is called the symbol for existential quantification. Relations between objects also can be expressed using quantifiers. For example, in the domain of integers (denoting the quantifier by \"n\", a customary choice for an integer) we can indirectly identify '5' by its relation with the number '25':\n\nIf we want to point out specifically that the domain of integers is meant, we could write:\n\nHere, ∈ = \"is a member of...\" and ∈ is called the symbol for set membership; and ℤ denotes the set of integers.\n\nThere are a variety of expressions that serve the same purpose in various ontologies, and they are accordingly all quantifier expressions. Quantifier \"variance\" is then one argument concerning exactly what expressions can be construed as quantifiers, and just which arguments of a quantifier, that is, which substitutions for ‘such-and-such’, are permissible.\n\nHirsch says the notion of quantifier variance is a concept concerning how languages work, and is not connected to the ontological question of what 'really' exists. That view is not universal.\n\nThe thesis underlying quantifier variance was stated by Putnam:\n\nCiting this quotation from Putnam, Wasserman states: \"This thesis – the thesis that there are many meanings for the existential quantifier that are equally neutral and equally adequate for describing all the facts – is often referred to as ‘the doctrine of quantifier variance’\".\n\nHirsch's quantifier variance has been connected to Carnap's idea of a linguistic framework as a 'neo'-Carnapian view, namely, \"the view that there are a number of equally good meanings of the logical quantifiers; choosing one of these frameworks is to be understood analogously to choosing a Carnapian framework.\" Of course, not all philosophers (notably Quine and the 'neo'-Quineans) subscribe to the notion of multiple linguistic frameworks. See meta-ontology.\n\nHirsch himself suggests some care in connecting \"his version\" of quantifier variance with Carnap: \"Let's not call any philosophers quantifier variantists unless they are clearly committed to the idea that (most of) the things that exist are completely independent of language.\" In this connection Hirsch says \"I have a problem, however, in calling Carnap a quantifier variantist, insofar as he is often viewed as a verificationist anti-realist.\" Although Thomasson does not think Carnap is properly considered to be an antirealist, she still disassociates Carnap from Hirsch's version of quantifier variance: \"I’ll argue, however, that Carnap in fact is not committed to quantifier variance in anything like Hirsch’s sense, and that he [Carnap] does not rely on it in his ways of deflating metaphysical debates.\"\n\n"}
{"id": "25546470", "url": "https://en.wikipedia.org/wiki?curid=25546470", "title": "Seven-dimensional space", "text": "Seven-dimensional space\n\nIn mathematics, a sequence of \"n\" real numbers can be understood as a location in \"n\"-dimensional space. When \"n\" = 7, the set of all such locations is called 7-dimensional space. Often such a space is studied as a vector space, without any notion of distance. Seven-dimensional Euclidean space is seven-dimensional space equipped with a Euclidean metric, which is defined by the dot product.\n\nMore generally, the term may refer to a seven-dimensional vector space over any field, such as a seven-dimensional complex vector space, which has 14 real dimensions. It may also refer to a seven-dimensional manifold such as a 7-sphere, or a variety of other geometric constructions.\n\nSeven-dimensional spaces have a number of special properties, many of them related to the octonions. An especially distinctive property is that a cross product can be defined only in three or seven dimensions. This is related to Hurwitz's theorem, which prohibits the existence of algebraic structures like the quaternions and octonions in dimensions other than 2, 4, and 8. The first exotic spheres ever discovered were seven-dimensional.\n\nA polytope in seven dimensions is called a 7-polytope. The most studied are the regular polytopes, of which there are only three in seven dimensions: the 7-simplex, 7-cube, and 7-orthoplex. A wider family are the uniform 7-polytopes, constructed from fundamental symmetry domains of reflection, each domain defined by a Coxeter group. Each uniform polytope is defined by a ringed Coxeter-Dynkin diagram. The 7-demicube is a unique polytope from the D family, and 3, 2, and 1 polytopes from the E family.\n\nThe 6-sphere or hypersphere in seven-dimensional Euclidean space is the six-dimensional surface equidistant from a point, e.g. the origin. It has symbol , with formal definition for the 6-sphere with radius \"r\" of\n\nThe volume of the space bounded by this 6-sphere is\n\nwhich is 4.72477 × \"r\", or 0.0369 of the 7-cube that contains the 6-sphere.\n\nA cross product, that is a vector-valued, bilinear, anticommutative and orthogonal product of two vectors, is defined in seven dimensions. Along with the more usual cross product in three dimensions it is the only such product, except for trivial products.\n\nIn 1956, John Milnor constructed an exotic sphere in 7 dimensions and showed that there are at least 7 differentiable structures on the 7-sphere. In 1963 he showed that the exact number of such structures is 28.\n\n\n"}
{"id": "39932177", "url": "https://en.wikipedia.org/wiki?curid=39932177", "title": "Shannon capacity of a graph", "text": "Shannon capacity of a graph\n\nIn graph theory, the Shannon capacity of a graph is a graph invariant defined from the number of independent sets of strong graph products. It measures the Shannon capacity of a communications channel defined from the graph, and is upper bounded by the Lovász number, which can be computed in polynomial time. However, the computational complexity of the Shannon capacity itself remains unknown.\n\nThe Shannon capacity models the amount of information that can be transmitted across a noisy communication channel in which certain signal values can be confused with each other. In this application, the confusion graph or confusability graph describes the pairs of values that can be confused. For instance, suppose that a communications channel has five discrete signal values, any one of which can be transmitted in a single time step. These values may be modeled mathematically as the five numbers 0, 1, 2, 3, or 4 in modular arithmetic modulo 5. However, suppose that when a value \"i\" is sent across the channel, the value that is received is \"i\" + \"ξ\" (mod 5) where \"ξ\" represents the noise on the channel and may be any real number in the open interval −1 < \"ξ\" < 1. Thus, if the recipient receives a value such as 3.6, it is impossible to determine whether it was originally transmitted as a 3 or as a 4; the two values 3 and 4 can be confused with each other. This situation can be modeled by a graph, a cycle \"C\" of length 5, in which the vertices correspond to the five values that can be transmitted and the edges of the graph represent values that can be confused with each other.\n\nFor this example, it is possible to choose two values that can be transmitted in each time step without ambiguity, for instance, the values 1 and 3. These values are far enough apart that they can't be confused with each other: when the recipient receives a value \"x\" in the range 0 < \"x\" < 2, it can deduce that the value that was sent must have been 1, and when the recipient receives a value \"x\" in the range 2 < \"x\" < 4, it can deduce that the value that was sent must have been 3. In this way, in \"n\" steps of communication, the sender can communicate up to 2 different messages. Two is the maximum number of values that the recipient can distinguish from each other: every subset of three or more of the values 0, 1, 2, 3, 4 includes at least one pair that can be confused with each other. Even though the channel has five values that can be sent per time step, effectively only two of them can be used with this coding scheme.\n\nHowever, more complicated coding schemes allow a greater amount of information to be sent across the same channel, by using codewords of length greater than one. For instance, suppose that in two consecutive steps the sender transmits one of the five code words \"11\", \"23\", \"35\", \"54\", or \"42\". (Here, the quotation marks indicate that these words should be interpreted as strings of symbols, not as decimal numbers.) Each pair of these code words includes at least one position where its values differ by two or more modulo 5; for instance, \"11\" and \"23\" differ by two in their second position, while \"23\" and \"42\" differ by two in their first position. Therefore, a recipient of one of these code words will always be able to determine unambiguously which one was sent: no two of these code words can be confused with each other. By using this method, in \"n\" steps of communication, the sender can communicate up to 5 messages, significantly more than the 2 that could be transmitted with the simpler one-digit code. The effective number of values that can be transmitted per unit time step is (5) = .\nIn graph-theoretic terms, this means that the Shannon capacity of the 5-cycle is at least . As showed, this bound is tight: it is not possible to find a more complicated system of code words that allows even more different messages to be sent in the same amount of time, so the Shannon capacity of the 5-cycle is exactly .\n\nIf a graph \"G\" represents a set of symbols and the pairs of symbols that can be confused with each other, then a subset \"S\" of symbols avoids all confusable pairs if and only if \"S\" is an independent set in the graph, a subset of vertices that does not include both endpoints of any edge. The maximum possible size of a subset of the symbols that can all be distinguished from each other is the independence number \"α\"(\"G\") of the graph, the size of its maximum independent set. For instance, \"α\"(\"C\") = 2: the 5-cycle has independent sets of two vertices, but not larger.\n\nFor codewords of longer lengths, one can use independent sets in larger graphs to describe the sets of codewords that can be transmitted without confusion. For instance, for the same example of five symbols whose confusion graph is \"C\", there are 25 strings of length two that can be used in a length-2 coding scheme. These strings may be represented by the vertices of a graph with 25 vertices. In this graph, each vertex has eight neighbors, the eight strings that it can be confused with. A subset of length-two strings forms a code with no possible confusion if and only if it corresponds to an independent set of this graph. The set of code words {\"11\", \"23\", \"35\", \"54\", \"42\"} forms one of these independent sets, of maximum size.\n\nIf \"G\" is a graph representing the signals and confusable pairs of a channel, then the graph representing the length-two codewords and their confusable pairs is \"G\" ⊠ \"G\", where the symbol \"⊠\" represents the strong product of graphs. This is a graph that has a vertex for each pair (\"u\",\"v\") of a vertex in the first argument of the product and a vertex in the second argument of the product. Two distinct pairs (\"u\",\"v\") and (\"u\",\"v\") are adjacent in the strong product if and only if \"u\" and \"u\" are identical or adjacent, and \"v\" and \"v\" are identical or adjacent. More generally, the codewords of length \"k\" can be represented by the graph \"G\", the \"k\"-fold strong product of \"G\" with itself, and the maximum number of codewords of this length that can be transmitted without confusion is given by the independence number \"α\"(\"G\"). The effective number of signals transmitted per unit time step is the \"k\"th root of this number, \"α\"(\"G\").\n\nUsing these concepts, the Shannon capacity may be defined as\nthe limit (as \"k\" becomes arbitrarily large) of the effective number of signals per time step of arbitrarily long confusion-free codes.\n\nThe computational complexity of the Shannon capacity is unknown, and even the value of the Shannon capacity for certain small graphs such as \"C\" (a cycle graph of seven vertices) remains unknown.\n\nA natural approach to this problem would be to compute a finite number of powers of the given graph \"G\", find their independence numbers, and infer from these numbers some information about the limiting behavior of the sequence from which the Shannon capacity is defined. However (even ignoring the computational difficulty of computing the independence numbers of these graphs, an NP-hard problem) the unpredictable behavior of the sequence of independence numbers of powers of \"G\" implies that this approach cannot be used to accurately approximate the Shannon capacity.\n\nIn part because the Shannon capacity is difficult to compute, researchers have looked for other graph invariants that are easy to compute and that provide bounds on the Shannon capacity.\n\nThe Lovász number ϑ(\"G\") is a different graph invariant, that can be computed numerically to high accuracy in polynomial time by an algorithm based on the ellipsoid method. The Shannon capacity of a graph \"G\" is bounded from below by α(\"G\"), and from above by ϑ(\"G\"). In some cases, ϑ(\"G\") and the Shannon capacity coincide; for instance, for the graph of a pentagon, both are equal to . However, there exist other graphs for which the Shannon capacity and the Lovász number differ.\n\nHaemers provided another upper bound on the Shannon capacity, which is sometimes better than Lovász bound:\nwhere \"B\" is an \"n\" × \"n\" matrix over some field, such that \"b\" ≠ 0 and \"b\" = 0 if vertices \"i\" and \"j\" are not adjacent.\n"}
{"id": "12085484", "url": "https://en.wikipedia.org/wiki?curid=12085484", "title": "Slepian's lemma", "text": "Slepian's lemma\n\nIn probability theory, Slepian's lemma (1962), named after David Slepian, is a Gaussian comparison inequality. It states that for Gaussian random variables formula_1 and formula_2 in formula_3 satisfying formula_4,\n\nthe following inequality holds for all real numbers formula_8:\n\nWhile this intuitive-seeming result is true for Gaussian processes, it is not in general true for other random variables—not even those with expectation 0.\n\nAs a corollary, if formula_10 is a centered stationary Gaussian process such that formula_11 for all formula_12, it holds for any real number formula_13 that\n\nSlepian's lemma was first proven by Slepian in 1962, and has since been used in reliability theory, extreme value theory and areas of pure probability. It has also been re-proven in several different forms.\n\n"}
{"id": "36089423", "url": "https://en.wikipedia.org/wiki?curid=36089423", "title": "SolveIT Software", "text": "SolveIT Software\n\nSolveIT Software Pty Ltd is a provider of advanced planning and scheduling enterprise software for supply and demand optimisation and predictive modelling. Based in Adelaide, South Australia, 70% of its turnover is generated from software deployed in the mining and bulk material handling sectors.\n\nThe company was set up in 2005 by four academics who were also experienced business people, all recent immigrants to Australia. The team was headed by ex-Ernst and Young consultant Matthew Michalewicz, who had moved to Adelaide in 2004 after selling his last company, NuTech. The other three partners were Zbigniew Michalewicz Ph.D, Martin Schmidt and Constantin Chiriac, all four of which were co-authors of the book \"Adaptive Business Intelligence®\".\n\nThe company first developed an optimization and predictive modeling platform based on Artificial Intelligence, and then built its supply chain applications for planning, scheduling, and demand forecasting on this platform. Early customers included Orlando Wines, ABB Grain, the Fosters wine brands and later Pernod Ricard that were also located in the Barossa Valley region.\n\nIn 2008, Rio Tinto Iron Ore asked the company to improve its mining planning and scheduling operations based at Pilbara. SolveIT succeeded in applying its advanced planning and scheduling product, based on non-linear optimization, to the Rio Tinto mine scheduling problem, after many other vendors had failed over a period of ten years.\n\nWith 30 employees at this point, it then won an additional contract in the mining sector with BHP Billiton/Mitsubishi Alliance Coal, leading to subsequent tender wins in the sector, including: BHP Billiton; CBH Group; Fortescue Metals Group, Hills Holdings, Pacific National Coal; and Xstrata Coal.\n\nOn 3 September 2012, SolveIT announced it was acquired by Schneider Electric, a global specialist in energy management.\n\nHeadquartered in Adelaide, the company has over 150 staff based across operational offices in: Melbourne; Brisbane; Perth; and Chişinău, Moldova.\n\nThe company develops advanced planning and scheduling business optimisation software, which helps manage complex operations using artificial intelligence. Most of the products were initially developed around the key South Australian industries of wine and grain handling, and today SolveIt has a specialist mining division due to early adoption of the company's software within the mining market. The software helps companies accurately predict and plan their production, supply chain, shipping and currency hedging.\n\nDue to the scientific optimisation components embedded in the company’s software products, it sometimes uses a prize-based system to recruit the required high-level of talent. In 2011, the company used a Magic square problem, won by University of Nottingham graduate Yuri Bykov, who developed a program which solved a constrained version of a 2600 by 2600 magic square within a minute.\n\nIn 2011, the company won the Australian National iAward in the e-Logistics and Supply Chain category for its Supply Chain Network Optimiser (SCNO). In February 2012, SolveIT and Schneider Electric became co-organisers of the Integrated Planning and Optimisation Summit, held at the Adelaide Convention Centre.\n\n\nThe company’s mining division provides integrated planning, scheduling and optimisation for the whole traditional mining supply chain network of mine, process plant, transport network, port and trading desk. Variables into the systems allow for asset management, workforce variability, maintenance, accommodation and market factors.\n\n"}
{"id": "17395221", "url": "https://en.wikipedia.org/wiki?curid=17395221", "title": "Subdirect product", "text": "Subdirect product\n\nIn mathematics, especially in the areas of abstract algebra known as universal algebra, group theory, ring theory, and module theory, a subdirect product is a subalgebra of a direct product that depends fully on all its factors without however necessarily being the whole direct product. The notion was introduced by Birkhoff in 1944 and has proved to be a powerful generalization of the notion of direct product.\n\nA subdirect product is a subalgebra (in the sense of universal algebra) \"A\" of a direct product Π\"A\" such that every induced projection (the composite \"ps\": \"A\" → \"A\" of a projection \"p\": Π\"A\" → \"A\" with the subalgebra inclusion \"s\": \"A\" → Π\"A\") is surjective.\n\nA direct (subdirect) representation of an algebra \"A\" is a direct (subdirect) product isomorphic to \"A\".\n\nAn algebra is called subdirectly irreducible if it is not subdirectly representable by \"simpler\" algebras. Subdirect irreducibles are to subdirect product of algebras roughly as primes are to multiplication of integers.\n\n\n\n"}
{"id": "29803312", "url": "https://en.wikipedia.org/wiki?curid=29803312", "title": "System size expansion", "text": "System size expansion\n\nThe system size expansion, also known as van Kampen's expansion or the Ω-expansion, is a technique pioneered by Nico van Kampen used in the analysis of stochastic processes. Specifically, it allows one to find an approximation to the solution of a master equation with nonlinear transition rates. The leading order term of the expansion is given by the linear noise approximation, in which the master equation is approximated by a Fokker–Planck equation with linear coefficients determined by the transition rates and stoichiometry of the system.\n\nLess formally, it is normally straightforward to write down a mathematical description of a system where processes happen randomly (for example, radioactive atoms randomly decay in a physical system, or genes that are expressed stochastically in a cell). However, these mathematical descriptions are often too difficult to solve for the study of the systems statistics (for example, the mean and variance of the number of atoms or proteins as a function of time). The system size expansion allows one to obtain an approximate statistical description that can be solved much more easily than the master equation.\n\nSystems that admit a treatment with the system size expansion may be described by a probability distribution formula_1, giving the probability of observing the system in state formula_2 at time formula_3. formula_2 may be, for example, a vector with elements corresponding to the number of molecules of different chemical species in a system. In a system of size formula_5 (intuitively interpreted as the volume), we will adopt the following nomenclature: formula_6 is a vector of macroscopic copy numbers, formula_7 is a vector of concentrations, and formula_8 is a vector of deterministic concentrations, as they would appear according to the rate equation in an infinite system. formula_9 and formula_6 are thus quantities subject to stochastic effects.\n\nA master equation describes the time evolution of this probability. Henceforth, a system of chemical reactions will be discussed to provide a concrete example, although the nomenclature of \"species\" and \"reactions\" is generalisable. A system involving formula_11 species and formula_12 reactions can be described with the master equation:\n\nHere, formula_5 is the system size, formula_15 is an operator which will be addressed later, formula_16 is the stoichiometric matrix for the system (in which element formula_16 gives the stoichiometric coefficient for species formula_18 in reaction formula_19), and formula_20 is the rate of reaction formula_19 given a state formula_9 and system size formula_5.\n\nformula_24 is a step operator, removing formula_16 from the formula_18th element of its argument. For example, formula_27. This formalism will be useful later.\n\nThe above equation can be interpreted as follows. The initial sum on the RHS is over all reactions. For each reaction formula_19, the brackets immediately following the sum give two terms. The term with the simple coefficient −1 gives the probability flux away from a given state formula_6 due to reaction formula_19 changing the state. The term preceded by the product of step operators gives the probability flux due to reaction formula_19 changing a different state formula_32 into state formula_6. The product of step operators constructs this state formula_32.\n\nFor example, consider the (linear) chemical system involving two chemical species formula_35 and formula_36 and the reaction formula_37. In this system, formula_38 (species), formula_39 (reactions). A state of the system is a vector formula_40, where formula_41 are the number of molecules of formula_35 and formula_36 respectively. Let formula_44, so that the rate of reaction 1 (the only reaction) depends on the concentration of formula_35. The stoichiometry matrix is formula_46.\n\nThen the master equation reads:\n\nwhere formula_48 is the shift caused by the action of the product of step operators, required to change state formula_6 to a precursor state formula_50.\n\nIf the master equation possesses nonlinear transition rates, it may be impossible to solve it analytically. The system size expansion utilises the ansatz that the variance of the steady-state probability distribution of constituent numbers in a population scales like the system size. This ansatz is used to expand the master equation in terms of a small parameter given by the inverse system size.\n\nSpecifically, let us write the formula_51, the copy number of component formula_18, as a sum of its \"deterministic\" value (a scaled-up concentration) and a random variable formula_53, scaled by formula_54:\n\nThe probability distribution of formula_6 can then be rewritten in the vector of random variables formula_53:\n\nLet us consider how to write reaction rates formula_59 and the step operator formula_15 in terms of this new random variable. Taylor expansion of the transition rates gives:\n\nThe step operator has the effect formula_62 and hence formula_63:\n\nWe are now in a position to recast the master equation.\n\nThis rather frightening expression makes a bit more sense when we gather terms in different powers of formula_5. First, terms of order formula_54 give\n\nThese terms cancel, due to the macroscopic reaction equation\n\nThe terms of order formula_70 are more interesting:\n\nwhich can be written as\n\nwhere\n\nand\n\nThe time evolution of formula_75 is then governed by the linear Fokker–Planck equation with coefficient matrices formula_76 and formula_77 (in the large-formula_5 limit, terms of formula_79 may be neglected, termed the linear noise approximation). With knowledge of the reaction rates formula_80 and stoichiometry formula_81, the moments of formula_75 can then be calculated.\n\nThe linear noise approximation has become a popular technique for estimating the size of intrinsic noise in terms of coefficients of variation and Fano factors for molecular species in intracellular pathways. The second moment obtained from the linear noise approximation (on which the noise measures are based) are exact only if the pathway is composed of first-order reactions. However bimolecular reactions such as enzyme-substrate, protein-protein and protein-DNA interactions are ubiquitous elements of all known pathways; for such cases, the linear noise approximation can give estimates which are accurate in the limit of large reaction volumes. Since this limit is taken at constant concentrations, it follows that the linear noise approximation gives accurate results in the limit of large molecule numbers and becomes less reliable for pathways characterized by many species with low copy numbers of molecules. \n\nA number of studies have elucidated cases of the insufficiency of the linear noise approximation in biological contexts by comparison of its predictions with those of stochastic simulations. This has led to the investigation of higher order terms of the system size expansion that go beyond the linear approximation. These terms have been used to obtain more accurate moment estimates for the mean concentrations and for the variances of the concentration fluctuations in intracellular pathways. In particular, the leading order corrections to the linear noise approximation yield corrections of the conventional rate equations. Terms of higher order have also been used to obtain corrections to the variances and covariances estimates of the linear noise approximation. The linear noise approximation and corrections to it can be computed using the open source software intrinsic Noise Analyzer. The corrections have been shown to be particularly considerable for allosteric and non-allosteric enzyme-mediated reactions in intracellular compartments.\n"}
{"id": "38526066", "url": "https://en.wikipedia.org/wiki?curid=38526066", "title": "Systems medicine", "text": "Systems medicine\n\nSystems medicine is an interdisciplinary field of study that looks at the systems of the human body as part of an integrated whole, incorporating biochemical, physiological, and environment interactions. Systems medicine draws on systems science and systems biology, and considers complex interactions within the human body in light of a patient's genomics, behavior and environment.\n\nThe earliest uses of the term \"systems medicine\" appeared in 1992, in an article on systems medicine and pharmacology by B.J. Zeng and in a paper on systems biomedicine by T. Kamada.\n\nAn important topic in systems medicine and systems biomedicine is the development of computational models that describe disease progression and the effect of therapeutic interventions. \n\nMore recent approaches include the redefinition of disease phenotypes based on common mechanisms rather than symptoms. These provide then therapeutic targets including network pharmacology and drug repurposing. Since 2018, there is a dedicated scientific journal, Systems Medicine, published by Marie-Ann Liebert and with Jan Baumbach and Harald Schmidt as co-editors in chief. \n"}
{"id": "54608601", "url": "https://en.wikipedia.org/wiki?curid=54608601", "title": "Ziegler spectrum", "text": "Ziegler spectrum\n\nIn mathematics, the (right) Ziegler spectrum of a ring \"R\" is a topological space whose points are (isomorphism classes of) indecomposable pure-injective right modules. Its closed subsets correspond to theories of modules closed under arbitrary products and direct summands. Ziegler spectra are named after Martin Ziegler, who first defined and studied them.\n\nLet \"R\" be a ring. A (right) pp-n-formula is a formula in the language of (right) \"R\"-modules of the form\n\nwhere formula_2 are natural numbers, formula_3 is an formula_4 matrix with entries from \"R\", and formula_5 is an formula_6-tuple of variables and formula_7 is an formula_8-tuple of variables.\n\nThe (right) Ziegler spectrum, formula_9, of \"R\" is the topological space whose points are isomorphism classes of indecomposable pure-injective right modules, denoted by formula_10; The topology\nhas the sets\n\nas subbasis of open sets, where formula_12 range over\n(right) pp-1-formulae. One can show that these sets form a basis.\n\nZiegler spectra are rarely hausdorff and often fail to have the formula_13-property. However they are always compact and have a basis of compact open sets given by the sets formula_14 where formula_12 are pp-1-formulae.\n\nWhen the ring \"R\" is countable formula_9 is sober. It is not currently known if all Ziegler spectra are sober.\n"}
