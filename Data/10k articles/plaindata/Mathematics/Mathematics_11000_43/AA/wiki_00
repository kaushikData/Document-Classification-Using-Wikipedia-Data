{"id": "48254929", "url": "https://en.wikipedia.org/wiki?curid=48254929", "title": "Alain Chenciner", "text": "Alain Chenciner\n\nAlain Chenciner (23 October 1943, Villeneuve-sur-Lot) is a French mathematician, specializing in dynamical systems with applications to celestial mechanics.\nChenciner studied from 1963 to 1965 at the École polytechnique and did research from 1966 for CNRS (Attachée de Recherche) at the École Polytechnique (at the Centre de Mathématiques founded by Laurent Schwartz). In 1971 he received his Ph.D. from the University of Paris XI under Jean Cerf with thesis \"Sur la géométrie des strates de petites codimensions de l'espace des fonctions différentiables réelles sur une variété\" (Chenciner's thesis defense involved Henri Cartan, Laurent Schwartz and René Thom). Chenciner became a \"maître de conférences\" at the University of Paris XI, then in 1973 at the University of Paris VII, and in 1975 at the University of Nice. He returned in 1978 as \"maître de conférences\" to the University of Paris VII, where in 1981 he became professor extraordinarius (\"professeur de première classe\") and in 1991 professor ordinarius (\"professeur en classe exceptionelle\"). In 2012 he became professor emeritus.\n\nAt the beginning of his career Chenciner worked on differential topology and its applications to dynamical systems, following the pioneering efforts of Stephen Smale, and also worked on singularity theory. Later in his career he worked on mathematical problems of celestial mechanics (specifically, the three-body problem and the n-body problem) and studied bifurcations at elliptical fixed points of dynamical systems.\n\nChenciner, with Jacques Laskar, founded in 1992 the research group \"astronomie et systèmes dynamiques\" at the Observatory of Paris. In 2002 he was an invited speaker at the International Congress of Mathematicians in Beijing and gave a talk \"Action minimizing solutions of the Newtonian n-body problem: from homology to symmetry\". He became in 2012 a fellow of the American Mathematical Society. He gave a eulogy on 9 July 2012 at cimetière du Montparnasse to mark the centenary of the death of Henri Poincaré.\n\nChenciner's doctoral students include Daniel Bennequin.\n\n\n"}
{"id": "21796451", "url": "https://en.wikipedia.org/wiki?curid=21796451", "title": "Altenberg Workshops in Theoretical Biology", "text": "Altenberg Workshops in Theoretical Biology\n\nThe Altenberg Workshops in Theoretical Biology are expert meetings focused on a key issue of biological theory, hosted by the Konrad Lorenz Institute for Evolution and Cognition Research (KLI) since 1996. The workshops are organized by leading experts in their field, who invite a group of international top level scientists as participants for a 3-day working meeting in the Lorenz Mansion at Altenberg near Vienna, Austria. By this procedure the KLI intends to generate new conceptual advances and research initiatives in the biosciences, which, due to their explicit interdisciplinary nature, are attractive to a wide variety of scientists from practically all fields of biology and the neighboring disciplines.\n\n\n"}
{"id": "4416073", "url": "https://en.wikipedia.org/wiki?curid=4416073", "title": "Approximations of π", "text": "Approximations of π\n\nApproximations for the mathematical constant pi () in the history of mathematics reached an accuracy within 0.04% of the true value before the beginning of the Common Era (Archimedes). In Chinese mathematics, this was improved to approximations correct to what corresponds to about seven decimal digits by the 5th century.\n\nFurther progress was not made until the 15th century (Jamshīd al-Kāshī). Early modern mathematicians reached an accuracy of 35 digits by the beginning of the 17th century (Ludolph van Ceulen), and 126 digits by the 19th century (Jurij Vega), surpassing the accuracy required for any conceivable application outside of pure mathematics.\n\nThe record of manual approximation of is held by William Shanks, who calculated 527 digits correctly in the years preceding 1873. Since the middle of the 20th century, the approximation of has been the task of electronic digital computers; , the record is 22.4trillion digits. (For a comprehensive account, see Chronology of computation of.)\n\nThe best known approximations to dating to before the Common Era were accurate to two decimal places; this was improved upon in Chinese mathematics in particular by the mid-first millennium, to an accuracy of seven decimal places. After this, no further progress was made until the late medieval period.\n\nSome Egyptologists\nhave claimed that the ancient Egyptians used an approximation of as from as early as the Old Kingdom.\nThis claim has met with skepticism.\n\nBabylonian mathematics usually approximated to 3, sufficient for the architectural projects of the time (notably also reflected in the description of Solomon's Temple in the Hebrew Bible).\nThe Babylonians were aware that this was an approximation, and one Old Babylonian mathematical tablet excavated near Susa in 1936 (dated to between the 19th and 17th centuries BCE) gives a better approximation of as , about 0.5 percent below the exact value.\n\nAt about the same time, the Egyptian Rhind Mathematical Papyrus (dated to the Second Intermediate Period, c. 1600 BCE, although stated to be a copy of an older, Middle Kingdom text) implies an approximation of as ≈ 3.16 (accurate to 0.6 percent) by calculating the area of a circle by approximating the circle by an octagon.\n\nAstronomical calculations in the \"Shatapatha Brahmana\" (c. 6th century BCE) use a fractional approximation of .\n\nIn the 3rd century BCE, Archimedes proved the sharp inequalities  <  < , by means of regular 96-gons (accuracies of 2·10 and 4·10, respectively).\n\nIn the 2nd century CE, Ptolemy, used the value , the first known approximation accurate to three decimal places (accuracy 2·10).\n\nThe Chinese mathematician Liu Hui in 263 CE computed to between and by inscribing a 96-gon and 192-gon; the average of these two values is (accuracy 9·10).\nHe also suggested that 3.14 was a good enough approximation for practical purposes. He has also frequently been credited with a later and more accurate result (accuracy 2·10), although some scholars instead believe that this is due to the later (5th-century) Chinese mathematician Zu Chongzhi.\nZu Chongzhi is known to have computed between 3.1415926 and 3.1415927, which was correct to seven decimal places. He gave two other approximations of: and . The latter fraction is the best possible rational approximation of using fewer than five decimal digits in the numerator and denominator. Zu Chongzhi's result surpasses the accuracy reached in Hellenistic mathematics, and would remain without improvement for close to a millennium.\n\nIn Gupta-era India (6th century), mathematician Aryabhata in his astronomical treatise Āryabhaṭīya calculated the value of to five significant figures (). using it to calculate an approximation of the Earth's circumference. Aryabhata stated that his result \"approximately\" (\"\" \"approaching\") gave the circumference of a circle. His 15th-century commentator Nilakantha Somayaji (Kerala school of astronomy and mathematics) has argued that the word means not only that this is an approximation, but that the value is incommensurable (irrational).\n\nBy the 5th century CE, was known to about seven digits in Chinese mathematics, and to about five in Indian mathematics. Further progress was not made for nearly a millennium, until the 14th century, when Indian mathematician and astronomer Madhava of Sangamagrama, founder of the Kerala school of astronomy and mathematics, discovered the infinite series for , now known as the Madhava–Leibniz series, and gave two methods for computing the value of . One of these methods is to obtain a rapidly converging series by transforming the original infinite series of . By doing so, he obtained the infinite series\n\nand used the first 21 terms to compute an approximation of correct to 11 decimal places as .\n\nThe other method he used was to add a remainder term to the original series of . He used the remainder term\n\nin the infinite series expansion of to improve the approximation of to 13 decimal places of accuracy when  = 75.\n\nJamshīd al-Kāshī (Kāshānī), a Persian astronomer and mathematician, correctly computed 2 to 9 sexagesimal digits in 1424. This figure is equivalent to 17 decimal digits as\n\nwhich equates to\n\nHe achieved this level of accuracy by calculating the perimeter of a regular polygon with 3 × 2 sides.\n\nIn the second half of the 16th century, the French mathematician François Viète discovered an infinite product that converged on known as Viète's formula.\n\nThe German-Dutch mathematician Ludolph van Ceulen (\"circa\" 1600) computed the first 35 decimal places of with a 2-gon. He was so proud of this accomplishment that he had them inscribed on his tombstone.\n\nIn \"Cyclometricus\" (1621), Willebrord Snellius demonstrated that the perimeter of the inscribed polygon converges on the circumference twice as fast as does the perimeter of the corresponding circumscribed polygon. This was proved by Christiaan Huygens in 1654. Snellius was able to obtain seven digits of from a 96-sided polygon.\n\nIn 1789, the Slovene mathematician Jurij Vega calculated the first 140 decimal places for , of which the first 126 were correct and held the world record for 52 years until 1841, when William Rutherford calculated 208 decimal places, of which the first 152 were correct. Vega improved John Machin's formula from 1706 and his method is still mentioned today.\n\nThe magnitude of such precision (152 decimal places) can be put into context by the fact that the circumference of the largest known object, the observable universe, can be calculated from its diameter (93billion light-years) to a precision of less than one Planck length (at , the shortest unit of length that has real meaning) using expressed to just 62 decimal places.\n\nThe English amateur mathematician William Shanks, a man of independent means, spent over 20 years calculating to 707 decimal places. This was accomplished in 1873, with the first 527 places correct. He would calculate new digits all morning and would then spend all afternoon checking his morning's work. This was the longest expansion of until the advent of the electronic digital computer three-quarters of a century later.\n\nIn 1910, the Indian mathematician Srinivasa Ramanujan found several rapidly converging infinite series of , including\n\nwhich computes a further eight decimal places of with each term in the series. His series are now the basis for the fastest algorithms currently used to calculate . See also Ramanujan–Sato series.\n\nFrom the mid-20th century onwards, all calculations of have been done with the help of calculators or computers.\n\nIn 1944, D. F. Ferguson, with the aid of a mechanical desk calculator, found that William Shanks had made a mistake in the 528th decimal place, and that all succeeding digits were incorrect.\n\nIn the early years of the computer, an expansion of to decimal places was computed by Maryland mathematician Daniel Shanks (no relation to the above-mentioned William Shanks) and his team at the United States Naval Research Laboratory in Washington, D.C. In 1961, Shanks and his team used two different power series for calculating the digits of . For one, it was known that any error would produce a value slightly high, and for the other, it was known that any error would produce a value slightly low. And hence, as long as the two series produced the same digits, there was a very high confidence that they were correct. The first 100,265 digits of were published in 1962. The authors outlined what would be needed to calculate to 1 million decimal places and concluded that the task was beyond that day's technology, but would be possible in five to seven years.\n\nIn 1989, the Chudnovsky brothers computed to over 1 billion decimal places on the supercomputer IBM 3090 using the following variation of Ramanujan's infinite series of :\n\nIn 1999, Yasumasa Kanada and his team at the University of Tokyo computed to over 200 billion decimal places on the supercomputer HITACHI SR8000/MPP (128 nodes) using another variation of Ramanujan's infinite series of . In October 2005, they claimed to have calculated it to 1.24 trillion places.\n\nRecords since then have all been accomplished on personal computers using the Chudnovsky algorithm. In 2009, Fabrice Bellard computed just under 2.7 trillion digits, and from 2010 onward, all records have been set using Alexander Yee's y-cruncher software. , the record stands at 22,459,157,718,361 ( × 10) digits. The limitation on further expansion is primarily storage space for the computation.\n\nIn November 2002, Yasumasa Kanada and a team of 9 others used the Hitachi SR8000, a 64-node supercomputer with 1 terabyte of main memory, to calculate to roughly 1.24 trillion digits in around 600 hours.\n\nIn August 2009, a Japanese supercomputer called the T2K Open Supercomputer more than doubled the previous record by calculating to roughly 2.6 trillion digits in approximately 73 hours and 36 minutes.\n\nIn December 2009, Fabrice Bellard used a home computer to compute 2.7 trillion decimal digits of . Calculations were performed in base 2 (binary), then the result was converted to base 10 (decimal). The calculation, conversion, and verification steps took a total of 131 days.\n\nIn August 2010, Shigeru Kondo used Alexander Yee's y-cruncher to calculate 5 trillion digits of . This was the world record for any type of calculation, but significantly it was performed on a home computer built by Kondo. The calculation was done between 4 May and 3 August, with the primary and secondary verifications taking 64 and 66 hours respectively.\n\nIn October 2011, Shigeru Kondo broke his own record by computing ten trillion (10) and fifty digits using the same method but with better hardware.\n\nIn December 2013, Kondo broke his own record for a second time when he computed 12.1 trillion digits of .\n\nIn October 2014, Sandon Van Ness, going by the pseudonym \"houkouonchi\" used y-cruncher to calculate 13.3 trillion digits of .\n\nIn November 2016, Peter Trueb and his sponsors computed on y-cruncher and fully verified 22.4 trillion digits of . The computation took (with three interruptions) 105 days to complete.\n\nDepending on the purpose of a calculation, can be approximated by using fractions for ease of calculation. The most notable such approximations are (relative error of about 4·10) and (relative error of about 8·10).\n\nOf some notability are legal or historical texts purportedly \"defining \" to have some rational value, such as the \"Indiana Pi Bill\" of 1897, which stated \"the ratio of the diameter and circumference is as five-fourths to four\" (which would imply \"\") and a passage in the Hebrew Bible that implies that .\n\nThe so-called \"Indiana Pi Bill\" of 1897 has often been characterized as an attempt to \"legislate the value of Pi\". Rather, the bill dealt with a purported solution to the problem of geometrically \"squaring the circle\".\n\nThe bill was nearly passed by the Indiana General Assembly in the U.S., and has been claimed to imply a number of different values for , although the closest it comes to explicitly asserting one is the wording \"the ratio of the diameter and circumference is as five-fourths to four\", which would make , a discrepancy of nearly 2 percent. A mathematics professor who happened to be present the day the bill was brought up for consideration in the Senate, after it had passed in the House, helped to stop the passage of the bill on its second reading, after which the assembly thoroughly ridiculed it before tabling it indefinitely.\n\nIt is sometimes claimed that the Hebrew Bible implies that \" equals three\", based on a passage in and giving measurements for the round basin located in front of the Temple in Jerusalem as having a diameter of 10 cubits and a circumference of 30 cubits.\n\nThe issue is discussed in the Talmud and in Rabbinic literature. Among the many explanations and comments are these:\n\nThere is still some debate on this passage in biblical scholarship. Many reconstructions of the basin show a wider brim (or flared lip) extending outward from the bowl itself by several inches to match the description given in In the succeeding verses, the rim is described as \"a handbreadth thick; and the brim thereof was wrought like the brim of a cup, like the flower of a lily: it received and held three thousand baths\" , which suggests a shape that can be encompassed with a string shorter than the total length of the brim, e.g., a Lilium flower or a Teacup.\n\nArchimedes, in his \"Measurement of a Circle\", created the first algorithm for the calculation of based on the idea that the perimeter of any (convex) polygon inscribed in a circle is less than the circumference of the circle, which, in turn, is less than the perimeter of any circumscribed polygon. He started with inscribed and circumscribed regular hexagons, whose perimeters are readily determined. He then shows how to calculate the perimeters of regular polygons of twice as many sides that are inscribed and circumscribed about the same circle. This is a recursive procedure which would be described today as follows: Let and denote the perimeters of regular polygons of sides that are inscribed and circumscribed about the same circle, respectively. Then,\nArchimedes uses this to successively compute and . Using these last values he obtains\nIt is not known why Archimedes stopped at a 96-sided polygon; it only takes patience to extend the computations. Heron reports in his \"Metrica\" (about 60 CE) that Archimedes continued the computation in a now lost book, but then attributes an incorrect value to him.\n\nArchimedes uses no trigonometry in this computation and the difficulty in applying the method lies in obtaining good approximations for the square roots that are involved. Trigonometry, in the form of a table of chord lengths in a circle, was probably used by Claudius Ptolemy of Alexandria to obtain the value of given in the \"Almagest\" (circa 150 CE).\n\nAdvances in the approximation of (when the methods are known) were made by increasing the number of sides of the polygons used in the computation. A trigonometric improvement by Willebrord Snell (1621) obtains better bounds from a pair of bounds gotten from the polygon method. Thus, more accurate results were obtained from polygons with fewer sides. Viète's formula, published by François Viète in 1593, was derived by Viète using a closely related polygonal method, but with areas rather than perimeters of polygons whose numbers of sides are powers of two.\n\nThe last major attempt to compute by this method was carried out by Grienberger in 1630 who calculated 39 decimal places of using Snell's refinement.\n\nFor fast calculations, one may use formulae such as Machin's:\ntogether with the Taylor series expansion of the function arctan(\"x\"). This formula is most easily verified using polar coordinates of complex numbers, producing:\n\nformula_10\n\nFormulae of this kind are known as \"Machin-like formulae\". Machin's particular formula was used well into the computer era for calculating record numbers of digits of , but more recently other similar formulae have been used as well.\n\nFor instance, Shanks and his team used the following Machin-like formula in 1961 to compute the first 100,000 digits of :\n\nformula_11\n\nand they used another Machin-like formula,\n\nformula_12\n\nas a check.\n\nThe record as of December 2002 by Yasumasa Kanada of Tokyo University stood at 1,241,100,000,000 digits. The following Machin-like formulae were used for this:\nK. Takano (1982).\nF. C. W. Störmer (1896).\n\nOther formulae that have been used to compute estimates of include:\n\nLiu Hui (see also Viète's formula):\n\nMadhava:\n\nEuler:\n\nNewton / Euler Convergence Transformation:\nwhere (2k+1)!! denotes the product of the odd integers up to 2k+1.\n\nRamanujan:\n\nDavid Chudnovsky and Gregory Chudnovsky:\n\nRamanujan's work is the basis for the Chudnovsky algorithm, the fastest algorithms used, as of the turn of the millennium, to calculate .\n\nExtremely long decimal expansions of are typically computed with iterative formulae like the Gauss–Legendre algorithm and Borwein's algorithm. The latter, found in 1985 by Jonathan and Peter Borwein, converges extremely quickly:\n\nFor formula_21 and\nwhere formula_23, the sequence formula_24 converges quartically to , giving about 100 digits in three steps and over a trillion digits after 20 steps. However, it is known that using an algorithm such as the Chudnovsky algorithm (which converges linearly) is faster than these iterative formulae.\n\nThe first one million digits of and are available from Project Gutenberg (see external links below). A former calculation record (December 2002) by Yasumasa Kanada of Tokyo University stood at 1.24 trillion digits, which were computed in September 2002 on a 64-node Hitachi supercomputer with 1 terabyte of main memory, which carries out 2 trillion operations per second, nearly twice as many as the computer used for the previous record (206 billion digits). The following Machin-like formulæ were used for this:\n\nThese approximations have so many digits that they are no longer of any practical use, except for testing new supercomputers. Properties like the potential normality of will always depend on the infinite string of digits on the end, not on any finite computation.\n\nHistorically, base 60 was used for calculations. In this base, can be approximated to eight (decimal) significant figures with the number 3:8:29:44, which is\n\nIn addition, the following expressions can be used to estimate :\n\nPi can be obtained from a circle if its radius and area are known using the relationship:\n\nIf a circle with radius ' is drawn with its center at the point (0, 0), any point whose distance from the origin is less than ' will fall inside the circle. The Pythagorean theorem gives the distance from any point (, ) to the center:\n\nMathematical \"graph paper\" is formed by imagining a 1×1 square centered around each cell (, ), where and are integers between − and . Squares whose center resides inside or exactly on the border of the circle can then be counted by testing whether, for each cell (, ),\n\nThe total number of cells satisfying that condition thus approximates the area of the circle, which then can be used to calculate an approximation of . Closer approximations can be produced by using larger values of .\n\nMathematically, this formula can be written:\n\nIn other words, begin by choosing a value for . Consider all cells (, ) in which both and are integers between − and . Starting at 0, add 1 for each cell whose distance to the origin (0,0) is less than or equal to \"\". When finished, divide the sum, representing the area of a circle of radius , by to find the approximation of .\nFor example, if is 5, then the cells considered are:\n\nThe 12 cells (0, ±5), (±5, 0), (±3, ±4), (±4, ±3) are \"exactly on\" the circle, and 69 cells are \"completely inside\", so the approximate area is 81, and is calculated to be approximately 3.24 because 81 / 5 = 3.24. Results for some values of are shown in the table below:\n\nFor related results see The circle problem: number of points (x,y) in square lattice with x^2 + y^2 <= n.\n\nSimilarly, the more complex approximations of given below involve repeated calculations of some sort, yielding closer and closer approximations with increasing numbers of calculations.\n\nBesides its simple continued fraction representation [3; 7, 15, 1, 292, 1, 1...], which displays no discernible pattern, has many generalized continued fraction representations generated by a simple rule, including these two.\n\nThe Gregory–Leibniz series\nis the power series for arctan(x) specialized to  = 1. It converges too slowly to be of practical interest. However, the power series converges much faster for smaller values of formula_57, which leads to formulae where formula_58 arises as the sum of small angles with rational tangents, known as Machin-like formulae.\n\nKnowing that 4 arctan 1 = , the formula can be simplified to get:\n\nwith a convergence such that each additional 10 terms yields at least three more digits.\n\nObserving an equilateral triangle and noting that\nyields\nwith a convergence such that each additional five terms yields at least three more digits.\n\nThe Gauss–Legendre algorithm or Salamin–Brent algorithm was discovered independently by Richard Brent and Eugene Salamin in 1975. This can compute formula_58 to formula_63 digits in time proportional to formula_64, much faster than the trigonometric formulae.\n\nThe Bailey–Borwein–Plouffe formula (BBP) for calculating was discovered in 1995 by Simon Plouffe. Using math, the formula can compute any particular digit of —returning the hexadecimal value of the digit—without having to compute the intervening digits (digit extraction).\nIn 1996, Simon Plouffe derived an algorithm to extract the th decimal digit of (using base10 math to extract a base10 digit), and which can do so with an improved speed of time. The algorithm requires virtually no memory for the storage of an array or matrix so the one-millionth digit of can be computed using a pocket calculator. However, it would be quite tedious and impractical to do so.\nThe calculation speed of Plouffe's formula was improved to by Fabrice Bellard, who derived an alternative formula (albeit only in base2 math) for computing .\n\nMany other expressions for were developed and published by Indian mathematician Srinivasa Ramanujan. He worked with mathematician Godfrey Harold Hardy in England for a number of years.\n\nExtremely long decimal expansions of are typically computed with the Gauss–Legendre algorithm and Borwein's algorithm; the Salamin–Brent algorithm, which was invented in 1976, has also been used.\n\nIn 1997, David H. Bailey, Peter Borwein and Simon Plouffe published a paper (Bailey, 1997) on a new formula for as an infinite series:\n\nThis formula permits one to fairly readily compute the \"k\"th binary or hexadecimal digit of , without having to compute the preceding \"k\" − 1 digits. Bailey's website contains the derivation as well as implementations in various programming languages. The PiHex project computed 64 bits around the quadrillionth bit of (which turns out to be 0).\n\nFabrice Bellard further improved on BBP with his formula:\n\nOther formulae that have been used to compute estimates of include:\nThis converges extraordinarily rapidly. Ramanujan's work is the basis for the fastest algorithms used, as of the turn of the millennium, to calculate .\n\nPi Hex was a project to compute three specific binary digits of using a distributed network of several hundred computers. In 2000, after two years, the project finished computing the five trillionth (5*10), the forty trillionth, and the quadrillionth (10) bits. All three of them turned out to be 0.\n\nOver the years, several programs have been written for calculating to many digits on personal computers.\n\nMost computer algebra systems can calculate and other common mathematical constants to any desired precision.\n\nFunctions for calculating are also included in many general libraries for arbitrary-precision arithmetic, for instance Class Library for Numbers and MPFR.\n\nPrograms designed for calculating may have better performance than general-purpose mathematical software. They typically implement checkpointing and efficient disk swapping to facilitate extremely long-running and memory-expensive computations.\n\n"}
{"id": "1367647", "url": "https://en.wikipedia.org/wiki?curid=1367647", "title": "Band sum", "text": "Band sum\n\nIn geometric topology, a band sum of two \"n\"-dimensional knots \"K\" and \"K\" along an (\"n\" + 1)-dimensional 1-handle \"h\" called a \"band\" is an \"n\"-dimensional knot \"K\" such that:\n\n\n\"K\" is the \"n\"-dimensional knot obtained by this surgery.\n\nA band sum is thus a generalization of the usual connected sum of knots.\n\n\n"}
{"id": "39986184", "url": "https://en.wikipedia.org/wiki?curid=39986184", "title": "Blake canonical form", "text": "Blake canonical form\n\nIn Boolean logic, a formula for a Boolean function \"f\" is in Blake canonical form (BCF), also called the complete sum of prime implicants, the complete sum, or the disjunctive prime form, when it is a disjunction of all the prime implicants of \"f\". The Blake canonical form is a disjunctive normal form.\n\nThe Blake canonical form is not necessarily minimal, however all the terms of a minimal sum are contained in the Blake canonical form.\n\nIt was introduced in 1937 by Archie Blake, who called it the \"simplified canonical form\"; it was named in honor of Blake by Frank Markham Brown in 1990.\n\nBlake discussed three methods for calculating the canonical form: exhaustion of implicants, iterated consensus, and multiplication. The iterated consensus method was rediscovered by Samson and Mills, Quine, and Bing.\n\n"}
{"id": "37184509", "url": "https://en.wikipedia.org/wiki?curid=37184509", "title": "Brewer sum", "text": "Brewer sum\n\nIn mathematics, Brewer sums are finite character sum introduced by related to Jacobsthal sums.\n\nThe Brewer sum is given by\nwhere \"D\" is the Dickson polynomial (or \"Brewer polynomial\") given by \nand () is the Legendre symbol.\n\nThe Brewer sum is zero when \"n\" is coprime to \"q\"−1.\n\n"}
{"id": "44846510", "url": "https://en.wikipedia.org/wiki?curid=44846510", "title": "Cobordism ring", "text": "Cobordism ring\n\nIn mathematics, the oriented cobordism ring is a ring where elements are oriented cobordism classes of manifolds, the multiplication is given by the Cartesian product of manifolds and the addition is given as the disjoint union of manifolds. The ring is graded by dimensions of manifolds and is denoted by\nwhere formula_2 consists of oriented cobordism classes of manifolds of dimension \"n\". One can also define an unoriented cobordism ring, denoted by formula_3. If \"O\" is replaced \"U\", then one gets the complex cobordism ring, oriented or unoriented.\n\nIn general, one writes formula_4 for the cobordism ring of manifolds with structure \"B\".\n\nA theorem of Thom says:\nwhere \"MO\" is the Thom spectrum.\n\n"}
{"id": "149848", "url": "https://en.wikipedia.org/wiki?curid=149848", "title": "Combinatory logic", "text": "Combinatory logic\n\nCombinatory logic is a notation to eliminate the need for quantified variables in mathematical logic. It was introduced by Moses Schönfinkel and Haskell Curry, and has more recently been used in computer science as a theoretical model of computation and also as a basis for the design of functional programming languages. It is based on combinators which were introduced by Schönfinkel in 1920 with the idea of providing an analogous way to build up functions - and to remove any mention of variables - particularly in predicate logic. A combinator is a higher-order function that uses only function application and earlier defined combinators to define a result from its arguments.\n\nCombinatory logic was originally intended as a 'pre-logic' that would clarify the role of quantified variables in logic, essentially by eliminating them. Another way of eliminating quantified variables is Quine's predicate functor logic. While the expressive power of combinatory logic typically exceeds that of first-order logic, the expressive power of predicate functor logic is identical to that of first order logic (Quine 1960, 1966, 1976).\n\nThe original inventor of combinatory logic, Moses Schönfinkel, published nothing on combinatory logic after his original 1924 paper. Haskell Curry rediscovered the combinators while working as an instructor at Princeton University in late 1927. In the latter 1930s, Alonzo Church and his students at Princeton invented a rival formalism for functional abstraction, the lambda calculus, which proved more popular than combinatory logic. The upshot of these historical contingencies was that until theoretical computer science began taking an interest in combinatory logic in the 1960s and 1970s, nearly all work on the subject was by Haskell Curry and his students, or by Robert Feys in Belgium. Curry and Feys (1958), and Curry \"et al.\" (1972) survey the early history of combinatory logic. For a more modern treatment of combinatory logic and the lambda calculus together, see the book by Barendregt, which reviews the models Dana Scott devised for combinatory logic in the 1960s and 1970s.\nIn computer science, combinatory logic is used as a simplified model of computation, used in computability theory and proof theory. Despite its simplicity, combinatory logic captures many essential features of computation.\n\nCombinatory logic can be viewed as a variant of the lambda calculus, in which lambda expressions (representing functional abstraction) are replaced by a limited set of \"combinators\", primitive functions from which bound variables are absent. It is easy to transform lambda expressions into combinator expressions, and combinator reduction is much simpler than lambda reduction. Hence combinatory logic has been used to model some non-strict functional programming languages and hardware. The purest form of this view is the programming language Unlambda, whose sole primitives are the S and K combinators augmented with character input/output. Although not a practical programming language, Unlambda is of some theoretical interest.\n\nCombinatory logic can be given a variety of interpretations. Many early papers by Curry showed how to translate axiom sets for conventional logic into combinatory logic equations (Hindley and Meredith 1990). Dana Scott in the 1960s and 1970s showed how to marry model theory and combinatory logic.\n\nLambda calculus is concerned with objects called \"lambda-terms\", which can be represented by\nthe following three forms of strings:\n\nwhere \"v\" is a variable name drawn from a predefined infinite set of\nvariable names, and and are lambda-terms.\n\nTerms of the form are called \"abstractions\". The variable \"v\" is\ncalled the formal parameter of the abstraction, and is the \"body\"\nof the abstraction. The term represents the function which, applied\nto an argument, binds the formal parameter \"v\" to the argument and then\ncomputes the resulting value of — that is, it returns , with\nevery occurrence of \"v\" replaced by the argument.\n\nTerms of the form are called \"applications\". Applications model\nfunction invocation or execution: the function represented by is to be\ninvoked, with as its argument, and the result is computed. If \n(sometimes called the \"applicand\") is an abstraction, the term may be\n\"reduced\": , the argument, may be substituted into the body of \nin place of the formal parameter of , and the result is a new lambda\nterm which is \"equivalent\" to the old one. If a lambda term contains no\nsubterms of the form then it cannot be reduced, and is said to\nbe in normal form.\n\nThe expression represents the result of taking the term and replacing all free occurrences of in it with . Thus we write\n\nBy convention, we take as shorthand for (i.e., application is left associative).\n\nThe motivation for this definition of reduction is that it captures\nthe essential behavior of all mathematical functions. For example,\nconsider the function that computes the square of a number. We might\nwrite\n\n(Using \"\" to indicate multiplication.) \"x\" here is the formal parameter of the function. To evaluate the square for a particular\nargument, say 3, we insert it into the definition in place of the\nformal parameter:\n\nTo evaluate the resulting expression , we would have to resort to\nour knowledge of multiplication and the number 3. Since any\ncomputation is simply a composition of the evaluation of suitable\nfunctions on suitable primitive arguments, this simple substitution\nprinciple suffices to capture the essential mechanism of computation.\nMoreover, in lambda calculus, notions such as '3' and \" can be\nrepresented without any need for externally defined primitive\noperators or constants. It is possible to identify terms in lambda calculus, which, when suitably interpreted, behave like the\nnumber 3 and like the multiplication operator, q.v. Church encoding.\n\nLambda calculus is known to be computationally equivalent in power to\nmany other plausible models for computation (including Turing machines); that is, any calculation that can be accomplished in any\nof these other models can be expressed in lambda calculus, and\nvice versa. According to the Church-Turing thesis, both models\ncan express any possible computation.\n\nIt is perhaps surprising that lambda-calculus can represent any\nconceivable computation using only the simple notions of function\nabstraction and application based on simple textual substitution of\nterms for variables. But even more remarkable is that abstraction is\nnot even required. \"Combinatory logic\" is a model of computation\nequivalent to lambda calculus, but without abstraction. The advantage\nof this is that evaluating expressions in lambda calculus is quite complicated\nbecause the semantics of substitution must be specified with great care to\navoid variable capture problems. In contrast, evaluating expressions in\ncombinatory logic is much simpler, because there is no notion of substitution.\n\nSince abstraction is the only way to manufacture functions in the\nlambda calculus, something must replace it in the combinatory\ncalculus. Instead of abstraction, combinatory calculus provides a\nlimited set of primitive functions out of which other functions may be\nbuilt.\n\nA combinatory term has one of the following forms:\nwhere is a variable, is one of the primitive functions, and is the application of combinatory terms and . The primitive functions themselves are \"combinators\", or functions that, when seen as lambda terms, contain no free variables.\nTo shorten the notations, a general convention is that , or even , denotes the term . This is the same general convention (left-associativity) as for multiple application in lambda calculus.\n\nIn combinatory logic, each primitive combinator comes with a reduction rule of the form\n\nwhere \"E\" is a term mentioning only variables from the set . It is in this way that primitive combinators behave as functions.\n\nThe simplest example of a combinator is I, the identity\ncombinator, defined by\n\nfor all terms \"x\". Another simple combinator is K, which\nmanufactures constant functions: (K \"x\") is the function which,\nfor any argument, returns \"x\", so we say\n\nfor all terms \"x\" and \"y\". Or, following the convention for\nmultiple application,\n\nA third combinator is S, which is a generalized version of\napplication:\n\nS applies \"x\" to \"y\" after first substituting \"z\" into\neach of them. Or put another way, \"x\" is applied to \"y\" inside the\nenvironment \"z\".\n\nGiven S and K, I itself is unnecessary, since it can\nbe built from the other two:\n\nfor any term \"x\". Note that although ((S K K)\n\"x\") = (I \"x\") for any \"x\", (S K K)\nitself is not equal to I. We say the terms are extensionally equal. Extensional equality captures the\nmathematical notion of the equality of functions: that two functions\nare \"equal\" if they always produce the same results for the same\narguments. In contrast, the terms themselves, together with the\nreduction of primitive combinators, capture the notion of\n\"intensional equality\" of functions: that two functions are \"equal\"\nonly if they have identical implementations up to the expansion of primitive\ncombinators when these ones are applied to enough arguments. There are many ways to\nimplement an identity function; (S K K) and I\nare among these ways. (S K S) is yet another. We\nwill use the word \"equivalent\" to indicate extensional equality,\nreserving \"equal\" for identical combinatorial terms.\n\nA more interesting combinator is the fixed point combinator or Y combinator, which can be used to implement recursion.\n\nS and K can be composed to produce combinators that are extensionally equal to \"any\" lambda term, and therefore, by Church's thesis, to any computable function whatsoever. The proof is to present a transformation, \"T\"[ ], which converts an arbitrary lambda term into an equivalent combinator.\n\n\"T\"[ ] may be defined as follows:\n\nThis process is also known as \"abstraction elimination\". This definition is exhaustive: any lambda expression will be subject to exactly one of these rules (see Summary of lambda calculus above).\n\nIt is related to the process of \"bracket abstraction\", which takes an expression \"E\" built from variables and application and produces a combinator expression [x]E in which the variable x is not free, such that [\"x\"]\"E x\" = \"E\" holds.\nA very simple algorithm for bracket abstraction is defined by induction on the structure of expressions as follows:\nBracket abstraction induces a translation from lambda terms to combinator expressions, by interpreting lambda-abstractions using the bracket abstraction algorithm.\n\nFor example, we will convert the lambda term \"λx\".\"λy\".(\"y\" \"x\") to a\ncombinatorial term:\n\nIf we apply this combinatorial term to any two terms \"x\" and \"y\", it\nreduces as follows:\n\nThe combinatory representation, (S (K (S I)) (S (K K) I)) is much\nlonger than the representation as a lambda term, \"λx\".\"λy\".(y x). This is typical. In general, the \"T\"[ ] construction may expand a lambda\nterm of length \"n\" to a combinatorial term of length\nΘ(\"n\") .\n\nThe \"T\"[ ] transformation is motivated by a desire to eliminate\nabstraction. Two special cases, rules 3 and 4, are trivial: \"λx\".\"x\" is\nclearly equivalent to I, and \"λx\".\"E\" is clearly equivalent to\n(K \"T\"[\"E\"]) if \"x\" does not appear free in \"E\".\n\nThe first two rules are also simple: Variables convert to themselves,\nand applications, which are allowed in combinatory terms, are\nconverted to combinators simply by converting the applicand and the\nargument to combinators.\n\nIt is rules 5 and 6 that are of interest. Rule 5 simply says that to convert a complex abstraction to a combinator, we must first convert its body to a combinator, and then eliminate the abstraction. Rule 6 actually eliminates the abstraction.\n\n\"λx\".(\"E\"₁ \"E\"₂) is a function which takes an argument, say \"a\", and\nsubstitutes it into the lambda term (\"E\"₁ \"E\"₂) in place of \"x\",\nyielding (\"E\"₁ \"E\"₂)[\"x\" : = \"a\"]. But substituting \"a\" into (\"E\"₁ \"E\"₂) in place of \"x\" is just the same as substituting it into both \"E\"₁ and \"E\"₂, so\n\nBy extensional equality,\n\nTherefore, to find a combinator equivalent to \"λx\".(\"E\"₁ \"E\"₂), it is\nsufficient to find a combinator equivalent to (S \"λx\".\"E\"₁ \"λx\".\"E\"₂), and\n\nevidently fits the bill. \"E\"₁ and \"E\"₂ each contain strictly fewer\napplications than (\"E\"₁ \"E\"₂), so the recursion must terminate in a lambda\nterm with no applications at all—either a variable, or a term of the\nform \"λx\".\"E\".\n\nThe combinators generated by the \"T\"[ ] transformation can be made\nsmaller if we take into account the \"η-reduction\" rule:\n\n\"λx\".(\"E\" x) is the function which takes an argument, \"x\", and\napplies the function \"E\" to it; this is extensionally equal to the\nfunction \"E\" itself. It is therefore sufficient to convert \"E\" to\ncombinatorial form.\n\nTaking this simplification into account, the example above becomes:\n\nThis combinator is equivalent to the earlier, longer one:\n\nSimilarly, the original version of the \"T\"[ ] transformation\ntransformed the identity function \"λf\".\"λx\".(\"f\" \"x\") into (S (S (K S) (S (K K) I)) (K I)). With the η-reduction rule, \"λf\".\"λx\".(\"f\" \"x\") is\ntransformed into I.\n\nThere are one-point bases from which every combinator can be composed extensionally equal to \"any\" lambda term. The simplest example of such a basis is {X} where:\n\nIt is not difficult to verify that:\n\nSince {K, S} is a basis, it follows that {X} is a basis too. The Iota programming language uses X as its sole combinator.\n\nAnother simple example of a one-point basis is:\n\nX' does not need η contraction in order to produce K and S. In fact, there exist infinitely many such bases.\n\nIn addition to S and K, Schönfinkel's paper included two combinators which are now called B and C, with the following reductions:\n\nHe also explains how they in turn can be expressed using only S and K:\n\nThese combinators are extremely useful when translating predicate logic or lambda calculus into combinator expressions. They were also used by Curry, and much later by David Turner, whose name has been associated with their computational use. Using them, we can extend the rules for the transformation as follows:\n\n\nUsing B and C combinators, the transformation of\n\"λx\".\"λy\".(\"y\" \"x\") looks like this:\n\nAnd indeed, (C I \"x\" \"y\") does reduce to (\"y\" \"x\"):\n\nThe motivation here is that B and C are limited versions of S.\nWhereas S takes a value and substitutes it into both the applicand and\nits argument before performing the application, C performs the\nsubstitution only in the applicand, and B only in the argument.\n\nThe modern names for the combinators come from Haskell Curry's doctoral thesis of 1930 (see B, C, K, W System). In Schönfinkel's original paper, what we now call S, K, I, B and C were called S, C, I, Z, and T respectively.\n\nThe reduction in combinator size that results from the new transformation rules\ncan also be achieved without introducing B and C, as demonstrated in Section 3.2 of.\nA distinction must be made between the CL as described in this article and the CL calculus. The distinction corresponds to that between the λ and the λ calculus. Unlike the λ calculus, the λ calculus restricts abstractions to:\nAs a consequence, combinator K is not present in the λ calculus nor in the CL calculus. The constants of CL are: I, B, C and S, which form a basis from which all CL terms can be composed (modulo equality). Every λ term can be converted into an equal CL combinator according to rules similar to those presented above for the conversion of λ terms into CL combinators. See chapter 9 in Barendregt (1984).\n\nThe conversion \"L\"[ ] from combinatorial terms to lambda terms is\ntrivial:\n\nNote, however, that this transformation is not the inverse\ntransformation of any of the versions of \"T\"[ ] that we have seen.\n\nA normal form is any combinatory term in which the primitive combinators that occur, if any, are not applied to enough arguments to be simplified. It is undecidable whether a general combinatory term has a normal form; whether two combinatory terms are equivalent, etc. This is equivalent to the undecidability of the corresponding problems for lambda terms. However, a direct proof is as follows:\n\nFirst, observe that the term\n\nhas no normal form, because it reduces to itself after three steps, as\nfollows:\n\nand clearly no other reduction order can make the expression shorter.\n\nNow, suppose N were a combinator for detecting normal forms,\nsuch that\n\nNow let\n\nnow consider the term (S I I \"Z\"). Does (S I I \"Z\") have a normal\nform? It does if and only if the following do also:\n\nNow we need to apply N to (S I I \"Z\").\nEither (S I I \"Z\") has a normal form, or it does not. If it \"does\"\nhave a normal form, then the foregoing reduces as follows:\n\nbut Ω does \"not\" have a normal form, so we have a contradiction. But\nif (S I I \"Z\") does \"not\" have a normal form, the foregoing reduces as\nfollows:\n\nwhich means that the normal form of (S I I \"Z\") is simply I, another\ncontradiction. Therefore, the hypothetical normal-form combinator N\ncannot exist.\n\nThe combinatory logic analogue of Rice's theorem says that there is no complete nontrivial predicate. A \"predicate\" is a combinator that, when applied, returns either T or F. A predicate N is \"nontrivial\" if there are two arguments \"A\" and \"B\" such that N \"A\" = T and N \"B\" = F. A combinator N is \"complete\" if and only if N\"M\" has a normal form for every argument \"M\". The analogue of Rice's theorem then says that every complete predicate is trivial. The proof of this theorem is rather simple.\n\nProof: By reductio ad absurdum. Suppose there is a complete non trivial predicate, say N. Because N is supposed to be non trivial there are combinators \"A\" and \"B\" such that\n\nFixed point theorem gives: ABSURDUM = (NEGATION ABSURDUM), for\n\nBecause N is supposed to be complete either:\n\n\nHence (N ABSURDUM) is neither T nor F, which contradicts the presupposition that N would be a complete non trivial predicate. Q.E.D.\n\nFrom this undecidability theorem it immediately follows that there is no complete predicate that can discriminate between terms that have a normal form and terms that do not have a normal form. It also follows that there is no complete predicate, say EQUAL, such that:\nIf EQUAL would exist, then for all \"A\", \"λx.\"(EQUAL \"x A\") would have to be a complete non trivial predicate.\n\nDavid Turner used his combinators to implement the SASL programming language.\n\nKenneth E. Iverson used primitives based on Curry's combinators in his J programming language, a successor to APL. This enabled what Iverson called tacit programming, that is, programming in functional expressions containing no variables, along with powerful tools for working with such programs. It turns out that tacit programming is possible in any APL-like language with user-defined operators.\n\nThe Curry–Howard isomorphism implies a connection between logic and programming: every proof of a theorem of intuitionistic logic corresponds to a reduction of a typed lambda term, and conversely. Moreover, theorems can be identified with function type signatures. Specifically, a typed combinatory logic corresponds to a Hilbert system in proof theory.\n\nThe K and S combinators correspond to the axioms\nand function application corresponds to the detachment (modus ponens) rule\nThe calculus consisting of AK, AS, and MP is complete for the implicational fragment of the intuitionistic logic, which can be seen as follows. Consider the set \"W\" of all deductively closed sets of formulas, ordered by inclusion. Then formula_6 is an intuitionistic Kripke frame, and we define a model formula_7 in this frame by\nThis definition obeys the conditions on satisfaction of →: on one hand, if formula_9, and formula_10 is such that formula_11 and formula_12, then formula_13 by modus ponens. On the other hand, if formula_14, then formula_15 by the deduction theorem, thus the deductive closure of formula_16 is an element formula_10 such that formula_11, formula_12, and formula_20.\n\nLet \"A\" be any formula which is not provable in the calculus. Then \"A\" does not belong to the deductive closure \"X\" of the empty set, thus formula_21, and \"A\" is not intuitionistically valid.\n\n\n\n"}
{"id": "48037177", "url": "https://en.wikipedia.org/wiki?curid=48037177", "title": "Communications in Algebra", "text": "Communications in Algebra\n\nCommunications in Algebra is a monthly peer-reviewed scientific journal covering algebra, including commutative algebra, ring theory, module theory, non-associative algebra (including Lie algebras and Jordan algebras), group theory, and algebraic geometry. It was established in 1974 and is published by Taylor & Francis. The editor-in-chief is Lance W. Small (University of California, San Diego). Earl J. Taft (Rutgers University) was the founding editor.\n\nThe journal is abstracted and indexed in CompuMath Citation Index, Current Contents/Chemical, Earth, and Physical Sciences, Mathematical Reviews, MathSciNet, Science Citation Index Expanded (SCIE), and Zentralblatt MATH. According to the \"Journal Citation Reports\", the journal has a 2014 impact factor of 0.388, ranking it 253th out of 310 journals in the category \"Mathematics\".\n"}
{"id": "46903016", "url": "https://en.wikipedia.org/wiki?curid=46903016", "title": "Competitive regret", "text": "Competitive regret\n\nIn decision theory, competitive regret is the relative regret compared to an oracle with limited or unlimited power in the process of distribution estimation.\n\nConsider estimating a discrete probability distribution formula_1 on a discrete set formula_2 based on data formula_3, the regret of an estimator formula_4 is defined as\n\nwhere formula_6 is the set of all possible probability distribution, and\n\nwhere formula_8 is the Kullback–Leibler divergence between formula_1 and formula_4.\n\nThe oracle is restricted to have access to partial information of the true distribution formula_1 by knowing the location of formula_1 in the parameter space up to a partition. Given a partition formula_13 of the parameter space, and suppose the oracle knows the subset formula_14 where the true formula_15. The oracle will have regret as\n\nThe competitive regret to the oracle will be\n\nThe oracle knows exactly formula_1, but can only choose the estimator among natural estimators.A natural estimator assigns equal probability to the symbols which appear the same number of time in the sample. The regret of the oracle is\n\nand the competitive regret is\n\nFor the estimator formula_4 proposed in Acharya et al.(2013),\n\nHere formula_23 denotes the k-dimensional unit simplex surface. The partition formula_24 denotes the permutation class on formula_23, where formula_1 and formula_27 are partitioned into the same subset if and only if formula_27 is a permutation of formula_1.\n"}
{"id": "37438", "url": "https://en.wikipedia.org/wiki?curid=37438", "title": "Complex system", "text": "Complex system\n\nA complex system is a system composed of many components which may interact with each other. Examples of complex systems are Earth's global climate, organisms, the human brain, infrastructure such as power grid, transportation or communication systems, social and economic organizations (like cities), an ecosystem, a living cell, and ultimately the entire universe.\n\nComplex systems are systems whose behavior is intrinsically difficult to model due to the dependencies, competitions, relationships, or other types of interactions between their parts or between a given system and its environment. Systems that are \"complex\" have distinct properties that arise from these relationships, such as nonlinearity, emergence, spontaneous order, adaptation, and feedback loops, among others. Because such systems appear in a wide variety of fields, the commonalities among them have become the topic of their own independent area of research. In many cases it is useful to represent such a system as a network where the nodes represent the components and the links their interactions.\n\nThe term \"complex systems\" often refers to the study of complex systems, which is an approach to science that investigates how relationships between a system's parts give rise to its collective behaviors and how the system interacts and forms relationships with its environment. The study of complex systems regards collective, or system-wide, behaviors as the fundamental object of study; for this reason, complex systems can be understood as an alternative paradigm to reductionism, which attempts to explain systems in terms of their constituent parts and the individual interactions between them.\n\nAs an interdisciplinary domain, complex systems draws contributions from many different fields, such as the study of self-organization from physics, that of spontaneous order from the social sciences, chaos from mathematics, adaptation from biology, and many others. \"Complex systems\" is therefore often used as a broad term encompassing a research approach to problems in many diverse disciplines, including statistical physics, information theory, nonlinear dynamics, anthropology, computer science, meteorology, sociology, economics, psychology, and biology.\n\nComplex systems is chiefly concerned with the behaviors and properties of \"systems\". A system, broadly defined, is a set of entities that, through their interactions, relationships, or dependencies, form a unified whole. It is always defined in terms of its \"boundary\", which determines the entities that are or are not part of the system. Entities lying outside the system then become part of the system's \"environment\".\n\nA system can exhibit \"properties\" that produce \"behaviors\" which are distinct from the properties and behaviors of its parts; these system-wide or \"global\" properties and behaviors are characteristics of how the system interacts with or appears to its environment, or of how its parts behave (say, in response to external stimuli) by virtue of being within the system. The notion of \"behavior\" implies that the study of systems is also concerned with processes that take place over time (or, in mathematics, some other phase space parameterization). Because of their broad, interdisciplinary applicability, systems concepts play a central role in complex systems.\n\nAs a field of study, complex systems is a subset of systems theory. General systems theory focuses similarly on the collective behaviors of interacting entities, but it studies a much broader class of systems, including non-complex systems where traditional reductionist approaches may remain viable. Indeed, systems theory seeks to explore and describe \"all\" classes of systems, and the invention of categories that are useful to researchers across widely varying fields is one of systems theory's main objectives.\n\nAs it relates to complex systems, systems theory contributes an emphasis on the way relationships and dependencies between a system's parts can determine system-wide properties. It also contributes the interdisciplinary perspective of the study of complex systems: the notion that shared properties link systems across disciplines, justifying the pursuit of modeling approaches applicable to complex systems wherever they appear. Specific concepts important to complex systems, such as emergence, feedback loops, and adaptation, also originate in systems theory.\n\nSystems exhibit complexity means that their behaviors cannot be easily implied from the very properties that make them difficult to model, and the complex behaviors are governed entirely, or almost entirely, by the behaviors those properties produce. Any modeling approach that ignores such difficulties or characterizes them as noise, then, will necessarily produce models that are neither accurate nor useful. As yet no fully general theory of complex systems has emerged for addressing these problems, so researchers must solve them in domain-specific contexts. Researchers in complex systems address these problems by viewing the chief task of modeling to be capturing, rather than reducing, the complexity of their respective systems of interest.\n\nWhile no generally accepted exact definition of complexity exists yet, there are many archetypal examples of complexity. Systems can be complex if, for instance, they have chaotic behavior (behavior that exhibits extreme sensitivity to initial conditions), or if they have emergent properties (properties that are not apparent from their components in isolation but which result from the relationships and dependencies they form when placed together in a system), or if they are computationally intractable to model (if they depend on a number of parameters that grows too rapidly with respect to the size of the system).\n\nThe interacting components of a complex system form a network, which is a collection of discrete objects and relationships between them, usually depicted as a graph of vertices connected by edges. Networks can describe the relationships between individuals within an organization, between logic gates in a circuit, between genes in gene regulatory networks, or between any other set of related entities.\n\nNetworks often describe the sources of complexity in complex systems. Studying complex systems as networks therefore enables many useful applications of graph theory and network science. Some complex systems, for example, are also complex networks, which have properties such as phase transitions and power-law degree distributions that readily lend themselves to emergent or chaotic behavior. The fact that the number of edges in a complete graph grows quadratically in the number of vertices sheds additional light on the source of complexity in large networks: as a network grows, the number of relationships between entities quickly dwarfs the number of entities in the network.\n\nComplex systems often have nonlinear behavior, meaning they may respond in different ways to the same input depending on their state or context. In mathematics and physics, nonlinearity describes systems in which a change in the size of the input does not produce a proportional change in the size of the output. For a given change in input, such systems may yield significantly greater than or less than proportional changes in output, or even no output at all, depending on the current state of the system or its parameter values.\n\nOf particular interest to complex systems are nonlinear dynamical systems, which are systems of differential equations that have one or more nonlinear terms. Some nonlinear dynamical systems, such as the Lorenz system, can produce a mathematical phenomenon known as chaos. Chaos as it applies to complex systems refers to the sensitive dependence on initial conditions, or \"butterfly effect,\" that a complex system can exhibit. In such a system, small changes to initial conditions can lead to dramatically different outcomes. Chaotic behavior can therefore be extremely hard to model numerically, because small rounding errors at an intermediate stage of computation can cause the model to generate completely inaccurate output. Furthermore, if a complex system returns to a state similar to one it held previously, it may behave completely differently in response to exactly the same stimuli, so chaos also poses challenges for extrapolating from past experience.\n\nAnother common feature of complex systems is the presence of emergent behaviors and properties: these are traits of a system which are not apparent from its components in isolation but which result from the interactions, dependencies, or relationships they form when placed together in a system. Emergence broadly describes the appearance of such behaviors and properties, and has applications to systems studied in both the social and physical sciences. While emergence is often used to refer only to the appearance of unplanned organized behavior in a complex system, emergence can also refer to the breakdown of organization; it describes any phenomena which are difficult or even impossible to predict from the smaller entities that make up the system.\n\nOne example of complex system whose emergent properties have been studied extensively is cellular automata. In a cellular automaton, a grid of cells, each having one of finitely many states, evolves over time according to a simple set of rules. These rules guide the \"interactions\" of each cell with its neighbors. Although the rules are only defined locally, they have been shown capable of producing globally interesting behavior, for example in Conway's Game of Life.\n\nWhen emergence describes the appearance of unplanned order, it is spontaneous order (in the social sciences) or self-organization (in physical sciences). Spontaneous order can be seen in herd behavior, whereby a group of individuals coordinates their actions without centralized planning. Self-organization can be seen in the global symmetry of certain crystals, for instance the apparent radial symmetry of snowflakes, which arises from purely local attractive and repulsive forces both between water molecules and between water molecules and their surrounding environment.\n\nComplex adaptive systems are special cases of complex systems that are adaptive in that they have the capacity to change and learn from experience. Examples of complex adaptive systems include the stock market, social insect and ant colonies, the biosphere and the ecosystem, the brain and the immune system, the cell and the developing embryo, the cities, manufacturing businesses and any human social group-based endeavor in a cultural and social system such as political parties or communities.\n\nComplex systems may have the following features:\n\n\n\n\n\n\n\n\n\nAlthough it is arguable that humans have been studying complex systems for thousands of years, the modern scientific study of complex systems is relatively young in comparison to established fields of science such as physics and chemistry. The history of the scientific study of these systems follows several different research trends.\n\nIn the area of mathematics, arguably the largest contribution to the study of complex systems was the discovery of chaos in deterministic systems, a feature of certain dynamical systems that is strongly related to nonlinearity. The study of neural networks was also integral in advancing the mathematics needed to study complex systems.\n\nThe notion of self-organizing systems is tied with work in nonequilibrium thermodynamics, including that pioneered by chemist and Nobel laureate Ilya Prigogine in his study of dissipative structures. Even older is the work by Hartree-Fock c.s. on the quantum-chemistry equations and later calculations of the structure of molecules which can be regarded as one of the earliest examples of emergence and emergent wholes in science.\n\nOne complex system containing humans is the classical political economy of the Scottish Enlightenment, later developed by the Austrian school of economics, which argues that order in market systems is spontaneous (or emergent) in that it is the result of human action, but not the execution of any human design.\n\nUpon this the Austrian school developed from the 19th to the early 20th century the economic calculation problem, along with the concept of dispersed knowledge, which were to fuel debates against the then-dominant Keynesian economics. This debate would notably lead economists, politicians and other parties to explore the question of computational complexity.\n\nA pioneer in the field, and inspired by Karl Popper's and Warren Weaver's works, Nobel prize economist and philosopher Friedrich Hayek dedicated much of his work, from early to the late 20th century, to the study of complex phenomena, not constraining his work to human economies but venturing into other fields such as psychology, biology and cybernetics. Gregory Bateson played a key role in establishing the connection between anthropology and systems theory; he recognized that the interactive parts of cultures function much like ecosystems.\n\nWhile the explicit study of complex systems dates at least to the 1970s, the first research institute focused on complex systems, the Santa Fe Institute, was founded in 1984. Early Santa Fe Institute participants included physics Nobel laureates Murray Gell-Mann and Philip Anderson, economics Nobel laureate Kenneth Arrow, and Manhattan Project scientists George Cowan and Herb Anderson. Today, there are over 50 institutes and research centers focusing on complex systems. A scientific society called Complex Systems Society organizes every year a general conference on these topics.\n\nThe traditional approach to dealing with complexity is to reduce or constrain it. Typically, this involves compartmentalisation: dividing a large system into separate parts. Organizations, for instance, divide their work into departments that each deal with separate issues. Engineering systems are often designed using modular components. However, modular designs become susceptible to failure when issues arise that bridge the divisions.\n\nAs projects and acquisitions become increasingly complex, companies and governments are challenged to find effective ways to manage mega-acquisitions such as the Army Future Combat Systems. Acquisitions such as the FCS rely on a web of interrelated parts which interact unpredictably. As acquisitions become more network-centric and complex, businesses will be forced to find ways to manage complexity while governments will be challenged to provide effective governance to ensure flexibility and resiliency.\n\nOver the last decades, within the emerging field of complexity economics new predictive tools have been developed to explain economic growth. Such is the case with the models built by the Santa Fe Institute in 1989 and the more recent economic complexity index (ECI), introduced by the MIT physicist Cesar A. Hidalgo and the Harvard economist Ricardo Hausmann. Based on the ECI, Hausmann, Hidalgo and their team of The Observatory of Economic Complexity have produced GDP forecasts for the year 2020.\n\nFocusing on issues of student persistence with their studies, Forsman, Moll and Linder explore the \"viability of using complexity science as a frame to extend methodological applications for physics education research\", finding that \"framing a social network analysis within a complexity science perspective offers a new and powerful applicability across a broad range of PER topics\".\n\nOne of Friedrich Hayek's main contributions to early complexity theory is his distinction between the human capacity to predict the behaviour of simple systems and its capacity to predict the behaviour of complex systems through modeling. He believed that economics and the sciences of complex phenomena in general, which in his view included biology, psychology, and so on, could not be modeled after the sciences that deal with essentially simple phenomena like physics. Hayek would notably explain that complex phenomena, through modeling, can only allow pattern predictions, compared with the precise predictions that can be made out of non-complex phenomena.\n\nComplexity theory is rooted in chaos theory, which in turn has its origins more than a century ago in the work of the French mathematician Henri Poincaré. Chaos is sometimes viewed as extremely complicated information, rather than as an absence of order. Chaotic systems remain deterministic, though their long-term behavior can be difficult to predict with any accuracy. With perfect knowledge of the initial conditions and of the relevant equations describing the chaotic system's behavior, one can theoretically make perfectly accurate predictions about the future of the system, though in practice this is impossible to do with arbitrary accuracy. Ilya Prigogine argued that complexity is non-deterministic, and gives no way whatsoever to precisely predict the future.\n\nThe emergence of complexity theory shows a domain between deterministic order and randomness which is complex. This is referred as the \"edge of chaos\".\n\nWhen one analyzes complex systems, sensitivity to initial conditions, for example, is not an issue as important as it is within chaos theory, in which it prevails. As stated by Colander, the study of complexity is the opposite of the study of chaos. Complexity is about how a huge number of extremely complicated and dynamic sets of relationships can generate some simple behavioral patterns, whereas chaotic behavior, in the sense of deterministic chaos, is the result of a relatively small number of non-linear interactions.\n\nTherefore, the main difference between chaotic systems and complex systems is their history. Chaotic systems do not rely on their history as complex ones do. Chaotic behaviour pushes a system in equilibrium into chaotic order, which means, in other words, out of what we traditionally define as 'order'. On the other hand, complex systems evolve far from equilibrium at the edge of chaos. They evolve at a critical state built up by a history of irreversible and unexpected events, which physicist Murray Gell-Mann called \"an accumulation of frozen accidents\". In a sense chaotic systems can be regarded as a subset of complex systems distinguished precisely by this absence of historical dependence. Many real complex systems are, in practice and over long but finite time periods, robust. However, they do possess the potential for radical qualitative change of kind whilst retaining systemic integrity. Metamorphosis serves as perhaps more than a metaphor for such transformations.\n\nA complex system is usually composed of many components and their interactions. Such a system can be represented by a network where nodes represent the components and links represent their interactions.\nOther examples are social networks, airline networks, biological networks and climate networks.\nNetworks can also fail and recover spontaneously. For modeling this phenomenon see Majdandzik et al.\nInteracting complex systems can be modeled as networks of networks. For their breakdown and recovery properties see Gao et al.\n\nThe computational law of reachable optimality is established as a general form of computation for ordered systems and it reveals complexity computation is a compound computation of optimal choice and optimality driven reaching pattern over time underlying a specific and any experience path of ordered system within the general limitation of system integrity.\n\nThe computational law of reachable optimality has four key components as described below.\n\n1. Reachability of Optimality: Any intended optimality shall be reachable. Unreachable optimality has no meaning for a member in the ordered system and even for the ordered system itself.\n\n2. Prevailing and Consistency: Maximizing reachability to explore best available optimality is the prevailing computation logic for all members in the ordered system and is accommodated by the ordered system.\n\n3. Conditionality: Realizable tradeoff between reachability and optimality depends primarily upon the initial bet capacity and how the bet capacity evolves along with the payoff table update path triggered by bet behavior and empowered by the underlying law of reward and punishment. Precisely, it is a sequence of conditional events where the next event happens upon reached status quo from experience path.\n\n4. Robustness: The more challenge a reachable optimality can accommodate, the more robust it is in term of path integrity.\n\nThere are also four computation features in the law of reachable optimality.\n\n1. Optimal Choice: Computation in realizing Optimal Choice can be very simple or very complex. A simple rule in Optimal Choice is to accept whatever is reached, Reward As You Go (RAYG). A Reachable Optimality computation reduces into optimizing reachability when RAYG is adopted. The Optimal Choice computation can be more complex when multiple NE strategies present in a reached game.\n\n2. Initial Status: Computation is assumed to start at an interested beginning even the absolute beginning of an ordered system in nature may not and need not present. An assumed neutral Initial Status facilitates an artificial or a simulating computation and is not expected to change the prevalence of any findings.\n\n3. Territory: An ordered system shall have a territory where the universal computation sponsored by the system will produce an optimal solution still within the territory.\n\n4. Reaching Pattern: The forms of Reaching Pattern in the computation space, or the Optimality Driven Reaching Pattern in the computation space, primarily depend upon the nature and dimensions of measure space underlying a computation space and the law of punishment and reward underlying the realized experience path of reaching. There are five basic forms of experience path we are interested in, persistently positive reinforcement experience path, persistently negative reinforcement experience path, mixed persistent pattern experience path, decaying scale experience path and selection experience path.\n\nThe compound computation in selection experience path includes current and lagging interaction, dynamic topological transformation and implies both invariance and variance characteristics in an ordered system's experience path.\n\nIn addition, the computation law of reachable optimality gives out the boundary between complexity model, chaotic model and determination model. When RAYG is the Optimal Choice computation, and the reaching pattern is a persistently positive experience path, persistently negative experience path, or mixed persistent pattern experience path, the underlying computation shall be a simple system computation adopting determination rules. If the reaching pattern has no persistent pattern experienced in RAYG regime, the underlying computation hints there is a chaotic system. When the optimal choice computation involves non-RAYG computation, it's a complexity computation driving the compound effect.\n\n\n\n"}
{"id": "616985", "url": "https://en.wikipedia.org/wiki?curid=616985", "title": "Computability logic", "text": "Computability logic\n\nComputability logic (CoL) is a research program and mathematical framework for redeveloping logic as a systematic formal theory of computability, as opposed to classical logic which is a formal theory of truth. It was introduced and so named by Giorgi Japaridze in 2003. \n\nComparing CoL with classical logic, while formulas in the latter represent true/false statements, in CoL they represent computational problems. In classical logic, validity of a formula is understood as being always true, i.e., true regardless of the interpretation of its non-logical primitives (atoms), based solely on form rather than meaning. Similarly, in CoL validity means being always computable. More generally, classical logic tells us when the truth of a given statement always follows from the truth of a given set of other statements. Similarly, CoL tells us when the computability of a given problem \"A\" always follows from the computability of other given problems \"B,…,B\". Moreover, it provides a uniform way to actually construct a solution (algorithm) for such an \"A\" from any known solutions of \"B,…,B\".\n\nCoL understands computational problems in their most general – interactive sense. They are formalized as games played by a machine against its environment, and computability means existence of a machine that wins the game against any possible behavior by the environment. Defining what such game-playing machines mean, CoL provides a generalization of the Church-Turing thesis to the interactive level. The classical concept of truth turns out to be a special, zero-interactivity-degree case of computability. This makes classical logic a special fragment of CoL. Being a conservative extension of the former, computability logic is, at the same time, by an order of magnitude more expressive, constructive and computationally meaningful. Besides classical logic, independence-friendly (IF) logic and certain proper extensions of linear logic and intuitionistic logic also turn out to be natural fragments of CoL. Hence meaningful concepts of \"intuitionistic truth\", \"linear-logic truth\" and \"IF-logic truth\" can be derived from the semantics of CoL.\n\nProviding a systematic answer to the fundamental question of what can be computed and how, CoL claims a wide range of potential application areas. Those include constructive applied theories, knowledge base systems, systems for planning and action. Out of these, only applications in constructive applied theories have been extensively explored so far: a series of CoL-based number theories, termed \"clarithmetics\", have been constructed as computationally and complexity-theoretically meaningful alternatives to the classical-logic-based Peano arithmetic and its variations such as systems of bounded arithmetic.\n\nThe traditional proof systems such as natural deduction or sequent calculus turn out to be insufficient for axiomatizing any more or less nontrivial fragments of CoL. This has necessitated developing alternative, more general and flexible methods of proof, such as cirquent calculus.\n\n The full language of CoL is an extension of the language of classical first-order logic. Its logical vocabulary has several sorts of conjunctions, disjunctions, quantifiers, implications, negations and so called recurrence operators. This collection includes all connectives and quantifiers of classical logic. The language also has two sorts of nonlogical atoms: \"elementary\" and \"general\". Elementary atoms, which are nothing but the atoms of classical logic, represent \"elementary problems\", i.e., games with no moves that are automatically won by the machine when true and lost when false. General atoms, on the other hand, can be interpreted as any games, elementary or non-elementary. Both semantically and syntactically, classical logic is nothing but the fragment of CoL obtained by forbidding general atoms in its language, and forbidding all operators other than ¬, ∧, ∨, →, ∀, ∃.\n\nJaparidze has repeatedly pointed out that the language of CoL is open-ended, and may undergo further extensions. Due to the expressiveness of this language, advances in CoL, such as constructing axiomatizations or building CoL-based applied theories, have usually been limited to one or another proper fragment of the language.\n\nThe games underlying the semantics of CoL are called \"static games\". Such games impose no regulations on which player can or should move in a given situation, and it is generally up to the player to decide whether to move or wait for moves by its adversary. It is always a possibility that the adversary moves while a given player is \"thinking\" on its next step. However, static games are defined in such a way that it never hurts a player to delay its own moves, so such games never become contests of speed. All elementary games are automatically static, and so are the games allowed to be interpretations of general atoms. There are two players in static games: the \"machine\" and the \"environment\". The machine can only follow algorithmic strategies, while there are no restrictions on the behavior of the environment. Each run (play) is won by one of these players and lost by the other.\n\nThe logical operators of CoL are understood as operations on games. Here we informally survey some of those operations. For simplicity we assume that the domain of discourse is always {0,1,2,…}. \n\nThe operation ¬ of \"negation\" (\"not\") switches the roles of the two players, turning moves and wins by the machine into those by the environment, and vice versa. For instance, if \"Chess\" is the game of chess (but with ties ruled out) from the white player’s perspective, then ¬\"Chess\" is the same game from the black player’s perspective. \n\nThe \"parallel conjunction\" ∧ (\"pand\") and \"parallel disjunction\" ∨ (\"por\") combine games in a parallel fashion. A run of \"A\"∧\"B\" or \"A\"∨\"B\" is a simultaneous play in the two conjuncts. The machine wins \"A\"∧\"B\" (resp. \"A\"∨\"B\") if it wins both (resp. at least one) of those. So, for instance, \"Chess\"∨¬\"Chess\" is a game on two boards, one played white and one black, and where the task of the machine is to win on at least one board. Such a game can be easily won regardless who the adversary is, by copying his moves from one board to the other. The \"parallel implication\" operator → (\"pimplication\") is defined by \"A\"→\"B\" = ¬\"A\"∨\"B\". The intuitive meaning of this operation is \"reducing\" \"B\" to \"A\", i.e., solving \"A\" as long as the adversary solves \"B\". The \"parallel quantifiers\" ∧ (\"pall\") and ∨ (\"pexists\") can be defined by ∧\"xA\"(\"x\") = \"A\"(0)∧\"A\"(1)∧\"A\"(2)∧... and ∨\"xA\"(\"x\") = \"A\"(0)∨\"A\"(1)∨\"A\"(2)∨... These are thus simultaneous plays of \"A\"(0),\"A\"(1),\"A\"(2),…, each on a separate board. The machine wins ∧\"xA\"(\"x\") (resp. ∨\"xA\"(\"x\")) if it wins all (resp. some) of these games. The \"blind quantifiers\" ∀ (\"blall\") and ∃ (\"blexists\"), on the other hand, generate single-board games. A run of ∀\"xA\"(\"x\") or ∃\"xA\"(\"x\") is a single run of \"A\". The machine wins ∀\"xA\"(\"x\") (resp. ∃\"xA\"(\"x\")) if such a run is a won run of \"A\"(\"x\") for all (resp. at least one) possible values of \"x\". \n\nAll of the operators characterized so far behave exactly like their classical counterparts when they are applied to elementary (moveless) games, and validate the same principles. This is why CoL uses the same symbols for those operators as classical logic does. When such operators are applied to non-elementary games, however, their behavior is no longer classical. So, for instance, if \"p\" is an elementary atom and \"P\" a general atom, \"p\"→\"p\"∧\"p\" is valid while \"P\"→\"P\"∧\"P\" is not. The principle of the excluded middle \"P\"∨¬\"P\", however, remains valid. The same principle is invalid with all three other sorts (choice, sequential and toggling) of disjunction. The \"choice disjunction\" ⊔ (\"chor\") of games \"A\" and \"B\", written \"A\"⊔\"B\", is a game where, in order to win, the machine has to choose one of the two disjuncts and then win in the chosen component. The \"sequential disjunction\" (\"sor\") \"A\"\"B\" starts as \"A\"; it also ends as \"A\" unless the machine makes a \"switch\" move, in which case \"A\" is abandoned and the game restarts and continues as \"B\". In the \"toggling disjunction\" (\"tor\") \"A\"⩛\"B\", the machine may switch between \"A\" and \"B\" any finite number of times. Each disjunction operator has its dual conjunction, obtained by interchanging the roles of the two players. The corresponding quantifiers can further be defined as infinite conjunctions or disjunctions in the same way as in the case of the parallel quantifiers. Each sort disjunction also induces a corresponding implication operation the same way as this was the case with the parallel implication →. For instance, the \"choice implication\" (\"chimplication\") \"A\"⊐\"B\" is defined as ¬\"A\"⊔\"B\". \n\nThen comes the \"recurrence\" group of operators. The \"parallel recurrence\" (\"precurrence\") of \"A\" can be defined as the infinite parallel conjunction \"A\"∧A∧A∧... The sequential (\"srecurrence\") and toggling (\"trecurrence\") sorts of recurrences can be defined similarly. The corresponding \"corecurrence\" operators, on the other hand, can be defined as infinite disjunctions instead. \"Branching recurrence\" (\"brecurrence\") ⫰, which is the strongest sort of recurrence, does not have a corresponding conjunction. ⫰\"A\" is a game that starts and proceeds as \"A\". At any time, however, the environment is allowed to make a \"replicative\" move, which creates two copies of the then-current position of \"A\", thus splitting the play into two parallel threads with a common past but possibly different future developments. In the same fashion, the environment can further replicate any of positions of any thread, thus creating more and more threads of \"A\". Those threads are played in parallel, and the machine needs to win \"A\" in all threads to be the winner in ⫰\"A\". \"Branching corecurrence\" (\"cobrecurrence\") ⫯ is defined symmetrically by interchanging \"machine\" and \"environment\". \n\nEach sort of recurrence further induces a corresponding weak version of implication and weak version of negation. The former is said to be a \"rimplication\", and the latter a \"refutation\". The \"branching rimplication\" (\"brimplication\") \"A\"⟜\"B\" is nothing but ⫰\"A\"→\"B\", and the \"branching refutation\" (\"brefutation\") of \"A\" is \"A\"⟜⊥, where ⊥ is the always-lost elementary game. Similarly for all other sorts of rimplication and refutation.\n\nThe language of CoL offers a systematic way to specify an infinite variety of computational problems, with or without names established in the literature. Below are some examples. \n\nLet \"f\" be a unary function. The problem of computing \"f\" will be written as ⊓x⊔y(\"y\"=\"f\"(\"x\")). According to the semantics of CoL, this is a game where the first move (\"input\") is by the environment, which should choose a value \"m\" for \"x\". Intuitively, this amounts to asking the machine to tell the value of \"f\"(\"m\"). The game continues as ⊔y(\"y\"=\"f\"(\"m\")). Now the machine is expected to make a move (\"output\"), which should be choosing a value \"n\" for \"y\". This amounts to saying that \"n\" is the value of \"f\"(m). The game is now brought down to the elementary \"n\"=\"f\"(\"m\"), which is won by the machine if and only if \"n\" is indeed the value of \"f\"(\"m\"). \n\nLet \"p\" be a unary predicate. Then ⊓\"x\"(\"p\"(\"x\")⊔¬\"p\"(\"x\")) expresses the problem of deciding \"p\", ⊓\"x\"(\"p\"(\"x\")&¬\"p\"(\"x\")) expresses the problem of semidieciding \"p\", and ⊓\"x\"(\"p\"(\"x\")⩛¬\"p\"(\"x\")) the problem of recursively approximating \"p\". \n\nLet \"p\" and \"q\" be two unary predicates. Then ⊓\"x\"(\"p\"(\"x\")⊔¬\"p\"(\"x\"))⟜⊓\"x\"(\"q\"(\"x\")⊔¬\"q\"(\"x\")) expresses the problem of Turing-reducing \"q\" to \"p\" (in the sense that \"q\" is Turing reducible to \"p\" if and only if the interactive problem ⊓\"x\"(\"p\"(\"x\")⊔¬\"p\"(\"x\"))⟜⊓\"x\"(\"q\"(\"x\")⊔¬\"q\"(\"x\")) is computable). ⊓\"x\"(\"p\"(\"x\")⊔¬\"p\"(\"x\"))→⊓\"x\"(\"q\"(\"x\")⊔¬\"q\"(\"x\")) does the same but for the stronger version of Turing reduction where the oracle for \"p\" can be queried only once. ⊓x⊔\"y\"(\"q\"(\"x\")↔\"p\"(\"y\")) does the same for the problem of many-one reducing \"q\" to \"p\". With more complex expressions one can capture all kinds of nameless yet potentially meaningful relations and operations on computational problems, such as, for instance, \"Turing-reducing the problem of semideciding \"r\" to the problem of many-one reducing \"q\" to \"p\"\". Imposing time or space restrictions on the work of the machine, one further gets complexity-theoretic counterparts of such relations and operations.\n\nThe known deductive systems for various fragments of CoL share the property that a solution (algorithm) can be automatically extracted from a proof of a problem in the system. This property is further inherited by all applied theories based on those systems. So, in order to find a solution for a given problem, it is sufficient to express it in the language of CoL and then find a proof of that expression. Another way to look at this phenomenon is to think of a formula \"G\" of CoL as program specification (goal). Then a proof of \"G\" is – more precisely, translates into – a program meeting that specification. There is no need to verify that the specification is met, because the proof itself is, in fact, such a verification. \n\nExamples of CoL-based applied theories are the so-called \"clarithmetics\". These are number theories based on CoL in the same sense as Peano arithmetic PA is based on classical logic. Such a system is usually a conservative extension of PA. It typically includes all Peano axioms, and adds to them one or two extra-Peano axioms such as ⊓\"x\"⊔\"y\"(\"y\"=\"x\"') expressing the computability of the successor function. Typically it also has one or two non-logical rules of inference, such as constructive versions of induction or comprehension. Through routine variations in such rules one can obtain sound and complete systems characterizing one or another interactive computational complexity class \"C\". This is in the sense that a problem belongs to \"C\" if and only if it has a proof in the theory. So, such a theory can be used for finding not merely algorithmic solutions, but also efficient ones on demand, such as solutions that run in polynomial time or logarithmic space. It should be pointed out that all claritmetical theories share the same logical postulates, and only their non-logical postulates vary depending on the target complexity class. Their notable distinguishing feature from other approaches with similar aspirations (such as bounded arithmetic) is that they extend rather than weaken PA, preserving the full deductive power and convenience of the latter.\n\n\n\n"}
{"id": "55589991", "url": "https://en.wikipedia.org/wiki?curid=55589991", "title": "Cora Barbara Hennel", "text": "Cora Barbara Hennel\n\nCora Barbara Hennel (January 21, 1886 – June 26, 1947) was an Indiana mathematician active in the first half of the 20th century.\n\nHennel was born in Evansville, Indiana to Joseph H. and Anna Marie Thuman Hennel. After high school graduation Cora and her older sister Cecilia taught in country grade schools to save money for college. In 1903, both Hennels entered Indiana University and shortly thereafter, convinced their parents to move with their younger sister, Edith, to Bloomington. All three sisters attended and graduated from Indiana University. Hennel earned her earned her A.B. in Mathematics in 1907, her Masters in 1908, and in 1912, became the first person to earn a Ph.D. in Mathematics from Indiana University. As an undergraduate, Cora was the class poet and active in student affairs; in graduate school, she was a founding member and Secretary of the Euclidean Circle, a mathematics club for faculty and students.\n\nAs she worked toward her doctorate, Cora served as Instructor in the Department of Mathematics. She continued in this role after receipt of her degree and in 1916, she was appointed Assistant Professor of Mathematics. She was promoted to Associate Professor in 1923 and became a full Professor in 1936. Hennel spent the entirety of her academic career at Indiana University and was still teaching at the time of her death in 1947.\n\nHennel was an active member of the Indiana University faculty, serving as president of the Bloomington chapter of the American Association of University Professors, the American Association of University Women and served as chair of the Indiana Section of the Mathematical Association of America. Professionally, she was a member of Phi Beta Kappa, Sigma Xi, Pi Lambda Theta, Mortar Board, and the Indiana Academy of Sciences.\n\nIn 1958, Cecilia Hennel established the Cora B. Hennel Memorial Scholarship to honor her sister. The Hennel Scholarships are awarded to students who have demonstrated high ability in mathematics. The department continues to remember Hennel and in 1995, named the faculty/student lounge in the renovated Rawles Hall the Cora B. Hennel Room.\n\n"}
{"id": "87047", "url": "https://en.wikipedia.org/wiki?curid=87047", "title": "Cryptology ePrint Archive", "text": "Cryptology ePrint Archive\n\nThe Cryptology ePrint Archive is an electronic archive (eprint) of new results in the field of cryptography, maintained by the International Association for Cryptologic Research. It contains articles covering many of the most recent advances in cryptography, that did not (yet) undergo any refereeing process.\n\n\n"}
{"id": "31649951", "url": "https://en.wikipedia.org/wiki?curid=31649951", "title": "Cyclohedron", "text": "Cyclohedron\n\nIn geometry, the cyclohedron or Bott–Taubes polytope is a certain (\"n\" − 1)-dimensional polytope that is useful in studying knot invariants.\n\nThe configuration space of \"n\" distinct points on the circle \"S\" is an \"n\"-dimensional manifold, which can be compactified into a manifold with corners by allowing the points to approach each other. This compactification can be factored as formula_1, where \"W\" is the cyclohedron.\n\n"}
{"id": "8302", "url": "https://en.wikipedia.org/wiki?curid=8302", "title": "David Hilbert", "text": "David Hilbert\n\nDavid Hilbert (; ; 23 January 1862 – 14 February 1943) was a German mathematician. He is recognized as one of the most influential and universal mathematicians of the 19th and early 20th centuries. Hilbert discovered and developed a broad range of fundamental ideas in many areas, including invariant theory, calculus of variations, commutative algebra, algebraic number theory, the foundations of geometry, spectral theory of operators and its application to integral equations, mathematical physics, and foundations of mathematics (particularly proof theory).\n\nHilbert adopted and warmly defended Georg Cantor's set theory and transfinite numbers. A famous example of his leadership in mathematics is his 1900 presentation of a collection of problems that set the course for much of the mathematical research of the 20th century.\n\nHilbert and his students contributed significantly to establishing rigor and developed important tools used in modern mathematical physics. Hilbert is known as one of the founders of proof theory and mathematical logic, as well as for being among the first to distinguish between mathematics and metamathematics.\n\nHilbert, the first of two children of Otto and Maria Therese (Erdtmann) Hilbert, was born in the Province of Prussia, Kingdom of Prussia, either in Königsberg (according to Hilbert's own statement) or in Wehlau (known since 1946 as Znamensk) near Königsberg where his father worked at the time of his birth.\n\nIn late 1872, Hilbert entered the Friedrichskolleg Gymnasium (\"Collegium fridericianum\", the same school that Immanuel Kant had attended 140 years before); but, after an unhappy period, he transferred to (late 1879) and graduated from (early 1880) the more science-oriented Wilhelm Gymnasium. Upon graduation, in autumn 1880, Hilbert enrolled at the University of Königsberg, the \"Albertina\". In early 1882, Hermann Minkowski (two years younger than Hilbert and also a native of Königsberg but had gone to Berlin for three semesters), returned to Königsberg and entered the university. Hilbert developed a lifelong friendship with the shy, gifted Minkowski.\n\nIn 1884, Adolf Hurwitz arrived from Göttingen as an Extraordinarius (i.e., an associate professor). An intense and fruitful scientific exchange among the three began, and Minkowski and Hilbert especially would exercise a reciprocal influence over each other at various times in their scientific careers. Hilbert obtained his doctorate in 1885, with a dissertation, written under Ferdinand von Lindemann, titled \"Über invariante Eigenschaften spezieller binärer Formen, insbesondere der Kugelfunktionen\" (\"On the invariant properties of special binary forms, in particular the spherical harmonic functions\").\n\nHilbert remained at the University of Königsberg as a \"Privatdozent\" (senior lecturer) from 1886 to 1895. In 1895, as a result of intervention on his behalf by Felix Klein, he obtained the position of Professor of Mathematics at the University of Göttingen. During the Klein and Hilbert years, Göttingen became the preeminent institution in the mathematical world. He remained there for the rest of his life.\n\nAmong Hilbert's students were Hermann Weyl, chess champion Emanuel Lasker, Ernst Zermelo, and Carl Gustav Hempel. John von Neumann was his assistant. At the University of Göttingen, Hilbert was surrounded by a social circle of some of the most important mathematicians of the 20th century, such as Emmy Noether and Alonzo Church.\n\nAmong his 69 Ph.D. students in Göttingen were many who later became famous mathematicians, including (with date of thesis): Otto Blumenthal (1898), Felix Bernstein (1901), Hermann Weyl (1908), Richard Courant (1910), Erich Hecke (1910), Hugo Steinhaus (1911), and Wilhelm Ackermann (1925). Between 1902 and 1939 Hilbert was editor of the \"Mathematische Annalen\", the leading mathematical journal of the time.\n\nAround 1925, Hilbert developed pernicious anemia, a then-untreatable vitamin deficiency whose primary symptom is exhaustion; his assistant Eugene Wigner described him as subject to \"enormous fatigue\" and how he \"seemed quite old\", and that even after eventually being diagnosed and treated, he \"was hardly a scientist after 1925, and certainly not a Hilbert.\"\n\nHilbert lived to see the Nazis purge many of the prominent faculty members at University of Göttingen in 1933. Those forced out included Hermann Weyl (who had taken Hilbert's chair when he retired in 1930), Emmy Noether and Edmund Landau. One who had to leave Germany, Paul Bernays, had collaborated with Hilbert in mathematical logic, and co-authored with him the important book \"Grundlagen der Mathematik\" (which eventually appeared in two volumes, in 1934 and 1939). This was a sequel to the Hilbert-Ackermann book \"Principles of Mathematical Logic\" from 1928. Hermann Weyl's successor was Helmut Hasse.\n\nAbout a year later, Hilbert attended a banquet and was seated next to the new Minister of Education, Bernhard Rust. Rust asked whether \"the \"Mathematical Institute\" really suffered so much because of the departure of the Jews\". Hilbert replied,\n\"Suffered? It doesn't exist any longer, does it!\"\nBy the time Hilbert died in 1943, the Nazis had nearly completely restaffed the university, as many of the former faculty had either been Jewish or married to Jews. Hilbert's funeral was attended by fewer than a dozen people, only two of whom were fellow academics, among them Arnold Sommerfeld, a theoretical physicist and also a native of Königsberg. News of his death only became known to the wider world six months after he had died.\n\nThe epitaph on his tombstone in Göttingen consists of the famous lines he spoke at the conclusion of his retirement address to the Society of German Scientists and Physicians on 8 September 1930. The words were given in response to the Latin maxim: \"Ignoramus et ignorabimus\" or \"We do not know, we shall not know\":\n\nIn English:\n\nThe day before Hilbert pronounced these phrases at the 1930 annual meeting of the Society of German Scientists and Physicians, Kurt Gödel—in a round table discussion during the Conference on Epistemology held jointly with the Society meetings—tentatively announced the first expression of his incompleteness theorem. Gödel's incompleteness theorems show that even elementary axiomatic systems such as Peano arithmetic are either self-contradicting or contain logical propositions that are impossible to prove or disprove.\n\nIn 1892, Hilbert married Käthe Jerosch (1864–1945), \"the daughter of a Königsberg merchant, an outspoken young lady with an independence of mind that matched his own\". While at Königsberg they had their one child, Franz Hilbert (1893–1969).\n\nHilbert's son Franz suffered throughout his life from an undiagnosed mental illness. His inferior intellect was a terrible disappointment to his father and this misfortune was a matter of distress to the mathematicians and students at Göttingen.\n\nHilbert considered the mathematician Hermann Minkowski to be his \"best and truest friend\".\n\nHilbert was baptized and raised a Calvinist in the Prussian Evangelical Church. He later on left the Church and became an agnostic. He also argued that mathematical truth was independent of the existence of God or other \"a priori\" assumptions.\n\nHilbert's first work on invariant functions led him to the demonstration in 1888 of his famous \"finiteness theorem\". Twenty years earlier, Paul Gordan had demonstrated the theorem of the finiteness of generators for binary forms using a complex computational approach. Attempts to generalize his method to functions with more than two variables failed because of the enormous difficulty of the calculations involved. In order to solve what had become known in some circles as \"Gordan's Problem\", Hilbert realized that it was necessary to take a completely different path. As a result, he demonstrated \"Hilbert's basis theorem\", showing the existence of a finite set of generators, for the invariants of quantics in any number of variables, but in an abstract form. That is, while demonstrating the existence of such a set, it was not a constructive proof — it did not display \"an object\" — but rather, it was an existence proof and relied on use of the law of excluded middle in an infinite extension.\n\nHilbert sent his results to the \"Mathematische Annalen\". Gordan, the house expert on the theory of invariants for the \"Mathematische Annalen\", could not appreciate the revolutionary nature of Hilbert's theorem and rejected the article, criticizing the exposition because it was insufficiently comprehensive. His comment was:\n\nKlein, on the other hand, recognized the importance of the work, and guaranteed that it would be published without any alterations. Encouraged by Klein, Hilbert extended his method in a second article, providing estimations on the maximum degree of the minimum set of generators, and he sent it once more to the \"Annalen\". After having read the manuscript, Klein wrote to him, saying:\n\nLater, after the usefulness of Hilbert's method was universally recognized, Gordan himself would say:\n\nFor all his successes, the nature of his proof stirred up more trouble than Hilbert could have imagined at the time. Although Kronecker had conceded, Hilbert would later respond to others' similar criticisms that \"many different constructions are subsumed under one fundamental idea\" — in other words (to quote Reid): \"Through a proof of existence, Hilbert had been able to obtain a construction\"; \"the proof\" (i.e. the symbols on the page) \"was\" \"the object\". Not all were convinced. While Kronecker would die soon afterwards, his constructivist philosophy would continue with the young Brouwer and his developing intuitionist \"school\", much to Hilbert's torment in his later years. Indeed, Hilbert would lose his \"gifted pupil\" Weyl to intuitionism — \"Hilbert was disturbed by his former student's fascination with the ideas of Brouwer, which aroused in Hilbert the memory of Kronecker\". Brouwer the intuitionist in particular opposed the use of the Law of Excluded Middle over infinite sets (as Hilbert had used it). Hilbert would respond:\n\nThe text \"Grundlagen der Geometrie\" (tr.: \"Foundations of Geometry\") published by Hilbert in 1899 proposes a formal set, called Hilbert's axioms, substituting for the traditional axioms of Euclid. They avoid weaknesses identified in those of Euclid, whose works at the time were still used textbook-fashion. It is difficult to specify the axioms used by Hilbert without referring to the publication history of the \"Grundlagen\" since Hilbert changed and modified them several times. The original monograph was quickly followed by a French translation, in which Hilbert added V.2, the Completeness Axiom. An English translation, authorized by Hilbert, was made by E.J. Townsend and copyrighted in 1902. This translation incorporated the changes made in the French translation and so is considered to be a translation of the 2nd edition. Hilbert continued to make changes in the text and several editions appeared in German. The 7th edition was the last to appear in Hilbert's lifetime. New editions followed the 7th, but the main text was essentially not revised.\nHilbert's approach signaled the shift to the modern axiomatic method. In this, Hilbert was anticipated by Moritz Pasch's work from 1882. Axioms are not taken as self-evident truths. Geometry may treat \"things\", about which we have powerful intuitions, but it is not necessary to assign any explicit meaning to the undefined concepts. The elements, such as point, line, plane, and others, could be substituted, as Hilbert is reported to have said to Schoenflies and Kötter, by tables, chairs, glasses of beer and other such objects. It is their defined relationships that are discussed.\n\nHilbert first enumerates the undefined concepts: point, line, plane, lying on (a relation between points and lines, points and planes, and lines and planes), betweenness, congruence of pairs of points (line segments), and congruence of angles. The axioms unify both the plane geometry and solid geometry of Euclid in a single system.\n\nHilbert put forth a most influential list of 23 unsolved problems at the International Congress of Mathematicians in Paris in 1900. This is generally reckoned as the most successful and deeply considered compilation of open problems ever to be produced by an individual mathematician.\n\nAfter re-working the foundations of classical geometry, Hilbert could have extrapolated to the rest of mathematics. His approach differed, however, from the later 'foundationalist' Russell-Whitehead or 'encyclopedist' Nicolas Bourbaki, and from his contemporary Giuseppe Peano. The mathematical community as a whole could enlist in problems, which he had identified as crucial aspects of the areas of mathematics he took to be key.\n\nThe problem set was launched as a talk \"The Problems of Mathematics\" presented during the course of the Second International Congress of Mathematicians held in Paris. The introduction of the speech that Hilbert gave said:\n\nHe presented fewer than half the problems at the Congress, which were published in the acts of the Congress. In a subsequent publication, he extended the panorama, and arrived at the formulation of the now-canonical 23 Problems of Hilbert. See also Hilbert's twenty-fourth problem. The full text is important, since the exegesis of the questions still can be a matter of inevitable debate, whenever it is asked how many have been solved.\n\nSome of these were solved within a short time. Others have been discussed throughout the 20th century, with a few now taken to be unsuitably open-ended to come to closure. Some even continue to this day to remain a challenge for mathematicians.\n\nIn an account that had become standard by the mid-century, Hilbert's problem set was also a kind of manifesto, that opened the way for the development of the formalist school, one of three major schools of mathematics of the 20th century. According to the formalist, mathematics is manipulation of symbols according to agreed upon formal rules. It is therefore an autonomous activity of thought. There is, however, room to doubt whether Hilbert's own views were simplistically formalist in this sense.\n\nIn 1920 he proposed explicitly a research project (in \"metamathematics\", as it was then termed) that became known as Hilbert's program. He wanted mathematics to be formulated on a solid and complete logical foundation. He believed that in principle this could be done, by showing that:\n\n\nHe seems to have had both technical and philosophical reasons for formulating this proposal. It affirmed his dislike of what had become known as the \"ignorabimus\", still an active issue in his time in German thought, and traced back in that formulation to Emil du Bois-Reymond.\n\nThis program is still recognizable in the most popular philosophy of mathematics, where it is usually called \"formalism\". For example, the Bourbaki group adopted a watered-down and selective version of it as adequate to the requirements of their twin projects of (a) writing encyclopedic foundational works, and (b) supporting the axiomatic method as a research tool. This approach has been successful and influential in relation with Hilbert's work in algebra and functional analysis, but has failed to engage in the same way with his interests in physics and logic.\n\nHilbert wrote in 1919:\n\nHilbert published his views on the foundations of mathematics in the 2-volume work Grundlagen der Mathematik.\n\nHilbert and the mathematicians who worked with him in his enterprise were committed to the project. His attempt to support axiomatized mathematics with definitive principles, which could banish theoretical uncertainties, ended in failure.\n\nGödel demonstrated that any non-contradictory formal system, which was comprehensive enough to include at least arithmetic, cannot demonstrate its completeness by way of its own axioms. In 1931 his incompleteness theorem showed that Hilbert's grand plan was impossible as stated. The second point cannot in any reasonable way be combined with the first point, as long as the axiom system is genuinely finitary.\n\nNevertheless, the subsequent achievements of proof theory at the very least \"clarified\" consistency as it relates to theories of central concern to mathematicians. Hilbert's work had started logic on this course of clarification; the need to understand Gödel's work then led to the development of recursion theory and then mathematical logic as an autonomous discipline in the 1930s. The basis for later theoretical computer science, in the work of Alonzo Church and Alan Turing, also grew directly out of this 'debate'.\n\nAround 1909, Hilbert dedicated himself to the study of differential and integral equations; his work had direct consequences for important parts of modern functional analysis. In order to carry out these studies, Hilbert introduced the concept of an infinite dimensional Euclidean space, later called Hilbert space. His work in this part of analysis provided the basis for important contributions to the mathematics of physics in the next two decades, though from an unanticipated direction.\nLater on, Stefan Banach amplified the concept, defining Banach spaces. Hilbert spaces are an important class of objects in the area of functional analysis, particularly of the spectral theory of self-adjoint linear operators, that grew up around it during the 20th century.\n\nUntil 1912, Hilbert was almost exclusively a \"pure\" mathematician. When planning a visit from Bonn, where he was immersed in studying physics, his fellow mathematician and friend Hermann Minkowski joked he had to spend 10 days in quarantine before being able to visit Hilbert. In fact, Minkowski seems responsible for most of Hilbert's physics investigations prior to 1912, including their joint seminar in the subject in 1905.\n\nIn 1912, three years after his friend's death, Hilbert turned his focus to the subject almost exclusively. He arranged to have a \"physics tutor\" for himself. He started studying kinetic gas theory and moved on to elementary radiation theory and the molecular theory of matter. Even after the war started in 1914, he continued seminars and classes where the works of Albert Einstein and others were followed closely.\n\nBy 1907 Einstein had framed the fundamentals of the theory of gravity, but then struggled for nearly 8 years with a confounding problem of putting the theory into final form. By early summer 1915, Hilbert's interest in physics had focused on general relativity, and he invited Einstein to Göttingen to deliver a week of lectures on the subject. Einstein received an enthusiastic reception at Göttingen. Over the summer Einstein learned that Hilbert was also working on the field equations and redoubled his own efforts. During November 1915 Einstein published several papers culminating in \"The Field Equations of Gravitation\" (see Einstein field equations). Nearly simultaneously David Hilbert published \"The Foundations of Physics\", an axiomatic derivation of the field equations (see Einstein–Hilbert action). Hilbert fully credited Einstein as the originator of the theory, and no public priority dispute concerning the field equations ever arose between the two men during their lives. See more at priority.\n\nAdditionally, Hilbert's work anticipated and assisted several advances in the mathematical formulation of quantum mechanics. His work was a key aspect of Hermann Weyl and John von Neumann's work on the mathematical equivalence of Werner Heisenberg's matrix mechanics and Erwin Schrödinger's wave equation and his namesake Hilbert space plays an important part in quantum theory. In 1926 von Neumann showed that if atomic states were understood as vectors in Hilbert space, then they would correspond with both Schrödinger's wave function theory and Heisenberg's matrices.\n\nThroughout this immersion in physics, Hilbert worked on putting rigor into the mathematics of physics. While highly dependent on higher mathematics, physicists tended to be \"sloppy\" with it. To a \"pure\" mathematician like Hilbert, this was both \"ugly\" and difficult to understand. As he began to understand physics and how physicists were using mathematics, he developed a coherent mathematical theory for what he found, most importantly in the area of integral equations. When his colleague Richard Courant wrote the now classic \"Methoden der mathematischen Physik\" (Methods of Mathematical Physics) including some of Hilbert's ideas, he added Hilbert's name as author even though Hilbert had not directly contributed to the writing. Hilbert said \"Physics is too hard for physicists\", implying that the necessary mathematics was generally beyond them; the Courant-Hilbert book made it easier for them.\n\nHilbert unified the field of algebraic number theory with his 1897 treatise \"Zahlbericht\" (literally \"report on numbers\"). He also resolved a significant number-theory problem formulated by Waring in 1770. As with the finiteness theorem, he used an existence proof that shows there must be solutions for the problem rather than providing a mechanism to produce the answers. He then had little more to publish on the subject; but the emergence of Hilbert modular forms in the dissertation of a student means his name is further attached to a major area.\n\nHe made a series of conjectures on class field theory. The concepts were highly influential, and his own contribution lives on in the names of the Hilbert class field and of the Hilbert symbol of local class field theory. Results were mostly proved by 1930, after work by Teiji Takagi.\n\nHilbert did not work in the central areas of analytic number theory, but his name has become known for the Hilbert–Pólya conjecture, for reasons that are anecdotal.\n\nHis collected works (\"Gesammelte Abhandlungen\") have been published several times. The original versions of his papers contained \"many technical errors of varying degree\"; when the collection was first published, the errors were corrected and it was found that this could be done without major changes in the statements of the theorems, with one exception—a claimed proof of the continuum hypothesis. The errors were nonetheless so numerous and significant that it took Olga Taussky-Todd three years to make the corrections.\n\n\n\n"}
{"id": "38607512", "url": "https://en.wikipedia.org/wiki?curid=38607512", "title": "Delone set", "text": "Delone set\n\nIn the mathematical theory of metric spaces, ε-nets, ε-packings, ε-coverings, uniformly discrete sets, relatively dense sets, and Delone sets (named after Boris Delone) are several closely related definitions of well-spaced sets of points, and the packing radius and covering radius of these sets measure how well-spaced they are. These sets have applications in coding theory, approximation algorithms, and the theory of quasicrystals.\n\nIf (\"M\",\"d\") is a metric space, and \"X\" is a subset of \"M\", then the packing radius of \"X\" is half of the infimum of distances between distinct members of \"X\". If the packing radius is \"r\", then open balls of radius \"r\" centered at the points of \"X\" will all be disjoint from each other, and each open ball centered at one of the points of \"X\" with radius \"2r\" will be disjoint from the rest of \"X\". The covering radius of \"X\" is the infimum of the numbers \"r\" such that every point of \"M\" is within distance \"r\" of at least one point in \"X\"; that is, it is the smallest radius such that closed balls of that radius centered at the points of \"X\" have all of \"M\" as their union. An ε-packing is a set \"X\" of packing radius ≥ ε/2 (equivalently, minimum distance ≥ ε), an ε-covering is a set \"X\" of covering radius ≤ ε, and an ε-net is a set that is both an ε-packing and an ε-covering. A set is uniformly discrete if it has a nonzero packing radius, and relatively dense if it has a finite covering radius. A Delone set is a set that is both uniformly discrete and relatively dense; thus, every ε-net is Delone, but not vice versa.\n\nAs the most restrictive of the definitions above, ε-nets are at least as difficult to construct as ε-packings, ε-coverings, and Delone sets. However, whenever the points of \"M\" have a well-ordering, transfinite induction shows that it is possible to construct an ε-net \"N\", by including in \"N\" every point for which the infimum of distances to the set of earlier points in the ordering is at least ε. For finite sets of points in a Euclidean space of bounded dimension, each point may be tested in constant time by mapping it to a grid of cells of diameter ε, and using a hash table to test which nearby cells already contain points of \"N\"; thus, in this case, an ε-net can be constructed in linear time.\n\nFor more general finite or compact metric spaces, an alternative algorithm of Teo Gonzalez based on the farthest-first traversal can be used to construct a finite ε-net. This algorithm initializes the net \"N\" to be empty, and then repeatedly adds to \"N\" the farthest point in \"M\" from \"N\", breaking ties arbitrarily and stopping when all points of \"M\" are within distance ε of \"N\". In spaces of bounded doubling dimension, Gonzalez' algorithm can be implemented in O(\"n\" log \"n\") time for point sets with a polynomial ratio between their farthest and closest distances, and approximated in the same time bound for arbitrary point sets.\n\nIn the theory of error-correcting codes, the metric space containing a block code \"C\" consists of strings of a fixed length, say \"n\", taken over an alphabet of size \"q\" (can be thought of as vectors), with the Hamming metric. This space is denoted by formula_1. The covering radius and packing radius of this metric space are related to the code's ability to correct errors.\n\n describe an algorithmic paradigm that they call \"net and prune\" for designing approximation algorithms for certain types of geometric optimization problems defined on sets of points in Euclidean spaces. An algorithm of this type works by performing the following steps:\nIn both cases, the expected number of remaining points decreases by a constant factor, so the time is dominated by the testing step. As they show, this paradigm can be used to construct fast approximation algorithms for k-center clustering, finding a pair of points with median distance, and several related problems.\n\nA hierarchical system of nets, called a \"net-tree\", may be used in spaces of bounded doubling dimension to construct well-separated pair decompositions, geometric spanners, and approximate nearest neighbors.\n\nFor points in Euclidean space, a set \"X\" is a Meyer set if it is relatively dense and its difference set \"X\" − \"X\" is uniformly discrete. Equivalently, \"X\" is a Meyer set if both \"X\" and \"X\" − \"X\" are Delone. Meyer sets are named after Yves Meyer, who introduced them (with a different but equivalent definition based on harmonic analysis) as a mathematical model for quasicrystals. They include the point sets of lattices, Penrose tilings, and the Minkowski sums of these sets with finite sets.\n\nThe Voronoi cells of symmetric Delone sets form space-filling polyhedra called plesiohedra.\n\n"}
{"id": "33962500", "url": "https://en.wikipedia.org/wiki?curid=33962500", "title": "Denjoy–Wolff theorem", "text": "Denjoy–Wolff theorem\n\nIn mathematics, the Denjoy–Wolff theorem is a theorem in complex analysis and dynamical systems concerning fixed points and iterations of holomorphic mappings of the unit disc in the complex numbers into itself. The result was proved independently in 1926 by the French mathematician Arnaud Denjoy and the Dutch mathematician Julius Wolff.\n\nTheorem. Let \"D\" be the open unit disk in C and let \"f\" be a holomorphic function mapping \"D\" into \"D\" which is not an automorphism of \"D\" (i.e. a Möbius transformation). Then there is a unique point \"z\" in the closure of \"D\" such that the iterates of \"f\" tend to \"z\" uniformly on compact subsets of \"D\". If \"z\" lies in \"D\", it is the unique fixed point of \"f\". The mapping \"f\" leaves invariant hyperbolic disks centered on \"z\", if \"z\" lies in \"D\", and disks tangent to the unit circle at \"z\", if \"z\" lies on the boundary of \"D\".\n\nWhen the fixed point is at \"z\" = 0, the hyperbolic disks centred at \"z\" are just the Euclidean disks with centre 0. Otherwise \"f\" can be conjugated by a Möbius transformation so that the fixed point is zero. An elementary proof of the theorem is given below, taken from and . Two other short proofs can be found in .\n\nIf \"f\" has a fixed point \"z\" in \"D\" then, after conjugating by a Möbius transformation, it can be assumed that \"z\" = 0. Let \"M\"(\"r\") be the maximum modulus of \"f\" on \"|z|\" = \"r\" < 1. By the Schwarz lemma\n\nfor |\"z\"| ≤ \"r\", where\n\nIt follows by iteration that\n\nfor |\"z\"| ≤ \"r\". These two inequalities imply the result in this case.\n\nWhen \"f\" acts in \"D\" without fixed points, Wolff showed that there is a point \"z\" on the boundary such that the iterates of \"f\" leave invariant each disk tangent to the boundary at that point.\n\nTake a sequence formula_4 increasing to 1 and set\n\nBy applying Rouché's theorem to formula_6 and formula_7, formula_8 has exactly one zero formula_9 in \"D\". \nPassing to a subsequence if necessary, it can be assumed that formula_10 The point \"z\" cannot lie in \"D\", because, \nby passing to the limit, \"z\" would have to be a fixed point. The result for the case of fixed points implies that the maps formula_8 leave invariant all Euclidean disks whose hyperbolic center is located at formula_9. Explicit computations show that, as \"k\" increases, one can choose such disks so that they tend to any given disk tangent to the boundary at \"z\". By continuity, \"f\" leaves each such disk Δ invariant.\n\nTo see that formula_13 converges uniformly on compacta to the constant \"z\", it is enough to show that the same is true for any subsequence formula_14, convergent in the same sense to \"g\", say. Such limits exist by Montel's theorem, and if\n\"g\" is non-constant, it can also be assumed that formula_15 has a limit, \"h\" say. But then\n\nfor \"w\" in \"D\".\n\nSince \"h\" is holomorphic and \"g\"(\"D\") open,\n\nfor all \"w\".\n\nSetting formula_18, it can also be assumed that formula_19 is convergent to \"F\" say.\n\nBut then \"f\"(\"F\"(\"w\")) = \"w\" = \"f\"(\"F\"(\"w\")), contradicting the fact that \"f\" is not an automorphism.\n\nHence every subsequence tends to some constant uniformly on compacta in \"D\".\n\nThe invariance of Δ implies each such constant lies in the closure of each disk Δ, and hence their intersection, the single point \"z\". By Montel's theorem, it follows that formula_13 converges uniformly on compacta to the constant \"z\".\n\n"}
{"id": "58794547", "url": "https://en.wikipedia.org/wiki?curid=58794547", "title": "Direct Autonomous Authentication", "text": "Direct Autonomous Authentication\n\nDirect Autonomous Authentication (DAA) is a cybersecurity platform developed by San Francisco-based technology company Averon.\n\nThe DAA platform enables secure authentication of a mobile user whilst simultaneously preserving privacy of the user.\n\nThe technology was developed in stealth from late 2015, first publicly introduced by Averon in 2018, and featured at the 2018 Consumer Electronics Show as a new technology that combats the increasing threats of cybercrime and consumer account hacking.\nIn contrast to legacy methods of cybersecurity, the DAA platform bypasses end user actions, and rather than focusing on the authentication of a user's device, DAA instead provides autonomous authentication of a user's mobile phone number, since the mobile phone number continues to be associated with the user even when they lose, destroy or upgrade their mobile phone. \n\nThe DAA method uses a proprietary mix of technology developed by Averon that works inside the secure mobile network data pipelines together with encrypted technology already within every smartphone. The combination of these autonomous authentication methods has been described by research analysts as a faster, more secure, and stronger method of cybersecurity than traditional methods.\n\nBlockchain technology incorporated into the DAA platform ensures the privacy of end users. No identifiable personal data is maintained on the platform, therefore public disclosure of one's authentic identity (such as for the purpose of verified social media interactions) is voluntary. DAA technology affords the end user full control over identity disclosure in any given online interaction, which can be controlled by the end user in varying degrees from fully anonymous to fully identified publicly. In cases involving the need for anonymity with regard to an end user's safety, such as in cases of whistleblowers or political activists, the DAA platform's blockchain technology provides a method for both complete anonymity with the option of voluntary verification of limited but often needed data (such as verifying an anonymous user's general location). Thus DAA technology alleviates the heretofore insurmountable challenge of protecting user privacy with the need for authentication.\n\nThe DAA technology platform was designed to be seamlessly adopted for utilization in a wide variety of industries and use cases in which mobile authentication of users is required.\n\nSince its introduction to the market in 2018, the DAA platform has been recognized by a number of industry groups for its innovation, including winning the Gold prize at the 2018 Edison Awards, a Cybersecurity Excellence Award, and a BIG Innovation 2018 Award.\n"}
{"id": "44431245", "url": "https://en.wikipedia.org/wiki?curid=44431245", "title": "Efficiency (network science)", "text": "Efficiency (network science)\n\nIn network science, the efficiency of a network is a measure of how efficiently it exchanges information.\n\nThe average efficiency of a network formula_2 is defined as:\n\nwhere formula_4 denotes the total nodes in a network and formula_5 denotes the length of the shortest path between a node formula_1 and another node formula_7.\n\nAs an alternative to the average path length formula_8 of a network, the global efficiency of a network is defined as:\n\nwhere formula_10 is the \"ideal\" graph on formula_4 nodes wherein all possible edges are present. The global efficiency of network is a measure comparable to formula_12, rather than just the average path length itself. The key distinction is that formula_12 measures efficiency in a system where only one packet of information is being moved through the network and formula_14 measures the efficiency where all the nodes are exchanging packets of information with each other.\n\nAs an alternative to the clustering coefficient of a network, the local efficiency of a network is defined as:\n\nwhere formula_16 is the local subgraph consisting only of a node formula_1's immediate neighbors, but not the node formula_1 itself.\n\nBroadly speaking, the efficiency of a network can be used to quantify small world behavior in networks. Efficiency can also be used to determine cost-effective structures in weighted and unweighted networks. \nFor these reasons the concept of efficiency has been used across the many diverse applications of network science.\n\nBeyond human constructed networks, efficiency is a useful metric when talking about physical biological networks. In any facet of biology, the scarcity of resource plays a key role, and biological networks are no exception. Efficiency is used in neuroscience to discuss information transfer across neural networks, where the physical space and resource constraints are a major factor. Efficiency has also been used in the study of ant colony tunnel systems, which are usually composed of large rooms as well as many sprawling tunnels. This application to ant colonies is not too surprising because the large structure of a colony must serve as a transportation network for various resources, most namely food.\n"}
{"id": "9718355", "url": "https://en.wikipedia.org/wiki?curid=9718355", "title": "Eilenberg's inequality", "text": "Eilenberg's inequality\n\nEilenberg's inequality is a mathematical inequality for Lipschitz-continuous functions.\n\nLet \"ƒ\" : \"X\" → \"Y\" be a Lipschitz-continuous function between separable metric spaces whose Lipschitz constant is denoted by Lip \"ƒ\". Then, Eilenberg's inequality states that\n\nfor any \"A\" ⊂ \"X\" and all 0 ≤ \"n\" ≤ \"m\", where\n\nThe Eilenberg's Inequality is a key ingredient for the proof of the Coarea formula.\n"}
{"id": "10976022", "url": "https://en.wikipedia.org/wiki?curid=10976022", "title": "Euclidean shortest path", "text": "Euclidean shortest path\n\nThe Euclidean shortest path problem is a problem in computational geometry: given a set of polyhedral obstacles in a Euclidean space, and two points, find the shortest path between the points that does not intersect any of the obstacles.\n\nIn two dimensions, the problem can be solved in polynomial time in a model of computation allowing addition and comparisons of real numbers, despite theoretical difficulties involving the numerical precision needed to perform such calculations. These algorithms are based on two different principles, either performing a shortest path algorithm such as Dijkstra's algorithm on a visibility graph derived from the obstacles or (in an approach called the \"continuous Dijkstra\" method) propagating a wavefront from one of the points until it meets the other.\n\nIn three (and higher) dimensions the problem is NP-hard in the general case, but there exist efficient approximation algorithms that run in polynomial time based on the idea of finding a suitable sample of points on the obstacle edges and performing a visibility graph calculation using these sample points.\n\nThere are many results on computing shortest paths which stays on a polyhedral surface. Given two points s and t, say on the surface\nof a convex polyhedron, the problem is to compute a shortest path that never leaves the surface and connects s with t. \nThis is a generalization of the problem from 2-dimension but it is much easier than the 3-dimensional problem.\n\nAlso, there are variations of this problem, where the obstacles are \"weighted\", i.e., one can go through an obstacle, but it incurs\nan extra cost to go through an obstacle. The standard problem is the special case where the obstacles have infinite weight. This is\ntermed as the \"weighted region problem\" in the literature.\n\n\n\n"}
{"id": "453204", "url": "https://en.wikipedia.org/wiki?curid=453204", "title": "Glossary of order theory", "text": "Glossary of order theory\n\nThis is a glossary of some terms used in various branches of mathematics that are related to the fields of order, lattice, and domain theory. Note that there is a structured list of order topics available as well. Other helpful resources might be the following overview articles:\n\n\nIn the following, partial orders will usually just be denoted by their carrier sets. As long as the intended meaning is clear from the context, ≤ will suffice to denote the corresponding relational symbol, even without prior introduction. Furthermore, < will denote the strict order induced by ≤.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe definitions given here are consistent with those that can be found in the following standard reference books:\n\n\nSpecific definitions:\n"}
{"id": "36704617", "url": "https://en.wikipedia.org/wiki?curid=36704617", "title": "Hardy–Littlewood inequality", "text": "Hardy–Littlewood inequality\n\nIn mathematical analysis, the Hardy–Littlewood inequality, named after G. H. Hardy and John Edensor Littlewood, states that if \"f\" and \"g\" are nonnegative measurable real functions vanishing at infinity that are defined on \"n\"-dimensional Euclidean space R then\n\nwhere \"f\" and \"g\" are the symmetric decreasing rearrangements of \"f\"(\"x\") and \"g\"(\"x\"), respectively.\n\nFrom layer cake representation we have:\n\nwhere formula_4 denotes the indicator function of the subset \"E\" given by\n\nAnalogously, formula_6 denotes the indicator function of the subset \"E\" given by\n\n"}
{"id": "3253502", "url": "https://en.wikipedia.org/wiki?curid=3253502", "title": "IEEE 854-1987", "text": "IEEE 854-1987\n\nIEEE Std 854-1987, the \"Standard for Radix-Independent Floating-Point Arithmetic\", was the first Institute of Electrical and Electronics Engineers (IEEE) standard for floating-point arithmetic with radix 2 or radix 10 (not more general than that, despite the title).\n\nThe standard was published in 1987, nearly immediately superseded by IEEE 754-1985 but never terminated (the year of ratification appears after the dash). IEEE 854 did not specify any formats, whereas IEEE 754-1985 did. IEEE 754 specifies floating-point arithmetic for both radix 2 (binary) and radix 10 (decimal), including specifying two alternative formats for radix 10 floating-point values. IEEE 754-1985 was only superseded in 2008 by IEEE 754-2008. IEEE 754-2008 also has many other updates to the IEEE floating point standardisation.\n"}
{"id": "22352470", "url": "https://en.wikipedia.org/wiki?curid=22352470", "title": "Invertible module", "text": "Invertible module\n\nIn mathematics, particularly commutative algebra, an invertible module is intuitively a module that has an inverse with respect to the tensor product. Invertible modules form the foundation for the definition of invertible sheaves in algebraic geometry.\n\nFormally, a finitely generated module \"M\" over a ring \"R\" is said to be invertible if it is locally a free module of rank 1. In other words, formula_1 for all primes \"P\" of \"R\". Now, if \"M\" is an invertible \"R\"-module, then its dual is its inverse with respect to the tensor product, i.e. formula_2.\n\nThe theory of invertible modules is closely related to the theory of codimension one varieties including the theory of divisors.\n\n\n"}
{"id": "58573060", "url": "https://en.wikipedia.org/wiki?curid=58573060", "title": "Karen Willcox", "text": "Karen Willcox\n\nKaren Elizabeth Willcox is an applied mathematician best known for her work on reduced-order modeling and the study of multi-fidelity methods. She is currently the director of the Institute for Computational Engineering and Sciences and professor of Aerospace Engineering and Engineering Mechanics at the University of Texas at Austin, Texas.\n\nWillcox was born and raised in New Zealand where she earned a bachelor's degree in Engineering Science from the University of Auckland in the year 1994. She subsequently moved to Boston, Massachusetts to join the Massachusetts Institute of Technology (MIT) for graduate studies. At MIT, she received a master's degree in Aeronautics and Astronautics in 1996 and a PhD in the same subject in the year 2000. Her thesis, titled 'Reduced-Order Aerodynamic Models for Aeroelastic Control of Turbomachines', was completed under the supervision of Jaime Peraire and James Paduano. While at MIT, Willcox played for the MIT Women's Rugby team. She is also an avid marathon-runner and an experienced mountain climber.\n\nWillcox had long wanted to be an astronaut. She made the shortlist of candidates for NASA's astronaut training program in 2009 and 2013, but both attempts remained unsuccessful.\n\nFollowing her doctoral studies, Willcox worked at Boeing Phantom Works in the Blended-Wing-Body aircraft design group for a year. In 2001, she joined the Department of Aeronautics and Astronautics at MIT as a professor. In 2008, She additionally became a founding co-director of the MIT Center for Computational Engineering. She stayed at MIT until July 2018. During this period, she also had short-term visiting appointments at Sandia National Laboratories, the University of Auckland and Singapore University of Technology and Design. In August 2018, Willcox joined the University of Texas at Austin to succeed J. Tinsley Oden as the director of the Institute for Computational Engineering and Sciences.\n\nWillcox has served on the editorial board of several journals; she is currently a Section editor for the SIAM Journal on Scientific Computing and an Associate editor for the AIAA Journal.\n\nIn addition to research, Willcox is involved in science education and policy as well. An advocate for innovation in teaching, she served as co-chair of the Online Education Policy Initiative at MIT. In 2015, she received a First in the World grant from the US Department of Education.\n\nWillcox became a Member of the New Zealand Order of Merit in 2017.\nShe was elected as a fellow of the Society for Industrial and Applied Mathematics in 2018, \"for contributions to model reduction and multifidelity methods, with applications in optimization, control, design, and uncertainty quantification of large-scale systems\".\n\n"}
{"id": "26066467", "url": "https://en.wikipedia.org/wiki?curid=26066467", "title": "Kish grid", "text": "Kish grid\n\nThe Kish grid or Kish selection grid is a method for selecting members within a household to be interviewed. It uses a pre-assigned table of random numbers to find the person to be interviewed. It was developed by statistician Leslie Kish in 1949.\n\nIt is a technique widely used in survey research. However, in telephone surveys, the next-birthday method is sometimes preferred to the Kish grid.\n"}
{"id": "33074893", "url": "https://en.wikipedia.org/wiki?curid=33074893", "title": "List of long mathematical proofs", "text": "List of long mathematical proofs\n\nThis is a list of unusually long mathematical proofs.\n\n, the longest mathematical proof, measured by number of published journal pages, is the classification of finite simple groups with well over 10000 pages. There are several proofs that would be far longer than this if the details of the computer calculations they depend on were published in full.\n\nThe length of unusually long proofs has increased with time. As a rough rule of thumb, 100 pages in 1900, or 200 pages in 1950, or 500 pages in 2000 is unusually long for a proof.\n\n\nThere are many mathematical theorems that have been checked by long computer calculations. If these were written out as proofs many would be far longer than most of the proofs above. There is not really a clear distinction between computer calculations and proofs, as several of the proofs above, such as the 4-color theorem and the Kepler conjecture, use long computer calculations as well as many pages of mathematical argument. For the computer calculations in this section, the mathematical arguments are only a few pages long, and the length is due to long but routine calculations. Some typical examples of such theorems include:\n\n\nKurt Gödel showed how to find explicit examples of statements in formal systems that are provable in that system but whose shortest proof is absurdly long. For example, the statement:\nis provable in Peano arithmetic but the shortest proof has at least a googolplex symbols. It has a short proof in a more powerful system: in fact it is easily provable in Peano arithmetic together with the statement that Peano arithmetic is consistent (which cannot be proved in Peano arithmetic by Gödel's incompleteness theorem).\n\nIn this argument, Peano arithmetic can be replaced by any more powerful consistent system, and a googolplex can be replaced by any number that can be described concisely in the system.\n\nHarvey Friedman found some explicit natural examples of this phenomenon, giving some explicit statements in Peano arithmetic and other formal systems whose shortest proofs are ridiculously long . For example, the statement\nis provable in Peano arithmetic, but the shortest proof has length at least \"A\"(1000), where \"A\"(0)=1 and \"A\"(\"n\"+1)=2. The statement is a special case of Kruskal's theorem and has a short proof in second order arithmetic.\n\n\n"}
{"id": "4109196", "url": "https://en.wikipedia.org/wiki?curid=4109196", "title": "Logical matrix", "text": "Logical matrix\n\nA logical matrix, binary matrix, relation matrix, Boolean matrix, or (0,1) matrix is a matrix with entries from the Boolean domain Such a matrix can be used to represent a binary relation between a pair of finite sets.\n\nIf \"R\" is a binary relation between the finite indexed sets \"X\" and \"Y\" (so ), then \"R\" can be represented by the logical matrix \"M\" whose row and column indices index the elements of \"X\" and \"Y\", respectively, such that the entries of \"M\" are defined by:\n\nIn order to designate the row and column numbers of the matrix, the sets \"X\" and \"Y\" are indexed with positive integers: \"i\" ranges from 1 to the cardinality (size) of \"X\" and \"j\" ranges from 1 to the cardinality of \"Y\". See the entry on indexed sets for more detail.\n\nThe binary relation \"R\" on the set is defined so that \"aRb\" holds if and only if \"a\" divides \"b\" evenly, with no remainder. For example, 2\"R\"4 holds because 2 divides 4 without leaving a remainder, but 3\"R\"4 does not hold because when 3 divides 4 there is a remainder of 1. The following set is the set of pairs for which the relation \"R\" holds. \nThe corresponding representation as a logical matrix is:\n\n\nThe matrix representation of the equality relation on a finite set is the identity matrix I, that is, the matrix whose entries on the diagonal are all 1, while the others are all 0. More generally, if relation \"R\" satisfies I ⊂ \"R\", then R is a reflexive relation.\n\nIf the Boolean domain is viewed as a semiring, where addition corresponds to logical OR and multiplication to logical AND, the matrix representation of the composition of two relations is equal to the matrix product of the matrix representations of these relation.\nThis product can be computed in expected time O(\"n\").\n\nFrequently operations on binary matrices are defined in terms of modular arithmetic mod 2—that is, the elements are treated as elements of the Galois field . They arise in a variety of representations and have a number of more restricted special forms. They are applied e.g. in XOR-satisfiability.\n\nThe number of distinct \"m\"-by-\"n\" binary matrices is equal to 2, and is thus finite.\n\nLet \"n\" and \"m\" be given and let \"U\" denote the set of all logical \"m\" × \"n\" matrices. Then \"U\" has a partial order given by\n\nIn fact, \"U\" forms a Boolean algebra with the operations and and or between two matrices applied component-wise. The complement of a logical matrix is obtained by swapping all zeros and ones for their opposite.\n\nEvery logical matrix a = ( a ) has an transpose a = ( a ). Suppose \"a\" is a logical matrix with no columns or rows identically zero. Then the matrix product, using Boolean arithmetic, a a is the \"m\" × \"m\" identity matrix, and the product a a is the \"n\" × \"n\" identity.\n\nAs a mathematical structure, the Boolean algebra \"U\" forms a lattice ordered by inclusion; additionally it is a multiplicative lattice due to matrix multiplication. \n\nEvery logical matrix in \"U\" corresponds to a binary relation. These listed operations on \"U\", and ordering, correspond to a calculus of relations, where the matrix multiplication represents composition of relations.\n\nIf \"m\" or \"n\" equals one, then the \"m\" × \"n\" logical matrix (M) is a logical vector. If \"m\" = 1 the vector is a row vector, and if \"n\" = 1 it is a column vector. In either case the index equaling one is dropped from denotation of the vector.\n\nSuppose formula_4 are two logical vectors. The outer product of \"P\" and \"Q\" results in an \"m\" × \"n\" rectangular relation:\n\nIn concept analysis a relation is studied by determining the maximal rectangular relations contained in it.\n\n\n"}
{"id": "463225", "url": "https://en.wikipedia.org/wiki?curid=463225", "title": "Metric signature", "text": "Metric signature\n\nThe signature of a metric tensor \"g\" (or equivalently, a real quadratic form thought of as a real symmetric bilinear form on a finite-dimensional vector space) is the number (counted with multiplicity) of positive, zero, and negative eigenvalues of the real symmetric matrix of the metric tensor with respect to a basis. In physics, the \"v\" represents for the time or virtual dimension, and the \"p\" for the space and physical dimension. Alternatively, it can be defined as the dimensions of a maximal positive and null subspace. By Sylvester's law of inertia these numbers do not depend on the choice of basis. The signature thus classifies the metric up to a choice of basis. The signature is often denoted by a pair of integers implying \"r\" = 0 or as an explicit list of signs of eigenvalues such as or for the signature , respectively.\n\nThe signature is said to be indefinite or mixed if both \"v\" and \"p\" are nonzero, and degenerate if \"r\" is nonzero. A Riemannian metric is a metric with a positive definite signature . The Lorentzian metric is a metric signature with two eigenvalues.\n\nThere is another notion of signature of a nondegenerate metric tensor given by a single number \"s\" defined as , where \"v\" and \"p\" are as above, which is equivalent to the above definition when the dimension \"n\" = \"v\" + \"p\" is given or implicit. For example, \"s\" = 1 − 3 = −2 for and its mirroring \"s' \" = −\"s\" = +2 for .\n\nThe signature of a metric tensor is defined as the signature of the corresponding quadratic form. It is the number of positive and zero eigenvalues of any matrix (i.e. in any basis for the underlying vector space) representing the form, counted with their algebraic multiplicity. Usually, is required, which is the same as saying a metric tensor must be nondegenerate, i.e. no nonzero vector is orthogonal to all vectors.\n\nBy Sylvester's law of inertia, the numbers are basis independent.\n\nBy the spectral theorem a symmetric \"n\" × \"n\" matrix over the reals is always diagonalizable, and has therefore exactly \"n\" real eigenvalues (counted with algebraic multiplicity). Thus .\n\nAccording to Sylvester's law of inertia, the signature of the scalar product (a.k.a. real symmetric bilinear form), \"g\" does not depend on the choice of basis. Moreover, for every metric \"g\" of signature there exists a basis such that \n\nThe number \"v\" (resp. \"p\") is the maximal dimension of a vector subspace on which the scalar product \"g\" is positive-definite (resp. negative-definite), and \"r\" is the dimension of the radical of the scalar product \"g\" or the null subspace of symmetric matrix of the scalar product. Thus a nondegenerate scalar product has signature , with . A duality of the special cases correspond to two scalar eigenvalues which can be transformed into each other by the mirroring reciprocally.\n\nThe signature of the identity matrix is where . The diagonal matrix of a signature is the number of positive, negative and zero numbers on its main diagonal.\n\nThe following matrices have both the same signature , therefore they are congruent because of Sylvester's law of inertia:\n\nThe standard scalar product defined on formula_2 has the n-dimensional signatures , where and rank .\n\nIn physics, the Minkowski space is a spacetime manifold formula_3 with \"v=1\" and \"p=3\" bases, and has a scalar product defined by either the formula_4 matrix:\nwhich has signature formula_6 and known as space-supremacy or space-like; Or the mirroring signature formula_7, known virtual-supremacy or time-like with the formula_8 matrix. \n\nThere are some methods for computing the signature of a matrix.\n\nIn mathematics, the usual convention for any Riemannian manifold is to use a positive-definite metric tensor (meaning that after diagonalization, elements on the diagonal are all positive).\n\nIn theoretical physics, spacetime is modeled by a pseudo-Riemannian manifold. The signature counts how many time-like or space-like characters are in the spacetime, in the sense defined by special relativity: as used in particle physics, the metric has an eigenvalue on the time-like subspace, and its mirroring eigenvalue on the space-like subspace.\nIn the specific case of the Minkowski metric,\nthe metric signature is formula_11 or (+, −, −, −) if its eigenvalue is defined in the time direction, or formula_6 or (−, +, +, +) if the eigenvalue is defined in the three spatial directions \"x\", \"y\" and \"z\". \n\nIf a metric is regular everywhere then the signature of the metric is constant. However if one allows for metrics that are degenerate or discontinuous on some hypersurfaces, then signature of the metric may change at these surfaces. Such signature changing metrics may possibly have applications in cosmology and quantum gravity.\n\n"}
{"id": "32953045", "url": "https://en.wikipedia.org/wiki?curid=32953045", "title": "Mixed volume", "text": "Mixed volume\n\nIn mathematics, more specifically, in convex geometry, the mixed volume is a way to associate a non-negative number to an formula_1-tuple of convex bodies in the formula_1-dimensional space. This number depends on the size and shape of the bodies and on their relative orientation to each other.\n\nLet formula_3 be convex bodies in formula_4 and consider the function\n\nwhere formula_6 stands for the formula_1-dimensional volume and its argument is the Minkowski sum of the scaled convex bodies formula_8. One can show that formula_9 is a homogeneous polynomial of degree formula_1, therefore it can be written as\n\nwhere the functions formula_12 are symmetric. Then formula_13 is called the mixed volume of formula_14.\n\nEquivalently,\n\n\n\nLet formula_24 be a convex body and let formula_25 be the Euclidean ball of unit radius. The mixed volume\n\nis called the \"j\"-th quermassintegral of formula_27.\n\nThe definition of mixed volume yields the Steiner formula (named after Jakob Steiner):\n\nThe \"j\"-th intrinsic volume of formula_27 is a different normalization of the quermassintegral, defined by\n\nwhere formula_32 is the volume of the formula_33-dimensional unit ball.\n\nHadwiger's theorem asserts that every valuation on convex bodies in formula_4 that is continuous and invariant under rigid motions of formula_4 is a linear combination of the quermassintegrals (or, equivalently, of the intrinsic volumes).\n"}
{"id": "34049712", "url": "https://en.wikipedia.org/wiki?curid=34049712", "title": "Otto Frostman", "text": "Otto Frostman\n\nOtto Albin Frostman (3 January 1907 – 29 December 1977) was a Swedish mathematician, known for his work in potential theory and complex analysis.\n\nFrostman earned his Ph.D. in 1935 at Lund University under the Hungarian-born mathematician Marcel Riesz, the younger brother of F. Riesz. In potential theory, Frostman's lemma is named after him. He supervised the 1971 Stockholm University Ph.D. thesis of Bernt Lindström, which initiated the \"Stockholm School\" of topological combinatorics (combining simplicial homology and enumerative combinatorics).\n\n"}
{"id": "28001680", "url": "https://en.wikipedia.org/wiki?curid=28001680", "title": "Paul Erdős Award", "text": "Paul Erdős Award\n\nThe Paul Erdős Award, named after Paul Erdős, is given by the\nWorld Federation of National Mathematics Competitions for those who \"have played a significant role in the development of mathematical challenges at the national or international level and which have been a stimulus for the enrichment of mathematics learning\". The awards have been given in two-year periods since 1996.\n\n\n"}
{"id": "7852591", "url": "https://en.wikipedia.org/wiki?curid=7852591", "title": "Polynomial matrix", "text": "Polynomial matrix\n\nIn mathematics, a polynomial matrix or matrix of polynomials is a matrix whose elements are univariate or multivariate polynomials. Equivalently, a polynomial matrix is a polynomial whose coefficients are matrices. \n\nA univariate polynomial matrix \"P\" of degree \"p\" is defined as:\n\nwhere formula_2 denotes a matrix of constant coefficients, and formula_3 is non-zero. \nAn example 3×3 polynomial matrix, degree 2:\n\nWe can express this by saying that for a ring \"R\", the rings formula_5 and\nformula_6 are isomorphic.\n\n\nNote that polynomial matrices are \"not\" to be confused with monomial matrices, which are simply matrices with exactly one non-zero entry in each row and column.\n\nIf by λ we denote any element of the field over which we constructed the matrix, by \"I\" the identity matrix, and we let \"A\" be a polynomial matrix, then the matrix λ\"I\" − \"A\" is the characteristic matrix of the matrix \"A\". Its determinant, |λ\"I\" − \"A\"| is the characteristic polynomial of the matrix \"A\".\n\n"}
{"id": "2093844", "url": "https://en.wikipedia.org/wiki?curid=2093844", "title": "Radical axis", "text": "Radical axis\n\nThe radical axis (or power line) of two circles is the locus of points at which tangents drawn to both circles have the same length. The radical axis is always a straight line and always perpendicular to the line connecting the centers of the circles, albeit closer to the circumference of the larger circle. If the circles intersect, the radical axis is the line passing through the intersection points; similarly, if the circles are tangent, the radical axis is simply the common tangent. \n\nFor any point P on the radical axis, there is a unique circle centered on P that intersects both circles at right angles (orthogonally); conversely, the center of any circle that cuts both circles orthogonally must lie on the radical axis. In technical language, each point P on the radical axis has the same power with respect to both circles\n\nwhere \"r\" and \"r\" are the radii of the two circles, \"d\" and \"d\" are distances from P to the centers of the two circles, and \"R\" is the radius of the unique orthogonal circle centered on P.\n\nIn general, two disjoint, non-concentric circles can be aligned with the circles of bipolar coordinates; in that case, the radical axis is simply the \"y\"-axis; every circle on that axis that passes through the two foci intersect the two circles orthogonally. Thus, two radii of such a circle are tangent to both circles, satisfying the definition of the radical axis. The collection of all circles with the same radical axis and with centers on the same line is known as a pencil of coaxal circles.\n\nConsider three circles A, B and C, no two of which are concentric. The radical axis theorem states that the three radical axes (for each pair of circles) intersect in one point called the radical center, or are parallel. In technical language, the three radical axes are \"concurrent\" (share a common point); if they are parallel, they concur at a point of infinity.\n\nA simple proof is as follows. The radical axis of circles A and B is defined as the line along which the tangents to those circles are equal in length \"a\"=\"b\". Similarly, the tangents to circles B and C must be equal in length on their radical axis. By the transitivity of equality, all three tangents are equal \"a\"=\"b\"=\"c\" at the intersection point r of those two radical axes. Hence, the radical axis for circles A and C must pass through the same point r, since \"a\"=\"c\" there. This common intersection point r is the radical center.\n\nThere is a unique circle with its center at the radical center that is orthogonal to all three circles. This follows, also by transitivity, because each radical axis, being the locus of centers of circles that cut each pair of given circles orthogonally, requires all three circles to have equal radius at the intersection of all three axes.\n\nThe radical axis of two circles A and B can be constructed by drawing a line through any two of its points. Such a point can be found by drawing a circle C that intersects both circles A and B in two points. The two lines passing through each pair of intersection points are the radical axes of A and C and of B and C. These two lines intersect in a point J that is the radical center of all three circles, as described above; therefore, this point also lies on the radical axis of A and B. Repeating this process with another such circle D provides a second point K. The radical axis is the line passing through both J and K.\n\nA special case of this approach, seen in Figure 3, is carried out with antihomologous points from an internal or external center of similarity. Consider two rays emanating from an external homothetic center E. Let the antihomologous pairs of intersection points of these rays with the two given circles be denoted as P and Q, and S and T, respectively. These four points lie on a common circle that intersects the two given circles in two points each. Hence, the two lines joining P and S, and Q and T intersect at the radical center of the three circles, which lies on the radical axis of the given circles. Similarly, the line joining two antihomologous points on separate circles and their tangents form an isosceles triangle, with both tangents being of equal length. Therefore, such tangents meet on the radical axis.\n\nReferring to Figure 4, the radical axis (red) is perpendicular to the blue line segment joining the centers B and V of the two given circles, intersecting that line segment at a point K between the two circles. Therefore, it suffices to find the distance \"x\" or \"x\" from K to B or V, respectively, where \"x\"+\"x\" equals \"D\", the distance between B and V.\n\nConsider a point J on the radical axis, and let its distances to B and V be denoted as \"d\" and \"d\", respectively. Since J must have the same power with respect to both circles, it follows that\n\nwhere \"r\" and \"r\" are the radii of the two given circles. By the Pythagorean theorem, the distances \"d\" and \"d\" can be expressed in terms of \"x\", \"x\" and \"L\", the distance from J to K\n\nBy cancelling \"L\" on both sides of the equation, the equation can be written \n\nDividing both sides by \"D\" = \"x\"+\"x\" yields the equation\n\nAdding this equation to \"x\"+\"x\" = \"D\" yields a formula for \"x\"\n\nSubtracting the same equation yields the corresponding formula for \"x\"\n\nIf the circles are represented in trilinear coordinates in the usual way, then their radical center is conveniently given as a certain determinant. Specifically, let \"X\" = \"x\" : \"y\" : \"z\" denote a variable point in the plane of a triangle \"ABC\" with sidelengths \"a\" = |\"BC\"|, \"b\" = |\"CA\"|, \"c\" = |\"AB\"|, and represent the circles as follows:\n\nThen the radical center is the point\n\nThe radical plane of two nonconcentric spheres in three dimensions is defined similarly: it is the locus of points from which tangents to the two spheres have the same length. The fact that this locus is a plane follows by rotation in the third dimension from the fact that the radical axis is a straight line. \n\nThe same definition can be applied to hyperspheres in Euclidean space of any dimension, giving the radical hyperplane of two nonconcentric hyperspheres.\n\n\n"}
{"id": "42455846", "url": "https://en.wikipedia.org/wiki?curid=42455846", "title": "Ran space", "text": "Ran space\n\nIn mathematics, the Ran space (or Ran's space) of a topological space \"X\" is a topological space formula_1 whose underlying set is the set of all nonempty finite subsets of \"X\": for a metric space \"X\" the topology is induced by the Hausdorff distance. The notion is named after Ziv Ran. It seems the notion was first introduced and popularized by Alexander Beilinson and Vladimir Drinfeld in the context of Chiral algebras.\nIn general, the topology of the Ran space is generated by sets\n\nfor any disjoint open subsets formula_3.\n\nThere is an analog of a Ran space for a scheme: the Ran prestack of a quasi-projective scheme \"X\" over a field \"k\", denoted by formula_1, is the category where the objects are triples formula_5 consisting of a finitely generated \"k\"-algebra \"R\", a nonempty set \"S\" and a map of sets formula_6 and where a morphism formula_7 consists of a \"k\"-algebra homomorphism formula_8, a surjective map formula_9 that commutes with formula_10 and formula_11. Roughly, an \"R\"-point of formula_1 is a nonempty finite set of \"R\"-rational points of \"X\" \"with labels\" given by formula_10. A theorem of Beilinson and Drinfeld continues to hold: formula_1 is acyclic if \"X\" is connected.\n\nA theorem of Beilinson and Drinfeld states that the Ran space of a connected manifold is weakly contractible.\n\nIf \"F\" is a cosheaf on the Ran space formula_15, then its space of global sections is called the topological chiral homology of \"M\" with coefficients in \"F\". If \"A\" is, roughly, a family of commutative algebras parametrized by points in \"M\", then there is a factorizable sheaf associated to \"A\". Via this construction, one also obtains the topological chiral homology with coefficients in \"A\". The construction is a generalization of Hochschild homology.\n\n"}
{"id": "48536047", "url": "https://en.wikipedia.org/wiki?curid=48536047", "title": "Romanovski polynomials", "text": "Romanovski polynomials\n\nIn mathematics, Romanovski polynomials is an informal term for one of three finite subsets of real orthogonal polynomials discovered by Vsevolod Romanovsky (Romanovski in French transcription) within the context of probability distribution functions in statistics. They form an orthogonal subset of a more general family of little-known Routh polynomials introduced by Edward John Routh in 1884. The term Romanovski polynomials was put forward by Raposo, with reference to the so-called 'pseudo-Jacobi polynomials in Lesky's classification scheme. It seems more consistent to refer to them as Romanovski–Routh polynomials, by analogy with the terms Romanovski–Bessel and Romanovski–Jacobi used by Lesky for two other sets of orthogonal polynomials.\n\nIn some contrast to the standard classical orthogonal polynomials, the polynomials under consideration differ, in so far as for arbitrary parameters only \"a finite number of them are orthogonal\", as discussed in more detail below.\n\nThe Romanovski polynomials solve the following version of the hypergeometric differential equation\n\nCuriously, they have been omitted from the standard textbooks on special functions in mathematical physics and in mathematics and have only a relatively scarce presence elsewhere in the mathematical literature.\n\nThe weight functions are \n\nthey solve Pearson's differential equation \n\nthat assures the self-adjointness of the differential operator of the hypergeometric \nordinary differential equation.\n\nFor and , the weight function of the Romanovski polynomials takes the shape of the Cauchy distribution, whence the associated polynomials are also denoted as Cauchy polynomials in their applications in random matrix theory.\n\nThe Rodrigues formula specifies the polynomial as\n\nwhere is a normalization constant. This constant is related to the coefficient of the term of degree in the polynomial by the expression\n,\\quad \\lambda_n=-n\\left({t^{(\\alpha,\\beta)}_n}' +\n\\tfrac{1}{2}(n-1)s\"(x)\\right),\nwhich holds for .\n\nAs shown by Askey this finite sequence of real orthogonal polynomials can be expressed in terms of Jacobi polynomials of imaginary argument and thereby is frequently referred to as complexified Jacobi polynomials. Namely, the Romanovski equation () can be formally obtained from the Jacobi equation,\n\"(x) + t^{(\\gamma,\\delta)}_1(x) {P_n^{(\\gamma,\\delta )}}'(x) + \\lambda_n P^{(\\gamma, \\delta)}_n(x) = 0, \\\\[4pt]\n&\\qquad t^{(\\gamma, \\delta)}_1(x)=\\delta -\\gamma -(\\gamma +\\delta +2)x, \\quad\n\\lambda_n= n(n+\\gamma +\\delta +1),\\quad x\\in \\left[-1,1 \\right],\nvia the replacements, for real , \n"}
{"id": "3386222", "url": "https://en.wikipedia.org/wiki?curid=3386222", "title": "Samuel Vince", "text": "Samuel Vince\n\nSamuel Vince (6 April 1749 – 28 November 1821) was an English clergyman, mathematician and astronomer at the University of Cambridge.\n\nHe was born in Fressingfield. The son of a plasterer, Vince was admitted as a sizar to Caius College, Cambridge in 1771. In 1775 he was Senior Wrangler, and Winner of the Smith Prize at Cambridge. Migrating to Sidney Sussex College in 1777, he gained his M.A. in 1778 and was ordained a clergyman in 1779.\n\nHe was awarded the Copley Medal in 1780 and was Plumian Professor of Astronomy and Experimental Philosophy at Cambridge from 1796 until his death. He became Archdeacon of Bedford in 1809, and died in Ramsgate.\n\nAs a mathematician, Vince wrote on many aspects of his expertise, including logarithms and imaginary numbers. His \"Observations on the Theory of the Motion and Resistance of Fluids\" and \"Experiments upon the Resistance of Bodies Moving in Fluids\" had later importance to aviation history. He was also author of the influential \"A Complete System of Astronomy\" (3 vols. 1797-1808).\n\nVince also published the pamphlet \"The Credibility of Christianity Vindicated, In Answer to Mr. Hume's Objections; In Two Discourses Preached Before the University of Cambridge by the Rev. S. Vince\". In this work, Vince made an apology of the Christian religion and, like Charles Babbage, sought to present rational arguments in favor of the belief in miracles, against David Hume's criticism. A review of this work with direct quotations can be found in \"The British Critic\", Volume 12, 1798.\n\n"}
{"id": "31587409", "url": "https://en.wikipedia.org/wiki?curid=31587409", "title": "Sanov's theorem", "text": "Sanov's theorem\n\nIn information theory, Sanov's theorem gives a bound on the probability of observing an atypical sequence of samples from a given probability distribution.\n\nLet \"A\" be a set of probability distributions over an alphabet \"X\", and let \"q\" be an arbitrary distribution over \"X\" (where \"q\" may or may not be in \"A\"). Suppose we draw \"n\" i.i.d. samples from \"q\", represented by the vector formula_1. Further, let us ask that the empirical distribution, formula_2, of the samples falls within the set \"A\"—formally, we write formula_3. Then,\n\nwhere\n\nIn words, the probability of drawing an atypical distribution is proportional to the KL distance from the true distribution to the atypical one; in the case that we consider a set of possible atypical distributions, there is a dominant atypical distribution, given by the information projection.\n\nFurthermore, if \"A\" is the closure of its interior,\n\n"}
{"id": "1186980", "url": "https://en.wikipedia.org/wiki?curid=1186980", "title": "Social software (social procedure)", "text": "Social software (social procedure)\n\nIn philosophy and the social sciences, social software is an interdisciplinary research program that borrows\nmathematical tools and techniques from game theory and computer science in order to analyze and design social procedures. The goals of research in this field are modeling social situations, developing theories of correctness, and designing social procedures.\n\nWork under the term social software has been going on since about 1996, and conferences in Copenhagen, London, Utrecht and New York, have been partly or wholly devoted to it. Much of the work is carried out at the City University of New York under the leadership of Rohit Jivanlal Parikh, who was influential in the development of the field.\n\nCurrent research in the area of social software include the analysis of social procedures and examination of them for fairness, appropriateness, correctness and efficiency. For example, an election procedure could be a simple majority vote, Borda count, a Single Transferable vote (STV), or Approval voting. All of these procedures can be examined for various properties like monotonicity. Monotonicity has the property that voting for a candidate should not harm that candidate. This may seem obvious, true\nunder any system, but it is something which can happen in STV. Another question would be the ability to elect a Condorcet winner in case there is one. \n\nOther principles which are considered by researchers in social software include the concept that a procedure for fair division should be Pareto optimal, equitable and envy free. A procedure for auctions should be one which would encourage bidders to bid their actual valuation – a property which holds with the Vickrey auction.\n\nWhat is new in social software compared to older fields is the use of tools from computer science like program logic, analysis of algorithms and epistemic logic. Like programs, social procedures dovetail into each other. For instance an airport provides runways for planes to land, but it also provides security checks, and it must provide for ways in which buses and taxis can take arriving passengers to their local destinations. The entire mechanism can be analyzed in the way in which a complex computer program can be analyzed. The Banach-Knaster procedure for dividing a cake fairly, or the Brams and Taylor procedure for fair division have been analyzed in this way. To point to the need for epistemic logic, a building not only needs restrooms, for obvious reasons, it also needs signs indicating where they are. Thus epistemic considerations enter in addition to structural ones. For a more urgent example, in addition to medicines, physicians also need tests to indicate what a patient’s problem is.\n\n\n\n"}
{"id": "10988372", "url": "https://en.wikipedia.org/wiki?curid=10988372", "title": "Statistical interference", "text": "Statistical interference\n\nWhen two probability distributions overlap, statistical interference exists. Knowledge of the distributions can be used to determine the likelihood that one parameter exceeds another, and by how much.\n\nThis technique can be used for dimensioning of mechanical parts, determining when an applied load exceeds the strength of a structure, and in many other situations. This type of analysis can also be used to estimate the \"probability of failure\" or the \"frequency of failure\".\n\nMechanical parts are usually designed to fit precisely together. For example, if a shaft is designed to have a \"sliding fit\" in a hole, the shaft must be a little smaller than the hole. (Traditional tolerances may suggest that all dimensions fall within those intended tolerances. A process capability study of actual production, however, may reveal normal distributions with long tails.) Both the shaft and hole sizes will usually form normal distributions with some average (arithmetic mean) and standard deviation.\n\nWith two such normal distributions, a distribution of interference can be calculated. The derived distribution will also be normal, and its average will be equal to the difference between the means of the two base distributions. The variance of the derived distribution will be the sum of the variances of the two base distributions.\n\nThis derived distribution can be used to determine how often the difference in dimensions will be less than zero (i.e., the shaft cannot fit in the hole), how often the difference will be less than the required sliding gap (the shaft fits, but too tightly), and how often the difference will be greater than the maximum acceptable gap (the shaft fits, but not tightly enough).\n\nPhysical properties and the conditions of use are also inherently variable. For example, the applied load (stress) on a mechanical part may vary. The measured strength of that part (tensile strength, etc.) may also be variable. The part will break when the stress exceeds the strength.\nWith two normal distributions, the statistical interference may be calculated as above. (This problem is also workable for transformed units such as the log-normal distribution). With other distributions, or combinations of different distributions, a Monte Carlo method or simulation is often the most practical way to quantify the effects of statistical interference.\n\n\n"}
{"id": "18093481", "url": "https://en.wikipedia.org/wiki?curid=18093481", "title": "Strong Law of Small Numbers", "text": "Strong Law of Small Numbers\n\nIn mathematics, the \"Strong Law of Small Numbers\" is the humorous title of a popular paper by mathematician Richard K. Guy and also the so-called law that proclaims: \n\nIn other words, any given small number appears in far more contexts than may seem reasonable, leading to many apparently surprising coincidences in mathematics, simply because small numbers appear so often and yet are so few. Guy's paper gives 35 examples in support of this thesis. This can lead inexperienced mathematicians to conclude that these concepts are related, when in fact they are not.\n\nGuy's observation has since become part of mathematical folklore, and is commonly referenced by other authors.\n\nThe original strong law of small numbers was quickly followed by the second strong law of small numbers:\n\nThe second strong law of small numbers emphasizes the fact that two arithmetic functions taking equal values at small arguments do not necessarily coincide.\n\n\n"}
{"id": "40354723", "url": "https://en.wikipedia.org/wiki?curid=40354723", "title": "Structural complexity (applied mathematics)", "text": "Structural complexity (applied mathematics)\n\nStructural complexity is a science of applied mathematics, that aims at relating fundamental physical or biological aspects of a complex system with the mathematical description of the morphological complexity that the system exhibits, by establishing rigorous relations between mathematical and physical properties of such system (Ricca 2005). \n\nStructural complexity emerges from all systems that display morphological organization (Nicolis & Prigogine 1989). Filamentary structures, for instance, are an example of coherent structures that emerge, interact and evolve in many physical and biological systems, such as mass distribution in the Universe, vortex filaments in turbulent flows, neural networks in our brain and genetic material (such as DNA) in a cell. In general information on the degree of morphological disorder present in the system tells us something important about fundamental physical or biological processes.\n\nStructural complexity methods are based on applications of differential geometry and topology (and in particular knot theory) to interpret physical properties of dynamical systems (Abraham & Shaw 1992; Ricca 2009), such as relations between kinetic energy and tangles of vortex filaments in a turbulent flow or magnetic energy and braiding of magnetic fields in the solar corona, including aspects of topological fluid dynamics.\n\n"}
{"id": "14554100", "url": "https://en.wikipedia.org/wiki?curid=14554100", "title": "Substructural type system", "text": "Substructural type system\n\nSubstructural type systems are a family of type systems analogous to substructural logics where one or more of the structural rules are absent or only allowed under controlled circumstances. Such systems are useful for constraining access to system resources such as files, locks and memory by keeping track of changes of state that occur and preventing invalid states.\n\nSeveral type systems have emerged by discarding some of the structural rules of exchange, weakening, and contraction:\n\nThe explanation for affine type systems is best understood if rephrased as “every \"occurrence\" of a variable is used at most once”.\n\nLinear types corresponds to linear logic and ensures that objects are used exactly once, allowing the system to safely deallocate an object after its use.\n\nThe Clean programming language makes use of uniqueness types (a variant of linear types) to help support concurrency, input/output, and in-place update of arrays.\n\nLinear type systems allow references but not aliases. To enforce this, a reference goes out of scope after appearing on the right-hand side of an assignment, thus ensuring that only one reference to any object exists at once. Note that passing a reference as an argument to a function is a form of assignment, as the function parameter will be assigned the value inside the function, and therefore such use of a reference also causes it to go out of scope.\n\nA linear type system is similar to C++'s unique_ptr class, which behaves like a pointer but can only be moved (i.e. not copied) in an assignment. Although the linearity constraint is checked at compile time, dereferencing an invalidated unique_ptr causes undefined behavior at run-time.\n\nThe single-reference property makes linear type systems suitable as programming languages for quantum computation, as it reflects the no-cloning theorem of quantum states. From the category theory point of view, no-cloning is a statement that there is no diagonal functor which could duplicate states; similarly, from the combinator point of view, there is no K-combinator which can destroy states. From the lambda calculus point of view, a variable \"x\" can appear exactly once in a term.\n\nLinear type systems are the internal language of closed symmetric monoidal categories, much in the same way that simply typed lambda calculus is the language of Cartesian closed categories. More precisely, one may construct functors between the category of linear type systems and the category of closed symmetric monoidal categories.\n\nAffine types are a version of linear types allowing to \"discard\" (i.e. \"not use\") a resource, corresponding to affine logic. An affine resource \"can\" only be used once, while a linear one \"must\" be used once.\n\nRelevant types correspond to relevant logic which allows exchange and contraction, but not weakening, which translates to every variable being used at least once.\n\nOrdered types correspond to noncommutative logic where exchange, contraction and weakening are discarded. This can be used to model stack-based memory allocation (contrast with linear types which can be used to model heap-based memory allocation). Without the exchange property, an object may only be used when at the top of the modelled stack, after which it is popped off resulting in every variable being used exactly once in the order it was introduced.\n\nThe following programming languages support linear or affine types:\n\n\n"}
{"id": "4342970", "url": "https://en.wikipedia.org/wiki?curid=4342970", "title": "Surface (mathematics)", "text": "Surface (mathematics)\n\nIn mathematics, a surface is a generalization of a plane which needs not be flat – that is, the curvature is not necessarily zero. This is analogous to a curve generalizing a straight line.\nThere are several more precise definitions, depending on the context and the mathematical tools that are used for the study.\n\nThe mathematical concept of surface is an idealization of what is meant by \"surface\" in common language, science, and computer graphics.\n\nOften, a surface is defined by equations that are satisfied by the coordinates of its points. This is the case of the graph of a continuous function of two variables. The set of the zeros of a function of three variables is a surface, which is called an implicit surface. If the defining three-variate function is a polynomial, the surface is an algebraic surface. For example, the unit sphere is an algebraic surface, as it may be defined by the implicit equation\n\nA surface may also be defined as the image, in some space of dimension at least 3, of a continuous function of two variables (some further conditions are required to insure that the image is not a curve). In this case, one says that one has a parametric surface, which is \"parametrized\" by these two variables, called \"parameters\". For example, the unit sphere may be parametrized by the Euler angles, also called longitude and latitude by \n\nParametric equations of surfaces are often irregular at some points. For example, all but two points of the unit sphere, are the image, by the above parametrization, of exactly one pair of Euler angles (modulo ). For the remaining two points (the north and south poles), one has , and the longitude may take any values. Also, there are surfaces for which there cannot exist a single parametrization that covers the whole surface. Therefore, one often considers surfaces which are parametrized by several parametric equations, whose images cover the surface. This is formalized by the concept of manifold: in the context of manifolds, typically in topology and differential geometry, a surface is a manifold of dimension two; this means that a surface is a topological space such that every point has a neighborhood which is homeomorphic to an open subset of the Euclidean plane (see Surface (topology) and Surface (differential geometry)). This allows defining surfaces in spaces of dimension higher than three, and even \"abstract surfaces\", which are not contained in any other space. On the other hand, this excludes surfaces that have singularities, such as the vertex of a conical surface or points where a surface crosses itself.\n\nIn classical geometry, a surface is generally defined as a locus of a point or a line. For example, a sphere is the locus of a point which is at a given distance of a fixed point, called the center; a conical surface is the locus of a line passing through a fixed point and crossing a curve; a surface of revolution is the locus of a curve rotating around a line. A ruled surface is the locus of a moving line satisfying some constraints; in modern terminology, a ruled surface is a surface, which is a union of lines.\n\nIn this article, several kinds of surfaces are considered and compared. An unambiguous terminology is thus necessary to distinguish them. Therefore, we call topological surfaces the surfaces that are manifolds of dimension two (the surfaces considered in Surface (topology)). We call differential surfaces the surfaces that are differentiable manifolds (the surfaces considered in Surface (differential geometry)). Every differential surface is a topological surface, but the converse is false.\n\nFor simplicity, unless otherwise stated, \"surface\" will mean a surface in the Euclidean space of dimension 3 or in . A surface that is not supposed to be included in another space is called an abstract surface.\n\n\nA parametric surface is the image of an open subset of the Euclidean plane (typically formula_3) by a continuous function, in a topological space, generally a Euclidean space of dimension at least three. Usually the function is supposed to be continuously differentiable, and this will be always the case in this article.\n\nSpecifically, a parametric surface in formula_4 is given by three functions of two variables and , called \"parameters\"\n\nAs the image of such a function may be a curve (for example if the three functions are constant with respect to ), a further condition is required, generally that, for almost all values of the parameters, the Jacobian matrix \nhas rank two. Here \"almost all\" means that the values of the parameters where the rank is two contain a dense open subset of the range of the parametrization. For surfaces in a space of higher dimension, the condition is the same, except for the number of columns of the Jacobian matrix.\n\nA point where the above Jacobian matrix has rank two is called \"regular\", or, more properly, the parametrization is called \"regular\" at .\n\nThe tangent plane at a regular point is the unique plane passing through and having a direction parallel to the two row vectors of the Jacobian matrix. The tangent plane is an affine concept, because its definition is independent of the choice of a metric. In other words, any affine transformation maps the tangent plane to the surface at a point to the tangent plane to the image of the surface at the image of the point.\n\nThe normal line, or simply normal at a point of a surface is the unique line passing through the point and perpendicular to the tangent plane. A normal vector is a vector which is parallel to the normal.\n\nFor other differential invariants of surfaces, in the neighborhood of a point, see Differential geometry of surfaces.\n\nA point of a parametric surface which is not regular is irregular. There are several kinds of irregular points.\n\nIt may occur that an irregular point becomes regular, if one changes the parametrization. This is the case of the poles in the parametrization of the unit sphere by Euler angles: it suffices to permute the role of the different coordinate axes for changing the poles.\n\nOn the other hand, let us consider the circular cone of parametric equation\nThe apex of the cone is the origin , and is obtained for . It is an irregular point that remains irregular, whichever parametrization is chosen (otherwise, there would exist a unique tangent plane). Such an irregular point, where the tangent plane is undefined, is said singular.\n\nThere is another kind of singular points. There are the self-crossing points, that is the points where the surface crosses itself. In other words, these are the points which are obtained for (at least) two different values of the parameters.\n\nLet be a function of two real variables. This is a parametric surface, parametrized as \nEvery point of this surface is regular, as the two first columns of the Jacobian matrix form the identity matrix of rank two.\n\nA rational surface is a surface that may be parametrized by rational functions of two variables. That is, if are, for , polynomials in two indeterminates, then the parametric surface, defined by \nis a rational surface.\n\nA rational surface is an algebraic surface, but most algebraic surfaces are not rational.\n\nAn implicit surface in a Euclidean space (or, more generally, in an affine space) of dimension 3 is the set of the common zeros of a differentiable function of three variables\n\nImplicit means that the equation defines implicitly one of the variables as a function of the other variables. This is made more exact by the implicit function theorem: if , and the partial derivative in of is not zero at , then there exists a differentiable function such that\nin a neighbourhood of . In other words, the implicit surface is the graph of a function near a point of the surface where the partial derivative in is nonzero. An implicit surface has thus, locally, a parametric representation, except at the points of the surface where the three partial derivatives are zero.\n\nA point of the surface where at least one partial derivative of is nonzero is called regular. At such a point formula_12, the tangent plane and the direction of the normal are well defined, and may be deduced, with the implicit function theorem from the definition given above, in . The direction of the normal is the gradient, that is the vector\nThe tangent plane is defined by its implicit equation\n\nA singular point of an implicit surface (in formula_4) is a point of the surface where the implicit equation holds and the three partial derivatives of its defining function are all zero. Therefore, the singular points are the solutions of a system of four equations in three indeterminates. As most such systems have no solution, many surfaces do not have any singular point. A surface with no singular point is called \"regular\" or \"non-singular\".\n\nThe study of surfaces near their singular points and the classification of the singular points is singularity theory. A singular point is isolated if there is no other singular point in a neighborhood of it. Otherwise, the singular points may form a curve. This is in particular the case for self-crossing surfaces.\n\nOriginally, an algebraic surface was a surface which may be defined by an implicit equation\nwhere is a polynomial in three indeterminates, with real coefficients.\n\nThe concept has been extended in several directions, by defining surfaces over arbitrary fields, and by considering surfaces in spaces of arbitrary dimension or in projective spaces. Abstract algebraic surfaces, which are not explicitly embedded in another space, are also considered.\n\nPolynomials with coefficients in any field are accepted for defining an algebraic surface. \nHowever, the field of coefficients of a polynomial is not well defined, as, for example, a polynomial with rational coefficients may also be considered as a polynomial with real or complex coefficients. Therefore, the concept of \"point\" of the surface has been generalized in the following way:\n\nGiven a polynomial , let be the smallest field containing the coefficients, and be an algebraically closed extension of , of infinite transcendence degree. Then a \"point\" of the surface is an element of which is a solution of the equation\nIf the polynomial has real coefficients, the field is the complex field, and a point of the surface that belongs to formula_18 (a usual point) is called a \"real point\". A point that belongs to is called \"rational over \", or simply a \"rational point\", if is the field of rational numbers.\n\nA projective surface in a projective space of dimension three is the set of points whose homogeneous coordinates are zeros of a single homogeneous polynomial in three variables. More generally, a projective surface is a subset of a projective space, which is a projective variety of dimension two.\n\nProjective surfaces are strongly related to affine surfaces (that is, ordinary algebraic surfaces). One passes from a projective surface to the corresponding affine surface by setting to one some coordinate or indeterminate of the defining polynomials (usually the last one). Conversely, one passes from an affine surface to its associated projective surface (called \"projective completion\") by homogenizing the defining polynomial (in case of surfaces in a space of dimension three), or by homogenizing all polynomials of the defining ideal (for surfaces in a space of higher dimension).\n\nOne cannot define the concept of algebraic surface in a space of dimension higher than three without a general definition of an algebraic variety and of the dimension of an algebraic variety. In fact, an algebraic surface is an \"algebraic variety of dimension two\".\n\nMore precisely, an algebraic surface in a space of dimension is the set of the common zeros of at least polynomials, but these polynomials must satisfy further conditions that may be not immediate to verify. Firstly, the polynomials must not define a variety or an algebraic set of higher dimension, which is typically the case if one of the polynomials is in the ideal generated by the others. Generally, polynomials define an algebraic set of dimension two or higher. If the dimension is two, the algebraic set may have several irreducible components. If there is only one component the polynomials define a surface, which is a complete intersection. If there are several components, then one needs further polynomials for selecting a specific component.\n\nMost authors consider as an algebraic surface only algebraic varieties of dimension two, but some also consider as surfaces algebraic sets all of whose irreducible components have the dimension two.\n\nIn the case of surfaces in a space of dimension three, every surface is a complete intersection, and a surface is defined by a single polynomial, which is irreducible or not, depending on whether non-irreducible algebraic sets of dimension two are considered as surfaces or not.\n\nIn topology, a surface is generally defined as a manifold of dimension two. This means that a topological surface is a topological space such that every point has a neighborhood that is homeomorphic to an open subset of a Euclidean plane.\n\nEvery topological surface is homeomorphic to a polyhedral surface such that all facets are triangles. The combinatorial study of such arrangements of triangles (or, more generally, of higher-dimensional simplexes) is the starting object of algebraic topology. This allows the characterization of the properties of surfaces in terms of purely algebraic invariants, such as the genus and homology groups.\n\nThe homeomorphism classes of surfaces have been completely described (see Surface (topology)).\n\n"}
{"id": "8434205", "url": "https://en.wikipedia.org/wiki?curid=8434205", "title": "Table of mathematical symbols by introduction date", "text": "Table of mathematical symbols by introduction date\n\nThe following table lists many specialized symbols commonly used in mathematics, ordered by their introduction date. \n\n\n"}
{"id": "3504251", "url": "https://en.wikipedia.org/wiki?curid=3504251", "title": "Transylvania lottery", "text": "Transylvania lottery\n\nIn mathematical combinatorics, the Transylvanian lottery is a lottery where three numbers between 1 and 14 are picked by the player for any given ticket, and three numbers are chosen randomly. The player wins if two of their numbers, on a given ticket, are among the random ones. The problem of how many tickets the player must buy in order to be certain of winning can be solved by the use of the Fano plane.\n\nThe solution is to buy a total of 14 tickets, in two sets of seven. One set of seven is every line of a Fano plane with the numbers 1-7, the other with 8-14, i.e.:\n\n1-2-5, 1-3-6, 1-4-7, 2-3-7, 2-4-6, 3-4-5, 5-6-7, 8-9-12, 8-10-13, 8-11-14, 9-10-14, 9-11-13, 10-11-12, 12-13-14.\n\nBecause at least two of the winning numbers must be either high (8-14) or low (1-7), and every high and low pair is represented by exactly one ticket, you would be guaranteed at least two correct numbers on one ticket with these 14 purchases. 21/26 of the time you will have one ticket with two numbers matched. If all three winning numbers are either high or low you would either have one ticket with all three numbers (1/26 chance of this occurring), or three different tickets that each matched two (4/26 chance).\n\n\n"}
{"id": "1028194", "url": "https://en.wikipedia.org/wiki?curid=1028194", "title": "Warsaw School (mathematics)", "text": "Warsaw School (mathematics)\n\nWarsaw School of Mathematics is the name given to a group of mathematicians who worked at Warsaw, Poland, in the two decades between the World Wars, especially in the fields of logic, set theory, point-set topology and real analysis. They published in the journal \"Fundamenta Mathematicae\", founded in 1920—one of the world's first specialist pure-mathematics journals. It was in this journal, in 1933, that Alfred Tarski—whose illustrious career would a few years later take him to the University of California, Berkeley—published his celebrated theorem on the undefinability of the notion of truth.\n\nNotable members of the Warsaw School of Mathematics have included:\n\nAdditionally, notable logicians of the Lwów-Warsaw School of Logic, working at Warsaw, have included:\n\nFourier analysis has been advanced at Warsaw by:\n\n"}
{"id": "7561937", "url": "https://en.wikipedia.org/wiki?curid=7561937", "title": "Étienne Ghys", "text": "Étienne Ghys\n\nÉtienne Ghys (born 29 December 1954) is a French mathematician. His research focuses mainly on geometry and dynamical systems, though his mathematical interests are broad. He also expresses much interest in the historical development of mathematical ideas, especially the contributions of Henri Poincaré.\n\nHe co-authored the computer graphics mathematical movie \"Dimensions: A walk through mathematics!\"\n\nAlumnus of the École normale supérieure de Saint-Cloud, he is currently a CNRS \"directeur de recherche\" at the École normale supérieure in Lyon. He is also editor-in-chief of the Publications Mathématiques de l'IHÉS and a member of the French Academy of Sciences.\n\nHe was an invited speaker at the ICM of Kyoto in 1990, and a plenary speaker at the ICM of Madrid in 2006.\n\nIn 2015, he was awarded the inaugural Clay Award for Dissemination of Mathematical Knowledge.\n\nHis doctoral students include Serge Cantat.\n\n"}
