{"id": "173457", "url": "https://en.wikipedia.org/wiki?curid=173457", "title": "9", "text": "9\n\n9 (nine) is the natural number following and preceding .\n\n9 is a composite number, its proper divisors being and . It is 3 times 3 and hence the third square number. Nine is a Motzkin number. It is the first composite lucky number, along with the first composite odd number and only single-digit composite odd number.\n\n9 is the only positive perfect power that is one more than another positive perfect power, by Mihăilescu's Theorem.\n\n9 is the highest single-digit number in the decimal system. It is the second non-unitary square prime of the form (\"p\") and the first that is odd. All subsequent squares of this form are odd.\n\nSince , 9 is an exponential factorial.\n\nA polygon with nine sides is called a nonagon or enneagon. A group of nine of anything is called an ennead.\n\nIn base 10 a positive number is divisible by 9 if and only if its digital root is 9. That is, if any natural number is multiplied by 9, and the digits of the answer are repeatedly added until it is just one digit, the sum will be nine:\n\nThere are other interesting patterns involving multiples of nine:\n\nThis works for all the multiples of 9. is the only other such that a number is divisible by \"n\" if and only if its digital root is divisible by \"n\". In base-\"N\", the divisors of have this property. Another consequence of 9 being , is that it is also a Kaprekar number.\n\nThe difference between a base-10 positive integer and the sum of its digits is a whole multiple of nine. Examples:\n\nCasting out nines is a quick way of testing the calculations of sums, differences, products, and quotients of integers, known as long ago as the 12th century.\n\nSix recurring nines appear in the decimal places 762 through 767 of , see Six nines in pi.\n\nIf dividing a number by the amount of 9s corresponding to its number of digits, the number is turned into a repeating decimal. (e.g. )\n\nThere are nine Heegner numbers.\n\nIn probability, the nine is a logarithmic measure of the probability of an event, defined as the negative of the base- logarithm of the probability of the event's complement. For example, an event that is 99% likely to occur has an unlikelihood of 1% or 0.01, which amounts to of probability. Zero probability gives zero nines . A 100% probability is considered to be impossible in most circumstances: that results in infinite improbability. The effectivity of processes and the availability of systems can be expressed (as a rule of thumb, not explicitly) as a series of \"nines\". For example, \"five nines\" (99.999%) availability implies a total downtime of no more than five minutes per year – typically a very high degree of reliability; but never 100%.\n\nAccording to Georges Ifrah, the origin of the 9 integers can be attributed to ancient Indian civilization, and was adopted by subsequent civilizations in conjunction with the .\nIn the beginning, various Indians wrote 9 similar to the modern closing question mark without the bottom dot. The Kshatrapa, Andhra and Gupta started curving the bottom vertical line coming up with a -look-alike. The Nagari continued the bottom stroke to make a circle and enclose the 3-look-alike, in much the same way that the \"@\" character encircles a lowercase \"a\". As time went on, the enclosing circle became bigger and its line continued beyond the circle downwards, as the 3-look-alike became smaller. Soon, all that was left of the 3-look-alike was a squiggle. The Arabs simply connected that squiggle to the downward stroke at the middle and subsequent European change was purely cosmetic.\n\nWhile the shape of the 9 character has an ascender in most modern typefaces, in typefaces with text figures the character usually has a descender, as, for example, in .\n\nThis numeral resembles an inverted \"6\". To disambiguate the two on objects and documents that can be inverted, the 9 is often underlined, as is done for the 6. Another distinction from the 6 is that it is sometimes handwritten with a straight stem, resembling a raised lower-case letter q.\n\n\n\nNine is a number that appears often in Indian culture and mythology. Some instances are enumerated below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA human pregnancy normally lasts nine months, the basis of Naegele's rule.\n\n\n\n\n\n"}
{"id": "18584082", "url": "https://en.wikipedia.org/wiki?curid=18584082", "title": "Abbe error", "text": "Abbe error\n\nAbbe error, named after Ernst Abbe, also called sine error, describes the magnification of angular error over distance. For example, when one measures a point that is 1 meter away at 45 degrees, an angular error of 1 degree corresponds to a positional error of over 1.745 cm, equivalent to a distance-measurement error of 1.745%. \n\nIn machine design, some components are particularly sensitive to angular errors. For example, slight deviations from parallelism of the spindle axis of a lathe to the tool motion along the bed of the machine can lead to relatively large (undesired) taper along the part (i.e a non-cylindrical part). Vernier calipers are not free from abbe error, while screw gauges are free from abbe error. Abbe error is the product of the abbe offset and the sine of angular error in the system.\n\nAbbe error can be detrimental to dead reckoning.\n\nFormula: \n\nformula_2 the error.\n\nformula_3 the distance.\n\nformula_4 the angle.\n"}
{"id": "35280549", "url": "https://en.wikipedia.org/wiki?curid=35280549", "title": "Alexander Schrijver", "text": "Alexander Schrijver\n\nAlexander (Lex) Schrijver (born 4 May 1948 in Amsterdam) is a Dutch mathematician and computer scientist, a professor of discrete mathematics and optimization at the University of Amsterdam and a fellow at the Centrum Wiskunde & Informatica in Amsterdam. Since 1993 he has been co-editor in chief of the journal \"Combinatorica\".\n\nSchrijver earned his Ph.D. in 1977 from the Vrije Universiteit in Amsterdam, under the supervision of Pieter Cornelis Baayen. He worked for the Centrum Wiskunde & Informatica (under its former name as the Mathematisch Centrum) in pure mathematics from 1973 to 1979, and was a professor at Tilburg University from 1983 to 1989. In 1989 he rejoined the Centrum Wiskunde & Informatica, and in 1990 he also became a professor at the University of Amsterdam. In 2005, he stepped down from management at CWI and instead became a CWI Fellow.\n\nSchrijver was one of the winners of the Delbert Ray Fulkerson Prize of the American Mathematical Society in 1982 for his work with Martin Grötschel and László Lovász on applications of the ellipsoid method to combinatorial optimization; he won the same prize in 2003 for his research on minimization of submodular functions. He won the INFORMS Frederick W. Lanchester Prize in 1986 for his book \"Theory of Linear and Integer Programming\", and again in 2004 for his book \"Combinatorial Optimization: Polyhedra and Efficiency\". In 2003, he won the George B. Dantzig Prize of the Mathematical Programming Society and SIAM for \"deep and fundamental research contributions to discrete optimization\". In 2006, he was a joint winner of the INFORMS John von Neumann Theory Prize with Grötschel and Lovász for their work in combinatorial optimization, and in particular for their joint work in the book \"Geometric Algorithms and Combinatorial Optimization\" showing the polynomial-time equivalence of separation and optimization. In 2008, his work with Adri Steenbeek on scheduling the Dutch train system was honored with INFORMS' Franz Edelman Award for Achievement in Operations Research and the Management Sciences. He won the SIGMA prize of the Dutch SURF foundation in 2008, for a mathematics education project. In 2015 he won the EURO Gold Medal, the highest distinction within Operations Research in Europe.\n\nIn 2005 Schrijver won the Spinoza Prize of the NWO, the highest scientific award in the Netherlands, for his research in combinatorics and algorithms. Later in the same year he became a Knight of the Order of the Netherlands Lion. In 2002, Schrijver received an honorary doctorate from the University of Waterloo in Canada, and in 2011 he received another one from Eötvös Loránd University in Hungary.\n\nSchrijver became a member of the Royal Netherlands Academy of Arts and Sciences in 1995. He became a corresponding member of the North Rhine-Westphalia Academy for Sciences and Arts in 2005, joined the German Academy of Sciences Leopoldina in 2006, and was elected to the Academia Europaea in 2008. In 2012 he became a fellow of the American Mathematical Society.\n\n"}
{"id": "405512", "url": "https://en.wikipedia.org/wiki?curid=405512", "title": "Almost periodic function", "text": "Almost periodic function\n\nIn mathematics, an almost periodic function is, loosely speaking, a function of a real number that is periodic to within any desired level of accuracy, given suitably long, well-distributed \"almost-periods\". The concept was first studied by Harald Bohr and later generalized by Vyacheslav Stepanov, Hermann Weyl and Abram Samoilovitch Besicovitch, amongst others. There is also a notion of almost periodic functions on locally compact abelian groups, first studied by John von Neumann.\n\nAlmost periodicity is a property of dynamical systems that appear to retrace their paths through phase space, but not exactly. An example would be a planetary system, with planets in orbits moving with periods that are not commensurable (i.e., with a period vector that is not proportional to a vector of integers). A theorem of Kronecker from diophantine approximation can be used to show that any particular configuration that occurs once, will recur to within any specified accuracy: if we wait long enough we can observe the planets all return to within a second of arc to the positions they once were in.\n\nThere are several inequivalent definitions of almost periodic functions. The first was given by Harald Bohr. His interest was initially in finite Dirichlet series. In fact by truncating the series for the Riemann zeta function \"ζ\"(\"s\") to make it finite, one gets finite sums of terms of the type\n\nwith \"s\" written as (\"σ\" + \"it\") – the sum of its real part \"σ\" and imaginary part \"it\". Fixing \"σ\", so restricting attention to a single vertical line in the complex plane, we can see this also as\n\nTaking a \"finite\" sum of such terms avoids difficulties of analytic continuation to the region σ < 1. Here the 'frequencies' log \"n\" will not all be commensurable (they are as linearly independent over the rational numbers as the integers \"n\" are multiplicatively independent – which comes down to their prime factorizations).\n\nWith this initial motivation to consider types of trigonometric polynomial with independent frequencies, mathematical analysis was applied to discuss the closure of this set of basic functions, in various norms.\n\nThe theory was developed using other norms by Besicovitch, Stepanov, Weyl, von Neumann, Turing, Bochner and others in the 1920s and 1930s.\n\n defined the uniformly almost-periodic functions as the closure of the trigonometric polynomials with respect to the uniform norm \n(on bounded functions \"f\" on R). In other words, a function \"f\" is uniformly almost periodic if for every \"ε\" > 0 there is a finite linear combination of sine and cosine waves that is of distance less than \"ε\" from \"f\" with respect to the uniform norm. Bohr proved that this definition was equivalent to the existence of a relatively dense set of \"ε\" almost-periods, for all \"ε\" > 0: that is, translations \"T\"(\"ε\") = \"T\" of the variable \"t\" making\n\nAn alternative definition due to Bochner (1926) is equivalent to that of Bohr and is relatively simple to state:\nA function \"f\" is almost periodic if every sequence {\"ƒ\"(\"t\" + \"T\")} of translations of \"f\" has a subsequence that converges uniformly for \"t\" in (−∞, +∞).\n\nThe Bohr almost periodic functions are essentially the same as continuous functions on the Bohr compactification of the reals.\n\nThe space \"S\" of Stepanov almost periodic functions (for \"p\" ≥ 1) was introduced by V.V. . It contains the space of Bohr almost periodic functions. It is the closure of the trigonometric polynomials under the norm\n\nfor any fixed positive value of \"r\"; for different values of \"r\" these norms give the same topology and so the same space of almost periodic functions (though the norm on this space depends on the choice of \"r\").\n\nThe space \"W\" of Weyl almost periodic functions (for \"p\" ≥ 1) was introduced by . It contains the space \"S\" of Stepanov almost periodic functions. \nIt is the closure of the trigonometric polynomials under the seminorm\n\nWarning: there are nonzero functions \"ƒ\" with ||\"ƒ\"|| = 0, such as any bounded function of compact support, so to get a Banach space one has to quotient out by these functions.\n\nThe space \"B\" of Besicovitch almost periodic functions was introduced by .\nIt is the closure of the trigonometric polynomials under the seminorm\n\nWarning: there are nonzero functions \"ƒ\" with ||\"ƒ\"|| = 0, such as any bounded function of compact support, so to get a Banach space one has to quotient out by these functions.\n\nThe Besicovitch almost periodic functions in \"B\" have an expansion (not necessarily convergent) as\n\nwith Σ\"a\" finite and \"λ\" real. Conversely every such series is the expansion of some Besicovitch periodic function (which is not unique).\n\nThe space \"B\" of Besicovitch almost periodic functions (for \"p\" ≥ 1) contains the space \"W\" of Weyl almost periodic functions. If one quotients out a subspace of \"null\" functions, it can be identified with the space of \"L\" functions on the Bohr compactification of the reals.\n\nWith these theoretical developments and the advent of abstract methods (the Peter–Weyl theorem, Pontryagin duality and Banach algebras) a general theory became possible. The general idea of almost-periodicity in relation to a locally compact abelian group \"G\" becomes that of a function \"F\" in \"L\"(\"G\"), such that its translates by \"G\" form a relatively compact set.\nEquivalently, the space of almost periodic functions is the norm closure of the finite linear combinations of characters of \"G\". If \"G\" is compact the almost periodic functions are the same as the continuous functions.\n\nThe Bohr compactification of \"G\" is the compact abelian group of all possibly discontinuous characters of the dual group of \"G\", and is a compact group containing \"G\" as a dense subgroup. The space of uniform almost periodic functions on \"G\" can be identified with the space of all continuous functions on the Bohr compactification of \"G\". More generally the Bohr compactification can be defined for any topological group \"G\", and the spaces of continuous or \"L\" functions on the Bohr compactification can be considered as almost periodic functions on \"G\".\nFor locally compact connected groups \"G\" the map from \"G\" to its Bohr compactification is injective if and only if \"G\" is a central extension of a compact group, or equivalently the product of a compact group and a finite-dimensional vector space.\n\nIn speech processing, audio signal processing, and music synthesis, a quasiperiodic signal, sometimes called a quasiharmonic signal, is a waveform that is virtually periodic microscopically, but not necessarily periodic macroscopically. This does not give a quasiperiodic function in the sense of the Wikipedia article of that name, but something more akin to an almost periodic function, being a nearly periodic function where any one period is virtually identical to its adjacent periods but not necessarily similar to periods much farther away in time. This is the case for musical tones (after the initial attack transient) where all partials or overtones are harmonic (that is all overtones are at frequencies that are an integer multiple of a fundamental frequency of the tone).\n\nWhen a signal formula_9 is fully periodic with period formula_10, then the signal exactly satisfies\n\nor\n\nThe Fourier series representation would be\n\nor\n\nwhere formula_15 is the fundamental frequency and the Fourier coefficients are\n\nThe fundamental frequency formula_20, and Fourier coefficients formula_21, formula_22, formula_23, or formula_24, are constants, i.e. they are not functions of time. The harmonic frequencies are exact integer multiples of the fundamental frequency.\n\nWhen formula_9 is quasiperiodic then\n\nor\n\nwhere\n\nNow the Fourier series representation would be\n\nor\n\nor\n\nwhere formula_32 is the possibly \"time-varying\" fundamental frequency and the Fourier coefficients are\n\nand the instantaneous frequency for each partial is\n\nWhereas in this quasiperiodic case, the fundamental frequency formula_36, the harmonic frequencies formula_37, and the Fourier coefficients formula_38, formula_39, formula_40, or formula_41 are not necessarily constant, and are functions of time albeit \"slowly varying\" functions of time. Stated differently these functions of time are bandlimited to much less than the fundamental frequency for formula_9 to be considered to be quasiperiodic.\n\nThe partial frequencies formula_37 are very nearly harmonic but not necessarily exactly so. The time-derivative of formula_41, that is formula_45, has the effect of detuning the partials from their exact integer harmonic value formula_46. A rapidly changing formula_41 means that the instantaneous frequency for that partial is severely detuned from the integer harmonic value which would mean that formula_9 is not quasiperiodic.\n\n\n"}
{"id": "1736486", "url": "https://en.wikipedia.org/wiki?curid=1736486", "title": "Alpha (finance)", "text": "Alpha (finance)\n\nAlpha is a measure of the active return on an investment, the performance of that investment compared with a suitable market index. An alpha of 1% means the investment's return on investment over a selected period of time was 1% better than the market during that same period; a negative alpha means the investment underperformed the market. Alpha, along with beta, is one of two key coefficients in the capital asset pricing model used in modern portfolio theory and is closely related to other important quantities such as standard deviation, R-squared and the Sharpe ratio.\n\nIn modern financial markets, where index funds are widely available for purchase, alpha is commonly used to judge the performance of mutual funds and similar investments. As these funds include various fees normally expressed in percent terms, the fund has to maintain an alpha greater than its fees in order to provide positive gains compared with an index fund. Historically, the vast majority of traditional funds have had negative alphas, which has led to a flight of capital to index funds and non-traditional hedge funds.\n\nIt is also possible to analyze a portfolio of investments and calculate a theoretical performance, most commonly using the capital asset pricing model (CAPM). Returns on that portfolio can be compared with the theoretical returns, in which case the measure is known as Jensen's alpha. This is useful for non-traditional or highly focused funds, where a single stock index might not be representative of the investment's holdings.\n\nThe alpha coefficient (formula_1) is a parameter in the single index model (SIM). It is the intercept of the security characteristic line (SCL), that is, the coefficient of the constant in a market model regression.\n\nIt can be shown that in an efficient market, the expected value of the alpha coefficient is zero. Therefore, the alpha coefficient indicates how an investment has performed after accounting for the risk it involved:\n\n\nFor instance, although a return of 20% may appear good, the investment can still have a negative alpha if it's involved in an excessively risky position.\n\nIn this context, because returns are being compared with the theoretical return of CAPM and not to a market index, it would be more accurate to use the term of Jensen's alpha.\n\nA belief in efficient markets spawned the creation of market capitalization weighted index funds that seek to replicate the performance of investing in an entire market in the weights that each of the equity securities comprises in the overall market. The best examples for the US are the S&P 500 and the Wilshire 5000 which approximately represent the 500 most widely held equities and the largest 5000 securities respectively, accounting for approximately 80%+ and 99%+ of the total market capitalization of the US market as a whole.\n\nIn fact, to many investors, this phenomenon created a new standard of performance that must be matched: an investment manager should not only avoid losing money for the client and should make a certain amount of money, but in fact should make more money than the passive strategy of investing in everything equally (since this strategy appeared to be statistically more likely to be successful than the strategy of any one investment manager). The name for the additional return above the expected return of the beta adjusted return of the market is called \"Alpha\".\n\nBesides an investment manager simply making more money than a passive strategy, there is another issue:\nalthough the strategy of investing in every stock appeared to perform better than 75 percent of investment managers (see index fund), the price of the stock market as a whole fluctuates up and down, and could be on a downward decline for many years before returning to its previous price.\n\nThe passive strategy appeared to generate the market-beating return over periods of 10 years or more. This strategy may be risky for those who feel they might need to withdraw their money before a 10-year holding period, for example. Thus investment managers who employ a strategy which is less likely to lose money in a particular year are often chosen by those investors who feel that they might need to withdraw their money sooner.\n\nInvestors can use both alpha and beta to judge a manager's performance. If the manager has had a high alpha, but also a high beta, investors might not find that acceptable, because of the chance they might have to withdraw their money when the investment is doing poorly.\n\nThese concepts not only apply to investment managers, but to any kind of investment.\n\n\n"}
{"id": "7500545", "url": "https://en.wikipedia.org/wiki?curid=7500545", "title": "Brunn–Minkowski theorem", "text": "Brunn–Minkowski theorem\n\nIn mathematics, the Brunn–Minkowski theorem (or Brunn–Minkowski inequality) is an inequality relating the volumes (or more generally Lebesgue measures) of compact subsets of Euclidean space. The original version of the Brunn–Minkowski theorem (Hermann Brunn 1887; Hermann Minkowski 1896) applied to convex sets; the generalization to compact nonconvex sets stated here is due to Lazar Lyusternik (1935).\n\nLet \"n\" ≥ 1 and let \"μ\" denote the Lebesgue measure on R. Let \"A\" and \"B\" be two nonempty compact subsets of R. Then the following inequality holds:\n\nwhere \"A\" + \"B\" denotes the Minkowski sum:\n\nThe proof of the Brunn–Minkowski theorem establishes that the function\n\nis concave in the sense that, for every pair of nonempty compact subsets \"A\" and \"B\" of R and every 0 ≤ \"t\" ≤ 1,\n\nFor convex sets \"A\" and \"B\" of positive measure, the inequality in the theorem is strict\nfor 0 < \"t\" < 1 unless \"A\" and \"B\" are positive homothetic, i.e. are equal up to translation and dilation by a positive factor.\n\n\n"}
{"id": "13409455", "url": "https://en.wikipedia.org/wiki?curid=13409455", "title": "Clarkson's inequalities", "text": "Clarkson's inequalities\n\nIn mathematics, Clarkson's inequalities, named after James A. Clarkson, are results in the theory of \"L\" spaces. They give bounds for the \"L\"-norms of the sum and difference of two measurable functions in \"L\" in terms of the \"L\"-norms of those functions individually.\n\nLet (\"X\", Σ, \"μ\") be a measure space; let \"f\", \"g\" : \"X\" → R be measurable functions in \"L\". Then, for 2 ≤ \"p\" < +∞,\n\nFor 1 < \"p\" < 2,\n\nwhere\n\ni.e., \"q\" = \"p\" ⁄ (\"p\" − 1).\n\nThe case \"p\" ≥ 2 is somewhat easier to prove, being a simple application of the triangle inequality and the convexity of\n\n"}
{"id": "294390", "url": "https://en.wikipedia.org/wiki?curid=294390", "title": "Commutative property", "text": "Commutative property\n\nIn mathematics, a binary operation is commutative if changing the order of the operands does not change the result. It is a fundamental property of many binary operations, and many mathematical proofs depend on it. Most familiar as the name of the property that says or , the property can also be used in more advanced settings. The name is needed because there are operations, such as division and subtraction, that do not have it (for example, ); such operations are \"not\" commutative, and so are referred to as \"noncommutative operations\". The idea that simple operations such as the multiplication and addition of numbers are commutative, was for many years implicitly assumed. Thus, this property was not named until the 19th century, when mathematics started to become formalized. A corresponding property exists for binary relations; a binary relation is said to be symmetric if the relation applies regardless of the order of its operands; for example, equality is symmetric as two equal mathematical objects are equal regardless of their order.\n\nThe \"commutative property\" (or \"commutative law\") is a property generally associated with binary operations and functions. If the commutative property holds for a pair of elements under a certain binary operation then the two elements are said to \"commute\" under that operation.\n\nThe term \"commutative\" is used in several related senses.\n\n\nTwo well-known examples of commutative binary operations:\n\n\nSome noncommutative binary operations:\n\nSubtraction is noncommutative, since formula_3.\n\nDivision is noncommutative, since formula_4.\n\nSome truth functions are noncommutative, since the truth tables for the functions are different when one changes the order of the operands. For example, the truth tables for \"f\" (A, B) A Λ ¬B (A AND NOT B) and \"f\" (B, A) B Λ ¬A are\n\nFor the eight noncommutative functions, B\"qp\" = C\"pq\"; M\"qp\" = L\"pq\"; C\"qp\" = B\"pq\"; L\"qp\" = M\"pq\"; F\"qp\" = G\"pq\"; I\"qp\" = H\"pq\"; G\"qp\" = F\"pq\"; H\"qp\" = I\"pq\".\n\nMatrix multiplication is almost always noncommutative, for example:\n\nThe vector product (or cross product) of two vectors in three dimensions is anti-commutative; i.e., \"b\" × \"a\" = −(\"a\" × \"b\").\n\nRecords of the implicit use of the commutative property go back to ancient times. The Egyptians used the commutative property of multiplication to simplify computing products. Euclid is known to have assumed the commutative property of multiplication in his book \"Elements\". Formal uses of the commutative property arose in the late 18th and early 19th centuries, when mathematicians began to work on a theory of functions. Today the commutative property is a well-known and basic property used in most branches of mathematics.\n\nThe first recorded use of the term \"commutative\" was in a memoir by François Servois in 1814, which used the word \"commutatives\" when describing functions that have what is now called the commutative property. The word is a combination of the French word \"commuter\" meaning \"to substitute or switch\" and the suffix \"-ative\" meaning \"tending to\" so the word literally means \"tending to substitute or switch.\" The term then appeared in English in 1838 in Duncan Farquharson Gregory's article entitled \"On the real nature of symbolical algebra\" published in 1840 in the Transactions of the Royal Society of Edinburgh.\n\nIn truth-functional propositional logic, \"commutation\", or \"commutativity\" refer to two valid rules of replacement. The rules allow one to transpose propositional variables within logical expressions in logical proofs. The rules are:\n\nand \n\nwhere \"formula_8\" is a metalogical symbol representing \"can be replaced in a proof with.\"\n\n\"Commutativity\" is a property of some logical connectives of truth functional propositional logic. The following logical equivalences demonstrate that commutativity is a property of particular connectives. The following are truth-functional tautologies.\n\n\nIn group and set theory, many algebraic structures are called commutative when certain operands satisfy the commutative property. In higher branches of mathematics, such as analysis and linear algebra the commutativity of well-known operations (such as addition and multiplication on real and complex numbers) is often used (or implicitly assumed) in proofs.\n\n\nThe associative property is closely related to the commutative property. The associative property of an expression containing two or more occurrences of the same operator states that the order operations are performed in does not affect the final result, as long as the order of terms doesn't change. In contrast, the commutative property states that the order of the terms does not affect the final result.\n\nMost commutative operations encountered in practice are also associative. However, commutativity does not imply associativity. A counterexample is the function\n\nwhich is clearly commutative (interchanging \"x\" and \"y\" does not affect the result), but it is not associative (since, for example, formula_14 but formula_15).\nMore such examples may be found in commutative non-associative magmas.\n\nSome forms of symmetry can be directly linked to commutativity. When a commutative operator is written as a binary function then the resulting function is symmetric across the line \"y = x\". As an example, if we let a function \"f\" represent addition (a commutative operation) so that \"f\"(\"x\",\"y\") = \"x\" + \"y\" then \"f\" is a symmetric function, which can be seen in the adjacent image.\n\nFor relations, a symmetric relation is analogous to a commutative operation, in that if a relation \"R\" is symmetric, then formula_16.\n\nIn quantum mechanics as formulated by Schrödinger, physical variables are represented by linear operators such as \"x\" (meaning multiply by \"x\"), and formula_17. These two operators do not commute as may be seen by considering the effect of their compositions formula_18 and formula_19 (also called products of operators) on a one-dimensional wave function formula_20:\n\nAccording to the uncertainty principle of Heisenberg, if the two operators representing a pair of variables do not commute, then that pair of variables are mutually complementary, which means they cannot be simultaneously measured or known precisely. For example, the position and the linear momentum in the \"x\"-direction of a particle are represented by the operators formula_22 and formula_23, respectively (where formula_24 is the reduced Planck constant). This is the same example except for the constant formula_25, so again the operators do not commute and the physical meaning is that the position and linear momentum in a given direction are complementary.\n\n\n\n\n"}
{"id": "2783513", "url": "https://en.wikipedia.org/wiki?curid=2783513", "title": "Complex dimension", "text": "Complex dimension\n\nIn mathematics, complex dimension usually refers to the dimension of a complex manifold \"M\", or a complex algebraic variety \"V\". If the complex dimension is \"d\", the real dimension will be 2\"d\". That is, the smooth manifold \"M\" has dimension 2\"d\"; and away from any singular point \"V\" will also be a smooth manifold of dimension 2\"d\". \n\nHowever, for a real algebraic variety (that is a variety defined by equations with real coefficients), its dimension refers commonly to its complex dimension, and its real dimension refers to the maximum of the dimensions of the manifolds contained in the set of its real points. The real dimension is not greater than the dimension, and equals it if the variety is irreducible and has real points that are nonsingular.\nFor example, the equation formula_1 defines a variety of (complex) dimension 2 (a surface), but of real dimension 0 — it has only one real point, (0, 0, 0), which is singular.\n\nThe same points apply to codimension. For example a smooth complex hypersurface in complex projective space of dimension \"n\" will be a manifold of dimension 2(\"n\" − 1). A complex hyperplane does not separate a complex projective space into two components, because it has real codimension 2.\n"}
{"id": "20232529", "url": "https://en.wikipedia.org/wiki?curid=20232529", "title": "Davenport–Schinzel sequence", "text": "Davenport–Schinzel sequence\n\nIn combinatorics, a Davenport–Schinzel sequence is a sequence of symbols in which the number of times any two symbols may appear in alternation is limited. The maximum possible length of a Davenport–Schinzel sequence is bounded by the number of its distinct symbols multiplied by a small but nonconstant factor that depends on the number of alternations that are allowed. Davenport–Schinzel sequences were first defined in 1965 by Harold Davenport and Andrzej Schinzel to analyze linear differential equations. Following these sequences and their length bounds have also become a standard tool in discrete geometry and in the analysis of geometric algorithms.\n\nA finite sequence \"U\" = \"u\", \"u\", \"u\", is said to be a Davenport–Schinzel sequence of order \"s\" if it satisfies the following two properties:\n\nFor instance, the sequence\nis a Davenport–Schinzel sequence of order 3: it contains alternating subsequences of length four, such as ...1, ... 2, ... 1, ... 2, ... (which appears in four different ways as a subsequence of the whole sequence) but it does not contain any alternating subsequences of length five.\n\nIf a Davenport–Schinzel sequence of order \"s\" includes \"n\" distinct values, it is called an (\"n\",\"s\") Davenport–Schinzel sequence, or a \"DS\"(\"n\",\"s\")-sequence.\n\nThe complexity of \"DS\"(\"n\",\"s\")-sequence has been analyzed asymptotically in the limit as \"n\" goes to infinity, with the assumption that \"s\" is a fixed constant, and nearly tight bounds are known for all \"s\". Let λ(\"n\") denote the length of the longest \"DS\"(\"n\",\"s\")-sequence. The best bounds known on λ involve the inverse Ackermann function\nwhere \"A\" is the Ackermann function. Due to the very rapid growth of the Ackermann function, its inverse α grows very slowly, and is at most four for problems of any practical size.\n\nUsing big O and big Θ notation, the following bounds are known:\n\nThe value of λ(\"n\") is also known when \"s\" is variable but \"n\" is a small constant:\n\nWhen \"s\" is a function of \"n\" the upper and lower bounds on Davenport-Schinzel sequences are not tight.\n\nThe \"lower envelope\" of a set of functions ƒ(\"x\") of a real variable \"x\" is the function given by their pointwise minimum:\nSuppose that these functions are particularly well behaved: they are all continuous, and any two of them are equal on at most \"s\" values. With these assumptions, the real line can be partitioned into finitely many intervals within which one function has values smaller than all of the other functions. The sequence of these intervals, labeled by the minimizing function within each interval, forms a Davenport–Schinzel sequence of order \"s\". Thus, any upper bound on the complexity of a Davenport–Schinzel sequence of this order also bounds the number of intervals in this representation of the lower envelope.\n\nIn the original application of Davenport and Schinzel, the functions under consideration were a set of different solutions to the same homogeneous linear differential equation of order \"s\". Any two distinct solutions can have at most \"s\" values in common, so the lower envelope of a set of \"n\" distinct solutions forms a \"DS\"(\"n\",\"s\")-sequence.\n\nThe same concept of a lower envelope can also be applied to functions that are only piecewise continuous or that are defined only over intervals of the real line; however, in this case, the points of discontinuity of the functions and the endpoints of the interval within which each function is defined add to the order of the sequence. For instance, a non-vertical line segment in the plane can be interpreted as the graph of a function mapping an interval of \"x\" values to their corresponding \"y\" values, and the lower envelope of a collection of line segments forms a Davenport–Schinzel sequence of order three because any two line segments can form an alternating subsequence with length at most four.\n\n\n\n"}
{"id": "39975535", "url": "https://en.wikipedia.org/wiki?curid=39975535", "title": "Demonic non-determinism", "text": "Demonic non-determinism\n\nA term coined by C.A.R Hoare, which describes the execution of a non-deterministic program where all choices that are made favour non-termination.\n"}
{"id": "141163", "url": "https://en.wikipedia.org/wiki?curid=141163", "title": "Denormal number", "text": "Denormal number\n\nIn computer science, denormal numbers or denormalized numbers (now often called subnormal numbers) fill the underflow gap around zero in floating-point arithmetic. Any non-zero number with magnitude smaller than the smallest normal number is 'subnormal'.\n\nIn a normal floating-point value, there are no leading zeros in the significand; instead leading zeros are moved to the exponent. So 0.0123 would be written as 1.23 × 10. Denormal numbers are numbers where this representation would result in an exponent that is below the minimum exponent (the exponent usually having a limited range).\nSuch numbers are represented using leading zeros in the significand.\n\nThe significand (or mantissa) of an IEEE floating point number is the part of a floating-point number that represents the significant digits. For a positive normalised number it can be represented as \"m\".\"m\"\"m\"\"m\"...\"m\"\"m\" (where \"m\" represents a significant digit and \"p\" is the precision, and \"m\" is non-zero). Notice that for a binary radix, the leading binary digit is always 1. In a denormal number, since the exponent is the least that it can be, zero is the leading significand digit (0.\"m\"\"m\"\"m\"...\"m\"\"m\"), allowing the representation of numbers closer to zero than the smallest normal number. A floating point number may be recognized as denormal whenever its exponent is the least value possible.\n\nBy filling the underflow gap like this, significant digits are lost, but not as abruptly as when using the \"flush to zero on underflow\" approach (discarding all significant digits when underflow is reached). Hence the production of a denormal number is sometimes called gradual underflow because it allows a calculation to lose precision slowly when the result is small.\n\nIn IEEE 754-2008, denormal numbers are renamed \"subnormal numbers\", and are supported in both binary and decimal formats. In binary interchange formats, subnormal numbers are encoded with a biased exponent of 0, but are interpreted with the value of the smallest allowed exponent, which is one greater (i.e., as if it were encoded as a 1). In decimal interchange formats they require no special encoding because the format supports unnormalized numbers directly.\n\nMathematically speaking, the normalized floating point numbers of a given sign are roughly logarithmically spaced, and as such any finite-sized normal float cannot include zero. The denormal floats are a linearly-spaced set of values which span the gap between the negative and positive normal floats.\n\nDenormal numbers provide the guarantee that addition and subtraction of floating-point numbers never underflows; two nearby floating-point numbers always have a representable non-zero difference. Without gradual underflow, the subtraction \"a\"−\"b\" can underflow and produce zero even though the values are not equal. This can, in turn, lead to division by zero errors that cannot occur when gradual underflow is used.\n\nDenormal numbers were implemented in the Intel 8087 while the IEEE 754 standard was being written. They were by far the most controversial feature in the K-C-S format proposal that was eventually adopted, but this implementation demonstrated that denormals could be supported in a practical implementation. Some implementations of floating point units do not directly support denormal numbers in hardware, but rather trap to some kind of software support. While this may be transparent to the user, it can result in calculations which produce or consume denormal numbers being much slower than similar calculations on normal numbers.\n\nSome systems handle denormal values in hardware, in the same way as normal values. Others leave the handling of denormal values to system software, only handling normal values and zero in hardware. Handling denormal values in software always leads to a significant decrease in performance. When denormal values are entirely computed in hardware, implementation techniques exist to allow their processing at speeds comparable to normal numbers; however, the speed of computation is significantly reduced on many modern processors; in extreme cases, instructions involving denormal operands may run as much as 100 times slower.\n\nThis speed difference can be a security risk. Researchers showed that it provides a timing side channel that allows a malicious web site to extract page content from another site inside a web browser.\n\nSome applications need to contain code to avoid denormal numbers, either to maintain accuracy, or in order to avoid the performance penalty in some processors. For instance, in audio processing applications, denormal values usually represent a signal so quiet that it is out of the human hearing range. Because of this, a common measure to avoid denormals on processors where there would be a performance penalty is to cut the signal to zero once it reaches denormal levels or mix in an extremely quiet noise signal. Other methods of preventing denormal numbers include adding a DC offset, quantizing numbers, adding a nyquist signal, etc. Since the SSE2 processor extension, Intel has provided such a functionality in CPU hardware, which rounds denormalized numbers to zero.\n\nIntel's C and Fortran compilers enable the denormals-are-zero (DAZ) and flush-to-zero (FTZ) flags for SSE by default for optimization levels higher than -O0. The effect of DAZ is to treat denormal input arguments to floating point operations as zero, and the effect of FTZ is to return zero instead of a denormal float for operations which would result in a denormal float, even if the input arguments are not themselves denormal. clang and gcc have varying default states depending on platform and optimization level. A non-C99-compliant method of enabling the DAZ and FTZ flags on targets supporting SSE is given below, but is not widely supported. It is known to work on Mac OS X since at least 2006.\n\nFor other SSE instruction-set platforms where the C library has not yet implemented the above flag, the following may work:\n\nThe _MM_SET_DENORMALS_ZERO_MODE macro wraps a better interface for the code above. It allows for switching the mode on or off, while preserving any other configuration in the CSR.\nMost compilers will already provide the previous macro by default, otherwise the following code snippet can be used.\n\nThe default denormal behavior is ABI and therefore well behaved software should save and restore the denormal mode before returning to the caller or calling out to unsuspecting library/OS code.\n\n\nSee also various papers on William Kahan's web site for examples of where denormal numbers help improve the results of calculations.\n"}
{"id": "2549191", "url": "https://en.wikipedia.org/wiki?curid=2549191", "title": "Dynkin system", "text": "Dynkin system\n\nA Dynkin system, named after Eugene Dynkin, is a collection of subsets of another universal set formula_1 satisfying a set of axioms weaker than those of σ-algebra. Dynkin systems are sometimes referred to as λ-systems (Dynkin himself used this term) or d-system. These set families have applications in measure theory and probability.\n\nA major application of λ-systems is the π-λ theorem, see below.\n\nLet Ω be a nonempty set, and let formula_2 be a collection of subsets of Ω (i.e., formula_2 is a subset of the power set of Ω). Then formula_2 is a Dynkin system if\n\nEquivalently, formula_2 is a Dynkin system if\n\nThe second definition is generally preferred as it usually is easier to check.\n\nAn important fact is that a Dynkin system which is also a π-system (i.e., closed under finite intersection) is a σ-algebra. This can be verified by noting that condition 3 and closure under finite intersection implies closure under countable unions.\n\nGiven any collection formula_14 of subsets of formula_1, there exists a unique Dynkin system denoted formula_16 which is minimal with respect to containing formula_17. That is, if formula_18 is any Dynkin system containing formula_17, then formula_20. formula_16 is called the Dynkin system generated by formula_14. Note formula_23. For another example, let formula_24 and formula_25; then formula_26.\n\nIf formula_27 is a π-system and formula_2 is a Dynkin system with formula_29, then formula_30. In other words, the σ-algebra generated by formula_27 is contained in formula_2.\n\nOne application of Dynkin's π-λ theorem is the uniqueness of a measure that evaluates the length of an interval (known as the Lebesgue measure):\n\nLet (Ω, \"B\", \"λ\") be the unit interval [0,1] with the Lebesgue measure on Borel sets. Let μ be another measure on Ω satisfying μ[(\"a\",\"b\")] = \"b\" − \"a\", and let \"D\" be the family of sets \"S\" such that μ[S] = λ[S]. Let \"I\" = { (\"a\",\"b\"),[\"a\",\"b\"),(\"a\",\"b\"],[\"a\",\"b\"] : 0 < \"a\" ≤ \"b\" < 1 }, and observe that \"I\" is closed under finite intersections, that \"I\" ⊂ \"D\", and that \"B\" is the σ-algebra generated by \"I\". It may be shown that \"D\" satisfies the above conditions for a Dynkin-system. From Dynkin's π-λ Theorem it follows that \"D\" in fact includes all of \"B\", which is equivalent to showing that the Lebesgue measure is unique on \"B\".\n\nAdditional applications are in the article on π-systems.\n\n"}
{"id": "56912030", "url": "https://en.wikipedia.org/wiki?curid=56912030", "title": "EURO Gold Medal", "text": "EURO Gold Medal\n\nThe EURO Gold medal of the Association of European Operational Research Societies (EURO) is the most important European scientific prize for operations research.\n\nThe Prize is awarded when a EURO Conference is held (usually twice every three years), to an individual (or sometimes a group) for an outstanding contribution \nto the field of operations research. The Prize is intended to reflect contributions that have stood the test of time, and hence it is awarded for a body of work, rather than a single piece.\n\nThe award is a medal in gold, a diploma, and a citation. The Prize has been awarded since 1985. \n\n\n"}
{"id": "10978612", "url": "https://en.wikipedia.org/wiki?curid=10978612", "title": "Effective dimension", "text": "Effective dimension\n\nIn mathematics, effective dimension is a modification of Hausdorff dimension and other fractal dimensions which places it in a computability theory setting. There are several horse Wang's variations (various notions of effective dimension) of which the most common is effective Hausdorff dimension. Dimension, in mathematics, is a particular way of describing the size of an object (contrasting with measure and other, different, notions of size). Hausdorff dimension generalizes the well-known integer dimensions assigned to points, lines, planes, etc. by allowing one to distinguish between objects of intermediate size between these integer-dimensional objects. For example, fractal subsets of the plane may have intermediate dimension between 1 and 2, as they are \"larger\" than lines or curves, and yet \"smaller\" than filled circles or rectangles. Effective dimension modifies Hausdorff dimension by requiring that objects with small effective dimension be not only small but also locatable (or partially locatable) in a computable sense. As such, objects with large Hausdorff dimension also have large effective dimension, and objects with small effective dimension have small Hausdorff dimension, but an object can have small Hausdorff but large effective dimension. An example is an algorithmically random point on a line, which has Hausdorff dimension 0 (since it is a point) but effective dimension 1 (because, roughly speaking, it can't be effectively localized any better than a small interval, which has Hausdorff dimension 1).\n\nThis article will define effective dimension for subsets of Cantor space 2; closely related definitions exist for subsets of Euclidean space R. We will move freely between considering a set \"X\" of natural numbers, the infinite sequence formula_1 given by the characteristic function of \"X\", and the real number with binary expansion 0.\"X\".\n\nA \"martingale\" on Cantor space 2 is a function \"d\": 2 → R from Cantor space to nonnegative reals which satisfies the fairness condition:\n\nA martingale is thought of as a betting strategy, and the function formula_3 gives the capital of the better after seeing a sequence σ of 0s and 1s. The fairness condition then says that the capital after a sequence σ is the average of the capital after seeing σ0 and σ1; in other words the martingale gives a betting scheme for a bookie with 2:1 odds offered on either of two \"equally likely\" options, hence the name fair.\n\nA \"supermartingale\" on Cantor space is a function \"d\" as above which satisfies a modified fairness condition:\n\nA supermartingale is a betting strategy where the expected capital after a bet is no more than the capital before a bet, in contrast to a martingale where the two are always equal. This allows more flexibility, and is very similar in the non-effective case, since whenever a supermartingale \"d\" is given, there is a modified function \"d\"' which wins at least as much money as \"d\" and which is actually a martingale. However it is useful to allow the additional flexibility once one starts talking about actually giving algorithms to determine the betting strategy, as some algorithms lend themselves more naturally to producing supermartingales than martingales.\n\nAn \"s\"-\"gale\" is a function \"d\" as above of the form\n\nfor \"e\" some martingale.\n\nAn \"s\"-\"supergale\" is a function \"d\" as above of the form\n\nfor \"e\" some supermartingale.\n\nAn \"s\"-(super)gale is a betting strategy where some amount of capital is lost to inflation at each step. Note that \"s\"-gales and \"s\"-supergales are examples of supermartingales, and the 1-gales and 1-supergales are precisely the martingales and supermartingales.\n\nCollectively, these objects are known as \"gales\".\n\nA gale \"d\" \"succeeds\" on a subset \"X\" of the natural numbers if formula_7 where formula_8 denotes the \"n\"-digit string consisting of the first \"n\" digits of \"X\".\n\nA gale \"d\" \"succeeds strongly\" on \"X\" if formula_9.\n\nAll of these notions of various gales have no effective content, but one must necessarily restrict oneself to a small class of gales, since some gale can be found which succeeds on any given set. After all, if one knows a sequence of coin flips in advance, it is easy to make money by simply betting on the known outcomes of each flip. A standard way of doing this is to require the gales to be either computable or close to computable:\n\nA gale \"d\" is called \"constructive\", \"c.e.\", or \"lower semi-computable\" if the numbers formula_3 are uniformly left-c.e. reals (i.e. can uniformly be written as the limit of an increasing computable sequence of rationals).\n\nThe effective Hausdorff dimension of a set of natural numbers \"X\" is formula_11.\n\nThe effective packing dimension of \"X\" is formula_12.\n\nKolmogorov complexity can be thought of as a lower bound on the algorithmic compressibility of a finite sequence (of characters or binary digits). It assigns to each such sequence \"w\" a natural number \"K(w)\" that, intuitively, measures the minimum length of a computer program (written in some fixed programming language) that takes no input and will output \"w\" when run.\n\nThe effective Hausdorff dimension of a set of natural numbers \"X\" is formula_13.\n\nThe effective packing dimension of a set \"X\" is formula_14.\n\nFrom this one can see that both the effective Hausdorff dimension and the effective packing dimension of a set are between 0 and 1, with the effective packing dimension always at least as large as the effective Hausdorff dimension. Every random sequence will have effective Hausdorff and packing dimensions equal to 1, although there are also nonrandom sequences with effective Hausdorff and packing dimensions of 1.\n\nIf \"Z\" is a subset of 2, its Hausdorff dimension is formula_15.\n\nThe packing dimension of \"Z\" is formula_16.\n\nThus the effective Hausdorff and packing dimensions of a set formula_17 are simply the classical Hausdorff and packing dimensions of formula_18 (respectively) when we restrict our attention to c.e. gales.\n\nDefine the following:\nA consequence of the above is that these all have Hausdorff dimension formula_25.\n\nformula_26 and formula_27 all have packing dimension 1.\n\nformula_28 and formula_29 all have packing dimension formula_25.\n\n"}
{"id": "54434627", "url": "https://en.wikipedia.org/wiki?curid=54434627", "title": "Eric Katz", "text": "Eric Katz\n\nEric Katz is a mathematician working in combinatorial algebraic geometry and arithmetic geometry. He is currently an assistant professor in the Department of Mathematics at Ohio State University.\n\nIn joint work with Karim Adiprasito and June Huh, he resolved the Heron–Rota–Welsh conjecture on the log-concavity of the characteristic polynomial of matroids. With Joseph Rabinoff and David Zureick-Brown, he has given bounds on rational and torsion points on curves.\n\nKatz went to Beachwood High School. He obtained his Ph.D. from Stanford University in 2004 with a thesis written under the direction of Yakov Eliashberg and Ravi Vakil.\n"}
{"id": "1526304", "url": "https://en.wikipedia.org/wiki?curid=1526304", "title": "Financial risk management", "text": "Financial risk management\n\nFinancial risk management is the practice of economic value in a firm by using financial instruments to manage exposure to risk: operational risk, credit risk and market risk, foreign exchange risk, shape risk, volatility risk, liquidity risk, inflation risk, business risk, legal risk, reputational risk, sector risk etc. Similar to general risk management, financial risk management requires identifying its sources, measuring it, and plans to address them.\n\nFinancial risk management can be qualitative and quantitative. As a specialization of risk management, financial risk management focuses on when and how to hedge using financial instruments to manage costly exposures to risk.\n\nIn the banking sector worldwide, the Basel Accords are generally adopted by internationally active banks for tracking, reporting and exposing operational, credit and market risks.\n\nFinance theory (i.e., financial economics) prescribes that a firm should take on a project if it increases shareholder value. Finance theory also shows that firm managers cannot create value for shareholders, also called its investors, by taking on projects that shareholders could do for themselves at the same cost.\n\nWhen applied to financial risk management, this implies that firm managers should not hedge risks that investors can hedge for themselves at the same cost. This notion was captured by the so-called \"hedging irrelevance proposition\": \"In a perfect market, the firm cannot create value by hedging a risk when the price of bearing that risk within the firm is the same as the price of bearing it outside of the firm.\" In practice, financial markets are not likely to be perfect markets.\n\nThis suggests that firm managers likely have many opportunities to create value for shareholders using financial risk management, wherein they have to determine which risks are cheaper for the firm to manage than the shareholders. Market risks that result in unique risks for the firm are commonly the best candidates for financial risk management.\n\nThe concepts of financial risk management change dramatically in the international realm. Multinational Corporations are faced with many different obstacles in overcoming these challenges. There has been some research on the risks firms must consider when operating in many countries, such as the three kinds of foreign exchange exposure for various future time horizons: transactions exposure, accounting exposure, and economic exposure.\n\nFRM® (Certified Financial Risk Manager Program) is an international professional certification offered by GARP (The Global Association of Risk Professionals). FRM certificants are to be found in more than 190 countries and territories worldwide. Successful candidates take an average of two years to earn their FRM Certification.<ref name=\"FRM Q/A\"> GARP Frequently-Asked-Questions -EXAM regulations-, http://www.garp.org/#!/frm/frequently-asked-questions</ref> FRMs are employed at major banks (Bank of America, Bank of China, ICBC...) and corporates (Goldman Sachs, KPMG, Deloitte, PIMCO, JP Morgan, BlackRock..).\nThe FRM curriculum is updated annually by risk professionals employed internationally at major banks, asset management firms, hedge funds, consulting firms, and regulators. The Exam curriculum:\n\n\n"}
{"id": "47526601", "url": "https://en.wikipedia.org/wiki?curid=47526601", "title": "Flip graph", "text": "Flip graph\n\nA flip graph is a graph whose vertices are combinatorial or geometric objects, and whose edges link two of these objects when they can be obtained from one another by an elementary operation called a flip. Flip graphs are special cases of geometric graphs.\n\nAmong noticeable flip graphs, one finds the 1-skeleton of polytopes such as associahedra or cyclohedra.\n\nA prototypical flip graph is that of a convex formula_1-gon formula_2. The vertices of this graph are the triangulations of formula_2, and two triangulations are adjacent in it whenever they differ by a single interior edge. In this case, the flip operation consists in exchanging the diagonals of a convex quadrilateral. These diagonals are the interior edges by which two triangulations adjacent in the flip graph differ. The resulting flip graph is both the Hasse diagram of the Tamari lattice and the 1-skeleton of the formula_4-dimensional associahedron.\n\nThis basic construction can be generalized in a number of ways.\n\nLet formula_5 be a triangulation of a finite set of points formula_6. Under some conditions, one may transform formula_5 into another triangulation of formula_8 by a flip. This operation consists in modifying the way formula_5 triangulates a circuit (a minimally affinely dependent subset of formula_8). More precisely, if some triangulation formula_11 of a circuit formula_12 is a subset of formula_5, and if all the cells (faces of maximal dimension) of formula_11 have the same link formula_15 in formula_5, then one can perform a flip within formula_5 by replacing formula_18 by formula_19, where\n\nand formula_21 is, by Radon's partition theorem, the unique other triangulation of formula_22. The conditions just stated, under which a flip is possible, make sure that this operation results in a triangulation of formula_8. The corresponding flip graph, whose vertices are the triangulations of formula_8 and whose edges correspond to flips between them, is a natural generalization of the flip graph of a convex polygon, as the two flip graphs coincide when formula_8 is the set of the vertices of a convex formula_1-gon.\n\nAnother kind of flip graphs is obtained by considering the triangulations of a topological surface: consider such a surface formula_27, place a finite number formula_1 of points on it, and connect them by arcs in such a way that any two arcs never cross. When this set of arcs is maximal, it decomposes formula_27 into triangles. If in addition there are no multiple arcs (distinct arcs with the same pair of vertices), nor loops, this set of arcs defines a triangulation of formula_27.\n\nIn this setting, two triangulations of formula_27 that can be obtained from one another by a continuous transformation are identical.\n\nTwo triangulations are related by a flip when they differ by exactly one of the arcs they are composed of. Note that, these two triangulations necessarily have the same number of vertices. As in the Euclidean case, the flip graph of formula_27 is the graph whose vertices are the triangulations of formula_27 with formula_1 vertices and whose edges correspond to flips between them. This definition can be straightforwardly extended to bordered topological surfaces.\nThe flip graph of a surface generalises that of a formula_1-gon, as the two coincide when the surface is a topological disk with formula_1 points placed on its boundary.\n\nA number of other flip graphs can be defined using alternative definitions of a triangulation. For instance, the flip graph whose vertices are the centrally-symmetric triangulations of a formula_37-gon and whose edges correspond to the operation of doing two centrally-symmetric flips is the 1-skeleton of the formula_38-dimensional cyclohedron. One can also consider an alternative flip graph of a topological surface, defined by allowing multiple arcs and loops in the triangulations of this surface.\n\nFlip graphs may also be defined using combinatorial objects other than triangulations. An example of such combinatorial objects are the domino tilings of a given region in the plane. In this case, a flip can be performed when two adjacent dominos cover a square: it consists in rotating these dominos by 90 degrees around the center of the square, resulting in a different domino tiling of the same region.\n\nApart from associahedra and cyclohedra, a number of polytopes have the property that their 1-skeleton is a flip graph. For instance, if formula_8 is a finite set of points in formula_40, the regular triangulations of formula_8 are the ones that can be obtained by projecting some faces of a formula_42-dimensional polytope on formula_40. The subgraph induced by these triangulations in the flip graph of formula_8 is the 1-skeleton of a polytope, the secondary polytope of formula_8.\n\nPolytopal flip graphs are, by this property, connected. As shown by Klaus Wagner in the 1930s, the flip graph of the topological sphere is connected. Among the connected flip graphs, one also finds the flip graphs of any finite 2-dimensional set of points. In higher dimensional Euclidean spaces, the situation is much more complicated. Finite sets of points of formula_40 with disconnected flip graphs have been found whenever formula_38 is at least 5.\n\nIt is yet unknown whether the flip graphs of finite 3- and 4-dimensional sets of points are always connected or not.\n"}
{"id": "51414", "url": "https://en.wikipedia.org/wiki?curid=51414", "title": "Fundamental theorem of algebra", "text": "Fundamental theorem of algebra\n\nThe fundamental theorem of algebra states that every non-constant single-variable polynomial with complex coefficients has at least one complex root. This includes polynomials with real coefficients, since every real number is a complex number with an imaginary part equal to zero.\n\nEquivalently (by definition), the theorem states that the field of complex numbers is algebraically closed.\n\nThe theorem is also stated as follows: every non-zero, single-variable, degree \"n\" polynomial with complex coefficients has, counted with multiplicity, exactly \"n\" complex roots. The equivalence of the two statements can be proven through the use of successive polynomial division.\n\nIn spite of its name, there is no purely algebraic proof of the theorem, since any proof must use some form of completeness, which is not an algebraic concept. Additionally, it is not fundamental for modern algebra; its name was given at a time when algebra was synonymous with theory of equations.\n\nPeter Roth, in his book \"Arithmetica Philosophica\" (published in 1608, at Nürnberg, by Johann Lantzenberger), wrote that a polynomial equation of degree \"n\" (with real coefficients) \"may\" have \"n\" solutions. Albert Girard, in his book \"L'invention nouvelle en l'Algèbre\" (published in 1629), asserted that a polynomial equation of degree \"n\" has \"n\" solutions, but he did not state that they had to be real numbers. Furthermore, he added that his assertion holds \"unless the equation is incomplete\", by which he meant that no coefficient is equal to 0. However, when he explains in detail what he means, it is clear that he actually believes that his assertion is always true; for instance, he shows that the equation formula_1 although incomplete, has four solutions (counting multiplicities): 1 (twice), formula_2 and formula_3\n\nAs will be mentioned again below, it follows from the fundamental theorem of algebra that every non-constant polynomial with real coefficients can be written as a product of polynomials with real coefficients whose degree are either 1 or 2. However, in 1702 Leibniz said that no polynomial of the type \"x\" + \"a\" (with \"a\" real and distinct from 0) can be written in such a way. Later, Nikolaus Bernoulli made the same assertion concerning the polynomial \"x\" − 4\"x\" + 2\"x\" + 4\"x\" + 4, but he got a letter from Euler in 1742 in which he was told that his polynomial happened to be equal to\n\nAlso, Euler mentioned that\n\nA first attempt at proving the theorem was made by d'Alembert in 1746, but his proof was incomplete. Among other problems, it assumed implicitly a theorem (now known as Puiseux's theorem) which would not be proved until more than a century later, and furthermore the proof assumed the fundamental theorem of algebra. Other attempts were made by Euler (1749), de Foncenex (1759), Lagrange (1772), and Laplace (1795). These last four attempts assumed implicitly Girard's assertion; to be more precise, the existence of solutions was assumed and all that remained to be proved was that their form was \"a\" + \"bi\" for some real numbers \"a\" and \"b\". In modern terms, Euler, de Foncenex, Lagrange, and Laplace were assuming the existence of a splitting field of the polynomial \"p\"(\"z\").\n\nAt the end of the 18th century, two new proofs were published which did not assume the existence of roots, but neither of which was complete. One of them, due to James Wood and mainly algebraic, was published in 1798 and it was totally ignored. Wood's proof had an algebraic gap. The other one was published by Gauss in 1799 and it was mainly geometric, but it had a topological gap, filled by Alexander Ostrowski in 1920, as discussed in Smale 1981 (Smale writes, \"...I wish to point out what an immense gap Gauss' proof contained. It is a subtle point even today that a real algebraic plane curve cannot enter a disk without leaving. In fact even though Gauss redid this proof 50 years later, the gap remained. It was not until 1920 that Gauss' proof was completed. In the reference Gauss, A. Ostrowski has a paper which does this and gives an excellent discussion of the problem as well...\"). A rigorous proof was first published by Argand in 1806 (and revisited in 1813); it was here that, for the first time, the fundamental theorem of algebra was stated for polynomials with complex coefficients, rather than just real coefficients. Gauss produced two other proofs in 1816 and another version of his original proof in 1849.\n\nThe first textbook containing a proof of the theorem was Cauchy's \"Cours d'analyse de l'École Royale Polytechnique\" (1821). It contained Argand's proof, although Argand is not credited for it.\n\nNone of the proofs mentioned so far is constructive. It was Weierstrass who raised for the first time, in the middle of the 19th century, the problem of finding a constructive proof of the fundamental theorem of algebra. He presented his solution, that amounts in modern terms to a combination of the Durand–Kerner method with the homotopy continuation principle, in 1891. Another proof of this kind was obtained by Hellmuth Kneser in 1940 and simplified by his son Martin Kneser in 1981.\n\nWithout using countable choice, it is not possible to constructively prove the fundamental theorem of algebra for complex numbers based on the Dedekind real numbers (which are not constructively equivalent to the Cauchy real numbers without countable choice). However, Fred Richman proved a reformulated version of the theorem that does work.\n\nAll proofs below involve some analysis, or at least the topological concept of continuity of real or complex functions. Some also use differentiable or even analytic functions. This fact has led to the remark that the Fundamental Theorem of Algebra is neither fundamental, nor a theorem of algebra.\n\nSome proofs of the theorem only prove that any non-constant polynomial with real coefficients has some complex root. This is enough to establish the theorem in the general case because, given a non-constant polynomial \"p\"(\"z\") with complex coefficients, the polynomial\n\nhas only real coefficients and, if \"z\" is a zero of \"q\"(\"z\"), then either \"z\" or its conjugate is a root of \"p\"(\"z\").\n\nA large number of non-algebraic proofs of the theorem use the fact (sometimes called \"growth lemma\") that an \"n\"-th degree polynomial function \"p\"(\"z\") whose dominant coefficient is 1 behaves like \"z\" when |\"z\"| is large enough. A more precise statement is: there is some positive real number \"R\" such that:\n\nwhen |\"z\"| > \"R\".\n\nFind a closed disk \"D\" of radius \"r\" centered at the origin such that |\"p\"(\"z\")| > |\"p\"(0)| whenever |\"z\"| ≥ \"r\". The minimum of |\"p\"(\"z\")| on \"D\", which must exist since \"D\" is compact, is therefore achieved at some point \"z\" in the interior of \"D\", but not at any point of its boundary. The Maximum modulus principle (applied to 1/\"p\"(\"z\")) implies then that \"p\"(\"z\") = 0. In other words, \"z\" is a zero of \"p\"(\"z\").\n\nA variation of this proof does not require the use of the maximum modulus principle (in fact, the same argument with minor changes also gives a proof of the maximum modulus principle for holomorphic functions). If we assume by contradiction that \"a\" := \"p\"(\"z\") ≠ 0, then, expanding \"p\"(\"z\") in powers of \"z\" − \"z\" we can write\n\nHere, the \"c\" are simply the coefficients of the polynomial \"z\" → \"p\"(\"z\" + \"z\"), and we let \"k\" be the index of the first coefficient following the constant term that is non-zero. But now we see that for \"z\" sufficiently close to \"z\" this has behavior asymptotically similar to the simpler polynomial formula_9, in the sense that (as is easy to check) the function:\n\nis bounded by some positive constant \"M\" in some neighborhood of \"z\". Therefore if we define formula_11 and let formula_12, then for any sufficiently small positive number \"r\" (so that the bound \"M\" mentioned above holds), using the triangle inequality we see that\n\nWhen \"r\" is sufficiently close to 0 this upper bound for |\"p\"(\"z\")| is strictly smaller than |\"a\"|, in contradiction to the definition of \"z\". (Geometrically, we have found an explicit direction θ such that if one approaches \"z\" from that direction one can obtain values \"p\"(\"z\") smaller in absolute value than |\"p\"(\"z\")|.)\n\nAnother analytic proof can be obtained along this line of thought observing that, since |\"p\"(\"z\")| > |\"p\"(0)| outside \"D\", the minimum of |\"p\"(\"z\")| on the whole complex plane is achieved at \"z\". If |\"p\"(\"z\")| > 0, then 1/\"p\" is a bounded holomorphic function in the entire complex plane since, for each complex number \"z\", |1/\"p\"(\"z\")| ≤ |1/\"p\"(\"z\")|. Applying Liouville's theorem, which states that a bounded entire function must be constant, this would imply that 1/\"p\" is constant and therefore that \"p\" is constant. This gives a contradiction, and hence \"p\"(\"z\") = 0.\n\nYet another analytic proof uses the argument principle. Let \"R\" be a positive real number large enough so that every root of \"p\"(\"z\") has absolute value smaller than \"R\"; such a number must exist because every non-constant polynomial function of degree \"n\" has at most \"n\" zeros. For each \"r\" > \"R\", consider the number\n\nwhere \"c\"(\"r\") is the circle centered at 0 with radius \"r\" oriented counterclockwise; then the argument principle says that this number is the number \"N\" of zeros of \"p\"(\"z\") in the open ball centered at 0 with radius \"r\", which, since \"r\" > \"R\", is the total number of zeros of \"p\"(\"z\"). On the other hand, the integral of \"n\"/\"z\" along \"c\"(\"r\") divided by 2π\"i\" is equal to \"n\". But the difference between the two numbers is\n\nThe numerator of the rational expression being integrated has degree at most \"n\" - 1 and the degree of the denominator is \"n\" + 1. Therefore, the number above tends to 0 as \"r\" → +∞. But the number is also equal to \"N\" − \"n\" and so \"N\" = \"n\".\n\nStill another complex-analytic proof can be given by combining linear algebra with the Cauchy theorem. To establish that every complex polynomial of degree \"n\" > 0 has a zero, it suffices to show that every complex square matrix of size \"n\" > 0 has a (complex) eigenvalue. The proof of the latter statement is by contradiction.\n\nLet \"A\" be a complex square matrix of size \"n\" > 0 and let \"I\" be the unit matrix of the same size. Assume \"A\" has no eigenvalues. Consider the resolvent function\n\nwhich is a meromorphic function on the complex plane with values in the vector space of matrices. The eigenvalues of \"A\" are precisely the poles of \"R\"(\"z\"). Since, by assumption, \"A\" has no eigenvalues, the function \"R\"(\"z\") is an entire function and Cauchy theorem implies that\n\nOn the other hand, \"R\"(\"z\") expanded as a geometric series gives:\n\nThis formula is valid outside the closed disc of radius formula_19 (the operator norm of \"A\"). Let formula_20 Then\n\n(in which only the summand \"k\" = 0 has a nonzero integral). This is a contradiction, and so \"A\" has an eigenvalue.\n\nFinally, Rouché's theorem gives perhaps the shortest proof of the theorem.\n\nSuppose the minimum of |\"p\"(\"z\")| on the whole complex plane is achieved at \"z\"; it was seen at the proof which uses Liouville's theorem that such a number must exist. We can write \"p\"(\"z\") as a polynomial in \"z\" − \"z\": there is some natural number \"k\" and there are some complex numbers \"c\", \"c\", ..., \"c\" such that \"c\" ≠ 0 and:\n\nIf \"p\"(\"z\") is nonzero, it follows that if \"a\" is a \"k\" root of −\"p\"(\"z\")/\"c\" and if \"t\" is positive and sufficiently small, then |\"p\"(\"z\" + \"ta\")| < |\"p\"(\"z\")|, which is impossible, since |\"p\"(\"z\")| is the minimum of |\"p\"| on \"D\".\n\nFor another topological proof by contradiction, suppose that the polynomial \"p\"(\"z\") has no roots, and consequently is never equal to 0. Think of the polynomial as a map from the complex plane into the complex plane. It maps any circle |\"z\"| = \"R\" into a closed loop, a curve \"P\"(\"R\"). We will consider what happens to the winding number of \"P\"(\"R\") at the extremes when \"R\" is very large and when \"R\" = 0. When \"R\" is a sufficiently large number, then the leading term \"z\" of \"p\"(\"z\") dominates all other terms combined; in other words, \n\nWhen \"z\" traverses the circle formula_24 once counter-clockwise formula_25 then formula_26 winds \"n\" times counter-clockwise formula_27 around the origin (0,0), and \"P\"(\"R\") likewise. At the other extreme, with |\"z\"| = 0, the curve \"P\"(0) is merely the single point \"p\"(0), which must be nonzero because \"p\"(\"z\") is never zero. Thus \"p\"(0) must be distinct from the origin (0,0), which denotes 0 in the complex plane. The winding number of \"P\"(0) around the origin (0,0) is thus 0. Now changing \"R\" continuously will deform the loop continuously. At some \"R\" the winding number must change. But that can only happen if the curve \"P\"(\"R\") includes the origin (0,0) for some \"R\". But then for some \"z\" on that circle |\"z\"| = \"R\" we have \"p\"(\"z\") = 0, contradicting our original assumption. Therefore, \"p\"(\"z\") has at least one zero.\n\nThese proofs use two facts about real numbers that require only a small amount of analysis (more precisely, the intermediate value theorem):\n\nThe second fact, together with the quadratic formula, implies the theorem for real quadratic polynomials. In other words, algebraic proofs of the fundamental theorem actually show that if \"R\" is any real-closed field, then its extension \"C\" = \"R\"() is algebraically closed.\n\nAs mentioned above, it suffices to check the statement \"every non-constant polynomial \"p\"(\"z\") with real coefficients has a complex root\". This statement can be proved by induction on the greatest non-negative integer \"k\" such that 2 divides the degree \"n\" of \"p\"(\"z\"). Let \"a\" be the coefficient of \"z\" in \"p\"(\"z\") and let \"F\" be a splitting field of \"p\"(\"z\") over \"C\"; in other words, the field \"F\" contains \"C\" and there are elements \"z\", \"z\", ..., \"z\" in \"F\" such that\n\nIf \"k\" = 0, then \"n\" is odd, and therefore \"p\"(\"z\") has a real root. Now, suppose that \"n\" = 2\"m\" (with \"m\" odd and \"k\" > 0) and that the theorem is already proved when the degree of the polynomial has the form 2\"m\"′ with \"m\"′ odd. For a real number \"t\", define:\n\nThen the coefficients of \"q\"(\"z\") are symmetric polynomials in the \"z\" with real coefficients. Therefore, they can be expressed as polynomials with real coefficients in the elementary symmetric polynomials, that is, in −\"a\", \"a\", ..., (−1)\"a\". So \"q\"(\"z\") has in fact \"real\" coefficients. Furthermore, the degree of \"q\"(\"z\") is \"n\"(\"n\" − 1)/2 = 2\"m\"(\"n\" − 1), and \"m\"(\"n\" − 1) is an odd number. So, using the induction hypothesis, \"q\" has at least one complex root; in other words, \"z\" + \"z\" + \"tzz\" is complex for two distinct elements \"i\" and \"j\" from {1, ..., \"n\"}. Since there are more real numbers than pairs (\"i\", \"j\"), one can find distinct real numbers \"t\" and \"s\" such that \"z\" + \"z\" + \"tzz\" and \"z\" + \"z\" + \"szz\" are complex (for the same \"i\" and \"j\"). So, both \"z\" + \"z\" and \"zz\" are complex numbers. It is easy to check that every complex number has a complex square root, thus every complex polynomial of degree 2 has a complex root by the quadratic formula. It follows that \"z\" and \"z\" are complex numbers, since they are roots of the quadratic polynomial \"z\" −  (\"z\" + \"z\")\"z\" + \"zz\".\n\nJoseph Shipman showed in 2007 that the assumption that odd degree polynomials have roots is stronger than necessary; any field in which polynomials of prime degree have roots is algebraically closed (so \"odd\" can be replaced by \"odd prime\" and furthermore this holds for fields of all characteristics). For axiomatization of algebraically closed fields, this is the best possible, as there are counterexamples if a single prime is excluded. However, these counterexamples rely on −1 having a square root. If we take a field where −1 has no square root, and every polynomial of degree \"n\" ∈ \"I\" has a root, where \"I\" is any fixed infinite set of odd numbers, then every polynomial \"f\"(\"x\") of odd degree has a root (since has a root, where \"k\" is chosen so that ). Mohsen Aliabadi generalized Shipman's result for any field in 2013, proving that the sufficient condition for an arbitrary field (of any characteristic) to be algebraically closed is having a root for any polynomial of prime degree.\n\nAnother algebraic proof of the fundamental theorem can be given using Galois theory. It suffices to show that C has no proper finite field extension. Let \"K\"/C be a finite extension. Since the normal closure of \"K\" over R still has a finite degree over C (or R), we may assume without loss of generality that \"K\" is a normal extension of R (hence it is a Galois extension, as every algebraic extension of a field of characteristic 0 is separable). Let \"G\" be the Galois group of this extension, and let \"H\" be a Sylow 2-subgroup of \"G\", so that the order of \"H\" is a power of 2, and the index of \"H\" in \"G\" is odd. By the fundamental theorem of Galois theory, there exists a subextension \"L\" of \"K\"/R such that Gal(\"K\"/\"L\") = \"H\". As [\"L\":R] = [\"G\":\"H\"] is odd, and there are no nonlinear irreducible real polynomials of odd degree, we must have \"L\" = R, thus [\"K\":R] and [\"K\":C] are powers of 2. Assuming by way of contradiction that [\"K\":C] > 1, we conclude that the 2-group Gal(\"K\"/C) contains a subgroup of index 2, so there exists a subextension \"M\" of C of degree 2. However, C has no extension of degree 2, because every quadratic complex polynomial has a complex root, as mentioned above. This shows that [\"K\":C] = 1, and therefore \"K\" = C, which completes the proof.\n\nThere exists still another way to approach the fundamental theorem of algebra, due to J. M. Almira and A. Romero: by Riemannian geometric arguments. The main idea here is to prove that the existence of a non-constant polynomial \"p\"(\"z\") without zeros implies the existence of a flat Riemannian metric over the sphere S. This leads to a contradiction, since the sphere is not flat.\n\nA Riemannian surface (\"M\", \"g\") is said to be flat if its Gaussian curvature, which we denote by \"K\", is identically null. Now, Gauss–Bonnet theorem, when applied to the sphere S, claims that\n\nwhich proves that the sphere is not flat.\n\nLet us now assume that \"n\" > 0 and \n\nfor each complex number \"z\". Let us define \n\nObviously, \"p*\"(\"z\") ≠ 0 for all \"z\" in C. Consider the polynomial \"f\"(\"z\") = \"p\"(\"z\")\"p*\"(\"z\"). Then \"f\"(\"z\") ≠ 0 for each \"z\" in C. Furthermore,\n\nWe can use this functional equation to prove that \"g\", given by\n\nfor \"w\" in C, and\n\nfor \"w\" ∈ S\\{0}, is a well defined Riemannian metric over the sphere S (which we identify with the extended complex plane C ∪ {∞}).\n\nNow, a simple computation shows that\n\nsince the real part of an analytic function is harmonic. This proves that \"K\" = 0.\n\nSince the fundamental theorem of algebra can be seen as the statement that the field of complex numbers is algebraically closed, it follows that any theorem concerning algebraically closed fields applies to the field of complex numbers. Here are a few more consequences of the theorem, which are either about the field of real numbers or about the relationship between the field of real numbers and the field of complex numbers:\n\n\n\n\n\n\nWhile the fundamental theorem of algebra states a general existence result, it is of some interest, both from the theoretical and from the practical point of view, to have information on the location of the zeros of a given polynomial. The simpler result in this direction is a bound on the modulus: all zeros ζ of a monic polynomial formula_37 satisfy an inequality |ζ| ≤ \"R\", where\n\nNotice that, as stated, this is not yet an existence result but rather an example of what is called an a priori bound: it says that \"if there are solutions\" then they lie inside the closed disk of center the origin and radius \"R\". However, once coupled with the fundamental theorem of algebra it says that the disk contains in fact at least one solution. More generally, a bound can be given directly in terms of any p-norm of the \"n\"-vector of coefficients formula_39 that is |ζ| ≤ \"R\", where \"R\" is precisely the \"q\"-norm of the 2-vector formula_40 \"q\" being the conjugate exponent of \"p\", formula_41 for any 1 ≤ \"p\" ≤ ∞. Thus, the modulus of any solution is also bounded by\n\nfor 1 < \"p\" < ∞, and in particular\n\n(where we define \"a\" to mean 1, which is reasonable since 1 is indeed the \"n\"-th coefficient of our polynomial). The case of a generic polynomial of degree \"n\", \n\nis of course reduced to the case of a monic, dividing all coefficients by \"a\" ≠ 0. Also, in case that 0 is not a root, i.e. \"a\" ≠ 0, bounds from below on the roots ζ follow immediately as bounds from above on formula_46, that is, the roots of \n\nFinally, the distance formula_48 from the roots ζ to any point formula_49 can be estimated from below and above, seeing formula_50 as zeros of the polynomial formula_51, whose coefficients are the Taylor expansion of \"P\"(\"z\") at formula_52\n\nLet ζ be a root of the polynomial \n\nin order to prove the inequality |ζ| ≤ \"R\" we can assume, of course, |ζ| > 1. Writing the equation as \n\nand using the Hölder's inequality we find \n\nNow, if \"p\" = 1, this is \n\nthus \n\nIn the case 1 < \"p\" ≤ ∞, taking into account the summation formula for a geometric progression, we have\n\nthus \n\nand simplifying, \n\nTherefore \n\nholds, for all 1 ≤ \"p\" ≤ ∞.\n\n\n\n"}
{"id": "27864034", "url": "https://en.wikipedia.org/wiki?curid=27864034", "title": "Fuzzy-trace theory", "text": "Fuzzy-trace theory\n\nFuzzy-trace theory (FTT) is a theory of cognition originally proposed by Charles Brainerd and Valerie F. Reyna that draws upon dual-trace conceptions to predict and explain cognitive phenomena, particularly in the memory and reasoning domains. The theory has been used in areas such as cognitive psychology, human development, and social psychology to explain, for instance, false memory and its development, probability judgments, medical decision making, risk perception and estimation, and biases and fallacies in decision making.\n\nFTT was initially proposed in the 1990s as an attempt to unify findings from the memory and reasoning domains that could not be predicted or explained by earlier approaches to cognition and its development (e.g., constructivism and information processing). One of such challenges was the statistical independence between memory and reasoning, that is, memory for background facts of problem situations is often unrelated to accuracy in reasoning tasks. Such findings called for a rethinking of the memory-reasoning relation, which in FTT took the form of a dual-process theory linking basic concepts from psycholinguistic and Gestalt theory to memory and reasoning. More specifically, FTT posits that people form two types of mental representations about a past event, called verbatim and gist traces. Gist traces are fuzzy representations of a past event (e.g., its bottom-line meaning), hence the name fuzzy-trace theory, whereas verbatim traces are detailed representations of a past event. Although people are capable of processing both verbatim and gist information, they prefer to reason with gist traces rather than verbatim. This implies, for example, that even if people are capable of understanding ratio concepts like probabilities and prevalence rates, which are the standard for the presentation of health- and risk-related data, their choice in decision situations will usually be governed by the bottom-line meaning of it (e.g., \"the risk is high\" or \"the risk is low\"; \"the outcome is bad\" or \"the outcome is good\") rather than the actual numbers. More importantly, in FTT, memory-reasoning independence can be explained in terms of preferred modes of processing when one performs a memory task (e.g., retrieval of verbatim traces) relative to when one performs a reasoning task (e.g., preference for reasoning with gist traces).\n\nIn 1999, a similar approach was applied to human vision. It suggested that human vision has two types of processing: one that aggregates local spatial receptive fields, and one that parses the local receptive field. People used prior experience, gists, to decide which process dominates a perceptual decision. The work attempted to link Gestalt theory and psychophysics (i.e., independent linear filters). This theory was further developed into fuzzy image processing and used in information processing technology and edge detection.\n\nFTT posits two types of memory processes (verbatim and gist) and, therefore, it is often referred to as a dual process theory of memory. According to FTT, retrieval of verbatim traces (recollective retrieval) is characterized by mental reinstatement of the contextual features of a past event, whereas retrieval of gist traces (nonrecollective retrieval) is not. In fact, gist processes form representations of an event's semantic features rather than its surface details, the latter being a property of verbatim processes. In the memory domain, FTT's notion of verbatim and gist representations has been influential in explaining true memories (i.e., memories about events that actually happened) as well as false memories (i.e., memories about events that never happened). The following five principles have been used to predict and explain true and false memory phenomena:\n\nThe principle of parallel storage asserts that the encoding and storage of verbatim and gist information operate in parallel rather than in a serial fashion. For instance, suppose that a person is presented with the word \"apple\" in red color. On the one hand, according to the principle of parallel storage of verbatim and gist traces, verbatim features of the target item (e.g., the word was apple, it was presented in red, printed in boldface and italic, and all but the first letter were presented in lowercase) and gist features (e.g., the word was a type of fruit) would be encoded and stored simultaneously via distinct pathways. Conversely, if verbatim and gist traces are stored in a serial fashion, then gist features of the target item (the word was a type of fruit) would be derived from its verbatim features and, therefore, the formation of gist traces would depend on the encoding and storage of verbatim traces. The latter idea was often assumed by early memory models. However, despite the intuitive appeal of the serial processing approach, research suggests that the encoding and storage of gist traces do not depend on verbatim ones. Several studies have converged on the finding that the meaning of target items is encoded independently of, and even prior to, the encoding of the surface form of the same items. Ankrum and Palmer, for example, found that when participants are presented with a familiar word (e.g., apple) for a very brief period (100 milliseconds), they are able to identify the word itself (\"was it apple?\") better than its letters (\"did it contain the letter L?\").\n\nSimilar to the principle of parallel storage, retrieval of verbatim and gist traces also occur via dissociated pathways. According to the principle of dissociated retrieval, recollective and nonrecollective retrieval processes are independent of each other. Consequently, this principle allows verbatim and gist processes to be differentially influenced by factors such as the type of retrieval cues and the availability of each form of representation. In connection with Tulving's encoding specificity principle, items that were actually presented in the past are better cues for verbatim traces than items that were not. Similarly, items that were not presented in the past but preserve the meaning of presented items are usually better cues for gist traces. Suppose, for example, that subjects of an experiment are presented with a word list containing several dog breeds, such as poodle, bulldog, greyhound, doberman, beagle, collie, boxer, mastif, husky, and terrier. During a recognition test, the words poodle, spaniel, and chair are presented. According to the principle of dissociated retrieval, retrieval of verbatim and gist traces does not depend on each other and, therefore, different types of test probes might serve as better cues to one type of trace than another. In this example, test probes such as poodle (targets, or studied items) will be better retrieval cues for verbatim traces than gist, whereas test probes such as spaniel (related distractors, non-studied items but related to targets) will be better retrieval cues for gist traces than verbatim. Chair, on the other hand, would neither be a better cue for verbatim traces nor for gist traces because it was not presented and is not related to dogs. If verbatim and gist processes were dependent, then factors that affect one process would also affect the other in the same direction. However, several experiments showing, for example, differential forgetting rates between memory for the surface details and memory for the bottom-line meaning of past events favor the notion of dissociated retrieval of verbatim and gist traces. In the case of forgetting rates, those experiments have shown that, over time, verbatim traces become inaccessible at a faster rate than gist traces. Brainerd, Reyna, and Kneer, for instance, found that delay drives true recognition rates (supported by both verbatim and gist traces) and false recognition rates (supported by gist and suppressed by verbatim traces) in opposite directions, namely true memory decays over time while false memory increases.\n\nThe principle of opponent processes describes the interaction between verbatim and gist processes in creating true and false memories. Whereas true memory is supported by both verbatim and gist processes, false memory is supported by gist processes and suppressed by verbatim processes. In other words, verbatim and gist processes work in opposition to one another when it comes to false memories. Suppose, for example, that one is presented with a word list such as lemon, apple, pear, and citrus. During a recognition test, the items lemon (target), orange (related distractor), and fan (unrelated distractor) are shown. In this case, retrieval of a gist trace (fruits) supports acceptance of both test probes lemon (true memory) and orange (false memory), whereas retrieval of a verbatim trace (lemon) only supports acceptance of the test probe lemon. In addition, retrieval of an exclusory verbatim trace (\"I saw only the words lemon, apple, pear, and citrus\") suppresses acceptance of false but related items such as orange through an operation known as recollection rejection. If neither verbatim nor gist traces are retrieved, then one might accept any test probe on the basis of response bias.<br>\nThis principle plays a key role in FTT's explanation of experimental dissociations between true and false memories (e.g., when a variable affects one type of memory without affecting the other, or when it produces opposite effects on them). The time of exposure of each word during study and the number of repetitions have been shown to produce such dissociations. More specifically, while true memory follows a monotonically increasing function when plotted against presentation duration, false memory rates exhibit an inverted-U pattern when plotted as a function of presentation duration. Similarly, repetition is monotonically related to true memory (true memory increases as a function of the number of repetitions) and is non-monotonically related to false memory (repetition produces an inverted-U relation with false memory).\n\nRetrieval phenomenologies are spontaneous mental experiences associated with the act of remembering. It was first systematically characterized by E. K. Strong in the early 1900s. Strong identified two distinct types of introspective phenomena associated with memory retrieval that have since been termed recollection (or remembrance) and familiarity. Whereas the former is characterized as retrieval associated with recollection of past experiences, the latter lacks such association. The two forms of experiences can be illustrated by everyday expressions such as \"I remember that!\" (recollection) and \"That seems familiar...\" (familiarity). In FTT, retrieval of verbatim traces often produces recollective phenomenology and thus is frequently referred to as recollective retrieval. However, one feature of FTT is that recollective phenomenology is not particular to one type of memory process as posited by other dual-process theories of memory. Instead, FTT posits that retrieval of gist traces can also produce recollective phenomenology under some circumstances. When gist resemblance between a false item and memory is high and compelling, this gives rise to a phenomenon called phantom recollection, which is a vivid, but false, memory deemed to be true.\n\nThe principle of developmental variability in dual processes posits that verbatim and gist processes show variability across the lifespan. More specifically, verbatim and gist processes have been shown to improve between early childhood and young adulthood. Regarding verbatim processes, older children are better at retrieval of verbatim traces than younger children, although even very young children (4-year-olds) are able to retrieve verbatim information at above chance level. For instance, source memory accuracy greatly increases between 4-year-olds and 6-year-olds, and memory for nonsense words (i.e., words without a meaning, such as neppez) has been shown to increase between 7- and 10-year-olds. Gist processes also improve with age. For example, semantic clustering in free recall increases from 8-year-olds to 14-year-olds, and meaning connection across words and sentences has been shown to improve between 6- and 9-year-olds. In particular, the notion that gist memory improves with age plays a central role in FTT's prediction of age increases in false memory, a counterintuitive pattern that has been called developmental reversal.\n\nRegarding old age, several studies suggest that verbatim memory declines between early and late adulthood, while gist memory remains fairly stable. Experiments indicate that older adults perform worse on tasks that require retrieval of surface features from studied items relative to younger adults. In addition, results with measurement models that quantify verbatim and gist processes indicate that older adults are less able to use verbatim traces during recall than younger adults.\n\nWhen people try to remember past events (e.g., a birthday party or the last dinner), they often commit two types of errors: errors of omission and errors of commission. The former is known as forgetting, while the latter is better known as false memories. False memories can be separated into spontaneous and implanted false memories. Spontaneous false memories result from endogenous (internal) processes, such as meaning processing, while implanted false memories are the result of exogenous (external) processes, such as the suggestion of false information by an outside source (e.g., an interviewer asking misleading questions). Research had first suggested that younger children are more susceptible to suggestion of false information than adults. However, research has since indicated that younger children are much less likely to form false memories than older children and adults. Moreover, in opposition to common sense, true memories are not more stable than false ones. Studies have shown that false memories are actually more persistent than true memories. According to FTT, such pattern arises because false memories are supported by memory traces that are less susceptible to interference and forgetting (gist traces) than traces that suppress them and also support true memories (verbatim traces).\n\nFTT, as it applies to reasoning, is adapted from dual process models of human cognition. It differs from the traditional dual process model in that it makes a distinction between impulsivity and intuition—which are combined in System 1 according to traditional dual process theories—and then makes the claim that expertise and advanced cognition relies on intuition. The distinction between intuition and analysis depends on what kind of representation is used to process information. The mental representations described by FTT are categorized as either \"gist\" or \"verbatim\" representations:\nGenerally, most adults display what is called a \"fuzzy processing preference,\" meaning that they rely on the least precise gist representations necessary to make a decision, despite parallel processing of both gist and verbatim representations. Both processes increase with age, though the verbatim process develops sooner than the gist, and is thus more heavily relied on in adolescence.\n\nIn this regard, the theory expands on research that has illustrated the role of memory representations in reasoning processes, the intersection of which has been previously underexplored. However, it should be noted that in certain circumstances, FTT predicts independence between memory and reasoning, specifically between reasoning tasks that rely on gist representations and memory tests that rely on verbatim representations. An example of this is research between the risky choice framing task and working memory, in which better working memory is not associated with a reduction in bias.\n\nFTT thus explains inconsistencies or biases in reasoning to be dependent on retrieval cues that access stored values and principles that are gist representations, which can be filtered through experience and cultural, affective, and developmental factors. This dependence on gist results in a vulnerability of reasoning to processing interference from overlapping classes of events, but can also explain expert reasoning in that a person can treat superficially different reasoning problems in the same way if the problems share an underlying gist.\n\nFTT posits that when people are presented with statistical information, they extract representations of the gist of the information (qualitatively) as well as the exact verbatim information (quantitatively). The gist that is encoded is often a basic categorical distinction between \"no risk\" and \"some risk.\" However, in situations when both choices in the decision have a level of uncertainty or risk, then another level of precision would be required, e.g., \"low risk\" or \"high risk.\" An illustration of this principle can be found in FTT's explanation of the common framing effect.\n\nFraming effects occur when linguistically different descriptions of equivalent options lead to inconsistent choices. A famous example of a risky choice framing task is the Asian Disease Problem. This task requires the participants to imagine that their country is about to face a disease which is expected to kill 600 people. They have to choose among two programs to combat this disease. Subjects are presented with options that are framed as either gains (lives saved) or losses (lives lost). The possible options, as well as the categorical gists that are posited to be encoded by FTT are displayed below.\n\nIt is commonly found that people prefer the sure option when the options are framed as gains (program A) and the risky option when they are framed as losses (program D), despite the fact that the expected values for all the programs are equivalent. This is in contrast to a normative point of view that would indicate that if respondents prefer the sure option in the positive frame, they should also prefer the sure option in the negative frame.\n\nThe explanation for this effect according to FTT is that people will tend to operate on the simplest gist that is permitted to make a decision. In the case of this framing question, the gain frame presents a situation in which people prefer the gist of some people being saved to the possibility that some are saved or no one could be saved, and conversely, that the possibility of some people dying or no one dying is preferable to the option that some people will surely die.\n\nCritical tests have been conducted to provide evidence in support of this explanation in favor of other theoretical explanations (i.e., Prospect theory) by presenting a modified version of this task that eliminates some mathematically redundant wording, e.g., program B would instead indicate that \"If program B is adopted, there is 1/3 probability that 600 people will be saved.\" FTT predicts, in this case, that the elimination of the additional gist (the explicit possible death in program B) would result in indifference and eliminate the framing effect, which is indeed what was found.\n\nThe dual-process assumption of FTT has also been used to explain common biases of probability judgment, including the conjunction and disjunction fallacies. The conjunction fallacy occurs when people mistakenly judge a specific set of circumstances to be more probable than a more general set that includes the specific set. This fallacy is famously demonstrated by the Linda problem: that given a description of a woman named Linda who is an outspoken philosophy major who is concerned about discrimination and social justice, people will judge \"Linda is a bank teller and is active in the feminist movement\" to be more probable than \"Linda is a bank teller\", despite the fact that the latter statement is entirely inclusive of the former. FTT explains this phenomenon to not be a matter of encoding, given that priming participants to understand the inclusive nature of the categories tends not to reduce the bias. Instead, this is the result of the salience of relational gist, which contributes to a tendency to judge relative numerosity instead of merely applying the principle of class inclusion.\n\nErrors of probability perception are also associated with the theory's predictions of contradictory relationships between risk perception and risky behavior. Specifically, that endorsement of accurate principles of objective risk is actually associated with greater risk-taking, whereas measures that assess global, gist-based judgments of risk had a protective effect (consistent with other predictions from FTT in the field of medical decision making). Since gist processing develops after verbatim processing as people age, this finding lends explanation to the increase in risk-taking that occurs during adolescence.\n\nFTT has also been applied in the domains of consumer behavior and economics. For example, since the theory posits that people rely primarily on gist representations in making decisions, and that culture and experience can affect consumers' gist representations, factors such as cultural similarity and personal relevance have been used to explain consumers' perceptions of the risk of food-borne contamination and their intentions to reduce consumption of certain foods. In other words, one's evaluation of how \"at-risk\" he or she is can be inﬂuenced both by specific information learned as well as by the fuzzy representations of culture experience, and perceived proximity. In practice this resulted in greater consumer concern when the threat of a food-borne-illness was described in a culturally similar location, regardless of geographical proximity or other verbatim details.\n\nEvidence was also found in consumer research in support of FTT's \"editing\" hypothesis, namely that extremely low-probability risks can be simplified by gist processing to be represented as \"essentially nil.\" For example, one study found that people were willing to pay more for a safer product if safety was expressed relatively (i.e., product A is safer than product B) than they were if safety was expressed with statistics of actual incidence of safety hazards.\nThis result is in contrast to most prescriptive decision rules that predict that formally equivalent methods of communicating risk information should have identical effects on risk-taking behavior, even if the pertinent displays are different. These findings are predicted by FTT (and related models), which suggest that people reason on the basis of simplified representations rather than on the literal information available.\n\nLike other people, clinicians apply cognitive heuristics and fall into systematic errors which affect decisions in everyday life. Research has shown that patients and their physicians have difficulty understanding a host of numerical concepts, especially risks and probabilities, and this often implies some problems with numeracy, or mathematical proficiency. For example, physicians and patients both demonstrate great difficulty understanding the probabilities of certain genetic risks and were prone to the same errors, despite vast differences in medical knowledge.\nThough traditional dual process theory generally predicts that decisions made by computation are superior to those made by intuition, FTT assumes the opposite: that intuitive processing is more sophisticated and is capable of making better decisions, and that increases in expertise are accompanied by reliance on intuitive, gist-based reasoning rather than on literal, verbatim reasoning.\nFTT predicts that simply educating people with statistics regarding risk factors can hinder prevention efforts. Due to low prevalence of HIV or cancer, for example, people tend to overestimate their risks, and consequently interventions stressing the actual numbers may move people toward complacency as opposed to risk reduction. When women learn that their actual risks for breast cancer are lower than they thought, they return for screening at a lower rate. Also, some interventions to discourage adolescent drug use by presenting the risks have been shown to be ineffective or can even backfire.\n\nThe conclusion drawn from this evidence is that health-care professionals and health policymakers need to package, present, and explain information in more meaningful ways that facilitate forming an appropriate gist. Such strategies would include explaining quantities qualitatively, displaying information visually, and tailoring the format to trigger the appropriate gist and to cue the retrieval of health-related knowledge and values. Web-based interventions have been designed using these principles, which have been found to increase the patient's willingness to escalate care, as well as gain knowledge and make an informed choice.\n\nTheory-driven research using principles from FTT provides empirically supported recommendations that can be applied in many fields. For example, it provides specific recommendations regarding interventions aiming at reducing adolescent risk taking. Moreover, according to FTT, precise information does not necessarily work to communicate health-related information, which has obvious implications to public policy and procedures for improving treatment adherence in particular. Specifically, FTT principles suggest examples of how to display risk proportions in order to be comprehensible for both patients and health care professionals:\n\n\nIn addition, memory principles in FTT provide recommendations to eyewitness testimony. Children are often called upon to testify in courts, most commonly in cases of maltreatment, divorce, and child custody. Contrary to common sense, FTT posits that children can be reliable witnesses as long as they are encouraged to report verbatim memories and their reports are protected from suggestion of false information. More specifically:\n\n\n"}
{"id": "38764823", "url": "https://en.wikipedia.org/wiki?curid=38764823", "title": "Georgia Benkart", "text": "Georgia Benkart\n\nGeorgia McClure Benkart (born 1949) is an American mathematician who is known for her work in the structure and representation theory of Lie algebras and related algebraic structures.\nShe has published over 100 journal articles and co-authored 3 American Mathematical Society Memoirs in four broad categories: modular Lie algebras; combinatorics of Lie algebra representations; graded algebras and superalgebras; and quantum groups and related structures.\n\nBenkart has made a contribution to the classification of simple modular Lie algebras. Her work with J. M. Osborn on toroidal rank-one Lie algebras became one of the building blocks of the classification. The complete description of Hamiltonian Lie algebras (with Gregory, Osborn, Strade, Wilson) can stand alone, and also has applications in the theory of pro-p groups.\n\nIn 2009 she published, jointly with T. Gregory and A. Premet, the first complete proof of the recognition theorem for graded Lie algebras in characteristics at least 5.\n\nIn the early 90s Benkart and Efim Zelmanov started to work on classification of root-graded Lie algebras and intersection matrix algebras. The latter were introduced by P. Slodowy in his work on singularities. Berman and Moody recognized that these algebras (generalizations of affine Kac–Moody algebras ) are universal root graded Lie algebras and classified them for simply laced root systems. Benkart and Zelmanov tackled the remaining cases involving the so-called Magic Freudenthal–Tits “Square” and extended this square to exceptional Lie superalgebras.\n\nLater Benkart extended these results in two directions. In a series of papers with A. Elduque she developed the theory of root graded Lie superalgebras. In a second series of works with B. Allison, A. Pianzola, E. Neher, et al. she determined the universal central covers of these algebras.\n\nOne of the pillars of the representation theory of quantum groups (and applications to combinatorics) is Kashiwara's theory of crystal bases. These are highly invariant bases which are well suited for decompositions of tensor products. In a paper with S.-J. Kang and M. Kashiwara, Benkart extended the theory of crystal bases to quantum superalgebras.\n\nBenkart's work on noncommutative algebras related to algebraic combinatorics became a basic tool in the construction of tensor categories.\n\nBenkart received her B.S. from the Ohio State University in 1970 and an M. Phil. in Mathematics from Yale University in 1973. She completed her doctoral work at Yale under Nathan Jacobson and wrote a dissertation entitled \"Inner Ideals and the Structure of Lie Algebras.\" She was awarded a Ph.D. in Mathematics from the Yale University in 1974.\n\nUpon completing her doctoral degree, Benkart began her long career at the University of Wisconsin–Madison, first as a MacDuffee Instructor and eventually as a E. B. Van Vleck Professor of Mathematics until she retired from teaching in 2006. She held visiting positions at the Mathematical Sciences Research Institute in Berkeley, California, the Institute for Advanced Study in Princeton, New Jersey, the Aspen Center for Physics, and the University of Virginia.\n\nBenkart received a Woodrow Wilson Fellowship from the Woodrow Wilson National Fellowship Foundation. Her work at Wisconsin was recognized by a Romnes Fellowship in 1985, a Distinguished Teaching Award in 1987, and a WARF Mid-Career Faculty Research Award in 1996. In 2008 the University of California Lie Groups and Lie Algebras meeting was held in Benkart's honor. She has given numerous talks and series of lectures throughout the U.S., Canada, France, Germany, Hong Kong, Korea, Mexico, and Spain, including two invited lectures at the Joint Mathematics Meetings and a plenary lecture at a meeting of the Canadian Mathematical Society.\n\nIn 2000–2002 Benkart was named a Polya Lecturer by the Mathematical Association of America. \nShe was elected a Fellow of the American Mathematical Society (AMS) in the inaugural class of 2013.\n\nShe has served on the editorial boards of the American Mathematical Society for Surveys and Monographs and Abstracts, the Journal of Algebra, the Korean Mathematical Colloquium, the Nova Journal of Algebra and Geometry, Communications in Algebra, and Algebras, Groups, and Geometries. She served as the Associate Secretary of the American Mathematical Society for the Central Section from 2010–2016.\nBenkart has been active in the Association for Women in Mathematics (AWM). She was elected and served as President of the AWM from 2009–2011. In 2014 she was selected to deliver the AWM-AMS Noether Lecture. The title of her talk was Walking on Graphs the Representation Theory Way.\n\nIn 2014 at the International Congress of Mathematicians held in Seoul, she delivered the ICM Emmy Noether Lecture.\n\nIn 2017 she was selected as a fellow of the Association for Women in Mathematics in the inaugural class.\n\n\n"}
{"id": "49917455", "url": "https://en.wikipedia.org/wiki?curid=49917455", "title": "Giovanni Battista Nicolai", "text": "Giovanni Battista Nicolai\n\nGiovanni Battista Nicolai (1726 – 1793) was an Italian mathematician.\n\nHe was a priest, analyst and scholar in Padua.\n\n"}
{"id": "325806", "url": "https://en.wikipedia.org/wiki?curid=325806", "title": "Graph (discrete mathematics)", "text": "Graph (discrete mathematics)\n\nIn mathematics, and more specifically in graph theory, a graph is a structure amounting to a set of objects in which some pairs of the objects are in some sense \"related\". The objects correspond to mathematical abstractions called \"vertices\" (also called \"nodes\" or \"points\") and each of the related pairs of vertices is called an \"edge\" (also called an \"arc\" or \"line\"). Typically, a graph is depicted in diagrammatic form as a set of dots for the vertices, joined by lines or curves for the edges. Graphs are one of the objects of study in discrete mathematics.\n\nThe edges may be directed or undirected. For example, if the vertices represent people at a party, and there is an edge between two people if they shake hands, then this graph is undirected because any person \"A\" can shake hands with a person \"B\" only if \"B\" also shakes hands with \"A\". In contrast, if any edge from a person \"A\" to a person \"B\" corresponds to \"A\"'s admiring \"B\", then this graph is directed, because admiration is not necessarily reciprocated. The former type of graph is called an \"undirected graph\" and the edges are called \"undirected edges\" while the latter type of graph is called a \"directed graph\" and the edges are called \"directed edges\".\n\nGraphs are the basic subject studied by graph theory. The word \"graph\" was first used in this sense by James Joseph Sylvester in 1878.\n\nDefinitions in graph theory vary. The following are some of the more basic ways of defining graphs and related mathematical structures.\n\nIn one very common sense of the term, a \"graph\" is an ordered pair comprising a set \"V\" of \"vertices\", \"nodes\" or \"points\" together with a set \"E\" of \"edges\", \"arcs\" or \"lines\", which are 2-element subsets of \"V\" (i.e., an edge is associated with two vertices, and the association takes the form of the unordered pair of the vertices). To avoid ambiguity, this type of graph may be described precisely as \"undirected\" and \"simple\".\n\nOther senses of \"graph\" stem from different conceptions of the edge set. In one more general conception, \"E\" is a set together with a relation of \"incidence\" that associates with each edge two vertices. In another generalized notion, \"E\" is a multiset of unordered pairs of (not necessarily distinct) vertices. Many authors call these types of object multigraphs or pseudographs.\n\nAll of these variants and others are described more fully below.\n\nThe vertices belonging to an edge are called the \"ends\" or \"end vertices\" of the edge. A vertex may exist in a graph and not belong to an edge.\n\n\"V\" and \"E\" are usually taken to be finite, and many of the well-known results are not true (or are rather different) for \"infinite graphs\" because many of the arguments fail in the infinite case. Moreover, \"V\" is often assumed to be non-empty, but \"E\" is allowed to be the empty set. The \"order\" of a graph is |\"V\"|, its number of vertices. The \"size\" of a graph is |\"E\"|, its number of edges. The \"degree\" or \"valency\" of a vertex is the number of edges that connect to it, where an edge that connects to the vertex at both ends (a loop) is counted twice.\n\nFor an edge }, graph theorists usually use the somewhat shorter notation \"xy\".\n\nThe edges \"E\" of an undirected graph \"G\" induce a symmetric binary relation ~ on \"V\" that is called the \"adjacency relation\" of \"G\". Specifically, for each edge }, the vertices \"x\" and \"y\" are said to be \"adjacent\" to one another, which is denoted .\n\nAs stated above, in different contexts it may be useful to refine the term \"graph\" with different degrees of generality. Whenever it is necessary to draw a strict distinction, the following terms are used. Most commonly, in modern texts in graph theory, unless stated otherwise, \"graph\" means \"undirected simple finite graph\" (see the definitions below).\n\nAn \"undirected graph\" is a graph in which edges have no orientation. The edge is identical to the edge . That is, they are not ordered pairs, but unordered pairs—i.e., sets of two vertices } (or 2-multisets in the case of loops). The maximum number of edges in an undirected graph without a loop is , where \"n\" is the number of nodes in the graph.\n\nA \"directed graph\" or \"digraph\" is a graph in which edges have orientations. It is written as an ordered pair (sometimes ) with\n\nAn arrow is considered to be directed \"from\" \"x\" \"to\" \"y\"; \"y\" is called the \"head\" and \"x\" is called the \"tail\" of the arrow; \"y\" is said to be a \"direct successor\" of \"x\" and \"x\" is said to be a \"direct predecessor\" of \"y\". If a path leads from \"x\" to \"y\", then \"y\" is said to be a \"successor\" of \"x\" and \"reachable\" from \"x\", and \"x\" is said to be a \"predecessor\" of \"y\". The arrow is called the \"inverted arrow\" of .\n\nA directed graph \"G\" is called \"symmetric\" if, for every arrow in \"G\", the corresponding inverted arrow also belongs to \"G\". A symmetric loopless directed graph is equivalent to a simple undirected graph , where the pairs of inverse arrows in \"A\" correspond one-to-one with the edges in \"E\"; thus the number of edges in \"G′\" is , that is half the number of arrows in \"G\".\n\nAn \"oriented graph\" is a directed graph in which at most one of and may be arrows of the graph. That is, it is a directed graph that can be formed as an orientation of an undirected graph. However, some authors use \"oriented graph\" to mean the same as \"directed graph\".\n\nA \"mixed graph\" is a graph in which some edges may be directed and some may be undirected. It is written as an ordered triple with \"V\", \"E\", and \"A\" defined as above. Directed and undirected graphs are special cases.\n\n\"Multiple edges\" are two or more edges that connect the same two vertices. A \"loop\" is an edge (directed or undirected) that connects a vertex to itself; it may be permitted or not, according to the application. In this context, an edge with two different ends is called a \"link\".\n\nA \"multigraph\", as opposed to a simple graph, is an undirected graph in which multiple edges (and sometimes loops) are allowed.\n\nWhere graphs are defined so as to \"disallow\" both multiple edges and loops, a multigraph is often defined to mean a graph which can have both multiple edges and loops, although many use the term \"pseudograph\" for this meaning. Where graphs are defined so as to \"allow\" both multiple edges and loops, a multigraph is often defined to mean a graph without loops.\n\nA simple graph is an undirected graph with neither multiple edges nor loops. In a simple graph the edges form a \"set\" (rather than a multiset) and each edge is an unordered pair of \"distinct\" vertices. Thus, we can define a simple graph to be a set \"V\" of vertices together with a set \"E\" of edges, which are 2-element subsets of \"V\".\n\nIn a simple graph with \"n\" vertices, the degree of every vertex is at most .\n\nA \"quiver\" or \"multidigraph\" is a directed multigraph. A quiver may have directed loops in it. Thus, a quiver is a set \"V\" of vertices, a set \"E\" of edges, and two functions formula_1, formula_2. The map \"s\" assigns to each edge its \"source\" (or \"tail\"), while the map \"t\" assigns to each edge its \"target\" (or \"head\").\n\nA \"weighted graph\" is a graph in which a number (the weight) is assigned to each edge. Such weights might represent for example costs, lengths or capacities, depending on the problem at hand. Some authors call such a graph a \"network\". Weighted correlation networks can be defined by soft-thresholding the pairwise correlations among variables (e.g. gene measurements). Such graphs arise in many contexts, for example in shortest path problems such as the traveling salesman problem.\n\nIn certain situations it can be helpful to allow edges with only one end, called \"half-edges\", or no ends, called \"loose edges\"; see the articles Signed graphs and Biased graphs.\n\nA \"regular graph\" is a graph in which each vertex has the same number of neighbours, i.e., every vertex has the same degree. A regular graph with vertices of degree \"k\" is called a \"k\"‑regular graph or regular graph of degree \"k\".\n\nA \"complete graph\" is a graph in which each pair of vertices is joined by an edge. A complete graph contains all possible edges.\n\nA \"finite graph\" is a graph in which the vertex set and the edge set are finite sets. Otherwise, it is called an \"infinite graph\".\n\nMost commonly in graph theory it is implied that the graphs discussed are finite. If the graphs are infinite, that is usually specifically stated.\n\nIn an undirected graph, an unordered pair of vertices } is called \"connected\" if a path leads from \"x\" to \"y\". Otherwise, the unordered pair is called \"disconnected\".\n\nA \"connected graph\" is an undirected graph in which every unordered pair of vertices in the graph is connected. Otherwise, it is called a \"disconnected graph\".\n\nIn a directed graph, an ordered pair of vertices is called \"strongly connected\" if a directed path leads from \"x\" to \"y\". Otherwise, the ordered pair is called \"weakly connected\" if an undirected path leads from \"x\" to \"y\" after replacing all of its directed edges with undirected edges. Otherwise, the ordered pair is called \"disconnected\".\n\nA \"strongly connected graph\" is a directed graph in which every ordered pair of vertices in the graph is strongly connected. Otherwise, it is called a \"weakly connected graph\" if every ordered pair of vertices in the graph is weakly connected. Otherwise it is called a \"disconnected graph\".\n\nA \"k-vertex-connected graph\" or \"k-edge-connected graph\" is a graph in which no set of vertices (respectively, edges) exists that, when removed, disconnects the graph. A \"k\"-vertex-connected graph is often called simply a \"k-connected graph\".\n\nA \"bipartite graph\" is a graph in which the vertex set can be partitioned into two sets, \"W\" and \"X\", so that no two vertices in \"W\" share a common edge and no two vertices in \"X\" share a common edge. Alternatively, it is a graph with a chromatic number of 2.\n\nIn a complete bipartite graph, the vertex set is the union of two disjoint sets, \"W\" and \"X\", so that every vertex in \"W\" is adjacent to every vertex in \"X\" but there are no edges within \"W\" or \"X\".\n\nA \"path graph\" or \"linear graph\" of order is a graph in which the vertices can be listed in an order \"v\", \"v\", …, \"v\" such that the edges are the } where \"i\" = 1, 2, …, \"n\" − 1. Path graphs can be characterized as connected graphs in which the degree of all but two vertices is 2 and the degree of the two remaining vertices is 1. If a path graph occurs as a subgraph of another graph, it is a path in that graph.\n\nA \"planar graph\" is a graph whose vertices and edges can be drawn in a plane such that no two of the edges intersect.\n\nA \"cycle graph\" or \"circular graph\" of order is a graph in which the vertices can be listed in an order \"v\", \"v\", …, \"v\" such that the edges are the } where \"i\" = 1, 2, …, \"n\" − 1, plus the edge }. Cycle graphs can be characterized as connected graphs in which the degree of all vertices is 2. If a cycle graph occurs as a subgraph of another graph, it is a cycle or circuit in that graph.\n\nA \"tree\" is a connected graph with no cycles.\n\nA \"forest\" is a graph with no cycles, i.e. the disjoint union of one or more trees.\n\nMore advanced kinds of graphs are:\n\nTwo edges of a graph are called \"adjacent\" if they share a common vertex. Two arrows of a directed graph are called \"consecutive\" if the head of the first one is the tail of the second one. Similarly, two vertices are called \"adjacent\" if they share a common edge (\"consecutive\" if the first one is the tail and the second one is the head of an arrow), in which case the common edge is said to \"join\" the two vertices. An edge and a vertex on that edge are called \"incident\".\n\nThe graph with only one vertex and no edges is called the \"trivial graph\". A graph with only vertices and no edges is known as an \"edgeless graph\". The graph with no vertices and no edges is sometimes called the \"null graph\" or \"empty graph\", but the terminology is not consistent and not all mathematicians allow this object.\n\nNormally, the vertices of a graph, by their nature as elements of a set, are distinguishable. This kind of graph may be called \"vertex-labeled\". However, for many questions it is better to treat vertices as indistinguishable. (Of course, the vertices may be still distinguishable by the properties of the graph itself, e.g., by the numbers of incident edges.) The same remarks apply to edges, so graphs with labeled edges are called \"edge-labeled\". Graphs with labels attached to edges or vertices are more generally designated as \"labeled\". Consequently, graphs in which vertices are indistinguishable and edges are indistinguishable are called \"unlabeled\". (Note that in the literature, the term \"labeled\" may apply to other kinds of labeling, besides that which serves only to distinguish different vertices or edges.)\n\nThe category of all graphs is the slice category Set ↓ \"D\" where \"D\": Set → Set is the functor taking a set \"s\" to \"s\" × \"s\".\n\n\nThere are several operations that produce new graphs from initial ones, which might be classified into the following categories:\n\nIn a hypergraph, an edge can join more than two vertices.\n\nAn undirected graph can be seen as a simplicial complex consisting of 1-simplices (the edges) and 0-simplices (the vertices). As such, complexes are generalizations of graphs since they allow for higher-dimensional simplices.\n\nEvery graph gives rise to a matroid.\n\nIn model theory, a graph is just a structure. But in that case, there is no limitation on the number of edges: it can be any cardinal number, see continuous graph.\n\nIn computational biology, power graph analysis introduces power graphs as an alternative representation of undirected graphs.\n\nIn geographic information systems, geometric networks are closely modeled after graphs, and borrow many concepts from graph theory to perform spatial analysis on road networks or utility grids.\n\n\n\n"}
{"id": "23029956", "url": "https://en.wikipedia.org/wiki?curid=23029956", "title": "Homological integration", "text": "Homological integration\n\nIn the mathematical fields of differential geometry and geometric measure theory, homological integration or geometric integration is a method for extending the notion of the integral to manifolds. Rather than functions or differential forms, the integral is defined over currents on a manifold.\n\nThe theory is \"homological\" because currents themselves are defined by duality with differential forms. To wit, the space of -currents on a manifold is defined as the dual space, in the sense of distributions, of the space of -forms on . Thus there is a pairing between -currents and -forms , denoted here by\nUnder this duality pairing, the exterior derivative \ngoes over to a boundary operator\ndefined by\nfor all . This is a homological rather than cohomological construction.\n\n"}
{"id": "30656533", "url": "https://en.wikipedia.org/wiki?curid=30656533", "title": "Hyers–Ulam–Rassias stability", "text": "Hyers–Ulam–Rassias stability\n\nThe stability problem of functional equations originated from a question of Stanisław Ulam, posed in 1940, concerning the stability of group homomorphisms. In the next year, Donald H. Hyers gave a partial affirmative answer to the question of Ulam in the context of Banach spaces in the case of \"additive\" mappings, that was the first significant breakthrough and a step toward more solutions in this area. Since then, a large number of papers have been published in connection with various generalizations of Ulam’s problem and Hyers’s theorem. In 1978, Themistocles M. Rassias succeeded in extending Hyers’s theorem for mappings between Banach spaces by considering an unbounded Cauchy difference subject to a continuity condition upon the mapping. He was the first to prove the stability of the \"linear mapping\". This result of Rassias attracted several mathematicians worldwide who began to be stimulated to investigate the stability problems of functional equations.\n\nBy regarding a large influence of S. M. Ulam, D. H. Hyers, and Th. M. Rassias on the study of stability problems of functional equations, the stability phenomenon proved by Th. M. Rassias led to the development of what is now known as Hyers–Ulam–Rassias stability of functional equations. For an extensive presentation of the stability of functional equations in the context of Ulam's problem, the interested reader is referred to the books by S.-M. Jung, S. Czerwik, Y.J. Cho, C. Park, Th.M. Rassias and R. Saadati, Y.J. Cho, Th.M. Rassias and R. Saadati, and Pl. Kannappan, as well as to the following papers . In 1950, T. Aoki considered an unbounded Cauchy difference which was generalised later by Rassias to the linear case. This result is known as Hyers–Ulam–Aoki stability of the additive mapping. Aoki (1950) had not considered continuity upon the mapping, whereas Rassias (1978) imposed extra continuity hypothesis which yielded a formally stronger conclusion. \n\n"}
{"id": "30115275", "url": "https://en.wikipedia.org/wiki?curid=30115275", "title": "Illumination problem", "text": "Illumination problem\n\nThe illumination problem is a resolved mathematical problem first posed by Ernst Straus in the 1950s. Straus asked if a room with mirrored walls can always be illuminated by a single point light source, allowing for repeated reflection of light off the mirrored walls. Alternatively, the question can be stated as asking that if a billiard table can be constructed in any required shape, is there a shape possible such that there is a point where it is impossible to the billiard ball in a at another point, assuming the ball is point-like and continues infinitely rather than stopping due to friction.\n\nThe problem was first solved in 1958 by Roger Penrose using ellipses to form the \"Penrose unilluminable room\". He showed there exists a room with curved walls that must always have dark regions if lit only by a single point source. This problem was also solved for polygonal rooms by George Tokarsky in 1995 for 2 dimensions, which showed there exists an unilluminable polygonal 26-sided room with a \"dark spot\" which is not illuminated from another point in the room, even allowing for repeated reflections. This was a borderline case, however, since a finite number of dark \"points\" (rather than regions) are unilluminable from any given position of the point source. An improved solution was put forward by D. Castro in 1997, with a 24-sided room with the same properties.\n\nIn 2016, Lelièvre, Monteil and Weiss showed that a light source in a polygonal room whose angles (in degrees) are all rational numbers will illuminate the entire polygon, with the possible exception of a finite number of points.\n"}
{"id": "1721092", "url": "https://en.wikipedia.org/wiki?curid=1721092", "title": "Iverson bracket", "text": "Iverson bracket\n\nIn mathematics, the Iverson bracket, named after Kenneth E. Iverson, is a notation that \ngeneralises the Kronecker delta. It converts any logical proposition into a number that is 1 if the proposition is satisfied, and 0 otherwise, and is generally written \nby putting the proposition inside square brackets:\nwhere is a statement that can be true or false.\n\nIn the context of summation, the notation can be used to write any sum as an infinite sum without limits: \nIf formula_2 is any\nproperty of the integer formula_3,\n\nNote that by this convention, a summand formula_5 must evaluate to 0 regardless of whether formula_6 is defined.\nLikewise for products:\n\nThe notation was originally introduced by Kenneth E. Iverson in his programming language APL, though restricted to single relational operators enclosed in parentheses, while the generalisation to arbitrary statements, notational restriction to square brackets, and applications to summation, was advocated by Donald Knuth to avoid ambiguity in parenthesized logical expressions.\n\nThere is a direct correspondence between arithmetic on Iverson brackets, logic, and set operations. For instance, let \"A\" and \"B\" be sets and formula_8 any property of integers; then we have\n\nThe notation allows moving boundary conditions of summations (or integrals) as a separate factor into the summand, freeing up space around the summation operator, but more importantly allowing it to be manipulated algebraically.\n\nWe mechanically derive a well-known sum manipulation rule using Iverson brackets:\n\nThe well-known rule formula_17 is likewise easily derived:\n\nFor instance, the Euler phi function that counts the number of positive integers up to \"n\" which are coprime to \"n\" can be expressed by\n\nAnother use of the Iverson bracket is to simplify equations with special cases. For example, the formula\n\nis valid for but is off by for . To get an identity valid for all positive integers (i.e., all values for which formula_21 is defined), a correction term involving the Iverson bracket may be added:\n\nMany common functions, especially those with a natural piecewise definition, may be expressed in terms of the Iverson bracket. The Kronecker delta notation is a specific case of Iverson notation when the condition is equality. That is,\n\nThe indicator function, often denoted formula_24, formula_25 or formula_26, is an Iverson bracket with set membership as its condition:\n\nThe Heaviside step function, sign function, and absolute value function and are also easily expressed in this notation:\nand\n\nThe comparison functions max and min (returning the larger or smaller of two arguments) may be written as\n\nThe floor and ceiling functions can be expressed as\nand\nwhere the index formula_35 of summation is understood to range over all the integers.\n\nThe ramp function can be expressed\n\nThe trichotomy of the reals is equivalent to the following identity:\n\nThe Möbius function has the property (and can be defined by recurrence as)\n\nIn the 1830s, Guglielmo Libri Carucci dalla Sommaja used formula_39 as a replacement for what would now be written formula_40, as well as variants\nsuch as formula_41 for formula_42. Indeed, following the usual conventions, those quantities are equal where defined: formula_39 is 1 if \"x\" > 0, 0 if \"x\" = 0, and undefined otherwise.\n\n"}
{"id": "57124645", "url": "https://en.wikipedia.org/wiki?curid=57124645", "title": "Kubilius model", "text": "Kubilius model\n\nIn mathematics, the Kubilius model relies on a clarification and extension of a finite probability space on which the behaviour of additive arithmetic functions can be modeled by sum of independent random variables.\n\nThe method was introduced in Jonas Kubilius's monograph \"Tikimybiniai metodai skaičių teorijoje\" (published in Lithuanian in 1959) / \"Probabilistic Methods in the Theory of Numbers\" (published in English in 1964) .\n\nEugenijus Manstavičius and Fritz Schweiger wrote about Kubilius's work in 1992, \"the most impressive work has been done on the statistical theory of arithmetic functions which almost created a new research area called Probabilistic Number Theory. A monograph (\"Probabilistic Methods in the Theory of Numbers\") devoted to this topic was translated into English in 1964 and became very influential.\"\n\n"}
{"id": "8269328", "url": "https://en.wikipedia.org/wiki?curid=8269328", "title": "List of things named after Srinivasa Ramanujan", "text": "List of things named after Srinivasa Ramanujan\n\nSrinivasa Ramanujan (1887 – 1920) is the eponym of all of the topics listed below.\n\n\n\n\n\n"}
{"id": "40735675", "url": "https://en.wikipedia.org/wiki?curid=40735675", "title": "Liénard–Chipart criterion", "text": "Liénard–Chipart criterion\n\nIn control system theory, the Liénard–Chipart criterion is a stability criterion modified from the Routh–Hurwitz stability criterion, proposed by A. Liénard and M. H. Chipart. This criterion has a computational advantage over the Routh–Hurwitz criterion because it involves only about half the number of determinant computations.\n\nThe Routh–Hurwitz stability criterion says that a necessary and sufficient condition for all the roots of the polynomial with real coefficients\n\nto have negative real parts (i.e. formula_2 is Hurwitz stable) is that\n\nwhere formula_4 is the \"i\"-th principal minor of the Hurwitz matrix associated with formula_2.\n\nUsing the same notation as above, the Liénard–Chipart criterion is that formula_2 is Hurwitz-stable if and only if any one of the four conditions is satisfied:\n\nHence one can see that by choosing one of these conditions, the number of determinants required to be evaluated is reduced.\n"}
{"id": "14557383", "url": "https://en.wikipedia.org/wiki?curid=14557383", "title": "Lucky numbers of Euler", "text": "Lucky numbers of Euler\n\nEuler's “lucky” numbers are positive integers \"n\" such that for all integers \"k\" with , the polynomial produces a prime number.\n\nWhen \"k\" is equal to \"n\", the value cannot be prime anymore since is divisible by \"n\". Since the polynomial can be rewritten as , using the integers \"k\" with produces the same set of numbers as .\n\nLeonhard Euler published the polynomial which produces prime numbers for all integer values of \"k\" from 1 to 40. Only 7 lucky numbers of Euler exist, namely 1, 2, 3, 5, 11, 17 and 41 .\n\nThe primes of the form \"k\" - \"k\" + 41 are\n\nThese numbers are not related to the lucky numbers generated by a sieve algorithm. In fact, the only number which is both lucky and Euler-lucky is 3, since all other Euler-lucky numbers are congruent to 2 mod 3, but no lucky numbers are congruent to 2 mod 3.\n\n\n"}
{"id": "34063376", "url": "https://en.wikipedia.org/wiki?curid=34063376", "title": "Matrix Chernoff bound", "text": "Matrix Chernoff bound\n\nFor certain applications in linear algebra, it is useful to know properties of the probability distribution of the largest eigenvalue of a finite sum of random matrices. Suppose formula_1 is a finite sequence of random matrices. Analogous to the well-known Chernoff bound for sums of scalars, a bound on the following is sought for a given parameter \"t\":\nThe following theorems answer this general question under various assumptions; these assumptions are named below by analogy to their classical, scalar counterparts. All of these theorems can be found in , as the specific application of a general result which is derived below. A summary of related works is given.\n\nConsider a finite sequence formula_3 of fixed,\nself-adjoint matrices with dimension formula_4, and let formula_5 be a finite sequence of independent standard normal or independent Rademacher random variables.\n\nThen, for all formula_6,\nwhere\n\nConsider a finite sequence formula_9 of fixed, self-adjoint matrices with dimension formula_10, and let formula_5 be a finite sequence of independent standard normal or independent Rademacher random variables.\nDefine the variance parameter\n\nThen, for all formula_6,\n\nThe classical Chernoff bounds concern the sum of independent, nonnegative, and uniformly bounded random variables.\nIn the matrix setting, the analogous theorem concerns a sum of positive-semidefinite random matrices subjected to a uniform eigenvalue bound.\n\nConsider a finite sequence formula_1 of independent, random, self-adjoint matrices with dimension formula_4.\nAssume that each random matrix satisfies\nalmost surely.\n\nDefine\nThen\n\nConsider a sequence formula_21 of independent, random, self-adjoint matrices that satisfy\nalmost surely.\n\nCompute the minimum and maximum eigenvalues of the average expectation,\nThen\nThe binary information divergence is defined as\nfor formula_27.\n\nIn the scalar setting, Bennett and Bernstein inequalities describe the upper tail of a sum of independent, zero-mean random variables that are either bounded or subexponential. In the matrix\ncase, the analogous results concern a sum of zero-mean random matrices.\n\nConsider a finite sequence formula_1 of independent, random, self-adjoint matrices with dimension formula_4.\nAssume that each random matrix satisfies\nalmost surely.\n\nCompute the norm of the total variance,\n\nThen, the following chain of inequalities holds for all formula_32:\nThe function formula_34 is defined as formula_35 for formula_36.\n\nConsider a finite sequence formula_1 of independent, random, self-adjoint matrices with dimension formula_4.\nAssume that \nfor formula_40.\n\nCompute the variance parameter,\n\nThen, the following chain of inequalities holds for all formula_32:\n\nConsider a finite sequence formula_44 of independent, random, matrices with dimension formula_10.\nAssume that each random matrix satisfies\nalmost surely.\nDefine the variance parameter\n\nThen, for all formula_32\n\nholds.\n\nThe scalar version of Azuma's inequality states that a scalar martingale exhibits normal concentration about its mean value, and the scale for deviations is controlled by the total maximum squared range of the difference sequence.\nThe following is the extension in matrix setting.\n\nConsider a finite adapted sequence formula_1 of self-adjoint matrices with dimension formula_4, and a fixed sequence formula_3 of self-adjoint matrices that satisfy\nalmost surely.\n\nCompute the variance parameter\n\nThen, for all formula_32\n\nThe constant 1/8 can be improved to 1/2 when there is additional information available. One case occurs when each summand formula_57 is conditionally symmetric.\nAnother example requires the assumption that formula_57 commutes almost surely with formula_59.\n\nPlacing addition assumption that the summands in Matrix Azuma are independent gives a matrix extension of Hoeffding's inequalities.\n\nConsider a finite sequence formula_1 of independent, random, self-adjoint matrices with dimension formula_4, and let formula_3 be a sequence of fixed self-adjoint matrices.\nAssume that each random matrix satisfies\nalmost surely.\n\nThen, for all formula_32\nwhere\n\nAn improvement of this result was established in :\nfor all formula_32\nwhere\n\nIn scalar setting, McDiarmid's inequality provides one common way of bounding the differences by applying Azuma's inequality to a Doob martingale. A version of the bounded differences inequality holds in the matrix setting.\n\nLet formula_70 be an independent, family of random variables, and let formula_71 be a function that maps formula_72 variables to a self-adjoint matrix of dimension formula_4.\nConsider a sequence formula_3 of fixed self-adjoint matrices that satisfy\nwhere formula_76 and formula_77 range over all possible values of formula_78 for each index formula_79.\nCompute the variance parameter\n\nThen, for all formula_32\nwhere formula_83.\n\nAn improvement of this result was established in (see also ):\nfor all formula_32\nwhere formula_83 and\nformula_8\n\nThe first bounds of this type were derived by . Recall the theorem above for self-adjoint matrix Gaussian and Rademacher bounds:\nFor a finite sequence formula_3 of fixed,\nself-adjoint matrices with dimension formula_4 and for formula_5 a finite sequence of independent standard normal or independent Rademacher random variables, then \nwhere\nAhlswede and Winter would give the same result, except with \nBy comparison, the formula_94 in the theorem above commutes formula_95 and formula_96; that is, it is the largest eigenvalue of the sum rather than the sum of the largest eigenvalues. It is never larger than the Ahlswede–Winter value (by the norm triangle inequality), but can be much smaller. Therefore, the theorem above gives a tighter bound than the Ahlswede–Winter result.\n\nThe chief contribution of was the extension of the Laplace-transform method used to prove the scalar Chernoff bound (see Chernoff bound#Theorem for additive form (absolute error)) to the case of self-adjoint matrices. The procedure given in the derivation below. All of the recent works on this topic follow this same procedure, and the chief differences follow from subsequent steps. Ahlswede & Winter use the Golden–Thompson inequality to proceed, whereas Tropp uses Lieb's Theorem.\n\nSuppose one wished to vary the length of the series (\"n\") and the dimensions of the\nmatrices (\"d\") while keeping the right-hand side approximately constant. Then\nn must vary approximately as the log of \"d\". Several papers have attempted to establish a bound without a dependence on dimensions. Rudelson and Vershynin give a result for matrices which are the outer product of two vectors. provide a result without the dimensional dependence for low rank matrices. The original result was derived independently from the Ahlswede–Winter approach, but proves a similar result using the Ahlswede–Winter approach.\n\nFinally, Oliveira proves a result for matrix martingales independently from the Ahlswede–Winter framework. Tropp slightly improves on the result using the Ahlswede–Winter framework. Neither result is presented in this article.\n\nThe Laplace transform argument found in is a significant result in its own right:\nLet formula_97 be a random self-adjoint matrix. Then\n\nTo prove this, fix formula_99. Then\nThe second-to-last inequality is Markov's inequality. The last inequality holds since formula_101. Since the left-most quantity is independent of formula_102, the infimum over formula_103 remains an upper bound for it.\n\nThus, our task is to understand formula_104 Nevertheless, since trace and expectation are both linear, we can commute them, so it is sufficient to consider formula_105, which we call the matrix generating function. This is where the methods of and diverge. The immediately following presentation follows .\n\nThe Golden–Thompson inequality implies that \nSuppose formula_107. We can find an upper bound for formula_108 by iterating this result. Noting that formula_109, then\nIterating this, we get\n\nSo far we have found a bound with an infimum over formula_102. In turn, this can be bounded. At any rate, one can see how the Ahlswede–Winter bound arises as the sum of largest eigenvalues.\n\nThe major contribution of is the application of Lieb's theorem where had applied the Golden–Thompson inequality. Tropp's corollary is the following: If formula_113 is a fixed self-adjoint matrix and formula_114 is a random self-adjoint matrix, then\nProof: Let formula_116. Then Lieb's theorem tells us that\nis concave.\nThe final step is to use Jensen's inequality to move the expectation inside the function:\n\nThis gives us the major result of the paper: the subadditivity of the log of the matrix generating function.\n\nLet formula_119 be a finite sequence of independent, random self-adjoint matrices. Then for all formula_120,\n\nProof: It is sufficient to let formula_122. Expanding the definitions, we need to show that \n\nTo complete the proof, we use the law of total expectation. Let formula_124 be the expectation conditioned on formula_125. Since we assume all the formula_126 are independent, \nDefine formula_128.\n\nFinally, we have\nwhere at every step m we use Tropp's corollary with \n\nThe following is immediate from the previous result:\nAll of the theorems given above are derived from this bound; the theorems consist in various ways to bound the infimum. These steps are significantly simpler than the proofs given.\n\n"}
{"id": "24836552", "url": "https://en.wikipedia.org/wiki?curid=24836552", "title": "Mean dependence", "text": "Mean dependence\n\nIn probability theory, a random variable \"Y\" is said to be mean independent of random variable \"X\" if and only if its conditional mean E(\"Y\" | \"X\"=\"x\") equals its (unconditional) mean E(\"Y\") for all \"x\" such that the probability that \"X\" = \"x\" is not zero. \"Y\" is said to be mean dependent on \"X\" if \"E\"(\"Y\" | \"X\"=\"x\") is not constant for all \"x\" for which the probability is non-zero.\n\nAccording to and , stochastic independence implies mean independence, but the converse is not true.\n\nMoreover, mean independence implies uncorrelatedness while the converse is not true.\n\nThe concept of mean independence is often used in econometrics to have a middle ground between the strong assumption of independent random variables (formula_1) and the weak assumption of uncorrelated random variables formula_2\n\n"}
{"id": "19873", "url": "https://en.wikipedia.org/wiki?curid=19873", "title": "Measure (mathematics)", "text": "Measure (mathematics)\n\nIn mathematical analysis, a measure on a set is a systematic way to assign a number to each suitable subset of that set, intuitively interpreted as its size. In this sense, a measure is a generalization of the concepts of length, area, and volume. A particularly important example is the Lebesgue measure on a Euclidean space, which assigns the conventional length, area, and volume of Euclidean geometry to suitable subsets of the -dimensional Euclidean space . For instance, the Lebesgue measure of the interval in the real numbers is its length in the everyday sense of the word – specifically, 1.\n\nTechnically, a measure is a function that assigns a non-negative real number or to (certain) subsets of a set (\"see\" Definition below). It must further be countably additive: the measure of a 'large' subset that can be decomposed into a finite (or countably infinite) number of 'smaller' disjoint subsets is equal to the sum of the measures of the \"smaller\" subsets. In general, if one wants to associate a \"consistent\" size to \"each\" subset of a given set while satisfying the other axioms of a measure, one only finds trivial examples like the counting measure. This problem was resolved by defining measure only on a sub-collection of all subsets; the so-called \"measurable\" subsets, which are required to form a -algebra. This means that countable unions, countable intersections and complements of measurable subsets are measurable. Non-measurable sets in a Euclidean space, on which the Lebesgue measure cannot be defined consistently, are necessarily complicated in the sense of being badly mixed up with their complement. Indeed, their existence is a non-trivial consequence of the axiom of choice.\n\nMeasure theory was developed in successive stages during the late 19th and early 20th centuries by Émile Borel, Henri Lebesgue, Johann Radon, and Maurice Fréchet, among others. The main applications of measures are in the foundations of the Lebesgue integral, in Andrey Kolmogorov's axiomatisation of probability theory and in ergodic theory. In integration theory, specifying a measure allows one to define integrals on spaces more general than subsets of Euclidean space; moreover, the integral with respect to the Lebesgue measure on Euclidean spaces is more general and has a richer theory than its predecessor, the Riemann integral. Probability theory considers measures that assign to the whole set the size 1, and considers measurable subsets to be events whose probability is given by the measure. Ergodic theory considers measures that are invariant under, or arise naturally from, a dynamical system.\n\nLet be a set and a -algebra over . A function from to the extended real number line is called a measure if it satisfies the following properties:\n\n\nOne may require that at least one set has finite measure. Then the empty set automatically has measure zero because of countable additivity, because\nwhich implies (since the sum on the right thus converges to a finite value) that formula_1.\n\nIf only the second and third conditions of the definition of measure above are met, and takes on at most one of the values , then is called a signed measure.\n\nThe pair is called a measurable space, the members of Σ are called measurable sets. If formula_6 and formula_7 are two measurable spaces, then a function formula_8 is called measurable if for every -measurable set formula_9, the inverse image is -measurable – i.e.: formula_10. In this setup, the composition of measurable functions is measurable, making the measurable spaces and measurable functions a category, with the measurable spaces as objects and the set of measurable functions as arrows. See also Measurable function#Term usage variations about another setup.\n\nA triple is called a measure space. A probability measure is a measure with total measure one – i.e. . A probability space is a measure space with a probability measure.\n\nFor measure spaces that are also topological spaces various compatibility conditions can be placed for the measure and the topology. Most measures met in practice in analysis (and in many cases also in probability theory) are Radon measures. Radon measures have an alternative definition in terms of linear functionals on the locally convex space of continuous functions with compact support. This approach is taken by Bourbaki (2004) and a number of other sources. For more details, see the article on Radon measures.\n\nSome important measures are listed here.\n\n\nOther 'named' measures used in various theories include: Borel measure, Jordan measure, ergodic measure, Euler measure, Gaussian measure, Baire measure, Radon measure, Young measure, and Loeb measure.\nIn physics an example of a measure is spatial distribution of mass (see e.g., gravity potential), or another non-negative extensive property, conserved (see conservation law for a list of these) or not. Negative values lead to signed measures, see \"generalizations\" below.\n\n\nLet be a measure.\n\nIf and are measurable sets with then\n\nFor any countable sequence of (not necessarily disjoint) measurable sets in Σ:\n\nIf are measurable sets and is a subset of for all , then the union of the sets is measurable, and\n\nIf are measurable sets and for all , then the intersection of the sets is measurable; furthermore, if at least one of the has finite measure, then\n\nThis property is false without the assumption that at least one of the has finite measure. For instance, for each , let , which all have infinite Lebesgue measure, but the intersection is empty.\n\nA measure space is called finite if is a finite real number (rather than ∞). Nonzero finite measures are analogous to probability measures in the sense that any finite measure is proportional to the probability measure formula_15. A measure is called \"σ-finite\" if can be decomposed into a countable union of measurable sets of finite measure. Analogously, a set in a measure space is said to have a \"σ-finite measure\" if it is a countable union of sets with finite measure.\n\nFor example, the real numbers with the standard Lebesgue measure are σ-finite but not finite. Consider the closed intervals for all integers ; there are countably many such intervals, each has measure 1, and their union is the entire real line. Alternatively, consider the real numbers with the counting measure, which assigns to each finite set of reals the number of points in the set. This measure space is not σ-finite, because every set with finite measure contains only finitely many points, and it would take uncountably many such sets to cover the entire real line. The σ-finite measure spaces have some very convenient properties; σ-finiteness can be compared in this respect to the Lindelöf property of topological spaces. They can be also thought of as a vague generalization of the idea that a measure space may have 'uncountable measure'.\nA measure is said to be s-finite if it is a countable sum of bounded measures. S-finite measures are more general than sigma-finite ones and have applications in the theory of stochastic processes.\n\nA measurable set is called a \"null set\" if . A subset of a null set is called a \"negligible set\". A negligible set need not be measurable, but every measurable negligible set is automatically a null set. A measure is called \"complete\" if every negligible set is measurable.\n\nA measure can be extended to a complete one by considering the σ-algebra of subsets which differ by a negligible set from a measurable set , that is, such that the symmetric difference of and is contained in a null set. One defines to equal .\n\nMeasures are required to be countably additive. However, the condition can be strengthened as follows.\nFor any set formula_16 and any set of nonnegative formula_17 define:\nThat is, we define the sum of the formula_19 to be the supremum of all the sums of finitely many of them.\n\nA measure formula_20 on formula_21 is formula_22-additive if for any formula_23 and any family of disjoint sets formula_24 the following hold:\nNote that the second condition is equivalent to the statement that the ideal of null sets is formula_22-complete.\n\nIf the axiom of choice is assumed to be true, it can be proved that not all subsets of Euclidean space are Lebesgue measurable; examples of such sets include the Vitali set, and the non-measurable sets postulated by the Hausdorff paradox and the Banach–Tarski paradox.\n\nFor certain purposes, it is useful to have a \"measure\" whose values are not restricted to the non-negative reals or infinity. For instance, a countably additive set function with values in the (signed) real numbers is called a \"signed measure\", while such a function with values in the complex numbers is called a \"complex measure\". Measures that take values in Banach spaces have been studied extensively. A measure that takes values in the set of self-adjoint projections on a Hilbert space is called a \"projection-valued measure\"; these are used in functional analysis for the spectral theorem. When it is necessary to distinguish the usual measures which take non-negative values from generalizations, the term positive measure is used. Positive measures are closed under conical combination but not general linear combination, while signed measures are the linear closure of positive measures.\n\nAnother generalization is the \"finitely additive measure\", which are sometimes called contents. This is the same as a measure except that instead of requiring \"countable\" additivity we require only \"finite\" additivity. Historically, this definition was used first. It turns out that in general, finitely additive measures are connected with notions such as Banach limits, the dual of \"L\" and the Stone–Čech compactification. All these are linked in one way or another to the axiom of choice.\n\nA charge is a generalization in both directions: it is a finitely additive, signed measure.\n\n\n"}
{"id": "51474451", "url": "https://en.wikipedia.org/wiki?curid=51474451", "title": "Menger space", "text": "Menger space\n\nIn mathematics, a Menger space is a topological space that satisfies a certain a basic selection principle that generalizes σ-compactness. A Menger space is a space in which for every sequence of open covers formula_1 of the space there are finite sets formula_2 such that the family formula_3 covers the space.\n\nIn 1924, Karl Menger \nintroduced the following basis property for metric spaces: \nEvery basis of the topology contains a countable family of sets with vanishing \ndiameters that covers the space. Soon thereafter, \nWitold Hurewicz \nobserved that Menger's basis property can be reformulated to the above form using sequences of open covers.\n\nMenger conjectured that in ZFC every Menger metric space is σ-compact. \nFremlin and Miller \nproved that Menger's conjecture is false, by showing that there is,\nin ZFC, a set of real numbers that is Menger but not σ-compact. \nThe Fremlin-Miller proof was dichotomic, and the set witnessing the failure\nof the conjecture heavily depends on whether a certain (undecidable) axiom\nholds or not.\n\nBartoszyński and Tsaban\ngave a uniform ZFC example of a Menger subset of the real line that is not σ-compact.\n\nFor subsets of the real line, the Menger property can be characterized using continuous functions into the Baire space formula_4.\nFor functions formula_5, write formula_6 if formula_7 for all but finitely many natural numbers formula_8. A subset formula_9 of formula_4 is dominating if for each function formula_11 there is a function formula_12 such that formula_13. Hurewicz proved that a subset of the real line is Menger iff every continuous image of that space into the Baire space is not dominating. In particular, every subset of the real line of cardinality less than the dominating number formula_14 is Menger.\n\nThe cardinality of Bartoszyński and Tsaban's counter-example to Menger's conjecture is\nformula_14.\n\n"}
{"id": "14766741", "url": "https://en.wikipedia.org/wiki?curid=14766741", "title": "Model K", "text": "Model K\n\nThe Model K was an early relay binary adder built in 1937 by George Robert Stibitz, a scientist at Bell Laboratories.\n"}
{"id": "35263118", "url": "https://en.wikipedia.org/wiki?curid=35263118", "title": "Philip Hartman", "text": "Philip Hartman\n\nPhilip Hartman (May 16, 1915 – August 28, 2015) was an American mathematician at Johns Hopkins University working on differential equations who introduced the Hartman–Grobman theorem. He served as Chairman of the Mathematics Department at Johns Hopkins for several years. He has an Erdös number of 2.\n\nHis book gives a necessary and sufficient condition for solutions of ordinary initial value problems to be unique and to depend on a class C manner on the initial conditions for solutions.\n\nHe died in August 2015 at the age of 100.\n"}
{"id": "52991874", "url": "https://en.wikipedia.org/wiki?curid=52991874", "title": "Pohlke's theorem", "text": "Pohlke's theorem\n\nPohlke's theorem is the fundamental theorem of axonometry. It was established 1853 by the German painter and teacher of descriptive geometry Karl Wilhelm Pohlke. The first proof of the theorem was published 1864 by the German mathematician Hermann Amandus Schwarz, who was a student of Pohlke. Therefore the theorem is sometimes called theorem of Pohlke and Schwarz, too.\n\nFor a mapping of a unit cube, one has to apply an additional scaling either in the space or in the plane. Because a parallprojection and a scaling preserves ratios one can map an arbitrary point formula_4 by the axonometric procedure below.\n\nPohlke's theorem can be stated in terms of linear algebra as:\n\nPohlke's theorem is the justification for the following easy procedure to construct a scaled parallel projection of a 3-dimensional object using coordinates,:\nIn order to get undistorted pictures, one has to choose the images of the axes and the forshortenings carefully (see Axonometry). In order to get an orthographic projection only the images of the axes are free and the forshortenings are determined. (see ).\n\nSchwarz formulated and proved the more general statement:\n\nand used a theorem of L’Huilier: \n\n\n"}
{"id": "30067072", "url": "https://en.wikipedia.org/wiki?curid=30067072", "title": "Polyvector field", "text": "Polyvector field\n\nIn mathematics, a multivector field, polyvector field of degree \"k\" , or \"k\"-vector field, on a manifold \"M\", is a section of the \"k\"th exterior power of the tangent bundle, formula_1. Polyvector fields of degree \"k\" are dual to \"k\"-forms.\n\n"}
{"id": "44224167", "url": "https://en.wikipedia.org/wiki?curid=44224167", "title": "Process validation", "text": "Process validation\n\nProcess Validation is the analysis of data gathered throughout the design and manufacturing of a product in order to confirm that the process can reliably output products of a determined standard. Regulatory authorities like EMA and FDA have published guidelines relating to process validation. The purpose of process validation is to ensure varied inputs lead to consistent and high quality outputs. Process validation is an ongoing process that must be frequently adapted as manufacturing feedback is gathered. End-to-end validation of production processes is essential in determining product quality because quality cannot always be determined by finished-product inspection. Process validation can be broken down into 3 steps: process design, process qualification, and continued process verification.\n\nIn this stage data from the development phase are gathered and analyzed to define the commercial manufacturing process. By understanding the commercial process a framework for quality specifications can be established and used as the foundation of a control strategy. Process design is the first of three stages of process validation. Data from the development phase is gathered and analyzed to understand end-to-end system processes. These data are used to establish benchmarks for quality and production control.\n\nDesign of experiments is used to discover possible relationships and sources of variation as quickly as possible. A cost benefit analysis should be conducted to determine if such an operation is necessary.\n\nQuality by Design is an approach to pharmaceutical manufacturing that stresses quality should be built into products rather than tested into products; that product quality should be considered at the earliest possible stage rather than at the end of the manufacturing process. Input variables are isolated in order to identify the root cause of potential quality issues and the manufacturing process is adapted accordingly.\n\nProcess Analytical Technology is used to measure critical process parameters (CPP) and critical quality attributes (CQA). PAT facilitates measurement of quantitative production variables in real time and allows access to relevant manufacturing feedback. PAT can also be used in the design process to generate a process qualification.\n\nCritical Process Parameters Operating parameters that are considered essential to maintaining product output within specified quality target guidelines.\n\nCritical Quality Attributes (CQA) are chemical, physical, biological and microbiological attributes that can be defined, measured, and continually monitored to ensure final product outputs remain within acceptable quality limits. CQA are an essential aspect of a manufacturing control strategy and should be identified in stage 1 of Process Validation: Process design. During this stage acceptable limits, baselines, and data collection and measurement protocols should be established. Data from the design process and data collected during production should be kept by the manufacturer and used to evaluated product quality and process control. Historical data can also help manufacturers better understand operational process and input variables as well as better identify true deviations from quality standards compared to false positives. Should a serious product quality issue arise, historical data would be essential in identifying the sources of errors and implementing corrective measures.\n\nIn this stage the process design is assessed to conclude if the process is able to meet determined manufacturing targets. In this stage all production processes and manufacturing equipment is proofed to confirm quality and output capabilities. Critical quality attributes are evaluated and critical process parameters taken into account to confirm product quality. Once the process qualification stage has been successfully accomplished production can begin. Process Qualification is the second phase of process validation...\n\nContinued Process Verification is the ongoing monitoring of all aspects of the production cycle. It aims to ensure that all levels of production are controlled and regulated. Deviations from prescribed output methods and final product irregularities are flagged by a process analytics database system. The FDA requires production data be recorded (FDA requirements (§ 211.180(e)). Continued process verification is stage 3 of process validation.\n\nThe European Medicines Agency defines a similar process known as Ongoing Process Verification. This alternative method of process validation is recommended by the EMA for validating processes on a continuous basis. Continuous Process Verification analyses Critical Process Parameters and Critical Quality Attributes in real time to confirm production remain within acceptable levels and meet standards set by ICH Q8, Pharmaceutical Quality Systems, and Good manufacturing practice.\n\n\n"}
{"id": "11950956", "url": "https://en.wikipedia.org/wiki?curid=11950956", "title": "Salvatore Pincherle", "text": "Salvatore Pincherle\n\nSalvatore Pincherle (March 11, 1853 – July 10, 1936) was an Italian mathematician. He contributed significantly to (and arguably helped to found) the field of functional analysis, established the Italian Mathematical Union (Italian: \"Unione Matematica Italiana\"), and was president of the Third International Congress of Mathematicians. The Pincherle derivative is named after him.\n\nPincherle was born into a Jewish family in Trieste (then part of the Austrian Littoral) and spent his childhood in Marseille, France. After completing his basic schooling in Marseille, he left in 1869 to study mathematics at the University of Pisa, where he was a student under both Enrico Betti and Ulisse Dini. After he graduated in 1874, he taught at a school in Pavia until he received a scholarship in 1877.\n\nAfter winning the scholarship and studying abroad at the University of Berlin, Pincherle met Karl Weierstrass. Pincherle contributed to Weierstrass' theory of analytic functions, and in 1880, influenced by Weierstrass, he wrote an expository paper in the \"Giornale di Matematiche\", which proved to be a significant paper in the field of analysis. Throughout his life, Pincherle's work greatly reflected the influence that Weierstrass had on him. He later collaborated with Vito Volterra and explored Laplace transforms and other parts of functional analysis.\n\nFrom 1880 until 1928, Pincherle was a Professor of Mathematics at the University of Bologna. In 1901, collaborating with Ugo Amaldi, he published his main scientific book, \"Le Operazioni Distributive e loro Applicazioni all'Analisi\".\n\nIn Bologna in 1922, he established the Italian Mathematical Union and became its first President and held the position until 1936. In 1924, he attended the Second International Congress of Mathematicians in Toronto, Ontario, Canada. Four years later, he became President of the Third International Congress and played a significant role in re-admitting German mathematicians after a ban imposed because of World War I. At this Congress, Jacques Hadamard declared in his review lecture that Pincherle was one of the most prominent founders of functional analysis. Following the Third Congress, Pincherle retired from university.\n\nIn honor of the centenary of his birth, the Italian Mathematical Union edited a selection of 62 of his notes and treatises; they were published in 1954 in Rome.\n\n"}
{"id": "1830167", "url": "https://en.wikipedia.org/wiki?curid=1830167", "title": "Saturation arithmetic", "text": "Saturation arithmetic\n\nSaturation arithmetic is a version of arithmetic in which all operations such as addition and multiplication are limited to a fixed range between a minimum and maximum value. \n\nIf the result of an operation is greater than the maximum, it is set (\"clamped\") to the maximum; if it is below the minimum, it is clamped to the minimum. The name comes from how the value becomes \"saturated\" once it reaches the extreme values; further additions to a maximum or subtractions from a minimum will not change the result.\n\nFor example, if the valid range of values is from -100 to 100, the following operations produce the following values:\n\nAs can be seen from these examples, familiar properties like associativity and distributivity may fail in saturation arithmetic. This makes it unpleasant to deal with in abstract mathematics, but it has an important role to play in digital hardware and algorithms.\n\nTypically, general-purpose microprocessors do not implement integer arithmetic operations using saturation arithmetic; instead, they use the easier-to-implement modular arithmetic, in which values exceeding the maximum value \"wrap around\" to the minimum value, like the hours on a clock passing from 12 to 1. In hardware, modular arithmetic with a minimum of zero and a maximum of r-1, where r is the radix can be implemented by simply discarding all but the lowest \"n\" digits. For binary hardware, which the vast majority of modern hardware is, the radix is 2 and the digits are bits.\n\nHowever, although more difficult to implement, saturation arithmetic has numerous practical advantages. The result is as numerically close to the true answer as possible; for 8-bit binary signed arithmetic, when the correct answer is 130, it is considerably less surprising to get an answer of 127 from saturating arithmetic than to get an answer of −126 from modular arithmetic. Likewise, for 8-bit binary unsigned arithmetic, when the correct answer is 258, it is less surprising to get an answer of 255 from saturating arithmetic than to get an answer of 2 from modular arithmetic.\n\nSaturation arithmetic also enables overflow of additions and multiplications to be detected consistently without an overflow bit or excessive computation, by simple comparison with the maximum or minimum value (provided the datum is not permitted to take on these values).\n\nAdditionally, saturation arithmetic enables efficient algorithms for many problems, particularly in digital signal processing. For example, adjusting the volume level of a sound signal can result in overflow, and saturation causes significantly less distortion to the sound than wrap-around. In the words of researchers G. A. Constantinides et al.:\n\nSaturation arithmetic operations are available on many modern platforms, and in particular was one of the extensions made by the Intel MMX platform, specifically for such signal processing applications. This functionality is also available in wider versions in the SSE2 and AVX2 integer instruction sets.\n\nSaturation arithmetic for integers has also been implemented in software for a number of programming languages including C, C++, such as the GNU Compiler Collection,\nand Eiffel. This helps programmers anticipate and understand the effects of overflow better. On the other hand, saturation is challenging to implement efficiently in software on a machine with only modular arithmetic operations, since simple implementations require branches that create huge pipeline delays. However, it is possible to implement saturating addition and subtraction in software without branches, using only modular arithmetic and bitwise logical operations that are available on all modern CPUs and their predecessors, including all x86 CPUs (back to the original Intel 8086) and some popular 8-bit CPUs (some of which, such as the Zilog Z80, are still in production). (However, on simple 8-bit and 16-bit CPUs, a branching algorithm might actually be faster if programmed in assembly, since there are no pipelines to stall and each instruction always takes multiple clock cycles.)\n\nAlthough saturation arithmetic is less popular for integer arithmetic in hardware, the IEEE floating-point standard, the most popular abstraction for dealing with approximate real numbers, uses a form of saturation in which overflow is converted into \"infinity\" or \"negative infinity\", and any other operation on this result continues to produce the same value. This has the advantage over simple saturation that later operations which decrease the value will not end up producing a misleadingly \"reasonable\" result, such as in the computation formula_1. Alternatively, there may be special states such as \"exponent overflow\" (and \"exponent underflow\") that will similarly persist through subsequent operations, or cause immediate termination, or be tested for as in codice_1 as in FORTRAN for the IBM704 (October 1956)\n\n\n"}
{"id": "743106", "url": "https://en.wikipedia.org/wiki?curid=743106", "title": "Scott continuity", "text": "Scott continuity\n\nIn mathematics, given two partially ordered sets \"P\" and \"Q\", a function formula_1 between them is Scott-continuous (named after the mathematician Dana Scott) if it preserves all directed suprema, i.e. if for every directed subset \"D\" of \"P\" with supremum in \"P\" its image has a supremum in \"Q\", and that supremum is the image of the supremum of \"D\": that is, formula_2, where formula_3 is the directed join. When formula_4 is the poset of truth values, i.e. Sierpiński space, then the formula_5 are characteristic functions, and thus, Sierpiński space is the classifying topos for open sets.\n\nA subset \"O\" of a partially ordered set \"P\" is called Scott-open if it is an upper set and if it is inaccessible by directed joins, i.e. if all directed sets \"D\" with supremum in \"O\" have non-empty intersection with \"O\". The Scott-open subsets of a partially ordered set \"P\" form a topology on \"P\", the Scott topology. A function between partially ordered sets is Scott-continuous if and only if it is continuous with respect to the Scott topology.\n\nThe Scott topology was first defined by Dana Scott for complete lattices and later defined for arbitrary partially ordered sets.\n\nScott-continuous functions show up in the study of models for lambda calculi and the denotational semantics of computer programs.\n\nA Scott-continuous function is always monotonic.\n\nA subset of a partially ordered set is closed with respect to the Scott topology induced by the partial order if and only if it is a lower set and closed under suprema of directed subsets.\n\nA directed complete partial order (dcpo) with the Scott topology is always a Kolmogorov space (i.e., it satisfies the T separation axiom). However, a dcpo with the Scott topology is a Hausdorff space if and only if the order is trivial. The Scott-open sets form a complete lattice when ordered by inclusion.\n\nFor any topological space satisfying the T separation axiom, the topology induces an order relation on that space, the specialization order: if and only if every open neighbourhood of \"x\" is also an open neighbourhood of \"y\". The order relation of a dcpo \"D\" can be reconstructed from the Scott-open sets as the specialization order induced by the Scott topology. However, a dcpo equipped with the Scott topology need not be sober: the specialization order induced by the topology of a sober space makes that space into a dcpo, but the Scott topology derived from this order is finer than the original topology.\n\nThe open sets in a given topological space when ordered by inclusion form a lattice on which the Scott topology can be defined. A subset \"X\" of a topological space \"T\" is compact with respect to the topology on \"T\" (in the sense that every open cover of \"X\" contains a finite subcover of \"X\") if and only if the set of open neighbourhoods of \"X\" is open with respect to the Scott topology.\n\nFor CPO, the cartesian closed category of dcpo's, two particularly notable examples of Scott-continuous functions are curry and apply.\n\n"}
{"id": "49713321", "url": "https://en.wikipedia.org/wiki?curid=49713321", "title": "Segre's theorem", "text": "Segre's theorem\n\nIn projective geometry Segre's theorem, named after the Italian mathematician Beniamino Segre, is the statement:\n\nThis statement was assumed 1949 by the two Finnish mathematicians G. Järnefelt and P. Kustaanheimo and its proof was published in 1955 by B. Segre.\n\nA finite pappian projective plane can be imagined as the projective closure of the real plane (by a line at infinity), where the real numbers are replaced by a finite field . \"Odd order\" means that is odd. An oval is a curve similar to a circle (see definition below): any line meets it in at most 2 points and through any point of it there is exactly one tangent. The standard examples are the nondegenerate projective conic sections.\n\nFor pappian projective planes of \"even\" order there are always ovals which are not conics. In an infinite plane there exist ovals, which are not conics. In the real plane one just glues a half of a circle and a suitable ellipse smoothly.\n\nThe proof of Segre's theorem, shown below, uses the 3-point version of Pascal's theorem and a property of a finite field of odd order, namely, that the product of all the nonzero elements equals -1.\n\nIf formula_4 the line formula_2 is an \"exterior\" (or \"passing\") line; in case formula_6 a \"tangent line\" and if formula_7 the line is a \"secant line\".\n\nFor \"finite\" planes (i.e. the set of points is finite) we have a more convenient characterization:\n\nLet be formula_1 an oval in a pappian projective plane of characteristic formula_14. \nformula_1 is a nondegenerate conic if and only if statement (P3)\nholds:\nLet the projective plane be coordinatized inhomogeneously over a field formula_22 \nsuch that formula_23 is the tangent at formula_24, the x-axis is the tangent at the point formula_25 and formula_1 contains the point formula_27. Furthermore, we set formula_28 (s. image)\nThe oval formula_1 can be described by a function formula_30 such that:\nThe tangent at point formula_32 will be described using a function formula_33 such that its equation is\nHence (s. image)\nI: if formula_1 is a non degenerate conic we have formula_38 and formula_39 and one calculates easily that formula_40 are collinear.\n\nII: If formula_1 is an oval with property (P3), the slope of the line formula_42 is equal to the slope of the line formula_43, that means:\nWith formula_47 one gets\n(i) and (ii) yield\nA consequence of (ii) and (v) is \nHence formula_1 is a nondegenerate conic.\n\n\"Remark:\"\nProperty (P3) is fulfilled for any oval in a pappian projective plane of characteristic 2 with a nucleus (all tangents meet at the nucleus). Hence in this case (P3) is also true for non-conic ovals.\n\nAny oval formula_1 in a \"finite pappian\" projective plane of \"odd\" order is a nondegenerate conic section.\nFor the proof we show that the oval has property (P3) of the 3-point version of Pascal's theorem.\n\nLet be formula_16 any triangle on formula_1 and formula_40 defined as described in (P3). \nThe pappian plane will be coordinatized inhomogeneously over a finite field \nformula_22, such thatformula_61 and formula_62 is the common point of the tangents at formula_63 and formula_64. The oval formula_1 can be described using a bijective function formula_66:\nFor a point formula_68, the expression formula_69 is the slope of the secant formula_70 Because both the functions formula_71 and formula_72 are bijections from\nformula_73 to formula_74, and formula_75 a bijection from formula_73 onto formula_77, where formula_78 is the slope of the tangent at formula_79, for formula_80 we get\n(Remark: For formula_82 we have: \nformula_83)\nHence\nBecause the slopes of line formula_85 and tangent\nformula_86 both are formula_87, it follows that\nformula_88.\nThis is true for any triangle formula_89.\n\nSo: (P3) of the 3-point Pascal theorem holds and the oval is a non degenerate conic.\n\n\n"}
{"id": "659942", "url": "https://en.wikipedia.org/wiki?curid=659942", "title": "Square (algebra)", "text": "Square (algebra)\n\nIn mathematics, a square is the result of multiplying a number by itself. The verb \"to square\" is used to denote this operation. Squaring is the same as raising to the power 2, and is denoted by a superscript 2; for instance, the square of 3 may be written as 3, which is the number 9.\nIn some cases when superscripts are not available, as for instance in programming languages or plain text files, the notations \"x\"^2 or \"x\"**2 may be used in place of \"x\".\n\nThe adjective which corresponds to squaring is \"quadratic\".\n\nThe square of an integer may also be called a square number or a perfect square. In algebra, the operation of squaring is often generalized to polynomials, other expressions, or values in systems of mathematical values other than the numbers. For instance, the square of the linear polynomial is the quadratic polynomial .\n\nOne of the important properties of squaring, for numbers as well as in many other mathematical systems, is that (for all numbers ), the square of is the same as the square of its additive inverse . That is, the square function satisfies the identity . This can also be expressed by saying that the squaring function is an even function.\n\nThe squaring function preserves the order of positive numbers: larger numbers have larger squares. In other words, squaring is a monotonic function on the interval . On the negative numbers, numbers with greater absolute value have greater squares, so squaring is a monotonically decreasing function on . Hence, zero is the (global) minimum of the square function.\nThe square of a number is less than (that is ) if and only if , that is, if belongs to the open interval . This implies that the square of an integer is never less than the original number .\n\nEvery positive real number is the square of exactly two numbers, one of which is strictly positive and the other of which is strictly negative. Zero is the square of only one number, itself. For this reason, it is possible to define the square root function, which associates with a non-negative real number the non-negative number whose square is the original number.\n\nNo square root can be taken of a negative number within the system of real numbers, because squares of all real numbers are non-negative. The lack of real square roots for the negative numbers can be used to expand the real number system to the complex numbers, by postulating the imaginary unit , which is one of the square roots of −1.\n\nThe property \"every non negative real number is a square\" has been generalized to the notion of a real closed field, which is an ordered field such that every non negative element is a square and every polynomial of odd degree has a root. The real closed fields cannot be distinguished from the field of real numbers by their algebraic properties: every property of the real numbers, which may be expressed in first-order logic (that is expressed by a formula in which the variables that are quantified by ∀ or ∃ represent elements, not sets), is true for every real closed field, and conversely every property of the first-order logic, which is true for a specific real closed field is also true for the real numbers.\n\nThere are several major uses of the squaring function in geometry.\n\nThe name of the squaring function shows its importance in the definition of the area: it comes from the fact that the area of a square with sides of length   is equal to . The area depends quadratically on the size: the area of a shape  times larger is  times greater. This holds for areas in three dimensions as well as in the plane: for instance, the surface area of a sphere is proportional to the square of its radius, a fact that is manifested physically by the inverse-square law describing how the strength of physical forces such as gravity varies according to distance.\nThe squaring function is related to distance through the Pythagorean theorem and its generalization, the parallelogram law. Euclidean distance is not a smooth function: the three-dimensional graph of distance from a fixed point forms a cone, with a non-smooth point at the tip of the cone. However, the square of the distance (denoted or ), which has a paraboloid as its graph, is a smooth and analytic function. The dot product of a Euclidean vector with itself is equal to the square of its length: . This is further generalised to quadratic forms in linear spaces. The inertia tensor in mechanics is an example of a quadratic form. It demonstrates a quadratic relation of the moment of inertia to the size (length).\n\nThere are infinitely many Pythagorean triples, sets of three positive integers such that the sum of the squares of the first two equals the square of the third. Each of these triples gives the integer sides of a right triangle.\n\nThe squaring function is defined in any field or ring. An element in the image of this function is called a \"square\", and the inverse images of a square are called \"square roots\".\n\nThe notion of squaring is particularly important in the finite fields Z/\"pZ formed by the numbers modulo an odd prime number . A non-zero element of this field is called a quadratic residue if it is a square in Z/\"pZ, and otherwise, it is called a quadratic non-residue. Zero, while a square, is not considered to be a quadratic residue. Every finite field of this type has exactly quadratic residues and exactly quadratic non-residues. The quadratic residues form a group under multiplication. The properties of quadratic residues are widely used in number theory.\n\nMore generally, in rings, the squaring function may have different properties that are sometimes used to classify rings.\n\nZero may be the square of some non-zero elements. A commutative ring such that the square of a non zero element is never zero is called a reduced ring. More generally, in a commutative ring, a radical ideal is an ideal  such that formula_1 implies formula_2. Both notions are important in algebraic geometry, because of Hilbert's Nullstellensatz.\n\nAn element of a ring that is equal to its own square is called an idempotent. In any ring, 0 and 1 are idempotents. There are no other idempotents in fields and more generally in integral domains. However, \nthe ring of the integers modulo  has idempotents, where is the number of distinct prime factors of .\nA commutative ring in which every element is equal to its square (every element is idempotent) is called a Boolean ring; an example from computer science is the ring whose elements are binary numbers, with bitwise AND as the multiplication operation and bitwise XOR as the addition operation.\n\nIn a supercommutative algebra (away from 2), the square of any \"odd\" element equals to zero.\n\nIf \"A\" is a commutative semigroup, then one has\nIn the language of quadratic forms, this equality says that the squaring function is a \"form permitting composition\". In fact, the squaring function is the foundation upon which other quadratic forms are constructed which also permit composition. The procedure was introduced by L. E. Dickson to produce the octonions out of quaternions by doubling. The doubling method was formalized by A. A. Albert who started with the real number field ℝ and the squaring function, doubling it to obtain the complex number field with quadratic form x + y, and then doubling again to obtain quaternions. The doubling procedure is called the Cayley–Dickson process and the structures produced are composition algebras.\n\nThe squaring function can be used with ℂ as the start for another use of the Cayley–Dickson process leading to bicomplex, biquaternion, and bioctonion composition algebras.\n\nThe complex square function  is a twofold cover of the complex plane, such that each non-zero complex number has exactly two square roots. This map is related to parabolic coordinates.\n\nSquares are ubiquitous in algebra, more generally, in almost every branch of mathematics, and also in physics where many units are defined using squares and inverse squares: see below.\n\nLeast squares is the standard method used with overdetermined systems.\n\nSquaring is used in statistics and probability theory in determining the standard deviation of a set of values, or a random variable. The deviation of each value  from the mean formula_4 of the set is defined as the difference formula_5. These deviations are squared, then a mean is taken of the new set of numbers (each of which is positive). This mean is the variance, and its square root is the standard deviation. In finance, the volatility of a financial instrument is the standard deviation of its values.\n\n\n\n\n"}
{"id": "5577596", "url": "https://en.wikipedia.org/wiki?curid=5577596", "title": "Squashed entanglement", "text": "Squashed entanglement\n\nSquashed entanglement, also called CMI entanglement (CMI can be pronounced \"see me\"), is an information theoretic measure of quantum entanglement for a bipartite quantum system. If formula_1 is the density matrix of a system formula_2 composed of two subsystems formula_3 and formula_4, then the CMI entanglement formula_5 of system formula_2 is defined by\n\nwhere formula_7 is the set of all density matrices formula_8 for a tripartite system formula_9 such that formula_10. Thus, CMI entanglement is defined as an extremum of a functional formula_11 of formula_8. We define formula_11, the quantum Conditional Mutual Information (CMI), below. A more general version of Eq.(1) replaces the ``min\" (minimum) in Eq.(1) by an ``inf\" (infimum). When formula_1 is a pure state, \nformula_15, in agreement with the definition of entanglement of formation for pure states. Here formula_16 is the Von Neumann entropy of density matrix formula_17.\n\nCMI entanglement has its roots in classical (non-quantum) information theory, as we explain next.\n\nGiven any two random variables formula_18, classical information theory defines the mutual information, a measure of correlations, as\n\nFor three random variables formula_19, it defines the CMI as\nIt can be shown that formula_20.\n\nNow suppose formula_8 is the density matrix for a tripartite system formula_9. We will represent the partial trace of formula_8 with respect to one or two of its subsystems by formula_8 with the symbol for the traced system erased. For example, formula_25. One can define a quantum analogue of Eq.(2) by\n\nand a quantum analogue of Eq.(3) by\n\nIt can be shown that formula_26. This inequality is often called the strong-subadditivity property of quantum entropy.\n\nConsider three random variables formula_27 with probability distribution formula_28, which we will abbreviate as formula_29. For those special formula_30 of the form\n\nit can be shown that formula_31. Probability distributions of the form Eq.(6) are in fact described by the Bayesian network shown in Fig.1.\n\nOne can define a classical CMI entanglement by\n\nwhere formula_7 is the set of all probability distributions formula_33 in three random variables formula_19, such that formula_35for all formula_36. Because, given a probability distribution formula_37, one can always extend it to a probability distribution formula_38 that satisfies Eq.(6), it follows that the classical CMI entanglement, formula_39, is zero for all formula_37. The fact that formula_39 always vanishes is an important motivation for the definition of formula_42. We want a measure of quantum entanglement that vanishes in the classical regime.\n\nSuppose formula_43 for formula_44 is a set of non-negative numbers that add up to one, and formula_45 for formula_44 is an orthonormal basis for the Hilbert space associated with a quantum system formula_47. Suppose formula_48 and formula_49, for formula_44 are density matrices for the systems formula_3 and formula_4, respectively. It can be shown that the following density matrix\n\nsatisfies formula_53. Eq.(8) is the quantum counterpart of Eq.(6). Tracing the density matrix of Eq.(8) over formula_47, we get formula_55, which is a separable state. Therefore, formula_56 given by Eq.(1) vanishes for all separable states.\n\nWhen formula_1 is a pure state, one gets\nformula_15. This\nagrees with the definition of entanglement of formation for pure states, as given in Ben96.\n\nNext suppose formula_59 for formula_44 are some states in the Hilbert space associated with a quantum system formula_2. Let formula_7 be the set of density matrices defined previously for Eq.(1). Define formula_63 to be the set of all density matrices formula_8 that are elements of formula_7 and have the special form formula_66. It can be shown that if we replace in Eq.(1) the set formula_7 by its proper subset formula_63, then Eq.(1) reduces to the definition of entanglement of formation for mixed states, as given in Ben96. formula_7 and formula_63 represent different degrees of knowledge as to how formula_8 was created. formula_7 represents total ignorance.\n\nSince CMI entanglement reduces to entanglement of formation if one minimizes over formula_63 instead of formula_7, one expects that CMI entanglement inherits many desirable properties from entanglement of formation.\n\nThe important inequality formula_26 was first proved by Lieb and Ruskai in LR73.\n\nClassical CMI, given by Eq.(3), first entered information theory lore, shortly after Shannon's seminal 1948 paper and at least as early as 1954 in McG54. The quantum CMI, given by Eq.(5), was first defined by Cerf and Adami in Cer96. However, it appears that Cerf and Adami did not realize the relation of CMI to entanglement or the possibility of obtaining a measure of quantum entanglement based on CMI; this can be inferred, for example, from a later paper, Cer97, where they try to use formula_76 instead of CMI to understand entanglement. The first paper to explicitly point out a connection between CMI and quantum entanglement appears to be Tuc99.\n\nThe final definition Eq.(1) of CMI entanglement was first given by Tucci in a series of 6 papers. (See, for example, Eq.(8) of Tuc02 and Eq.(42) of Tuc01a). In Tuc00b, he pointed out the classical probability motivation of Eq.(1), and its connection to the definitions of entanglement of formation for pure and mixed states. In Tuc01a, he presented an algorithm and computer program, based on the Arimoto-Blahut method of information theory, for calculating CMI entanglement numerically. In Tuc01b, he calculated CMI entanglement analytically, for a mixed state of two qubits.\n\nIn Hay03, Hayden, Jozsa, Petz and Winter explored the connection between quantum CMI and separability.\n\nIt was not however, until Chr03, that it was shown that CMI entanglement is in fact an entanglement measure, i.e. that it does not increase under Local Operations and Classical Communication (LOCC). The proof adapted Ben96 arguments about entanglement of formation. In Chr03, they also proved many other interesting inequalities concerning CMI entanglement, including that it was additive, and explored its connection to other measures of entanglement. The name squashed entanglement first appeared in Chr03. In Chr05, Christandl and Winter calculated analytically the CMI entanglement of some interesting states.\n\nIn Ali03, Alicki and Fannes proved the continuity of CMI entanglement. In BCY10, Brandao, Christandl and Yard showed that CMI entanglement is zero if and only if the state is separable. In Hua14, Huang proved that computing squashed entanglement is NP-hard.\n\n\n"}
{"id": "19145800", "url": "https://en.wikipedia.org/wiki?curid=19145800", "title": "Standard part function", "text": "Standard part function\n\nIn non-standard analysis, the standard part function is a function from the limited (finite) hyperreal numbers to the real numbers. Briefly, the standard part function \"rounds off\" a finite hyperreal to the nearest real. It associates to every such hyperreal formula_1, the unique real formula_2 infinitely close to it, i.e. formula_3 is infinitesimal. As such, it is a mathematical implementation of the historical concept of adequality introduced by Pierre de Fermat, as well as Leibniz's Transcendental law of homogeneity.\n\nThe standard part function was first defined by Abraham Robinson who used the notation formula_4 for the standard part of a hyperreal formula_1 (see Robinson 1974). This concept plays a key role in defining the concepts of the calculus, such as continuity, the derivative, and the integral, in non-standard analysis. The latter theory is a rigorous formalisation of calculations with infinitesimals. The standard part of \"x\" is sometimes referred to as its shadow.\n\nNonstandard analysis deals primarily with the pair formula_6, where the hyperreals formula_7 are an ordered field extension of the reals formula_8, and contain infinitesimals, in addition to the reals. In the hyperreal line every real number has a collection of numbers (called a monad, or halo) of hyperreals infinitely close to it. The standard part function associates to a hyperreal \"x\", the unique standard real number \"x\" which is infinitely close to it. The relationship is expressed symbolically by writing\n\nThe standard part of any infinitesimal is 0. Thus if \"N\" is an infinite hypernatural, then 1/\"N\" is infinitesimal, and st(1/\"N\") = 0.\n\nIf a hyperreal formula_10 is represented by a Cauchy sequence formula_11 in the ultrapower construction, then \nMore generally, each finite formula_13 defines a Dedekind cut on the subset formula_6 (via the total order on formula_7) and the corresponding real number is the standard part of \"u\".\n\nThe standard part function \"st\" is not defined by an internal set. There are several ways of explaining this. Perhaps the simplest is that its domain L, which is the collection of limited (i.e. finite) hyperreals, is not an internal set. Namely, since L is bounded (by any infinite hypernatural, for instance), L would have to have a least upper bound if L were internal, but L doesn't have a least upper bound. Alternatively, the range of \"st\" is formula_16 which is not internal; in fact every internal set in formula_17 which is a subset of formula_8 is necessarily \"finite\", see (Goldblatt, 1998).\n\nAll the traditional notions of calculus are expressed in terms of the standard part function, as follows.\nThe standard part function is used to define the derivative of a function \"f\". If \"f\" is a real function, and \"h\" is infinitesimal, and if \"f\"′(\"x\") exists, then\nAlternatively, if formula_20, one takes an infinitesimal increment formula_21, and computes the corresponding formula_22. One forms the ratio formula_23. The derivative is then defined as the standard part of the ratio:\nGiven a function formula_25 on formula_26, one defines the integral formula_27 as the standard part of an infinite Riemann sum formula_28 when the value of formula_21 is taken to be infinitesimal, exploiting a hyperfinite partition of the interval [a,b].\nGiven a sequence formula_30, its limit is defined by formula_31 where formula_32 is an infinite index. Here the limit is said to exist if the standard part is the same regardless of the infinite index chosen.\nA real function formula_25 is continuous at a real point formula_1 if and only if the composition formula_35 is \"constant\" on the halo of formula_1. See microcontinuity for more details.\n\n\n"}
{"id": "4031908", "url": "https://en.wikipedia.org/wiki?curid=4031908", "title": "Thiele's interpolation formula", "text": "Thiele's interpolation formula\n\nIn mathematics, Thiele's interpolation formula is a formula that defines a rational function formula_1 from a finite set of inputs formula_2 and their function values formula_3. The problem of generating a function whose graph passes through a given set of function values is called interpolation. This interpolation formula is named after the Danish mathematician Thorvald N. Thiele. It is expressed as a continued fraction, where ρ represents the reciprocal difference:\n"}
{"id": "54550463", "url": "https://en.wikipedia.org/wiki?curid=54550463", "title": "William Kolakoski", "text": "William Kolakoski\n\nWilliam George Kolakoski (Sept 17, 1944 – July 26, 1997), known as Bill to family and friends, was an American artist and recreational mathematician who is most famous for devising and giving his name to the Kolakoski sequence, a self-generating sequence of integers that has been extensively studied by mathematicians since he first described it in the \"American Mathematical Monthly\" in 1965.\n\nKolakoski was born Sept 17, 1944, in Pittsburgh, PA, the son of George Leon Kolakoski and his wife Eleanor (née Gale). He had many interests as a boy, including art, philosophy and mathematics, but chose to study fine art at the Carnegie Institute of Technology (CIT) (now Carnegie Mellon University) because he felt that, while he could study mathematics and philosophy independently, he needed the support of others to make a career in art. His fellow students were struck by his sharp intelligence, breadth of knowledge and skills in many different fields, including the ability to play good chess without making a particular study of the game.\n\nHe graduated from CIT with honors as a Bachelor of Fine Arts in painting in 1967 and worked for a time at United States Steel as a draftsman. However, because he suffered from schizophrenia and had to take constant medication to avoid psychosis and delusions, he was unable to keep in steady employment or to develop his artistic career as he wanted. He eventually moved to West Virginia, where he met his wife Loretta and found a position as an artist-in-residence in Fairmont. In 1996, he was diagnosed with lung cancer and he passed away July 26, 1997, at the Fairmont General Hospital.\n\nThis sequence of integers was first discussed by the professional mathematician Rufus Oldenburger in 1939, but attracted little attention at that time. It consists of an infinite series of 1s and 2s that begins like this:\n\nEach symbol occurs in a \"run\" of either one or two consecutive terms and writing down the lengths of these runs gives exactly the same sequence:\n\nConversely, one can say that each term of the Kolakoski sequence generates a run of one or two future terms. The first 1 of the sequence generates a run of \"1\", i.e. itself; the first 2 generates a run of \"22\", which includes itself; the second 2 generates a run of \"11\"; and so on. This animation illustrates the process:\n\nWilliam Kolakoski devised the sequence independently of Oldenburger and introduced it to his fellow students while at the Carnegie Institute of Technology. He submitted it to the \"American Mathematical Monthly\" (AMM) and it was published as \"Advanced Problem 5304\" in the following form:\n\nIt was then called the Kolakoski sequence as mathematicians investigated it further.\n\nDespite the simplicity with which the sequence can be described and generated, it poses several interesting and complex mathematical problems, some of which remain unsolved after more than fifty years of analysis. Until almost the end of his life, Kolakoski himself was not aware of how much attention it had received from professional mathematicians after he had published notice of it in the \"AMM\". However, he eventually received a letter from an architect called William Huff that mentioned the sequence. The letter prompted Loretta Kolakoski to ask her husband's friend Mike Vargo, a writer who had first met him at CIT, to carry out further research when Kolakoski was in hospital during his final illness. Vargo discovered many references to the Kolakoski sequence on the internet and was able to inform his friend before Kolakoski passed away. Vargo felt that Kolakoski had been quietly pleased by the news, feeling that it vindicated his belief in the importance and beauty of the sequence.\n\nBecause he suffered from schizophrenia, Kolakoski was preoccupied with the topics of free will and determinism throughout his life. Despite his high intelligence and ability to master many different skills with little effort, his illness was, in the words of Mike Vargo, \"this thing living within him that was always threatening to literally \"take over\" his mind and transport it into regions of chaos and delusion.\" While wanting to feel himself free, Kolakoski was well aware that he could not control his own brain without pharmaceutical help and was forced to accept determinism. Vargo therefore deduced that his friend searched for a benevolent order in the universe, of which the Kolakoski sequence was one possible expression. The sequence is entirely deterministic, yet behaves in an unpredictable and strangely beautiful way. Kolakoski continued to explore the sequence for many years, creating a corpus of material that is now held as the William Kolakoski Collection at Carnegie Mellon University Libraries and overseen by the mathematician Clark Kimberling.\n\n\n"}
